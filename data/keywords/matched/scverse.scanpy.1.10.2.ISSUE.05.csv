id,quality_attribute,keyword,matched_word,match_idx,sentence,source,author,repo,version,wiki,url
https://github.com/scverse/scanpy/issues/458:252,modifiability,paramet,parameters,252,"Documentation for pl.scatter; Hi,. Some of the parameters documented online for `pl.scatter` are not accepted by the method: `ncols`, `wspace`, `hspace`. And some are not documented: `right_margin`, `left_margin`. The difference seems to come from the parameters described in `{scatter_bulk}` and used for `pl.scatter` documentation. What should be fixed `scatter_bulk` or inputs for `pl.scatter`? Bérénice.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/458
https://github.com/scverse/scanpy/issues/458:373,safety,input,inputs,373,"Documentation for pl.scatter; Hi,. Some of the parameters documented online for `pl.scatter` are not accepted by the method: `ncols`, `wspace`, `hspace`. And some are not documented: `right_margin`, `left_margin`. The difference seems to come from the parameters described in `{scatter_bulk}` and used for `pl.scatter` documentation. What should be fixed `scatter_bulk` or inputs for `pl.scatter`? Bérénice.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/458
https://github.com/scverse/scanpy/issues/458:0,usability,Document,Documentation,0,"Documentation for pl.scatter; Hi,. Some of the parameters documented online for `pl.scatter` are not accepted by the method: `ncols`, `wspace`, `hspace`. And some are not documented: `right_margin`, `left_margin`. The difference seems to come from the parameters described in `{scatter_bulk}` and used for `pl.scatter` documentation. What should be fixed `scatter_bulk` or inputs for `pl.scatter`? Bérénice.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/458
https://github.com/scverse/scanpy/issues/458:58,usability,document,documented,58,"Documentation for pl.scatter; Hi,. Some of the parameters documented online for `pl.scatter` are not accepted by the method: `ncols`, `wspace`, `hspace`. And some are not documented: `right_margin`, `left_margin`. The difference seems to come from the parameters described in `{scatter_bulk}` and used for `pl.scatter` documentation. What should be fixed `scatter_bulk` or inputs for `pl.scatter`? Bérénice.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/458
https://github.com/scverse/scanpy/issues/458:171,usability,document,documented,171,"Documentation for pl.scatter; Hi,. Some of the parameters documented online for `pl.scatter` are not accepted by the method: `ncols`, `wspace`, `hspace`. And some are not documented: `right_margin`, `left_margin`. The difference seems to come from the parameters described in `{scatter_bulk}` and used for `pl.scatter` documentation. What should be fixed `scatter_bulk` or inputs for `pl.scatter`? Bérénice.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/458
https://github.com/scverse/scanpy/issues/458:319,usability,document,documentation,319,"Documentation for pl.scatter; Hi,. Some of the parameters documented online for `pl.scatter` are not accepted by the method: `ncols`, `wspace`, `hspace`. And some are not documented: `right_margin`, `left_margin`. The difference seems to come from the parameters described in `{scatter_bulk}` and used for `pl.scatter` documentation. What should be fixed `scatter_bulk` or inputs for `pl.scatter`? Bérénice.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/458
https://github.com/scverse/scanpy/issues/458:373,usability,input,inputs,373,"Documentation for pl.scatter; Hi,. Some of the parameters documented online for `pl.scatter` are not accepted by the method: `ncols`, `wspace`, `hspace`. And some are not documented: `right_margin`, `left_margin`. The difference seems to come from the parameters described in `{scatter_bulk}` and used for `pl.scatter` documentation. What should be fixed `scatter_bulk` or inputs for `pl.scatter`? Bérénice.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/458
https://github.com/scverse/scanpy/issues/460:996,availability,cluster,cluster,996,"Additional parameters in some functions - new functions; Dear all,. I am writing to ask you some other functionalities. I have just moved from Seurat to Scanpy and I am finding Scanpy a very nice and well done Python package. . 1. I wrote a function to show the 3D plot of the UMAP, tSNE and PCA spaces. In the `scanpy.tl.tsne` function is not possible to change the number of components, it calculates only the first two components, even if the `scanpy.pl.tsne` function has a parameter `component`. May you add a parameter like the `n_components` of the `scanpy.tl.umap` function? 2. In the `rank_genes_groups` function the log2FC values are provided only for ‘t-test’ based methods. May you return the log2FC values (maybe named log2FC) for all the implemented statistical methods? 3. I think that two parameters in the `rank_genes_groups` function should be added. - `min_pCells` to test only the genes that are detected in a minimum fraction of cells of either of the two populations (e.g., cluster 0 vs rest). For instance, min_pCells=0.3 means that at least 30% of the cells must express that gene. - `positive`, if it is True, the function should return only positive marker genes for each population. 4. A function showing the volcano plots (based on the log2FC) can help (I can write it if the log2FC values are provided). Thank you in advance. Best,. Andrea",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460
https://github.com/scverse/scanpy/issues/460:996,deployability,cluster,cluster,996,"Additional parameters in some functions - new functions; Dear all,. I am writing to ask you some other functionalities. I have just moved from Seurat to Scanpy and I am finding Scanpy a very nice and well done Python package. . 1. I wrote a function to show the 3D plot of the UMAP, tSNE and PCA spaces. In the `scanpy.tl.tsne` function is not possible to change the number of components, it calculates only the first two components, even if the `scanpy.pl.tsne` function has a parameter `component`. May you add a parameter like the `n_components` of the `scanpy.tl.umap` function? 2. In the `rank_genes_groups` function the log2FC values are provided only for ‘t-test’ based methods. May you return the log2FC values (maybe named log2FC) for all the implemented statistical methods? 3. I think that two parameters in the `rank_genes_groups` function should be added. - `min_pCells` to test only the genes that are detected in a minimum fraction of cells of either of the two populations (e.g., cluster 0 vs rest). For instance, min_pCells=0.3 means that at least 30% of the cells must express that gene. - `positive`, if it is True, the function should return only positive marker genes for each population. 4. A function showing the volcano plots (based on the log2FC) can help (I can write it if the log2FC values are provided). Thank you in advance. Best,. Andrea",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460
https://github.com/scverse/scanpy/issues/460:377,integrability,compon,components,377,"Additional parameters in some functions - new functions; Dear all,. I am writing to ask you some other functionalities. I have just moved from Seurat to Scanpy and I am finding Scanpy a very nice and well done Python package. . 1. I wrote a function to show the 3D plot of the UMAP, tSNE and PCA spaces. In the `scanpy.tl.tsne` function is not possible to change the number of components, it calculates only the first two components, even if the `scanpy.pl.tsne` function has a parameter `component`. May you add a parameter like the `n_components` of the `scanpy.tl.umap` function? 2. In the `rank_genes_groups` function the log2FC values are provided only for ‘t-test’ based methods. May you return the log2FC values (maybe named log2FC) for all the implemented statistical methods? 3. I think that two parameters in the `rank_genes_groups` function should be added. - `min_pCells` to test only the genes that are detected in a minimum fraction of cells of either of the two populations (e.g., cluster 0 vs rest). For instance, min_pCells=0.3 means that at least 30% of the cells must express that gene. - `positive`, if it is True, the function should return only positive marker genes for each population. 4. A function showing the volcano plots (based on the log2FC) can help (I can write it if the log2FC values are provided). Thank you in advance. Best,. Andrea",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460
https://github.com/scverse/scanpy/issues/460:422,integrability,compon,components,422,"Additional parameters in some functions - new functions; Dear all,. I am writing to ask you some other functionalities. I have just moved from Seurat to Scanpy and I am finding Scanpy a very nice and well done Python package. . 1. I wrote a function to show the 3D plot of the UMAP, tSNE and PCA spaces. In the `scanpy.tl.tsne` function is not possible to change the number of components, it calculates only the first two components, even if the `scanpy.pl.tsne` function has a parameter `component`. May you add a parameter like the `n_components` of the `scanpy.tl.umap` function? 2. In the `rank_genes_groups` function the log2FC values are provided only for ‘t-test’ based methods. May you return the log2FC values (maybe named log2FC) for all the implemented statistical methods? 3. I think that two parameters in the `rank_genes_groups` function should be added. - `min_pCells` to test only the genes that are detected in a minimum fraction of cells of either of the two populations (e.g., cluster 0 vs rest). For instance, min_pCells=0.3 means that at least 30% of the cells must express that gene. - `positive`, if it is True, the function should return only positive marker genes for each population. 4. A function showing the volcano plots (based on the log2FC) can help (I can write it if the log2FC values are provided). Thank you in advance. Best,. Andrea",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460
https://github.com/scverse/scanpy/issues/460:489,integrability,compon,component,489,"Additional parameters in some functions - new functions; Dear all,. I am writing to ask you some other functionalities. I have just moved from Seurat to Scanpy and I am finding Scanpy a very nice and well done Python package. . 1. I wrote a function to show the 3D plot of the UMAP, tSNE and PCA spaces. In the `scanpy.tl.tsne` function is not possible to change the number of components, it calculates only the first two components, even if the `scanpy.pl.tsne` function has a parameter `component`. May you add a parameter like the `n_components` of the `scanpy.tl.umap` function? 2. In the `rank_genes_groups` function the log2FC values are provided only for ‘t-test’ based methods. May you return the log2FC values (maybe named log2FC) for all the implemented statistical methods? 3. I think that two parameters in the `rank_genes_groups` function should be added. - `min_pCells` to test only the genes that are detected in a minimum fraction of cells of either of the two populations (e.g., cluster 0 vs rest). For instance, min_pCells=0.3 means that at least 30% of the cells must express that gene. - `positive`, if it is True, the function should return only positive marker genes for each population. 4. A function showing the volcano plots (based on the log2FC) can help (I can write it if the log2FC values are provided). Thank you in advance. Best,. Andrea",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460
https://github.com/scverse/scanpy/issues/460:377,interoperability,compon,components,377,"Additional parameters in some functions - new functions; Dear all,. I am writing to ask you some other functionalities. I have just moved from Seurat to Scanpy and I am finding Scanpy a very nice and well done Python package. . 1. I wrote a function to show the 3D plot of the UMAP, tSNE and PCA spaces. In the `scanpy.tl.tsne` function is not possible to change the number of components, it calculates only the first two components, even if the `scanpy.pl.tsne` function has a parameter `component`. May you add a parameter like the `n_components` of the `scanpy.tl.umap` function? 2. In the `rank_genes_groups` function the log2FC values are provided only for ‘t-test’ based methods. May you return the log2FC values (maybe named log2FC) for all the implemented statistical methods? 3. I think that two parameters in the `rank_genes_groups` function should be added. - `min_pCells` to test only the genes that are detected in a minimum fraction of cells of either of the two populations (e.g., cluster 0 vs rest). For instance, min_pCells=0.3 means that at least 30% of the cells must express that gene. - `positive`, if it is True, the function should return only positive marker genes for each population. 4. A function showing the volcano plots (based on the log2FC) can help (I can write it if the log2FC values are provided). Thank you in advance. Best,. Andrea",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460
https://github.com/scverse/scanpy/issues/460:422,interoperability,compon,components,422,"Additional parameters in some functions - new functions; Dear all,. I am writing to ask you some other functionalities. I have just moved from Seurat to Scanpy and I am finding Scanpy a very nice and well done Python package. . 1. I wrote a function to show the 3D plot of the UMAP, tSNE and PCA spaces. In the `scanpy.tl.tsne` function is not possible to change the number of components, it calculates only the first two components, even if the `scanpy.pl.tsne` function has a parameter `component`. May you add a parameter like the `n_components` of the `scanpy.tl.umap` function? 2. In the `rank_genes_groups` function the log2FC values are provided only for ‘t-test’ based methods. May you return the log2FC values (maybe named log2FC) for all the implemented statistical methods? 3. I think that two parameters in the `rank_genes_groups` function should be added. - `min_pCells` to test only the genes that are detected in a minimum fraction of cells of either of the two populations (e.g., cluster 0 vs rest). For instance, min_pCells=0.3 means that at least 30% of the cells must express that gene. - `positive`, if it is True, the function should return only positive marker genes for each population. 4. A function showing the volcano plots (based on the log2FC) can help (I can write it if the log2FC values are provided). Thank you in advance. Best,. Andrea",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460
https://github.com/scverse/scanpy/issues/460:489,interoperability,compon,component,489,"Additional parameters in some functions - new functions; Dear all,. I am writing to ask you some other functionalities. I have just moved from Seurat to Scanpy and I am finding Scanpy a very nice and well done Python package. . 1. I wrote a function to show the 3D plot of the UMAP, tSNE and PCA spaces. In the `scanpy.tl.tsne` function is not possible to change the number of components, it calculates only the first two components, even if the `scanpy.pl.tsne` function has a parameter `component`. May you add a parameter like the `n_components` of the `scanpy.tl.umap` function? 2. In the `rank_genes_groups` function the log2FC values are provided only for ‘t-test’ based methods. May you return the log2FC values (maybe named log2FC) for all the implemented statistical methods? 3. I think that two parameters in the `rank_genes_groups` function should be added. - `min_pCells` to test only the genes that are detected in a minimum fraction of cells of either of the two populations (e.g., cluster 0 vs rest). For instance, min_pCells=0.3 means that at least 30% of the cells must express that gene. - `positive`, if it is True, the function should return only positive marker genes for each population. 4. A function showing the volcano plots (based on the log2FC) can help (I can write it if the log2FC values are provided). Thank you in advance. Best,. Andrea",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460
https://github.com/scverse/scanpy/issues/460:11,modifiability,paramet,parameters,11,"Additional parameters in some functions - new functions; Dear all,. I am writing to ask you some other functionalities. I have just moved from Seurat to Scanpy and I am finding Scanpy a very nice and well done Python package. . 1. I wrote a function to show the 3D plot of the UMAP, tSNE and PCA spaces. In the `scanpy.tl.tsne` function is not possible to change the number of components, it calculates only the first two components, even if the `scanpy.pl.tsne` function has a parameter `component`. May you add a parameter like the `n_components` of the `scanpy.tl.umap` function? 2. In the `rank_genes_groups` function the log2FC values are provided only for ‘t-test’ based methods. May you return the log2FC values (maybe named log2FC) for all the implemented statistical methods? 3. I think that two parameters in the `rank_genes_groups` function should be added. - `min_pCells` to test only the genes that are detected in a minimum fraction of cells of either of the two populations (e.g., cluster 0 vs rest). For instance, min_pCells=0.3 means that at least 30% of the cells must express that gene. - `positive`, if it is True, the function should return only positive marker genes for each population. 4. A function showing the volcano plots (based on the log2FC) can help (I can write it if the log2FC values are provided). Thank you in advance. Best,. Andrea",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460
https://github.com/scverse/scanpy/issues/460:217,modifiability,pac,package,217,"Additional parameters in some functions - new functions; Dear all,. I am writing to ask you some other functionalities. I have just moved from Seurat to Scanpy and I am finding Scanpy a very nice and well done Python package. . 1. I wrote a function to show the 3D plot of the UMAP, tSNE and PCA spaces. In the `scanpy.tl.tsne` function is not possible to change the number of components, it calculates only the first two components, even if the `scanpy.pl.tsne` function has a parameter `component`. May you add a parameter like the `n_components` of the `scanpy.tl.umap` function? 2. In the `rank_genes_groups` function the log2FC values are provided only for ‘t-test’ based methods. May you return the log2FC values (maybe named log2FC) for all the implemented statistical methods? 3. I think that two parameters in the `rank_genes_groups` function should be added. - `min_pCells` to test only the genes that are detected in a minimum fraction of cells of either of the two populations (e.g., cluster 0 vs rest). For instance, min_pCells=0.3 means that at least 30% of the cells must express that gene. - `positive`, if it is True, the function should return only positive marker genes for each population. 4. A function showing the volcano plots (based on the log2FC) can help (I can write it if the log2FC values are provided). Thank you in advance. Best,. Andrea",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460
https://github.com/scverse/scanpy/issues/460:377,modifiability,compon,components,377,"Additional parameters in some functions - new functions; Dear all,. I am writing to ask you some other functionalities. I have just moved from Seurat to Scanpy and I am finding Scanpy a very nice and well done Python package. . 1. I wrote a function to show the 3D plot of the UMAP, tSNE and PCA spaces. In the `scanpy.tl.tsne` function is not possible to change the number of components, it calculates only the first two components, even if the `scanpy.pl.tsne` function has a parameter `component`. May you add a parameter like the `n_components` of the `scanpy.tl.umap` function? 2. In the `rank_genes_groups` function the log2FC values are provided only for ‘t-test’ based methods. May you return the log2FC values (maybe named log2FC) for all the implemented statistical methods? 3. I think that two parameters in the `rank_genes_groups` function should be added. - `min_pCells` to test only the genes that are detected in a minimum fraction of cells of either of the two populations (e.g., cluster 0 vs rest). For instance, min_pCells=0.3 means that at least 30% of the cells must express that gene. - `positive`, if it is True, the function should return only positive marker genes for each population. 4. A function showing the volcano plots (based on the log2FC) can help (I can write it if the log2FC values are provided). Thank you in advance. Best,. Andrea",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460
https://github.com/scverse/scanpy/issues/460:422,modifiability,compon,components,422,"Additional parameters in some functions - new functions; Dear all,. I am writing to ask you some other functionalities. I have just moved from Seurat to Scanpy and I am finding Scanpy a very nice and well done Python package. . 1. I wrote a function to show the 3D plot of the UMAP, tSNE and PCA spaces. In the `scanpy.tl.tsne` function is not possible to change the number of components, it calculates only the first two components, even if the `scanpy.pl.tsne` function has a parameter `component`. May you add a parameter like the `n_components` of the `scanpy.tl.umap` function? 2. In the `rank_genes_groups` function the log2FC values are provided only for ‘t-test’ based methods. May you return the log2FC values (maybe named log2FC) for all the implemented statistical methods? 3. I think that two parameters in the `rank_genes_groups` function should be added. - `min_pCells` to test only the genes that are detected in a minimum fraction of cells of either of the two populations (e.g., cluster 0 vs rest). For instance, min_pCells=0.3 means that at least 30% of the cells must express that gene. - `positive`, if it is True, the function should return only positive marker genes for each population. 4. A function showing the volcano plots (based on the log2FC) can help (I can write it if the log2FC values are provided). Thank you in advance. Best,. Andrea",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460
https://github.com/scverse/scanpy/issues/460:478,modifiability,paramet,parameter,478,"Additional parameters in some functions - new functions; Dear all,. I am writing to ask you some other functionalities. I have just moved from Seurat to Scanpy and I am finding Scanpy a very nice and well done Python package. . 1. I wrote a function to show the 3D plot of the UMAP, tSNE and PCA spaces. In the `scanpy.tl.tsne` function is not possible to change the number of components, it calculates only the first two components, even if the `scanpy.pl.tsne` function has a parameter `component`. May you add a parameter like the `n_components` of the `scanpy.tl.umap` function? 2. In the `rank_genes_groups` function the log2FC values are provided only for ‘t-test’ based methods. May you return the log2FC values (maybe named log2FC) for all the implemented statistical methods? 3. I think that two parameters in the `rank_genes_groups` function should be added. - `min_pCells` to test only the genes that are detected in a minimum fraction of cells of either of the two populations (e.g., cluster 0 vs rest). For instance, min_pCells=0.3 means that at least 30% of the cells must express that gene. - `positive`, if it is True, the function should return only positive marker genes for each population. 4. A function showing the volcano plots (based on the log2FC) can help (I can write it if the log2FC values are provided). Thank you in advance. Best,. Andrea",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460
https://github.com/scverse/scanpy/issues/460:489,modifiability,compon,component,489,"Additional parameters in some functions - new functions; Dear all,. I am writing to ask you some other functionalities. I have just moved from Seurat to Scanpy and I am finding Scanpy a very nice and well done Python package. . 1. I wrote a function to show the 3D plot of the UMAP, tSNE and PCA spaces. In the `scanpy.tl.tsne` function is not possible to change the number of components, it calculates only the first two components, even if the `scanpy.pl.tsne` function has a parameter `component`. May you add a parameter like the `n_components` of the `scanpy.tl.umap` function? 2. In the `rank_genes_groups` function the log2FC values are provided only for ‘t-test’ based methods. May you return the log2FC values (maybe named log2FC) for all the implemented statistical methods? 3. I think that two parameters in the `rank_genes_groups` function should be added. - `min_pCells` to test only the genes that are detected in a minimum fraction of cells of either of the two populations (e.g., cluster 0 vs rest). For instance, min_pCells=0.3 means that at least 30% of the cells must express that gene. - `positive`, if it is True, the function should return only positive marker genes for each population. 4. A function showing the volcano plots (based on the log2FC) can help (I can write it if the log2FC values are provided). Thank you in advance. Best,. Andrea",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460
https://github.com/scverse/scanpy/issues/460:515,modifiability,paramet,parameter,515,"Additional parameters in some functions - new functions; Dear all,. I am writing to ask you some other functionalities. I have just moved from Seurat to Scanpy and I am finding Scanpy a very nice and well done Python package. . 1. I wrote a function to show the 3D plot of the UMAP, tSNE and PCA spaces. In the `scanpy.tl.tsne` function is not possible to change the number of components, it calculates only the first two components, even if the `scanpy.pl.tsne` function has a parameter `component`. May you add a parameter like the `n_components` of the `scanpy.tl.umap` function? 2. In the `rank_genes_groups` function the log2FC values are provided only for ‘t-test’ based methods. May you return the log2FC values (maybe named log2FC) for all the implemented statistical methods? 3. I think that two parameters in the `rank_genes_groups` function should be added. - `min_pCells` to test only the genes that are detected in a minimum fraction of cells of either of the two populations (e.g., cluster 0 vs rest). For instance, min_pCells=0.3 means that at least 30% of the cells must express that gene. - `positive`, if it is True, the function should return only positive marker genes for each population. 4. A function showing the volcano plots (based on the log2FC) can help (I can write it if the log2FC values are provided). Thank you in advance. Best,. Andrea",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460
https://github.com/scverse/scanpy/issues/460:805,modifiability,paramet,parameters,805,"Additional parameters in some functions - new functions; Dear all,. I am writing to ask you some other functionalities. I have just moved from Seurat to Scanpy and I am finding Scanpy a very nice and well done Python package. . 1. I wrote a function to show the 3D plot of the UMAP, tSNE and PCA spaces. In the `scanpy.tl.tsne` function is not possible to change the number of components, it calculates only the first two components, even if the `scanpy.pl.tsne` function has a parameter `component`. May you add a parameter like the `n_components` of the `scanpy.tl.umap` function? 2. In the `rank_genes_groups` function the log2FC values are provided only for ‘t-test’ based methods. May you return the log2FC values (maybe named log2FC) for all the implemented statistical methods? 3. I think that two parameters in the `rank_genes_groups` function should be added. - `min_pCells` to test only the genes that are detected in a minimum fraction of cells of either of the two populations (e.g., cluster 0 vs rest). For instance, min_pCells=0.3 means that at least 30% of the cells must express that gene. - `positive`, if it is True, the function should return only positive marker genes for each population. 4. A function showing the volcano plots (based on the log2FC) can help (I can write it if the log2FC values are provided). Thank you in advance. Best,. Andrea",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460
https://github.com/scverse/scanpy/issues/460:665,safety,test,test,665,"Additional parameters in some functions - new functions; Dear all,. I am writing to ask you some other functionalities. I have just moved from Seurat to Scanpy and I am finding Scanpy a very nice and well done Python package. . 1. I wrote a function to show the 3D plot of the UMAP, tSNE and PCA spaces. In the `scanpy.tl.tsne` function is not possible to change the number of components, it calculates only the first two components, even if the `scanpy.pl.tsne` function has a parameter `component`. May you add a parameter like the `n_components` of the `scanpy.tl.umap` function? 2. In the `rank_genes_groups` function the log2FC values are provided only for ‘t-test’ based methods. May you return the log2FC values (maybe named log2FC) for all the implemented statistical methods? 3. I think that two parameters in the `rank_genes_groups` function should be added. - `min_pCells` to test only the genes that are detected in a minimum fraction of cells of either of the two populations (e.g., cluster 0 vs rest). For instance, min_pCells=0.3 means that at least 30% of the cells must express that gene. - `positive`, if it is True, the function should return only positive marker genes for each population. 4. A function showing the volcano plots (based on the log2FC) can help (I can write it if the log2FC values are provided). Thank you in advance. Best,. Andrea",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460
https://github.com/scverse/scanpy/issues/460:887,safety,test,test,887,"Additional parameters in some functions - new functions; Dear all,. I am writing to ask you some other functionalities. I have just moved from Seurat to Scanpy and I am finding Scanpy a very nice and well done Python package. . 1. I wrote a function to show the 3D plot of the UMAP, tSNE and PCA spaces. In the `scanpy.tl.tsne` function is not possible to change the number of components, it calculates only the first two components, even if the `scanpy.pl.tsne` function has a parameter `component`. May you add a parameter like the `n_components` of the `scanpy.tl.umap` function? 2. In the `rank_genes_groups` function the log2FC values are provided only for ‘t-test’ based methods. May you return the log2FC values (maybe named log2FC) for all the implemented statistical methods? 3. I think that two parameters in the `rank_genes_groups` function should be added. - `min_pCells` to test only the genes that are detected in a minimum fraction of cells of either of the two populations (e.g., cluster 0 vs rest). For instance, min_pCells=0.3 means that at least 30% of the cells must express that gene. - `positive`, if it is True, the function should return only positive marker genes for each population. 4. A function showing the volcano plots (based on the log2FC) can help (I can write it if the log2FC values are provided). Thank you in advance. Best,. Andrea",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460
https://github.com/scverse/scanpy/issues/460:916,safety,detect,detected,916,"Additional parameters in some functions - new functions; Dear all,. I am writing to ask you some other functionalities. I have just moved from Seurat to Scanpy and I am finding Scanpy a very nice and well done Python package. . 1. I wrote a function to show the 3D plot of the UMAP, tSNE and PCA spaces. In the `scanpy.tl.tsne` function is not possible to change the number of components, it calculates only the first two components, even if the `scanpy.pl.tsne` function has a parameter `component`. May you add a parameter like the `n_components` of the `scanpy.tl.umap` function? 2. In the `rank_genes_groups` function the log2FC values are provided only for ‘t-test’ based methods. May you return the log2FC values (maybe named log2FC) for all the implemented statistical methods? 3. I think that two parameters in the `rank_genes_groups` function should be added. - `min_pCells` to test only the genes that are detected in a minimum fraction of cells of either of the two populations (e.g., cluster 0 vs rest). For instance, min_pCells=0.3 means that at least 30% of the cells must express that gene. - `positive`, if it is True, the function should return only positive marker genes for each population. 4. A function showing the volcano plots (based on the log2FC) can help (I can write it if the log2FC values are provided). Thank you in advance. Best,. Andrea",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460
https://github.com/scverse/scanpy/issues/460:916,security,detect,detected,916,"Additional parameters in some functions - new functions; Dear all,. I am writing to ask you some other functionalities. I have just moved from Seurat to Scanpy and I am finding Scanpy a very nice and well done Python package. . 1. I wrote a function to show the 3D plot of the UMAP, tSNE and PCA spaces. In the `scanpy.tl.tsne` function is not possible to change the number of components, it calculates only the first two components, even if the `scanpy.pl.tsne` function has a parameter `component`. May you add a parameter like the `n_components` of the `scanpy.tl.umap` function? 2. In the `rank_genes_groups` function the log2FC values are provided only for ‘t-test’ based methods. May you return the log2FC values (maybe named log2FC) for all the implemented statistical methods? 3. I think that two parameters in the `rank_genes_groups` function should be added. - `min_pCells` to test only the genes that are detected in a minimum fraction of cells of either of the two populations (e.g., cluster 0 vs rest). For instance, min_pCells=0.3 means that at least 30% of the cells must express that gene. - `positive`, if it is True, the function should return only positive marker genes for each population. 4. A function showing the volcano plots (based on the log2FC) can help (I can write it if the log2FC values are provided). Thank you in advance. Best,. Andrea",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460
https://github.com/scverse/scanpy/issues/460:665,testability,test,test,665,"Additional parameters in some functions - new functions; Dear all,. I am writing to ask you some other functionalities. I have just moved from Seurat to Scanpy and I am finding Scanpy a very nice and well done Python package. . 1. I wrote a function to show the 3D plot of the UMAP, tSNE and PCA spaces. In the `scanpy.tl.tsne` function is not possible to change the number of components, it calculates only the first two components, even if the `scanpy.pl.tsne` function has a parameter `component`. May you add a parameter like the `n_components` of the `scanpy.tl.umap` function? 2. In the `rank_genes_groups` function the log2FC values are provided only for ‘t-test’ based methods. May you return the log2FC values (maybe named log2FC) for all the implemented statistical methods? 3. I think that two parameters in the `rank_genes_groups` function should be added. - `min_pCells` to test only the genes that are detected in a minimum fraction of cells of either of the two populations (e.g., cluster 0 vs rest). For instance, min_pCells=0.3 means that at least 30% of the cells must express that gene. - `positive`, if it is True, the function should return only positive marker genes for each population. 4. A function showing the volcano plots (based on the log2FC) can help (I can write it if the log2FC values are provided). Thank you in advance. Best,. Andrea",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460
https://github.com/scverse/scanpy/issues/460:887,testability,test,test,887,"Additional parameters in some functions - new functions; Dear all,. I am writing to ask you some other functionalities. I have just moved from Seurat to Scanpy and I am finding Scanpy a very nice and well done Python package. . 1. I wrote a function to show the 3D plot of the UMAP, tSNE and PCA spaces. In the `scanpy.tl.tsne` function is not possible to change the number of components, it calculates only the first two components, even if the `scanpy.pl.tsne` function has a parameter `component`. May you add a parameter like the `n_components` of the `scanpy.tl.umap` function? 2. In the `rank_genes_groups` function the log2FC values are provided only for ‘t-test’ based methods. May you return the log2FC values (maybe named log2FC) for all the implemented statistical methods? 3. I think that two parameters in the `rank_genes_groups` function should be added. - `min_pCells` to test only the genes that are detected in a minimum fraction of cells of either of the two populations (e.g., cluster 0 vs rest). For instance, min_pCells=0.3 means that at least 30% of the cells must express that gene. - `positive`, if it is True, the function should return only positive marker genes for each population. 4. A function showing the volcano plots (based on the log2FC) can help (I can write it if the log2FC values are provided). Thank you in advance. Best,. Andrea",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460
https://github.com/scverse/scanpy/issues/460:930,usability,minim,minimum,930,"Additional parameters in some functions - new functions; Dear all,. I am writing to ask you some other functionalities. I have just moved from Seurat to Scanpy and I am finding Scanpy a very nice and well done Python package. . 1. I wrote a function to show the 3D plot of the UMAP, tSNE and PCA spaces. In the `scanpy.tl.tsne` function is not possible to change the number of components, it calculates only the first two components, even if the `scanpy.pl.tsne` function has a parameter `component`. May you add a parameter like the `n_components` of the `scanpy.tl.umap` function? 2. In the `rank_genes_groups` function the log2FC values are provided only for ‘t-test’ based methods. May you return the log2FC values (maybe named log2FC) for all the implemented statistical methods? 3. I think that two parameters in the `rank_genes_groups` function should be added. - `min_pCells` to test only the genes that are detected in a minimum fraction of cells of either of the two populations (e.g., cluster 0 vs rest). For instance, min_pCells=0.3 means that at least 30% of the cells must express that gene. - `positive`, if it is True, the function should return only positive marker genes for each population. 4. A function showing the volcano plots (based on the log2FC) can help (I can write it if the log2FC values are provided). Thank you in advance. Best,. Andrea",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460
https://github.com/scverse/scanpy/issues/460:1276,usability,help,help,1276,"Additional parameters in some functions - new functions; Dear all,. I am writing to ask you some other functionalities. I have just moved from Seurat to Scanpy and I am finding Scanpy a very nice and well done Python package. . 1. I wrote a function to show the 3D plot of the UMAP, tSNE and PCA spaces. In the `scanpy.tl.tsne` function is not possible to change the number of components, it calculates only the first two components, even if the `scanpy.pl.tsne` function has a parameter `component`. May you add a parameter like the `n_components` of the `scanpy.tl.umap` function? 2. In the `rank_genes_groups` function the log2FC values are provided only for ‘t-test’ based methods. May you return the log2FC values (maybe named log2FC) for all the implemented statistical methods? 3. I think that two parameters in the `rank_genes_groups` function should be added. - `min_pCells` to test only the genes that are detected in a minimum fraction of cells of either of the two populations (e.g., cluster 0 vs rest). For instance, min_pCells=0.3 means that at least 30% of the cells must express that gene. - `positive`, if it is True, the function should return only positive marker genes for each population. 4. A function showing the volcano plots (based on the log2FC) can help (I can write it if the log2FC values are provided). Thank you in advance. Best,. Andrea",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460
https://github.com/scverse/scanpy/pull/461:11,modifiability,paramet,parameters,11,Additional parameters in some functions - new functions #460;,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/461
https://github.com/scverse/scanpy/pull/462:91,availability,sli,slightly,91,"Update numba usage in qc metrics; This is a small update to use numba more effectively and slightly decrease test times for calculating qc metrics. * I've enabled no python mode for `top_segment_proportions_sparse_csr`. * I've removed `numba` from functions currently only used for testing. * This mainly reduces test time. If anyone wants to use the `top_proportions` function to make `plotScater` type plots, maybe these should get re-enabled. Test times are still not great, but since it's due to numba compilation I'm not sure much can be done about it. The ideal solution is it becoming possible to have numba functions which are both parallel and cached.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/462
https://github.com/scverse/scanpy/pull/462:0,deployability,Updat,Update,0,"Update numba usage in qc metrics; This is a small update to use numba more effectively and slightly decrease test times for calculating qc metrics. * I've enabled no python mode for `top_segment_proportions_sparse_csr`. * I've removed `numba` from functions currently only used for testing. * This mainly reduces test time. If anyone wants to use the `top_proportions` function to make `plotScater` type plots, maybe these should get re-enabled. Test times are still not great, but since it's due to numba compilation I'm not sure much can be done about it. The ideal solution is it becoming possible to have numba functions which are both parallel and cached.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/462
https://github.com/scverse/scanpy/pull/462:50,deployability,updat,update,50,"Update numba usage in qc metrics; This is a small update to use numba more effectively and slightly decrease test times for calculating qc metrics. * I've enabled no python mode for `top_segment_proportions_sparse_csr`. * I've removed `numba` from functions currently only used for testing. * This mainly reduces test time. If anyone wants to use the `top_proportions` function to make `plotScater` type plots, maybe these should get re-enabled. Test times are still not great, but since it's due to numba compilation I'm not sure much can be done about it. The ideal solution is it becoming possible to have numba functions which are both parallel and cached.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/462
https://github.com/scverse/scanpy/pull/462:258,energy efficiency,current,currently,258,"Update numba usage in qc metrics; This is a small update to use numba more effectively and slightly decrease test times for calculating qc metrics. * I've enabled no python mode for `top_segment_proportions_sparse_csr`. * I've removed `numba` from functions currently only used for testing. * This mainly reduces test time. If anyone wants to use the `top_proportions` function to make `plotScater` type plots, maybe these should get re-enabled. Test times are still not great, but since it's due to numba compilation I'm not sure much can be done about it. The ideal solution is it becoming possible to have numba functions which are both parallel and cached.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/462
https://github.com/scverse/scanpy/pull/462:305,energy efficiency,reduc,reduces,305,"Update numba usage in qc metrics; This is a small update to use numba more effectively and slightly decrease test times for calculating qc metrics. * I've enabled no python mode for `top_segment_proportions_sparse_csr`. * I've removed `numba` from functions currently only used for testing. * This mainly reduces test time. If anyone wants to use the `top_proportions` function to make `plotScater` type plots, maybe these should get re-enabled. Test times are still not great, but since it's due to numba compilation I'm not sure much can be done about it. The ideal solution is it becoming possible to have numba functions which are both parallel and cached.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/462
https://github.com/scverse/scanpy/pull/462:114,performance,time,times,114,"Update numba usage in qc metrics; This is a small update to use numba more effectively and slightly decrease test times for calculating qc metrics. * I've enabled no python mode for `top_segment_proportions_sparse_csr`. * I've removed `numba` from functions currently only used for testing. * This mainly reduces test time. If anyone wants to use the `top_proportions` function to make `plotScater` type plots, maybe these should get re-enabled. Test times are still not great, but since it's due to numba compilation I'm not sure much can be done about it. The ideal solution is it becoming possible to have numba functions which are both parallel and cached.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/462
https://github.com/scverse/scanpy/pull/462:318,performance,time,time,318,"Update numba usage in qc metrics; This is a small update to use numba more effectively and slightly decrease test times for calculating qc metrics. * I've enabled no python mode for `top_segment_proportions_sparse_csr`. * I've removed `numba` from functions currently only used for testing. * This mainly reduces test time. If anyone wants to use the `top_proportions` function to make `plotScater` type plots, maybe these should get re-enabled. Test times are still not great, but since it's due to numba compilation I'm not sure much can be done about it. The ideal solution is it becoming possible to have numba functions which are both parallel and cached.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/462
https://github.com/scverse/scanpy/pull/462:451,performance,time,times,451,"Update numba usage in qc metrics; This is a small update to use numba more effectively and slightly decrease test times for calculating qc metrics. * I've enabled no python mode for `top_segment_proportions_sparse_csr`. * I've removed `numba` from functions currently only used for testing. * This mainly reduces test time. If anyone wants to use the `top_proportions` function to make `plotScater` type plots, maybe these should get re-enabled. Test times are still not great, but since it's due to numba compilation I'm not sure much can be done about it. The ideal solution is it becoming possible to have numba functions which are both parallel and cached.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/462
https://github.com/scverse/scanpy/pull/462:640,performance,parallel,parallel,640,"Update numba usage in qc metrics; This is a small update to use numba more effectively and slightly decrease test times for calculating qc metrics. * I've enabled no python mode for `top_segment_proportions_sparse_csr`. * I've removed `numba` from functions currently only used for testing. * This mainly reduces test time. If anyone wants to use the `top_proportions` function to make `plotScater` type plots, maybe these should get re-enabled. Test times are still not great, but since it's due to numba compilation I'm not sure much can be done about it. The ideal solution is it becoming possible to have numba functions which are both parallel and cached.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/462
https://github.com/scverse/scanpy/pull/462:653,performance,cach,cached,653,"Update numba usage in qc metrics; This is a small update to use numba more effectively and slightly decrease test times for calculating qc metrics. * I've enabled no python mode for `top_segment_proportions_sparse_csr`. * I've removed `numba` from functions currently only used for testing. * This mainly reduces test time. If anyone wants to use the `top_proportions` function to make `plotScater` type plots, maybe these should get re-enabled. Test times are still not great, but since it's due to numba compilation I'm not sure much can be done about it. The ideal solution is it becoming possible to have numba functions which are both parallel and cached.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/462
https://github.com/scverse/scanpy/pull/462:91,reliability,sli,slightly,91,"Update numba usage in qc metrics; This is a small update to use numba more effectively and slightly decrease test times for calculating qc metrics. * I've enabled no python mode for `top_segment_proportions_sparse_csr`. * I've removed `numba` from functions currently only used for testing. * This mainly reduces test time. If anyone wants to use the `top_proportions` function to make `plotScater` type plots, maybe these should get re-enabled. Test times are still not great, but since it's due to numba compilation I'm not sure much can be done about it. The ideal solution is it becoming possible to have numba functions which are both parallel and cached.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/462
https://github.com/scverse/scanpy/pull/462:0,safety,Updat,Update,0,"Update numba usage in qc metrics; This is a small update to use numba more effectively and slightly decrease test times for calculating qc metrics. * I've enabled no python mode for `top_segment_proportions_sparse_csr`. * I've removed `numba` from functions currently only used for testing. * This mainly reduces test time. If anyone wants to use the `top_proportions` function to make `plotScater` type plots, maybe these should get re-enabled. Test times are still not great, but since it's due to numba compilation I'm not sure much can be done about it. The ideal solution is it becoming possible to have numba functions which are both parallel and cached.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/462
https://github.com/scverse/scanpy/pull/462:50,safety,updat,update,50,"Update numba usage in qc metrics; This is a small update to use numba more effectively and slightly decrease test times for calculating qc metrics. * I've enabled no python mode for `top_segment_proportions_sparse_csr`. * I've removed `numba` from functions currently only used for testing. * This mainly reduces test time. If anyone wants to use the `top_proportions` function to make `plotScater` type plots, maybe these should get re-enabled. Test times are still not great, but since it's due to numba compilation I'm not sure much can be done about it. The ideal solution is it becoming possible to have numba functions which are both parallel and cached.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/462
https://github.com/scverse/scanpy/pull/462:109,safety,test,test,109,"Update numba usage in qc metrics; This is a small update to use numba more effectively and slightly decrease test times for calculating qc metrics. * I've enabled no python mode for `top_segment_proportions_sparse_csr`. * I've removed `numba` from functions currently only used for testing. * This mainly reduces test time. If anyone wants to use the `top_proportions` function to make `plotScater` type plots, maybe these should get re-enabled. Test times are still not great, but since it's due to numba compilation I'm not sure much can be done about it. The ideal solution is it becoming possible to have numba functions which are both parallel and cached.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/462
https://github.com/scverse/scanpy/pull/462:282,safety,test,testing,282,"Update numba usage in qc metrics; This is a small update to use numba more effectively and slightly decrease test times for calculating qc metrics. * I've enabled no python mode for `top_segment_proportions_sparse_csr`. * I've removed `numba` from functions currently only used for testing. * This mainly reduces test time. If anyone wants to use the `top_proportions` function to make `plotScater` type plots, maybe these should get re-enabled. Test times are still not great, but since it's due to numba compilation I'm not sure much can be done about it. The ideal solution is it becoming possible to have numba functions which are both parallel and cached.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/462
https://github.com/scverse/scanpy/pull/462:313,safety,test,test,313,"Update numba usage in qc metrics; This is a small update to use numba more effectively and slightly decrease test times for calculating qc metrics. * I've enabled no python mode for `top_segment_proportions_sparse_csr`. * I've removed `numba` from functions currently only used for testing. * This mainly reduces test time. If anyone wants to use the `top_proportions` function to make `plotScater` type plots, maybe these should get re-enabled. Test times are still not great, but since it's due to numba compilation I'm not sure much can be done about it. The ideal solution is it becoming possible to have numba functions which are both parallel and cached.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/462
https://github.com/scverse/scanpy/pull/462:446,safety,Test,Test,446,"Update numba usage in qc metrics; This is a small update to use numba more effectively and slightly decrease test times for calculating qc metrics. * I've enabled no python mode for `top_segment_proportions_sparse_csr`. * I've removed `numba` from functions currently only used for testing. * This mainly reduces test time. If anyone wants to use the `top_proportions` function to make `plotScater` type plots, maybe these should get re-enabled. Test times are still not great, but since it's due to numba compilation I'm not sure much can be done about it. The ideal solution is it becoming possible to have numba functions which are both parallel and cached.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/462
https://github.com/scverse/scanpy/pull/462:0,security,Updat,Update,0,"Update numba usage in qc metrics; This is a small update to use numba more effectively and slightly decrease test times for calculating qc metrics. * I've enabled no python mode for `top_segment_proportions_sparse_csr`. * I've removed `numba` from functions currently only used for testing. * This mainly reduces test time. If anyone wants to use the `top_proportions` function to make `plotScater` type plots, maybe these should get re-enabled. Test times are still not great, but since it's due to numba compilation I'm not sure much can be done about it. The ideal solution is it becoming possible to have numba functions which are both parallel and cached.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/462
https://github.com/scverse/scanpy/pull/462:50,security,updat,update,50,"Update numba usage in qc metrics; This is a small update to use numba more effectively and slightly decrease test times for calculating qc metrics. * I've enabled no python mode for `top_segment_proportions_sparse_csr`. * I've removed `numba` from functions currently only used for testing. * This mainly reduces test time. If anyone wants to use the `top_proportions` function to make `plotScater` type plots, maybe these should get re-enabled. Test times are still not great, but since it's due to numba compilation I'm not sure much can be done about it. The ideal solution is it becoming possible to have numba functions which are both parallel and cached.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/462
https://github.com/scverse/scanpy/pull/462:109,testability,test,test,109,"Update numba usage in qc metrics; This is a small update to use numba more effectively and slightly decrease test times for calculating qc metrics. * I've enabled no python mode for `top_segment_proportions_sparse_csr`. * I've removed `numba` from functions currently only used for testing. * This mainly reduces test time. If anyone wants to use the `top_proportions` function to make `plotScater` type plots, maybe these should get re-enabled. Test times are still not great, but since it's due to numba compilation I'm not sure much can be done about it. The ideal solution is it becoming possible to have numba functions which are both parallel and cached.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/462
https://github.com/scverse/scanpy/pull/462:282,testability,test,testing,282,"Update numba usage in qc metrics; This is a small update to use numba more effectively and slightly decrease test times for calculating qc metrics. * I've enabled no python mode for `top_segment_proportions_sparse_csr`. * I've removed `numba` from functions currently only used for testing. * This mainly reduces test time. If anyone wants to use the `top_proportions` function to make `plotScater` type plots, maybe these should get re-enabled. Test times are still not great, but since it's due to numba compilation I'm not sure much can be done about it. The ideal solution is it becoming possible to have numba functions which are both parallel and cached.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/462
https://github.com/scverse/scanpy/pull/462:313,testability,test,test,313,"Update numba usage in qc metrics; This is a small update to use numba more effectively and slightly decrease test times for calculating qc metrics. * I've enabled no python mode for `top_segment_proportions_sparse_csr`. * I've removed `numba` from functions currently only used for testing. * This mainly reduces test time. If anyone wants to use the `top_proportions` function to make `plotScater` type plots, maybe these should get re-enabled. Test times are still not great, but since it's due to numba compilation I'm not sure much can be done about it. The ideal solution is it becoming possible to have numba functions which are both parallel and cached.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/462
https://github.com/scverse/scanpy/pull/462:446,testability,Test,Test,446,"Update numba usage in qc metrics; This is a small update to use numba more effectively and slightly decrease test times for calculating qc metrics. * I've enabled no python mode for `top_segment_proportions_sparse_csr`. * I've removed `numba` from functions currently only used for testing. * This mainly reduces test time. If anyone wants to use the `top_proportions` function to make `plotScater` type plots, maybe these should get re-enabled. Test times are still not great, but since it's due to numba compilation I'm not sure much can be done about it. The ideal solution is it becoming possible to have numba functions which are both parallel and cached.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/462
https://github.com/scverse/scanpy/pull/462:75,usability,effectiv,effectively,75,"Update numba usage in qc metrics; This is a small update to use numba more effectively and slightly decrease test times for calculating qc metrics. * I've enabled no python mode for `top_segment_proportions_sparse_csr`. * I've removed `numba` from functions currently only used for testing. * This mainly reduces test time. If anyone wants to use the `top_proportions` function to make `plotScater` type plots, maybe these should get re-enabled. Test times are still not great, but since it's due to numba compilation I'm not sure much can be done about it. The ideal solution is it becoming possible to have numba functions which are both parallel and cached.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/462
https://github.com/scverse/scanpy/issues/463:0,interoperability,Convers,Conversion,0,"Conversion to/from ExpressionSet (R)?; Great package! . Since the AnnData class structure is similar to R's ExpressionSet and many single-cell programs are developed in R, I am wondering if there is an EASY way to read/write the objects from python to R and vice versa.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/463
https://github.com/scverse/scanpy/issues/463:45,modifiability,pac,package,45,"Conversion to/from ExpressionSet (R)?; Great package! . Since the AnnData class structure is similar to R's ExpressionSet and many single-cell programs are developed in R, I am wondering if there is an EASY way to read/write the objects from python to R and vice versa.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/463
https://github.com/scverse/scanpy/issues/464:32,deployability,API,APIs,32,Transition away from positional APIs; We never wanted APIs that can be used with more that ~2 positional parameters. We should go to keyword-only-parameters. This can be done via [legacy-api-wrap](https://github.com/flying-sheep/legacy-api-wrap),MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/464
https://github.com/scverse/scanpy/issues/464:54,deployability,API,APIs,54,Transition away from positional APIs; We never wanted APIs that can be used with more that ~2 positional parameters. We should go to keyword-only-parameters. This can be done via [legacy-api-wrap](https://github.com/flying-sheep/legacy-api-wrap),MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/464
https://github.com/scverse/scanpy/issues/464:187,deployability,api,api-wrap,187,Transition away from positional APIs; We never wanted APIs that can be used with more that ~2 positional parameters. We should go to keyword-only-parameters. This can be done via [legacy-api-wrap](https://github.com/flying-sheep/legacy-api-wrap),MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/464
https://github.com/scverse/scanpy/issues/464:236,deployability,api,api-wrap,236,Transition away from positional APIs; We never wanted APIs that can be used with more that ~2 positional parameters. We should go to keyword-only-parameters. This can be done via [legacy-api-wrap](https://github.com/flying-sheep/legacy-api-wrap),MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/464
https://github.com/scverse/scanpy/issues/464:32,integrability,API,APIs,32,Transition away from positional APIs; We never wanted APIs that can be used with more that ~2 positional parameters. We should go to keyword-only-parameters. This can be done via [legacy-api-wrap](https://github.com/flying-sheep/legacy-api-wrap),MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/464
https://github.com/scverse/scanpy/issues/464:54,integrability,API,APIs,54,Transition away from positional APIs; We never wanted APIs that can be used with more that ~2 positional parameters. We should go to keyword-only-parameters. This can be done via [legacy-api-wrap](https://github.com/flying-sheep/legacy-api-wrap),MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/464
https://github.com/scverse/scanpy/issues/464:187,integrability,api,api-wrap,187,Transition away from positional APIs; We never wanted APIs that can be used with more that ~2 positional parameters. We should go to keyword-only-parameters. This can be done via [legacy-api-wrap](https://github.com/flying-sheep/legacy-api-wrap),MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/464
https://github.com/scverse/scanpy/issues/464:236,integrability,api,api-wrap,236,Transition away from positional APIs; We never wanted APIs that can be used with more that ~2 positional parameters. We should go to keyword-only-parameters. This can be done via [legacy-api-wrap](https://github.com/flying-sheep/legacy-api-wrap),MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/464
https://github.com/scverse/scanpy/issues/464:32,interoperability,API,APIs,32,Transition away from positional APIs; We never wanted APIs that can be used with more that ~2 positional parameters. We should go to keyword-only-parameters. This can be done via [legacy-api-wrap](https://github.com/flying-sheep/legacy-api-wrap),MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/464
https://github.com/scverse/scanpy/issues/464:54,interoperability,API,APIs,54,Transition away from positional APIs; We never wanted APIs that can be used with more that ~2 positional parameters. We should go to keyword-only-parameters. This can be done via [legacy-api-wrap](https://github.com/flying-sheep/legacy-api-wrap),MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/464
https://github.com/scverse/scanpy/issues/464:187,interoperability,api,api-wrap,187,Transition away from positional APIs; We never wanted APIs that can be used with more that ~2 positional parameters. We should go to keyword-only-parameters. This can be done via [legacy-api-wrap](https://github.com/flying-sheep/legacy-api-wrap),MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/464
https://github.com/scverse/scanpy/issues/464:236,interoperability,api,api-wrap,236,Transition away from positional APIs; We never wanted APIs that can be used with more that ~2 positional parameters. We should go to keyword-only-parameters. This can be done via [legacy-api-wrap](https://github.com/flying-sheep/legacy-api-wrap),MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/464
https://github.com/scverse/scanpy/issues/464:105,modifiability,paramet,parameters,105,Transition away from positional APIs; We never wanted APIs that can be used with more that ~2 positional parameters. We should go to keyword-only-parameters. This can be done via [legacy-api-wrap](https://github.com/flying-sheep/legacy-api-wrap),MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/464
https://github.com/scverse/scanpy/issues/464:146,modifiability,paramet,parameters,146,Transition away from positional APIs; We never wanted APIs that can be used with more that ~2 positional parameters. We should go to keyword-only-parameters. This can be done via [legacy-api-wrap](https://github.com/flying-sheep/legacy-api-wrap),MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/464
https://github.com/scverse/scanpy/issues/465:232,energy efficiency,Current,Currently,232,"sc.pl.stacked_violin and rank_genes_groups_stacked_violin `column_palette`; Hi,. When using the sc.pl.stacked_violin and sc.pl.rank_genes_groups_stacked_violin functions, would it be possible to implement a `column_palette` option? Currently the `row_palette` plots colors according to the genes, which is really helpful to colorize genes by sets. Still, it would be interesting to plot the violin colors according to the groups using a `column_palette` option. I guess that one option should override the other.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/465
https://github.com/scverse/scanpy/issues/465:313,usability,help,helpful,313,"sc.pl.stacked_violin and rank_genes_groups_stacked_violin `column_palette`; Hi,. When using the sc.pl.stacked_violin and sc.pl.rank_genes_groups_stacked_violin functions, would it be possible to implement a `column_palette` option? Currently the `row_palette` plots colors according to the genes, which is really helpful to colorize genes by sets. Still, it would be interesting to plot the violin colors according to the groups using a `column_palette` option. I guess that one option should override the other.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/465
https://github.com/scverse/scanpy/pull/466:13,integrability,transform,transform,13,test for pca transform; Adding test for pca.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/466
https://github.com/scverse/scanpy/pull/466:13,interoperability,transform,transform,13,test for pca transform; Adding test for pca.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/466
https://github.com/scverse/scanpy/pull/466:0,safety,test,test,0,test for pca transform; Adding test for pca.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/466
https://github.com/scverse/scanpy/pull/466:31,safety,test,test,31,test for pca transform; Adding test for pca.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/466
https://github.com/scverse/scanpy/pull/466:0,testability,test,test,0,test for pca transform; Adding test for pca.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/466
https://github.com/scverse/scanpy/pull/466:31,testability,test,test,31,test for pca transform; Adding test for pca.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/466
https://github.com/scverse/scanpy/pull/467:247,availability,mainten,maintenance,247,"Queries fix; Following up on #242. Here's my solution to the current queries being pretty unreliable for me (due to issue with bioservices module). It's all a pretty thin wrapper around `pybiomart`, which has a nice API and is well tested but has maintenance issues. . Currently I've replaced the `gene_coordinates` query with a more generic `biomart_annotations` – the example covers the functionality of `gene_coordinates`. I'm debating how to add tests given that they're network based (could fail when nothing is wrong with the code) and can take a while.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/467
https://github.com/scverse/scanpy/pull/467:139,deployability,modul,module,139,"Queries fix; Following up on #242. Here's my solution to the current queries being pretty unreliable for me (due to issue with bioservices module). It's all a pretty thin wrapper around `pybiomart`, which has a nice API and is well tested but has maintenance issues. . Currently I've replaced the `gene_coordinates` query with a more generic `biomart_annotations` – the example covers the functionality of `gene_coordinates`. I'm debating how to add tests given that they're network based (could fail when nothing is wrong with the code) and can take a while.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/467
https://github.com/scverse/scanpy/pull/467:216,deployability,API,API,216,"Queries fix; Following up on #242. Here's my solution to the current queries being pretty unreliable for me (due to issue with bioservices module). It's all a pretty thin wrapper around `pybiomart`, which has a nice API and is well tested but has maintenance issues. . Currently I've replaced the `gene_coordinates` query with a more generic `biomart_annotations` – the example covers the functionality of `gene_coordinates`. I'm debating how to add tests given that they're network based (could fail when nothing is wrong with the code) and can take a while.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/467
https://github.com/scverse/scanpy/pull/467:496,deployability,fail,fail,496,"Queries fix; Following up on #242. Here's my solution to the current queries being pretty unreliable for me (due to issue with bioservices module). It's all a pretty thin wrapper around `pybiomart`, which has a nice API and is well tested but has maintenance issues. . Currently I've replaced the `gene_coordinates` query with a more generic `biomart_annotations` – the example covers the functionality of `gene_coordinates`. I'm debating how to add tests given that they're network based (could fail when nothing is wrong with the code) and can take a while.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/467
https://github.com/scverse/scanpy/pull/467:61,energy efficiency,current,current,61,"Queries fix; Following up on #242. Here's my solution to the current queries being pretty unreliable for me (due to issue with bioservices module). It's all a pretty thin wrapper around `pybiomart`, which has a nice API and is well tested but has maintenance issues. . Currently I've replaced the `gene_coordinates` query with a more generic `biomart_annotations` – the example covers the functionality of `gene_coordinates`. I'm debating how to add tests given that they're network based (could fail when nothing is wrong with the code) and can take a while.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/467
https://github.com/scverse/scanpy/pull/467:269,energy efficiency,Current,Currently,269,"Queries fix; Following up on #242. Here's my solution to the current queries being pretty unreliable for me (due to issue with bioservices module). It's all a pretty thin wrapper around `pybiomart`, which has a nice API and is well tested but has maintenance issues. . Currently I've replaced the `gene_coordinates` query with a more generic `biomart_annotations` – the example covers the functionality of `gene_coordinates`. I'm debating how to add tests given that they're network based (could fail when nothing is wrong with the code) and can take a while.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/467
https://github.com/scverse/scanpy/pull/467:171,integrability,wrap,wrapper,171,"Queries fix; Following up on #242. Here's my solution to the current queries being pretty unreliable for me (due to issue with bioservices module). It's all a pretty thin wrapper around `pybiomart`, which has a nice API and is well tested but has maintenance issues. . Currently I've replaced the `gene_coordinates` query with a more generic `biomart_annotations` – the example covers the functionality of `gene_coordinates`. I'm debating how to add tests given that they're network based (could fail when nothing is wrong with the code) and can take a while.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/467
https://github.com/scverse/scanpy/pull/467:216,integrability,API,API,216,"Queries fix; Following up on #242. Here's my solution to the current queries being pretty unreliable for me (due to issue with bioservices module). It's all a pretty thin wrapper around `pybiomart`, which has a nice API and is well tested but has maintenance issues. . Currently I've replaced the `gene_coordinates` query with a more generic `biomart_annotations` – the example covers the functionality of `gene_coordinates`. I'm debating how to add tests given that they're network based (could fail when nothing is wrong with the code) and can take a while.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/467
https://github.com/scverse/scanpy/pull/467:171,interoperability,wrapper,wrapper,171,"Queries fix; Following up on #242. Here's my solution to the current queries being pretty unreliable for me (due to issue with bioservices module). It's all a pretty thin wrapper around `pybiomart`, which has a nice API and is well tested but has maintenance issues. . Currently I've replaced the `gene_coordinates` query with a more generic `biomart_annotations` – the example covers the functionality of `gene_coordinates`. I'm debating how to add tests given that they're network based (could fail when nothing is wrong with the code) and can take a while.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/467
https://github.com/scverse/scanpy/pull/467:216,interoperability,API,API,216,"Queries fix; Following up on #242. Here's my solution to the current queries being pretty unreliable for me (due to issue with bioservices module). It's all a pretty thin wrapper around `pybiomart`, which has a nice API and is well tested but has maintenance issues. . Currently I've replaced the `gene_coordinates` query with a more generic `biomart_annotations` – the example covers the functionality of `gene_coordinates`. I'm debating how to add tests given that they're network based (could fail when nothing is wrong with the code) and can take a while.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/467
https://github.com/scverse/scanpy/pull/467:139,modifiability,modul,module,139,"Queries fix; Following up on #242. Here's my solution to the current queries being pretty unreliable for me (due to issue with bioservices module). It's all a pretty thin wrapper around `pybiomart`, which has a nice API and is well tested but has maintenance issues. . Currently I've replaced the `gene_coordinates` query with a more generic `biomart_annotations` – the example covers the functionality of `gene_coordinates`. I'm debating how to add tests given that they're network based (could fail when nothing is wrong with the code) and can take a while.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/467
https://github.com/scverse/scanpy/pull/467:475,performance,network,network,475,"Queries fix; Following up on #242. Here's my solution to the current queries being pretty unreliable for me (due to issue with bioservices module). It's all a pretty thin wrapper around `pybiomart`, which has a nice API and is well tested but has maintenance issues. . Currently I've replaced the `gene_coordinates` query with a more generic `biomart_annotations` – the example covers the functionality of `gene_coordinates`. I'm debating how to add tests given that they're network based (could fail when nothing is wrong with the code) and can take a while.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/467
https://github.com/scverse/scanpy/pull/467:247,reliability,mainten,maintenance,247,"Queries fix; Following up on #242. Here's my solution to the current queries being pretty unreliable for me (due to issue with bioservices module). It's all a pretty thin wrapper around `pybiomart`, which has a nice API and is well tested but has maintenance issues. . Currently I've replaced the `gene_coordinates` query with a more generic `biomart_annotations` – the example covers the functionality of `gene_coordinates`. I'm debating how to add tests given that they're network based (could fail when nothing is wrong with the code) and can take a while.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/467
https://github.com/scverse/scanpy/pull/467:496,reliability,fail,fail,496,"Queries fix; Following up on #242. Here's my solution to the current queries being pretty unreliable for me (due to issue with bioservices module). It's all a pretty thin wrapper around `pybiomart`, which has a nice API and is well tested but has maintenance issues. . Currently I've replaced the `gene_coordinates` query with a more generic `biomart_annotations` – the example covers the functionality of `gene_coordinates`. I'm debating how to add tests given that they're network based (could fail when nothing is wrong with the code) and can take a while.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/467
https://github.com/scverse/scanpy/pull/467:139,safety,modul,module,139,"Queries fix; Following up on #242. Here's my solution to the current queries being pretty unreliable for me (due to issue with bioservices module). It's all a pretty thin wrapper around `pybiomart`, which has a nice API and is well tested but has maintenance issues. . Currently I've replaced the `gene_coordinates` query with a more generic `biomart_annotations` – the example covers the functionality of `gene_coordinates`. I'm debating how to add tests given that they're network based (could fail when nothing is wrong with the code) and can take a while.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/467
https://github.com/scverse/scanpy/pull/467:232,safety,test,tested,232,"Queries fix; Following up on #242. Here's my solution to the current queries being pretty unreliable for me (due to issue with bioservices module). It's all a pretty thin wrapper around `pybiomart`, which has a nice API and is well tested but has maintenance issues. . Currently I've replaced the `gene_coordinates` query with a more generic `biomart_annotations` – the example covers the functionality of `gene_coordinates`. I'm debating how to add tests given that they're network based (could fail when nothing is wrong with the code) and can take a while.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/467
https://github.com/scverse/scanpy/pull/467:450,safety,test,tests,450,"Queries fix; Following up on #242. Here's my solution to the current queries being pretty unreliable for me (due to issue with bioservices module). It's all a pretty thin wrapper around `pybiomart`, which has a nice API and is well tested but has maintenance issues. . Currently I've replaced the `gene_coordinates` query with a more generic `biomart_annotations` – the example covers the functionality of `gene_coordinates`. I'm debating how to add tests given that they're network based (could fail when nothing is wrong with the code) and can take a while.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/467
https://github.com/scverse/scanpy/pull/467:475,security,network,network,475,"Queries fix; Following up on #242. Here's my solution to the current queries being pretty unreliable for me (due to issue with bioservices module). It's all a pretty thin wrapper around `pybiomart`, which has a nice API and is well tested but has maintenance issues. . Currently I've replaced the `gene_coordinates` query with a more generic `biomart_annotations` – the example covers the functionality of `gene_coordinates`. I'm debating how to add tests given that they're network based (could fail when nothing is wrong with the code) and can take a while.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/467
https://github.com/scverse/scanpy/pull/467:232,testability,test,tested,232,"Queries fix; Following up on #242. Here's my solution to the current queries being pretty unreliable for me (due to issue with bioservices module). It's all a pretty thin wrapper around `pybiomart`, which has a nice API and is well tested but has maintenance issues. . Currently I've replaced the `gene_coordinates` query with a more generic `biomart_annotations` – the example covers the functionality of `gene_coordinates`. I'm debating how to add tests given that they're network based (could fail when nothing is wrong with the code) and can take a while.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/467
https://github.com/scverse/scanpy/pull/467:450,testability,test,tests,450,"Queries fix; Following up on #242. Here's my solution to the current queries being pretty unreliable for me (due to issue with bioservices module). It's all a pretty thin wrapper around `pybiomart`, which has a nice API and is well tested but has maintenance issues. . Currently I've replaced the `gene_coordinates` query with a more generic `biomart_annotations` – the example covers the functionality of `gene_coordinates`. I'm debating how to add tests given that they're network based (could fail when nothing is wrong with the code) and can take a while.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/467
https://github.com/scverse/scanpy/issues/468:36,availability,slo,slows,36,"scanpy cannot be reloaded (and this slows development); Hi Alex, . Here is an interesting bug with scanpy. For developers, it is useful to be able to reload a previously imported module within the environment containing useful variables and data for testing (a sample scRNA dataset) after changing scanpy's source code. However, scanpy cannot be reloaded. This means that to test, one has to stop the kernel, restart, reload all of the data needed for a plot and then test a plotting function that was just modified (for instance). . Here is a way to demonstrate the reload failure easily:. 1. open utils.py and add the print statement to track the descend_classes_and_funcs() function. ```py. #utils.py. def annotate_doc_types(mod: ModuleType, root: str):. for c_or_f in descend_classes_and_funcs(mod, root):. print(c_or_f) #added line to track descend_classes_and_funcs() function--TR. c_or_f.getdoc = partial(getdoc, c_or_f). ```. 2. open ipython. ```py. import scanpy as sc. # prints out a bunch of function names from the descend_classes_and_funcs() function. import importlib. importlib.reload(sc). # endless loop of function names from the descend_classes_and_funcs() function. # due to recursive yield statement. ```. So what is the purpose of this function? And can it be altered to allow reload? It is called when __init__.py is run by sc.annotate_doc_types(sys.modules[__name__], 'scanpy'). . ```py. #utils.py. def descend_classes_and_funcs(mod: ModuleType, root: str):. for obj in vars(mod).values():. if not getattr(obj, '__module__', getattr(obj, '__qualname__', getattr(obj, '__name__', ''))).startswith(root):. continue. if isinstance(obj, Callable):. yield obj. if isinstance(obj, type):. yield from (m for m in vars(obj).values() if isinstance(m, Callable)). elif isinstance(obj, ModuleType):. yield from descend_classes_and_funcs(obj, root). ```. _________________________________________________________. It is possible to remove the scanpy manually by:. ```py. import sys. sys.mo",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/468
https://github.com/scverse/scanpy/issues/468:574,availability,failur,failure,574,"scanpy cannot be reloaded (and this slows development); Hi Alex, . Here is an interesting bug with scanpy. For developers, it is useful to be able to reload a previously imported module within the environment containing useful variables and data for testing (a sample scRNA dataset) after changing scanpy's source code. However, scanpy cannot be reloaded. This means that to test, one has to stop the kernel, restart, reload all of the data needed for a plot and then test a plotting function that was just modified (for instance). . Here is a way to demonstrate the reload failure easily:. 1. open utils.py and add the print statement to track the descend_classes_and_funcs() function. ```py. #utils.py. def annotate_doc_types(mod: ModuleType, root: str):. for c_or_f in descend_classes_and_funcs(mod, root):. print(c_or_f) #added line to track descend_classes_and_funcs() function--TR. c_or_f.getdoc = partial(getdoc, c_or_f). ```. 2. open ipython. ```py. import scanpy as sc. # prints out a bunch of function names from the descend_classes_and_funcs() function. import importlib. importlib.reload(sc). # endless loop of function names from the descend_classes_and_funcs() function. # due to recursive yield statement. ```. So what is the purpose of this function? And can it be altered to allow reload? It is called when __init__.py is run by sc.annotate_doc_types(sys.modules[__name__], 'scanpy'). . ```py. #utils.py. def descend_classes_and_funcs(mod: ModuleType, root: str):. for obj in vars(mod).values():. if not getattr(obj, '__module__', getattr(obj, '__qualname__', getattr(obj, '__name__', ''))).startswith(root):. continue. if isinstance(obj, Callable):. yield obj. if isinstance(obj, type):. yield from (m for m in vars(obj).values() if isinstance(m, Callable)). elif isinstance(obj, ModuleType):. yield from descend_classes_and_funcs(obj, root). ```. _________________________________________________________. It is possible to remove the scanpy manually by:. ```py. import sys. sys.mo",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/468
https://github.com/scverse/scanpy/issues/468:626,availability,state,statement,626,"scanpy cannot be reloaded (and this slows development); Hi Alex, . Here is an interesting bug with scanpy. For developers, it is useful to be able to reload a previously imported module within the environment containing useful variables and data for testing (a sample scRNA dataset) after changing scanpy's source code. However, scanpy cannot be reloaded. This means that to test, one has to stop the kernel, restart, reload all of the data needed for a plot and then test a plotting function that was just modified (for instance). . Here is a way to demonstrate the reload failure easily:. 1. open utils.py and add the print statement to track the descend_classes_and_funcs() function. ```py. #utils.py. def annotate_doc_types(mod: ModuleType, root: str):. for c_or_f in descend_classes_and_funcs(mod, root):. print(c_or_f) #added line to track descend_classes_and_funcs() function--TR. c_or_f.getdoc = partial(getdoc, c_or_f). ```. 2. open ipython. ```py. import scanpy as sc. # prints out a bunch of function names from the descend_classes_and_funcs() function. import importlib. importlib.reload(sc). # endless loop of function names from the descend_classes_and_funcs() function. # due to recursive yield statement. ```. So what is the purpose of this function? And can it be altered to allow reload? It is called when __init__.py is run by sc.annotate_doc_types(sys.modules[__name__], 'scanpy'). . ```py. #utils.py. def descend_classes_and_funcs(mod: ModuleType, root: str):. for obj in vars(mod).values():. if not getattr(obj, '__module__', getattr(obj, '__qualname__', getattr(obj, '__name__', ''))).startswith(root):. continue. if isinstance(obj, Callable):. yield obj. if isinstance(obj, type):. yield from (m for m in vars(obj).values() if isinstance(m, Callable)). elif isinstance(obj, ModuleType):. yield from descend_classes_and_funcs(obj, root). ```. _________________________________________________________. It is possible to remove the scanpy manually by:. ```py. import sys. sys.mo",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/468
https://github.com/scverse/scanpy/issues/468:1210,availability,state,statement,1210,"the environment containing useful variables and data for testing (a sample scRNA dataset) after changing scanpy's source code. However, scanpy cannot be reloaded. This means that to test, one has to stop the kernel, restart, reload all of the data needed for a plot and then test a plotting function that was just modified (for instance). . Here is a way to demonstrate the reload failure easily:. 1. open utils.py and add the print statement to track the descend_classes_and_funcs() function. ```py. #utils.py. def annotate_doc_types(mod: ModuleType, root: str):. for c_or_f in descend_classes_and_funcs(mod, root):. print(c_or_f) #added line to track descend_classes_and_funcs() function--TR. c_or_f.getdoc = partial(getdoc, c_or_f). ```. 2. open ipython. ```py. import scanpy as sc. # prints out a bunch of function names from the descend_classes_and_funcs() function. import importlib. importlib.reload(sc). # endless loop of function names from the descend_classes_and_funcs() function. # due to recursive yield statement. ```. So what is the purpose of this function? And can it be altered to allow reload? It is called when __init__.py is run by sc.annotate_doc_types(sys.modules[__name__], 'scanpy'). . ```py. #utils.py. def descend_classes_and_funcs(mod: ModuleType, root: str):. for obj in vars(mod).values():. if not getattr(obj, '__module__', getattr(obj, '__qualname__', getattr(obj, '__name__', ''))).startswith(root):. continue. if isinstance(obj, Callable):. yield obj. if isinstance(obj, type):. yield from (m for m in vars(obj).values() if isinstance(m, Callable)). elif isinstance(obj, ModuleType):. yield from descend_classes_and_funcs(obj, root). ```. _________________________________________________________. It is possible to remove the scanpy manually by:. ```py. import sys. sys.modules.pop('scanpy'). ```. and then import scanpy from scratch. Is that the preferred way to re-import scanpy for developers working to enhance scanpy package? Thank you for your work on scanpy!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/468
https://github.com/scverse/scanpy/issues/468:179,deployability,modul,module,179,"scanpy cannot be reloaded (and this slows development); Hi Alex, . Here is an interesting bug with scanpy. For developers, it is useful to be able to reload a previously imported module within the environment containing useful variables and data for testing (a sample scRNA dataset) after changing scanpy's source code. However, scanpy cannot be reloaded. This means that to test, one has to stop the kernel, restart, reload all of the data needed for a plot and then test a plotting function that was just modified (for instance). . Here is a way to demonstrate the reload failure easily:. 1. open utils.py and add the print statement to track the descend_classes_and_funcs() function. ```py. #utils.py. def annotate_doc_types(mod: ModuleType, root: str):. for c_or_f in descend_classes_and_funcs(mod, root):. print(c_or_f) #added line to track descend_classes_and_funcs() function--TR. c_or_f.getdoc = partial(getdoc, c_or_f). ```. 2. open ipython. ```py. import scanpy as sc. # prints out a bunch of function names from the descend_classes_and_funcs() function. import importlib. importlib.reload(sc). # endless loop of function names from the descend_classes_and_funcs() function. # due to recursive yield statement. ```. So what is the purpose of this function? And can it be altered to allow reload? It is called when __init__.py is run by sc.annotate_doc_types(sys.modules[__name__], 'scanpy'). . ```py. #utils.py. def descend_classes_and_funcs(mod: ModuleType, root: str):. for obj in vars(mod).values():. if not getattr(obj, '__module__', getattr(obj, '__qualname__', getattr(obj, '__name__', ''))).startswith(root):. continue. if isinstance(obj, Callable):. yield obj. if isinstance(obj, type):. yield from (m for m in vars(obj).values() if isinstance(m, Callable)). elif isinstance(obj, ModuleType):. yield from descend_classes_and_funcs(obj, root). ```. _________________________________________________________. It is possible to remove the scanpy manually by:. ```py. import sys. sys.mo",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/468
https://github.com/scverse/scanpy/issues/468:209,deployability,contain,containing,209,"scanpy cannot be reloaded (and this slows development); Hi Alex, . Here is an interesting bug with scanpy. For developers, it is useful to be able to reload a previously imported module within the environment containing useful variables and data for testing (a sample scRNA dataset) after changing scanpy's source code. However, scanpy cannot be reloaded. This means that to test, one has to stop the kernel, restart, reload all of the data needed for a plot and then test a plotting function that was just modified (for instance). . Here is a way to demonstrate the reload failure easily:. 1. open utils.py and add the print statement to track the descend_classes_and_funcs() function. ```py. #utils.py. def annotate_doc_types(mod: ModuleType, root: str):. for c_or_f in descend_classes_and_funcs(mod, root):. print(c_or_f) #added line to track descend_classes_and_funcs() function--TR. c_or_f.getdoc = partial(getdoc, c_or_f). ```. 2. open ipython. ```py. import scanpy as sc. # prints out a bunch of function names from the descend_classes_and_funcs() function. import importlib. importlib.reload(sc). # endless loop of function names from the descend_classes_and_funcs() function. # due to recursive yield statement. ```. So what is the purpose of this function? And can it be altered to allow reload? It is called when __init__.py is run by sc.annotate_doc_types(sys.modules[__name__], 'scanpy'). . ```py. #utils.py. def descend_classes_and_funcs(mod: ModuleType, root: str):. for obj in vars(mod).values():. if not getattr(obj, '__module__', getattr(obj, '__qualname__', getattr(obj, '__name__', ''))).startswith(root):. continue. if isinstance(obj, Callable):. yield obj. if isinstance(obj, type):. yield from (m for m in vars(obj).values() if isinstance(m, Callable)). elif isinstance(obj, ModuleType):. yield from descend_classes_and_funcs(obj, root). ```. _________________________________________________________. It is possible to remove the scanpy manually by:. ```py. import sys. sys.mo",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/468
https://github.com/scverse/scanpy/issues/468:574,deployability,fail,failure,574,"scanpy cannot be reloaded (and this slows development); Hi Alex, . Here is an interesting bug with scanpy. For developers, it is useful to be able to reload a previously imported module within the environment containing useful variables and data for testing (a sample scRNA dataset) after changing scanpy's source code. However, scanpy cannot be reloaded. This means that to test, one has to stop the kernel, restart, reload all of the data needed for a plot and then test a plotting function that was just modified (for instance). . Here is a way to demonstrate the reload failure easily:. 1. open utils.py and add the print statement to track the descend_classes_and_funcs() function. ```py. #utils.py. def annotate_doc_types(mod: ModuleType, root: str):. for c_or_f in descend_classes_and_funcs(mod, root):. print(c_or_f) #added line to track descend_classes_and_funcs() function--TR. c_or_f.getdoc = partial(getdoc, c_or_f). ```. 2. open ipython. ```py. import scanpy as sc. # prints out a bunch of function names from the descend_classes_and_funcs() function. import importlib. importlib.reload(sc). # endless loop of function names from the descend_classes_and_funcs() function. # due to recursive yield statement. ```. So what is the purpose of this function? And can it be altered to allow reload? It is called when __init__.py is run by sc.annotate_doc_types(sys.modules[__name__], 'scanpy'). . ```py. #utils.py. def descend_classes_and_funcs(mod: ModuleType, root: str):. for obj in vars(mod).values():. if not getattr(obj, '__module__', getattr(obj, '__qualname__', getattr(obj, '__name__', ''))).startswith(root):. continue. if isinstance(obj, Callable):. yield obj. if isinstance(obj, type):. yield from (m for m in vars(obj).values() if isinstance(m, Callable)). elif isinstance(obj, ModuleType):. yield from descend_classes_and_funcs(obj, root). ```. _________________________________________________________. It is possible to remove the scanpy manually by:. ```py. import sys. sys.mo",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/468
https://github.com/scverse/scanpy/issues/468:733,deployability,Modul,ModuleType,733,"scanpy cannot be reloaded (and this slows development); Hi Alex, . Here is an interesting bug with scanpy. For developers, it is useful to be able to reload a previously imported module within the environment containing useful variables and data for testing (a sample scRNA dataset) after changing scanpy's source code. However, scanpy cannot be reloaded. This means that to test, one has to stop the kernel, restart, reload all of the data needed for a plot and then test a plotting function that was just modified (for instance). . Here is a way to demonstrate the reload failure easily:. 1. open utils.py and add the print statement to track the descend_classes_and_funcs() function. ```py. #utils.py. def annotate_doc_types(mod: ModuleType, root: str):. for c_or_f in descend_classes_and_funcs(mod, root):. print(c_or_f) #added line to track descend_classes_and_funcs() function--TR. c_or_f.getdoc = partial(getdoc, c_or_f). ```. 2. open ipython. ```py. import scanpy as sc. # prints out a bunch of function names from the descend_classes_and_funcs() function. import importlib. importlib.reload(sc). # endless loop of function names from the descend_classes_and_funcs() function. # due to recursive yield statement. ```. So what is the purpose of this function? And can it be altered to allow reload? It is called when __init__.py is run by sc.annotate_doc_types(sys.modules[__name__], 'scanpy'). . ```py. #utils.py. def descend_classes_and_funcs(mod: ModuleType, root: str):. for obj in vars(mod).values():. if not getattr(obj, '__module__', getattr(obj, '__qualname__', getattr(obj, '__name__', ''))).startswith(root):. continue. if isinstance(obj, Callable):. yield obj. if isinstance(obj, type):. yield from (m for m in vars(obj).values() if isinstance(m, Callable)). elif isinstance(obj, ModuleType):. yield from descend_classes_and_funcs(obj, root). ```. _________________________________________________________. It is possible to remove the scanpy manually by:. ```py. import sys. sys.mo",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/468
https://github.com/scverse/scanpy/issues/468:1372,deployability,modul,modules,1372,"the environment containing useful variables and data for testing (a sample scRNA dataset) after changing scanpy's source code. However, scanpy cannot be reloaded. This means that to test, one has to stop the kernel, restart, reload all of the data needed for a plot and then test a plotting function that was just modified (for instance). . Here is a way to demonstrate the reload failure easily:. 1. open utils.py and add the print statement to track the descend_classes_and_funcs() function. ```py. #utils.py. def annotate_doc_types(mod: ModuleType, root: str):. for c_or_f in descend_classes_and_funcs(mod, root):. print(c_or_f) #added line to track descend_classes_and_funcs() function--TR. c_or_f.getdoc = partial(getdoc, c_or_f). ```. 2. open ipython. ```py. import scanpy as sc. # prints out a bunch of function names from the descend_classes_and_funcs() function. import importlib. importlib.reload(sc). # endless loop of function names from the descend_classes_and_funcs() function. # due to recursive yield statement. ```. So what is the purpose of this function? And can it be altered to allow reload? It is called when __init__.py is run by sc.annotate_doc_types(sys.modules[__name__], 'scanpy'). . ```py. #utils.py. def descend_classes_and_funcs(mod: ModuleType, root: str):. for obj in vars(mod).values():. if not getattr(obj, '__module__', getattr(obj, '__qualname__', getattr(obj, '__name__', ''))).startswith(root):. continue. if isinstance(obj, Callable):. yield obj. if isinstance(obj, type):. yield from (m for m in vars(obj).values() if isinstance(m, Callable)). elif isinstance(obj, ModuleType):. yield from descend_classes_and_funcs(obj, root). ```. _________________________________________________________. It is possible to remove the scanpy manually by:. ```py. import sys. sys.modules.pop('scanpy'). ```. and then import scanpy from scratch. Is that the preferred way to re-import scanpy for developers working to enhance scanpy package? Thank you for your work on scanpy!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/468
https://github.com/scverse/scanpy/issues/468:1457,deployability,Modul,ModuleType,1457,"the environment containing useful variables and data for testing (a sample scRNA dataset) after changing scanpy's source code. However, scanpy cannot be reloaded. This means that to test, one has to stop the kernel, restart, reload all of the data needed for a plot and then test a plotting function that was just modified (for instance). . Here is a way to demonstrate the reload failure easily:. 1. open utils.py and add the print statement to track the descend_classes_and_funcs() function. ```py. #utils.py. def annotate_doc_types(mod: ModuleType, root: str):. for c_or_f in descend_classes_and_funcs(mod, root):. print(c_or_f) #added line to track descend_classes_and_funcs() function--TR. c_or_f.getdoc = partial(getdoc, c_or_f). ```. 2. open ipython. ```py. import scanpy as sc. # prints out a bunch of function names from the descend_classes_and_funcs() function. import importlib. importlib.reload(sc). # endless loop of function names from the descend_classes_and_funcs() function. # due to recursive yield statement. ```. So what is the purpose of this function? And can it be altered to allow reload? It is called when __init__.py is run by sc.annotate_doc_types(sys.modules[__name__], 'scanpy'). . ```py. #utils.py. def descend_classes_and_funcs(mod: ModuleType, root: str):. for obj in vars(mod).values():. if not getattr(obj, '__module__', getattr(obj, '__qualname__', getattr(obj, '__name__', ''))).startswith(root):. continue. if isinstance(obj, Callable):. yield obj. if isinstance(obj, type):. yield from (m for m in vars(obj).values() if isinstance(m, Callable)). elif isinstance(obj, ModuleType):. yield from descend_classes_and_funcs(obj, root). ```. _________________________________________________________. It is possible to remove the scanpy manually by:. ```py. import sys. sys.modules.pop('scanpy'). ```. and then import scanpy from scratch. Is that the preferred way to re-import scanpy for developers working to enhance scanpy package? Thank you for your work on scanpy!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/468
https://github.com/scverse/scanpy/issues/468:1627,deployability,continu,continue,1627,"the environment containing useful variables and data for testing (a sample scRNA dataset) after changing scanpy's source code. However, scanpy cannot be reloaded. This means that to test, one has to stop the kernel, restart, reload all of the data needed for a plot and then test a plotting function that was just modified (for instance). . Here is a way to demonstrate the reload failure easily:. 1. open utils.py and add the print statement to track the descend_classes_and_funcs() function. ```py. #utils.py. def annotate_doc_types(mod: ModuleType, root: str):. for c_or_f in descend_classes_and_funcs(mod, root):. print(c_or_f) #added line to track descend_classes_and_funcs() function--TR. c_or_f.getdoc = partial(getdoc, c_or_f). ```. 2. open ipython. ```py. import scanpy as sc. # prints out a bunch of function names from the descend_classes_and_funcs() function. import importlib. importlib.reload(sc). # endless loop of function names from the descend_classes_and_funcs() function. # due to recursive yield statement. ```. So what is the purpose of this function? And can it be altered to allow reload? It is called when __init__.py is run by sc.annotate_doc_types(sys.modules[__name__], 'scanpy'). . ```py. #utils.py. def descend_classes_and_funcs(mod: ModuleType, root: str):. for obj in vars(mod).values():. if not getattr(obj, '__module__', getattr(obj, '__qualname__', getattr(obj, '__name__', ''))).startswith(root):. continue. if isinstance(obj, Callable):. yield obj. if isinstance(obj, type):. yield from (m for m in vars(obj).values() if isinstance(m, Callable)). elif isinstance(obj, ModuleType):. yield from descend_classes_and_funcs(obj, root). ```. _________________________________________________________. It is possible to remove the scanpy manually by:. ```py. import sys. sys.modules.pop('scanpy'). ```. and then import scanpy from scratch. Is that the preferred way to re-import scanpy for developers working to enhance scanpy package? Thank you for your work on scanpy!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/468
https://github.com/scverse/scanpy/issues/468:1798,deployability,Modul,ModuleType,1798,"the environment containing useful variables and data for testing (a sample scRNA dataset) after changing scanpy's source code. However, scanpy cannot be reloaded. This means that to test, one has to stop the kernel, restart, reload all of the data needed for a plot and then test a plotting function that was just modified (for instance). . Here is a way to demonstrate the reload failure easily:. 1. open utils.py and add the print statement to track the descend_classes_and_funcs() function. ```py. #utils.py. def annotate_doc_types(mod: ModuleType, root: str):. for c_or_f in descend_classes_and_funcs(mod, root):. print(c_or_f) #added line to track descend_classes_and_funcs() function--TR. c_or_f.getdoc = partial(getdoc, c_or_f). ```. 2. open ipython. ```py. import scanpy as sc. # prints out a bunch of function names from the descend_classes_and_funcs() function. import importlib. importlib.reload(sc). # endless loop of function names from the descend_classes_and_funcs() function. # due to recursive yield statement. ```. So what is the purpose of this function? And can it be altered to allow reload? It is called when __init__.py is run by sc.annotate_doc_types(sys.modules[__name__], 'scanpy'). . ```py. #utils.py. def descend_classes_and_funcs(mod: ModuleType, root: str):. for obj in vars(mod).values():. if not getattr(obj, '__module__', getattr(obj, '__qualname__', getattr(obj, '__name__', ''))).startswith(root):. continue. if isinstance(obj, Callable):. yield obj. if isinstance(obj, type):. yield from (m for m in vars(obj).values() if isinstance(m, Callable)). elif isinstance(obj, ModuleType):. yield from descend_classes_and_funcs(obj, root). ```. _________________________________________________________. It is possible to remove the scanpy manually by:. ```py. import sys. sys.modules.pop('scanpy'). ```. and then import scanpy from scratch. Is that the preferred way to re-import scanpy for developers working to enhance scanpy package? Thank you for your work on scanpy!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/468
https://github.com/scverse/scanpy/issues/468:1998,deployability,modul,modules,1998,"the environment containing useful variables and data for testing (a sample scRNA dataset) after changing scanpy's source code. However, scanpy cannot be reloaded. This means that to test, one has to stop the kernel, restart, reload all of the data needed for a plot and then test a plotting function that was just modified (for instance). . Here is a way to demonstrate the reload failure easily:. 1. open utils.py and add the print statement to track the descend_classes_and_funcs() function. ```py. #utils.py. def annotate_doc_types(mod: ModuleType, root: str):. for c_or_f in descend_classes_and_funcs(mod, root):. print(c_or_f) #added line to track descend_classes_and_funcs() function--TR. c_or_f.getdoc = partial(getdoc, c_or_f). ```. 2. open ipython. ```py. import scanpy as sc. # prints out a bunch of function names from the descend_classes_and_funcs() function. import importlib. importlib.reload(sc). # endless loop of function names from the descend_classes_and_funcs() function. # due to recursive yield statement. ```. So what is the purpose of this function? And can it be altered to allow reload? It is called when __init__.py is run by sc.annotate_doc_types(sys.modules[__name__], 'scanpy'). . ```py. #utils.py. def descend_classes_and_funcs(mod: ModuleType, root: str):. for obj in vars(mod).values():. if not getattr(obj, '__module__', getattr(obj, '__qualname__', getattr(obj, '__name__', ''))).startswith(root):. continue. if isinstance(obj, Callable):. yield obj. if isinstance(obj, type):. yield from (m for m in vars(obj).values() if isinstance(m, Callable)). elif isinstance(obj, ModuleType):. yield from descend_classes_and_funcs(obj, root). ```. _________________________________________________________. It is possible to remove the scanpy manually by:. ```py. import sys. sys.modules.pop('scanpy'). ```. and then import scanpy from scratch. Is that the preferred way to re-import scanpy for developers working to enhance scanpy package? Thank you for your work on scanpy!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/468
https://github.com/scverse/scanpy/issues/468:626,integrability,state,statement,626,"scanpy cannot be reloaded (and this slows development); Hi Alex, . Here is an interesting bug with scanpy. For developers, it is useful to be able to reload a previously imported module within the environment containing useful variables and data for testing (a sample scRNA dataset) after changing scanpy's source code. However, scanpy cannot be reloaded. This means that to test, one has to stop the kernel, restart, reload all of the data needed for a plot and then test a plotting function that was just modified (for instance). . Here is a way to demonstrate the reload failure easily:. 1. open utils.py and add the print statement to track the descend_classes_and_funcs() function. ```py. #utils.py. def annotate_doc_types(mod: ModuleType, root: str):. for c_or_f in descend_classes_and_funcs(mod, root):. print(c_or_f) #added line to track descend_classes_and_funcs() function--TR. c_or_f.getdoc = partial(getdoc, c_or_f). ```. 2. open ipython. ```py. import scanpy as sc. # prints out a bunch of function names from the descend_classes_and_funcs() function. import importlib. importlib.reload(sc). # endless loop of function names from the descend_classes_and_funcs() function. # due to recursive yield statement. ```. So what is the purpose of this function? And can it be altered to allow reload? It is called when __init__.py is run by sc.annotate_doc_types(sys.modules[__name__], 'scanpy'). . ```py. #utils.py. def descend_classes_and_funcs(mod: ModuleType, root: str):. for obj in vars(mod).values():. if not getattr(obj, '__module__', getattr(obj, '__qualname__', getattr(obj, '__name__', ''))).startswith(root):. continue. if isinstance(obj, Callable):. yield obj. if isinstance(obj, type):. yield from (m for m in vars(obj).values() if isinstance(m, Callable)). elif isinstance(obj, ModuleType):. yield from descend_classes_and_funcs(obj, root). ```. _________________________________________________________. It is possible to remove the scanpy manually by:. ```py. import sys. sys.mo",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/468
https://github.com/scverse/scanpy/issues/468:1210,integrability,state,statement,1210,"the environment containing useful variables and data for testing (a sample scRNA dataset) after changing scanpy's source code. However, scanpy cannot be reloaded. This means that to test, one has to stop the kernel, restart, reload all of the data needed for a plot and then test a plotting function that was just modified (for instance). . Here is a way to demonstrate the reload failure easily:. 1. open utils.py and add the print statement to track the descend_classes_and_funcs() function. ```py. #utils.py. def annotate_doc_types(mod: ModuleType, root: str):. for c_or_f in descend_classes_and_funcs(mod, root):. print(c_or_f) #added line to track descend_classes_and_funcs() function--TR. c_or_f.getdoc = partial(getdoc, c_or_f). ```. 2. open ipython. ```py. import scanpy as sc. # prints out a bunch of function names from the descend_classes_and_funcs() function. import importlib. importlib.reload(sc). # endless loop of function names from the descend_classes_and_funcs() function. # due to recursive yield statement. ```. So what is the purpose of this function? And can it be altered to allow reload? It is called when __init__.py is run by sc.annotate_doc_types(sys.modules[__name__], 'scanpy'). . ```py. #utils.py. def descend_classes_and_funcs(mod: ModuleType, root: str):. for obj in vars(mod).values():. if not getattr(obj, '__module__', getattr(obj, '__qualname__', getattr(obj, '__name__', ''))).startswith(root):. continue. if isinstance(obj, Callable):. yield obj. if isinstance(obj, type):. yield from (m for m in vars(obj).values() if isinstance(m, Callable)). elif isinstance(obj, ModuleType):. yield from descend_classes_and_funcs(obj, root). ```. _________________________________________________________. It is possible to remove the scanpy manually by:. ```py. import sys. sys.modules.pop('scanpy'). ```. and then import scanpy from scratch. Is that the preferred way to re-import scanpy for developers working to enhance scanpy package? Thank you for your work on scanpy!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/468
https://github.com/scverse/scanpy/issues/468:179,modifiability,modul,module,179,"scanpy cannot be reloaded (and this slows development); Hi Alex, . Here is an interesting bug with scanpy. For developers, it is useful to be able to reload a previously imported module within the environment containing useful variables and data for testing (a sample scRNA dataset) after changing scanpy's source code. However, scanpy cannot be reloaded. This means that to test, one has to stop the kernel, restart, reload all of the data needed for a plot and then test a plotting function that was just modified (for instance). . Here is a way to demonstrate the reload failure easily:. 1. open utils.py and add the print statement to track the descend_classes_and_funcs() function. ```py. #utils.py. def annotate_doc_types(mod: ModuleType, root: str):. for c_or_f in descend_classes_and_funcs(mod, root):. print(c_or_f) #added line to track descend_classes_and_funcs() function--TR. c_or_f.getdoc = partial(getdoc, c_or_f). ```. 2. open ipython. ```py. import scanpy as sc. # prints out a bunch of function names from the descend_classes_and_funcs() function. import importlib. importlib.reload(sc). # endless loop of function names from the descend_classes_and_funcs() function. # due to recursive yield statement. ```. So what is the purpose of this function? And can it be altered to allow reload? It is called when __init__.py is run by sc.annotate_doc_types(sys.modules[__name__], 'scanpy'). . ```py. #utils.py. def descend_classes_and_funcs(mod: ModuleType, root: str):. for obj in vars(mod).values():. if not getattr(obj, '__module__', getattr(obj, '__qualname__', getattr(obj, '__name__', ''))).startswith(root):. continue. if isinstance(obj, Callable):. yield obj. if isinstance(obj, type):. yield from (m for m in vars(obj).values() if isinstance(m, Callable)). elif isinstance(obj, ModuleType):. yield from descend_classes_and_funcs(obj, root). ```. _________________________________________________________. It is possible to remove the scanpy manually by:. ```py. import sys. sys.mo",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/468
https://github.com/scverse/scanpy/issues/468:227,modifiability,variab,variables,227,"scanpy cannot be reloaded (and this slows development); Hi Alex, . Here is an interesting bug with scanpy. For developers, it is useful to be able to reload a previously imported module within the environment containing useful variables and data for testing (a sample scRNA dataset) after changing scanpy's source code. However, scanpy cannot be reloaded. This means that to test, one has to stop the kernel, restart, reload all of the data needed for a plot and then test a plotting function that was just modified (for instance). . Here is a way to demonstrate the reload failure easily:. 1. open utils.py and add the print statement to track the descend_classes_and_funcs() function. ```py. #utils.py. def annotate_doc_types(mod: ModuleType, root: str):. for c_or_f in descend_classes_and_funcs(mod, root):. print(c_or_f) #added line to track descend_classes_and_funcs() function--TR. c_or_f.getdoc = partial(getdoc, c_or_f). ```. 2. open ipython. ```py. import scanpy as sc. # prints out a bunch of function names from the descend_classes_and_funcs() function. import importlib. importlib.reload(sc). # endless loop of function names from the descend_classes_and_funcs() function. # due to recursive yield statement. ```. So what is the purpose of this function? And can it be altered to allow reload? It is called when __init__.py is run by sc.annotate_doc_types(sys.modules[__name__], 'scanpy'). . ```py. #utils.py. def descend_classes_and_funcs(mod: ModuleType, root: str):. for obj in vars(mod).values():. if not getattr(obj, '__module__', getattr(obj, '__qualname__', getattr(obj, '__name__', ''))).startswith(root):. continue. if isinstance(obj, Callable):. yield obj. if isinstance(obj, type):. yield from (m for m in vars(obj).values() if isinstance(m, Callable)). elif isinstance(obj, ModuleType):. yield from descend_classes_and_funcs(obj, root). ```. _________________________________________________________. It is possible to remove the scanpy manually by:. ```py. import sys. sys.mo",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/468
https://github.com/scverse/scanpy/issues/468:733,modifiability,Modul,ModuleType,733,"scanpy cannot be reloaded (and this slows development); Hi Alex, . Here is an interesting bug with scanpy. For developers, it is useful to be able to reload a previously imported module within the environment containing useful variables and data for testing (a sample scRNA dataset) after changing scanpy's source code. However, scanpy cannot be reloaded. This means that to test, one has to stop the kernel, restart, reload all of the data needed for a plot and then test a plotting function that was just modified (for instance). . Here is a way to demonstrate the reload failure easily:. 1. open utils.py and add the print statement to track the descend_classes_and_funcs() function. ```py. #utils.py. def annotate_doc_types(mod: ModuleType, root: str):. for c_or_f in descend_classes_and_funcs(mod, root):. print(c_or_f) #added line to track descend_classes_and_funcs() function--TR. c_or_f.getdoc = partial(getdoc, c_or_f). ```. 2. open ipython. ```py. import scanpy as sc. # prints out a bunch of function names from the descend_classes_and_funcs() function. import importlib. importlib.reload(sc). # endless loop of function names from the descend_classes_and_funcs() function. # due to recursive yield statement. ```. So what is the purpose of this function? And can it be altered to allow reload? It is called when __init__.py is run by sc.annotate_doc_types(sys.modules[__name__], 'scanpy'). . ```py. #utils.py. def descend_classes_and_funcs(mod: ModuleType, root: str):. for obj in vars(mod).values():. if not getattr(obj, '__module__', getattr(obj, '__qualname__', getattr(obj, '__name__', ''))).startswith(root):. continue. if isinstance(obj, Callable):. yield obj. if isinstance(obj, type):. yield from (m for m in vars(obj).values() if isinstance(m, Callable)). elif isinstance(obj, ModuleType):. yield from descend_classes_and_funcs(obj, root). ```. _________________________________________________________. It is possible to remove the scanpy manually by:. ```py. import sys. sys.mo",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/468
https://github.com/scverse/scanpy/issues/468:1372,modifiability,modul,modules,1372,"the environment containing useful variables and data for testing (a sample scRNA dataset) after changing scanpy's source code. However, scanpy cannot be reloaded. This means that to test, one has to stop the kernel, restart, reload all of the data needed for a plot and then test a plotting function that was just modified (for instance). . Here is a way to demonstrate the reload failure easily:. 1. open utils.py and add the print statement to track the descend_classes_and_funcs() function. ```py. #utils.py. def annotate_doc_types(mod: ModuleType, root: str):. for c_or_f in descend_classes_and_funcs(mod, root):. print(c_or_f) #added line to track descend_classes_and_funcs() function--TR. c_or_f.getdoc = partial(getdoc, c_or_f). ```. 2. open ipython. ```py. import scanpy as sc. # prints out a bunch of function names from the descend_classes_and_funcs() function. import importlib. importlib.reload(sc). # endless loop of function names from the descend_classes_and_funcs() function. # due to recursive yield statement. ```. So what is the purpose of this function? And can it be altered to allow reload? It is called when __init__.py is run by sc.annotate_doc_types(sys.modules[__name__], 'scanpy'). . ```py. #utils.py. def descend_classes_and_funcs(mod: ModuleType, root: str):. for obj in vars(mod).values():. if not getattr(obj, '__module__', getattr(obj, '__qualname__', getattr(obj, '__name__', ''))).startswith(root):. continue. if isinstance(obj, Callable):. yield obj. if isinstance(obj, type):. yield from (m for m in vars(obj).values() if isinstance(m, Callable)). elif isinstance(obj, ModuleType):. yield from descend_classes_and_funcs(obj, root). ```. _________________________________________________________. It is possible to remove the scanpy manually by:. ```py. import sys. sys.modules.pop('scanpy'). ```. and then import scanpy from scratch. Is that the preferred way to re-import scanpy for developers working to enhance scanpy package? Thank you for your work on scanpy!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/468
https://github.com/scverse/scanpy/issues/468:1457,modifiability,Modul,ModuleType,1457,"the environment containing useful variables and data for testing (a sample scRNA dataset) after changing scanpy's source code. However, scanpy cannot be reloaded. This means that to test, one has to stop the kernel, restart, reload all of the data needed for a plot and then test a plotting function that was just modified (for instance). . Here is a way to demonstrate the reload failure easily:. 1. open utils.py and add the print statement to track the descend_classes_and_funcs() function. ```py. #utils.py. def annotate_doc_types(mod: ModuleType, root: str):. for c_or_f in descend_classes_and_funcs(mod, root):. print(c_or_f) #added line to track descend_classes_and_funcs() function--TR. c_or_f.getdoc = partial(getdoc, c_or_f). ```. 2. open ipython. ```py. import scanpy as sc. # prints out a bunch of function names from the descend_classes_and_funcs() function. import importlib. importlib.reload(sc). # endless loop of function names from the descend_classes_and_funcs() function. # due to recursive yield statement. ```. So what is the purpose of this function? And can it be altered to allow reload? It is called when __init__.py is run by sc.annotate_doc_types(sys.modules[__name__], 'scanpy'). . ```py. #utils.py. def descend_classes_and_funcs(mod: ModuleType, root: str):. for obj in vars(mod).values():. if not getattr(obj, '__module__', getattr(obj, '__qualname__', getattr(obj, '__name__', ''))).startswith(root):. continue. if isinstance(obj, Callable):. yield obj. if isinstance(obj, type):. yield from (m for m in vars(obj).values() if isinstance(m, Callable)). elif isinstance(obj, ModuleType):. yield from descend_classes_and_funcs(obj, root). ```. _________________________________________________________. It is possible to remove the scanpy manually by:. ```py. import sys. sys.modules.pop('scanpy'). ```. and then import scanpy from scratch. Is that the preferred way to re-import scanpy for developers working to enhance scanpy package? Thank you for your work on scanpy!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/468
https://github.com/scverse/scanpy/issues/468:1798,modifiability,Modul,ModuleType,1798,"the environment containing useful variables and data for testing (a sample scRNA dataset) after changing scanpy's source code. However, scanpy cannot be reloaded. This means that to test, one has to stop the kernel, restart, reload all of the data needed for a plot and then test a plotting function that was just modified (for instance). . Here is a way to demonstrate the reload failure easily:. 1. open utils.py and add the print statement to track the descend_classes_and_funcs() function. ```py. #utils.py. def annotate_doc_types(mod: ModuleType, root: str):. for c_or_f in descend_classes_and_funcs(mod, root):. print(c_or_f) #added line to track descend_classes_and_funcs() function--TR. c_or_f.getdoc = partial(getdoc, c_or_f). ```. 2. open ipython. ```py. import scanpy as sc. # prints out a bunch of function names from the descend_classes_and_funcs() function. import importlib. importlib.reload(sc). # endless loop of function names from the descend_classes_and_funcs() function. # due to recursive yield statement. ```. So what is the purpose of this function? And can it be altered to allow reload? It is called when __init__.py is run by sc.annotate_doc_types(sys.modules[__name__], 'scanpy'). . ```py. #utils.py. def descend_classes_and_funcs(mod: ModuleType, root: str):. for obj in vars(mod).values():. if not getattr(obj, '__module__', getattr(obj, '__qualname__', getattr(obj, '__name__', ''))).startswith(root):. continue. if isinstance(obj, Callable):. yield obj. if isinstance(obj, type):. yield from (m for m in vars(obj).values() if isinstance(m, Callable)). elif isinstance(obj, ModuleType):. yield from descend_classes_and_funcs(obj, root). ```. _________________________________________________________. It is possible to remove the scanpy manually by:. ```py. import sys. sys.modules.pop('scanpy'). ```. and then import scanpy from scratch. Is that the preferred way to re-import scanpy for developers working to enhance scanpy package? Thank you for your work on scanpy!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/468
https://github.com/scverse/scanpy/issues/468:1998,modifiability,modul,modules,1998,"the environment containing useful variables and data for testing (a sample scRNA dataset) after changing scanpy's source code. However, scanpy cannot be reloaded. This means that to test, one has to stop the kernel, restart, reload all of the data needed for a plot and then test a plotting function that was just modified (for instance). . Here is a way to demonstrate the reload failure easily:. 1. open utils.py and add the print statement to track the descend_classes_and_funcs() function. ```py. #utils.py. def annotate_doc_types(mod: ModuleType, root: str):. for c_or_f in descend_classes_and_funcs(mod, root):. print(c_or_f) #added line to track descend_classes_and_funcs() function--TR. c_or_f.getdoc = partial(getdoc, c_or_f). ```. 2. open ipython. ```py. import scanpy as sc. # prints out a bunch of function names from the descend_classes_and_funcs() function. import importlib. importlib.reload(sc). # endless loop of function names from the descend_classes_and_funcs() function. # due to recursive yield statement. ```. So what is the purpose of this function? And can it be altered to allow reload? It is called when __init__.py is run by sc.annotate_doc_types(sys.modules[__name__], 'scanpy'). . ```py. #utils.py. def descend_classes_and_funcs(mod: ModuleType, root: str):. for obj in vars(mod).values():. if not getattr(obj, '__module__', getattr(obj, '__qualname__', getattr(obj, '__name__', ''))).startswith(root):. continue. if isinstance(obj, Callable):. yield obj. if isinstance(obj, type):. yield from (m for m in vars(obj).values() if isinstance(m, Callable)). elif isinstance(obj, ModuleType):. yield from descend_classes_and_funcs(obj, root). ```. _________________________________________________________. It is possible to remove the scanpy manually by:. ```py. import sys. sys.modules.pop('scanpy'). ```. and then import scanpy from scratch. Is that the preferred way to re-import scanpy for developers working to enhance scanpy package? Thank you for your work on scanpy!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/468
https://github.com/scverse/scanpy/issues/468:2150,modifiability,pac,package,2150,"the environment containing useful variables and data for testing (a sample scRNA dataset) after changing scanpy's source code. However, scanpy cannot be reloaded. This means that to test, one has to stop the kernel, restart, reload all of the data needed for a plot and then test a plotting function that was just modified (for instance). . Here is a way to demonstrate the reload failure easily:. 1. open utils.py and add the print statement to track the descend_classes_and_funcs() function. ```py. #utils.py. def annotate_doc_types(mod: ModuleType, root: str):. for c_or_f in descend_classes_and_funcs(mod, root):. print(c_or_f) #added line to track descend_classes_and_funcs() function--TR. c_or_f.getdoc = partial(getdoc, c_or_f). ```. 2. open ipython. ```py. import scanpy as sc. # prints out a bunch of function names from the descend_classes_and_funcs() function. import importlib. importlib.reload(sc). # endless loop of function names from the descend_classes_and_funcs() function. # due to recursive yield statement. ```. So what is the purpose of this function? And can it be altered to allow reload? It is called when __init__.py is run by sc.annotate_doc_types(sys.modules[__name__], 'scanpy'). . ```py. #utils.py. def descend_classes_and_funcs(mod: ModuleType, root: str):. for obj in vars(mod).values():. if not getattr(obj, '__module__', getattr(obj, '__qualname__', getattr(obj, '__name__', ''))).startswith(root):. continue. if isinstance(obj, Callable):. yield obj. if isinstance(obj, type):. yield from (m for m in vars(obj).values() if isinstance(m, Callable)). elif isinstance(obj, ModuleType):. yield from descend_classes_and_funcs(obj, root). ```. _________________________________________________________. It is possible to remove the scanpy manually by:. ```py. import sys. sys.modules.pop('scanpy'). ```. and then import scanpy from scratch. Is that the preferred way to re-import scanpy for developers working to enhance scanpy package? Thank you for your work on scanpy!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/468
https://github.com/scverse/scanpy/issues/468:574,performance,failur,failure,574,"scanpy cannot be reloaded (and this slows development); Hi Alex, . Here is an interesting bug with scanpy. For developers, it is useful to be able to reload a previously imported module within the environment containing useful variables and data for testing (a sample scRNA dataset) after changing scanpy's source code. However, scanpy cannot be reloaded. This means that to test, one has to stop the kernel, restart, reload all of the data needed for a plot and then test a plotting function that was just modified (for instance). . Here is a way to demonstrate the reload failure easily:. 1. open utils.py and add the print statement to track the descend_classes_and_funcs() function. ```py. #utils.py. def annotate_doc_types(mod: ModuleType, root: str):. for c_or_f in descend_classes_and_funcs(mod, root):. print(c_or_f) #added line to track descend_classes_and_funcs() function--TR. c_or_f.getdoc = partial(getdoc, c_or_f). ```. 2. open ipython. ```py. import scanpy as sc. # prints out a bunch of function names from the descend_classes_and_funcs() function. import importlib. importlib.reload(sc). # endless loop of function names from the descend_classes_and_funcs() function. # due to recursive yield statement. ```. So what is the purpose of this function? And can it be altered to allow reload? It is called when __init__.py is run by sc.annotate_doc_types(sys.modules[__name__], 'scanpy'). . ```py. #utils.py. def descend_classes_and_funcs(mod: ModuleType, root: str):. for obj in vars(mod).values():. if not getattr(obj, '__module__', getattr(obj, '__qualname__', getattr(obj, '__name__', ''))).startswith(root):. continue. if isinstance(obj, Callable):. yield obj. if isinstance(obj, type):. yield from (m for m in vars(obj).values() if isinstance(m, Callable)). elif isinstance(obj, ModuleType):. yield from descend_classes_and_funcs(obj, root). ```. _________________________________________________________. It is possible to remove the scanpy manually by:. ```py. import sys. sys.mo",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/468
https://github.com/scverse/scanpy/issues/468:36,reliability,slo,slows,36,"scanpy cannot be reloaded (and this slows development); Hi Alex, . Here is an interesting bug with scanpy. For developers, it is useful to be able to reload a previously imported module within the environment containing useful variables and data for testing (a sample scRNA dataset) after changing scanpy's source code. However, scanpy cannot be reloaded. This means that to test, one has to stop the kernel, restart, reload all of the data needed for a plot and then test a plotting function that was just modified (for instance). . Here is a way to demonstrate the reload failure easily:. 1. open utils.py and add the print statement to track the descend_classes_and_funcs() function. ```py. #utils.py. def annotate_doc_types(mod: ModuleType, root: str):. for c_or_f in descend_classes_and_funcs(mod, root):. print(c_or_f) #added line to track descend_classes_and_funcs() function--TR. c_or_f.getdoc = partial(getdoc, c_or_f). ```. 2. open ipython. ```py. import scanpy as sc. # prints out a bunch of function names from the descend_classes_and_funcs() function. import importlib. importlib.reload(sc). # endless loop of function names from the descend_classes_and_funcs() function. # due to recursive yield statement. ```. So what is the purpose of this function? And can it be altered to allow reload? It is called when __init__.py is run by sc.annotate_doc_types(sys.modules[__name__], 'scanpy'). . ```py. #utils.py. def descend_classes_and_funcs(mod: ModuleType, root: str):. for obj in vars(mod).values():. if not getattr(obj, '__module__', getattr(obj, '__qualname__', getattr(obj, '__name__', ''))).startswith(root):. continue. if isinstance(obj, Callable):. yield obj. if isinstance(obj, type):. yield from (m for m in vars(obj).values() if isinstance(m, Callable)). elif isinstance(obj, ModuleType):. yield from descend_classes_and_funcs(obj, root). ```. _________________________________________________________. It is possible to remove the scanpy manually by:. ```py. import sys. sys.mo",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/468
https://github.com/scverse/scanpy/issues/468:574,reliability,fail,failure,574,"scanpy cannot be reloaded (and this slows development); Hi Alex, . Here is an interesting bug with scanpy. For developers, it is useful to be able to reload a previously imported module within the environment containing useful variables and data for testing (a sample scRNA dataset) after changing scanpy's source code. However, scanpy cannot be reloaded. This means that to test, one has to stop the kernel, restart, reload all of the data needed for a plot and then test a plotting function that was just modified (for instance). . Here is a way to demonstrate the reload failure easily:. 1. open utils.py and add the print statement to track the descend_classes_and_funcs() function. ```py. #utils.py. def annotate_doc_types(mod: ModuleType, root: str):. for c_or_f in descend_classes_and_funcs(mod, root):. print(c_or_f) #added line to track descend_classes_and_funcs() function--TR. c_or_f.getdoc = partial(getdoc, c_or_f). ```. 2. open ipython. ```py. import scanpy as sc. # prints out a bunch of function names from the descend_classes_and_funcs() function. import importlib. importlib.reload(sc). # endless loop of function names from the descend_classes_and_funcs() function. # due to recursive yield statement. ```. So what is the purpose of this function? And can it be altered to allow reload? It is called when __init__.py is run by sc.annotate_doc_types(sys.modules[__name__], 'scanpy'). . ```py. #utils.py. def descend_classes_and_funcs(mod: ModuleType, root: str):. for obj in vars(mod).values():. if not getattr(obj, '__module__', getattr(obj, '__qualname__', getattr(obj, '__name__', ''))).startswith(root):. continue. if isinstance(obj, Callable):. yield obj. if isinstance(obj, type):. yield from (m for m in vars(obj).values() if isinstance(m, Callable)). elif isinstance(obj, ModuleType):. yield from descend_classes_and_funcs(obj, root). ```. _________________________________________________________. It is possible to remove the scanpy manually by:. ```py. import sys. sys.mo",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/468
https://github.com/scverse/scanpy/issues/468:179,safety,modul,module,179,"scanpy cannot be reloaded (and this slows development); Hi Alex, . Here is an interesting bug with scanpy. For developers, it is useful to be able to reload a previously imported module within the environment containing useful variables and data for testing (a sample scRNA dataset) after changing scanpy's source code. However, scanpy cannot be reloaded. This means that to test, one has to stop the kernel, restart, reload all of the data needed for a plot and then test a plotting function that was just modified (for instance). . Here is a way to demonstrate the reload failure easily:. 1. open utils.py and add the print statement to track the descend_classes_and_funcs() function. ```py. #utils.py. def annotate_doc_types(mod: ModuleType, root: str):. for c_or_f in descend_classes_and_funcs(mod, root):. print(c_or_f) #added line to track descend_classes_and_funcs() function--TR. c_or_f.getdoc = partial(getdoc, c_or_f). ```. 2. open ipython. ```py. import scanpy as sc. # prints out a bunch of function names from the descend_classes_and_funcs() function. import importlib. importlib.reload(sc). # endless loop of function names from the descend_classes_and_funcs() function. # due to recursive yield statement. ```. So what is the purpose of this function? And can it be altered to allow reload? It is called when __init__.py is run by sc.annotate_doc_types(sys.modules[__name__], 'scanpy'). . ```py. #utils.py. def descend_classes_and_funcs(mod: ModuleType, root: str):. for obj in vars(mod).values():. if not getattr(obj, '__module__', getattr(obj, '__qualname__', getattr(obj, '__name__', ''))).startswith(root):. continue. if isinstance(obj, Callable):. yield obj. if isinstance(obj, type):. yield from (m for m in vars(obj).values() if isinstance(m, Callable)). elif isinstance(obj, ModuleType):. yield from descend_classes_and_funcs(obj, root). ```. _________________________________________________________. It is possible to remove the scanpy manually by:. ```py. import sys. sys.mo",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/468
https://github.com/scverse/scanpy/issues/468:250,safety,test,testing,250,"scanpy cannot be reloaded (and this slows development); Hi Alex, . Here is an interesting bug with scanpy. For developers, it is useful to be able to reload a previously imported module within the environment containing useful variables and data for testing (a sample scRNA dataset) after changing scanpy's source code. However, scanpy cannot be reloaded. This means that to test, one has to stop the kernel, restart, reload all of the data needed for a plot and then test a plotting function that was just modified (for instance). . Here is a way to demonstrate the reload failure easily:. 1. open utils.py and add the print statement to track the descend_classes_and_funcs() function. ```py. #utils.py. def annotate_doc_types(mod: ModuleType, root: str):. for c_or_f in descend_classes_and_funcs(mod, root):. print(c_or_f) #added line to track descend_classes_and_funcs() function--TR. c_or_f.getdoc = partial(getdoc, c_or_f). ```. 2. open ipython. ```py. import scanpy as sc. # prints out a bunch of function names from the descend_classes_and_funcs() function. import importlib. importlib.reload(sc). # endless loop of function names from the descend_classes_and_funcs() function. # due to recursive yield statement. ```. So what is the purpose of this function? And can it be altered to allow reload? It is called when __init__.py is run by sc.annotate_doc_types(sys.modules[__name__], 'scanpy'). . ```py. #utils.py. def descend_classes_and_funcs(mod: ModuleType, root: str):. for obj in vars(mod).values():. if not getattr(obj, '__module__', getattr(obj, '__qualname__', getattr(obj, '__name__', ''))).startswith(root):. continue. if isinstance(obj, Callable):. yield obj. if isinstance(obj, type):. yield from (m for m in vars(obj).values() if isinstance(m, Callable)). elif isinstance(obj, ModuleType):. yield from descend_classes_and_funcs(obj, root). ```. _________________________________________________________. It is possible to remove the scanpy manually by:. ```py. import sys. sys.mo",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/468
https://github.com/scverse/scanpy/issues/468:375,safety,test,test,375,"scanpy cannot be reloaded (and this slows development); Hi Alex, . Here is an interesting bug with scanpy. For developers, it is useful to be able to reload a previously imported module within the environment containing useful variables and data for testing (a sample scRNA dataset) after changing scanpy's source code. However, scanpy cannot be reloaded. This means that to test, one has to stop the kernel, restart, reload all of the data needed for a plot and then test a plotting function that was just modified (for instance). . Here is a way to demonstrate the reload failure easily:. 1. open utils.py and add the print statement to track the descend_classes_and_funcs() function. ```py. #utils.py. def annotate_doc_types(mod: ModuleType, root: str):. for c_or_f in descend_classes_and_funcs(mod, root):. print(c_or_f) #added line to track descend_classes_and_funcs() function--TR. c_or_f.getdoc = partial(getdoc, c_or_f). ```. 2. open ipython. ```py. import scanpy as sc. # prints out a bunch of function names from the descend_classes_and_funcs() function. import importlib. importlib.reload(sc). # endless loop of function names from the descend_classes_and_funcs() function. # due to recursive yield statement. ```. So what is the purpose of this function? And can it be altered to allow reload? It is called when __init__.py is run by sc.annotate_doc_types(sys.modules[__name__], 'scanpy'). . ```py. #utils.py. def descend_classes_and_funcs(mod: ModuleType, root: str):. for obj in vars(mod).values():. if not getattr(obj, '__module__', getattr(obj, '__qualname__', getattr(obj, '__name__', ''))).startswith(root):. continue. if isinstance(obj, Callable):. yield obj. if isinstance(obj, type):. yield from (m for m in vars(obj).values() if isinstance(m, Callable)). elif isinstance(obj, ModuleType):. yield from descend_classes_and_funcs(obj, root). ```. _________________________________________________________. It is possible to remove the scanpy manually by:. ```py. import sys. sys.mo",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/468
https://github.com/scverse/scanpy/issues/468:468,safety,test,test,468,"scanpy cannot be reloaded (and this slows development); Hi Alex, . Here is an interesting bug with scanpy. For developers, it is useful to be able to reload a previously imported module within the environment containing useful variables and data for testing (a sample scRNA dataset) after changing scanpy's source code. However, scanpy cannot be reloaded. This means that to test, one has to stop the kernel, restart, reload all of the data needed for a plot and then test a plotting function that was just modified (for instance). . Here is a way to demonstrate the reload failure easily:. 1. open utils.py and add the print statement to track the descend_classes_and_funcs() function. ```py. #utils.py. def annotate_doc_types(mod: ModuleType, root: str):. for c_or_f in descend_classes_and_funcs(mod, root):. print(c_or_f) #added line to track descend_classes_and_funcs() function--TR. c_or_f.getdoc = partial(getdoc, c_or_f). ```. 2. open ipython. ```py. import scanpy as sc. # prints out a bunch of function names from the descend_classes_and_funcs() function. import importlib. importlib.reload(sc). # endless loop of function names from the descend_classes_and_funcs() function. # due to recursive yield statement. ```. So what is the purpose of this function? And can it be altered to allow reload? It is called when __init__.py is run by sc.annotate_doc_types(sys.modules[__name__], 'scanpy'). . ```py. #utils.py. def descend_classes_and_funcs(mod: ModuleType, root: str):. for obj in vars(mod).values():. if not getattr(obj, '__module__', getattr(obj, '__qualname__', getattr(obj, '__name__', ''))).startswith(root):. continue. if isinstance(obj, Callable):. yield obj. if isinstance(obj, type):. yield from (m for m in vars(obj).values() if isinstance(m, Callable)). elif isinstance(obj, ModuleType):. yield from descend_classes_and_funcs(obj, root). ```. _________________________________________________________. It is possible to remove the scanpy manually by:. ```py. import sys. sys.mo",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/468
https://github.com/scverse/scanpy/issues/468:733,safety,Modul,ModuleType,733,"scanpy cannot be reloaded (and this slows development); Hi Alex, . Here is an interesting bug with scanpy. For developers, it is useful to be able to reload a previously imported module within the environment containing useful variables and data for testing (a sample scRNA dataset) after changing scanpy's source code. However, scanpy cannot be reloaded. This means that to test, one has to stop the kernel, restart, reload all of the data needed for a plot and then test a plotting function that was just modified (for instance). . Here is a way to demonstrate the reload failure easily:. 1. open utils.py and add the print statement to track the descend_classes_and_funcs() function. ```py. #utils.py. def annotate_doc_types(mod: ModuleType, root: str):. for c_or_f in descend_classes_and_funcs(mod, root):. print(c_or_f) #added line to track descend_classes_and_funcs() function--TR. c_or_f.getdoc = partial(getdoc, c_or_f). ```. 2. open ipython. ```py. import scanpy as sc. # prints out a bunch of function names from the descend_classes_and_funcs() function. import importlib. importlib.reload(sc). # endless loop of function names from the descend_classes_and_funcs() function. # due to recursive yield statement. ```. So what is the purpose of this function? And can it be altered to allow reload? It is called when __init__.py is run by sc.annotate_doc_types(sys.modules[__name__], 'scanpy'). . ```py. #utils.py. def descend_classes_and_funcs(mod: ModuleType, root: str):. for obj in vars(mod).values():. if not getattr(obj, '__module__', getattr(obj, '__qualname__', getattr(obj, '__name__', ''))).startswith(root):. continue. if isinstance(obj, Callable):. yield obj. if isinstance(obj, type):. yield from (m for m in vars(obj).values() if isinstance(m, Callable)). elif isinstance(obj, ModuleType):. yield from descend_classes_and_funcs(obj, root). ```. _________________________________________________________. It is possible to remove the scanpy manually by:. ```py. import sys. sys.mo",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/468
https://github.com/scverse/scanpy/issues/468:1372,safety,modul,modules,1372,"the environment containing useful variables and data for testing (a sample scRNA dataset) after changing scanpy's source code. However, scanpy cannot be reloaded. This means that to test, one has to stop the kernel, restart, reload all of the data needed for a plot and then test a plotting function that was just modified (for instance). . Here is a way to demonstrate the reload failure easily:. 1. open utils.py and add the print statement to track the descend_classes_and_funcs() function. ```py. #utils.py. def annotate_doc_types(mod: ModuleType, root: str):. for c_or_f in descend_classes_and_funcs(mod, root):. print(c_or_f) #added line to track descend_classes_and_funcs() function--TR. c_or_f.getdoc = partial(getdoc, c_or_f). ```. 2. open ipython. ```py. import scanpy as sc. # prints out a bunch of function names from the descend_classes_and_funcs() function. import importlib. importlib.reload(sc). # endless loop of function names from the descend_classes_and_funcs() function. # due to recursive yield statement. ```. So what is the purpose of this function? And can it be altered to allow reload? It is called when __init__.py is run by sc.annotate_doc_types(sys.modules[__name__], 'scanpy'). . ```py. #utils.py. def descend_classes_and_funcs(mod: ModuleType, root: str):. for obj in vars(mod).values():. if not getattr(obj, '__module__', getattr(obj, '__qualname__', getattr(obj, '__name__', ''))).startswith(root):. continue. if isinstance(obj, Callable):. yield obj. if isinstance(obj, type):. yield from (m for m in vars(obj).values() if isinstance(m, Callable)). elif isinstance(obj, ModuleType):. yield from descend_classes_and_funcs(obj, root). ```. _________________________________________________________. It is possible to remove the scanpy manually by:. ```py. import sys. sys.modules.pop('scanpy'). ```. and then import scanpy from scratch. Is that the preferred way to re-import scanpy for developers working to enhance scanpy package? Thank you for your work on scanpy!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/468
https://github.com/scverse/scanpy/issues/468:1457,safety,Modul,ModuleType,1457,"the environment containing useful variables and data for testing (a sample scRNA dataset) after changing scanpy's source code. However, scanpy cannot be reloaded. This means that to test, one has to stop the kernel, restart, reload all of the data needed for a plot and then test a plotting function that was just modified (for instance). . Here is a way to demonstrate the reload failure easily:. 1. open utils.py and add the print statement to track the descend_classes_and_funcs() function. ```py. #utils.py. def annotate_doc_types(mod: ModuleType, root: str):. for c_or_f in descend_classes_and_funcs(mod, root):. print(c_or_f) #added line to track descend_classes_and_funcs() function--TR. c_or_f.getdoc = partial(getdoc, c_or_f). ```. 2. open ipython. ```py. import scanpy as sc. # prints out a bunch of function names from the descend_classes_and_funcs() function. import importlib. importlib.reload(sc). # endless loop of function names from the descend_classes_and_funcs() function. # due to recursive yield statement. ```. So what is the purpose of this function? And can it be altered to allow reload? It is called when __init__.py is run by sc.annotate_doc_types(sys.modules[__name__], 'scanpy'). . ```py. #utils.py. def descend_classes_and_funcs(mod: ModuleType, root: str):. for obj in vars(mod).values():. if not getattr(obj, '__module__', getattr(obj, '__qualname__', getattr(obj, '__name__', ''))).startswith(root):. continue. if isinstance(obj, Callable):. yield obj. if isinstance(obj, type):. yield from (m for m in vars(obj).values() if isinstance(m, Callable)). elif isinstance(obj, ModuleType):. yield from descend_classes_and_funcs(obj, root). ```. _________________________________________________________. It is possible to remove the scanpy manually by:. ```py. import sys. sys.modules.pop('scanpy'). ```. and then import scanpy from scratch. Is that the preferred way to re-import scanpy for developers working to enhance scanpy package? Thank you for your work on scanpy!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/468
https://github.com/scverse/scanpy/issues/468:1798,safety,Modul,ModuleType,1798,"the environment containing useful variables and data for testing (a sample scRNA dataset) after changing scanpy's source code. However, scanpy cannot be reloaded. This means that to test, one has to stop the kernel, restart, reload all of the data needed for a plot and then test a plotting function that was just modified (for instance). . Here is a way to demonstrate the reload failure easily:. 1. open utils.py and add the print statement to track the descend_classes_and_funcs() function. ```py. #utils.py. def annotate_doc_types(mod: ModuleType, root: str):. for c_or_f in descend_classes_and_funcs(mod, root):. print(c_or_f) #added line to track descend_classes_and_funcs() function--TR. c_or_f.getdoc = partial(getdoc, c_or_f). ```. 2. open ipython. ```py. import scanpy as sc. # prints out a bunch of function names from the descend_classes_and_funcs() function. import importlib. importlib.reload(sc). # endless loop of function names from the descend_classes_and_funcs() function. # due to recursive yield statement. ```. So what is the purpose of this function? And can it be altered to allow reload? It is called when __init__.py is run by sc.annotate_doc_types(sys.modules[__name__], 'scanpy'). . ```py. #utils.py. def descend_classes_and_funcs(mod: ModuleType, root: str):. for obj in vars(mod).values():. if not getattr(obj, '__module__', getattr(obj, '__qualname__', getattr(obj, '__name__', ''))).startswith(root):. continue. if isinstance(obj, Callable):. yield obj. if isinstance(obj, type):. yield from (m for m in vars(obj).values() if isinstance(m, Callable)). elif isinstance(obj, ModuleType):. yield from descend_classes_and_funcs(obj, root). ```. _________________________________________________________. It is possible to remove the scanpy manually by:. ```py. import sys. sys.modules.pop('scanpy'). ```. and then import scanpy from scratch. Is that the preferred way to re-import scanpy for developers working to enhance scanpy package? Thank you for your work on scanpy!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/468
https://github.com/scverse/scanpy/issues/468:1998,safety,modul,modules,1998,"the environment containing useful variables and data for testing (a sample scRNA dataset) after changing scanpy's source code. However, scanpy cannot be reloaded. This means that to test, one has to stop the kernel, restart, reload all of the data needed for a plot and then test a plotting function that was just modified (for instance). . Here is a way to demonstrate the reload failure easily:. 1. open utils.py and add the print statement to track the descend_classes_and_funcs() function. ```py. #utils.py. def annotate_doc_types(mod: ModuleType, root: str):. for c_or_f in descend_classes_and_funcs(mod, root):. print(c_or_f) #added line to track descend_classes_and_funcs() function--TR. c_or_f.getdoc = partial(getdoc, c_or_f). ```. 2. open ipython. ```py. import scanpy as sc. # prints out a bunch of function names from the descend_classes_and_funcs() function. import importlib. importlib.reload(sc). # endless loop of function names from the descend_classes_and_funcs() function. # due to recursive yield statement. ```. So what is the purpose of this function? And can it be altered to allow reload? It is called when __init__.py is run by sc.annotate_doc_types(sys.modules[__name__], 'scanpy'). . ```py. #utils.py. def descend_classes_and_funcs(mod: ModuleType, root: str):. for obj in vars(mod).values():. if not getattr(obj, '__module__', getattr(obj, '__qualname__', getattr(obj, '__name__', ''))).startswith(root):. continue. if isinstance(obj, Callable):. yield obj. if isinstance(obj, type):. yield from (m for m in vars(obj).values() if isinstance(m, Callable)). elif isinstance(obj, ModuleType):. yield from descend_classes_and_funcs(obj, root). ```. _________________________________________________________. It is possible to remove the scanpy manually by:. ```py. import sys. sys.modules.pop('scanpy'). ```. and then import scanpy from scratch. Is that the preferred way to re-import scanpy for developers working to enhance scanpy package? Thank you for your work on scanpy!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/468
https://github.com/scverse/scanpy/issues/468:507,security,modif,modified,507,"scanpy cannot be reloaded (and this slows development); Hi Alex, . Here is an interesting bug with scanpy. For developers, it is useful to be able to reload a previously imported module within the environment containing useful variables and data for testing (a sample scRNA dataset) after changing scanpy's source code. However, scanpy cannot be reloaded. This means that to test, one has to stop the kernel, restart, reload all of the data needed for a plot and then test a plotting function that was just modified (for instance). . Here is a way to demonstrate the reload failure easily:. 1. open utils.py and add the print statement to track the descend_classes_and_funcs() function. ```py. #utils.py. def annotate_doc_types(mod: ModuleType, root: str):. for c_or_f in descend_classes_and_funcs(mod, root):. print(c_or_f) #added line to track descend_classes_and_funcs() function--TR. c_or_f.getdoc = partial(getdoc, c_or_f). ```. 2. open ipython. ```py. import scanpy as sc. # prints out a bunch of function names from the descend_classes_and_funcs() function. import importlib. importlib.reload(sc). # endless loop of function names from the descend_classes_and_funcs() function. # due to recursive yield statement. ```. So what is the purpose of this function? And can it be altered to allow reload? It is called when __init__.py is run by sc.annotate_doc_types(sys.modules[__name__], 'scanpy'). . ```py. #utils.py. def descend_classes_and_funcs(mod: ModuleType, root: str):. for obj in vars(mod).values():. if not getattr(obj, '__module__', getattr(obj, '__qualname__', getattr(obj, '__name__', ''))).startswith(root):. continue. if isinstance(obj, Callable):. yield obj. if isinstance(obj, type):. yield from (m for m in vars(obj).values() if isinstance(m, Callable)). elif isinstance(obj, ModuleType):. yield from descend_classes_and_funcs(obj, root). ```. _________________________________________________________. It is possible to remove the scanpy manually by:. ```py. import sys. sys.mo",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/468
https://github.com/scverse/scanpy/issues/468:250,testability,test,testing,250,"scanpy cannot be reloaded (and this slows development); Hi Alex, . Here is an interesting bug with scanpy. For developers, it is useful to be able to reload a previously imported module within the environment containing useful variables and data for testing (a sample scRNA dataset) after changing scanpy's source code. However, scanpy cannot be reloaded. This means that to test, one has to stop the kernel, restart, reload all of the data needed for a plot and then test a plotting function that was just modified (for instance). . Here is a way to demonstrate the reload failure easily:. 1. open utils.py and add the print statement to track the descend_classes_and_funcs() function. ```py. #utils.py. def annotate_doc_types(mod: ModuleType, root: str):. for c_or_f in descend_classes_and_funcs(mod, root):. print(c_or_f) #added line to track descend_classes_and_funcs() function--TR. c_or_f.getdoc = partial(getdoc, c_or_f). ```. 2. open ipython. ```py. import scanpy as sc. # prints out a bunch of function names from the descend_classes_and_funcs() function. import importlib. importlib.reload(sc). # endless loop of function names from the descend_classes_and_funcs() function. # due to recursive yield statement. ```. So what is the purpose of this function? And can it be altered to allow reload? It is called when __init__.py is run by sc.annotate_doc_types(sys.modules[__name__], 'scanpy'). . ```py. #utils.py. def descend_classes_and_funcs(mod: ModuleType, root: str):. for obj in vars(mod).values():. if not getattr(obj, '__module__', getattr(obj, '__qualname__', getattr(obj, '__name__', ''))).startswith(root):. continue. if isinstance(obj, Callable):. yield obj. if isinstance(obj, type):. yield from (m for m in vars(obj).values() if isinstance(m, Callable)). elif isinstance(obj, ModuleType):. yield from descend_classes_and_funcs(obj, root). ```. _________________________________________________________. It is possible to remove the scanpy manually by:. ```py. import sys. sys.mo",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/468
https://github.com/scverse/scanpy/issues/468:375,testability,test,test,375,"scanpy cannot be reloaded (and this slows development); Hi Alex, . Here is an interesting bug with scanpy. For developers, it is useful to be able to reload a previously imported module within the environment containing useful variables and data for testing (a sample scRNA dataset) after changing scanpy's source code. However, scanpy cannot be reloaded. This means that to test, one has to stop the kernel, restart, reload all of the data needed for a plot and then test a plotting function that was just modified (for instance). . Here is a way to demonstrate the reload failure easily:. 1. open utils.py and add the print statement to track the descend_classes_and_funcs() function. ```py. #utils.py. def annotate_doc_types(mod: ModuleType, root: str):. for c_or_f in descend_classes_and_funcs(mod, root):. print(c_or_f) #added line to track descend_classes_and_funcs() function--TR. c_or_f.getdoc = partial(getdoc, c_or_f). ```. 2. open ipython. ```py. import scanpy as sc. # prints out a bunch of function names from the descend_classes_and_funcs() function. import importlib. importlib.reload(sc). # endless loop of function names from the descend_classes_and_funcs() function. # due to recursive yield statement. ```. So what is the purpose of this function? And can it be altered to allow reload? It is called when __init__.py is run by sc.annotate_doc_types(sys.modules[__name__], 'scanpy'). . ```py. #utils.py. def descend_classes_and_funcs(mod: ModuleType, root: str):. for obj in vars(mod).values():. if not getattr(obj, '__module__', getattr(obj, '__qualname__', getattr(obj, '__name__', ''))).startswith(root):. continue. if isinstance(obj, Callable):. yield obj. if isinstance(obj, type):. yield from (m for m in vars(obj).values() if isinstance(m, Callable)). elif isinstance(obj, ModuleType):. yield from descend_classes_and_funcs(obj, root). ```. _________________________________________________________. It is possible to remove the scanpy manually by:. ```py. import sys. sys.mo",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/468
https://github.com/scverse/scanpy/issues/468:468,testability,test,test,468,"scanpy cannot be reloaded (and this slows development); Hi Alex, . Here is an interesting bug with scanpy. For developers, it is useful to be able to reload a previously imported module within the environment containing useful variables and data for testing (a sample scRNA dataset) after changing scanpy's source code. However, scanpy cannot be reloaded. This means that to test, one has to stop the kernel, restart, reload all of the data needed for a plot and then test a plotting function that was just modified (for instance). . Here is a way to demonstrate the reload failure easily:. 1. open utils.py and add the print statement to track the descend_classes_and_funcs() function. ```py. #utils.py. def annotate_doc_types(mod: ModuleType, root: str):. for c_or_f in descend_classes_and_funcs(mod, root):. print(c_or_f) #added line to track descend_classes_and_funcs() function--TR. c_or_f.getdoc = partial(getdoc, c_or_f). ```. 2. open ipython. ```py. import scanpy as sc. # prints out a bunch of function names from the descend_classes_and_funcs() function. import importlib. importlib.reload(sc). # endless loop of function names from the descend_classes_and_funcs() function. # due to recursive yield statement. ```. So what is the purpose of this function? And can it be altered to allow reload? It is called when __init__.py is run by sc.annotate_doc_types(sys.modules[__name__], 'scanpy'). . ```py. #utils.py. def descend_classes_and_funcs(mod: ModuleType, root: str):. for obj in vars(mod).values():. if not getattr(obj, '__module__', getattr(obj, '__qualname__', getattr(obj, '__name__', ''))).startswith(root):. continue. if isinstance(obj, Callable):. yield obj. if isinstance(obj, type):. yield from (m for m in vars(obj).values() if isinstance(m, Callable)). elif isinstance(obj, ModuleType):. yield from descend_classes_and_funcs(obj, root). ```. _________________________________________________________. It is possible to remove the scanpy manually by:. ```py. import sys. sys.mo",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/468
https://github.com/scverse/scanpy/issues/468:392,usability,stop,stop,392,"scanpy cannot be reloaded (and this slows development); Hi Alex, . Here is an interesting bug with scanpy. For developers, it is useful to be able to reload a previously imported module within the environment containing useful variables and data for testing (a sample scRNA dataset) after changing scanpy's source code. However, scanpy cannot be reloaded. This means that to test, one has to stop the kernel, restart, reload all of the data needed for a plot and then test a plotting function that was just modified (for instance). . Here is a way to demonstrate the reload failure easily:. 1. open utils.py and add the print statement to track the descend_classes_and_funcs() function. ```py. #utils.py. def annotate_doc_types(mod: ModuleType, root: str):. for c_or_f in descend_classes_and_funcs(mod, root):. print(c_or_f) #added line to track descend_classes_and_funcs() function--TR. c_or_f.getdoc = partial(getdoc, c_or_f). ```. 2. open ipython. ```py. import scanpy as sc. # prints out a bunch of function names from the descend_classes_and_funcs() function. import importlib. importlib.reload(sc). # endless loop of function names from the descend_classes_and_funcs() function. # due to recursive yield statement. ```. So what is the purpose of this function? And can it be altered to allow reload? It is called when __init__.py is run by sc.annotate_doc_types(sys.modules[__name__], 'scanpy'). . ```py. #utils.py. def descend_classes_and_funcs(mod: ModuleType, root: str):. for obj in vars(mod).values():. if not getattr(obj, '__module__', getattr(obj, '__qualname__', getattr(obj, '__name__', ''))).startswith(root):. continue. if isinstance(obj, Callable):. yield obj. if isinstance(obj, type):. yield from (m for m in vars(obj).values() if isinstance(m, Callable)). elif isinstance(obj, ModuleType):. yield from descend_classes_and_funcs(obj, root). ```. _________________________________________________________. It is possible to remove the scanpy manually by:. ```py. import sys. sys.mo",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/468
https://github.com/scverse/scanpy/issues/468:2075,usability,prefer,preferred,2075,"the environment containing useful variables and data for testing (a sample scRNA dataset) after changing scanpy's source code. However, scanpy cannot be reloaded. This means that to test, one has to stop the kernel, restart, reload all of the data needed for a plot and then test a plotting function that was just modified (for instance). . Here is a way to demonstrate the reload failure easily:. 1. open utils.py and add the print statement to track the descend_classes_and_funcs() function. ```py. #utils.py. def annotate_doc_types(mod: ModuleType, root: str):. for c_or_f in descend_classes_and_funcs(mod, root):. print(c_or_f) #added line to track descend_classes_and_funcs() function--TR. c_or_f.getdoc = partial(getdoc, c_or_f). ```. 2. open ipython. ```py. import scanpy as sc. # prints out a bunch of function names from the descend_classes_and_funcs() function. import importlib. importlib.reload(sc). # endless loop of function names from the descend_classes_and_funcs() function. # due to recursive yield statement. ```. So what is the purpose of this function? And can it be altered to allow reload? It is called when __init__.py is run by sc.annotate_doc_types(sys.modules[__name__], 'scanpy'). . ```py. #utils.py. def descend_classes_and_funcs(mod: ModuleType, root: str):. for obj in vars(mod).values():. if not getattr(obj, '__module__', getattr(obj, '__qualname__', getattr(obj, '__name__', ''))).startswith(root):. continue. if isinstance(obj, Callable):. yield obj. if isinstance(obj, type):. yield from (m for m in vars(obj).values() if isinstance(m, Callable)). elif isinstance(obj, ModuleType):. yield from descend_classes_and_funcs(obj, root). ```. _________________________________________________________. It is possible to remove the scanpy manually by:. ```py. import sys. sys.modules.pop('scanpy'). ```. and then import scanpy from scratch. Is that the preferred way to re-import scanpy for developers working to enhance scanpy package? Thank you for your work on scanpy!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/468
https://github.com/scverse/scanpy/issues/469:289,deployability,observ,observed,289,"Wilcoxon in rank_gene_group ignores genes on chunk borders; Using `method=wilcoxon` in the `rank_genes_groups` function chunks the genes into groups (to avoid using too much memory). The scores for the genes on the endpoints of these chunks are never computed, however. This can easily be observed by looking at a dataset containing 5000 genes (so that the chunk size is 2000) and setting `rankby_abs=True`. The genes with indices 2000, 4000, 6000, .... will all be selected as marker genes with p-values of 0. This is due to the fact that the rank-sums for these genes are left at 0 (they are never computed), which is then judged to be EXTREMELY significant. . I will submit a PR with a fix for this in a minute :)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/469
https://github.com/scverse/scanpy/issues/469:322,deployability,contain,containing,322,"Wilcoxon in rank_gene_group ignores genes on chunk borders; Using `method=wilcoxon` in the `rank_genes_groups` function chunks the genes into groups (to avoid using too much memory). The scores for the genes on the endpoints of these chunks are never computed, however. This can easily be observed by looking at a dataset containing 5000 genes (so that the chunk size is 2000) and setting `rankby_abs=True`. The genes with indices 2000, 4000, 6000, .... will all be selected as marker genes with p-values of 0. This is due to the fact that the rank-sums for these genes are left at 0 (they are never computed), which is then judged to be EXTREMELY significant. . I will submit a PR with a fix for this in a minute :)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/469
https://github.com/scverse/scanpy/issues/469:215,integrability,endpoint,endpoints,215,"Wilcoxon in rank_gene_group ignores genes on chunk borders; Using `method=wilcoxon` in the `rank_genes_groups` function chunks the genes into groups (to avoid using too much memory). The scores for the genes on the endpoints of these chunks are never computed, however. This can easily be observed by looking at a dataset containing 5000 genes (so that the chunk size is 2000) and setting `rankby_abs=True`. The genes with indices 2000, 4000, 6000, .... will all be selected as marker genes with p-values of 0. This is due to the fact that the rank-sums for these genes are left at 0 (they are never computed), which is then judged to be EXTREMELY significant. . I will submit a PR with a fix for this in a minute :)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/469
https://github.com/scverse/scanpy/issues/469:670,integrability,sub,submit,670,"Wilcoxon in rank_gene_group ignores genes on chunk borders; Using `method=wilcoxon` in the `rank_genes_groups` function chunks the genes into groups (to avoid using too much memory). The scores for the genes on the endpoints of these chunks are never computed, however. This can easily be observed by looking at a dataset containing 5000 genes (so that the chunk size is 2000) and setting `rankby_abs=True`. The genes with indices 2000, 4000, 6000, .... will all be selected as marker genes with p-values of 0. This is due to the fact that the rank-sums for these genes are left at 0 (they are never computed), which is then judged to be EXTREMELY significant. . I will submit a PR with a fix for this in a minute :)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/469
https://github.com/scverse/scanpy/issues/469:174,performance,memor,memory,174,"Wilcoxon in rank_gene_group ignores genes on chunk borders; Using `method=wilcoxon` in the `rank_genes_groups` function chunks the genes into groups (to avoid using too much memory). The scores for the genes on the endpoints of these chunks are never computed, however. This can easily be observed by looking at a dataset containing 5000 genes (so that the chunk size is 2000) and setting `rankby_abs=True`. The genes with indices 2000, 4000, 6000, .... will all be selected as marker genes with p-values of 0. This is due to the fact that the rank-sums for these genes are left at 0 (they are never computed), which is then judged to be EXTREMELY significant. . I will submit a PR with a fix for this in a minute :)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/469
https://github.com/scverse/scanpy/issues/469:153,safety,avoid,avoid,153,"Wilcoxon in rank_gene_group ignores genes on chunk borders; Using `method=wilcoxon` in the `rank_genes_groups` function chunks the genes into groups (to avoid using too much memory). The scores for the genes on the endpoints of these chunks are never computed, however. This can easily be observed by looking at a dataset containing 5000 genes (so that the chunk size is 2000) and setting `rankby_abs=True`. The genes with indices 2000, 4000, 6000, .... will all be selected as marker genes with p-values of 0. This is due to the fact that the rank-sums for these genes are left at 0 (they are never computed), which is then judged to be EXTREMELY significant. . I will submit a PR with a fix for this in a minute :)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/469
https://github.com/scverse/scanpy/issues/469:648,security,sign,significant,648,"Wilcoxon in rank_gene_group ignores genes on chunk borders; Using `method=wilcoxon` in the `rank_genes_groups` function chunks the genes into groups (to avoid using too much memory). The scores for the genes on the endpoints of these chunks are never computed, however. This can easily be observed by looking at a dataset containing 5000 genes (so that the chunk size is 2000) and setting `rankby_abs=True`. The genes with indices 2000, 4000, 6000, .... will all be selected as marker genes with p-values of 0. This is due to the fact that the rank-sums for these genes are left at 0 (they are never computed), which is then judged to be EXTREMELY significant. . I will submit a PR with a fix for this in a minute :)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/469
https://github.com/scverse/scanpy/issues/469:289,testability,observ,observed,289,"Wilcoxon in rank_gene_group ignores genes on chunk borders; Using `method=wilcoxon` in the `rank_genes_groups` function chunks the genes into groups (to avoid using too much memory). The scores for the genes on the endpoints of these chunks are never computed, however. This can easily be observed by looking at a dataset containing 5000 genes (so that the chunk size is 2000) and setting `rankby_abs=True`. The genes with indices 2000, 4000, 6000, .... will all be selected as marker genes with p-values of 0. This is due to the fact that the rank-sums for these genes are left at 0 (they are never computed), which is then judged to be EXTREMELY significant. . I will submit a PR with a fix for this in a minute :)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/469
https://github.com/scverse/scanpy/issues/469:174,usability,memor,memory,174,"Wilcoxon in rank_gene_group ignores genes on chunk borders; Using `method=wilcoxon` in the `rank_genes_groups` function chunks the genes into groups (to avoid using too much memory). The scores for the genes on the endpoints of these chunks are never computed, however. This can easily be observed by looking at a dataset containing 5000 genes (so that the chunk size is 2000) and setting `rankby_abs=True`. The genes with indices 2000, 4000, 6000, .... will all be selected as marker genes with p-values of 0. This is due to the fact that the rank-sums for these genes are left at 0 (they are never computed), which is then judged to be EXTREMELY significant. . I will submit a PR with a fix for this in a minute :)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/469
https://github.com/scverse/scanpy/issues/470:28,deployability,log,logistic,28,rankby_abs does nothing for logistic regression in rank_genes_groups; The `rankby_abs` parameter is ignored for `method=logreg` in the `rank_genes_groups` function.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/470
https://github.com/scverse/scanpy/issues/470:120,deployability,log,logreg,120,rankby_abs does nothing for logistic regression in rank_genes_groups; The `rankby_abs` parameter is ignored for `method=logreg` in the `rank_genes_groups` function.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/470
https://github.com/scverse/scanpy/issues/470:87,modifiability,paramet,parameter,87,rankby_abs does nothing for logistic regression in rank_genes_groups; The `rankby_abs` parameter is ignored for `method=logreg` in the `rank_genes_groups` function.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/470
https://github.com/scverse/scanpy/issues/470:11,reliability,doe,does,11,rankby_abs does nothing for logistic regression in rank_genes_groups; The `rankby_abs` parameter is ignored for `method=logreg` in the `rank_genes_groups` function.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/470
https://github.com/scverse/scanpy/issues/470:28,safety,log,logistic,28,rankby_abs does nothing for logistic regression in rank_genes_groups; The `rankby_abs` parameter is ignored for `method=logreg` in the `rank_genes_groups` function.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/470
https://github.com/scverse/scanpy/issues/470:120,safety,log,logreg,120,rankby_abs does nothing for logistic regression in rank_genes_groups; The `rankby_abs` parameter is ignored for `method=logreg` in the `rank_genes_groups` function.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/470
https://github.com/scverse/scanpy/issues/470:28,security,log,logistic,28,rankby_abs does nothing for logistic regression in rank_genes_groups; The `rankby_abs` parameter is ignored for `method=logreg` in the `rank_genes_groups` function.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/470
https://github.com/scverse/scanpy/issues/470:120,security,log,logreg,120,rankby_abs does nothing for logistic regression in rank_genes_groups; The `rankby_abs` parameter is ignored for `method=logreg` in the `rank_genes_groups` function.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/470
https://github.com/scverse/scanpy/issues/470:28,testability,log,logistic,28,rankby_abs does nothing for logistic regression in rank_genes_groups; The `rankby_abs` parameter is ignored for `method=logreg` in the `rank_genes_groups` function.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/470
https://github.com/scverse/scanpy/issues/470:37,testability,regress,regression,37,rankby_abs does nothing for logistic regression in rank_genes_groups; The `rankby_abs` parameter is ignored for `method=logreg` in the `rank_genes_groups` function.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/470
https://github.com/scverse/scanpy/issues/470:120,testability,log,logreg,120,rankby_abs does nothing for logistic regression in rank_genes_groups; The `rankby_abs` parameter is ignored for `method=logreg` in the `rank_genes_groups` function.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/470
https://github.com/scverse/scanpy/pull/471:112,availability,error,error,112,"Add fixes for bugs in rank_genes_groups; This includes fixes for both #469 and #470 . #469 was a small indexing error. To fix #470, a `rankby_abs` check is included in the `logreg` section of the method that mirrors the `rankby_abs` checks in the other two methods. This PR additionally updates `select_groups` function in `scanpy/utils.py.` I was having some issues when the clusters that I was using were labelled by integers (i.e. when `adata.obs[key].cat.catagories.values.dtype` was some form of integer) AND when I was looking at a subset of the clusters (e.g. `groups=[0,1]`, not when `groups='all'`). At the start of the `rank_genes_groups` function, these cluster labels are converted into strings in the `groups_order` variable. In the `select_groups` function (line 667 of the original utils.py file), however, we call . ``` . np.where(adata.obs[key].cat.categories.values == name)[0][0]. ```. which fails with an error (since `name` is a string from `select_groups` and the elements of `adata.obs[key].cat.categories.values` are integers). Thus, this PR includes a check for the `dtype` of `adata.obs[key].cat.categories.values` - if it is numeric, we instead look at . ``` . np.where(adata.obs[key].cat.categories.values == float(name))[0][0]. ```. This error should only appear if the cluster labels are integers (since this is the only time that the cluster labels are converted to strings for `groups_order` in `rank_genes_groups`) but the above fix should also work if the cluster labels are any floating point numbers (just in case the `rank_genes_groups` is ever generalized in this way). ([Here](https://docs.scipy.org/doc/numpy-1.13.0/reference/arrays.scalars.html) is a link to the numpy type hierarchy). Edit: added a line number",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/471
https://github.com/scverse/scanpy/pull/471:376,availability,cluster,clusters,376,"Add fixes for bugs in rank_genes_groups; This includes fixes for both #469 and #470 . #469 was a small indexing error. To fix #470, a `rankby_abs` check is included in the `logreg` section of the method that mirrors the `rankby_abs` checks in the other two methods. This PR additionally updates `select_groups` function in `scanpy/utils.py.` I was having some issues when the clusters that I was using were labelled by integers (i.e. when `adata.obs[key].cat.catagories.values.dtype` was some form of integer) AND when I was looking at a subset of the clusters (e.g. `groups=[0,1]`, not when `groups='all'`). At the start of the `rank_genes_groups` function, these cluster labels are converted into strings in the `groups_order` variable. In the `select_groups` function (line 667 of the original utils.py file), however, we call . ``` . np.where(adata.obs[key].cat.categories.values == name)[0][0]. ```. which fails with an error (since `name` is a string from `select_groups` and the elements of `adata.obs[key].cat.categories.values` are integers). Thus, this PR includes a check for the `dtype` of `adata.obs[key].cat.categories.values` - if it is numeric, we instead look at . ``` . np.where(adata.obs[key].cat.categories.values == float(name))[0][0]. ```. This error should only appear if the cluster labels are integers (since this is the only time that the cluster labels are converted to strings for `groups_order` in `rank_genes_groups`) but the above fix should also work if the cluster labels are any floating point numbers (just in case the `rank_genes_groups` is ever generalized in this way). ([Here](https://docs.scipy.org/doc/numpy-1.13.0/reference/arrays.scalars.html) is a link to the numpy type hierarchy). Edit: added a line number",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/471
https://github.com/scverse/scanpy/pull/471:552,availability,cluster,clusters,552,"Add fixes for bugs in rank_genes_groups; This includes fixes for both #469 and #470 . #469 was a small indexing error. To fix #470, a `rankby_abs` check is included in the `logreg` section of the method that mirrors the `rankby_abs` checks in the other two methods. This PR additionally updates `select_groups` function in `scanpy/utils.py.` I was having some issues when the clusters that I was using were labelled by integers (i.e. when `adata.obs[key].cat.catagories.values.dtype` was some form of integer) AND when I was looking at a subset of the clusters (e.g. `groups=[0,1]`, not when `groups='all'`). At the start of the `rank_genes_groups` function, these cluster labels are converted into strings in the `groups_order` variable. In the `select_groups` function (line 667 of the original utils.py file), however, we call . ``` . np.where(adata.obs[key].cat.categories.values == name)[0][0]. ```. which fails with an error (since `name` is a string from `select_groups` and the elements of `adata.obs[key].cat.categories.values` are integers). Thus, this PR includes a check for the `dtype` of `adata.obs[key].cat.categories.values` - if it is numeric, we instead look at . ``` . np.where(adata.obs[key].cat.categories.values == float(name))[0][0]. ```. This error should only appear if the cluster labels are integers (since this is the only time that the cluster labels are converted to strings for `groups_order` in `rank_genes_groups`) but the above fix should also work if the cluster labels are any floating point numbers (just in case the `rank_genes_groups` is ever generalized in this way). ([Here](https://docs.scipy.org/doc/numpy-1.13.0/reference/arrays.scalars.html) is a link to the numpy type hierarchy). Edit: added a line number",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/471
https://github.com/scverse/scanpy/pull/471:665,availability,cluster,cluster,665,"Add fixes for bugs in rank_genes_groups; This includes fixes for both #469 and #470 . #469 was a small indexing error. To fix #470, a `rankby_abs` check is included in the `logreg` section of the method that mirrors the `rankby_abs` checks in the other two methods. This PR additionally updates `select_groups` function in `scanpy/utils.py.` I was having some issues when the clusters that I was using were labelled by integers (i.e. when `adata.obs[key].cat.catagories.values.dtype` was some form of integer) AND when I was looking at a subset of the clusters (e.g. `groups=[0,1]`, not when `groups='all'`). At the start of the `rank_genes_groups` function, these cluster labels are converted into strings in the `groups_order` variable. In the `select_groups` function (line 667 of the original utils.py file), however, we call . ``` . np.where(adata.obs[key].cat.categories.values == name)[0][0]. ```. which fails with an error (since `name` is a string from `select_groups` and the elements of `adata.obs[key].cat.categories.values` are integers). Thus, this PR includes a check for the `dtype` of `adata.obs[key].cat.categories.values` - if it is numeric, we instead look at . ``` . np.where(adata.obs[key].cat.categories.values == float(name))[0][0]. ```. This error should only appear if the cluster labels are integers (since this is the only time that the cluster labels are converted to strings for `groups_order` in `rank_genes_groups`) but the above fix should also work if the cluster labels are any floating point numbers (just in case the `rank_genes_groups` is ever generalized in this way). ([Here](https://docs.scipy.org/doc/numpy-1.13.0/reference/arrays.scalars.html) is a link to the numpy type hierarchy). Edit: added a line number",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/471
https://github.com/scverse/scanpy/pull/471:925,availability,error,error,925,"Add fixes for bugs in rank_genes_groups; This includes fixes for both #469 and #470 . #469 was a small indexing error. To fix #470, a `rankby_abs` check is included in the `logreg` section of the method that mirrors the `rankby_abs` checks in the other two methods. This PR additionally updates `select_groups` function in `scanpy/utils.py.` I was having some issues when the clusters that I was using were labelled by integers (i.e. when `adata.obs[key].cat.catagories.values.dtype` was some form of integer) AND when I was looking at a subset of the clusters (e.g. `groups=[0,1]`, not when `groups='all'`). At the start of the `rank_genes_groups` function, these cluster labels are converted into strings in the `groups_order` variable. In the `select_groups` function (line 667 of the original utils.py file), however, we call . ``` . np.where(adata.obs[key].cat.categories.values == name)[0][0]. ```. which fails with an error (since `name` is a string from `select_groups` and the elements of `adata.obs[key].cat.categories.values` are integers). Thus, this PR includes a check for the `dtype` of `adata.obs[key].cat.categories.values` - if it is numeric, we instead look at . ``` . np.where(adata.obs[key].cat.categories.values == float(name))[0][0]. ```. This error should only appear if the cluster labels are integers (since this is the only time that the cluster labels are converted to strings for `groups_order` in `rank_genes_groups`) but the above fix should also work if the cluster labels are any floating point numbers (just in case the `rank_genes_groups` is ever generalized in this way). ([Here](https://docs.scipy.org/doc/numpy-1.13.0/reference/arrays.scalars.html) is a link to the numpy type hierarchy). Edit: added a line number",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/471
https://github.com/scverse/scanpy/pull/471:1267,availability,error,error,1267,"Add fixes for bugs in rank_genes_groups; This includes fixes for both #469 and #470 . #469 was a small indexing error. To fix #470, a `rankby_abs` check is included in the `logreg` section of the method that mirrors the `rankby_abs` checks in the other two methods. This PR additionally updates `select_groups` function in `scanpy/utils.py.` I was having some issues when the clusters that I was using were labelled by integers (i.e. when `adata.obs[key].cat.catagories.values.dtype` was some form of integer) AND when I was looking at a subset of the clusters (e.g. `groups=[0,1]`, not when `groups='all'`). At the start of the `rank_genes_groups` function, these cluster labels are converted into strings in the `groups_order` variable. In the `select_groups` function (line 667 of the original utils.py file), however, we call . ``` . np.where(adata.obs[key].cat.categories.values == name)[0][0]. ```. which fails with an error (since `name` is a string from `select_groups` and the elements of `adata.obs[key].cat.categories.values` are integers). Thus, this PR includes a check for the `dtype` of `adata.obs[key].cat.categories.values` - if it is numeric, we instead look at . ``` . np.where(adata.obs[key].cat.categories.values == float(name))[0][0]. ```. This error should only appear if the cluster labels are integers (since this is the only time that the cluster labels are converted to strings for `groups_order` in `rank_genes_groups`) but the above fix should also work if the cluster labels are any floating point numbers (just in case the `rank_genes_groups` is ever generalized in this way). ([Here](https://docs.scipy.org/doc/numpy-1.13.0/reference/arrays.scalars.html) is a link to the numpy type hierarchy). Edit: added a line number",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/471
https://github.com/scverse/scanpy/pull/471:1299,availability,cluster,cluster,1299,"Add fixes for bugs in rank_genes_groups; This includes fixes for both #469 and #470 . #469 was a small indexing error. To fix #470, a `rankby_abs` check is included in the `logreg` section of the method that mirrors the `rankby_abs` checks in the other two methods. This PR additionally updates `select_groups` function in `scanpy/utils.py.` I was having some issues when the clusters that I was using were labelled by integers (i.e. when `adata.obs[key].cat.catagories.values.dtype` was some form of integer) AND when I was looking at a subset of the clusters (e.g. `groups=[0,1]`, not when `groups='all'`). At the start of the `rank_genes_groups` function, these cluster labels are converted into strings in the `groups_order` variable. In the `select_groups` function (line 667 of the original utils.py file), however, we call . ``` . np.where(adata.obs[key].cat.categories.values == name)[0][0]. ```. which fails with an error (since `name` is a string from `select_groups` and the elements of `adata.obs[key].cat.categories.values` are integers). Thus, this PR includes a check for the `dtype` of `adata.obs[key].cat.categories.values` - if it is numeric, we instead look at . ``` . np.where(adata.obs[key].cat.categories.values == float(name))[0][0]. ```. This error should only appear if the cluster labels are integers (since this is the only time that the cluster labels are converted to strings for `groups_order` in `rank_genes_groups`) but the above fix should also work if the cluster labels are any floating point numbers (just in case the `rank_genes_groups` is ever generalized in this way). ([Here](https://docs.scipy.org/doc/numpy-1.13.0/reference/arrays.scalars.html) is a link to the numpy type hierarchy). Edit: added a line number",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/471
https://github.com/scverse/scanpy/pull/471:1365,availability,cluster,cluster,1365,"Add fixes for bugs in rank_genes_groups; This includes fixes for both #469 and #470 . #469 was a small indexing error. To fix #470, a `rankby_abs` check is included in the `logreg` section of the method that mirrors the `rankby_abs` checks in the other two methods. This PR additionally updates `select_groups` function in `scanpy/utils.py.` I was having some issues when the clusters that I was using were labelled by integers (i.e. when `adata.obs[key].cat.catagories.values.dtype` was some form of integer) AND when I was looking at a subset of the clusters (e.g. `groups=[0,1]`, not when `groups='all'`). At the start of the `rank_genes_groups` function, these cluster labels are converted into strings in the `groups_order` variable. In the `select_groups` function (line 667 of the original utils.py file), however, we call . ``` . np.where(adata.obs[key].cat.categories.values == name)[0][0]. ```. which fails with an error (since `name` is a string from `select_groups` and the elements of `adata.obs[key].cat.categories.values` are integers). Thus, this PR includes a check for the `dtype` of `adata.obs[key].cat.categories.values` - if it is numeric, we instead look at . ``` . np.where(adata.obs[key].cat.categories.values == float(name))[0][0]. ```. This error should only appear if the cluster labels are integers (since this is the only time that the cluster labels are converted to strings for `groups_order` in `rank_genes_groups`) but the above fix should also work if the cluster labels are any floating point numbers (just in case the `rank_genes_groups` is ever generalized in this way). ([Here](https://docs.scipy.org/doc/numpy-1.13.0/reference/arrays.scalars.html) is a link to the numpy type hierarchy). Edit: added a line number",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/471
https://github.com/scverse/scanpy/pull/471:1490,availability,cluster,cluster,1490,"Add fixes for bugs in rank_genes_groups; This includes fixes for both #469 and #470 . #469 was a small indexing error. To fix #470, a `rankby_abs` check is included in the `logreg` section of the method that mirrors the `rankby_abs` checks in the other two methods. This PR additionally updates `select_groups` function in `scanpy/utils.py.` I was having some issues when the clusters that I was using were labelled by integers (i.e. when `adata.obs[key].cat.catagories.values.dtype` was some form of integer) AND when I was looking at a subset of the clusters (e.g. `groups=[0,1]`, not when `groups='all'`). At the start of the `rank_genes_groups` function, these cluster labels are converted into strings in the `groups_order` variable. In the `select_groups` function (line 667 of the original utils.py file), however, we call . ``` . np.where(adata.obs[key].cat.categories.values == name)[0][0]. ```. which fails with an error (since `name` is a string from `select_groups` and the elements of `adata.obs[key].cat.categories.values` are integers). Thus, this PR includes a check for the `dtype` of `adata.obs[key].cat.categories.values` - if it is numeric, we instead look at . ``` . np.where(adata.obs[key].cat.categories.values == float(name))[0][0]. ```. This error should only appear if the cluster labels are integers (since this is the only time that the cluster labels are converted to strings for `groups_order` in `rank_genes_groups`) but the above fix should also work if the cluster labels are any floating point numbers (just in case the `rank_genes_groups` is ever generalized in this way). ([Here](https://docs.scipy.org/doc/numpy-1.13.0/reference/arrays.scalars.html) is a link to the numpy type hierarchy). Edit: added a line number",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/471
https://github.com/scverse/scanpy/pull/471:173,deployability,log,logreg,173,"Add fixes for bugs in rank_genes_groups; This includes fixes for both #469 and #470 . #469 was a small indexing error. To fix #470, a `rankby_abs` check is included in the `logreg` section of the method that mirrors the `rankby_abs` checks in the other two methods. This PR additionally updates `select_groups` function in `scanpy/utils.py.` I was having some issues when the clusters that I was using were labelled by integers (i.e. when `adata.obs[key].cat.catagories.values.dtype` was some form of integer) AND when I was looking at a subset of the clusters (e.g. `groups=[0,1]`, not when `groups='all'`). At the start of the `rank_genes_groups` function, these cluster labels are converted into strings in the `groups_order` variable. In the `select_groups` function (line 667 of the original utils.py file), however, we call . ``` . np.where(adata.obs[key].cat.categories.values == name)[0][0]. ```. which fails with an error (since `name` is a string from `select_groups` and the elements of `adata.obs[key].cat.categories.values` are integers). Thus, this PR includes a check for the `dtype` of `adata.obs[key].cat.categories.values` - if it is numeric, we instead look at . ``` . np.where(adata.obs[key].cat.categories.values == float(name))[0][0]. ```. This error should only appear if the cluster labels are integers (since this is the only time that the cluster labels are converted to strings for `groups_order` in `rank_genes_groups`) but the above fix should also work if the cluster labels are any floating point numbers (just in case the `rank_genes_groups` is ever generalized in this way). ([Here](https://docs.scipy.org/doc/numpy-1.13.0/reference/arrays.scalars.html) is a link to the numpy type hierarchy). Edit: added a line number",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/471
https://github.com/scverse/scanpy/pull/471:287,deployability,updat,updates,287,"Add fixes for bugs in rank_genes_groups; This includes fixes for both #469 and #470 . #469 was a small indexing error. To fix #470, a `rankby_abs` check is included in the `logreg` section of the method that mirrors the `rankby_abs` checks in the other two methods. This PR additionally updates `select_groups` function in `scanpy/utils.py.` I was having some issues when the clusters that I was using were labelled by integers (i.e. when `adata.obs[key].cat.catagories.values.dtype` was some form of integer) AND when I was looking at a subset of the clusters (e.g. `groups=[0,1]`, not when `groups='all'`). At the start of the `rank_genes_groups` function, these cluster labels are converted into strings in the `groups_order` variable. In the `select_groups` function (line 667 of the original utils.py file), however, we call . ``` . np.where(adata.obs[key].cat.categories.values == name)[0][0]. ```. which fails with an error (since `name` is a string from `select_groups` and the elements of `adata.obs[key].cat.categories.values` are integers). Thus, this PR includes a check for the `dtype` of `adata.obs[key].cat.categories.values` - if it is numeric, we instead look at . ``` . np.where(adata.obs[key].cat.categories.values == float(name))[0][0]. ```. This error should only appear if the cluster labels are integers (since this is the only time that the cluster labels are converted to strings for `groups_order` in `rank_genes_groups`) but the above fix should also work if the cluster labels are any floating point numbers (just in case the `rank_genes_groups` is ever generalized in this way). ([Here](https://docs.scipy.org/doc/numpy-1.13.0/reference/arrays.scalars.html) is a link to the numpy type hierarchy). Edit: added a line number",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/471
https://github.com/scverse/scanpy/pull/471:376,deployability,cluster,clusters,376,"Add fixes for bugs in rank_genes_groups; This includes fixes for both #469 and #470 . #469 was a small indexing error. To fix #470, a `rankby_abs` check is included in the `logreg` section of the method that mirrors the `rankby_abs` checks in the other two methods. This PR additionally updates `select_groups` function in `scanpy/utils.py.` I was having some issues when the clusters that I was using were labelled by integers (i.e. when `adata.obs[key].cat.catagories.values.dtype` was some form of integer) AND when I was looking at a subset of the clusters (e.g. `groups=[0,1]`, not when `groups='all'`). At the start of the `rank_genes_groups` function, these cluster labels are converted into strings in the `groups_order` variable. In the `select_groups` function (line 667 of the original utils.py file), however, we call . ``` . np.where(adata.obs[key].cat.categories.values == name)[0][0]. ```. which fails with an error (since `name` is a string from `select_groups` and the elements of `adata.obs[key].cat.categories.values` are integers). Thus, this PR includes a check for the `dtype` of `adata.obs[key].cat.categories.values` - if it is numeric, we instead look at . ``` . np.where(adata.obs[key].cat.categories.values == float(name))[0][0]. ```. This error should only appear if the cluster labels are integers (since this is the only time that the cluster labels are converted to strings for `groups_order` in `rank_genes_groups`) but the above fix should also work if the cluster labels are any floating point numbers (just in case the `rank_genes_groups` is ever generalized in this way). ([Here](https://docs.scipy.org/doc/numpy-1.13.0/reference/arrays.scalars.html) is a link to the numpy type hierarchy). Edit: added a line number",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/471
https://github.com/scverse/scanpy/pull/471:552,deployability,cluster,clusters,552,"Add fixes for bugs in rank_genes_groups; This includes fixes for both #469 and #470 . #469 was a small indexing error. To fix #470, a `rankby_abs` check is included in the `logreg` section of the method that mirrors the `rankby_abs` checks in the other two methods. This PR additionally updates `select_groups` function in `scanpy/utils.py.` I was having some issues when the clusters that I was using were labelled by integers (i.e. when `adata.obs[key].cat.catagories.values.dtype` was some form of integer) AND when I was looking at a subset of the clusters (e.g. `groups=[0,1]`, not when `groups='all'`). At the start of the `rank_genes_groups` function, these cluster labels are converted into strings in the `groups_order` variable. In the `select_groups` function (line 667 of the original utils.py file), however, we call . ``` . np.where(adata.obs[key].cat.categories.values == name)[0][0]. ```. which fails with an error (since `name` is a string from `select_groups` and the elements of `adata.obs[key].cat.categories.values` are integers). Thus, this PR includes a check for the `dtype` of `adata.obs[key].cat.categories.values` - if it is numeric, we instead look at . ``` . np.where(adata.obs[key].cat.categories.values == float(name))[0][0]. ```. This error should only appear if the cluster labels are integers (since this is the only time that the cluster labels are converted to strings for `groups_order` in `rank_genes_groups`) but the above fix should also work if the cluster labels are any floating point numbers (just in case the `rank_genes_groups` is ever generalized in this way). ([Here](https://docs.scipy.org/doc/numpy-1.13.0/reference/arrays.scalars.html) is a link to the numpy type hierarchy). Edit: added a line number",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/471
https://github.com/scverse/scanpy/pull/471:665,deployability,cluster,cluster,665,"Add fixes for bugs in rank_genes_groups; This includes fixes for both #469 and #470 . #469 was a small indexing error. To fix #470, a `rankby_abs` check is included in the `logreg` section of the method that mirrors the `rankby_abs` checks in the other two methods. This PR additionally updates `select_groups` function in `scanpy/utils.py.` I was having some issues when the clusters that I was using were labelled by integers (i.e. when `adata.obs[key].cat.catagories.values.dtype` was some form of integer) AND when I was looking at a subset of the clusters (e.g. `groups=[0,1]`, not when `groups='all'`). At the start of the `rank_genes_groups` function, these cluster labels are converted into strings in the `groups_order` variable. In the `select_groups` function (line 667 of the original utils.py file), however, we call . ``` . np.where(adata.obs[key].cat.categories.values == name)[0][0]. ```. which fails with an error (since `name` is a string from `select_groups` and the elements of `adata.obs[key].cat.categories.values` are integers). Thus, this PR includes a check for the `dtype` of `adata.obs[key].cat.categories.values` - if it is numeric, we instead look at . ``` . np.where(adata.obs[key].cat.categories.values == float(name))[0][0]. ```. This error should only appear if the cluster labels are integers (since this is the only time that the cluster labels are converted to strings for `groups_order` in `rank_genes_groups`) but the above fix should also work if the cluster labels are any floating point numbers (just in case the `rank_genes_groups` is ever generalized in this way). ([Here](https://docs.scipy.org/doc/numpy-1.13.0/reference/arrays.scalars.html) is a link to the numpy type hierarchy). Edit: added a line number",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/471
https://github.com/scverse/scanpy/pull/471:911,deployability,fail,fails,911,"Add fixes for bugs in rank_genes_groups; This includes fixes for both #469 and #470 . #469 was a small indexing error. To fix #470, a `rankby_abs` check is included in the `logreg` section of the method that mirrors the `rankby_abs` checks in the other two methods. This PR additionally updates `select_groups` function in `scanpy/utils.py.` I was having some issues when the clusters that I was using were labelled by integers (i.e. when `adata.obs[key].cat.catagories.values.dtype` was some form of integer) AND when I was looking at a subset of the clusters (e.g. `groups=[0,1]`, not when `groups='all'`). At the start of the `rank_genes_groups` function, these cluster labels are converted into strings in the `groups_order` variable. In the `select_groups` function (line 667 of the original utils.py file), however, we call . ``` . np.where(adata.obs[key].cat.categories.values == name)[0][0]. ```. which fails with an error (since `name` is a string from `select_groups` and the elements of `adata.obs[key].cat.categories.values` are integers). Thus, this PR includes a check for the `dtype` of `adata.obs[key].cat.categories.values` - if it is numeric, we instead look at . ``` . np.where(adata.obs[key].cat.categories.values == float(name))[0][0]. ```. This error should only appear if the cluster labels are integers (since this is the only time that the cluster labels are converted to strings for `groups_order` in `rank_genes_groups`) but the above fix should also work if the cluster labels are any floating point numbers (just in case the `rank_genes_groups` is ever generalized in this way). ([Here](https://docs.scipy.org/doc/numpy-1.13.0/reference/arrays.scalars.html) is a link to the numpy type hierarchy). Edit: added a line number",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/471
https://github.com/scverse/scanpy/pull/471:1299,deployability,cluster,cluster,1299,"Add fixes for bugs in rank_genes_groups; This includes fixes for both #469 and #470 . #469 was a small indexing error. To fix #470, a `rankby_abs` check is included in the `logreg` section of the method that mirrors the `rankby_abs` checks in the other two methods. This PR additionally updates `select_groups` function in `scanpy/utils.py.` I was having some issues when the clusters that I was using were labelled by integers (i.e. when `adata.obs[key].cat.catagories.values.dtype` was some form of integer) AND when I was looking at a subset of the clusters (e.g. `groups=[0,1]`, not when `groups='all'`). At the start of the `rank_genes_groups` function, these cluster labels are converted into strings in the `groups_order` variable. In the `select_groups` function (line 667 of the original utils.py file), however, we call . ``` . np.where(adata.obs[key].cat.categories.values == name)[0][0]. ```. which fails with an error (since `name` is a string from `select_groups` and the elements of `adata.obs[key].cat.categories.values` are integers). Thus, this PR includes a check for the `dtype` of `adata.obs[key].cat.categories.values` - if it is numeric, we instead look at . ``` . np.where(adata.obs[key].cat.categories.values == float(name))[0][0]. ```. This error should only appear if the cluster labels are integers (since this is the only time that the cluster labels are converted to strings for `groups_order` in `rank_genes_groups`) but the above fix should also work if the cluster labels are any floating point numbers (just in case the `rank_genes_groups` is ever generalized in this way). ([Here](https://docs.scipy.org/doc/numpy-1.13.0/reference/arrays.scalars.html) is a link to the numpy type hierarchy). Edit: added a line number",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/471
https://github.com/scverse/scanpy/pull/471:1365,deployability,cluster,cluster,1365,"Add fixes for bugs in rank_genes_groups; This includes fixes for both #469 and #470 . #469 was a small indexing error. To fix #470, a `rankby_abs` check is included in the `logreg` section of the method that mirrors the `rankby_abs` checks in the other two methods. This PR additionally updates `select_groups` function in `scanpy/utils.py.` I was having some issues when the clusters that I was using were labelled by integers (i.e. when `adata.obs[key].cat.catagories.values.dtype` was some form of integer) AND when I was looking at a subset of the clusters (e.g. `groups=[0,1]`, not when `groups='all'`). At the start of the `rank_genes_groups` function, these cluster labels are converted into strings in the `groups_order` variable. In the `select_groups` function (line 667 of the original utils.py file), however, we call . ``` . np.where(adata.obs[key].cat.categories.values == name)[0][0]. ```. which fails with an error (since `name` is a string from `select_groups` and the elements of `adata.obs[key].cat.categories.values` are integers). Thus, this PR includes a check for the `dtype` of `adata.obs[key].cat.categories.values` - if it is numeric, we instead look at . ``` . np.where(adata.obs[key].cat.categories.values == float(name))[0][0]. ```. This error should only appear if the cluster labels are integers (since this is the only time that the cluster labels are converted to strings for `groups_order` in `rank_genes_groups`) but the above fix should also work if the cluster labels are any floating point numbers (just in case the `rank_genes_groups` is ever generalized in this way). ([Here](https://docs.scipy.org/doc/numpy-1.13.0/reference/arrays.scalars.html) is a link to the numpy type hierarchy). Edit: added a line number",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/471
https://github.com/scverse/scanpy/pull/471:1490,deployability,cluster,cluster,1490,"Add fixes for bugs in rank_genes_groups; This includes fixes for both #469 and #470 . #469 was a small indexing error. To fix #470, a `rankby_abs` check is included in the `logreg` section of the method that mirrors the `rankby_abs` checks in the other two methods. This PR additionally updates `select_groups` function in `scanpy/utils.py.` I was having some issues when the clusters that I was using were labelled by integers (i.e. when `adata.obs[key].cat.catagories.values.dtype` was some form of integer) AND when I was looking at a subset of the clusters (e.g. `groups=[0,1]`, not when `groups='all'`). At the start of the `rank_genes_groups` function, these cluster labels are converted into strings in the `groups_order` variable. In the `select_groups` function (line 667 of the original utils.py file), however, we call . ``` . np.where(adata.obs[key].cat.categories.values == name)[0][0]. ```. which fails with an error (since `name` is a string from `select_groups` and the elements of `adata.obs[key].cat.categories.values` are integers). Thus, this PR includes a check for the `dtype` of `adata.obs[key].cat.categories.values` - if it is numeric, we instead look at . ``` . np.where(adata.obs[key].cat.categories.values == float(name))[0][0]. ```. This error should only appear if the cluster labels are integers (since this is the only time that the cluster labels are converted to strings for `groups_order` in `rank_genes_groups`) but the above fix should also work if the cluster labels are any floating point numbers (just in case the `rank_genes_groups` is ever generalized in this way). ([Here](https://docs.scipy.org/doc/numpy-1.13.0/reference/arrays.scalars.html) is a link to the numpy type hierarchy). Edit: added a line number",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/471
https://github.com/scverse/scanpy/pull/471:538,integrability,sub,subset,538,"Add fixes for bugs in rank_genes_groups; This includes fixes for both #469 and #470 . #469 was a small indexing error. To fix #470, a `rankby_abs` check is included in the `logreg` section of the method that mirrors the `rankby_abs` checks in the other two methods. This PR additionally updates `select_groups` function in `scanpy/utils.py.` I was having some issues when the clusters that I was using were labelled by integers (i.e. when `adata.obs[key].cat.catagories.values.dtype` was some form of integer) AND when I was looking at a subset of the clusters (e.g. `groups=[0,1]`, not when `groups='all'`). At the start of the `rank_genes_groups` function, these cluster labels are converted into strings in the `groups_order` variable. In the `select_groups` function (line 667 of the original utils.py file), however, we call . ``` . np.where(adata.obs[key].cat.categories.values == name)[0][0]. ```. which fails with an error (since `name` is a string from `select_groups` and the elements of `adata.obs[key].cat.categories.values` are integers). Thus, this PR includes a check for the `dtype` of `adata.obs[key].cat.categories.values` - if it is numeric, we instead look at . ``` . np.where(adata.obs[key].cat.categories.values == float(name))[0][0]. ```. This error should only appear if the cluster labels are integers (since this is the only time that the cluster labels are converted to strings for `groups_order` in `rank_genes_groups`) but the above fix should also work if the cluster labels are any floating point numbers (just in case the `rank_genes_groups` is ever generalized in this way). ([Here](https://docs.scipy.org/doc/numpy-1.13.0/reference/arrays.scalars.html) is a link to the numpy type hierarchy). Edit: added a line number",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/471
https://github.com/scverse/scanpy/pull/471:729,modifiability,variab,variable,729,"Add fixes for bugs in rank_genes_groups; This includes fixes for both #469 and #470 . #469 was a small indexing error. To fix #470, a `rankby_abs` check is included in the `logreg` section of the method that mirrors the `rankby_abs` checks in the other two methods. This PR additionally updates `select_groups` function in `scanpy/utils.py.` I was having some issues when the clusters that I was using were labelled by integers (i.e. when `adata.obs[key].cat.catagories.values.dtype` was some form of integer) AND when I was looking at a subset of the clusters (e.g. `groups=[0,1]`, not when `groups='all'`). At the start of the `rank_genes_groups` function, these cluster labels are converted into strings in the `groups_order` variable. In the `select_groups` function (line 667 of the original utils.py file), however, we call . ``` . np.where(adata.obs[key].cat.categories.values == name)[0][0]. ```. which fails with an error (since `name` is a string from `select_groups` and the elements of `adata.obs[key].cat.categories.values` are integers). Thus, this PR includes a check for the `dtype` of `adata.obs[key].cat.categories.values` - if it is numeric, we instead look at . ``` . np.where(adata.obs[key].cat.categories.values == float(name))[0][0]. ```. This error should only appear if the cluster labels are integers (since this is the only time that the cluster labels are converted to strings for `groups_order` in `rank_genes_groups`) but the above fix should also work if the cluster labels are any floating point numbers (just in case the `rank_genes_groups` is ever generalized in this way). ([Here](https://docs.scipy.org/doc/numpy-1.13.0/reference/arrays.scalars.html) is a link to the numpy type hierarchy). Edit: added a line number",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/471
https://github.com/scverse/scanpy/pull/471:1673,modifiability,scal,scalars,1673,"Add fixes for bugs in rank_genes_groups; This includes fixes for both #469 and #470 . #469 was a small indexing error. To fix #470, a `rankby_abs` check is included in the `logreg` section of the method that mirrors the `rankby_abs` checks in the other two methods. This PR additionally updates `select_groups` function in `scanpy/utils.py.` I was having some issues when the clusters that I was using were labelled by integers (i.e. when `adata.obs[key].cat.catagories.values.dtype` was some form of integer) AND when I was looking at a subset of the clusters (e.g. `groups=[0,1]`, not when `groups='all'`). At the start of the `rank_genes_groups` function, these cluster labels are converted into strings in the `groups_order` variable. In the `select_groups` function (line 667 of the original utils.py file), however, we call . ``` . np.where(adata.obs[key].cat.categories.values == name)[0][0]. ```. which fails with an error (since `name` is a string from `select_groups` and the elements of `adata.obs[key].cat.categories.values` are integers). Thus, this PR includes a check for the `dtype` of `adata.obs[key].cat.categories.values` - if it is numeric, we instead look at . ``` . np.where(adata.obs[key].cat.categories.values == float(name))[0][0]. ```. This error should only appear if the cluster labels are integers (since this is the only time that the cluster labels are converted to strings for `groups_order` in `rank_genes_groups`) but the above fix should also work if the cluster labels are any floating point numbers (just in case the `rank_genes_groups` is ever generalized in this way). ([Here](https://docs.scipy.org/doc/numpy-1.13.0/reference/arrays.scalars.html) is a link to the numpy type hierarchy). Edit: added a line number",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/471
https://github.com/scverse/scanpy/pull/471:112,performance,error,error,112,"Add fixes for bugs in rank_genes_groups; This includes fixes for both #469 and #470 . #469 was a small indexing error. To fix #470, a `rankby_abs` check is included in the `logreg` section of the method that mirrors the `rankby_abs` checks in the other two methods. This PR additionally updates `select_groups` function in `scanpy/utils.py.` I was having some issues when the clusters that I was using were labelled by integers (i.e. when `adata.obs[key].cat.catagories.values.dtype` was some form of integer) AND when I was looking at a subset of the clusters (e.g. `groups=[0,1]`, not when `groups='all'`). At the start of the `rank_genes_groups` function, these cluster labels are converted into strings in the `groups_order` variable. In the `select_groups` function (line 667 of the original utils.py file), however, we call . ``` . np.where(adata.obs[key].cat.categories.values == name)[0][0]. ```. which fails with an error (since `name` is a string from `select_groups` and the elements of `adata.obs[key].cat.categories.values` are integers). Thus, this PR includes a check for the `dtype` of `adata.obs[key].cat.categories.values` - if it is numeric, we instead look at . ``` . np.where(adata.obs[key].cat.categories.values == float(name))[0][0]. ```. This error should only appear if the cluster labels are integers (since this is the only time that the cluster labels are converted to strings for `groups_order` in `rank_genes_groups`) but the above fix should also work if the cluster labels are any floating point numbers (just in case the `rank_genes_groups` is ever generalized in this way). ([Here](https://docs.scipy.org/doc/numpy-1.13.0/reference/arrays.scalars.html) is a link to the numpy type hierarchy). Edit: added a line number",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/471
https://github.com/scverse/scanpy/pull/471:925,performance,error,error,925,"Add fixes for bugs in rank_genes_groups; This includes fixes for both #469 and #470 . #469 was a small indexing error. To fix #470, a `rankby_abs` check is included in the `logreg` section of the method that mirrors the `rankby_abs` checks in the other two methods. This PR additionally updates `select_groups` function in `scanpy/utils.py.` I was having some issues when the clusters that I was using were labelled by integers (i.e. when `adata.obs[key].cat.catagories.values.dtype` was some form of integer) AND when I was looking at a subset of the clusters (e.g. `groups=[0,1]`, not when `groups='all'`). At the start of the `rank_genes_groups` function, these cluster labels are converted into strings in the `groups_order` variable. In the `select_groups` function (line 667 of the original utils.py file), however, we call . ``` . np.where(adata.obs[key].cat.categories.values == name)[0][0]. ```. which fails with an error (since `name` is a string from `select_groups` and the elements of `adata.obs[key].cat.categories.values` are integers). Thus, this PR includes a check for the `dtype` of `adata.obs[key].cat.categories.values` - if it is numeric, we instead look at . ``` . np.where(adata.obs[key].cat.categories.values == float(name))[0][0]. ```. This error should only appear if the cluster labels are integers (since this is the only time that the cluster labels are converted to strings for `groups_order` in `rank_genes_groups`) but the above fix should also work if the cluster labels are any floating point numbers (just in case the `rank_genes_groups` is ever generalized in this way). ([Here](https://docs.scipy.org/doc/numpy-1.13.0/reference/arrays.scalars.html) is a link to the numpy type hierarchy). Edit: added a line number",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/471
https://github.com/scverse/scanpy/pull/471:1267,performance,error,error,1267,"Add fixes for bugs in rank_genes_groups; This includes fixes for both #469 and #470 . #469 was a small indexing error. To fix #470, a `rankby_abs` check is included in the `logreg` section of the method that mirrors the `rankby_abs` checks in the other two methods. This PR additionally updates `select_groups` function in `scanpy/utils.py.` I was having some issues when the clusters that I was using were labelled by integers (i.e. when `adata.obs[key].cat.catagories.values.dtype` was some form of integer) AND when I was looking at a subset of the clusters (e.g. `groups=[0,1]`, not when `groups='all'`). At the start of the `rank_genes_groups` function, these cluster labels are converted into strings in the `groups_order` variable. In the `select_groups` function (line 667 of the original utils.py file), however, we call . ``` . np.where(adata.obs[key].cat.categories.values == name)[0][0]. ```. which fails with an error (since `name` is a string from `select_groups` and the elements of `adata.obs[key].cat.categories.values` are integers). Thus, this PR includes a check for the `dtype` of `adata.obs[key].cat.categories.values` - if it is numeric, we instead look at . ``` . np.where(adata.obs[key].cat.categories.values == float(name))[0][0]. ```. This error should only appear if the cluster labels are integers (since this is the only time that the cluster labels are converted to strings for `groups_order` in `rank_genes_groups`) but the above fix should also work if the cluster labels are any floating point numbers (just in case the `rank_genes_groups` is ever generalized in this way). ([Here](https://docs.scipy.org/doc/numpy-1.13.0/reference/arrays.scalars.html) is a link to the numpy type hierarchy). Edit: added a line number",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/471
https://github.com/scverse/scanpy/pull/471:1351,performance,time,time,1351,"Add fixes for bugs in rank_genes_groups; This includes fixes for both #469 and #470 . #469 was a small indexing error. To fix #470, a `rankby_abs` check is included in the `logreg` section of the method that mirrors the `rankby_abs` checks in the other two methods. This PR additionally updates `select_groups` function in `scanpy/utils.py.` I was having some issues when the clusters that I was using were labelled by integers (i.e. when `adata.obs[key].cat.catagories.values.dtype` was some form of integer) AND when I was looking at a subset of the clusters (e.g. `groups=[0,1]`, not when `groups='all'`). At the start of the `rank_genes_groups` function, these cluster labels are converted into strings in the `groups_order` variable. In the `select_groups` function (line 667 of the original utils.py file), however, we call . ``` . np.where(adata.obs[key].cat.categories.values == name)[0][0]. ```. which fails with an error (since `name` is a string from `select_groups` and the elements of `adata.obs[key].cat.categories.values` are integers). Thus, this PR includes a check for the `dtype` of `adata.obs[key].cat.categories.values` - if it is numeric, we instead look at . ``` . np.where(adata.obs[key].cat.categories.values == float(name))[0][0]. ```. This error should only appear if the cluster labels are integers (since this is the only time that the cluster labels are converted to strings for `groups_order` in `rank_genes_groups`) but the above fix should also work if the cluster labels are any floating point numbers (just in case the `rank_genes_groups` is ever generalized in this way). ([Here](https://docs.scipy.org/doc/numpy-1.13.0/reference/arrays.scalars.html) is a link to the numpy type hierarchy). Edit: added a line number",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/471
https://github.com/scverse/scanpy/pull/471:911,reliability,fail,fails,911,"Add fixes for bugs in rank_genes_groups; This includes fixes for both #469 and #470 . #469 was a small indexing error. To fix #470, a `rankby_abs` check is included in the `logreg` section of the method that mirrors the `rankby_abs` checks in the other two methods. This PR additionally updates `select_groups` function in `scanpy/utils.py.` I was having some issues when the clusters that I was using were labelled by integers (i.e. when `adata.obs[key].cat.catagories.values.dtype` was some form of integer) AND when I was looking at a subset of the clusters (e.g. `groups=[0,1]`, not when `groups='all'`). At the start of the `rank_genes_groups` function, these cluster labels are converted into strings in the `groups_order` variable. In the `select_groups` function (line 667 of the original utils.py file), however, we call . ``` . np.where(adata.obs[key].cat.categories.values == name)[0][0]. ```. which fails with an error (since `name` is a string from `select_groups` and the elements of `adata.obs[key].cat.categories.values` are integers). Thus, this PR includes a check for the `dtype` of `adata.obs[key].cat.categories.values` - if it is numeric, we instead look at . ``` . np.where(adata.obs[key].cat.categories.values == float(name))[0][0]. ```. This error should only appear if the cluster labels are integers (since this is the only time that the cluster labels are converted to strings for `groups_order` in `rank_genes_groups`) but the above fix should also work if the cluster labels are any floating point numbers (just in case the `rank_genes_groups` is ever generalized in this way). ([Here](https://docs.scipy.org/doc/numpy-1.13.0/reference/arrays.scalars.html) is a link to the numpy type hierarchy). Edit: added a line number",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/471
https://github.com/scverse/scanpy/pull/471:112,safety,error,error,112,"Add fixes for bugs in rank_genes_groups; This includes fixes for both #469 and #470 . #469 was a small indexing error. To fix #470, a `rankby_abs` check is included in the `logreg` section of the method that mirrors the `rankby_abs` checks in the other two methods. This PR additionally updates `select_groups` function in `scanpy/utils.py.` I was having some issues when the clusters that I was using were labelled by integers (i.e. when `adata.obs[key].cat.catagories.values.dtype` was some form of integer) AND when I was looking at a subset of the clusters (e.g. `groups=[0,1]`, not when `groups='all'`). At the start of the `rank_genes_groups` function, these cluster labels are converted into strings in the `groups_order` variable. In the `select_groups` function (line 667 of the original utils.py file), however, we call . ``` . np.where(adata.obs[key].cat.categories.values == name)[0][0]. ```. which fails with an error (since `name` is a string from `select_groups` and the elements of `adata.obs[key].cat.categories.values` are integers). Thus, this PR includes a check for the `dtype` of `adata.obs[key].cat.categories.values` - if it is numeric, we instead look at . ``` . np.where(adata.obs[key].cat.categories.values == float(name))[0][0]. ```. This error should only appear if the cluster labels are integers (since this is the only time that the cluster labels are converted to strings for `groups_order` in `rank_genes_groups`) but the above fix should also work if the cluster labels are any floating point numbers (just in case the `rank_genes_groups` is ever generalized in this way). ([Here](https://docs.scipy.org/doc/numpy-1.13.0/reference/arrays.scalars.html) is a link to the numpy type hierarchy). Edit: added a line number",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/471
https://github.com/scverse/scanpy/pull/471:173,safety,log,logreg,173,"Add fixes for bugs in rank_genes_groups; This includes fixes for both #469 and #470 . #469 was a small indexing error. To fix #470, a `rankby_abs` check is included in the `logreg` section of the method that mirrors the `rankby_abs` checks in the other two methods. This PR additionally updates `select_groups` function in `scanpy/utils.py.` I was having some issues when the clusters that I was using were labelled by integers (i.e. when `adata.obs[key].cat.catagories.values.dtype` was some form of integer) AND when I was looking at a subset of the clusters (e.g. `groups=[0,1]`, not when `groups='all'`). At the start of the `rank_genes_groups` function, these cluster labels are converted into strings in the `groups_order` variable. In the `select_groups` function (line 667 of the original utils.py file), however, we call . ``` . np.where(adata.obs[key].cat.categories.values == name)[0][0]. ```. which fails with an error (since `name` is a string from `select_groups` and the elements of `adata.obs[key].cat.categories.values` are integers). Thus, this PR includes a check for the `dtype` of `adata.obs[key].cat.categories.values` - if it is numeric, we instead look at . ``` . np.where(adata.obs[key].cat.categories.values == float(name))[0][0]. ```. This error should only appear if the cluster labels are integers (since this is the only time that the cluster labels are converted to strings for `groups_order` in `rank_genes_groups`) but the above fix should also work if the cluster labels are any floating point numbers (just in case the `rank_genes_groups` is ever generalized in this way). ([Here](https://docs.scipy.org/doc/numpy-1.13.0/reference/arrays.scalars.html) is a link to the numpy type hierarchy). Edit: added a line number",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/471
https://github.com/scverse/scanpy/pull/471:287,safety,updat,updates,287,"Add fixes for bugs in rank_genes_groups; This includes fixes for both #469 and #470 . #469 was a small indexing error. To fix #470, a `rankby_abs` check is included in the `logreg` section of the method that mirrors the `rankby_abs` checks in the other two methods. This PR additionally updates `select_groups` function in `scanpy/utils.py.` I was having some issues when the clusters that I was using were labelled by integers (i.e. when `adata.obs[key].cat.catagories.values.dtype` was some form of integer) AND when I was looking at a subset of the clusters (e.g. `groups=[0,1]`, not when `groups='all'`). At the start of the `rank_genes_groups` function, these cluster labels are converted into strings in the `groups_order` variable. In the `select_groups` function (line 667 of the original utils.py file), however, we call . ``` . np.where(adata.obs[key].cat.categories.values == name)[0][0]. ```. which fails with an error (since `name` is a string from `select_groups` and the elements of `adata.obs[key].cat.categories.values` are integers). Thus, this PR includes a check for the `dtype` of `adata.obs[key].cat.categories.values` - if it is numeric, we instead look at . ``` . np.where(adata.obs[key].cat.categories.values == float(name))[0][0]. ```. This error should only appear if the cluster labels are integers (since this is the only time that the cluster labels are converted to strings for `groups_order` in `rank_genes_groups`) but the above fix should also work if the cluster labels are any floating point numbers (just in case the `rank_genes_groups` is ever generalized in this way). ([Here](https://docs.scipy.org/doc/numpy-1.13.0/reference/arrays.scalars.html) is a link to the numpy type hierarchy). Edit: added a line number",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/471
https://github.com/scverse/scanpy/pull/471:925,safety,error,error,925,"Add fixes for bugs in rank_genes_groups; This includes fixes for both #469 and #470 . #469 was a small indexing error. To fix #470, a `rankby_abs` check is included in the `logreg` section of the method that mirrors the `rankby_abs` checks in the other two methods. This PR additionally updates `select_groups` function in `scanpy/utils.py.` I was having some issues when the clusters that I was using were labelled by integers (i.e. when `adata.obs[key].cat.catagories.values.dtype` was some form of integer) AND when I was looking at a subset of the clusters (e.g. `groups=[0,1]`, not when `groups='all'`). At the start of the `rank_genes_groups` function, these cluster labels are converted into strings in the `groups_order` variable. In the `select_groups` function (line 667 of the original utils.py file), however, we call . ``` . np.where(adata.obs[key].cat.categories.values == name)[0][0]. ```. which fails with an error (since `name` is a string from `select_groups` and the elements of `adata.obs[key].cat.categories.values` are integers). Thus, this PR includes a check for the `dtype` of `adata.obs[key].cat.categories.values` - if it is numeric, we instead look at . ``` . np.where(adata.obs[key].cat.categories.values == float(name))[0][0]. ```. This error should only appear if the cluster labels are integers (since this is the only time that the cluster labels are converted to strings for `groups_order` in `rank_genes_groups`) but the above fix should also work if the cluster labels are any floating point numbers (just in case the `rank_genes_groups` is ever generalized in this way). ([Here](https://docs.scipy.org/doc/numpy-1.13.0/reference/arrays.scalars.html) is a link to the numpy type hierarchy). Edit: added a line number",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/471
https://github.com/scverse/scanpy/pull/471:1267,safety,error,error,1267,"Add fixes for bugs in rank_genes_groups; This includes fixes for both #469 and #470 . #469 was a small indexing error. To fix #470, a `rankby_abs` check is included in the `logreg` section of the method that mirrors the `rankby_abs` checks in the other two methods. This PR additionally updates `select_groups` function in `scanpy/utils.py.` I was having some issues when the clusters that I was using were labelled by integers (i.e. when `adata.obs[key].cat.catagories.values.dtype` was some form of integer) AND when I was looking at a subset of the clusters (e.g. `groups=[0,1]`, not when `groups='all'`). At the start of the `rank_genes_groups` function, these cluster labels are converted into strings in the `groups_order` variable. In the `select_groups` function (line 667 of the original utils.py file), however, we call . ``` . np.where(adata.obs[key].cat.categories.values == name)[0][0]. ```. which fails with an error (since `name` is a string from `select_groups` and the elements of `adata.obs[key].cat.categories.values` are integers). Thus, this PR includes a check for the `dtype` of `adata.obs[key].cat.categories.values` - if it is numeric, we instead look at . ``` . np.where(adata.obs[key].cat.categories.values == float(name))[0][0]. ```. This error should only appear if the cluster labels are integers (since this is the only time that the cluster labels are converted to strings for `groups_order` in `rank_genes_groups`) but the above fix should also work if the cluster labels are any floating point numbers (just in case the `rank_genes_groups` is ever generalized in this way). ([Here](https://docs.scipy.org/doc/numpy-1.13.0/reference/arrays.scalars.html) is a link to the numpy type hierarchy). Edit: added a line number",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/471
https://github.com/scverse/scanpy/pull/471:173,security,log,logreg,173,"Add fixes for bugs in rank_genes_groups; This includes fixes for both #469 and #470 . #469 was a small indexing error. To fix #470, a `rankby_abs` check is included in the `logreg` section of the method that mirrors the `rankby_abs` checks in the other two methods. This PR additionally updates `select_groups` function in `scanpy/utils.py.` I was having some issues when the clusters that I was using were labelled by integers (i.e. when `adata.obs[key].cat.catagories.values.dtype` was some form of integer) AND when I was looking at a subset of the clusters (e.g. `groups=[0,1]`, not when `groups='all'`). At the start of the `rank_genes_groups` function, these cluster labels are converted into strings in the `groups_order` variable. In the `select_groups` function (line 667 of the original utils.py file), however, we call . ``` . np.where(adata.obs[key].cat.categories.values == name)[0][0]. ```. which fails with an error (since `name` is a string from `select_groups` and the elements of `adata.obs[key].cat.categories.values` are integers). Thus, this PR includes a check for the `dtype` of `adata.obs[key].cat.categories.values` - if it is numeric, we instead look at . ``` . np.where(adata.obs[key].cat.categories.values == float(name))[0][0]. ```. This error should only appear if the cluster labels are integers (since this is the only time that the cluster labels are converted to strings for `groups_order` in `rank_genes_groups`) but the above fix should also work if the cluster labels are any floating point numbers (just in case the `rank_genes_groups` is ever generalized in this way). ([Here](https://docs.scipy.org/doc/numpy-1.13.0/reference/arrays.scalars.html) is a link to the numpy type hierarchy). Edit: added a line number",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/471
https://github.com/scverse/scanpy/pull/471:287,security,updat,updates,287,"Add fixes for bugs in rank_genes_groups; This includes fixes for both #469 and #470 . #469 was a small indexing error. To fix #470, a `rankby_abs` check is included in the `logreg` section of the method that mirrors the `rankby_abs` checks in the other two methods. This PR additionally updates `select_groups` function in `scanpy/utils.py.` I was having some issues when the clusters that I was using were labelled by integers (i.e. when `adata.obs[key].cat.catagories.values.dtype` was some form of integer) AND when I was looking at a subset of the clusters (e.g. `groups=[0,1]`, not when `groups='all'`). At the start of the `rank_genes_groups` function, these cluster labels are converted into strings in the `groups_order` variable. In the `select_groups` function (line 667 of the original utils.py file), however, we call . ``` . np.where(adata.obs[key].cat.categories.values == name)[0][0]. ```. which fails with an error (since `name` is a string from `select_groups` and the elements of `adata.obs[key].cat.categories.values` are integers). Thus, this PR includes a check for the `dtype` of `adata.obs[key].cat.categories.values` - if it is numeric, we instead look at . ``` . np.where(adata.obs[key].cat.categories.values == float(name))[0][0]. ```. This error should only appear if the cluster labels are integers (since this is the only time that the cluster labels are converted to strings for `groups_order` in `rank_genes_groups`) but the above fix should also work if the cluster labels are any floating point numbers (just in case the `rank_genes_groups` is ever generalized in this way). ([Here](https://docs.scipy.org/doc/numpy-1.13.0/reference/arrays.scalars.html) is a link to the numpy type hierarchy). Edit: added a line number",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/471
https://github.com/scverse/scanpy/pull/471:173,testability,log,logreg,173,"Add fixes for bugs in rank_genes_groups; This includes fixes for both #469 and #470 . #469 was a small indexing error. To fix #470, a `rankby_abs` check is included in the `logreg` section of the method that mirrors the `rankby_abs` checks in the other two methods. This PR additionally updates `select_groups` function in `scanpy/utils.py.` I was having some issues when the clusters that I was using were labelled by integers (i.e. when `adata.obs[key].cat.catagories.values.dtype` was some form of integer) AND when I was looking at a subset of the clusters (e.g. `groups=[0,1]`, not when `groups='all'`). At the start of the `rank_genes_groups` function, these cluster labels are converted into strings in the `groups_order` variable. In the `select_groups` function (line 667 of the original utils.py file), however, we call . ``` . np.where(adata.obs[key].cat.categories.values == name)[0][0]. ```. which fails with an error (since `name` is a string from `select_groups` and the elements of `adata.obs[key].cat.categories.values` are integers). Thus, this PR includes a check for the `dtype` of `adata.obs[key].cat.categories.values` - if it is numeric, we instead look at . ``` . np.where(adata.obs[key].cat.categories.values == float(name))[0][0]. ```. This error should only appear if the cluster labels are integers (since this is the only time that the cluster labels are converted to strings for `groups_order` in `rank_genes_groups`) but the above fix should also work if the cluster labels are any floating point numbers (just in case the `rank_genes_groups` is ever generalized in this way). ([Here](https://docs.scipy.org/doc/numpy-1.13.0/reference/arrays.scalars.html) is a link to the numpy type hierarchy). Edit: added a line number",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/471
https://github.com/scverse/scanpy/pull/471:112,usability,error,error,112,"Add fixes for bugs in rank_genes_groups; This includes fixes for both #469 and #470 . #469 was a small indexing error. To fix #470, a `rankby_abs` check is included in the `logreg` section of the method that mirrors the `rankby_abs` checks in the other two methods. This PR additionally updates `select_groups` function in `scanpy/utils.py.` I was having some issues when the clusters that I was using were labelled by integers (i.e. when `adata.obs[key].cat.catagories.values.dtype` was some form of integer) AND when I was looking at a subset of the clusters (e.g. `groups=[0,1]`, not when `groups='all'`). At the start of the `rank_genes_groups` function, these cluster labels are converted into strings in the `groups_order` variable. In the `select_groups` function (line 667 of the original utils.py file), however, we call . ``` . np.where(adata.obs[key].cat.categories.values == name)[0][0]. ```. which fails with an error (since `name` is a string from `select_groups` and the elements of `adata.obs[key].cat.categories.values` are integers). Thus, this PR includes a check for the `dtype` of `adata.obs[key].cat.categories.values` - if it is numeric, we instead look at . ``` . np.where(adata.obs[key].cat.categories.values == float(name))[0][0]. ```. This error should only appear if the cluster labels are integers (since this is the only time that the cluster labels are converted to strings for `groups_order` in `rank_genes_groups`) but the above fix should also work if the cluster labels are any floating point numbers (just in case the `rank_genes_groups` is ever generalized in this way). ([Here](https://docs.scipy.org/doc/numpy-1.13.0/reference/arrays.scalars.html) is a link to the numpy type hierarchy). Edit: added a line number",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/471
https://github.com/scverse/scanpy/pull/471:925,usability,error,error,925,"Add fixes for bugs in rank_genes_groups; This includes fixes for both #469 and #470 . #469 was a small indexing error. To fix #470, a `rankby_abs` check is included in the `logreg` section of the method that mirrors the `rankby_abs` checks in the other two methods. This PR additionally updates `select_groups` function in `scanpy/utils.py.` I was having some issues when the clusters that I was using were labelled by integers (i.e. when `adata.obs[key].cat.catagories.values.dtype` was some form of integer) AND when I was looking at a subset of the clusters (e.g. `groups=[0,1]`, not when `groups='all'`). At the start of the `rank_genes_groups` function, these cluster labels are converted into strings in the `groups_order` variable. In the `select_groups` function (line 667 of the original utils.py file), however, we call . ``` . np.where(adata.obs[key].cat.categories.values == name)[0][0]. ```. which fails with an error (since `name` is a string from `select_groups` and the elements of `adata.obs[key].cat.categories.values` are integers). Thus, this PR includes a check for the `dtype` of `adata.obs[key].cat.categories.values` - if it is numeric, we instead look at . ``` . np.where(adata.obs[key].cat.categories.values == float(name))[0][0]. ```. This error should only appear if the cluster labels are integers (since this is the only time that the cluster labels are converted to strings for `groups_order` in `rank_genes_groups`) but the above fix should also work if the cluster labels are any floating point numbers (just in case the `rank_genes_groups` is ever generalized in this way). ([Here](https://docs.scipy.org/doc/numpy-1.13.0/reference/arrays.scalars.html) is a link to the numpy type hierarchy). Edit: added a line number",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/471
https://github.com/scverse/scanpy/pull/471:1267,usability,error,error,1267,"Add fixes for bugs in rank_genes_groups; This includes fixes for both #469 and #470 . #469 was a small indexing error. To fix #470, a `rankby_abs` check is included in the `logreg` section of the method that mirrors the `rankby_abs` checks in the other two methods. This PR additionally updates `select_groups` function in `scanpy/utils.py.` I was having some issues when the clusters that I was using were labelled by integers (i.e. when `adata.obs[key].cat.catagories.values.dtype` was some form of integer) AND when I was looking at a subset of the clusters (e.g. `groups=[0,1]`, not when `groups='all'`). At the start of the `rank_genes_groups` function, these cluster labels are converted into strings in the `groups_order` variable. In the `select_groups` function (line 667 of the original utils.py file), however, we call . ``` . np.where(adata.obs[key].cat.categories.values == name)[0][0]. ```. which fails with an error (since `name` is a string from `select_groups` and the elements of `adata.obs[key].cat.categories.values` are integers). Thus, this PR includes a check for the `dtype` of `adata.obs[key].cat.categories.values` - if it is numeric, we instead look at . ``` . np.where(adata.obs[key].cat.categories.values == float(name))[0][0]. ```. This error should only appear if the cluster labels are integers (since this is the only time that the cluster labels are converted to strings for `groups_order` in `rank_genes_groups`) but the above fix should also work if the cluster labels are any floating point numbers (just in case the `rank_genes_groups` is ever generalized in this way). ([Here](https://docs.scipy.org/doc/numpy-1.13.0/reference/arrays.scalars.html) is a link to the numpy type hierarchy). Edit: added a line number",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/471
https://github.com/scverse/scanpy/issues/472:9,availability,operat,operations,9,"Tracking operations performed on AnnData objects; When exploring various options of preprocessing data, I try to avoid having several copies of AnnData objects in memory if they're not sparse, so I save them to h5ad at key steps. Sometimes alas, after a few iterations I re-write stuff and forget what operations have been performed in my ""X"" (particularly in the preprocessing steps). So, because being lazy makes me creative, I started tracking these in the object itself (see example https://gist.github.com/afrendeiro/7ccaf324bfdbff042ae36f734f544860) by decorating the preprocessing functions post hoc (this could even easily be used to save the values of kwargs passed potentially). I wonder if an internal implementation of this would be of broad interest, particularly for functions which modify ""X"" inplace? Of course this would be no replacement for proper documentation of one's steps, etc but I thought it could be an interesting addition to scanpy in any case.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/472
https://github.com/scverse/scanpy/issues/472:302,availability,operat,operations,302,"Tracking operations performed on AnnData objects; When exploring various options of preprocessing data, I try to avoid having several copies of AnnData objects in memory if they're not sparse, so I save them to h5ad at key steps. Sometimes alas, after a few iterations I re-write stuff and forget what operations have been performed in my ""X"" (particularly in the preprocessing steps). So, because being lazy makes me creative, I started tracking these in the object itself (see example https://gist.github.com/afrendeiro/7ccaf324bfdbff042ae36f734f544860) by decorating the preprocessing functions post hoc (this could even easily be used to save the values of kwargs passed potentially). I wonder if an internal implementation of this would be of broad interest, particularly for functions which modify ""X"" inplace? Of course this would be no replacement for proper documentation of one's steps, etc but I thought it could be an interesting addition to scanpy in any case.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/472
https://github.com/scverse/scanpy/issues/472:559,modifiability,deco,decorating,559,"Tracking operations performed on AnnData objects; When exploring various options of preprocessing data, I try to avoid having several copies of AnnData objects in memory if they're not sparse, so I save them to h5ad at key steps. Sometimes alas, after a few iterations I re-write stuff and forget what operations have been performed in my ""X"" (particularly in the preprocessing steps). So, because being lazy makes me creative, I started tracking these in the object itself (see example https://gist.github.com/afrendeiro/7ccaf324bfdbff042ae36f734f544860) by decorating the preprocessing functions post hoc (this could even easily be used to save the values of kwargs passed potentially). I wonder if an internal implementation of this would be of broad interest, particularly for functions which modify ""X"" inplace? Of course this would be no replacement for proper documentation of one's steps, etc but I thought it could be an interesting addition to scanpy in any case.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/472
https://github.com/scverse/scanpy/issues/472:20,performance,perform,performed,20,"Tracking operations performed on AnnData objects; When exploring various options of preprocessing data, I try to avoid having several copies of AnnData objects in memory if they're not sparse, so I save them to h5ad at key steps. Sometimes alas, after a few iterations I re-write stuff and forget what operations have been performed in my ""X"" (particularly in the preprocessing steps). So, because being lazy makes me creative, I started tracking these in the object itself (see example https://gist.github.com/afrendeiro/7ccaf324bfdbff042ae36f734f544860) by decorating the preprocessing functions post hoc (this could even easily be used to save the values of kwargs passed potentially). I wonder if an internal implementation of this would be of broad interest, particularly for functions which modify ""X"" inplace? Of course this would be no replacement for proper documentation of one's steps, etc but I thought it could be an interesting addition to scanpy in any case.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/472
https://github.com/scverse/scanpy/issues/472:163,performance,memor,memory,163,"Tracking operations performed on AnnData objects; When exploring various options of preprocessing data, I try to avoid having several copies of AnnData objects in memory if they're not sparse, so I save them to h5ad at key steps. Sometimes alas, after a few iterations I re-write stuff and forget what operations have been performed in my ""X"" (particularly in the preprocessing steps). So, because being lazy makes me creative, I started tracking these in the object itself (see example https://gist.github.com/afrendeiro/7ccaf324bfdbff042ae36f734f544860) by decorating the preprocessing functions post hoc (this could even easily be used to save the values of kwargs passed potentially). I wonder if an internal implementation of this would be of broad interest, particularly for functions which modify ""X"" inplace? Of course this would be no replacement for proper documentation of one's steps, etc but I thought it could be an interesting addition to scanpy in any case.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/472
https://github.com/scverse/scanpy/issues/472:323,performance,perform,performed,323,"Tracking operations performed on AnnData objects; When exploring various options of preprocessing data, I try to avoid having several copies of AnnData objects in memory if they're not sparse, so I save them to h5ad at key steps. Sometimes alas, after a few iterations I re-write stuff and forget what operations have been performed in my ""X"" (particularly in the preprocessing steps). So, because being lazy makes me creative, I started tracking these in the object itself (see example https://gist.github.com/afrendeiro/7ccaf324bfdbff042ae36f734f544860) by decorating the preprocessing functions post hoc (this could even easily be used to save the values of kwargs passed potentially). I wonder if an internal implementation of this would be of broad interest, particularly for functions which modify ""X"" inplace? Of course this would be no replacement for proper documentation of one's steps, etc but I thought it could be an interesting addition to scanpy in any case.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/472
https://github.com/scverse/scanpy/issues/472:113,safety,avoid,avoid,113,"Tracking operations performed on AnnData objects; When exploring various options of preprocessing data, I try to avoid having several copies of AnnData objects in memory if they're not sparse, so I save them to h5ad at key steps. Sometimes alas, after a few iterations I re-write stuff and forget what operations have been performed in my ""X"" (particularly in the preprocessing steps). So, because being lazy makes me creative, I started tracking these in the object itself (see example https://gist.github.com/afrendeiro/7ccaf324bfdbff042ae36f734f544860) by decorating the preprocessing functions post hoc (this could even easily be used to save the values of kwargs passed potentially). I wonder if an internal implementation of this would be of broad interest, particularly for functions which modify ""X"" inplace? Of course this would be no replacement for proper documentation of one's steps, etc but I thought it could be an interesting addition to scanpy in any case.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/472
https://github.com/scverse/scanpy/issues/472:797,security,modif,modify,797,"Tracking operations performed on AnnData objects; When exploring various options of preprocessing data, I try to avoid having several copies of AnnData objects in memory if they're not sparse, so I save them to h5ad at key steps. Sometimes alas, after a few iterations I re-write stuff and forget what operations have been performed in my ""X"" (particularly in the preprocessing steps). So, because being lazy makes me creative, I started tracking these in the object itself (see example https://gist.github.com/afrendeiro/7ccaf324bfdbff042ae36f734f544860) by decorating the preprocessing functions post hoc (this could even easily be used to save the values of kwargs passed potentially). I wonder if an internal implementation of this would be of broad interest, particularly for functions which modify ""X"" inplace? Of course this would be no replacement for proper documentation of one's steps, etc but I thought it could be an interesting addition to scanpy in any case.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/472
https://github.com/scverse/scanpy/issues/472:20,usability,perform,performed,20,"Tracking operations performed on AnnData objects; When exploring various options of preprocessing data, I try to avoid having several copies of AnnData objects in memory if they're not sparse, so I save them to h5ad at key steps. Sometimes alas, after a few iterations I re-write stuff and forget what operations have been performed in my ""X"" (particularly in the preprocessing steps). So, because being lazy makes me creative, I started tracking these in the object itself (see example https://gist.github.com/afrendeiro/7ccaf324bfdbff042ae36f734f544860) by decorating the preprocessing functions post hoc (this could even easily be used to save the values of kwargs passed potentially). I wonder if an internal implementation of this would be of broad interest, particularly for functions which modify ""X"" inplace? Of course this would be no replacement for proper documentation of one's steps, etc but I thought it could be an interesting addition to scanpy in any case.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/472
https://github.com/scverse/scanpy/issues/472:163,usability,memor,memory,163,"Tracking operations performed on AnnData objects; When exploring various options of preprocessing data, I try to avoid having several copies of AnnData objects in memory if they're not sparse, so I save them to h5ad at key steps. Sometimes alas, after a few iterations I re-write stuff and forget what operations have been performed in my ""X"" (particularly in the preprocessing steps). So, because being lazy makes me creative, I started tracking these in the object itself (see example https://gist.github.com/afrendeiro/7ccaf324bfdbff042ae36f734f544860) by decorating the preprocessing functions post hoc (this could even easily be used to save the values of kwargs passed potentially). I wonder if an internal implementation of this would be of broad interest, particularly for functions which modify ""X"" inplace? Of course this would be no replacement for proper documentation of one's steps, etc but I thought it could be an interesting addition to scanpy in any case.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/472
https://github.com/scverse/scanpy/issues/472:323,usability,perform,performed,323,"Tracking operations performed on AnnData objects; When exploring various options of preprocessing data, I try to avoid having several copies of AnnData objects in memory if they're not sparse, so I save them to h5ad at key steps. Sometimes alas, after a few iterations I re-write stuff and forget what operations have been performed in my ""X"" (particularly in the preprocessing steps). So, because being lazy makes me creative, I started tracking these in the object itself (see example https://gist.github.com/afrendeiro/7ccaf324bfdbff042ae36f734f544860) by decorating the preprocessing functions post hoc (this could even easily be used to save the values of kwargs passed potentially). I wonder if an internal implementation of this would be of broad interest, particularly for functions which modify ""X"" inplace? Of course this would be no replacement for proper documentation of one's steps, etc but I thought it could be an interesting addition to scanpy in any case.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/472
https://github.com/scverse/scanpy/issues/472:867,usability,document,documentation,867,"Tracking operations performed on AnnData objects; When exploring various options of preprocessing data, I try to avoid having several copies of AnnData objects in memory if they're not sparse, so I save them to h5ad at key steps. Sometimes alas, after a few iterations I re-write stuff and forget what operations have been performed in my ""X"" (particularly in the preprocessing steps). So, because being lazy makes me creative, I started tracking these in the object itself (see example https://gist.github.com/afrendeiro/7ccaf324bfdbff042ae36f734f544860) by decorating the preprocessing functions post hoc (this could even easily be used to save the values of kwargs passed potentially). I wonder if an internal implementation of this would be of broad interest, particularly for functions which modify ""X"" inplace? Of course this would be no replacement for proper documentation of one's steps, etc but I thought it could be an interesting addition to scanpy in any case.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/472
https://github.com/scverse/scanpy/issues/473:48,deployability,fail,fail,48,"Facecolor of scatter plots for categorical data fail to be non-transparent.; Hi,. Even when we set `sc.settings.set_figure_params(transparent=False)` the scatter plot for **CATEGORYCAL** data is transparented. (ex. `sc.pl.umap(adata, color='louvain')` ). Continuous data goes well, so I think it is a bug. When you switch Jupyterlab's theme into dark, it will be easy to check. Best,. Yoshiaki",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/473
https://github.com/scverse/scanpy/issues/473:255,deployability,Continu,Continuous,255,"Facecolor of scatter plots for categorical data fail to be non-transparent.; Hi,. Even when we set `sc.settings.set_figure_params(transparent=False)` the scatter plot for **CATEGORYCAL** data is transparented. (ex. `sc.pl.umap(adata, color='louvain')` ). Continuous data goes well, so I think it is a bug. When you switch Jupyterlab's theme into dark, it will be easy to check. Best,. Yoshiaki",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/473
https://github.com/scverse/scanpy/issues/473:48,reliability,fail,fail,48,"Facecolor of scatter plots for categorical data fail to be non-transparent.; Hi,. Even when we set `sc.settings.set_figure_params(transparent=False)` the scatter plot for **CATEGORYCAL** data is transparented. (ex. `sc.pl.umap(adata, color='louvain')` ). Continuous data goes well, so I think it is a bug. When you switch Jupyterlab's theme into dark, it will be easy to check. Best,. Yoshiaki",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/473
https://github.com/scverse/scanpy/pull/474:0,availability,Down,Downsample,0,"Downsample total counts; Update to `downsample_counts` to allow downsampling total counts, similar to normalization by `cellranger aggr` (I'm pretty sure on this, there's a lot going on in their code). Additionally, enabled caching for the `numba`'d function, which cuts down on test time. As adding this feature meant renaming `target_counts` to `counts_per_cell`, this becomes a breaking change. Since it's breaking, I've also gone ahead and set `replace=False` by default as mentioned before (#340). Definitely willing to make changes. I've implemented this since I'm doing some integration work and figured it'd be nice to be able to try the basic `cellranger` strategy. Edit: The failing PAGA test occurs locally on master as well, but I don't think I broke that.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/474
https://github.com/scverse/scanpy/pull/474:64,availability,down,downsampling,64,"Downsample total counts; Update to `downsample_counts` to allow downsampling total counts, similar to normalization by `cellranger aggr` (I'm pretty sure on this, there's a lot going on in their code). Additionally, enabled caching for the `numba`'d function, which cuts down on test time. As adding this feature meant renaming `target_counts` to `counts_per_cell`, this becomes a breaking change. Since it's breaking, I've also gone ahead and set `replace=False` by default as mentioned before (#340). Definitely willing to make changes. I've implemented this since I'm doing some integration work and figured it'd be nice to be able to try the basic `cellranger` strategy. Edit: The failing PAGA test occurs locally on master as well, but I don't think I broke that.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/474
https://github.com/scverse/scanpy/pull/474:271,availability,down,down,271,"Downsample total counts; Update to `downsample_counts` to allow downsampling total counts, similar to normalization by `cellranger aggr` (I'm pretty sure on this, there's a lot going on in their code). Additionally, enabled caching for the `numba`'d function, which cuts down on test time. As adding this feature meant renaming `target_counts` to `counts_per_cell`, this becomes a breaking change. Since it's breaking, I've also gone ahead and set `replace=False` by default as mentioned before (#340). Definitely willing to make changes. I've implemented this since I'm doing some integration work and figured it'd be nice to be able to try the basic `cellranger` strategy. Edit: The failing PAGA test occurs locally on master as well, but I don't think I broke that.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/474
https://github.com/scverse/scanpy/pull/474:25,deployability,Updat,Update,25,"Downsample total counts; Update to `downsample_counts` to allow downsampling total counts, similar to normalization by `cellranger aggr` (I'm pretty sure on this, there's a lot going on in their code). Additionally, enabled caching for the `numba`'d function, which cuts down on test time. As adding this feature meant renaming `target_counts` to `counts_per_cell`, this becomes a breaking change. Since it's breaking, I've also gone ahead and set `replace=False` by default as mentioned before (#340). Definitely willing to make changes. I've implemented this since I'm doing some integration work and figured it'd be nice to be able to try the basic `cellranger` strategy. Edit: The failing PAGA test occurs locally on master as well, but I don't think I broke that.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/474
https://github.com/scverse/scanpy/pull/474:582,deployability,integr,integration,582,"Downsample total counts; Update to `downsample_counts` to allow downsampling total counts, similar to normalization by `cellranger aggr` (I'm pretty sure on this, there's a lot going on in their code). Additionally, enabled caching for the `numba`'d function, which cuts down on test time. As adding this feature meant renaming `target_counts` to `counts_per_cell`, this becomes a breaking change. Since it's breaking, I've also gone ahead and set `replace=False` by default as mentioned before (#340). Definitely willing to make changes. I've implemented this since I'm doing some integration work and figured it'd be nice to be able to try the basic `cellranger` strategy. Edit: The failing PAGA test occurs locally on master as well, but I don't think I broke that.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/474
https://github.com/scverse/scanpy/pull/474:685,deployability,fail,failing,685,"Downsample total counts; Update to `downsample_counts` to allow downsampling total counts, similar to normalization by `cellranger aggr` (I'm pretty sure on this, there's a lot going on in their code). Additionally, enabled caching for the `numba`'d function, which cuts down on test time. As adding this feature meant renaming `target_counts` to `counts_per_cell`, this becomes a breaking change. Since it's breaking, I've also gone ahead and set `replace=False` by default as mentioned before (#340). Definitely willing to make changes. I've implemented this since I'm doing some integration work and figured it'd be nice to be able to try the basic `cellranger` strategy. Edit: The failing PAGA test occurs locally on master as well, but I don't think I broke that.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/474
https://github.com/scverse/scanpy/pull/474:582,integrability,integr,integration,582,"Downsample total counts; Update to `downsample_counts` to allow downsampling total counts, similar to normalization by `cellranger aggr` (I'm pretty sure on this, there's a lot going on in their code). Additionally, enabled caching for the `numba`'d function, which cuts down on test time. As adding this feature meant renaming `target_counts` to `counts_per_cell`, this becomes a breaking change. Since it's breaking, I've also gone ahead and set `replace=False` by default as mentioned before (#340). Definitely willing to make changes. I've implemented this since I'm doing some integration work and figured it'd be nice to be able to try the basic `cellranger` strategy. Edit: The failing PAGA test occurs locally on master as well, but I don't think I broke that.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/474
https://github.com/scverse/scanpy/pull/474:582,interoperability,integr,integration,582,"Downsample total counts; Update to `downsample_counts` to allow downsampling total counts, similar to normalization by `cellranger aggr` (I'm pretty sure on this, there's a lot going on in their code). Additionally, enabled caching for the `numba`'d function, which cuts down on test time. As adding this feature meant renaming `target_counts` to `counts_per_cell`, this becomes a breaking change. Since it's breaking, I've also gone ahead and set `replace=False` by default as mentioned before (#340). Definitely willing to make changes. I've implemented this since I'm doing some integration work and figured it'd be nice to be able to try the basic `cellranger` strategy. Edit: The failing PAGA test occurs locally on master as well, but I don't think I broke that.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/474
https://github.com/scverse/scanpy/pull/474:582,modifiability,integr,integration,582,"Downsample total counts; Update to `downsample_counts` to allow downsampling total counts, similar to normalization by `cellranger aggr` (I'm pretty sure on this, there's a lot going on in their code). Additionally, enabled caching for the `numba`'d function, which cuts down on test time. As adding this feature meant renaming `target_counts` to `counts_per_cell`, this becomes a breaking change. Since it's breaking, I've also gone ahead and set `replace=False` by default as mentioned before (#340). Definitely willing to make changes. I've implemented this since I'm doing some integration work and figured it'd be nice to be able to try the basic `cellranger` strategy. Edit: The failing PAGA test occurs locally on master as well, but I don't think I broke that.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/474
https://github.com/scverse/scanpy/pull/474:224,performance,cach,caching,224,"Downsample total counts; Update to `downsample_counts` to allow downsampling total counts, similar to normalization by `cellranger aggr` (I'm pretty sure on this, there's a lot going on in their code). Additionally, enabled caching for the `numba`'d function, which cuts down on test time. As adding this feature meant renaming `target_counts` to `counts_per_cell`, this becomes a breaking change. Since it's breaking, I've also gone ahead and set `replace=False` by default as mentioned before (#340). Definitely willing to make changes. I've implemented this since I'm doing some integration work and figured it'd be nice to be able to try the basic `cellranger` strategy. Edit: The failing PAGA test occurs locally on master as well, but I don't think I broke that.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/474
https://github.com/scverse/scanpy/pull/474:284,performance,time,time,284,"Downsample total counts; Update to `downsample_counts` to allow downsampling total counts, similar to normalization by `cellranger aggr` (I'm pretty sure on this, there's a lot going on in their code). Additionally, enabled caching for the `numba`'d function, which cuts down on test time. As adding this feature meant renaming `target_counts` to `counts_per_cell`, this becomes a breaking change. Since it's breaking, I've also gone ahead and set `replace=False` by default as mentioned before (#340). Definitely willing to make changes. I've implemented this since I'm doing some integration work and figured it'd be nice to be able to try the basic `cellranger` strategy. Edit: The failing PAGA test occurs locally on master as well, but I don't think I broke that.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/474
https://github.com/scverse/scanpy/pull/474:582,reliability,integr,integration,582,"Downsample total counts; Update to `downsample_counts` to allow downsampling total counts, similar to normalization by `cellranger aggr` (I'm pretty sure on this, there's a lot going on in their code). Additionally, enabled caching for the `numba`'d function, which cuts down on test time. As adding this feature meant renaming `target_counts` to `counts_per_cell`, this becomes a breaking change. Since it's breaking, I've also gone ahead and set `replace=False` by default as mentioned before (#340). Definitely willing to make changes. I've implemented this since I'm doing some integration work and figured it'd be nice to be able to try the basic `cellranger` strategy. Edit: The failing PAGA test occurs locally on master as well, but I don't think I broke that.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/474
https://github.com/scverse/scanpy/pull/474:685,reliability,fail,failing,685,"Downsample total counts; Update to `downsample_counts` to allow downsampling total counts, similar to normalization by `cellranger aggr` (I'm pretty sure on this, there's a lot going on in their code). Additionally, enabled caching for the `numba`'d function, which cuts down on test time. As adding this feature meant renaming `target_counts` to `counts_per_cell`, this becomes a breaking change. Since it's breaking, I've also gone ahead and set `replace=False` by default as mentioned before (#340). Definitely willing to make changes. I've implemented this since I'm doing some integration work and figured it'd be nice to be able to try the basic `cellranger` strategy. Edit: The failing PAGA test occurs locally on master as well, but I don't think I broke that.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/474
https://github.com/scverse/scanpy/pull/474:25,safety,Updat,Update,25,"Downsample total counts; Update to `downsample_counts` to allow downsampling total counts, similar to normalization by `cellranger aggr` (I'm pretty sure on this, there's a lot going on in their code). Additionally, enabled caching for the `numba`'d function, which cuts down on test time. As adding this feature meant renaming `target_counts` to `counts_per_cell`, this becomes a breaking change. Since it's breaking, I've also gone ahead and set `replace=False` by default as mentioned before (#340). Definitely willing to make changes. I've implemented this since I'm doing some integration work and figured it'd be nice to be able to try the basic `cellranger` strategy. Edit: The failing PAGA test occurs locally on master as well, but I don't think I broke that.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/474
https://github.com/scverse/scanpy/pull/474:279,safety,test,test,279,"Downsample total counts; Update to `downsample_counts` to allow downsampling total counts, similar to normalization by `cellranger aggr` (I'm pretty sure on this, there's a lot going on in their code). Additionally, enabled caching for the `numba`'d function, which cuts down on test time. As adding this feature meant renaming `target_counts` to `counts_per_cell`, this becomes a breaking change. Since it's breaking, I've also gone ahead and set `replace=False` by default as mentioned before (#340). Definitely willing to make changes. I've implemented this since I'm doing some integration work and figured it'd be nice to be able to try the basic `cellranger` strategy. Edit: The failing PAGA test occurs locally on master as well, but I don't think I broke that.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/474
https://github.com/scverse/scanpy/pull/474:698,safety,test,test,698,"Downsample total counts; Update to `downsample_counts` to allow downsampling total counts, similar to normalization by `cellranger aggr` (I'm pretty sure on this, there's a lot going on in their code). Additionally, enabled caching for the `numba`'d function, which cuts down on test time. As adding this feature meant renaming `target_counts` to `counts_per_cell`, this becomes a breaking change. Since it's breaking, I've also gone ahead and set `replace=False` by default as mentioned before (#340). Definitely willing to make changes. I've implemented this since I'm doing some integration work and figured it'd be nice to be able to try the basic `cellranger` strategy. Edit: The failing PAGA test occurs locally on master as well, but I don't think I broke that.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/474
https://github.com/scverse/scanpy/pull/474:25,security,Updat,Update,25,"Downsample total counts; Update to `downsample_counts` to allow downsampling total counts, similar to normalization by `cellranger aggr` (I'm pretty sure on this, there's a lot going on in their code). Additionally, enabled caching for the `numba`'d function, which cuts down on test time. As adding this feature meant renaming `target_counts` to `counts_per_cell`, this becomes a breaking change. Since it's breaking, I've also gone ahead and set `replace=False` by default as mentioned before (#340). Definitely willing to make changes. I've implemented this since I'm doing some integration work and figured it'd be nice to be able to try the basic `cellranger` strategy. Edit: The failing PAGA test occurs locally on master as well, but I don't think I broke that.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/474
https://github.com/scverse/scanpy/pull/474:582,security,integr,integration,582,"Downsample total counts; Update to `downsample_counts` to allow downsampling total counts, similar to normalization by `cellranger aggr` (I'm pretty sure on this, there's a lot going on in their code). Additionally, enabled caching for the `numba`'d function, which cuts down on test time. As adding this feature meant renaming `target_counts` to `counts_per_cell`, this becomes a breaking change. Since it's breaking, I've also gone ahead and set `replace=False` by default as mentioned before (#340). Definitely willing to make changes. I've implemented this since I'm doing some integration work and figured it'd be nice to be able to try the basic `cellranger` strategy. Edit: The failing PAGA test occurs locally on master as well, but I don't think I broke that.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/474
https://github.com/scverse/scanpy/pull/474:279,testability,test,test,279,"Downsample total counts; Update to `downsample_counts` to allow downsampling total counts, similar to normalization by `cellranger aggr` (I'm pretty sure on this, there's a lot going on in their code). Additionally, enabled caching for the `numba`'d function, which cuts down on test time. As adding this feature meant renaming `target_counts` to `counts_per_cell`, this becomes a breaking change. Since it's breaking, I've also gone ahead and set `replace=False` by default as mentioned before (#340). Definitely willing to make changes. I've implemented this since I'm doing some integration work and figured it'd be nice to be able to try the basic `cellranger` strategy. Edit: The failing PAGA test occurs locally on master as well, but I don't think I broke that.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/474
https://github.com/scverse/scanpy/pull/474:582,testability,integr,integration,582,"Downsample total counts; Update to `downsample_counts` to allow downsampling total counts, similar to normalization by `cellranger aggr` (I'm pretty sure on this, there's a lot going on in their code). Additionally, enabled caching for the `numba`'d function, which cuts down on test time. As adding this feature meant renaming `target_counts` to `counts_per_cell`, this becomes a breaking change. Since it's breaking, I've also gone ahead and set `replace=False` by default as mentioned before (#340). Definitely willing to make changes. I've implemented this since I'm doing some integration work and figured it'd be nice to be able to try the basic `cellranger` strategy. Edit: The failing PAGA test occurs locally on master as well, but I don't think I broke that.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/474
https://github.com/scverse/scanpy/pull/474:698,testability,test,test,698,"Downsample total counts; Update to `downsample_counts` to allow downsampling total counts, similar to normalization by `cellranger aggr` (I'm pretty sure on this, there's a lot going on in their code). Additionally, enabled caching for the `numba`'d function, which cuts down on test time. As adding this feature meant renaming `target_counts` to `counts_per_cell`, this becomes a breaking change. Since it's breaking, I've also gone ahead and set `replace=False` by default as mentioned before (#340). Definitely willing to make changes. I've implemented this since I'm doing some integration work and figured it'd be nice to be able to try the basic `cellranger` strategy. Edit: The failing PAGA test occurs locally on master as well, but I don't think I broke that.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/474
https://github.com/scverse/scanpy/issues/475:20,security,modif,modifies,20,"`rank_genes_groups` modifies its argument `groups`; `rank_genes_groups` modifies its argument `groups`:. ```python. import scanpy as sc. adata = sc.datasets.pbmc68k_reduced(). groups = [""1""]. print(groups). # [""1""]. sc.tl.rank_genes_groups(adata, ""louvain"", groups=groups, reference=""2""). print(groups). # [""1"", ""2""]. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/475
https://github.com/scverse/scanpy/issues/475:72,security,modif,modifies,72,"`rank_genes_groups` modifies its argument `groups`; `rank_genes_groups` modifies its argument `groups`:. ```python. import scanpy as sc. adata = sc.datasets.pbmc68k_reduced(). groups = [""1""]. print(groups). # [""1""]. sc.tl.rank_genes_groups(adata, ""louvain"", groups=groups, reference=""2""). print(groups). # [""1"", ""2""]. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/475
https://github.com/scverse/scanpy/issues/476:178,reliability,doe,does,178,"UMAP color pallet when plotting a gene; Hi guys, . I can't change the color palette when plotting a gene with sc.pl.umap functions. . if I plot louvain or certain categories, it does change the color scheme. e.g. `sc.pl.umap(adata, color=['louvain'], palette='tab20') `. but If I plot a gene . `sc.pl.umap(adata, color=['NANOG'], palette='plasma')`. it doesn't change the color scheme and I always get 'viridis' color palette. . Is there any way I can adjust this? . Thanks a lot,. Tsotne.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/476
https://github.com/scverse/scanpy/issues/476:353,reliability,doe,doesn,353,"UMAP color pallet when plotting a gene; Hi guys, . I can't change the color palette when plotting a gene with sc.pl.umap functions. . if I plot louvain or certain categories, it does change the color scheme. e.g. `sc.pl.umap(adata, color=['louvain'], palette='tab20') `. but If I plot a gene . `sc.pl.umap(adata, color=['NANOG'], palette='plasma')`. it doesn't change the color scheme and I always get 'viridis' color palette. . Is there any way I can adjust this? . Thanks a lot,. Tsotne.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/476
https://github.com/scverse/scanpy/pull/477:151,deployability,continu,continuous,151,"Document color_map argument for scatter plots; Documenting `color_map` argument for scatter plots. This should reduce confusion about how to provide a continuous palette (#476). I'd also be up for having the arguments have names like `cont_palette` and `cat_palette`, which could be more clear.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/477
https://github.com/scverse/scanpy/pull/477:111,energy efficiency,reduc,reduce,111,"Document color_map argument for scatter plots; Documenting `color_map` argument for scatter plots. This should reduce confusion about how to provide a continuous palette (#476). I'd also be up for having the arguments have names like `cont_palette` and `cat_palette`, which could be more clear.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/477
https://github.com/scverse/scanpy/pull/477:0,usability,Document,Document,0,"Document color_map argument for scatter plots; Documenting `color_map` argument for scatter plots. This should reduce confusion about how to provide a continuous palette (#476). I'd also be up for having the arguments have names like `cont_palette` and `cat_palette`, which could be more clear.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/477
https://github.com/scverse/scanpy/pull/477:47,usability,Document,Documenting,47,"Document color_map argument for scatter plots; Documenting `color_map` argument for scatter plots. This should reduce confusion about how to provide a continuous palette (#476). I'd also be up for having the arguments have names like `cont_palette` and `cat_palette`, which could be more clear.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/477
https://github.com/scverse/scanpy/pull/477:288,usability,clear,clear,288,"Document color_map argument for scatter plots; Documenting `color_map` argument for scatter plots. This should reduce confusion about how to provide a continuous palette (#476). I'd also be up for having the arguments have names like `cont_palette` and `cat_palette`, which could be more clear.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/477
https://github.com/scverse/scanpy/issues/478:178,deployability,api,api,178,"scatter plot array size argument gives inconsistent behavour; I'm trying to use an array for the size argument to my umap/scatterplot with the following code. ```. import scanpy.api as sc. import numpy as np. sc.settings.figdir = ""testdir"". sc.settings.file_format_figs = ""png"". sc.logging.print_versions(). ```. With these libraries. `scanpy==1.3.7 anndata==0.6.16 numpy==1.16.1 scipy==1.2.0 pandas==0.23.4 scikit-learn==0.20.2 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 `. Running the following code bit. I use some dummy variable for size. ```. somedata = sc.datasets.paul15(). sc.pp.pca(somedata). sc.pp.neighbors(somedata, n_neighbors=4, n_pcs=20). sc.tl.umap(somedata, spread=1, min_dist=0.1, random_state=42). sc.tl.leiden(somedata, resolution=0.5, random_state=42). z = np.abs(somedata.obsm['X_pca'][:,0])**1. sc.pl.umap(somedata, color=['1110007C09Rik'], size=z, cmap='viridis', save='continuous_expr.png'). sc.pl.umap(somedata, color=['leiden'], size=z, cmap='viridis', save='group_value.png'). ```. I get the following two figure as output. ![umapcontinuous_expr](https://user-images.githubusercontent.com/715716/52612879-951a3300-2e59-11e9-9dad-a8afc60a4b54.png). ![umapgroup_value](https://user-images.githubusercontent.com/715716/52612880-95b2c980-2e59-11e9-9a44-81dd84e3274d.png). I would expect to see a similar size allocation/distribution but they are very different. I Could not really find a cause for this looking at the scatter plot function so it might be somewhere deeper. . I'm need help with getting some grasp on how to interpret this issue and if possible how to map the size argument to the same data points over different plots.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/478
https://github.com/scverse/scanpy/issues/478:282,deployability,log,logging,282,"scatter plot array size argument gives inconsistent behavour; I'm trying to use an array for the size argument to my umap/scatterplot with the following code. ```. import scanpy.api as sc. import numpy as np. sc.settings.figdir = ""testdir"". sc.settings.file_format_figs = ""png"". sc.logging.print_versions(). ```. With these libraries. `scanpy==1.3.7 anndata==0.6.16 numpy==1.16.1 scipy==1.2.0 pandas==0.23.4 scikit-learn==0.20.2 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 `. Running the following code bit. I use some dummy variable for size. ```. somedata = sc.datasets.paul15(). sc.pp.pca(somedata). sc.pp.neighbors(somedata, n_neighbors=4, n_pcs=20). sc.tl.umap(somedata, spread=1, min_dist=0.1, random_state=42). sc.tl.leiden(somedata, resolution=0.5, random_state=42). z = np.abs(somedata.obsm['X_pca'][:,0])**1. sc.pl.umap(somedata, color=['1110007C09Rik'], size=z, cmap='viridis', save='continuous_expr.png'). sc.pl.umap(somedata, color=['leiden'], size=z, cmap='viridis', save='group_value.png'). ```. I get the following two figure as output. ![umapcontinuous_expr](https://user-images.githubusercontent.com/715716/52612879-951a3300-2e59-11e9-9dad-a8afc60a4b54.png). ![umapgroup_value](https://user-images.githubusercontent.com/715716/52612880-95b2c980-2e59-11e9-9a44-81dd84e3274d.png). I would expect to see a similar size allocation/distribution but they are very different. I Could not really find a cause for this looking at the scatter plot function so it might be somewhere deeper. . I'm need help with getting some grasp on how to interpret this issue and if possible how to map the size argument to the same data points over different plots.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/478
https://github.com/scverse/scanpy/issues/478:1345,energy efficiency,alloc,allocation,1345,"scatter plot array size argument gives inconsistent behavour; I'm trying to use an array for the size argument to my umap/scatterplot with the following code. ```. import scanpy.api as sc. import numpy as np. sc.settings.figdir = ""testdir"". sc.settings.file_format_figs = ""png"". sc.logging.print_versions(). ```. With these libraries. `scanpy==1.3.7 anndata==0.6.16 numpy==1.16.1 scipy==1.2.0 pandas==0.23.4 scikit-learn==0.20.2 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 `. Running the following code bit. I use some dummy variable for size. ```. somedata = sc.datasets.paul15(). sc.pp.pca(somedata). sc.pp.neighbors(somedata, n_neighbors=4, n_pcs=20). sc.tl.umap(somedata, spread=1, min_dist=0.1, random_state=42). sc.tl.leiden(somedata, resolution=0.5, random_state=42). z = np.abs(somedata.obsm['X_pca'][:,0])**1. sc.pl.umap(somedata, color=['1110007C09Rik'], size=z, cmap='viridis', save='continuous_expr.png'). sc.pl.umap(somedata, color=['leiden'], size=z, cmap='viridis', save='group_value.png'). ```. I get the following two figure as output. ![umapcontinuous_expr](https://user-images.githubusercontent.com/715716/52612879-951a3300-2e59-11e9-9dad-a8afc60a4b54.png). ![umapgroup_value](https://user-images.githubusercontent.com/715716/52612880-95b2c980-2e59-11e9-9a44-81dd84e3274d.png). I would expect to see a similar size allocation/distribution but they are very different. I Could not really find a cause for this looking at the scatter plot function so it might be somewhere deeper. . I'm need help with getting some grasp on how to interpret this issue and if possible how to map the size argument to the same data points over different plots.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/478
https://github.com/scverse/scanpy/issues/478:178,integrability,api,api,178,"scatter plot array size argument gives inconsistent behavour; I'm trying to use an array for the size argument to my umap/scatterplot with the following code. ```. import scanpy.api as sc. import numpy as np. sc.settings.figdir = ""testdir"". sc.settings.file_format_figs = ""png"". sc.logging.print_versions(). ```. With these libraries. `scanpy==1.3.7 anndata==0.6.16 numpy==1.16.1 scipy==1.2.0 pandas==0.23.4 scikit-learn==0.20.2 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 `. Running the following code bit. I use some dummy variable for size. ```. somedata = sc.datasets.paul15(). sc.pp.pca(somedata). sc.pp.neighbors(somedata, n_neighbors=4, n_pcs=20). sc.tl.umap(somedata, spread=1, min_dist=0.1, random_state=42). sc.tl.leiden(somedata, resolution=0.5, random_state=42). z = np.abs(somedata.obsm['X_pca'][:,0])**1. sc.pl.umap(somedata, color=['1110007C09Rik'], size=z, cmap='viridis', save='continuous_expr.png'). sc.pl.umap(somedata, color=['leiden'], size=z, cmap='viridis', save='group_value.png'). ```. I get the following two figure as output. ![umapcontinuous_expr](https://user-images.githubusercontent.com/715716/52612879-951a3300-2e59-11e9-9dad-a8afc60a4b54.png). ![umapgroup_value](https://user-images.githubusercontent.com/715716/52612880-95b2c980-2e59-11e9-9a44-81dd84e3274d.png). I would expect to see a similar size allocation/distribution but they are very different. I Could not really find a cause for this looking at the scatter plot function so it might be somewhere deeper. . I'm need help with getting some grasp on how to interpret this issue and if possible how to map the size argument to the same data points over different plots.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/478
https://github.com/scverse/scanpy/issues/478:178,interoperability,api,api,178,"scatter plot array size argument gives inconsistent behavour; I'm trying to use an array for the size argument to my umap/scatterplot with the following code. ```. import scanpy.api as sc. import numpy as np. sc.settings.figdir = ""testdir"". sc.settings.file_format_figs = ""png"". sc.logging.print_versions(). ```. With these libraries. `scanpy==1.3.7 anndata==0.6.16 numpy==1.16.1 scipy==1.2.0 pandas==0.23.4 scikit-learn==0.20.2 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 `. Running the following code bit. I use some dummy variable for size. ```. somedata = sc.datasets.paul15(). sc.pp.pca(somedata). sc.pp.neighbors(somedata, n_neighbors=4, n_pcs=20). sc.tl.umap(somedata, spread=1, min_dist=0.1, random_state=42). sc.tl.leiden(somedata, resolution=0.5, random_state=42). z = np.abs(somedata.obsm['X_pca'][:,0])**1. sc.pl.umap(somedata, color=['1110007C09Rik'], size=z, cmap='viridis', save='continuous_expr.png'). sc.pl.umap(somedata, color=['leiden'], size=z, cmap='viridis', save='group_value.png'). ```. I get the following two figure as output. ![umapcontinuous_expr](https://user-images.githubusercontent.com/715716/52612879-951a3300-2e59-11e9-9dad-a8afc60a4b54.png). ![umapgroup_value](https://user-images.githubusercontent.com/715716/52612880-95b2c980-2e59-11e9-9a44-81dd84e3274d.png). I would expect to see a similar size allocation/distribution but they are very different. I Could not really find a cause for this looking at the scatter plot function so it might be somewhere deeper. . I'm need help with getting some grasp on how to interpret this issue and if possible how to map the size argument to the same data points over different plots.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/478
https://github.com/scverse/scanpy/issues/478:1356,interoperability,distribut,distribution,1356,"scatter plot array size argument gives inconsistent behavour; I'm trying to use an array for the size argument to my umap/scatterplot with the following code. ```. import scanpy.api as sc. import numpy as np. sc.settings.figdir = ""testdir"". sc.settings.file_format_figs = ""png"". sc.logging.print_versions(). ```. With these libraries. `scanpy==1.3.7 anndata==0.6.16 numpy==1.16.1 scipy==1.2.0 pandas==0.23.4 scikit-learn==0.20.2 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 `. Running the following code bit. I use some dummy variable for size. ```. somedata = sc.datasets.paul15(). sc.pp.pca(somedata). sc.pp.neighbors(somedata, n_neighbors=4, n_pcs=20). sc.tl.umap(somedata, spread=1, min_dist=0.1, random_state=42). sc.tl.leiden(somedata, resolution=0.5, random_state=42). z = np.abs(somedata.obsm['X_pca'][:,0])**1. sc.pl.umap(somedata, color=['1110007C09Rik'], size=z, cmap='viridis', save='continuous_expr.png'). sc.pl.umap(somedata, color=['leiden'], size=z, cmap='viridis', save='group_value.png'). ```. I get the following two figure as output. ![umapcontinuous_expr](https://user-images.githubusercontent.com/715716/52612879-951a3300-2e59-11e9-9dad-a8afc60a4b54.png). ![umapgroup_value](https://user-images.githubusercontent.com/715716/52612880-95b2c980-2e59-11e9-9a44-81dd84e3274d.png). I would expect to see a similar size allocation/distribution but they are very different. I Could not really find a cause for this looking at the scatter plot function so it might be somewhere deeper. . I'm need help with getting some grasp on how to interpret this issue and if possible how to map the size argument to the same data points over different plots.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/478
https://github.com/scverse/scanpy/issues/478:536,modifiability,variab,variable,536,"scatter plot array size argument gives inconsistent behavour; I'm trying to use an array for the size argument to my umap/scatterplot with the following code. ```. import scanpy.api as sc. import numpy as np. sc.settings.figdir = ""testdir"". sc.settings.file_format_figs = ""png"". sc.logging.print_versions(). ```. With these libraries. `scanpy==1.3.7 anndata==0.6.16 numpy==1.16.1 scipy==1.2.0 pandas==0.23.4 scikit-learn==0.20.2 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 `. Running the following code bit. I use some dummy variable for size. ```. somedata = sc.datasets.paul15(). sc.pp.pca(somedata). sc.pp.neighbors(somedata, n_neighbors=4, n_pcs=20). sc.tl.umap(somedata, spread=1, min_dist=0.1, random_state=42). sc.tl.leiden(somedata, resolution=0.5, random_state=42). z = np.abs(somedata.obsm['X_pca'][:,0])**1. sc.pl.umap(somedata, color=['1110007C09Rik'], size=z, cmap='viridis', save='continuous_expr.png'). sc.pl.umap(somedata, color=['leiden'], size=z, cmap='viridis', save='group_value.png'). ```. I get the following two figure as output. ![umapcontinuous_expr](https://user-images.githubusercontent.com/715716/52612879-951a3300-2e59-11e9-9dad-a8afc60a4b54.png). ![umapgroup_value](https://user-images.githubusercontent.com/715716/52612880-95b2c980-2e59-11e9-9a44-81dd84e3274d.png). I would expect to see a similar size allocation/distribution but they are very different. I Could not really find a cause for this looking at the scatter plot function so it might be somewhere deeper. . I'm need help with getting some grasp on how to interpret this issue and if possible how to map the size argument to the same data points over different plots.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/478
https://github.com/scverse/scanpy/issues/478:231,safety,test,testdir,231,"scatter plot array size argument gives inconsistent behavour; I'm trying to use an array for the size argument to my umap/scatterplot with the following code. ```. import scanpy.api as sc. import numpy as np. sc.settings.figdir = ""testdir"". sc.settings.file_format_figs = ""png"". sc.logging.print_versions(). ```. With these libraries. `scanpy==1.3.7 anndata==0.6.16 numpy==1.16.1 scipy==1.2.0 pandas==0.23.4 scikit-learn==0.20.2 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 `. Running the following code bit. I use some dummy variable for size. ```. somedata = sc.datasets.paul15(). sc.pp.pca(somedata). sc.pp.neighbors(somedata, n_neighbors=4, n_pcs=20). sc.tl.umap(somedata, spread=1, min_dist=0.1, random_state=42). sc.tl.leiden(somedata, resolution=0.5, random_state=42). z = np.abs(somedata.obsm['X_pca'][:,0])**1. sc.pl.umap(somedata, color=['1110007C09Rik'], size=z, cmap='viridis', save='continuous_expr.png'). sc.pl.umap(somedata, color=['leiden'], size=z, cmap='viridis', save='group_value.png'). ```. I get the following two figure as output. ![umapcontinuous_expr](https://user-images.githubusercontent.com/715716/52612879-951a3300-2e59-11e9-9dad-a8afc60a4b54.png). ![umapgroup_value](https://user-images.githubusercontent.com/715716/52612880-95b2c980-2e59-11e9-9a44-81dd84e3274d.png). I would expect to see a similar size allocation/distribution but they are very different. I Could not really find a cause for this looking at the scatter plot function so it might be somewhere deeper. . I'm need help with getting some grasp on how to interpret this issue and if possible how to map the size argument to the same data points over different plots.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/478
https://github.com/scverse/scanpy/issues/478:282,safety,log,logging,282,"scatter plot array size argument gives inconsistent behavour; I'm trying to use an array for the size argument to my umap/scatterplot with the following code. ```. import scanpy.api as sc. import numpy as np. sc.settings.figdir = ""testdir"". sc.settings.file_format_figs = ""png"". sc.logging.print_versions(). ```. With these libraries. `scanpy==1.3.7 anndata==0.6.16 numpy==1.16.1 scipy==1.2.0 pandas==0.23.4 scikit-learn==0.20.2 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 `. Running the following code bit. I use some dummy variable for size. ```. somedata = sc.datasets.paul15(). sc.pp.pca(somedata). sc.pp.neighbors(somedata, n_neighbors=4, n_pcs=20). sc.tl.umap(somedata, spread=1, min_dist=0.1, random_state=42). sc.tl.leiden(somedata, resolution=0.5, random_state=42). z = np.abs(somedata.obsm['X_pca'][:,0])**1. sc.pl.umap(somedata, color=['1110007C09Rik'], size=z, cmap='viridis', save='continuous_expr.png'). sc.pl.umap(somedata, color=['leiden'], size=z, cmap='viridis', save='group_value.png'). ```. I get the following two figure as output. ![umapcontinuous_expr](https://user-images.githubusercontent.com/715716/52612879-951a3300-2e59-11e9-9dad-a8afc60a4b54.png). ![umapgroup_value](https://user-images.githubusercontent.com/715716/52612880-95b2c980-2e59-11e9-9a44-81dd84e3274d.png). I would expect to see a similar size allocation/distribution but they are very different. I Could not really find a cause for this looking at the scatter plot function so it might be somewhere deeper. . I'm need help with getting some grasp on how to interpret this issue and if possible how to map the size argument to the same data points over different plots.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/478
https://github.com/scverse/scanpy/issues/478:282,security,log,logging,282,"scatter plot array size argument gives inconsistent behavour; I'm trying to use an array for the size argument to my umap/scatterplot with the following code. ```. import scanpy.api as sc. import numpy as np. sc.settings.figdir = ""testdir"". sc.settings.file_format_figs = ""png"". sc.logging.print_versions(). ```. With these libraries. `scanpy==1.3.7 anndata==0.6.16 numpy==1.16.1 scipy==1.2.0 pandas==0.23.4 scikit-learn==0.20.2 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 `. Running the following code bit. I use some dummy variable for size. ```. somedata = sc.datasets.paul15(). sc.pp.pca(somedata). sc.pp.neighbors(somedata, n_neighbors=4, n_pcs=20). sc.tl.umap(somedata, spread=1, min_dist=0.1, random_state=42). sc.tl.leiden(somedata, resolution=0.5, random_state=42). z = np.abs(somedata.obsm['X_pca'][:,0])**1. sc.pl.umap(somedata, color=['1110007C09Rik'], size=z, cmap='viridis', save='continuous_expr.png'). sc.pl.umap(somedata, color=['leiden'], size=z, cmap='viridis', save='group_value.png'). ```. I get the following two figure as output. ![umapcontinuous_expr](https://user-images.githubusercontent.com/715716/52612879-951a3300-2e59-11e9-9dad-a8afc60a4b54.png). ![umapgroup_value](https://user-images.githubusercontent.com/715716/52612880-95b2c980-2e59-11e9-9a44-81dd84e3274d.png). I would expect to see a similar size allocation/distribution but they are very different. I Could not really find a cause for this looking at the scatter plot function so it might be somewhere deeper. . I'm need help with getting some grasp on how to interpret this issue and if possible how to map the size argument to the same data points over different plots.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/478
https://github.com/scverse/scanpy/issues/478:231,testability,test,testdir,231,"scatter plot array size argument gives inconsistent behavour; I'm trying to use an array for the size argument to my umap/scatterplot with the following code. ```. import scanpy.api as sc. import numpy as np. sc.settings.figdir = ""testdir"". sc.settings.file_format_figs = ""png"". sc.logging.print_versions(). ```. With these libraries. `scanpy==1.3.7 anndata==0.6.16 numpy==1.16.1 scipy==1.2.0 pandas==0.23.4 scikit-learn==0.20.2 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 `. Running the following code bit. I use some dummy variable for size. ```. somedata = sc.datasets.paul15(). sc.pp.pca(somedata). sc.pp.neighbors(somedata, n_neighbors=4, n_pcs=20). sc.tl.umap(somedata, spread=1, min_dist=0.1, random_state=42). sc.tl.leiden(somedata, resolution=0.5, random_state=42). z = np.abs(somedata.obsm['X_pca'][:,0])**1. sc.pl.umap(somedata, color=['1110007C09Rik'], size=z, cmap='viridis', save='continuous_expr.png'). sc.pl.umap(somedata, color=['leiden'], size=z, cmap='viridis', save='group_value.png'). ```. I get the following two figure as output. ![umapcontinuous_expr](https://user-images.githubusercontent.com/715716/52612879-951a3300-2e59-11e9-9dad-a8afc60a4b54.png). ![umapgroup_value](https://user-images.githubusercontent.com/715716/52612880-95b2c980-2e59-11e9-9a44-81dd84e3274d.png). I would expect to see a similar size allocation/distribution but they are very different. I Could not really find a cause for this looking at the scatter plot function so it might be somewhere deeper. . I'm need help with getting some grasp on how to interpret this issue and if possible how to map the size argument to the same data points over different plots.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/478
https://github.com/scverse/scanpy/issues/478:282,testability,log,logging,282,"scatter plot array size argument gives inconsistent behavour; I'm trying to use an array for the size argument to my umap/scatterplot with the following code. ```. import scanpy.api as sc. import numpy as np. sc.settings.figdir = ""testdir"". sc.settings.file_format_figs = ""png"". sc.logging.print_versions(). ```. With these libraries. `scanpy==1.3.7 anndata==0.6.16 numpy==1.16.1 scipy==1.2.0 pandas==0.23.4 scikit-learn==0.20.2 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 `. Running the following code bit. I use some dummy variable for size. ```. somedata = sc.datasets.paul15(). sc.pp.pca(somedata). sc.pp.neighbors(somedata, n_neighbors=4, n_pcs=20). sc.tl.umap(somedata, spread=1, min_dist=0.1, random_state=42). sc.tl.leiden(somedata, resolution=0.5, random_state=42). z = np.abs(somedata.obsm['X_pca'][:,0])**1. sc.pl.umap(somedata, color=['1110007C09Rik'], size=z, cmap='viridis', save='continuous_expr.png'). sc.pl.umap(somedata, color=['leiden'], size=z, cmap='viridis', save='group_value.png'). ```. I get the following two figure as output. ![umapcontinuous_expr](https://user-images.githubusercontent.com/715716/52612879-951a3300-2e59-11e9-9dad-a8afc60a4b54.png). ![umapgroup_value](https://user-images.githubusercontent.com/715716/52612880-95b2c980-2e59-11e9-9a44-81dd84e3274d.png). I would expect to see a similar size allocation/distribution but they are very different. I Could not really find a cause for this looking at the scatter plot function so it might be somewhere deeper. . I'm need help with getting some grasp on how to interpret this issue and if possible how to map the size argument to the same data points over different plots.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/478
https://github.com/scverse/scanpy/issues/478:415,usability,learn,learn,415,"scatter plot array size argument gives inconsistent behavour; I'm trying to use an array for the size argument to my umap/scatterplot with the following code. ```. import scanpy.api as sc. import numpy as np. sc.settings.figdir = ""testdir"". sc.settings.file_format_figs = ""png"". sc.logging.print_versions(). ```. With these libraries. `scanpy==1.3.7 anndata==0.6.16 numpy==1.16.1 scipy==1.2.0 pandas==0.23.4 scikit-learn==0.20.2 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 `. Running the following code bit. I use some dummy variable for size. ```. somedata = sc.datasets.paul15(). sc.pp.pca(somedata). sc.pp.neighbors(somedata, n_neighbors=4, n_pcs=20). sc.tl.umap(somedata, spread=1, min_dist=0.1, random_state=42). sc.tl.leiden(somedata, resolution=0.5, random_state=42). z = np.abs(somedata.obsm['X_pca'][:,0])**1. sc.pl.umap(somedata, color=['1110007C09Rik'], size=z, cmap='viridis', save='continuous_expr.png'). sc.pl.umap(somedata, color=['leiden'], size=z, cmap='viridis', save='group_value.png'). ```. I get the following two figure as output. ![umapcontinuous_expr](https://user-images.githubusercontent.com/715716/52612879-951a3300-2e59-11e9-9dad-a8afc60a4b54.png). ![umapgroup_value](https://user-images.githubusercontent.com/715716/52612880-95b2c980-2e59-11e9-9a44-81dd84e3274d.png). I would expect to see a similar size allocation/distribution but they are very different. I Could not really find a cause for this looking at the scatter plot function so it might be somewhere deeper. . I'm need help with getting some grasp on how to interpret this issue and if possible how to map the size argument to the same data points over different plots.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/478
https://github.com/scverse/scanpy/issues/478:1095,usability,user,user-images,1095,"scatter plot array size argument gives inconsistent behavour; I'm trying to use an array for the size argument to my umap/scatterplot with the following code. ```. import scanpy.api as sc. import numpy as np. sc.settings.figdir = ""testdir"". sc.settings.file_format_figs = ""png"". sc.logging.print_versions(). ```. With these libraries. `scanpy==1.3.7 anndata==0.6.16 numpy==1.16.1 scipy==1.2.0 pandas==0.23.4 scikit-learn==0.20.2 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 `. Running the following code bit. I use some dummy variable for size. ```. somedata = sc.datasets.paul15(). sc.pp.pca(somedata). sc.pp.neighbors(somedata, n_neighbors=4, n_pcs=20). sc.tl.umap(somedata, spread=1, min_dist=0.1, random_state=42). sc.tl.leiden(somedata, resolution=0.5, random_state=42). z = np.abs(somedata.obsm['X_pca'][:,0])**1. sc.pl.umap(somedata, color=['1110007C09Rik'], size=z, cmap='viridis', save='continuous_expr.png'). sc.pl.umap(somedata, color=['leiden'], size=z, cmap='viridis', save='group_value.png'). ```. I get the following two figure as output. ![umapcontinuous_expr](https://user-images.githubusercontent.com/715716/52612879-951a3300-2e59-11e9-9dad-a8afc60a4b54.png). ![umapgroup_value](https://user-images.githubusercontent.com/715716/52612880-95b2c980-2e59-11e9-9a44-81dd84e3274d.png). I would expect to see a similar size allocation/distribution but they are very different. I Could not really find a cause for this looking at the scatter plot function so it might be somewhere deeper. . I'm need help with getting some grasp on how to interpret this issue and if possible how to map the size argument to the same data points over different plots.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/478
https://github.com/scverse/scanpy/issues/478:1215,usability,user,user-images,1215,"scatter plot array size argument gives inconsistent behavour; I'm trying to use an array for the size argument to my umap/scatterplot with the following code. ```. import scanpy.api as sc. import numpy as np. sc.settings.figdir = ""testdir"". sc.settings.file_format_figs = ""png"". sc.logging.print_versions(). ```. With these libraries. `scanpy==1.3.7 anndata==0.6.16 numpy==1.16.1 scipy==1.2.0 pandas==0.23.4 scikit-learn==0.20.2 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 `. Running the following code bit. I use some dummy variable for size. ```. somedata = sc.datasets.paul15(). sc.pp.pca(somedata). sc.pp.neighbors(somedata, n_neighbors=4, n_pcs=20). sc.tl.umap(somedata, spread=1, min_dist=0.1, random_state=42). sc.tl.leiden(somedata, resolution=0.5, random_state=42). z = np.abs(somedata.obsm['X_pca'][:,0])**1. sc.pl.umap(somedata, color=['1110007C09Rik'], size=z, cmap='viridis', save='continuous_expr.png'). sc.pl.umap(somedata, color=['leiden'], size=z, cmap='viridis', save='group_value.png'). ```. I get the following two figure as output. ![umapcontinuous_expr](https://user-images.githubusercontent.com/715716/52612879-951a3300-2e59-11e9-9dad-a8afc60a4b54.png). ![umapgroup_value](https://user-images.githubusercontent.com/715716/52612880-95b2c980-2e59-11e9-9a44-81dd84e3274d.png). I would expect to see a similar size allocation/distribution but they are very different. I Could not really find a cause for this looking at the scatter plot function so it might be somewhere deeper. . I'm need help with getting some grasp on how to interpret this issue and if possible how to map the size argument to the same data points over different plots.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/478
https://github.com/scverse/scanpy/issues/478:1520,usability,help,help,1520,"scatter plot array size argument gives inconsistent behavour; I'm trying to use an array for the size argument to my umap/scatterplot with the following code. ```. import scanpy.api as sc. import numpy as np. sc.settings.figdir = ""testdir"". sc.settings.file_format_figs = ""png"". sc.logging.print_versions(). ```. With these libraries. `scanpy==1.3.7 anndata==0.6.16 numpy==1.16.1 scipy==1.2.0 pandas==0.23.4 scikit-learn==0.20.2 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 `. Running the following code bit. I use some dummy variable for size. ```. somedata = sc.datasets.paul15(). sc.pp.pca(somedata). sc.pp.neighbors(somedata, n_neighbors=4, n_pcs=20). sc.tl.umap(somedata, spread=1, min_dist=0.1, random_state=42). sc.tl.leiden(somedata, resolution=0.5, random_state=42). z = np.abs(somedata.obsm['X_pca'][:,0])**1. sc.pl.umap(somedata, color=['1110007C09Rik'], size=z, cmap='viridis', save='continuous_expr.png'). sc.pl.umap(somedata, color=['leiden'], size=z, cmap='viridis', save='group_value.png'). ```. I get the following two figure as output. ![umapcontinuous_expr](https://user-images.githubusercontent.com/715716/52612879-951a3300-2e59-11e9-9dad-a8afc60a4b54.png). ![umapgroup_value](https://user-images.githubusercontent.com/715716/52612880-95b2c980-2e59-11e9-9a44-81dd84e3274d.png). I would expect to see a similar size allocation/distribution but they are very different. I Could not really find a cause for this looking at the scatter plot function so it might be somewhere deeper. . I'm need help with getting some grasp on how to interpret this issue and if possible how to map the size argument to the same data points over different plots.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/478
https://github.com/scverse/scanpy/issues/479:32,usability,user,users,32,"multimodal analysis; Dear,. Now users can analyze and explore multi-modal data with Seurat, are there any equivalent methods in Scanpy ? Cellranger3 adds some new features and now supports multi-modal data too. Using Seurat with multi-modal data: https://satijalab.org/seurat/multimodal_vignette.html. here is an related issue: https://github.com/theislab/scanpy/issues/351.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/479
https://github.com/scverse/scanpy/issues/479:62,usability,multi-mod,multi-modal,62,"multimodal analysis; Dear,. Now users can analyze and explore multi-modal data with Seurat, are there any equivalent methods in Scanpy ? Cellranger3 adds some new features and now supports multi-modal data too. Using Seurat with multi-modal data: https://satijalab.org/seurat/multimodal_vignette.html. here is an related issue: https://github.com/theislab/scanpy/issues/351.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/479
https://github.com/scverse/scanpy/issues/479:180,usability,support,supports,180,"multimodal analysis; Dear,. Now users can analyze and explore multi-modal data with Seurat, are there any equivalent methods in Scanpy ? Cellranger3 adds some new features and now supports multi-modal data too. Using Seurat with multi-modal data: https://satijalab.org/seurat/multimodal_vignette.html. here is an related issue: https://github.com/theislab/scanpy/issues/351.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/479
https://github.com/scverse/scanpy/issues/479:189,usability,multi-mod,multi-modal,189,"multimodal analysis; Dear,. Now users can analyze and explore multi-modal data with Seurat, are there any equivalent methods in Scanpy ? Cellranger3 adds some new features and now supports multi-modal data too. Using Seurat with multi-modal data: https://satijalab.org/seurat/multimodal_vignette.html. here is an related issue: https://github.com/theislab/scanpy/issues/351.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/479
https://github.com/scverse/scanpy/issues/479:229,usability,multi-mod,multi-modal,229,"multimodal analysis; Dear,. Now users can analyze and explore multi-modal data with Seurat, are there any equivalent methods in Scanpy ? Cellranger3 adds some new features and now supports multi-modal data too. Using Seurat with multi-modal data: https://satijalab.org/seurat/multimodal_vignette.html. here is an related issue: https://github.com/theislab/scanpy/issues/351.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/479
https://github.com/scverse/scanpy/issues/480:46,availability,cluster,clusters,46,"sc.pl.tracksplot tracks do not align with the clusters; Hi guys,. sorry for opening so many issues, I really love using tracksplot for visualizing the expression in different clusters, but the small issue I have is that tracks actually never align well with the clusters ""timeline"" at the bottom. especially the first and last tracks are always misaligned. I couldn't find the list of arguments that I can pass through sc.pl.tracksplot command maybe just changing margins or size of the plot might make it better. . ![tracksplotin_final](https://user-images.githubusercontent.com/43454880/52679490-1f11dc80-2eea-11e9-8a6e-511beefb4c1d.png). in this example, cluster 0, 8,9,10 tracks are misaligned to the timeline below. . Thanks a lot,. Tsotne.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/480
https://github.com/scverse/scanpy/issues/480:175,availability,cluster,clusters,175,"sc.pl.tracksplot tracks do not align with the clusters; Hi guys,. sorry for opening so many issues, I really love using tracksplot for visualizing the expression in different clusters, but the small issue I have is that tracks actually never align well with the clusters ""timeline"" at the bottom. especially the first and last tracks are always misaligned. I couldn't find the list of arguments that I can pass through sc.pl.tracksplot command maybe just changing margins or size of the plot might make it better. . ![tracksplotin_final](https://user-images.githubusercontent.com/43454880/52679490-1f11dc80-2eea-11e9-8a6e-511beefb4c1d.png). in this example, cluster 0, 8,9,10 tracks are misaligned to the timeline below. . Thanks a lot,. Tsotne.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/480
https://github.com/scverse/scanpy/issues/480:262,availability,cluster,clusters,262,"sc.pl.tracksplot tracks do not align with the clusters; Hi guys,. sorry for opening so many issues, I really love using tracksplot for visualizing the expression in different clusters, but the small issue I have is that tracks actually never align well with the clusters ""timeline"" at the bottom. especially the first and last tracks are always misaligned. I couldn't find the list of arguments that I can pass through sc.pl.tracksplot command maybe just changing margins or size of the plot might make it better. . ![tracksplotin_final](https://user-images.githubusercontent.com/43454880/52679490-1f11dc80-2eea-11e9-8a6e-511beefb4c1d.png). in this example, cluster 0, 8,9,10 tracks are misaligned to the timeline below. . Thanks a lot,. Tsotne.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/480
https://github.com/scverse/scanpy/issues/480:658,availability,cluster,cluster,658,"sc.pl.tracksplot tracks do not align with the clusters; Hi guys,. sorry for opening so many issues, I really love using tracksplot for visualizing the expression in different clusters, but the small issue I have is that tracks actually never align well with the clusters ""timeline"" at the bottom. especially the first and last tracks are always misaligned. I couldn't find the list of arguments that I can pass through sc.pl.tracksplot command maybe just changing margins or size of the plot might make it better. . ![tracksplotin_final](https://user-images.githubusercontent.com/43454880/52679490-1f11dc80-2eea-11e9-8a6e-511beefb4c1d.png). in this example, cluster 0, 8,9,10 tracks are misaligned to the timeline below. . Thanks a lot,. Tsotne.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/480
https://github.com/scverse/scanpy/issues/480:46,deployability,cluster,clusters,46,"sc.pl.tracksplot tracks do not align with the clusters; Hi guys,. sorry for opening so many issues, I really love using tracksplot for visualizing the expression in different clusters, but the small issue I have is that tracks actually never align well with the clusters ""timeline"" at the bottom. especially the first and last tracks are always misaligned. I couldn't find the list of arguments that I can pass through sc.pl.tracksplot command maybe just changing margins or size of the plot might make it better. . ![tracksplotin_final](https://user-images.githubusercontent.com/43454880/52679490-1f11dc80-2eea-11e9-8a6e-511beefb4c1d.png). in this example, cluster 0, 8,9,10 tracks are misaligned to the timeline below. . Thanks a lot,. Tsotne.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/480
https://github.com/scverse/scanpy/issues/480:175,deployability,cluster,clusters,175,"sc.pl.tracksplot tracks do not align with the clusters; Hi guys,. sorry for opening so many issues, I really love using tracksplot for visualizing the expression in different clusters, but the small issue I have is that tracks actually never align well with the clusters ""timeline"" at the bottom. especially the first and last tracks are always misaligned. I couldn't find the list of arguments that I can pass through sc.pl.tracksplot command maybe just changing margins or size of the plot might make it better. . ![tracksplotin_final](https://user-images.githubusercontent.com/43454880/52679490-1f11dc80-2eea-11e9-8a6e-511beefb4c1d.png). in this example, cluster 0, 8,9,10 tracks are misaligned to the timeline below. . Thanks a lot,. Tsotne.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/480
https://github.com/scverse/scanpy/issues/480:262,deployability,cluster,clusters,262,"sc.pl.tracksplot tracks do not align with the clusters; Hi guys,. sorry for opening so many issues, I really love using tracksplot for visualizing the expression in different clusters, but the small issue I have is that tracks actually never align well with the clusters ""timeline"" at the bottom. especially the first and last tracks are always misaligned. I couldn't find the list of arguments that I can pass through sc.pl.tracksplot command maybe just changing margins or size of the plot might make it better. . ![tracksplotin_final](https://user-images.githubusercontent.com/43454880/52679490-1f11dc80-2eea-11e9-8a6e-511beefb4c1d.png). in this example, cluster 0, 8,9,10 tracks are misaligned to the timeline below. . Thanks a lot,. Tsotne.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/480
https://github.com/scverse/scanpy/issues/480:658,deployability,cluster,cluster,658,"sc.pl.tracksplot tracks do not align with the clusters; Hi guys,. sorry for opening so many issues, I really love using tracksplot for visualizing the expression in different clusters, but the small issue I have is that tracks actually never align well with the clusters ""timeline"" at the bottom. especially the first and last tracks are always misaligned. I couldn't find the list of arguments that I can pass through sc.pl.tracksplot command maybe just changing margins or size of the plot might make it better. . ![tracksplotin_final](https://user-images.githubusercontent.com/43454880/52679490-1f11dc80-2eea-11e9-8a6e-511beefb4c1d.png). in this example, cluster 0, 8,9,10 tracks are misaligned to the timeline below. . Thanks a lot,. Tsotne.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/480
https://github.com/scverse/scanpy/issues/480:272,performance,time,timeline,272,"sc.pl.tracksplot tracks do not align with the clusters; Hi guys,. sorry for opening so many issues, I really love using tracksplot for visualizing the expression in different clusters, but the small issue I have is that tracks actually never align well with the clusters ""timeline"" at the bottom. especially the first and last tracks are always misaligned. I couldn't find the list of arguments that I can pass through sc.pl.tracksplot command maybe just changing margins or size of the plot might make it better. . ![tracksplotin_final](https://user-images.githubusercontent.com/43454880/52679490-1f11dc80-2eea-11e9-8a6e-511beefb4c1d.png). in this example, cluster 0, 8,9,10 tracks are misaligned to the timeline below. . Thanks a lot,. Tsotne.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/480
https://github.com/scverse/scanpy/issues/480:705,performance,time,timeline,705,"sc.pl.tracksplot tracks do not align with the clusters; Hi guys,. sorry for opening so many issues, I really love using tracksplot for visualizing the expression in different clusters, but the small issue I have is that tracks actually never align well with the clusters ""timeline"" at the bottom. especially the first and last tracks are always misaligned. I couldn't find the list of arguments that I can pass through sc.pl.tracksplot command maybe just changing margins or size of the plot might make it better. . ![tracksplotin_final](https://user-images.githubusercontent.com/43454880/52679490-1f11dc80-2eea-11e9-8a6e-511beefb4c1d.png). in this example, cluster 0, 8,9,10 tracks are misaligned to the timeline below. . Thanks a lot,. Tsotne.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/480
https://github.com/scverse/scanpy/issues/480:135,usability,visual,visualizing,135,"sc.pl.tracksplot tracks do not align with the clusters; Hi guys,. sorry for opening so many issues, I really love using tracksplot for visualizing the expression in different clusters, but the small issue I have is that tracks actually never align well with the clusters ""timeline"" at the bottom. especially the first and last tracks are always misaligned. I couldn't find the list of arguments that I can pass through sc.pl.tracksplot command maybe just changing margins or size of the plot might make it better. . ![tracksplotin_final](https://user-images.githubusercontent.com/43454880/52679490-1f11dc80-2eea-11e9-8a6e-511beefb4c1d.png). in this example, cluster 0, 8,9,10 tracks are misaligned to the timeline below. . Thanks a lot,. Tsotne.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/480
https://github.com/scverse/scanpy/issues/480:436,usability,command,command,436,"sc.pl.tracksplot tracks do not align with the clusters; Hi guys,. sorry for opening so many issues, I really love using tracksplot for visualizing the expression in different clusters, but the small issue I have is that tracks actually never align well with the clusters ""timeline"" at the bottom. especially the first and last tracks are always misaligned. I couldn't find the list of arguments that I can pass through sc.pl.tracksplot command maybe just changing margins or size of the plot might make it better. . ![tracksplotin_final](https://user-images.githubusercontent.com/43454880/52679490-1f11dc80-2eea-11e9-8a6e-511beefb4c1d.png). in this example, cluster 0, 8,9,10 tracks are misaligned to the timeline below. . Thanks a lot,. Tsotne.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/480
https://github.com/scverse/scanpy/issues/480:546,usability,user,user-images,546,"sc.pl.tracksplot tracks do not align with the clusters; Hi guys,. sorry for opening so many issues, I really love using tracksplot for visualizing the expression in different clusters, but the small issue I have is that tracks actually never align well with the clusters ""timeline"" at the bottom. especially the first and last tracks are always misaligned. I couldn't find the list of arguments that I can pass through sc.pl.tracksplot command maybe just changing margins or size of the plot might make it better. . ![tracksplotin_final](https://user-images.githubusercontent.com/43454880/52679490-1f11dc80-2eea-11e9-8a6e-511beefb4c1d.png). in this example, cluster 0, 8,9,10 tracks are misaligned to the timeline below. . Thanks a lot,. Tsotne.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/480
https://github.com/scverse/scanpy/issues/482:15,availability,error,error,15,"Github install error; Tried to install via `$ pip3 install -e .` but returned this error:. ```. Obtaining file://path/to/scanpy_1.4/scanpy. Complete output from command python setup.py egg_info:. /path/to/miniconda3/envs/bio/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`. from ._conv import register_converters as _register_converters. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""path/to/scanpy/setup.py"", line 11, in <module>. from scanpy import __author__, __email__. File ""path/to/scanpy/scanpy/__init__.py"", line 26, in <module>. check_versions(). File ""path/to/scanpy/scanpy/utils.py"", line 38, in check_versions. .format(__version__, anndata.__version__)). NameError: name '__version__' is not defined. ----------------------------------------. Command ""python setup.py egg_info"" failed with error code 1 in path/to/scanpy/. ```. The variable `__version__` in line 38 in utils.py is not defined.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/482
https://github.com/scverse/scanpy/issues/482:83,availability,error,error,83,"Github install error; Tried to install via `$ pip3 install -e .` but returned this error:. ```. Obtaining file://path/to/scanpy_1.4/scanpy. Complete output from command python setup.py egg_info:. /path/to/miniconda3/envs/bio/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`. from ._conv import register_converters as _register_converters. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""path/to/scanpy/setup.py"", line 11, in <module>. from scanpy import __author__, __email__. File ""path/to/scanpy/scanpy/__init__.py"", line 26, in <module>. check_versions(). File ""path/to/scanpy/scanpy/utils.py"", line 38, in check_versions. .format(__version__, anndata.__version__)). NameError: name '__version__' is not defined. ----------------------------------------. Command ""python setup.py egg_info"" failed with error code 1 in path/to/scanpy/. ```. The variable `__version__` in line 38 in utils.py is not defined.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/482
https://github.com/scverse/scanpy/issues/482:1015,availability,error,error,1015,"Github install error; Tried to install via `$ pip3 install -e .` but returned this error:. ```. Obtaining file://path/to/scanpy_1.4/scanpy. Complete output from command python setup.py egg_info:. /path/to/miniconda3/envs/bio/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`. from ._conv import register_converters as _register_converters. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""path/to/scanpy/setup.py"", line 11, in <module>. from scanpy import __author__, __email__. File ""path/to/scanpy/scanpy/__init__.py"", line 26, in <module>. check_versions(). File ""path/to/scanpy/scanpy/utils.py"", line 38, in check_versions. .format(__version__, anndata.__version__)). NameError: name '__version__' is not defined. ----------------------------------------. Command ""python setup.py egg_info"" failed with error code 1 in path/to/scanpy/. ```. The variable `__version__` in line 38 in utils.py is not defined.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/482
https://github.com/scverse/scanpy/issues/482:7,deployability,instal,install,7,"Github install error; Tried to install via `$ pip3 install -e .` but returned this error:. ```. Obtaining file://path/to/scanpy_1.4/scanpy. Complete output from command python setup.py egg_info:. /path/to/miniconda3/envs/bio/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`. from ._conv import register_converters as _register_converters. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""path/to/scanpy/setup.py"", line 11, in <module>. from scanpy import __author__, __email__. File ""path/to/scanpy/scanpy/__init__.py"", line 26, in <module>. check_versions(). File ""path/to/scanpy/scanpy/utils.py"", line 38, in check_versions. .format(__version__, anndata.__version__)). NameError: name '__version__' is not defined. ----------------------------------------. Command ""python setup.py egg_info"" failed with error code 1 in path/to/scanpy/. ```. The variable `__version__` in line 38 in utils.py is not defined.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/482
https://github.com/scverse/scanpy/issues/482:31,deployability,instal,install,31,"Github install error; Tried to install via `$ pip3 install -e .` but returned this error:. ```. Obtaining file://path/to/scanpy_1.4/scanpy. Complete output from command python setup.py egg_info:. /path/to/miniconda3/envs/bio/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`. from ._conv import register_converters as _register_converters. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""path/to/scanpy/setup.py"", line 11, in <module>. from scanpy import __author__, __email__. File ""path/to/scanpy/scanpy/__init__.py"", line 26, in <module>. check_versions(). File ""path/to/scanpy/scanpy/utils.py"", line 38, in check_versions. .format(__version__, anndata.__version__)). NameError: name '__version__' is not defined. ----------------------------------------. Command ""python setup.py egg_info"" failed with error code 1 in path/to/scanpy/. ```. The variable `__version__` in line 38 in utils.py is not defined.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/482
https://github.com/scverse/scanpy/issues/482:51,deployability,instal,install,51,"Github install error; Tried to install via `$ pip3 install -e .` but returned this error:. ```. Obtaining file://path/to/scanpy_1.4/scanpy. Complete output from command python setup.py egg_info:. /path/to/miniconda3/envs/bio/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`. from ._conv import register_converters as _register_converters. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""path/to/scanpy/setup.py"", line 11, in <module>. from scanpy import __author__, __email__. File ""path/to/scanpy/scanpy/__init__.py"", line 26, in <module>. check_versions(). File ""path/to/scanpy/scanpy/utils.py"", line 38, in check_versions. .format(__version__, anndata.__version__)). NameError: name '__version__' is not defined. ----------------------------------------. Command ""python setup.py egg_info"" failed with error code 1 in path/to/scanpy/. ```. The variable `__version__` in line 38 in utils.py is not defined.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/482
https://github.com/scverse/scanpy/issues/482:582,deployability,modul,module,582,"Github install error; Tried to install via `$ pip3 install -e .` but returned this error:. ```. Obtaining file://path/to/scanpy_1.4/scanpy. Complete output from command python setup.py egg_info:. /path/to/miniconda3/envs/bio/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`. from ._conv import register_converters as _register_converters. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""path/to/scanpy/setup.py"", line 11, in <module>. from scanpy import __author__, __email__. File ""path/to/scanpy/scanpy/__init__.py"", line 26, in <module>. check_versions(). File ""path/to/scanpy/scanpy/utils.py"", line 38, in check_versions. .format(__version__, anndata.__version__)). NameError: name '__version__' is not defined. ----------------------------------------. Command ""python setup.py egg_info"" failed with error code 1 in path/to/scanpy/. ```. The variable `__version__` in line 38 in utils.py is not defined.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/482
https://github.com/scverse/scanpy/issues/482:636,deployability,modul,module,636,"Github install error; Tried to install via `$ pip3 install -e .` but returned this error:. ```. Obtaining file://path/to/scanpy_1.4/scanpy. Complete output from command python setup.py egg_info:. /path/to/miniconda3/envs/bio/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`. from ._conv import register_converters as _register_converters. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""path/to/scanpy/setup.py"", line 11, in <module>. from scanpy import __author__, __email__. File ""path/to/scanpy/scanpy/__init__.py"", line 26, in <module>. check_versions(). File ""path/to/scanpy/scanpy/utils.py"", line 38, in check_versions. .format(__version__, anndata.__version__)). NameError: name '__version__' is not defined. ----------------------------------------. Command ""python setup.py egg_info"" failed with error code 1 in path/to/scanpy/. ```. The variable `__version__` in line 38 in utils.py is not defined.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/482
https://github.com/scverse/scanpy/issues/482:742,deployability,modul,module,742,"Github install error; Tried to install via `$ pip3 install -e .` but returned this error:. ```. Obtaining file://path/to/scanpy_1.4/scanpy. Complete output from command python setup.py egg_info:. /path/to/miniconda3/envs/bio/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`. from ._conv import register_converters as _register_converters. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""path/to/scanpy/setup.py"", line 11, in <module>. from scanpy import __author__, __email__. File ""path/to/scanpy/scanpy/__init__.py"", line 26, in <module>. check_versions(). File ""path/to/scanpy/scanpy/utils.py"", line 38, in check_versions. .format(__version__, anndata.__version__)). NameError: name '__version__' is not defined. ----------------------------------------. Command ""python setup.py egg_info"" failed with error code 1 in path/to/scanpy/. ```. The variable `__version__` in line 38 in utils.py is not defined.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/482
https://github.com/scverse/scanpy/issues/482:1003,deployability,fail,failed,1003,"Github install error; Tried to install via `$ pip3 install -e .` but returned this error:. ```. Obtaining file://path/to/scanpy_1.4/scanpy. Complete output from command python setup.py egg_info:. /path/to/miniconda3/envs/bio/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`. from ._conv import register_converters as _register_converters. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""path/to/scanpy/setup.py"", line 11, in <module>. from scanpy import __author__, __email__. File ""path/to/scanpy/scanpy/__init__.py"", line 26, in <module>. check_versions(). File ""path/to/scanpy/scanpy/utils.py"", line 38, in check_versions. .format(__version__, anndata.__version__)). NameError: name '__version__' is not defined. ----------------------------------------. Command ""python setup.py egg_info"" failed with error code 1 in path/to/scanpy/. ```. The variable `__version__` in line 38 in utils.py is not defined.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/482
https://github.com/scverse/scanpy/issues/482:289,interoperability,Convers,Conversion,289,"Github install error; Tried to install via `$ pip3 install -e .` but returned this error:. ```. Obtaining file://path/to/scanpy_1.4/scanpy. Complete output from command python setup.py egg_info:. /path/to/miniconda3/envs/bio/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`. from ._conv import register_converters as _register_converters. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""path/to/scanpy/setup.py"", line 11, in <module>. from scanpy import __author__, __email__. File ""path/to/scanpy/scanpy/__init__.py"", line 26, in <module>. check_versions(). File ""path/to/scanpy/scanpy/utils.py"", line 38, in check_versions. .format(__version__, anndata.__version__)). NameError: name '__version__' is not defined. ----------------------------------------. Command ""python setup.py egg_info"" failed with error code 1 in path/to/scanpy/. ```. The variable `__version__` in line 38 in utils.py is not defined.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/482
https://github.com/scverse/scanpy/issues/482:837,interoperability,format,format,837,"Github install error; Tried to install via `$ pip3 install -e .` but returned this error:. ```. Obtaining file://path/to/scanpy_1.4/scanpy. Complete output from command python setup.py egg_info:. /path/to/miniconda3/envs/bio/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`. from ._conv import register_converters as _register_converters. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""path/to/scanpy/setup.py"", line 11, in <module>. from scanpy import __author__, __email__. File ""path/to/scanpy/scanpy/__init__.py"", line 26, in <module>. check_versions(). File ""path/to/scanpy/scanpy/utils.py"", line 38, in check_versions. .format(__version__, anndata.__version__)). NameError: name '__version__' is not defined. ----------------------------------------. Command ""python setup.py egg_info"" failed with error code 1 in path/to/scanpy/. ```. The variable `__version__` in line 38 in utils.py is not defined.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/482
https://github.com/scverse/scanpy/issues/482:244,modifiability,pac,packages,244,"Github install error; Tried to install via `$ pip3 install -e .` but returned this error:. ```. Obtaining file://path/to/scanpy_1.4/scanpy. Complete output from command python setup.py egg_info:. /path/to/miniconda3/envs/bio/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`. from ._conv import register_converters as _register_converters. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""path/to/scanpy/setup.py"", line 11, in <module>. from scanpy import __author__, __email__. File ""path/to/scanpy/scanpy/__init__.py"", line 26, in <module>. check_versions(). File ""path/to/scanpy/scanpy/utils.py"", line 38, in check_versions. .format(__version__, anndata.__version__)). NameError: name '__version__' is not defined. ----------------------------------------. Command ""python setup.py egg_info"" failed with error code 1 in path/to/scanpy/. ```. The variable `__version__` in line 38 in utils.py is not defined.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/482
https://github.com/scverse/scanpy/issues/482:582,modifiability,modul,module,582,"Github install error; Tried to install via `$ pip3 install -e .` but returned this error:. ```. Obtaining file://path/to/scanpy_1.4/scanpy. Complete output from command python setup.py egg_info:. /path/to/miniconda3/envs/bio/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`. from ._conv import register_converters as _register_converters. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""path/to/scanpy/setup.py"", line 11, in <module>. from scanpy import __author__, __email__. File ""path/to/scanpy/scanpy/__init__.py"", line 26, in <module>. check_versions(). File ""path/to/scanpy/scanpy/utils.py"", line 38, in check_versions. .format(__version__, anndata.__version__)). NameError: name '__version__' is not defined. ----------------------------------------. Command ""python setup.py egg_info"" failed with error code 1 in path/to/scanpy/. ```. The variable `__version__` in line 38 in utils.py is not defined.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/482
https://github.com/scverse/scanpy/issues/482:636,modifiability,modul,module,636,"Github install error; Tried to install via `$ pip3 install -e .` but returned this error:. ```. Obtaining file://path/to/scanpy_1.4/scanpy. Complete output from command python setup.py egg_info:. /path/to/miniconda3/envs/bio/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`. from ._conv import register_converters as _register_converters. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""path/to/scanpy/setup.py"", line 11, in <module>. from scanpy import __author__, __email__. File ""path/to/scanpy/scanpy/__init__.py"", line 26, in <module>. check_versions(). File ""path/to/scanpy/scanpy/utils.py"", line 38, in check_versions. .format(__version__, anndata.__version__)). NameError: name '__version__' is not defined. ----------------------------------------. Command ""python setup.py egg_info"" failed with error code 1 in path/to/scanpy/. ```. The variable `__version__` in line 38 in utils.py is not defined.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/482
https://github.com/scverse/scanpy/issues/482:742,modifiability,modul,module,742,"Github install error; Tried to install via `$ pip3 install -e .` but returned this error:. ```. Obtaining file://path/to/scanpy_1.4/scanpy. Complete output from command python setup.py egg_info:. /path/to/miniconda3/envs/bio/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`. from ._conv import register_converters as _register_converters. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""path/to/scanpy/setup.py"", line 11, in <module>. from scanpy import __author__, __email__. File ""path/to/scanpy/scanpy/__init__.py"", line 26, in <module>. check_versions(). File ""path/to/scanpy/scanpy/utils.py"", line 38, in check_versions. .format(__version__, anndata.__version__)). NameError: name '__version__' is not defined. ----------------------------------------. Command ""python setup.py egg_info"" failed with error code 1 in path/to/scanpy/. ```. The variable `__version__` in line 38 in utils.py is not defined.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/482
https://github.com/scverse/scanpy/issues/482:1057,modifiability,variab,variable,1057,"Github install error; Tried to install via `$ pip3 install -e .` but returned this error:. ```. Obtaining file://path/to/scanpy_1.4/scanpy. Complete output from command python setup.py egg_info:. /path/to/miniconda3/envs/bio/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`. from ._conv import register_converters as _register_converters. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""path/to/scanpy/setup.py"", line 11, in <module>. from scanpy import __author__, __email__. File ""path/to/scanpy/scanpy/__init__.py"", line 26, in <module>. check_versions(). File ""path/to/scanpy/scanpy/utils.py"", line 38, in check_versions. .format(__version__, anndata.__version__)). NameError: name '__version__' is not defined. ----------------------------------------. Command ""python setup.py egg_info"" failed with error code 1 in path/to/scanpy/. ```. The variable `__version__` in line 38 in utils.py is not defined.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/482
https://github.com/scverse/scanpy/issues/482:15,performance,error,error,15,"Github install error; Tried to install via `$ pip3 install -e .` but returned this error:. ```. Obtaining file://path/to/scanpy_1.4/scanpy. Complete output from command python setup.py egg_info:. /path/to/miniconda3/envs/bio/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`. from ._conv import register_converters as _register_converters. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""path/to/scanpy/setup.py"", line 11, in <module>. from scanpy import __author__, __email__. File ""path/to/scanpy/scanpy/__init__.py"", line 26, in <module>. check_versions(). File ""path/to/scanpy/scanpy/utils.py"", line 38, in check_versions. .format(__version__, anndata.__version__)). NameError: name '__version__' is not defined. ----------------------------------------. Command ""python setup.py egg_info"" failed with error code 1 in path/to/scanpy/. ```. The variable `__version__` in line 38 in utils.py is not defined.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/482
https://github.com/scverse/scanpy/issues/482:83,performance,error,error,83,"Github install error; Tried to install via `$ pip3 install -e .` but returned this error:. ```. Obtaining file://path/to/scanpy_1.4/scanpy. Complete output from command python setup.py egg_info:. /path/to/miniconda3/envs/bio/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`. from ._conv import register_converters as _register_converters. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""path/to/scanpy/setup.py"", line 11, in <module>. from scanpy import __author__, __email__. File ""path/to/scanpy/scanpy/__init__.py"", line 26, in <module>. check_versions(). File ""path/to/scanpy/scanpy/utils.py"", line 38, in check_versions. .format(__version__, anndata.__version__)). NameError: name '__version__' is not defined. ----------------------------------------. Command ""python setup.py egg_info"" failed with error code 1 in path/to/scanpy/. ```. The variable `__version__` in line 38 in utils.py is not defined.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/482
https://github.com/scverse/scanpy/issues/482:1015,performance,error,error,1015,"Github install error; Tried to install via `$ pip3 install -e .` but returned this error:. ```. Obtaining file://path/to/scanpy_1.4/scanpy. Complete output from command python setup.py egg_info:. /path/to/miniconda3/envs/bio/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`. from ._conv import register_converters as _register_converters. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""path/to/scanpy/setup.py"", line 11, in <module>. from scanpy import __author__, __email__. File ""path/to/scanpy/scanpy/__init__.py"", line 26, in <module>. check_versions(). File ""path/to/scanpy/scanpy/utils.py"", line 38, in check_versions. .format(__version__, anndata.__version__)). NameError: name '__version__' is not defined. ----------------------------------------. Command ""python setup.py egg_info"" failed with error code 1 in path/to/scanpy/. ```. The variable `__version__` in line 38 in utils.py is not defined.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/482
https://github.com/scverse/scanpy/issues/482:1003,reliability,fail,failed,1003,"Github install error; Tried to install via `$ pip3 install -e .` but returned this error:. ```. Obtaining file://path/to/scanpy_1.4/scanpy. Complete output from command python setup.py egg_info:. /path/to/miniconda3/envs/bio/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`. from ._conv import register_converters as _register_converters. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""path/to/scanpy/setup.py"", line 11, in <module>. from scanpy import __author__, __email__. File ""path/to/scanpy/scanpy/__init__.py"", line 26, in <module>. check_versions(). File ""path/to/scanpy/scanpy/utils.py"", line 38, in check_versions. .format(__version__, anndata.__version__)). NameError: name '__version__' is not defined. ----------------------------------------. Command ""python setup.py egg_info"" failed with error code 1 in path/to/scanpy/. ```. The variable `__version__` in line 38 in utils.py is not defined.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/482
https://github.com/scverse/scanpy/issues/482:15,safety,error,error,15,"Github install error; Tried to install via `$ pip3 install -e .` but returned this error:. ```. Obtaining file://path/to/scanpy_1.4/scanpy. Complete output from command python setup.py egg_info:. /path/to/miniconda3/envs/bio/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`. from ._conv import register_converters as _register_converters. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""path/to/scanpy/setup.py"", line 11, in <module>. from scanpy import __author__, __email__. File ""path/to/scanpy/scanpy/__init__.py"", line 26, in <module>. check_versions(). File ""path/to/scanpy/scanpy/utils.py"", line 38, in check_versions. .format(__version__, anndata.__version__)). NameError: name '__version__' is not defined. ----------------------------------------. Command ""python setup.py egg_info"" failed with error code 1 in path/to/scanpy/. ```. The variable `__version__` in line 38 in utils.py is not defined.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/482
https://github.com/scverse/scanpy/issues/482:83,safety,error,error,83,"Github install error; Tried to install via `$ pip3 install -e .` but returned this error:. ```. Obtaining file://path/to/scanpy_1.4/scanpy. Complete output from command python setup.py egg_info:. /path/to/miniconda3/envs/bio/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`. from ._conv import register_converters as _register_converters. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""path/to/scanpy/setup.py"", line 11, in <module>. from scanpy import __author__, __email__. File ""path/to/scanpy/scanpy/__init__.py"", line 26, in <module>. check_versions(). File ""path/to/scanpy/scanpy/utils.py"", line 38, in check_versions. .format(__version__, anndata.__version__)). NameError: name '__version__' is not defined. ----------------------------------------. Command ""python setup.py egg_info"" failed with error code 1 in path/to/scanpy/. ```. The variable `__version__` in line 38 in utils.py is not defined.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/482
https://github.com/scverse/scanpy/issues/482:140,safety,Compl,Complete,140,"Github install error; Tried to install via `$ pip3 install -e .` but returned this error:. ```. Obtaining file://path/to/scanpy_1.4/scanpy. Complete output from command python setup.py egg_info:. /path/to/miniconda3/envs/bio/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`. from ._conv import register_converters as _register_converters. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""path/to/scanpy/setup.py"", line 11, in <module>. from scanpy import __author__, __email__. File ""path/to/scanpy/scanpy/__init__.py"", line 26, in <module>. check_versions(). File ""path/to/scanpy/scanpy/utils.py"", line 38, in check_versions. .format(__version__, anndata.__version__)). NameError: name '__version__' is not defined. ----------------------------------------. Command ""python setup.py egg_info"" failed with error code 1 in path/to/scanpy/. ```. The variable `__version__` in line 38 in utils.py is not defined.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/482
https://github.com/scverse/scanpy/issues/482:582,safety,modul,module,582,"Github install error; Tried to install via `$ pip3 install -e .` but returned this error:. ```. Obtaining file://path/to/scanpy_1.4/scanpy. Complete output from command python setup.py egg_info:. /path/to/miniconda3/envs/bio/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`. from ._conv import register_converters as _register_converters. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""path/to/scanpy/setup.py"", line 11, in <module>. from scanpy import __author__, __email__. File ""path/to/scanpy/scanpy/__init__.py"", line 26, in <module>. check_versions(). File ""path/to/scanpy/scanpy/utils.py"", line 38, in check_versions. .format(__version__, anndata.__version__)). NameError: name '__version__' is not defined. ----------------------------------------. Command ""python setup.py egg_info"" failed with error code 1 in path/to/scanpy/. ```. The variable `__version__` in line 38 in utils.py is not defined.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/482
https://github.com/scverse/scanpy/issues/482:636,safety,modul,module,636,"Github install error; Tried to install via `$ pip3 install -e .` but returned this error:. ```. Obtaining file://path/to/scanpy_1.4/scanpy. Complete output from command python setup.py egg_info:. /path/to/miniconda3/envs/bio/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`. from ._conv import register_converters as _register_converters. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""path/to/scanpy/setup.py"", line 11, in <module>. from scanpy import __author__, __email__. File ""path/to/scanpy/scanpy/__init__.py"", line 26, in <module>. check_versions(). File ""path/to/scanpy/scanpy/utils.py"", line 38, in check_versions. .format(__version__, anndata.__version__)). NameError: name '__version__' is not defined. ----------------------------------------. Command ""python setup.py egg_info"" failed with error code 1 in path/to/scanpy/. ```. The variable `__version__` in line 38 in utils.py is not defined.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/482
https://github.com/scverse/scanpy/issues/482:742,safety,modul,module,742,"Github install error; Tried to install via `$ pip3 install -e .` but returned this error:. ```. Obtaining file://path/to/scanpy_1.4/scanpy. Complete output from command python setup.py egg_info:. /path/to/miniconda3/envs/bio/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`. from ._conv import register_converters as _register_converters. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""path/to/scanpy/setup.py"", line 11, in <module>. from scanpy import __author__, __email__. File ""path/to/scanpy/scanpy/__init__.py"", line 26, in <module>. check_versions(). File ""path/to/scanpy/scanpy/utils.py"", line 38, in check_versions. .format(__version__, anndata.__version__)). NameError: name '__version__' is not defined. ----------------------------------------. Command ""python setup.py egg_info"" failed with error code 1 in path/to/scanpy/. ```. The variable `__version__` in line 38 in utils.py is not defined.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/482
https://github.com/scverse/scanpy/issues/482:1015,safety,error,error,1015,"Github install error; Tried to install via `$ pip3 install -e .` but returned this error:. ```. Obtaining file://path/to/scanpy_1.4/scanpy. Complete output from command python setup.py egg_info:. /path/to/miniconda3/envs/bio/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`. from ._conv import register_converters as _register_converters. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""path/to/scanpy/setup.py"", line 11, in <module>. from scanpy import __author__, __email__. File ""path/to/scanpy/scanpy/__init__.py"", line 26, in <module>. check_versions(). File ""path/to/scanpy/scanpy/utils.py"", line 38, in check_versions. .format(__version__, anndata.__version__)). NameError: name '__version__' is not defined. ----------------------------------------. Command ""python setup.py egg_info"" failed with error code 1 in path/to/scanpy/. ```. The variable `__version__` in line 38 in utils.py is not defined.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/482
https://github.com/scverse/scanpy/issues/482:140,security,Compl,Complete,140,"Github install error; Tried to install via `$ pip3 install -e .` but returned this error:. ```. Obtaining file://path/to/scanpy_1.4/scanpy. Complete output from command python setup.py egg_info:. /path/to/miniconda3/envs/bio/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`. from ._conv import register_converters as _register_converters. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""path/to/scanpy/setup.py"", line 11, in <module>. from scanpy import __author__, __email__. File ""path/to/scanpy/scanpy/__init__.py"", line 26, in <module>. check_versions(). File ""path/to/scanpy/scanpy/utils.py"", line 38, in check_versions. .format(__version__, anndata.__version__)). NameError: name '__version__' is not defined. ----------------------------------------. Command ""python setup.py egg_info"" failed with error code 1 in path/to/scanpy/. ```. The variable `__version__` in line 38 in utils.py is not defined.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/482
https://github.com/scverse/scanpy/issues/482:517,testability,Trace,Traceback,517,"Github install error; Tried to install via `$ pip3 install -e .` but returned this error:. ```. Obtaining file://path/to/scanpy_1.4/scanpy. Complete output from command python setup.py egg_info:. /path/to/miniconda3/envs/bio/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`. from ._conv import register_converters as _register_converters. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""path/to/scanpy/setup.py"", line 11, in <module>. from scanpy import __author__, __email__. File ""path/to/scanpy/scanpy/__init__.py"", line 26, in <module>. check_versions(). File ""path/to/scanpy/scanpy/utils.py"", line 38, in check_versions. .format(__version__, anndata.__version__)). NameError: name '__version__' is not defined. ----------------------------------------. Command ""python setup.py egg_info"" failed with error code 1 in path/to/scanpy/. ```. The variable `__version__` in line 38 in utils.py is not defined.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/482
https://github.com/scverse/scanpy/issues/482:15,usability,error,error,15,"Github install error; Tried to install via `$ pip3 install -e .` but returned this error:. ```. Obtaining file://path/to/scanpy_1.4/scanpy. Complete output from command python setup.py egg_info:. /path/to/miniconda3/envs/bio/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`. from ._conv import register_converters as _register_converters. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""path/to/scanpy/setup.py"", line 11, in <module>. from scanpy import __author__, __email__. File ""path/to/scanpy/scanpy/__init__.py"", line 26, in <module>. check_versions(). File ""path/to/scanpy/scanpy/utils.py"", line 38, in check_versions. .format(__version__, anndata.__version__)). NameError: name '__version__' is not defined. ----------------------------------------. Command ""python setup.py egg_info"" failed with error code 1 in path/to/scanpy/. ```. The variable `__version__` in line 38 in utils.py is not defined.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/482
https://github.com/scverse/scanpy/issues/482:83,usability,error,error,83,"Github install error; Tried to install via `$ pip3 install -e .` but returned this error:. ```. Obtaining file://path/to/scanpy_1.4/scanpy. Complete output from command python setup.py egg_info:. /path/to/miniconda3/envs/bio/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`. from ._conv import register_converters as _register_converters. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""path/to/scanpy/setup.py"", line 11, in <module>. from scanpy import __author__, __email__. File ""path/to/scanpy/scanpy/__init__.py"", line 26, in <module>. check_versions(). File ""path/to/scanpy/scanpy/utils.py"", line 38, in check_versions. .format(__version__, anndata.__version__)). NameError: name '__version__' is not defined. ----------------------------------------. Command ""python setup.py egg_info"" failed with error code 1 in path/to/scanpy/. ```. The variable `__version__` in line 38 in utils.py is not defined.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/482
https://github.com/scverse/scanpy/issues/482:161,usability,command,command,161,"Github install error; Tried to install via `$ pip3 install -e .` but returned this error:. ```. Obtaining file://path/to/scanpy_1.4/scanpy. Complete output from command python setup.py egg_info:. /path/to/miniconda3/envs/bio/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`. from ._conv import register_converters as _register_converters. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""path/to/scanpy/setup.py"", line 11, in <module>. from scanpy import __author__, __email__. File ""path/to/scanpy/scanpy/__init__.py"", line 26, in <module>. check_versions(). File ""path/to/scanpy/scanpy/utils.py"", line 38, in check_versions. .format(__version__, anndata.__version__)). NameError: name '__version__' is not defined. ----------------------------------------. Command ""python setup.py egg_info"" failed with error code 1 in path/to/scanpy/. ```. The variable `__version__` in line 38 in utils.py is not defined.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/482
https://github.com/scverse/scanpy/issues/482:968,usability,Command,Command,968,"Github install error; Tried to install via `$ pip3 install -e .` but returned this error:. ```. Obtaining file://path/to/scanpy_1.4/scanpy. Complete output from command python setup.py egg_info:. /path/to/miniconda3/envs/bio/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`. from ._conv import register_converters as _register_converters. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""path/to/scanpy/setup.py"", line 11, in <module>. from scanpy import __author__, __email__. File ""path/to/scanpy/scanpy/__init__.py"", line 26, in <module>. check_versions(). File ""path/to/scanpy/scanpy/utils.py"", line 38, in check_versions. .format(__version__, anndata.__version__)). NameError: name '__version__' is not defined. ----------------------------------------. Command ""python setup.py egg_info"" failed with error code 1 in path/to/scanpy/. ```. The variable `__version__` in line 38 in utils.py is not defined.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/482
https://github.com/scverse/scanpy/issues/482:1015,usability,error,error,1015,"Github install error; Tried to install via `$ pip3 install -e .` but returned this error:. ```. Obtaining file://path/to/scanpy_1.4/scanpy. Complete output from command python setup.py egg_info:. /path/to/miniconda3/envs/bio/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`. from ._conv import register_converters as _register_converters. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""path/to/scanpy/setup.py"", line 11, in <module>. from scanpy import __author__, __email__. File ""path/to/scanpy/scanpy/__init__.py"", line 26, in <module>. check_versions(). File ""path/to/scanpy/scanpy/utils.py"", line 38, in check_versions. .format(__version__, anndata.__version__)). NameError: name '__version__' is not defined. ----------------------------------------. Command ""python setup.py egg_info"" failed with error code 1 in path/to/scanpy/. ```. The variable `__version__` in line 38 in utils.py is not defined.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/482
https://github.com/scverse/scanpy/issues/483:0,availability,Error,Error,0,"Error when plotting PAGA; I noticed that `sc.pl.paga` function has this piece of code. ```py. # compute positions. if pos is None:. adj_tree = None. if layout in {'rt', 'rt_circular', 'eq_tree'}:. adj_tree = adata.uns['paga']['connectivities_tree']. pos = _compute_pos(. adjacency_solid, layout=layout, random_state=random_state, init_pos=init_pos, layout_kwds=layout_kwds, adj_tree=adj_tree, root=root). ```. and layout is, by default, `None`. This may result in passing an empty `adj_tree` to the `_compute_pos()` function. Actually this is exactly what happened to me:. ```pytb. >>> sc.pl.paga(data, color='leiden'). ---------------------------------------------------------------------------. UnboundLocalError Traceback (most recent call last). <ipython-input-138-ed5614508f4e> in <module>(). ----> 1 sc.pl.paga(data, color='leiden')#, layout='rt'). /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/scanpy-1.4+8.g86f189e-py3.6.egg/scanpy/plotting/_tools/paga.py in paga(adata, threshold, color, layout, layout_kwds, init_pos, root, labels, single_component, solid_edges, dashed_edges, transitions, fontsize, fontweight, text_kwds, node_size_scale, node_size_power, edge_width_scale, min_edge_width, max_edge_width, arrowsize, title, left_margin, random_state, pos, normalize_to_color, cmap, cax, colorbar, cb_kwds, frameon, add_pos, export_to_gexf, use_raw, colors, groups, plot, show, save, ax). 443 adj_tree = adata.uns['paga']['connectivities_tree']. 444 pos = _compute_pos(. --> 445 adjacency_solid, layout=layout, random_state=random_state, init_pos=init_pos, layout_kwds=layout_kwds, adj_tree=adj_tree, root=root). 446 . 447 if plot:. UnboundLocalError: local variable 'adj_tree' referenced before assignment. ```. is this intended?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/483
https://github.com/scverse/scanpy/issues/483:787,deployability,modul,module,787,"Error when plotting PAGA; I noticed that `sc.pl.paga` function has this piece of code. ```py. # compute positions. if pos is None:. adj_tree = None. if layout in {'rt', 'rt_circular', 'eq_tree'}:. adj_tree = adata.uns['paga']['connectivities_tree']. pos = _compute_pos(. adjacency_solid, layout=layout, random_state=random_state, init_pos=init_pos, layout_kwds=layout_kwds, adj_tree=adj_tree, root=root). ```. and layout is, by default, `None`. This may result in passing an empty `adj_tree` to the `_compute_pos()` function. Actually this is exactly what happened to me:. ```pytb. >>> sc.pl.paga(data, color='leiden'). ---------------------------------------------------------------------------. UnboundLocalError Traceback (most recent call last). <ipython-input-138-ed5614508f4e> in <module>(). ----> 1 sc.pl.paga(data, color='leiden')#, layout='rt'). /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/scanpy-1.4+8.g86f189e-py3.6.egg/scanpy/plotting/_tools/paga.py in paga(adata, threshold, color, layout, layout_kwds, init_pos, root, labels, single_component, solid_edges, dashed_edges, transitions, fontsize, fontweight, text_kwds, node_size_scale, node_size_power, edge_width_scale, min_edge_width, max_edge_width, arrowsize, title, left_margin, random_state, pos, normalize_to_color, cmap, cax, colorbar, cb_kwds, frameon, add_pos, export_to_gexf, use_raw, colors, groups, plot, show, save, ax). 443 adj_tree = adata.uns['paga']['connectivities_tree']. 444 pos = _compute_pos(. --> 445 adjacency_solid, layout=layout, random_state=random_state, init_pos=init_pos, layout_kwds=layout_kwds, adj_tree=adj_tree, root=root). 446 . 447 if plot:. UnboundLocalError: local variable 'adj_tree' referenced before assignment. ```. is this intended?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/483
https://github.com/scverse/scanpy/issues/483:892,deployability,Version,Versions,892,"Error when plotting PAGA; I noticed that `sc.pl.paga` function has this piece of code. ```py. # compute positions. if pos is None:. adj_tree = None. if layout in {'rt', 'rt_circular', 'eq_tree'}:. adj_tree = adata.uns['paga']['connectivities_tree']. pos = _compute_pos(. adjacency_solid, layout=layout, random_state=random_state, init_pos=init_pos, layout_kwds=layout_kwds, adj_tree=adj_tree, root=root). ```. and layout is, by default, `None`. This may result in passing an empty `adj_tree` to the `_compute_pos()` function. Actually this is exactly what happened to me:. ```pytb. >>> sc.pl.paga(data, color='leiden'). ---------------------------------------------------------------------------. UnboundLocalError Traceback (most recent call last). <ipython-input-138-ed5614508f4e> in <module>(). ----> 1 sc.pl.paga(data, color='leiden')#, layout='rt'). /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/scanpy-1.4+8.g86f189e-py3.6.egg/scanpy/plotting/_tools/paga.py in paga(adata, threshold, color, layout, layout_kwds, init_pos, root, labels, single_component, solid_edges, dashed_edges, transitions, fontsize, fontweight, text_kwds, node_size_scale, node_size_power, edge_width_scale, min_edge_width, max_edge_width, arrowsize, title, left_margin, random_state, pos, normalize_to_color, cmap, cax, colorbar, cb_kwds, frameon, add_pos, export_to_gexf, use_raw, colors, groups, plot, show, save, ax). 443 adj_tree = adata.uns['paga']['connectivities_tree']. 444 pos = _compute_pos(. --> 445 adjacency_solid, layout=layout, random_state=random_state, init_pos=init_pos, layout_kwds=layout_kwds, adj_tree=adj_tree, root=root). 446 . 447 if plot:. UnboundLocalError: local variable 'adj_tree' referenced before assignment. ```. is this intended?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/483
https://github.com/scverse/scanpy/issues/483:892,integrability,Version,Versions,892,"Error when plotting PAGA; I noticed that `sc.pl.paga` function has this piece of code. ```py. # compute positions. if pos is None:. adj_tree = None. if layout in {'rt', 'rt_circular', 'eq_tree'}:. adj_tree = adata.uns['paga']['connectivities_tree']. pos = _compute_pos(. adjacency_solid, layout=layout, random_state=random_state, init_pos=init_pos, layout_kwds=layout_kwds, adj_tree=adj_tree, root=root). ```. and layout is, by default, `None`. This may result in passing an empty `adj_tree` to the `_compute_pos()` function. Actually this is exactly what happened to me:. ```pytb. >>> sc.pl.paga(data, color='leiden'). ---------------------------------------------------------------------------. UnboundLocalError Traceback (most recent call last). <ipython-input-138-ed5614508f4e> in <module>(). ----> 1 sc.pl.paga(data, color='leiden')#, layout='rt'). /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/scanpy-1.4+8.g86f189e-py3.6.egg/scanpy/plotting/_tools/paga.py in paga(adata, threshold, color, layout, layout_kwds, init_pos, root, labels, single_component, solid_edges, dashed_edges, transitions, fontsize, fontweight, text_kwds, node_size_scale, node_size_power, edge_width_scale, min_edge_width, max_edge_width, arrowsize, title, left_margin, random_state, pos, normalize_to_color, cmap, cax, colorbar, cb_kwds, frameon, add_pos, export_to_gexf, use_raw, colors, groups, plot, show, save, ax). 443 adj_tree = adata.uns['paga']['connectivities_tree']. 444 pos = _compute_pos(. --> 445 adjacency_solid, layout=layout, random_state=random_state, init_pos=init_pos, layout_kwds=layout_kwds, adj_tree=adj_tree, root=root). 446 . 447 if plot:. UnboundLocalError: local variable 'adj_tree' referenced before assignment. ```. is this intended?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/483
https://github.com/scverse/scanpy/issues/483:787,modifiability,modul,module,787,"Error when plotting PAGA; I noticed that `sc.pl.paga` function has this piece of code. ```py. # compute positions. if pos is None:. adj_tree = None. if layout in {'rt', 'rt_circular', 'eq_tree'}:. adj_tree = adata.uns['paga']['connectivities_tree']. pos = _compute_pos(. adjacency_solid, layout=layout, random_state=random_state, init_pos=init_pos, layout_kwds=layout_kwds, adj_tree=adj_tree, root=root). ```. and layout is, by default, `None`. This may result in passing an empty `adj_tree` to the `_compute_pos()` function. Actually this is exactly what happened to me:. ```pytb. >>> sc.pl.paga(data, color='leiden'). ---------------------------------------------------------------------------. UnboundLocalError Traceback (most recent call last). <ipython-input-138-ed5614508f4e> in <module>(). ----> 1 sc.pl.paga(data, color='leiden')#, layout='rt'). /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/scanpy-1.4+8.g86f189e-py3.6.egg/scanpy/plotting/_tools/paga.py in paga(adata, threshold, color, layout, layout_kwds, init_pos, root, labels, single_component, solid_edges, dashed_edges, transitions, fontsize, fontweight, text_kwds, node_size_scale, node_size_power, edge_width_scale, min_edge_width, max_edge_width, arrowsize, title, left_margin, random_state, pos, normalize_to_color, cmap, cax, colorbar, cb_kwds, frameon, add_pos, export_to_gexf, use_raw, colors, groups, plot, show, save, ax). 443 adj_tree = adata.uns['paga']['connectivities_tree']. 444 pos = _compute_pos(. --> 445 adjacency_solid, layout=layout, random_state=random_state, init_pos=init_pos, layout_kwds=layout_kwds, adj_tree=adj_tree, root=root). 446 . 447 if plot:. UnboundLocalError: local variable 'adj_tree' referenced before assignment. ```. is this intended?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/483
https://github.com/scverse/scanpy/issues/483:892,modifiability,Version,Versions,892,"Error when plotting PAGA; I noticed that `sc.pl.paga` function has this piece of code. ```py. # compute positions. if pos is None:. adj_tree = None. if layout in {'rt', 'rt_circular', 'eq_tree'}:. adj_tree = adata.uns['paga']['connectivities_tree']. pos = _compute_pos(. adjacency_solid, layout=layout, random_state=random_state, init_pos=init_pos, layout_kwds=layout_kwds, adj_tree=adj_tree, root=root). ```. and layout is, by default, `None`. This may result in passing an empty `adj_tree` to the `_compute_pos()` function. Actually this is exactly what happened to me:. ```pytb. >>> sc.pl.paga(data, color='leiden'). ---------------------------------------------------------------------------. UnboundLocalError Traceback (most recent call last). <ipython-input-138-ed5614508f4e> in <module>(). ----> 1 sc.pl.paga(data, color='leiden')#, layout='rt'). /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/scanpy-1.4+8.g86f189e-py3.6.egg/scanpy/plotting/_tools/paga.py in paga(adata, threshold, color, layout, layout_kwds, init_pos, root, labels, single_component, solid_edges, dashed_edges, transitions, fontsize, fontweight, text_kwds, node_size_scale, node_size_power, edge_width_scale, min_edge_width, max_edge_width, arrowsize, title, left_margin, random_state, pos, normalize_to_color, cmap, cax, colorbar, cb_kwds, frameon, add_pos, export_to_gexf, use_raw, colors, groups, plot, show, save, ax). 443 adj_tree = adata.uns['paga']['connectivities_tree']. 444 pos = _compute_pos(. --> 445 adjacency_solid, layout=layout, random_state=random_state, init_pos=init_pos, layout_kwds=layout_kwds, adj_tree=adj_tree, root=root). 446 . 447 if plot:. UnboundLocalError: local variable 'adj_tree' referenced before assignment. ```. is this intended?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/483
https://github.com/scverse/scanpy/issues/483:924,modifiability,pac,packages,924,"Error when plotting PAGA; I noticed that `sc.pl.paga` function has this piece of code. ```py. # compute positions. if pos is None:. adj_tree = None. if layout in {'rt', 'rt_circular', 'eq_tree'}:. adj_tree = adata.uns['paga']['connectivities_tree']. pos = _compute_pos(. adjacency_solid, layout=layout, random_state=random_state, init_pos=init_pos, layout_kwds=layout_kwds, adj_tree=adj_tree, root=root). ```. and layout is, by default, `None`. This may result in passing an empty `adj_tree` to the `_compute_pos()` function. Actually this is exactly what happened to me:. ```pytb. >>> sc.pl.paga(data, color='leiden'). ---------------------------------------------------------------------------. UnboundLocalError Traceback (most recent call last). <ipython-input-138-ed5614508f4e> in <module>(). ----> 1 sc.pl.paga(data, color='leiden')#, layout='rt'). /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/scanpy-1.4+8.g86f189e-py3.6.egg/scanpy/plotting/_tools/paga.py in paga(adata, threshold, color, layout, layout_kwds, init_pos, root, labels, single_component, solid_edges, dashed_edges, transitions, fontsize, fontweight, text_kwds, node_size_scale, node_size_power, edge_width_scale, min_edge_width, max_edge_width, arrowsize, title, left_margin, random_state, pos, normalize_to_color, cmap, cax, colorbar, cb_kwds, frameon, add_pos, export_to_gexf, use_raw, colors, groups, plot, show, save, ax). 443 adj_tree = adata.uns['paga']['connectivities_tree']. 444 pos = _compute_pos(. --> 445 adjacency_solid, layout=layout, random_state=random_state, init_pos=init_pos, layout_kwds=layout_kwds, adj_tree=adj_tree, root=root). 446 . 447 if plot:. UnboundLocalError: local variable 'adj_tree' referenced before assignment. ```. is this intended?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/483
https://github.com/scverse/scanpy/issues/483:1700,modifiability,variab,variable,1700,"Error when plotting PAGA; I noticed that `sc.pl.paga` function has this piece of code. ```py. # compute positions. if pos is None:. adj_tree = None. if layout in {'rt', 'rt_circular', 'eq_tree'}:. adj_tree = adata.uns['paga']['connectivities_tree']. pos = _compute_pos(. adjacency_solid, layout=layout, random_state=random_state, init_pos=init_pos, layout_kwds=layout_kwds, adj_tree=adj_tree, root=root). ```. and layout is, by default, `None`. This may result in passing an empty `adj_tree` to the `_compute_pos()` function. Actually this is exactly what happened to me:. ```pytb. >>> sc.pl.paga(data, color='leiden'). ---------------------------------------------------------------------------. UnboundLocalError Traceback (most recent call last). <ipython-input-138-ed5614508f4e> in <module>(). ----> 1 sc.pl.paga(data, color='leiden')#, layout='rt'). /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/scanpy-1.4+8.g86f189e-py3.6.egg/scanpy/plotting/_tools/paga.py in paga(adata, threshold, color, layout, layout_kwds, init_pos, root, labels, single_component, solid_edges, dashed_edges, transitions, fontsize, fontweight, text_kwds, node_size_scale, node_size_power, edge_width_scale, min_edge_width, max_edge_width, arrowsize, title, left_margin, random_state, pos, normalize_to_color, cmap, cax, colorbar, cb_kwds, frameon, add_pos, export_to_gexf, use_raw, colors, groups, plot, show, save, ax). 443 adj_tree = adata.uns['paga']['connectivities_tree']. 444 pos = _compute_pos(. --> 445 adjacency_solid, layout=layout, random_state=random_state, init_pos=init_pos, layout_kwds=layout_kwds, adj_tree=adj_tree, root=root). 446 . 447 if plot:. UnboundLocalError: local variable 'adj_tree' referenced before assignment. ```. is this intended?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/483
https://github.com/scverse/scanpy/issues/483:0,performance,Error,Error,0,"Error when plotting PAGA; I noticed that `sc.pl.paga` function has this piece of code. ```py. # compute positions. if pos is None:. adj_tree = None. if layout in {'rt', 'rt_circular', 'eq_tree'}:. adj_tree = adata.uns['paga']['connectivities_tree']. pos = _compute_pos(. adjacency_solid, layout=layout, random_state=random_state, init_pos=init_pos, layout_kwds=layout_kwds, adj_tree=adj_tree, root=root). ```. and layout is, by default, `None`. This may result in passing an empty `adj_tree` to the `_compute_pos()` function. Actually this is exactly what happened to me:. ```pytb. >>> sc.pl.paga(data, color='leiden'). ---------------------------------------------------------------------------. UnboundLocalError Traceback (most recent call last). <ipython-input-138-ed5614508f4e> in <module>(). ----> 1 sc.pl.paga(data, color='leiden')#, layout='rt'). /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/scanpy-1.4+8.g86f189e-py3.6.egg/scanpy/plotting/_tools/paga.py in paga(adata, threshold, color, layout, layout_kwds, init_pos, root, labels, single_component, solid_edges, dashed_edges, transitions, fontsize, fontweight, text_kwds, node_size_scale, node_size_power, edge_width_scale, min_edge_width, max_edge_width, arrowsize, title, left_margin, random_state, pos, normalize_to_color, cmap, cax, colorbar, cb_kwds, frameon, add_pos, export_to_gexf, use_raw, colors, groups, plot, show, save, ax). 443 adj_tree = adata.uns['paga']['connectivities_tree']. 444 pos = _compute_pos(. --> 445 adjacency_solid, layout=layout, random_state=random_state, init_pos=init_pos, layout_kwds=layout_kwds, adj_tree=adj_tree, root=root). 446 . 447 if plot:. UnboundLocalError: local variable 'adj_tree' referenced before assignment. ```. is this intended?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/483
https://github.com/scverse/scanpy/issues/483:0,safety,Error,Error,0,"Error when plotting PAGA; I noticed that `sc.pl.paga` function has this piece of code. ```py. # compute positions. if pos is None:. adj_tree = None. if layout in {'rt', 'rt_circular', 'eq_tree'}:. adj_tree = adata.uns['paga']['connectivities_tree']. pos = _compute_pos(. adjacency_solid, layout=layout, random_state=random_state, init_pos=init_pos, layout_kwds=layout_kwds, adj_tree=adj_tree, root=root). ```. and layout is, by default, `None`. This may result in passing an empty `adj_tree` to the `_compute_pos()` function. Actually this is exactly what happened to me:. ```pytb. >>> sc.pl.paga(data, color='leiden'). ---------------------------------------------------------------------------. UnboundLocalError Traceback (most recent call last). <ipython-input-138-ed5614508f4e> in <module>(). ----> 1 sc.pl.paga(data, color='leiden')#, layout='rt'). /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/scanpy-1.4+8.g86f189e-py3.6.egg/scanpy/plotting/_tools/paga.py in paga(adata, threshold, color, layout, layout_kwds, init_pos, root, labels, single_component, solid_edges, dashed_edges, transitions, fontsize, fontweight, text_kwds, node_size_scale, node_size_power, edge_width_scale, min_edge_width, max_edge_width, arrowsize, title, left_margin, random_state, pos, normalize_to_color, cmap, cax, colorbar, cb_kwds, frameon, add_pos, export_to_gexf, use_raw, colors, groups, plot, show, save, ax). 443 adj_tree = adata.uns['paga']['connectivities_tree']. 444 pos = _compute_pos(. --> 445 adjacency_solid, layout=layout, random_state=random_state, init_pos=init_pos, layout_kwds=layout_kwds, adj_tree=adj_tree, root=root). 446 . 447 if plot:. UnboundLocalError: local variable 'adj_tree' referenced before assignment. ```. is this intended?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/483
https://github.com/scverse/scanpy/issues/483:759,safety,input,input-,759,"Error when plotting PAGA; I noticed that `sc.pl.paga` function has this piece of code. ```py. # compute positions. if pos is None:. adj_tree = None. if layout in {'rt', 'rt_circular', 'eq_tree'}:. adj_tree = adata.uns['paga']['connectivities_tree']. pos = _compute_pos(. adjacency_solid, layout=layout, random_state=random_state, init_pos=init_pos, layout_kwds=layout_kwds, adj_tree=adj_tree, root=root). ```. and layout is, by default, `None`. This may result in passing an empty `adj_tree` to the `_compute_pos()` function. Actually this is exactly what happened to me:. ```pytb. >>> sc.pl.paga(data, color='leiden'). ---------------------------------------------------------------------------. UnboundLocalError Traceback (most recent call last). <ipython-input-138-ed5614508f4e> in <module>(). ----> 1 sc.pl.paga(data, color='leiden')#, layout='rt'). /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/scanpy-1.4+8.g86f189e-py3.6.egg/scanpy/plotting/_tools/paga.py in paga(adata, threshold, color, layout, layout_kwds, init_pos, root, labels, single_component, solid_edges, dashed_edges, transitions, fontsize, fontweight, text_kwds, node_size_scale, node_size_power, edge_width_scale, min_edge_width, max_edge_width, arrowsize, title, left_margin, random_state, pos, normalize_to_color, cmap, cax, colorbar, cb_kwds, frameon, add_pos, export_to_gexf, use_raw, colors, groups, plot, show, save, ax). 443 adj_tree = adata.uns['paga']['connectivities_tree']. 444 pos = _compute_pos(. --> 445 adjacency_solid, layout=layout, random_state=random_state, init_pos=init_pos, layout_kwds=layout_kwds, adj_tree=adj_tree, root=root). 446 . 447 if plot:. UnboundLocalError: local variable 'adj_tree' referenced before assignment. ```. is this intended?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/483
https://github.com/scverse/scanpy/issues/483:787,safety,modul,module,787,"Error when plotting PAGA; I noticed that `sc.pl.paga` function has this piece of code. ```py. # compute positions. if pos is None:. adj_tree = None. if layout in {'rt', 'rt_circular', 'eq_tree'}:. adj_tree = adata.uns['paga']['connectivities_tree']. pos = _compute_pos(. adjacency_solid, layout=layout, random_state=random_state, init_pos=init_pos, layout_kwds=layout_kwds, adj_tree=adj_tree, root=root). ```. and layout is, by default, `None`. This may result in passing an empty `adj_tree` to the `_compute_pos()` function. Actually this is exactly what happened to me:. ```pytb. >>> sc.pl.paga(data, color='leiden'). ---------------------------------------------------------------------------. UnboundLocalError Traceback (most recent call last). <ipython-input-138-ed5614508f4e> in <module>(). ----> 1 sc.pl.paga(data, color='leiden')#, layout='rt'). /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/scanpy-1.4+8.g86f189e-py3.6.egg/scanpy/plotting/_tools/paga.py in paga(adata, threshold, color, layout, layout_kwds, init_pos, root, labels, single_component, solid_edges, dashed_edges, transitions, fontsize, fontweight, text_kwds, node_size_scale, node_size_power, edge_width_scale, min_edge_width, max_edge_width, arrowsize, title, left_margin, random_state, pos, normalize_to_color, cmap, cax, colorbar, cb_kwds, frameon, add_pos, export_to_gexf, use_raw, colors, groups, plot, show, save, ax). 443 adj_tree = adata.uns['paga']['connectivities_tree']. 444 pos = _compute_pos(. --> 445 adjacency_solid, layout=layout, random_state=random_state, init_pos=init_pos, layout_kwds=layout_kwds, adj_tree=adj_tree, root=root). 446 . 447 if plot:. UnboundLocalError: local variable 'adj_tree' referenced before assignment. ```. is this intended?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/483
https://github.com/scverse/scanpy/issues/483:715,testability,Trace,Traceback,715,"Error when plotting PAGA; I noticed that `sc.pl.paga` function has this piece of code. ```py. # compute positions. if pos is None:. adj_tree = None. if layout in {'rt', 'rt_circular', 'eq_tree'}:. adj_tree = adata.uns['paga']['connectivities_tree']. pos = _compute_pos(. adjacency_solid, layout=layout, random_state=random_state, init_pos=init_pos, layout_kwds=layout_kwds, adj_tree=adj_tree, root=root). ```. and layout is, by default, `None`. This may result in passing an empty `adj_tree` to the `_compute_pos()` function. Actually this is exactly what happened to me:. ```pytb. >>> sc.pl.paga(data, color='leiden'). ---------------------------------------------------------------------------. UnboundLocalError Traceback (most recent call last). <ipython-input-138-ed5614508f4e> in <module>(). ----> 1 sc.pl.paga(data, color='leiden')#, layout='rt'). /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/scanpy-1.4+8.g86f189e-py3.6.egg/scanpy/plotting/_tools/paga.py in paga(adata, threshold, color, layout, layout_kwds, init_pos, root, labels, single_component, solid_edges, dashed_edges, transitions, fontsize, fontweight, text_kwds, node_size_scale, node_size_power, edge_width_scale, min_edge_width, max_edge_width, arrowsize, title, left_margin, random_state, pos, normalize_to_color, cmap, cax, colorbar, cb_kwds, frameon, add_pos, export_to_gexf, use_raw, colors, groups, plot, show, save, ax). 443 adj_tree = adata.uns['paga']['connectivities_tree']. 444 pos = _compute_pos(. --> 445 adjacency_solid, layout=layout, random_state=random_state, init_pos=init_pos, layout_kwds=layout_kwds, adj_tree=adj_tree, root=root). 446 . 447 if plot:. UnboundLocalError: local variable 'adj_tree' referenced before assignment. ```. is this intended?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/483
https://github.com/scverse/scanpy/issues/483:0,usability,Error,Error,0,"Error when plotting PAGA; I noticed that `sc.pl.paga` function has this piece of code. ```py. # compute positions. if pos is None:. adj_tree = None. if layout in {'rt', 'rt_circular', 'eq_tree'}:. adj_tree = adata.uns['paga']['connectivities_tree']. pos = _compute_pos(. adjacency_solid, layout=layout, random_state=random_state, init_pos=init_pos, layout_kwds=layout_kwds, adj_tree=adj_tree, root=root). ```. and layout is, by default, `None`. This may result in passing an empty `adj_tree` to the `_compute_pos()` function. Actually this is exactly what happened to me:. ```pytb. >>> sc.pl.paga(data, color='leiden'). ---------------------------------------------------------------------------. UnboundLocalError Traceback (most recent call last). <ipython-input-138-ed5614508f4e> in <module>(). ----> 1 sc.pl.paga(data, color='leiden')#, layout='rt'). /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/scanpy-1.4+8.g86f189e-py3.6.egg/scanpy/plotting/_tools/paga.py in paga(adata, threshold, color, layout, layout_kwds, init_pos, root, labels, single_component, solid_edges, dashed_edges, transitions, fontsize, fontweight, text_kwds, node_size_scale, node_size_power, edge_width_scale, min_edge_width, max_edge_width, arrowsize, title, left_margin, random_state, pos, normalize_to_color, cmap, cax, colorbar, cb_kwds, frameon, add_pos, export_to_gexf, use_raw, colors, groups, plot, show, save, ax). 443 adj_tree = adata.uns['paga']['connectivities_tree']. 444 pos = _compute_pos(. --> 445 adjacency_solid, layout=layout, random_state=random_state, init_pos=init_pos, layout_kwds=layout_kwds, adj_tree=adj_tree, root=root). 446 . 447 if plot:. UnboundLocalError: local variable 'adj_tree' referenced before assignment. ```. is this intended?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/483
https://github.com/scverse/scanpy/issues/483:759,usability,input,input-,759,"Error when plotting PAGA; I noticed that `sc.pl.paga` function has this piece of code. ```py. # compute positions. if pos is None:. adj_tree = None. if layout in {'rt', 'rt_circular', 'eq_tree'}:. adj_tree = adata.uns['paga']['connectivities_tree']. pos = _compute_pos(. adjacency_solid, layout=layout, random_state=random_state, init_pos=init_pos, layout_kwds=layout_kwds, adj_tree=adj_tree, root=root). ```. and layout is, by default, `None`. This may result in passing an empty `adj_tree` to the `_compute_pos()` function. Actually this is exactly what happened to me:. ```pytb. >>> sc.pl.paga(data, color='leiden'). ---------------------------------------------------------------------------. UnboundLocalError Traceback (most recent call last). <ipython-input-138-ed5614508f4e> in <module>(). ----> 1 sc.pl.paga(data, color='leiden')#, layout='rt'). /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/scanpy-1.4+8.g86f189e-py3.6.egg/scanpy/plotting/_tools/paga.py in paga(adata, threshold, color, layout, layout_kwds, init_pos, root, labels, single_component, solid_edges, dashed_edges, transitions, fontsize, fontweight, text_kwds, node_size_scale, node_size_power, edge_width_scale, min_edge_width, max_edge_width, arrowsize, title, left_margin, random_state, pos, normalize_to_color, cmap, cax, colorbar, cb_kwds, frameon, add_pos, export_to_gexf, use_raw, colors, groups, plot, show, save, ax). 443 adj_tree = adata.uns['paga']['connectivities_tree']. 444 pos = _compute_pos(. --> 445 adjacency_solid, layout=layout, random_state=random_state, init_pos=init_pos, layout_kwds=layout_kwds, adj_tree=adj_tree, root=root). 446 . 447 if plot:. UnboundLocalError: local variable 'adj_tree' referenced before assignment. ```. is this intended?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/483
https://github.com/scverse/scanpy/pull/485:0,deployability,Instal,Install,0,Install packages necessary to run distributed tests.; This should allow Travis to run `test_preprocessing_distributed.py`.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/485
https://github.com/scverse/scanpy/pull/485:34,interoperability,distribut,distributed,34,Install packages necessary to run distributed tests.; This should allow Travis to run `test_preprocessing_distributed.py`.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/485
https://github.com/scverse/scanpy/pull/485:8,modifiability,pac,packages,8,Install packages necessary to run distributed tests.; This should allow Travis to run `test_preprocessing_distributed.py`.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/485
https://github.com/scverse/scanpy/pull/485:46,safety,test,tests,46,Install packages necessary to run distributed tests.; This should allow Travis to run `test_preprocessing_distributed.py`.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/485
https://github.com/scverse/scanpy/pull/485:46,testability,test,tests,46,Install packages necessary to run distributed tests.; This should allow Travis to run `test_preprocessing_distributed.py`.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/485
https://github.com/scverse/scanpy/pull/486:321,deployability,fail,fail,321,"Fix default verbosity level from the erroneous 'warning' to 'warn'.; This is a result of a change in fc840961c4a9f49cfcea975d01f79e5345fc521e. The problem is not exposed in tests since `scanpy/tests/conftest.py` overrides the default verbosity to `hint`. However, if the line in that file is commented out then the tests fail. I'm not sure of a simple way to add a regression test, but this change fixes the problem as verified by a manual test.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/486
https://github.com/scverse/scanpy/pull/486:321,reliability,fail,fail,321,"Fix default verbosity level from the erroneous 'warning' to 'warn'.; This is a result of a change in fc840961c4a9f49cfcea975d01f79e5345fc521e. The problem is not exposed in tests since `scanpy/tests/conftest.py` overrides the default verbosity to `hint`. However, if the line in that file is commented out then the tests fail. I'm not sure of a simple way to add a regression test, but this change fixes the problem as verified by a manual test.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/486
https://github.com/scverse/scanpy/pull/486:173,safety,test,tests,173,"Fix default verbosity level from the erroneous 'warning' to 'warn'.; This is a result of a change in fc840961c4a9f49cfcea975d01f79e5345fc521e. The problem is not exposed in tests since `scanpy/tests/conftest.py` overrides the default verbosity to `hint`. However, if the line in that file is commented out then the tests fail. I'm not sure of a simple way to add a regression test, but this change fixes the problem as verified by a manual test.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/486
https://github.com/scverse/scanpy/pull/486:193,safety,test,tests,193,"Fix default verbosity level from the erroneous 'warning' to 'warn'.; This is a result of a change in fc840961c4a9f49cfcea975d01f79e5345fc521e. The problem is not exposed in tests since `scanpy/tests/conftest.py` overrides the default verbosity to `hint`. However, if the line in that file is commented out then the tests fail. I'm not sure of a simple way to add a regression test, but this change fixes the problem as verified by a manual test.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/486
https://github.com/scverse/scanpy/pull/486:315,safety,test,tests,315,"Fix default verbosity level from the erroneous 'warning' to 'warn'.; This is a result of a change in fc840961c4a9f49cfcea975d01f79e5345fc521e. The problem is not exposed in tests since `scanpy/tests/conftest.py` overrides the default verbosity to `hint`. However, if the line in that file is commented out then the tests fail. I'm not sure of a simple way to add a regression test, but this change fixes the problem as verified by a manual test.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/486
https://github.com/scverse/scanpy/pull/486:376,safety,test,test,376,"Fix default verbosity level from the erroneous 'warning' to 'warn'.; This is a result of a change in fc840961c4a9f49cfcea975d01f79e5345fc521e. The problem is not exposed in tests since `scanpy/tests/conftest.py` overrides the default verbosity to `hint`. However, if the line in that file is commented out then the tests fail. I'm not sure of a simple way to add a regression test, but this change fixes the problem as verified by a manual test.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/486
https://github.com/scverse/scanpy/pull/486:440,safety,test,test,440,"Fix default verbosity level from the erroneous 'warning' to 'warn'.; This is a result of a change in fc840961c4a9f49cfcea975d01f79e5345fc521e. The problem is not exposed in tests since `scanpy/tests/conftest.py` overrides the default verbosity to `hint`. However, if the line in that file is commented out then the tests fail. I'm not sure of a simple way to add a regression test, but this change fixes the problem as verified by a manual test.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/486
https://github.com/scverse/scanpy/pull/486:162,security,expos,exposed,162,"Fix default verbosity level from the erroneous 'warning' to 'warn'.; This is a result of a change in fc840961c4a9f49cfcea975d01f79e5345fc521e. The problem is not exposed in tests since `scanpy/tests/conftest.py` overrides the default verbosity to `hint`. However, if the line in that file is commented out then the tests fail. I'm not sure of a simple way to add a regression test, but this change fixes the problem as verified by a manual test.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/486
https://github.com/scverse/scanpy/pull/486:173,testability,test,tests,173,"Fix default verbosity level from the erroneous 'warning' to 'warn'.; This is a result of a change in fc840961c4a9f49cfcea975d01f79e5345fc521e. The problem is not exposed in tests since `scanpy/tests/conftest.py` overrides the default verbosity to `hint`. However, if the line in that file is commented out then the tests fail. I'm not sure of a simple way to add a regression test, but this change fixes the problem as verified by a manual test.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/486
https://github.com/scverse/scanpy/pull/486:193,testability,test,tests,193,"Fix default verbosity level from the erroneous 'warning' to 'warn'.; This is a result of a change in fc840961c4a9f49cfcea975d01f79e5345fc521e. The problem is not exposed in tests since `scanpy/tests/conftest.py` overrides the default verbosity to `hint`. However, if the line in that file is commented out then the tests fail. I'm not sure of a simple way to add a regression test, but this change fixes the problem as verified by a manual test.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/486
https://github.com/scverse/scanpy/pull/486:315,testability,test,tests,315,"Fix default verbosity level from the erroneous 'warning' to 'warn'.; This is a result of a change in fc840961c4a9f49cfcea975d01f79e5345fc521e. The problem is not exposed in tests since `scanpy/tests/conftest.py` overrides the default verbosity to `hint`. However, if the line in that file is commented out then the tests fail. I'm not sure of a simple way to add a regression test, but this change fixes the problem as verified by a manual test.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/486
https://github.com/scverse/scanpy/pull/486:345,testability,simpl,simple,345,"Fix default verbosity level from the erroneous 'warning' to 'warn'.; This is a result of a change in fc840961c4a9f49cfcea975d01f79e5345fc521e. The problem is not exposed in tests since `scanpy/tests/conftest.py` overrides the default verbosity to `hint`. However, if the line in that file is commented out then the tests fail. I'm not sure of a simple way to add a regression test, but this change fixes the problem as verified by a manual test.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/486
https://github.com/scverse/scanpy/pull/486:365,testability,regress,regression,365,"Fix default verbosity level from the erroneous 'warning' to 'warn'.; This is a result of a change in fc840961c4a9f49cfcea975d01f79e5345fc521e. The problem is not exposed in tests since `scanpy/tests/conftest.py` overrides the default verbosity to `hint`. However, if the line in that file is commented out then the tests fail. I'm not sure of a simple way to add a regression test, but this change fixes the problem as verified by a manual test.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/486
https://github.com/scverse/scanpy/pull/486:376,testability,test,test,376,"Fix default verbosity level from the erroneous 'warning' to 'warn'.; This is a result of a change in fc840961c4a9f49cfcea975d01f79e5345fc521e. The problem is not exposed in tests since `scanpy/tests/conftest.py` overrides the default verbosity to `hint`. However, if the line in that file is commented out then the tests fail. I'm not sure of a simple way to add a regression test, but this change fixes the problem as verified by a manual test.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/486
https://github.com/scverse/scanpy/pull/486:419,testability,verif,verified,419,"Fix default verbosity level from the erroneous 'warning' to 'warn'.; This is a result of a change in fc840961c4a9f49cfcea975d01f79e5345fc521e. The problem is not exposed in tests since `scanpy/tests/conftest.py` overrides the default verbosity to `hint`. However, if the line in that file is commented out then the tests fail. I'm not sure of a simple way to add a regression test, but this change fixes the problem as verified by a manual test.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/486
https://github.com/scverse/scanpy/pull/486:440,testability,test,test,440,"Fix default verbosity level from the erroneous 'warning' to 'warn'.; This is a result of a change in fc840961c4a9f49cfcea975d01f79e5345fc521e. The problem is not exposed in tests since `scanpy/tests/conftest.py` overrides the default verbosity to `hint`. However, if the line in that file is commented out then the tests fail. I'm not sure of a simple way to add a regression test, but this change fixes the problem as verified by a manual test.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/486
https://github.com/scverse/scanpy/pull/486:248,usability,hint,hint,248,"Fix default verbosity level from the erroneous 'warning' to 'warn'.; This is a result of a change in fc840961c4a9f49cfcea975d01f79e5345fc521e. The problem is not exposed in tests since `scanpy/tests/conftest.py` overrides the default verbosity to `hint`. However, if the line in that file is commented out then the tests fail. I'm not sure of a simple way to add a regression test, but this change fixes the problem as verified by a manual test.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/486
https://github.com/scverse/scanpy/pull/486:345,usability,simpl,simple,345,"Fix default verbosity level from the erroneous 'warning' to 'warn'.; This is a result of a change in fc840961c4a9f49cfcea975d01f79e5345fc521e. The problem is not exposed in tests since `scanpy/tests/conftest.py` overrides the default verbosity to `hint`. However, if the line in that file is commented out then the tests fail. I'm not sure of a simple way to add a regression test, but this change fixes the problem as verified by a manual test.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/486
https://github.com/scverse/scanpy/issues/487:14,availability,error,error,14,"PAGA plotting error; . Running `sc.pl.paga(adata)` in v1.4 returns an error:. ```. Traceback (most recent call last):. File ""<ipython-input-412-3baa85828ec9>"", line 1, in <module>. sc.pl.paga(adata). File ""/path/to/scanpy/scanpy/plotting/_tools/paga.py"", line 445, in paga. adjacency_solid, layout=layout, random_state=random_state, init_pos=init_pos, layout_kwds=layout_kwds, adj_tree=adj_tree, root=root). UnboundLocalError: local variable 'adj_tree' referenced before assignment. ```. There is a conditional before the referenced line, which assigns value to `adj_tree`, and indeed, running these works fine:. ```. sc.pl.paga(adata, layout='rt'). sc.pl.paga(adata, layout='rt_circular'). sc.pl.paga(adata, layout='eq_tree'). ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/487
https://github.com/scverse/scanpy/issues/487:70,availability,error,error,70,"PAGA plotting error; . Running `sc.pl.paga(adata)` in v1.4 returns an error:. ```. Traceback (most recent call last):. File ""<ipython-input-412-3baa85828ec9>"", line 1, in <module>. sc.pl.paga(adata). File ""/path/to/scanpy/scanpy/plotting/_tools/paga.py"", line 445, in paga. adjacency_solid, layout=layout, random_state=random_state, init_pos=init_pos, layout_kwds=layout_kwds, adj_tree=adj_tree, root=root). UnboundLocalError: local variable 'adj_tree' referenced before assignment. ```. There is a conditional before the referenced line, which assigns value to `adj_tree`, and indeed, running these works fine:. ```. sc.pl.paga(adata, layout='rt'). sc.pl.paga(adata, layout='rt_circular'). sc.pl.paga(adata, layout='eq_tree'). ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/487
https://github.com/scverse/scanpy/issues/487:172,deployability,modul,module,172,"PAGA plotting error; . Running `sc.pl.paga(adata)` in v1.4 returns an error:. ```. Traceback (most recent call last):. File ""<ipython-input-412-3baa85828ec9>"", line 1, in <module>. sc.pl.paga(adata). File ""/path/to/scanpy/scanpy/plotting/_tools/paga.py"", line 445, in paga. adjacency_solid, layout=layout, random_state=random_state, init_pos=init_pos, layout_kwds=layout_kwds, adj_tree=adj_tree, root=root). UnboundLocalError: local variable 'adj_tree' referenced before assignment. ```. There is a conditional before the referenced line, which assigns value to `adj_tree`, and indeed, running these works fine:. ```. sc.pl.paga(adata, layout='rt'). sc.pl.paga(adata, layout='rt_circular'). sc.pl.paga(adata, layout='eq_tree'). ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/487
https://github.com/scverse/scanpy/issues/487:172,modifiability,modul,module,172,"PAGA plotting error; . Running `sc.pl.paga(adata)` in v1.4 returns an error:. ```. Traceback (most recent call last):. File ""<ipython-input-412-3baa85828ec9>"", line 1, in <module>. sc.pl.paga(adata). File ""/path/to/scanpy/scanpy/plotting/_tools/paga.py"", line 445, in paga. adjacency_solid, layout=layout, random_state=random_state, init_pos=init_pos, layout_kwds=layout_kwds, adj_tree=adj_tree, root=root). UnboundLocalError: local variable 'adj_tree' referenced before assignment. ```. There is a conditional before the referenced line, which assigns value to `adj_tree`, and indeed, running these works fine:. ```. sc.pl.paga(adata, layout='rt'). sc.pl.paga(adata, layout='rt_circular'). sc.pl.paga(adata, layout='eq_tree'). ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/487
https://github.com/scverse/scanpy/issues/487:433,modifiability,variab,variable,433,"PAGA plotting error; . Running `sc.pl.paga(adata)` in v1.4 returns an error:. ```. Traceback (most recent call last):. File ""<ipython-input-412-3baa85828ec9>"", line 1, in <module>. sc.pl.paga(adata). File ""/path/to/scanpy/scanpy/plotting/_tools/paga.py"", line 445, in paga. adjacency_solid, layout=layout, random_state=random_state, init_pos=init_pos, layout_kwds=layout_kwds, adj_tree=adj_tree, root=root). UnboundLocalError: local variable 'adj_tree' referenced before assignment. ```. There is a conditional before the referenced line, which assigns value to `adj_tree`, and indeed, running these works fine:. ```. sc.pl.paga(adata, layout='rt'). sc.pl.paga(adata, layout='rt_circular'). sc.pl.paga(adata, layout='eq_tree'). ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/487
https://github.com/scverse/scanpy/issues/487:14,performance,error,error,14,"PAGA plotting error; . Running `sc.pl.paga(adata)` in v1.4 returns an error:. ```. Traceback (most recent call last):. File ""<ipython-input-412-3baa85828ec9>"", line 1, in <module>. sc.pl.paga(adata). File ""/path/to/scanpy/scanpy/plotting/_tools/paga.py"", line 445, in paga. adjacency_solid, layout=layout, random_state=random_state, init_pos=init_pos, layout_kwds=layout_kwds, adj_tree=adj_tree, root=root). UnboundLocalError: local variable 'adj_tree' referenced before assignment. ```. There is a conditional before the referenced line, which assigns value to `adj_tree`, and indeed, running these works fine:. ```. sc.pl.paga(adata, layout='rt'). sc.pl.paga(adata, layout='rt_circular'). sc.pl.paga(adata, layout='eq_tree'). ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/487
https://github.com/scverse/scanpy/issues/487:70,performance,error,error,70,"PAGA plotting error; . Running `sc.pl.paga(adata)` in v1.4 returns an error:. ```. Traceback (most recent call last):. File ""<ipython-input-412-3baa85828ec9>"", line 1, in <module>. sc.pl.paga(adata). File ""/path/to/scanpy/scanpy/plotting/_tools/paga.py"", line 445, in paga. adjacency_solid, layout=layout, random_state=random_state, init_pos=init_pos, layout_kwds=layout_kwds, adj_tree=adj_tree, root=root). UnboundLocalError: local variable 'adj_tree' referenced before assignment. ```. There is a conditional before the referenced line, which assigns value to `adj_tree`, and indeed, running these works fine:. ```. sc.pl.paga(adata, layout='rt'). sc.pl.paga(adata, layout='rt_circular'). sc.pl.paga(adata, layout='eq_tree'). ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/487
https://github.com/scverse/scanpy/issues/487:14,safety,error,error,14,"PAGA plotting error; . Running `sc.pl.paga(adata)` in v1.4 returns an error:. ```. Traceback (most recent call last):. File ""<ipython-input-412-3baa85828ec9>"", line 1, in <module>. sc.pl.paga(adata). File ""/path/to/scanpy/scanpy/plotting/_tools/paga.py"", line 445, in paga. adjacency_solid, layout=layout, random_state=random_state, init_pos=init_pos, layout_kwds=layout_kwds, adj_tree=adj_tree, root=root). UnboundLocalError: local variable 'adj_tree' referenced before assignment. ```. There is a conditional before the referenced line, which assigns value to `adj_tree`, and indeed, running these works fine:. ```. sc.pl.paga(adata, layout='rt'). sc.pl.paga(adata, layout='rt_circular'). sc.pl.paga(adata, layout='eq_tree'). ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/487
https://github.com/scverse/scanpy/issues/487:70,safety,error,error,70,"PAGA plotting error; . Running `sc.pl.paga(adata)` in v1.4 returns an error:. ```. Traceback (most recent call last):. File ""<ipython-input-412-3baa85828ec9>"", line 1, in <module>. sc.pl.paga(adata). File ""/path/to/scanpy/scanpy/plotting/_tools/paga.py"", line 445, in paga. adjacency_solid, layout=layout, random_state=random_state, init_pos=init_pos, layout_kwds=layout_kwds, adj_tree=adj_tree, root=root). UnboundLocalError: local variable 'adj_tree' referenced before assignment. ```. There is a conditional before the referenced line, which assigns value to `adj_tree`, and indeed, running these works fine:. ```. sc.pl.paga(adata, layout='rt'). sc.pl.paga(adata, layout='rt_circular'). sc.pl.paga(adata, layout='eq_tree'). ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/487
https://github.com/scverse/scanpy/issues/487:134,safety,input,input-,134,"PAGA plotting error; . Running `sc.pl.paga(adata)` in v1.4 returns an error:. ```. Traceback (most recent call last):. File ""<ipython-input-412-3baa85828ec9>"", line 1, in <module>. sc.pl.paga(adata). File ""/path/to/scanpy/scanpy/plotting/_tools/paga.py"", line 445, in paga. adjacency_solid, layout=layout, random_state=random_state, init_pos=init_pos, layout_kwds=layout_kwds, adj_tree=adj_tree, root=root). UnboundLocalError: local variable 'adj_tree' referenced before assignment. ```. There is a conditional before the referenced line, which assigns value to `adj_tree`, and indeed, running these works fine:. ```. sc.pl.paga(adata, layout='rt'). sc.pl.paga(adata, layout='rt_circular'). sc.pl.paga(adata, layout='eq_tree'). ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/487
https://github.com/scverse/scanpy/issues/487:172,safety,modul,module,172,"PAGA plotting error; . Running `sc.pl.paga(adata)` in v1.4 returns an error:. ```. Traceback (most recent call last):. File ""<ipython-input-412-3baa85828ec9>"", line 1, in <module>. sc.pl.paga(adata). File ""/path/to/scanpy/scanpy/plotting/_tools/paga.py"", line 445, in paga. adjacency_solid, layout=layout, random_state=random_state, init_pos=init_pos, layout_kwds=layout_kwds, adj_tree=adj_tree, root=root). UnboundLocalError: local variable 'adj_tree' referenced before assignment. ```. There is a conditional before the referenced line, which assigns value to `adj_tree`, and indeed, running these works fine:. ```. sc.pl.paga(adata, layout='rt'). sc.pl.paga(adata, layout='rt_circular'). sc.pl.paga(adata, layout='eq_tree'). ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/487
https://github.com/scverse/scanpy/issues/487:83,testability,Trace,Traceback,83,"PAGA plotting error; . Running `sc.pl.paga(adata)` in v1.4 returns an error:. ```. Traceback (most recent call last):. File ""<ipython-input-412-3baa85828ec9>"", line 1, in <module>. sc.pl.paga(adata). File ""/path/to/scanpy/scanpy/plotting/_tools/paga.py"", line 445, in paga. adjacency_solid, layout=layout, random_state=random_state, init_pos=init_pos, layout_kwds=layout_kwds, adj_tree=adj_tree, root=root). UnboundLocalError: local variable 'adj_tree' referenced before assignment. ```. There is a conditional before the referenced line, which assigns value to `adj_tree`, and indeed, running these works fine:. ```. sc.pl.paga(adata, layout='rt'). sc.pl.paga(adata, layout='rt_circular'). sc.pl.paga(adata, layout='eq_tree'). ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/487
https://github.com/scverse/scanpy/issues/487:14,usability,error,error,14,"PAGA plotting error; . Running `sc.pl.paga(adata)` in v1.4 returns an error:. ```. Traceback (most recent call last):. File ""<ipython-input-412-3baa85828ec9>"", line 1, in <module>. sc.pl.paga(adata). File ""/path/to/scanpy/scanpy/plotting/_tools/paga.py"", line 445, in paga. adjacency_solid, layout=layout, random_state=random_state, init_pos=init_pos, layout_kwds=layout_kwds, adj_tree=adj_tree, root=root). UnboundLocalError: local variable 'adj_tree' referenced before assignment. ```. There is a conditional before the referenced line, which assigns value to `adj_tree`, and indeed, running these works fine:. ```. sc.pl.paga(adata, layout='rt'). sc.pl.paga(adata, layout='rt_circular'). sc.pl.paga(adata, layout='eq_tree'). ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/487
https://github.com/scverse/scanpy/issues/487:70,usability,error,error,70,"PAGA plotting error; . Running `sc.pl.paga(adata)` in v1.4 returns an error:. ```. Traceback (most recent call last):. File ""<ipython-input-412-3baa85828ec9>"", line 1, in <module>. sc.pl.paga(adata). File ""/path/to/scanpy/scanpy/plotting/_tools/paga.py"", line 445, in paga. adjacency_solid, layout=layout, random_state=random_state, init_pos=init_pos, layout_kwds=layout_kwds, adj_tree=adj_tree, root=root). UnboundLocalError: local variable 'adj_tree' referenced before assignment. ```. There is a conditional before the referenced line, which assigns value to `adj_tree`, and indeed, running these works fine:. ```. sc.pl.paga(adata, layout='rt'). sc.pl.paga(adata, layout='rt_circular'). sc.pl.paga(adata, layout='eq_tree'). ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/487
https://github.com/scverse/scanpy/issues/487:134,usability,input,input-,134,"PAGA plotting error; . Running `sc.pl.paga(adata)` in v1.4 returns an error:. ```. Traceback (most recent call last):. File ""<ipython-input-412-3baa85828ec9>"", line 1, in <module>. sc.pl.paga(adata). File ""/path/to/scanpy/scanpy/plotting/_tools/paga.py"", line 445, in paga. adjacency_solid, layout=layout, random_state=random_state, init_pos=init_pos, layout_kwds=layout_kwds, adj_tree=adj_tree, root=root). UnboundLocalError: local variable 'adj_tree' referenced before assignment. ```. There is a conditional before the referenced line, which assigns value to `adj_tree`, and indeed, running these works fine:. ```. sc.pl.paga(adata, layout='rt'). sc.pl.paga(adata, layout='rt_circular'). sc.pl.paga(adata, layout='eq_tree'). ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/487
https://github.com/scverse/scanpy/issues/488:87,usability,behavi,behaviour,87,"Why n_genes is not equal to the number of genes with non-zero expression?; I find this behaviour surprising:. ```. >> sc.pp.filter_cells(adata, min_genes=200). >> sc.pp.filter_genes(adata, min_cells=100). >> list(np.sum(adata.X.toarray() != 0, 1))[:10]. [814, 1410, 3580, 3514, 1456, 856, 795, 2154, 671, 4051]. >> adata.obs[""n_genes""].tolist()[:10]. [869, 1513, 4058, 4251, 1580, 908, 840, 2458, 695, 4708]. ```. I would expect both lists to be equal, not ""mostly equal"". Is there any good explanation for this behaviour?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/488
https://github.com/scverse/scanpy/issues/488:512,usability,behavi,behaviour,512,"Why n_genes is not equal to the number of genes with non-zero expression?; I find this behaviour surprising:. ```. >> sc.pp.filter_cells(adata, min_genes=200). >> sc.pp.filter_genes(adata, min_cells=100). >> list(np.sum(adata.X.toarray() != 0, 1))[:10]. [814, 1410, 3580, 3514, 1456, 856, 795, 2154, 671, 4051]. >> adata.obs[""n_genes""].tolist()[:10]. [869, 1513, 4058, 4251, 1580, 908, 840, 2458, 695, 4708]. ```. I would expect both lists to be equal, not ""mostly equal"". Is there any good explanation for this behaviour?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/488
https://github.com/scverse/scanpy/issues/489:106,security,access,accession,106,"[Suggestion] Expression atlas datasets?; I've written up a little script for making an `AnnData` given an accession for the EBI single cell expression atlas ([gist](https://gist.github.com/ivirshup/e7f0c435474d9b06b622c63d4221afe6)), and was wondering if it'd be useful to add under `sc.datasets`. It could be nice to make it easier to try out methods/ have examples which weren't for pbmcs.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/489
https://github.com/scverse/scanpy/issues/491:41,availability,cluster,clusters,41,"How to infer biological meaning from sub-clusters seen on UMAP ?; Is there a metric I can use to prove similarity/differences between sub-clusters on a UMAP plot ? For example, is there a way to prove that cells in cluster 1 are/are not biologically similar to cells in cluster 2 ? Or another thing I would like to know is how similar/different is cluster 1 to all other clusters in this data-set and I wanted to know what metric (some sort of distance metric?) can help prove this ? Also, if I see 5 sub-clusters of cells for a set of samples, is it okay to interpret this as 5 biologically different populations or is that reading too much into the clustering output ? Thank you.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/491
https://github.com/scverse/scanpy/issues/491:138,availability,cluster,clusters,138,"How to infer biological meaning from sub-clusters seen on UMAP ?; Is there a metric I can use to prove similarity/differences between sub-clusters on a UMAP plot ? For example, is there a way to prove that cells in cluster 1 are/are not biologically similar to cells in cluster 2 ? Or another thing I would like to know is how similar/different is cluster 1 to all other clusters in this data-set and I wanted to know what metric (some sort of distance metric?) can help prove this ? Also, if I see 5 sub-clusters of cells for a set of samples, is it okay to interpret this as 5 biologically different populations or is that reading too much into the clustering output ? Thank you.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/491
https://github.com/scverse/scanpy/issues/491:215,availability,cluster,cluster,215,"How to infer biological meaning from sub-clusters seen on UMAP ?; Is there a metric I can use to prove similarity/differences between sub-clusters on a UMAP plot ? For example, is there a way to prove that cells in cluster 1 are/are not biologically similar to cells in cluster 2 ? Or another thing I would like to know is how similar/different is cluster 1 to all other clusters in this data-set and I wanted to know what metric (some sort of distance metric?) can help prove this ? Also, if I see 5 sub-clusters of cells for a set of samples, is it okay to interpret this as 5 biologically different populations or is that reading too much into the clustering output ? Thank you.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/491
https://github.com/scverse/scanpy/issues/491:270,availability,cluster,cluster,270,"How to infer biological meaning from sub-clusters seen on UMAP ?; Is there a metric I can use to prove similarity/differences between sub-clusters on a UMAP plot ? For example, is there a way to prove that cells in cluster 1 are/are not biologically similar to cells in cluster 2 ? Or another thing I would like to know is how similar/different is cluster 1 to all other clusters in this data-set and I wanted to know what metric (some sort of distance metric?) can help prove this ? Also, if I see 5 sub-clusters of cells for a set of samples, is it okay to interpret this as 5 biologically different populations or is that reading too much into the clustering output ? Thank you.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/491
https://github.com/scverse/scanpy/issues/491:348,availability,cluster,cluster,348,"How to infer biological meaning from sub-clusters seen on UMAP ?; Is there a metric I can use to prove similarity/differences between sub-clusters on a UMAP plot ? For example, is there a way to prove that cells in cluster 1 are/are not biologically similar to cells in cluster 2 ? Or another thing I would like to know is how similar/different is cluster 1 to all other clusters in this data-set and I wanted to know what metric (some sort of distance metric?) can help prove this ? Also, if I see 5 sub-clusters of cells for a set of samples, is it okay to interpret this as 5 biologically different populations or is that reading too much into the clustering output ? Thank you.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/491
https://github.com/scverse/scanpy/issues/491:371,availability,cluster,clusters,371,"How to infer biological meaning from sub-clusters seen on UMAP ?; Is there a metric I can use to prove similarity/differences between sub-clusters on a UMAP plot ? For example, is there a way to prove that cells in cluster 1 are/are not biologically similar to cells in cluster 2 ? Or another thing I would like to know is how similar/different is cluster 1 to all other clusters in this data-set and I wanted to know what metric (some sort of distance metric?) can help prove this ? Also, if I see 5 sub-clusters of cells for a set of samples, is it okay to interpret this as 5 biologically different populations or is that reading too much into the clustering output ? Thank you.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/491
https://github.com/scverse/scanpy/issues/491:505,availability,cluster,clusters,505,"How to infer biological meaning from sub-clusters seen on UMAP ?; Is there a metric I can use to prove similarity/differences between sub-clusters on a UMAP plot ? For example, is there a way to prove that cells in cluster 1 are/are not biologically similar to cells in cluster 2 ? Or another thing I would like to know is how similar/different is cluster 1 to all other clusters in this data-set and I wanted to know what metric (some sort of distance metric?) can help prove this ? Also, if I see 5 sub-clusters of cells for a set of samples, is it okay to interpret this as 5 biologically different populations or is that reading too much into the clustering output ? Thank you.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/491
https://github.com/scverse/scanpy/issues/491:651,availability,cluster,clustering,651,"How to infer biological meaning from sub-clusters seen on UMAP ?; Is there a metric I can use to prove similarity/differences between sub-clusters on a UMAP plot ? For example, is there a way to prove that cells in cluster 1 are/are not biologically similar to cells in cluster 2 ? Or another thing I would like to know is how similar/different is cluster 1 to all other clusters in this data-set and I wanted to know what metric (some sort of distance metric?) can help prove this ? Also, if I see 5 sub-clusters of cells for a set of samples, is it okay to interpret this as 5 biologically different populations or is that reading too much into the clustering output ? Thank you.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/491
https://github.com/scverse/scanpy/issues/491:41,deployability,cluster,clusters,41,"How to infer biological meaning from sub-clusters seen on UMAP ?; Is there a metric I can use to prove similarity/differences between sub-clusters on a UMAP plot ? For example, is there a way to prove that cells in cluster 1 are/are not biologically similar to cells in cluster 2 ? Or another thing I would like to know is how similar/different is cluster 1 to all other clusters in this data-set and I wanted to know what metric (some sort of distance metric?) can help prove this ? Also, if I see 5 sub-clusters of cells for a set of samples, is it okay to interpret this as 5 biologically different populations or is that reading too much into the clustering output ? Thank you.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/491
https://github.com/scverse/scanpy/issues/491:138,deployability,cluster,clusters,138,"How to infer biological meaning from sub-clusters seen on UMAP ?; Is there a metric I can use to prove similarity/differences between sub-clusters on a UMAP plot ? For example, is there a way to prove that cells in cluster 1 are/are not biologically similar to cells in cluster 2 ? Or another thing I would like to know is how similar/different is cluster 1 to all other clusters in this data-set and I wanted to know what metric (some sort of distance metric?) can help prove this ? Also, if I see 5 sub-clusters of cells for a set of samples, is it okay to interpret this as 5 biologically different populations or is that reading too much into the clustering output ? Thank you.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/491
https://github.com/scverse/scanpy/issues/491:215,deployability,cluster,cluster,215,"How to infer biological meaning from sub-clusters seen on UMAP ?; Is there a metric I can use to prove similarity/differences between sub-clusters on a UMAP plot ? For example, is there a way to prove that cells in cluster 1 are/are not biologically similar to cells in cluster 2 ? Or another thing I would like to know is how similar/different is cluster 1 to all other clusters in this data-set and I wanted to know what metric (some sort of distance metric?) can help prove this ? Also, if I see 5 sub-clusters of cells for a set of samples, is it okay to interpret this as 5 biologically different populations or is that reading too much into the clustering output ? Thank you.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/491
https://github.com/scverse/scanpy/issues/491:270,deployability,cluster,cluster,270,"How to infer biological meaning from sub-clusters seen on UMAP ?; Is there a metric I can use to prove similarity/differences between sub-clusters on a UMAP plot ? For example, is there a way to prove that cells in cluster 1 are/are not biologically similar to cells in cluster 2 ? Or another thing I would like to know is how similar/different is cluster 1 to all other clusters in this data-set and I wanted to know what metric (some sort of distance metric?) can help prove this ? Also, if I see 5 sub-clusters of cells for a set of samples, is it okay to interpret this as 5 biologically different populations or is that reading too much into the clustering output ? Thank you.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/491
https://github.com/scverse/scanpy/issues/491:348,deployability,cluster,cluster,348,"How to infer biological meaning from sub-clusters seen on UMAP ?; Is there a metric I can use to prove similarity/differences between sub-clusters on a UMAP plot ? For example, is there a way to prove that cells in cluster 1 are/are not biologically similar to cells in cluster 2 ? Or another thing I would like to know is how similar/different is cluster 1 to all other clusters in this data-set and I wanted to know what metric (some sort of distance metric?) can help prove this ? Also, if I see 5 sub-clusters of cells for a set of samples, is it okay to interpret this as 5 biologically different populations or is that reading too much into the clustering output ? Thank you.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/491
https://github.com/scverse/scanpy/issues/491:371,deployability,cluster,clusters,371,"How to infer biological meaning from sub-clusters seen on UMAP ?; Is there a metric I can use to prove similarity/differences between sub-clusters on a UMAP plot ? For example, is there a way to prove that cells in cluster 1 are/are not biologically similar to cells in cluster 2 ? Or another thing I would like to know is how similar/different is cluster 1 to all other clusters in this data-set and I wanted to know what metric (some sort of distance metric?) can help prove this ? Also, if I see 5 sub-clusters of cells for a set of samples, is it okay to interpret this as 5 biologically different populations or is that reading too much into the clustering output ? Thank you.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/491
https://github.com/scverse/scanpy/issues/491:505,deployability,cluster,clusters,505,"How to infer biological meaning from sub-clusters seen on UMAP ?; Is there a metric I can use to prove similarity/differences between sub-clusters on a UMAP plot ? For example, is there a way to prove that cells in cluster 1 are/are not biologically similar to cells in cluster 2 ? Or another thing I would like to know is how similar/different is cluster 1 to all other clusters in this data-set and I wanted to know what metric (some sort of distance metric?) can help prove this ? Also, if I see 5 sub-clusters of cells for a set of samples, is it okay to interpret this as 5 biologically different populations or is that reading too much into the clustering output ? Thank you.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/491
https://github.com/scverse/scanpy/issues/491:651,deployability,cluster,clustering,651,"How to infer biological meaning from sub-clusters seen on UMAP ?; Is there a metric I can use to prove similarity/differences between sub-clusters on a UMAP plot ? For example, is there a way to prove that cells in cluster 1 are/are not biologically similar to cells in cluster 2 ? Or another thing I would like to know is how similar/different is cluster 1 to all other clusters in this data-set and I wanted to know what metric (some sort of distance metric?) can help prove this ? Also, if I see 5 sub-clusters of cells for a set of samples, is it okay to interpret this as 5 biologically different populations or is that reading too much into the clustering output ? Thank you.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/491
https://github.com/scverse/scanpy/issues/491:37,integrability,sub,sub-clusters,37,"How to infer biological meaning from sub-clusters seen on UMAP ?; Is there a metric I can use to prove similarity/differences between sub-clusters on a UMAP plot ? For example, is there a way to prove that cells in cluster 1 are/are not biologically similar to cells in cluster 2 ? Or another thing I would like to know is how similar/different is cluster 1 to all other clusters in this data-set and I wanted to know what metric (some sort of distance metric?) can help prove this ? Also, if I see 5 sub-clusters of cells for a set of samples, is it okay to interpret this as 5 biologically different populations or is that reading too much into the clustering output ? Thank you.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/491
https://github.com/scverse/scanpy/issues/491:134,integrability,sub,sub-clusters,134,"How to infer biological meaning from sub-clusters seen on UMAP ?; Is there a metric I can use to prove similarity/differences between sub-clusters on a UMAP plot ? For example, is there a way to prove that cells in cluster 1 are/are not biologically similar to cells in cluster 2 ? Or another thing I would like to know is how similar/different is cluster 1 to all other clusters in this data-set and I wanted to know what metric (some sort of distance metric?) can help prove this ? Also, if I see 5 sub-clusters of cells for a set of samples, is it okay to interpret this as 5 biologically different populations or is that reading too much into the clustering output ? Thank you.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/491
https://github.com/scverse/scanpy/issues/491:501,integrability,sub,sub-clusters,501,"How to infer biological meaning from sub-clusters seen on UMAP ?; Is there a metric I can use to prove similarity/differences between sub-clusters on a UMAP plot ? For example, is there a way to prove that cells in cluster 1 are/are not biologically similar to cells in cluster 2 ? Or another thing I would like to know is how similar/different is cluster 1 to all other clusters in this data-set and I wanted to know what metric (some sort of distance metric?) can help prove this ? Also, if I see 5 sub-clusters of cells for a set of samples, is it okay to interpret this as 5 biologically different populations or is that reading too much into the clustering output ? Thank you.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/491
https://github.com/scverse/scanpy/issues/491:466,usability,help,help,466,"How to infer biological meaning from sub-clusters seen on UMAP ?; Is there a metric I can use to prove similarity/differences between sub-clusters on a UMAP plot ? For example, is there a way to prove that cells in cluster 1 are/are not biologically similar to cells in cluster 2 ? Or another thing I would like to know is how similar/different is cluster 1 to all other clusters in this data-set and I wanted to know what metric (some sort of distance metric?) can help prove this ? Also, if I see 5 sub-clusters of cells for a set of samples, is it okay to interpret this as 5 biologically different populations or is that reading too much into the clustering output ? Thank you.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/491
https://github.com/scverse/scanpy/issues/492:213,reliability,doe,does,213,"Regressing out genes; In Seurat, there is an option of selecting a list of genes in the pre-processing regressOut function. I was wondering if there was similar functionality in Scanpy. Doing something like below does not work for me as a lot of the cells have 0 expression, giving me a PerfectSeparationError. `adata.obs[gene] = adata[:, adata.var_names==gene].X`. `sc.pp.regress_out(adata,gene)`. Any help would be appreciated. Thank you!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/492
https://github.com/scverse/scanpy/issues/492:0,testability,Regress,Regressing,0,"Regressing out genes; In Seurat, there is an option of selecting a list of genes in the pre-processing regressOut function. I was wondering if there was similar functionality in Scanpy. Doing something like below does not work for me as a lot of the cells have 0 expression, giving me a PerfectSeparationError. `adata.obs[gene] = adata[:, adata.var_names==gene].X`. `sc.pp.regress_out(adata,gene)`. Any help would be appreciated. Thank you!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/492
https://github.com/scverse/scanpy/issues/492:103,testability,regress,regressOut,103,"Regressing out genes; In Seurat, there is an option of selecting a list of genes in the pre-processing regressOut function. I was wondering if there was similar functionality in Scanpy. Doing something like below does not work for me as a lot of the cells have 0 expression, giving me a PerfectSeparationError. `adata.obs[gene] = adata[:, adata.var_names==gene].X`. `sc.pp.regress_out(adata,gene)`. Any help would be appreciated. Thank you!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/492
https://github.com/scverse/scanpy/issues/492:403,usability,help,help,403,"Regressing out genes; In Seurat, there is an option of selecting a list of genes in the pre-processing regressOut function. I was wondering if there was similar functionality in Scanpy. Doing something like below does not work for me as a lot of the cells have 0 expression, giving me a PerfectSeparationError. `adata.obs[gene] = adata[:, adata.var_names==gene].X`. `sc.pp.regress_out(adata,gene)`. Any help would be appreciated. Thank you!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/492
https://github.com/scverse/scanpy/pull/493:73,integrability,wrap,wrapper,73,Palantir for Single cell trajectory detection; @falexwolf . I'm adding a wrapper for Palantir by [Setty et al. (2018)](https://doi.org/10.1101/385328). Please let me know if you have any comments.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/493
https://github.com/scverse/scanpy/pull/493:73,interoperability,wrapper,wrapper,73,Palantir for Single cell trajectory detection; @falexwolf . I'm adding a wrapper for Palantir by [Setty et al. (2018)](https://doi.org/10.1101/385328). Please let me know if you have any comments.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/493
https://github.com/scverse/scanpy/pull/493:36,safety,detect,detection,36,Palantir for Single cell trajectory detection; @falexwolf . I'm adding a wrapper for Palantir by [Setty et al. (2018)](https://doi.org/10.1101/385328). Please let me know if you have any comments.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/493
https://github.com/scverse/scanpy/pull/493:36,security,detect,detection,36,Palantir for Single cell trajectory detection; @falexwolf . I'm adding a wrapper for Palantir by [Setty et al. (2018)](https://doi.org/10.1101/385328). Please let me know if you have any comments.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/493
https://github.com/scverse/scanpy/pull/494:47,reliability,doe,does,47,[Bugfix] I had forgotten what `all(dataframe)` does; Just spent an hour re-figuring out that `all(dataframe)` does `all(bool(c) for c in dataframe.columns)` 😢. Now finding out where I've written that.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/494
https://github.com/scverse/scanpy/pull/494:110,reliability,doe,does,110,[Bugfix] I had forgotten what `all(dataframe)` does; Just spent an hour re-figuring out that `all(dataframe)` does `all(bool(c) for c in dataframe.columns)` 😢. Now finding out where I've written that.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/494
https://github.com/scverse/scanpy/issues/495:274,energy efficiency,load,loaded,274,"Annotating cells help; How were the bulk labels generated then assigned to cells in the pbmc68k dataset? I'm trying to do the same on my data. Ideally I would like to use my own list to label cells. For example a cell has Gene X and Gene Y, then the 'bulk_label' in .obs is loaded with a string 'Cell Z'. Thank you",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/495
https://github.com/scverse/scanpy/issues/495:274,performance,load,loaded,274,"Annotating cells help; How were the bulk labels generated then assigned to cells in the pbmc68k dataset? I'm trying to do the same on my data. Ideally I would like to use my own list to label cells. For example a cell has Gene X and Gene Y, then the 'bulk_label' in .obs is loaded with a string 'Cell Z'. Thank you",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/495
https://github.com/scverse/scanpy/issues/495:17,usability,help,help,17,"Annotating cells help; How were the bulk labels generated then assigned to cells in the pbmc68k dataset? I'm trying to do the same on my data. Ideally I would like to use my own list to label cells. For example a cell has Gene X and Gene Y, then the 'bulk_label' in .obs is loaded with a string 'Cell Z'. Thank you",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/495
https://github.com/scverse/scanpy/issues/496:0,availability,Error,Error,0,"Error related to verbosity; @flying-sheep: After the recent changes I am getting the following error:. ```bash. /apps/scanpy/scanpy/logging.py in _settings_verbosity_greater_or_equal_than(v). 36 def _settings_verbosity_greater_or_equal_than(v):. 37 if isinstance(settings.verbosity, str):. ---> 38 settings_v = _VERBOSITY_LEVELS_FROM_STRINGS[settings.verbosity]. 39 else:. 40 settings_v = settings.verbosity. KeyError: 'warning'. ```. The problem is solved by setting the verbosity level. E.g. ```. sc.settings.verbosity = 3. ```.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/496
https://github.com/scverse/scanpy/issues/496:95,availability,error,error,95,"Error related to verbosity; @flying-sheep: After the recent changes I am getting the following error:. ```bash. /apps/scanpy/scanpy/logging.py in _settings_verbosity_greater_or_equal_than(v). 36 def _settings_verbosity_greater_or_equal_than(v):. 37 if isinstance(settings.verbosity, str):. ---> 38 settings_v = _VERBOSITY_LEVELS_FROM_STRINGS[settings.verbosity]. 39 else:. 40 settings_v = settings.verbosity. KeyError: 'warning'. ```. The problem is solved by setting the verbosity level. E.g. ```. sc.settings.verbosity = 3. ```.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/496
https://github.com/scverse/scanpy/issues/496:132,deployability,log,logging,132,"Error related to verbosity; @flying-sheep: After the recent changes I am getting the following error:. ```bash. /apps/scanpy/scanpy/logging.py in _settings_verbosity_greater_or_equal_than(v). 36 def _settings_verbosity_greater_or_equal_than(v):. 37 if isinstance(settings.verbosity, str):. ---> 38 settings_v = _VERBOSITY_LEVELS_FROM_STRINGS[settings.verbosity]. 39 else:. 40 settings_v = settings.verbosity. KeyError: 'warning'. ```. The problem is solved by setting the verbosity level. E.g. ```. sc.settings.verbosity = 3. ```.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/496
https://github.com/scverse/scanpy/issues/496:0,performance,Error,Error,0,"Error related to verbosity; @flying-sheep: After the recent changes I am getting the following error:. ```bash. /apps/scanpy/scanpy/logging.py in _settings_verbosity_greater_or_equal_than(v). 36 def _settings_verbosity_greater_or_equal_than(v):. 37 if isinstance(settings.verbosity, str):. ---> 38 settings_v = _VERBOSITY_LEVELS_FROM_STRINGS[settings.verbosity]. 39 else:. 40 settings_v = settings.verbosity. KeyError: 'warning'. ```. The problem is solved by setting the verbosity level. E.g. ```. sc.settings.verbosity = 3. ```.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/496
https://github.com/scverse/scanpy/issues/496:95,performance,error,error,95,"Error related to verbosity; @flying-sheep: After the recent changes I am getting the following error:. ```bash. /apps/scanpy/scanpy/logging.py in _settings_verbosity_greater_or_equal_than(v). 36 def _settings_verbosity_greater_or_equal_than(v):. 37 if isinstance(settings.verbosity, str):. ---> 38 settings_v = _VERBOSITY_LEVELS_FROM_STRINGS[settings.verbosity]. 39 else:. 40 settings_v = settings.verbosity. KeyError: 'warning'. ```. The problem is solved by setting the verbosity level. E.g. ```. sc.settings.verbosity = 3. ```.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/496
https://github.com/scverse/scanpy/issues/496:0,safety,Error,Error,0,"Error related to verbosity; @flying-sheep: After the recent changes I am getting the following error:. ```bash. /apps/scanpy/scanpy/logging.py in _settings_verbosity_greater_or_equal_than(v). 36 def _settings_verbosity_greater_or_equal_than(v):. 37 if isinstance(settings.verbosity, str):. ---> 38 settings_v = _VERBOSITY_LEVELS_FROM_STRINGS[settings.verbosity]. 39 else:. 40 settings_v = settings.verbosity. KeyError: 'warning'. ```. The problem is solved by setting the verbosity level. E.g. ```. sc.settings.verbosity = 3. ```.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/496
https://github.com/scverse/scanpy/issues/496:95,safety,error,error,95,"Error related to verbosity; @flying-sheep: After the recent changes I am getting the following error:. ```bash. /apps/scanpy/scanpy/logging.py in _settings_verbosity_greater_or_equal_than(v). 36 def _settings_verbosity_greater_or_equal_than(v):. 37 if isinstance(settings.verbosity, str):. ---> 38 settings_v = _VERBOSITY_LEVELS_FROM_STRINGS[settings.verbosity]. 39 else:. 40 settings_v = settings.verbosity. KeyError: 'warning'. ```. The problem is solved by setting the verbosity level. E.g. ```. sc.settings.verbosity = 3. ```.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/496
https://github.com/scverse/scanpy/issues/496:132,safety,log,logging,132,"Error related to verbosity; @flying-sheep: After the recent changes I am getting the following error:. ```bash. /apps/scanpy/scanpy/logging.py in _settings_verbosity_greater_or_equal_than(v). 36 def _settings_verbosity_greater_or_equal_than(v):. 37 if isinstance(settings.verbosity, str):. ---> 38 settings_v = _VERBOSITY_LEVELS_FROM_STRINGS[settings.verbosity]. 39 else:. 40 settings_v = settings.verbosity. KeyError: 'warning'. ```. The problem is solved by setting the verbosity level. E.g. ```. sc.settings.verbosity = 3. ```.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/496
https://github.com/scverse/scanpy/issues/496:132,security,log,logging,132,"Error related to verbosity; @flying-sheep: After the recent changes I am getting the following error:. ```bash. /apps/scanpy/scanpy/logging.py in _settings_verbosity_greater_or_equal_than(v). 36 def _settings_verbosity_greater_or_equal_than(v):. 37 if isinstance(settings.verbosity, str):. ---> 38 settings_v = _VERBOSITY_LEVELS_FROM_STRINGS[settings.verbosity]. 39 else:. 40 settings_v = settings.verbosity. KeyError: 'warning'. ```. The problem is solved by setting the verbosity level. E.g. ```. sc.settings.verbosity = 3. ```.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/496
https://github.com/scverse/scanpy/issues/496:132,testability,log,logging,132,"Error related to verbosity; @flying-sheep: After the recent changes I am getting the following error:. ```bash. /apps/scanpy/scanpy/logging.py in _settings_verbosity_greater_or_equal_than(v). 36 def _settings_verbosity_greater_or_equal_than(v):. 37 if isinstance(settings.verbosity, str):. ---> 38 settings_v = _VERBOSITY_LEVELS_FROM_STRINGS[settings.verbosity]. 39 else:. 40 settings_v = settings.verbosity. KeyError: 'warning'. ```. The problem is solved by setting the verbosity level. E.g. ```. sc.settings.verbosity = 3. ```.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/496
https://github.com/scverse/scanpy/issues/496:0,usability,Error,Error,0,"Error related to verbosity; @flying-sheep: After the recent changes I am getting the following error:. ```bash. /apps/scanpy/scanpy/logging.py in _settings_verbosity_greater_or_equal_than(v). 36 def _settings_verbosity_greater_or_equal_than(v):. 37 if isinstance(settings.verbosity, str):. ---> 38 settings_v = _VERBOSITY_LEVELS_FROM_STRINGS[settings.verbosity]. 39 else:. 40 settings_v = settings.verbosity. KeyError: 'warning'. ```. The problem is solved by setting the verbosity level. E.g. ```. sc.settings.verbosity = 3. ```.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/496
https://github.com/scverse/scanpy/issues/496:95,usability,error,error,95,"Error related to verbosity; @flying-sheep: After the recent changes I am getting the following error:. ```bash. /apps/scanpy/scanpy/logging.py in _settings_verbosity_greater_or_equal_than(v). 36 def _settings_verbosity_greater_or_equal_than(v):. 37 if isinstance(settings.verbosity, str):. ---> 38 settings_v = _VERBOSITY_LEVELS_FROM_STRINGS[settings.verbosity]. 39 else:. 40 settings_v = settings.verbosity. KeyError: 'warning'. ```. The problem is solved by setting the verbosity level. E.g. ```. sc.settings.verbosity = 3. ```.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/496
https://github.com/scverse/scanpy/issues/497:134,availability,replic,replicate,134,"Seurat method 'vst' from FindVariableFeatures in highly_variable_genes function; Hi,. More of a request than an issue. I am trying to replicate FindVariableFeatures with option selection.method = ""vst"" in seurat by using highly_variable_genes function in scanpy,i went through the documentation but could not find this option,is it available and am i missing something or is it not implemented yet. It would be nice to have this option. Thank you. Sasi.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/497
https://github.com/scverse/scanpy/issues/497:332,availability,avail,available,332,"Seurat method 'vst' from FindVariableFeatures in highly_variable_genes function; Hi,. More of a request than an issue. I am trying to replicate FindVariableFeatures with option selection.method = ""vst"" in seurat by using highly_variable_genes function in scanpy,i went through the documentation but could not find this option,is it available and am i missing something or is it not implemented yet. It would be nice to have this option. Thank you. Sasi.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/497
https://github.com/scverse/scanpy/issues/497:332,reliability,availab,available,332,"Seurat method 'vst' from FindVariableFeatures in highly_variable_genes function; Hi,. More of a request than an issue. I am trying to replicate FindVariableFeatures with option selection.method = ""vst"" in seurat by using highly_variable_genes function in scanpy,i went through the documentation but could not find this option,is it available and am i missing something or is it not implemented yet. It would be nice to have this option. Thank you. Sasi.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/497
https://github.com/scverse/scanpy/issues/497:332,safety,avail,available,332,"Seurat method 'vst' from FindVariableFeatures in highly_variable_genes function; Hi,. More of a request than an issue. I am trying to replicate FindVariableFeatures with option selection.method = ""vst"" in seurat by using highly_variable_genes function in scanpy,i went through the documentation but could not find this option,is it available and am i missing something or is it not implemented yet. It would be nice to have this option. Thank you. Sasi.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/497
https://github.com/scverse/scanpy/issues/497:332,security,availab,available,332,"Seurat method 'vst' from FindVariableFeatures in highly_variable_genes function; Hi,. More of a request than an issue. I am trying to replicate FindVariableFeatures with option selection.method = ""vst"" in seurat by using highly_variable_genes function in scanpy,i went through the documentation but could not find this option,is it available and am i missing something or is it not implemented yet. It would be nice to have this option. Thank you. Sasi.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/497
https://github.com/scverse/scanpy/issues/497:281,usability,document,documentation,281,"Seurat method 'vst' from FindVariableFeatures in highly_variable_genes function; Hi,. More of a request than an issue. I am trying to replicate FindVariableFeatures with option selection.method = ""vst"" in seurat by using highly_variable_genes function in scanpy,i went through the documentation but could not find this option,is it available and am i missing something or is it not implemented yet. It would be nice to have this option. Thank you. Sasi.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/497
https://github.com/scverse/scanpy/issues/499:439,availability,error,error,439,"Calculate qc metrics with sns.jointplot; Dear,. When I Calculate qc metrics for visualization according to the example in https://scanpy.readthedocs.io/en/latest/api/scanpy.pp.calculate_qc_metrics.html#scanpy.pp.calculate_qc_metrics:. ```py. >>> adata = sc.datasets.pbmc3k(). >>> sc.pp.calculate_qc_metrics(adata, inplace=True). >>> sns.jointplot(adata.obs, ""log1p_total_counts"", ""log1p_n_genes_by_counts"", kind=""hex""). ```. The following error occurred:. AttributeError: 'str' object has no attribute 'get'. It seems that sns.jointplot are not compatible well with adata.obs, anybody who can help me ?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/499
https://github.com/scverse/scanpy/issues/499:162,deployability,api,api,162,"Calculate qc metrics with sns.jointplot; Dear,. When I Calculate qc metrics for visualization according to the example in https://scanpy.readthedocs.io/en/latest/api/scanpy.pp.calculate_qc_metrics.html#scanpy.pp.calculate_qc_metrics:. ```py. >>> adata = sc.datasets.pbmc3k(). >>> sc.pp.calculate_qc_metrics(adata, inplace=True). >>> sns.jointplot(adata.obs, ""log1p_total_counts"", ""log1p_n_genes_by_counts"", kind=""hex""). ```. The following error occurred:. AttributeError: 'str' object has no attribute 'get'. It seems that sns.jointplot are not compatible well with adata.obs, anybody who can help me ?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/499
https://github.com/scverse/scanpy/issues/499:162,integrability,api,api,162,"Calculate qc metrics with sns.jointplot; Dear,. When I Calculate qc metrics for visualization according to the example in https://scanpy.readthedocs.io/en/latest/api/scanpy.pp.calculate_qc_metrics.html#scanpy.pp.calculate_qc_metrics:. ```py. >>> adata = sc.datasets.pbmc3k(). >>> sc.pp.calculate_qc_metrics(adata, inplace=True). >>> sns.jointplot(adata.obs, ""log1p_total_counts"", ""log1p_n_genes_by_counts"", kind=""hex""). ```. The following error occurred:. AttributeError: 'str' object has no attribute 'get'. It seems that sns.jointplot are not compatible well with adata.obs, anybody who can help me ?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/499
https://github.com/scverse/scanpy/issues/499:162,interoperability,api,api,162,"Calculate qc metrics with sns.jointplot; Dear,. When I Calculate qc metrics for visualization according to the example in https://scanpy.readthedocs.io/en/latest/api/scanpy.pp.calculate_qc_metrics.html#scanpy.pp.calculate_qc_metrics:. ```py. >>> adata = sc.datasets.pbmc3k(). >>> sc.pp.calculate_qc_metrics(adata, inplace=True). >>> sns.jointplot(adata.obs, ""log1p_total_counts"", ""log1p_n_genes_by_counts"", kind=""hex""). ```. The following error occurred:. AttributeError: 'str' object has no attribute 'get'. It seems that sns.jointplot are not compatible well with adata.obs, anybody who can help me ?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/499
https://github.com/scverse/scanpy/issues/499:545,interoperability,compatib,compatible,545,"Calculate qc metrics with sns.jointplot; Dear,. When I Calculate qc metrics for visualization according to the example in https://scanpy.readthedocs.io/en/latest/api/scanpy.pp.calculate_qc_metrics.html#scanpy.pp.calculate_qc_metrics:. ```py. >>> adata = sc.datasets.pbmc3k(). >>> sc.pp.calculate_qc_metrics(adata, inplace=True). >>> sns.jointplot(adata.obs, ""log1p_total_counts"", ""log1p_n_genes_by_counts"", kind=""hex""). ```. The following error occurred:. AttributeError: 'str' object has no attribute 'get'. It seems that sns.jointplot are not compatible well with adata.obs, anybody who can help me ?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/499
https://github.com/scverse/scanpy/issues/499:439,performance,error,error,439,"Calculate qc metrics with sns.jointplot; Dear,. When I Calculate qc metrics for visualization according to the example in https://scanpy.readthedocs.io/en/latest/api/scanpy.pp.calculate_qc_metrics.html#scanpy.pp.calculate_qc_metrics:. ```py. >>> adata = sc.datasets.pbmc3k(). >>> sc.pp.calculate_qc_metrics(adata, inplace=True). >>> sns.jointplot(adata.obs, ""log1p_total_counts"", ""log1p_n_genes_by_counts"", kind=""hex""). ```. The following error occurred:. AttributeError: 'str' object has no attribute 'get'. It seems that sns.jointplot are not compatible well with adata.obs, anybody who can help me ?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/499
https://github.com/scverse/scanpy/issues/499:439,safety,error,error,439,"Calculate qc metrics with sns.jointplot; Dear,. When I Calculate qc metrics for visualization according to the example in https://scanpy.readthedocs.io/en/latest/api/scanpy.pp.calculate_qc_metrics.html#scanpy.pp.calculate_qc_metrics:. ```py. >>> adata = sc.datasets.pbmc3k(). >>> sc.pp.calculate_qc_metrics(adata, inplace=True). >>> sns.jointplot(adata.obs, ""log1p_total_counts"", ""log1p_n_genes_by_counts"", kind=""hex""). ```. The following error occurred:. AttributeError: 'str' object has no attribute 'get'. It seems that sns.jointplot are not compatible well with adata.obs, anybody who can help me ?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/499
https://github.com/scverse/scanpy/issues/499:80,usability,visual,visualization,80,"Calculate qc metrics with sns.jointplot; Dear,. When I Calculate qc metrics for visualization according to the example in https://scanpy.readthedocs.io/en/latest/api/scanpy.pp.calculate_qc_metrics.html#scanpy.pp.calculate_qc_metrics:. ```py. >>> adata = sc.datasets.pbmc3k(). >>> sc.pp.calculate_qc_metrics(adata, inplace=True). >>> sns.jointplot(adata.obs, ""log1p_total_counts"", ""log1p_n_genes_by_counts"", kind=""hex""). ```. The following error occurred:. AttributeError: 'str' object has no attribute 'get'. It seems that sns.jointplot are not compatible well with adata.obs, anybody who can help me ?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/499
https://github.com/scverse/scanpy/issues/499:439,usability,error,error,439,"Calculate qc metrics with sns.jointplot; Dear,. When I Calculate qc metrics for visualization according to the example in https://scanpy.readthedocs.io/en/latest/api/scanpy.pp.calculate_qc_metrics.html#scanpy.pp.calculate_qc_metrics:. ```py. >>> adata = sc.datasets.pbmc3k(). >>> sc.pp.calculate_qc_metrics(adata, inplace=True). >>> sns.jointplot(adata.obs, ""log1p_total_counts"", ""log1p_n_genes_by_counts"", kind=""hex""). ```. The following error occurred:. AttributeError: 'str' object has no attribute 'get'. It seems that sns.jointplot are not compatible well with adata.obs, anybody who can help me ?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/499
https://github.com/scverse/scanpy/issues/499:593,usability,help,help,593,"Calculate qc metrics with sns.jointplot; Dear,. When I Calculate qc metrics for visualization according to the example in https://scanpy.readthedocs.io/en/latest/api/scanpy.pp.calculate_qc_metrics.html#scanpy.pp.calculate_qc_metrics:. ```py. >>> adata = sc.datasets.pbmc3k(). >>> sc.pp.calculate_qc_metrics(adata, inplace=True). >>> sns.jointplot(adata.obs, ""log1p_total_counts"", ""log1p_n_genes_by_counts"", kind=""hex""). ```. The following error occurred:. AttributeError: 'str' object has no attribute 'get'. It seems that sns.jointplot are not compatible well with adata.obs, anybody who can help me ?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/499
https://github.com/scverse/scanpy/issues/501:87,deployability,updat,updated,87,"sc.pp.filter don't filter the same number between sc.1.3.7 and sc.1.4; Hi all,. I have updated my scanpy to version 1.4 (was working on 1.3.7 before) and did not get the same filtering output using sc.pp.filter_ working with the same input dataset (10X). By running in sc1.3.7: sc.pp.filter_genes(adata, min_counts=2). -> 267 genes were filtered out and I was able to follow up on my analysis until the end. However, after switching to the new version, I could not get any filtering anymore. By scaling up, the first filtering I got was with a min of counts of 4 (sc.pp.filter_genes(adata, min_counts=4)). ""filtered out 655 genes that are detected in less than 4 counts"". Not sure what is going on there and which setting I should use then. Any feedback will be more than appreciated. Thank you.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/501
https://github.com/scverse/scanpy/issues/501:108,deployability,version,version,108,"sc.pp.filter don't filter the same number between sc.1.3.7 and sc.1.4; Hi all,. I have updated my scanpy to version 1.4 (was working on 1.3.7 before) and did not get the same filtering output using sc.pp.filter_ working with the same input dataset (10X). By running in sc1.3.7: sc.pp.filter_genes(adata, min_counts=2). -> 267 genes were filtered out and I was able to follow up on my analysis until the end. However, after switching to the new version, I could not get any filtering anymore. By scaling up, the first filtering I got was with a min of counts of 4 (sc.pp.filter_genes(adata, min_counts=4)). ""filtered out 655 genes that are detected in less than 4 counts"". Not sure what is going on there and which setting I should use then. Any feedback will be more than appreciated. Thank you.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/501
https://github.com/scverse/scanpy/issues/501:444,deployability,version,version,444,"sc.pp.filter don't filter the same number between sc.1.3.7 and sc.1.4; Hi all,. I have updated my scanpy to version 1.4 (was working on 1.3.7 before) and did not get the same filtering output using sc.pp.filter_ working with the same input dataset (10X). By running in sc1.3.7: sc.pp.filter_genes(adata, min_counts=2). -> 267 genes were filtered out and I was able to follow up on my analysis until the end. However, after switching to the new version, I could not get any filtering anymore. By scaling up, the first filtering I got was with a min of counts of 4 (sc.pp.filter_genes(adata, min_counts=4)). ""filtered out 655 genes that are detected in less than 4 counts"". Not sure what is going on there and which setting I should use then. Any feedback will be more than appreciated. Thank you.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/501
https://github.com/scverse/scanpy/issues/501:6,integrability,filter,filter,6,"sc.pp.filter don't filter the same number between sc.1.3.7 and sc.1.4; Hi all,. I have updated my scanpy to version 1.4 (was working on 1.3.7 before) and did not get the same filtering output using sc.pp.filter_ working with the same input dataset (10X). By running in sc1.3.7: sc.pp.filter_genes(adata, min_counts=2). -> 267 genes were filtered out and I was able to follow up on my analysis until the end. However, after switching to the new version, I could not get any filtering anymore. By scaling up, the first filtering I got was with a min of counts of 4 (sc.pp.filter_genes(adata, min_counts=4)). ""filtered out 655 genes that are detected in less than 4 counts"". Not sure what is going on there and which setting I should use then. Any feedback will be more than appreciated. Thank you.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/501
https://github.com/scverse/scanpy/issues/501:19,integrability,filter,filter,19,"sc.pp.filter don't filter the same number between sc.1.3.7 and sc.1.4; Hi all,. I have updated my scanpy to version 1.4 (was working on 1.3.7 before) and did not get the same filtering output using sc.pp.filter_ working with the same input dataset (10X). By running in sc1.3.7: sc.pp.filter_genes(adata, min_counts=2). -> 267 genes were filtered out and I was able to follow up on my analysis until the end. However, after switching to the new version, I could not get any filtering anymore. By scaling up, the first filtering I got was with a min of counts of 4 (sc.pp.filter_genes(adata, min_counts=4)). ""filtered out 655 genes that are detected in less than 4 counts"". Not sure what is going on there and which setting I should use then. Any feedback will be more than appreciated. Thank you.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/501
https://github.com/scverse/scanpy/issues/501:108,integrability,version,version,108,"sc.pp.filter don't filter the same number between sc.1.3.7 and sc.1.4; Hi all,. I have updated my scanpy to version 1.4 (was working on 1.3.7 before) and did not get the same filtering output using sc.pp.filter_ working with the same input dataset (10X). By running in sc1.3.7: sc.pp.filter_genes(adata, min_counts=2). -> 267 genes were filtered out and I was able to follow up on my analysis until the end. However, after switching to the new version, I could not get any filtering anymore. By scaling up, the first filtering I got was with a min of counts of 4 (sc.pp.filter_genes(adata, min_counts=4)). ""filtered out 655 genes that are detected in less than 4 counts"". Not sure what is going on there and which setting I should use then. Any feedback will be more than appreciated. Thank you.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/501
https://github.com/scverse/scanpy/issues/501:175,integrability,filter,filtering,175,"sc.pp.filter don't filter the same number between sc.1.3.7 and sc.1.4; Hi all,. I have updated my scanpy to version 1.4 (was working on 1.3.7 before) and did not get the same filtering output using sc.pp.filter_ working with the same input dataset (10X). By running in sc1.3.7: sc.pp.filter_genes(adata, min_counts=2). -> 267 genes were filtered out and I was able to follow up on my analysis until the end. However, after switching to the new version, I could not get any filtering anymore. By scaling up, the first filtering I got was with a min of counts of 4 (sc.pp.filter_genes(adata, min_counts=4)). ""filtered out 655 genes that are detected in less than 4 counts"". Not sure what is going on there and which setting I should use then. Any feedback will be more than appreciated. Thank you.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/501
https://github.com/scverse/scanpy/issues/501:337,integrability,filter,filtered,337,"sc.pp.filter don't filter the same number between sc.1.3.7 and sc.1.4; Hi all,. I have updated my scanpy to version 1.4 (was working on 1.3.7 before) and did not get the same filtering output using sc.pp.filter_ working with the same input dataset (10X). By running in sc1.3.7: sc.pp.filter_genes(adata, min_counts=2). -> 267 genes were filtered out and I was able to follow up on my analysis until the end. However, after switching to the new version, I could not get any filtering anymore. By scaling up, the first filtering I got was with a min of counts of 4 (sc.pp.filter_genes(adata, min_counts=4)). ""filtered out 655 genes that are detected in less than 4 counts"". Not sure what is going on there and which setting I should use then. Any feedback will be more than appreciated. Thank you.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/501
https://github.com/scverse/scanpy/issues/501:444,integrability,version,version,444,"sc.pp.filter don't filter the same number between sc.1.3.7 and sc.1.4; Hi all,. I have updated my scanpy to version 1.4 (was working on 1.3.7 before) and did not get the same filtering output using sc.pp.filter_ working with the same input dataset (10X). By running in sc1.3.7: sc.pp.filter_genes(adata, min_counts=2). -> 267 genes were filtered out and I was able to follow up on my analysis until the end. However, after switching to the new version, I could not get any filtering anymore. By scaling up, the first filtering I got was with a min of counts of 4 (sc.pp.filter_genes(adata, min_counts=4)). ""filtered out 655 genes that are detected in less than 4 counts"". Not sure what is going on there and which setting I should use then. Any feedback will be more than appreciated. Thank you.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/501
https://github.com/scverse/scanpy/issues/501:473,integrability,filter,filtering,473,"sc.pp.filter don't filter the same number between sc.1.3.7 and sc.1.4; Hi all,. I have updated my scanpy to version 1.4 (was working on 1.3.7 before) and did not get the same filtering output using sc.pp.filter_ working with the same input dataset (10X). By running in sc1.3.7: sc.pp.filter_genes(adata, min_counts=2). -> 267 genes were filtered out and I was able to follow up on my analysis until the end. However, after switching to the new version, I could not get any filtering anymore. By scaling up, the first filtering I got was with a min of counts of 4 (sc.pp.filter_genes(adata, min_counts=4)). ""filtered out 655 genes that are detected in less than 4 counts"". Not sure what is going on there and which setting I should use then. Any feedback will be more than appreciated. Thank you.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/501
https://github.com/scverse/scanpy/issues/501:517,integrability,filter,filtering,517,"sc.pp.filter don't filter the same number between sc.1.3.7 and sc.1.4; Hi all,. I have updated my scanpy to version 1.4 (was working on 1.3.7 before) and did not get the same filtering output using sc.pp.filter_ working with the same input dataset (10X). By running in sc1.3.7: sc.pp.filter_genes(adata, min_counts=2). -> 267 genes were filtered out and I was able to follow up on my analysis until the end. However, after switching to the new version, I could not get any filtering anymore. By scaling up, the first filtering I got was with a min of counts of 4 (sc.pp.filter_genes(adata, min_counts=4)). ""filtered out 655 genes that are detected in less than 4 counts"". Not sure what is going on there and which setting I should use then. Any feedback will be more than appreciated. Thank you.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/501
https://github.com/scverse/scanpy/issues/501:607,integrability,filter,filtered,607,"sc.pp.filter don't filter the same number between sc.1.3.7 and sc.1.4; Hi all,. I have updated my scanpy to version 1.4 (was working on 1.3.7 before) and did not get the same filtering output using sc.pp.filter_ working with the same input dataset (10X). By running in sc1.3.7: sc.pp.filter_genes(adata, min_counts=2). -> 267 genes were filtered out and I was able to follow up on my analysis until the end. However, after switching to the new version, I could not get any filtering anymore. By scaling up, the first filtering I got was with a min of counts of 4 (sc.pp.filter_genes(adata, min_counts=4)). ""filtered out 655 genes that are detected in less than 4 counts"". Not sure what is going on there and which setting I should use then. Any feedback will be more than appreciated. Thank you.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/501
https://github.com/scverse/scanpy/issues/501:108,modifiability,version,version,108,"sc.pp.filter don't filter the same number between sc.1.3.7 and sc.1.4; Hi all,. I have updated my scanpy to version 1.4 (was working on 1.3.7 before) and did not get the same filtering output using sc.pp.filter_ working with the same input dataset (10X). By running in sc1.3.7: sc.pp.filter_genes(adata, min_counts=2). -> 267 genes were filtered out and I was able to follow up on my analysis until the end. However, after switching to the new version, I could not get any filtering anymore. By scaling up, the first filtering I got was with a min of counts of 4 (sc.pp.filter_genes(adata, min_counts=4)). ""filtered out 655 genes that are detected in less than 4 counts"". Not sure what is going on there and which setting I should use then. Any feedback will be more than appreciated. Thank you.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/501
https://github.com/scverse/scanpy/issues/501:444,modifiability,version,version,444,"sc.pp.filter don't filter the same number between sc.1.3.7 and sc.1.4; Hi all,. I have updated my scanpy to version 1.4 (was working on 1.3.7 before) and did not get the same filtering output using sc.pp.filter_ working with the same input dataset (10X). By running in sc1.3.7: sc.pp.filter_genes(adata, min_counts=2). -> 267 genes were filtered out and I was able to follow up on my analysis until the end. However, after switching to the new version, I could not get any filtering anymore. By scaling up, the first filtering I got was with a min of counts of 4 (sc.pp.filter_genes(adata, min_counts=4)). ""filtered out 655 genes that are detected in less than 4 counts"". Not sure what is going on there and which setting I should use then. Any feedback will be more than appreciated. Thank you.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/501
https://github.com/scverse/scanpy/issues/501:495,modifiability,scal,scaling,495,"sc.pp.filter don't filter the same number between sc.1.3.7 and sc.1.4; Hi all,. I have updated my scanpy to version 1.4 (was working on 1.3.7 before) and did not get the same filtering output using sc.pp.filter_ working with the same input dataset (10X). By running in sc1.3.7: sc.pp.filter_genes(adata, min_counts=2). -> 267 genes were filtered out and I was able to follow up on my analysis until the end. However, after switching to the new version, I could not get any filtering anymore. By scaling up, the first filtering I got was with a min of counts of 4 (sc.pp.filter_genes(adata, min_counts=4)). ""filtered out 655 genes that are detected in less than 4 counts"". Not sure what is going on there and which setting I should use then. Any feedback will be more than appreciated. Thank you.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/501
https://github.com/scverse/scanpy/issues/501:87,safety,updat,updated,87,"sc.pp.filter don't filter the same number between sc.1.3.7 and sc.1.4; Hi all,. I have updated my scanpy to version 1.4 (was working on 1.3.7 before) and did not get the same filtering output using sc.pp.filter_ working with the same input dataset (10X). By running in sc1.3.7: sc.pp.filter_genes(adata, min_counts=2). -> 267 genes were filtered out and I was able to follow up on my analysis until the end. However, after switching to the new version, I could not get any filtering anymore. By scaling up, the first filtering I got was with a min of counts of 4 (sc.pp.filter_genes(adata, min_counts=4)). ""filtered out 655 genes that are detected in less than 4 counts"". Not sure what is going on there and which setting I should use then. Any feedback will be more than appreciated. Thank you.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/501
https://github.com/scverse/scanpy/issues/501:234,safety,input,input,234,"sc.pp.filter don't filter the same number between sc.1.3.7 and sc.1.4; Hi all,. I have updated my scanpy to version 1.4 (was working on 1.3.7 before) and did not get the same filtering output using sc.pp.filter_ working with the same input dataset (10X). By running in sc1.3.7: sc.pp.filter_genes(adata, min_counts=2). -> 267 genes were filtered out and I was able to follow up on my analysis until the end. However, after switching to the new version, I could not get any filtering anymore. By scaling up, the first filtering I got was with a min of counts of 4 (sc.pp.filter_genes(adata, min_counts=4)). ""filtered out 655 genes that are detected in less than 4 counts"". Not sure what is going on there and which setting I should use then. Any feedback will be more than appreciated. Thank you.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/501
https://github.com/scverse/scanpy/issues/501:639,safety,detect,detected,639,"sc.pp.filter don't filter the same number between sc.1.3.7 and sc.1.4; Hi all,. I have updated my scanpy to version 1.4 (was working on 1.3.7 before) and did not get the same filtering output using sc.pp.filter_ working with the same input dataset (10X). By running in sc1.3.7: sc.pp.filter_genes(adata, min_counts=2). -> 267 genes were filtered out and I was able to follow up on my analysis until the end. However, after switching to the new version, I could not get any filtering anymore. By scaling up, the first filtering I got was with a min of counts of 4 (sc.pp.filter_genes(adata, min_counts=4)). ""filtered out 655 genes that are detected in less than 4 counts"". Not sure what is going on there and which setting I should use then. Any feedback will be more than appreciated. Thank you.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/501
https://github.com/scverse/scanpy/issues/501:87,security,updat,updated,87,"sc.pp.filter don't filter the same number between sc.1.3.7 and sc.1.4; Hi all,. I have updated my scanpy to version 1.4 (was working on 1.3.7 before) and did not get the same filtering output using sc.pp.filter_ working with the same input dataset (10X). By running in sc1.3.7: sc.pp.filter_genes(adata, min_counts=2). -> 267 genes were filtered out and I was able to follow up on my analysis until the end. However, after switching to the new version, I could not get any filtering anymore. By scaling up, the first filtering I got was with a min of counts of 4 (sc.pp.filter_genes(adata, min_counts=4)). ""filtered out 655 genes that are detected in less than 4 counts"". Not sure what is going on there and which setting I should use then. Any feedback will be more than appreciated. Thank you.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/501
https://github.com/scverse/scanpy/issues/501:639,security,detect,detected,639,"sc.pp.filter don't filter the same number between sc.1.3.7 and sc.1.4; Hi all,. I have updated my scanpy to version 1.4 (was working on 1.3.7 before) and did not get the same filtering output using sc.pp.filter_ working with the same input dataset (10X). By running in sc1.3.7: sc.pp.filter_genes(adata, min_counts=2). -> 267 genes were filtered out and I was able to follow up on my analysis until the end. However, after switching to the new version, I could not get any filtering anymore. By scaling up, the first filtering I got was with a min of counts of 4 (sc.pp.filter_genes(adata, min_counts=4)). ""filtered out 655 genes that are detected in less than 4 counts"". Not sure what is going on there and which setting I should use then. Any feedback will be more than appreciated. Thank you.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/501
https://github.com/scverse/scanpy/issues/501:234,usability,input,input,234,"sc.pp.filter don't filter the same number between sc.1.3.7 and sc.1.4; Hi all,. I have updated my scanpy to version 1.4 (was working on 1.3.7 before) and did not get the same filtering output using sc.pp.filter_ working with the same input dataset (10X). By running in sc1.3.7: sc.pp.filter_genes(adata, min_counts=2). -> 267 genes were filtered out and I was able to follow up on my analysis until the end. However, after switching to the new version, I could not get any filtering anymore. By scaling up, the first filtering I got was with a min of counts of 4 (sc.pp.filter_genes(adata, min_counts=4)). ""filtered out 655 genes that are detected in less than 4 counts"". Not sure what is going on there and which setting I should use then. Any feedback will be more than appreciated. Thank you.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/501
https://github.com/scverse/scanpy/issues/501:745,usability,feedback,feedback,745,"sc.pp.filter don't filter the same number between sc.1.3.7 and sc.1.4; Hi all,. I have updated my scanpy to version 1.4 (was working on 1.3.7 before) and did not get the same filtering output using sc.pp.filter_ working with the same input dataset (10X). By running in sc1.3.7: sc.pp.filter_genes(adata, min_counts=2). -> 267 genes were filtered out and I was able to follow up on my analysis until the end. However, after switching to the new version, I could not get any filtering anymore. By scaling up, the first filtering I got was with a min of counts of 4 (sc.pp.filter_genes(adata, min_counts=4)). ""filtered out 655 genes that are detected in less than 4 counts"". Not sure what is going on there and which setting I should use then. Any feedback will be more than appreciated. Thank you.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/501
https://github.com/scverse/scanpy/issues/502:54,availability,error,error,54,"sc.pl.rank_genes_groups_dotplot ; I get the following error when I tun dotplot:. ```pytb. ValueError Traceback (most recent call last). <ipython-input-54-afab88c299fa> in <module>(). ----> 1 sc.pl.rank_genes_groups_dotplot(vitro,['MYL2'], groupby='louvain'). /projects/sysbio/projects/czi/immune/anaconda2/envs/py36/lib/python3.6/site-packages/scanpy/plotting/_tools/__init__.py in rank_genes_groups_dotplot(adata, groups, n_genes, groupby, key, show, save, **kwds). 409 . 410 _rank_genes_groups_plot(adata, plot_type='dotplot', groups=groups, n_genes=n_genes,. --> 411 groupby=groupby, key=key, show=show, save=save, **kwds). 412 . 413 . /projects/sysbio/projects/czi/immune/anaconda2/envs/py36/lib/python3.6/site-packages/scanpy/plotting/_tools/__init__.py in _rank_genes_groups_plot(adata, plot_type, groups, n_genes, groupby, key, show, save, **kwds). 291 . 292 # sum(list, []) is used to flatten the gene list. --> 293 gene_names = sum([list(adata.uns[key]['names'][x][:n_genes]) for x in group_names], []). 294 . 295 if plot_type == 'dotplot':. /projects/sysbio/projects/czi/immune/anaconda2/envs/py36/lib/python3.6/site-packages/scanpy/plotting/_tools/__init__.py in <listcomp>(.0). 291 . 292 # sum(list, []) is used to flatten the gene list. --> 293 gene_names = sum([list(adata.uns[key]['names'][x][:n_genes]) for x in group_names], []). 294 . 295 if plot_type == 'dotplot':. ValueError: no field of name MYL2. ```. Do we need to store marker genes within the adata object?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/502
https://github.com/scverse/scanpy/issues/502:172,deployability,modul,module,172,"sc.pl.rank_genes_groups_dotplot ; I get the following error when I tun dotplot:. ```pytb. ValueError Traceback (most recent call last). <ipython-input-54-afab88c299fa> in <module>(). ----> 1 sc.pl.rank_genes_groups_dotplot(vitro,['MYL2'], groupby='louvain'). /projects/sysbio/projects/czi/immune/anaconda2/envs/py36/lib/python3.6/site-packages/scanpy/plotting/_tools/__init__.py in rank_genes_groups_dotplot(adata, groups, n_genes, groupby, key, show, save, **kwds). 409 . 410 _rank_genes_groups_plot(adata, plot_type='dotplot', groups=groups, n_genes=n_genes,. --> 411 groupby=groupby, key=key, show=show, save=save, **kwds). 412 . 413 . /projects/sysbio/projects/czi/immune/anaconda2/envs/py36/lib/python3.6/site-packages/scanpy/plotting/_tools/__init__.py in _rank_genes_groups_plot(adata, plot_type, groups, n_genes, groupby, key, show, save, **kwds). 291 . 292 # sum(list, []) is used to flatten the gene list. --> 293 gene_names = sum([list(adata.uns[key]['names'][x][:n_genes]) for x in group_names], []). 294 . 295 if plot_type == 'dotplot':. /projects/sysbio/projects/czi/immune/anaconda2/envs/py36/lib/python3.6/site-packages/scanpy/plotting/_tools/__init__.py in <listcomp>(.0). 291 . 292 # sum(list, []) is used to flatten the gene list. --> 293 gene_names = sum([list(adata.uns[key]['names'][x][:n_genes]) for x in group_names], []). 294 . 295 if plot_type == 'dotplot':. ValueError: no field of name MYL2. ```. Do we need to store marker genes within the adata object?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/502
https://github.com/scverse/scanpy/issues/502:172,modifiability,modul,module,172,"sc.pl.rank_genes_groups_dotplot ; I get the following error when I tun dotplot:. ```pytb. ValueError Traceback (most recent call last). <ipython-input-54-afab88c299fa> in <module>(). ----> 1 sc.pl.rank_genes_groups_dotplot(vitro,['MYL2'], groupby='louvain'). /projects/sysbio/projects/czi/immune/anaconda2/envs/py36/lib/python3.6/site-packages/scanpy/plotting/_tools/__init__.py in rank_genes_groups_dotplot(adata, groups, n_genes, groupby, key, show, save, **kwds). 409 . 410 _rank_genes_groups_plot(adata, plot_type='dotplot', groups=groups, n_genes=n_genes,. --> 411 groupby=groupby, key=key, show=show, save=save, **kwds). 412 . 413 . /projects/sysbio/projects/czi/immune/anaconda2/envs/py36/lib/python3.6/site-packages/scanpy/plotting/_tools/__init__.py in _rank_genes_groups_plot(adata, plot_type, groups, n_genes, groupby, key, show, save, **kwds). 291 . 292 # sum(list, []) is used to flatten the gene list. --> 293 gene_names = sum([list(adata.uns[key]['names'][x][:n_genes]) for x in group_names], []). 294 . 295 if plot_type == 'dotplot':. /projects/sysbio/projects/czi/immune/anaconda2/envs/py36/lib/python3.6/site-packages/scanpy/plotting/_tools/__init__.py in <listcomp>(.0). 291 . 292 # sum(list, []) is used to flatten the gene list. --> 293 gene_names = sum([list(adata.uns[key]['names'][x][:n_genes]) for x in group_names], []). 294 . 295 if plot_type == 'dotplot':. ValueError: no field of name MYL2. ```. Do we need to store marker genes within the adata object?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/502
https://github.com/scverse/scanpy/issues/502:335,modifiability,pac,packages,335,"sc.pl.rank_genes_groups_dotplot ; I get the following error when I tun dotplot:. ```pytb. ValueError Traceback (most recent call last). <ipython-input-54-afab88c299fa> in <module>(). ----> 1 sc.pl.rank_genes_groups_dotplot(vitro,['MYL2'], groupby='louvain'). /projects/sysbio/projects/czi/immune/anaconda2/envs/py36/lib/python3.6/site-packages/scanpy/plotting/_tools/__init__.py in rank_genes_groups_dotplot(adata, groups, n_genes, groupby, key, show, save, **kwds). 409 . 410 _rank_genes_groups_plot(adata, plot_type='dotplot', groups=groups, n_genes=n_genes,. --> 411 groupby=groupby, key=key, show=show, save=save, **kwds). 412 . 413 . /projects/sysbio/projects/czi/immune/anaconda2/envs/py36/lib/python3.6/site-packages/scanpy/plotting/_tools/__init__.py in _rank_genes_groups_plot(adata, plot_type, groups, n_genes, groupby, key, show, save, **kwds). 291 . 292 # sum(list, []) is used to flatten the gene list. --> 293 gene_names = sum([list(adata.uns[key]['names'][x][:n_genes]) for x in group_names], []). 294 . 295 if plot_type == 'dotplot':. /projects/sysbio/projects/czi/immune/anaconda2/envs/py36/lib/python3.6/site-packages/scanpy/plotting/_tools/__init__.py in <listcomp>(.0). 291 . 292 # sum(list, []) is used to flatten the gene list. --> 293 gene_names = sum([list(adata.uns[key]['names'][x][:n_genes]) for x in group_names], []). 294 . 295 if plot_type == 'dotplot':. ValueError: no field of name MYL2. ```. Do we need to store marker genes within the adata object?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/502
https://github.com/scverse/scanpy/issues/502:715,modifiability,pac,packages,715,"sc.pl.rank_genes_groups_dotplot ; I get the following error when I tun dotplot:. ```pytb. ValueError Traceback (most recent call last). <ipython-input-54-afab88c299fa> in <module>(). ----> 1 sc.pl.rank_genes_groups_dotplot(vitro,['MYL2'], groupby='louvain'). /projects/sysbio/projects/czi/immune/anaconda2/envs/py36/lib/python3.6/site-packages/scanpy/plotting/_tools/__init__.py in rank_genes_groups_dotplot(adata, groups, n_genes, groupby, key, show, save, **kwds). 409 . 410 _rank_genes_groups_plot(adata, plot_type='dotplot', groups=groups, n_genes=n_genes,. --> 411 groupby=groupby, key=key, show=show, save=save, **kwds). 412 . 413 . /projects/sysbio/projects/czi/immune/anaconda2/envs/py36/lib/python3.6/site-packages/scanpy/plotting/_tools/__init__.py in _rank_genes_groups_plot(adata, plot_type, groups, n_genes, groupby, key, show, save, **kwds). 291 . 292 # sum(list, []) is used to flatten the gene list. --> 293 gene_names = sum([list(adata.uns[key]['names'][x][:n_genes]) for x in group_names], []). 294 . 295 if plot_type == 'dotplot':. /projects/sysbio/projects/czi/immune/anaconda2/envs/py36/lib/python3.6/site-packages/scanpy/plotting/_tools/__init__.py in <listcomp>(.0). 291 . 292 # sum(list, []) is used to flatten the gene list. --> 293 gene_names = sum([list(adata.uns[key]['names'][x][:n_genes]) for x in group_names], []). 294 . 295 if plot_type == 'dotplot':. ValueError: no field of name MYL2. ```. Do we need to store marker genes within the adata object?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/502
https://github.com/scverse/scanpy/issues/502:1127,modifiability,pac,packages,1127,"sc.pl.rank_genes_groups_dotplot ; I get the following error when I tun dotplot:. ```pytb. ValueError Traceback (most recent call last). <ipython-input-54-afab88c299fa> in <module>(). ----> 1 sc.pl.rank_genes_groups_dotplot(vitro,['MYL2'], groupby='louvain'). /projects/sysbio/projects/czi/immune/anaconda2/envs/py36/lib/python3.6/site-packages/scanpy/plotting/_tools/__init__.py in rank_genes_groups_dotplot(adata, groups, n_genes, groupby, key, show, save, **kwds). 409 . 410 _rank_genes_groups_plot(adata, plot_type='dotplot', groups=groups, n_genes=n_genes,. --> 411 groupby=groupby, key=key, show=show, save=save, **kwds). 412 . 413 . /projects/sysbio/projects/czi/immune/anaconda2/envs/py36/lib/python3.6/site-packages/scanpy/plotting/_tools/__init__.py in _rank_genes_groups_plot(adata, plot_type, groups, n_genes, groupby, key, show, save, **kwds). 291 . 292 # sum(list, []) is used to flatten the gene list. --> 293 gene_names = sum([list(adata.uns[key]['names'][x][:n_genes]) for x in group_names], []). 294 . 295 if plot_type == 'dotplot':. /projects/sysbio/projects/czi/immune/anaconda2/envs/py36/lib/python3.6/site-packages/scanpy/plotting/_tools/__init__.py in <listcomp>(.0). 291 . 292 # sum(list, []) is used to flatten the gene list. --> 293 gene_names = sum([list(adata.uns[key]['names'][x][:n_genes]) for x in group_names], []). 294 . 295 if plot_type == 'dotplot':. ValueError: no field of name MYL2. ```. Do we need to store marker genes within the adata object?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/502
https://github.com/scverse/scanpy/issues/502:54,performance,error,error,54,"sc.pl.rank_genes_groups_dotplot ; I get the following error when I tun dotplot:. ```pytb. ValueError Traceback (most recent call last). <ipython-input-54-afab88c299fa> in <module>(). ----> 1 sc.pl.rank_genes_groups_dotplot(vitro,['MYL2'], groupby='louvain'). /projects/sysbio/projects/czi/immune/anaconda2/envs/py36/lib/python3.6/site-packages/scanpy/plotting/_tools/__init__.py in rank_genes_groups_dotplot(adata, groups, n_genes, groupby, key, show, save, **kwds). 409 . 410 _rank_genes_groups_plot(adata, plot_type='dotplot', groups=groups, n_genes=n_genes,. --> 411 groupby=groupby, key=key, show=show, save=save, **kwds). 412 . 413 . /projects/sysbio/projects/czi/immune/anaconda2/envs/py36/lib/python3.6/site-packages/scanpy/plotting/_tools/__init__.py in _rank_genes_groups_plot(adata, plot_type, groups, n_genes, groupby, key, show, save, **kwds). 291 . 292 # sum(list, []) is used to flatten the gene list. --> 293 gene_names = sum([list(adata.uns[key]['names'][x][:n_genes]) for x in group_names], []). 294 . 295 if plot_type == 'dotplot':. /projects/sysbio/projects/czi/immune/anaconda2/envs/py36/lib/python3.6/site-packages/scanpy/plotting/_tools/__init__.py in <listcomp>(.0). 291 . 292 # sum(list, []) is used to flatten the gene list. --> 293 gene_names = sum([list(adata.uns[key]['names'][x][:n_genes]) for x in group_names], []). 294 . 295 if plot_type == 'dotplot':. ValueError: no field of name MYL2. ```. Do we need to store marker genes within the adata object?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/502
https://github.com/scverse/scanpy/issues/502:54,safety,error,error,54,"sc.pl.rank_genes_groups_dotplot ; I get the following error when I tun dotplot:. ```pytb. ValueError Traceback (most recent call last). <ipython-input-54-afab88c299fa> in <module>(). ----> 1 sc.pl.rank_genes_groups_dotplot(vitro,['MYL2'], groupby='louvain'). /projects/sysbio/projects/czi/immune/anaconda2/envs/py36/lib/python3.6/site-packages/scanpy/plotting/_tools/__init__.py in rank_genes_groups_dotplot(adata, groups, n_genes, groupby, key, show, save, **kwds). 409 . 410 _rank_genes_groups_plot(adata, plot_type='dotplot', groups=groups, n_genes=n_genes,. --> 411 groupby=groupby, key=key, show=show, save=save, **kwds). 412 . 413 . /projects/sysbio/projects/czi/immune/anaconda2/envs/py36/lib/python3.6/site-packages/scanpy/plotting/_tools/__init__.py in _rank_genes_groups_plot(adata, plot_type, groups, n_genes, groupby, key, show, save, **kwds). 291 . 292 # sum(list, []) is used to flatten the gene list. --> 293 gene_names = sum([list(adata.uns[key]['names'][x][:n_genes]) for x in group_names], []). 294 . 295 if plot_type == 'dotplot':. /projects/sysbio/projects/czi/immune/anaconda2/envs/py36/lib/python3.6/site-packages/scanpy/plotting/_tools/__init__.py in <listcomp>(.0). 291 . 292 # sum(list, []) is used to flatten the gene list. --> 293 gene_names = sum([list(adata.uns[key]['names'][x][:n_genes]) for x in group_names], []). 294 . 295 if plot_type == 'dotplot':. ValueError: no field of name MYL2. ```. Do we need to store marker genes within the adata object?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/502
https://github.com/scverse/scanpy/issues/502:145,safety,input,input-,145,"sc.pl.rank_genes_groups_dotplot ; I get the following error when I tun dotplot:. ```pytb. ValueError Traceback (most recent call last). <ipython-input-54-afab88c299fa> in <module>(). ----> 1 sc.pl.rank_genes_groups_dotplot(vitro,['MYL2'], groupby='louvain'). /projects/sysbio/projects/czi/immune/anaconda2/envs/py36/lib/python3.6/site-packages/scanpy/plotting/_tools/__init__.py in rank_genes_groups_dotplot(adata, groups, n_genes, groupby, key, show, save, **kwds). 409 . 410 _rank_genes_groups_plot(adata, plot_type='dotplot', groups=groups, n_genes=n_genes,. --> 411 groupby=groupby, key=key, show=show, save=save, **kwds). 412 . 413 . /projects/sysbio/projects/czi/immune/anaconda2/envs/py36/lib/python3.6/site-packages/scanpy/plotting/_tools/__init__.py in _rank_genes_groups_plot(adata, plot_type, groups, n_genes, groupby, key, show, save, **kwds). 291 . 292 # sum(list, []) is used to flatten the gene list. --> 293 gene_names = sum([list(adata.uns[key]['names'][x][:n_genes]) for x in group_names], []). 294 . 295 if plot_type == 'dotplot':. /projects/sysbio/projects/czi/immune/anaconda2/envs/py36/lib/python3.6/site-packages/scanpy/plotting/_tools/__init__.py in <listcomp>(.0). 291 . 292 # sum(list, []) is used to flatten the gene list. --> 293 gene_names = sum([list(adata.uns[key]['names'][x][:n_genes]) for x in group_names], []). 294 . 295 if plot_type == 'dotplot':. ValueError: no field of name MYL2. ```. Do we need to store marker genes within the adata object?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/502
https://github.com/scverse/scanpy/issues/502:172,safety,modul,module,172,"sc.pl.rank_genes_groups_dotplot ; I get the following error when I tun dotplot:. ```pytb. ValueError Traceback (most recent call last). <ipython-input-54-afab88c299fa> in <module>(). ----> 1 sc.pl.rank_genes_groups_dotplot(vitro,['MYL2'], groupby='louvain'). /projects/sysbio/projects/czi/immune/anaconda2/envs/py36/lib/python3.6/site-packages/scanpy/plotting/_tools/__init__.py in rank_genes_groups_dotplot(adata, groups, n_genes, groupby, key, show, save, **kwds). 409 . 410 _rank_genes_groups_plot(adata, plot_type='dotplot', groups=groups, n_genes=n_genes,. --> 411 groupby=groupby, key=key, show=show, save=save, **kwds). 412 . 413 . /projects/sysbio/projects/czi/immune/anaconda2/envs/py36/lib/python3.6/site-packages/scanpy/plotting/_tools/__init__.py in _rank_genes_groups_plot(adata, plot_type, groups, n_genes, groupby, key, show, save, **kwds). 291 . 292 # sum(list, []) is used to flatten the gene list. --> 293 gene_names = sum([list(adata.uns[key]['names'][x][:n_genes]) for x in group_names], []). 294 . 295 if plot_type == 'dotplot':. /projects/sysbio/projects/czi/immune/anaconda2/envs/py36/lib/python3.6/site-packages/scanpy/plotting/_tools/__init__.py in <listcomp>(.0). 291 . 292 # sum(list, []) is used to flatten the gene list. --> 293 gene_names = sum([list(adata.uns[key]['names'][x][:n_genes]) for x in group_names], []). 294 . 295 if plot_type == 'dotplot':. ValueError: no field of name MYL2. ```. Do we need to store marker genes within the adata object?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/502
https://github.com/scverse/scanpy/issues/502:101,testability,Trace,Traceback,101,"sc.pl.rank_genes_groups_dotplot ; I get the following error when I tun dotplot:. ```pytb. ValueError Traceback (most recent call last). <ipython-input-54-afab88c299fa> in <module>(). ----> 1 sc.pl.rank_genes_groups_dotplot(vitro,['MYL2'], groupby='louvain'). /projects/sysbio/projects/czi/immune/anaconda2/envs/py36/lib/python3.6/site-packages/scanpy/plotting/_tools/__init__.py in rank_genes_groups_dotplot(adata, groups, n_genes, groupby, key, show, save, **kwds). 409 . 410 _rank_genes_groups_plot(adata, plot_type='dotplot', groups=groups, n_genes=n_genes,. --> 411 groupby=groupby, key=key, show=show, save=save, **kwds). 412 . 413 . /projects/sysbio/projects/czi/immune/anaconda2/envs/py36/lib/python3.6/site-packages/scanpy/plotting/_tools/__init__.py in _rank_genes_groups_plot(adata, plot_type, groups, n_genes, groupby, key, show, save, **kwds). 291 . 292 # sum(list, []) is used to flatten the gene list. --> 293 gene_names = sum([list(adata.uns[key]['names'][x][:n_genes]) for x in group_names], []). 294 . 295 if plot_type == 'dotplot':. /projects/sysbio/projects/czi/immune/anaconda2/envs/py36/lib/python3.6/site-packages/scanpy/plotting/_tools/__init__.py in <listcomp>(.0). 291 . 292 # sum(list, []) is used to flatten the gene list. --> 293 gene_names = sum([list(adata.uns[key]['names'][x][:n_genes]) for x in group_names], []). 294 . 295 if plot_type == 'dotplot':. ValueError: no field of name MYL2. ```. Do we need to store marker genes within the adata object?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/502
https://github.com/scverse/scanpy/issues/502:54,usability,error,error,54,"sc.pl.rank_genes_groups_dotplot ; I get the following error when I tun dotplot:. ```pytb. ValueError Traceback (most recent call last). <ipython-input-54-afab88c299fa> in <module>(). ----> 1 sc.pl.rank_genes_groups_dotplot(vitro,['MYL2'], groupby='louvain'). /projects/sysbio/projects/czi/immune/anaconda2/envs/py36/lib/python3.6/site-packages/scanpy/plotting/_tools/__init__.py in rank_genes_groups_dotplot(adata, groups, n_genes, groupby, key, show, save, **kwds). 409 . 410 _rank_genes_groups_plot(adata, plot_type='dotplot', groups=groups, n_genes=n_genes,. --> 411 groupby=groupby, key=key, show=show, save=save, **kwds). 412 . 413 . /projects/sysbio/projects/czi/immune/anaconda2/envs/py36/lib/python3.6/site-packages/scanpy/plotting/_tools/__init__.py in _rank_genes_groups_plot(adata, plot_type, groups, n_genes, groupby, key, show, save, **kwds). 291 . 292 # sum(list, []) is used to flatten the gene list. --> 293 gene_names = sum([list(adata.uns[key]['names'][x][:n_genes]) for x in group_names], []). 294 . 295 if plot_type == 'dotplot':. /projects/sysbio/projects/czi/immune/anaconda2/envs/py36/lib/python3.6/site-packages/scanpy/plotting/_tools/__init__.py in <listcomp>(.0). 291 . 292 # sum(list, []) is used to flatten the gene list. --> 293 gene_names = sum([list(adata.uns[key]['names'][x][:n_genes]) for x in group_names], []). 294 . 295 if plot_type == 'dotplot':. ValueError: no field of name MYL2. ```. Do we need to store marker genes within the adata object?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/502
https://github.com/scverse/scanpy/issues/502:145,usability,input,input-,145,"sc.pl.rank_genes_groups_dotplot ; I get the following error when I tun dotplot:. ```pytb. ValueError Traceback (most recent call last). <ipython-input-54-afab88c299fa> in <module>(). ----> 1 sc.pl.rank_genes_groups_dotplot(vitro,['MYL2'], groupby='louvain'). /projects/sysbio/projects/czi/immune/anaconda2/envs/py36/lib/python3.6/site-packages/scanpy/plotting/_tools/__init__.py in rank_genes_groups_dotplot(adata, groups, n_genes, groupby, key, show, save, **kwds). 409 . 410 _rank_genes_groups_plot(adata, plot_type='dotplot', groups=groups, n_genes=n_genes,. --> 411 groupby=groupby, key=key, show=show, save=save, **kwds). 412 . 413 . /projects/sysbio/projects/czi/immune/anaconda2/envs/py36/lib/python3.6/site-packages/scanpy/plotting/_tools/__init__.py in _rank_genes_groups_plot(adata, plot_type, groups, n_genes, groupby, key, show, save, **kwds). 291 . 292 # sum(list, []) is used to flatten the gene list. --> 293 gene_names = sum([list(adata.uns[key]['names'][x][:n_genes]) for x in group_names], []). 294 . 295 if plot_type == 'dotplot':. /projects/sysbio/projects/czi/immune/anaconda2/envs/py36/lib/python3.6/site-packages/scanpy/plotting/_tools/__init__.py in <listcomp>(.0). 291 . 292 # sum(list, []) is used to flatten the gene list. --> 293 gene_names = sum([list(adata.uns[key]['names'][x][:n_genes]) for x in group_names], []). 294 . 295 if plot_type == 'dotplot':. ValueError: no field of name MYL2. ```. Do we need to store marker genes within the adata object?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/502
https://github.com/scverse/scanpy/pull/503:100,integrability,wrap,wrapper,100,Harmony framework for connecting scRNA-seq data from discrete time points; @falexwolf. I'm adding a wrapper for Harmony by [Setty et al. (2018)](https://doi.org/10.1101/471078). Please let me know if you have any comments,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/503
https://github.com/scverse/scanpy/pull/503:100,interoperability,wrapper,wrapper,100,Harmony framework for connecting scRNA-seq data from discrete time points; @falexwolf. I'm adding a wrapper for Harmony by [Setty et al. (2018)](https://doi.org/10.1101/471078). Please let me know if you have any comments,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/503
https://github.com/scverse/scanpy/pull/503:62,performance,time,time,62,Harmony framework for connecting scRNA-seq data from discrete time points; @falexwolf. I'm adding a wrapper for Harmony by [Setty et al. (2018)](https://doi.org/10.1101/471078). Please let me know if you have any comments,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/503
https://github.com/scverse/scanpy/issues/504:10,availability,error,error,10,"sc.tl.pca error: no field of name X_pca; I'm using scanpy==1.4 anndata==0.6.18 numpy==1.15.4 scipy==1.2.1 pandas==0.24.1 scikit-learn==0.20.2 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 on Mac 10.12.6 with python 3.7.1. I'm trying to do a pca on a annData object. `sc.tl.pca(adata, svd_solver='arpack')`. and get the following error even after restarting the jupyter notebook:. ```. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-26-eb775d53dbfd> in <module>. ----> 1 sc.tl.pca(adata, svd_solver='arpack'). ~/miniconda3/lib/python3.7/site-packages/scanpy/preprocessing/_simple.py in pca(data, n_comps, zero_center, svd_solver, random_state, return_info, use_highly_variable, dtype, copy, chunked, chunk_size). 504 . 505 if data_is_AnnData:. --> 506 adata.obsm['X_pca'] = X_pca. 507 if use_highly_variable:. 508 adata.varm['PCs'] = np.zeros(shape=(adata.n_vars, n_comps)). ValueError: no field of name X_pca. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/504
https://github.com/scverse/scanpy/issues/504:338,availability,error,error,338,"sc.tl.pca error: no field of name X_pca; I'm using scanpy==1.4 anndata==0.6.18 numpy==1.15.4 scipy==1.2.1 pandas==0.24.1 scikit-learn==0.20.2 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 on Mac 10.12.6 with python 3.7.1. I'm trying to do a pca on a annData object. `sc.tl.pca(adata, svd_solver='arpack')`. and get the following error even after restarting the jupyter notebook:. ```. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-26-eb775d53dbfd> in <module>. ----> 1 sc.tl.pca(adata, svd_solver='arpack'). ~/miniconda3/lib/python3.7/site-packages/scanpy/preprocessing/_simple.py in pca(data, n_comps, zero_center, svd_solver, random_state, return_info, use_highly_variable, dtype, copy, chunked, chunk_size). 504 . 505 if data_is_AnnData:. --> 506 adata.obsm['X_pca'] = X_pca. 507 if use_highly_variable:. 508 adata.varm['PCs'] = np.zeros(shape=(adata.n_vars, n_comps)). ValueError: no field of name X_pca. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/504
https://github.com/scverse/scanpy/issues/504:553,deployability,modul,module,553,"sc.tl.pca error: no field of name X_pca; I'm using scanpy==1.4 anndata==0.6.18 numpy==1.15.4 scipy==1.2.1 pandas==0.24.1 scikit-learn==0.20.2 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 on Mac 10.12.6 with python 3.7.1. I'm trying to do a pca on a annData object. `sc.tl.pca(adata, svd_solver='arpack')`. and get the following error even after restarting the jupyter notebook:. ```. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-26-eb775d53dbfd> in <module>. ----> 1 sc.tl.pca(adata, svd_solver='arpack'). ~/miniconda3/lib/python3.7/site-packages/scanpy/preprocessing/_simple.py in pca(data, n_comps, zero_center, svd_solver, random_state, return_info, use_highly_variable, dtype, copy, chunked, chunk_size). 504 . 505 if data_is_AnnData:. --> 506 adata.obsm['X_pca'] = X_pca. 507 if use_highly_variable:. 508 adata.varm['PCs'] = np.zeros(shape=(adata.n_vars, n_comps)). ValueError: no field of name X_pca. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/504
https://github.com/scverse/scanpy/issues/504:553,modifiability,modul,module,553,"sc.tl.pca error: no field of name X_pca; I'm using scanpy==1.4 anndata==0.6.18 numpy==1.15.4 scipy==1.2.1 pandas==0.24.1 scikit-learn==0.20.2 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 on Mac 10.12.6 with python 3.7.1. I'm trying to do a pca on a annData object. `sc.tl.pca(adata, svd_solver='arpack')`. and get the following error even after restarting the jupyter notebook:. ```. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-26-eb775d53dbfd> in <module>. ----> 1 sc.tl.pca(adata, svd_solver='arpack'). ~/miniconda3/lib/python3.7/site-packages/scanpy/preprocessing/_simple.py in pca(data, n_comps, zero_center, svd_solver, random_state, return_info, use_highly_variable, dtype, copy, chunked, chunk_size). 504 . 505 if data_is_AnnData:. --> 506 adata.obsm['X_pca'] = X_pca. 507 if use_highly_variable:. 508 adata.varm['PCs'] = np.zeros(shape=(adata.n_vars, n_comps)). ValueError: no field of name X_pca. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/504
https://github.com/scverse/scanpy/issues/504:641,modifiability,pac,packages,641,"sc.tl.pca error: no field of name X_pca; I'm using scanpy==1.4 anndata==0.6.18 numpy==1.15.4 scipy==1.2.1 pandas==0.24.1 scikit-learn==0.20.2 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 on Mac 10.12.6 with python 3.7.1. I'm trying to do a pca on a annData object. `sc.tl.pca(adata, svd_solver='arpack')`. and get the following error even after restarting the jupyter notebook:. ```. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-26-eb775d53dbfd> in <module>. ----> 1 sc.tl.pca(adata, svd_solver='arpack'). ~/miniconda3/lib/python3.7/site-packages/scanpy/preprocessing/_simple.py in pca(data, n_comps, zero_center, svd_solver, random_state, return_info, use_highly_variable, dtype, copy, chunked, chunk_size). 504 . 505 if data_is_AnnData:. --> 506 adata.obsm['X_pca'] = X_pca. 507 if use_highly_variable:. 508 adata.varm['PCs'] = np.zeros(shape=(adata.n_vars, n_comps)). ValueError: no field of name X_pca. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/504
https://github.com/scverse/scanpy/issues/504:10,performance,error,error,10,"sc.tl.pca error: no field of name X_pca; I'm using scanpy==1.4 anndata==0.6.18 numpy==1.15.4 scipy==1.2.1 pandas==0.24.1 scikit-learn==0.20.2 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 on Mac 10.12.6 with python 3.7.1. I'm trying to do a pca on a annData object. `sc.tl.pca(adata, svd_solver='arpack')`. and get the following error even after restarting the jupyter notebook:. ```. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-26-eb775d53dbfd> in <module>. ----> 1 sc.tl.pca(adata, svd_solver='arpack'). ~/miniconda3/lib/python3.7/site-packages/scanpy/preprocessing/_simple.py in pca(data, n_comps, zero_center, svd_solver, random_state, return_info, use_highly_variable, dtype, copy, chunked, chunk_size). 504 . 505 if data_is_AnnData:. --> 506 adata.obsm['X_pca'] = X_pca. 507 if use_highly_variable:. 508 adata.varm['PCs'] = np.zeros(shape=(adata.n_vars, n_comps)). ValueError: no field of name X_pca. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/504
https://github.com/scverse/scanpy/issues/504:338,performance,error,error,338,"sc.tl.pca error: no field of name X_pca; I'm using scanpy==1.4 anndata==0.6.18 numpy==1.15.4 scipy==1.2.1 pandas==0.24.1 scikit-learn==0.20.2 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 on Mac 10.12.6 with python 3.7.1. I'm trying to do a pca on a annData object. `sc.tl.pca(adata, svd_solver='arpack')`. and get the following error even after restarting the jupyter notebook:. ```. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-26-eb775d53dbfd> in <module>. ----> 1 sc.tl.pca(adata, svd_solver='arpack'). ~/miniconda3/lib/python3.7/site-packages/scanpy/preprocessing/_simple.py in pca(data, n_comps, zero_center, svd_solver, random_state, return_info, use_highly_variable, dtype, copy, chunked, chunk_size). 504 . 505 if data_is_AnnData:. --> 506 adata.obsm['X_pca'] = X_pca. 507 if use_highly_variable:. 508 adata.varm['PCs'] = np.zeros(shape=(adata.n_vars, n_comps)). ValueError: no field of name X_pca. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/504
https://github.com/scverse/scanpy/issues/504:10,safety,error,error,10,"sc.tl.pca error: no field of name X_pca; I'm using scanpy==1.4 anndata==0.6.18 numpy==1.15.4 scipy==1.2.1 pandas==0.24.1 scikit-learn==0.20.2 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 on Mac 10.12.6 with python 3.7.1. I'm trying to do a pca on a annData object. `sc.tl.pca(adata, svd_solver='arpack')`. and get the following error even after restarting the jupyter notebook:. ```. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-26-eb775d53dbfd> in <module>. ----> 1 sc.tl.pca(adata, svd_solver='arpack'). ~/miniconda3/lib/python3.7/site-packages/scanpy/preprocessing/_simple.py in pca(data, n_comps, zero_center, svd_solver, random_state, return_info, use_highly_variable, dtype, copy, chunked, chunk_size). 504 . 505 if data_is_AnnData:. --> 506 adata.obsm['X_pca'] = X_pca. 507 if use_highly_variable:. 508 adata.varm['PCs'] = np.zeros(shape=(adata.n_vars, n_comps)). ValueError: no field of name X_pca. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/504
https://github.com/scverse/scanpy/issues/504:338,safety,error,error,338,"sc.tl.pca error: no field of name X_pca; I'm using scanpy==1.4 anndata==0.6.18 numpy==1.15.4 scipy==1.2.1 pandas==0.24.1 scikit-learn==0.20.2 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 on Mac 10.12.6 with python 3.7.1. I'm trying to do a pca on a annData object. `sc.tl.pca(adata, svd_solver='arpack')`. and get the following error even after restarting the jupyter notebook:. ```. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-26-eb775d53dbfd> in <module>. ----> 1 sc.tl.pca(adata, svd_solver='arpack'). ~/miniconda3/lib/python3.7/site-packages/scanpy/preprocessing/_simple.py in pca(data, n_comps, zero_center, svd_solver, random_state, return_info, use_highly_variable, dtype, copy, chunked, chunk_size). 504 . 505 if data_is_AnnData:. --> 506 adata.obsm['X_pca'] = X_pca. 507 if use_highly_variable:. 508 adata.varm['PCs'] = np.zeros(shape=(adata.n_vars, n_comps)). ValueError: no field of name X_pca. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/504
https://github.com/scverse/scanpy/issues/504:526,safety,input,input-,526,"sc.tl.pca error: no field of name X_pca; I'm using scanpy==1.4 anndata==0.6.18 numpy==1.15.4 scipy==1.2.1 pandas==0.24.1 scikit-learn==0.20.2 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 on Mac 10.12.6 with python 3.7.1. I'm trying to do a pca on a annData object. `sc.tl.pca(adata, svd_solver='arpack')`. and get the following error even after restarting the jupyter notebook:. ```. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-26-eb775d53dbfd> in <module>. ----> 1 sc.tl.pca(adata, svd_solver='arpack'). ~/miniconda3/lib/python3.7/site-packages/scanpy/preprocessing/_simple.py in pca(data, n_comps, zero_center, svd_solver, random_state, return_info, use_highly_variable, dtype, copy, chunked, chunk_size). 504 . 505 if data_is_AnnData:. --> 506 adata.obsm['X_pca'] = X_pca. 507 if use_highly_variable:. 508 adata.varm['PCs'] = np.zeros(shape=(adata.n_vars, n_comps)). ValueError: no field of name X_pca. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/504
https://github.com/scverse/scanpy/issues/504:553,safety,modul,module,553,"sc.tl.pca error: no field of name X_pca; I'm using scanpy==1.4 anndata==0.6.18 numpy==1.15.4 scipy==1.2.1 pandas==0.24.1 scikit-learn==0.20.2 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 on Mac 10.12.6 with python 3.7.1. I'm trying to do a pca on a annData object. `sc.tl.pca(adata, svd_solver='arpack')`. and get the following error even after restarting the jupyter notebook:. ```. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-26-eb775d53dbfd> in <module>. ----> 1 sc.tl.pca(adata, svd_solver='arpack'). ~/miniconda3/lib/python3.7/site-packages/scanpy/preprocessing/_simple.py in pca(data, n_comps, zero_center, svd_solver, random_state, return_info, use_highly_variable, dtype, copy, chunked, chunk_size). 504 . 505 if data_is_AnnData:. --> 506 adata.obsm['X_pca'] = X_pca. 507 if use_highly_variable:. 508 adata.varm['PCs'] = np.zeros(shape=(adata.n_vars, n_comps)). ValueError: no field of name X_pca. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/504
https://github.com/scverse/scanpy/issues/504:482,testability,Trace,Traceback,482,"sc.tl.pca error: no field of name X_pca; I'm using scanpy==1.4 anndata==0.6.18 numpy==1.15.4 scipy==1.2.1 pandas==0.24.1 scikit-learn==0.20.2 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 on Mac 10.12.6 with python 3.7.1. I'm trying to do a pca on a annData object. `sc.tl.pca(adata, svd_solver='arpack')`. and get the following error even after restarting the jupyter notebook:. ```. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-26-eb775d53dbfd> in <module>. ----> 1 sc.tl.pca(adata, svd_solver='arpack'). ~/miniconda3/lib/python3.7/site-packages/scanpy/preprocessing/_simple.py in pca(data, n_comps, zero_center, svd_solver, random_state, return_info, use_highly_variable, dtype, copy, chunked, chunk_size). 504 . 505 if data_is_AnnData:. --> 506 adata.obsm['X_pca'] = X_pca. 507 if use_highly_variable:. 508 adata.varm['PCs'] = np.zeros(shape=(adata.n_vars, n_comps)). ValueError: no field of name X_pca. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/504
https://github.com/scverse/scanpy/issues/504:10,usability,error,error,10,"sc.tl.pca error: no field of name X_pca; I'm using scanpy==1.4 anndata==0.6.18 numpy==1.15.4 scipy==1.2.1 pandas==0.24.1 scikit-learn==0.20.2 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 on Mac 10.12.6 with python 3.7.1. I'm trying to do a pca on a annData object. `sc.tl.pca(adata, svd_solver='arpack')`. and get the following error even after restarting the jupyter notebook:. ```. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-26-eb775d53dbfd> in <module>. ----> 1 sc.tl.pca(adata, svd_solver='arpack'). ~/miniconda3/lib/python3.7/site-packages/scanpy/preprocessing/_simple.py in pca(data, n_comps, zero_center, svd_solver, random_state, return_info, use_highly_variable, dtype, copy, chunked, chunk_size). 504 . 505 if data_is_AnnData:. --> 506 adata.obsm['X_pca'] = X_pca. 507 if use_highly_variable:. 508 adata.varm['PCs'] = np.zeros(shape=(adata.n_vars, n_comps)). ValueError: no field of name X_pca. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/504
https://github.com/scverse/scanpy/issues/504:128,usability,learn,learn,128,"sc.tl.pca error: no field of name X_pca; I'm using scanpy==1.4 anndata==0.6.18 numpy==1.15.4 scipy==1.2.1 pandas==0.24.1 scikit-learn==0.20.2 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 on Mac 10.12.6 with python 3.7.1. I'm trying to do a pca on a annData object. `sc.tl.pca(adata, svd_solver='arpack')`. and get the following error even after restarting the jupyter notebook:. ```. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-26-eb775d53dbfd> in <module>. ----> 1 sc.tl.pca(adata, svd_solver='arpack'). ~/miniconda3/lib/python3.7/site-packages/scanpy/preprocessing/_simple.py in pca(data, n_comps, zero_center, svd_solver, random_state, return_info, use_highly_variable, dtype, copy, chunked, chunk_size). 504 . 505 if data_is_AnnData:. --> 506 adata.obsm['X_pca'] = X_pca. 507 if use_highly_variable:. 508 adata.varm['PCs'] = np.zeros(shape=(adata.n_vars, n_comps)). ValueError: no field of name X_pca. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/504
https://github.com/scverse/scanpy/issues/504:338,usability,error,error,338,"sc.tl.pca error: no field of name X_pca; I'm using scanpy==1.4 anndata==0.6.18 numpy==1.15.4 scipy==1.2.1 pandas==0.24.1 scikit-learn==0.20.2 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 on Mac 10.12.6 with python 3.7.1. I'm trying to do a pca on a annData object. `sc.tl.pca(adata, svd_solver='arpack')`. and get the following error even after restarting the jupyter notebook:. ```. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-26-eb775d53dbfd> in <module>. ----> 1 sc.tl.pca(adata, svd_solver='arpack'). ~/miniconda3/lib/python3.7/site-packages/scanpy/preprocessing/_simple.py in pca(data, n_comps, zero_center, svd_solver, random_state, return_info, use_highly_variable, dtype, copy, chunked, chunk_size). 504 . 505 if data_is_AnnData:. --> 506 adata.obsm['X_pca'] = X_pca. 507 if use_highly_variable:. 508 adata.varm['PCs'] = np.zeros(shape=(adata.n_vars, n_comps)). ValueError: no field of name X_pca. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/504
https://github.com/scverse/scanpy/issues/504:526,usability,input,input-,526,"sc.tl.pca error: no field of name X_pca; I'm using scanpy==1.4 anndata==0.6.18 numpy==1.15.4 scipy==1.2.1 pandas==0.24.1 scikit-learn==0.20.2 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 on Mac 10.12.6 with python 3.7.1. I'm trying to do a pca on a annData object. `sc.tl.pca(adata, svd_solver='arpack')`. and get the following error even after restarting the jupyter notebook:. ```. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-26-eb775d53dbfd> in <module>. ----> 1 sc.tl.pca(adata, svd_solver='arpack'). ~/miniconda3/lib/python3.7/site-packages/scanpy/preprocessing/_simple.py in pca(data, n_comps, zero_center, svd_solver, random_state, return_info, use_highly_variable, dtype, copy, chunked, chunk_size). 504 . 505 if data_is_AnnData:. --> 506 adata.obsm['X_pca'] = X_pca. 507 if use_highly_variable:. 508 adata.varm['PCs'] = np.zeros(shape=(adata.n_vars, n_comps)). ValueError: no field of name X_pca. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/504
https://github.com/scverse/scanpy/issues/505:0,energy efficiency,heat,heatmap,0,"heatmap for PCA and other functions; Hi,. Thanks for developing this amazingly fast package. . Is it possible to add some more functions to facilitate PC selection for making the graph? like the ones from Seurat ```PCHeatmap```, ```JackStraw``` and ```PCElbowPlot``` as shown in Seurat's tutorial. [https://satijalab.org/seurat/pbmc3k_tutorial.html](url)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/505
https://github.com/scverse/scanpy/issues/505:84,modifiability,pac,package,84,"heatmap for PCA and other functions; Hi,. Thanks for developing this amazingly fast package. . Is it possible to add some more functions to facilitate PC selection for making the graph? like the ones from Seurat ```PCHeatmap```, ```JackStraw``` and ```PCElbowPlot``` as shown in Seurat's tutorial. [https://satijalab.org/seurat/pbmc3k_tutorial.html](url)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/505
https://github.com/scverse/scanpy/issues/506:301,availability,error,error,301,"Exporting raw data in CSV; Hi Guys, . this it perhaps rather a question than issue, . Is there a way to export raw data in csv format? If I do this . `adata.write_csvs(""filename"", skip_data=False)` . it works perfectly fine . but with . `adata.raw.write_csvs(""filename"", skip_data=False)`. I get this error. `AttributeError: 'Raw' object has no attribute 'write_csvs'`. Thanks,.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/506
https://github.com/scverse/scanpy/issues/506:127,interoperability,format,format,127,"Exporting raw data in CSV; Hi Guys, . this it perhaps rather a question than issue, . Is there a way to export raw data in csv format? If I do this . `adata.write_csvs(""filename"", skip_data=False)` . it works perfectly fine . but with . `adata.raw.write_csvs(""filename"", skip_data=False)`. I get this error. `AttributeError: 'Raw' object has no attribute 'write_csvs'`. Thanks,.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/506
https://github.com/scverse/scanpy/issues/506:301,performance,error,error,301,"Exporting raw data in CSV; Hi Guys, . this it perhaps rather a question than issue, . Is there a way to export raw data in csv format? If I do this . `adata.write_csvs(""filename"", skip_data=False)` . it works perfectly fine . but with . `adata.raw.write_csvs(""filename"", skip_data=False)`. I get this error. `AttributeError: 'Raw' object has no attribute 'write_csvs'`. Thanks,.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/506
https://github.com/scverse/scanpy/issues/506:301,safety,error,error,301,"Exporting raw data in CSV; Hi Guys, . this it perhaps rather a question than issue, . Is there a way to export raw data in csv format? If I do this . `adata.write_csvs(""filename"", skip_data=False)` . it works perfectly fine . but with . `adata.raw.write_csvs(""filename"", skip_data=False)`. I get this error. `AttributeError: 'Raw' object has no attribute 'write_csvs'`. Thanks,.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/506
https://github.com/scverse/scanpy/issues/506:301,usability,error,error,301,"Exporting raw data in CSV; Hi Guys, . this it perhaps rather a question than issue, . Is there a way to export raw data in csv format? If I do this . `adata.write_csvs(""filename"", skip_data=False)` . it works perfectly fine . but with . `adata.raw.write_csvs(""filename"", skip_data=False)`. I get this error. `AttributeError: 'Raw' object has no attribute 'write_csvs'`. Thanks,.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/506
https://github.com/scverse/scanpy/issues/507:218,availability,error,error,218,"Arrays for row_attrs or col_attrs; Hey,. I just stumbled into an issue with scanpy. If I have an gene or cell annotation that is not 1-dimensional (I store ERCCs as a M x 92 float matrix in col_attrs), it generates an error:. ""Exception: Data must be 1-dimensional"". However, it is allowed by the loompy nomenclature to have multiple dimensions. . And actually loompy package accepts it. Could it be corrected, so it can be accepted as a valid Loom file/annotation? Thanks in advance",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/507
https://github.com/scverse/scanpy/issues/507:368,modifiability,pac,package,368,"Arrays for row_attrs or col_attrs; Hey,. I just stumbled into an issue with scanpy. If I have an gene or cell annotation that is not 1-dimensional (I store ERCCs as a M x 92 float matrix in col_attrs), it generates an error:. ""Exception: Data must be 1-dimensional"". However, it is allowed by the loompy nomenclature to have multiple dimensions. . And actually loompy package accepts it. Could it be corrected, so it can be accepted as a valid Loom file/annotation? Thanks in advance",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/507
https://github.com/scverse/scanpy/issues/507:218,performance,error,error,218,"Arrays for row_attrs or col_attrs; Hey,. I just stumbled into an issue with scanpy. If I have an gene or cell annotation that is not 1-dimensional (I store ERCCs as a M x 92 float matrix in col_attrs), it generates an error:. ""Exception: Data must be 1-dimensional"". However, it is allowed by the loompy nomenclature to have multiple dimensions. . And actually loompy package accepts it. Could it be corrected, so it can be accepted as a valid Loom file/annotation? Thanks in advance",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/507
https://github.com/scverse/scanpy/issues/507:218,safety,error,error,218,"Arrays for row_attrs or col_attrs; Hey,. I just stumbled into an issue with scanpy. If I have an gene or cell annotation that is not 1-dimensional (I store ERCCs as a M x 92 float matrix in col_attrs), it generates an error:. ""Exception: Data must be 1-dimensional"". However, it is allowed by the loompy nomenclature to have multiple dimensions. . And actually loompy package accepts it. Could it be corrected, so it can be accepted as a valid Loom file/annotation? Thanks in advance",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/507
https://github.com/scverse/scanpy/issues/507:227,safety,Except,Exception,227,"Arrays for row_attrs or col_attrs; Hey,. I just stumbled into an issue with scanpy. If I have an gene or cell annotation that is not 1-dimensional (I store ERCCs as a M x 92 float matrix in col_attrs), it generates an error:. ""Exception: Data must be 1-dimensional"". However, it is allowed by the loompy nomenclature to have multiple dimensions. . And actually loompy package accepts it. Could it be corrected, so it can be accepted as a valid Loom file/annotation? Thanks in advance",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/507
https://github.com/scverse/scanpy/issues/507:438,safety,valid,valid,438,"Arrays for row_attrs or col_attrs; Hey,. I just stumbled into an issue with scanpy. If I have an gene or cell annotation that is not 1-dimensional (I store ERCCs as a M x 92 float matrix in col_attrs), it generates an error:. ""Exception: Data must be 1-dimensional"". However, it is allowed by the loompy nomenclature to have multiple dimensions. . And actually loompy package accepts it. Could it be corrected, so it can be accepted as a valid Loom file/annotation? Thanks in advance",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/507
https://github.com/scverse/scanpy/issues/507:218,usability,error,error,218,"Arrays for row_attrs or col_attrs; Hey,. I just stumbled into an issue with scanpy. If I have an gene or cell annotation that is not 1-dimensional (I store ERCCs as a M x 92 float matrix in col_attrs), it generates an error:. ""Exception: Data must be 1-dimensional"". However, it is allowed by the loompy nomenclature to have multiple dimensions. . And actually loompy package accepts it. Could it be corrected, so it can be accepted as a valid Loom file/annotation? Thanks in advance",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/507
https://github.com/scverse/scanpy/pull/508:192,deployability,stack,stackoverflow,192,"Replace `is` for `==` in string testing.; Using `is` in a test, tests for identity and not equality, this leads to. inconsistent behaviours when testing for equality with strings. See https://stackoverflow.com/questions/1504717/why-does-comparing-strings-using-either-or-is-sometimes-produce-a-differe",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/508
https://github.com/scverse/scanpy/pull/508:232,reliability,doe,does-comparing-strings-using-either-or-is-sometimes-produce-a-differe,232,"Replace `is` for `==` in string testing.; Using `is` in a test, tests for identity and not equality, this leads to. inconsistent behaviours when testing for equality with strings. See https://stackoverflow.com/questions/1504717/why-does-comparing-strings-using-either-or-is-sometimes-produce-a-differe",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/508
https://github.com/scverse/scanpy/pull/508:32,safety,test,testing,32,"Replace `is` for `==` in string testing.; Using `is` in a test, tests for identity and not equality, this leads to. inconsistent behaviours when testing for equality with strings. See https://stackoverflow.com/questions/1504717/why-does-comparing-strings-using-either-or-is-sometimes-produce-a-differe",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/508
https://github.com/scverse/scanpy/pull/508:58,safety,test,test,58,"Replace `is` for `==` in string testing.; Using `is` in a test, tests for identity and not equality, this leads to. inconsistent behaviours when testing for equality with strings. See https://stackoverflow.com/questions/1504717/why-does-comparing-strings-using-either-or-is-sometimes-produce-a-differe",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/508
https://github.com/scverse/scanpy/pull/508:64,safety,test,tests,64,"Replace `is` for `==` in string testing.; Using `is` in a test, tests for identity and not equality, this leads to. inconsistent behaviours when testing for equality with strings. See https://stackoverflow.com/questions/1504717/why-does-comparing-strings-using-either-or-is-sometimes-produce-a-differe",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/508
https://github.com/scverse/scanpy/pull/508:145,safety,test,testing,145,"Replace `is` for `==` in string testing.; Using `is` in a test, tests for identity and not equality, this leads to. inconsistent behaviours when testing for equality with strings. See https://stackoverflow.com/questions/1504717/why-does-comparing-strings-using-either-or-is-sometimes-produce-a-differe",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/508
https://github.com/scverse/scanpy/pull/508:74,security,ident,identity,74,"Replace `is` for `==` in string testing.; Using `is` in a test, tests for identity and not equality, this leads to. inconsistent behaviours when testing for equality with strings. See https://stackoverflow.com/questions/1504717/why-does-comparing-strings-using-either-or-is-sometimes-produce-a-differe",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/508
https://github.com/scverse/scanpy/pull/508:32,testability,test,testing,32,"Replace `is` for `==` in string testing.; Using `is` in a test, tests for identity and not equality, this leads to. inconsistent behaviours when testing for equality with strings. See https://stackoverflow.com/questions/1504717/why-does-comparing-strings-using-either-or-is-sometimes-produce-a-differe",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/508
https://github.com/scverse/scanpy/pull/508:58,testability,test,test,58,"Replace `is` for `==` in string testing.; Using `is` in a test, tests for identity and not equality, this leads to. inconsistent behaviours when testing for equality with strings. See https://stackoverflow.com/questions/1504717/why-does-comparing-strings-using-either-or-is-sometimes-produce-a-differe",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/508
https://github.com/scverse/scanpy/pull/508:64,testability,test,tests,64,"Replace `is` for `==` in string testing.; Using `is` in a test, tests for identity and not equality, this leads to. inconsistent behaviours when testing for equality with strings. See https://stackoverflow.com/questions/1504717/why-does-comparing-strings-using-either-or-is-sometimes-produce-a-differe",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/508
https://github.com/scverse/scanpy/pull/508:145,testability,test,testing,145,"Replace `is` for `==` in string testing.; Using `is` in a test, tests for identity and not equality, this leads to. inconsistent behaviours when testing for equality with strings. See https://stackoverflow.com/questions/1504717/why-does-comparing-strings-using-either-or-is-sometimes-produce-a-differe",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/508
https://github.com/scverse/scanpy/pull/508:129,usability,behavi,behaviours,129,"Replace `is` for `==` in string testing.; Using `is` in a test, tests for identity and not equality, this leads to. inconsistent behaviours when testing for equality with strings. See https://stackoverflow.com/questions/1504717/why-does-comparing-strings-using-either-or-is-sometimes-produce-a-differe",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/508
https://github.com/scverse/scanpy/issues/509:0,availability,Error,Error,0,"Error in sc.pp.highly_varaible_genes; adata = sc.read_10x_mtx(input_file). sc.pp.highly_variable_genes(adata). -------------------------------------------------------------------------. Output:. ValueError: Bin edges must be unique: array([ -inf, 9.99999996e-13, 9.99999996e-13, 9.99999996e-13,. 9.99999996e-13, 9.99999996e-13, 9.99999996e-13, 9.99999996e-13,. 3.46620451e-04, 6.93240901e-04, 2.42634304e-03, 7.27903098e-03,. 1.97573602e-02, 4.47139777e-02, 8.52686986e-02, 1.50433296e-01,. 2.66013424e-01, 5.22116709e-01, 1.29667579e+00, 4.65706696e+02,. inf]). You can drop duplicate edges by setting the 'duplicates' kwarg",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/509
https://github.com/scverse/scanpy/issues/509:0,performance,Error,Error,0,"Error in sc.pp.highly_varaible_genes; adata = sc.read_10x_mtx(input_file). sc.pp.highly_variable_genes(adata). -------------------------------------------------------------------------. Output:. ValueError: Bin edges must be unique: array([ -inf, 9.99999996e-13, 9.99999996e-13, 9.99999996e-13,. 9.99999996e-13, 9.99999996e-13, 9.99999996e-13, 9.99999996e-13,. 3.46620451e-04, 6.93240901e-04, 2.42634304e-03, 7.27903098e-03,. 1.97573602e-02, 4.47139777e-02, 8.52686986e-02, 1.50433296e-01,. 2.66013424e-01, 5.22116709e-01, 1.29667579e+00, 4.65706696e+02,. inf]). You can drop duplicate edges by setting the 'duplicates' kwarg",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/509
https://github.com/scverse/scanpy/issues/509:0,safety,Error,Error,0,"Error in sc.pp.highly_varaible_genes; adata = sc.read_10x_mtx(input_file). sc.pp.highly_variable_genes(adata). -------------------------------------------------------------------------. Output:. ValueError: Bin edges must be unique: array([ -inf, 9.99999996e-13, 9.99999996e-13, 9.99999996e-13,. 9.99999996e-13, 9.99999996e-13, 9.99999996e-13, 9.99999996e-13,. 3.46620451e-04, 6.93240901e-04, 2.42634304e-03, 7.27903098e-03,. 1.97573602e-02, 4.47139777e-02, 8.52686986e-02, 1.50433296e-01,. 2.66013424e-01, 5.22116709e-01, 1.29667579e+00, 4.65706696e+02,. inf]). You can drop duplicate edges by setting the 'duplicates' kwarg",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/509
https://github.com/scverse/scanpy/issues/509:0,usability,Error,Error,0,"Error in sc.pp.highly_varaible_genes; adata = sc.read_10x_mtx(input_file). sc.pp.highly_variable_genes(adata). -------------------------------------------------------------------------. Output:. ValueError: Bin edges must be unique: array([ -inf, 9.99999996e-13, 9.99999996e-13, 9.99999996e-13,. 9.99999996e-13, 9.99999996e-13, 9.99999996e-13, 9.99999996e-13,. 3.46620451e-04, 6.93240901e-04, 2.42634304e-03, 7.27903098e-03,. 1.97573602e-02, 4.47139777e-02, 8.52686986e-02, 1.50433296e-01,. 2.66013424e-01, 5.22116709e-01, 1.29667579e+00, 4.65706696e+02,. inf]). You can drop duplicate edges by setting the 'duplicates' kwarg",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/509
https://github.com/scverse/scanpy/issues/510:14,availability,cluster,clustering,14,"Geneset based clustering and CytOF Compatibility; I want to start off by mentioning how much I love scanpy. I recommend this package to everyone I know who is doing single cell sequencing. I love how PAGA and UMAP can be integrated together. PAGA makes appreciation of data topology so simple. In addition, it's so easy to do velocity analysis with the scvelo integration. Installing scanpy as well as hdf5/loom compatibility is remarkably easier on python than in R, which gives scanpy users an obvious advantage. . I've learned so much using this package and it has allowed me to display my data in a simple and intuitive way. Truly, from the bottom of my heart, thank you, this package is awesome and I deeply appreciate all the work the scanpy team and Theis lab has put into it. . With that said, I find myself wanting to do things with scanpy that aren't currently supported. . (1) Clustering cells based on a restricted set of genes as opposed to principle components: I understand that this is a biased approach, but I find myself wanting to do this to explore datasets and how genes vary together. I believe it would aid in understanding data topology to reduce the dimensionality of the dataset by looking at a subset of key genes that are picked by an expert who has domain knowledge. I think this is best integrated with a separate sc.pp.neighbors-like function that uses a gene list, as opposed to PCs. Integration into downstream plotting can be done with separate functions that have a similar name (sc.tl.umap_genelist or sc.tl.umap_sparse; sc.tl.paga_genelist or sc.tl.paga_sparse). . I envision that this would also be a nice stepping stone towards multi-modal analysis with CITE-seq. . (2) I want to use PAGA and UMAP to display my CytOF data as well as integrate scRNA-seq/CytOF data together. I saw a pre-print on bioRxiv that indicated that you are working on this aspect of PAGA/scanpy. This was very exciting to me and I am glad that this is being developed. Is it possible to",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/510
https://github.com/scverse/scanpy/issues/510:888,availability,Cluster,Clustering,888,"Geneset based clustering and CytOF Compatibility; I want to start off by mentioning how much I love scanpy. I recommend this package to everyone I know who is doing single cell sequencing. I love how PAGA and UMAP can be integrated together. PAGA makes appreciation of data topology so simple. In addition, it's so easy to do velocity analysis with the scvelo integration. Installing scanpy as well as hdf5/loom compatibility is remarkably easier on python than in R, which gives scanpy users an obvious advantage. . I've learned so much using this package and it has allowed me to display my data in a simple and intuitive way. Truly, from the bottom of my heart, thank you, this package is awesome and I deeply appreciate all the work the scanpy team and Theis lab has put into it. . With that said, I find myself wanting to do things with scanpy that aren't currently supported. . (1) Clustering cells based on a restricted set of genes as opposed to principle components: I understand that this is a biased approach, but I find myself wanting to do this to explore datasets and how genes vary together. I believe it would aid in understanding data topology to reduce the dimensionality of the dataset by looking at a subset of key genes that are picked by an expert who has domain knowledge. I think this is best integrated with a separate sc.pp.neighbors-like function that uses a gene list, as opposed to PCs. Integration into downstream plotting can be done with separate functions that have a similar name (sc.tl.umap_genelist or sc.tl.umap_sparse; sc.tl.paga_genelist or sc.tl.paga_sparse). . I envision that this would also be a nice stepping stone towards multi-modal analysis with CITE-seq. . (2) I want to use PAGA and UMAP to display my CytOF data as well as integrate scRNA-seq/CytOF data together. I saw a pre-print on bioRxiv that indicated that you are working on this aspect of PAGA/scanpy. This was very exciting to me and I am glad that this is being developed. Is it possible to",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/510
https://github.com/scverse/scanpy/issues/510:1433,availability,down,downstream,1433,"y easier on python than in R, which gives scanpy users an obvious advantage. . I've learned so much using this package and it has allowed me to display my data in a simple and intuitive way. Truly, from the bottom of my heart, thank you, this package is awesome and I deeply appreciate all the work the scanpy team and Theis lab has put into it. . With that said, I find myself wanting to do things with scanpy that aren't currently supported. . (1) Clustering cells based on a restricted set of genes as opposed to principle components: I understand that this is a biased approach, but I find myself wanting to do this to explore datasets and how genes vary together. I believe it would aid in understanding data topology to reduce the dimensionality of the dataset by looking at a subset of key genes that are picked by an expert who has domain knowledge. I think this is best integrated with a separate sc.pp.neighbors-like function that uses a gene list, as opposed to PCs. Integration into downstream plotting can be done with separate functions that have a similar name (sc.tl.umap_genelist or sc.tl.umap_sparse; sc.tl.paga_genelist or sc.tl.paga_sparse). . I envision that this would also be a nice stepping stone towards multi-modal analysis with CITE-seq. . (2) I want to use PAGA and UMAP to display my CytOF data as well as integrate scRNA-seq/CytOF data together. I saw a pre-print on bioRxiv that indicated that you are working on this aspect of PAGA/scanpy. This was very exciting to me and I am glad that this is being developed. Is it possible to access the current version of this software? . I think that major partitions identified with CytOF or scRNA-seq can be linked together providing a coarse-grained mechanism to demonstrate how heterogeneity identified with each technique relates to each other based on a given experimental time point. . (3) Histogram integration in the plotting api for QC metrics would be helpful. While scatter plots and violin plots are effective, I fi",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/510
https://github.com/scverse/scanpy/issues/510:14,deployability,cluster,clustering,14,"Geneset based clustering and CytOF Compatibility; I want to start off by mentioning how much I love scanpy. I recommend this package to everyone I know who is doing single cell sequencing. I love how PAGA and UMAP can be integrated together. PAGA makes appreciation of data topology so simple. In addition, it's so easy to do velocity analysis with the scvelo integration. Installing scanpy as well as hdf5/loom compatibility is remarkably easier on python than in R, which gives scanpy users an obvious advantage. . I've learned so much using this package and it has allowed me to display my data in a simple and intuitive way. Truly, from the bottom of my heart, thank you, this package is awesome and I deeply appreciate all the work the scanpy team and Theis lab has put into it. . With that said, I find myself wanting to do things with scanpy that aren't currently supported. . (1) Clustering cells based on a restricted set of genes as opposed to principle components: I understand that this is a biased approach, but I find myself wanting to do this to explore datasets and how genes vary together. I believe it would aid in understanding data topology to reduce the dimensionality of the dataset by looking at a subset of key genes that are picked by an expert who has domain knowledge. I think this is best integrated with a separate sc.pp.neighbors-like function that uses a gene list, as opposed to PCs. Integration into downstream plotting can be done with separate functions that have a similar name (sc.tl.umap_genelist or sc.tl.umap_sparse; sc.tl.paga_genelist or sc.tl.paga_sparse). . I envision that this would also be a nice stepping stone towards multi-modal analysis with CITE-seq. . (2) I want to use PAGA and UMAP to display my CytOF data as well as integrate scRNA-seq/CytOF data together. I saw a pre-print on bioRxiv that indicated that you are working on this aspect of PAGA/scanpy. This was very exciting to me and I am glad that this is being developed. Is it possible to",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/510
https://github.com/scverse/scanpy/issues/510:221,deployability,integr,integrated,221,"Geneset based clustering and CytOF Compatibility; I want to start off by mentioning how much I love scanpy. I recommend this package to everyone I know who is doing single cell sequencing. I love how PAGA and UMAP can be integrated together. PAGA makes appreciation of data topology so simple. In addition, it's so easy to do velocity analysis with the scvelo integration. Installing scanpy as well as hdf5/loom compatibility is remarkably easier on python than in R, which gives scanpy users an obvious advantage. . I've learned so much using this package and it has allowed me to display my data in a simple and intuitive way. Truly, from the bottom of my heart, thank you, this package is awesome and I deeply appreciate all the work the scanpy team and Theis lab has put into it. . With that said, I find myself wanting to do things with scanpy that aren't currently supported. . (1) Clustering cells based on a restricted set of genes as opposed to principle components: I understand that this is a biased approach, but I find myself wanting to do this to explore datasets and how genes vary together. I believe it would aid in understanding data topology to reduce the dimensionality of the dataset by looking at a subset of key genes that are picked by an expert who has domain knowledge. I think this is best integrated with a separate sc.pp.neighbors-like function that uses a gene list, as opposed to PCs. Integration into downstream plotting can be done with separate functions that have a similar name (sc.tl.umap_genelist or sc.tl.umap_sparse; sc.tl.paga_genelist or sc.tl.paga_sparse). . I envision that this would also be a nice stepping stone towards multi-modal analysis with CITE-seq. . (2) I want to use PAGA and UMAP to display my CytOF data as well as integrate scRNA-seq/CytOF data together. I saw a pre-print on bioRxiv that indicated that you are working on this aspect of PAGA/scanpy. This was very exciting to me and I am glad that this is being developed. Is it possible to",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/510
https://github.com/scverse/scanpy/issues/510:360,deployability,integr,integration,360,"Geneset based clustering and CytOF Compatibility; I want to start off by mentioning how much I love scanpy. I recommend this package to everyone I know who is doing single cell sequencing. I love how PAGA and UMAP can be integrated together. PAGA makes appreciation of data topology so simple. In addition, it's so easy to do velocity analysis with the scvelo integration. Installing scanpy as well as hdf5/loom compatibility is remarkably easier on python than in R, which gives scanpy users an obvious advantage. . I've learned so much using this package and it has allowed me to display my data in a simple and intuitive way. Truly, from the bottom of my heart, thank you, this package is awesome and I deeply appreciate all the work the scanpy team and Theis lab has put into it. . With that said, I find myself wanting to do things with scanpy that aren't currently supported. . (1) Clustering cells based on a restricted set of genes as opposed to principle components: I understand that this is a biased approach, but I find myself wanting to do this to explore datasets and how genes vary together. I believe it would aid in understanding data topology to reduce the dimensionality of the dataset by looking at a subset of key genes that are picked by an expert who has domain knowledge. I think this is best integrated with a separate sc.pp.neighbors-like function that uses a gene list, as opposed to PCs. Integration into downstream plotting can be done with separate functions that have a similar name (sc.tl.umap_genelist or sc.tl.umap_sparse; sc.tl.paga_genelist or sc.tl.paga_sparse). . I envision that this would also be a nice stepping stone towards multi-modal analysis with CITE-seq. . (2) I want to use PAGA and UMAP to display my CytOF data as well as integrate scRNA-seq/CytOF data together. I saw a pre-print on bioRxiv that indicated that you are working on this aspect of PAGA/scanpy. This was very exciting to me and I am glad that this is being developed. Is it possible to",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/510
https://github.com/scverse/scanpy/issues/510:373,deployability,Instal,Installing,373,"Geneset based clustering and CytOF Compatibility; I want to start off by mentioning how much I love scanpy. I recommend this package to everyone I know who is doing single cell sequencing. I love how PAGA and UMAP can be integrated together. PAGA makes appreciation of data topology so simple. In addition, it's so easy to do velocity analysis with the scvelo integration. Installing scanpy as well as hdf5/loom compatibility is remarkably easier on python than in R, which gives scanpy users an obvious advantage. . I've learned so much using this package and it has allowed me to display my data in a simple and intuitive way. Truly, from the bottom of my heart, thank you, this package is awesome and I deeply appreciate all the work the scanpy team and Theis lab has put into it. . With that said, I find myself wanting to do things with scanpy that aren't currently supported. . (1) Clustering cells based on a restricted set of genes as opposed to principle components: I understand that this is a biased approach, but I find myself wanting to do this to explore datasets and how genes vary together. I believe it would aid in understanding data topology to reduce the dimensionality of the dataset by looking at a subset of key genes that are picked by an expert who has domain knowledge. I think this is best integrated with a separate sc.pp.neighbors-like function that uses a gene list, as opposed to PCs. Integration into downstream plotting can be done with separate functions that have a similar name (sc.tl.umap_genelist or sc.tl.umap_sparse; sc.tl.paga_genelist or sc.tl.paga_sparse). . I envision that this would also be a nice stepping stone towards multi-modal analysis with CITE-seq. . (2) I want to use PAGA and UMAP to display my CytOF data as well as integrate scRNA-seq/CytOF data together. I saw a pre-print on bioRxiv that indicated that you are working on this aspect of PAGA/scanpy. This was very exciting to me and I am glad that this is being developed. Is it possible to",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/510
https://github.com/scverse/scanpy/issues/510:888,deployability,Cluster,Clustering,888,"Geneset based clustering and CytOF Compatibility; I want to start off by mentioning how much I love scanpy. I recommend this package to everyone I know who is doing single cell sequencing. I love how PAGA and UMAP can be integrated together. PAGA makes appreciation of data topology so simple. In addition, it's so easy to do velocity analysis with the scvelo integration. Installing scanpy as well as hdf5/loom compatibility is remarkably easier on python than in R, which gives scanpy users an obvious advantage. . I've learned so much using this package and it has allowed me to display my data in a simple and intuitive way. Truly, from the bottom of my heart, thank you, this package is awesome and I deeply appreciate all the work the scanpy team and Theis lab has put into it. . With that said, I find myself wanting to do things with scanpy that aren't currently supported. . (1) Clustering cells based on a restricted set of genes as opposed to principle components: I understand that this is a biased approach, but I find myself wanting to do this to explore datasets and how genes vary together. I believe it would aid in understanding data topology to reduce the dimensionality of the dataset by looking at a subset of key genes that are picked by an expert who has domain knowledge. I think this is best integrated with a separate sc.pp.neighbors-like function that uses a gene list, as opposed to PCs. Integration into downstream plotting can be done with separate functions that have a similar name (sc.tl.umap_genelist or sc.tl.umap_sparse; sc.tl.paga_genelist or sc.tl.paga_sparse). . I envision that this would also be a nice stepping stone towards multi-modal analysis with CITE-seq. . (2) I want to use PAGA and UMAP to display my CytOF data as well as integrate scRNA-seq/CytOF data together. I saw a pre-print on bioRxiv that indicated that you are working on this aspect of PAGA/scanpy. This was very exciting to me and I am glad that this is being developed. Is it possible to",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/510
https://github.com/scverse/scanpy/issues/510:1317,deployability,integr,integrated,1317," do velocity analysis with the scvelo integration. Installing scanpy as well as hdf5/loom compatibility is remarkably easier on python than in R, which gives scanpy users an obvious advantage. . I've learned so much using this package and it has allowed me to display my data in a simple and intuitive way. Truly, from the bottom of my heart, thank you, this package is awesome and I deeply appreciate all the work the scanpy team and Theis lab has put into it. . With that said, I find myself wanting to do things with scanpy that aren't currently supported. . (1) Clustering cells based on a restricted set of genes as opposed to principle components: I understand that this is a biased approach, but I find myself wanting to do this to explore datasets and how genes vary together. I believe it would aid in understanding data topology to reduce the dimensionality of the dataset by looking at a subset of key genes that are picked by an expert who has domain knowledge. I think this is best integrated with a separate sc.pp.neighbors-like function that uses a gene list, as opposed to PCs. Integration into downstream plotting can be done with separate functions that have a similar name (sc.tl.umap_genelist or sc.tl.umap_sparse; sc.tl.paga_genelist or sc.tl.paga_sparse). . I envision that this would also be a nice stepping stone towards multi-modal analysis with CITE-seq. . (2) I want to use PAGA and UMAP to display my CytOF data as well as integrate scRNA-seq/CytOF data together. I saw a pre-print on bioRxiv that indicated that you are working on this aspect of PAGA/scanpy. This was very exciting to me and I am glad that this is being developed. Is it possible to access the current version of this software? . I think that major partitions identified with CytOF or scRNA-seq can be linked together providing a coarse-grained mechanism to demonstrate how heterogeneity identified with each technique relates to each other based on a given experimental time point. . (3) Histogram integ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/510
https://github.com/scverse/scanpy/issues/510:1416,deployability,Integr,Integration,1416,"ity is remarkably easier on python than in R, which gives scanpy users an obvious advantage. . I've learned so much using this package and it has allowed me to display my data in a simple and intuitive way. Truly, from the bottom of my heart, thank you, this package is awesome and I deeply appreciate all the work the scanpy team and Theis lab has put into it. . With that said, I find myself wanting to do things with scanpy that aren't currently supported. . (1) Clustering cells based on a restricted set of genes as opposed to principle components: I understand that this is a biased approach, but I find myself wanting to do this to explore datasets and how genes vary together. I believe it would aid in understanding data topology to reduce the dimensionality of the dataset by looking at a subset of key genes that are picked by an expert who has domain knowledge. I think this is best integrated with a separate sc.pp.neighbors-like function that uses a gene list, as opposed to PCs. Integration into downstream plotting can be done with separate functions that have a similar name (sc.tl.umap_genelist or sc.tl.umap_sparse; sc.tl.paga_genelist or sc.tl.paga_sparse). . I envision that this would also be a nice stepping stone towards multi-modal analysis with CITE-seq. . (2) I want to use PAGA and UMAP to display my CytOF data as well as integrate scRNA-seq/CytOF data together. I saw a pre-print on bioRxiv that indicated that you are working on this aspect of PAGA/scanpy. This was very exciting to me and I am glad that this is being developed. Is it possible to access the current version of this software? . I think that major partitions identified with CytOF or scRNA-seq can be linked together providing a coarse-grained mechanism to demonstrate how heterogeneity identified with each technique relates to each other based on a given experimental time point. . (3) Histogram integration in the plotting api for QC metrics would be helpful. While scatter plots and violin plots are",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/510
https://github.com/scverse/scanpy/issues/510:1773,deployability,integr,integrate,1773,"a in a simple and intuitive way. Truly, from the bottom of my heart, thank you, this package is awesome and I deeply appreciate all the work the scanpy team and Theis lab has put into it. . With that said, I find myself wanting to do things with scanpy that aren't currently supported. . (1) Clustering cells based on a restricted set of genes as opposed to principle components: I understand that this is a biased approach, but I find myself wanting to do this to explore datasets and how genes vary together. I believe it would aid in understanding data topology to reduce the dimensionality of the dataset by looking at a subset of key genes that are picked by an expert who has domain knowledge. I think this is best integrated with a separate sc.pp.neighbors-like function that uses a gene list, as opposed to PCs. Integration into downstream plotting can be done with separate functions that have a similar name (sc.tl.umap_genelist or sc.tl.umap_sparse; sc.tl.paga_genelist or sc.tl.paga_sparse). . I envision that this would also be a nice stepping stone towards multi-modal analysis with CITE-seq. . (2) I want to use PAGA and UMAP to display my CytOF data as well as integrate scRNA-seq/CytOF data together. I saw a pre-print on bioRxiv that indicated that you are working on this aspect of PAGA/scanpy. This was very exciting to me and I am glad that this is being developed. Is it possible to access the current version of this software? . I think that major partitions identified with CytOF or scRNA-seq can be linked together providing a coarse-grained mechanism to demonstrate how heterogeneity identified with each technique relates to each other based on a given experimental time point. . (3) Histogram integration in the plotting api for QC metrics would be helpful. While scatter plots and violin plots are effective, I find myself wanting to make cuts (bounds on percent mitochondrial or nGenes) in my data based off histograms. Again, thank you so much for the amazing software!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/510
https://github.com/scverse/scanpy/issues/510:2020,deployability,version,version,2020,"a in a simple and intuitive way. Truly, from the bottom of my heart, thank you, this package is awesome and I deeply appreciate all the work the scanpy team and Theis lab has put into it. . With that said, I find myself wanting to do things with scanpy that aren't currently supported. . (1) Clustering cells based on a restricted set of genes as opposed to principle components: I understand that this is a biased approach, but I find myself wanting to do this to explore datasets and how genes vary together. I believe it would aid in understanding data topology to reduce the dimensionality of the dataset by looking at a subset of key genes that are picked by an expert who has domain knowledge. I think this is best integrated with a separate sc.pp.neighbors-like function that uses a gene list, as opposed to PCs. Integration into downstream plotting can be done with separate functions that have a similar name (sc.tl.umap_genelist or sc.tl.umap_sparse; sc.tl.paga_genelist or sc.tl.paga_sparse). . I envision that this would also be a nice stepping stone towards multi-modal analysis with CITE-seq. . (2) I want to use PAGA and UMAP to display my CytOF data as well as integrate scRNA-seq/CytOF data together. I saw a pre-print on bioRxiv that indicated that you are working on this aspect of PAGA/scanpy. This was very exciting to me and I am glad that this is being developed. Is it possible to access the current version of this software? . I think that major partitions identified with CytOF or scRNA-seq can be linked together providing a coarse-grained mechanism to demonstrate how heterogeneity identified with each technique relates to each other based on a given experimental time point. . (3) Histogram integration in the plotting api for QC metrics would be helpful. While scatter plots and violin plots are effective, I find myself wanting to make cuts (bounds on percent mitochondrial or nGenes) in my data based off histograms. Again, thank you so much for the amazing software!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/510
https://github.com/scverse/scanpy/issues/510:2317,deployability,integr,integration,2317,"a in a simple and intuitive way. Truly, from the bottom of my heart, thank you, this package is awesome and I deeply appreciate all the work the scanpy team and Theis lab has put into it. . With that said, I find myself wanting to do things with scanpy that aren't currently supported. . (1) Clustering cells based on a restricted set of genes as opposed to principle components: I understand that this is a biased approach, but I find myself wanting to do this to explore datasets and how genes vary together. I believe it would aid in understanding data topology to reduce the dimensionality of the dataset by looking at a subset of key genes that are picked by an expert who has domain knowledge. I think this is best integrated with a separate sc.pp.neighbors-like function that uses a gene list, as opposed to PCs. Integration into downstream plotting can be done with separate functions that have a similar name (sc.tl.umap_genelist or sc.tl.umap_sparse; sc.tl.paga_genelist or sc.tl.paga_sparse). . I envision that this would also be a nice stepping stone towards multi-modal analysis with CITE-seq. . (2) I want to use PAGA and UMAP to display my CytOF data as well as integrate scRNA-seq/CytOF data together. I saw a pre-print on bioRxiv that indicated that you are working on this aspect of PAGA/scanpy. This was very exciting to me and I am glad that this is being developed. Is it possible to access the current version of this software? . I think that major partitions identified with CytOF or scRNA-seq can be linked together providing a coarse-grained mechanism to demonstrate how heterogeneity identified with each technique relates to each other based on a given experimental time point. . (3) Histogram integration in the plotting api for QC metrics would be helpful. While scatter plots and violin plots are effective, I find myself wanting to make cuts (bounds on percent mitochondrial or nGenes) in my data based off histograms. Again, thank you so much for the amazing software!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/510
https://github.com/scverse/scanpy/issues/510:2345,deployability,api,api,2345,"a in a simple and intuitive way. Truly, from the bottom of my heart, thank you, this package is awesome and I deeply appreciate all the work the scanpy team and Theis lab has put into it. . With that said, I find myself wanting to do things with scanpy that aren't currently supported. . (1) Clustering cells based on a restricted set of genes as opposed to principle components: I understand that this is a biased approach, but I find myself wanting to do this to explore datasets and how genes vary together. I believe it would aid in understanding data topology to reduce the dimensionality of the dataset by looking at a subset of key genes that are picked by an expert who has domain knowledge. I think this is best integrated with a separate sc.pp.neighbors-like function that uses a gene list, as opposed to PCs. Integration into downstream plotting can be done with separate functions that have a similar name (sc.tl.umap_genelist or sc.tl.umap_sparse; sc.tl.paga_genelist or sc.tl.paga_sparse). . I envision that this would also be a nice stepping stone towards multi-modal analysis with CITE-seq. . (2) I want to use PAGA and UMAP to display my CytOF data as well as integrate scRNA-seq/CytOF data together. I saw a pre-print on bioRxiv that indicated that you are working on this aspect of PAGA/scanpy. This was very exciting to me and I am glad that this is being developed. Is it possible to access the current version of this software? . I think that major partitions identified with CytOF or scRNA-seq can be linked together providing a coarse-grained mechanism to demonstrate how heterogeneity identified with each technique relates to each other based on a given experimental time point. . (3) Histogram integration in the plotting api for QC metrics would be helpful. While scatter plots and violin plots are effective, I find myself wanting to make cuts (bounds on percent mitochondrial or nGenes) in my data based off histograms. Again, thank you so much for the amazing software!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/510
https://github.com/scverse/scanpy/issues/510:861,energy efficiency,current,currently,861,"Geneset based clustering and CytOF Compatibility; I want to start off by mentioning how much I love scanpy. I recommend this package to everyone I know who is doing single cell sequencing. I love how PAGA and UMAP can be integrated together. PAGA makes appreciation of data topology so simple. In addition, it's so easy to do velocity analysis with the scvelo integration. Installing scanpy as well as hdf5/loom compatibility is remarkably easier on python than in R, which gives scanpy users an obvious advantage. . I've learned so much using this package and it has allowed me to display my data in a simple and intuitive way. Truly, from the bottom of my heart, thank you, this package is awesome and I deeply appreciate all the work the scanpy team and Theis lab has put into it. . With that said, I find myself wanting to do things with scanpy that aren't currently supported. . (1) Clustering cells based on a restricted set of genes as opposed to principle components: I understand that this is a biased approach, but I find myself wanting to do this to explore datasets and how genes vary together. I believe it would aid in understanding data topology to reduce the dimensionality of the dataset by looking at a subset of key genes that are picked by an expert who has domain knowledge. I think this is best integrated with a separate sc.pp.neighbors-like function that uses a gene list, as opposed to PCs. Integration into downstream plotting can be done with separate functions that have a similar name (sc.tl.umap_genelist or sc.tl.umap_sparse; sc.tl.paga_genelist or sc.tl.paga_sparse). . I envision that this would also be a nice stepping stone towards multi-modal analysis with CITE-seq. . (2) I want to use PAGA and UMAP to display my CytOF data as well as integrate scRNA-seq/CytOF data together. I saw a pre-print on bioRxiv that indicated that you are working on this aspect of PAGA/scanpy. This was very exciting to me and I am glad that this is being developed. Is it possible to",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/510
https://github.com/scverse/scanpy/issues/510:1164,energy efficiency,reduc,reduce,1164,"ngle cell sequencing. I love how PAGA and UMAP can be integrated together. PAGA makes appreciation of data topology so simple. In addition, it's so easy to do velocity analysis with the scvelo integration. Installing scanpy as well as hdf5/loom compatibility is remarkably easier on python than in R, which gives scanpy users an obvious advantage. . I've learned so much using this package and it has allowed me to display my data in a simple and intuitive way. Truly, from the bottom of my heart, thank you, this package is awesome and I deeply appreciate all the work the scanpy team and Theis lab has put into it. . With that said, I find myself wanting to do things with scanpy that aren't currently supported. . (1) Clustering cells based on a restricted set of genes as opposed to principle components: I understand that this is a biased approach, but I find myself wanting to do this to explore datasets and how genes vary together. I believe it would aid in understanding data topology to reduce the dimensionality of the dataset by looking at a subset of key genes that are picked by an expert who has domain knowledge. I think this is best integrated with a separate sc.pp.neighbors-like function that uses a gene list, as opposed to PCs. Integration into downstream plotting can be done with separate functions that have a similar name (sc.tl.umap_genelist or sc.tl.umap_sparse; sc.tl.paga_genelist or sc.tl.paga_sparse). . I envision that this would also be a nice stepping stone towards multi-modal analysis with CITE-seq. . (2) I want to use PAGA and UMAP to display my CytOF data as well as integrate scRNA-seq/CytOF data together. I saw a pre-print on bioRxiv that indicated that you are working on this aspect of PAGA/scanpy. This was very exciting to me and I am glad that this is being developed. Is it possible to access the current version of this software? . I think that major partitions identified with CytOF or scRNA-seq can be linked together providing a coarse-grained mech",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/510
https://github.com/scverse/scanpy/issues/510:2012,energy efficiency,current,current,2012,"a in a simple and intuitive way. Truly, from the bottom of my heart, thank you, this package is awesome and I deeply appreciate all the work the scanpy team and Theis lab has put into it. . With that said, I find myself wanting to do things with scanpy that aren't currently supported. . (1) Clustering cells based on a restricted set of genes as opposed to principle components: I understand that this is a biased approach, but I find myself wanting to do this to explore datasets and how genes vary together. I believe it would aid in understanding data topology to reduce the dimensionality of the dataset by looking at a subset of key genes that are picked by an expert who has domain knowledge. I think this is best integrated with a separate sc.pp.neighbors-like function that uses a gene list, as opposed to PCs. Integration into downstream plotting can be done with separate functions that have a similar name (sc.tl.umap_genelist or sc.tl.umap_sparse; sc.tl.paga_genelist or sc.tl.paga_sparse). . I envision that this would also be a nice stepping stone towards multi-modal analysis with CITE-seq. . (2) I want to use PAGA and UMAP to display my CytOF data as well as integrate scRNA-seq/CytOF data together. I saw a pre-print on bioRxiv that indicated that you are working on this aspect of PAGA/scanpy. This was very exciting to me and I am glad that this is being developed. Is it possible to access the current version of this software? . I think that major partitions identified with CytOF or scRNA-seq can be linked together providing a coarse-grained mechanism to demonstrate how heterogeneity identified with each technique relates to each other based on a given experimental time point. . (3) Histogram integration in the plotting api for QC metrics would be helpful. While scatter plots and violin plots are effective, I find myself wanting to make cuts (bounds on percent mitochondrial or nGenes) in my data based off histograms. Again, thank you so much for the amazing software!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/510
https://github.com/scverse/scanpy/issues/510:221,integrability,integr,integrated,221,"Geneset based clustering and CytOF Compatibility; I want to start off by mentioning how much I love scanpy. I recommend this package to everyone I know who is doing single cell sequencing. I love how PAGA and UMAP can be integrated together. PAGA makes appreciation of data topology so simple. In addition, it's so easy to do velocity analysis with the scvelo integration. Installing scanpy as well as hdf5/loom compatibility is remarkably easier on python than in R, which gives scanpy users an obvious advantage. . I've learned so much using this package and it has allowed me to display my data in a simple and intuitive way. Truly, from the bottom of my heart, thank you, this package is awesome and I deeply appreciate all the work the scanpy team and Theis lab has put into it. . With that said, I find myself wanting to do things with scanpy that aren't currently supported. . (1) Clustering cells based on a restricted set of genes as opposed to principle components: I understand that this is a biased approach, but I find myself wanting to do this to explore datasets and how genes vary together. I believe it would aid in understanding data topology to reduce the dimensionality of the dataset by looking at a subset of key genes that are picked by an expert who has domain knowledge. I think this is best integrated with a separate sc.pp.neighbors-like function that uses a gene list, as opposed to PCs. Integration into downstream plotting can be done with separate functions that have a similar name (sc.tl.umap_genelist or sc.tl.umap_sparse; sc.tl.paga_genelist or sc.tl.paga_sparse). . I envision that this would also be a nice stepping stone towards multi-modal analysis with CITE-seq. . (2) I want to use PAGA and UMAP to display my CytOF data as well as integrate scRNA-seq/CytOF data together. I saw a pre-print on bioRxiv that indicated that you are working on this aspect of PAGA/scanpy. This was very exciting to me and I am glad that this is being developed. Is it possible to",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/510
https://github.com/scverse/scanpy/issues/510:360,integrability,integr,integration,360,"Geneset based clustering and CytOF Compatibility; I want to start off by mentioning how much I love scanpy. I recommend this package to everyone I know who is doing single cell sequencing. I love how PAGA and UMAP can be integrated together. PAGA makes appreciation of data topology so simple. In addition, it's so easy to do velocity analysis with the scvelo integration. Installing scanpy as well as hdf5/loom compatibility is remarkably easier on python than in R, which gives scanpy users an obvious advantage. . I've learned so much using this package and it has allowed me to display my data in a simple and intuitive way. Truly, from the bottom of my heart, thank you, this package is awesome and I deeply appreciate all the work the scanpy team and Theis lab has put into it. . With that said, I find myself wanting to do things with scanpy that aren't currently supported. . (1) Clustering cells based on a restricted set of genes as opposed to principle components: I understand that this is a biased approach, but I find myself wanting to do this to explore datasets and how genes vary together. I believe it would aid in understanding data topology to reduce the dimensionality of the dataset by looking at a subset of key genes that are picked by an expert who has domain knowledge. I think this is best integrated with a separate sc.pp.neighbors-like function that uses a gene list, as opposed to PCs. Integration into downstream plotting can be done with separate functions that have a similar name (sc.tl.umap_genelist or sc.tl.umap_sparse; sc.tl.paga_genelist or sc.tl.paga_sparse). . I envision that this would also be a nice stepping stone towards multi-modal analysis with CITE-seq. . (2) I want to use PAGA and UMAP to display my CytOF data as well as integrate scRNA-seq/CytOF data together. I saw a pre-print on bioRxiv that indicated that you are working on this aspect of PAGA/scanpy. This was very exciting to me and I am glad that this is being developed. Is it possible to",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/510
https://github.com/scverse/scanpy/issues/510:964,integrability,compon,components,964,"Geneset based clustering and CytOF Compatibility; I want to start off by mentioning how much I love scanpy. I recommend this package to everyone I know who is doing single cell sequencing. I love how PAGA and UMAP can be integrated together. PAGA makes appreciation of data topology so simple. In addition, it's so easy to do velocity analysis with the scvelo integration. Installing scanpy as well as hdf5/loom compatibility is remarkably easier on python than in R, which gives scanpy users an obvious advantage. . I've learned so much using this package and it has allowed me to display my data in a simple and intuitive way. Truly, from the bottom of my heart, thank you, this package is awesome and I deeply appreciate all the work the scanpy team and Theis lab has put into it. . With that said, I find myself wanting to do things with scanpy that aren't currently supported. . (1) Clustering cells based on a restricted set of genes as opposed to principle components: I understand that this is a biased approach, but I find myself wanting to do this to explore datasets and how genes vary together. I believe it would aid in understanding data topology to reduce the dimensionality of the dataset by looking at a subset of key genes that are picked by an expert who has domain knowledge. I think this is best integrated with a separate sc.pp.neighbors-like function that uses a gene list, as opposed to PCs. Integration into downstream plotting can be done with separate functions that have a similar name (sc.tl.umap_genelist or sc.tl.umap_sparse; sc.tl.paga_genelist or sc.tl.paga_sparse). . I envision that this would also be a nice stepping stone towards multi-modal analysis with CITE-seq. . (2) I want to use PAGA and UMAP to display my CytOF data as well as integrate scRNA-seq/CytOF data together. I saw a pre-print on bioRxiv that indicated that you are working on this aspect of PAGA/scanpy. This was very exciting to me and I am glad that this is being developed. Is it possible to",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/510
https://github.com/scverse/scanpy/issues/510:1221,integrability,sub,subset,1221,"egrated together. PAGA makes appreciation of data topology so simple. In addition, it's so easy to do velocity analysis with the scvelo integration. Installing scanpy as well as hdf5/loom compatibility is remarkably easier on python than in R, which gives scanpy users an obvious advantage. . I've learned so much using this package and it has allowed me to display my data in a simple and intuitive way. Truly, from the bottom of my heart, thank you, this package is awesome and I deeply appreciate all the work the scanpy team and Theis lab has put into it. . With that said, I find myself wanting to do things with scanpy that aren't currently supported. . (1) Clustering cells based on a restricted set of genes as opposed to principle components: I understand that this is a biased approach, but I find myself wanting to do this to explore datasets and how genes vary together. I believe it would aid in understanding data topology to reduce the dimensionality of the dataset by looking at a subset of key genes that are picked by an expert who has domain knowledge. I think this is best integrated with a separate sc.pp.neighbors-like function that uses a gene list, as opposed to PCs. Integration into downstream plotting can be done with separate functions that have a similar name (sc.tl.umap_genelist or sc.tl.umap_sparse; sc.tl.paga_genelist or sc.tl.paga_sparse). . I envision that this would also be a nice stepping stone towards multi-modal analysis with CITE-seq. . (2) I want to use PAGA and UMAP to display my CytOF data as well as integrate scRNA-seq/CytOF data together. I saw a pre-print on bioRxiv that indicated that you are working on this aspect of PAGA/scanpy. This was very exciting to me and I am glad that this is being developed. Is it possible to access the current version of this software? . I think that major partitions identified with CytOF or scRNA-seq can be linked together providing a coarse-grained mechanism to demonstrate how heterogeneity identified with ea",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/510
https://github.com/scverse/scanpy/issues/510:1317,integrability,integr,integrated,1317," do velocity analysis with the scvelo integration. Installing scanpy as well as hdf5/loom compatibility is remarkably easier on python than in R, which gives scanpy users an obvious advantage. . I've learned so much using this package and it has allowed me to display my data in a simple and intuitive way. Truly, from the bottom of my heart, thank you, this package is awesome and I deeply appreciate all the work the scanpy team and Theis lab has put into it. . With that said, I find myself wanting to do things with scanpy that aren't currently supported. . (1) Clustering cells based on a restricted set of genes as opposed to principle components: I understand that this is a biased approach, but I find myself wanting to do this to explore datasets and how genes vary together. I believe it would aid in understanding data topology to reduce the dimensionality of the dataset by looking at a subset of key genes that are picked by an expert who has domain knowledge. I think this is best integrated with a separate sc.pp.neighbors-like function that uses a gene list, as opposed to PCs. Integration into downstream plotting can be done with separate functions that have a similar name (sc.tl.umap_genelist or sc.tl.umap_sparse; sc.tl.paga_genelist or sc.tl.paga_sparse). . I envision that this would also be a nice stepping stone towards multi-modal analysis with CITE-seq. . (2) I want to use PAGA and UMAP to display my CytOF data as well as integrate scRNA-seq/CytOF data together. I saw a pre-print on bioRxiv that indicated that you are working on this aspect of PAGA/scanpy. This was very exciting to me and I am glad that this is being developed. Is it possible to access the current version of this software? . I think that major partitions identified with CytOF or scRNA-seq can be linked together providing a coarse-grained mechanism to demonstrate how heterogeneity identified with each technique relates to each other based on a given experimental time point. . (3) Histogram integ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/510
https://github.com/scverse/scanpy/issues/510:1416,integrability,Integr,Integration,1416,"ity is remarkably easier on python than in R, which gives scanpy users an obvious advantage. . I've learned so much using this package and it has allowed me to display my data in a simple and intuitive way. Truly, from the bottom of my heart, thank you, this package is awesome and I deeply appreciate all the work the scanpy team and Theis lab has put into it. . With that said, I find myself wanting to do things with scanpy that aren't currently supported. . (1) Clustering cells based on a restricted set of genes as opposed to principle components: I understand that this is a biased approach, but I find myself wanting to do this to explore datasets and how genes vary together. I believe it would aid in understanding data topology to reduce the dimensionality of the dataset by looking at a subset of key genes that are picked by an expert who has domain knowledge. I think this is best integrated with a separate sc.pp.neighbors-like function that uses a gene list, as opposed to PCs. Integration into downstream plotting can be done with separate functions that have a similar name (sc.tl.umap_genelist or sc.tl.umap_sparse; sc.tl.paga_genelist or sc.tl.paga_sparse). . I envision that this would also be a nice stepping stone towards multi-modal analysis with CITE-seq. . (2) I want to use PAGA and UMAP to display my CytOF data as well as integrate scRNA-seq/CytOF data together. I saw a pre-print on bioRxiv that indicated that you are working on this aspect of PAGA/scanpy. This was very exciting to me and I am glad that this is being developed. Is it possible to access the current version of this software? . I think that major partitions identified with CytOF or scRNA-seq can be linked together providing a coarse-grained mechanism to demonstrate how heterogeneity identified with each technique relates to each other based on a given experimental time point. . (3) Histogram integration in the plotting api for QC metrics would be helpful. While scatter plots and violin plots are",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/510
https://github.com/scverse/scanpy/issues/510:1773,integrability,integr,integrate,1773,"a in a simple and intuitive way. Truly, from the bottom of my heart, thank you, this package is awesome and I deeply appreciate all the work the scanpy team and Theis lab has put into it. . With that said, I find myself wanting to do things with scanpy that aren't currently supported. . (1) Clustering cells based on a restricted set of genes as opposed to principle components: I understand that this is a biased approach, but I find myself wanting to do this to explore datasets and how genes vary together. I believe it would aid in understanding data topology to reduce the dimensionality of the dataset by looking at a subset of key genes that are picked by an expert who has domain knowledge. I think this is best integrated with a separate sc.pp.neighbors-like function that uses a gene list, as opposed to PCs. Integration into downstream plotting can be done with separate functions that have a similar name (sc.tl.umap_genelist or sc.tl.umap_sparse; sc.tl.paga_genelist or sc.tl.paga_sparse). . I envision that this would also be a nice stepping stone towards multi-modal analysis with CITE-seq. . (2) I want to use PAGA and UMAP to display my CytOF data as well as integrate scRNA-seq/CytOF data together. I saw a pre-print on bioRxiv that indicated that you are working on this aspect of PAGA/scanpy. This was very exciting to me and I am glad that this is being developed. Is it possible to access the current version of this software? . I think that major partitions identified with CytOF or scRNA-seq can be linked together providing a coarse-grained mechanism to demonstrate how heterogeneity identified with each technique relates to each other based on a given experimental time point. . (3) Histogram integration in the plotting api for QC metrics would be helpful. While scatter plots and violin plots are effective, I find myself wanting to make cuts (bounds on percent mitochondrial or nGenes) in my data based off histograms. Again, thank you so much for the amazing software!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/510
https://github.com/scverse/scanpy/issues/510:2020,integrability,version,version,2020,"a in a simple and intuitive way. Truly, from the bottom of my heart, thank you, this package is awesome and I deeply appreciate all the work the scanpy team and Theis lab has put into it. . With that said, I find myself wanting to do things with scanpy that aren't currently supported. . (1) Clustering cells based on a restricted set of genes as opposed to principle components: I understand that this is a biased approach, but I find myself wanting to do this to explore datasets and how genes vary together. I believe it would aid in understanding data topology to reduce the dimensionality of the dataset by looking at a subset of key genes that are picked by an expert who has domain knowledge. I think this is best integrated with a separate sc.pp.neighbors-like function that uses a gene list, as opposed to PCs. Integration into downstream plotting can be done with separate functions that have a similar name (sc.tl.umap_genelist or sc.tl.umap_sparse; sc.tl.paga_genelist or sc.tl.paga_sparse). . I envision that this would also be a nice stepping stone towards multi-modal analysis with CITE-seq. . (2) I want to use PAGA and UMAP to display my CytOF data as well as integrate scRNA-seq/CytOF data together. I saw a pre-print on bioRxiv that indicated that you are working on this aspect of PAGA/scanpy. This was very exciting to me and I am glad that this is being developed. Is it possible to access the current version of this software? . I think that major partitions identified with CytOF or scRNA-seq can be linked together providing a coarse-grained mechanism to demonstrate how heterogeneity identified with each technique relates to each other based on a given experimental time point. . (3) Histogram integration in the plotting api for QC metrics would be helpful. While scatter plots and violin plots are effective, I find myself wanting to make cuts (bounds on percent mitochondrial or nGenes) in my data based off histograms. Again, thank you so much for the amazing software!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/510
https://github.com/scverse/scanpy/issues/510:2317,integrability,integr,integration,2317,"a in a simple and intuitive way. Truly, from the bottom of my heart, thank you, this package is awesome and I deeply appreciate all the work the scanpy team and Theis lab has put into it. . With that said, I find myself wanting to do things with scanpy that aren't currently supported. . (1) Clustering cells based on a restricted set of genes as opposed to principle components: I understand that this is a biased approach, but I find myself wanting to do this to explore datasets and how genes vary together. I believe it would aid in understanding data topology to reduce the dimensionality of the dataset by looking at a subset of key genes that are picked by an expert who has domain knowledge. I think this is best integrated with a separate sc.pp.neighbors-like function that uses a gene list, as opposed to PCs. Integration into downstream plotting can be done with separate functions that have a similar name (sc.tl.umap_genelist or sc.tl.umap_sparse; sc.tl.paga_genelist or sc.tl.paga_sparse). . I envision that this would also be a nice stepping stone towards multi-modal analysis with CITE-seq. . (2) I want to use PAGA and UMAP to display my CytOF data as well as integrate scRNA-seq/CytOF data together. I saw a pre-print on bioRxiv that indicated that you are working on this aspect of PAGA/scanpy. This was very exciting to me and I am glad that this is being developed. Is it possible to access the current version of this software? . I think that major partitions identified with CytOF or scRNA-seq can be linked together providing a coarse-grained mechanism to demonstrate how heterogeneity identified with each technique relates to each other based on a given experimental time point. . (3) Histogram integration in the plotting api for QC metrics would be helpful. While scatter plots and violin plots are effective, I find myself wanting to make cuts (bounds on percent mitochondrial or nGenes) in my data based off histograms. Again, thank you so much for the amazing software!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/510
https://github.com/scverse/scanpy/issues/510:2345,integrability,api,api,2345,"a in a simple and intuitive way. Truly, from the bottom of my heart, thank you, this package is awesome and I deeply appreciate all the work the scanpy team and Theis lab has put into it. . With that said, I find myself wanting to do things with scanpy that aren't currently supported. . (1) Clustering cells based on a restricted set of genes as opposed to principle components: I understand that this is a biased approach, but I find myself wanting to do this to explore datasets and how genes vary together. I believe it would aid in understanding data topology to reduce the dimensionality of the dataset by looking at a subset of key genes that are picked by an expert who has domain knowledge. I think this is best integrated with a separate sc.pp.neighbors-like function that uses a gene list, as opposed to PCs. Integration into downstream plotting can be done with separate functions that have a similar name (sc.tl.umap_genelist or sc.tl.umap_sparse; sc.tl.paga_genelist or sc.tl.paga_sparse). . I envision that this would also be a nice stepping stone towards multi-modal analysis with CITE-seq. . (2) I want to use PAGA and UMAP to display my CytOF data as well as integrate scRNA-seq/CytOF data together. I saw a pre-print on bioRxiv that indicated that you are working on this aspect of PAGA/scanpy. This was very exciting to me and I am glad that this is being developed. Is it possible to access the current version of this software? . I think that major partitions identified with CytOF or scRNA-seq can be linked together providing a coarse-grained mechanism to demonstrate how heterogeneity identified with each technique relates to each other based on a given experimental time point. . (3) Histogram integration in the plotting api for QC metrics would be helpful. While scatter plots and violin plots are effective, I find myself wanting to make cuts (bounds on percent mitochondrial or nGenes) in my data based off histograms. Again, thank you so much for the amazing software!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/510
https://github.com/scverse/scanpy/issues/510:35,interoperability,Compatib,Compatibility,35,"Geneset based clustering and CytOF Compatibility; I want to start off by mentioning how much I love scanpy. I recommend this package to everyone I know who is doing single cell sequencing. I love how PAGA and UMAP can be integrated together. PAGA makes appreciation of data topology so simple. In addition, it's so easy to do velocity analysis with the scvelo integration. Installing scanpy as well as hdf5/loom compatibility is remarkably easier on python than in R, which gives scanpy users an obvious advantage. . I've learned so much using this package and it has allowed me to display my data in a simple and intuitive way. Truly, from the bottom of my heart, thank you, this package is awesome and I deeply appreciate all the work the scanpy team and Theis lab has put into it. . With that said, I find myself wanting to do things with scanpy that aren't currently supported. . (1) Clustering cells based on a restricted set of genes as opposed to principle components: I understand that this is a biased approach, but I find myself wanting to do this to explore datasets and how genes vary together. I believe it would aid in understanding data topology to reduce the dimensionality of the dataset by looking at a subset of key genes that are picked by an expert who has domain knowledge. I think this is best integrated with a separate sc.pp.neighbors-like function that uses a gene list, as opposed to PCs. Integration into downstream plotting can be done with separate functions that have a similar name (sc.tl.umap_genelist or sc.tl.umap_sparse; sc.tl.paga_genelist or sc.tl.paga_sparse). . I envision that this would also be a nice stepping stone towards multi-modal analysis with CITE-seq. . (2) I want to use PAGA and UMAP to display my CytOF data as well as integrate scRNA-seq/CytOF data together. I saw a pre-print on bioRxiv that indicated that you are working on this aspect of PAGA/scanpy. This was very exciting to me and I am glad that this is being developed. Is it possible to",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/510
https://github.com/scverse/scanpy/issues/510:221,interoperability,integr,integrated,221,"Geneset based clustering and CytOF Compatibility; I want to start off by mentioning how much I love scanpy. I recommend this package to everyone I know who is doing single cell sequencing. I love how PAGA and UMAP can be integrated together. PAGA makes appreciation of data topology so simple. In addition, it's so easy to do velocity analysis with the scvelo integration. Installing scanpy as well as hdf5/loom compatibility is remarkably easier on python than in R, which gives scanpy users an obvious advantage. . I've learned so much using this package and it has allowed me to display my data in a simple and intuitive way. Truly, from the bottom of my heart, thank you, this package is awesome and I deeply appreciate all the work the scanpy team and Theis lab has put into it. . With that said, I find myself wanting to do things with scanpy that aren't currently supported. . (1) Clustering cells based on a restricted set of genes as opposed to principle components: I understand that this is a biased approach, but I find myself wanting to do this to explore datasets and how genes vary together. I believe it would aid in understanding data topology to reduce the dimensionality of the dataset by looking at a subset of key genes that are picked by an expert who has domain knowledge. I think this is best integrated with a separate sc.pp.neighbors-like function that uses a gene list, as opposed to PCs. Integration into downstream plotting can be done with separate functions that have a similar name (sc.tl.umap_genelist or sc.tl.umap_sparse; sc.tl.paga_genelist or sc.tl.paga_sparse). . I envision that this would also be a nice stepping stone towards multi-modal analysis with CITE-seq. . (2) I want to use PAGA and UMAP to display my CytOF data as well as integrate scRNA-seq/CytOF data together. I saw a pre-print on bioRxiv that indicated that you are working on this aspect of PAGA/scanpy. This was very exciting to me and I am glad that this is being developed. Is it possible to",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/510
https://github.com/scverse/scanpy/issues/510:360,interoperability,integr,integration,360,"Geneset based clustering and CytOF Compatibility; I want to start off by mentioning how much I love scanpy. I recommend this package to everyone I know who is doing single cell sequencing. I love how PAGA and UMAP can be integrated together. PAGA makes appreciation of data topology so simple. In addition, it's so easy to do velocity analysis with the scvelo integration. Installing scanpy as well as hdf5/loom compatibility is remarkably easier on python than in R, which gives scanpy users an obvious advantage. . I've learned so much using this package and it has allowed me to display my data in a simple and intuitive way. Truly, from the bottom of my heart, thank you, this package is awesome and I deeply appreciate all the work the scanpy team and Theis lab has put into it. . With that said, I find myself wanting to do things with scanpy that aren't currently supported. . (1) Clustering cells based on a restricted set of genes as opposed to principle components: I understand that this is a biased approach, but I find myself wanting to do this to explore datasets and how genes vary together. I believe it would aid in understanding data topology to reduce the dimensionality of the dataset by looking at a subset of key genes that are picked by an expert who has domain knowledge. I think this is best integrated with a separate sc.pp.neighbors-like function that uses a gene list, as opposed to PCs. Integration into downstream plotting can be done with separate functions that have a similar name (sc.tl.umap_genelist or sc.tl.umap_sparse; sc.tl.paga_genelist or sc.tl.paga_sparse). . I envision that this would also be a nice stepping stone towards multi-modal analysis with CITE-seq. . (2) I want to use PAGA and UMAP to display my CytOF data as well as integrate scRNA-seq/CytOF data together. I saw a pre-print on bioRxiv that indicated that you are working on this aspect of PAGA/scanpy. This was very exciting to me and I am glad that this is being developed. Is it possible to",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/510
https://github.com/scverse/scanpy/issues/510:412,interoperability,compatib,compatibility,412,"Geneset based clustering and CytOF Compatibility; I want to start off by mentioning how much I love scanpy. I recommend this package to everyone I know who is doing single cell sequencing. I love how PAGA and UMAP can be integrated together. PAGA makes appreciation of data topology so simple. In addition, it's so easy to do velocity analysis with the scvelo integration. Installing scanpy as well as hdf5/loom compatibility is remarkably easier on python than in R, which gives scanpy users an obvious advantage. . I've learned so much using this package and it has allowed me to display my data in a simple and intuitive way. Truly, from the bottom of my heart, thank you, this package is awesome and I deeply appreciate all the work the scanpy team and Theis lab has put into it. . With that said, I find myself wanting to do things with scanpy that aren't currently supported. . (1) Clustering cells based on a restricted set of genes as opposed to principle components: I understand that this is a biased approach, but I find myself wanting to do this to explore datasets and how genes vary together. I believe it would aid in understanding data topology to reduce the dimensionality of the dataset by looking at a subset of key genes that are picked by an expert who has domain knowledge. I think this is best integrated with a separate sc.pp.neighbors-like function that uses a gene list, as opposed to PCs. Integration into downstream plotting can be done with separate functions that have a similar name (sc.tl.umap_genelist or sc.tl.umap_sparse; sc.tl.paga_genelist or sc.tl.paga_sparse). . I envision that this would also be a nice stepping stone towards multi-modal analysis with CITE-seq. . (2) I want to use PAGA and UMAP to display my CytOF data as well as integrate scRNA-seq/CytOF data together. I saw a pre-print on bioRxiv that indicated that you are working on this aspect of PAGA/scanpy. This was very exciting to me and I am glad that this is being developed. Is it possible to",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/510
https://github.com/scverse/scanpy/issues/510:964,interoperability,compon,components,964,"Geneset based clustering and CytOF Compatibility; I want to start off by mentioning how much I love scanpy. I recommend this package to everyone I know who is doing single cell sequencing. I love how PAGA and UMAP can be integrated together. PAGA makes appreciation of data topology so simple. In addition, it's so easy to do velocity analysis with the scvelo integration. Installing scanpy as well as hdf5/loom compatibility is remarkably easier on python than in R, which gives scanpy users an obvious advantage. . I've learned so much using this package and it has allowed me to display my data in a simple and intuitive way. Truly, from the bottom of my heart, thank you, this package is awesome and I deeply appreciate all the work the scanpy team and Theis lab has put into it. . With that said, I find myself wanting to do things with scanpy that aren't currently supported. . (1) Clustering cells based on a restricted set of genes as opposed to principle components: I understand that this is a biased approach, but I find myself wanting to do this to explore datasets and how genes vary together. I believe it would aid in understanding data topology to reduce the dimensionality of the dataset by looking at a subset of key genes that are picked by an expert who has domain knowledge. I think this is best integrated with a separate sc.pp.neighbors-like function that uses a gene list, as opposed to PCs. Integration into downstream plotting can be done with separate functions that have a similar name (sc.tl.umap_genelist or sc.tl.umap_sparse; sc.tl.paga_genelist or sc.tl.paga_sparse). . I envision that this would also be a nice stepping stone towards multi-modal analysis with CITE-seq. . (2) I want to use PAGA and UMAP to display my CytOF data as well as integrate scRNA-seq/CytOF data together. I saw a pre-print on bioRxiv that indicated that you are working on this aspect of PAGA/scanpy. This was very exciting to me and I am glad that this is being developed. Is it possible to",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/510
https://github.com/scverse/scanpy/issues/510:1317,interoperability,integr,integrated,1317," do velocity analysis with the scvelo integration. Installing scanpy as well as hdf5/loom compatibility is remarkably easier on python than in R, which gives scanpy users an obvious advantage. . I've learned so much using this package and it has allowed me to display my data in a simple and intuitive way. Truly, from the bottom of my heart, thank you, this package is awesome and I deeply appreciate all the work the scanpy team and Theis lab has put into it. . With that said, I find myself wanting to do things with scanpy that aren't currently supported. . (1) Clustering cells based on a restricted set of genes as opposed to principle components: I understand that this is a biased approach, but I find myself wanting to do this to explore datasets and how genes vary together. I believe it would aid in understanding data topology to reduce the dimensionality of the dataset by looking at a subset of key genes that are picked by an expert who has domain knowledge. I think this is best integrated with a separate sc.pp.neighbors-like function that uses a gene list, as opposed to PCs. Integration into downstream plotting can be done with separate functions that have a similar name (sc.tl.umap_genelist or sc.tl.umap_sparse; sc.tl.paga_genelist or sc.tl.paga_sparse). . I envision that this would also be a nice stepping stone towards multi-modal analysis with CITE-seq. . (2) I want to use PAGA and UMAP to display my CytOF data as well as integrate scRNA-seq/CytOF data together. I saw a pre-print on bioRxiv that indicated that you are working on this aspect of PAGA/scanpy. This was very exciting to me and I am glad that this is being developed. Is it possible to access the current version of this software? . I think that major partitions identified with CytOF or scRNA-seq can be linked together providing a coarse-grained mechanism to demonstrate how heterogeneity identified with each technique relates to each other based on a given experimental time point. . (3) Histogram integ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/510
https://github.com/scverse/scanpy/issues/510:1416,interoperability,Integr,Integration,1416,"ity is remarkably easier on python than in R, which gives scanpy users an obvious advantage. . I've learned so much using this package and it has allowed me to display my data in a simple and intuitive way. Truly, from the bottom of my heart, thank you, this package is awesome and I deeply appreciate all the work the scanpy team and Theis lab has put into it. . With that said, I find myself wanting to do things with scanpy that aren't currently supported. . (1) Clustering cells based on a restricted set of genes as opposed to principle components: I understand that this is a biased approach, but I find myself wanting to do this to explore datasets and how genes vary together. I believe it would aid in understanding data topology to reduce the dimensionality of the dataset by looking at a subset of key genes that are picked by an expert who has domain knowledge. I think this is best integrated with a separate sc.pp.neighbors-like function that uses a gene list, as opposed to PCs. Integration into downstream plotting can be done with separate functions that have a similar name (sc.tl.umap_genelist or sc.tl.umap_sparse; sc.tl.paga_genelist or sc.tl.paga_sparse). . I envision that this would also be a nice stepping stone towards multi-modal analysis with CITE-seq. . (2) I want to use PAGA and UMAP to display my CytOF data as well as integrate scRNA-seq/CytOF data together. I saw a pre-print on bioRxiv that indicated that you are working on this aspect of PAGA/scanpy. This was very exciting to me and I am glad that this is being developed. Is it possible to access the current version of this software? . I think that major partitions identified with CytOF or scRNA-seq can be linked together providing a coarse-grained mechanism to demonstrate how heterogeneity identified with each technique relates to each other based on a given experimental time point. . (3) Histogram integration in the plotting api for QC metrics would be helpful. While scatter plots and violin plots are",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/510
https://github.com/scverse/scanpy/issues/510:1773,interoperability,integr,integrate,1773,"a in a simple and intuitive way. Truly, from the bottom of my heart, thank you, this package is awesome and I deeply appreciate all the work the scanpy team and Theis lab has put into it. . With that said, I find myself wanting to do things with scanpy that aren't currently supported. . (1) Clustering cells based on a restricted set of genes as opposed to principle components: I understand that this is a biased approach, but I find myself wanting to do this to explore datasets and how genes vary together. I believe it would aid in understanding data topology to reduce the dimensionality of the dataset by looking at a subset of key genes that are picked by an expert who has domain knowledge. I think this is best integrated with a separate sc.pp.neighbors-like function that uses a gene list, as opposed to PCs. Integration into downstream plotting can be done with separate functions that have a similar name (sc.tl.umap_genelist or sc.tl.umap_sparse; sc.tl.paga_genelist or sc.tl.paga_sparse). . I envision that this would also be a nice stepping stone towards multi-modal analysis with CITE-seq. . (2) I want to use PAGA and UMAP to display my CytOF data as well as integrate scRNA-seq/CytOF data together. I saw a pre-print on bioRxiv that indicated that you are working on this aspect of PAGA/scanpy. This was very exciting to me and I am glad that this is being developed. Is it possible to access the current version of this software? . I think that major partitions identified with CytOF or scRNA-seq can be linked together providing a coarse-grained mechanism to demonstrate how heterogeneity identified with each technique relates to each other based on a given experimental time point. . (3) Histogram integration in the plotting api for QC metrics would be helpful. While scatter plots and violin plots are effective, I find myself wanting to make cuts (bounds on percent mitochondrial or nGenes) in my data based off histograms. Again, thank you so much for the amazing software!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/510
https://github.com/scverse/scanpy/issues/510:2192,interoperability,heterogen,heterogeneity,2192,"a in a simple and intuitive way. Truly, from the bottom of my heart, thank you, this package is awesome and I deeply appreciate all the work the scanpy team and Theis lab has put into it. . With that said, I find myself wanting to do things with scanpy that aren't currently supported. . (1) Clustering cells based on a restricted set of genes as opposed to principle components: I understand that this is a biased approach, but I find myself wanting to do this to explore datasets and how genes vary together. I believe it would aid in understanding data topology to reduce the dimensionality of the dataset by looking at a subset of key genes that are picked by an expert who has domain knowledge. I think this is best integrated with a separate sc.pp.neighbors-like function that uses a gene list, as opposed to PCs. Integration into downstream plotting can be done with separate functions that have a similar name (sc.tl.umap_genelist or sc.tl.umap_sparse; sc.tl.paga_genelist or sc.tl.paga_sparse). . I envision that this would also be a nice stepping stone towards multi-modal analysis with CITE-seq. . (2) I want to use PAGA and UMAP to display my CytOF data as well as integrate scRNA-seq/CytOF data together. I saw a pre-print on bioRxiv that indicated that you are working on this aspect of PAGA/scanpy. This was very exciting to me and I am glad that this is being developed. Is it possible to access the current version of this software? . I think that major partitions identified with CytOF or scRNA-seq can be linked together providing a coarse-grained mechanism to demonstrate how heterogeneity identified with each technique relates to each other based on a given experimental time point. . (3) Histogram integration in the plotting api for QC metrics would be helpful. While scatter plots and violin plots are effective, I find myself wanting to make cuts (bounds on percent mitochondrial or nGenes) in my data based off histograms. Again, thank you so much for the amazing software!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/510
https://github.com/scverse/scanpy/issues/510:2317,interoperability,integr,integration,2317,"a in a simple and intuitive way. Truly, from the bottom of my heart, thank you, this package is awesome and I deeply appreciate all the work the scanpy team and Theis lab has put into it. . With that said, I find myself wanting to do things with scanpy that aren't currently supported. . (1) Clustering cells based on a restricted set of genes as opposed to principle components: I understand that this is a biased approach, but I find myself wanting to do this to explore datasets and how genes vary together. I believe it would aid in understanding data topology to reduce the dimensionality of the dataset by looking at a subset of key genes that are picked by an expert who has domain knowledge. I think this is best integrated with a separate sc.pp.neighbors-like function that uses a gene list, as opposed to PCs. Integration into downstream plotting can be done with separate functions that have a similar name (sc.tl.umap_genelist or sc.tl.umap_sparse; sc.tl.paga_genelist or sc.tl.paga_sparse). . I envision that this would also be a nice stepping stone towards multi-modal analysis with CITE-seq. . (2) I want to use PAGA and UMAP to display my CytOF data as well as integrate scRNA-seq/CytOF data together. I saw a pre-print on bioRxiv that indicated that you are working on this aspect of PAGA/scanpy. This was very exciting to me and I am glad that this is being developed. Is it possible to access the current version of this software? . I think that major partitions identified with CytOF or scRNA-seq can be linked together providing a coarse-grained mechanism to demonstrate how heterogeneity identified with each technique relates to each other based on a given experimental time point. . (3) Histogram integration in the plotting api for QC metrics would be helpful. While scatter plots and violin plots are effective, I find myself wanting to make cuts (bounds on percent mitochondrial or nGenes) in my data based off histograms. Again, thank you so much for the amazing software!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/510
https://github.com/scverse/scanpy/issues/510:2345,interoperability,api,api,2345,"a in a simple and intuitive way. Truly, from the bottom of my heart, thank you, this package is awesome and I deeply appreciate all the work the scanpy team and Theis lab has put into it. . With that said, I find myself wanting to do things with scanpy that aren't currently supported. . (1) Clustering cells based on a restricted set of genes as opposed to principle components: I understand that this is a biased approach, but I find myself wanting to do this to explore datasets and how genes vary together. I believe it would aid in understanding data topology to reduce the dimensionality of the dataset by looking at a subset of key genes that are picked by an expert who has domain knowledge. I think this is best integrated with a separate sc.pp.neighbors-like function that uses a gene list, as opposed to PCs. Integration into downstream plotting can be done with separate functions that have a similar name (sc.tl.umap_genelist or sc.tl.umap_sparse; sc.tl.paga_genelist or sc.tl.paga_sparse). . I envision that this would also be a nice stepping stone towards multi-modal analysis with CITE-seq. . (2) I want to use PAGA and UMAP to display my CytOF data as well as integrate scRNA-seq/CytOF data together. I saw a pre-print on bioRxiv that indicated that you are working on this aspect of PAGA/scanpy. This was very exciting to me and I am glad that this is being developed. Is it possible to access the current version of this software? . I think that major partitions identified with CytOF or scRNA-seq can be linked together providing a coarse-grained mechanism to demonstrate how heterogeneity identified with each technique relates to each other based on a given experimental time point. . (3) Histogram integration in the plotting api for QC metrics would be helpful. While scatter plots and violin plots are effective, I find myself wanting to make cuts (bounds on percent mitochondrial or nGenes) in my data based off histograms. Again, thank you so much for the amazing software!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/510
https://github.com/scverse/scanpy/issues/510:125,modifiability,pac,package,125,"Geneset based clustering and CytOF Compatibility; I want to start off by mentioning how much I love scanpy. I recommend this package to everyone I know who is doing single cell sequencing. I love how PAGA and UMAP can be integrated together. PAGA makes appreciation of data topology so simple. In addition, it's so easy to do velocity analysis with the scvelo integration. Installing scanpy as well as hdf5/loom compatibility is remarkably easier on python than in R, which gives scanpy users an obvious advantage. . I've learned so much using this package and it has allowed me to display my data in a simple and intuitive way. Truly, from the bottom of my heart, thank you, this package is awesome and I deeply appreciate all the work the scanpy team and Theis lab has put into it. . With that said, I find myself wanting to do things with scanpy that aren't currently supported. . (1) Clustering cells based on a restricted set of genes as opposed to principle components: I understand that this is a biased approach, but I find myself wanting to do this to explore datasets and how genes vary together. I believe it would aid in understanding data topology to reduce the dimensionality of the dataset by looking at a subset of key genes that are picked by an expert who has domain knowledge. I think this is best integrated with a separate sc.pp.neighbors-like function that uses a gene list, as opposed to PCs. Integration into downstream plotting can be done with separate functions that have a similar name (sc.tl.umap_genelist or sc.tl.umap_sparse; sc.tl.paga_genelist or sc.tl.paga_sparse). . I envision that this would also be a nice stepping stone towards multi-modal analysis with CITE-seq. . (2) I want to use PAGA and UMAP to display my CytOF data as well as integrate scRNA-seq/CytOF data together. I saw a pre-print on bioRxiv that indicated that you are working on this aspect of PAGA/scanpy. This was very exciting to me and I am glad that this is being developed. Is it possible to",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/510
https://github.com/scverse/scanpy/issues/510:221,modifiability,integr,integrated,221,"Geneset based clustering and CytOF Compatibility; I want to start off by mentioning how much I love scanpy. I recommend this package to everyone I know who is doing single cell sequencing. I love how PAGA and UMAP can be integrated together. PAGA makes appreciation of data topology so simple. In addition, it's so easy to do velocity analysis with the scvelo integration. Installing scanpy as well as hdf5/loom compatibility is remarkably easier on python than in R, which gives scanpy users an obvious advantage. . I've learned so much using this package and it has allowed me to display my data in a simple and intuitive way. Truly, from the bottom of my heart, thank you, this package is awesome and I deeply appreciate all the work the scanpy team and Theis lab has put into it. . With that said, I find myself wanting to do things with scanpy that aren't currently supported. . (1) Clustering cells based on a restricted set of genes as opposed to principle components: I understand that this is a biased approach, but I find myself wanting to do this to explore datasets and how genes vary together. I believe it would aid in understanding data topology to reduce the dimensionality of the dataset by looking at a subset of key genes that are picked by an expert who has domain knowledge. I think this is best integrated with a separate sc.pp.neighbors-like function that uses a gene list, as opposed to PCs. Integration into downstream plotting can be done with separate functions that have a similar name (sc.tl.umap_genelist or sc.tl.umap_sparse; sc.tl.paga_genelist or sc.tl.paga_sparse). . I envision that this would also be a nice stepping stone towards multi-modal analysis with CITE-seq. . (2) I want to use PAGA and UMAP to display my CytOF data as well as integrate scRNA-seq/CytOF data together. I saw a pre-print on bioRxiv that indicated that you are working on this aspect of PAGA/scanpy. This was very exciting to me and I am glad that this is being developed. Is it possible to",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/510
https://github.com/scverse/scanpy/issues/510:360,modifiability,integr,integration,360,"Geneset based clustering and CytOF Compatibility; I want to start off by mentioning how much I love scanpy. I recommend this package to everyone I know who is doing single cell sequencing. I love how PAGA and UMAP can be integrated together. PAGA makes appreciation of data topology so simple. In addition, it's so easy to do velocity analysis with the scvelo integration. Installing scanpy as well as hdf5/loom compatibility is remarkably easier on python than in R, which gives scanpy users an obvious advantage. . I've learned so much using this package and it has allowed me to display my data in a simple and intuitive way. Truly, from the bottom of my heart, thank you, this package is awesome and I deeply appreciate all the work the scanpy team and Theis lab has put into it. . With that said, I find myself wanting to do things with scanpy that aren't currently supported. . (1) Clustering cells based on a restricted set of genes as opposed to principle components: I understand that this is a biased approach, but I find myself wanting to do this to explore datasets and how genes vary together. I believe it would aid in understanding data topology to reduce the dimensionality of the dataset by looking at a subset of key genes that are picked by an expert who has domain knowledge. I think this is best integrated with a separate sc.pp.neighbors-like function that uses a gene list, as opposed to PCs. Integration into downstream plotting can be done with separate functions that have a similar name (sc.tl.umap_genelist or sc.tl.umap_sparse; sc.tl.paga_genelist or sc.tl.paga_sparse). . I envision that this would also be a nice stepping stone towards multi-modal analysis with CITE-seq. . (2) I want to use PAGA and UMAP to display my CytOF data as well as integrate scRNA-seq/CytOF data together. I saw a pre-print on bioRxiv that indicated that you are working on this aspect of PAGA/scanpy. This was very exciting to me and I am glad that this is being developed. Is it possible to",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/510
https://github.com/scverse/scanpy/issues/510:549,modifiability,pac,package,549,"Geneset based clustering and CytOF Compatibility; I want to start off by mentioning how much I love scanpy. I recommend this package to everyone I know who is doing single cell sequencing. I love how PAGA and UMAP can be integrated together. PAGA makes appreciation of data topology so simple. In addition, it's so easy to do velocity analysis with the scvelo integration. Installing scanpy as well as hdf5/loom compatibility is remarkably easier on python than in R, which gives scanpy users an obvious advantage. . I've learned so much using this package and it has allowed me to display my data in a simple and intuitive way. Truly, from the bottom of my heart, thank you, this package is awesome and I deeply appreciate all the work the scanpy team and Theis lab has put into it. . With that said, I find myself wanting to do things with scanpy that aren't currently supported. . (1) Clustering cells based on a restricted set of genes as opposed to principle components: I understand that this is a biased approach, but I find myself wanting to do this to explore datasets and how genes vary together. I believe it would aid in understanding data topology to reduce the dimensionality of the dataset by looking at a subset of key genes that are picked by an expert who has domain knowledge. I think this is best integrated with a separate sc.pp.neighbors-like function that uses a gene list, as opposed to PCs. Integration into downstream plotting can be done with separate functions that have a similar name (sc.tl.umap_genelist or sc.tl.umap_sparse; sc.tl.paga_genelist or sc.tl.paga_sparse). . I envision that this would also be a nice stepping stone towards multi-modal analysis with CITE-seq. . (2) I want to use PAGA and UMAP to display my CytOF data as well as integrate scRNA-seq/CytOF data together. I saw a pre-print on bioRxiv that indicated that you are working on this aspect of PAGA/scanpy. This was very exciting to me and I am glad that this is being developed. Is it possible to",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/510
https://github.com/scverse/scanpy/issues/510:681,modifiability,pac,package,681,"Geneset based clustering and CytOF Compatibility; I want to start off by mentioning how much I love scanpy. I recommend this package to everyone I know who is doing single cell sequencing. I love how PAGA and UMAP can be integrated together. PAGA makes appreciation of data topology so simple. In addition, it's so easy to do velocity analysis with the scvelo integration. Installing scanpy as well as hdf5/loom compatibility is remarkably easier on python than in R, which gives scanpy users an obvious advantage. . I've learned so much using this package and it has allowed me to display my data in a simple and intuitive way. Truly, from the bottom of my heart, thank you, this package is awesome and I deeply appreciate all the work the scanpy team and Theis lab has put into it. . With that said, I find myself wanting to do things with scanpy that aren't currently supported. . (1) Clustering cells based on a restricted set of genes as opposed to principle components: I understand that this is a biased approach, but I find myself wanting to do this to explore datasets and how genes vary together. I believe it would aid in understanding data topology to reduce the dimensionality of the dataset by looking at a subset of key genes that are picked by an expert who has domain knowledge. I think this is best integrated with a separate sc.pp.neighbors-like function that uses a gene list, as opposed to PCs. Integration into downstream plotting can be done with separate functions that have a similar name (sc.tl.umap_genelist or sc.tl.umap_sparse; sc.tl.paga_genelist or sc.tl.paga_sparse). . I envision that this would also be a nice stepping stone towards multi-modal analysis with CITE-seq. . (2) I want to use PAGA and UMAP to display my CytOF data as well as integrate scRNA-seq/CytOF data together. I saw a pre-print on bioRxiv that indicated that you are working on this aspect of PAGA/scanpy. This was very exciting to me and I am glad that this is being developed. Is it possible to",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/510
https://github.com/scverse/scanpy/issues/510:964,modifiability,compon,components,964,"Geneset based clustering and CytOF Compatibility; I want to start off by mentioning how much I love scanpy. I recommend this package to everyone I know who is doing single cell sequencing. I love how PAGA and UMAP can be integrated together. PAGA makes appreciation of data topology so simple. In addition, it's so easy to do velocity analysis with the scvelo integration. Installing scanpy as well as hdf5/loom compatibility is remarkably easier on python than in R, which gives scanpy users an obvious advantage. . I've learned so much using this package and it has allowed me to display my data in a simple and intuitive way. Truly, from the bottom of my heart, thank you, this package is awesome and I deeply appreciate all the work the scanpy team and Theis lab has put into it. . With that said, I find myself wanting to do things with scanpy that aren't currently supported. . (1) Clustering cells based on a restricted set of genes as opposed to principle components: I understand that this is a biased approach, but I find myself wanting to do this to explore datasets and how genes vary together. I believe it would aid in understanding data topology to reduce the dimensionality of the dataset by looking at a subset of key genes that are picked by an expert who has domain knowledge. I think this is best integrated with a separate sc.pp.neighbors-like function that uses a gene list, as opposed to PCs. Integration into downstream plotting can be done with separate functions that have a similar name (sc.tl.umap_genelist or sc.tl.umap_sparse; sc.tl.paga_genelist or sc.tl.paga_sparse). . I envision that this would also be a nice stepping stone towards multi-modal analysis with CITE-seq. . (2) I want to use PAGA and UMAP to display my CytOF data as well as integrate scRNA-seq/CytOF data together. I saw a pre-print on bioRxiv that indicated that you are working on this aspect of PAGA/scanpy. This was very exciting to me and I am glad that this is being developed. Is it possible to",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/510
https://github.com/scverse/scanpy/issues/510:1317,modifiability,integr,integrated,1317," do velocity analysis with the scvelo integration. Installing scanpy as well as hdf5/loom compatibility is remarkably easier on python than in R, which gives scanpy users an obvious advantage. . I've learned so much using this package and it has allowed me to display my data in a simple and intuitive way. Truly, from the bottom of my heart, thank you, this package is awesome and I deeply appreciate all the work the scanpy team and Theis lab has put into it. . With that said, I find myself wanting to do things with scanpy that aren't currently supported. . (1) Clustering cells based on a restricted set of genes as opposed to principle components: I understand that this is a biased approach, but I find myself wanting to do this to explore datasets and how genes vary together. I believe it would aid in understanding data topology to reduce the dimensionality of the dataset by looking at a subset of key genes that are picked by an expert who has domain knowledge. I think this is best integrated with a separate sc.pp.neighbors-like function that uses a gene list, as opposed to PCs. Integration into downstream plotting can be done with separate functions that have a similar name (sc.tl.umap_genelist or sc.tl.umap_sparse; sc.tl.paga_genelist or sc.tl.paga_sparse). . I envision that this would also be a nice stepping stone towards multi-modal analysis with CITE-seq. . (2) I want to use PAGA and UMAP to display my CytOF data as well as integrate scRNA-seq/CytOF data together. I saw a pre-print on bioRxiv that indicated that you are working on this aspect of PAGA/scanpy. This was very exciting to me and I am glad that this is being developed. Is it possible to access the current version of this software? . I think that major partitions identified with CytOF or scRNA-seq can be linked together providing a coarse-grained mechanism to demonstrate how heterogeneity identified with each technique relates to each other based on a given experimental time point. . (3) Histogram integ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/510
https://github.com/scverse/scanpy/issues/510:1416,modifiability,Integr,Integration,1416,"ity is remarkably easier on python than in R, which gives scanpy users an obvious advantage. . I've learned so much using this package and it has allowed me to display my data in a simple and intuitive way. Truly, from the bottom of my heart, thank you, this package is awesome and I deeply appreciate all the work the scanpy team and Theis lab has put into it. . With that said, I find myself wanting to do things with scanpy that aren't currently supported. . (1) Clustering cells based on a restricted set of genes as opposed to principle components: I understand that this is a biased approach, but I find myself wanting to do this to explore datasets and how genes vary together. I believe it would aid in understanding data topology to reduce the dimensionality of the dataset by looking at a subset of key genes that are picked by an expert who has domain knowledge. I think this is best integrated with a separate sc.pp.neighbors-like function that uses a gene list, as opposed to PCs. Integration into downstream plotting can be done with separate functions that have a similar name (sc.tl.umap_genelist or sc.tl.umap_sparse; sc.tl.paga_genelist or sc.tl.paga_sparse). . I envision that this would also be a nice stepping stone towards multi-modal analysis with CITE-seq. . (2) I want to use PAGA and UMAP to display my CytOF data as well as integrate scRNA-seq/CytOF data together. I saw a pre-print on bioRxiv that indicated that you are working on this aspect of PAGA/scanpy. This was very exciting to me and I am glad that this is being developed. Is it possible to access the current version of this software? . I think that major partitions identified with CytOF or scRNA-seq can be linked together providing a coarse-grained mechanism to demonstrate how heterogeneity identified with each technique relates to each other based on a given experimental time point. . (3) Histogram integration in the plotting api for QC metrics would be helpful. While scatter plots and violin plots are",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/510
https://github.com/scverse/scanpy/issues/510:1773,modifiability,integr,integrate,1773,"a in a simple and intuitive way. Truly, from the bottom of my heart, thank you, this package is awesome and I deeply appreciate all the work the scanpy team and Theis lab has put into it. . With that said, I find myself wanting to do things with scanpy that aren't currently supported. . (1) Clustering cells based on a restricted set of genes as opposed to principle components: I understand that this is a biased approach, but I find myself wanting to do this to explore datasets and how genes vary together. I believe it would aid in understanding data topology to reduce the dimensionality of the dataset by looking at a subset of key genes that are picked by an expert who has domain knowledge. I think this is best integrated with a separate sc.pp.neighbors-like function that uses a gene list, as opposed to PCs. Integration into downstream plotting can be done with separate functions that have a similar name (sc.tl.umap_genelist or sc.tl.umap_sparse; sc.tl.paga_genelist or sc.tl.paga_sparse). . I envision that this would also be a nice stepping stone towards multi-modal analysis with CITE-seq. . (2) I want to use PAGA and UMAP to display my CytOF data as well as integrate scRNA-seq/CytOF data together. I saw a pre-print on bioRxiv that indicated that you are working on this aspect of PAGA/scanpy. This was very exciting to me and I am glad that this is being developed. Is it possible to access the current version of this software? . I think that major partitions identified with CytOF or scRNA-seq can be linked together providing a coarse-grained mechanism to demonstrate how heterogeneity identified with each technique relates to each other based on a given experimental time point. . (3) Histogram integration in the plotting api for QC metrics would be helpful. While scatter plots and violin plots are effective, I find myself wanting to make cuts (bounds on percent mitochondrial or nGenes) in my data based off histograms. Again, thank you so much for the amazing software!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/510
https://github.com/scverse/scanpy/issues/510:2020,modifiability,version,version,2020,"a in a simple and intuitive way. Truly, from the bottom of my heart, thank you, this package is awesome and I deeply appreciate all the work the scanpy team and Theis lab has put into it. . With that said, I find myself wanting to do things with scanpy that aren't currently supported. . (1) Clustering cells based on a restricted set of genes as opposed to principle components: I understand that this is a biased approach, but I find myself wanting to do this to explore datasets and how genes vary together. I believe it would aid in understanding data topology to reduce the dimensionality of the dataset by looking at a subset of key genes that are picked by an expert who has domain knowledge. I think this is best integrated with a separate sc.pp.neighbors-like function that uses a gene list, as opposed to PCs. Integration into downstream plotting can be done with separate functions that have a similar name (sc.tl.umap_genelist or sc.tl.umap_sparse; sc.tl.paga_genelist or sc.tl.paga_sparse). . I envision that this would also be a nice stepping stone towards multi-modal analysis with CITE-seq. . (2) I want to use PAGA and UMAP to display my CytOF data as well as integrate scRNA-seq/CytOF data together. I saw a pre-print on bioRxiv that indicated that you are working on this aspect of PAGA/scanpy. This was very exciting to me and I am glad that this is being developed. Is it possible to access the current version of this software? . I think that major partitions identified with CytOF or scRNA-seq can be linked together providing a coarse-grained mechanism to demonstrate how heterogeneity identified with each technique relates to each other based on a given experimental time point. . (3) Histogram integration in the plotting api for QC metrics would be helpful. While scatter plots and violin plots are effective, I find myself wanting to make cuts (bounds on percent mitochondrial or nGenes) in my data based off histograms. Again, thank you so much for the amazing software!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/510
https://github.com/scverse/scanpy/issues/510:2317,modifiability,integr,integration,2317,"a in a simple and intuitive way. Truly, from the bottom of my heart, thank you, this package is awesome and I deeply appreciate all the work the scanpy team and Theis lab has put into it. . With that said, I find myself wanting to do things with scanpy that aren't currently supported. . (1) Clustering cells based on a restricted set of genes as opposed to principle components: I understand that this is a biased approach, but I find myself wanting to do this to explore datasets and how genes vary together. I believe it would aid in understanding data topology to reduce the dimensionality of the dataset by looking at a subset of key genes that are picked by an expert who has domain knowledge. I think this is best integrated with a separate sc.pp.neighbors-like function that uses a gene list, as opposed to PCs. Integration into downstream plotting can be done with separate functions that have a similar name (sc.tl.umap_genelist or sc.tl.umap_sparse; sc.tl.paga_genelist or sc.tl.paga_sparse). . I envision that this would also be a nice stepping stone towards multi-modal analysis with CITE-seq. . (2) I want to use PAGA and UMAP to display my CytOF data as well as integrate scRNA-seq/CytOF data together. I saw a pre-print on bioRxiv that indicated that you are working on this aspect of PAGA/scanpy. This was very exciting to me and I am glad that this is being developed. Is it possible to access the current version of this software? . I think that major partitions identified with CytOF or scRNA-seq can be linked together providing a coarse-grained mechanism to demonstrate how heterogeneity identified with each technique relates to each other based on a given experimental time point. . (3) Histogram integration in the plotting api for QC metrics would be helpful. While scatter plots and violin plots are effective, I find myself wanting to make cuts (bounds on percent mitochondrial or nGenes) in my data based off histograms. Again, thank you so much for the amazing software!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/510
https://github.com/scverse/scanpy/issues/510:2289,performance,time,time,2289,"a in a simple and intuitive way. Truly, from the bottom of my heart, thank you, this package is awesome and I deeply appreciate all the work the scanpy team and Theis lab has put into it. . With that said, I find myself wanting to do things with scanpy that aren't currently supported. . (1) Clustering cells based on a restricted set of genes as opposed to principle components: I understand that this is a biased approach, but I find myself wanting to do this to explore datasets and how genes vary together. I believe it would aid in understanding data topology to reduce the dimensionality of the dataset by looking at a subset of key genes that are picked by an expert who has domain knowledge. I think this is best integrated with a separate sc.pp.neighbors-like function that uses a gene list, as opposed to PCs. Integration into downstream plotting can be done with separate functions that have a similar name (sc.tl.umap_genelist or sc.tl.umap_sparse; sc.tl.paga_genelist or sc.tl.paga_sparse). . I envision that this would also be a nice stepping stone towards multi-modal analysis with CITE-seq. . (2) I want to use PAGA and UMAP to display my CytOF data as well as integrate scRNA-seq/CytOF data together. I saw a pre-print on bioRxiv that indicated that you are working on this aspect of PAGA/scanpy. This was very exciting to me and I am glad that this is being developed. Is it possible to access the current version of this software? . I think that major partitions identified with CytOF or scRNA-seq can be linked together providing a coarse-grained mechanism to demonstrate how heterogeneity identified with each technique relates to each other based on a given experimental time point. . (3) Histogram integration in the plotting api for QC metrics would be helpful. While scatter plots and violin plots are effective, I find myself wanting to make cuts (bounds on percent mitochondrial or nGenes) in my data based off histograms. Again, thank you so much for the amazing software!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/510
https://github.com/scverse/scanpy/issues/510:221,reliability,integr,integrated,221,"Geneset based clustering and CytOF Compatibility; I want to start off by mentioning how much I love scanpy. I recommend this package to everyone I know who is doing single cell sequencing. I love how PAGA and UMAP can be integrated together. PAGA makes appreciation of data topology so simple. In addition, it's so easy to do velocity analysis with the scvelo integration. Installing scanpy as well as hdf5/loom compatibility is remarkably easier on python than in R, which gives scanpy users an obvious advantage. . I've learned so much using this package and it has allowed me to display my data in a simple and intuitive way. Truly, from the bottom of my heart, thank you, this package is awesome and I deeply appreciate all the work the scanpy team and Theis lab has put into it. . With that said, I find myself wanting to do things with scanpy that aren't currently supported. . (1) Clustering cells based on a restricted set of genes as opposed to principle components: I understand that this is a biased approach, but I find myself wanting to do this to explore datasets and how genes vary together. I believe it would aid in understanding data topology to reduce the dimensionality of the dataset by looking at a subset of key genes that are picked by an expert who has domain knowledge. I think this is best integrated with a separate sc.pp.neighbors-like function that uses a gene list, as opposed to PCs. Integration into downstream plotting can be done with separate functions that have a similar name (sc.tl.umap_genelist or sc.tl.umap_sparse; sc.tl.paga_genelist or sc.tl.paga_sparse). . I envision that this would also be a nice stepping stone towards multi-modal analysis with CITE-seq. . (2) I want to use PAGA and UMAP to display my CytOF data as well as integrate scRNA-seq/CytOF data together. I saw a pre-print on bioRxiv that indicated that you are working on this aspect of PAGA/scanpy. This was very exciting to me and I am glad that this is being developed. Is it possible to",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/510
https://github.com/scverse/scanpy/issues/510:360,reliability,integr,integration,360,"Geneset based clustering and CytOF Compatibility; I want to start off by mentioning how much I love scanpy. I recommend this package to everyone I know who is doing single cell sequencing. I love how PAGA and UMAP can be integrated together. PAGA makes appreciation of data topology so simple. In addition, it's so easy to do velocity analysis with the scvelo integration. Installing scanpy as well as hdf5/loom compatibility is remarkably easier on python than in R, which gives scanpy users an obvious advantage. . I've learned so much using this package and it has allowed me to display my data in a simple and intuitive way. Truly, from the bottom of my heart, thank you, this package is awesome and I deeply appreciate all the work the scanpy team and Theis lab has put into it. . With that said, I find myself wanting to do things with scanpy that aren't currently supported. . (1) Clustering cells based on a restricted set of genes as opposed to principle components: I understand that this is a biased approach, but I find myself wanting to do this to explore datasets and how genes vary together. I believe it would aid in understanding data topology to reduce the dimensionality of the dataset by looking at a subset of key genes that are picked by an expert who has domain knowledge. I think this is best integrated with a separate sc.pp.neighbors-like function that uses a gene list, as opposed to PCs. Integration into downstream plotting can be done with separate functions that have a similar name (sc.tl.umap_genelist or sc.tl.umap_sparse; sc.tl.paga_genelist or sc.tl.paga_sparse). . I envision that this would also be a nice stepping stone towards multi-modal analysis with CITE-seq. . (2) I want to use PAGA and UMAP to display my CytOF data as well as integrate scRNA-seq/CytOF data together. I saw a pre-print on bioRxiv that indicated that you are working on this aspect of PAGA/scanpy. This was very exciting to me and I am glad that this is being developed. Is it possible to",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/510
https://github.com/scverse/scanpy/issues/510:1317,reliability,integr,integrated,1317," do velocity analysis with the scvelo integration. Installing scanpy as well as hdf5/loom compatibility is remarkably easier on python than in R, which gives scanpy users an obvious advantage. . I've learned so much using this package and it has allowed me to display my data in a simple and intuitive way. Truly, from the bottom of my heart, thank you, this package is awesome and I deeply appreciate all the work the scanpy team and Theis lab has put into it. . With that said, I find myself wanting to do things with scanpy that aren't currently supported. . (1) Clustering cells based on a restricted set of genes as opposed to principle components: I understand that this is a biased approach, but I find myself wanting to do this to explore datasets and how genes vary together. I believe it would aid in understanding data topology to reduce the dimensionality of the dataset by looking at a subset of key genes that are picked by an expert who has domain knowledge. I think this is best integrated with a separate sc.pp.neighbors-like function that uses a gene list, as opposed to PCs. Integration into downstream plotting can be done with separate functions that have a similar name (sc.tl.umap_genelist or sc.tl.umap_sparse; sc.tl.paga_genelist or sc.tl.paga_sparse). . I envision that this would also be a nice stepping stone towards multi-modal analysis with CITE-seq. . (2) I want to use PAGA and UMAP to display my CytOF data as well as integrate scRNA-seq/CytOF data together. I saw a pre-print on bioRxiv that indicated that you are working on this aspect of PAGA/scanpy. This was very exciting to me and I am glad that this is being developed. Is it possible to access the current version of this software? . I think that major partitions identified with CytOF or scRNA-seq can be linked together providing a coarse-grained mechanism to demonstrate how heterogeneity identified with each technique relates to each other based on a given experimental time point. . (3) Histogram integ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/510
https://github.com/scverse/scanpy/issues/510:1416,reliability,Integr,Integration,1416,"ity is remarkably easier on python than in R, which gives scanpy users an obvious advantage. . I've learned so much using this package and it has allowed me to display my data in a simple and intuitive way. Truly, from the bottom of my heart, thank you, this package is awesome and I deeply appreciate all the work the scanpy team and Theis lab has put into it. . With that said, I find myself wanting to do things with scanpy that aren't currently supported. . (1) Clustering cells based on a restricted set of genes as opposed to principle components: I understand that this is a biased approach, but I find myself wanting to do this to explore datasets and how genes vary together. I believe it would aid in understanding data topology to reduce the dimensionality of the dataset by looking at a subset of key genes that are picked by an expert who has domain knowledge. I think this is best integrated with a separate sc.pp.neighbors-like function that uses a gene list, as opposed to PCs. Integration into downstream plotting can be done with separate functions that have a similar name (sc.tl.umap_genelist or sc.tl.umap_sparse; sc.tl.paga_genelist or sc.tl.paga_sparse). . I envision that this would also be a nice stepping stone towards multi-modal analysis with CITE-seq. . (2) I want to use PAGA and UMAP to display my CytOF data as well as integrate scRNA-seq/CytOF data together. I saw a pre-print on bioRxiv that indicated that you are working on this aspect of PAGA/scanpy. This was very exciting to me and I am glad that this is being developed. Is it possible to access the current version of this software? . I think that major partitions identified with CytOF or scRNA-seq can be linked together providing a coarse-grained mechanism to demonstrate how heterogeneity identified with each technique relates to each other based on a given experimental time point. . (3) Histogram integration in the plotting api for QC metrics would be helpful. While scatter plots and violin plots are",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/510
https://github.com/scverse/scanpy/issues/510:1773,reliability,integr,integrate,1773,"a in a simple and intuitive way. Truly, from the bottom of my heart, thank you, this package is awesome and I deeply appreciate all the work the scanpy team and Theis lab has put into it. . With that said, I find myself wanting to do things with scanpy that aren't currently supported. . (1) Clustering cells based on a restricted set of genes as opposed to principle components: I understand that this is a biased approach, but I find myself wanting to do this to explore datasets and how genes vary together. I believe it would aid in understanding data topology to reduce the dimensionality of the dataset by looking at a subset of key genes that are picked by an expert who has domain knowledge. I think this is best integrated with a separate sc.pp.neighbors-like function that uses a gene list, as opposed to PCs. Integration into downstream plotting can be done with separate functions that have a similar name (sc.tl.umap_genelist or sc.tl.umap_sparse; sc.tl.paga_genelist or sc.tl.paga_sparse). . I envision that this would also be a nice stepping stone towards multi-modal analysis with CITE-seq. . (2) I want to use PAGA and UMAP to display my CytOF data as well as integrate scRNA-seq/CytOF data together. I saw a pre-print on bioRxiv that indicated that you are working on this aspect of PAGA/scanpy. This was very exciting to me and I am glad that this is being developed. Is it possible to access the current version of this software? . I think that major partitions identified with CytOF or scRNA-seq can be linked together providing a coarse-grained mechanism to demonstrate how heterogeneity identified with each technique relates to each other based on a given experimental time point. . (3) Histogram integration in the plotting api for QC metrics would be helpful. While scatter plots and violin plots are effective, I find myself wanting to make cuts (bounds on percent mitochondrial or nGenes) in my data based off histograms. Again, thank you so much for the amazing software!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/510
https://github.com/scverse/scanpy/issues/510:2317,reliability,integr,integration,2317,"a in a simple and intuitive way. Truly, from the bottom of my heart, thank you, this package is awesome and I deeply appreciate all the work the scanpy team and Theis lab has put into it. . With that said, I find myself wanting to do things with scanpy that aren't currently supported. . (1) Clustering cells based on a restricted set of genes as opposed to principle components: I understand that this is a biased approach, but I find myself wanting to do this to explore datasets and how genes vary together. I believe it would aid in understanding data topology to reduce the dimensionality of the dataset by looking at a subset of key genes that are picked by an expert who has domain knowledge. I think this is best integrated with a separate sc.pp.neighbors-like function that uses a gene list, as opposed to PCs. Integration into downstream plotting can be done with separate functions that have a similar name (sc.tl.umap_genelist or sc.tl.umap_sparse; sc.tl.paga_genelist or sc.tl.paga_sparse). . I envision that this would also be a nice stepping stone towards multi-modal analysis with CITE-seq. . (2) I want to use PAGA and UMAP to display my CytOF data as well as integrate scRNA-seq/CytOF data together. I saw a pre-print on bioRxiv that indicated that you are working on this aspect of PAGA/scanpy. This was very exciting to me and I am glad that this is being developed. Is it possible to access the current version of this software? . I think that major partitions identified with CytOF or scRNA-seq can be linked together providing a coarse-grained mechanism to demonstrate how heterogeneity identified with each technique relates to each other based on a given experimental time point. . (3) Histogram integration in the plotting api for QC metrics would be helpful. While scatter plots and violin plots are effective, I find myself wanting to make cuts (bounds on percent mitochondrial or nGenes) in my data based off histograms. Again, thank you so much for the amazing software!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/510
https://github.com/scverse/scanpy/issues/510:221,security,integr,integrated,221,"Geneset based clustering and CytOF Compatibility; I want to start off by mentioning how much I love scanpy. I recommend this package to everyone I know who is doing single cell sequencing. I love how PAGA and UMAP can be integrated together. PAGA makes appreciation of data topology so simple. In addition, it's so easy to do velocity analysis with the scvelo integration. Installing scanpy as well as hdf5/loom compatibility is remarkably easier on python than in R, which gives scanpy users an obvious advantage. . I've learned so much using this package and it has allowed me to display my data in a simple and intuitive way. Truly, from the bottom of my heart, thank you, this package is awesome and I deeply appreciate all the work the scanpy team and Theis lab has put into it. . With that said, I find myself wanting to do things with scanpy that aren't currently supported. . (1) Clustering cells based on a restricted set of genes as opposed to principle components: I understand that this is a biased approach, but I find myself wanting to do this to explore datasets and how genes vary together. I believe it would aid in understanding data topology to reduce the dimensionality of the dataset by looking at a subset of key genes that are picked by an expert who has domain knowledge. I think this is best integrated with a separate sc.pp.neighbors-like function that uses a gene list, as opposed to PCs. Integration into downstream plotting can be done with separate functions that have a similar name (sc.tl.umap_genelist or sc.tl.umap_sparse; sc.tl.paga_genelist or sc.tl.paga_sparse). . I envision that this would also be a nice stepping stone towards multi-modal analysis with CITE-seq. . (2) I want to use PAGA and UMAP to display my CytOF data as well as integrate scRNA-seq/CytOF data together. I saw a pre-print on bioRxiv that indicated that you are working on this aspect of PAGA/scanpy. This was very exciting to me and I am glad that this is being developed. Is it possible to",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/510
https://github.com/scverse/scanpy/issues/510:360,security,integr,integration,360,"Geneset based clustering and CytOF Compatibility; I want to start off by mentioning how much I love scanpy. I recommend this package to everyone I know who is doing single cell sequencing. I love how PAGA and UMAP can be integrated together. PAGA makes appreciation of data topology so simple. In addition, it's so easy to do velocity analysis with the scvelo integration. Installing scanpy as well as hdf5/loom compatibility is remarkably easier on python than in R, which gives scanpy users an obvious advantage. . I've learned so much using this package and it has allowed me to display my data in a simple and intuitive way. Truly, from the bottom of my heart, thank you, this package is awesome and I deeply appreciate all the work the scanpy team and Theis lab has put into it. . With that said, I find myself wanting to do things with scanpy that aren't currently supported. . (1) Clustering cells based on a restricted set of genes as opposed to principle components: I understand that this is a biased approach, but I find myself wanting to do this to explore datasets and how genes vary together. I believe it would aid in understanding data topology to reduce the dimensionality of the dataset by looking at a subset of key genes that are picked by an expert who has domain knowledge. I think this is best integrated with a separate sc.pp.neighbors-like function that uses a gene list, as opposed to PCs. Integration into downstream plotting can be done with separate functions that have a similar name (sc.tl.umap_genelist or sc.tl.umap_sparse; sc.tl.paga_genelist or sc.tl.paga_sparse). . I envision that this would also be a nice stepping stone towards multi-modal analysis with CITE-seq. . (2) I want to use PAGA and UMAP to display my CytOF data as well as integrate scRNA-seq/CytOF data together. I saw a pre-print on bioRxiv that indicated that you are working on this aspect of PAGA/scanpy. This was very exciting to me and I am glad that this is being developed. Is it possible to",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/510
https://github.com/scverse/scanpy/issues/510:748,security,team,team,748,"Geneset based clustering and CytOF Compatibility; I want to start off by mentioning how much I love scanpy. I recommend this package to everyone I know who is doing single cell sequencing. I love how PAGA and UMAP can be integrated together. PAGA makes appreciation of data topology so simple. In addition, it's so easy to do velocity analysis with the scvelo integration. Installing scanpy as well as hdf5/loom compatibility is remarkably easier on python than in R, which gives scanpy users an obvious advantage. . I've learned so much using this package and it has allowed me to display my data in a simple and intuitive way. Truly, from the bottom of my heart, thank you, this package is awesome and I deeply appreciate all the work the scanpy team and Theis lab has put into it. . With that said, I find myself wanting to do things with scanpy that aren't currently supported. . (1) Clustering cells based on a restricted set of genes as opposed to principle components: I understand that this is a biased approach, but I find myself wanting to do this to explore datasets and how genes vary together. I believe it would aid in understanding data topology to reduce the dimensionality of the dataset by looking at a subset of key genes that are picked by an expert who has domain knowledge. I think this is best integrated with a separate sc.pp.neighbors-like function that uses a gene list, as opposed to PCs. Integration into downstream plotting can be done with separate functions that have a similar name (sc.tl.umap_genelist or sc.tl.umap_sparse; sc.tl.paga_genelist or sc.tl.paga_sparse). . I envision that this would also be a nice stepping stone towards multi-modal analysis with CITE-seq. . (2) I want to use PAGA and UMAP to display my CytOF data as well as integrate scRNA-seq/CytOF data together. I saw a pre-print on bioRxiv that indicated that you are working on this aspect of PAGA/scanpy. This was very exciting to me and I am glad that this is being developed. Is it possible to",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/510
https://github.com/scverse/scanpy/issues/510:1317,security,integr,integrated,1317," do velocity analysis with the scvelo integration. Installing scanpy as well as hdf5/loom compatibility is remarkably easier on python than in R, which gives scanpy users an obvious advantage. . I've learned so much using this package and it has allowed me to display my data in a simple and intuitive way. Truly, from the bottom of my heart, thank you, this package is awesome and I deeply appreciate all the work the scanpy team and Theis lab has put into it. . With that said, I find myself wanting to do things with scanpy that aren't currently supported. . (1) Clustering cells based on a restricted set of genes as opposed to principle components: I understand that this is a biased approach, but I find myself wanting to do this to explore datasets and how genes vary together. I believe it would aid in understanding data topology to reduce the dimensionality of the dataset by looking at a subset of key genes that are picked by an expert who has domain knowledge. I think this is best integrated with a separate sc.pp.neighbors-like function that uses a gene list, as opposed to PCs. Integration into downstream plotting can be done with separate functions that have a similar name (sc.tl.umap_genelist or sc.tl.umap_sparse; sc.tl.paga_genelist or sc.tl.paga_sparse). . I envision that this would also be a nice stepping stone towards multi-modal analysis with CITE-seq. . (2) I want to use PAGA and UMAP to display my CytOF data as well as integrate scRNA-seq/CytOF data together. I saw a pre-print on bioRxiv that indicated that you are working on this aspect of PAGA/scanpy. This was very exciting to me and I am glad that this is being developed. Is it possible to access the current version of this software? . I think that major partitions identified with CytOF or scRNA-seq can be linked together providing a coarse-grained mechanism to demonstrate how heterogeneity identified with each technique relates to each other based on a given experimental time point. . (3) Histogram integ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/510
https://github.com/scverse/scanpy/issues/510:1416,security,Integr,Integration,1416,"ity is remarkably easier on python than in R, which gives scanpy users an obvious advantage. . I've learned so much using this package and it has allowed me to display my data in a simple and intuitive way. Truly, from the bottom of my heart, thank you, this package is awesome and I deeply appreciate all the work the scanpy team and Theis lab has put into it. . With that said, I find myself wanting to do things with scanpy that aren't currently supported. . (1) Clustering cells based on a restricted set of genes as opposed to principle components: I understand that this is a biased approach, but I find myself wanting to do this to explore datasets and how genes vary together. I believe it would aid in understanding data topology to reduce the dimensionality of the dataset by looking at a subset of key genes that are picked by an expert who has domain knowledge. I think this is best integrated with a separate sc.pp.neighbors-like function that uses a gene list, as opposed to PCs. Integration into downstream plotting can be done with separate functions that have a similar name (sc.tl.umap_genelist or sc.tl.umap_sparse; sc.tl.paga_genelist or sc.tl.paga_sparse). . I envision that this would also be a nice stepping stone towards multi-modal analysis with CITE-seq. . (2) I want to use PAGA and UMAP to display my CytOF data as well as integrate scRNA-seq/CytOF data together. I saw a pre-print on bioRxiv that indicated that you are working on this aspect of PAGA/scanpy. This was very exciting to me and I am glad that this is being developed. Is it possible to access the current version of this software? . I think that major partitions identified with CytOF or scRNA-seq can be linked together providing a coarse-grained mechanism to demonstrate how heterogeneity identified with each technique relates to each other based on a given experimental time point. . (3) Histogram integration in the plotting api for QC metrics would be helpful. While scatter plots and violin plots are",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/510
https://github.com/scverse/scanpy/issues/510:1773,security,integr,integrate,1773,"a in a simple and intuitive way. Truly, from the bottom of my heart, thank you, this package is awesome and I deeply appreciate all the work the scanpy team and Theis lab has put into it. . With that said, I find myself wanting to do things with scanpy that aren't currently supported. . (1) Clustering cells based on a restricted set of genes as opposed to principle components: I understand that this is a biased approach, but I find myself wanting to do this to explore datasets and how genes vary together. I believe it would aid in understanding data topology to reduce the dimensionality of the dataset by looking at a subset of key genes that are picked by an expert who has domain knowledge. I think this is best integrated with a separate sc.pp.neighbors-like function that uses a gene list, as opposed to PCs. Integration into downstream plotting can be done with separate functions that have a similar name (sc.tl.umap_genelist or sc.tl.umap_sparse; sc.tl.paga_genelist or sc.tl.paga_sparse). . I envision that this would also be a nice stepping stone towards multi-modal analysis with CITE-seq. . (2) I want to use PAGA and UMAP to display my CytOF data as well as integrate scRNA-seq/CytOF data together. I saw a pre-print on bioRxiv that indicated that you are working on this aspect of PAGA/scanpy. This was very exciting to me and I am glad that this is being developed. Is it possible to access the current version of this software? . I think that major partitions identified with CytOF or scRNA-seq can be linked together providing a coarse-grained mechanism to demonstrate how heterogeneity identified with each technique relates to each other based on a given experimental time point. . (3) Histogram integration in the plotting api for QC metrics would be helpful. While scatter plots and violin plots are effective, I find myself wanting to make cuts (bounds on percent mitochondrial or nGenes) in my data based off histograms. Again, thank you so much for the amazing software!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/510
https://github.com/scverse/scanpy/issues/510:2001,security,access,access,2001,"a in a simple and intuitive way. Truly, from the bottom of my heart, thank you, this package is awesome and I deeply appreciate all the work the scanpy team and Theis lab has put into it. . With that said, I find myself wanting to do things with scanpy that aren't currently supported. . (1) Clustering cells based on a restricted set of genes as opposed to principle components: I understand that this is a biased approach, but I find myself wanting to do this to explore datasets and how genes vary together. I believe it would aid in understanding data topology to reduce the dimensionality of the dataset by looking at a subset of key genes that are picked by an expert who has domain knowledge. I think this is best integrated with a separate sc.pp.neighbors-like function that uses a gene list, as opposed to PCs. Integration into downstream plotting can be done with separate functions that have a similar name (sc.tl.umap_genelist or sc.tl.umap_sparse; sc.tl.paga_genelist or sc.tl.paga_sparse). . I envision that this would also be a nice stepping stone towards multi-modal analysis with CITE-seq. . (2) I want to use PAGA and UMAP to display my CytOF data as well as integrate scRNA-seq/CytOF data together. I saw a pre-print on bioRxiv that indicated that you are working on this aspect of PAGA/scanpy. This was very exciting to me and I am glad that this is being developed. Is it possible to access the current version of this software? . I think that major partitions identified with CytOF or scRNA-seq can be linked together providing a coarse-grained mechanism to demonstrate how heterogeneity identified with each technique relates to each other based on a given experimental time point. . (3) Histogram integration in the plotting api for QC metrics would be helpful. While scatter plots and violin plots are effective, I find myself wanting to make cuts (bounds on percent mitochondrial or nGenes) in my data based off histograms. Again, thank you so much for the amazing software!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/510
https://github.com/scverse/scanpy/issues/510:2078,security,ident,identified,2078,"a in a simple and intuitive way. Truly, from the bottom of my heart, thank you, this package is awesome and I deeply appreciate all the work the scanpy team and Theis lab has put into it. . With that said, I find myself wanting to do things with scanpy that aren't currently supported. . (1) Clustering cells based on a restricted set of genes as opposed to principle components: I understand that this is a biased approach, but I find myself wanting to do this to explore datasets and how genes vary together. I believe it would aid in understanding data topology to reduce the dimensionality of the dataset by looking at a subset of key genes that are picked by an expert who has domain knowledge. I think this is best integrated with a separate sc.pp.neighbors-like function that uses a gene list, as opposed to PCs. Integration into downstream plotting can be done with separate functions that have a similar name (sc.tl.umap_genelist or sc.tl.umap_sparse; sc.tl.paga_genelist or sc.tl.paga_sparse). . I envision that this would also be a nice stepping stone towards multi-modal analysis with CITE-seq. . (2) I want to use PAGA and UMAP to display my CytOF data as well as integrate scRNA-seq/CytOF data together. I saw a pre-print on bioRxiv that indicated that you are working on this aspect of PAGA/scanpy. This was very exciting to me and I am glad that this is being developed. Is it possible to access the current version of this software? . I think that major partitions identified with CytOF or scRNA-seq can be linked together providing a coarse-grained mechanism to demonstrate how heterogeneity identified with each technique relates to each other based on a given experimental time point. . (3) Histogram integration in the plotting api for QC metrics would be helpful. While scatter plots and violin plots are effective, I find myself wanting to make cuts (bounds on percent mitochondrial or nGenes) in my data based off histograms. Again, thank you so much for the amazing software!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/510
https://github.com/scverse/scanpy/issues/510:2206,security,ident,identified,2206,"a in a simple and intuitive way. Truly, from the bottom of my heart, thank you, this package is awesome and I deeply appreciate all the work the scanpy team and Theis lab has put into it. . With that said, I find myself wanting to do things with scanpy that aren't currently supported. . (1) Clustering cells based on a restricted set of genes as opposed to principle components: I understand that this is a biased approach, but I find myself wanting to do this to explore datasets and how genes vary together. I believe it would aid in understanding data topology to reduce the dimensionality of the dataset by looking at a subset of key genes that are picked by an expert who has domain knowledge. I think this is best integrated with a separate sc.pp.neighbors-like function that uses a gene list, as opposed to PCs. Integration into downstream plotting can be done with separate functions that have a similar name (sc.tl.umap_genelist or sc.tl.umap_sparse; sc.tl.paga_genelist or sc.tl.paga_sparse). . I envision that this would also be a nice stepping stone towards multi-modal analysis with CITE-seq. . (2) I want to use PAGA and UMAP to display my CytOF data as well as integrate scRNA-seq/CytOF data together. I saw a pre-print on bioRxiv that indicated that you are working on this aspect of PAGA/scanpy. This was very exciting to me and I am glad that this is being developed. Is it possible to access the current version of this software? . I think that major partitions identified with CytOF or scRNA-seq can be linked together providing a coarse-grained mechanism to demonstrate how heterogeneity identified with each technique relates to each other based on a given experimental time point. . (3) Histogram integration in the plotting api for QC metrics would be helpful. While scatter plots and violin plots are effective, I find myself wanting to make cuts (bounds on percent mitochondrial or nGenes) in my data based off histograms. Again, thank you so much for the amazing software!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/510
https://github.com/scverse/scanpy/issues/510:2317,security,integr,integration,2317,"a in a simple and intuitive way. Truly, from the bottom of my heart, thank you, this package is awesome and I deeply appreciate all the work the scanpy team and Theis lab has put into it. . With that said, I find myself wanting to do things with scanpy that aren't currently supported. . (1) Clustering cells based on a restricted set of genes as opposed to principle components: I understand that this is a biased approach, but I find myself wanting to do this to explore datasets and how genes vary together. I believe it would aid in understanding data topology to reduce the dimensionality of the dataset by looking at a subset of key genes that are picked by an expert who has domain knowledge. I think this is best integrated with a separate sc.pp.neighbors-like function that uses a gene list, as opposed to PCs. Integration into downstream plotting can be done with separate functions that have a similar name (sc.tl.umap_genelist or sc.tl.umap_sparse; sc.tl.paga_genelist or sc.tl.paga_sparse). . I envision that this would also be a nice stepping stone towards multi-modal analysis with CITE-seq. . (2) I want to use PAGA and UMAP to display my CytOF data as well as integrate scRNA-seq/CytOF data together. I saw a pre-print on bioRxiv that indicated that you are working on this aspect of PAGA/scanpy. This was very exciting to me and I am glad that this is being developed. Is it possible to access the current version of this software? . I think that major partitions identified with CytOF or scRNA-seq can be linked together providing a coarse-grained mechanism to demonstrate how heterogeneity identified with each technique relates to each other based on a given experimental time point. . (3) Histogram integration in the plotting api for QC metrics would be helpful. While scatter plots and violin plots are effective, I find myself wanting to make cuts (bounds on percent mitochondrial or nGenes) in my data based off histograms. Again, thank you so much for the amazing software!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/510
https://github.com/scverse/scanpy/issues/510:221,testability,integr,integrated,221,"Geneset based clustering and CytOF Compatibility; I want to start off by mentioning how much I love scanpy. I recommend this package to everyone I know who is doing single cell sequencing. I love how PAGA and UMAP can be integrated together. PAGA makes appreciation of data topology so simple. In addition, it's so easy to do velocity analysis with the scvelo integration. Installing scanpy as well as hdf5/loom compatibility is remarkably easier on python than in R, which gives scanpy users an obvious advantage. . I've learned so much using this package and it has allowed me to display my data in a simple and intuitive way. Truly, from the bottom of my heart, thank you, this package is awesome and I deeply appreciate all the work the scanpy team and Theis lab has put into it. . With that said, I find myself wanting to do things with scanpy that aren't currently supported. . (1) Clustering cells based on a restricted set of genes as opposed to principle components: I understand that this is a biased approach, but I find myself wanting to do this to explore datasets and how genes vary together. I believe it would aid in understanding data topology to reduce the dimensionality of the dataset by looking at a subset of key genes that are picked by an expert who has domain knowledge. I think this is best integrated with a separate sc.pp.neighbors-like function that uses a gene list, as opposed to PCs. Integration into downstream plotting can be done with separate functions that have a similar name (sc.tl.umap_genelist or sc.tl.umap_sparse; sc.tl.paga_genelist or sc.tl.paga_sparse). . I envision that this would also be a nice stepping stone towards multi-modal analysis with CITE-seq. . (2) I want to use PAGA and UMAP to display my CytOF data as well as integrate scRNA-seq/CytOF data together. I saw a pre-print on bioRxiv that indicated that you are working on this aspect of PAGA/scanpy. This was very exciting to me and I am glad that this is being developed. Is it possible to",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/510
https://github.com/scverse/scanpy/issues/510:286,testability,simpl,simple,286,"Geneset based clustering and CytOF Compatibility; I want to start off by mentioning how much I love scanpy. I recommend this package to everyone I know who is doing single cell sequencing. I love how PAGA and UMAP can be integrated together. PAGA makes appreciation of data topology so simple. In addition, it's so easy to do velocity analysis with the scvelo integration. Installing scanpy as well as hdf5/loom compatibility is remarkably easier on python than in R, which gives scanpy users an obvious advantage. . I've learned so much using this package and it has allowed me to display my data in a simple and intuitive way. Truly, from the bottom of my heart, thank you, this package is awesome and I deeply appreciate all the work the scanpy team and Theis lab has put into it. . With that said, I find myself wanting to do things with scanpy that aren't currently supported. . (1) Clustering cells based on a restricted set of genes as opposed to principle components: I understand that this is a biased approach, but I find myself wanting to do this to explore datasets and how genes vary together. I believe it would aid in understanding data topology to reduce the dimensionality of the dataset by looking at a subset of key genes that are picked by an expert who has domain knowledge. I think this is best integrated with a separate sc.pp.neighbors-like function that uses a gene list, as opposed to PCs. Integration into downstream plotting can be done with separate functions that have a similar name (sc.tl.umap_genelist or sc.tl.umap_sparse; sc.tl.paga_genelist or sc.tl.paga_sparse). . I envision that this would also be a nice stepping stone towards multi-modal analysis with CITE-seq. . (2) I want to use PAGA and UMAP to display my CytOF data as well as integrate scRNA-seq/CytOF data together. I saw a pre-print on bioRxiv that indicated that you are working on this aspect of PAGA/scanpy. This was very exciting to me and I am glad that this is being developed. Is it possible to",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/510
https://github.com/scverse/scanpy/issues/510:360,testability,integr,integration,360,"Geneset based clustering and CytOF Compatibility; I want to start off by mentioning how much I love scanpy. I recommend this package to everyone I know who is doing single cell sequencing. I love how PAGA and UMAP can be integrated together. PAGA makes appreciation of data topology so simple. In addition, it's so easy to do velocity analysis with the scvelo integration. Installing scanpy as well as hdf5/loom compatibility is remarkably easier on python than in R, which gives scanpy users an obvious advantage. . I've learned so much using this package and it has allowed me to display my data in a simple and intuitive way. Truly, from the bottom of my heart, thank you, this package is awesome and I deeply appreciate all the work the scanpy team and Theis lab has put into it. . With that said, I find myself wanting to do things with scanpy that aren't currently supported. . (1) Clustering cells based on a restricted set of genes as opposed to principle components: I understand that this is a biased approach, but I find myself wanting to do this to explore datasets and how genes vary together. I believe it would aid in understanding data topology to reduce the dimensionality of the dataset by looking at a subset of key genes that are picked by an expert who has domain knowledge. I think this is best integrated with a separate sc.pp.neighbors-like function that uses a gene list, as opposed to PCs. Integration into downstream plotting can be done with separate functions that have a similar name (sc.tl.umap_genelist or sc.tl.umap_sparse; sc.tl.paga_genelist or sc.tl.paga_sparse). . I envision that this would also be a nice stepping stone towards multi-modal analysis with CITE-seq. . (2) I want to use PAGA and UMAP to display my CytOF data as well as integrate scRNA-seq/CytOF data together. I saw a pre-print on bioRxiv that indicated that you are working on this aspect of PAGA/scanpy. This was very exciting to me and I am glad that this is being developed. Is it possible to",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/510
https://github.com/scverse/scanpy/issues/510:603,testability,simpl,simple,603,"Geneset based clustering and CytOF Compatibility; I want to start off by mentioning how much I love scanpy. I recommend this package to everyone I know who is doing single cell sequencing. I love how PAGA and UMAP can be integrated together. PAGA makes appreciation of data topology so simple. In addition, it's so easy to do velocity analysis with the scvelo integration. Installing scanpy as well as hdf5/loom compatibility is remarkably easier on python than in R, which gives scanpy users an obvious advantage. . I've learned so much using this package and it has allowed me to display my data in a simple and intuitive way. Truly, from the bottom of my heart, thank you, this package is awesome and I deeply appreciate all the work the scanpy team and Theis lab has put into it. . With that said, I find myself wanting to do things with scanpy that aren't currently supported. . (1) Clustering cells based on a restricted set of genes as opposed to principle components: I understand that this is a biased approach, but I find myself wanting to do this to explore datasets and how genes vary together. I believe it would aid in understanding data topology to reduce the dimensionality of the dataset by looking at a subset of key genes that are picked by an expert who has domain knowledge. I think this is best integrated with a separate sc.pp.neighbors-like function that uses a gene list, as opposed to PCs. Integration into downstream plotting can be done with separate functions that have a similar name (sc.tl.umap_genelist or sc.tl.umap_sparse; sc.tl.paga_genelist or sc.tl.paga_sparse). . I envision that this would also be a nice stepping stone towards multi-modal analysis with CITE-seq. . (2) I want to use PAGA and UMAP to display my CytOF data as well as integrate scRNA-seq/CytOF data together. I saw a pre-print on bioRxiv that indicated that you are working on this aspect of PAGA/scanpy. This was very exciting to me and I am glad that this is being developed. Is it possible to",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/510
https://github.com/scverse/scanpy/issues/510:978,testability,understand,understand,978,"Geneset based clustering and CytOF Compatibility; I want to start off by mentioning how much I love scanpy. I recommend this package to everyone I know who is doing single cell sequencing. I love how PAGA and UMAP can be integrated together. PAGA makes appreciation of data topology so simple. In addition, it's so easy to do velocity analysis with the scvelo integration. Installing scanpy as well as hdf5/loom compatibility is remarkably easier on python than in R, which gives scanpy users an obvious advantage. . I've learned so much using this package and it has allowed me to display my data in a simple and intuitive way. Truly, from the bottom of my heart, thank you, this package is awesome and I deeply appreciate all the work the scanpy team and Theis lab has put into it. . With that said, I find myself wanting to do things with scanpy that aren't currently supported. . (1) Clustering cells based on a restricted set of genes as opposed to principle components: I understand that this is a biased approach, but I find myself wanting to do this to explore datasets and how genes vary together. I believe it would aid in understanding data topology to reduce the dimensionality of the dataset by looking at a subset of key genes that are picked by an expert who has domain knowledge. I think this is best integrated with a separate sc.pp.neighbors-like function that uses a gene list, as opposed to PCs. Integration into downstream plotting can be done with separate functions that have a similar name (sc.tl.umap_genelist or sc.tl.umap_sparse; sc.tl.paga_genelist or sc.tl.paga_sparse). . I envision that this would also be a nice stepping stone towards multi-modal analysis with CITE-seq. . (2) I want to use PAGA and UMAP to display my CytOF data as well as integrate scRNA-seq/CytOF data together. I saw a pre-print on bioRxiv that indicated that you are working on this aspect of PAGA/scanpy. This was very exciting to me and I am glad that this is being developed. Is it possible to",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/510
https://github.com/scverse/scanpy/issues/510:1133,testability,understand,understanding,1133,"yone I know who is doing single cell sequencing. I love how PAGA and UMAP can be integrated together. PAGA makes appreciation of data topology so simple. In addition, it's so easy to do velocity analysis with the scvelo integration. Installing scanpy as well as hdf5/loom compatibility is remarkably easier on python than in R, which gives scanpy users an obvious advantage. . I've learned so much using this package and it has allowed me to display my data in a simple and intuitive way. Truly, from the bottom of my heart, thank you, this package is awesome and I deeply appreciate all the work the scanpy team and Theis lab has put into it. . With that said, I find myself wanting to do things with scanpy that aren't currently supported. . (1) Clustering cells based on a restricted set of genes as opposed to principle components: I understand that this is a biased approach, but I find myself wanting to do this to explore datasets and how genes vary together. I believe it would aid in understanding data topology to reduce the dimensionality of the dataset by looking at a subset of key genes that are picked by an expert who has domain knowledge. I think this is best integrated with a separate sc.pp.neighbors-like function that uses a gene list, as opposed to PCs. Integration into downstream plotting can be done with separate functions that have a similar name (sc.tl.umap_genelist or sc.tl.umap_sparse; sc.tl.paga_genelist or sc.tl.paga_sparse). . I envision that this would also be a nice stepping stone towards multi-modal analysis with CITE-seq. . (2) I want to use PAGA and UMAP to display my CytOF data as well as integrate scRNA-seq/CytOF data together. I saw a pre-print on bioRxiv that indicated that you are working on this aspect of PAGA/scanpy. This was very exciting to me and I am glad that this is being developed. Is it possible to access the current version of this software? . I think that major partitions identified with CytOF or scRNA-seq can be linked together prov",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/510
https://github.com/scverse/scanpy/issues/510:1317,testability,integr,integrated,1317," do velocity analysis with the scvelo integration. Installing scanpy as well as hdf5/loom compatibility is remarkably easier on python than in R, which gives scanpy users an obvious advantage. . I've learned so much using this package and it has allowed me to display my data in a simple and intuitive way. Truly, from the bottom of my heart, thank you, this package is awesome and I deeply appreciate all the work the scanpy team and Theis lab has put into it. . With that said, I find myself wanting to do things with scanpy that aren't currently supported. . (1) Clustering cells based on a restricted set of genes as opposed to principle components: I understand that this is a biased approach, but I find myself wanting to do this to explore datasets and how genes vary together. I believe it would aid in understanding data topology to reduce the dimensionality of the dataset by looking at a subset of key genes that are picked by an expert who has domain knowledge. I think this is best integrated with a separate sc.pp.neighbors-like function that uses a gene list, as opposed to PCs. Integration into downstream plotting can be done with separate functions that have a similar name (sc.tl.umap_genelist or sc.tl.umap_sparse; sc.tl.paga_genelist or sc.tl.paga_sparse). . I envision that this would also be a nice stepping stone towards multi-modal analysis with CITE-seq. . (2) I want to use PAGA and UMAP to display my CytOF data as well as integrate scRNA-seq/CytOF data together. I saw a pre-print on bioRxiv that indicated that you are working on this aspect of PAGA/scanpy. This was very exciting to me and I am glad that this is being developed. Is it possible to access the current version of this software? . I think that major partitions identified with CytOF or scRNA-seq can be linked together providing a coarse-grained mechanism to demonstrate how heterogeneity identified with each technique relates to each other based on a given experimental time point. . (3) Histogram integ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/510
https://github.com/scverse/scanpy/issues/510:1416,testability,Integr,Integration,1416,"ity is remarkably easier on python than in R, which gives scanpy users an obvious advantage. . I've learned so much using this package and it has allowed me to display my data in a simple and intuitive way. Truly, from the bottom of my heart, thank you, this package is awesome and I deeply appreciate all the work the scanpy team and Theis lab has put into it. . With that said, I find myself wanting to do things with scanpy that aren't currently supported. . (1) Clustering cells based on a restricted set of genes as opposed to principle components: I understand that this is a biased approach, but I find myself wanting to do this to explore datasets and how genes vary together. I believe it would aid in understanding data topology to reduce the dimensionality of the dataset by looking at a subset of key genes that are picked by an expert who has domain knowledge. I think this is best integrated with a separate sc.pp.neighbors-like function that uses a gene list, as opposed to PCs. Integration into downstream plotting can be done with separate functions that have a similar name (sc.tl.umap_genelist or sc.tl.umap_sparse; sc.tl.paga_genelist or sc.tl.paga_sparse). . I envision that this would also be a nice stepping stone towards multi-modal analysis with CITE-seq. . (2) I want to use PAGA and UMAP to display my CytOF data as well as integrate scRNA-seq/CytOF data together. I saw a pre-print on bioRxiv that indicated that you are working on this aspect of PAGA/scanpy. This was very exciting to me and I am glad that this is being developed. Is it possible to access the current version of this software? . I think that major partitions identified with CytOF or scRNA-seq can be linked together providing a coarse-grained mechanism to demonstrate how heterogeneity identified with each technique relates to each other based on a given experimental time point. . (3) Histogram integration in the plotting api for QC metrics would be helpful. While scatter plots and violin plots are",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/510
https://github.com/scverse/scanpy/issues/510:1773,testability,integr,integrate,1773,"a in a simple and intuitive way. Truly, from the bottom of my heart, thank you, this package is awesome and I deeply appreciate all the work the scanpy team and Theis lab has put into it. . With that said, I find myself wanting to do things with scanpy that aren't currently supported. . (1) Clustering cells based on a restricted set of genes as opposed to principle components: I understand that this is a biased approach, but I find myself wanting to do this to explore datasets and how genes vary together. I believe it would aid in understanding data topology to reduce the dimensionality of the dataset by looking at a subset of key genes that are picked by an expert who has domain knowledge. I think this is best integrated with a separate sc.pp.neighbors-like function that uses a gene list, as opposed to PCs. Integration into downstream plotting can be done with separate functions that have a similar name (sc.tl.umap_genelist or sc.tl.umap_sparse; sc.tl.paga_genelist or sc.tl.paga_sparse). . I envision that this would also be a nice stepping stone towards multi-modal analysis with CITE-seq. . (2) I want to use PAGA and UMAP to display my CytOF data as well as integrate scRNA-seq/CytOF data together. I saw a pre-print on bioRxiv that indicated that you are working on this aspect of PAGA/scanpy. This was very exciting to me and I am glad that this is being developed. Is it possible to access the current version of this software? . I think that major partitions identified with CytOF or scRNA-seq can be linked together providing a coarse-grained mechanism to demonstrate how heterogeneity identified with each technique relates to each other based on a given experimental time point. . (3) Histogram integration in the plotting api for QC metrics would be helpful. While scatter plots and violin plots are effective, I find myself wanting to make cuts (bounds on percent mitochondrial or nGenes) in my data based off histograms. Again, thank you so much for the amazing software!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/510
https://github.com/scverse/scanpy/issues/510:2317,testability,integr,integration,2317,"a in a simple and intuitive way. Truly, from the bottom of my heart, thank you, this package is awesome and I deeply appreciate all the work the scanpy team and Theis lab has put into it. . With that said, I find myself wanting to do things with scanpy that aren't currently supported. . (1) Clustering cells based on a restricted set of genes as opposed to principle components: I understand that this is a biased approach, but I find myself wanting to do this to explore datasets and how genes vary together. I believe it would aid in understanding data topology to reduce the dimensionality of the dataset by looking at a subset of key genes that are picked by an expert who has domain knowledge. I think this is best integrated with a separate sc.pp.neighbors-like function that uses a gene list, as opposed to PCs. Integration into downstream plotting can be done with separate functions that have a similar name (sc.tl.umap_genelist or sc.tl.umap_sparse; sc.tl.paga_genelist or sc.tl.paga_sparse). . I envision that this would also be a nice stepping stone towards multi-modal analysis with CITE-seq. . (2) I want to use PAGA and UMAP to display my CytOF data as well as integrate scRNA-seq/CytOF data together. I saw a pre-print on bioRxiv that indicated that you are working on this aspect of PAGA/scanpy. This was very exciting to me and I am glad that this is being developed. Is it possible to access the current version of this software? . I think that major partitions identified with CytOF or scRNA-seq can be linked together providing a coarse-grained mechanism to demonstrate how heterogeneity identified with each technique relates to each other based on a given experimental time point. . (3) Histogram integration in the plotting api for QC metrics would be helpful. While scatter plots and violin plots are effective, I find myself wanting to make cuts (bounds on percent mitochondrial or nGenes) in my data based off histograms. Again, thank you so much for the amazing software!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/510
https://github.com/scverse/scanpy/issues/510:286,usability,simpl,simple,286,"Geneset based clustering and CytOF Compatibility; I want to start off by mentioning how much I love scanpy. I recommend this package to everyone I know who is doing single cell sequencing. I love how PAGA and UMAP can be integrated together. PAGA makes appreciation of data topology so simple. In addition, it's so easy to do velocity analysis with the scvelo integration. Installing scanpy as well as hdf5/loom compatibility is remarkably easier on python than in R, which gives scanpy users an obvious advantage. . I've learned so much using this package and it has allowed me to display my data in a simple and intuitive way. Truly, from the bottom of my heart, thank you, this package is awesome and I deeply appreciate all the work the scanpy team and Theis lab has put into it. . With that said, I find myself wanting to do things with scanpy that aren't currently supported. . (1) Clustering cells based on a restricted set of genes as opposed to principle components: I understand that this is a biased approach, but I find myself wanting to do this to explore datasets and how genes vary together. I believe it would aid in understanding data topology to reduce the dimensionality of the dataset by looking at a subset of key genes that are picked by an expert who has domain knowledge. I think this is best integrated with a separate sc.pp.neighbors-like function that uses a gene list, as opposed to PCs. Integration into downstream plotting can be done with separate functions that have a similar name (sc.tl.umap_genelist or sc.tl.umap_sparse; sc.tl.paga_genelist or sc.tl.paga_sparse). . I envision that this would also be a nice stepping stone towards multi-modal analysis with CITE-seq. . (2) I want to use PAGA and UMAP to display my CytOF data as well as integrate scRNA-seq/CytOF data together. I saw a pre-print on bioRxiv that indicated that you are working on this aspect of PAGA/scanpy. This was very exciting to me and I am glad that this is being developed. Is it possible to",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/510
https://github.com/scverse/scanpy/issues/510:487,usability,user,users,487,"Geneset based clustering and CytOF Compatibility; I want to start off by mentioning how much I love scanpy. I recommend this package to everyone I know who is doing single cell sequencing. I love how PAGA and UMAP can be integrated together. PAGA makes appreciation of data topology so simple. In addition, it's so easy to do velocity analysis with the scvelo integration. Installing scanpy as well as hdf5/loom compatibility is remarkably easier on python than in R, which gives scanpy users an obvious advantage. . I've learned so much using this package and it has allowed me to display my data in a simple and intuitive way. Truly, from the bottom of my heart, thank you, this package is awesome and I deeply appreciate all the work the scanpy team and Theis lab has put into it. . With that said, I find myself wanting to do things with scanpy that aren't currently supported. . (1) Clustering cells based on a restricted set of genes as opposed to principle components: I understand that this is a biased approach, but I find myself wanting to do this to explore datasets and how genes vary together. I believe it would aid in understanding data topology to reduce the dimensionality of the dataset by looking at a subset of key genes that are picked by an expert who has domain knowledge. I think this is best integrated with a separate sc.pp.neighbors-like function that uses a gene list, as opposed to PCs. Integration into downstream plotting can be done with separate functions that have a similar name (sc.tl.umap_genelist or sc.tl.umap_sparse; sc.tl.paga_genelist or sc.tl.paga_sparse). . I envision that this would also be a nice stepping stone towards multi-modal analysis with CITE-seq. . (2) I want to use PAGA and UMAP to display my CytOF data as well as integrate scRNA-seq/CytOF data together. I saw a pre-print on bioRxiv that indicated that you are working on this aspect of PAGA/scanpy. This was very exciting to me and I am glad that this is being developed. Is it possible to",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/510
https://github.com/scverse/scanpy/issues/510:522,usability,learn,learned,522,"Geneset based clustering and CytOF Compatibility; I want to start off by mentioning how much I love scanpy. I recommend this package to everyone I know who is doing single cell sequencing. I love how PAGA and UMAP can be integrated together. PAGA makes appreciation of data topology so simple. In addition, it's so easy to do velocity analysis with the scvelo integration. Installing scanpy as well as hdf5/loom compatibility is remarkably easier on python than in R, which gives scanpy users an obvious advantage. . I've learned so much using this package and it has allowed me to display my data in a simple and intuitive way. Truly, from the bottom of my heart, thank you, this package is awesome and I deeply appreciate all the work the scanpy team and Theis lab has put into it. . With that said, I find myself wanting to do things with scanpy that aren't currently supported. . (1) Clustering cells based on a restricted set of genes as opposed to principle components: I understand that this is a biased approach, but I find myself wanting to do this to explore datasets and how genes vary together. I believe it would aid in understanding data topology to reduce the dimensionality of the dataset by looking at a subset of key genes that are picked by an expert who has domain knowledge. I think this is best integrated with a separate sc.pp.neighbors-like function that uses a gene list, as opposed to PCs. Integration into downstream plotting can be done with separate functions that have a similar name (sc.tl.umap_genelist or sc.tl.umap_sparse; sc.tl.paga_genelist or sc.tl.paga_sparse). . I envision that this would also be a nice stepping stone towards multi-modal analysis with CITE-seq. . (2) I want to use PAGA and UMAP to display my CytOF data as well as integrate scRNA-seq/CytOF data together. I saw a pre-print on bioRxiv that indicated that you are working on this aspect of PAGA/scanpy. This was very exciting to me and I am glad that this is being developed. Is it possible to",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/510
https://github.com/scverse/scanpy/issues/510:603,usability,simpl,simple,603,"Geneset based clustering and CytOF Compatibility; I want to start off by mentioning how much I love scanpy. I recommend this package to everyone I know who is doing single cell sequencing. I love how PAGA and UMAP can be integrated together. PAGA makes appreciation of data topology so simple. In addition, it's so easy to do velocity analysis with the scvelo integration. Installing scanpy as well as hdf5/loom compatibility is remarkably easier on python than in R, which gives scanpy users an obvious advantage. . I've learned so much using this package and it has allowed me to display my data in a simple and intuitive way. Truly, from the bottom of my heart, thank you, this package is awesome and I deeply appreciate all the work the scanpy team and Theis lab has put into it. . With that said, I find myself wanting to do things with scanpy that aren't currently supported. . (1) Clustering cells based on a restricted set of genes as opposed to principle components: I understand that this is a biased approach, but I find myself wanting to do this to explore datasets and how genes vary together. I believe it would aid in understanding data topology to reduce the dimensionality of the dataset by looking at a subset of key genes that are picked by an expert who has domain knowledge. I think this is best integrated with a separate sc.pp.neighbors-like function that uses a gene list, as opposed to PCs. Integration into downstream plotting can be done with separate functions that have a similar name (sc.tl.umap_genelist or sc.tl.umap_sparse; sc.tl.paga_genelist or sc.tl.paga_sparse). . I envision that this would also be a nice stepping stone towards multi-modal analysis with CITE-seq. . (2) I want to use PAGA and UMAP to display my CytOF data as well as integrate scRNA-seq/CytOF data together. I saw a pre-print on bioRxiv that indicated that you are working on this aspect of PAGA/scanpy. This was very exciting to me and I am glad that this is being developed. Is it possible to",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/510
https://github.com/scverse/scanpy/issues/510:614,usability,intuit,intuitive,614,"Geneset based clustering and CytOF Compatibility; I want to start off by mentioning how much I love scanpy. I recommend this package to everyone I know who is doing single cell sequencing. I love how PAGA and UMAP can be integrated together. PAGA makes appreciation of data topology so simple. In addition, it's so easy to do velocity analysis with the scvelo integration. Installing scanpy as well as hdf5/loom compatibility is remarkably easier on python than in R, which gives scanpy users an obvious advantage. . I've learned so much using this package and it has allowed me to display my data in a simple and intuitive way. Truly, from the bottom of my heart, thank you, this package is awesome and I deeply appreciate all the work the scanpy team and Theis lab has put into it. . With that said, I find myself wanting to do things with scanpy that aren't currently supported. . (1) Clustering cells based on a restricted set of genes as opposed to principle components: I understand that this is a biased approach, but I find myself wanting to do this to explore datasets and how genes vary together. I believe it would aid in understanding data topology to reduce the dimensionality of the dataset by looking at a subset of key genes that are picked by an expert who has domain knowledge. I think this is best integrated with a separate sc.pp.neighbors-like function that uses a gene list, as opposed to PCs. Integration into downstream plotting can be done with separate functions that have a similar name (sc.tl.umap_genelist or sc.tl.umap_sparse; sc.tl.paga_genelist or sc.tl.paga_sparse). . I envision that this would also be a nice stepping stone towards multi-modal analysis with CITE-seq. . (2) I want to use PAGA and UMAP to display my CytOF data as well as integrate scRNA-seq/CytOF data together. I saw a pre-print on bioRxiv that indicated that you are working on this aspect of PAGA/scanpy. This was very exciting to me and I am glad that this is being developed. Is it possible to",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/510
https://github.com/scverse/scanpy/issues/510:871,usability,support,supported,871,"Geneset based clustering and CytOF Compatibility; I want to start off by mentioning how much I love scanpy. I recommend this package to everyone I know who is doing single cell sequencing. I love how PAGA and UMAP can be integrated together. PAGA makes appreciation of data topology so simple. In addition, it's so easy to do velocity analysis with the scvelo integration. Installing scanpy as well as hdf5/loom compatibility is remarkably easier on python than in R, which gives scanpy users an obvious advantage. . I've learned so much using this package and it has allowed me to display my data in a simple and intuitive way. Truly, from the bottom of my heart, thank you, this package is awesome and I deeply appreciate all the work the scanpy team and Theis lab has put into it. . With that said, I find myself wanting to do things with scanpy that aren't currently supported. . (1) Clustering cells based on a restricted set of genes as opposed to principle components: I understand that this is a biased approach, but I find myself wanting to do this to explore datasets and how genes vary together. I believe it would aid in understanding data topology to reduce the dimensionality of the dataset by looking at a subset of key genes that are picked by an expert who has domain knowledge. I think this is best integrated with a separate sc.pp.neighbors-like function that uses a gene list, as opposed to PCs. Integration into downstream plotting can be done with separate functions that have a similar name (sc.tl.umap_genelist or sc.tl.umap_sparse; sc.tl.paga_genelist or sc.tl.paga_sparse). . I envision that this would also be a nice stepping stone towards multi-modal analysis with CITE-seq. . (2) I want to use PAGA and UMAP to display my CytOF data as well as integrate scRNA-seq/CytOF data together. I saw a pre-print on bioRxiv that indicated that you are working on this aspect of PAGA/scanpy. This was very exciting to me and I am glad that this is being developed. Is it possible to",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/510
https://github.com/scverse/scanpy/issues/510:1667,usability,multi-mod,multi-modal,1667,"a in a simple and intuitive way. Truly, from the bottom of my heart, thank you, this package is awesome and I deeply appreciate all the work the scanpy team and Theis lab has put into it. . With that said, I find myself wanting to do things with scanpy that aren't currently supported. . (1) Clustering cells based on a restricted set of genes as opposed to principle components: I understand that this is a biased approach, but I find myself wanting to do this to explore datasets and how genes vary together. I believe it would aid in understanding data topology to reduce the dimensionality of the dataset by looking at a subset of key genes that are picked by an expert who has domain knowledge. I think this is best integrated with a separate sc.pp.neighbors-like function that uses a gene list, as opposed to PCs. Integration into downstream plotting can be done with separate functions that have a similar name (sc.tl.umap_genelist or sc.tl.umap_sparse; sc.tl.paga_genelist or sc.tl.paga_sparse). . I envision that this would also be a nice stepping stone towards multi-modal analysis with CITE-seq. . (2) I want to use PAGA and UMAP to display my CytOF data as well as integrate scRNA-seq/CytOF data together. I saw a pre-print on bioRxiv that indicated that you are working on this aspect of PAGA/scanpy. This was very exciting to me and I am glad that this is being developed. Is it possible to access the current version of this software? . I think that major partitions identified with CytOF or scRNA-seq can be linked together providing a coarse-grained mechanism to demonstrate how heterogeneity identified with each technique relates to each other based on a given experimental time point. . (3) Histogram integration in the plotting api for QC metrics would be helpful. While scatter plots and violin plots are effective, I find myself wanting to make cuts (bounds on percent mitochondrial or nGenes) in my data based off histograms. Again, thank you so much for the amazing software!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/510
https://github.com/scverse/scanpy/issues/510:1848,usability,indicat,indicated,1848,"a in a simple and intuitive way. Truly, from the bottom of my heart, thank you, this package is awesome and I deeply appreciate all the work the scanpy team and Theis lab has put into it. . With that said, I find myself wanting to do things with scanpy that aren't currently supported. . (1) Clustering cells based on a restricted set of genes as opposed to principle components: I understand that this is a biased approach, but I find myself wanting to do this to explore datasets and how genes vary together. I believe it would aid in understanding data topology to reduce the dimensionality of the dataset by looking at a subset of key genes that are picked by an expert who has domain knowledge. I think this is best integrated with a separate sc.pp.neighbors-like function that uses a gene list, as opposed to PCs. Integration into downstream plotting can be done with separate functions that have a similar name (sc.tl.umap_genelist or sc.tl.umap_sparse; sc.tl.paga_genelist or sc.tl.paga_sparse). . I envision that this would also be a nice stepping stone towards multi-modal analysis with CITE-seq. . (2) I want to use PAGA and UMAP to display my CytOF data as well as integrate scRNA-seq/CytOF data together. I saw a pre-print on bioRxiv that indicated that you are working on this aspect of PAGA/scanpy. This was very exciting to me and I am glad that this is being developed. Is it possible to access the current version of this software? . I think that major partitions identified with CytOF or scRNA-seq can be linked together providing a coarse-grained mechanism to demonstrate how heterogeneity identified with each technique relates to each other based on a given experimental time point. . (3) Histogram integration in the plotting api for QC metrics would be helpful. While scatter plots and violin plots are effective, I find myself wanting to make cuts (bounds on percent mitochondrial or nGenes) in my data based off histograms. Again, thank you so much for the amazing software!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/510
https://github.com/scverse/scanpy/issues/510:2373,usability,help,helpful,2373,"a in a simple and intuitive way. Truly, from the bottom of my heart, thank you, this package is awesome and I deeply appreciate all the work the scanpy team and Theis lab has put into it. . With that said, I find myself wanting to do things with scanpy that aren't currently supported. . (1) Clustering cells based on a restricted set of genes as opposed to principle components: I understand that this is a biased approach, but I find myself wanting to do this to explore datasets and how genes vary together. I believe it would aid in understanding data topology to reduce the dimensionality of the dataset by looking at a subset of key genes that are picked by an expert who has domain knowledge. I think this is best integrated with a separate sc.pp.neighbors-like function that uses a gene list, as opposed to PCs. Integration into downstream plotting can be done with separate functions that have a similar name (sc.tl.umap_genelist or sc.tl.umap_sparse; sc.tl.paga_genelist or sc.tl.paga_sparse). . I envision that this would also be a nice stepping stone towards multi-modal analysis with CITE-seq. . (2) I want to use PAGA and UMAP to display my CytOF data as well as integrate scRNA-seq/CytOF data together. I saw a pre-print on bioRxiv that indicated that you are working on this aspect of PAGA/scanpy. This was very exciting to me and I am glad that this is being developed. Is it possible to access the current version of this software? . I think that major partitions identified with CytOF or scRNA-seq can be linked together providing a coarse-grained mechanism to demonstrate how heterogeneity identified with each technique relates to each other based on a given experimental time point. . (3) Histogram integration in the plotting api for QC metrics would be helpful. While scatter plots and violin plots are effective, I find myself wanting to make cuts (bounds on percent mitochondrial or nGenes) in my data based off histograms. Again, thank you so much for the amazing software!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/510
https://github.com/scverse/scanpy/issues/510:2423,usability,effectiv,effective,2423,"a in a simple and intuitive way. Truly, from the bottom of my heart, thank you, this package is awesome and I deeply appreciate all the work the scanpy team and Theis lab has put into it. . With that said, I find myself wanting to do things with scanpy that aren't currently supported. . (1) Clustering cells based on a restricted set of genes as opposed to principle components: I understand that this is a biased approach, but I find myself wanting to do this to explore datasets and how genes vary together. I believe it would aid in understanding data topology to reduce the dimensionality of the dataset by looking at a subset of key genes that are picked by an expert who has domain knowledge. I think this is best integrated with a separate sc.pp.neighbors-like function that uses a gene list, as opposed to PCs. Integration into downstream plotting can be done with separate functions that have a similar name (sc.tl.umap_genelist or sc.tl.umap_sparse; sc.tl.paga_genelist or sc.tl.paga_sparse). . I envision that this would also be a nice stepping stone towards multi-modal analysis with CITE-seq. . (2) I want to use PAGA and UMAP to display my CytOF data as well as integrate scRNA-seq/CytOF data together. I saw a pre-print on bioRxiv that indicated that you are working on this aspect of PAGA/scanpy. This was very exciting to me and I am glad that this is being developed. Is it possible to access the current version of this software? . I think that major partitions identified with CytOF or scRNA-seq can be linked together providing a coarse-grained mechanism to demonstrate how heterogeneity identified with each technique relates to each other based on a given experimental time point. . (3) Histogram integration in the plotting api for QC metrics would be helpful. While scatter plots and violin plots are effective, I find myself wanting to make cuts (bounds on percent mitochondrial or nGenes) in my data based off histograms. Again, thank you so much for the amazing software!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/510
https://github.com/scverse/scanpy/issues/511:288,availability,cluster,cluster,288,"memory requirements; Hi,. I am trying to run the full 1.3M 10X mouse cell dataset (using the 1M_neurons_filtered_gene_bc_matrices_h5.h5 file from 10X website). I have 126GB RAM and Intel® Xeon(R) W-2123 CPU @ 3.60GHz × 8 which is above the requirements you mention needed to run the full cluster.py method without subsampling. (https://github.com/theislab/scanpy_usage/tree/master/170522_visualizing_one_million_cells). I get a memory error at the normalization and filter_genes_dispersion stage, should i modify the code in anyway? (without subsampling). Thanks,Shobi",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/511
https://github.com/scverse/scanpy/issues/511:435,availability,error,error,435,"memory requirements; Hi,. I am trying to run the full 1.3M 10X mouse cell dataset (using the 1M_neurons_filtered_gene_bc_matrices_h5.h5 file from 10X website). I have 126GB RAM and Intel® Xeon(R) W-2123 CPU @ 3.60GHz × 8 which is above the requirements you mention needed to run the full cluster.py method without subsampling. (https://github.com/theislab/scanpy_usage/tree/master/170522_visualizing_one_million_cells). I get a memory error at the normalization and filter_genes_dispersion stage, should i modify the code in anyway? (without subsampling). Thanks,Shobi",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/511
https://github.com/scverse/scanpy/issues/511:288,deployability,cluster,cluster,288,"memory requirements; Hi,. I am trying to run the full 1.3M 10X mouse cell dataset (using the 1M_neurons_filtered_gene_bc_matrices_h5.h5 file from 10X website). I have 126GB RAM and Intel® Xeon(R) W-2123 CPU @ 3.60GHz × 8 which is above the requirements you mention needed to run the full cluster.py method without subsampling. (https://github.com/theislab/scanpy_usage/tree/master/170522_visualizing_one_million_cells). I get a memory error at the normalization and filter_genes_dispersion stage, should i modify the code in anyway? (without subsampling). Thanks,Shobi",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/511
https://github.com/scverse/scanpy/issues/511:490,deployability,stage,stage,490,"memory requirements; Hi,. I am trying to run the full 1.3M 10X mouse cell dataset (using the 1M_neurons_filtered_gene_bc_matrices_h5.h5 file from 10X website). I have 126GB RAM and Intel® Xeon(R) W-2123 CPU @ 3.60GHz × 8 which is above the requirements you mention needed to run the full cluster.py method without subsampling. (https://github.com/theislab/scanpy_usage/tree/master/170522_visualizing_one_million_cells). I get a memory error at the normalization and filter_genes_dispersion stage, should i modify the code in anyway? (without subsampling). Thanks,Shobi",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/511
https://github.com/scverse/scanpy/issues/511:203,energy efficiency,CPU,CPU,203,"memory requirements; Hi,. I am trying to run the full 1.3M 10X mouse cell dataset (using the 1M_neurons_filtered_gene_bc_matrices_h5.h5 file from 10X website). I have 126GB RAM and Intel® Xeon(R) W-2123 CPU @ 3.60GHz × 8 which is above the requirements you mention needed to run the full cluster.py method without subsampling. (https://github.com/theislab/scanpy_usage/tree/master/170522_visualizing_one_million_cells). I get a memory error at the normalization and filter_genes_dispersion stage, should i modify the code in anyway? (without subsampling). Thanks,Shobi",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/511
https://github.com/scverse/scanpy/issues/511:314,integrability,sub,subsampling,314,"memory requirements; Hi,. I am trying to run the full 1.3M 10X mouse cell dataset (using the 1M_neurons_filtered_gene_bc_matrices_h5.h5 file from 10X website). I have 126GB RAM and Intel® Xeon(R) W-2123 CPU @ 3.60GHz × 8 which is above the requirements you mention needed to run the full cluster.py method without subsampling. (https://github.com/theislab/scanpy_usage/tree/master/170522_visualizing_one_million_cells). I get a memory error at the normalization and filter_genes_dispersion stage, should i modify the code in anyway? (without subsampling). Thanks,Shobi",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/511
https://github.com/scverse/scanpy/issues/511:542,integrability,sub,subsampling,542,"memory requirements; Hi,. I am trying to run the full 1.3M 10X mouse cell dataset (using the 1M_neurons_filtered_gene_bc_matrices_h5.h5 file from 10X website). I have 126GB RAM and Intel® Xeon(R) W-2123 CPU @ 3.60GHz × 8 which is above the requirements you mention needed to run the full cluster.py method without subsampling. (https://github.com/theislab/scanpy_usage/tree/master/170522_visualizing_one_million_cells). I get a memory error at the normalization and filter_genes_dispersion stage, should i modify the code in anyway? (without subsampling). Thanks,Shobi",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/511
https://github.com/scverse/scanpy/issues/511:0,performance,memor,memory,0,"memory requirements; Hi,. I am trying to run the full 1.3M 10X mouse cell dataset (using the 1M_neurons_filtered_gene_bc_matrices_h5.h5 file from 10X website). I have 126GB RAM and Intel® Xeon(R) W-2123 CPU @ 3.60GHz × 8 which is above the requirements you mention needed to run the full cluster.py method without subsampling. (https://github.com/theislab/scanpy_usage/tree/master/170522_visualizing_one_million_cells). I get a memory error at the normalization and filter_genes_dispersion stage, should i modify the code in anyway? (without subsampling). Thanks,Shobi",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/511
https://github.com/scverse/scanpy/issues/511:203,performance,CPU,CPU,203,"memory requirements; Hi,. I am trying to run the full 1.3M 10X mouse cell dataset (using the 1M_neurons_filtered_gene_bc_matrices_h5.h5 file from 10X website). I have 126GB RAM and Intel® Xeon(R) W-2123 CPU @ 3.60GHz × 8 which is above the requirements you mention needed to run the full cluster.py method without subsampling. (https://github.com/theislab/scanpy_usage/tree/master/170522_visualizing_one_million_cells). I get a memory error at the normalization and filter_genes_dispersion stage, should i modify the code in anyway? (without subsampling). Thanks,Shobi",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/511
https://github.com/scverse/scanpy/issues/511:428,performance,memor,memory,428,"memory requirements; Hi,. I am trying to run the full 1.3M 10X mouse cell dataset (using the 1M_neurons_filtered_gene_bc_matrices_h5.h5 file from 10X website). I have 126GB RAM and Intel® Xeon(R) W-2123 CPU @ 3.60GHz × 8 which is above the requirements you mention needed to run the full cluster.py method without subsampling. (https://github.com/theislab/scanpy_usage/tree/master/170522_visualizing_one_million_cells). I get a memory error at the normalization and filter_genes_dispersion stage, should i modify the code in anyway? (without subsampling). Thanks,Shobi",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/511
https://github.com/scverse/scanpy/issues/511:435,performance,error,error,435,"memory requirements; Hi,. I am trying to run the full 1.3M 10X mouse cell dataset (using the 1M_neurons_filtered_gene_bc_matrices_h5.h5 file from 10X website). I have 126GB RAM and Intel® Xeon(R) W-2123 CPU @ 3.60GHz × 8 which is above the requirements you mention needed to run the full cluster.py method without subsampling. (https://github.com/theislab/scanpy_usage/tree/master/170522_visualizing_one_million_cells). I get a memory error at the normalization and filter_genes_dispersion stage, should i modify the code in anyway? (without subsampling). Thanks,Shobi",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/511
https://github.com/scverse/scanpy/issues/511:435,safety,error,error,435,"memory requirements; Hi,. I am trying to run the full 1.3M 10X mouse cell dataset (using the 1M_neurons_filtered_gene_bc_matrices_h5.h5 file from 10X website). I have 126GB RAM and Intel® Xeon(R) W-2123 CPU @ 3.60GHz × 8 which is above the requirements you mention needed to run the full cluster.py method without subsampling. (https://github.com/theislab/scanpy_usage/tree/master/170522_visualizing_one_million_cells). I get a memory error at the normalization and filter_genes_dispersion stage, should i modify the code in anyway? (without subsampling). Thanks,Shobi",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/511
https://github.com/scverse/scanpy/issues/511:506,security,modif,modify,506,"memory requirements; Hi,. I am trying to run the full 1.3M 10X mouse cell dataset (using the 1M_neurons_filtered_gene_bc_matrices_h5.h5 file from 10X website). I have 126GB RAM and Intel® Xeon(R) W-2123 CPU @ 3.60GHz × 8 which is above the requirements you mention needed to run the full cluster.py method without subsampling. (https://github.com/theislab/scanpy_usage/tree/master/170522_visualizing_one_million_cells). I get a memory error at the normalization and filter_genes_dispersion stage, should i modify the code in anyway? (without subsampling). Thanks,Shobi",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/511
https://github.com/scverse/scanpy/issues/511:0,usability,memor,memory,0,"memory requirements; Hi,. I am trying to run the full 1.3M 10X mouse cell dataset (using the 1M_neurons_filtered_gene_bc_matrices_h5.h5 file from 10X website). I have 126GB RAM and Intel® Xeon(R) W-2123 CPU @ 3.60GHz × 8 which is above the requirements you mention needed to run the full cluster.py method without subsampling. (https://github.com/theislab/scanpy_usage/tree/master/170522_visualizing_one_million_cells). I get a memory error at the normalization and filter_genes_dispersion stage, should i modify the code in anyway? (without subsampling). Thanks,Shobi",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/511
https://github.com/scverse/scanpy/issues/511:63,usability,mous,mouse,63,"memory requirements; Hi,. I am trying to run the full 1.3M 10X mouse cell dataset (using the 1M_neurons_filtered_gene_bc_matrices_h5.h5 file from 10X website). I have 126GB RAM and Intel® Xeon(R) W-2123 CPU @ 3.60GHz × 8 which is above the requirements you mention needed to run the full cluster.py method without subsampling. (https://github.com/theislab/scanpy_usage/tree/master/170522_visualizing_one_million_cells). I get a memory error at the normalization and filter_genes_dispersion stage, should i modify the code in anyway? (without subsampling). Thanks,Shobi",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/511
https://github.com/scverse/scanpy/issues/511:428,usability,memor,memory,428,"memory requirements; Hi,. I am trying to run the full 1.3M 10X mouse cell dataset (using the 1M_neurons_filtered_gene_bc_matrices_h5.h5 file from 10X website). I have 126GB RAM and Intel® Xeon(R) W-2123 CPU @ 3.60GHz × 8 which is above the requirements you mention needed to run the full cluster.py method without subsampling. (https://github.com/theislab/scanpy_usage/tree/master/170522_visualizing_one_million_cells). I get a memory error at the normalization and filter_genes_dispersion stage, should i modify the code in anyway? (without subsampling). Thanks,Shobi",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/511
https://github.com/scverse/scanpy/issues/511:435,usability,error,error,435,"memory requirements; Hi,. I am trying to run the full 1.3M 10X mouse cell dataset (using the 1M_neurons_filtered_gene_bc_matrices_h5.h5 file from 10X website). I have 126GB RAM and Intel® Xeon(R) W-2123 CPU @ 3.60GHz × 8 which is above the requirements you mention needed to run the full cluster.py method without subsampling. (https://github.com/theislab/scanpy_usage/tree/master/170522_visualizing_one_million_cells). I get a memory error at the normalization and filter_genes_dispersion stage, should i modify the code in anyway? (without subsampling). Thanks,Shobi",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/511
https://github.com/scverse/scanpy/pull/512:982,availability,cluster,clustermap,982,"Add row or column scaling option to matrixplot; Especially when we visualize raw counts, sometimes it's hard to see the differences between the expression of one gene across cell types in the heatmap since one value can simply dominate the dynamic range of expression. I think we can add a scaling option to matrixplot, which squashes expression values between 0 and 1 to make markers more pronounced. Heatmap of the raw values:. ![image](https://user-images.githubusercontent.com/1140359/53700880-06983200-3dc5-11e9-8bd6-e001fd3d078d.png). Heatmap of the logarithmized values (which also helps a bit but not for all genes):. ![image](https://user-images.githubusercontent.com/1140359/53700890-19ab0200-3dc5-11e9-872d-791eec295262.png). Heatmap of the col-normalized values:. ![image](https://user-images.githubusercontent.com/1140359/53700893-2c253b80-3dc5-11e9-968d-b7a89eb65fbc.png). PS: The option is actually borrowed from Seaborn (https://seaborn.pydata.org/generated/seaborn.clustermap.html). . PPS: There is an edge case such as division by zero. Also, `swap_axes` option makes 'row'/'col' naming a bit confusing. Let me know if you have suggestions about these or the standardization idea in general.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/512
https://github.com/scverse/scanpy/pull/512:556,deployability,log,logarithmized,556,"Add row or column scaling option to matrixplot; Especially when we visualize raw counts, sometimes it's hard to see the differences between the expression of one gene across cell types in the heatmap since one value can simply dominate the dynamic range of expression. I think we can add a scaling option to matrixplot, which squashes expression values between 0 and 1 to make markers more pronounced. Heatmap of the raw values:. ![image](https://user-images.githubusercontent.com/1140359/53700880-06983200-3dc5-11e9-8bd6-e001fd3d078d.png). Heatmap of the logarithmized values (which also helps a bit but not for all genes):. ![image](https://user-images.githubusercontent.com/1140359/53700890-19ab0200-3dc5-11e9-872d-791eec295262.png). Heatmap of the col-normalized values:. ![image](https://user-images.githubusercontent.com/1140359/53700893-2c253b80-3dc5-11e9-968d-b7a89eb65fbc.png). PS: The option is actually borrowed from Seaborn (https://seaborn.pydata.org/generated/seaborn.clustermap.html). . PPS: There is an edge case such as division by zero. Also, `swap_axes` option makes 'row'/'col' naming a bit confusing. Let me know if you have suggestions about these or the standardization idea in general.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/512
https://github.com/scverse/scanpy/pull/512:982,deployability,cluster,clustermap,982,"Add row or column scaling option to matrixplot; Especially when we visualize raw counts, sometimes it's hard to see the differences between the expression of one gene across cell types in the heatmap since one value can simply dominate the dynamic range of expression. I think we can add a scaling option to matrixplot, which squashes expression values between 0 and 1 to make markers more pronounced. Heatmap of the raw values:. ![image](https://user-images.githubusercontent.com/1140359/53700880-06983200-3dc5-11e9-8bd6-e001fd3d078d.png). Heatmap of the logarithmized values (which also helps a bit but not for all genes):. ![image](https://user-images.githubusercontent.com/1140359/53700890-19ab0200-3dc5-11e9-872d-791eec295262.png). Heatmap of the col-normalized values:. ![image](https://user-images.githubusercontent.com/1140359/53700893-2c253b80-3dc5-11e9-968d-b7a89eb65fbc.png). PS: The option is actually borrowed from Seaborn (https://seaborn.pydata.org/generated/seaborn.clustermap.html). . PPS: There is an edge case such as division by zero. Also, `swap_axes` option makes 'row'/'col' naming a bit confusing. Let me know if you have suggestions about these or the standardization idea in general.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/512
https://github.com/scverse/scanpy/pull/512:192,energy efficiency,heat,heatmap,192,"Add row or column scaling option to matrixplot; Especially when we visualize raw counts, sometimes it's hard to see the differences between the expression of one gene across cell types in the heatmap since one value can simply dominate the dynamic range of expression. I think we can add a scaling option to matrixplot, which squashes expression values between 0 and 1 to make markers more pronounced. Heatmap of the raw values:. ![image](https://user-images.githubusercontent.com/1140359/53700880-06983200-3dc5-11e9-8bd6-e001fd3d078d.png). Heatmap of the logarithmized values (which also helps a bit but not for all genes):. ![image](https://user-images.githubusercontent.com/1140359/53700890-19ab0200-3dc5-11e9-872d-791eec295262.png). Heatmap of the col-normalized values:. ![image](https://user-images.githubusercontent.com/1140359/53700893-2c253b80-3dc5-11e9-968d-b7a89eb65fbc.png). PS: The option is actually borrowed from Seaborn (https://seaborn.pydata.org/generated/seaborn.clustermap.html). . PPS: There is an edge case such as division by zero. Also, `swap_axes` option makes 'row'/'col' naming a bit confusing. Let me know if you have suggestions about these or the standardization idea in general.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/512
https://github.com/scverse/scanpy/pull/512:402,energy efficiency,Heat,Heatmap,402,"Add row or column scaling option to matrixplot; Especially when we visualize raw counts, sometimes it's hard to see the differences between the expression of one gene across cell types in the heatmap since one value can simply dominate the dynamic range of expression. I think we can add a scaling option to matrixplot, which squashes expression values between 0 and 1 to make markers more pronounced. Heatmap of the raw values:. ![image](https://user-images.githubusercontent.com/1140359/53700880-06983200-3dc5-11e9-8bd6-e001fd3d078d.png). Heatmap of the logarithmized values (which also helps a bit but not for all genes):. ![image](https://user-images.githubusercontent.com/1140359/53700890-19ab0200-3dc5-11e9-872d-791eec295262.png). Heatmap of the col-normalized values:. ![image](https://user-images.githubusercontent.com/1140359/53700893-2c253b80-3dc5-11e9-968d-b7a89eb65fbc.png). PS: The option is actually borrowed from Seaborn (https://seaborn.pydata.org/generated/seaborn.clustermap.html). . PPS: There is an edge case such as division by zero. Also, `swap_axes` option makes 'row'/'col' naming a bit confusing. Let me know if you have suggestions about these or the standardization idea in general.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/512
https://github.com/scverse/scanpy/pull/512:541,energy efficiency,Heat,Heatmap,541,"Add row or column scaling option to matrixplot; Especially when we visualize raw counts, sometimes it's hard to see the differences between the expression of one gene across cell types in the heatmap since one value can simply dominate the dynamic range of expression. I think we can add a scaling option to matrixplot, which squashes expression values between 0 and 1 to make markers more pronounced. Heatmap of the raw values:. ![image](https://user-images.githubusercontent.com/1140359/53700880-06983200-3dc5-11e9-8bd6-e001fd3d078d.png). Heatmap of the logarithmized values (which also helps a bit but not for all genes):. ![image](https://user-images.githubusercontent.com/1140359/53700890-19ab0200-3dc5-11e9-872d-791eec295262.png). Heatmap of the col-normalized values:. ![image](https://user-images.githubusercontent.com/1140359/53700893-2c253b80-3dc5-11e9-968d-b7a89eb65fbc.png). PS: The option is actually borrowed from Seaborn (https://seaborn.pydata.org/generated/seaborn.clustermap.html). . PPS: There is an edge case such as division by zero. Also, `swap_axes` option makes 'row'/'col' naming a bit confusing. Let me know if you have suggestions about these or the standardization idea in general.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/512
https://github.com/scverse/scanpy/pull/512:737,energy efficiency,Heat,Heatmap,737,"Add row or column scaling option to matrixplot; Especially when we visualize raw counts, sometimes it's hard to see the differences between the expression of one gene across cell types in the heatmap since one value can simply dominate the dynamic range of expression. I think we can add a scaling option to matrixplot, which squashes expression values between 0 and 1 to make markers more pronounced. Heatmap of the raw values:. ![image](https://user-images.githubusercontent.com/1140359/53700880-06983200-3dc5-11e9-8bd6-e001fd3d078d.png). Heatmap of the logarithmized values (which also helps a bit but not for all genes):. ![image](https://user-images.githubusercontent.com/1140359/53700890-19ab0200-3dc5-11e9-872d-791eec295262.png). Heatmap of the col-normalized values:. ![image](https://user-images.githubusercontent.com/1140359/53700893-2c253b80-3dc5-11e9-968d-b7a89eb65fbc.png). PS: The option is actually borrowed from Seaborn (https://seaborn.pydata.org/generated/seaborn.clustermap.html). . PPS: There is an edge case such as division by zero. Also, `swap_axes` option makes 'row'/'col' naming a bit confusing. Let me know if you have suggestions about these or the standardization idea in general.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/512
https://github.com/scverse/scanpy/pull/512:1177,integrability,standardiz,standardization,1177,"Add row or column scaling option to matrixplot; Especially when we visualize raw counts, sometimes it's hard to see the differences between the expression of one gene across cell types in the heatmap since one value can simply dominate the dynamic range of expression. I think we can add a scaling option to matrixplot, which squashes expression values between 0 and 1 to make markers more pronounced. Heatmap of the raw values:. ![image](https://user-images.githubusercontent.com/1140359/53700880-06983200-3dc5-11e9-8bd6-e001fd3d078d.png). Heatmap of the logarithmized values (which also helps a bit but not for all genes):. ![image](https://user-images.githubusercontent.com/1140359/53700890-19ab0200-3dc5-11e9-872d-791eec295262.png). Heatmap of the col-normalized values:. ![image](https://user-images.githubusercontent.com/1140359/53700893-2c253b80-3dc5-11e9-968d-b7a89eb65fbc.png). PS: The option is actually borrowed from Seaborn (https://seaborn.pydata.org/generated/seaborn.clustermap.html). . PPS: There is an edge case such as division by zero. Also, `swap_axes` option makes 'row'/'col' naming a bit confusing. Let me know if you have suggestions about these or the standardization idea in general.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/512
https://github.com/scverse/scanpy/pull/512:1177,interoperability,standard,standardization,1177,"Add row or column scaling option to matrixplot; Especially when we visualize raw counts, sometimes it's hard to see the differences between the expression of one gene across cell types in the heatmap since one value can simply dominate the dynamic range of expression. I think we can add a scaling option to matrixplot, which squashes expression values between 0 and 1 to make markers more pronounced. Heatmap of the raw values:. ![image](https://user-images.githubusercontent.com/1140359/53700880-06983200-3dc5-11e9-8bd6-e001fd3d078d.png). Heatmap of the logarithmized values (which also helps a bit but not for all genes):. ![image](https://user-images.githubusercontent.com/1140359/53700890-19ab0200-3dc5-11e9-872d-791eec295262.png). Heatmap of the col-normalized values:. ![image](https://user-images.githubusercontent.com/1140359/53700893-2c253b80-3dc5-11e9-968d-b7a89eb65fbc.png). PS: The option is actually borrowed from Seaborn (https://seaborn.pydata.org/generated/seaborn.clustermap.html). . PPS: There is an edge case such as division by zero. Also, `swap_axes` option makes 'row'/'col' naming a bit confusing. Let me know if you have suggestions about these or the standardization idea in general.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/512
https://github.com/scverse/scanpy/pull/512:18,modifiability,scal,scaling,18,"Add row or column scaling option to matrixplot; Especially when we visualize raw counts, sometimes it's hard to see the differences between the expression of one gene across cell types in the heatmap since one value can simply dominate the dynamic range of expression. I think we can add a scaling option to matrixplot, which squashes expression values between 0 and 1 to make markers more pronounced. Heatmap of the raw values:. ![image](https://user-images.githubusercontent.com/1140359/53700880-06983200-3dc5-11e9-8bd6-e001fd3d078d.png). Heatmap of the logarithmized values (which also helps a bit but not for all genes):. ![image](https://user-images.githubusercontent.com/1140359/53700890-19ab0200-3dc5-11e9-872d-791eec295262.png). Heatmap of the col-normalized values:. ![image](https://user-images.githubusercontent.com/1140359/53700893-2c253b80-3dc5-11e9-968d-b7a89eb65fbc.png). PS: The option is actually borrowed from Seaborn (https://seaborn.pydata.org/generated/seaborn.clustermap.html). . PPS: There is an edge case such as division by zero. Also, `swap_axes` option makes 'row'/'col' naming a bit confusing. Let me know if you have suggestions about these or the standardization idea in general.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/512
https://github.com/scverse/scanpy/pull/512:290,modifiability,scal,scaling,290,"Add row or column scaling option to matrixplot; Especially when we visualize raw counts, sometimes it's hard to see the differences between the expression of one gene across cell types in the heatmap since one value can simply dominate the dynamic range of expression. I think we can add a scaling option to matrixplot, which squashes expression values between 0 and 1 to make markers more pronounced. Heatmap of the raw values:. ![image](https://user-images.githubusercontent.com/1140359/53700880-06983200-3dc5-11e9-8bd6-e001fd3d078d.png). Heatmap of the logarithmized values (which also helps a bit but not for all genes):. ![image](https://user-images.githubusercontent.com/1140359/53700890-19ab0200-3dc5-11e9-872d-791eec295262.png). Heatmap of the col-normalized values:. ![image](https://user-images.githubusercontent.com/1140359/53700893-2c253b80-3dc5-11e9-968d-b7a89eb65fbc.png). PS: The option is actually borrowed from Seaborn (https://seaborn.pydata.org/generated/seaborn.clustermap.html). . PPS: There is an edge case such as division by zero. Also, `swap_axes` option makes 'row'/'col' naming a bit confusing. Let me know if you have suggestions about these or the standardization idea in general.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/512
https://github.com/scverse/scanpy/pull/512:556,safety,log,logarithmized,556,"Add row or column scaling option to matrixplot; Especially when we visualize raw counts, sometimes it's hard to see the differences between the expression of one gene across cell types in the heatmap since one value can simply dominate the dynamic range of expression. I think we can add a scaling option to matrixplot, which squashes expression values between 0 and 1 to make markers more pronounced. Heatmap of the raw values:. ![image](https://user-images.githubusercontent.com/1140359/53700880-06983200-3dc5-11e9-8bd6-e001fd3d078d.png). Heatmap of the logarithmized values (which also helps a bit but not for all genes):. ![image](https://user-images.githubusercontent.com/1140359/53700890-19ab0200-3dc5-11e9-872d-791eec295262.png). Heatmap of the col-normalized values:. ![image](https://user-images.githubusercontent.com/1140359/53700893-2c253b80-3dc5-11e9-968d-b7a89eb65fbc.png). PS: The option is actually borrowed from Seaborn (https://seaborn.pydata.org/generated/seaborn.clustermap.html). . PPS: There is an edge case such as division by zero. Also, `swap_axes` option makes 'row'/'col' naming a bit confusing. Let me know if you have suggestions about these or the standardization idea in general.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/512
https://github.com/scverse/scanpy/pull/512:556,security,log,logarithmized,556,"Add row or column scaling option to matrixplot; Especially when we visualize raw counts, sometimes it's hard to see the differences between the expression of one gene across cell types in the heatmap since one value can simply dominate the dynamic range of expression. I think we can add a scaling option to matrixplot, which squashes expression values between 0 and 1 to make markers more pronounced. Heatmap of the raw values:. ![image](https://user-images.githubusercontent.com/1140359/53700880-06983200-3dc5-11e9-8bd6-e001fd3d078d.png). Heatmap of the logarithmized values (which also helps a bit but not for all genes):. ![image](https://user-images.githubusercontent.com/1140359/53700890-19ab0200-3dc5-11e9-872d-791eec295262.png). Heatmap of the col-normalized values:. ![image](https://user-images.githubusercontent.com/1140359/53700893-2c253b80-3dc5-11e9-968d-b7a89eb65fbc.png). PS: The option is actually borrowed from Seaborn (https://seaborn.pydata.org/generated/seaborn.clustermap.html). . PPS: There is an edge case such as division by zero. Also, `swap_axes` option makes 'row'/'col' naming a bit confusing. Let me know if you have suggestions about these or the standardization idea in general.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/512
https://github.com/scverse/scanpy/pull/512:220,testability,simpl,simply,220,"Add row or column scaling option to matrixplot; Especially when we visualize raw counts, sometimes it's hard to see the differences between the expression of one gene across cell types in the heatmap since one value can simply dominate the dynamic range of expression. I think we can add a scaling option to matrixplot, which squashes expression values between 0 and 1 to make markers more pronounced. Heatmap of the raw values:. ![image](https://user-images.githubusercontent.com/1140359/53700880-06983200-3dc5-11e9-8bd6-e001fd3d078d.png). Heatmap of the logarithmized values (which also helps a bit but not for all genes):. ![image](https://user-images.githubusercontent.com/1140359/53700890-19ab0200-3dc5-11e9-872d-791eec295262.png). Heatmap of the col-normalized values:. ![image](https://user-images.githubusercontent.com/1140359/53700893-2c253b80-3dc5-11e9-968d-b7a89eb65fbc.png). PS: The option is actually borrowed from Seaborn (https://seaborn.pydata.org/generated/seaborn.clustermap.html). . PPS: There is an edge case such as division by zero. Also, `swap_axes` option makes 'row'/'col' naming a bit confusing. Let me know if you have suggestions about these or the standardization idea in general.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/512
https://github.com/scverse/scanpy/pull/512:556,testability,log,logarithmized,556,"Add row or column scaling option to matrixplot; Especially when we visualize raw counts, sometimes it's hard to see the differences between the expression of one gene across cell types in the heatmap since one value can simply dominate the dynamic range of expression. I think we can add a scaling option to matrixplot, which squashes expression values between 0 and 1 to make markers more pronounced. Heatmap of the raw values:. ![image](https://user-images.githubusercontent.com/1140359/53700880-06983200-3dc5-11e9-8bd6-e001fd3d078d.png). Heatmap of the logarithmized values (which also helps a bit but not for all genes):. ![image](https://user-images.githubusercontent.com/1140359/53700890-19ab0200-3dc5-11e9-872d-791eec295262.png). Heatmap of the col-normalized values:. ![image](https://user-images.githubusercontent.com/1140359/53700893-2c253b80-3dc5-11e9-968d-b7a89eb65fbc.png). PS: The option is actually borrowed from Seaborn (https://seaborn.pydata.org/generated/seaborn.clustermap.html). . PPS: There is an edge case such as division by zero. Also, `swap_axes` option makes 'row'/'col' naming a bit confusing. Let me know if you have suggestions about these or the standardization idea in general.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/512
https://github.com/scverse/scanpy/pull/512:67,usability,visual,visualize,67,"Add row or column scaling option to matrixplot; Especially when we visualize raw counts, sometimes it's hard to see the differences between the expression of one gene across cell types in the heatmap since one value can simply dominate the dynamic range of expression. I think we can add a scaling option to matrixplot, which squashes expression values between 0 and 1 to make markers more pronounced. Heatmap of the raw values:. ![image](https://user-images.githubusercontent.com/1140359/53700880-06983200-3dc5-11e9-8bd6-e001fd3d078d.png). Heatmap of the logarithmized values (which also helps a bit but not for all genes):. ![image](https://user-images.githubusercontent.com/1140359/53700890-19ab0200-3dc5-11e9-872d-791eec295262.png). Heatmap of the col-normalized values:. ![image](https://user-images.githubusercontent.com/1140359/53700893-2c253b80-3dc5-11e9-968d-b7a89eb65fbc.png). PS: The option is actually borrowed from Seaborn (https://seaborn.pydata.org/generated/seaborn.clustermap.html). . PPS: There is an edge case such as division by zero. Also, `swap_axes` option makes 'row'/'col' naming a bit confusing. Let me know if you have suggestions about these or the standardization idea in general.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/512
https://github.com/scverse/scanpy/pull/512:220,usability,simpl,simply,220,"Add row or column scaling option to matrixplot; Especially when we visualize raw counts, sometimes it's hard to see the differences between the expression of one gene across cell types in the heatmap since one value can simply dominate the dynamic range of expression. I think we can add a scaling option to matrixplot, which squashes expression values between 0 and 1 to make markers more pronounced. Heatmap of the raw values:. ![image](https://user-images.githubusercontent.com/1140359/53700880-06983200-3dc5-11e9-8bd6-e001fd3d078d.png). Heatmap of the logarithmized values (which also helps a bit but not for all genes):. ![image](https://user-images.githubusercontent.com/1140359/53700890-19ab0200-3dc5-11e9-872d-791eec295262.png). Heatmap of the col-normalized values:. ![image](https://user-images.githubusercontent.com/1140359/53700893-2c253b80-3dc5-11e9-968d-b7a89eb65fbc.png). PS: The option is actually borrowed from Seaborn (https://seaborn.pydata.org/generated/seaborn.clustermap.html). . PPS: There is an edge case such as division by zero. Also, `swap_axes` option makes 'row'/'col' naming a bit confusing. Let me know if you have suggestions about these or the standardization idea in general.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/512
https://github.com/scverse/scanpy/pull/512:447,usability,user,user-images,447,"Add row or column scaling option to matrixplot; Especially when we visualize raw counts, sometimes it's hard to see the differences between the expression of one gene across cell types in the heatmap since one value can simply dominate the dynamic range of expression. I think we can add a scaling option to matrixplot, which squashes expression values between 0 and 1 to make markers more pronounced. Heatmap of the raw values:. ![image](https://user-images.githubusercontent.com/1140359/53700880-06983200-3dc5-11e9-8bd6-e001fd3d078d.png). Heatmap of the logarithmized values (which also helps a bit but not for all genes):. ![image](https://user-images.githubusercontent.com/1140359/53700890-19ab0200-3dc5-11e9-872d-791eec295262.png). Heatmap of the col-normalized values:. ![image](https://user-images.githubusercontent.com/1140359/53700893-2c253b80-3dc5-11e9-968d-b7a89eb65fbc.png). PS: The option is actually borrowed from Seaborn (https://seaborn.pydata.org/generated/seaborn.clustermap.html). . PPS: There is an edge case such as division by zero. Also, `swap_axes` option makes 'row'/'col' naming a bit confusing. Let me know if you have suggestions about these or the standardization idea in general.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/512
https://github.com/scverse/scanpy/pull/512:589,usability,help,helps,589,"Add row or column scaling option to matrixplot; Especially when we visualize raw counts, sometimes it's hard to see the differences between the expression of one gene across cell types in the heatmap since one value can simply dominate the dynamic range of expression. I think we can add a scaling option to matrixplot, which squashes expression values between 0 and 1 to make markers more pronounced. Heatmap of the raw values:. ![image](https://user-images.githubusercontent.com/1140359/53700880-06983200-3dc5-11e9-8bd6-e001fd3d078d.png). Heatmap of the logarithmized values (which also helps a bit but not for all genes):. ![image](https://user-images.githubusercontent.com/1140359/53700890-19ab0200-3dc5-11e9-872d-791eec295262.png). Heatmap of the col-normalized values:. ![image](https://user-images.githubusercontent.com/1140359/53700893-2c253b80-3dc5-11e9-968d-b7a89eb65fbc.png). PS: The option is actually borrowed from Seaborn (https://seaborn.pydata.org/generated/seaborn.clustermap.html). . PPS: There is an edge case such as division by zero. Also, `swap_axes` option makes 'row'/'col' naming a bit confusing. Let me know if you have suggestions about these or the standardization idea in general.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/512
https://github.com/scverse/scanpy/pull/512:643,usability,user,user-images,643,"Add row or column scaling option to matrixplot; Especially when we visualize raw counts, sometimes it's hard to see the differences between the expression of one gene across cell types in the heatmap since one value can simply dominate the dynamic range of expression. I think we can add a scaling option to matrixplot, which squashes expression values between 0 and 1 to make markers more pronounced. Heatmap of the raw values:. ![image](https://user-images.githubusercontent.com/1140359/53700880-06983200-3dc5-11e9-8bd6-e001fd3d078d.png). Heatmap of the logarithmized values (which also helps a bit but not for all genes):. ![image](https://user-images.githubusercontent.com/1140359/53700890-19ab0200-3dc5-11e9-872d-791eec295262.png). Heatmap of the col-normalized values:. ![image](https://user-images.githubusercontent.com/1140359/53700893-2c253b80-3dc5-11e9-968d-b7a89eb65fbc.png). PS: The option is actually borrowed from Seaborn (https://seaborn.pydata.org/generated/seaborn.clustermap.html). . PPS: There is an edge case such as division by zero. Also, `swap_axes` option makes 'row'/'col' naming a bit confusing. Let me know if you have suggestions about these or the standardization idea in general.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/512
https://github.com/scverse/scanpy/pull/512:793,usability,user,user-images,793,"Add row or column scaling option to matrixplot; Especially when we visualize raw counts, sometimes it's hard to see the differences between the expression of one gene across cell types in the heatmap since one value can simply dominate the dynamic range of expression. I think we can add a scaling option to matrixplot, which squashes expression values between 0 and 1 to make markers more pronounced. Heatmap of the raw values:. ![image](https://user-images.githubusercontent.com/1140359/53700880-06983200-3dc5-11e9-8bd6-e001fd3d078d.png). Heatmap of the logarithmized values (which also helps a bit but not for all genes):. ![image](https://user-images.githubusercontent.com/1140359/53700890-19ab0200-3dc5-11e9-872d-791eec295262.png). Heatmap of the col-normalized values:. ![image](https://user-images.githubusercontent.com/1140359/53700893-2c253b80-3dc5-11e9-968d-b7a89eb65fbc.png). PS: The option is actually borrowed from Seaborn (https://seaborn.pydata.org/generated/seaborn.clustermap.html). . PPS: There is an edge case such as division by zero. Also, `swap_axes` option makes 'row'/'col' naming a bit confusing. Let me know if you have suggestions about these or the standardization idea in general.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/512
https://github.com/scverse/scanpy/issues/513:24,availability,cluster,cluster,24,"Color cells from only 1 cluster at a time; Is there a way to color cells belonging only to a certain cluster on the UMAP plot ? For example, can I have only cells in cluster 0 in a dark color and everything else as grey colored dots in the background ? I would also like to do the same with other adata observations like Patient ID etc. . Thank you.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/513
https://github.com/scverse/scanpy/issues/513:101,availability,cluster,cluster,101,"Color cells from only 1 cluster at a time; Is there a way to color cells belonging only to a certain cluster on the UMAP plot ? For example, can I have only cells in cluster 0 in a dark color and everything else as grey colored dots in the background ? I would also like to do the same with other adata observations like Patient ID etc. . Thank you.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/513
https://github.com/scverse/scanpy/issues/513:166,availability,cluster,cluster,166,"Color cells from only 1 cluster at a time; Is there a way to color cells belonging only to a certain cluster on the UMAP plot ? For example, can I have only cells in cluster 0 in a dark color and everything else as grey colored dots in the background ? I would also like to do the same with other adata observations like Patient ID etc. . Thank you.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/513
https://github.com/scverse/scanpy/issues/513:24,deployability,cluster,cluster,24,"Color cells from only 1 cluster at a time; Is there a way to color cells belonging only to a certain cluster on the UMAP plot ? For example, can I have only cells in cluster 0 in a dark color and everything else as grey colored dots in the background ? I would also like to do the same with other adata observations like Patient ID etc. . Thank you.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/513
https://github.com/scverse/scanpy/issues/513:101,deployability,cluster,cluster,101,"Color cells from only 1 cluster at a time; Is there a way to color cells belonging only to a certain cluster on the UMAP plot ? For example, can I have only cells in cluster 0 in a dark color and everything else as grey colored dots in the background ? I would also like to do the same with other adata observations like Patient ID etc. . Thank you.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/513
https://github.com/scverse/scanpy/issues/513:166,deployability,cluster,cluster,166,"Color cells from only 1 cluster at a time; Is there a way to color cells belonging only to a certain cluster on the UMAP plot ? For example, can I have only cells in cluster 0 in a dark color and everything else as grey colored dots in the background ? I would also like to do the same with other adata observations like Patient ID etc. . Thank you.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/513
https://github.com/scverse/scanpy/issues/513:303,deployability,observ,observations,303,"Color cells from only 1 cluster at a time; Is there a way to color cells belonging only to a certain cluster on the UMAP plot ? For example, can I have only cells in cluster 0 in a dark color and everything else as grey colored dots in the background ? I would also like to do the same with other adata observations like Patient ID etc. . Thank you.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/513
https://github.com/scverse/scanpy/issues/513:37,performance,time,time,37,"Color cells from only 1 cluster at a time; Is there a way to color cells belonging only to a certain cluster on the UMAP plot ? For example, can I have only cells in cluster 0 in a dark color and everything else as grey colored dots in the background ? I would also like to do the same with other adata observations like Patient ID etc. . Thank you.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/513
https://github.com/scverse/scanpy/issues/513:303,testability,observ,observations,303,"Color cells from only 1 cluster at a time; Is there a way to color cells belonging only to a certain cluster on the UMAP plot ? For example, can I have only cells in cluster 0 in a dark color and everything else as grey colored dots in the background ? I would also like to do the same with other adata observations like Patient ID etc. . Thank you.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/513
https://github.com/scverse/scanpy/issues/514:102,availability,error,error,102,"Issues on bbknn; Hej all. it seems there is a problem on the batch correction with bbknn. It gives an error at the pca step of bbknn, but I have problem understanding if this is due to the bbknn package itself or the wrapper of scanpy around it, or if it is due to my data, even though it worked when I used it previously. ```python. sc.external.pp.bbknn(all_data_flt, batch_key='batch', n_pcs=15,). ```. gives the error. ```python. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-23-a9dd619ada2e> in <module>. 1 #sc.neighbors.neighbors(all_data_flt, n_neighbors=40, n_pcs=15). ----> 2 sc.external.pp.bbknn(all_data_flt, n_pcs=15). 3 #sc.tools.umap(all_data_flt). ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/preprocessing/_bbknn.py in bbknn(adata, batch_key, save_knn, copy, **kwargs). 82 params = locals(). 83 kwargs = params.pop('kwargs'). ---> 84 return bbknn(**params, **kwargs). ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/bbknn/__init__.py in bbknn(adata, batch_key, save_knn, copy, **kwargs). 215 batch_list = adata.obs[batch_key].values. 216 #call BBKNN proper. --> 217 bbknn_out = bbknn_pca_matrix(pca=pca,batch_list=batch_list,save_knn=save_knn,**kwargs). 218 #optionally save knn_indices. 219 if save_knn:. TypeError: bbknn_pca_matrix() got an unexpected keyword argument 'bbknn'. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/514
https://github.com/scverse/scanpy/issues/514:415,availability,error,error,415,"Issues on bbknn; Hej all. it seems there is a problem on the batch correction with bbknn. It gives an error at the pca step of bbknn, but I have problem understanding if this is due to the bbknn package itself or the wrapper of scanpy around it, or if it is due to my data, even though it worked when I used it previously. ```python. sc.external.pp.bbknn(all_data_flt, batch_key='batch', n_pcs=15,). ```. gives the error. ```python. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-23-a9dd619ada2e> in <module>. 1 #sc.neighbors.neighbors(all_data_flt, n_neighbors=40, n_pcs=15). ----> 2 sc.external.pp.bbknn(all_data_flt, n_pcs=15). 3 #sc.tools.umap(all_data_flt). ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/preprocessing/_bbknn.py in bbknn(adata, batch_key, save_knn, copy, **kwargs). 82 params = locals(). 83 kwargs = params.pop('kwargs'). ---> 84 return bbknn(**params, **kwargs). ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/bbknn/__init__.py in bbknn(adata, batch_key, save_knn, copy, **kwargs). 215 batch_list = adata.obs[batch_key].values. 216 #call BBKNN proper. --> 217 bbknn_out = bbknn_pca_matrix(pca=pca,batch_list=batch_list,save_knn=save_knn,**kwargs). 218 #optionally save knn_indices. 219 if save_knn:. TypeError: bbknn_pca_matrix() got an unexpected keyword argument 'bbknn'. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/514
https://github.com/scverse/scanpy/issues/514:591,deployability,modul,module,591,"Issues on bbknn; Hej all. it seems there is a problem on the batch correction with bbknn. It gives an error at the pca step of bbknn, but I have problem understanding if this is due to the bbknn package itself or the wrapper of scanpy around it, or if it is due to my data, even though it worked when I used it previously. ```python. sc.external.pp.bbknn(all_data_flt, batch_key='batch', n_pcs=15,). ```. gives the error. ```python. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-23-a9dd619ada2e> in <module>. 1 #sc.neighbors.neighbors(all_data_flt, n_neighbors=40, n_pcs=15). ----> 2 sc.external.pp.bbknn(all_data_flt, n_pcs=15). 3 #sc.tools.umap(all_data_flt). ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/preprocessing/_bbknn.py in bbknn(adata, batch_key, save_knn, copy, **kwargs). 82 params = locals(). 83 kwargs = params.pop('kwargs'). ---> 84 return bbknn(**params, **kwargs). ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/bbknn/__init__.py in bbknn(adata, batch_key, save_knn, copy, **kwargs). 215 batch_list = adata.obs[batch_key].values. 216 #call BBKNN proper. --> 217 bbknn_out = bbknn_pca_matrix(pca=pca,batch_list=batch_list,save_knn=save_knn,**kwargs). 218 #optionally save knn_indices. 219 if save_knn:. TypeError: bbknn_pca_matrix() got an unexpected keyword argument 'bbknn'. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/514
https://github.com/scverse/scanpy/issues/514:61,integrability,batch,batch,61,"Issues on bbknn; Hej all. it seems there is a problem on the batch correction with bbknn. It gives an error at the pca step of bbknn, but I have problem understanding if this is due to the bbknn package itself or the wrapper of scanpy around it, or if it is due to my data, even though it worked when I used it previously. ```python. sc.external.pp.bbknn(all_data_flt, batch_key='batch', n_pcs=15,). ```. gives the error. ```python. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-23-a9dd619ada2e> in <module>. 1 #sc.neighbors.neighbors(all_data_flt, n_neighbors=40, n_pcs=15). ----> 2 sc.external.pp.bbknn(all_data_flt, n_pcs=15). 3 #sc.tools.umap(all_data_flt). ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/preprocessing/_bbknn.py in bbknn(adata, batch_key, save_knn, copy, **kwargs). 82 params = locals(). 83 kwargs = params.pop('kwargs'). ---> 84 return bbknn(**params, **kwargs). ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/bbknn/__init__.py in bbknn(adata, batch_key, save_knn, copy, **kwargs). 215 batch_list = adata.obs[batch_key].values. 216 #call BBKNN proper. --> 217 bbknn_out = bbknn_pca_matrix(pca=pca,batch_list=batch_list,save_knn=save_knn,**kwargs). 218 #optionally save knn_indices. 219 if save_knn:. TypeError: bbknn_pca_matrix() got an unexpected keyword argument 'bbknn'. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/514
https://github.com/scverse/scanpy/issues/514:217,integrability,wrap,wrapper,217,"Issues on bbknn; Hej all. it seems there is a problem on the batch correction with bbknn. It gives an error at the pca step of bbknn, but I have problem understanding if this is due to the bbknn package itself or the wrapper of scanpy around it, or if it is due to my data, even though it worked when I used it previously. ```python. sc.external.pp.bbknn(all_data_flt, batch_key='batch', n_pcs=15,). ```. gives the error. ```python. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-23-a9dd619ada2e> in <module>. 1 #sc.neighbors.neighbors(all_data_flt, n_neighbors=40, n_pcs=15). ----> 2 sc.external.pp.bbknn(all_data_flt, n_pcs=15). 3 #sc.tools.umap(all_data_flt). ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/preprocessing/_bbknn.py in bbknn(adata, batch_key, save_knn, copy, **kwargs). 82 params = locals(). 83 kwargs = params.pop('kwargs'). ---> 84 return bbknn(**params, **kwargs). ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/bbknn/__init__.py in bbknn(adata, batch_key, save_knn, copy, **kwargs). 215 batch_list = adata.obs[batch_key].values. 216 #call BBKNN proper. --> 217 bbknn_out = bbknn_pca_matrix(pca=pca,batch_list=batch_list,save_knn=save_knn,**kwargs). 218 #optionally save knn_indices. 219 if save_knn:. TypeError: bbknn_pca_matrix() got an unexpected keyword argument 'bbknn'. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/514
https://github.com/scverse/scanpy/issues/514:380,integrability,batch,batch,380,"Issues on bbknn; Hej all. it seems there is a problem on the batch correction with bbknn. It gives an error at the pca step of bbknn, but I have problem understanding if this is due to the bbknn package itself or the wrapper of scanpy around it, or if it is due to my data, even though it worked when I used it previously. ```python. sc.external.pp.bbknn(all_data_flt, batch_key='batch', n_pcs=15,). ```. gives the error. ```python. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-23-a9dd619ada2e> in <module>. 1 #sc.neighbors.neighbors(all_data_flt, n_neighbors=40, n_pcs=15). ----> 2 sc.external.pp.bbknn(all_data_flt, n_pcs=15). 3 #sc.tools.umap(all_data_flt). ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/preprocessing/_bbknn.py in bbknn(adata, batch_key, save_knn, copy, **kwargs). 82 params = locals(). 83 kwargs = params.pop('kwargs'). ---> 84 return bbknn(**params, **kwargs). ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/bbknn/__init__.py in bbknn(adata, batch_key, save_knn, copy, **kwargs). 215 batch_list = adata.obs[batch_key].values. 216 #call BBKNN proper. --> 217 bbknn_out = bbknn_pca_matrix(pca=pca,batch_list=batch_list,save_knn=save_knn,**kwargs). 218 #optionally save knn_indices. 219 if save_knn:. TypeError: bbknn_pca_matrix() got an unexpected keyword argument 'bbknn'. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/514
https://github.com/scverse/scanpy/issues/514:217,interoperability,wrapper,wrapper,217,"Issues on bbknn; Hej all. it seems there is a problem on the batch correction with bbknn. It gives an error at the pca step of bbknn, but I have problem understanding if this is due to the bbknn package itself or the wrapper of scanpy around it, or if it is due to my data, even though it worked when I used it previously. ```python. sc.external.pp.bbknn(all_data_flt, batch_key='batch', n_pcs=15,). ```. gives the error. ```python. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-23-a9dd619ada2e> in <module>. 1 #sc.neighbors.neighbors(all_data_flt, n_neighbors=40, n_pcs=15). ----> 2 sc.external.pp.bbknn(all_data_flt, n_pcs=15). 3 #sc.tools.umap(all_data_flt). ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/preprocessing/_bbknn.py in bbknn(adata, batch_key, save_knn, copy, **kwargs). 82 params = locals(). 83 kwargs = params.pop('kwargs'). ---> 84 return bbknn(**params, **kwargs). ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/bbknn/__init__.py in bbknn(adata, batch_key, save_knn, copy, **kwargs). 215 batch_list = adata.obs[batch_key].values. 216 #call BBKNN proper. --> 217 bbknn_out = bbknn_pca_matrix(pca=pca,batch_list=batch_list,save_knn=save_knn,**kwargs). 218 #optionally save knn_indices. 219 if save_knn:. TypeError: bbknn_pca_matrix() got an unexpected keyword argument 'bbknn'. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/514
https://github.com/scverse/scanpy/issues/514:195,modifiability,pac,package,195,"Issues on bbknn; Hej all. it seems there is a problem on the batch correction with bbknn. It gives an error at the pca step of bbknn, but I have problem understanding if this is due to the bbknn package itself or the wrapper of scanpy around it, or if it is due to my data, even though it worked when I used it previously. ```python. sc.external.pp.bbknn(all_data_flt, batch_key='batch', n_pcs=15,). ```. gives the error. ```python. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-23-a9dd619ada2e> in <module>. 1 #sc.neighbors.neighbors(all_data_flt, n_neighbors=40, n_pcs=15). ----> 2 sc.external.pp.bbknn(all_data_flt, n_pcs=15). 3 #sc.tools.umap(all_data_flt). ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/preprocessing/_bbknn.py in bbknn(adata, batch_key, save_knn, copy, **kwargs). 82 params = locals(). 83 kwargs = params.pop('kwargs'). ---> 84 return bbknn(**params, **kwargs). ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/bbknn/__init__.py in bbknn(adata, batch_key, save_knn, copy, **kwargs). 215 batch_list = adata.obs[batch_key].values. 216 #call BBKNN proper. --> 217 bbknn_out = bbknn_pca_matrix(pca=pca,batch_list=batch_list,save_knn=save_knn,**kwargs). 218 #optionally save knn_indices. 219 if save_knn:. TypeError: bbknn_pca_matrix() got an unexpected keyword argument 'bbknn'. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/514
https://github.com/scverse/scanpy/issues/514:591,modifiability,modul,module,591,"Issues on bbknn; Hej all. it seems there is a problem on the batch correction with bbknn. It gives an error at the pca step of bbknn, but I have problem understanding if this is due to the bbknn package itself or the wrapper of scanpy around it, or if it is due to my data, even though it worked when I used it previously. ```python. sc.external.pp.bbknn(all_data_flt, batch_key='batch', n_pcs=15,). ```. gives the error. ```python. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-23-a9dd619ada2e> in <module>. 1 #sc.neighbors.neighbors(all_data_flt, n_neighbors=40, n_pcs=15). ----> 2 sc.external.pp.bbknn(all_data_flt, n_pcs=15). 3 #sc.tools.umap(all_data_flt). ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/preprocessing/_bbknn.py in bbknn(adata, batch_key, save_knn, copy, **kwargs). 82 params = locals(). 83 kwargs = params.pop('kwargs'). ---> 84 return bbknn(**params, **kwargs). ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/bbknn/__init__.py in bbknn(adata, batch_key, save_knn, copy, **kwargs). 215 batch_list = adata.obs[batch_key].values. 216 #call BBKNN proper. --> 217 bbknn_out = bbknn_pca_matrix(pca=pca,batch_list=batch_list,save_knn=save_knn,**kwargs). 218 #optionally save knn_indices. 219 if save_knn:. TypeError: bbknn_pca_matrix() got an unexpected keyword argument 'bbknn'. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/514
https://github.com/scverse/scanpy/issues/514:796,modifiability,pac,packages,796,"Issues on bbknn; Hej all. it seems there is a problem on the batch correction with bbknn. It gives an error at the pca step of bbknn, but I have problem understanding if this is due to the bbknn package itself or the wrapper of scanpy around it, or if it is due to my data, even though it worked when I used it previously. ```python. sc.external.pp.bbknn(all_data_flt, batch_key='batch', n_pcs=15,). ```. gives the error. ```python. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-23-a9dd619ada2e> in <module>. 1 #sc.neighbors.neighbors(all_data_flt, n_neighbors=40, n_pcs=15). ----> 2 sc.external.pp.bbknn(all_data_flt, n_pcs=15). 3 #sc.tools.umap(all_data_flt). ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/preprocessing/_bbknn.py in bbknn(adata, batch_key, save_knn, copy, **kwargs). 82 params = locals(). 83 kwargs = params.pop('kwargs'). ---> 84 return bbknn(**params, **kwargs). ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/bbknn/__init__.py in bbknn(adata, batch_key, save_knn, copy, **kwargs). 215 batch_list = adata.obs[batch_key].values. 216 #call BBKNN proper. --> 217 bbknn_out = bbknn_pca_matrix(pca=pca,batch_list=batch_list,save_knn=save_knn,**kwargs). 218 #optionally save knn_indices. 219 if save_knn:. TypeError: bbknn_pca_matrix() got an unexpected keyword argument 'bbknn'. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/514
https://github.com/scverse/scanpy/issues/514:1031,modifiability,pac,packages,1031,"Issues on bbknn; Hej all. it seems there is a problem on the batch correction with bbknn. It gives an error at the pca step of bbknn, but I have problem understanding if this is due to the bbknn package itself or the wrapper of scanpy around it, or if it is due to my data, even though it worked when I used it previously. ```python. sc.external.pp.bbknn(all_data_flt, batch_key='batch', n_pcs=15,). ```. gives the error. ```python. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-23-a9dd619ada2e> in <module>. 1 #sc.neighbors.neighbors(all_data_flt, n_neighbors=40, n_pcs=15). ----> 2 sc.external.pp.bbknn(all_data_flt, n_pcs=15). 3 #sc.tools.umap(all_data_flt). ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/preprocessing/_bbknn.py in bbknn(adata, batch_key, save_knn, copy, **kwargs). 82 params = locals(). 83 kwargs = params.pop('kwargs'). ---> 84 return bbknn(**params, **kwargs). ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/bbknn/__init__.py in bbknn(adata, batch_key, save_knn, copy, **kwargs). 215 batch_list = adata.obs[batch_key].values. 216 #call BBKNN proper. --> 217 bbknn_out = bbknn_pca_matrix(pca=pca,batch_list=batch_list,save_knn=save_knn,**kwargs). 218 #optionally save knn_indices. 219 if save_knn:. TypeError: bbknn_pca_matrix() got an unexpected keyword argument 'bbknn'. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/514
https://github.com/scverse/scanpy/issues/514:61,performance,batch,batch,61,"Issues on bbknn; Hej all. it seems there is a problem on the batch correction with bbknn. It gives an error at the pca step of bbknn, but I have problem understanding if this is due to the bbknn package itself or the wrapper of scanpy around it, or if it is due to my data, even though it worked when I used it previously. ```python. sc.external.pp.bbknn(all_data_flt, batch_key='batch', n_pcs=15,). ```. gives the error. ```python. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-23-a9dd619ada2e> in <module>. 1 #sc.neighbors.neighbors(all_data_flt, n_neighbors=40, n_pcs=15). ----> 2 sc.external.pp.bbknn(all_data_flt, n_pcs=15). 3 #sc.tools.umap(all_data_flt). ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/preprocessing/_bbknn.py in bbknn(adata, batch_key, save_knn, copy, **kwargs). 82 params = locals(). 83 kwargs = params.pop('kwargs'). ---> 84 return bbknn(**params, **kwargs). ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/bbknn/__init__.py in bbknn(adata, batch_key, save_knn, copy, **kwargs). 215 batch_list = adata.obs[batch_key].values. 216 #call BBKNN proper. --> 217 bbknn_out = bbknn_pca_matrix(pca=pca,batch_list=batch_list,save_knn=save_knn,**kwargs). 218 #optionally save knn_indices. 219 if save_knn:. TypeError: bbknn_pca_matrix() got an unexpected keyword argument 'bbknn'. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/514
https://github.com/scverse/scanpy/issues/514:102,performance,error,error,102,"Issues on bbknn; Hej all. it seems there is a problem on the batch correction with bbknn. It gives an error at the pca step of bbknn, but I have problem understanding if this is due to the bbknn package itself or the wrapper of scanpy around it, or if it is due to my data, even though it worked when I used it previously. ```python. sc.external.pp.bbknn(all_data_flt, batch_key='batch', n_pcs=15,). ```. gives the error. ```python. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-23-a9dd619ada2e> in <module>. 1 #sc.neighbors.neighbors(all_data_flt, n_neighbors=40, n_pcs=15). ----> 2 sc.external.pp.bbknn(all_data_flt, n_pcs=15). 3 #sc.tools.umap(all_data_flt). ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/preprocessing/_bbknn.py in bbknn(adata, batch_key, save_knn, copy, **kwargs). 82 params = locals(). 83 kwargs = params.pop('kwargs'). ---> 84 return bbknn(**params, **kwargs). ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/bbknn/__init__.py in bbknn(adata, batch_key, save_knn, copy, **kwargs). 215 batch_list = adata.obs[batch_key].values. 216 #call BBKNN proper. --> 217 bbknn_out = bbknn_pca_matrix(pca=pca,batch_list=batch_list,save_knn=save_knn,**kwargs). 218 #optionally save knn_indices. 219 if save_knn:. TypeError: bbknn_pca_matrix() got an unexpected keyword argument 'bbknn'. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/514
https://github.com/scverse/scanpy/issues/514:380,performance,batch,batch,380,"Issues on bbknn; Hej all. it seems there is a problem on the batch correction with bbknn. It gives an error at the pca step of bbknn, but I have problem understanding if this is due to the bbknn package itself or the wrapper of scanpy around it, or if it is due to my data, even though it worked when I used it previously. ```python. sc.external.pp.bbknn(all_data_flt, batch_key='batch', n_pcs=15,). ```. gives the error. ```python. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-23-a9dd619ada2e> in <module>. 1 #sc.neighbors.neighbors(all_data_flt, n_neighbors=40, n_pcs=15). ----> 2 sc.external.pp.bbknn(all_data_flt, n_pcs=15). 3 #sc.tools.umap(all_data_flt). ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/preprocessing/_bbknn.py in bbknn(adata, batch_key, save_knn, copy, **kwargs). 82 params = locals(). 83 kwargs = params.pop('kwargs'). ---> 84 return bbknn(**params, **kwargs). ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/bbknn/__init__.py in bbknn(adata, batch_key, save_knn, copy, **kwargs). 215 batch_list = adata.obs[batch_key].values. 216 #call BBKNN proper. --> 217 bbknn_out = bbknn_pca_matrix(pca=pca,batch_list=batch_list,save_knn=save_knn,**kwargs). 218 #optionally save knn_indices. 219 if save_knn:. TypeError: bbknn_pca_matrix() got an unexpected keyword argument 'bbknn'. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/514
https://github.com/scverse/scanpy/issues/514:415,performance,error,error,415,"Issues on bbknn; Hej all. it seems there is a problem on the batch correction with bbknn. It gives an error at the pca step of bbknn, but I have problem understanding if this is due to the bbknn package itself or the wrapper of scanpy around it, or if it is due to my data, even though it worked when I used it previously. ```python. sc.external.pp.bbknn(all_data_flt, batch_key='batch', n_pcs=15,). ```. gives the error. ```python. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-23-a9dd619ada2e> in <module>. 1 #sc.neighbors.neighbors(all_data_flt, n_neighbors=40, n_pcs=15). ----> 2 sc.external.pp.bbknn(all_data_flt, n_pcs=15). 3 #sc.tools.umap(all_data_flt). ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/preprocessing/_bbknn.py in bbknn(adata, batch_key, save_knn, copy, **kwargs). 82 params = locals(). 83 kwargs = params.pop('kwargs'). ---> 84 return bbknn(**params, **kwargs). ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/bbknn/__init__.py in bbknn(adata, batch_key, save_knn, copy, **kwargs). 215 batch_list = adata.obs[batch_key].values. 216 #call BBKNN proper. --> 217 bbknn_out = bbknn_pca_matrix(pca=pca,batch_list=batch_list,save_knn=save_knn,**kwargs). 218 #optionally save knn_indices. 219 if save_knn:. TypeError: bbknn_pca_matrix() got an unexpected keyword argument 'bbknn'. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/514
https://github.com/scverse/scanpy/issues/514:102,safety,error,error,102,"Issues on bbknn; Hej all. it seems there is a problem on the batch correction with bbknn. It gives an error at the pca step of bbknn, but I have problem understanding if this is due to the bbknn package itself or the wrapper of scanpy around it, or if it is due to my data, even though it worked when I used it previously. ```python. sc.external.pp.bbknn(all_data_flt, batch_key='batch', n_pcs=15,). ```. gives the error. ```python. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-23-a9dd619ada2e> in <module>. 1 #sc.neighbors.neighbors(all_data_flt, n_neighbors=40, n_pcs=15). ----> 2 sc.external.pp.bbknn(all_data_flt, n_pcs=15). 3 #sc.tools.umap(all_data_flt). ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/preprocessing/_bbknn.py in bbknn(adata, batch_key, save_knn, copy, **kwargs). 82 params = locals(). 83 kwargs = params.pop('kwargs'). ---> 84 return bbknn(**params, **kwargs). ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/bbknn/__init__.py in bbknn(adata, batch_key, save_knn, copy, **kwargs). 215 batch_list = adata.obs[batch_key].values. 216 #call BBKNN proper. --> 217 bbknn_out = bbknn_pca_matrix(pca=pca,batch_list=batch_list,save_knn=save_knn,**kwargs). 218 #optionally save knn_indices. 219 if save_knn:. TypeError: bbknn_pca_matrix() got an unexpected keyword argument 'bbknn'. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/514
https://github.com/scverse/scanpy/issues/514:415,safety,error,error,415,"Issues on bbknn; Hej all. it seems there is a problem on the batch correction with bbknn. It gives an error at the pca step of bbknn, but I have problem understanding if this is due to the bbknn package itself or the wrapper of scanpy around it, or if it is due to my data, even though it worked when I used it previously. ```python. sc.external.pp.bbknn(all_data_flt, batch_key='batch', n_pcs=15,). ```. gives the error. ```python. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-23-a9dd619ada2e> in <module>. 1 #sc.neighbors.neighbors(all_data_flt, n_neighbors=40, n_pcs=15). ----> 2 sc.external.pp.bbknn(all_data_flt, n_pcs=15). 3 #sc.tools.umap(all_data_flt). ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/preprocessing/_bbknn.py in bbknn(adata, batch_key, save_knn, copy, **kwargs). 82 params = locals(). 83 kwargs = params.pop('kwargs'). ---> 84 return bbknn(**params, **kwargs). ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/bbknn/__init__.py in bbknn(adata, batch_key, save_knn, copy, **kwargs). 215 batch_list = adata.obs[batch_key].values. 216 #call BBKNN proper. --> 217 bbknn_out = bbknn_pca_matrix(pca=pca,batch_list=batch_list,save_knn=save_knn,**kwargs). 218 #optionally save knn_indices. 219 if save_knn:. TypeError: bbknn_pca_matrix() got an unexpected keyword argument 'bbknn'. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/514
https://github.com/scverse/scanpy/issues/514:564,safety,input,input-,564,"Issues on bbknn; Hej all. it seems there is a problem on the batch correction with bbknn. It gives an error at the pca step of bbknn, but I have problem understanding if this is due to the bbknn package itself or the wrapper of scanpy around it, or if it is due to my data, even though it worked when I used it previously. ```python. sc.external.pp.bbknn(all_data_flt, batch_key='batch', n_pcs=15,). ```. gives the error. ```python. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-23-a9dd619ada2e> in <module>. 1 #sc.neighbors.neighbors(all_data_flt, n_neighbors=40, n_pcs=15). ----> 2 sc.external.pp.bbknn(all_data_flt, n_pcs=15). 3 #sc.tools.umap(all_data_flt). ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/preprocessing/_bbknn.py in bbknn(adata, batch_key, save_knn, copy, **kwargs). 82 params = locals(). 83 kwargs = params.pop('kwargs'). ---> 84 return bbknn(**params, **kwargs). ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/bbknn/__init__.py in bbknn(adata, batch_key, save_knn, copy, **kwargs). 215 batch_list = adata.obs[batch_key].values. 216 #call BBKNN proper. --> 217 bbknn_out = bbknn_pca_matrix(pca=pca,batch_list=batch_list,save_knn=save_knn,**kwargs). 218 #optionally save knn_indices. 219 if save_knn:. TypeError: bbknn_pca_matrix() got an unexpected keyword argument 'bbknn'. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/514
https://github.com/scverse/scanpy/issues/514:591,safety,modul,module,591,"Issues on bbknn; Hej all. it seems there is a problem on the batch correction with bbknn. It gives an error at the pca step of bbknn, but I have problem understanding if this is due to the bbknn package itself or the wrapper of scanpy around it, or if it is due to my data, even though it worked when I used it previously. ```python. sc.external.pp.bbknn(all_data_flt, batch_key='batch', n_pcs=15,). ```. gives the error. ```python. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-23-a9dd619ada2e> in <module>. 1 #sc.neighbors.neighbors(all_data_flt, n_neighbors=40, n_pcs=15). ----> 2 sc.external.pp.bbknn(all_data_flt, n_pcs=15). 3 #sc.tools.umap(all_data_flt). ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/preprocessing/_bbknn.py in bbknn(adata, batch_key, save_knn, copy, **kwargs). 82 params = locals(). 83 kwargs = params.pop('kwargs'). ---> 84 return bbknn(**params, **kwargs). ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/bbknn/__init__.py in bbknn(adata, batch_key, save_knn, copy, **kwargs). 215 batch_list = adata.obs[batch_key].values. 216 #call BBKNN proper. --> 217 bbknn_out = bbknn_pca_matrix(pca=pca,batch_list=batch_list,save_knn=save_knn,**kwargs). 218 #optionally save knn_indices. 219 if save_knn:. TypeError: bbknn_pca_matrix() got an unexpected keyword argument 'bbknn'. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/514
https://github.com/scverse/scanpy/issues/514:153,testability,understand,understanding,153,"Issues on bbknn; Hej all. it seems there is a problem on the batch correction with bbknn. It gives an error at the pca step of bbknn, but I have problem understanding if this is due to the bbknn package itself or the wrapper of scanpy around it, or if it is due to my data, even though it worked when I used it previously. ```python. sc.external.pp.bbknn(all_data_flt, batch_key='batch', n_pcs=15,). ```. gives the error. ```python. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-23-a9dd619ada2e> in <module>. 1 #sc.neighbors.neighbors(all_data_flt, n_neighbors=40, n_pcs=15). ----> 2 sc.external.pp.bbknn(all_data_flt, n_pcs=15). 3 #sc.tools.umap(all_data_flt). ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/preprocessing/_bbknn.py in bbknn(adata, batch_key, save_knn, copy, **kwargs). 82 params = locals(). 83 kwargs = params.pop('kwargs'). ---> 84 return bbknn(**params, **kwargs). ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/bbknn/__init__.py in bbknn(adata, batch_key, save_knn, copy, **kwargs). 215 batch_list = adata.obs[batch_key].values. 216 #call BBKNN proper. --> 217 bbknn_out = bbknn_pca_matrix(pca=pca,batch_list=batch_list,save_knn=save_knn,**kwargs). 218 #optionally save knn_indices. 219 if save_knn:. TypeError: bbknn_pca_matrix() got an unexpected keyword argument 'bbknn'. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/514
https://github.com/scverse/scanpy/issues/514:520,testability,Trace,Traceback,520,"Issues on bbknn; Hej all. it seems there is a problem on the batch correction with bbknn. It gives an error at the pca step of bbknn, but I have problem understanding if this is due to the bbknn package itself or the wrapper of scanpy around it, or if it is due to my data, even though it worked when I used it previously. ```python. sc.external.pp.bbknn(all_data_flt, batch_key='batch', n_pcs=15,). ```. gives the error. ```python. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-23-a9dd619ada2e> in <module>. 1 #sc.neighbors.neighbors(all_data_flt, n_neighbors=40, n_pcs=15). ----> 2 sc.external.pp.bbknn(all_data_flt, n_pcs=15). 3 #sc.tools.umap(all_data_flt). ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/preprocessing/_bbknn.py in bbknn(adata, batch_key, save_knn, copy, **kwargs). 82 params = locals(). 83 kwargs = params.pop('kwargs'). ---> 84 return bbknn(**params, **kwargs). ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/bbknn/__init__.py in bbknn(adata, batch_key, save_knn, copy, **kwargs). 215 batch_list = adata.obs[batch_key].values. 216 #call BBKNN proper. --> 217 bbknn_out = bbknn_pca_matrix(pca=pca,batch_list=batch_list,save_knn=save_knn,**kwargs). 218 #optionally save knn_indices. 219 if save_knn:. TypeError: bbknn_pca_matrix() got an unexpected keyword argument 'bbknn'. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/514
https://github.com/scverse/scanpy/issues/514:102,usability,error,error,102,"Issues on bbknn; Hej all. it seems there is a problem on the batch correction with bbknn. It gives an error at the pca step of bbknn, but I have problem understanding if this is due to the bbknn package itself or the wrapper of scanpy around it, or if it is due to my data, even though it worked when I used it previously. ```python. sc.external.pp.bbknn(all_data_flt, batch_key='batch', n_pcs=15,). ```. gives the error. ```python. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-23-a9dd619ada2e> in <module>. 1 #sc.neighbors.neighbors(all_data_flt, n_neighbors=40, n_pcs=15). ----> 2 sc.external.pp.bbknn(all_data_flt, n_pcs=15). 3 #sc.tools.umap(all_data_flt). ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/preprocessing/_bbknn.py in bbknn(adata, batch_key, save_knn, copy, **kwargs). 82 params = locals(). 83 kwargs = params.pop('kwargs'). ---> 84 return bbknn(**params, **kwargs). ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/bbknn/__init__.py in bbknn(adata, batch_key, save_knn, copy, **kwargs). 215 batch_list = adata.obs[batch_key].values. 216 #call BBKNN proper. --> 217 bbknn_out = bbknn_pca_matrix(pca=pca,batch_list=batch_list,save_knn=save_knn,**kwargs). 218 #optionally save knn_indices. 219 if save_knn:. TypeError: bbknn_pca_matrix() got an unexpected keyword argument 'bbknn'. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/514
https://github.com/scverse/scanpy/issues/514:415,usability,error,error,415,"Issues on bbknn; Hej all. it seems there is a problem on the batch correction with bbknn. It gives an error at the pca step of bbknn, but I have problem understanding if this is due to the bbknn package itself or the wrapper of scanpy around it, or if it is due to my data, even though it worked when I used it previously. ```python. sc.external.pp.bbknn(all_data_flt, batch_key='batch', n_pcs=15,). ```. gives the error. ```python. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-23-a9dd619ada2e> in <module>. 1 #sc.neighbors.neighbors(all_data_flt, n_neighbors=40, n_pcs=15). ----> 2 sc.external.pp.bbknn(all_data_flt, n_pcs=15). 3 #sc.tools.umap(all_data_flt). ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/preprocessing/_bbknn.py in bbknn(adata, batch_key, save_knn, copy, **kwargs). 82 params = locals(). 83 kwargs = params.pop('kwargs'). ---> 84 return bbknn(**params, **kwargs). ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/bbknn/__init__.py in bbknn(adata, batch_key, save_knn, copy, **kwargs). 215 batch_list = adata.obs[batch_key].values. 216 #call BBKNN proper. --> 217 bbknn_out = bbknn_pca_matrix(pca=pca,batch_list=batch_list,save_knn=save_knn,**kwargs). 218 #optionally save knn_indices. 219 if save_knn:. TypeError: bbknn_pca_matrix() got an unexpected keyword argument 'bbknn'. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/514
https://github.com/scverse/scanpy/issues/514:564,usability,input,input-,564,"Issues on bbknn; Hej all. it seems there is a problem on the batch correction with bbknn. It gives an error at the pca step of bbknn, but I have problem understanding if this is due to the bbknn package itself or the wrapper of scanpy around it, or if it is due to my data, even though it worked when I used it previously. ```python. sc.external.pp.bbknn(all_data_flt, batch_key='batch', n_pcs=15,). ```. gives the error. ```python. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-23-a9dd619ada2e> in <module>. 1 #sc.neighbors.neighbors(all_data_flt, n_neighbors=40, n_pcs=15). ----> 2 sc.external.pp.bbknn(all_data_flt, n_pcs=15). 3 #sc.tools.umap(all_data_flt). ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/preprocessing/_bbknn.py in bbknn(adata, batch_key, save_knn, copy, **kwargs). 82 params = locals(). 83 kwargs = params.pop('kwargs'). ---> 84 return bbknn(**params, **kwargs). ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/bbknn/__init__.py in bbknn(adata, batch_key, save_knn, copy, **kwargs). 215 batch_list = adata.obs[batch_key].values. 216 #call BBKNN proper. --> 217 bbknn_out = bbknn_pca_matrix(pca=pca,batch_list=batch_list,save_knn=save_knn,**kwargs). 218 #optionally save knn_indices. 219 if save_knn:. TypeError: bbknn_pca_matrix() got an unexpected keyword argument 'bbknn'. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/514
https://github.com/scverse/scanpy/issues/514:727,usability,tool,tools,727,"Issues on bbknn; Hej all. it seems there is a problem on the batch correction with bbknn. It gives an error at the pca step of bbknn, but I have problem understanding if this is due to the bbknn package itself or the wrapper of scanpy around it, or if it is due to my data, even though it worked when I used it previously. ```python. sc.external.pp.bbknn(all_data_flt, batch_key='batch', n_pcs=15,). ```. gives the error. ```python. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-23-a9dd619ada2e> in <module>. 1 #sc.neighbors.neighbors(all_data_flt, n_neighbors=40, n_pcs=15). ----> 2 sc.external.pp.bbknn(all_data_flt, n_pcs=15). 3 #sc.tools.umap(all_data_flt). ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/preprocessing/_bbknn.py in bbknn(adata, batch_key, save_knn, copy, **kwargs). 82 params = locals(). 83 kwargs = params.pop('kwargs'). ---> 84 return bbknn(**params, **kwargs). ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/bbknn/__init__.py in bbknn(adata, batch_key, save_knn, copy, **kwargs). 215 batch_list = adata.obs[batch_key].values. 216 #call BBKNN proper. --> 217 bbknn_out = bbknn_pca_matrix(pca=pca,batch_list=batch_list,save_knn=save_knn,**kwargs). 218 #optionally save knn_indices. 219 if save_knn:. TypeError: bbknn_pca_matrix() got an unexpected keyword argument 'bbknn'. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/514
https://github.com/scverse/scanpy/issues/515:0,availability,Error,Error,0,"Error when writing to h5ad; I encountered the following error when trying to save data to h5ad file:. ```. ... storing 'run' as categorical. ... storing 'batch' as categorical. ... storing 'dis_stat' as categorical. ... storing 'org_day' as categorical. ... storing 'louvain' as categorical. ... storing 'louvain_1' as categorical. ... storing 'louvain_2' as categorical. ... storing 'split_cell_type' as categorical. ... storing 'split_major_cell_type' as categorical. ... storing 'phase' as categorical. ... storing 'split_major_cell_type2' as categorical. ... storing 'feature_types-190111-3' as categorical. ... storing 'feature_types-190111-4' as categorical. ... storing 'feature_types-190111-5' as categorical. ... storing 'feature_types-190111-6' as categorical. ... storing 'feature_types-190111-7' as categorical. ... storing 'feature_types-190111-8' as categorical. ... storing 'feature_types-180418-4' as categorical. ... storing 'feature_types-180418-5' as categorical. ... storing 'feature_types-180418-6' as categorical. ... storing 'feature_types-180418-7' as categorical. ... storing 'feature_types-180905-3' as categorical. ... storing 'feature_types-180905-4' as categorical. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-72-19c7ca58c3a2> in <module>. ----> 1 df_dev.write_h5ad('2019-03-04_OTUD6B_dev_sig.h5'). /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/anndata/base.py in write_h5ad(self, filename, compression, compression_opts, force_dense). 1951 . 1952 _write_h5ad(filename, self, compression=compression,. -> 1953 compression_opts=compression_opts, force_dense=force_dense). 1954 . 1955 if self.isbacked:. /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/anndata/readwrite/write.py in _write_h5ad(filename, adata, force_dense, **kwargs). 217 if not dirname.is_dir():. 218 dirname.mkdir(parents=True, exist_ok=True). --> 219",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/515
https://github.com/scverse/scanpy/issues/515:56,availability,error,error,56,"Error when writing to h5ad; I encountered the following error when trying to save data to h5ad file:. ```. ... storing 'run' as categorical. ... storing 'batch' as categorical. ... storing 'dis_stat' as categorical. ... storing 'org_day' as categorical. ... storing 'louvain' as categorical. ... storing 'louvain_1' as categorical. ... storing 'louvain_2' as categorical. ... storing 'split_cell_type' as categorical. ... storing 'split_major_cell_type' as categorical. ... storing 'phase' as categorical. ... storing 'split_major_cell_type2' as categorical. ... storing 'feature_types-190111-3' as categorical. ... storing 'feature_types-190111-4' as categorical. ... storing 'feature_types-190111-5' as categorical. ... storing 'feature_types-190111-6' as categorical. ... storing 'feature_types-190111-7' as categorical. ... storing 'feature_types-190111-8' as categorical. ... storing 'feature_types-180418-4' as categorical. ... storing 'feature_types-180418-5' as categorical. ... storing 'feature_types-180418-6' as categorical. ... storing 'feature_types-180418-7' as categorical. ... storing 'feature_types-180905-3' as categorical. ... storing 'feature_types-180905-4' as categorical. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-72-19c7ca58c3a2> in <module>. ----> 1 df_dev.write_h5ad('2019-03-04_OTUD6B_dev_sig.h5'). /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/anndata/base.py in write_h5ad(self, filename, compression, compression_opts, force_dense). 1951 . 1952 _write_h5ad(filename, self, compression=compression,. -> 1953 compression_opts=compression_opts, force_dense=force_dense). 1954 . 1955 if self.isbacked:. /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/anndata/readwrite/write.py in _write_h5ad(filename, adata, force_dense, **kwargs). 217 if not dirname.is_dir():. 218 dirname.mkdir(parents=True, exist_ok=True). --> 219",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/515
https://github.com/scverse/scanpy/issues/515:3811,availability,operat,operation,3811,"k]):. /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/pandas/core/generic.py in stat_func(self, axis, skipna, level, numeric_only, **kwargs). 10954 skipna=skipna). 10955 return self._reduce(f, name, axis=axis, skipna=skipna,. > 10956 numeric_only=numeric_only). 10957 . 10958 return set_function_name(stat_func, name, cls). /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/pandas/core/series.py in _reduce(self, op, name, axis, skipna, numeric_only, filter_type, **kwds). 3613 # dispatch to ExtensionArray interface. 3614 if isinstance(delegate, ExtensionArray):. -> 3615 return delegate._reduce(name, skipna=skipna, **kwds). 3616 elif is_datetime64_dtype(delegate):. 3617 # use DatetimeIndex implementation to handle skipna correctly. /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/pandas/core/arrays/categorical.py in _reduce(self, name, axis, skipna, **kwargs). 2179 msg = 'Categorical cannot perform the operation {op}'. 2180 raise TypeError(msg.format(op=name)). -> 2181 return func(**kwargs). 2182 . 2183 def min(self, numeric_only=None, **kwargs):. /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/pandas/core/arrays/categorical.py in max(self, numeric_only, **kwargs). 2222 max : the maximum of this `Categorical`. 2223 """""". -> 2224 self.check_for_ordered('max'). 2225 if numeric_only:. 2226 good = self._codes != -1. /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/pandas/core/arrays/categorical.py in check_for_ordered(self, op). 1517 raise TypeError(""Categorical is not ordered for operation {op}\n"". 1518 ""you can use .as_ordered() to change the "". -> 1519 ""Categorical to an ordered one\n"".format(op=op)). 1520 . 1521 def _values_for_argsort(self):. TypeError: Categorical is not ordered for operation max. you can use .as_ordered() to change the Categorical to an ordered one. ```. I was confused for two reasons:. 1) All of my columns in obs",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/515
https://github.com/scverse/scanpy/issues/515:4453,availability,operat,operation,4453,"7 . 10958 return set_function_name(stat_func, name, cls). /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/pandas/core/series.py in _reduce(self, op, name, axis, skipna, numeric_only, filter_type, **kwds). 3613 # dispatch to ExtensionArray interface. 3614 if isinstance(delegate, ExtensionArray):. -> 3615 return delegate._reduce(name, skipna=skipna, **kwds). 3616 elif is_datetime64_dtype(delegate):. 3617 # use DatetimeIndex implementation to handle skipna correctly. /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/pandas/core/arrays/categorical.py in _reduce(self, name, axis, skipna, **kwargs). 2179 msg = 'Categorical cannot perform the operation {op}'. 2180 raise TypeError(msg.format(op=name)). -> 2181 return func(**kwargs). 2182 . 2183 def min(self, numeric_only=None, **kwargs):. /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/pandas/core/arrays/categorical.py in max(self, numeric_only, **kwargs). 2222 max : the maximum of this `Categorical`. 2223 """""". -> 2224 self.check_for_ordered('max'). 2225 if numeric_only:. 2226 good = self._codes != -1. /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/pandas/core/arrays/categorical.py in check_for_ordered(self, op). 1517 raise TypeError(""Categorical is not ordered for operation {op}\n"". 1518 ""you can use .as_ordered() to change the "". -> 1519 ""Categorical to an ordered one\n"".format(op=op)). 1520 . 1521 def _values_for_argsort(self):. TypeError: Categorical is not ordered for operation max. you can use .as_ordered() to change the Categorical to an ordered one. ```. I was confused for two reasons:. 1) All of my columns in obs are already converted to pandas ordered categorical data but they are still ""forced"" to be converted again into unordered categorical data;. 2) because the columns are now unordered categorical data , it raised the final error which I did not encounter in earlier versions. . Thanks in advance!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/515
https://github.com/scverse/scanpy/issues/515:4665,availability,operat,operation,4665,"7 . 10958 return set_function_name(stat_func, name, cls). /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/pandas/core/series.py in _reduce(self, op, name, axis, skipna, numeric_only, filter_type, **kwds). 3613 # dispatch to ExtensionArray interface. 3614 if isinstance(delegate, ExtensionArray):. -> 3615 return delegate._reduce(name, skipna=skipna, **kwds). 3616 elif is_datetime64_dtype(delegate):. 3617 # use DatetimeIndex implementation to handle skipna correctly. /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/pandas/core/arrays/categorical.py in _reduce(self, name, axis, skipna, **kwargs). 2179 msg = 'Categorical cannot perform the operation {op}'. 2180 raise TypeError(msg.format(op=name)). -> 2181 return func(**kwargs). 2182 . 2183 def min(self, numeric_only=None, **kwargs):. /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/pandas/core/arrays/categorical.py in max(self, numeric_only, **kwargs). 2222 max : the maximum of this `Categorical`. 2223 """""". -> 2224 self.check_for_ordered('max'). 2225 if numeric_only:. 2226 good = self._codes != -1. /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/pandas/core/arrays/categorical.py in check_for_ordered(self, op). 1517 raise TypeError(""Categorical is not ordered for operation {op}\n"". 1518 ""you can use .as_ordered() to change the "". -> 1519 ""Categorical to an ordered one\n"".format(op=op)). 1520 . 1521 def _values_for_argsort(self):. TypeError: Categorical is not ordered for operation max. you can use .as_ordered() to change the Categorical to an ordered one. ```. I was confused for two reasons:. 1) All of my columns in obs are already converted to pandas ordered categorical data but they are still ""forced"" to be converted again into unordered categorical data;. 2) because the columns are now unordered categorical data , it raised the final error which I did not encounter in earlier versions. . Thanks in advance!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/515
https://github.com/scverse/scanpy/issues/515:5038,availability,error,error,5038,"7 . 10958 return set_function_name(stat_func, name, cls). /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/pandas/core/series.py in _reduce(self, op, name, axis, skipna, numeric_only, filter_type, **kwds). 3613 # dispatch to ExtensionArray interface. 3614 if isinstance(delegate, ExtensionArray):. -> 3615 return delegate._reduce(name, skipna=skipna, **kwds). 3616 elif is_datetime64_dtype(delegate):. 3617 # use DatetimeIndex implementation to handle skipna correctly. /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/pandas/core/arrays/categorical.py in _reduce(self, name, axis, skipna, **kwargs). 2179 msg = 'Categorical cannot perform the operation {op}'. 2180 raise TypeError(msg.format(op=name)). -> 2181 return func(**kwargs). 2182 . 2183 def min(self, numeric_only=None, **kwargs):. /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/pandas/core/arrays/categorical.py in max(self, numeric_only, **kwargs). 2222 max : the maximum of this `Categorical`. 2223 """""". -> 2224 self.check_for_ordered('max'). 2225 if numeric_only:. 2226 good = self._codes != -1. /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/pandas/core/arrays/categorical.py in check_for_ordered(self, op). 1517 raise TypeError(""Categorical is not ordered for operation {op}\n"". 1518 ""you can use .as_ordered() to change the "". -> 1519 ""Categorical to an ordered one\n"".format(op=op)). 1520 . 1521 def _values_for_argsort(self):. TypeError: Categorical is not ordered for operation max. you can use .as_ordered() to change the Categorical to an ordered one. ```. I was confused for two reasons:. 1) All of my columns in obs are already converted to pandas ordered categorical data but they are still ""forced"" to be converted again into unordered categorical data;. 2) because the columns are now unordered categorical data , it raised the final error which I did not encounter in earlier versions. . Thanks in advance!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/515
https://github.com/scverse/scanpy/issues/515:1353,deployability,modul,module,1353,"as categorical. ... storing 'split_cell_type' as categorical. ... storing 'split_major_cell_type' as categorical. ... storing 'phase' as categorical. ... storing 'split_major_cell_type2' as categorical. ... storing 'feature_types-190111-3' as categorical. ... storing 'feature_types-190111-4' as categorical. ... storing 'feature_types-190111-5' as categorical. ... storing 'feature_types-190111-6' as categorical. ... storing 'feature_types-190111-7' as categorical. ... storing 'feature_types-190111-8' as categorical. ... storing 'feature_types-180418-4' as categorical. ... storing 'feature_types-180418-5' as categorical. ... storing 'feature_types-180418-6' as categorical. ... storing 'feature_types-180418-7' as categorical. ... storing 'feature_types-180905-3' as categorical. ... storing 'feature_types-180905-4' as categorical. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-72-19c7ca58c3a2> in <module>. ----> 1 df_dev.write_h5ad('2019-03-04_OTUD6B_dev_sig.h5'). /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/anndata/base.py in write_h5ad(self, filename, compression, compression_opts, force_dense). 1951 . 1952 _write_h5ad(filename, self, compression=compression,. -> 1953 compression_opts=compression_opts, force_dense=force_dense). 1954 . 1955 if self.isbacked:. /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/anndata/readwrite/write.py in _write_h5ad(filename, adata, force_dense, **kwargs). 217 if not dirname.is_dir():. 218 dirname.mkdir(parents=True, exist_ok=True). --> 219 d = adata._to_dict_fixed_width_arrays(). 220 # we're writing to a different location than the backing file. 221 # - load the matrix into the memory... /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/anndata/base.py in _to_dict_fixed_width_arrays(self). 2183 """""". 2184 self.strings_to_categoricals(). -> 2185 obs_rec, uns_obs =",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/515
https://github.com/scverse/scanpy/issues/515:5081,deployability,version,versions,5081,"7 . 10958 return set_function_name(stat_func, name, cls). /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/pandas/core/series.py in _reduce(self, op, name, axis, skipna, numeric_only, filter_type, **kwds). 3613 # dispatch to ExtensionArray interface. 3614 if isinstance(delegate, ExtensionArray):. -> 3615 return delegate._reduce(name, skipna=skipna, **kwds). 3616 elif is_datetime64_dtype(delegate):. 3617 # use DatetimeIndex implementation to handle skipna correctly. /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/pandas/core/arrays/categorical.py in _reduce(self, name, axis, skipna, **kwargs). 2179 msg = 'Categorical cannot perform the operation {op}'. 2180 raise TypeError(msg.format(op=name)). -> 2181 return func(**kwargs). 2182 . 2183 def min(self, numeric_only=None, **kwargs):. /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/pandas/core/arrays/categorical.py in max(self, numeric_only, **kwargs). 2222 max : the maximum of this `Categorical`. 2223 """""". -> 2224 self.check_for_ordered('max'). 2225 if numeric_only:. 2226 good = self._codes != -1. /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/pandas/core/arrays/categorical.py in check_for_ordered(self, op). 1517 raise TypeError(""Categorical is not ordered for operation {op}\n"". 1518 ""you can use .as_ordered() to change the "". -> 1519 ""Categorical to an ordered one\n"".format(op=op)). 1520 . 1521 def _values_for_argsort(self):. TypeError: Categorical is not ordered for operation max. you can use .as_ordered() to change the Categorical to an ordered one. ```. I was confused for two reasons:. 1) All of my columns in obs are already converted to pandas ordered categorical data but they are still ""forced"" to be converted again into unordered categorical data;. 2) because the columns are now unordered categorical data , it raised the final error which I did not encounter in earlier versions. . Thanks in advance!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/515
https://github.com/scverse/scanpy/issues/515:2117,energy efficiency,load,load,2117,"905-3' as categorical. ... storing 'feature_types-180905-4' as categorical. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-72-19c7ca58c3a2> in <module>. ----> 1 df_dev.write_h5ad('2019-03-04_OTUD6B_dev_sig.h5'). /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/anndata/base.py in write_h5ad(self, filename, compression, compression_opts, force_dense). 1951 . 1952 _write_h5ad(filename, self, compression=compression,. -> 1953 compression_opts=compression_opts, force_dense=force_dense). 1954 . 1955 if self.isbacked:. /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/anndata/readwrite/write.py in _write_h5ad(filename, adata, force_dense, **kwargs). 217 if not dirname.is_dir():. 218 dirname.mkdir(parents=True, exist_ok=True). --> 219 d = adata._to_dict_fixed_width_arrays(). 220 # we're writing to a different location than the backing file. 221 # - load the matrix into the memory... /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/anndata/base.py in _to_dict_fixed_width_arrays(self). 2183 """""". 2184 self.strings_to_categoricals(). -> 2185 obs_rec, uns_obs = df_to_records_fixed_width(self._obs). 2186 var_rec, uns_var = df_to_records_fixed_width(self._var). 2187 layers = self.layers.as_dict(). /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/anndata/base.py in df_to_records_fixed_width(df). 212 names.append(k). 213 if is_string_dtype(df[k]):. --> 214 max_len_index = df[k].map(len).max(). 215 arrays.append(df[k].values.astype('S{}'.format(max_len_index))). 216 elif is_categorical(df[k]):. /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/pandas/core/generic.py in stat_func(self, axis, skipna, level, numeric_only, **kwargs). 10954 skipna=skipna). 10955 return self._reduce(f, name, axis=axis, skipna=skipna,. > 10956 numeric_only=numeric_only). 10957 . 1095",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/515
https://github.com/scverse/scanpy/issues/515:2906,energy efficiency,core,core,2906,"args). 217 if not dirname.is_dir():. 218 dirname.mkdir(parents=True, exist_ok=True). --> 219 d = adata._to_dict_fixed_width_arrays(). 220 # we're writing to a different location than the backing file. 221 # - load the matrix into the memory... /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/anndata/base.py in _to_dict_fixed_width_arrays(self). 2183 """""". 2184 self.strings_to_categoricals(). -> 2185 obs_rec, uns_obs = df_to_records_fixed_width(self._obs). 2186 var_rec, uns_var = df_to_records_fixed_width(self._var). 2187 layers = self.layers.as_dict(). /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/anndata/base.py in df_to_records_fixed_width(df). 212 names.append(k). 213 if is_string_dtype(df[k]):. --> 214 max_len_index = df[k].map(len).max(). 215 arrays.append(df[k].values.astype('S{}'.format(max_len_index))). 216 elif is_categorical(df[k]):. /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/pandas/core/generic.py in stat_func(self, axis, skipna, level, numeric_only, **kwargs). 10954 skipna=skipna). 10955 return self._reduce(f, name, axis=axis, skipna=skipna,. > 10956 numeric_only=numeric_only). 10957 . 10958 return set_function_name(stat_func, name, cls). /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/pandas/core/series.py in _reduce(self, op, name, axis, skipna, numeric_only, filter_type, **kwds). 3613 # dispatch to ExtensionArray interface. 3614 if isinstance(delegate, ExtensionArray):. -> 3615 return delegate._reduce(name, skipna=skipna, **kwds). 3616 elif is_datetime64_dtype(delegate):. 3617 # use DatetimeIndex implementation to handle skipna correctly. /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/pandas/core/arrays/categorical.py in _reduce(self, name, axis, skipna, **kwargs). 2179 msg = 'Categorical cannot perform the operation {op}'. 2180 raise TypeError(msg.format(op=name)). -> 2181 return func(**kwargs). 2182 .",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/515
https://github.com/scverse/scanpy/issues/515:3253,energy efficiency,core,core,3253,"t_fixed_width_arrays(self). 2183 """""". 2184 self.strings_to_categoricals(). -> 2185 obs_rec, uns_obs = df_to_records_fixed_width(self._obs). 2186 var_rec, uns_var = df_to_records_fixed_width(self._var). 2187 layers = self.layers.as_dict(). /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/anndata/base.py in df_to_records_fixed_width(df). 212 names.append(k). 213 if is_string_dtype(df[k]):. --> 214 max_len_index = df[k].map(len).max(). 215 arrays.append(df[k].values.astype('S{}'.format(max_len_index))). 216 elif is_categorical(df[k]):. /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/pandas/core/generic.py in stat_func(self, axis, skipna, level, numeric_only, **kwargs). 10954 skipna=skipna). 10955 return self._reduce(f, name, axis=axis, skipna=skipna,. > 10956 numeric_only=numeric_only). 10957 . 10958 return set_function_name(stat_func, name, cls). /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/pandas/core/series.py in _reduce(self, op, name, axis, skipna, numeric_only, filter_type, **kwds). 3613 # dispatch to ExtensionArray interface. 3614 if isinstance(delegate, ExtensionArray):. -> 3615 return delegate._reduce(name, skipna=skipna, **kwds). 3616 elif is_datetime64_dtype(delegate):. 3617 # use DatetimeIndex implementation to handle skipna correctly. /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/pandas/core/arrays/categorical.py in _reduce(self, name, axis, skipna, **kwargs). 2179 msg = 'Categorical cannot perform the operation {op}'. 2180 raise TypeError(msg.format(op=name)). -> 2181 return func(**kwargs). 2182 . 2183 def min(self, numeric_only=None, **kwargs):. /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/pandas/core/arrays/categorical.py in max(self, numeric_only, **kwargs). 2222 max : the maximum of this `Categorical`. 2223 """""". -> 2224 self.check_for_ordered('max'). 2225 if numeric_only:. 2226 good = self._codes != -1",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/515
https://github.com/scverse/scanpy/issues/515:3693,energy efficiency,core,core,3693," = df[k].map(len).max(). 215 arrays.append(df[k].values.astype('S{}'.format(max_len_index))). 216 elif is_categorical(df[k]):. /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/pandas/core/generic.py in stat_func(self, axis, skipna, level, numeric_only, **kwargs). 10954 skipna=skipna). 10955 return self._reduce(f, name, axis=axis, skipna=skipna,. > 10956 numeric_only=numeric_only). 10957 . 10958 return set_function_name(stat_func, name, cls). /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/pandas/core/series.py in _reduce(self, op, name, axis, skipna, numeric_only, filter_type, **kwds). 3613 # dispatch to ExtensionArray interface. 3614 if isinstance(delegate, ExtensionArray):. -> 3615 return delegate._reduce(name, skipna=skipna, **kwds). 3616 elif is_datetime64_dtype(delegate):. 3617 # use DatetimeIndex implementation to handle skipna correctly. /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/pandas/core/arrays/categorical.py in _reduce(self, name, axis, skipna, **kwargs). 2179 msg = 'Categorical cannot perform the operation {op}'. 2180 raise TypeError(msg.format(op=name)). -> 2181 return func(**kwargs). 2182 . 2183 def min(self, numeric_only=None, **kwargs):. /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/pandas/core/arrays/categorical.py in max(self, numeric_only, **kwargs). 2222 max : the maximum of this `Categorical`. 2223 """""". -> 2224 self.check_for_ordered('max'). 2225 if numeric_only:. 2226 good = self._codes != -1. /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/pandas/core/arrays/categorical.py in check_for_ordered(self, op). 1517 raise TypeError(""Categorical is not ordered for operation {op}\n"". 1518 ""you can use .as_ordered() to change the "". -> 1519 ""Categorical to an ordered one\n"".format(op=op)). 1520 . 1521 def _values_for_argsort(self):. TypeError: Categorical is not ordered for operation max. you can use .as",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/515
https://github.com/scverse/scanpy/issues/515:4043,energy efficiency,core,core,4043,"xis=axis, skipna=skipna,. > 10956 numeric_only=numeric_only). 10957 . 10958 return set_function_name(stat_func, name, cls). /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/pandas/core/series.py in _reduce(self, op, name, axis, skipna, numeric_only, filter_type, **kwds). 3613 # dispatch to ExtensionArray interface. 3614 if isinstance(delegate, ExtensionArray):. -> 3615 return delegate._reduce(name, skipna=skipna, **kwds). 3616 elif is_datetime64_dtype(delegate):. 3617 # use DatetimeIndex implementation to handle skipna correctly. /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/pandas/core/arrays/categorical.py in _reduce(self, name, axis, skipna, **kwargs). 2179 msg = 'Categorical cannot perform the operation {op}'. 2180 raise TypeError(msg.format(op=name)). -> 2181 return func(**kwargs). 2182 . 2183 def min(self, numeric_only=None, **kwargs):. /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/pandas/core/arrays/categorical.py in max(self, numeric_only, **kwargs). 2222 max : the maximum of this `Categorical`. 2223 """""". -> 2224 self.check_for_ordered('max'). 2225 if numeric_only:. 2226 good = self._codes != -1. /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/pandas/core/arrays/categorical.py in check_for_ordered(self, op). 1517 raise TypeError(""Categorical is not ordered for operation {op}\n"". 1518 ""you can use .as_ordered() to change the "". -> 1519 ""Categorical to an ordered one\n"".format(op=op)). 1520 . 1521 def _values_for_argsort(self):. TypeError: Categorical is not ordered for operation max. you can use .as_ordered() to change the Categorical to an ordered one. ```. I was confused for two reasons:. 1) All of my columns in obs are already converted to pandas ordered categorical data but they are still ""forced"" to be converted again into unordered categorical data;. 2) because the columns are now unordered categorical data , it raised the final error w",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/515
https://github.com/scverse/scanpy/issues/515:4341,energy efficiency,core,core,4341,"7 . 10958 return set_function_name(stat_func, name, cls). /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/pandas/core/series.py in _reduce(self, op, name, axis, skipna, numeric_only, filter_type, **kwds). 3613 # dispatch to ExtensionArray interface. 3614 if isinstance(delegate, ExtensionArray):. -> 3615 return delegate._reduce(name, skipna=skipna, **kwds). 3616 elif is_datetime64_dtype(delegate):. 3617 # use DatetimeIndex implementation to handle skipna correctly. /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/pandas/core/arrays/categorical.py in _reduce(self, name, axis, skipna, **kwargs). 2179 msg = 'Categorical cannot perform the operation {op}'. 2180 raise TypeError(msg.format(op=name)). -> 2181 return func(**kwargs). 2182 . 2183 def min(self, numeric_only=None, **kwargs):. /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/pandas/core/arrays/categorical.py in max(self, numeric_only, **kwargs). 2222 max : the maximum of this `Categorical`. 2223 """""". -> 2224 self.check_for_ordered('max'). 2225 if numeric_only:. 2226 good = self._codes != -1. /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/pandas/core/arrays/categorical.py in check_for_ordered(self, op). 1517 raise TypeError(""Categorical is not ordered for operation {op}\n"". 1518 ""you can use .as_ordered() to change the "". -> 1519 ""Categorical to an ordered one\n"".format(op=op)). 1520 . 1521 def _values_for_argsort(self):. TypeError: Categorical is not ordered for operation max. you can use .as_ordered() to change the Categorical to an ordered one. ```. I was confused for two reasons:. 1) All of my columns in obs are already converted to pandas ordered categorical data but they are still ""forced"" to be converted again into unordered categorical data;. 2) because the columns are now unordered categorical data , it raised the final error which I did not encounter in earlier versions. . Thanks in advance!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/515
https://github.com/scverse/scanpy/issues/515:154,integrability,batch,batch,154,"Error when writing to h5ad; I encountered the following error when trying to save data to h5ad file:. ```. ... storing 'run' as categorical. ... storing 'batch' as categorical. ... storing 'dis_stat' as categorical. ... storing 'org_day' as categorical. ... storing 'louvain' as categorical. ... storing 'louvain_1' as categorical. ... storing 'louvain_2' as categorical. ... storing 'split_cell_type' as categorical. ... storing 'split_major_cell_type' as categorical. ... storing 'phase' as categorical. ... storing 'split_major_cell_type2' as categorical. ... storing 'feature_types-190111-3' as categorical. ... storing 'feature_types-190111-4' as categorical. ... storing 'feature_types-190111-5' as categorical. ... storing 'feature_types-190111-6' as categorical. ... storing 'feature_types-190111-7' as categorical. ... storing 'feature_types-190111-8' as categorical. ... storing 'feature_types-180418-4' as categorical. ... storing 'feature_types-180418-5' as categorical. ... storing 'feature_types-180418-6' as categorical. ... storing 'feature_types-180418-7' as categorical. ... storing 'feature_types-180905-3' as categorical. ... storing 'feature_types-180905-4' as categorical. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-72-19c7ca58c3a2> in <module>. ----> 1 df_dev.write_h5ad('2019-03-04_OTUD6B_dev_sig.h5'). /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/anndata/base.py in write_h5ad(self, filename, compression, compression_opts, force_dense). 1951 . 1952 _write_h5ad(filename, self, compression=compression,. -> 1953 compression_opts=compression_opts, force_dense=force_dense). 1954 . 1955 if self.isbacked:. /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/anndata/readwrite/write.py in _write_h5ad(filename, adata, force_dense, **kwargs). 217 if not dirname.is_dir():. 218 dirname.mkdir(parents=True, exist_ok=True). --> 219",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/515
https://github.com/scverse/scanpy/issues/515:3379,integrability,interfac,interface,3379,"elf._obs). 2186 var_rec, uns_var = df_to_records_fixed_width(self._var). 2187 layers = self.layers.as_dict(). /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/anndata/base.py in df_to_records_fixed_width(df). 212 names.append(k). 213 if is_string_dtype(df[k]):. --> 214 max_len_index = df[k].map(len).max(). 215 arrays.append(df[k].values.astype('S{}'.format(max_len_index))). 216 elif is_categorical(df[k]):. /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/pandas/core/generic.py in stat_func(self, axis, skipna, level, numeric_only, **kwargs). 10954 skipna=skipna). 10955 return self._reduce(f, name, axis=axis, skipna=skipna,. > 10956 numeric_only=numeric_only). 10957 . 10958 return set_function_name(stat_func, name, cls). /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/pandas/core/series.py in _reduce(self, op, name, axis, skipna, numeric_only, filter_type, **kwds). 3613 # dispatch to ExtensionArray interface. 3614 if isinstance(delegate, ExtensionArray):. -> 3615 return delegate._reduce(name, skipna=skipna, **kwds). 3616 elif is_datetime64_dtype(delegate):. 3617 # use DatetimeIndex implementation to handle skipna correctly. /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/pandas/core/arrays/categorical.py in _reduce(self, name, axis, skipna, **kwargs). 2179 msg = 'Categorical cannot perform the operation {op}'. 2180 raise TypeError(msg.format(op=name)). -> 2181 return func(**kwargs). 2182 . 2183 def min(self, numeric_only=None, **kwargs):. /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/pandas/core/arrays/categorical.py in max(self, numeric_only, **kwargs). 2222 max : the maximum of this `Categorical`. 2223 """""". -> 2224 self.check_for_ordered('max'). 2225 if numeric_only:. 2226 good = self._codes != -1. /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/pandas/core/arrays/categorical.py in check_for_ord",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/515
https://github.com/scverse/scanpy/issues/515:5081,integrability,version,versions,5081,"7 . 10958 return set_function_name(stat_func, name, cls). /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/pandas/core/series.py in _reduce(self, op, name, axis, skipna, numeric_only, filter_type, **kwds). 3613 # dispatch to ExtensionArray interface. 3614 if isinstance(delegate, ExtensionArray):. -> 3615 return delegate._reduce(name, skipna=skipna, **kwds). 3616 elif is_datetime64_dtype(delegate):. 3617 # use DatetimeIndex implementation to handle skipna correctly. /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/pandas/core/arrays/categorical.py in _reduce(self, name, axis, skipna, **kwargs). 2179 msg = 'Categorical cannot perform the operation {op}'. 2180 raise TypeError(msg.format(op=name)). -> 2181 return func(**kwargs). 2182 . 2183 def min(self, numeric_only=None, **kwargs):. /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/pandas/core/arrays/categorical.py in max(self, numeric_only, **kwargs). 2222 max : the maximum of this `Categorical`. 2223 """""". -> 2224 self.check_for_ordered('max'). 2225 if numeric_only:. 2226 good = self._codes != -1. /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/pandas/core/arrays/categorical.py in check_for_ordered(self, op). 1517 raise TypeError(""Categorical is not ordered for operation {op}\n"". 1518 ""you can use .as_ordered() to change the "". -> 1519 ""Categorical to an ordered one\n"".format(op=op)). 1520 . 1521 def _values_for_argsort(self):. TypeError: Categorical is not ordered for operation max. you can use .as_ordered() to change the Categorical to an ordered one. ```. I was confused for two reasons:. 1) All of my columns in obs are already converted to pandas ordered categorical data but they are still ""forced"" to be converted again into unordered categorical data;. 2) because the columns are now unordered categorical data , it raised the final error which I did not encounter in earlier versions. . Thanks in advance!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/515
https://github.com/scverse/scanpy/issues/515:2764,interoperability,format,format,2764,"-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/anndata/readwrite/write.py in _write_h5ad(filename, adata, force_dense, **kwargs). 217 if not dirname.is_dir():. 218 dirname.mkdir(parents=True, exist_ok=True). --> 219 d = adata._to_dict_fixed_width_arrays(). 220 # we're writing to a different location than the backing file. 221 # - load the matrix into the memory... /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/anndata/base.py in _to_dict_fixed_width_arrays(self). 2183 """""". 2184 self.strings_to_categoricals(). -> 2185 obs_rec, uns_obs = df_to_records_fixed_width(self._obs). 2186 var_rec, uns_var = df_to_records_fixed_width(self._var). 2187 layers = self.layers.as_dict(). /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/anndata/base.py in df_to_records_fixed_width(df). 212 names.append(k). 213 if is_string_dtype(df[k]):. --> 214 max_len_index = df[k].map(len).max(). 215 arrays.append(df[k].values.astype('S{}'.format(max_len_index))). 216 elif is_categorical(df[k]):. /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/pandas/core/generic.py in stat_func(self, axis, skipna, level, numeric_only, **kwargs). 10954 skipna=skipna). 10955 return self._reduce(f, name, axis=axis, skipna=skipna,. > 10956 numeric_only=numeric_only). 10957 . 10958 return set_function_name(stat_func, name, cls). /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/pandas/core/series.py in _reduce(self, op, name, axis, skipna, numeric_only, filter_type, **kwds). 3613 # dispatch to ExtensionArray interface. 3614 if isinstance(delegate, ExtensionArray):. -> 3615 return delegate._reduce(name, skipna=skipna, **kwds). 3616 elif is_datetime64_dtype(delegate):. 3617 # use DatetimeIndex implementation to handle skipna correctly. /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/pandas/core/arrays/categorical.py in _reduce(self, name, axis, skipna, **kwargs).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/515
https://github.com/scverse/scanpy/issues/515:3379,interoperability,interfac,interface,3379,"elf._obs). 2186 var_rec, uns_var = df_to_records_fixed_width(self._var). 2187 layers = self.layers.as_dict(). /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/anndata/base.py in df_to_records_fixed_width(df). 212 names.append(k). 213 if is_string_dtype(df[k]):. --> 214 max_len_index = df[k].map(len).max(). 215 arrays.append(df[k].values.astype('S{}'.format(max_len_index))). 216 elif is_categorical(df[k]):. /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/pandas/core/generic.py in stat_func(self, axis, skipna, level, numeric_only, **kwargs). 10954 skipna=skipna). 10955 return self._reduce(f, name, axis=axis, skipna=skipna,. > 10956 numeric_only=numeric_only). 10957 . 10958 return set_function_name(stat_func, name, cls). /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/pandas/core/series.py in _reduce(self, op, name, axis, skipna, numeric_only, filter_type, **kwds). 3613 # dispatch to ExtensionArray interface. 3614 if isinstance(delegate, ExtensionArray):. -> 3615 return delegate._reduce(name, skipna=skipna, **kwds). 3616 elif is_datetime64_dtype(delegate):. 3617 # use DatetimeIndex implementation to handle skipna correctly. /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/pandas/core/arrays/categorical.py in _reduce(self, name, axis, skipna, **kwargs). 2179 msg = 'Categorical cannot perform the operation {op}'. 2180 raise TypeError(msg.format(op=name)). -> 2181 return func(**kwargs). 2182 . 2183 def min(self, numeric_only=None, **kwargs):. /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/pandas/core/arrays/categorical.py in max(self, numeric_only, **kwargs). 2222 max : the maximum of this `Categorical`. 2223 """""". -> 2224 self.check_for_ordered('max'). 2225 if numeric_only:. 2226 good = self._codes != -1. /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/pandas/core/arrays/categorical.py in check_for_ord",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/515
https://github.com/scverse/scanpy/issues/515:3853,interoperability,format,format,3853,"/bin/anaconda3/lib/python3.6/site-packages/pandas/core/generic.py in stat_func(self, axis, skipna, level, numeric_only, **kwargs). 10954 skipna=skipna). 10955 return self._reduce(f, name, axis=axis, skipna=skipna,. > 10956 numeric_only=numeric_only). 10957 . 10958 return set_function_name(stat_func, name, cls). /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/pandas/core/series.py in _reduce(self, op, name, axis, skipna, numeric_only, filter_type, **kwds). 3613 # dispatch to ExtensionArray interface. 3614 if isinstance(delegate, ExtensionArray):. -> 3615 return delegate._reduce(name, skipna=skipna, **kwds). 3616 elif is_datetime64_dtype(delegate):. 3617 # use DatetimeIndex implementation to handle skipna correctly. /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/pandas/core/arrays/categorical.py in _reduce(self, name, axis, skipna, **kwargs). 2179 msg = 'Categorical cannot perform the operation {op}'. 2180 raise TypeError(msg.format(op=name)). -> 2181 return func(**kwargs). 2182 . 2183 def min(self, numeric_only=None, **kwargs):. /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/pandas/core/arrays/categorical.py in max(self, numeric_only, **kwargs). 2222 max : the maximum of this `Categorical`. 2223 """""". -> 2224 self.check_for_ordered('max'). 2225 if numeric_only:. 2226 good = self._codes != -1. /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/pandas/core/arrays/categorical.py in check_for_ordered(self, op). 1517 raise TypeError(""Categorical is not ordered for operation {op}\n"". 1518 ""you can use .as_ordered() to change the "". -> 1519 ""Categorical to an ordered one\n"".format(op=op)). 1520 . 1521 def _values_for_argsort(self):. TypeError: Categorical is not ordered for operation max. you can use .as_ordered() to change the Categorical to an ordered one. ```. I was confused for two reasons:. 1) All of my columns in obs are already converted to pandas ordered",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/515
https://github.com/scverse/scanpy/issues/515:4563,interoperability,format,format,4563,"7 . 10958 return set_function_name(stat_func, name, cls). /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/pandas/core/series.py in _reduce(self, op, name, axis, skipna, numeric_only, filter_type, **kwds). 3613 # dispatch to ExtensionArray interface. 3614 if isinstance(delegate, ExtensionArray):. -> 3615 return delegate._reduce(name, skipna=skipna, **kwds). 3616 elif is_datetime64_dtype(delegate):. 3617 # use DatetimeIndex implementation to handle skipna correctly. /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/pandas/core/arrays/categorical.py in _reduce(self, name, axis, skipna, **kwargs). 2179 msg = 'Categorical cannot perform the operation {op}'. 2180 raise TypeError(msg.format(op=name)). -> 2181 return func(**kwargs). 2182 . 2183 def min(self, numeric_only=None, **kwargs):. /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/pandas/core/arrays/categorical.py in max(self, numeric_only, **kwargs). 2222 max : the maximum of this `Categorical`. 2223 """""". -> 2224 self.check_for_ordered('max'). 2225 if numeric_only:. 2226 good = self._codes != -1. /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/pandas/core/arrays/categorical.py in check_for_ordered(self, op). 1517 raise TypeError(""Categorical is not ordered for operation {op}\n"". 1518 ""you can use .as_ordered() to change the "". -> 1519 ""Categorical to an ordered one\n"".format(op=op)). 1520 . 1521 def _values_for_argsort(self):. TypeError: Categorical is not ordered for operation max. you can use .as_ordered() to change the Categorical to an ordered one. ```. I was confused for two reasons:. 1) All of my columns in obs are already converted to pandas ordered categorical data but they are still ""forced"" to be converted again into unordered categorical data;. 2) because the columns are now unordered categorical data , it raised the final error which I did not encounter in earlier versions. . Thanks in advance!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/515
https://github.com/scverse/scanpy/issues/515:1353,modifiability,modul,module,1353,"as categorical. ... storing 'split_cell_type' as categorical. ... storing 'split_major_cell_type' as categorical. ... storing 'phase' as categorical. ... storing 'split_major_cell_type2' as categorical. ... storing 'feature_types-190111-3' as categorical. ... storing 'feature_types-190111-4' as categorical. ... storing 'feature_types-190111-5' as categorical. ... storing 'feature_types-190111-6' as categorical. ... storing 'feature_types-190111-7' as categorical. ... storing 'feature_types-190111-8' as categorical. ... storing 'feature_types-180418-4' as categorical. ... storing 'feature_types-180418-5' as categorical. ... storing 'feature_types-180418-6' as categorical. ... storing 'feature_types-180418-7' as categorical. ... storing 'feature_types-180905-3' as categorical. ... storing 'feature_types-180905-4' as categorical. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-72-19c7ca58c3a2> in <module>. ----> 1 df_dev.write_h5ad('2019-03-04_OTUD6B_dev_sig.h5'). /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/anndata/base.py in write_h5ad(self, filename, compression, compression_opts, force_dense). 1951 . 1952 _write_h5ad(filename, self, compression=compression,. -> 1953 compression_opts=compression_opts, force_dense=force_dense). 1954 . 1955 if self.isbacked:. /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/anndata/readwrite/write.py in _write_h5ad(filename, adata, force_dense, **kwargs). 217 if not dirname.is_dir():. 218 dirname.mkdir(parents=True, exist_ok=True). --> 219 d = adata._to_dict_fixed_width_arrays(). 220 # we're writing to a different location than the backing file. 221 # - load the matrix into the memory... /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/anndata/base.py in _to_dict_fixed_width_arrays(self). 2183 """""". 2184 self.strings_to_categoricals(). -> 2185 obs_rec, uns_obs =",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/515
https://github.com/scverse/scanpy/issues/515:1489,modifiability,pac,packages,1489,"categorical. ... storing 'split_major_cell_type2' as categorical. ... storing 'feature_types-190111-3' as categorical. ... storing 'feature_types-190111-4' as categorical. ... storing 'feature_types-190111-5' as categorical. ... storing 'feature_types-190111-6' as categorical. ... storing 'feature_types-190111-7' as categorical. ... storing 'feature_types-190111-8' as categorical. ... storing 'feature_types-180418-4' as categorical. ... storing 'feature_types-180418-5' as categorical. ... storing 'feature_types-180418-6' as categorical. ... storing 'feature_types-180418-7' as categorical. ... storing 'feature_types-180905-3' as categorical. ... storing 'feature_types-180905-4' as categorical. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-72-19c7ca58c3a2> in <module>. ----> 1 df_dev.write_h5ad('2019-03-04_OTUD6B_dev_sig.h5'). /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/anndata/base.py in write_h5ad(self, filename, compression, compression_opts, force_dense). 1951 . 1952 _write_h5ad(filename, self, compression=compression,. -> 1953 compression_opts=compression_opts, force_dense=force_dense). 1954 . 1955 if self.isbacked:. /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/anndata/readwrite/write.py in _write_h5ad(filename, adata, force_dense, **kwargs). 217 if not dirname.is_dir():. 218 dirname.mkdir(parents=True, exist_ok=True). --> 219 d = adata._to_dict_fixed_width_arrays(). 220 # we're writing to a different location than the backing file. 221 # - load the matrix into the memory... /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/anndata/base.py in _to_dict_fixed_width_arrays(self). 2183 """""". 2184 self.strings_to_categoricals(). -> 2185 obs_rec, uns_obs = df_to_records_fixed_width(self._obs). 2186 var_rec, uns_var = df_to_records_fixed_width(self._var). 2187 layers = self.layers.as_dict().",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/515
https://github.com/scverse/scanpy/issues/515:1823,modifiability,pac,packages,1823," storing 'feature_types-190111-8' as categorical. ... storing 'feature_types-180418-4' as categorical. ... storing 'feature_types-180418-5' as categorical. ... storing 'feature_types-180418-6' as categorical. ... storing 'feature_types-180418-7' as categorical. ... storing 'feature_types-180905-3' as categorical. ... storing 'feature_types-180905-4' as categorical. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-72-19c7ca58c3a2> in <module>. ----> 1 df_dev.write_h5ad('2019-03-04_OTUD6B_dev_sig.h5'). /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/anndata/base.py in write_h5ad(self, filename, compression, compression_opts, force_dense). 1951 . 1952 _write_h5ad(filename, self, compression=compression,. -> 1953 compression_opts=compression_opts, force_dense=force_dense). 1954 . 1955 if self.isbacked:. /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/anndata/readwrite/write.py in _write_h5ad(filename, adata, force_dense, **kwargs). 217 if not dirname.is_dir():. 218 dirname.mkdir(parents=True, exist_ok=True). --> 219 d = adata._to_dict_fixed_width_arrays(). 220 # we're writing to a different location than the backing file. 221 # - load the matrix into the memory... /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/anndata/base.py in _to_dict_fixed_width_arrays(self). 2183 """""". 2184 self.strings_to_categoricals(). -> 2185 obs_rec, uns_obs = df_to_records_fixed_width(self._obs). 2186 var_rec, uns_var = df_to_records_fixed_width(self._var). 2187 layers = self.layers.as_dict(). /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/anndata/base.py in df_to_records_fixed_width(df). 212 names.append(k). 213 if is_string_dtype(df[k]):. --> 214 max_len_index = df[k].map(len).max(). 215 arrays.append(df[k].values.astype('S{}'.format(max_len_index))). 216 elif is_categorical(df[k]):. /proj",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/515
https://github.com/scverse/scanpy/issues/515:2220,modifiability,pac,packages,2220,"----------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-72-19c7ca58c3a2> in <module>. ----> 1 df_dev.write_h5ad('2019-03-04_OTUD6B_dev_sig.h5'). /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/anndata/base.py in write_h5ad(self, filename, compression, compression_opts, force_dense). 1951 . 1952 _write_h5ad(filename, self, compression=compression,. -> 1953 compression_opts=compression_opts, force_dense=force_dense). 1954 . 1955 if self.isbacked:. /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/anndata/readwrite/write.py in _write_h5ad(filename, adata, force_dense, **kwargs). 217 if not dirname.is_dir():. 218 dirname.mkdir(parents=True, exist_ok=True). --> 219 d = adata._to_dict_fixed_width_arrays(). 220 # we're writing to a different location than the backing file. 221 # - load the matrix into the memory... /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/anndata/base.py in _to_dict_fixed_width_arrays(self). 2183 """""". 2184 self.strings_to_categoricals(). -> 2185 obs_rec, uns_obs = df_to_records_fixed_width(self._obs). 2186 var_rec, uns_var = df_to_records_fixed_width(self._var). 2187 layers = self.layers.as_dict(). /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/anndata/base.py in df_to_records_fixed_width(df). 212 names.append(k). 213 if is_string_dtype(df[k]):. --> 214 max_len_index = df[k].map(len).max(). 215 arrays.append(df[k].values.astype('S{}'.format(max_len_index))). 216 elif is_categorical(df[k]):. /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/pandas/core/generic.py in stat_func(self, axis, skipna, level, numeric_only, **kwargs). 10954 skipna=skipna). 10955 return self._reduce(f, name, axis=axis, skipna=skipna,. > 10956 numeric_only=numeric_only). 10957 . 10958 return set_function_name(stat_func, name, cls). /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/py",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/515
https://github.com/scverse/scanpy/issues/515:2462,modifiability,layer,layers,2462,"nda3/lib/python3.6/site-packages/anndata/base.py in write_h5ad(self, filename, compression, compression_opts, force_dense). 1951 . 1952 _write_h5ad(filename, self, compression=compression,. -> 1953 compression_opts=compression_opts, force_dense=force_dense). 1954 . 1955 if self.isbacked:. /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/anndata/readwrite/write.py in _write_h5ad(filename, adata, force_dense, **kwargs). 217 if not dirname.is_dir():. 218 dirname.mkdir(parents=True, exist_ok=True). --> 219 d = adata._to_dict_fixed_width_arrays(). 220 # we're writing to a different location than the backing file. 221 # - load the matrix into the memory... /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/anndata/base.py in _to_dict_fixed_width_arrays(self). 2183 """""". 2184 self.strings_to_categoricals(). -> 2185 obs_rec, uns_obs = df_to_records_fixed_width(self._obs). 2186 var_rec, uns_var = df_to_records_fixed_width(self._var). 2187 layers = self.layers.as_dict(). /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/anndata/base.py in df_to_records_fixed_width(df). 212 names.append(k). 213 if is_string_dtype(df[k]):. --> 214 max_len_index = df[k].map(len).max(). 215 arrays.append(df[k].values.astype('S{}'.format(max_len_index))). 216 elif is_categorical(df[k]):. /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/pandas/core/generic.py in stat_func(self, axis, skipna, level, numeric_only, **kwargs). 10954 skipna=skipna). 10955 return self._reduce(f, name, axis=axis, skipna=skipna,. > 10956 numeric_only=numeric_only). 10957 . 10958 return set_function_name(stat_func, name, cls). /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/pandas/core/series.py in _reduce(self, op, name, axis, skipna, numeric_only, filter_type, **kwds). 3613 # dispatch to ExtensionArray interface. 3614 if isinstance(delegate, ExtensionArray):. -> 3615 return delegate._red",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/515
https://github.com/scverse/scanpy/issues/515:2476,modifiability,layer,layers,2476,"n3.6/site-packages/anndata/base.py in write_h5ad(self, filename, compression, compression_opts, force_dense). 1951 . 1952 _write_h5ad(filename, self, compression=compression,. -> 1953 compression_opts=compression_opts, force_dense=force_dense). 1954 . 1955 if self.isbacked:. /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/anndata/readwrite/write.py in _write_h5ad(filename, adata, force_dense, **kwargs). 217 if not dirname.is_dir():. 218 dirname.mkdir(parents=True, exist_ok=True). --> 219 d = adata._to_dict_fixed_width_arrays(). 220 # we're writing to a different location than the backing file. 221 # - load the matrix into the memory... /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/anndata/base.py in _to_dict_fixed_width_arrays(self). 2183 """""". 2184 self.strings_to_categoricals(). -> 2185 obs_rec, uns_obs = df_to_records_fixed_width(self._obs). 2186 var_rec, uns_var = df_to_records_fixed_width(self._var). 2187 layers = self.layers.as_dict(). /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/anndata/base.py in df_to_records_fixed_width(df). 212 names.append(k). 213 if is_string_dtype(df[k]):. --> 214 max_len_index = df[k].map(len).max(). 215 arrays.append(df[k].values.astype('S{}'.format(max_len_index))). 216 elif is_categorical(df[k]):. /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/pandas/core/generic.py in stat_func(self, axis, skipna, level, numeric_only, **kwargs). 10954 skipna=skipna). 10955 return self._reduce(f, name, axis=axis, skipna=skipna,. > 10956 numeric_only=numeric_only). 10957 . 10958 return set_function_name(stat_func, name, cls). /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/pandas/core/series.py in _reduce(self, op, name, axis, skipna, numeric_only, filter_type, **kwds). 3613 # dispatch to ExtensionArray interface. 3614 if isinstance(delegate, ExtensionArray):. -> 3615 return delegate._reduce(name, skip",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/515
https://github.com/scverse/scanpy/issues/515:2562,modifiability,pac,packages,2562,"on_opts, force_dense). 1951 . 1952 _write_h5ad(filename, self, compression=compression,. -> 1953 compression_opts=compression_opts, force_dense=force_dense). 1954 . 1955 if self.isbacked:. /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/anndata/readwrite/write.py in _write_h5ad(filename, adata, force_dense, **kwargs). 217 if not dirname.is_dir():. 218 dirname.mkdir(parents=True, exist_ok=True). --> 219 d = adata._to_dict_fixed_width_arrays(). 220 # we're writing to a different location than the backing file. 221 # - load the matrix into the memory... /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/anndata/base.py in _to_dict_fixed_width_arrays(self). 2183 """""". 2184 self.strings_to_categoricals(). -> 2185 obs_rec, uns_obs = df_to_records_fixed_width(self._obs). 2186 var_rec, uns_var = df_to_records_fixed_width(self._var). 2187 layers = self.layers.as_dict(). /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/anndata/base.py in df_to_records_fixed_width(df). 212 names.append(k). 213 if is_string_dtype(df[k]):. --> 214 max_len_index = df[k].map(len).max(). 215 arrays.append(df[k].values.astype('S{}'.format(max_len_index))). 216 elif is_categorical(df[k]):. /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/pandas/core/generic.py in stat_func(self, axis, skipna, level, numeric_only, **kwargs). 10954 skipna=skipna). 10955 return self._reduce(f, name, axis=axis, skipna=skipna,. > 10956 numeric_only=numeric_only). 10957 . 10958 return set_function_name(stat_func, name, cls). /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/pandas/core/series.py in _reduce(self, op, name, axis, skipna, numeric_only, filter_type, **kwds). 3613 # dispatch to ExtensionArray interface. 3614 if isinstance(delegate, ExtensionArray):. -> 3615 return delegate._reduce(name, skipna=skipna, **kwds). 3616 elif is_datetime64_dtype(delegate):. 3617 # use DatetimeIndex ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/515
https://github.com/scverse/scanpy/issues/515:2890,modifiability,pac,packages,2890,"ce_dense, **kwargs). 217 if not dirname.is_dir():. 218 dirname.mkdir(parents=True, exist_ok=True). --> 219 d = adata._to_dict_fixed_width_arrays(). 220 # we're writing to a different location than the backing file. 221 # - load the matrix into the memory... /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/anndata/base.py in _to_dict_fixed_width_arrays(self). 2183 """""". 2184 self.strings_to_categoricals(). -> 2185 obs_rec, uns_obs = df_to_records_fixed_width(self._obs). 2186 var_rec, uns_var = df_to_records_fixed_width(self._var). 2187 layers = self.layers.as_dict(). /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/anndata/base.py in df_to_records_fixed_width(df). 212 names.append(k). 213 if is_string_dtype(df[k]):. --> 214 max_len_index = df[k].map(len).max(). 215 arrays.append(df[k].values.astype('S{}'.format(max_len_index))). 216 elif is_categorical(df[k]):. /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/pandas/core/generic.py in stat_func(self, axis, skipna, level, numeric_only, **kwargs). 10954 skipna=skipna). 10955 return self._reduce(f, name, axis=axis, skipna=skipna,. > 10956 numeric_only=numeric_only). 10957 . 10958 return set_function_name(stat_func, name, cls). /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/pandas/core/series.py in _reduce(self, op, name, axis, skipna, numeric_only, filter_type, **kwds). 3613 # dispatch to ExtensionArray interface. 3614 if isinstance(delegate, ExtensionArray):. -> 3615 return delegate._reduce(name, skipna=skipna, **kwds). 3616 elif is_datetime64_dtype(delegate):. 3617 # use DatetimeIndex implementation to handle skipna correctly. /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/pandas/core/arrays/categorical.py in _reduce(self, name, axis, skipna, **kwargs). 2179 msg = 'Categorical cannot perform the operation {op}'. 2180 raise TypeError(msg.format(op=name)). -> 2181 return func(**k",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/515
https://github.com/scverse/scanpy/issues/515:3237,modifiability,pac,packages,3237,".py in _to_dict_fixed_width_arrays(self). 2183 """""". 2184 self.strings_to_categoricals(). -> 2185 obs_rec, uns_obs = df_to_records_fixed_width(self._obs). 2186 var_rec, uns_var = df_to_records_fixed_width(self._var). 2187 layers = self.layers.as_dict(). /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/anndata/base.py in df_to_records_fixed_width(df). 212 names.append(k). 213 if is_string_dtype(df[k]):. --> 214 max_len_index = df[k].map(len).max(). 215 arrays.append(df[k].values.astype('S{}'.format(max_len_index))). 216 elif is_categorical(df[k]):. /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/pandas/core/generic.py in stat_func(self, axis, skipna, level, numeric_only, **kwargs). 10954 skipna=skipna). 10955 return self._reduce(f, name, axis=axis, skipna=skipna,. > 10956 numeric_only=numeric_only). 10957 . 10958 return set_function_name(stat_func, name, cls). /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/pandas/core/series.py in _reduce(self, op, name, axis, skipna, numeric_only, filter_type, **kwds). 3613 # dispatch to ExtensionArray interface. 3614 if isinstance(delegate, ExtensionArray):. -> 3615 return delegate._reduce(name, skipna=skipna, **kwds). 3616 elif is_datetime64_dtype(delegate):. 3617 # use DatetimeIndex implementation to handle skipna correctly. /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/pandas/core/arrays/categorical.py in _reduce(self, name, axis, skipna, **kwargs). 2179 msg = 'Categorical cannot perform the operation {op}'. 2180 raise TypeError(msg.format(op=name)). -> 2181 return func(**kwargs). 2182 . 2183 def min(self, numeric_only=None, **kwargs):. /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/pandas/core/arrays/categorical.py in max(self, numeric_only, **kwargs). 2222 max : the maximum of this `Categorical`. 2223 """""". -> 2224 self.check_for_ordered('max'). 2225 if numeric_only:. 2226 good = sel",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/515
https://github.com/scverse/scanpy/issues/515:3364,modifiability,Extens,ExtensionArray,3364,"fixed_width(self._obs). 2186 var_rec, uns_var = df_to_records_fixed_width(self._var). 2187 layers = self.layers.as_dict(). /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/anndata/base.py in df_to_records_fixed_width(df). 212 names.append(k). 213 if is_string_dtype(df[k]):. --> 214 max_len_index = df[k].map(len).max(). 215 arrays.append(df[k].values.astype('S{}'.format(max_len_index))). 216 elif is_categorical(df[k]):. /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/pandas/core/generic.py in stat_func(self, axis, skipna, level, numeric_only, **kwargs). 10954 skipna=skipna). 10955 return self._reduce(f, name, axis=axis, skipna=skipna,. > 10956 numeric_only=numeric_only). 10957 . 10958 return set_function_name(stat_func, name, cls). /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/pandas/core/series.py in _reduce(self, op, name, axis, skipna, numeric_only, filter_type, **kwds). 3613 # dispatch to ExtensionArray interface. 3614 if isinstance(delegate, ExtensionArray):. -> 3615 return delegate._reduce(name, skipna=skipna, **kwds). 3616 elif is_datetime64_dtype(delegate):. 3617 # use DatetimeIndex implementation to handle skipna correctly. /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/pandas/core/arrays/categorical.py in _reduce(self, name, axis, skipna, **kwargs). 2179 msg = 'Categorical cannot perform the operation {op}'. 2180 raise TypeError(msg.format(op=name)). -> 2181 return func(**kwargs). 2182 . 2183 def min(self, numeric_only=None, **kwargs):. /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/pandas/core/arrays/categorical.py in max(self, numeric_only, **kwargs). 2222 max : the maximum of this `Categorical`. 2223 """""". -> 2224 self.check_for_ordered('max'). 2225 if numeric_only:. 2226 good = self._codes != -1. /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/pandas/core/arrays/categorical.py in ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/515
https://github.com/scverse/scanpy/issues/515:3379,modifiability,interfac,interface,3379,"elf._obs). 2186 var_rec, uns_var = df_to_records_fixed_width(self._var). 2187 layers = self.layers.as_dict(). /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/anndata/base.py in df_to_records_fixed_width(df). 212 names.append(k). 213 if is_string_dtype(df[k]):. --> 214 max_len_index = df[k].map(len).max(). 215 arrays.append(df[k].values.astype('S{}'.format(max_len_index))). 216 elif is_categorical(df[k]):. /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/pandas/core/generic.py in stat_func(self, axis, skipna, level, numeric_only, **kwargs). 10954 skipna=skipna). 10955 return self._reduce(f, name, axis=axis, skipna=skipna,. > 10956 numeric_only=numeric_only). 10957 . 10958 return set_function_name(stat_func, name, cls). /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/pandas/core/series.py in _reduce(self, op, name, axis, skipna, numeric_only, filter_type, **kwds). 3613 # dispatch to ExtensionArray interface. 3614 if isinstance(delegate, ExtensionArray):. -> 3615 return delegate._reduce(name, skipna=skipna, **kwds). 3616 elif is_datetime64_dtype(delegate):. 3617 # use DatetimeIndex implementation to handle skipna correctly. /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/pandas/core/arrays/categorical.py in _reduce(self, name, axis, skipna, **kwargs). 2179 msg = 'Categorical cannot perform the operation {op}'. 2180 raise TypeError(msg.format(op=name)). -> 2181 return func(**kwargs). 2182 . 2183 def min(self, numeric_only=None, **kwargs):. /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/pandas/core/arrays/categorical.py in max(self, numeric_only, **kwargs). 2222 max : the maximum of this `Categorical`. 2223 """""". -> 2224 self.check_for_ordered('max'). 2225 if numeric_only:. 2226 good = self._codes != -1. /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/pandas/core/arrays/categorical.py in check_for_ord",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/515
https://github.com/scverse/scanpy/issues/515:3419,modifiability,Extens,ExtensionArray,3419,"ecords_fixed_width(self._var). 2187 layers = self.layers.as_dict(). /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/anndata/base.py in df_to_records_fixed_width(df). 212 names.append(k). 213 if is_string_dtype(df[k]):. --> 214 max_len_index = df[k].map(len).max(). 215 arrays.append(df[k].values.astype('S{}'.format(max_len_index))). 216 elif is_categorical(df[k]):. /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/pandas/core/generic.py in stat_func(self, axis, skipna, level, numeric_only, **kwargs). 10954 skipna=skipna). 10955 return self._reduce(f, name, axis=axis, skipna=skipna,. > 10956 numeric_only=numeric_only). 10957 . 10958 return set_function_name(stat_func, name, cls). /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/pandas/core/series.py in _reduce(self, op, name, axis, skipna, numeric_only, filter_type, **kwds). 3613 # dispatch to ExtensionArray interface. 3614 if isinstance(delegate, ExtensionArray):. -> 3615 return delegate._reduce(name, skipna=skipna, **kwds). 3616 elif is_datetime64_dtype(delegate):. 3617 # use DatetimeIndex implementation to handle skipna correctly. /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/pandas/core/arrays/categorical.py in _reduce(self, name, axis, skipna, **kwargs). 2179 msg = 'Categorical cannot perform the operation {op}'. 2180 raise TypeError(msg.format(op=name)). -> 2181 return func(**kwargs). 2182 . 2183 def min(self, numeric_only=None, **kwargs):. /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/pandas/core/arrays/categorical.py in max(self, numeric_only, **kwargs). 2222 max : the maximum of this `Categorical`. 2223 """""". -> 2224 self.check_for_ordered('max'). 2225 if numeric_only:. 2226 good = self._codes != -1. /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/pandas/core/arrays/categorical.py in check_for_ordered(self, op). 1517 raise TypeError(""Cate",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/515
https://github.com/scverse/scanpy/issues/515:3677,modifiability,pac,packages,3677," max_len_index = df[k].map(len).max(). 215 arrays.append(df[k].values.astype('S{}'.format(max_len_index))). 216 elif is_categorical(df[k]):. /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/pandas/core/generic.py in stat_func(self, axis, skipna, level, numeric_only, **kwargs). 10954 skipna=skipna). 10955 return self._reduce(f, name, axis=axis, skipna=skipna,. > 10956 numeric_only=numeric_only). 10957 . 10958 return set_function_name(stat_func, name, cls). /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/pandas/core/series.py in _reduce(self, op, name, axis, skipna, numeric_only, filter_type, **kwds). 3613 # dispatch to ExtensionArray interface. 3614 if isinstance(delegate, ExtensionArray):. -> 3615 return delegate._reduce(name, skipna=skipna, **kwds). 3616 elif is_datetime64_dtype(delegate):. 3617 # use DatetimeIndex implementation to handle skipna correctly. /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/pandas/core/arrays/categorical.py in _reduce(self, name, axis, skipna, **kwargs). 2179 msg = 'Categorical cannot perform the operation {op}'. 2180 raise TypeError(msg.format(op=name)). -> 2181 return func(**kwargs). 2182 . 2183 def min(self, numeric_only=None, **kwargs):. /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/pandas/core/arrays/categorical.py in max(self, numeric_only, **kwargs). 2222 max : the maximum of this `Categorical`. 2223 """""". -> 2224 self.check_for_ordered('max'). 2225 if numeric_only:. 2226 good = self._codes != -1. /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/pandas/core/arrays/categorical.py in check_for_ordered(self, op). 1517 raise TypeError(""Categorical is not ordered for operation {op}\n"". 1518 ""you can use .as_ordered() to change the "". -> 1519 ""Categorical to an ordered one\n"".format(op=op)). 1520 . 1521 def _values_for_argsort(self):. TypeError: Categorical is not ordered for operation max. y",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/515
https://github.com/scverse/scanpy/issues/515:4027,modifiability,pac,packages,4027,"uce(f, name, axis=axis, skipna=skipna,. > 10956 numeric_only=numeric_only). 10957 . 10958 return set_function_name(stat_func, name, cls). /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/pandas/core/series.py in _reduce(self, op, name, axis, skipna, numeric_only, filter_type, **kwds). 3613 # dispatch to ExtensionArray interface. 3614 if isinstance(delegate, ExtensionArray):. -> 3615 return delegate._reduce(name, skipna=skipna, **kwds). 3616 elif is_datetime64_dtype(delegate):. 3617 # use DatetimeIndex implementation to handle skipna correctly. /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/pandas/core/arrays/categorical.py in _reduce(self, name, axis, skipna, **kwargs). 2179 msg = 'Categorical cannot perform the operation {op}'. 2180 raise TypeError(msg.format(op=name)). -> 2181 return func(**kwargs). 2182 . 2183 def min(self, numeric_only=None, **kwargs):. /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/pandas/core/arrays/categorical.py in max(self, numeric_only, **kwargs). 2222 max : the maximum of this `Categorical`. 2223 """""". -> 2224 self.check_for_ordered('max'). 2225 if numeric_only:. 2226 good = self._codes != -1. /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/pandas/core/arrays/categorical.py in check_for_ordered(self, op). 1517 raise TypeError(""Categorical is not ordered for operation {op}\n"". 1518 ""you can use .as_ordered() to change the "". -> 1519 ""Categorical to an ordered one\n"".format(op=op)). 1520 . 1521 def _values_for_argsort(self):. TypeError: Categorical is not ordered for operation max. you can use .as_ordered() to change the Categorical to an ordered one. ```. I was confused for two reasons:. 1) All of my columns in obs are already converted to pandas ordered categorical data but they are still ""forced"" to be converted again into unordered categorical data;. 2) because the columns are now unordered categorical data , it raised the",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/515
https://github.com/scverse/scanpy/issues/515:4325,modifiability,pac,packages,4325,"7 . 10958 return set_function_name(stat_func, name, cls). /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/pandas/core/series.py in _reduce(self, op, name, axis, skipna, numeric_only, filter_type, **kwds). 3613 # dispatch to ExtensionArray interface. 3614 if isinstance(delegate, ExtensionArray):. -> 3615 return delegate._reduce(name, skipna=skipna, **kwds). 3616 elif is_datetime64_dtype(delegate):. 3617 # use DatetimeIndex implementation to handle skipna correctly. /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/pandas/core/arrays/categorical.py in _reduce(self, name, axis, skipna, **kwargs). 2179 msg = 'Categorical cannot perform the operation {op}'. 2180 raise TypeError(msg.format(op=name)). -> 2181 return func(**kwargs). 2182 . 2183 def min(self, numeric_only=None, **kwargs):. /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/pandas/core/arrays/categorical.py in max(self, numeric_only, **kwargs). 2222 max : the maximum of this `Categorical`. 2223 """""". -> 2224 self.check_for_ordered('max'). 2225 if numeric_only:. 2226 good = self._codes != -1. /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/pandas/core/arrays/categorical.py in check_for_ordered(self, op). 1517 raise TypeError(""Categorical is not ordered for operation {op}\n"". 1518 ""you can use .as_ordered() to change the "". -> 1519 ""Categorical to an ordered one\n"".format(op=op)). 1520 . 1521 def _values_for_argsort(self):. TypeError: Categorical is not ordered for operation max. you can use .as_ordered() to change the Categorical to an ordered one. ```. I was confused for two reasons:. 1) All of my columns in obs are already converted to pandas ordered categorical data but they are still ""forced"" to be converted again into unordered categorical data;. 2) because the columns are now unordered categorical data , it raised the final error which I did not encounter in earlier versions. . Thanks in advance!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/515
https://github.com/scverse/scanpy/issues/515:5081,modifiability,version,versions,5081,"7 . 10958 return set_function_name(stat_func, name, cls). /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/pandas/core/series.py in _reduce(self, op, name, axis, skipna, numeric_only, filter_type, **kwds). 3613 # dispatch to ExtensionArray interface. 3614 if isinstance(delegate, ExtensionArray):. -> 3615 return delegate._reduce(name, skipna=skipna, **kwds). 3616 elif is_datetime64_dtype(delegate):. 3617 # use DatetimeIndex implementation to handle skipna correctly. /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/pandas/core/arrays/categorical.py in _reduce(self, name, axis, skipna, **kwargs). 2179 msg = 'Categorical cannot perform the operation {op}'. 2180 raise TypeError(msg.format(op=name)). -> 2181 return func(**kwargs). 2182 . 2183 def min(self, numeric_only=None, **kwargs):. /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/pandas/core/arrays/categorical.py in max(self, numeric_only, **kwargs). 2222 max : the maximum of this `Categorical`. 2223 """""". -> 2224 self.check_for_ordered('max'). 2225 if numeric_only:. 2226 good = self._codes != -1. /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/pandas/core/arrays/categorical.py in check_for_ordered(self, op). 1517 raise TypeError(""Categorical is not ordered for operation {op}\n"". 1518 ""you can use .as_ordered() to change the "". -> 1519 ""Categorical to an ordered one\n"".format(op=op)). 1520 . 1521 def _values_for_argsort(self):. TypeError: Categorical is not ordered for operation max. you can use .as_ordered() to change the Categorical to an ordered one. ```. I was confused for two reasons:. 1) All of my columns in obs are already converted to pandas ordered categorical data but they are still ""forced"" to be converted again into unordered categorical data;. 2) because the columns are now unordered categorical data , it raised the final error which I did not encounter in earlier versions. . Thanks in advance!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/515
https://github.com/scverse/scanpy/issues/515:0,performance,Error,Error,0,"Error when writing to h5ad; I encountered the following error when trying to save data to h5ad file:. ```. ... storing 'run' as categorical. ... storing 'batch' as categorical. ... storing 'dis_stat' as categorical. ... storing 'org_day' as categorical. ... storing 'louvain' as categorical. ... storing 'louvain_1' as categorical. ... storing 'louvain_2' as categorical. ... storing 'split_cell_type' as categorical. ... storing 'split_major_cell_type' as categorical. ... storing 'phase' as categorical. ... storing 'split_major_cell_type2' as categorical. ... storing 'feature_types-190111-3' as categorical. ... storing 'feature_types-190111-4' as categorical. ... storing 'feature_types-190111-5' as categorical. ... storing 'feature_types-190111-6' as categorical. ... storing 'feature_types-190111-7' as categorical. ... storing 'feature_types-190111-8' as categorical. ... storing 'feature_types-180418-4' as categorical. ... storing 'feature_types-180418-5' as categorical. ... storing 'feature_types-180418-6' as categorical. ... storing 'feature_types-180418-7' as categorical. ... storing 'feature_types-180905-3' as categorical. ... storing 'feature_types-180905-4' as categorical. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-72-19c7ca58c3a2> in <module>. ----> 1 df_dev.write_h5ad('2019-03-04_OTUD6B_dev_sig.h5'). /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/anndata/base.py in write_h5ad(self, filename, compression, compression_opts, force_dense). 1951 . 1952 _write_h5ad(filename, self, compression=compression,. -> 1953 compression_opts=compression_opts, force_dense=force_dense). 1954 . 1955 if self.isbacked:. /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/anndata/readwrite/write.py in _write_h5ad(filename, adata, force_dense, **kwargs). 217 if not dirname.is_dir():. 218 dirname.mkdir(parents=True, exist_ok=True). --> 219",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/515
https://github.com/scverse/scanpy/issues/515:56,performance,error,error,56,"Error when writing to h5ad; I encountered the following error when trying to save data to h5ad file:. ```. ... storing 'run' as categorical. ... storing 'batch' as categorical. ... storing 'dis_stat' as categorical. ... storing 'org_day' as categorical. ... storing 'louvain' as categorical. ... storing 'louvain_1' as categorical. ... storing 'louvain_2' as categorical. ... storing 'split_cell_type' as categorical. ... storing 'split_major_cell_type' as categorical. ... storing 'phase' as categorical. ... storing 'split_major_cell_type2' as categorical. ... storing 'feature_types-190111-3' as categorical. ... storing 'feature_types-190111-4' as categorical. ... storing 'feature_types-190111-5' as categorical. ... storing 'feature_types-190111-6' as categorical. ... storing 'feature_types-190111-7' as categorical. ... storing 'feature_types-190111-8' as categorical. ... storing 'feature_types-180418-4' as categorical. ... storing 'feature_types-180418-5' as categorical. ... storing 'feature_types-180418-6' as categorical. ... storing 'feature_types-180418-7' as categorical. ... storing 'feature_types-180905-3' as categorical. ... storing 'feature_types-180905-4' as categorical. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-72-19c7ca58c3a2> in <module>. ----> 1 df_dev.write_h5ad('2019-03-04_OTUD6B_dev_sig.h5'). /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/anndata/base.py in write_h5ad(self, filename, compression, compression_opts, force_dense). 1951 . 1952 _write_h5ad(filename, self, compression=compression,. -> 1953 compression_opts=compression_opts, force_dense=force_dense). 1954 . 1955 if self.isbacked:. /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/anndata/readwrite/write.py in _write_h5ad(filename, adata, force_dense, **kwargs). 217 if not dirname.is_dir():. 218 dirname.mkdir(parents=True, exist_ok=True). --> 219",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/515
https://github.com/scverse/scanpy/issues/515:154,performance,batch,batch,154,"Error when writing to h5ad; I encountered the following error when trying to save data to h5ad file:. ```. ... storing 'run' as categorical. ... storing 'batch' as categorical. ... storing 'dis_stat' as categorical. ... storing 'org_day' as categorical. ... storing 'louvain' as categorical. ... storing 'louvain_1' as categorical. ... storing 'louvain_2' as categorical. ... storing 'split_cell_type' as categorical. ... storing 'split_major_cell_type' as categorical. ... storing 'phase' as categorical. ... storing 'split_major_cell_type2' as categorical. ... storing 'feature_types-190111-3' as categorical. ... storing 'feature_types-190111-4' as categorical. ... storing 'feature_types-190111-5' as categorical. ... storing 'feature_types-190111-6' as categorical. ... storing 'feature_types-190111-7' as categorical. ... storing 'feature_types-190111-8' as categorical. ... storing 'feature_types-180418-4' as categorical. ... storing 'feature_types-180418-5' as categorical. ... storing 'feature_types-180418-6' as categorical. ... storing 'feature_types-180418-7' as categorical. ... storing 'feature_types-180905-3' as categorical. ... storing 'feature_types-180905-4' as categorical. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-72-19c7ca58c3a2> in <module>. ----> 1 df_dev.write_h5ad('2019-03-04_OTUD6B_dev_sig.h5'). /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/anndata/base.py in write_h5ad(self, filename, compression, compression_opts, force_dense). 1951 . 1952 _write_h5ad(filename, self, compression=compression,. -> 1953 compression_opts=compression_opts, force_dense=force_dense). 1954 . 1955 if self.isbacked:. /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/anndata/readwrite/write.py in _write_h5ad(filename, adata, force_dense, **kwargs). 217 if not dirname.is_dir():. 218 dirname.mkdir(parents=True, exist_ok=True). --> 219",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/515
https://github.com/scverse/scanpy/issues/515:2117,performance,load,load,2117,"905-3' as categorical. ... storing 'feature_types-180905-4' as categorical. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-72-19c7ca58c3a2> in <module>. ----> 1 df_dev.write_h5ad('2019-03-04_OTUD6B_dev_sig.h5'). /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/anndata/base.py in write_h5ad(self, filename, compression, compression_opts, force_dense). 1951 . 1952 _write_h5ad(filename, self, compression=compression,. -> 1953 compression_opts=compression_opts, force_dense=force_dense). 1954 . 1955 if self.isbacked:. /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/anndata/readwrite/write.py in _write_h5ad(filename, adata, force_dense, **kwargs). 217 if not dirname.is_dir():. 218 dirname.mkdir(parents=True, exist_ok=True). --> 219 d = adata._to_dict_fixed_width_arrays(). 220 # we're writing to a different location than the backing file. 221 # - load the matrix into the memory... /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/anndata/base.py in _to_dict_fixed_width_arrays(self). 2183 """""". 2184 self.strings_to_categoricals(). -> 2185 obs_rec, uns_obs = df_to_records_fixed_width(self._obs). 2186 var_rec, uns_var = df_to_records_fixed_width(self._var). 2187 layers = self.layers.as_dict(). /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/anndata/base.py in df_to_records_fixed_width(df). 212 names.append(k). 213 if is_string_dtype(df[k]):. --> 214 max_len_index = df[k].map(len).max(). 215 arrays.append(df[k].values.astype('S{}'.format(max_len_index))). 216 elif is_categorical(df[k]):. /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/pandas/core/generic.py in stat_func(self, axis, skipna, level, numeric_only, **kwargs). 10954 skipna=skipna). 10955 return self._reduce(f, name, axis=axis, skipna=skipna,. > 10956 numeric_only=numeric_only). 10957 . 1095",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/515
https://github.com/scverse/scanpy/issues/515:2142,performance,memor,memory,2142," storing 'feature_types-180905-4' as categorical. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-72-19c7ca58c3a2> in <module>. ----> 1 df_dev.write_h5ad('2019-03-04_OTUD6B_dev_sig.h5'). /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/anndata/base.py in write_h5ad(self, filename, compression, compression_opts, force_dense). 1951 . 1952 _write_h5ad(filename, self, compression=compression,. -> 1953 compression_opts=compression_opts, force_dense=force_dense). 1954 . 1955 if self.isbacked:. /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/anndata/readwrite/write.py in _write_h5ad(filename, adata, force_dense, **kwargs). 217 if not dirname.is_dir():. 218 dirname.mkdir(parents=True, exist_ok=True). --> 219 d = adata._to_dict_fixed_width_arrays(). 220 # we're writing to a different location than the backing file. 221 # - load the matrix into the memory... /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/anndata/base.py in _to_dict_fixed_width_arrays(self). 2183 """""". 2184 self.strings_to_categoricals(). -> 2185 obs_rec, uns_obs = df_to_records_fixed_width(self._obs). 2186 var_rec, uns_var = df_to_records_fixed_width(self._var). 2187 layers = self.layers.as_dict(). /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/anndata/base.py in df_to_records_fixed_width(df). 212 names.append(k). 213 if is_string_dtype(df[k]):. --> 214 max_len_index = df[k].map(len).max(). 215 arrays.append(df[k].values.astype('S{}'.format(max_len_index))). 216 elif is_categorical(df[k]):. /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/pandas/core/generic.py in stat_func(self, axis, skipna, level, numeric_only, **kwargs). 10954 skipna=skipna). 10955 return self._reduce(f, name, axis=axis, skipna=skipna,. > 10956 numeric_only=numeric_only). 10957 . 10958 return set_function_name",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/515
https://github.com/scverse/scanpy/issues/515:3799,performance,perform,perform,3799,"tegorical(df[k]):. /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/pandas/core/generic.py in stat_func(self, axis, skipna, level, numeric_only, **kwargs). 10954 skipna=skipna). 10955 return self._reduce(f, name, axis=axis, skipna=skipna,. > 10956 numeric_only=numeric_only). 10957 . 10958 return set_function_name(stat_func, name, cls). /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/pandas/core/series.py in _reduce(self, op, name, axis, skipna, numeric_only, filter_type, **kwds). 3613 # dispatch to ExtensionArray interface. 3614 if isinstance(delegate, ExtensionArray):. -> 3615 return delegate._reduce(name, skipna=skipna, **kwds). 3616 elif is_datetime64_dtype(delegate):. 3617 # use DatetimeIndex implementation to handle skipna correctly. /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/pandas/core/arrays/categorical.py in _reduce(self, name, axis, skipna, **kwargs). 2179 msg = 'Categorical cannot perform the operation {op}'. 2180 raise TypeError(msg.format(op=name)). -> 2181 return func(**kwargs). 2182 . 2183 def min(self, numeric_only=None, **kwargs):. /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/pandas/core/arrays/categorical.py in max(self, numeric_only, **kwargs). 2222 max : the maximum of this `Categorical`. 2223 """""". -> 2224 self.check_for_ordered('max'). 2225 if numeric_only:. 2226 good = self._codes != -1. /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/pandas/core/arrays/categorical.py in check_for_ordered(self, op). 1517 raise TypeError(""Categorical is not ordered for operation {op}\n"". 1518 ""you can use .as_ordered() to change the "". -> 1519 ""Categorical to an ordered one\n"".format(op=op)). 1520 . 1521 def _values_for_argsort(self):. TypeError: Categorical is not ordered for operation max. you can use .as_ordered() to change the Categorical to an ordered one. ```. I was confused for two reasons:. 1) All of my c",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/515
https://github.com/scverse/scanpy/issues/515:5038,performance,error,error,5038,"7 . 10958 return set_function_name(stat_func, name, cls). /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/pandas/core/series.py in _reduce(self, op, name, axis, skipna, numeric_only, filter_type, **kwds). 3613 # dispatch to ExtensionArray interface. 3614 if isinstance(delegate, ExtensionArray):. -> 3615 return delegate._reduce(name, skipna=skipna, **kwds). 3616 elif is_datetime64_dtype(delegate):. 3617 # use DatetimeIndex implementation to handle skipna correctly. /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/pandas/core/arrays/categorical.py in _reduce(self, name, axis, skipna, **kwargs). 2179 msg = 'Categorical cannot perform the operation {op}'. 2180 raise TypeError(msg.format(op=name)). -> 2181 return func(**kwargs). 2182 . 2183 def min(self, numeric_only=None, **kwargs):. /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/pandas/core/arrays/categorical.py in max(self, numeric_only, **kwargs). 2222 max : the maximum of this `Categorical`. 2223 """""". -> 2224 self.check_for_ordered('max'). 2225 if numeric_only:. 2226 good = self._codes != -1. /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/pandas/core/arrays/categorical.py in check_for_ordered(self, op). 1517 raise TypeError(""Categorical is not ordered for operation {op}\n"". 1518 ""you can use .as_ordered() to change the "". -> 1519 ""Categorical to an ordered one\n"".format(op=op)). 1520 . 1521 def _values_for_argsort(self):. TypeError: Categorical is not ordered for operation max. you can use .as_ordered() to change the Categorical to an ordered one. ```. I was confused for two reasons:. 1) All of my columns in obs are already converted to pandas ordered categorical data but they are still ""forced"" to be converted again into unordered categorical data;. 2) because the columns are now unordered categorical data , it raised the final error which I did not encounter in earlier versions. . Thanks in advance!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/515
https://github.com/scverse/scanpy/issues/515:0,safety,Error,Error,0,"Error when writing to h5ad; I encountered the following error when trying to save data to h5ad file:. ```. ... storing 'run' as categorical. ... storing 'batch' as categorical. ... storing 'dis_stat' as categorical. ... storing 'org_day' as categorical. ... storing 'louvain' as categorical. ... storing 'louvain_1' as categorical. ... storing 'louvain_2' as categorical. ... storing 'split_cell_type' as categorical. ... storing 'split_major_cell_type' as categorical. ... storing 'phase' as categorical. ... storing 'split_major_cell_type2' as categorical. ... storing 'feature_types-190111-3' as categorical. ... storing 'feature_types-190111-4' as categorical. ... storing 'feature_types-190111-5' as categorical. ... storing 'feature_types-190111-6' as categorical. ... storing 'feature_types-190111-7' as categorical. ... storing 'feature_types-190111-8' as categorical. ... storing 'feature_types-180418-4' as categorical. ... storing 'feature_types-180418-5' as categorical. ... storing 'feature_types-180418-6' as categorical. ... storing 'feature_types-180418-7' as categorical. ... storing 'feature_types-180905-3' as categorical. ... storing 'feature_types-180905-4' as categorical. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-72-19c7ca58c3a2> in <module>. ----> 1 df_dev.write_h5ad('2019-03-04_OTUD6B_dev_sig.h5'). /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/anndata/base.py in write_h5ad(self, filename, compression, compression_opts, force_dense). 1951 . 1952 _write_h5ad(filename, self, compression=compression,. -> 1953 compression_opts=compression_opts, force_dense=force_dense). 1954 . 1955 if self.isbacked:. /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/anndata/readwrite/write.py in _write_h5ad(filename, adata, force_dense, **kwargs). 217 if not dirname.is_dir():. 218 dirname.mkdir(parents=True, exist_ok=True). --> 219",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/515
https://github.com/scverse/scanpy/issues/515:56,safety,error,error,56,"Error when writing to h5ad; I encountered the following error when trying to save data to h5ad file:. ```. ... storing 'run' as categorical. ... storing 'batch' as categorical. ... storing 'dis_stat' as categorical. ... storing 'org_day' as categorical. ... storing 'louvain' as categorical. ... storing 'louvain_1' as categorical. ... storing 'louvain_2' as categorical. ... storing 'split_cell_type' as categorical. ... storing 'split_major_cell_type' as categorical. ... storing 'phase' as categorical. ... storing 'split_major_cell_type2' as categorical. ... storing 'feature_types-190111-3' as categorical. ... storing 'feature_types-190111-4' as categorical. ... storing 'feature_types-190111-5' as categorical. ... storing 'feature_types-190111-6' as categorical. ... storing 'feature_types-190111-7' as categorical. ... storing 'feature_types-190111-8' as categorical. ... storing 'feature_types-180418-4' as categorical. ... storing 'feature_types-180418-5' as categorical. ... storing 'feature_types-180418-6' as categorical. ... storing 'feature_types-180418-7' as categorical. ... storing 'feature_types-180905-3' as categorical. ... storing 'feature_types-180905-4' as categorical. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-72-19c7ca58c3a2> in <module>. ----> 1 df_dev.write_h5ad('2019-03-04_OTUD6B_dev_sig.h5'). /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/anndata/base.py in write_h5ad(self, filename, compression, compression_opts, force_dense). 1951 . 1952 _write_h5ad(filename, self, compression=compression,. -> 1953 compression_opts=compression_opts, force_dense=force_dense). 1954 . 1955 if self.isbacked:. /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/anndata/readwrite/write.py in _write_h5ad(filename, adata, force_dense, **kwargs). 217 if not dirname.is_dir():. 218 dirname.mkdir(parents=True, exist_ok=True). --> 219",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/515
https://github.com/scverse/scanpy/issues/515:1326,safety,input,input-,1326,"l. ... storing 'louvain_2' as categorical. ... storing 'split_cell_type' as categorical. ... storing 'split_major_cell_type' as categorical. ... storing 'phase' as categorical. ... storing 'split_major_cell_type2' as categorical. ... storing 'feature_types-190111-3' as categorical. ... storing 'feature_types-190111-4' as categorical. ... storing 'feature_types-190111-5' as categorical. ... storing 'feature_types-190111-6' as categorical. ... storing 'feature_types-190111-7' as categorical. ... storing 'feature_types-190111-8' as categorical. ... storing 'feature_types-180418-4' as categorical. ... storing 'feature_types-180418-5' as categorical. ... storing 'feature_types-180418-6' as categorical. ... storing 'feature_types-180418-7' as categorical. ... storing 'feature_types-180905-3' as categorical. ... storing 'feature_types-180905-4' as categorical. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-72-19c7ca58c3a2> in <module>. ----> 1 df_dev.write_h5ad('2019-03-04_OTUD6B_dev_sig.h5'). /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/anndata/base.py in write_h5ad(self, filename, compression, compression_opts, force_dense). 1951 . 1952 _write_h5ad(filename, self, compression=compression,. -> 1953 compression_opts=compression_opts, force_dense=force_dense). 1954 . 1955 if self.isbacked:. /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/anndata/readwrite/write.py in _write_h5ad(filename, adata, force_dense, **kwargs). 217 if not dirname.is_dir():. 218 dirname.mkdir(parents=True, exist_ok=True). --> 219 d = adata._to_dict_fixed_width_arrays(). 220 # we're writing to a different location than the backing file. 221 # - load the matrix into the memory... /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/anndata/base.py in _to_dict_fixed_width_arrays(self). 2183 """""". 2184 self.strings_to_categoricals().",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/515
https://github.com/scverse/scanpy/issues/515:1353,safety,modul,module,1353,"as categorical. ... storing 'split_cell_type' as categorical. ... storing 'split_major_cell_type' as categorical. ... storing 'phase' as categorical. ... storing 'split_major_cell_type2' as categorical. ... storing 'feature_types-190111-3' as categorical. ... storing 'feature_types-190111-4' as categorical. ... storing 'feature_types-190111-5' as categorical. ... storing 'feature_types-190111-6' as categorical. ... storing 'feature_types-190111-7' as categorical. ... storing 'feature_types-190111-8' as categorical. ... storing 'feature_types-180418-4' as categorical. ... storing 'feature_types-180418-5' as categorical. ... storing 'feature_types-180418-6' as categorical. ... storing 'feature_types-180418-7' as categorical. ... storing 'feature_types-180905-3' as categorical. ... storing 'feature_types-180905-4' as categorical. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-72-19c7ca58c3a2> in <module>. ----> 1 df_dev.write_h5ad('2019-03-04_OTUD6B_dev_sig.h5'). /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/anndata/base.py in write_h5ad(self, filename, compression, compression_opts, force_dense). 1951 . 1952 _write_h5ad(filename, self, compression=compression,. -> 1953 compression_opts=compression_opts, force_dense=force_dense). 1954 . 1955 if self.isbacked:. /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/anndata/readwrite/write.py in _write_h5ad(filename, adata, force_dense, **kwargs). 217 if not dirname.is_dir():. 218 dirname.mkdir(parents=True, exist_ok=True). --> 219 d = adata._to_dict_fixed_width_arrays(). 220 # we're writing to a different location than the backing file. 221 # - load the matrix into the memory... /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/anndata/base.py in _to_dict_fixed_width_arrays(self). 2183 """""". 2184 self.strings_to_categoricals(). -> 2185 obs_rec, uns_obs =",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/515
https://github.com/scverse/scanpy/issues/515:5038,safety,error,error,5038,"7 . 10958 return set_function_name(stat_func, name, cls). /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/pandas/core/series.py in _reduce(self, op, name, axis, skipna, numeric_only, filter_type, **kwds). 3613 # dispatch to ExtensionArray interface. 3614 if isinstance(delegate, ExtensionArray):. -> 3615 return delegate._reduce(name, skipna=skipna, **kwds). 3616 elif is_datetime64_dtype(delegate):. 3617 # use DatetimeIndex implementation to handle skipna correctly. /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/pandas/core/arrays/categorical.py in _reduce(self, name, axis, skipna, **kwargs). 2179 msg = 'Categorical cannot perform the operation {op}'. 2180 raise TypeError(msg.format(op=name)). -> 2181 return func(**kwargs). 2182 . 2183 def min(self, numeric_only=None, **kwargs):. /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/pandas/core/arrays/categorical.py in max(self, numeric_only, **kwargs). 2222 max : the maximum of this `Categorical`. 2223 """""". -> 2224 self.check_for_ordered('max'). 2225 if numeric_only:. 2226 good = self._codes != -1. /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/pandas/core/arrays/categorical.py in check_for_ordered(self, op). 1517 raise TypeError(""Categorical is not ordered for operation {op}\n"". 1518 ""you can use .as_ordered() to change the "". -> 1519 ""Categorical to an ordered one\n"".format(op=op)). 1520 . 1521 def _values_for_argsort(self):. TypeError: Categorical is not ordered for operation max. you can use .as_ordered() to change the Categorical to an ordered one. ```. I was confused for two reasons:. 1) All of my columns in obs are already converted to pandas ordered categorical data but they are still ""forced"" to be converted again into unordered categorical data;. 2) because the columns are now unordered categorical data , it raised the final error which I did not encounter in earlier versions. . Thanks in advance!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/515
https://github.com/scverse/scanpy/issues/515:1282,testability,Trace,Traceback,1282,"cal. ... storing 'louvain_1' as categorical. ... storing 'louvain_2' as categorical. ... storing 'split_cell_type' as categorical. ... storing 'split_major_cell_type' as categorical. ... storing 'phase' as categorical. ... storing 'split_major_cell_type2' as categorical. ... storing 'feature_types-190111-3' as categorical. ... storing 'feature_types-190111-4' as categorical. ... storing 'feature_types-190111-5' as categorical. ... storing 'feature_types-190111-6' as categorical. ... storing 'feature_types-190111-7' as categorical. ... storing 'feature_types-190111-8' as categorical. ... storing 'feature_types-180418-4' as categorical. ... storing 'feature_types-180418-5' as categorical. ... storing 'feature_types-180418-6' as categorical. ... storing 'feature_types-180418-7' as categorical. ... storing 'feature_types-180905-3' as categorical. ... storing 'feature_types-180905-4' as categorical. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-72-19c7ca58c3a2> in <module>. ----> 1 df_dev.write_h5ad('2019-03-04_OTUD6B_dev_sig.h5'). /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/anndata/base.py in write_h5ad(self, filename, compression, compression_opts, force_dense). 1951 . 1952 _write_h5ad(filename, self, compression=compression,. -> 1953 compression_opts=compression_opts, force_dense=force_dense). 1954 . 1955 if self.isbacked:. /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/anndata/readwrite/write.py in _write_h5ad(filename, adata, force_dense, **kwargs). 217 if not dirname.is_dir():. 218 dirname.mkdir(parents=True, exist_ok=True). --> 219 d = adata._to_dict_fixed_width_arrays(). 220 # we're writing to a different location than the backing file. 221 # - load the matrix into the memory... /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/anndata/base.py in _to_dict_fixed_width_arrays(self). 2183",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/515
https://github.com/scverse/scanpy/issues/515:0,usability,Error,Error,0,"Error when writing to h5ad; I encountered the following error when trying to save data to h5ad file:. ```. ... storing 'run' as categorical. ... storing 'batch' as categorical. ... storing 'dis_stat' as categorical. ... storing 'org_day' as categorical. ... storing 'louvain' as categorical. ... storing 'louvain_1' as categorical. ... storing 'louvain_2' as categorical. ... storing 'split_cell_type' as categorical. ... storing 'split_major_cell_type' as categorical. ... storing 'phase' as categorical. ... storing 'split_major_cell_type2' as categorical. ... storing 'feature_types-190111-3' as categorical. ... storing 'feature_types-190111-4' as categorical. ... storing 'feature_types-190111-5' as categorical. ... storing 'feature_types-190111-6' as categorical. ... storing 'feature_types-190111-7' as categorical. ... storing 'feature_types-190111-8' as categorical. ... storing 'feature_types-180418-4' as categorical. ... storing 'feature_types-180418-5' as categorical. ... storing 'feature_types-180418-6' as categorical. ... storing 'feature_types-180418-7' as categorical. ... storing 'feature_types-180905-3' as categorical. ... storing 'feature_types-180905-4' as categorical. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-72-19c7ca58c3a2> in <module>. ----> 1 df_dev.write_h5ad('2019-03-04_OTUD6B_dev_sig.h5'). /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/anndata/base.py in write_h5ad(self, filename, compression, compression_opts, force_dense). 1951 . 1952 _write_h5ad(filename, self, compression=compression,. -> 1953 compression_opts=compression_opts, force_dense=force_dense). 1954 . 1955 if self.isbacked:. /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/anndata/readwrite/write.py in _write_h5ad(filename, adata, force_dense, **kwargs). 217 if not dirname.is_dir():. 218 dirname.mkdir(parents=True, exist_ok=True). --> 219",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/515
https://github.com/scverse/scanpy/issues/515:56,usability,error,error,56,"Error when writing to h5ad; I encountered the following error when trying to save data to h5ad file:. ```. ... storing 'run' as categorical. ... storing 'batch' as categorical. ... storing 'dis_stat' as categorical. ... storing 'org_day' as categorical. ... storing 'louvain' as categorical. ... storing 'louvain_1' as categorical. ... storing 'louvain_2' as categorical. ... storing 'split_cell_type' as categorical. ... storing 'split_major_cell_type' as categorical. ... storing 'phase' as categorical. ... storing 'split_major_cell_type2' as categorical. ... storing 'feature_types-190111-3' as categorical. ... storing 'feature_types-190111-4' as categorical. ... storing 'feature_types-190111-5' as categorical. ... storing 'feature_types-190111-6' as categorical. ... storing 'feature_types-190111-7' as categorical. ... storing 'feature_types-190111-8' as categorical. ... storing 'feature_types-180418-4' as categorical. ... storing 'feature_types-180418-5' as categorical. ... storing 'feature_types-180418-6' as categorical. ... storing 'feature_types-180418-7' as categorical. ... storing 'feature_types-180905-3' as categorical. ... storing 'feature_types-180905-4' as categorical. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-72-19c7ca58c3a2> in <module>. ----> 1 df_dev.write_h5ad('2019-03-04_OTUD6B_dev_sig.h5'). /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/anndata/base.py in write_h5ad(self, filename, compression, compression_opts, force_dense). 1951 . 1952 _write_h5ad(filename, self, compression=compression,. -> 1953 compression_opts=compression_opts, force_dense=force_dense). 1954 . 1955 if self.isbacked:. /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/anndata/readwrite/write.py in _write_h5ad(filename, adata, force_dense, **kwargs). 217 if not dirname.is_dir():. 218 dirname.mkdir(parents=True, exist_ok=True). --> 219",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/515
https://github.com/scverse/scanpy/issues/515:1326,usability,input,input-,1326,"l. ... storing 'louvain_2' as categorical. ... storing 'split_cell_type' as categorical. ... storing 'split_major_cell_type' as categorical. ... storing 'phase' as categorical. ... storing 'split_major_cell_type2' as categorical. ... storing 'feature_types-190111-3' as categorical. ... storing 'feature_types-190111-4' as categorical. ... storing 'feature_types-190111-5' as categorical. ... storing 'feature_types-190111-6' as categorical. ... storing 'feature_types-190111-7' as categorical. ... storing 'feature_types-190111-8' as categorical. ... storing 'feature_types-180418-4' as categorical. ... storing 'feature_types-180418-5' as categorical. ... storing 'feature_types-180418-6' as categorical. ... storing 'feature_types-180418-7' as categorical. ... storing 'feature_types-180905-3' as categorical. ... storing 'feature_types-180905-4' as categorical. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-72-19c7ca58c3a2> in <module>. ----> 1 df_dev.write_h5ad('2019-03-04_OTUD6B_dev_sig.h5'). /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/anndata/base.py in write_h5ad(self, filename, compression, compression_opts, force_dense). 1951 . 1952 _write_h5ad(filename, self, compression=compression,. -> 1953 compression_opts=compression_opts, force_dense=force_dense). 1954 . 1955 if self.isbacked:. /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/anndata/readwrite/write.py in _write_h5ad(filename, adata, force_dense, **kwargs). 217 if not dirname.is_dir():. 218 dirname.mkdir(parents=True, exist_ok=True). --> 219 d = adata._to_dict_fixed_width_arrays(). 220 # we're writing to a different location than the backing file. 221 # - load the matrix into the memory... /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/anndata/base.py in _to_dict_fixed_width_arrays(self). 2183 """""". 2184 self.strings_to_categoricals().",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/515
https://github.com/scverse/scanpy/issues/515:1446,usability,user,user,1446,"type' as categorical. ... storing 'phase' as categorical. ... storing 'split_major_cell_type2' as categorical. ... storing 'feature_types-190111-3' as categorical. ... storing 'feature_types-190111-4' as categorical. ... storing 'feature_types-190111-5' as categorical. ... storing 'feature_types-190111-6' as categorical. ... storing 'feature_types-190111-7' as categorical. ... storing 'feature_types-190111-8' as categorical. ... storing 'feature_types-180418-4' as categorical. ... storing 'feature_types-180418-5' as categorical. ... storing 'feature_types-180418-6' as categorical. ... storing 'feature_types-180418-7' as categorical. ... storing 'feature_types-180905-3' as categorical. ... storing 'feature_types-180905-4' as categorical. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-72-19c7ca58c3a2> in <module>. ----> 1 df_dev.write_h5ad('2019-03-04_OTUD6B_dev_sig.h5'). /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/anndata/base.py in write_h5ad(self, filename, compression, compression_opts, force_dense). 1951 . 1952 _write_h5ad(filename, self, compression=compression,. -> 1953 compression_opts=compression_opts, force_dense=force_dense). 1954 . 1955 if self.isbacked:. /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/anndata/readwrite/write.py in _write_h5ad(filename, adata, force_dense, **kwargs). 217 if not dirname.is_dir():. 218 dirname.mkdir(parents=True, exist_ok=True). --> 219 d = adata._to_dict_fixed_width_arrays(). 220 # we're writing to a different location than the backing file. 221 # - load the matrix into the memory... /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/anndata/base.py in _to_dict_fixed_width_arrays(self). 2183 """""". 2184 self.strings_to_categoricals(). -> 2185 obs_rec, uns_obs = df_to_records_fixed_width(self._obs). 2186 var_rec, uns_var = df_to_records_fixed_width(sel",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/515
https://github.com/scverse/scanpy/issues/515:1780,usability,user,user,1780," 'feature_types-190111-7' as categorical. ... storing 'feature_types-190111-8' as categorical. ... storing 'feature_types-180418-4' as categorical. ... storing 'feature_types-180418-5' as categorical. ... storing 'feature_types-180418-6' as categorical. ... storing 'feature_types-180418-7' as categorical. ... storing 'feature_types-180905-3' as categorical. ... storing 'feature_types-180905-4' as categorical. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-72-19c7ca58c3a2> in <module>. ----> 1 df_dev.write_h5ad('2019-03-04_OTUD6B_dev_sig.h5'). /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/anndata/base.py in write_h5ad(self, filename, compression, compression_opts, force_dense). 1951 . 1952 _write_h5ad(filename, self, compression=compression,. -> 1953 compression_opts=compression_opts, force_dense=force_dense). 1954 . 1955 if self.isbacked:. /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/anndata/readwrite/write.py in _write_h5ad(filename, adata, force_dense, **kwargs). 217 if not dirname.is_dir():. 218 dirname.mkdir(parents=True, exist_ok=True). --> 219 d = adata._to_dict_fixed_width_arrays(). 220 # we're writing to a different location than the backing file. 221 # - load the matrix into the memory... /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/anndata/base.py in _to_dict_fixed_width_arrays(self). 2183 """""". 2184 self.strings_to_categoricals(). -> 2185 obs_rec, uns_obs = df_to_records_fixed_width(self._obs). 2186 var_rec, uns_var = df_to_records_fixed_width(self._var). 2187 layers = self.layers.as_dict(). /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/anndata/base.py in df_to_records_fixed_width(df). 212 names.append(k). 213 if is_string_dtype(df[k]):. --> 214 max_len_index = df[k].map(len).max(). 215 arrays.append(df[k].values.astype('S{}'.format(max_len_ind",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/515
https://github.com/scverse/scanpy/issues/515:2142,usability,memor,memory,2142," storing 'feature_types-180905-4' as categorical. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-72-19c7ca58c3a2> in <module>. ----> 1 df_dev.write_h5ad('2019-03-04_OTUD6B_dev_sig.h5'). /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/anndata/base.py in write_h5ad(self, filename, compression, compression_opts, force_dense). 1951 . 1952 _write_h5ad(filename, self, compression=compression,. -> 1953 compression_opts=compression_opts, force_dense=force_dense). 1954 . 1955 if self.isbacked:. /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/anndata/readwrite/write.py in _write_h5ad(filename, adata, force_dense, **kwargs). 217 if not dirname.is_dir():. 218 dirname.mkdir(parents=True, exist_ok=True). --> 219 d = adata._to_dict_fixed_width_arrays(). 220 # we're writing to a different location than the backing file. 221 # - load the matrix into the memory... /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/anndata/base.py in _to_dict_fixed_width_arrays(self). 2183 """""". 2184 self.strings_to_categoricals(). -> 2185 obs_rec, uns_obs = df_to_records_fixed_width(self._obs). 2186 var_rec, uns_var = df_to_records_fixed_width(self._var). 2187 layers = self.layers.as_dict(). /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/anndata/base.py in df_to_records_fixed_width(df). 212 names.append(k). 213 if is_string_dtype(df[k]):. --> 214 max_len_index = df[k].map(len).max(). 215 arrays.append(df[k].values.astype('S{}'.format(max_len_index))). 216 elif is_categorical(df[k]):. /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/pandas/core/generic.py in stat_func(self, axis, skipna, level, numeric_only, **kwargs). 10954 skipna=skipna). 10955 return self._reduce(f, name, axis=axis, skipna=skipna,. > 10956 numeric_only=numeric_only). 10957 . 10958 return set_function_name",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/515
https://github.com/scverse/scanpy/issues/515:2177,usability,user,user,2177,"as categorical. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-72-19c7ca58c3a2> in <module>. ----> 1 df_dev.write_h5ad('2019-03-04_OTUD6B_dev_sig.h5'). /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/anndata/base.py in write_h5ad(self, filename, compression, compression_opts, force_dense). 1951 . 1952 _write_h5ad(filename, self, compression=compression,. -> 1953 compression_opts=compression_opts, force_dense=force_dense). 1954 . 1955 if self.isbacked:. /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/anndata/readwrite/write.py in _write_h5ad(filename, adata, force_dense, **kwargs). 217 if not dirname.is_dir():. 218 dirname.mkdir(parents=True, exist_ok=True). --> 219 d = adata._to_dict_fixed_width_arrays(). 220 # we're writing to a different location than the backing file. 221 # - load the matrix into the memory... /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/anndata/base.py in _to_dict_fixed_width_arrays(self). 2183 """""". 2184 self.strings_to_categoricals(). -> 2185 obs_rec, uns_obs = df_to_records_fixed_width(self._obs). 2186 var_rec, uns_var = df_to_records_fixed_width(self._var). 2187 layers = self.layers.as_dict(). /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/anndata/base.py in df_to_records_fixed_width(df). 212 names.append(k). 213 if is_string_dtype(df[k]):. --> 214 max_len_index = df[k].map(len).max(). 215 arrays.append(df[k].values.astype('S{}'.format(max_len_index))). 216 elif is_categorical(df[k]):. /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/pandas/core/generic.py in stat_func(self, axis, skipna, level, numeric_only, **kwargs). 10954 skipna=skipna). 10955 return self._reduce(f, name, axis=axis, skipna=skipna,. > 10956 numeric_only=numeric_only). 10957 . 10958 return set_function_name(stat_func, name, cls). /projects/",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/515
https://github.com/scverse/scanpy/issues/515:2519,usability,user,user,2519,"e_h5ad(self, filename, compression, compression_opts, force_dense). 1951 . 1952 _write_h5ad(filename, self, compression=compression,. -> 1953 compression_opts=compression_opts, force_dense=force_dense). 1954 . 1955 if self.isbacked:. /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/anndata/readwrite/write.py in _write_h5ad(filename, adata, force_dense, **kwargs). 217 if not dirname.is_dir():. 218 dirname.mkdir(parents=True, exist_ok=True). --> 219 d = adata._to_dict_fixed_width_arrays(). 220 # we're writing to a different location than the backing file. 221 # - load the matrix into the memory... /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/anndata/base.py in _to_dict_fixed_width_arrays(self). 2183 """""". 2184 self.strings_to_categoricals(). -> 2185 obs_rec, uns_obs = df_to_records_fixed_width(self._obs). 2186 var_rec, uns_var = df_to_records_fixed_width(self._var). 2187 layers = self.layers.as_dict(). /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/anndata/base.py in df_to_records_fixed_width(df). 212 names.append(k). 213 if is_string_dtype(df[k]):. --> 214 max_len_index = df[k].map(len).max(). 215 arrays.append(df[k].values.astype('S{}'.format(max_len_index))). 216 elif is_categorical(df[k]):. /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/pandas/core/generic.py in stat_func(self, axis, skipna, level, numeric_only, **kwargs). 10954 skipna=skipna). 10955 return self._reduce(f, name, axis=axis, skipna=skipna,. > 10956 numeric_only=numeric_only). 10957 . 10958 return set_function_name(stat_func, name, cls). /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/pandas/core/series.py in _reduce(self, op, name, axis, skipna, numeric_only, filter_type, **kwds). 3613 # dispatch to ExtensionArray interface. 3614 if isinstance(delegate, ExtensionArray):. -> 3615 return delegate._reduce(name, skipna=skipna, **kwds). 3616 elif is_datetime6",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/515
https://github.com/scverse/scanpy/issues/515:2847,usability,user,user,2847,"/write.py in _write_h5ad(filename, adata, force_dense, **kwargs). 217 if not dirname.is_dir():. 218 dirname.mkdir(parents=True, exist_ok=True). --> 219 d = adata._to_dict_fixed_width_arrays(). 220 # we're writing to a different location than the backing file. 221 # - load the matrix into the memory... /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/anndata/base.py in _to_dict_fixed_width_arrays(self). 2183 """""". 2184 self.strings_to_categoricals(). -> 2185 obs_rec, uns_obs = df_to_records_fixed_width(self._obs). 2186 var_rec, uns_var = df_to_records_fixed_width(self._var). 2187 layers = self.layers.as_dict(). /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/anndata/base.py in df_to_records_fixed_width(df). 212 names.append(k). 213 if is_string_dtype(df[k]):. --> 214 max_len_index = df[k].map(len).max(). 215 arrays.append(df[k].values.astype('S{}'.format(max_len_index))). 216 elif is_categorical(df[k]):. /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/pandas/core/generic.py in stat_func(self, axis, skipna, level, numeric_only, **kwargs). 10954 skipna=skipna). 10955 return self._reduce(f, name, axis=axis, skipna=skipna,. > 10956 numeric_only=numeric_only). 10957 . 10958 return set_function_name(stat_func, name, cls). /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/pandas/core/series.py in _reduce(self, op, name, axis, skipna, numeric_only, filter_type, **kwds). 3613 # dispatch to ExtensionArray interface. 3614 if isinstance(delegate, ExtensionArray):. -> 3615 return delegate._reduce(name, skipna=skipna, **kwds). 3616 elif is_datetime64_dtype(delegate):. 3617 # use DatetimeIndex implementation to handle skipna correctly. /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/pandas/core/arrays/categorical.py in _reduce(self, name, axis, skipna, **kwargs). 2179 msg = 'Categorical cannot perform the operation {op}'. 2180 raise TypeError(",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/515
https://github.com/scverse/scanpy/issues/515:3194,usability,user,user,3194,"nda3/lib/python3.6/site-packages/anndata/base.py in _to_dict_fixed_width_arrays(self). 2183 """""". 2184 self.strings_to_categoricals(). -> 2185 obs_rec, uns_obs = df_to_records_fixed_width(self._obs). 2186 var_rec, uns_var = df_to_records_fixed_width(self._var). 2187 layers = self.layers.as_dict(). /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/anndata/base.py in df_to_records_fixed_width(df). 212 names.append(k). 213 if is_string_dtype(df[k]):. --> 214 max_len_index = df[k].map(len).max(). 215 arrays.append(df[k].values.astype('S{}'.format(max_len_index))). 216 elif is_categorical(df[k]):. /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/pandas/core/generic.py in stat_func(self, axis, skipna, level, numeric_only, **kwargs). 10954 skipna=skipna). 10955 return self._reduce(f, name, axis=axis, skipna=skipna,. > 10956 numeric_only=numeric_only). 10957 . 10958 return set_function_name(stat_func, name, cls). /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/pandas/core/series.py in _reduce(self, op, name, axis, skipna, numeric_only, filter_type, **kwds). 3613 # dispatch to ExtensionArray interface. 3614 if isinstance(delegate, ExtensionArray):. -> 3615 return delegate._reduce(name, skipna=skipna, **kwds). 3616 elif is_datetime64_dtype(delegate):. 3617 # use DatetimeIndex implementation to handle skipna correctly. /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/pandas/core/arrays/categorical.py in _reduce(self, name, axis, skipna, **kwargs). 2179 msg = 'Categorical cannot perform the operation {op}'. 2180 raise TypeError(msg.format(op=name)). -> 2181 return func(**kwargs). 2182 . 2183 def min(self, numeric_only=None, **kwargs):. /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/pandas/core/arrays/categorical.py in max(self, numeric_only, **kwargs). 2222 max : the maximum of this `Categorical`. 2223 """""". -> 2224 self.check_for_ordered('",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/515
https://github.com/scverse/scanpy/issues/515:3634,usability,user,user,3634,"d(k). 213 if is_string_dtype(df[k]):. --> 214 max_len_index = df[k].map(len).max(). 215 arrays.append(df[k].values.astype('S{}'.format(max_len_index))). 216 elif is_categorical(df[k]):. /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/pandas/core/generic.py in stat_func(self, axis, skipna, level, numeric_only, **kwargs). 10954 skipna=skipna). 10955 return self._reduce(f, name, axis=axis, skipna=skipna,. > 10956 numeric_only=numeric_only). 10957 . 10958 return set_function_name(stat_func, name, cls). /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/pandas/core/series.py in _reduce(self, op, name, axis, skipna, numeric_only, filter_type, **kwds). 3613 # dispatch to ExtensionArray interface. 3614 if isinstance(delegate, ExtensionArray):. -> 3615 return delegate._reduce(name, skipna=skipna, **kwds). 3616 elif is_datetime64_dtype(delegate):. 3617 # use DatetimeIndex implementation to handle skipna correctly. /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/pandas/core/arrays/categorical.py in _reduce(self, name, axis, skipna, **kwargs). 2179 msg = 'Categorical cannot perform the operation {op}'. 2180 raise TypeError(msg.format(op=name)). -> 2181 return func(**kwargs). 2182 . 2183 def min(self, numeric_only=None, **kwargs):. /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/pandas/core/arrays/categorical.py in max(self, numeric_only, **kwargs). 2222 max : the maximum of this `Categorical`. 2223 """""". -> 2224 self.check_for_ordered('max'). 2225 if numeric_only:. 2226 good = self._codes != -1. /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/pandas/core/arrays/categorical.py in check_for_ordered(self, op). 1517 raise TypeError(""Categorical is not ordered for operation {op}\n"". 1518 ""you can use .as_ordered() to change the "". -> 1519 ""Categorical to an ordered one\n"".format(op=op)). 1520 . 1521 def _values_for_argsort(self):. TypeError: Ca",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/515
https://github.com/scverse/scanpy/issues/515:3799,usability,perform,perform,3799,"tegorical(df[k]):. /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/pandas/core/generic.py in stat_func(self, axis, skipna, level, numeric_only, **kwargs). 10954 skipna=skipna). 10955 return self._reduce(f, name, axis=axis, skipna=skipna,. > 10956 numeric_only=numeric_only). 10957 . 10958 return set_function_name(stat_func, name, cls). /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/pandas/core/series.py in _reduce(self, op, name, axis, skipna, numeric_only, filter_type, **kwds). 3613 # dispatch to ExtensionArray interface. 3614 if isinstance(delegate, ExtensionArray):. -> 3615 return delegate._reduce(name, skipna=skipna, **kwds). 3616 elif is_datetime64_dtype(delegate):. 3617 # use DatetimeIndex implementation to handle skipna correctly. /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/pandas/core/arrays/categorical.py in _reduce(self, name, axis, skipna, **kwargs). 2179 msg = 'Categorical cannot perform the operation {op}'. 2180 raise TypeError(msg.format(op=name)). -> 2181 return func(**kwargs). 2182 . 2183 def min(self, numeric_only=None, **kwargs):. /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/pandas/core/arrays/categorical.py in max(self, numeric_only, **kwargs). 2222 max : the maximum of this `Categorical`. 2223 """""". -> 2224 self.check_for_ordered('max'). 2225 if numeric_only:. 2226 good = self._codes != -1. /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/pandas/core/arrays/categorical.py in check_for_ordered(self, op). 1517 raise TypeError(""Categorical is not ordered for operation {op}\n"". 1518 ""you can use .as_ordered() to change the "". -> 1519 ""Categorical to an ordered one\n"".format(op=op)). 1520 . 1521 def _values_for_argsort(self):. TypeError: Categorical is not ordered for operation max. you can use .as_ordered() to change the Categorical to an ordered one. ```. I was confused for two reasons:. 1) All of my c",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/515
https://github.com/scverse/scanpy/issues/515:3984,usability,user,user,3984," 10954 skipna=skipna). 10955 return self._reduce(f, name, axis=axis, skipna=skipna,. > 10956 numeric_only=numeric_only). 10957 . 10958 return set_function_name(stat_func, name, cls). /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/pandas/core/series.py in _reduce(self, op, name, axis, skipna, numeric_only, filter_type, **kwds). 3613 # dispatch to ExtensionArray interface. 3614 if isinstance(delegate, ExtensionArray):. -> 3615 return delegate._reduce(name, skipna=skipna, **kwds). 3616 elif is_datetime64_dtype(delegate):. 3617 # use DatetimeIndex implementation to handle skipna correctly. /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/pandas/core/arrays/categorical.py in _reduce(self, name, axis, skipna, **kwargs). 2179 msg = 'Categorical cannot perform the operation {op}'. 2180 raise TypeError(msg.format(op=name)). -> 2181 return func(**kwargs). 2182 . 2183 def min(self, numeric_only=None, **kwargs):. /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/pandas/core/arrays/categorical.py in max(self, numeric_only, **kwargs). 2222 max : the maximum of this `Categorical`. 2223 """""". -> 2224 self.check_for_ordered('max'). 2225 if numeric_only:. 2226 good = self._codes != -1. /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/pandas/core/arrays/categorical.py in check_for_ordered(self, op). 1517 raise TypeError(""Categorical is not ordered for operation {op}\n"". 1518 ""you can use .as_ordered() to change the "". -> 1519 ""Categorical to an ordered one\n"".format(op=op)). 1520 . 1521 def _values_for_argsort(self):. TypeError: Categorical is not ordered for operation max. you can use .as_ordered() to change the Categorical to an ordered one. ```. I was confused for two reasons:. 1) All of my columns in obs are already converted to pandas ordered categorical data but they are still ""forced"" to be converted again into unordered categorical data;. 2) because the columns are n",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/515
https://github.com/scverse/scanpy/issues/515:4282,usability,user,user,4282,"7 . 10958 return set_function_name(stat_func, name, cls). /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/pandas/core/series.py in _reduce(self, op, name, axis, skipna, numeric_only, filter_type, **kwds). 3613 # dispatch to ExtensionArray interface. 3614 if isinstance(delegate, ExtensionArray):. -> 3615 return delegate._reduce(name, skipna=skipna, **kwds). 3616 elif is_datetime64_dtype(delegate):. 3617 # use DatetimeIndex implementation to handle skipna correctly. /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/pandas/core/arrays/categorical.py in _reduce(self, name, axis, skipna, **kwargs). 2179 msg = 'Categorical cannot perform the operation {op}'. 2180 raise TypeError(msg.format(op=name)). -> 2181 return func(**kwargs). 2182 . 2183 def min(self, numeric_only=None, **kwargs):. /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/pandas/core/arrays/categorical.py in max(self, numeric_only, **kwargs). 2222 max : the maximum of this `Categorical`. 2223 """""". -> 2224 self.check_for_ordered('max'). 2225 if numeric_only:. 2226 good = self._codes != -1. /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/pandas/core/arrays/categorical.py in check_for_ordered(self, op). 1517 raise TypeError(""Categorical is not ordered for operation {op}\n"". 1518 ""you can use .as_ordered() to change the "". -> 1519 ""Categorical to an ordered one\n"".format(op=op)). 1520 . 1521 def _values_for_argsort(self):. TypeError: Categorical is not ordered for operation max. you can use .as_ordered() to change the Categorical to an ordered one. ```. I was confused for two reasons:. 1) All of my columns in obs are already converted to pandas ordered categorical data but they are still ""forced"" to be converted again into unordered categorical data;. 2) because the columns are now unordered categorical data , it raised the final error which I did not encounter in earlier versions. . Thanks in advance!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/515
https://github.com/scverse/scanpy/issues/515:5038,usability,error,error,5038,"7 . 10958 return set_function_name(stat_func, name, cls). /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/pandas/core/series.py in _reduce(self, op, name, axis, skipna, numeric_only, filter_type, **kwds). 3613 # dispatch to ExtensionArray interface. 3614 if isinstance(delegate, ExtensionArray):. -> 3615 return delegate._reduce(name, skipna=skipna, **kwds). 3616 elif is_datetime64_dtype(delegate):. 3617 # use DatetimeIndex implementation to handle skipna correctly. /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/pandas/core/arrays/categorical.py in _reduce(self, name, axis, skipna, **kwargs). 2179 msg = 'Categorical cannot perform the operation {op}'. 2180 raise TypeError(msg.format(op=name)). -> 2181 return func(**kwargs). 2182 . 2183 def min(self, numeric_only=None, **kwargs):. /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/pandas/core/arrays/categorical.py in max(self, numeric_only, **kwargs). 2222 max : the maximum of this `Categorical`. 2223 """""". -> 2224 self.check_for_ordered('max'). 2225 if numeric_only:. 2226 good = self._codes != -1. /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/pandas/core/arrays/categorical.py in check_for_ordered(self, op). 1517 raise TypeError(""Categorical is not ordered for operation {op}\n"". 1518 ""you can use .as_ordered() to change the "". -> 1519 ""Categorical to an ordered one\n"".format(op=op)). 1520 . 1521 def _values_for_argsort(self):. TypeError: Categorical is not ordered for operation max. you can use .as_ordered() to change the Categorical to an ordered one. ```. I was confused for two reasons:. 1) All of my columns in obs are already converted to pandas ordered categorical data but they are still ""forced"" to be converted again into unordered categorical data;. 2) because the columns are now unordered categorical data , it raised the final error which I did not encounter in earlier versions. . Thanks in advance!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/515
https://github.com/scverse/scanpy/issues/516:293,availability,cluster,clustering,293,"Custom annotations and correlation matrix; Probably quite trivial to most, but was hoping someone could help out with how to create custom annotations? I'm interested in creating annotations specific for cell types that I'd define through a few marker genes, (this being aside from the leiden clustering already performed). I'd then be interested in running such an annotation against a list of genes and creating a correlation matrix, similar to what @fidelram showed with his correlation matrix in ""dendrograms, correlation and marker genes filtering #425"". Any help is much appreciated, thanks before hand!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/516
https://github.com/scverse/scanpy/issues/516:293,deployability,cluster,clustering,293,"Custom annotations and correlation matrix; Probably quite trivial to most, but was hoping someone could help out with how to create custom annotations? I'm interested in creating annotations specific for cell types that I'd define through a few marker genes, (this being aside from the leiden clustering already performed). I'd then be interested in running such an annotation against a list of genes and creating a correlation matrix, similar to what @fidelram showed with his correlation matrix in ""dendrograms, correlation and marker genes filtering #425"". Any help is much appreciated, thanks before hand!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/516
https://github.com/scverse/scanpy/issues/516:543,integrability,filter,filtering,543,"Custom annotations and correlation matrix; Probably quite trivial to most, but was hoping someone could help out with how to create custom annotations? I'm interested in creating annotations specific for cell types that I'd define through a few marker genes, (this being aside from the leiden clustering already performed). I'd then be interested in running such an annotation against a list of genes and creating a correlation matrix, similar to what @fidelram showed with his correlation matrix in ""dendrograms, correlation and marker genes filtering #425"". Any help is much appreciated, thanks before hand!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/516
https://github.com/scverse/scanpy/issues/516:191,interoperability,specif,specific,191,"Custom annotations and correlation matrix; Probably quite trivial to most, but was hoping someone could help out with how to create custom annotations? I'm interested in creating annotations specific for cell types that I'd define through a few marker genes, (this being aside from the leiden clustering already performed). I'd then be interested in running such an annotation against a list of genes and creating a correlation matrix, similar to what @fidelram showed with his correlation matrix in ""dendrograms, correlation and marker genes filtering #425"". Any help is much appreciated, thanks before hand!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/516
https://github.com/scverse/scanpy/issues/516:312,performance,perform,performed,312,"Custom annotations and correlation matrix; Probably quite trivial to most, but was hoping someone could help out with how to create custom annotations? I'm interested in creating annotations specific for cell types that I'd define through a few marker genes, (this being aside from the leiden clustering already performed). I'd then be interested in running such an annotation against a list of genes and creating a correlation matrix, similar to what @fidelram showed with his correlation matrix in ""dendrograms, correlation and marker genes filtering #425"". Any help is much appreciated, thanks before hand!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/516
https://github.com/scverse/scanpy/issues/516:0,usability,Custom,Custom,0,"Custom annotations and correlation matrix; Probably quite trivial to most, but was hoping someone could help out with how to create custom annotations? I'm interested in creating annotations specific for cell types that I'd define through a few marker genes, (this being aside from the leiden clustering already performed). I'd then be interested in running such an annotation against a list of genes and creating a correlation matrix, similar to what @fidelram showed with his correlation matrix in ""dendrograms, correlation and marker genes filtering #425"". Any help is much appreciated, thanks before hand!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/516
https://github.com/scverse/scanpy/issues/516:104,usability,help,help,104,"Custom annotations and correlation matrix; Probably quite trivial to most, but was hoping someone could help out with how to create custom annotations? I'm interested in creating annotations specific for cell types that I'd define through a few marker genes, (this being aside from the leiden clustering already performed). I'd then be interested in running such an annotation against a list of genes and creating a correlation matrix, similar to what @fidelram showed with his correlation matrix in ""dendrograms, correlation and marker genes filtering #425"". Any help is much appreciated, thanks before hand!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/516
https://github.com/scverse/scanpy/issues/516:132,usability,custom,custom,132,"Custom annotations and correlation matrix; Probably quite trivial to most, but was hoping someone could help out with how to create custom annotations? I'm interested in creating annotations specific for cell types that I'd define through a few marker genes, (this being aside from the leiden clustering already performed). I'd then be interested in running such an annotation against a list of genes and creating a correlation matrix, similar to what @fidelram showed with his correlation matrix in ""dendrograms, correlation and marker genes filtering #425"". Any help is much appreciated, thanks before hand!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/516
https://github.com/scverse/scanpy/issues/516:312,usability,perform,performed,312,"Custom annotations and correlation matrix; Probably quite trivial to most, but was hoping someone could help out with how to create custom annotations? I'm interested in creating annotations specific for cell types that I'd define through a few marker genes, (this being aside from the leiden clustering already performed). I'd then be interested in running such an annotation against a list of genes and creating a correlation matrix, similar to what @fidelram showed with his correlation matrix in ""dendrograms, correlation and marker genes filtering #425"". Any help is much appreciated, thanks before hand!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/516
https://github.com/scverse/scanpy/issues/516:564,usability,help,help,564,"Custom annotations and correlation matrix; Probably quite trivial to most, but was hoping someone could help out with how to create custom annotations? I'm interested in creating annotations specific for cell types that I'd define through a few marker genes, (this being aside from the leiden clustering already performed). I'd then be interested in running such an annotation against a list of genes and creating a correlation matrix, similar to what @fidelram showed with his correlation matrix in ""dendrograms, correlation and marker genes filtering #425"". Any help is much appreciated, thanks before hand!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/516
https://github.com/scverse/scanpy/issues/517:59,deployability,log,log-transformed,59,"Log2 fold changes in rank_genes_groups are calculated from log-transformed data; Hi all. I was looking through the `_rank_genes_groups` function and noticed that the fold-change calculations are based on the means calculated by `_get_mean_var`. The only problem with this is that (usually) the expression values at this point in the analysis are in log scale, so we are calculating the fold-changes of the log1p count values, and then further log2 transforming these fold changes. I know that different programs do it differently, but I think it's more intuitive to convert the matrix back to counts, calculate the fold change, and then report the log2 fold change. Any thoughts? For the actual differential testing, I think it's ok to run the tests on the log1p transformed data, as that seems to be the norm for many pipelines using the types of tests we are using. However, some pipelines do use raw count data, which might be interesting to implement if we want. Either way, I think it's a little unintuitive to report a log2 fold change of log expression values. I can submit a pull request to implement this if this is something you agree with, and can add a parameter to let the user decide whether to use log-transformed or raw-count data. Let me know what you think!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/517
https://github.com/scverse/scanpy/issues/517:349,deployability,log,log,349,"Log2 fold changes in rank_genes_groups are calculated from log-transformed data; Hi all. I was looking through the `_rank_genes_groups` function and noticed that the fold-change calculations are based on the means calculated by `_get_mean_var`. The only problem with this is that (usually) the expression values at this point in the analysis are in log scale, so we are calculating the fold-changes of the log1p count values, and then further log2 transforming these fold changes. I know that different programs do it differently, but I think it's more intuitive to convert the matrix back to counts, calculate the fold change, and then report the log2 fold change. Any thoughts? For the actual differential testing, I think it's ok to run the tests on the log1p transformed data, as that seems to be the norm for many pipelines using the types of tests we are using. However, some pipelines do use raw count data, which might be interesting to implement if we want. Either way, I think it's a little unintuitive to report a log2 fold change of log expression values. I can submit a pull request to implement this if this is something you agree with, and can add a parameter to let the user decide whether to use log-transformed or raw-count data. Let me know what you think!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/517
https://github.com/scverse/scanpy/issues/517:353,deployability,scale,scale,353,"Log2 fold changes in rank_genes_groups are calculated from log-transformed data; Hi all. I was looking through the `_rank_genes_groups` function and noticed that the fold-change calculations are based on the means calculated by `_get_mean_var`. The only problem with this is that (usually) the expression values at this point in the analysis are in log scale, so we are calculating the fold-changes of the log1p count values, and then further log2 transforming these fold changes. I know that different programs do it differently, but I think it's more intuitive to convert the matrix back to counts, calculate the fold change, and then report the log2 fold change. Any thoughts? For the actual differential testing, I think it's ok to run the tests on the log1p transformed data, as that seems to be the norm for many pipelines using the types of tests we are using. However, some pipelines do use raw count data, which might be interesting to implement if we want. Either way, I think it's a little unintuitive to report a log2 fold change of log expression values. I can submit a pull request to implement this if this is something you agree with, and can add a parameter to let the user decide whether to use log-transformed or raw-count data. Let me know what you think!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/517
https://github.com/scverse/scanpy/issues/517:819,deployability,pipelin,pipelines,819,"Log2 fold changes in rank_genes_groups are calculated from log-transformed data; Hi all. I was looking through the `_rank_genes_groups` function and noticed that the fold-change calculations are based on the means calculated by `_get_mean_var`. The only problem with this is that (usually) the expression values at this point in the analysis are in log scale, so we are calculating the fold-changes of the log1p count values, and then further log2 transforming these fold changes. I know that different programs do it differently, but I think it's more intuitive to convert the matrix back to counts, calculate the fold change, and then report the log2 fold change. Any thoughts? For the actual differential testing, I think it's ok to run the tests on the log1p transformed data, as that seems to be the norm for many pipelines using the types of tests we are using. However, some pipelines do use raw count data, which might be interesting to implement if we want. Either way, I think it's a little unintuitive to report a log2 fold change of log expression values. I can submit a pull request to implement this if this is something you agree with, and can add a parameter to let the user decide whether to use log-transformed or raw-count data. Let me know what you think!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/517
https://github.com/scverse/scanpy/issues/517:882,deployability,pipelin,pipelines,882,"Log2 fold changes in rank_genes_groups are calculated from log-transformed data; Hi all. I was looking through the `_rank_genes_groups` function and noticed that the fold-change calculations are based on the means calculated by `_get_mean_var`. The only problem with this is that (usually) the expression values at this point in the analysis are in log scale, so we are calculating the fold-changes of the log1p count values, and then further log2 transforming these fold changes. I know that different programs do it differently, but I think it's more intuitive to convert the matrix back to counts, calculate the fold change, and then report the log2 fold change. Any thoughts? For the actual differential testing, I think it's ok to run the tests on the log1p transformed data, as that seems to be the norm for many pipelines using the types of tests we are using. However, some pipelines do use raw count data, which might be interesting to implement if we want. Either way, I think it's a little unintuitive to report a log2 fold change of log expression values. I can submit a pull request to implement this if this is something you agree with, and can add a parameter to let the user decide whether to use log-transformed or raw-count data. Let me know what you think!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/517
https://github.com/scverse/scanpy/issues/517:1045,deployability,log,log,1045,"Log2 fold changes in rank_genes_groups are calculated from log-transformed data; Hi all. I was looking through the `_rank_genes_groups` function and noticed that the fold-change calculations are based on the means calculated by `_get_mean_var`. The only problem with this is that (usually) the expression values at this point in the analysis are in log scale, so we are calculating the fold-changes of the log1p count values, and then further log2 transforming these fold changes. I know that different programs do it differently, but I think it's more intuitive to convert the matrix back to counts, calculate the fold change, and then report the log2 fold change. Any thoughts? For the actual differential testing, I think it's ok to run the tests on the log1p transformed data, as that seems to be the norm for many pipelines using the types of tests we are using. However, some pipelines do use raw count data, which might be interesting to implement if we want. Either way, I think it's a little unintuitive to report a log2 fold change of log expression values. I can submit a pull request to implement this if this is something you agree with, and can add a parameter to let the user decide whether to use log-transformed or raw-count data. Let me know what you think!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/517
https://github.com/scverse/scanpy/issues/517:1213,deployability,log,log-transformed,1213,"Log2 fold changes in rank_genes_groups are calculated from log-transformed data; Hi all. I was looking through the `_rank_genes_groups` function and noticed that the fold-change calculations are based on the means calculated by `_get_mean_var`. The only problem with this is that (usually) the expression values at this point in the analysis are in log scale, so we are calculating the fold-changes of the log1p count values, and then further log2 transforming these fold changes. I know that different programs do it differently, but I think it's more intuitive to convert the matrix back to counts, calculate the fold change, and then report the log2 fold change. Any thoughts? For the actual differential testing, I think it's ok to run the tests on the log1p transformed data, as that seems to be the norm for many pipelines using the types of tests we are using. However, some pipelines do use raw count data, which might be interesting to implement if we want. Either way, I think it's a little unintuitive to report a log2 fold change of log expression values. I can submit a pull request to implement this if this is something you agree with, and can add a parameter to let the user decide whether to use log-transformed or raw-count data. Let me know what you think!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/517
https://github.com/scverse/scanpy/issues/517:353,energy efficiency,scale,scale,353,"Log2 fold changes in rank_genes_groups are calculated from log-transformed data; Hi all. I was looking through the `_rank_genes_groups` function and noticed that the fold-change calculations are based on the means calculated by `_get_mean_var`. The only problem with this is that (usually) the expression values at this point in the analysis are in log scale, so we are calculating the fold-changes of the log1p count values, and then further log2 transforming these fold changes. I know that different programs do it differently, but I think it's more intuitive to convert the matrix back to counts, calculate the fold change, and then report the log2 fold change. Any thoughts? For the actual differential testing, I think it's ok to run the tests on the log1p transformed data, as that seems to be the norm for many pipelines using the types of tests we are using. However, some pipelines do use raw count data, which might be interesting to implement if we want. Either way, I think it's a little unintuitive to report a log2 fold change of log expression values. I can submit a pull request to implement this if this is something you agree with, and can add a parameter to let the user decide whether to use log-transformed or raw-count data. Let me know what you think!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/517
https://github.com/scverse/scanpy/issues/517:63,integrability,transform,transformed,63,"Log2 fold changes in rank_genes_groups are calculated from log-transformed data; Hi all. I was looking through the `_rank_genes_groups` function and noticed that the fold-change calculations are based on the means calculated by `_get_mean_var`. The only problem with this is that (usually) the expression values at this point in the analysis are in log scale, so we are calculating the fold-changes of the log1p count values, and then further log2 transforming these fold changes. I know that different programs do it differently, but I think it's more intuitive to convert the matrix back to counts, calculate the fold change, and then report the log2 fold change. Any thoughts? For the actual differential testing, I think it's ok to run the tests on the log1p transformed data, as that seems to be the norm for many pipelines using the types of tests we are using. However, some pipelines do use raw count data, which might be interesting to implement if we want. Either way, I think it's a little unintuitive to report a log2 fold change of log expression values. I can submit a pull request to implement this if this is something you agree with, and can add a parameter to let the user decide whether to use log-transformed or raw-count data. Let me know what you think!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/517
https://github.com/scverse/scanpy/issues/517:448,integrability,transform,transforming,448,"Log2 fold changes in rank_genes_groups are calculated from log-transformed data; Hi all. I was looking through the `_rank_genes_groups` function and noticed that the fold-change calculations are based on the means calculated by `_get_mean_var`. The only problem with this is that (usually) the expression values at this point in the analysis are in log scale, so we are calculating the fold-changes of the log1p count values, and then further log2 transforming these fold changes. I know that different programs do it differently, but I think it's more intuitive to convert the matrix back to counts, calculate the fold change, and then report the log2 fold change. Any thoughts? For the actual differential testing, I think it's ok to run the tests on the log1p transformed data, as that seems to be the norm for many pipelines using the types of tests we are using. However, some pipelines do use raw count data, which might be interesting to implement if we want. Either way, I think it's a little unintuitive to report a log2 fold change of log expression values. I can submit a pull request to implement this if this is something you agree with, and can add a parameter to let the user decide whether to use log-transformed or raw-count data. Let me know what you think!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/517
https://github.com/scverse/scanpy/issues/517:763,integrability,transform,transformed,763,"Log2 fold changes in rank_genes_groups are calculated from log-transformed data; Hi all. I was looking through the `_rank_genes_groups` function and noticed that the fold-change calculations are based on the means calculated by `_get_mean_var`. The only problem with this is that (usually) the expression values at this point in the analysis are in log scale, so we are calculating the fold-changes of the log1p count values, and then further log2 transforming these fold changes. I know that different programs do it differently, but I think it's more intuitive to convert the matrix back to counts, calculate the fold change, and then report the log2 fold change. Any thoughts? For the actual differential testing, I think it's ok to run the tests on the log1p transformed data, as that seems to be the norm for many pipelines using the types of tests we are using. However, some pipelines do use raw count data, which might be interesting to implement if we want. Either way, I think it's a little unintuitive to report a log2 fold change of log expression values. I can submit a pull request to implement this if this is something you agree with, and can add a parameter to let the user decide whether to use log-transformed or raw-count data. Let me know what you think!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/517
https://github.com/scverse/scanpy/issues/517:819,integrability,pipelin,pipelines,819,"Log2 fold changes in rank_genes_groups are calculated from log-transformed data; Hi all. I was looking through the `_rank_genes_groups` function and noticed that the fold-change calculations are based on the means calculated by `_get_mean_var`. The only problem with this is that (usually) the expression values at this point in the analysis are in log scale, so we are calculating the fold-changes of the log1p count values, and then further log2 transforming these fold changes. I know that different programs do it differently, but I think it's more intuitive to convert the matrix back to counts, calculate the fold change, and then report the log2 fold change. Any thoughts? For the actual differential testing, I think it's ok to run the tests on the log1p transformed data, as that seems to be the norm for many pipelines using the types of tests we are using. However, some pipelines do use raw count data, which might be interesting to implement if we want. Either way, I think it's a little unintuitive to report a log2 fold change of log expression values. I can submit a pull request to implement this if this is something you agree with, and can add a parameter to let the user decide whether to use log-transformed or raw-count data. Let me know what you think!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/517
https://github.com/scverse/scanpy/issues/517:882,integrability,pipelin,pipelines,882,"Log2 fold changes in rank_genes_groups are calculated from log-transformed data; Hi all. I was looking through the `_rank_genes_groups` function and noticed that the fold-change calculations are based on the means calculated by `_get_mean_var`. The only problem with this is that (usually) the expression values at this point in the analysis are in log scale, so we are calculating the fold-changes of the log1p count values, and then further log2 transforming these fold changes. I know that different programs do it differently, but I think it's more intuitive to convert the matrix back to counts, calculate the fold change, and then report the log2 fold change. Any thoughts? For the actual differential testing, I think it's ok to run the tests on the log1p transformed data, as that seems to be the norm for many pipelines using the types of tests we are using. However, some pipelines do use raw count data, which might be interesting to implement if we want. Either way, I think it's a little unintuitive to report a log2 fold change of log expression values. I can submit a pull request to implement this if this is something you agree with, and can add a parameter to let the user decide whether to use log-transformed or raw-count data. Let me know what you think!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/517
https://github.com/scverse/scanpy/issues/517:1074,integrability,sub,submit,1074,"Log2 fold changes in rank_genes_groups are calculated from log-transformed data; Hi all. I was looking through the `_rank_genes_groups` function and noticed that the fold-change calculations are based on the means calculated by `_get_mean_var`. The only problem with this is that (usually) the expression values at this point in the analysis are in log scale, so we are calculating the fold-changes of the log1p count values, and then further log2 transforming these fold changes. I know that different programs do it differently, but I think it's more intuitive to convert the matrix back to counts, calculate the fold change, and then report the log2 fold change. Any thoughts? For the actual differential testing, I think it's ok to run the tests on the log1p transformed data, as that seems to be the norm for many pipelines using the types of tests we are using. However, some pipelines do use raw count data, which might be interesting to implement if we want. Either way, I think it's a little unintuitive to report a log2 fold change of log expression values. I can submit a pull request to implement this if this is something you agree with, and can add a parameter to let the user decide whether to use log-transformed or raw-count data. Let me know what you think!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/517
https://github.com/scverse/scanpy/issues/517:1217,integrability,transform,transformed,1217,"Log2 fold changes in rank_genes_groups are calculated from log-transformed data; Hi all. I was looking through the `_rank_genes_groups` function and noticed that the fold-change calculations are based on the means calculated by `_get_mean_var`. The only problem with this is that (usually) the expression values at this point in the analysis are in log scale, so we are calculating the fold-changes of the log1p count values, and then further log2 transforming these fold changes. I know that different programs do it differently, but I think it's more intuitive to convert the matrix back to counts, calculate the fold change, and then report the log2 fold change. Any thoughts? For the actual differential testing, I think it's ok to run the tests on the log1p transformed data, as that seems to be the norm for many pipelines using the types of tests we are using. However, some pipelines do use raw count data, which might be interesting to implement if we want. Either way, I think it's a little unintuitive to report a log2 fold change of log expression values. I can submit a pull request to implement this if this is something you agree with, and can add a parameter to let the user decide whether to use log-transformed or raw-count data. Let me know what you think!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/517
https://github.com/scverse/scanpy/issues/517:63,interoperability,transform,transformed,63,"Log2 fold changes in rank_genes_groups are calculated from log-transformed data; Hi all. I was looking through the `_rank_genes_groups` function and noticed that the fold-change calculations are based on the means calculated by `_get_mean_var`. The only problem with this is that (usually) the expression values at this point in the analysis are in log scale, so we are calculating the fold-changes of the log1p count values, and then further log2 transforming these fold changes. I know that different programs do it differently, but I think it's more intuitive to convert the matrix back to counts, calculate the fold change, and then report the log2 fold change. Any thoughts? For the actual differential testing, I think it's ok to run the tests on the log1p transformed data, as that seems to be the norm for many pipelines using the types of tests we are using. However, some pipelines do use raw count data, which might be interesting to implement if we want. Either way, I think it's a little unintuitive to report a log2 fold change of log expression values. I can submit a pull request to implement this if this is something you agree with, and can add a parameter to let the user decide whether to use log-transformed or raw-count data. Let me know what you think!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/517
https://github.com/scverse/scanpy/issues/517:448,interoperability,transform,transforming,448,"Log2 fold changes in rank_genes_groups are calculated from log-transformed data; Hi all. I was looking through the `_rank_genes_groups` function and noticed that the fold-change calculations are based on the means calculated by `_get_mean_var`. The only problem with this is that (usually) the expression values at this point in the analysis are in log scale, so we are calculating the fold-changes of the log1p count values, and then further log2 transforming these fold changes. I know that different programs do it differently, but I think it's more intuitive to convert the matrix back to counts, calculate the fold change, and then report the log2 fold change. Any thoughts? For the actual differential testing, I think it's ok to run the tests on the log1p transformed data, as that seems to be the norm for many pipelines using the types of tests we are using. However, some pipelines do use raw count data, which might be interesting to implement if we want. Either way, I think it's a little unintuitive to report a log2 fold change of log expression values. I can submit a pull request to implement this if this is something you agree with, and can add a parameter to let the user decide whether to use log-transformed or raw-count data. Let me know what you think!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/517
https://github.com/scverse/scanpy/issues/517:763,interoperability,transform,transformed,763,"Log2 fold changes in rank_genes_groups are calculated from log-transformed data; Hi all. I was looking through the `_rank_genes_groups` function and noticed that the fold-change calculations are based on the means calculated by `_get_mean_var`. The only problem with this is that (usually) the expression values at this point in the analysis are in log scale, so we are calculating the fold-changes of the log1p count values, and then further log2 transforming these fold changes. I know that different programs do it differently, but I think it's more intuitive to convert the matrix back to counts, calculate the fold change, and then report the log2 fold change. Any thoughts? For the actual differential testing, I think it's ok to run the tests on the log1p transformed data, as that seems to be the norm for many pipelines using the types of tests we are using. However, some pipelines do use raw count data, which might be interesting to implement if we want. Either way, I think it's a little unintuitive to report a log2 fold change of log expression values. I can submit a pull request to implement this if this is something you agree with, and can add a parameter to let the user decide whether to use log-transformed or raw-count data. Let me know what you think!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/517
https://github.com/scverse/scanpy/issues/517:1217,interoperability,transform,transformed,1217,"Log2 fold changes in rank_genes_groups are calculated from log-transformed data; Hi all. I was looking through the `_rank_genes_groups` function and noticed that the fold-change calculations are based on the means calculated by `_get_mean_var`. The only problem with this is that (usually) the expression values at this point in the analysis are in log scale, so we are calculating the fold-changes of the log1p count values, and then further log2 transforming these fold changes. I know that different programs do it differently, but I think it's more intuitive to convert the matrix back to counts, calculate the fold change, and then report the log2 fold change. Any thoughts? For the actual differential testing, I think it's ok to run the tests on the log1p transformed data, as that seems to be the norm for many pipelines using the types of tests we are using. However, some pipelines do use raw count data, which might be interesting to implement if we want. Either way, I think it's a little unintuitive to report a log2 fold change of log expression values. I can submit a pull request to implement this if this is something you agree with, and can add a parameter to let the user decide whether to use log-transformed or raw-count data. Let me know what you think!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/517
https://github.com/scverse/scanpy/issues/517:353,modifiability,scal,scale,353,"Log2 fold changes in rank_genes_groups are calculated from log-transformed data; Hi all. I was looking through the `_rank_genes_groups` function and noticed that the fold-change calculations are based on the means calculated by `_get_mean_var`. The only problem with this is that (usually) the expression values at this point in the analysis are in log scale, so we are calculating the fold-changes of the log1p count values, and then further log2 transforming these fold changes. I know that different programs do it differently, but I think it's more intuitive to convert the matrix back to counts, calculate the fold change, and then report the log2 fold change. Any thoughts? For the actual differential testing, I think it's ok to run the tests on the log1p transformed data, as that seems to be the norm for many pipelines using the types of tests we are using. However, some pipelines do use raw count data, which might be interesting to implement if we want. Either way, I think it's a little unintuitive to report a log2 fold change of log expression values. I can submit a pull request to implement this if this is something you agree with, and can add a parameter to let the user decide whether to use log-transformed or raw-count data. Let me know what you think!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/517
https://github.com/scverse/scanpy/issues/517:1165,modifiability,paramet,parameter,1165,"Log2 fold changes in rank_genes_groups are calculated from log-transformed data; Hi all. I was looking through the `_rank_genes_groups` function and noticed that the fold-change calculations are based on the means calculated by `_get_mean_var`. The only problem with this is that (usually) the expression values at this point in the analysis are in log scale, so we are calculating the fold-changes of the log1p count values, and then further log2 transforming these fold changes. I know that different programs do it differently, but I think it's more intuitive to convert the matrix back to counts, calculate the fold change, and then report the log2 fold change. Any thoughts? For the actual differential testing, I think it's ok to run the tests on the log1p transformed data, as that seems to be the norm for many pipelines using the types of tests we are using. However, some pipelines do use raw count data, which might be interesting to implement if we want. Either way, I think it's a little unintuitive to report a log2 fold change of log expression values. I can submit a pull request to implement this if this is something you agree with, and can add a parameter to let the user decide whether to use log-transformed or raw-count data. Let me know what you think!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/517
https://github.com/scverse/scanpy/issues/517:353,performance,scale,scale,353,"Log2 fold changes in rank_genes_groups are calculated from log-transformed data; Hi all. I was looking through the `_rank_genes_groups` function and noticed that the fold-change calculations are based on the means calculated by `_get_mean_var`. The only problem with this is that (usually) the expression values at this point in the analysis are in log scale, so we are calculating the fold-changes of the log1p count values, and then further log2 transforming these fold changes. I know that different programs do it differently, but I think it's more intuitive to convert the matrix back to counts, calculate the fold change, and then report the log2 fold change. Any thoughts? For the actual differential testing, I think it's ok to run the tests on the log1p transformed data, as that seems to be the norm for many pipelines using the types of tests we are using. However, some pipelines do use raw count data, which might be interesting to implement if we want. Either way, I think it's a little unintuitive to report a log2 fold change of log expression values. I can submit a pull request to implement this if this is something you agree with, and can add a parameter to let the user decide whether to use log-transformed or raw-count data. Let me know what you think!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/517
https://github.com/scverse/scanpy/issues/517:59,safety,log,log-transformed,59,"Log2 fold changes in rank_genes_groups are calculated from log-transformed data; Hi all. I was looking through the `_rank_genes_groups` function and noticed that the fold-change calculations are based on the means calculated by `_get_mean_var`. The only problem with this is that (usually) the expression values at this point in the analysis are in log scale, so we are calculating the fold-changes of the log1p count values, and then further log2 transforming these fold changes. I know that different programs do it differently, but I think it's more intuitive to convert the matrix back to counts, calculate the fold change, and then report the log2 fold change. Any thoughts? For the actual differential testing, I think it's ok to run the tests on the log1p transformed data, as that seems to be the norm for many pipelines using the types of tests we are using. However, some pipelines do use raw count data, which might be interesting to implement if we want. Either way, I think it's a little unintuitive to report a log2 fold change of log expression values. I can submit a pull request to implement this if this is something you agree with, and can add a parameter to let the user decide whether to use log-transformed or raw-count data. Let me know what you think!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/517
https://github.com/scverse/scanpy/issues/517:349,safety,log,log,349,"Log2 fold changes in rank_genes_groups are calculated from log-transformed data; Hi all. I was looking through the `_rank_genes_groups` function and noticed that the fold-change calculations are based on the means calculated by `_get_mean_var`. The only problem with this is that (usually) the expression values at this point in the analysis are in log scale, so we are calculating the fold-changes of the log1p count values, and then further log2 transforming these fold changes. I know that different programs do it differently, but I think it's more intuitive to convert the matrix back to counts, calculate the fold change, and then report the log2 fold change. Any thoughts? For the actual differential testing, I think it's ok to run the tests on the log1p transformed data, as that seems to be the norm for many pipelines using the types of tests we are using. However, some pipelines do use raw count data, which might be interesting to implement if we want. Either way, I think it's a little unintuitive to report a log2 fold change of log expression values. I can submit a pull request to implement this if this is something you agree with, and can add a parameter to let the user decide whether to use log-transformed or raw-count data. Let me know what you think!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/517
https://github.com/scverse/scanpy/issues/517:708,safety,test,testing,708,"Log2 fold changes in rank_genes_groups are calculated from log-transformed data; Hi all. I was looking through the `_rank_genes_groups` function and noticed that the fold-change calculations are based on the means calculated by `_get_mean_var`. The only problem with this is that (usually) the expression values at this point in the analysis are in log scale, so we are calculating the fold-changes of the log1p count values, and then further log2 transforming these fold changes. I know that different programs do it differently, but I think it's more intuitive to convert the matrix back to counts, calculate the fold change, and then report the log2 fold change. Any thoughts? For the actual differential testing, I think it's ok to run the tests on the log1p transformed data, as that seems to be the norm for many pipelines using the types of tests we are using. However, some pipelines do use raw count data, which might be interesting to implement if we want. Either way, I think it's a little unintuitive to report a log2 fold change of log expression values. I can submit a pull request to implement this if this is something you agree with, and can add a parameter to let the user decide whether to use log-transformed or raw-count data. Let me know what you think!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/517
https://github.com/scverse/scanpy/issues/517:744,safety,test,tests,744,"Log2 fold changes in rank_genes_groups are calculated from log-transformed data; Hi all. I was looking through the `_rank_genes_groups` function and noticed that the fold-change calculations are based on the means calculated by `_get_mean_var`. The only problem with this is that (usually) the expression values at this point in the analysis are in log scale, so we are calculating the fold-changes of the log1p count values, and then further log2 transforming these fold changes. I know that different programs do it differently, but I think it's more intuitive to convert the matrix back to counts, calculate the fold change, and then report the log2 fold change. Any thoughts? For the actual differential testing, I think it's ok to run the tests on the log1p transformed data, as that seems to be the norm for many pipelines using the types of tests we are using. However, some pipelines do use raw count data, which might be interesting to implement if we want. Either way, I think it's a little unintuitive to report a log2 fold change of log expression values. I can submit a pull request to implement this if this is something you agree with, and can add a parameter to let the user decide whether to use log-transformed or raw-count data. Let me know what you think!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/517
https://github.com/scverse/scanpy/issues/517:848,safety,test,tests,848,"Log2 fold changes in rank_genes_groups are calculated from log-transformed data; Hi all. I was looking through the `_rank_genes_groups` function and noticed that the fold-change calculations are based on the means calculated by `_get_mean_var`. The only problem with this is that (usually) the expression values at this point in the analysis are in log scale, so we are calculating the fold-changes of the log1p count values, and then further log2 transforming these fold changes. I know that different programs do it differently, but I think it's more intuitive to convert the matrix back to counts, calculate the fold change, and then report the log2 fold change. Any thoughts? For the actual differential testing, I think it's ok to run the tests on the log1p transformed data, as that seems to be the norm for many pipelines using the types of tests we are using. However, some pipelines do use raw count data, which might be interesting to implement if we want. Either way, I think it's a little unintuitive to report a log2 fold change of log expression values. I can submit a pull request to implement this if this is something you agree with, and can add a parameter to let the user decide whether to use log-transformed or raw-count data. Let me know what you think!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/517
https://github.com/scverse/scanpy/issues/517:1045,safety,log,log,1045,"Log2 fold changes in rank_genes_groups are calculated from log-transformed data; Hi all. I was looking through the `_rank_genes_groups` function and noticed that the fold-change calculations are based on the means calculated by `_get_mean_var`. The only problem with this is that (usually) the expression values at this point in the analysis are in log scale, so we are calculating the fold-changes of the log1p count values, and then further log2 transforming these fold changes. I know that different programs do it differently, but I think it's more intuitive to convert the matrix back to counts, calculate the fold change, and then report the log2 fold change. Any thoughts? For the actual differential testing, I think it's ok to run the tests on the log1p transformed data, as that seems to be the norm for many pipelines using the types of tests we are using. However, some pipelines do use raw count data, which might be interesting to implement if we want. Either way, I think it's a little unintuitive to report a log2 fold change of log expression values. I can submit a pull request to implement this if this is something you agree with, and can add a parameter to let the user decide whether to use log-transformed or raw-count data. Let me know what you think!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/517
https://github.com/scverse/scanpy/issues/517:1213,safety,log,log-transformed,1213,"Log2 fold changes in rank_genes_groups are calculated from log-transformed data; Hi all. I was looking through the `_rank_genes_groups` function and noticed that the fold-change calculations are based on the means calculated by `_get_mean_var`. The only problem with this is that (usually) the expression values at this point in the analysis are in log scale, so we are calculating the fold-changes of the log1p count values, and then further log2 transforming these fold changes. I know that different programs do it differently, but I think it's more intuitive to convert the matrix back to counts, calculate the fold change, and then report the log2 fold change. Any thoughts? For the actual differential testing, I think it's ok to run the tests on the log1p transformed data, as that seems to be the norm for many pipelines using the types of tests we are using. However, some pipelines do use raw count data, which might be interesting to implement if we want. Either way, I think it's a little unintuitive to report a log2 fold change of log expression values. I can submit a pull request to implement this if this is something you agree with, and can add a parameter to let the user decide whether to use log-transformed or raw-count data. Let me know what you think!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/517
https://github.com/scverse/scanpy/issues/517:59,security,log,log-transformed,59,"Log2 fold changes in rank_genes_groups are calculated from log-transformed data; Hi all. I was looking through the `_rank_genes_groups` function and noticed that the fold-change calculations are based on the means calculated by `_get_mean_var`. The only problem with this is that (usually) the expression values at this point in the analysis are in log scale, so we are calculating the fold-changes of the log1p count values, and then further log2 transforming these fold changes. I know that different programs do it differently, but I think it's more intuitive to convert the matrix back to counts, calculate the fold change, and then report the log2 fold change. Any thoughts? For the actual differential testing, I think it's ok to run the tests on the log1p transformed data, as that seems to be the norm for many pipelines using the types of tests we are using. However, some pipelines do use raw count data, which might be interesting to implement if we want. Either way, I think it's a little unintuitive to report a log2 fold change of log expression values. I can submit a pull request to implement this if this is something you agree with, and can add a parameter to let the user decide whether to use log-transformed or raw-count data. Let me know what you think!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/517
https://github.com/scverse/scanpy/issues/517:349,security,log,log,349,"Log2 fold changes in rank_genes_groups are calculated from log-transformed data; Hi all. I was looking through the `_rank_genes_groups` function and noticed that the fold-change calculations are based on the means calculated by `_get_mean_var`. The only problem with this is that (usually) the expression values at this point in the analysis are in log scale, so we are calculating the fold-changes of the log1p count values, and then further log2 transforming these fold changes. I know that different programs do it differently, but I think it's more intuitive to convert the matrix back to counts, calculate the fold change, and then report the log2 fold change. Any thoughts? For the actual differential testing, I think it's ok to run the tests on the log1p transformed data, as that seems to be the norm for many pipelines using the types of tests we are using. However, some pipelines do use raw count data, which might be interesting to implement if we want. Either way, I think it's a little unintuitive to report a log2 fold change of log expression values. I can submit a pull request to implement this if this is something you agree with, and can add a parameter to let the user decide whether to use log-transformed or raw-count data. Let me know what you think!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/517
https://github.com/scverse/scanpy/issues/517:1045,security,log,log,1045,"Log2 fold changes in rank_genes_groups are calculated from log-transformed data; Hi all. I was looking through the `_rank_genes_groups` function and noticed that the fold-change calculations are based on the means calculated by `_get_mean_var`. The only problem with this is that (usually) the expression values at this point in the analysis are in log scale, so we are calculating the fold-changes of the log1p count values, and then further log2 transforming these fold changes. I know that different programs do it differently, but I think it's more intuitive to convert the matrix back to counts, calculate the fold change, and then report the log2 fold change. Any thoughts? For the actual differential testing, I think it's ok to run the tests on the log1p transformed data, as that seems to be the norm for many pipelines using the types of tests we are using. However, some pipelines do use raw count data, which might be interesting to implement if we want. Either way, I think it's a little unintuitive to report a log2 fold change of log expression values. I can submit a pull request to implement this if this is something you agree with, and can add a parameter to let the user decide whether to use log-transformed or raw-count data. Let me know what you think!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/517
https://github.com/scverse/scanpy/issues/517:1213,security,log,log-transformed,1213,"Log2 fold changes in rank_genes_groups are calculated from log-transformed data; Hi all. I was looking through the `_rank_genes_groups` function and noticed that the fold-change calculations are based on the means calculated by `_get_mean_var`. The only problem with this is that (usually) the expression values at this point in the analysis are in log scale, so we are calculating the fold-changes of the log1p count values, and then further log2 transforming these fold changes. I know that different programs do it differently, but I think it's more intuitive to convert the matrix back to counts, calculate the fold change, and then report the log2 fold change. Any thoughts? For the actual differential testing, I think it's ok to run the tests on the log1p transformed data, as that seems to be the norm for many pipelines using the types of tests we are using. However, some pipelines do use raw count data, which might be interesting to implement if we want. Either way, I think it's a little unintuitive to report a log2 fold change of log expression values. I can submit a pull request to implement this if this is something you agree with, and can add a parameter to let the user decide whether to use log-transformed or raw-count data. Let me know what you think!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/517
https://github.com/scverse/scanpy/issues/517:59,testability,log,log-transformed,59,"Log2 fold changes in rank_genes_groups are calculated from log-transformed data; Hi all. I was looking through the `_rank_genes_groups` function and noticed that the fold-change calculations are based on the means calculated by `_get_mean_var`. The only problem with this is that (usually) the expression values at this point in the analysis are in log scale, so we are calculating the fold-changes of the log1p count values, and then further log2 transforming these fold changes. I know that different programs do it differently, but I think it's more intuitive to convert the matrix back to counts, calculate the fold change, and then report the log2 fold change. Any thoughts? For the actual differential testing, I think it's ok to run the tests on the log1p transformed data, as that seems to be the norm for many pipelines using the types of tests we are using. However, some pipelines do use raw count data, which might be interesting to implement if we want. Either way, I think it's a little unintuitive to report a log2 fold change of log expression values. I can submit a pull request to implement this if this is something you agree with, and can add a parameter to let the user decide whether to use log-transformed or raw-count data. Let me know what you think!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/517
https://github.com/scverse/scanpy/issues/517:349,testability,log,log,349,"Log2 fold changes in rank_genes_groups are calculated from log-transformed data; Hi all. I was looking through the `_rank_genes_groups` function and noticed that the fold-change calculations are based on the means calculated by `_get_mean_var`. The only problem with this is that (usually) the expression values at this point in the analysis are in log scale, so we are calculating the fold-changes of the log1p count values, and then further log2 transforming these fold changes. I know that different programs do it differently, but I think it's more intuitive to convert the matrix back to counts, calculate the fold change, and then report the log2 fold change. Any thoughts? For the actual differential testing, I think it's ok to run the tests on the log1p transformed data, as that seems to be the norm for many pipelines using the types of tests we are using. However, some pipelines do use raw count data, which might be interesting to implement if we want. Either way, I think it's a little unintuitive to report a log2 fold change of log expression values. I can submit a pull request to implement this if this is something you agree with, and can add a parameter to let the user decide whether to use log-transformed or raw-count data. Let me know what you think!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/517
https://github.com/scverse/scanpy/issues/517:708,testability,test,testing,708,"Log2 fold changes in rank_genes_groups are calculated from log-transformed data; Hi all. I was looking through the `_rank_genes_groups` function and noticed that the fold-change calculations are based on the means calculated by `_get_mean_var`. The only problem with this is that (usually) the expression values at this point in the analysis are in log scale, so we are calculating the fold-changes of the log1p count values, and then further log2 transforming these fold changes. I know that different programs do it differently, but I think it's more intuitive to convert the matrix back to counts, calculate the fold change, and then report the log2 fold change. Any thoughts? For the actual differential testing, I think it's ok to run the tests on the log1p transformed data, as that seems to be the norm for many pipelines using the types of tests we are using. However, some pipelines do use raw count data, which might be interesting to implement if we want. Either way, I think it's a little unintuitive to report a log2 fold change of log expression values. I can submit a pull request to implement this if this is something you agree with, and can add a parameter to let the user decide whether to use log-transformed or raw-count data. Let me know what you think!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/517
https://github.com/scverse/scanpy/issues/517:744,testability,test,tests,744,"Log2 fold changes in rank_genes_groups are calculated from log-transformed data; Hi all. I was looking through the `_rank_genes_groups` function and noticed that the fold-change calculations are based on the means calculated by `_get_mean_var`. The only problem with this is that (usually) the expression values at this point in the analysis are in log scale, so we are calculating the fold-changes of the log1p count values, and then further log2 transforming these fold changes. I know that different programs do it differently, but I think it's more intuitive to convert the matrix back to counts, calculate the fold change, and then report the log2 fold change. Any thoughts? For the actual differential testing, I think it's ok to run the tests on the log1p transformed data, as that seems to be the norm for many pipelines using the types of tests we are using. However, some pipelines do use raw count data, which might be interesting to implement if we want. Either way, I think it's a little unintuitive to report a log2 fold change of log expression values. I can submit a pull request to implement this if this is something you agree with, and can add a parameter to let the user decide whether to use log-transformed or raw-count data. Let me know what you think!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/517
https://github.com/scverse/scanpy/issues/517:848,testability,test,tests,848,"Log2 fold changes in rank_genes_groups are calculated from log-transformed data; Hi all. I was looking through the `_rank_genes_groups` function and noticed that the fold-change calculations are based on the means calculated by `_get_mean_var`. The only problem with this is that (usually) the expression values at this point in the analysis are in log scale, so we are calculating the fold-changes of the log1p count values, and then further log2 transforming these fold changes. I know that different programs do it differently, but I think it's more intuitive to convert the matrix back to counts, calculate the fold change, and then report the log2 fold change. Any thoughts? For the actual differential testing, I think it's ok to run the tests on the log1p transformed data, as that seems to be the norm for many pipelines using the types of tests we are using. However, some pipelines do use raw count data, which might be interesting to implement if we want. Either way, I think it's a little unintuitive to report a log2 fold change of log expression values. I can submit a pull request to implement this if this is something you agree with, and can add a parameter to let the user decide whether to use log-transformed or raw-count data. Let me know what you think!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/517
https://github.com/scverse/scanpy/issues/517:1045,testability,log,log,1045,"Log2 fold changes in rank_genes_groups are calculated from log-transformed data; Hi all. I was looking through the `_rank_genes_groups` function and noticed that the fold-change calculations are based on the means calculated by `_get_mean_var`. The only problem with this is that (usually) the expression values at this point in the analysis are in log scale, so we are calculating the fold-changes of the log1p count values, and then further log2 transforming these fold changes. I know that different programs do it differently, but I think it's more intuitive to convert the matrix back to counts, calculate the fold change, and then report the log2 fold change. Any thoughts? For the actual differential testing, I think it's ok to run the tests on the log1p transformed data, as that seems to be the norm for many pipelines using the types of tests we are using. However, some pipelines do use raw count data, which might be interesting to implement if we want. Either way, I think it's a little unintuitive to report a log2 fold change of log expression values. I can submit a pull request to implement this if this is something you agree with, and can add a parameter to let the user decide whether to use log-transformed or raw-count data. Let me know what you think!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/517
https://github.com/scverse/scanpy/issues/517:1213,testability,log,log-transformed,1213,"Log2 fold changes in rank_genes_groups are calculated from log-transformed data; Hi all. I was looking through the `_rank_genes_groups` function and noticed that the fold-change calculations are based on the means calculated by `_get_mean_var`. The only problem with this is that (usually) the expression values at this point in the analysis are in log scale, so we are calculating the fold-changes of the log1p count values, and then further log2 transforming these fold changes. I know that different programs do it differently, but I think it's more intuitive to convert the matrix back to counts, calculate the fold change, and then report the log2 fold change. Any thoughts? For the actual differential testing, I think it's ok to run the tests on the log1p transformed data, as that seems to be the norm for many pipelines using the types of tests we are using. However, some pipelines do use raw count data, which might be interesting to implement if we want. Either way, I think it's a little unintuitive to report a log2 fold change of log expression values. I can submit a pull request to implement this if this is something you agree with, and can add a parameter to let the user decide whether to use log-transformed or raw-count data. Let me know what you think!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/517
https://github.com/scverse/scanpy/issues/517:553,usability,intuit,intuitive,553,"Log2 fold changes in rank_genes_groups are calculated from log-transformed data; Hi all. I was looking through the `_rank_genes_groups` function and noticed that the fold-change calculations are based on the means calculated by `_get_mean_var`. The only problem with this is that (usually) the expression values at this point in the analysis are in log scale, so we are calculating the fold-changes of the log1p count values, and then further log2 transforming these fold changes. I know that different programs do it differently, but I think it's more intuitive to convert the matrix back to counts, calculate the fold change, and then report the log2 fold change. Any thoughts? For the actual differential testing, I think it's ok to run the tests on the log1p transformed data, as that seems to be the norm for many pipelines using the types of tests we are using. However, some pipelines do use raw count data, which might be interesting to implement if we want. Either way, I think it's a little unintuitive to report a log2 fold change of log expression values. I can submit a pull request to implement this if this is something you agree with, and can add a parameter to let the user decide whether to use log-transformed or raw-count data. Let me know what you think!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/517
https://github.com/scverse/scanpy/issues/517:1186,usability,user,user,1186,"Log2 fold changes in rank_genes_groups are calculated from log-transformed data; Hi all. I was looking through the `_rank_genes_groups` function and noticed that the fold-change calculations are based on the means calculated by `_get_mean_var`. The only problem with this is that (usually) the expression values at this point in the analysis are in log scale, so we are calculating the fold-changes of the log1p count values, and then further log2 transforming these fold changes. I know that different programs do it differently, but I think it's more intuitive to convert the matrix back to counts, calculate the fold change, and then report the log2 fold change. Any thoughts? For the actual differential testing, I think it's ok to run the tests on the log1p transformed data, as that seems to be the norm for many pipelines using the types of tests we are using. However, some pipelines do use raw count data, which might be interesting to implement if we want. Either way, I think it's a little unintuitive to report a log2 fold change of log expression values. I can submit a pull request to implement this if this is something you agree with, and can add a parameter to let the user decide whether to use log-transformed or raw-count data. Let me know what you think!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/517
https://github.com/scverse/scanpy/issues/518:0,deployability,Updat,Update,0,"Update setup.py to update dependent packages; After updating to the most recent version of scanpy, I had to separately update anndata, louvain, and leidenalg packages that are dependancies. Any dependent package updates of these types should just happen when the newest version is installed.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/518
https://github.com/scverse/scanpy/issues/518:19,deployability,updat,update,19,"Update setup.py to update dependent packages; After updating to the most recent version of scanpy, I had to separately update anndata, louvain, and leidenalg packages that are dependancies. Any dependent package updates of these types should just happen when the newest version is installed.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/518
https://github.com/scverse/scanpy/issues/518:26,deployability,depend,dependent,26,"Update setup.py to update dependent packages; After updating to the most recent version of scanpy, I had to separately update anndata, louvain, and leidenalg packages that are dependancies. Any dependent package updates of these types should just happen when the newest version is installed.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/518
https://github.com/scverse/scanpy/issues/518:52,deployability,updat,updating,52,"Update setup.py to update dependent packages; After updating to the most recent version of scanpy, I had to separately update anndata, louvain, and leidenalg packages that are dependancies. Any dependent package updates of these types should just happen when the newest version is installed.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/518
https://github.com/scverse/scanpy/issues/518:80,deployability,version,version,80,"Update setup.py to update dependent packages; After updating to the most recent version of scanpy, I had to separately update anndata, louvain, and leidenalg packages that are dependancies. Any dependent package updates of these types should just happen when the newest version is installed.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/518
https://github.com/scverse/scanpy/issues/518:119,deployability,updat,update,119,"Update setup.py to update dependent packages; After updating to the most recent version of scanpy, I had to separately update anndata, louvain, and leidenalg packages that are dependancies. Any dependent package updates of these types should just happen when the newest version is installed.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/518
https://github.com/scverse/scanpy/issues/518:176,deployability,depend,dependancies,176,"Update setup.py to update dependent packages; After updating to the most recent version of scanpy, I had to separately update anndata, louvain, and leidenalg packages that are dependancies. Any dependent package updates of these types should just happen when the newest version is installed.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/518
https://github.com/scverse/scanpy/issues/518:194,deployability,depend,dependent,194,"Update setup.py to update dependent packages; After updating to the most recent version of scanpy, I had to separately update anndata, louvain, and leidenalg packages that are dependancies. Any dependent package updates of these types should just happen when the newest version is installed.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/518
https://github.com/scverse/scanpy/issues/518:212,deployability,updat,updates,212,"Update setup.py to update dependent packages; After updating to the most recent version of scanpy, I had to separately update anndata, louvain, and leidenalg packages that are dependancies. Any dependent package updates of these types should just happen when the newest version is installed.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/518
https://github.com/scverse/scanpy/issues/518:270,deployability,version,version,270,"Update setup.py to update dependent packages; After updating to the most recent version of scanpy, I had to separately update anndata, louvain, and leidenalg packages that are dependancies. Any dependent package updates of these types should just happen when the newest version is installed.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/518
https://github.com/scverse/scanpy/issues/518:281,deployability,instal,installed,281,"Update setup.py to update dependent packages; After updating to the most recent version of scanpy, I had to separately update anndata, louvain, and leidenalg packages that are dependancies. Any dependent package updates of these types should just happen when the newest version is installed.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/518
https://github.com/scverse/scanpy/issues/518:26,integrability,depend,dependent,26,"Update setup.py to update dependent packages; After updating to the most recent version of scanpy, I had to separately update anndata, louvain, and leidenalg packages that are dependancies. Any dependent package updates of these types should just happen when the newest version is installed.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/518
https://github.com/scverse/scanpy/issues/518:80,integrability,version,version,80,"Update setup.py to update dependent packages; After updating to the most recent version of scanpy, I had to separately update anndata, louvain, and leidenalg packages that are dependancies. Any dependent package updates of these types should just happen when the newest version is installed.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/518
https://github.com/scverse/scanpy/issues/518:176,integrability,depend,dependancies,176,"Update setup.py to update dependent packages; After updating to the most recent version of scanpy, I had to separately update anndata, louvain, and leidenalg packages that are dependancies. Any dependent package updates of these types should just happen when the newest version is installed.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/518
https://github.com/scverse/scanpy/issues/518:194,integrability,depend,dependent,194,"Update setup.py to update dependent packages; After updating to the most recent version of scanpy, I had to separately update anndata, louvain, and leidenalg packages that are dependancies. Any dependent package updates of these types should just happen when the newest version is installed.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/518
https://github.com/scverse/scanpy/issues/518:270,integrability,version,version,270,"Update setup.py to update dependent packages; After updating to the most recent version of scanpy, I had to separately update anndata, louvain, and leidenalg packages that are dependancies. Any dependent package updates of these types should just happen when the newest version is installed.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/518
https://github.com/scverse/scanpy/issues/518:26,modifiability,depend,dependent,26,"Update setup.py to update dependent packages; After updating to the most recent version of scanpy, I had to separately update anndata, louvain, and leidenalg packages that are dependancies. Any dependent package updates of these types should just happen when the newest version is installed.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/518
https://github.com/scverse/scanpy/issues/518:36,modifiability,pac,packages,36,"Update setup.py to update dependent packages; After updating to the most recent version of scanpy, I had to separately update anndata, louvain, and leidenalg packages that are dependancies. Any dependent package updates of these types should just happen when the newest version is installed.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/518
https://github.com/scverse/scanpy/issues/518:80,modifiability,version,version,80,"Update setup.py to update dependent packages; After updating to the most recent version of scanpy, I had to separately update anndata, louvain, and leidenalg packages that are dependancies. Any dependent package updates of these types should just happen when the newest version is installed.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/518
https://github.com/scverse/scanpy/issues/518:158,modifiability,pac,packages,158,"Update setup.py to update dependent packages; After updating to the most recent version of scanpy, I had to separately update anndata, louvain, and leidenalg packages that are dependancies. Any dependent package updates of these types should just happen when the newest version is installed.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/518
https://github.com/scverse/scanpy/issues/518:176,modifiability,depend,dependancies,176,"Update setup.py to update dependent packages; After updating to the most recent version of scanpy, I had to separately update anndata, louvain, and leidenalg packages that are dependancies. Any dependent package updates of these types should just happen when the newest version is installed.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/518
https://github.com/scverse/scanpy/issues/518:194,modifiability,depend,dependent,194,"Update setup.py to update dependent packages; After updating to the most recent version of scanpy, I had to separately update anndata, louvain, and leidenalg packages that are dependancies. Any dependent package updates of these types should just happen when the newest version is installed.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/518
https://github.com/scverse/scanpy/issues/518:204,modifiability,pac,package,204,"Update setup.py to update dependent packages; After updating to the most recent version of scanpy, I had to separately update anndata, louvain, and leidenalg packages that are dependancies. Any dependent package updates of these types should just happen when the newest version is installed.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/518
https://github.com/scverse/scanpy/issues/518:270,modifiability,version,version,270,"Update setup.py to update dependent packages; After updating to the most recent version of scanpy, I had to separately update anndata, louvain, and leidenalg packages that are dependancies. Any dependent package updates of these types should just happen when the newest version is installed.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/518
https://github.com/scverse/scanpy/issues/518:0,safety,Updat,Update,0,"Update setup.py to update dependent packages; After updating to the most recent version of scanpy, I had to separately update anndata, louvain, and leidenalg packages that are dependancies. Any dependent package updates of these types should just happen when the newest version is installed.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/518
https://github.com/scverse/scanpy/issues/518:19,safety,updat,update,19,"Update setup.py to update dependent packages; After updating to the most recent version of scanpy, I had to separately update anndata, louvain, and leidenalg packages that are dependancies. Any dependent package updates of these types should just happen when the newest version is installed.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/518
https://github.com/scverse/scanpy/issues/518:26,safety,depend,dependent,26,"Update setup.py to update dependent packages; After updating to the most recent version of scanpy, I had to separately update anndata, louvain, and leidenalg packages that are dependancies. Any dependent package updates of these types should just happen when the newest version is installed.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/518
https://github.com/scverse/scanpy/issues/518:52,safety,updat,updating,52,"Update setup.py to update dependent packages; After updating to the most recent version of scanpy, I had to separately update anndata, louvain, and leidenalg packages that are dependancies. Any dependent package updates of these types should just happen when the newest version is installed.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/518
https://github.com/scverse/scanpy/issues/518:119,safety,updat,update,119,"Update setup.py to update dependent packages; After updating to the most recent version of scanpy, I had to separately update anndata, louvain, and leidenalg packages that are dependancies. Any dependent package updates of these types should just happen when the newest version is installed.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/518
https://github.com/scverse/scanpy/issues/518:176,safety,depend,dependancies,176,"Update setup.py to update dependent packages; After updating to the most recent version of scanpy, I had to separately update anndata, louvain, and leidenalg packages that are dependancies. Any dependent package updates of these types should just happen when the newest version is installed.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/518
https://github.com/scverse/scanpy/issues/518:194,safety,depend,dependent,194,"Update setup.py to update dependent packages; After updating to the most recent version of scanpy, I had to separately update anndata, louvain, and leidenalg packages that are dependancies. Any dependent package updates of these types should just happen when the newest version is installed.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/518
https://github.com/scverse/scanpy/issues/518:212,safety,updat,updates,212,"Update setup.py to update dependent packages; After updating to the most recent version of scanpy, I had to separately update anndata, louvain, and leidenalg packages that are dependancies. Any dependent package updates of these types should just happen when the newest version is installed.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/518
https://github.com/scverse/scanpy/issues/518:0,security,Updat,Update,0,"Update setup.py to update dependent packages; After updating to the most recent version of scanpy, I had to separately update anndata, louvain, and leidenalg packages that are dependancies. Any dependent package updates of these types should just happen when the newest version is installed.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/518
https://github.com/scverse/scanpy/issues/518:19,security,updat,update,19,"Update setup.py to update dependent packages; After updating to the most recent version of scanpy, I had to separately update anndata, louvain, and leidenalg packages that are dependancies. Any dependent package updates of these types should just happen when the newest version is installed.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/518
https://github.com/scverse/scanpy/issues/518:52,security,updat,updating,52,"Update setup.py to update dependent packages; After updating to the most recent version of scanpy, I had to separately update anndata, louvain, and leidenalg packages that are dependancies. Any dependent package updates of these types should just happen when the newest version is installed.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/518
https://github.com/scverse/scanpy/issues/518:119,security,updat,update,119,"Update setup.py to update dependent packages; After updating to the most recent version of scanpy, I had to separately update anndata, louvain, and leidenalg packages that are dependancies. Any dependent package updates of these types should just happen when the newest version is installed.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/518
https://github.com/scverse/scanpy/issues/518:212,security,updat,updates,212,"Update setup.py to update dependent packages; After updating to the most recent version of scanpy, I had to separately update anndata, louvain, and leidenalg packages that are dependancies. Any dependent package updates of these types should just happen when the newest version is installed.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/518
https://github.com/scverse/scanpy/issues/518:26,testability,depend,dependent,26,"Update setup.py to update dependent packages; After updating to the most recent version of scanpy, I had to separately update anndata, louvain, and leidenalg packages that are dependancies. Any dependent package updates of these types should just happen when the newest version is installed.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/518
https://github.com/scverse/scanpy/issues/518:176,testability,depend,dependancies,176,"Update setup.py to update dependent packages; After updating to the most recent version of scanpy, I had to separately update anndata, louvain, and leidenalg packages that are dependancies. Any dependent package updates of these types should just happen when the newest version is installed.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/518
https://github.com/scverse/scanpy/issues/518:194,testability,depend,dependent,194,"Update setup.py to update dependent packages; After updating to the most recent version of scanpy, I had to separately update anndata, louvain, and leidenalg packages that are dependancies. Any dependent package updates of these types should just happen when the newest version is installed.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/518
https://github.com/scverse/scanpy/pull/519:15,deployability,log,log,15,"Fix for double log in fold-changes and for indexing in wilcoxon test; This is a fix for the issue discussed in #517. I implemented it by adding a new parameter to indicate whether the data in `adata.X` is log-transformed or not, and set it to True as a default, assuming most users will have log-transomed data at this point. This fix is assuming the transformation was done with log1p, as in `sc.pp.log1p.` The matrix is transformed back to raw counts only for the fold-change calculations - everything else is left the same to maintain the tests in log-space. I also included a fix for a bug in the indexing that was implemented in the original chunking for the wilcoxon test. This was noted in #469 and the same fix is implemented in PR #471 - thanks for catching that bug!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/519
https://github.com/scverse/scanpy/pull/519:205,deployability,log,log-transformed,205,"Fix for double log in fold-changes and for indexing in wilcoxon test; This is a fix for the issue discussed in #517. I implemented it by adding a new parameter to indicate whether the data in `adata.X` is log-transformed or not, and set it to True as a default, assuming most users will have log-transomed data at this point. This fix is assuming the transformation was done with log1p, as in `sc.pp.log1p.` The matrix is transformed back to raw counts only for the fold-change calculations - everything else is left the same to maintain the tests in log-space. I also included a fix for a bug in the indexing that was implemented in the original chunking for the wilcoxon test. This was noted in #469 and the same fix is implemented in PR #471 - thanks for catching that bug!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/519
https://github.com/scverse/scanpy/pull/519:292,deployability,log,log-transomed,292,"Fix for double log in fold-changes and for indexing in wilcoxon test; This is a fix for the issue discussed in #517. I implemented it by adding a new parameter to indicate whether the data in `adata.X` is log-transformed or not, and set it to True as a default, assuming most users will have log-transomed data at this point. This fix is assuming the transformation was done with log1p, as in `sc.pp.log1p.` The matrix is transformed back to raw counts only for the fold-change calculations - everything else is left the same to maintain the tests in log-space. I also included a fix for a bug in the indexing that was implemented in the original chunking for the wilcoxon test. This was noted in #469 and the same fix is implemented in PR #471 - thanks for catching that bug!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/519
https://github.com/scverse/scanpy/pull/519:551,deployability,log,log-space,551,"Fix for double log in fold-changes and for indexing in wilcoxon test; This is a fix for the issue discussed in #517. I implemented it by adding a new parameter to indicate whether the data in `adata.X` is log-transformed or not, and set it to True as a default, assuming most users will have log-transomed data at this point. This fix is assuming the transformation was done with log1p, as in `sc.pp.log1p.` The matrix is transformed back to raw counts only for the fold-change calculations - everything else is left the same to maintain the tests in log-space. I also included a fix for a bug in the indexing that was implemented in the original chunking for the wilcoxon test. This was noted in #469 and the same fix is implemented in PR #471 - thanks for catching that bug!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/519
https://github.com/scverse/scanpy/pull/519:209,integrability,transform,transformed,209,"Fix for double log in fold-changes and for indexing in wilcoxon test; This is a fix for the issue discussed in #517. I implemented it by adding a new parameter to indicate whether the data in `adata.X` is log-transformed or not, and set it to True as a default, assuming most users will have log-transomed data at this point. This fix is assuming the transformation was done with log1p, as in `sc.pp.log1p.` The matrix is transformed back to raw counts only for the fold-change calculations - everything else is left the same to maintain the tests in log-space. I also included a fix for a bug in the indexing that was implemented in the original chunking for the wilcoxon test. This was noted in #469 and the same fix is implemented in PR #471 - thanks for catching that bug!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/519
https://github.com/scverse/scanpy/pull/519:351,integrability,transform,transformation,351,"Fix for double log in fold-changes and for indexing in wilcoxon test; This is a fix for the issue discussed in #517. I implemented it by adding a new parameter to indicate whether the data in `adata.X` is log-transformed or not, and set it to True as a default, assuming most users will have log-transomed data at this point. This fix is assuming the transformation was done with log1p, as in `sc.pp.log1p.` The matrix is transformed back to raw counts only for the fold-change calculations - everything else is left the same to maintain the tests in log-space. I also included a fix for a bug in the indexing that was implemented in the original chunking for the wilcoxon test. This was noted in #469 and the same fix is implemented in PR #471 - thanks for catching that bug!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/519
https://github.com/scverse/scanpy/pull/519:422,integrability,transform,transformed,422,"Fix for double log in fold-changes and for indexing in wilcoxon test; This is a fix for the issue discussed in #517. I implemented it by adding a new parameter to indicate whether the data in `adata.X` is log-transformed or not, and set it to True as a default, assuming most users will have log-transomed data at this point. This fix is assuming the transformation was done with log1p, as in `sc.pp.log1p.` The matrix is transformed back to raw counts only for the fold-change calculations - everything else is left the same to maintain the tests in log-space. I also included a fix for a bug in the indexing that was implemented in the original chunking for the wilcoxon test. This was noted in #469 and the same fix is implemented in PR #471 - thanks for catching that bug!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/519
https://github.com/scverse/scanpy/pull/519:209,interoperability,transform,transformed,209,"Fix for double log in fold-changes and for indexing in wilcoxon test; This is a fix for the issue discussed in #517. I implemented it by adding a new parameter to indicate whether the data in `adata.X` is log-transformed or not, and set it to True as a default, assuming most users will have log-transomed data at this point. This fix is assuming the transformation was done with log1p, as in `sc.pp.log1p.` The matrix is transformed back to raw counts only for the fold-change calculations - everything else is left the same to maintain the tests in log-space. I also included a fix for a bug in the indexing that was implemented in the original chunking for the wilcoxon test. This was noted in #469 and the same fix is implemented in PR #471 - thanks for catching that bug!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/519
https://github.com/scverse/scanpy/pull/519:351,interoperability,transform,transformation,351,"Fix for double log in fold-changes and for indexing in wilcoxon test; This is a fix for the issue discussed in #517. I implemented it by adding a new parameter to indicate whether the data in `adata.X` is log-transformed or not, and set it to True as a default, assuming most users will have log-transomed data at this point. This fix is assuming the transformation was done with log1p, as in `sc.pp.log1p.` The matrix is transformed back to raw counts only for the fold-change calculations - everything else is left the same to maintain the tests in log-space. I also included a fix for a bug in the indexing that was implemented in the original chunking for the wilcoxon test. This was noted in #469 and the same fix is implemented in PR #471 - thanks for catching that bug!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/519
https://github.com/scverse/scanpy/pull/519:422,interoperability,transform,transformed,422,"Fix for double log in fold-changes and for indexing in wilcoxon test; This is a fix for the issue discussed in #517. I implemented it by adding a new parameter to indicate whether the data in `adata.X` is log-transformed or not, and set it to True as a default, assuming most users will have log-transomed data at this point. This fix is assuming the transformation was done with log1p, as in `sc.pp.log1p.` The matrix is transformed back to raw counts only for the fold-change calculations - everything else is left the same to maintain the tests in log-space. I also included a fix for a bug in the indexing that was implemented in the original chunking for the wilcoxon test. This was noted in #469 and the same fix is implemented in PR #471 - thanks for catching that bug!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/519
https://github.com/scverse/scanpy/pull/519:150,modifiability,paramet,parameter,150,"Fix for double log in fold-changes and for indexing in wilcoxon test; This is a fix for the issue discussed in #517. I implemented it by adding a new parameter to indicate whether the data in `adata.X` is log-transformed or not, and set it to True as a default, assuming most users will have log-transomed data at this point. This fix is assuming the transformation was done with log1p, as in `sc.pp.log1p.` The matrix is transformed back to raw counts only for the fold-change calculations - everything else is left the same to maintain the tests in log-space. I also included a fix for a bug in the indexing that was implemented in the original chunking for the wilcoxon test. This was noted in #469 and the same fix is implemented in PR #471 - thanks for catching that bug!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/519
https://github.com/scverse/scanpy/pull/519:529,modifiability,maintain,maintain,529,"Fix for double log in fold-changes and for indexing in wilcoxon test; This is a fix for the issue discussed in #517. I implemented it by adding a new parameter to indicate whether the data in `adata.X` is log-transformed or not, and set it to True as a default, assuming most users will have log-transomed data at this point. This fix is assuming the transformation was done with log1p, as in `sc.pp.log1p.` The matrix is transformed back to raw counts only for the fold-change calculations - everything else is left the same to maintain the tests in log-space. I also included a fix for a bug in the indexing that was implemented in the original chunking for the wilcoxon test. This was noted in #469 and the same fix is implemented in PR #471 - thanks for catching that bug!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/519
https://github.com/scverse/scanpy/pull/519:15,safety,log,log,15,"Fix for double log in fold-changes and for indexing in wilcoxon test; This is a fix for the issue discussed in #517. I implemented it by adding a new parameter to indicate whether the data in `adata.X` is log-transformed or not, and set it to True as a default, assuming most users will have log-transomed data at this point. This fix is assuming the transformation was done with log1p, as in `sc.pp.log1p.` The matrix is transformed back to raw counts only for the fold-change calculations - everything else is left the same to maintain the tests in log-space. I also included a fix for a bug in the indexing that was implemented in the original chunking for the wilcoxon test. This was noted in #469 and the same fix is implemented in PR #471 - thanks for catching that bug!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/519
https://github.com/scverse/scanpy/pull/519:64,safety,test,test,64,"Fix for double log in fold-changes and for indexing in wilcoxon test; This is a fix for the issue discussed in #517. I implemented it by adding a new parameter to indicate whether the data in `adata.X` is log-transformed or not, and set it to True as a default, assuming most users will have log-transomed data at this point. This fix is assuming the transformation was done with log1p, as in `sc.pp.log1p.` The matrix is transformed back to raw counts only for the fold-change calculations - everything else is left the same to maintain the tests in log-space. I also included a fix for a bug in the indexing that was implemented in the original chunking for the wilcoxon test. This was noted in #469 and the same fix is implemented in PR #471 - thanks for catching that bug!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/519
https://github.com/scverse/scanpy/pull/519:205,safety,log,log-transformed,205,"Fix for double log in fold-changes and for indexing in wilcoxon test; This is a fix for the issue discussed in #517. I implemented it by adding a new parameter to indicate whether the data in `adata.X` is log-transformed or not, and set it to True as a default, assuming most users will have log-transomed data at this point. This fix is assuming the transformation was done with log1p, as in `sc.pp.log1p.` The matrix is transformed back to raw counts only for the fold-change calculations - everything else is left the same to maintain the tests in log-space. I also included a fix for a bug in the indexing that was implemented in the original chunking for the wilcoxon test. This was noted in #469 and the same fix is implemented in PR #471 - thanks for catching that bug!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/519
https://github.com/scverse/scanpy/pull/519:292,safety,log,log-transomed,292,"Fix for double log in fold-changes and for indexing in wilcoxon test; This is a fix for the issue discussed in #517. I implemented it by adding a new parameter to indicate whether the data in `adata.X` is log-transformed or not, and set it to True as a default, assuming most users will have log-transomed data at this point. This fix is assuming the transformation was done with log1p, as in `sc.pp.log1p.` The matrix is transformed back to raw counts only for the fold-change calculations - everything else is left the same to maintain the tests in log-space. I also included a fix for a bug in the indexing that was implemented in the original chunking for the wilcoxon test. This was noted in #469 and the same fix is implemented in PR #471 - thanks for catching that bug!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/519
https://github.com/scverse/scanpy/pull/519:529,safety,maintain,maintain,529,"Fix for double log in fold-changes and for indexing in wilcoxon test; This is a fix for the issue discussed in #517. I implemented it by adding a new parameter to indicate whether the data in `adata.X` is log-transformed or not, and set it to True as a default, assuming most users will have log-transomed data at this point. This fix is assuming the transformation was done with log1p, as in `sc.pp.log1p.` The matrix is transformed back to raw counts only for the fold-change calculations - everything else is left the same to maintain the tests in log-space. I also included a fix for a bug in the indexing that was implemented in the original chunking for the wilcoxon test. This was noted in #469 and the same fix is implemented in PR #471 - thanks for catching that bug!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/519
https://github.com/scverse/scanpy/pull/519:542,safety,test,tests,542,"Fix for double log in fold-changes and for indexing in wilcoxon test; This is a fix for the issue discussed in #517. I implemented it by adding a new parameter to indicate whether the data in `adata.X` is log-transformed or not, and set it to True as a default, assuming most users will have log-transomed data at this point. This fix is assuming the transformation was done with log1p, as in `sc.pp.log1p.` The matrix is transformed back to raw counts only for the fold-change calculations - everything else is left the same to maintain the tests in log-space. I also included a fix for a bug in the indexing that was implemented in the original chunking for the wilcoxon test. This was noted in #469 and the same fix is implemented in PR #471 - thanks for catching that bug!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/519
https://github.com/scverse/scanpy/pull/519:551,safety,log,log-space,551,"Fix for double log in fold-changes and for indexing in wilcoxon test; This is a fix for the issue discussed in #517. I implemented it by adding a new parameter to indicate whether the data in `adata.X` is log-transformed or not, and set it to True as a default, assuming most users will have log-transomed data at this point. This fix is assuming the transformation was done with log1p, as in `sc.pp.log1p.` The matrix is transformed back to raw counts only for the fold-change calculations - everything else is left the same to maintain the tests in log-space. I also included a fix for a bug in the indexing that was implemented in the original chunking for the wilcoxon test. This was noted in #469 and the same fix is implemented in PR #471 - thanks for catching that bug!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/519
https://github.com/scverse/scanpy/pull/519:673,safety,test,test,673,"Fix for double log in fold-changes and for indexing in wilcoxon test; This is a fix for the issue discussed in #517. I implemented it by adding a new parameter to indicate whether the data in `adata.X` is log-transformed or not, and set it to True as a default, assuming most users will have log-transomed data at this point. This fix is assuming the transformation was done with log1p, as in `sc.pp.log1p.` The matrix is transformed back to raw counts only for the fold-change calculations - everything else is left the same to maintain the tests in log-space. I also included a fix for a bug in the indexing that was implemented in the original chunking for the wilcoxon test. This was noted in #469 and the same fix is implemented in PR #471 - thanks for catching that bug!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/519
https://github.com/scverse/scanpy/pull/519:15,security,log,log,15,"Fix for double log in fold-changes and for indexing in wilcoxon test; This is a fix for the issue discussed in #517. I implemented it by adding a new parameter to indicate whether the data in `adata.X` is log-transformed or not, and set it to True as a default, assuming most users will have log-transomed data at this point. This fix is assuming the transformation was done with log1p, as in `sc.pp.log1p.` The matrix is transformed back to raw counts only for the fold-change calculations - everything else is left the same to maintain the tests in log-space. I also included a fix for a bug in the indexing that was implemented in the original chunking for the wilcoxon test. This was noted in #469 and the same fix is implemented in PR #471 - thanks for catching that bug!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/519
https://github.com/scverse/scanpy/pull/519:205,security,log,log-transformed,205,"Fix for double log in fold-changes and for indexing in wilcoxon test; This is a fix for the issue discussed in #517. I implemented it by adding a new parameter to indicate whether the data in `adata.X` is log-transformed or not, and set it to True as a default, assuming most users will have log-transomed data at this point. This fix is assuming the transformation was done with log1p, as in `sc.pp.log1p.` The matrix is transformed back to raw counts only for the fold-change calculations - everything else is left the same to maintain the tests in log-space. I also included a fix for a bug in the indexing that was implemented in the original chunking for the wilcoxon test. This was noted in #469 and the same fix is implemented in PR #471 - thanks for catching that bug!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/519
https://github.com/scverse/scanpy/pull/519:292,security,log,log-transomed,292,"Fix for double log in fold-changes and for indexing in wilcoxon test; This is a fix for the issue discussed in #517. I implemented it by adding a new parameter to indicate whether the data in `adata.X` is log-transformed or not, and set it to True as a default, assuming most users will have log-transomed data at this point. This fix is assuming the transformation was done with log1p, as in `sc.pp.log1p.` The matrix is transformed back to raw counts only for the fold-change calculations - everything else is left the same to maintain the tests in log-space. I also included a fix for a bug in the indexing that was implemented in the original chunking for the wilcoxon test. This was noted in #469 and the same fix is implemented in PR #471 - thanks for catching that bug!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/519
https://github.com/scverse/scanpy/pull/519:551,security,log,log-space,551,"Fix for double log in fold-changes and for indexing in wilcoxon test; This is a fix for the issue discussed in #517. I implemented it by adding a new parameter to indicate whether the data in `adata.X` is log-transformed or not, and set it to True as a default, assuming most users will have log-transomed data at this point. This fix is assuming the transformation was done with log1p, as in `sc.pp.log1p.` The matrix is transformed back to raw counts only for the fold-change calculations - everything else is left the same to maintain the tests in log-space. I also included a fix for a bug in the indexing that was implemented in the original chunking for the wilcoxon test. This was noted in #469 and the same fix is implemented in PR #471 - thanks for catching that bug!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/519
https://github.com/scverse/scanpy/pull/519:15,testability,log,log,15,"Fix for double log in fold-changes and for indexing in wilcoxon test; This is a fix for the issue discussed in #517. I implemented it by adding a new parameter to indicate whether the data in `adata.X` is log-transformed or not, and set it to True as a default, assuming most users will have log-transomed data at this point. This fix is assuming the transformation was done with log1p, as in `sc.pp.log1p.` The matrix is transformed back to raw counts only for the fold-change calculations - everything else is left the same to maintain the tests in log-space. I also included a fix for a bug in the indexing that was implemented in the original chunking for the wilcoxon test. This was noted in #469 and the same fix is implemented in PR #471 - thanks for catching that bug!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/519
https://github.com/scverse/scanpy/pull/519:64,testability,test,test,64,"Fix for double log in fold-changes and for indexing in wilcoxon test; This is a fix for the issue discussed in #517. I implemented it by adding a new parameter to indicate whether the data in `adata.X` is log-transformed or not, and set it to True as a default, assuming most users will have log-transomed data at this point. This fix is assuming the transformation was done with log1p, as in `sc.pp.log1p.` The matrix is transformed back to raw counts only for the fold-change calculations - everything else is left the same to maintain the tests in log-space. I also included a fix for a bug in the indexing that was implemented in the original chunking for the wilcoxon test. This was noted in #469 and the same fix is implemented in PR #471 - thanks for catching that bug!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/519
https://github.com/scverse/scanpy/pull/519:205,testability,log,log-transformed,205,"Fix for double log in fold-changes and for indexing in wilcoxon test; This is a fix for the issue discussed in #517. I implemented it by adding a new parameter to indicate whether the data in `adata.X` is log-transformed or not, and set it to True as a default, assuming most users will have log-transomed data at this point. This fix is assuming the transformation was done with log1p, as in `sc.pp.log1p.` The matrix is transformed back to raw counts only for the fold-change calculations - everything else is left the same to maintain the tests in log-space. I also included a fix for a bug in the indexing that was implemented in the original chunking for the wilcoxon test. This was noted in #469 and the same fix is implemented in PR #471 - thanks for catching that bug!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/519
https://github.com/scverse/scanpy/pull/519:292,testability,log,log-transomed,292,"Fix for double log in fold-changes and for indexing in wilcoxon test; This is a fix for the issue discussed in #517. I implemented it by adding a new parameter to indicate whether the data in `adata.X` is log-transformed or not, and set it to True as a default, assuming most users will have log-transomed data at this point. This fix is assuming the transformation was done with log1p, as in `sc.pp.log1p.` The matrix is transformed back to raw counts only for the fold-change calculations - everything else is left the same to maintain the tests in log-space. I also included a fix for a bug in the indexing that was implemented in the original chunking for the wilcoxon test. This was noted in #469 and the same fix is implemented in PR #471 - thanks for catching that bug!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/519
https://github.com/scverse/scanpy/pull/519:542,testability,test,tests,542,"Fix for double log in fold-changes and for indexing in wilcoxon test; This is a fix for the issue discussed in #517. I implemented it by adding a new parameter to indicate whether the data in `adata.X` is log-transformed or not, and set it to True as a default, assuming most users will have log-transomed data at this point. This fix is assuming the transformation was done with log1p, as in `sc.pp.log1p.` The matrix is transformed back to raw counts only for the fold-change calculations - everything else is left the same to maintain the tests in log-space. I also included a fix for a bug in the indexing that was implemented in the original chunking for the wilcoxon test. This was noted in #469 and the same fix is implemented in PR #471 - thanks for catching that bug!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/519
https://github.com/scverse/scanpy/pull/519:551,testability,log,log-space,551,"Fix for double log in fold-changes and for indexing in wilcoxon test; This is a fix for the issue discussed in #517. I implemented it by adding a new parameter to indicate whether the data in `adata.X` is log-transformed or not, and set it to True as a default, assuming most users will have log-transomed data at this point. This fix is assuming the transformation was done with log1p, as in `sc.pp.log1p.` The matrix is transformed back to raw counts only for the fold-change calculations - everything else is left the same to maintain the tests in log-space. I also included a fix for a bug in the indexing that was implemented in the original chunking for the wilcoxon test. This was noted in #469 and the same fix is implemented in PR #471 - thanks for catching that bug!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/519
https://github.com/scverse/scanpy/pull/519:673,testability,test,test,673,"Fix for double log in fold-changes and for indexing in wilcoxon test; This is a fix for the issue discussed in #517. I implemented it by adding a new parameter to indicate whether the data in `adata.X` is log-transformed or not, and set it to True as a default, assuming most users will have log-transomed data at this point. This fix is assuming the transformation was done with log1p, as in `sc.pp.log1p.` The matrix is transformed back to raw counts only for the fold-change calculations - everything else is left the same to maintain the tests in log-space. I also included a fix for a bug in the indexing that was implemented in the original chunking for the wilcoxon test. This was noted in #469 and the same fix is implemented in PR #471 - thanks for catching that bug!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/519
https://github.com/scverse/scanpy/pull/519:163,usability,indicat,indicate,163,"Fix for double log in fold-changes and for indexing in wilcoxon test; This is a fix for the issue discussed in #517. I implemented it by adding a new parameter to indicate whether the data in `adata.X` is log-transformed or not, and set it to True as a default, assuming most users will have log-transomed data at this point. This fix is assuming the transformation was done with log1p, as in `sc.pp.log1p.` The matrix is transformed back to raw counts only for the fold-change calculations - everything else is left the same to maintain the tests in log-space. I also included a fix for a bug in the indexing that was implemented in the original chunking for the wilcoxon test. This was noted in #469 and the same fix is implemented in PR #471 - thanks for catching that bug!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/519
https://github.com/scverse/scanpy/pull/519:276,usability,user,users,276,"Fix for double log in fold-changes and for indexing in wilcoxon test; This is a fix for the issue discussed in #517. I implemented it by adding a new parameter to indicate whether the data in `adata.X` is log-transformed or not, and set it to True as a default, assuming most users will have log-transomed data at this point. This fix is assuming the transformation was done with log1p, as in `sc.pp.log1p.` The matrix is transformed back to raw counts only for the fold-change calculations - everything else is left the same to maintain the tests in log-space. I also included a fix for a bug in the indexing that was implemented in the original chunking for the wilcoxon test. This was noted in #469 and the same fix is implemented in PR #471 - thanks for catching that bug!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/519
https://github.com/scverse/scanpy/issues/520:5,deployability,integr,integration,5,scvi integration; https://github.com/YosefLab/scVI/issues/206,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/520
https://github.com/scverse/scanpy/issues/520:5,integrability,integr,integration,5,scvi integration; https://github.com/YosefLab/scVI/issues/206,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/520
https://github.com/scverse/scanpy/issues/520:5,interoperability,integr,integration,5,scvi integration; https://github.com/YosefLab/scVI/issues/206,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/520
https://github.com/scverse/scanpy/issues/520:5,modifiability,integr,integration,5,scvi integration; https://github.com/YosefLab/scVI/issues/206,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/520
https://github.com/scverse/scanpy/issues/520:5,reliability,integr,integration,5,scvi integration; https://github.com/YosefLab/scVI/issues/206,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/520
https://github.com/scverse/scanpy/issues/520:5,security,integr,integration,5,scvi integration; https://github.com/YosefLab/scVI/issues/206,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/520
https://github.com/scverse/scanpy/issues/520:5,testability,integr,integration,5,scvi integration; https://github.com/YosefLab/scVI/issues/206,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/520
https://github.com/scverse/scanpy/issues/521:8,availability,cluster,cluster-color,8,"heatmap cluster-color bar ; Hi,. I would like to set the scatterplot color palette and the heatmap cluster-color bar palette to be the same so that the clusters in the scatterplot are the same as those in the heatmap cluster-color-bar. I am using matplotlib to plot a scatterplot (using some other embedding) and need the cluster colors to match those in the automatic colorbar on the heatmap clusterbar. If i globally set the palette using sc.set_figure_params() then the values of the heatmap plot also become the global cmap (rather than a gradient of red-blue or existing default). Thanks for your help. Shobi",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/521
https://github.com/scverse/scanpy/issues/521:99,availability,cluster,cluster-color,99,"heatmap cluster-color bar ; Hi,. I would like to set the scatterplot color palette and the heatmap cluster-color bar palette to be the same so that the clusters in the scatterplot are the same as those in the heatmap cluster-color-bar. I am using matplotlib to plot a scatterplot (using some other embedding) and need the cluster colors to match those in the automatic colorbar on the heatmap clusterbar. If i globally set the palette using sc.set_figure_params() then the values of the heatmap plot also become the global cmap (rather than a gradient of red-blue or existing default). Thanks for your help. Shobi",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/521
https://github.com/scverse/scanpy/issues/521:152,availability,cluster,clusters,152,"heatmap cluster-color bar ; Hi,. I would like to set the scatterplot color palette and the heatmap cluster-color bar palette to be the same so that the clusters in the scatterplot are the same as those in the heatmap cluster-color-bar. I am using matplotlib to plot a scatterplot (using some other embedding) and need the cluster colors to match those in the automatic colorbar on the heatmap clusterbar. If i globally set the palette using sc.set_figure_params() then the values of the heatmap plot also become the global cmap (rather than a gradient of red-blue or existing default). Thanks for your help. Shobi",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/521
https://github.com/scverse/scanpy/issues/521:217,availability,cluster,cluster-color-bar,217,"heatmap cluster-color bar ; Hi,. I would like to set the scatterplot color palette and the heatmap cluster-color bar palette to be the same so that the clusters in the scatterplot are the same as those in the heatmap cluster-color-bar. I am using matplotlib to plot a scatterplot (using some other embedding) and need the cluster colors to match those in the automatic colorbar on the heatmap clusterbar. If i globally set the palette using sc.set_figure_params() then the values of the heatmap plot also become the global cmap (rather than a gradient of red-blue or existing default). Thanks for your help. Shobi",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/521
https://github.com/scverse/scanpy/issues/521:322,availability,cluster,cluster,322,"heatmap cluster-color bar ; Hi,. I would like to set the scatterplot color palette and the heatmap cluster-color bar palette to be the same so that the clusters in the scatterplot are the same as those in the heatmap cluster-color-bar. I am using matplotlib to plot a scatterplot (using some other embedding) and need the cluster colors to match those in the automatic colorbar on the heatmap clusterbar. If i globally set the palette using sc.set_figure_params() then the values of the heatmap plot also become the global cmap (rather than a gradient of red-blue or existing default). Thanks for your help. Shobi",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/521
https://github.com/scverse/scanpy/issues/521:393,availability,cluster,clusterbar,393,"heatmap cluster-color bar ; Hi,. I would like to set the scatterplot color palette and the heatmap cluster-color bar palette to be the same so that the clusters in the scatterplot are the same as those in the heatmap cluster-color-bar. I am using matplotlib to plot a scatterplot (using some other embedding) and need the cluster colors to match those in the automatic colorbar on the heatmap clusterbar. If i globally set the palette using sc.set_figure_params() then the values of the heatmap plot also become the global cmap (rather than a gradient of red-blue or existing default). Thanks for your help. Shobi",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/521
https://github.com/scverse/scanpy/issues/521:8,deployability,cluster,cluster-color,8,"heatmap cluster-color bar ; Hi,. I would like to set the scatterplot color palette and the heatmap cluster-color bar palette to be the same so that the clusters in the scatterplot are the same as those in the heatmap cluster-color-bar. I am using matplotlib to plot a scatterplot (using some other embedding) and need the cluster colors to match those in the automatic colorbar on the heatmap clusterbar. If i globally set the palette using sc.set_figure_params() then the values of the heatmap plot also become the global cmap (rather than a gradient of red-blue or existing default). Thanks for your help. Shobi",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/521
https://github.com/scverse/scanpy/issues/521:99,deployability,cluster,cluster-color,99,"heatmap cluster-color bar ; Hi,. I would like to set the scatterplot color palette and the heatmap cluster-color bar palette to be the same so that the clusters in the scatterplot are the same as those in the heatmap cluster-color-bar. I am using matplotlib to plot a scatterplot (using some other embedding) and need the cluster colors to match those in the automatic colorbar on the heatmap clusterbar. If i globally set the palette using sc.set_figure_params() then the values of the heatmap plot also become the global cmap (rather than a gradient of red-blue or existing default). Thanks for your help. Shobi",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/521
https://github.com/scverse/scanpy/issues/521:152,deployability,cluster,clusters,152,"heatmap cluster-color bar ; Hi,. I would like to set the scatterplot color palette and the heatmap cluster-color bar palette to be the same so that the clusters in the scatterplot are the same as those in the heatmap cluster-color-bar. I am using matplotlib to plot a scatterplot (using some other embedding) and need the cluster colors to match those in the automatic colorbar on the heatmap clusterbar. If i globally set the palette using sc.set_figure_params() then the values of the heatmap plot also become the global cmap (rather than a gradient of red-blue or existing default). Thanks for your help. Shobi",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/521
https://github.com/scverse/scanpy/issues/521:217,deployability,cluster,cluster-color-bar,217,"heatmap cluster-color bar ; Hi,. I would like to set the scatterplot color palette and the heatmap cluster-color bar palette to be the same so that the clusters in the scatterplot are the same as those in the heatmap cluster-color-bar. I am using matplotlib to plot a scatterplot (using some other embedding) and need the cluster colors to match those in the automatic colorbar on the heatmap clusterbar. If i globally set the palette using sc.set_figure_params() then the values of the heatmap plot also become the global cmap (rather than a gradient of red-blue or existing default). Thanks for your help. Shobi",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/521
https://github.com/scverse/scanpy/issues/521:322,deployability,cluster,cluster,322,"heatmap cluster-color bar ; Hi,. I would like to set the scatterplot color palette and the heatmap cluster-color bar palette to be the same so that the clusters in the scatterplot are the same as those in the heatmap cluster-color-bar. I am using matplotlib to plot a scatterplot (using some other embedding) and need the cluster colors to match those in the automatic colorbar on the heatmap clusterbar. If i globally set the palette using sc.set_figure_params() then the values of the heatmap plot also become the global cmap (rather than a gradient of red-blue or existing default). Thanks for your help. Shobi",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/521
https://github.com/scverse/scanpy/issues/521:359,deployability,automat,automatic,359,"heatmap cluster-color bar ; Hi,. I would like to set the scatterplot color palette and the heatmap cluster-color bar palette to be the same so that the clusters in the scatterplot are the same as those in the heatmap cluster-color-bar. I am using matplotlib to plot a scatterplot (using some other embedding) and need the cluster colors to match those in the automatic colorbar on the heatmap clusterbar. If i globally set the palette using sc.set_figure_params() then the values of the heatmap plot also become the global cmap (rather than a gradient of red-blue or existing default). Thanks for your help. Shobi",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/521
https://github.com/scverse/scanpy/issues/521:393,deployability,cluster,clusterbar,393,"heatmap cluster-color bar ; Hi,. I would like to set the scatterplot color palette and the heatmap cluster-color bar palette to be the same so that the clusters in the scatterplot are the same as those in the heatmap cluster-color-bar. I am using matplotlib to plot a scatterplot (using some other embedding) and need the cluster colors to match those in the automatic colorbar on the heatmap clusterbar. If i globally set the palette using sc.set_figure_params() then the values of the heatmap plot also become the global cmap (rather than a gradient of red-blue or existing default). Thanks for your help. Shobi",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/521
https://github.com/scverse/scanpy/issues/521:0,energy efficiency,heat,heatmap,0,"heatmap cluster-color bar ; Hi,. I would like to set the scatterplot color palette and the heatmap cluster-color bar palette to be the same so that the clusters in the scatterplot are the same as those in the heatmap cluster-color-bar. I am using matplotlib to plot a scatterplot (using some other embedding) and need the cluster colors to match those in the automatic colorbar on the heatmap clusterbar. If i globally set the palette using sc.set_figure_params() then the values of the heatmap plot also become the global cmap (rather than a gradient of red-blue or existing default). Thanks for your help. Shobi",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/521
https://github.com/scverse/scanpy/issues/521:91,energy efficiency,heat,heatmap,91,"heatmap cluster-color bar ; Hi,. I would like to set the scatterplot color palette and the heatmap cluster-color bar palette to be the same so that the clusters in the scatterplot are the same as those in the heatmap cluster-color-bar. I am using matplotlib to plot a scatterplot (using some other embedding) and need the cluster colors to match those in the automatic colorbar on the heatmap clusterbar. If i globally set the palette using sc.set_figure_params() then the values of the heatmap plot also become the global cmap (rather than a gradient of red-blue or existing default). Thanks for your help. Shobi",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/521
https://github.com/scverse/scanpy/issues/521:209,energy efficiency,heat,heatmap,209,"heatmap cluster-color bar ; Hi,. I would like to set the scatterplot color palette and the heatmap cluster-color bar palette to be the same so that the clusters in the scatterplot are the same as those in the heatmap cluster-color-bar. I am using matplotlib to plot a scatterplot (using some other embedding) and need the cluster colors to match those in the automatic colorbar on the heatmap clusterbar. If i globally set the palette using sc.set_figure_params() then the values of the heatmap plot also become the global cmap (rather than a gradient of red-blue or existing default). Thanks for your help. Shobi",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/521
https://github.com/scverse/scanpy/issues/521:385,energy efficiency,heat,heatmap,385,"heatmap cluster-color bar ; Hi,. I would like to set the scatterplot color palette and the heatmap cluster-color bar palette to be the same so that the clusters in the scatterplot are the same as those in the heatmap cluster-color-bar. I am using matplotlib to plot a scatterplot (using some other embedding) and need the cluster colors to match those in the automatic colorbar on the heatmap clusterbar. If i globally set the palette using sc.set_figure_params() then the values of the heatmap plot also become the global cmap (rather than a gradient of red-blue or existing default). Thanks for your help. Shobi",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/521
https://github.com/scverse/scanpy/issues/521:487,energy efficiency,heat,heatmap,487,"heatmap cluster-color bar ; Hi,. I would like to set the scatterplot color palette and the heatmap cluster-color bar palette to be the same so that the clusters in the scatterplot are the same as those in the heatmap cluster-color-bar. I am using matplotlib to plot a scatterplot (using some other embedding) and need the cluster colors to match those in the automatic colorbar on the heatmap clusterbar. If i globally set the palette using sc.set_figure_params() then the values of the heatmap plot also become the global cmap (rather than a gradient of red-blue or existing default). Thanks for your help. Shobi",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/521
https://github.com/scverse/scanpy/issues/521:359,testability,automat,automatic,359,"heatmap cluster-color bar ; Hi,. I would like to set the scatterplot color palette and the heatmap cluster-color bar palette to be the same so that the clusters in the scatterplot are the same as those in the heatmap cluster-color-bar. I am using matplotlib to plot a scatterplot (using some other embedding) and need the cluster colors to match those in the automatic colorbar on the heatmap clusterbar. If i globally set the palette using sc.set_figure_params() then the values of the heatmap plot also become the global cmap (rather than a gradient of red-blue or existing default). Thanks for your help. Shobi",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/521
https://github.com/scverse/scanpy/issues/521:602,usability,help,help,602,"heatmap cluster-color bar ; Hi,. I would like to set the scatterplot color palette and the heatmap cluster-color bar palette to be the same so that the clusters in the scatterplot are the same as those in the heatmap cluster-color-bar. I am using matplotlib to plot a scatterplot (using some other embedding) and need the cluster colors to match those in the automatic colorbar on the heatmap clusterbar. If i globally set the palette using sc.set_figure_params() then the values of the heatmap plot also become the global cmap (rather than a gradient of red-blue or existing default). Thanks for your help. Shobi",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/521
https://github.com/scverse/scanpy/issues/522:1096,availability,avail,available,1096," copy, I introduced a year ago for various reasons (see below). Meanwhile, the following two functions can maybe direcly imported from UMAP, if not, we could make a PR there or use pynndescent? https://github.com/theislab/scanpy/blob/07606455c524b38c4efec475a0d7ba87251754bb/scanpy/neighbors/__init__.py#L105. https://github.com/theislab/scanpy/blob/07606455c524b38c4efec475a0d7ba87251754bb/scanpy/neighbors/__init__.py#L258. There is a chance that this can also be solved with an import from UMAP. https://github.com/theislab/scanpy/blob/07606455c524b38c4efec475a0d7ba87251754bb/scanpy/tools/_umap.py#L107. As just discussed, @Koncopd, can you look into this and make a PR that gets rid of the umap legacy code? Thank you so much! Alex. PS: Just wrote an explanation for the reasons why I intorduced the duplicated code in the first place. > The duplicated code in Scanpy came about as I wanted to very quickly move forward with a version 1.0 of Scanpy about a year ago. UMAP was just becoming available on GitHub and there wasn’t even a preprint, I think. It changed very quickly and there were dramatic bugs every now and then. Nonetheless it was clear that it’s a major improvement over existing solutions, both in terms of computational performance, quality of the result and ease of installation and use. I wanted to achieve two things: (i) I had to rewrite some parts of UMAP so that I could decompose it a neighbors computing and a dedicated embedding step; you know that in Scanpy, the neighborhood graph is used for many other things other than for the embedding (clustering and trajectory inference). I also added the Gaussian kernel solution that I had before switching to a “UMAP backend” for `pp.neighbors`; which was needed so that results for DPT could be reproduced. All of this would have been quite a discussion with Leland. Until we would have had settled on the “Scanpy needs” that certainly weren’t aligned with the development of an independent young package, PRs would have be",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/522
https://github.com/scverse/scanpy/issues/522:1675,availability,cluster,clustering,1675,"7ba87251754bb/scanpy/neighbors/__init__.py#L105. https://github.com/theislab/scanpy/blob/07606455c524b38c4efec475a0d7ba87251754bb/scanpy/neighbors/__init__.py#L258. There is a chance that this can also be solved with an import from UMAP. https://github.com/theislab/scanpy/blob/07606455c524b38c4efec475a0d7ba87251754bb/scanpy/tools/_umap.py#L107. As just discussed, @Koncopd, can you look into this and make a PR that gets rid of the umap legacy code? Thank you so much! Alex. PS: Just wrote an explanation for the reasons why I intorduced the duplicated code in the first place. > The duplicated code in Scanpy came about as I wanted to very quickly move forward with a version 1.0 of Scanpy about a year ago. UMAP was just becoming available on GitHub and there wasn’t even a preprint, I think. It changed very quickly and there were dramatic bugs every now and then. Nonetheless it was clear that it’s a major improvement over existing solutions, both in terms of computational performance, quality of the result and ease of installation and use. I wanted to achieve two things: (i) I had to rewrite some parts of UMAP so that I could decompose it a neighbors computing and a dedicated embedding step; you know that in Scanpy, the neighborhood graph is used for many other things other than for the embedding (clustering and trajectory inference). I also added the Gaussian kernel solution that I had before switching to a “UMAP backend” for `pp.neighbors`; which was needed so that results for DPT could be reproduced. All of this would have been quite a discussion with Leland. Until we would have had settled on the “Scanpy needs” that certainly weren’t aligned with the development of an independent young package, PRs would have been integrated to much time would have been lost. Finally, I wanted absolute reproducibility for Scanpy users, which could only be achieved by “freezing the code”. So, I asked Leland whether he is OK if I add a frozen version of umap as an intermediate solution.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/522
https://github.com/scverse/scanpy/issues/522:1033,deployability,version,version,1033,"that UMAP is mature, we can get rid of the temporary frozen umap copy, I introduced a year ago for various reasons (see below). Meanwhile, the following two functions can maybe direcly imported from UMAP, if not, we could make a PR there or use pynndescent? https://github.com/theislab/scanpy/blob/07606455c524b38c4efec475a0d7ba87251754bb/scanpy/neighbors/__init__.py#L105. https://github.com/theislab/scanpy/blob/07606455c524b38c4efec475a0d7ba87251754bb/scanpy/neighbors/__init__.py#L258. There is a chance that this can also be solved with an import from UMAP. https://github.com/theislab/scanpy/blob/07606455c524b38c4efec475a0d7ba87251754bb/scanpy/tools/_umap.py#L107. As just discussed, @Koncopd, can you look into this and make a PR that gets rid of the umap legacy code? Thank you so much! Alex. PS: Just wrote an explanation for the reasons why I intorduced the duplicated code in the first place. > The duplicated code in Scanpy came about as I wanted to very quickly move forward with a version 1.0 of Scanpy about a year ago. UMAP was just becoming available on GitHub and there wasn’t even a preprint, I think. It changed very quickly and there were dramatic bugs every now and then. Nonetheless it was clear that it’s a major improvement over existing solutions, both in terms of computational performance, quality of the result and ease of installation and use. I wanted to achieve two things: (i) I had to rewrite some parts of UMAP so that I could decompose it a neighbors computing and a dedicated embedding step; you know that in Scanpy, the neighborhood graph is used for many other things other than for the embedding (clustering and trajectory inference). I also added the Gaussian kernel solution that I had before switching to a “UMAP backend” for `pp.neighbors`; which was needed so that results for DPT could be reproduced. All of this would have been quite a discussion with Leland. Until we would have had settled on the “Scanpy needs” that certainly weren’t aligned with th",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/522
https://github.com/scverse/scanpy/issues/522:1390,deployability,instal,installation,1390,"7ba87251754bb/scanpy/neighbors/__init__.py#L105. https://github.com/theislab/scanpy/blob/07606455c524b38c4efec475a0d7ba87251754bb/scanpy/neighbors/__init__.py#L258. There is a chance that this can also be solved with an import from UMAP. https://github.com/theislab/scanpy/blob/07606455c524b38c4efec475a0d7ba87251754bb/scanpy/tools/_umap.py#L107. As just discussed, @Koncopd, can you look into this and make a PR that gets rid of the umap legacy code? Thank you so much! Alex. PS: Just wrote an explanation for the reasons why I intorduced the duplicated code in the first place. > The duplicated code in Scanpy came about as I wanted to very quickly move forward with a version 1.0 of Scanpy about a year ago. UMAP was just becoming available on GitHub and there wasn’t even a preprint, I think. It changed very quickly and there were dramatic bugs every now and then. Nonetheless it was clear that it’s a major improvement over existing solutions, both in terms of computational performance, quality of the result and ease of installation and use. I wanted to achieve two things: (i) I had to rewrite some parts of UMAP so that I could decompose it a neighbors computing and a dedicated embedding step; you know that in Scanpy, the neighborhood graph is used for many other things other than for the embedding (clustering and trajectory inference). I also added the Gaussian kernel solution that I had before switching to a “UMAP backend” for `pp.neighbors`; which was needed so that results for DPT could be reproduced. All of this would have been quite a discussion with Leland. Until we would have had settled on the “Scanpy needs” that certainly weren’t aligned with the development of an independent young package, PRs would have been integrated to much time would have been lost. Finally, I wanted absolute reproducibility for Scanpy users, which could only be achieved by “freezing the code”. So, I asked Leland whether he is OK if I add a frozen version of umap as an intermediate solution.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/522
https://github.com/scverse/scanpy/issues/522:1675,deployability,cluster,clustering,1675,"7ba87251754bb/scanpy/neighbors/__init__.py#L105. https://github.com/theislab/scanpy/blob/07606455c524b38c4efec475a0d7ba87251754bb/scanpy/neighbors/__init__.py#L258. There is a chance that this can also be solved with an import from UMAP. https://github.com/theislab/scanpy/blob/07606455c524b38c4efec475a0d7ba87251754bb/scanpy/tools/_umap.py#L107. As just discussed, @Koncopd, can you look into this and make a PR that gets rid of the umap legacy code? Thank you so much! Alex. PS: Just wrote an explanation for the reasons why I intorduced the duplicated code in the first place. > The duplicated code in Scanpy came about as I wanted to very quickly move forward with a version 1.0 of Scanpy about a year ago. UMAP was just becoming available on GitHub and there wasn’t even a preprint, I think. It changed very quickly and there were dramatic bugs every now and then. Nonetheless it was clear that it’s a major improvement over existing solutions, both in terms of computational performance, quality of the result and ease of installation and use. I wanted to achieve two things: (i) I had to rewrite some parts of UMAP so that I could decompose it a neighbors computing and a dedicated embedding step; you know that in Scanpy, the neighborhood graph is used for many other things other than for the embedding (clustering and trajectory inference). I also added the Gaussian kernel solution that I had before switching to a “UMAP backend” for `pp.neighbors`; which was needed so that results for DPT could be reproduced. All of this would have been quite a discussion with Leland. Until we would have had settled on the “Scanpy needs” that certainly weren’t aligned with the development of an independent young package, PRs would have been integrated to much time would have been lost. Finally, I wanted absolute reproducibility for Scanpy users, which could only be achieved by “freezing the code”. So, I asked Leland whether he is OK if I add a frozen version of umap as an intermediate solution.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/522
https://github.com/scverse/scanpy/issues/522:2104,deployability,integr,integrated,2104,"7ba87251754bb/scanpy/neighbors/__init__.py#L105. https://github.com/theislab/scanpy/blob/07606455c524b38c4efec475a0d7ba87251754bb/scanpy/neighbors/__init__.py#L258. There is a chance that this can also be solved with an import from UMAP. https://github.com/theislab/scanpy/blob/07606455c524b38c4efec475a0d7ba87251754bb/scanpy/tools/_umap.py#L107. As just discussed, @Koncopd, can you look into this and make a PR that gets rid of the umap legacy code? Thank you so much! Alex. PS: Just wrote an explanation for the reasons why I intorduced the duplicated code in the first place. > The duplicated code in Scanpy came about as I wanted to very quickly move forward with a version 1.0 of Scanpy about a year ago. UMAP was just becoming available on GitHub and there wasn’t even a preprint, I think. It changed very quickly and there were dramatic bugs every now and then. Nonetheless it was clear that it’s a major improvement over existing solutions, both in terms of computational performance, quality of the result and ease of installation and use. I wanted to achieve two things: (i) I had to rewrite some parts of UMAP so that I could decompose it a neighbors computing and a dedicated embedding step; you know that in Scanpy, the neighborhood graph is used for many other things other than for the embedding (clustering and trajectory inference). I also added the Gaussian kernel solution that I had before switching to a “UMAP backend” for `pp.neighbors`; which was needed so that results for DPT could be reproduced. All of this would have been quite a discussion with Leland. Until we would have had settled on the “Scanpy needs” that certainly weren’t aligned with the development of an independent young package, PRs would have been integrated to much time would have been lost. Finally, I wanted absolute reproducibility for Scanpy users, which could only be achieved by “freezing the code”. So, I asked Leland whether he is OK if I add a frozen version of umap as an intermediate solution.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/522
https://github.com/scverse/scanpy/issues/522:2318,deployability,version,version,2318,"7ba87251754bb/scanpy/neighbors/__init__.py#L105. https://github.com/theislab/scanpy/blob/07606455c524b38c4efec475a0d7ba87251754bb/scanpy/neighbors/__init__.py#L258. There is a chance that this can also be solved with an import from UMAP. https://github.com/theislab/scanpy/blob/07606455c524b38c4efec475a0d7ba87251754bb/scanpy/tools/_umap.py#L107. As just discussed, @Koncopd, can you look into this and make a PR that gets rid of the umap legacy code? Thank you so much! Alex. PS: Just wrote an explanation for the reasons why I intorduced the duplicated code in the first place. > The duplicated code in Scanpy came about as I wanted to very quickly move forward with a version 1.0 of Scanpy about a year ago. UMAP was just becoming available on GitHub and there wasn’t even a preprint, I think. It changed very quickly and there were dramatic bugs every now and then. Nonetheless it was clear that it’s a major improvement over existing solutions, both in terms of computational performance, quality of the result and ease of installation and use. I wanted to achieve two things: (i) I had to rewrite some parts of UMAP so that I could decompose it a neighbors computing and a dedicated embedding step; you know that in Scanpy, the neighborhood graph is used for many other things other than for the embedding (clustering and trajectory inference). I also added the Gaussian kernel solution that I had before switching to a “UMAP backend” for `pp.neighbors`; which was needed so that results for DPT could be reproduced. All of this would have been quite a discussion with Leland. Until we would have had settled on the “Scanpy needs” that certainly weren’t aligned with the development of an independent young package, PRs would have been integrated to much time would have been lost. Finally, I wanted absolute reproducibility for Scanpy users, which could only be achieved by “freezing the code”. So, I asked Leland whether he is OK if I add a frozen version of umap as an intermediate solution.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/522
https://github.com/scverse/scanpy/issues/522:1033,integrability,version,version,1033,"that UMAP is mature, we can get rid of the temporary frozen umap copy, I introduced a year ago for various reasons (see below). Meanwhile, the following two functions can maybe direcly imported from UMAP, if not, we could make a PR there or use pynndescent? https://github.com/theislab/scanpy/blob/07606455c524b38c4efec475a0d7ba87251754bb/scanpy/neighbors/__init__.py#L105. https://github.com/theislab/scanpy/blob/07606455c524b38c4efec475a0d7ba87251754bb/scanpy/neighbors/__init__.py#L258. There is a chance that this can also be solved with an import from UMAP. https://github.com/theislab/scanpy/blob/07606455c524b38c4efec475a0d7ba87251754bb/scanpy/tools/_umap.py#L107. As just discussed, @Koncopd, can you look into this and make a PR that gets rid of the umap legacy code? Thank you so much! Alex. PS: Just wrote an explanation for the reasons why I intorduced the duplicated code in the first place. > The duplicated code in Scanpy came about as I wanted to very quickly move forward with a version 1.0 of Scanpy about a year ago. UMAP was just becoming available on GitHub and there wasn’t even a preprint, I think. It changed very quickly and there were dramatic bugs every now and then. Nonetheless it was clear that it’s a major improvement over existing solutions, both in terms of computational performance, quality of the result and ease of installation and use. I wanted to achieve two things: (i) I had to rewrite some parts of UMAP so that I could decompose it a neighbors computing and a dedicated embedding step; you know that in Scanpy, the neighborhood graph is used for many other things other than for the embedding (clustering and trajectory inference). I also added the Gaussian kernel solution that I had before switching to a “UMAP backend” for `pp.neighbors`; which was needed so that results for DPT could be reproduced. All of this would have been quite a discussion with Leland. Until we would have had settled on the “Scanpy needs” that certainly weren’t aligned with th",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/522
https://github.com/scverse/scanpy/issues/522:2104,integrability,integr,integrated,2104,"7ba87251754bb/scanpy/neighbors/__init__.py#L105. https://github.com/theislab/scanpy/blob/07606455c524b38c4efec475a0d7ba87251754bb/scanpy/neighbors/__init__.py#L258. There is a chance that this can also be solved with an import from UMAP. https://github.com/theislab/scanpy/blob/07606455c524b38c4efec475a0d7ba87251754bb/scanpy/tools/_umap.py#L107. As just discussed, @Koncopd, can you look into this and make a PR that gets rid of the umap legacy code? Thank you so much! Alex. PS: Just wrote an explanation for the reasons why I intorduced the duplicated code in the first place. > The duplicated code in Scanpy came about as I wanted to very quickly move forward with a version 1.0 of Scanpy about a year ago. UMAP was just becoming available on GitHub and there wasn’t even a preprint, I think. It changed very quickly and there were dramatic bugs every now and then. Nonetheless it was clear that it’s a major improvement over existing solutions, both in terms of computational performance, quality of the result and ease of installation and use. I wanted to achieve two things: (i) I had to rewrite some parts of UMAP so that I could decompose it a neighbors computing and a dedicated embedding step; you know that in Scanpy, the neighborhood graph is used for many other things other than for the embedding (clustering and trajectory inference). I also added the Gaussian kernel solution that I had before switching to a “UMAP backend” for `pp.neighbors`; which was needed so that results for DPT could be reproduced. All of this would have been quite a discussion with Leland. Until we would have had settled on the “Scanpy needs” that certainly weren’t aligned with the development of an independent young package, PRs would have been integrated to much time would have been lost. Finally, I wanted absolute reproducibility for Scanpy users, which could only be achieved by “freezing the code”. So, I asked Leland whether he is OK if I add a frozen version of umap as an intermediate solution.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/522
https://github.com/scverse/scanpy/issues/522:2318,integrability,version,version,2318,"7ba87251754bb/scanpy/neighbors/__init__.py#L105. https://github.com/theislab/scanpy/blob/07606455c524b38c4efec475a0d7ba87251754bb/scanpy/neighbors/__init__.py#L258. There is a chance that this can also be solved with an import from UMAP. https://github.com/theislab/scanpy/blob/07606455c524b38c4efec475a0d7ba87251754bb/scanpy/tools/_umap.py#L107. As just discussed, @Koncopd, can you look into this and make a PR that gets rid of the umap legacy code? Thank you so much! Alex. PS: Just wrote an explanation for the reasons why I intorduced the duplicated code in the first place. > The duplicated code in Scanpy came about as I wanted to very quickly move forward with a version 1.0 of Scanpy about a year ago. UMAP was just becoming available on GitHub and there wasn’t even a preprint, I think. It changed very quickly and there were dramatic bugs every now and then. Nonetheless it was clear that it’s a major improvement over existing solutions, both in terms of computational performance, quality of the result and ease of installation and use. I wanted to achieve two things: (i) I had to rewrite some parts of UMAP so that I could decompose it a neighbors computing and a dedicated embedding step; you know that in Scanpy, the neighborhood graph is used for many other things other than for the embedding (clustering and trajectory inference). I also added the Gaussian kernel solution that I had before switching to a “UMAP backend” for `pp.neighbors`; which was needed so that results for DPT could be reproduced. All of this would have been quite a discussion with Leland. Until we would have had settled on the “Scanpy needs” that certainly weren’t aligned with the development of an independent young package, PRs would have been integrated to much time would have been lost. Finally, I wanted absolute reproducibility for Scanpy users, which could only be achieved by “freezing the code”. So, I asked Leland whether he is OK if I add a frozen version of umap as an intermediate solution.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/522
https://github.com/scverse/scanpy/issues/522:2104,interoperability,integr,integrated,2104,"7ba87251754bb/scanpy/neighbors/__init__.py#L105. https://github.com/theislab/scanpy/blob/07606455c524b38c4efec475a0d7ba87251754bb/scanpy/neighbors/__init__.py#L258. There is a chance that this can also be solved with an import from UMAP. https://github.com/theislab/scanpy/blob/07606455c524b38c4efec475a0d7ba87251754bb/scanpy/tools/_umap.py#L107. As just discussed, @Koncopd, can you look into this and make a PR that gets rid of the umap legacy code? Thank you so much! Alex. PS: Just wrote an explanation for the reasons why I intorduced the duplicated code in the first place. > The duplicated code in Scanpy came about as I wanted to very quickly move forward with a version 1.0 of Scanpy about a year ago. UMAP was just becoming available on GitHub and there wasn’t even a preprint, I think. It changed very quickly and there were dramatic bugs every now and then. Nonetheless it was clear that it’s a major improvement over existing solutions, both in terms of computational performance, quality of the result and ease of installation and use. I wanted to achieve two things: (i) I had to rewrite some parts of UMAP so that I could decompose it a neighbors computing and a dedicated embedding step; you know that in Scanpy, the neighborhood graph is used for many other things other than for the embedding (clustering and trajectory inference). I also added the Gaussian kernel solution that I had before switching to a “UMAP backend” for `pp.neighbors`; which was needed so that results for DPT could be reproduced. All of this would have been quite a discussion with Leland. Until we would have had settled on the “Scanpy needs” that certainly weren’t aligned with the development of an independent young package, PRs would have been integrated to much time would have been lost. Finally, I wanted absolute reproducibility for Scanpy users, which could only be achieved by “freezing the code”. So, I asked Leland whether he is OK if I add a frozen version of umap as an intermediate solution.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/522
https://github.com/scverse/scanpy/issues/522:1033,modifiability,version,version,1033,"that UMAP is mature, we can get rid of the temporary frozen umap copy, I introduced a year ago for various reasons (see below). Meanwhile, the following two functions can maybe direcly imported from UMAP, if not, we could make a PR there or use pynndescent? https://github.com/theislab/scanpy/blob/07606455c524b38c4efec475a0d7ba87251754bb/scanpy/neighbors/__init__.py#L105. https://github.com/theislab/scanpy/blob/07606455c524b38c4efec475a0d7ba87251754bb/scanpy/neighbors/__init__.py#L258. There is a chance that this can also be solved with an import from UMAP. https://github.com/theislab/scanpy/blob/07606455c524b38c4efec475a0d7ba87251754bb/scanpy/tools/_umap.py#L107. As just discussed, @Koncopd, can you look into this and make a PR that gets rid of the umap legacy code? Thank you so much! Alex. PS: Just wrote an explanation for the reasons why I intorduced the duplicated code in the first place. > The duplicated code in Scanpy came about as I wanted to very quickly move forward with a version 1.0 of Scanpy about a year ago. UMAP was just becoming available on GitHub and there wasn’t even a preprint, I think. It changed very quickly and there were dramatic bugs every now and then. Nonetheless it was clear that it’s a major improvement over existing solutions, both in terms of computational performance, quality of the result and ease of installation and use. I wanted to achieve two things: (i) I had to rewrite some parts of UMAP so that I could decompose it a neighbors computing and a dedicated embedding step; you know that in Scanpy, the neighborhood graph is used for many other things other than for the embedding (clustering and trajectory inference). I also added the Gaussian kernel solution that I had before switching to a “UMAP backend” for `pp.neighbors`; which was needed so that results for DPT could be reproduced. All of this would have been quite a discussion with Leland. Until we would have had settled on the “Scanpy needs” that certainly weren’t aligned with th",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/522
https://github.com/scverse/scanpy/issues/522:1500,modifiability,deco,decompose,1500,"7ba87251754bb/scanpy/neighbors/__init__.py#L105. https://github.com/theislab/scanpy/blob/07606455c524b38c4efec475a0d7ba87251754bb/scanpy/neighbors/__init__.py#L258. There is a chance that this can also be solved with an import from UMAP. https://github.com/theislab/scanpy/blob/07606455c524b38c4efec475a0d7ba87251754bb/scanpy/tools/_umap.py#L107. As just discussed, @Koncopd, can you look into this and make a PR that gets rid of the umap legacy code? Thank you so much! Alex. PS: Just wrote an explanation for the reasons why I intorduced the duplicated code in the first place. > The duplicated code in Scanpy came about as I wanted to very quickly move forward with a version 1.0 of Scanpy about a year ago. UMAP was just becoming available on GitHub and there wasn’t even a preprint, I think. It changed very quickly and there were dramatic bugs every now and then. Nonetheless it was clear that it’s a major improvement over existing solutions, both in terms of computational performance, quality of the result and ease of installation and use. I wanted to achieve two things: (i) I had to rewrite some parts of UMAP so that I could decompose it a neighbors computing and a dedicated embedding step; you know that in Scanpy, the neighborhood graph is used for many other things other than for the embedding (clustering and trajectory inference). I also added the Gaussian kernel solution that I had before switching to a “UMAP backend” for `pp.neighbors`; which was needed so that results for DPT could be reproduced. All of this would have been quite a discussion with Leland. Until we would have had settled on the “Scanpy needs” that certainly weren’t aligned with the development of an independent young package, PRs would have been integrated to much time would have been lost. Finally, I wanted absolute reproducibility for Scanpy users, which could only be achieved by “freezing the code”. So, I asked Leland whether he is OK if I add a frozen version of umap as an intermediate solution.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/522
https://github.com/scverse/scanpy/issues/522:2075,modifiability,pac,package,2075,"7ba87251754bb/scanpy/neighbors/__init__.py#L105. https://github.com/theislab/scanpy/blob/07606455c524b38c4efec475a0d7ba87251754bb/scanpy/neighbors/__init__.py#L258. There is a chance that this can also be solved with an import from UMAP. https://github.com/theislab/scanpy/blob/07606455c524b38c4efec475a0d7ba87251754bb/scanpy/tools/_umap.py#L107. As just discussed, @Koncopd, can you look into this and make a PR that gets rid of the umap legacy code? Thank you so much! Alex. PS: Just wrote an explanation for the reasons why I intorduced the duplicated code in the first place. > The duplicated code in Scanpy came about as I wanted to very quickly move forward with a version 1.0 of Scanpy about a year ago. UMAP was just becoming available on GitHub and there wasn’t even a preprint, I think. It changed very quickly and there were dramatic bugs every now and then. Nonetheless it was clear that it’s a major improvement over existing solutions, both in terms of computational performance, quality of the result and ease of installation and use. I wanted to achieve two things: (i) I had to rewrite some parts of UMAP so that I could decompose it a neighbors computing and a dedicated embedding step; you know that in Scanpy, the neighborhood graph is used for many other things other than for the embedding (clustering and trajectory inference). I also added the Gaussian kernel solution that I had before switching to a “UMAP backend” for `pp.neighbors`; which was needed so that results for DPT could be reproduced. All of this would have been quite a discussion with Leland. Until we would have had settled on the “Scanpy needs” that certainly weren’t aligned with the development of an independent young package, PRs would have been integrated to much time would have been lost. Finally, I wanted absolute reproducibility for Scanpy users, which could only be achieved by “freezing the code”. So, I asked Leland whether he is OK if I add a frozen version of umap as an intermediate solution.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/522
https://github.com/scverse/scanpy/issues/522:2104,modifiability,integr,integrated,2104,"7ba87251754bb/scanpy/neighbors/__init__.py#L105. https://github.com/theislab/scanpy/blob/07606455c524b38c4efec475a0d7ba87251754bb/scanpy/neighbors/__init__.py#L258. There is a chance that this can also be solved with an import from UMAP. https://github.com/theislab/scanpy/blob/07606455c524b38c4efec475a0d7ba87251754bb/scanpy/tools/_umap.py#L107. As just discussed, @Koncopd, can you look into this and make a PR that gets rid of the umap legacy code? Thank you so much! Alex. PS: Just wrote an explanation for the reasons why I intorduced the duplicated code in the first place. > The duplicated code in Scanpy came about as I wanted to very quickly move forward with a version 1.0 of Scanpy about a year ago. UMAP was just becoming available on GitHub and there wasn’t even a preprint, I think. It changed very quickly and there were dramatic bugs every now and then. Nonetheless it was clear that it’s a major improvement over existing solutions, both in terms of computational performance, quality of the result and ease of installation and use. I wanted to achieve two things: (i) I had to rewrite some parts of UMAP so that I could decompose it a neighbors computing and a dedicated embedding step; you know that in Scanpy, the neighborhood graph is used for many other things other than for the embedding (clustering and trajectory inference). I also added the Gaussian kernel solution that I had before switching to a “UMAP backend” for `pp.neighbors`; which was needed so that results for DPT could be reproduced. All of this would have been quite a discussion with Leland. Until we would have had settled on the “Scanpy needs” that certainly weren’t aligned with the development of an independent young package, PRs would have been integrated to much time would have been lost. Finally, I wanted absolute reproducibility for Scanpy users, which could only be achieved by “freezing the code”. So, I asked Leland whether he is OK if I add a frozen version of umap as an intermediate solution.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/522
https://github.com/scverse/scanpy/issues/522:2318,modifiability,version,version,2318,"7ba87251754bb/scanpy/neighbors/__init__.py#L105. https://github.com/theislab/scanpy/blob/07606455c524b38c4efec475a0d7ba87251754bb/scanpy/neighbors/__init__.py#L258. There is a chance that this can also be solved with an import from UMAP. https://github.com/theislab/scanpy/blob/07606455c524b38c4efec475a0d7ba87251754bb/scanpy/tools/_umap.py#L107. As just discussed, @Koncopd, can you look into this and make a PR that gets rid of the umap legacy code? Thank you so much! Alex. PS: Just wrote an explanation for the reasons why I intorduced the duplicated code in the first place. > The duplicated code in Scanpy came about as I wanted to very quickly move forward with a version 1.0 of Scanpy about a year ago. UMAP was just becoming available on GitHub and there wasn’t even a preprint, I think. It changed very quickly and there were dramatic bugs every now and then. Nonetheless it was clear that it’s a major improvement over existing solutions, both in terms of computational performance, quality of the result and ease of installation and use. I wanted to achieve two things: (i) I had to rewrite some parts of UMAP so that I could decompose it a neighbors computing and a dedicated embedding step; you know that in Scanpy, the neighborhood graph is used for many other things other than for the embedding (clustering and trajectory inference). I also added the Gaussian kernel solution that I had before switching to a “UMAP backend” for `pp.neighbors`; which was needed so that results for DPT could be reproduced. All of this would have been quite a discussion with Leland. Until we would have had settled on the “Scanpy needs” that certainly weren’t aligned with the development of an independent young package, PRs would have been integrated to much time would have been lost. Finally, I wanted absolute reproducibility for Scanpy users, which could only be achieved by “freezing the code”. So, I asked Leland whether he is OK if I add a frozen version of umap as an intermediate solution.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/522
https://github.com/scverse/scanpy/issues/522:2340,modifiability,interm,intermediate,2340,"7ba87251754bb/scanpy/neighbors/__init__.py#L105. https://github.com/theislab/scanpy/blob/07606455c524b38c4efec475a0d7ba87251754bb/scanpy/neighbors/__init__.py#L258. There is a chance that this can also be solved with an import from UMAP. https://github.com/theislab/scanpy/blob/07606455c524b38c4efec475a0d7ba87251754bb/scanpy/tools/_umap.py#L107. As just discussed, @Koncopd, can you look into this and make a PR that gets rid of the umap legacy code? Thank you so much! Alex. PS: Just wrote an explanation for the reasons why I intorduced the duplicated code in the first place. > The duplicated code in Scanpy came about as I wanted to very quickly move forward with a version 1.0 of Scanpy about a year ago. UMAP was just becoming available on GitHub and there wasn’t even a preprint, I think. It changed very quickly and there were dramatic bugs every now and then. Nonetheless it was clear that it’s a major improvement over existing solutions, both in terms of computational performance, quality of the result and ease of installation and use. I wanted to achieve two things: (i) I had to rewrite some parts of UMAP so that I could decompose it a neighbors computing and a dedicated embedding step; you know that in Scanpy, the neighborhood graph is used for many other things other than for the embedding (clustering and trajectory inference). I also added the Gaussian kernel solution that I had before switching to a “UMAP backend” for `pp.neighbors`; which was needed so that results for DPT could be reproduced. All of this would have been quite a discussion with Leland. Until we would have had settled on the “Scanpy needs” that certainly weren’t aligned with the development of an independent young package, PRs would have been integrated to much time would have been lost. Finally, I wanted absolute reproducibility for Scanpy users, which could only be achieved by “freezing the code”. So, I asked Leland whether he is OK if I add a frozen version of umap as an intermediate solution.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/522
https://github.com/scverse/scanpy/issues/522:1343,performance,perform,performance,1343,"8c4efec475a0d7ba87251754bb/scanpy/neighbors/__init__.py#L105. https://github.com/theislab/scanpy/blob/07606455c524b38c4efec475a0d7ba87251754bb/scanpy/neighbors/__init__.py#L258. There is a chance that this can also be solved with an import from UMAP. https://github.com/theislab/scanpy/blob/07606455c524b38c4efec475a0d7ba87251754bb/scanpy/tools/_umap.py#L107. As just discussed, @Koncopd, can you look into this and make a PR that gets rid of the umap legacy code? Thank you so much! Alex. PS: Just wrote an explanation for the reasons why I intorduced the duplicated code in the first place. > The duplicated code in Scanpy came about as I wanted to very quickly move forward with a version 1.0 of Scanpy about a year ago. UMAP was just becoming available on GitHub and there wasn’t even a preprint, I think. It changed very quickly and there were dramatic bugs every now and then. Nonetheless it was clear that it’s a major improvement over existing solutions, both in terms of computational performance, quality of the result and ease of installation and use. I wanted to achieve two things: (i) I had to rewrite some parts of UMAP so that I could decompose it a neighbors computing and a dedicated embedding step; you know that in Scanpy, the neighborhood graph is used for many other things other than for the embedding (clustering and trajectory inference). I also added the Gaussian kernel solution that I had before switching to a “UMAP backend” for `pp.neighbors`; which was needed so that results for DPT could be reproduced. All of this would have been quite a discussion with Leland. Until we would have had settled on the “Scanpy needs” that certainly weren’t aligned with the development of an independent young package, PRs would have been integrated to much time would have been lost. Finally, I wanted absolute reproducibility for Scanpy users, which could only be achieved by “freezing the code”. So, I asked Leland whether he is OK if I add a frozen version of umap as an intermedi",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/522
https://github.com/scverse/scanpy/issues/522:2123,performance,time,time,2123,"7ba87251754bb/scanpy/neighbors/__init__.py#L105. https://github.com/theislab/scanpy/blob/07606455c524b38c4efec475a0d7ba87251754bb/scanpy/neighbors/__init__.py#L258. There is a chance that this can also be solved with an import from UMAP. https://github.com/theislab/scanpy/blob/07606455c524b38c4efec475a0d7ba87251754bb/scanpy/tools/_umap.py#L107. As just discussed, @Koncopd, can you look into this and make a PR that gets rid of the umap legacy code? Thank you so much! Alex. PS: Just wrote an explanation for the reasons why I intorduced the duplicated code in the first place. > The duplicated code in Scanpy came about as I wanted to very quickly move forward with a version 1.0 of Scanpy about a year ago. UMAP was just becoming available on GitHub and there wasn’t even a preprint, I think. It changed very quickly and there were dramatic bugs every now and then. Nonetheless it was clear that it’s a major improvement over existing solutions, both in terms of computational performance, quality of the result and ease of installation and use. I wanted to achieve two things: (i) I had to rewrite some parts of UMAP so that I could decompose it a neighbors computing and a dedicated embedding step; you know that in Scanpy, the neighborhood graph is used for many other things other than for the embedding (clustering and trajectory inference). I also added the Gaussian kernel solution that I had before switching to a “UMAP backend” for `pp.neighbors`; which was needed so that results for DPT could be reproduced. All of this would have been quite a discussion with Leland. Until we would have had settled on the “Scanpy needs” that certainly weren’t aligned with the development of an independent young package, PRs would have been integrated to much time would have been lost. Finally, I wanted absolute reproducibility for Scanpy users, which could only be achieved by “freezing the code”. So, I asked Leland whether he is OK if I add a frozen version of umap as an intermediate solution.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/522
https://github.com/scverse/scanpy/issues/522:1096,reliability,availab,available,1096," copy, I introduced a year ago for various reasons (see below). Meanwhile, the following two functions can maybe direcly imported from UMAP, if not, we could make a PR there or use pynndescent? https://github.com/theislab/scanpy/blob/07606455c524b38c4efec475a0d7ba87251754bb/scanpy/neighbors/__init__.py#L105. https://github.com/theislab/scanpy/blob/07606455c524b38c4efec475a0d7ba87251754bb/scanpy/neighbors/__init__.py#L258. There is a chance that this can also be solved with an import from UMAP. https://github.com/theislab/scanpy/blob/07606455c524b38c4efec475a0d7ba87251754bb/scanpy/tools/_umap.py#L107. As just discussed, @Koncopd, can you look into this and make a PR that gets rid of the umap legacy code? Thank you so much! Alex. PS: Just wrote an explanation for the reasons why I intorduced the duplicated code in the first place. > The duplicated code in Scanpy came about as I wanted to very quickly move forward with a version 1.0 of Scanpy about a year ago. UMAP was just becoming available on GitHub and there wasn’t even a preprint, I think. It changed very quickly and there were dramatic bugs every now and then. Nonetheless it was clear that it’s a major improvement over existing solutions, both in terms of computational performance, quality of the result and ease of installation and use. I wanted to achieve two things: (i) I had to rewrite some parts of UMAP so that I could decompose it a neighbors computing and a dedicated embedding step; you know that in Scanpy, the neighborhood graph is used for many other things other than for the embedding (clustering and trajectory inference). I also added the Gaussian kernel solution that I had before switching to a “UMAP backend” for `pp.neighbors`; which was needed so that results for DPT could be reproduced. All of this would have been quite a discussion with Leland. Until we would have had settled on the “Scanpy needs” that certainly weren’t aligned with the development of an independent young package, PRs would have be",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/522
https://github.com/scverse/scanpy/issues/522:2104,reliability,integr,integrated,2104,"7ba87251754bb/scanpy/neighbors/__init__.py#L105. https://github.com/theislab/scanpy/blob/07606455c524b38c4efec475a0d7ba87251754bb/scanpy/neighbors/__init__.py#L258. There is a chance that this can also be solved with an import from UMAP. https://github.com/theislab/scanpy/blob/07606455c524b38c4efec475a0d7ba87251754bb/scanpy/tools/_umap.py#L107. As just discussed, @Koncopd, can you look into this and make a PR that gets rid of the umap legacy code? Thank you so much! Alex. PS: Just wrote an explanation for the reasons why I intorduced the duplicated code in the first place. > The duplicated code in Scanpy came about as I wanted to very quickly move forward with a version 1.0 of Scanpy about a year ago. UMAP was just becoming available on GitHub and there wasn’t even a preprint, I think. It changed very quickly and there were dramatic bugs every now and then. Nonetheless it was clear that it’s a major improvement over existing solutions, both in terms of computational performance, quality of the result and ease of installation and use. I wanted to achieve two things: (i) I had to rewrite some parts of UMAP so that I could decompose it a neighbors computing and a dedicated embedding step; you know that in Scanpy, the neighborhood graph is used for many other things other than for the embedding (clustering and trajectory inference). I also added the Gaussian kernel solution that I had before switching to a “UMAP backend” for `pp.neighbors`; which was needed so that results for DPT could be reproduced. All of this would have been quite a discussion with Leland. Until we would have had settled on the “Scanpy needs” that certainly weren’t aligned with the development of an independent young package, PRs would have been integrated to much time would have been lost. Finally, I wanted absolute reproducibility for Scanpy users, which could only be achieved by “freezing the code”. So, I asked Leland whether he is OK if I add a frozen version of umap as an intermediate solution.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/522
https://github.com/scverse/scanpy/issues/522:1096,safety,avail,available,1096," copy, I introduced a year ago for various reasons (see below). Meanwhile, the following two functions can maybe direcly imported from UMAP, if not, we could make a PR there or use pynndescent? https://github.com/theislab/scanpy/blob/07606455c524b38c4efec475a0d7ba87251754bb/scanpy/neighbors/__init__.py#L105. https://github.com/theislab/scanpy/blob/07606455c524b38c4efec475a0d7ba87251754bb/scanpy/neighbors/__init__.py#L258. There is a chance that this can also be solved with an import from UMAP. https://github.com/theislab/scanpy/blob/07606455c524b38c4efec475a0d7ba87251754bb/scanpy/tools/_umap.py#L107. As just discussed, @Koncopd, can you look into this and make a PR that gets rid of the umap legacy code? Thank you so much! Alex. PS: Just wrote an explanation for the reasons why I intorduced the duplicated code in the first place. > The duplicated code in Scanpy came about as I wanted to very quickly move forward with a version 1.0 of Scanpy about a year ago. UMAP was just becoming available on GitHub and there wasn’t even a preprint, I think. It changed very quickly and there were dramatic bugs every now and then. Nonetheless it was clear that it’s a major improvement over existing solutions, both in terms of computational performance, quality of the result and ease of installation and use. I wanted to achieve two things: (i) I had to rewrite some parts of UMAP so that I could decompose it a neighbors computing and a dedicated embedding step; you know that in Scanpy, the neighborhood graph is used for many other things other than for the embedding (clustering and trajectory inference). I also added the Gaussian kernel solution that I had before switching to a “UMAP backend” for `pp.neighbors`; which was needed so that results for DPT could be reproduced. All of this would have been quite a discussion with Leland. Until we would have had settled on the “Scanpy needs” that certainly weren’t aligned with the development of an independent young package, PRs would have be",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/522
https://github.com/scverse/scanpy/issues/522:1096,security,availab,available,1096," copy, I introduced a year ago for various reasons (see below). Meanwhile, the following two functions can maybe direcly imported from UMAP, if not, we could make a PR there or use pynndescent? https://github.com/theislab/scanpy/blob/07606455c524b38c4efec475a0d7ba87251754bb/scanpy/neighbors/__init__.py#L105. https://github.com/theislab/scanpy/blob/07606455c524b38c4efec475a0d7ba87251754bb/scanpy/neighbors/__init__.py#L258. There is a chance that this can also be solved with an import from UMAP. https://github.com/theislab/scanpy/blob/07606455c524b38c4efec475a0d7ba87251754bb/scanpy/tools/_umap.py#L107. As just discussed, @Koncopd, can you look into this and make a PR that gets rid of the umap legacy code? Thank you so much! Alex. PS: Just wrote an explanation for the reasons why I intorduced the duplicated code in the first place. > The duplicated code in Scanpy came about as I wanted to very quickly move forward with a version 1.0 of Scanpy about a year ago. UMAP was just becoming available on GitHub and there wasn’t even a preprint, I think. It changed very quickly and there were dramatic bugs every now and then. Nonetheless it was clear that it’s a major improvement over existing solutions, both in terms of computational performance, quality of the result and ease of installation and use. I wanted to achieve two things: (i) I had to rewrite some parts of UMAP so that I could decompose it a neighbors computing and a dedicated embedding step; you know that in Scanpy, the neighborhood graph is used for many other things other than for the embedding (clustering and trajectory inference). I also added the Gaussian kernel solution that I had before switching to a “UMAP backend” for `pp.neighbors`; which was needed so that results for DPT could be reproduced. All of this would have been quite a discussion with Leland. Until we would have had settled on the “Scanpy needs” that certainly weren’t aligned with the development of an independent young package, PRs would have be",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/522
https://github.com/scverse/scanpy/issues/522:2104,security,integr,integrated,2104,"7ba87251754bb/scanpy/neighbors/__init__.py#L105. https://github.com/theislab/scanpy/blob/07606455c524b38c4efec475a0d7ba87251754bb/scanpy/neighbors/__init__.py#L258. There is a chance that this can also be solved with an import from UMAP. https://github.com/theislab/scanpy/blob/07606455c524b38c4efec475a0d7ba87251754bb/scanpy/tools/_umap.py#L107. As just discussed, @Koncopd, can you look into this and make a PR that gets rid of the umap legacy code? Thank you so much! Alex. PS: Just wrote an explanation for the reasons why I intorduced the duplicated code in the first place. > The duplicated code in Scanpy came about as I wanted to very quickly move forward with a version 1.0 of Scanpy about a year ago. UMAP was just becoming available on GitHub and there wasn’t even a preprint, I think. It changed very quickly and there were dramatic bugs every now and then. Nonetheless it was clear that it’s a major improvement over existing solutions, both in terms of computational performance, quality of the result and ease of installation and use. I wanted to achieve two things: (i) I had to rewrite some parts of UMAP so that I could decompose it a neighbors computing and a dedicated embedding step; you know that in Scanpy, the neighborhood graph is used for many other things other than for the embedding (clustering and trajectory inference). I also added the Gaussian kernel solution that I had before switching to a “UMAP backend” for `pp.neighbors`; which was needed so that results for DPT could be reproduced. All of this would have been quite a discussion with Leland. Until we would have had settled on the “Scanpy needs” that certainly weren’t aligned with the development of an independent young package, PRs would have been integrated to much time would have been lost. Finally, I wanted absolute reproducibility for Scanpy users, which could only be achieved by “freezing the code”. So, I asked Leland whether he is OK if I add a frozen version of umap as an intermediate solution.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/522
https://github.com/scverse/scanpy/issues/522:2104,testability,integr,integrated,2104,"7ba87251754bb/scanpy/neighbors/__init__.py#L105. https://github.com/theislab/scanpy/blob/07606455c524b38c4efec475a0d7ba87251754bb/scanpy/neighbors/__init__.py#L258. There is a chance that this can also be solved with an import from UMAP. https://github.com/theislab/scanpy/blob/07606455c524b38c4efec475a0d7ba87251754bb/scanpy/tools/_umap.py#L107. As just discussed, @Koncopd, can you look into this and make a PR that gets rid of the umap legacy code? Thank you so much! Alex. PS: Just wrote an explanation for the reasons why I intorduced the duplicated code in the first place. > The duplicated code in Scanpy came about as I wanted to very quickly move forward with a version 1.0 of Scanpy about a year ago. UMAP was just becoming available on GitHub and there wasn’t even a preprint, I think. It changed very quickly and there were dramatic bugs every now and then. Nonetheless it was clear that it’s a major improvement over existing solutions, both in terms of computational performance, quality of the result and ease of installation and use. I wanted to achieve two things: (i) I had to rewrite some parts of UMAP so that I could decompose it a neighbors computing and a dedicated embedding step; you know that in Scanpy, the neighborhood graph is used for many other things other than for the embedding (clustering and trajectory inference). I also added the Gaussian kernel solution that I had before switching to a “UMAP backend” for `pp.neighbors`; which was needed so that results for DPT could be reproduced. All of this would have been quite a discussion with Leland. Until we would have had settled on the “Scanpy needs” that certainly weren’t aligned with the development of an independent young package, PRs would have been integrated to much time would have been lost. Finally, I wanted absolute reproducibility for Scanpy users, which could only be achieved by “freezing the code”. So, I asked Leland whether he is OK if I add a frozen version of umap as an intermediate solution.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/522
https://github.com/scverse/scanpy/issues/522:688,usability,tool,tools,688,"Getting rid of legacy umap code; Now that UMAP is mature, we can get rid of the temporary frozen umap copy, I introduced a year ago for various reasons (see below). Meanwhile, the following two functions can maybe direcly imported from UMAP, if not, we could make a PR there or use pynndescent? https://github.com/theislab/scanpy/blob/07606455c524b38c4efec475a0d7ba87251754bb/scanpy/neighbors/__init__.py#L105. https://github.com/theislab/scanpy/blob/07606455c524b38c4efec475a0d7ba87251754bb/scanpy/neighbors/__init__.py#L258. There is a chance that this can also be solved with an import from UMAP. https://github.com/theislab/scanpy/blob/07606455c524b38c4efec475a0d7ba87251754bb/scanpy/tools/_umap.py#L107. As just discussed, @Koncopd, can you look into this and make a PR that gets rid of the umap legacy code? Thank you so much! Alex. PS: Just wrote an explanation for the reasons why I intorduced the duplicated code in the first place. > The duplicated code in Scanpy came about as I wanted to very quickly move forward with a version 1.0 of Scanpy about a year ago. UMAP was just becoming available on GitHub and there wasn’t even a preprint, I think. It changed very quickly and there were dramatic bugs every now and then. Nonetheless it was clear that it’s a major improvement over existing solutions, both in terms of computational performance, quality of the result and ease of installation and use. I wanted to achieve two things: (i) I had to rewrite some parts of UMAP so that I could decompose it a neighbors computing and a dedicated embedding step; you know that in Scanpy, the neighborhood graph is used for many other things other than for the embedding (clustering and trajectory inference). I also added the Gaussian kernel solution that I had before switching to a “UMAP backend” for `pp.neighbors`; which was needed so that results for DPT could be reproduced. All of this would have been quite a discussion with Leland. Until we would have had settled on the “Scanpy needs” t",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/522
https://github.com/scverse/scanpy/issues/522:1251,usability,clear,clear,1251,"ould make a PR there or use pynndescent? https://github.com/theislab/scanpy/blob/07606455c524b38c4efec475a0d7ba87251754bb/scanpy/neighbors/__init__.py#L105. https://github.com/theislab/scanpy/blob/07606455c524b38c4efec475a0d7ba87251754bb/scanpy/neighbors/__init__.py#L258. There is a chance that this can also be solved with an import from UMAP. https://github.com/theislab/scanpy/blob/07606455c524b38c4efec475a0d7ba87251754bb/scanpy/tools/_umap.py#L107. As just discussed, @Koncopd, can you look into this and make a PR that gets rid of the umap legacy code? Thank you so much! Alex. PS: Just wrote an explanation for the reasons why I intorduced the duplicated code in the first place. > The duplicated code in Scanpy came about as I wanted to very quickly move forward with a version 1.0 of Scanpy about a year ago. UMAP was just becoming available on GitHub and there wasn’t even a preprint, I think. It changed very quickly and there were dramatic bugs every now and then. Nonetheless it was clear that it’s a major improvement over existing solutions, both in terms of computational performance, quality of the result and ease of installation and use. I wanted to achieve two things: (i) I had to rewrite some parts of UMAP so that I could decompose it a neighbors computing and a dedicated embedding step; you know that in Scanpy, the neighborhood graph is used for many other things other than for the embedding (clustering and trajectory inference). I also added the Gaussian kernel solution that I had before switching to a “UMAP backend” for `pp.neighbors`; which was needed so that results for DPT could be reproduced. All of this would have been quite a discussion with Leland. Until we would have had settled on the “Scanpy needs” that certainly weren’t aligned with the development of an independent young package, PRs would have been integrated to much time would have been lost. Finally, I wanted absolute reproducibility for Scanpy users, which could only be achieved by “freezing t",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/522
https://github.com/scverse/scanpy/issues/522:1343,usability,perform,performance,1343,"8c4efec475a0d7ba87251754bb/scanpy/neighbors/__init__.py#L105. https://github.com/theislab/scanpy/blob/07606455c524b38c4efec475a0d7ba87251754bb/scanpy/neighbors/__init__.py#L258. There is a chance that this can also be solved with an import from UMAP. https://github.com/theislab/scanpy/blob/07606455c524b38c4efec475a0d7ba87251754bb/scanpy/tools/_umap.py#L107. As just discussed, @Koncopd, can you look into this and make a PR that gets rid of the umap legacy code? Thank you so much! Alex. PS: Just wrote an explanation for the reasons why I intorduced the duplicated code in the first place. > The duplicated code in Scanpy came about as I wanted to very quickly move forward with a version 1.0 of Scanpy about a year ago. UMAP was just becoming available on GitHub and there wasn’t even a preprint, I think. It changed very quickly and there were dramatic bugs every now and then. Nonetheless it was clear that it’s a major improvement over existing solutions, both in terms of computational performance, quality of the result and ease of installation and use. I wanted to achieve two things: (i) I had to rewrite some parts of UMAP so that I could decompose it a neighbors computing and a dedicated embedding step; you know that in Scanpy, the neighborhood graph is used for many other things other than for the embedding (clustering and trajectory inference). I also added the Gaussian kernel solution that I had before switching to a “UMAP backend” for `pp.neighbors`; which was needed so that results for DPT could be reproduced. All of this would have been quite a discussion with Leland. Until we would have had settled on the “Scanpy needs” that certainly weren’t aligned with the development of an independent young package, PRs would have been integrated to much time would have been lost. Finally, I wanted absolute reproducibility for Scanpy users, which could only be achieved by “freezing the code”. So, I asked Leland whether he is OK if I add a frozen version of umap as an intermedi",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/522
https://github.com/scverse/scanpy/issues/522:2204,usability,user,users,2204,"7ba87251754bb/scanpy/neighbors/__init__.py#L105. https://github.com/theislab/scanpy/blob/07606455c524b38c4efec475a0d7ba87251754bb/scanpy/neighbors/__init__.py#L258. There is a chance that this can also be solved with an import from UMAP. https://github.com/theislab/scanpy/blob/07606455c524b38c4efec475a0d7ba87251754bb/scanpy/tools/_umap.py#L107. As just discussed, @Koncopd, can you look into this and make a PR that gets rid of the umap legacy code? Thank you so much! Alex. PS: Just wrote an explanation for the reasons why I intorduced the duplicated code in the first place. > The duplicated code in Scanpy came about as I wanted to very quickly move forward with a version 1.0 of Scanpy about a year ago. UMAP was just becoming available on GitHub and there wasn’t even a preprint, I think. It changed very quickly and there were dramatic bugs every now and then. Nonetheless it was clear that it’s a major improvement over existing solutions, both in terms of computational performance, quality of the result and ease of installation and use. I wanted to achieve two things: (i) I had to rewrite some parts of UMAP so that I could decompose it a neighbors computing and a dedicated embedding step; you know that in Scanpy, the neighborhood graph is used for many other things other than for the embedding (clustering and trajectory inference). I also added the Gaussian kernel solution that I had before switching to a “UMAP backend” for `pp.neighbors`; which was needed so that results for DPT could be reproduced. All of this would have been quite a discussion with Leland. Until we would have had settled on the “Scanpy needs” that certainly weren’t aligned with the development of an independent young package, PRs would have been integrated to much time would have been lost. Finally, I wanted absolute reproducibility for Scanpy users, which could only be achieved by “freezing the code”. So, I asked Leland whether he is OK if I add a frozen version of umap as an intermediate solution.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/522
https://github.com/scverse/scanpy/pull/523:207,interoperability,specif,specify,207,"Add `smallest_dot` parameter to dotplot; The dots in dotplot can be quite small and difficult to see. In order to help with visibility and customizability of the dotplot, added a `smallest_dot` parameter to specify the size of the smallest dot.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/523
https://github.com/scverse/scanpy/pull/523:19,modifiability,paramet,parameter,19,"Add `smallest_dot` parameter to dotplot; The dots in dotplot can be quite small and difficult to see. In order to help with visibility and customizability of the dotplot, added a `smallest_dot` parameter to specify the size of the smallest dot.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/523
https://github.com/scverse/scanpy/pull/523:194,modifiability,paramet,parameter,194,"Add `smallest_dot` parameter to dotplot; The dots in dotplot can be quite small and difficult to see. In order to help with visibility and customizability of the dotplot, added a `smallest_dot` parameter to specify the size of the smallest dot.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/523
https://github.com/scverse/scanpy/pull/523:114,usability,help,help,114,"Add `smallest_dot` parameter to dotplot; The dots in dotplot can be quite small and difficult to see. In order to help with visibility and customizability of the dotplot, added a `smallest_dot` parameter to specify the size of the smallest dot.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/523
https://github.com/scverse/scanpy/pull/523:139,usability,custom,customizability,139,"Add `smallest_dot` parameter to dotplot; The dots in dotplot can be quite small and difficult to see. In order to help with visibility and customizability of the dotplot, added a `smallest_dot` parameter to specify the size of the smallest dot.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/523
https://github.com/scverse/scanpy/pull/524:168,deployability,updat,updates,168,Add `smallest_dot` parameter to dotplot; Allow greater customizability and visibility when using dotplot by allowing the user to set the smallest dot size. This change updates the dots along with the dot size legend.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/524
https://github.com/scverse/scanpy/pull/524:19,modifiability,paramet,parameter,19,Add `smallest_dot` parameter to dotplot; Allow greater customizability and visibility when using dotplot by allowing the user to set the smallest dot size. This change updates the dots along with the dot size legend.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/524
https://github.com/scverse/scanpy/pull/524:168,safety,updat,updates,168,Add `smallest_dot` parameter to dotplot; Allow greater customizability and visibility when using dotplot by allowing the user to set the smallest dot size. This change updates the dots along with the dot size legend.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/524
https://github.com/scverse/scanpy/pull/524:168,security,updat,updates,168,Add `smallest_dot` parameter to dotplot; Allow greater customizability and visibility when using dotplot by allowing the user to set the smallest dot size. This change updates the dots along with the dot size legend.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/524
https://github.com/scverse/scanpy/pull/524:55,usability,custom,customizability,55,Add `smallest_dot` parameter to dotplot; Allow greater customizability and visibility when using dotplot by allowing the user to set the smallest dot size. This change updates the dots along with the dot size legend.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/524
https://github.com/scverse/scanpy/pull/524:121,usability,user,user,121,Add `smallest_dot` parameter to dotplot; Allow greater customizability and visibility when using dotplot by allowing the user to set the smallest dot size. This change updates the dots along with the dot size legend.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/524
https://github.com/scverse/scanpy/issues/526:2567,availability,down,downstream,2567,"ell cycle**. The lower part of the plot shows regress out applied to the cell cycle (following [the scanpy tutorial](https://nbviewer.jupyter.org/github/theislab/scanpy_usage/blob/master/180209_cell_cycle/cell_cycle.ipynb) and the 'alternative approach' described in the [seurat vignette](https://satijalab.org/seurat/cell_cycle_vignette.html#assign-cell-cycle-scores), i.e. I regressed out the difference between the G2M and S phase scores):. ```. adata.obs[""cell_cycle_diff""] = adata.obs[""S_score""] - adata.obs[""G2M_score""]. sc.pp.regress_out(adata, ['cell_cycle_diff']). ```. Like that, the differences between dividing and non-dividing cells should be preserved. . Again, in the *Savas* dataset, after regressing out the cell cycle effects, G1 is correctly separated from G2M/S. In *Lambrechts*, there is no clear separation. Having eyeballed at the UMAP-plot (below) it seems that the cell-cycle labels correlate with the cell type (i.e. cancer cells and myeloid cells got the G1 label assigned more likely than T cells). . **What is 'best practice'?**. I quickly discussed this offline with @flying-sheep, and he encouraged me to create this issue. . * Is it just a problem with visualizing the first PC's and `regress_out` should be applied regardless. * Should `regress_out` be skipped and only applied in a more downstream step when focusing on a single cell type? . * Are there any other situations where `regress_out` could do more harm than good? . **PCA plots before and after `regress_out`**. ![regress_out](https://user-images.githubusercontent.com/7051479/54083302-f088f500-4321-11e9-877a-1cbef6f4f489.png). **UMAP-plots**. The cell cycle label correlates with the cell type (other dataset, but to show what I mean): . ![2019-03-10_11:20:31_384x234](https://user-images.githubusercontent.com/7051479/54083671-29779880-4327-11e9-94d6-9be34383b909.png). ![2019-03-10_11:25:30_428x231](https://user-images.githubusercontent.com/7051479/54083675-3a280e80-4327-11e9-954f-34ef1404961b.png).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/526
https://github.com/scverse/scanpy/issues/526:778,deployability,contain,contains,778,"Limitations of regress_out as a standard processing step? ; According to the PBMC 3k tutorial, which I consider as the 'best practice' tutorial for scanpy, regressing out the fraction of mitochondrial reads and the number of detected genes is recommended as a 'standard processing step'. . Having analysed two different datasets, I am so sure anymore if this is a good idea. **number of detected genes**. I loaded these datasets into scanpy and processed them according to the 3k PBMC tutorial: . * [Savas et al., 2018](https://doi.org/10.1038/s41591-018-0078-7), ~6k cells, CD3+ T cells, BRCA. * [Lambrechts et al., 2018](https://doi.org/10.1038/s41591-018-0096-5), ~32k cells, whole tissue NSCLC. Regress-out seems to perfectly do its jobs on the *Savas et al.* dataset, that contains closely related cell types (1st row of figure): The 2nd PC is confounded by the number of detected genes and this effect is reduced. . On the *Lambrechts et al* dataset, that contains all kinds of cells (cancer, stromal, immune), this looks differently: Neither of the first 2 PCs seems to be related to the number of detected genes and it actually seems to me that I am 'loosing' information by applying `regress_out` (everything is now a single blob). . **cell cycle**. The lower part of the plot shows regress out applied to the cell cycle (following [the scanpy tutorial](https://nbviewer.jupyter.org/github/theislab/scanpy_usage/blob/master/180209_cell_cycle/cell_cycle.ipynb) and the 'alternative approach' described in the [seurat vignette](https://satijalab.org/seurat/cell_cycle_vignette.html#assign-cell-cycle-scores), i.e. I regressed out the difference between the G2M and S phase scores):. ```. adata.obs[""cell_cycle_diff""] = adata.obs[""S_score""] - adata.obs[""G2M_score""]. sc.pp.regress_out(adata, ['cell_cycle_diff']). ```. Like that, the differences between dividing and non-dividing cells should be preserved. . Again, in the *Savas* dataset, after regressing out the cell cycle effects, G1 is cor",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/526
https://github.com/scverse/scanpy/issues/526:962,deployability,contain,contains,962,"Limitations of regress_out as a standard processing step? ; According to the PBMC 3k tutorial, which I consider as the 'best practice' tutorial for scanpy, regressing out the fraction of mitochondrial reads and the number of detected genes is recommended as a 'standard processing step'. . Having analysed two different datasets, I am so sure anymore if this is a good idea. **number of detected genes**. I loaded these datasets into scanpy and processed them according to the 3k PBMC tutorial: . * [Savas et al., 2018](https://doi.org/10.1038/s41591-018-0078-7), ~6k cells, CD3+ T cells, BRCA. * [Lambrechts et al., 2018](https://doi.org/10.1038/s41591-018-0096-5), ~32k cells, whole tissue NSCLC. Regress-out seems to perfectly do its jobs on the *Savas et al.* dataset, that contains closely related cell types (1st row of figure): The 2nd PC is confounded by the number of detected genes and this effect is reduced. . On the *Lambrechts et al* dataset, that contains all kinds of cells (cancer, stromal, immune), this looks differently: Neither of the first 2 PCs seems to be related to the number of detected genes and it actually seems to me that I am 'loosing' information by applying `regress_out` (everything is now a single blob). . **cell cycle**. The lower part of the plot shows regress out applied to the cell cycle (following [the scanpy tutorial](https://nbviewer.jupyter.org/github/theislab/scanpy_usage/blob/master/180209_cell_cycle/cell_cycle.ipynb) and the 'alternative approach' described in the [seurat vignette](https://satijalab.org/seurat/cell_cycle_vignette.html#assign-cell-cycle-scores), i.e. I regressed out the difference between the G2M and S phase scores):. ```. adata.obs[""cell_cycle_diff""] = adata.obs[""S_score""] - adata.obs[""G2M_score""]. sc.pp.regress_out(adata, ['cell_cycle_diff']). ```. Like that, the differences between dividing and non-dividing cells should be preserved. . Again, in the *Savas* dataset, after regressing out the cell cycle effects, G1 is cor",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/526
https://github.com/scverse/scanpy/issues/526:407,energy efficiency,load,loaded,407,"Limitations of regress_out as a standard processing step? ; According to the PBMC 3k tutorial, which I consider as the 'best practice' tutorial for scanpy, regressing out the fraction of mitochondrial reads and the number of detected genes is recommended as a 'standard processing step'. . Having analysed two different datasets, I am so sure anymore if this is a good idea. **number of detected genes**. I loaded these datasets into scanpy and processed them according to the 3k PBMC tutorial: . * [Savas et al., 2018](https://doi.org/10.1038/s41591-018-0078-7), ~6k cells, CD3+ T cells, BRCA. * [Lambrechts et al., 2018](https://doi.org/10.1038/s41591-018-0096-5), ~32k cells, whole tissue NSCLC. Regress-out seems to perfectly do its jobs on the *Savas et al.* dataset, that contains closely related cell types (1st row of figure): The 2nd PC is confounded by the number of detected genes and this effect is reduced. . On the *Lambrechts et al* dataset, that contains all kinds of cells (cancer, stromal, immune), this looks differently: Neither of the first 2 PCs seems to be related to the number of detected genes and it actually seems to me that I am 'loosing' information by applying `regress_out` (everything is now a single blob). . **cell cycle**. The lower part of the plot shows regress out applied to the cell cycle (following [the scanpy tutorial](https://nbviewer.jupyter.org/github/theislab/scanpy_usage/blob/master/180209_cell_cycle/cell_cycle.ipynb) and the 'alternative approach' described in the [seurat vignette](https://satijalab.org/seurat/cell_cycle_vignette.html#assign-cell-cycle-scores), i.e. I regressed out the difference between the G2M and S phase scores):. ```. adata.obs[""cell_cycle_diff""] = adata.obs[""S_score""] - adata.obs[""G2M_score""]. sc.pp.regress_out(adata, ['cell_cycle_diff']). ```. Like that, the differences between dividing and non-dividing cells should be preserved. . Again, in the *Savas* dataset, after regressing out the cell cycle effects, G1 is cor",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/526
https://github.com/scverse/scanpy/issues/526:911,energy efficiency,reduc,reduced,911,"Limitations of regress_out as a standard processing step? ; According to the PBMC 3k tutorial, which I consider as the 'best practice' tutorial for scanpy, regressing out the fraction of mitochondrial reads and the number of detected genes is recommended as a 'standard processing step'. . Having analysed two different datasets, I am so sure anymore if this is a good idea. **number of detected genes**. I loaded these datasets into scanpy and processed them according to the 3k PBMC tutorial: . * [Savas et al., 2018](https://doi.org/10.1038/s41591-018-0078-7), ~6k cells, CD3+ T cells, BRCA. * [Lambrechts et al., 2018](https://doi.org/10.1038/s41591-018-0096-5), ~32k cells, whole tissue NSCLC. Regress-out seems to perfectly do its jobs on the *Savas et al.* dataset, that contains closely related cell types (1st row of figure): The 2nd PC is confounded by the number of detected genes and this effect is reduced. . On the *Lambrechts et al* dataset, that contains all kinds of cells (cancer, stromal, immune), this looks differently: Neither of the first 2 PCs seems to be related to the number of detected genes and it actually seems to me that I am 'loosing' information by applying `regress_out` (everything is now a single blob). . **cell cycle**. The lower part of the plot shows regress out applied to the cell cycle (following [the scanpy tutorial](https://nbviewer.jupyter.org/github/theislab/scanpy_usage/blob/master/180209_cell_cycle/cell_cycle.ipynb) and the 'alternative approach' described in the [seurat vignette](https://satijalab.org/seurat/cell_cycle_vignette.html#assign-cell-cycle-scores), i.e. I regressed out the difference between the G2M and S phase scores):. ```. adata.obs[""cell_cycle_diff""] = adata.obs[""S_score""] - adata.obs[""G2M_score""]. sc.pp.regress_out(adata, ['cell_cycle_diff']). ```. Like that, the differences between dividing and non-dividing cells should be preserved. . Again, in the *Savas* dataset, after regressing out the cell cycle effects, G1 is cor",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/526
https://github.com/scverse/scanpy/issues/526:32,interoperability,standard,standard,32,"Limitations of regress_out as a standard processing step? ; According to the PBMC 3k tutorial, which I consider as the 'best practice' tutorial for scanpy, regressing out the fraction of mitochondrial reads and the number of detected genes is recommended as a 'standard processing step'. . Having analysed two different datasets, I am so sure anymore if this is a good idea. **number of detected genes**. I loaded these datasets into scanpy and processed them according to the 3k PBMC tutorial: . * [Savas et al., 2018](https://doi.org/10.1038/s41591-018-0078-7), ~6k cells, CD3+ T cells, BRCA. * [Lambrechts et al., 2018](https://doi.org/10.1038/s41591-018-0096-5), ~32k cells, whole tissue NSCLC. Regress-out seems to perfectly do its jobs on the *Savas et al.* dataset, that contains closely related cell types (1st row of figure): The 2nd PC is confounded by the number of detected genes and this effect is reduced. . On the *Lambrechts et al* dataset, that contains all kinds of cells (cancer, stromal, immune), this looks differently: Neither of the first 2 PCs seems to be related to the number of detected genes and it actually seems to me that I am 'loosing' information by applying `regress_out` (everything is now a single blob). . **cell cycle**. The lower part of the plot shows regress out applied to the cell cycle (following [the scanpy tutorial](https://nbviewer.jupyter.org/github/theislab/scanpy_usage/blob/master/180209_cell_cycle/cell_cycle.ipynb) and the 'alternative approach' described in the [seurat vignette](https://satijalab.org/seurat/cell_cycle_vignette.html#assign-cell-cycle-scores), i.e. I regressed out the difference between the G2M and S phase scores):. ```. adata.obs[""cell_cycle_diff""] = adata.obs[""S_score""] - adata.obs[""G2M_score""]. sc.pp.regress_out(adata, ['cell_cycle_diff']). ```. Like that, the differences between dividing and non-dividing cells should be preserved. . Again, in the *Savas* dataset, after regressing out the cell cycle effects, G1 is cor",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/526
https://github.com/scverse/scanpy/issues/526:261,interoperability,standard,standard,261,"Limitations of regress_out as a standard processing step? ; According to the PBMC 3k tutorial, which I consider as the 'best practice' tutorial for scanpy, regressing out the fraction of mitochondrial reads and the number of detected genes is recommended as a 'standard processing step'. . Having analysed two different datasets, I am so sure anymore if this is a good idea. **number of detected genes**. I loaded these datasets into scanpy and processed them according to the 3k PBMC tutorial: . * [Savas et al., 2018](https://doi.org/10.1038/s41591-018-0078-7), ~6k cells, CD3+ T cells, BRCA. * [Lambrechts et al., 2018](https://doi.org/10.1038/s41591-018-0096-5), ~32k cells, whole tissue NSCLC. Regress-out seems to perfectly do its jobs on the *Savas et al.* dataset, that contains closely related cell types (1st row of figure): The 2nd PC is confounded by the number of detected genes and this effect is reduced. . On the *Lambrechts et al* dataset, that contains all kinds of cells (cancer, stromal, immune), this looks differently: Neither of the first 2 PCs seems to be related to the number of detected genes and it actually seems to me that I am 'loosing' information by applying `regress_out` (everything is now a single blob). . **cell cycle**. The lower part of the plot shows regress out applied to the cell cycle (following [the scanpy tutorial](https://nbviewer.jupyter.org/github/theislab/scanpy_usage/blob/master/180209_cell_cycle/cell_cycle.ipynb) and the 'alternative approach' described in the [seurat vignette](https://satijalab.org/seurat/cell_cycle_vignette.html#assign-cell-cycle-scores), i.e. I regressed out the difference between the G2M and S phase scores):. ```. adata.obs[""cell_cycle_diff""] = adata.obs[""S_score""] - adata.obs[""G2M_score""]. sc.pp.regress_out(adata, ['cell_cycle_diff']). ```. Like that, the differences between dividing and non-dividing cells should be preserved. . Again, in the *Savas* dataset, after regressing out the cell cycle effects, G1 is cor",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/526
https://github.com/scverse/scanpy/issues/526:407,performance,load,loaded,407,"Limitations of regress_out as a standard processing step? ; According to the PBMC 3k tutorial, which I consider as the 'best practice' tutorial for scanpy, regressing out the fraction of mitochondrial reads and the number of detected genes is recommended as a 'standard processing step'. . Having analysed two different datasets, I am so sure anymore if this is a good idea. **number of detected genes**. I loaded these datasets into scanpy and processed them according to the 3k PBMC tutorial: . * [Savas et al., 2018](https://doi.org/10.1038/s41591-018-0078-7), ~6k cells, CD3+ T cells, BRCA. * [Lambrechts et al., 2018](https://doi.org/10.1038/s41591-018-0096-5), ~32k cells, whole tissue NSCLC. Regress-out seems to perfectly do its jobs on the *Savas et al.* dataset, that contains closely related cell types (1st row of figure): The 2nd PC is confounded by the number of detected genes and this effect is reduced. . On the *Lambrechts et al* dataset, that contains all kinds of cells (cancer, stromal, immune), this looks differently: Neither of the first 2 PCs seems to be related to the number of detected genes and it actually seems to me that I am 'loosing' information by applying `regress_out` (everything is now a single blob). . **cell cycle**. The lower part of the plot shows regress out applied to the cell cycle (following [the scanpy tutorial](https://nbviewer.jupyter.org/github/theislab/scanpy_usage/blob/master/180209_cell_cycle/cell_cycle.ipynb) and the 'alternative approach' described in the [seurat vignette](https://satijalab.org/seurat/cell_cycle_vignette.html#assign-cell-cycle-scores), i.e. I regressed out the difference between the G2M and S phase scores):. ```. adata.obs[""cell_cycle_diff""] = adata.obs[""S_score""] - adata.obs[""G2M_score""]. sc.pp.regress_out(adata, ['cell_cycle_diff']). ```. Like that, the differences between dividing and non-dividing cells should be preserved. . Again, in the *Savas* dataset, after regressing out the cell cycle effects, G1 is cor",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/526
https://github.com/scverse/scanpy/issues/526:125,reliability,pra,practice,125,"Limitations of regress_out as a standard processing step? ; According to the PBMC 3k tutorial, which I consider as the 'best practice' tutorial for scanpy, regressing out the fraction of mitochondrial reads and the number of detected genes is recommended as a 'standard processing step'. . Having analysed two different datasets, I am so sure anymore if this is a good idea. **number of detected genes**. I loaded these datasets into scanpy and processed them according to the 3k PBMC tutorial: . * [Savas et al., 2018](https://doi.org/10.1038/s41591-018-0078-7), ~6k cells, CD3+ T cells, BRCA. * [Lambrechts et al., 2018](https://doi.org/10.1038/s41591-018-0096-5), ~32k cells, whole tissue NSCLC. Regress-out seems to perfectly do its jobs on the *Savas et al.* dataset, that contains closely related cell types (1st row of figure): The 2nd PC is confounded by the number of detected genes and this effect is reduced. . On the *Lambrechts et al* dataset, that contains all kinds of cells (cancer, stromal, immune), this looks differently: Neither of the first 2 PCs seems to be related to the number of detected genes and it actually seems to me that I am 'loosing' information by applying `regress_out` (everything is now a single blob). . **cell cycle**. The lower part of the plot shows regress out applied to the cell cycle (following [the scanpy tutorial](https://nbviewer.jupyter.org/github/theislab/scanpy_usage/blob/master/180209_cell_cycle/cell_cycle.ipynb) and the 'alternative approach' described in the [seurat vignette](https://satijalab.org/seurat/cell_cycle_vignette.html#assign-cell-cycle-scores), i.e. I regressed out the difference between the G2M and S phase scores):. ```. adata.obs[""cell_cycle_diff""] = adata.obs[""S_score""] - adata.obs[""G2M_score""]. sc.pp.regress_out(adata, ['cell_cycle_diff']). ```. Like that, the differences between dividing and non-dividing cells should be preserved. . Again, in the *Savas* dataset, after regressing out the cell cycle effects, G1 is cor",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/526
https://github.com/scverse/scanpy/issues/526:2291,reliability,pra,practice,2291,"ell cycle**. The lower part of the plot shows regress out applied to the cell cycle (following [the scanpy tutorial](https://nbviewer.jupyter.org/github/theislab/scanpy_usage/blob/master/180209_cell_cycle/cell_cycle.ipynb) and the 'alternative approach' described in the [seurat vignette](https://satijalab.org/seurat/cell_cycle_vignette.html#assign-cell-cycle-scores), i.e. I regressed out the difference between the G2M and S phase scores):. ```. adata.obs[""cell_cycle_diff""] = adata.obs[""S_score""] - adata.obs[""G2M_score""]. sc.pp.regress_out(adata, ['cell_cycle_diff']). ```. Like that, the differences between dividing and non-dividing cells should be preserved. . Again, in the *Savas* dataset, after regressing out the cell cycle effects, G1 is correctly separated from G2M/S. In *Lambrechts*, there is no clear separation. Having eyeballed at the UMAP-plot (below) it seems that the cell-cycle labels correlate with the cell type (i.e. cancer cells and myeloid cells got the G1 label assigned more likely than T cells). . **What is 'best practice'?**. I quickly discussed this offline with @flying-sheep, and he encouraged me to create this issue. . * Is it just a problem with visualizing the first PC's and `regress_out` should be applied regardless. * Should `regress_out` be skipped and only applied in a more downstream step when focusing on a single cell type? . * Are there any other situations where `regress_out` could do more harm than good? . **PCA plots before and after `regress_out`**. ![regress_out](https://user-images.githubusercontent.com/7051479/54083302-f088f500-4321-11e9-877a-1cbef6f4f489.png). **UMAP-plots**. The cell cycle label correlates with the cell type (other dataset, but to show what I mean): . ![2019-03-10_11:20:31_384x234](https://user-images.githubusercontent.com/7051479/54083671-29779880-4327-11e9-94d6-9be34383b909.png). ![2019-03-10_11:25:30_428x231](https://user-images.githubusercontent.com/7051479/54083675-3a280e80-4327-11e9-954f-34ef1404961b.png).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/526
https://github.com/scverse/scanpy/issues/526:225,safety,detect,detected,225,"Limitations of regress_out as a standard processing step? ; According to the PBMC 3k tutorial, which I consider as the 'best practice' tutorial for scanpy, regressing out the fraction of mitochondrial reads and the number of detected genes is recommended as a 'standard processing step'. . Having analysed two different datasets, I am so sure anymore if this is a good idea. **number of detected genes**. I loaded these datasets into scanpy and processed them according to the 3k PBMC tutorial: . * [Savas et al., 2018](https://doi.org/10.1038/s41591-018-0078-7), ~6k cells, CD3+ T cells, BRCA. * [Lambrechts et al., 2018](https://doi.org/10.1038/s41591-018-0096-5), ~32k cells, whole tissue NSCLC. Regress-out seems to perfectly do its jobs on the *Savas et al.* dataset, that contains closely related cell types (1st row of figure): The 2nd PC is confounded by the number of detected genes and this effect is reduced. . On the *Lambrechts et al* dataset, that contains all kinds of cells (cancer, stromal, immune), this looks differently: Neither of the first 2 PCs seems to be related to the number of detected genes and it actually seems to me that I am 'loosing' information by applying `regress_out` (everything is now a single blob). . **cell cycle**. The lower part of the plot shows regress out applied to the cell cycle (following [the scanpy tutorial](https://nbviewer.jupyter.org/github/theislab/scanpy_usage/blob/master/180209_cell_cycle/cell_cycle.ipynb) and the 'alternative approach' described in the [seurat vignette](https://satijalab.org/seurat/cell_cycle_vignette.html#assign-cell-cycle-scores), i.e. I regressed out the difference between the G2M and S phase scores):. ```. adata.obs[""cell_cycle_diff""] = adata.obs[""S_score""] - adata.obs[""G2M_score""]. sc.pp.regress_out(adata, ['cell_cycle_diff']). ```. Like that, the differences between dividing and non-dividing cells should be preserved. . Again, in the *Savas* dataset, after regressing out the cell cycle effects, G1 is cor",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/526
https://github.com/scverse/scanpy/issues/526:387,safety,detect,detected,387,"Limitations of regress_out as a standard processing step? ; According to the PBMC 3k tutorial, which I consider as the 'best practice' tutorial for scanpy, regressing out the fraction of mitochondrial reads and the number of detected genes is recommended as a 'standard processing step'. . Having analysed two different datasets, I am so sure anymore if this is a good idea. **number of detected genes**. I loaded these datasets into scanpy and processed them according to the 3k PBMC tutorial: . * [Savas et al., 2018](https://doi.org/10.1038/s41591-018-0078-7), ~6k cells, CD3+ T cells, BRCA. * [Lambrechts et al., 2018](https://doi.org/10.1038/s41591-018-0096-5), ~32k cells, whole tissue NSCLC. Regress-out seems to perfectly do its jobs on the *Savas et al.* dataset, that contains closely related cell types (1st row of figure): The 2nd PC is confounded by the number of detected genes and this effect is reduced. . On the *Lambrechts et al* dataset, that contains all kinds of cells (cancer, stromal, immune), this looks differently: Neither of the first 2 PCs seems to be related to the number of detected genes and it actually seems to me that I am 'loosing' information by applying `regress_out` (everything is now a single blob). . **cell cycle**. The lower part of the plot shows regress out applied to the cell cycle (following [the scanpy tutorial](https://nbviewer.jupyter.org/github/theislab/scanpy_usage/blob/master/180209_cell_cycle/cell_cycle.ipynb) and the 'alternative approach' described in the [seurat vignette](https://satijalab.org/seurat/cell_cycle_vignette.html#assign-cell-cycle-scores), i.e. I regressed out the difference between the G2M and S phase scores):. ```. adata.obs[""cell_cycle_diff""] = adata.obs[""S_score""] - adata.obs[""G2M_score""]. sc.pp.regress_out(adata, ['cell_cycle_diff']). ```. Like that, the differences between dividing and non-dividing cells should be preserved. . Again, in the *Savas* dataset, after regressing out the cell cycle effects, G1 is cor",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/526
https://github.com/scverse/scanpy/issues/526:877,safety,detect,detected,877,"Limitations of regress_out as a standard processing step? ; According to the PBMC 3k tutorial, which I consider as the 'best practice' tutorial for scanpy, regressing out the fraction of mitochondrial reads and the number of detected genes is recommended as a 'standard processing step'. . Having analysed two different datasets, I am so sure anymore if this is a good idea. **number of detected genes**. I loaded these datasets into scanpy and processed them according to the 3k PBMC tutorial: . * [Savas et al., 2018](https://doi.org/10.1038/s41591-018-0078-7), ~6k cells, CD3+ T cells, BRCA. * [Lambrechts et al., 2018](https://doi.org/10.1038/s41591-018-0096-5), ~32k cells, whole tissue NSCLC. Regress-out seems to perfectly do its jobs on the *Savas et al.* dataset, that contains closely related cell types (1st row of figure): The 2nd PC is confounded by the number of detected genes and this effect is reduced. . On the *Lambrechts et al* dataset, that contains all kinds of cells (cancer, stromal, immune), this looks differently: Neither of the first 2 PCs seems to be related to the number of detected genes and it actually seems to me that I am 'loosing' information by applying `regress_out` (everything is now a single blob). . **cell cycle**. The lower part of the plot shows regress out applied to the cell cycle (following [the scanpy tutorial](https://nbviewer.jupyter.org/github/theislab/scanpy_usage/blob/master/180209_cell_cycle/cell_cycle.ipynb) and the 'alternative approach' described in the [seurat vignette](https://satijalab.org/seurat/cell_cycle_vignette.html#assign-cell-cycle-scores), i.e. I regressed out the difference between the G2M and S phase scores):. ```. adata.obs[""cell_cycle_diff""] = adata.obs[""S_score""] - adata.obs[""G2M_score""]. sc.pp.regress_out(adata, ['cell_cycle_diff']). ```. Like that, the differences between dividing and non-dividing cells should be preserved. . Again, in the *Savas* dataset, after regressing out the cell cycle effects, G1 is cor",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/526
https://github.com/scverse/scanpy/issues/526:1105,safety,detect,detected,1105,"er as the 'best practice' tutorial for scanpy, regressing out the fraction of mitochondrial reads and the number of detected genes is recommended as a 'standard processing step'. . Having analysed two different datasets, I am so sure anymore if this is a good idea. **number of detected genes**. I loaded these datasets into scanpy and processed them according to the 3k PBMC tutorial: . * [Savas et al., 2018](https://doi.org/10.1038/s41591-018-0078-7), ~6k cells, CD3+ T cells, BRCA. * [Lambrechts et al., 2018](https://doi.org/10.1038/s41591-018-0096-5), ~32k cells, whole tissue NSCLC. Regress-out seems to perfectly do its jobs on the *Savas et al.* dataset, that contains closely related cell types (1st row of figure): The 2nd PC is confounded by the number of detected genes and this effect is reduced. . On the *Lambrechts et al* dataset, that contains all kinds of cells (cancer, stromal, immune), this looks differently: Neither of the first 2 PCs seems to be related to the number of detected genes and it actually seems to me that I am 'loosing' information by applying `regress_out` (everything is now a single blob). . **cell cycle**. The lower part of the plot shows regress out applied to the cell cycle (following [the scanpy tutorial](https://nbviewer.jupyter.org/github/theislab/scanpy_usage/blob/master/180209_cell_cycle/cell_cycle.ipynb) and the 'alternative approach' described in the [seurat vignette](https://satijalab.org/seurat/cell_cycle_vignette.html#assign-cell-cycle-scores), i.e. I regressed out the difference between the G2M and S phase scores):. ```. adata.obs[""cell_cycle_diff""] = adata.obs[""S_score""] - adata.obs[""G2M_score""]. sc.pp.regress_out(adata, ['cell_cycle_diff']). ```. Like that, the differences between dividing and non-dividing cells should be preserved. . Again, in the *Savas* dataset, after regressing out the cell cycle effects, G1 is correctly separated from G2M/S. In *Lambrechts*, there is no clear separation. Having eyeballed at the UMAP-plot",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/526
https://github.com/scverse/scanpy/issues/526:225,security,detect,detected,225,"Limitations of regress_out as a standard processing step? ; According to the PBMC 3k tutorial, which I consider as the 'best practice' tutorial for scanpy, regressing out the fraction of mitochondrial reads and the number of detected genes is recommended as a 'standard processing step'. . Having analysed two different datasets, I am so sure anymore if this is a good idea. **number of detected genes**. I loaded these datasets into scanpy and processed them according to the 3k PBMC tutorial: . * [Savas et al., 2018](https://doi.org/10.1038/s41591-018-0078-7), ~6k cells, CD3+ T cells, BRCA. * [Lambrechts et al., 2018](https://doi.org/10.1038/s41591-018-0096-5), ~32k cells, whole tissue NSCLC. Regress-out seems to perfectly do its jobs on the *Savas et al.* dataset, that contains closely related cell types (1st row of figure): The 2nd PC is confounded by the number of detected genes and this effect is reduced. . On the *Lambrechts et al* dataset, that contains all kinds of cells (cancer, stromal, immune), this looks differently: Neither of the first 2 PCs seems to be related to the number of detected genes and it actually seems to me that I am 'loosing' information by applying `regress_out` (everything is now a single blob). . **cell cycle**. The lower part of the plot shows regress out applied to the cell cycle (following [the scanpy tutorial](https://nbviewer.jupyter.org/github/theislab/scanpy_usage/blob/master/180209_cell_cycle/cell_cycle.ipynb) and the 'alternative approach' described in the [seurat vignette](https://satijalab.org/seurat/cell_cycle_vignette.html#assign-cell-cycle-scores), i.e. I regressed out the difference between the G2M and S phase scores):. ```. adata.obs[""cell_cycle_diff""] = adata.obs[""S_score""] - adata.obs[""G2M_score""]. sc.pp.regress_out(adata, ['cell_cycle_diff']). ```. Like that, the differences between dividing and non-dividing cells should be preserved. . Again, in the *Savas* dataset, after regressing out the cell cycle effects, G1 is cor",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/526
https://github.com/scverse/scanpy/issues/526:387,security,detect,detected,387,"Limitations of regress_out as a standard processing step? ; According to the PBMC 3k tutorial, which I consider as the 'best practice' tutorial for scanpy, regressing out the fraction of mitochondrial reads and the number of detected genes is recommended as a 'standard processing step'. . Having analysed two different datasets, I am so sure anymore if this is a good idea. **number of detected genes**. I loaded these datasets into scanpy and processed them according to the 3k PBMC tutorial: . * [Savas et al., 2018](https://doi.org/10.1038/s41591-018-0078-7), ~6k cells, CD3+ T cells, BRCA. * [Lambrechts et al., 2018](https://doi.org/10.1038/s41591-018-0096-5), ~32k cells, whole tissue NSCLC. Regress-out seems to perfectly do its jobs on the *Savas et al.* dataset, that contains closely related cell types (1st row of figure): The 2nd PC is confounded by the number of detected genes and this effect is reduced. . On the *Lambrechts et al* dataset, that contains all kinds of cells (cancer, stromal, immune), this looks differently: Neither of the first 2 PCs seems to be related to the number of detected genes and it actually seems to me that I am 'loosing' information by applying `regress_out` (everything is now a single blob). . **cell cycle**. The lower part of the plot shows regress out applied to the cell cycle (following [the scanpy tutorial](https://nbviewer.jupyter.org/github/theislab/scanpy_usage/blob/master/180209_cell_cycle/cell_cycle.ipynb) and the 'alternative approach' described in the [seurat vignette](https://satijalab.org/seurat/cell_cycle_vignette.html#assign-cell-cycle-scores), i.e. I regressed out the difference between the G2M and S phase scores):. ```. adata.obs[""cell_cycle_diff""] = adata.obs[""S_score""] - adata.obs[""G2M_score""]. sc.pp.regress_out(adata, ['cell_cycle_diff']). ```. Like that, the differences between dividing and non-dividing cells should be preserved. . Again, in the *Savas* dataset, after regressing out the cell cycle effects, G1 is cor",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/526
https://github.com/scverse/scanpy/issues/526:877,security,detect,detected,877,"Limitations of regress_out as a standard processing step? ; According to the PBMC 3k tutorial, which I consider as the 'best practice' tutorial for scanpy, regressing out the fraction of mitochondrial reads and the number of detected genes is recommended as a 'standard processing step'. . Having analysed two different datasets, I am so sure anymore if this is a good idea. **number of detected genes**. I loaded these datasets into scanpy and processed them according to the 3k PBMC tutorial: . * [Savas et al., 2018](https://doi.org/10.1038/s41591-018-0078-7), ~6k cells, CD3+ T cells, BRCA. * [Lambrechts et al., 2018](https://doi.org/10.1038/s41591-018-0096-5), ~32k cells, whole tissue NSCLC. Regress-out seems to perfectly do its jobs on the *Savas et al.* dataset, that contains closely related cell types (1st row of figure): The 2nd PC is confounded by the number of detected genes and this effect is reduced. . On the *Lambrechts et al* dataset, that contains all kinds of cells (cancer, stromal, immune), this looks differently: Neither of the first 2 PCs seems to be related to the number of detected genes and it actually seems to me that I am 'loosing' information by applying `regress_out` (everything is now a single blob). . **cell cycle**. The lower part of the plot shows regress out applied to the cell cycle (following [the scanpy tutorial](https://nbviewer.jupyter.org/github/theislab/scanpy_usage/blob/master/180209_cell_cycle/cell_cycle.ipynb) and the 'alternative approach' described in the [seurat vignette](https://satijalab.org/seurat/cell_cycle_vignette.html#assign-cell-cycle-scores), i.e. I regressed out the difference between the G2M and S phase scores):. ```. adata.obs[""cell_cycle_diff""] = adata.obs[""S_score""] - adata.obs[""G2M_score""]. sc.pp.regress_out(adata, ['cell_cycle_diff']). ```. Like that, the differences between dividing and non-dividing cells should be preserved. . Again, in the *Savas* dataset, after regressing out the cell cycle effects, G1 is cor",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/526
https://github.com/scverse/scanpy/issues/526:1105,security,detect,detected,1105,"er as the 'best practice' tutorial for scanpy, regressing out the fraction of mitochondrial reads and the number of detected genes is recommended as a 'standard processing step'. . Having analysed two different datasets, I am so sure anymore if this is a good idea. **number of detected genes**. I loaded these datasets into scanpy and processed them according to the 3k PBMC tutorial: . * [Savas et al., 2018](https://doi.org/10.1038/s41591-018-0078-7), ~6k cells, CD3+ T cells, BRCA. * [Lambrechts et al., 2018](https://doi.org/10.1038/s41591-018-0096-5), ~32k cells, whole tissue NSCLC. Regress-out seems to perfectly do its jobs on the *Savas et al.* dataset, that contains closely related cell types (1st row of figure): The 2nd PC is confounded by the number of detected genes and this effect is reduced. . On the *Lambrechts et al* dataset, that contains all kinds of cells (cancer, stromal, immune), this looks differently: Neither of the first 2 PCs seems to be related to the number of detected genes and it actually seems to me that I am 'loosing' information by applying `regress_out` (everything is now a single blob). . **cell cycle**. The lower part of the plot shows regress out applied to the cell cycle (following [the scanpy tutorial](https://nbviewer.jupyter.org/github/theislab/scanpy_usage/blob/master/180209_cell_cycle/cell_cycle.ipynb) and the 'alternative approach' described in the [seurat vignette](https://satijalab.org/seurat/cell_cycle_vignette.html#assign-cell-cycle-scores), i.e. I regressed out the difference between the G2M and S phase scores):. ```. adata.obs[""cell_cycle_diff""] = adata.obs[""S_score""] - adata.obs[""G2M_score""]. sc.pp.regress_out(adata, ['cell_cycle_diff']). ```. Like that, the differences between dividing and non-dividing cells should be preserved. . Again, in the *Savas* dataset, after regressing out the cell cycle effects, G1 is correctly separated from G2M/S. In *Lambrechts*, there is no clear separation. Having eyeballed at the UMAP-plot",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/526
https://github.com/scverse/scanpy/issues/526:156,testability,regress,regressing,156,"Limitations of regress_out as a standard processing step? ; According to the PBMC 3k tutorial, which I consider as the 'best practice' tutorial for scanpy, regressing out the fraction of mitochondrial reads and the number of detected genes is recommended as a 'standard processing step'. . Having analysed two different datasets, I am so sure anymore if this is a good idea. **number of detected genes**. I loaded these datasets into scanpy and processed them according to the 3k PBMC tutorial: . * [Savas et al., 2018](https://doi.org/10.1038/s41591-018-0078-7), ~6k cells, CD3+ T cells, BRCA. * [Lambrechts et al., 2018](https://doi.org/10.1038/s41591-018-0096-5), ~32k cells, whole tissue NSCLC. Regress-out seems to perfectly do its jobs on the *Savas et al.* dataset, that contains closely related cell types (1st row of figure): The 2nd PC is confounded by the number of detected genes and this effect is reduced. . On the *Lambrechts et al* dataset, that contains all kinds of cells (cancer, stromal, immune), this looks differently: Neither of the first 2 PCs seems to be related to the number of detected genes and it actually seems to me that I am 'loosing' information by applying `regress_out` (everything is now a single blob). . **cell cycle**. The lower part of the plot shows regress out applied to the cell cycle (following [the scanpy tutorial](https://nbviewer.jupyter.org/github/theislab/scanpy_usage/blob/master/180209_cell_cycle/cell_cycle.ipynb) and the 'alternative approach' described in the [seurat vignette](https://satijalab.org/seurat/cell_cycle_vignette.html#assign-cell-cycle-scores), i.e. I regressed out the difference between the G2M and S phase scores):. ```. adata.obs[""cell_cycle_diff""] = adata.obs[""S_score""] - adata.obs[""G2M_score""]. sc.pp.regress_out(adata, ['cell_cycle_diff']). ```. Like that, the differences between dividing and non-dividing cells should be preserved. . Again, in the *Savas* dataset, after regressing out the cell cycle effects, G1 is cor",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/526
https://github.com/scverse/scanpy/issues/526:699,testability,Regress,Regress-out,699,"Limitations of regress_out as a standard processing step? ; According to the PBMC 3k tutorial, which I consider as the 'best practice' tutorial for scanpy, regressing out the fraction of mitochondrial reads and the number of detected genes is recommended as a 'standard processing step'. . Having analysed two different datasets, I am so sure anymore if this is a good idea. **number of detected genes**. I loaded these datasets into scanpy and processed them according to the 3k PBMC tutorial: . * [Savas et al., 2018](https://doi.org/10.1038/s41591-018-0078-7), ~6k cells, CD3+ T cells, BRCA. * [Lambrechts et al., 2018](https://doi.org/10.1038/s41591-018-0096-5), ~32k cells, whole tissue NSCLC. Regress-out seems to perfectly do its jobs on the *Savas et al.* dataset, that contains closely related cell types (1st row of figure): The 2nd PC is confounded by the number of detected genes and this effect is reduced. . On the *Lambrechts et al* dataset, that contains all kinds of cells (cancer, stromal, immune), this looks differently: Neither of the first 2 PCs seems to be related to the number of detected genes and it actually seems to me that I am 'loosing' information by applying `regress_out` (everything is now a single blob). . **cell cycle**. The lower part of the plot shows regress out applied to the cell cycle (following [the scanpy tutorial](https://nbviewer.jupyter.org/github/theislab/scanpy_usage/blob/master/180209_cell_cycle/cell_cycle.ipynb) and the 'alternative approach' described in the [seurat vignette](https://satijalab.org/seurat/cell_cycle_vignette.html#assign-cell-cycle-scores), i.e. I regressed out the difference between the G2M and S phase scores):. ```. adata.obs[""cell_cycle_diff""] = adata.obs[""S_score""] - adata.obs[""G2M_score""]. sc.pp.regress_out(adata, ['cell_cycle_diff']). ```. Like that, the differences between dividing and non-dividing cells should be preserved. . Again, in the *Savas* dataset, after regressing out the cell cycle effects, G1 is cor",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/526
https://github.com/scverse/scanpy/issues/526:1292,testability,regress,regress,1292," analysed two different datasets, I am so sure anymore if this is a good idea. **number of detected genes**. I loaded these datasets into scanpy and processed them according to the 3k PBMC tutorial: . * [Savas et al., 2018](https://doi.org/10.1038/s41591-018-0078-7), ~6k cells, CD3+ T cells, BRCA. * [Lambrechts et al., 2018](https://doi.org/10.1038/s41591-018-0096-5), ~32k cells, whole tissue NSCLC. Regress-out seems to perfectly do its jobs on the *Savas et al.* dataset, that contains closely related cell types (1st row of figure): The 2nd PC is confounded by the number of detected genes and this effect is reduced. . On the *Lambrechts et al* dataset, that contains all kinds of cells (cancer, stromal, immune), this looks differently: Neither of the first 2 PCs seems to be related to the number of detected genes and it actually seems to me that I am 'loosing' information by applying `regress_out` (everything is now a single blob). . **cell cycle**. The lower part of the plot shows regress out applied to the cell cycle (following [the scanpy tutorial](https://nbviewer.jupyter.org/github/theislab/scanpy_usage/blob/master/180209_cell_cycle/cell_cycle.ipynb) and the 'alternative approach' described in the [seurat vignette](https://satijalab.org/seurat/cell_cycle_vignette.html#assign-cell-cycle-scores), i.e. I regressed out the difference between the G2M and S phase scores):. ```. adata.obs[""cell_cycle_diff""] = adata.obs[""S_score""] - adata.obs[""G2M_score""]. sc.pp.regress_out(adata, ['cell_cycle_diff']). ```. Like that, the differences between dividing and non-dividing cells should be preserved. . Again, in the *Savas* dataset, after regressing out the cell cycle effects, G1 is correctly separated from G2M/S. In *Lambrechts*, there is no clear separation. Having eyeballed at the UMAP-plot (below) it seems that the cell-cycle labels correlate with the cell type (i.e. cancer cells and myeloid cells got the G1 label assigned more likely than T cells). . **What is 'best pract",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/526
https://github.com/scverse/scanpy/issues/526:1623,testability,regress,regressed,1623,"://doi.org/10.1038/s41591-018-0096-5), ~32k cells, whole tissue NSCLC. Regress-out seems to perfectly do its jobs on the *Savas et al.* dataset, that contains closely related cell types (1st row of figure): The 2nd PC is confounded by the number of detected genes and this effect is reduced. . On the *Lambrechts et al* dataset, that contains all kinds of cells (cancer, stromal, immune), this looks differently: Neither of the first 2 PCs seems to be related to the number of detected genes and it actually seems to me that I am 'loosing' information by applying `regress_out` (everything is now a single blob). . **cell cycle**. The lower part of the plot shows regress out applied to the cell cycle (following [the scanpy tutorial](https://nbviewer.jupyter.org/github/theislab/scanpy_usage/blob/master/180209_cell_cycle/cell_cycle.ipynb) and the 'alternative approach' described in the [seurat vignette](https://satijalab.org/seurat/cell_cycle_vignette.html#assign-cell-cycle-scores), i.e. I regressed out the difference between the G2M and S phase scores):. ```. adata.obs[""cell_cycle_diff""] = adata.obs[""S_score""] - adata.obs[""G2M_score""]. sc.pp.regress_out(adata, ['cell_cycle_diff']). ```. Like that, the differences between dividing and non-dividing cells should be preserved. . Again, in the *Savas* dataset, after regressing out the cell cycle effects, G1 is correctly separated from G2M/S. In *Lambrechts*, there is no clear separation. Having eyeballed at the UMAP-plot (below) it seems that the cell-cycle labels correlate with the cell type (i.e. cancer cells and myeloid cells got the G1 label assigned more likely than T cells). . **What is 'best practice'?**. I quickly discussed this offline with @flying-sheep, and he encouraged me to create this issue. . * Is it just a problem with visualizing the first PC's and `regress_out` should be applied regardless. * Should `regress_out` be skipped and only applied in a more downstream step when focusing on a single cell type? . * Are ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/526
https://github.com/scverse/scanpy/issues/526:1952,testability,regress,regressing,1952,"that contains all kinds of cells (cancer, stromal, immune), this looks differently: Neither of the first 2 PCs seems to be related to the number of detected genes and it actually seems to me that I am 'loosing' information by applying `regress_out` (everything is now a single blob). . **cell cycle**. The lower part of the plot shows regress out applied to the cell cycle (following [the scanpy tutorial](https://nbviewer.jupyter.org/github/theislab/scanpy_usage/blob/master/180209_cell_cycle/cell_cycle.ipynb) and the 'alternative approach' described in the [seurat vignette](https://satijalab.org/seurat/cell_cycle_vignette.html#assign-cell-cycle-scores), i.e. I regressed out the difference between the G2M and S phase scores):. ```. adata.obs[""cell_cycle_diff""] = adata.obs[""S_score""] - adata.obs[""G2M_score""]. sc.pp.regress_out(adata, ['cell_cycle_diff']). ```. Like that, the differences between dividing and non-dividing cells should be preserved. . Again, in the *Savas* dataset, after regressing out the cell cycle effects, G1 is correctly separated from G2M/S. In *Lambrechts*, there is no clear separation. Having eyeballed at the UMAP-plot (below) it seems that the cell-cycle labels correlate with the cell type (i.e. cancer cells and myeloid cells got the G1 label assigned more likely than T cells). . **What is 'best practice'?**. I quickly discussed this offline with @flying-sheep, and he encouraged me to create this issue. . * Is it just a problem with visualizing the first PC's and `regress_out` should be applied regardless. * Should `regress_out` be skipped and only applied in a more downstream step when focusing on a single cell type? . * Are there any other situations where `regress_out` could do more harm than good? . **PCA plots before and after `regress_out`**. ![regress_out](https://user-images.githubusercontent.com/7051479/54083302-f088f500-4321-11e9-877a-1cbef6f4f489.png). **UMAP-plots**. The cell cycle label correlates with the cell type (other dataset, but ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/526
https://github.com/scverse/scanpy/issues/526:787,usability,close,closely,787,"Limitations of regress_out as a standard processing step? ; According to the PBMC 3k tutorial, which I consider as the 'best practice' tutorial for scanpy, regressing out the fraction of mitochondrial reads and the number of detected genes is recommended as a 'standard processing step'. . Having analysed two different datasets, I am so sure anymore if this is a good idea. **number of detected genes**. I loaded these datasets into scanpy and processed them according to the 3k PBMC tutorial: . * [Savas et al., 2018](https://doi.org/10.1038/s41591-018-0078-7), ~6k cells, CD3+ T cells, BRCA. * [Lambrechts et al., 2018](https://doi.org/10.1038/s41591-018-0096-5), ~32k cells, whole tissue NSCLC. Regress-out seems to perfectly do its jobs on the *Savas et al.* dataset, that contains closely related cell types (1st row of figure): The 2nd PC is confounded by the number of detected genes and this effect is reduced. . On the *Lambrechts et al* dataset, that contains all kinds of cells (cancer, stromal, immune), this looks differently: Neither of the first 2 PCs seems to be related to the number of detected genes and it actually seems to me that I am 'loosing' information by applying `regress_out` (everything is now a single blob). . **cell cycle**. The lower part of the plot shows regress out applied to the cell cycle (following [the scanpy tutorial](https://nbviewer.jupyter.org/github/theislab/scanpy_usage/blob/master/180209_cell_cycle/cell_cycle.ipynb) and the 'alternative approach' described in the [seurat vignette](https://satijalab.org/seurat/cell_cycle_vignette.html#assign-cell-cycle-scores), i.e. I regressed out the difference between the G2M and S phase scores):. ```. adata.obs[""cell_cycle_diff""] = adata.obs[""S_score""] - adata.obs[""G2M_score""]. sc.pp.regress_out(adata, ['cell_cycle_diff']). ```. Like that, the differences between dividing and non-dividing cells should be preserved. . Again, in the *Savas* dataset, after regressing out the cell cycle effects, G1 is cor",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/526
https://github.com/scverse/scanpy/issues/526:2058,usability,clear,clear,2058," 2 PCs seems to be related to the number of detected genes and it actually seems to me that I am 'loosing' information by applying `regress_out` (everything is now a single blob). . **cell cycle**. The lower part of the plot shows regress out applied to the cell cycle (following [the scanpy tutorial](https://nbviewer.jupyter.org/github/theislab/scanpy_usage/blob/master/180209_cell_cycle/cell_cycle.ipynb) and the 'alternative approach' described in the [seurat vignette](https://satijalab.org/seurat/cell_cycle_vignette.html#assign-cell-cycle-scores), i.e. I regressed out the difference between the G2M and S phase scores):. ```. adata.obs[""cell_cycle_diff""] = adata.obs[""S_score""] - adata.obs[""G2M_score""]. sc.pp.regress_out(adata, ['cell_cycle_diff']). ```. Like that, the differences between dividing and non-dividing cells should be preserved. . Again, in the *Savas* dataset, after regressing out the cell cycle effects, G1 is correctly separated from G2M/S. In *Lambrechts*, there is no clear separation. Having eyeballed at the UMAP-plot (below) it seems that the cell-cycle labels correlate with the cell type (i.e. cancer cells and myeloid cells got the G1 label assigned more likely than T cells). . **What is 'best practice'?**. I quickly discussed this offline with @flying-sheep, and he encouraged me to create this issue. . * Is it just a problem with visualizing the first PC's and `regress_out` should be applied regardless. * Should `regress_out` be skipped and only applied in a more downstream step when focusing on a single cell type? . * Are there any other situations where `regress_out` could do more harm than good? . **PCA plots before and after `regress_out`**. ![regress_out](https://user-images.githubusercontent.com/7051479/54083302-f088f500-4321-11e9-877a-1cbef6f4f489.png). **UMAP-plots**. The cell cycle label correlates with the cell type (other dataset, but to show what I mean): . ![2019-03-10_11:20:31_384x234](https://user-images.githubusercontent.com/7051479",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/526
https://github.com/scverse/scanpy/issues/526:2431,usability,visual,visualizing,2431,"ell cycle**. The lower part of the plot shows regress out applied to the cell cycle (following [the scanpy tutorial](https://nbviewer.jupyter.org/github/theislab/scanpy_usage/blob/master/180209_cell_cycle/cell_cycle.ipynb) and the 'alternative approach' described in the [seurat vignette](https://satijalab.org/seurat/cell_cycle_vignette.html#assign-cell-cycle-scores), i.e. I regressed out the difference between the G2M and S phase scores):. ```. adata.obs[""cell_cycle_diff""] = adata.obs[""S_score""] - adata.obs[""G2M_score""]. sc.pp.regress_out(adata, ['cell_cycle_diff']). ```. Like that, the differences between dividing and non-dividing cells should be preserved. . Again, in the *Savas* dataset, after regressing out the cell cycle effects, G1 is correctly separated from G2M/S. In *Lambrechts*, there is no clear separation. Having eyeballed at the UMAP-plot (below) it seems that the cell-cycle labels correlate with the cell type (i.e. cancer cells and myeloid cells got the G1 label assigned more likely than T cells). . **What is 'best practice'?**. I quickly discussed this offline with @flying-sheep, and he encouraged me to create this issue. . * Is it just a problem with visualizing the first PC's and `regress_out` should be applied regardless. * Should `regress_out` be skipped and only applied in a more downstream step when focusing on a single cell type? . * Are there any other situations where `regress_out` could do more harm than good? . **PCA plots before and after `regress_out`**. ![regress_out](https://user-images.githubusercontent.com/7051479/54083302-f088f500-4321-11e9-877a-1cbef6f4f489.png). **UMAP-plots**. The cell cycle label correlates with the cell type (other dataset, but to show what I mean): . ![2019-03-10_11:20:31_384x234](https://user-images.githubusercontent.com/7051479/54083671-29779880-4327-11e9-94d6-9be34383b909.png). ![2019-03-10_11:25:30_428x231](https://user-images.githubusercontent.com/7051479/54083675-3a280e80-4327-11e9-954f-34ef1404961b.png).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/526
https://github.com/scverse/scanpy/issues/526:2776,usability,user,user-images,2776,"ell cycle**. The lower part of the plot shows regress out applied to the cell cycle (following [the scanpy tutorial](https://nbviewer.jupyter.org/github/theislab/scanpy_usage/blob/master/180209_cell_cycle/cell_cycle.ipynb) and the 'alternative approach' described in the [seurat vignette](https://satijalab.org/seurat/cell_cycle_vignette.html#assign-cell-cycle-scores), i.e. I regressed out the difference between the G2M and S phase scores):. ```. adata.obs[""cell_cycle_diff""] = adata.obs[""S_score""] - adata.obs[""G2M_score""]. sc.pp.regress_out(adata, ['cell_cycle_diff']). ```. Like that, the differences between dividing and non-dividing cells should be preserved. . Again, in the *Savas* dataset, after regressing out the cell cycle effects, G1 is correctly separated from G2M/S. In *Lambrechts*, there is no clear separation. Having eyeballed at the UMAP-plot (below) it seems that the cell-cycle labels correlate with the cell type (i.e. cancer cells and myeloid cells got the G1 label assigned more likely than T cells). . **What is 'best practice'?**. I quickly discussed this offline with @flying-sheep, and he encouraged me to create this issue. . * Is it just a problem with visualizing the first PC's and `regress_out` should be applied regardless. * Should `regress_out` be skipped and only applied in a more downstream step when focusing on a single cell type? . * Are there any other situations where `regress_out` could do more harm than good? . **PCA plots before and after `regress_out`**. ![regress_out](https://user-images.githubusercontent.com/7051479/54083302-f088f500-4321-11e9-877a-1cbef6f4f489.png). **UMAP-plots**. The cell cycle label correlates with the cell type (other dataset, but to show what I mean): . ![2019-03-10_11:20:31_384x234](https://user-images.githubusercontent.com/7051479/54083671-29779880-4327-11e9-94d6-9be34383b909.png). ![2019-03-10_11:25:30_428x231](https://user-images.githubusercontent.com/7051479/54083675-3a280e80-4327-11e9-954f-34ef1404961b.png).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/526
https://github.com/scverse/scanpy/issues/526:3020,usability,user,user-images,3020,"ell cycle**. The lower part of the plot shows regress out applied to the cell cycle (following [the scanpy tutorial](https://nbviewer.jupyter.org/github/theislab/scanpy_usage/blob/master/180209_cell_cycle/cell_cycle.ipynb) and the 'alternative approach' described in the [seurat vignette](https://satijalab.org/seurat/cell_cycle_vignette.html#assign-cell-cycle-scores), i.e. I regressed out the difference between the G2M and S phase scores):. ```. adata.obs[""cell_cycle_diff""] = adata.obs[""S_score""] - adata.obs[""G2M_score""]. sc.pp.regress_out(adata, ['cell_cycle_diff']). ```. Like that, the differences between dividing and non-dividing cells should be preserved. . Again, in the *Savas* dataset, after regressing out the cell cycle effects, G1 is correctly separated from G2M/S. In *Lambrechts*, there is no clear separation. Having eyeballed at the UMAP-plot (below) it seems that the cell-cycle labels correlate with the cell type (i.e. cancer cells and myeloid cells got the G1 label assigned more likely than T cells). . **What is 'best practice'?**. I quickly discussed this offline with @flying-sheep, and he encouraged me to create this issue. . * Is it just a problem with visualizing the first PC's and `regress_out` should be applied regardless. * Should `regress_out` be skipped and only applied in a more downstream step when focusing on a single cell type? . * Are there any other situations where `regress_out` could do more harm than good? . **PCA plots before and after `regress_out`**. ![regress_out](https://user-images.githubusercontent.com/7051479/54083302-f088f500-4321-11e9-877a-1cbef6f4f489.png). **UMAP-plots**. The cell cycle label correlates with the cell type (other dataset, but to show what I mean): . ![2019-03-10_11:20:31_384x234](https://user-images.githubusercontent.com/7051479/54083671-29779880-4327-11e9-94d6-9be34383b909.png). ![2019-03-10_11:25:30_428x231](https://user-images.githubusercontent.com/7051479/54083675-3a280e80-4327-11e9-954f-34ef1404961b.png).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/526
https://github.com/scverse/scanpy/issues/526:3153,usability,user,user-images,3153,"ell cycle**. The lower part of the plot shows regress out applied to the cell cycle (following [the scanpy tutorial](https://nbviewer.jupyter.org/github/theislab/scanpy_usage/blob/master/180209_cell_cycle/cell_cycle.ipynb) and the 'alternative approach' described in the [seurat vignette](https://satijalab.org/seurat/cell_cycle_vignette.html#assign-cell-cycle-scores), i.e. I regressed out the difference between the G2M and S phase scores):. ```. adata.obs[""cell_cycle_diff""] = adata.obs[""S_score""] - adata.obs[""G2M_score""]. sc.pp.regress_out(adata, ['cell_cycle_diff']). ```. Like that, the differences between dividing and non-dividing cells should be preserved. . Again, in the *Savas* dataset, after regressing out the cell cycle effects, G1 is correctly separated from G2M/S. In *Lambrechts*, there is no clear separation. Having eyeballed at the UMAP-plot (below) it seems that the cell-cycle labels correlate with the cell type (i.e. cancer cells and myeloid cells got the G1 label assigned more likely than T cells). . **What is 'best practice'?**. I quickly discussed this offline with @flying-sheep, and he encouraged me to create this issue. . * Is it just a problem with visualizing the first PC's and `regress_out` should be applied regardless. * Should `regress_out` be skipped and only applied in a more downstream step when focusing on a single cell type? . * Are there any other situations where `regress_out` could do more harm than good? . **PCA plots before and after `regress_out`**. ![regress_out](https://user-images.githubusercontent.com/7051479/54083302-f088f500-4321-11e9-877a-1cbef6f4f489.png). **UMAP-plots**. The cell cycle label correlates with the cell type (other dataset, but to show what I mean): . ![2019-03-10_11:20:31_384x234](https://user-images.githubusercontent.com/7051479/54083671-29779880-4327-11e9-94d6-9be34383b909.png). ![2019-03-10_11:25:30_428x231](https://user-images.githubusercontent.com/7051479/54083675-3a280e80-4327-11e9-954f-34ef1404961b.png).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/526
https://github.com/scverse/scanpy/pull/527:121,availability,error,error,121,Quick fix for typo; Looks like there was a typo at the bottom of `scanpy/preprocessing/_dca.py` that was causing a parse error. This should fix it.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/527
https://github.com/scverse/scanpy/pull/527:121,performance,error,error,121,Quick fix for typo; Looks like there was a typo at the bottom of `scanpy/preprocessing/_dca.py` that was causing a parse error. This should fix it.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/527
https://github.com/scverse/scanpy/pull/527:121,safety,error,error,121,Quick fix for typo; Looks like there was a typo at the bottom of `scanpy/preprocessing/_dca.py` that was causing a parse error. This should fix it.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/527
https://github.com/scverse/scanpy/pull/527:121,usability,error,error,121,Quick fix for typo; Looks like there was a typo at the bottom of `scanpy/preprocessing/_dca.py` that was causing a parse error. This should fix it.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/527
https://github.com/scverse/scanpy/pull/528:220,deployability,integr,integrated,220,"standard_scale to plots; This PR extends the original PR #512 by @gokceneraslan which adds the `standard_scaling` parameter to matrixplot. . I added the same functionality to dotplot, heatmap and stacked_violin. Also, I integrated PR #524 by @sjfleming which adds a `smallest_dot` option to dotplot.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/528
https://github.com/scverse/scanpy/pull/528:184,energy efficiency,heat,heatmap,184,"standard_scale to plots; This PR extends the original PR #512 by @gokceneraslan which adds the `standard_scaling` parameter to matrixplot. . I added the same functionality to dotplot, heatmap and stacked_violin. Also, I integrated PR #524 by @sjfleming which adds a `smallest_dot` option to dotplot.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/528
https://github.com/scverse/scanpy/pull/528:220,integrability,integr,integrated,220,"standard_scale to plots; This PR extends the original PR #512 by @gokceneraslan which adds the `standard_scaling` parameter to matrixplot. . I added the same functionality to dotplot, heatmap and stacked_violin. Also, I integrated PR #524 by @sjfleming which adds a `smallest_dot` option to dotplot.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/528
https://github.com/scverse/scanpy/pull/528:220,interoperability,integr,integrated,220,"standard_scale to plots; This PR extends the original PR #512 by @gokceneraslan which adds the `standard_scaling` parameter to matrixplot. . I added the same functionality to dotplot, heatmap and stacked_violin. Also, I integrated PR #524 by @sjfleming which adds a `smallest_dot` option to dotplot.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/528
https://github.com/scverse/scanpy/pull/528:33,modifiability,exten,extends,33,"standard_scale to plots; This PR extends the original PR #512 by @gokceneraslan which adds the `standard_scaling` parameter to matrixplot. . I added the same functionality to dotplot, heatmap and stacked_violin. Also, I integrated PR #524 by @sjfleming which adds a `smallest_dot` option to dotplot.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/528
https://github.com/scverse/scanpy/pull/528:114,modifiability,paramet,parameter,114,"standard_scale to plots; This PR extends the original PR #512 by @gokceneraslan which adds the `standard_scaling` parameter to matrixplot. . I added the same functionality to dotplot, heatmap and stacked_violin. Also, I integrated PR #524 by @sjfleming which adds a `smallest_dot` option to dotplot.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/528
https://github.com/scverse/scanpy/pull/528:220,modifiability,integr,integrated,220,"standard_scale to plots; This PR extends the original PR #512 by @gokceneraslan which adds the `standard_scaling` parameter to matrixplot. . I added the same functionality to dotplot, heatmap and stacked_violin. Also, I integrated PR #524 by @sjfleming which adds a `smallest_dot` option to dotplot.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/528
https://github.com/scverse/scanpy/pull/528:220,reliability,integr,integrated,220,"standard_scale to plots; This PR extends the original PR #512 by @gokceneraslan which adds the `standard_scaling` parameter to matrixplot. . I added the same functionality to dotplot, heatmap and stacked_violin. Also, I integrated PR #524 by @sjfleming which adds a `smallest_dot` option to dotplot.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/528
https://github.com/scverse/scanpy/pull/528:220,security,integr,integrated,220,"standard_scale to plots; This PR extends the original PR #512 by @gokceneraslan which adds the `standard_scaling` parameter to matrixplot. . I added the same functionality to dotplot, heatmap and stacked_violin. Also, I integrated PR #524 by @sjfleming which adds a `smallest_dot` option to dotplot.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/528
https://github.com/scverse/scanpy/pull/528:220,testability,integr,integrated,220,"standard_scale to plots; This PR extends the original PR #512 by @gokceneraslan which adds the `standard_scaling` parameter to matrixplot. . I added the same functionality to dotplot, heatmap and stacked_violin. Also, I integrated PR #524 by @sjfleming which adds a `smallest_dot` option to dotplot.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/528
https://github.com/scverse/scanpy/issues/530:12,availability,error,error,12,"Math domain error in rank_genes_groups function; I get the error below when trying to run the following:. `>>> sc.tl.rank_genes_groups(adata, 'louvain', groups=['5','16','19','30'], reference='0', method='wilcoxon')`. ```bash. C:\Users\myuser\Anaconda3\lib\site-packages\scanpy\tools\_rank_genes_groups.py:298: RuntimeWarning: overflow encountered in long_scalars. (n_active * m_active * (n_active + m_active + 1) / 12)). ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-160-dd19114ff660> in <module>. 1 #adata.obs['groups'] = ['group 1'= ['0'], 'group 2'= ['5','16','19','30']]. ----> 2 sc.tl.rank_genes_groups(adata, 'louvain', groups=['5','16','19','30'], reference='0', method='wilcoxon') # wilcoxon-rank-sum/mann-whitney u test, the default of Seurat. ~\Anaconda3\lib\site-packages\scanpy\tools\_rank_genes_groups.py in rank_genes_groups(adata, groupby, use_raw, groups, reference, n_genes, rankby_abs, key_added, copy, method, corr_method, **kwds). 296 . 297 scores = (scores - (n_active * (n_active + m_active + 1) / 2)) / sqrt(. --> 298 (n_active * m_active * (n_active + m_active + 1) / 12)). 299 scores[np.isnan(scores)] = 0. 300 pvals = 2 * stats.distributions.norm.sf(np.abs(scores)). ValueError: math domain error. ```. Here `adata` is real data from our lab, not the tutorial data. Have been trying to replicate the cluster analysis tutorial. All previous steps work fine. Interestingly, if I remove group '5' from the list of groups it works. Also, this error only happens with the `wilcoxon` method, not with `t-test`.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/530
https://github.com/scverse/scanpy/issues/530:59,availability,error,error,59,"Math domain error in rank_genes_groups function; I get the error below when trying to run the following:. `>>> sc.tl.rank_genes_groups(adata, 'louvain', groups=['5','16','19','30'], reference='0', method='wilcoxon')`. ```bash. C:\Users\myuser\Anaconda3\lib\site-packages\scanpy\tools\_rank_genes_groups.py:298: RuntimeWarning: overflow encountered in long_scalars. (n_active * m_active * (n_active + m_active + 1) / 12)). ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-160-dd19114ff660> in <module>. 1 #adata.obs['groups'] = ['group 1'= ['0'], 'group 2'= ['5','16','19','30']]. ----> 2 sc.tl.rank_genes_groups(adata, 'louvain', groups=['5','16','19','30'], reference='0', method='wilcoxon') # wilcoxon-rank-sum/mann-whitney u test, the default of Seurat. ~\Anaconda3\lib\site-packages\scanpy\tools\_rank_genes_groups.py in rank_genes_groups(adata, groupby, use_raw, groups, reference, n_genes, rankby_abs, key_added, copy, method, corr_method, **kwds). 296 . 297 scores = (scores - (n_active * (n_active + m_active + 1) / 2)) / sqrt(. --> 298 (n_active * m_active * (n_active + m_active + 1) / 12)). 299 scores[np.isnan(scores)] = 0. 300 pvals = 2 * stats.distributions.norm.sf(np.abs(scores)). ValueError: math domain error. ```. Here `adata` is real data from our lab, not the tutorial data. Have been trying to replicate the cluster analysis tutorial. All previous steps work fine. Interestingly, if I remove group '5' from the list of groups it works. Also, this error only happens with the `wilcoxon` method, not with `t-test`.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/530
https://github.com/scverse/scanpy/issues/530:1310,availability,error,error,1310,"Math domain error in rank_genes_groups function; I get the error below when trying to run the following:. `>>> sc.tl.rank_genes_groups(adata, 'louvain', groups=['5','16','19','30'], reference='0', method='wilcoxon')`. ```bash. C:\Users\myuser\Anaconda3\lib\site-packages\scanpy\tools\_rank_genes_groups.py:298: RuntimeWarning: overflow encountered in long_scalars. (n_active * m_active * (n_active + m_active + 1) / 12)). ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-160-dd19114ff660> in <module>. 1 #adata.obs['groups'] = ['group 1'= ['0'], 'group 2'= ['5','16','19','30']]. ----> 2 sc.tl.rank_genes_groups(adata, 'louvain', groups=['5','16','19','30'], reference='0', method='wilcoxon') # wilcoxon-rank-sum/mann-whitney u test, the default of Seurat. ~\Anaconda3\lib\site-packages\scanpy\tools\_rank_genes_groups.py in rank_genes_groups(adata, groupby, use_raw, groups, reference, n_genes, rankby_abs, key_added, copy, method, corr_method, **kwds). 296 . 297 scores = (scores - (n_active * (n_active + m_active + 1) / 2)) / sqrt(. --> 298 (n_active * m_active * (n_active + m_active + 1) / 12)). 299 scores[np.isnan(scores)] = 0. 300 pvals = 2 * stats.distributions.norm.sf(np.abs(scores)). ValueError: math domain error. ```. Here `adata` is real data from our lab, not the tutorial data. Have been trying to replicate the cluster analysis tutorial. All previous steps work fine. Interestingly, if I remove group '5' from the list of groups it works. Also, this error only happens with the `wilcoxon` method, not with `t-test`.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/530
https://github.com/scverse/scanpy/issues/530:1405,availability,replic,replicate,1405,"Math domain error in rank_genes_groups function; I get the error below when trying to run the following:. `>>> sc.tl.rank_genes_groups(adata, 'louvain', groups=['5','16','19','30'], reference='0', method='wilcoxon')`. ```bash. C:\Users\myuser\Anaconda3\lib\site-packages\scanpy\tools\_rank_genes_groups.py:298: RuntimeWarning: overflow encountered in long_scalars. (n_active * m_active * (n_active + m_active + 1) / 12)). ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-160-dd19114ff660> in <module>. 1 #adata.obs['groups'] = ['group 1'= ['0'], 'group 2'= ['5','16','19','30']]. ----> 2 sc.tl.rank_genes_groups(adata, 'louvain', groups=['5','16','19','30'], reference='0', method='wilcoxon') # wilcoxon-rank-sum/mann-whitney u test, the default of Seurat. ~\Anaconda3\lib\site-packages\scanpy\tools\_rank_genes_groups.py in rank_genes_groups(adata, groupby, use_raw, groups, reference, n_genes, rankby_abs, key_added, copy, method, corr_method, **kwds). 296 . 297 scores = (scores - (n_active * (n_active + m_active + 1) / 2)) / sqrt(. --> 298 (n_active * m_active * (n_active + m_active + 1) / 12)). 299 scores[np.isnan(scores)] = 0. 300 pvals = 2 * stats.distributions.norm.sf(np.abs(scores)). ValueError: math domain error. ```. Here `adata` is real data from our lab, not the tutorial data. Have been trying to replicate the cluster analysis tutorial. All previous steps work fine. Interestingly, if I remove group '5' from the list of groups it works. Also, this error only happens with the `wilcoxon` method, not with `t-test`.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/530
https://github.com/scverse/scanpy/issues/530:1419,availability,cluster,cluster,1419,"Math domain error in rank_genes_groups function; I get the error below when trying to run the following:. `>>> sc.tl.rank_genes_groups(adata, 'louvain', groups=['5','16','19','30'], reference='0', method='wilcoxon')`. ```bash. C:\Users\myuser\Anaconda3\lib\site-packages\scanpy\tools\_rank_genes_groups.py:298: RuntimeWarning: overflow encountered in long_scalars. (n_active * m_active * (n_active + m_active + 1) / 12)). ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-160-dd19114ff660> in <module>. 1 #adata.obs['groups'] = ['group 1'= ['0'], 'group 2'= ['5','16','19','30']]. ----> 2 sc.tl.rank_genes_groups(adata, 'louvain', groups=['5','16','19','30'], reference='0', method='wilcoxon') # wilcoxon-rank-sum/mann-whitney u test, the default of Seurat. ~\Anaconda3\lib\site-packages\scanpy\tools\_rank_genes_groups.py in rank_genes_groups(adata, groupby, use_raw, groups, reference, n_genes, rankby_abs, key_added, copy, method, corr_method, **kwds). 296 . 297 scores = (scores - (n_active * (n_active + m_active + 1) / 2)) / sqrt(. --> 298 (n_active * m_active * (n_active + m_active + 1) / 12)). 299 scores[np.isnan(scores)] = 0. 300 pvals = 2 * stats.distributions.norm.sf(np.abs(scores)). ValueError: math domain error. ```. Here `adata` is real data from our lab, not the tutorial data. Have been trying to replicate the cluster analysis tutorial. All previous steps work fine. Interestingly, if I remove group '5' from the list of groups it works. Also, this error only happens with the `wilcoxon` method, not with `t-test`.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/530
https://github.com/scverse/scanpy/issues/530:1558,availability,error,error,1558,"Math domain error in rank_genes_groups function; I get the error below when trying to run the following:. `>>> sc.tl.rank_genes_groups(adata, 'louvain', groups=['5','16','19','30'], reference='0', method='wilcoxon')`. ```bash. C:\Users\myuser\Anaconda3\lib\site-packages\scanpy\tools\_rank_genes_groups.py:298: RuntimeWarning: overflow encountered in long_scalars. (n_active * m_active * (n_active + m_active + 1) / 12)). ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-160-dd19114ff660> in <module>. 1 #adata.obs['groups'] = ['group 1'= ['0'], 'group 2'= ['5','16','19','30']]. ----> 2 sc.tl.rank_genes_groups(adata, 'louvain', groups=['5','16','19','30'], reference='0', method='wilcoxon') # wilcoxon-rank-sum/mann-whitney u test, the default of Seurat. ~\Anaconda3\lib\site-packages\scanpy\tools\_rank_genes_groups.py in rank_genes_groups(adata, groupby, use_raw, groups, reference, n_genes, rankby_abs, key_added, copy, method, corr_method, **kwds). 296 . 297 scores = (scores - (n_active * (n_active + m_active + 1) / 2)) / sqrt(. --> 298 (n_active * m_active * (n_active + m_active + 1) / 12)). 299 scores[np.isnan(scores)] = 0. 300 pvals = 2 * stats.distributions.norm.sf(np.abs(scores)). ValueError: math domain error. ```. Here `adata` is real data from our lab, not the tutorial data. Have been trying to replicate the cluster analysis tutorial. All previous steps work fine. Interestingly, if I remove group '5' from the list of groups it works. Also, this error only happens with the `wilcoxon` method, not with `t-test`.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/530
https://github.com/scverse/scanpy/issues/530:582,deployability,modul,module,582,"Math domain error in rank_genes_groups function; I get the error below when trying to run the following:. `>>> sc.tl.rank_genes_groups(adata, 'louvain', groups=['5','16','19','30'], reference='0', method='wilcoxon')`. ```bash. C:\Users\myuser\Anaconda3\lib\site-packages\scanpy\tools\_rank_genes_groups.py:298: RuntimeWarning: overflow encountered in long_scalars. (n_active * m_active * (n_active + m_active + 1) / 12)). ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-160-dd19114ff660> in <module>. 1 #adata.obs['groups'] = ['group 1'= ['0'], 'group 2'= ['5','16','19','30']]. ----> 2 sc.tl.rank_genes_groups(adata, 'louvain', groups=['5','16','19','30'], reference='0', method='wilcoxon') # wilcoxon-rank-sum/mann-whitney u test, the default of Seurat. ~\Anaconda3\lib\site-packages\scanpy\tools\_rank_genes_groups.py in rank_genes_groups(adata, groupby, use_raw, groups, reference, n_genes, rankby_abs, key_added, copy, method, corr_method, **kwds). 296 . 297 scores = (scores - (n_active * (n_active + m_active + 1) / 2)) / sqrt(. --> 298 (n_active * m_active * (n_active + m_active + 1) / 12)). 299 scores[np.isnan(scores)] = 0. 300 pvals = 2 * stats.distributions.norm.sf(np.abs(scores)). ValueError: math domain error. ```. Here `adata` is real data from our lab, not the tutorial data. Have been trying to replicate the cluster analysis tutorial. All previous steps work fine. Interestingly, if I remove group '5' from the list of groups it works. Also, this error only happens with the `wilcoxon` method, not with `t-test`.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/530
https://github.com/scverse/scanpy/issues/530:1419,deployability,cluster,cluster,1419,"Math domain error in rank_genes_groups function; I get the error below when trying to run the following:. `>>> sc.tl.rank_genes_groups(adata, 'louvain', groups=['5','16','19','30'], reference='0', method='wilcoxon')`. ```bash. C:\Users\myuser\Anaconda3\lib\site-packages\scanpy\tools\_rank_genes_groups.py:298: RuntimeWarning: overflow encountered in long_scalars. (n_active * m_active * (n_active + m_active + 1) / 12)). ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-160-dd19114ff660> in <module>. 1 #adata.obs['groups'] = ['group 1'= ['0'], 'group 2'= ['5','16','19','30']]. ----> 2 sc.tl.rank_genes_groups(adata, 'louvain', groups=['5','16','19','30'], reference='0', method='wilcoxon') # wilcoxon-rank-sum/mann-whitney u test, the default of Seurat. ~\Anaconda3\lib\site-packages\scanpy\tools\_rank_genes_groups.py in rank_genes_groups(adata, groupby, use_raw, groups, reference, n_genes, rankby_abs, key_added, copy, method, corr_method, **kwds). 296 . 297 scores = (scores - (n_active * (n_active + m_active + 1) / 2)) / sqrt(. --> 298 (n_active * m_active * (n_active + m_active + 1) / 12)). 299 scores[np.isnan(scores)] = 0. 300 pvals = 2 * stats.distributions.norm.sf(np.abs(scores)). ValueError: math domain error. ```. Here `adata` is real data from our lab, not the tutorial data. Have been trying to replicate the cluster analysis tutorial. All previous steps work fine. Interestingly, if I remove group '5' from the list of groups it works. Also, this error only happens with the `wilcoxon` method, not with `t-test`.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/530
https://github.com/scverse/scanpy/issues/530:1247,interoperability,distribut,distributions,1247,"Math domain error in rank_genes_groups function; I get the error below when trying to run the following:. `>>> sc.tl.rank_genes_groups(adata, 'louvain', groups=['5','16','19','30'], reference='0', method='wilcoxon')`. ```bash. C:\Users\myuser\Anaconda3\lib\site-packages\scanpy\tools\_rank_genes_groups.py:298: RuntimeWarning: overflow encountered in long_scalars. (n_active * m_active * (n_active + m_active + 1) / 12)). ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-160-dd19114ff660> in <module>. 1 #adata.obs['groups'] = ['group 1'= ['0'], 'group 2'= ['5','16','19','30']]. ----> 2 sc.tl.rank_genes_groups(adata, 'louvain', groups=['5','16','19','30'], reference='0', method='wilcoxon') # wilcoxon-rank-sum/mann-whitney u test, the default of Seurat. ~\Anaconda3\lib\site-packages\scanpy\tools\_rank_genes_groups.py in rank_genes_groups(adata, groupby, use_raw, groups, reference, n_genes, rankby_abs, key_added, copy, method, corr_method, **kwds). 296 . 297 scores = (scores - (n_active * (n_active + m_active + 1) / 2)) / sqrt(. --> 298 (n_active * m_active * (n_active + m_active + 1) / 12)). 299 scores[np.isnan(scores)] = 0. 300 pvals = 2 * stats.distributions.norm.sf(np.abs(scores)). ValueError: math domain error. ```. Here `adata` is real data from our lab, not the tutorial data. Have been trying to replicate the cluster analysis tutorial. All previous steps work fine. Interestingly, if I remove group '5' from the list of groups it works. Also, this error only happens with the `wilcoxon` method, not with `t-test`.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/530
https://github.com/scverse/scanpy/issues/530:262,modifiability,pac,packages,262,"Math domain error in rank_genes_groups function; I get the error below when trying to run the following:. `>>> sc.tl.rank_genes_groups(adata, 'louvain', groups=['5','16','19','30'], reference='0', method='wilcoxon')`. ```bash. C:\Users\myuser\Anaconda3\lib\site-packages\scanpy\tools\_rank_genes_groups.py:298: RuntimeWarning: overflow encountered in long_scalars. (n_active * m_active * (n_active + m_active + 1) / 12)). ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-160-dd19114ff660> in <module>. 1 #adata.obs['groups'] = ['group 1'= ['0'], 'group 2'= ['5','16','19','30']]. ----> 2 sc.tl.rank_genes_groups(adata, 'louvain', groups=['5','16','19','30'], reference='0', method='wilcoxon') # wilcoxon-rank-sum/mann-whitney u test, the default of Seurat. ~\Anaconda3\lib\site-packages\scanpy\tools\_rank_genes_groups.py in rank_genes_groups(adata, groupby, use_raw, groups, reference, n_genes, rankby_abs, key_added, copy, method, corr_method, **kwds). 296 . 297 scores = (scores - (n_active * (n_active + m_active + 1) / 2)) / sqrt(. --> 298 (n_active * m_active * (n_active + m_active + 1) / 12)). 299 scores[np.isnan(scores)] = 0. 300 pvals = 2 * stats.distributions.norm.sf(np.abs(scores)). ValueError: math domain error. ```. Here `adata` is real data from our lab, not the tutorial data. Have been trying to replicate the cluster analysis tutorial. All previous steps work fine. Interestingly, if I remove group '5' from the list of groups it works. Also, this error only happens with the `wilcoxon` method, not with `t-test`.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/530
https://github.com/scverse/scanpy/issues/530:582,modifiability,modul,module,582,"Math domain error in rank_genes_groups function; I get the error below when trying to run the following:. `>>> sc.tl.rank_genes_groups(adata, 'louvain', groups=['5','16','19','30'], reference='0', method='wilcoxon')`. ```bash. C:\Users\myuser\Anaconda3\lib\site-packages\scanpy\tools\_rank_genes_groups.py:298: RuntimeWarning: overflow encountered in long_scalars. (n_active * m_active * (n_active + m_active + 1) / 12)). ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-160-dd19114ff660> in <module>. 1 #adata.obs['groups'] = ['group 1'= ['0'], 'group 2'= ['5','16','19','30']]. ----> 2 sc.tl.rank_genes_groups(adata, 'louvain', groups=['5','16','19','30'], reference='0', method='wilcoxon') # wilcoxon-rank-sum/mann-whitney u test, the default of Seurat. ~\Anaconda3\lib\site-packages\scanpy\tools\_rank_genes_groups.py in rank_genes_groups(adata, groupby, use_raw, groups, reference, n_genes, rankby_abs, key_added, copy, method, corr_method, **kwds). 296 . 297 scores = (scores - (n_active * (n_active + m_active + 1) / 2)) / sqrt(. --> 298 (n_active * m_active * (n_active + m_active + 1) / 12)). 299 scores[np.isnan(scores)] = 0. 300 pvals = 2 * stats.distributions.norm.sf(np.abs(scores)). ValueError: math domain error. ```. Here `adata` is real data from our lab, not the tutorial data. Have been trying to replicate the cluster analysis tutorial. All previous steps work fine. Interestingly, if I remove group '5' from the list of groups it works. Also, this error only happens with the `wilcoxon` method, not with `t-test`.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/530
https://github.com/scverse/scanpy/issues/530:867,modifiability,pac,packages,867,"Math domain error in rank_genes_groups function; I get the error below when trying to run the following:. `>>> sc.tl.rank_genes_groups(adata, 'louvain', groups=['5','16','19','30'], reference='0', method='wilcoxon')`. ```bash. C:\Users\myuser\Anaconda3\lib\site-packages\scanpy\tools\_rank_genes_groups.py:298: RuntimeWarning: overflow encountered in long_scalars. (n_active * m_active * (n_active + m_active + 1) / 12)). ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-160-dd19114ff660> in <module>. 1 #adata.obs['groups'] = ['group 1'= ['0'], 'group 2'= ['5','16','19','30']]. ----> 2 sc.tl.rank_genes_groups(adata, 'louvain', groups=['5','16','19','30'], reference='0', method='wilcoxon') # wilcoxon-rank-sum/mann-whitney u test, the default of Seurat. ~\Anaconda3\lib\site-packages\scanpy\tools\_rank_genes_groups.py in rank_genes_groups(adata, groupby, use_raw, groups, reference, n_genes, rankby_abs, key_added, copy, method, corr_method, **kwds). 296 . 297 scores = (scores - (n_active * (n_active + m_active + 1) / 2)) / sqrt(. --> 298 (n_active * m_active * (n_active + m_active + 1) / 12)). 299 scores[np.isnan(scores)] = 0. 300 pvals = 2 * stats.distributions.norm.sf(np.abs(scores)). ValueError: math domain error. ```. Here `adata` is real data from our lab, not the tutorial data. Have been trying to replicate the cluster analysis tutorial. All previous steps work fine. Interestingly, if I remove group '5' from the list of groups it works. Also, this error only happens with the `wilcoxon` method, not with `t-test`.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/530
https://github.com/scverse/scanpy/issues/530:12,performance,error,error,12,"Math domain error in rank_genes_groups function; I get the error below when trying to run the following:. `>>> sc.tl.rank_genes_groups(adata, 'louvain', groups=['5','16','19','30'], reference='0', method='wilcoxon')`. ```bash. C:\Users\myuser\Anaconda3\lib\site-packages\scanpy\tools\_rank_genes_groups.py:298: RuntimeWarning: overflow encountered in long_scalars. (n_active * m_active * (n_active + m_active + 1) / 12)). ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-160-dd19114ff660> in <module>. 1 #adata.obs['groups'] = ['group 1'= ['0'], 'group 2'= ['5','16','19','30']]. ----> 2 sc.tl.rank_genes_groups(adata, 'louvain', groups=['5','16','19','30'], reference='0', method='wilcoxon') # wilcoxon-rank-sum/mann-whitney u test, the default of Seurat. ~\Anaconda3\lib\site-packages\scanpy\tools\_rank_genes_groups.py in rank_genes_groups(adata, groupby, use_raw, groups, reference, n_genes, rankby_abs, key_added, copy, method, corr_method, **kwds). 296 . 297 scores = (scores - (n_active * (n_active + m_active + 1) / 2)) / sqrt(. --> 298 (n_active * m_active * (n_active + m_active + 1) / 12)). 299 scores[np.isnan(scores)] = 0. 300 pvals = 2 * stats.distributions.norm.sf(np.abs(scores)). ValueError: math domain error. ```. Here `adata` is real data from our lab, not the tutorial data. Have been trying to replicate the cluster analysis tutorial. All previous steps work fine. Interestingly, if I remove group '5' from the list of groups it works. Also, this error only happens with the `wilcoxon` method, not with `t-test`.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/530
https://github.com/scverse/scanpy/issues/530:59,performance,error,error,59,"Math domain error in rank_genes_groups function; I get the error below when trying to run the following:. `>>> sc.tl.rank_genes_groups(adata, 'louvain', groups=['5','16','19','30'], reference='0', method='wilcoxon')`. ```bash. C:\Users\myuser\Anaconda3\lib\site-packages\scanpy\tools\_rank_genes_groups.py:298: RuntimeWarning: overflow encountered in long_scalars. (n_active * m_active * (n_active + m_active + 1) / 12)). ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-160-dd19114ff660> in <module>. 1 #adata.obs['groups'] = ['group 1'= ['0'], 'group 2'= ['5','16','19','30']]. ----> 2 sc.tl.rank_genes_groups(adata, 'louvain', groups=['5','16','19','30'], reference='0', method='wilcoxon') # wilcoxon-rank-sum/mann-whitney u test, the default of Seurat. ~\Anaconda3\lib\site-packages\scanpy\tools\_rank_genes_groups.py in rank_genes_groups(adata, groupby, use_raw, groups, reference, n_genes, rankby_abs, key_added, copy, method, corr_method, **kwds). 296 . 297 scores = (scores - (n_active * (n_active + m_active + 1) / 2)) / sqrt(. --> 298 (n_active * m_active * (n_active + m_active + 1) / 12)). 299 scores[np.isnan(scores)] = 0. 300 pvals = 2 * stats.distributions.norm.sf(np.abs(scores)). ValueError: math domain error. ```. Here `adata` is real data from our lab, not the tutorial data. Have been trying to replicate the cluster analysis tutorial. All previous steps work fine. Interestingly, if I remove group '5' from the list of groups it works. Also, this error only happens with the `wilcoxon` method, not with `t-test`.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/530
https://github.com/scverse/scanpy/issues/530:1310,performance,error,error,1310,"Math domain error in rank_genes_groups function; I get the error below when trying to run the following:. `>>> sc.tl.rank_genes_groups(adata, 'louvain', groups=['5','16','19','30'], reference='0', method='wilcoxon')`. ```bash. C:\Users\myuser\Anaconda3\lib\site-packages\scanpy\tools\_rank_genes_groups.py:298: RuntimeWarning: overflow encountered in long_scalars. (n_active * m_active * (n_active + m_active + 1) / 12)). ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-160-dd19114ff660> in <module>. 1 #adata.obs['groups'] = ['group 1'= ['0'], 'group 2'= ['5','16','19','30']]. ----> 2 sc.tl.rank_genes_groups(adata, 'louvain', groups=['5','16','19','30'], reference='0', method='wilcoxon') # wilcoxon-rank-sum/mann-whitney u test, the default of Seurat. ~\Anaconda3\lib\site-packages\scanpy\tools\_rank_genes_groups.py in rank_genes_groups(adata, groupby, use_raw, groups, reference, n_genes, rankby_abs, key_added, copy, method, corr_method, **kwds). 296 . 297 scores = (scores - (n_active * (n_active + m_active + 1) / 2)) / sqrt(. --> 298 (n_active * m_active * (n_active + m_active + 1) / 12)). 299 scores[np.isnan(scores)] = 0. 300 pvals = 2 * stats.distributions.norm.sf(np.abs(scores)). ValueError: math domain error. ```. Here `adata` is real data from our lab, not the tutorial data. Have been trying to replicate the cluster analysis tutorial. All previous steps work fine. Interestingly, if I remove group '5' from the list of groups it works. Also, this error only happens with the `wilcoxon` method, not with `t-test`.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/530
https://github.com/scverse/scanpy/issues/530:1558,performance,error,error,1558,"Math domain error in rank_genes_groups function; I get the error below when trying to run the following:. `>>> sc.tl.rank_genes_groups(adata, 'louvain', groups=['5','16','19','30'], reference='0', method='wilcoxon')`. ```bash. C:\Users\myuser\Anaconda3\lib\site-packages\scanpy\tools\_rank_genes_groups.py:298: RuntimeWarning: overflow encountered in long_scalars. (n_active * m_active * (n_active + m_active + 1) / 12)). ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-160-dd19114ff660> in <module>. 1 #adata.obs['groups'] = ['group 1'= ['0'], 'group 2'= ['5','16','19','30']]. ----> 2 sc.tl.rank_genes_groups(adata, 'louvain', groups=['5','16','19','30'], reference='0', method='wilcoxon') # wilcoxon-rank-sum/mann-whitney u test, the default of Seurat. ~\Anaconda3\lib\site-packages\scanpy\tools\_rank_genes_groups.py in rank_genes_groups(adata, groupby, use_raw, groups, reference, n_genes, rankby_abs, key_added, copy, method, corr_method, **kwds). 296 . 297 scores = (scores - (n_active * (n_active + m_active + 1) / 2)) / sqrt(. --> 298 (n_active * m_active * (n_active + m_active + 1) / 12)). 299 scores[np.isnan(scores)] = 0. 300 pvals = 2 * stats.distributions.norm.sf(np.abs(scores)). ValueError: math domain error. ```. Here `adata` is real data from our lab, not the tutorial data. Have been trying to replicate the cluster analysis tutorial. All previous steps work fine. Interestingly, if I remove group '5' from the list of groups it works. Also, this error only happens with the `wilcoxon` method, not with `t-test`.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/530
https://github.com/scverse/scanpy/issues/530:12,safety,error,error,12,"Math domain error in rank_genes_groups function; I get the error below when trying to run the following:. `>>> sc.tl.rank_genes_groups(adata, 'louvain', groups=['5','16','19','30'], reference='0', method='wilcoxon')`. ```bash. C:\Users\myuser\Anaconda3\lib\site-packages\scanpy\tools\_rank_genes_groups.py:298: RuntimeWarning: overflow encountered in long_scalars. (n_active * m_active * (n_active + m_active + 1) / 12)). ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-160-dd19114ff660> in <module>. 1 #adata.obs['groups'] = ['group 1'= ['0'], 'group 2'= ['5','16','19','30']]. ----> 2 sc.tl.rank_genes_groups(adata, 'louvain', groups=['5','16','19','30'], reference='0', method='wilcoxon') # wilcoxon-rank-sum/mann-whitney u test, the default of Seurat. ~\Anaconda3\lib\site-packages\scanpy\tools\_rank_genes_groups.py in rank_genes_groups(adata, groupby, use_raw, groups, reference, n_genes, rankby_abs, key_added, copy, method, corr_method, **kwds). 296 . 297 scores = (scores - (n_active * (n_active + m_active + 1) / 2)) / sqrt(. --> 298 (n_active * m_active * (n_active + m_active + 1) / 12)). 299 scores[np.isnan(scores)] = 0. 300 pvals = 2 * stats.distributions.norm.sf(np.abs(scores)). ValueError: math domain error. ```. Here `adata` is real data from our lab, not the tutorial data. Have been trying to replicate the cluster analysis tutorial. All previous steps work fine. Interestingly, if I remove group '5' from the list of groups it works. Also, this error only happens with the `wilcoxon` method, not with `t-test`.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/530
https://github.com/scverse/scanpy/issues/530:59,safety,error,error,59,"Math domain error in rank_genes_groups function; I get the error below when trying to run the following:. `>>> sc.tl.rank_genes_groups(adata, 'louvain', groups=['5','16','19','30'], reference='0', method='wilcoxon')`. ```bash. C:\Users\myuser\Anaconda3\lib\site-packages\scanpy\tools\_rank_genes_groups.py:298: RuntimeWarning: overflow encountered in long_scalars. (n_active * m_active * (n_active + m_active + 1) / 12)). ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-160-dd19114ff660> in <module>. 1 #adata.obs['groups'] = ['group 1'= ['0'], 'group 2'= ['5','16','19','30']]. ----> 2 sc.tl.rank_genes_groups(adata, 'louvain', groups=['5','16','19','30'], reference='0', method='wilcoxon') # wilcoxon-rank-sum/mann-whitney u test, the default of Seurat. ~\Anaconda3\lib\site-packages\scanpy\tools\_rank_genes_groups.py in rank_genes_groups(adata, groupby, use_raw, groups, reference, n_genes, rankby_abs, key_added, copy, method, corr_method, **kwds). 296 . 297 scores = (scores - (n_active * (n_active + m_active + 1) / 2)) / sqrt(. --> 298 (n_active * m_active * (n_active + m_active + 1) / 12)). 299 scores[np.isnan(scores)] = 0. 300 pvals = 2 * stats.distributions.norm.sf(np.abs(scores)). ValueError: math domain error. ```. Here `adata` is real data from our lab, not the tutorial data. Have been trying to replicate the cluster analysis tutorial. All previous steps work fine. Interestingly, if I remove group '5' from the list of groups it works. Also, this error only happens with the `wilcoxon` method, not with `t-test`.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/530
https://github.com/scverse/scanpy/issues/530:554,safety,input,input-,554,"Math domain error in rank_genes_groups function; I get the error below when trying to run the following:. `>>> sc.tl.rank_genes_groups(adata, 'louvain', groups=['5','16','19','30'], reference='0', method='wilcoxon')`. ```bash. C:\Users\myuser\Anaconda3\lib\site-packages\scanpy\tools\_rank_genes_groups.py:298: RuntimeWarning: overflow encountered in long_scalars. (n_active * m_active * (n_active + m_active + 1) / 12)). ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-160-dd19114ff660> in <module>. 1 #adata.obs['groups'] = ['group 1'= ['0'], 'group 2'= ['5','16','19','30']]. ----> 2 sc.tl.rank_genes_groups(adata, 'louvain', groups=['5','16','19','30'], reference='0', method='wilcoxon') # wilcoxon-rank-sum/mann-whitney u test, the default of Seurat. ~\Anaconda3\lib\site-packages\scanpy\tools\_rank_genes_groups.py in rank_genes_groups(adata, groupby, use_raw, groups, reference, n_genes, rankby_abs, key_added, copy, method, corr_method, **kwds). 296 . 297 scores = (scores - (n_active * (n_active + m_active + 1) / 2)) / sqrt(. --> 298 (n_active * m_active * (n_active + m_active + 1) / 12)). 299 scores[np.isnan(scores)] = 0. 300 pvals = 2 * stats.distributions.norm.sf(np.abs(scores)). ValueError: math domain error. ```. Here `adata` is real data from our lab, not the tutorial data. Have been trying to replicate the cluster analysis tutorial. All previous steps work fine. Interestingly, if I remove group '5' from the list of groups it works. Also, this error only happens with the `wilcoxon` method, not with `t-test`.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/530
https://github.com/scverse/scanpy/issues/530:582,safety,modul,module,582,"Math domain error in rank_genes_groups function; I get the error below when trying to run the following:. `>>> sc.tl.rank_genes_groups(adata, 'louvain', groups=['5','16','19','30'], reference='0', method='wilcoxon')`. ```bash. C:\Users\myuser\Anaconda3\lib\site-packages\scanpy\tools\_rank_genes_groups.py:298: RuntimeWarning: overflow encountered in long_scalars. (n_active * m_active * (n_active + m_active + 1) / 12)). ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-160-dd19114ff660> in <module>. 1 #adata.obs['groups'] = ['group 1'= ['0'], 'group 2'= ['5','16','19','30']]. ----> 2 sc.tl.rank_genes_groups(adata, 'louvain', groups=['5','16','19','30'], reference='0', method='wilcoxon') # wilcoxon-rank-sum/mann-whitney u test, the default of Seurat. ~\Anaconda3\lib\site-packages\scanpy\tools\_rank_genes_groups.py in rank_genes_groups(adata, groupby, use_raw, groups, reference, n_genes, rankby_abs, key_added, copy, method, corr_method, **kwds). 296 . 297 scores = (scores - (n_active * (n_active + m_active + 1) / 2)) / sqrt(. --> 298 (n_active * m_active * (n_active + m_active + 1) / 12)). 299 scores[np.isnan(scores)] = 0. 300 pvals = 2 * stats.distributions.norm.sf(np.abs(scores)). ValueError: math domain error. ```. Here `adata` is real data from our lab, not the tutorial data. Have been trying to replicate the cluster analysis tutorial. All previous steps work fine. Interestingly, if I remove group '5' from the list of groups it works. Also, this error only happens with the `wilcoxon` method, not with `t-test`.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/530
https://github.com/scverse/scanpy/issues/530:817,safety,test,test,817,"Math domain error in rank_genes_groups function; I get the error below when trying to run the following:. `>>> sc.tl.rank_genes_groups(adata, 'louvain', groups=['5','16','19','30'], reference='0', method='wilcoxon')`. ```bash. C:\Users\myuser\Anaconda3\lib\site-packages\scanpy\tools\_rank_genes_groups.py:298: RuntimeWarning: overflow encountered in long_scalars. (n_active * m_active * (n_active + m_active + 1) / 12)). ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-160-dd19114ff660> in <module>. 1 #adata.obs['groups'] = ['group 1'= ['0'], 'group 2'= ['5','16','19','30']]. ----> 2 sc.tl.rank_genes_groups(adata, 'louvain', groups=['5','16','19','30'], reference='0', method='wilcoxon') # wilcoxon-rank-sum/mann-whitney u test, the default of Seurat. ~\Anaconda3\lib\site-packages\scanpy\tools\_rank_genes_groups.py in rank_genes_groups(adata, groupby, use_raw, groups, reference, n_genes, rankby_abs, key_added, copy, method, corr_method, **kwds). 296 . 297 scores = (scores - (n_active * (n_active + m_active + 1) / 2)) / sqrt(. --> 298 (n_active * m_active * (n_active + m_active + 1) / 12)). 299 scores[np.isnan(scores)] = 0. 300 pvals = 2 * stats.distributions.norm.sf(np.abs(scores)). ValueError: math domain error. ```. Here `adata` is real data from our lab, not the tutorial data. Have been trying to replicate the cluster analysis tutorial. All previous steps work fine. Interestingly, if I remove group '5' from the list of groups it works. Also, this error only happens with the `wilcoxon` method, not with `t-test`.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/530
https://github.com/scverse/scanpy/issues/530:1310,safety,error,error,1310,"Math domain error in rank_genes_groups function; I get the error below when trying to run the following:. `>>> sc.tl.rank_genes_groups(adata, 'louvain', groups=['5','16','19','30'], reference='0', method='wilcoxon')`. ```bash. C:\Users\myuser\Anaconda3\lib\site-packages\scanpy\tools\_rank_genes_groups.py:298: RuntimeWarning: overflow encountered in long_scalars. (n_active * m_active * (n_active + m_active + 1) / 12)). ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-160-dd19114ff660> in <module>. 1 #adata.obs['groups'] = ['group 1'= ['0'], 'group 2'= ['5','16','19','30']]. ----> 2 sc.tl.rank_genes_groups(adata, 'louvain', groups=['5','16','19','30'], reference='0', method='wilcoxon') # wilcoxon-rank-sum/mann-whitney u test, the default of Seurat. ~\Anaconda3\lib\site-packages\scanpy\tools\_rank_genes_groups.py in rank_genes_groups(adata, groupby, use_raw, groups, reference, n_genes, rankby_abs, key_added, copy, method, corr_method, **kwds). 296 . 297 scores = (scores - (n_active * (n_active + m_active + 1) / 2)) / sqrt(. --> 298 (n_active * m_active * (n_active + m_active + 1) / 12)). 299 scores[np.isnan(scores)] = 0. 300 pvals = 2 * stats.distributions.norm.sf(np.abs(scores)). ValueError: math domain error. ```. Here `adata` is real data from our lab, not the tutorial data. Have been trying to replicate the cluster analysis tutorial. All previous steps work fine. Interestingly, if I remove group '5' from the list of groups it works. Also, this error only happens with the `wilcoxon` method, not with `t-test`.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/530
https://github.com/scverse/scanpy/issues/530:1558,safety,error,error,1558,"Math domain error in rank_genes_groups function; I get the error below when trying to run the following:. `>>> sc.tl.rank_genes_groups(adata, 'louvain', groups=['5','16','19','30'], reference='0', method='wilcoxon')`. ```bash. C:\Users\myuser\Anaconda3\lib\site-packages\scanpy\tools\_rank_genes_groups.py:298: RuntimeWarning: overflow encountered in long_scalars. (n_active * m_active * (n_active + m_active + 1) / 12)). ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-160-dd19114ff660> in <module>. 1 #adata.obs['groups'] = ['group 1'= ['0'], 'group 2'= ['5','16','19','30']]. ----> 2 sc.tl.rank_genes_groups(adata, 'louvain', groups=['5','16','19','30'], reference='0', method='wilcoxon') # wilcoxon-rank-sum/mann-whitney u test, the default of Seurat. ~\Anaconda3\lib\site-packages\scanpy\tools\_rank_genes_groups.py in rank_genes_groups(adata, groupby, use_raw, groups, reference, n_genes, rankby_abs, key_added, copy, method, corr_method, **kwds). 296 . 297 scores = (scores - (n_active * (n_active + m_active + 1) / 2)) / sqrt(. --> 298 (n_active * m_active * (n_active + m_active + 1) / 12)). 299 scores[np.isnan(scores)] = 0. 300 pvals = 2 * stats.distributions.norm.sf(np.abs(scores)). ValueError: math domain error. ```. Here `adata` is real data from our lab, not the tutorial data. Have been trying to replicate the cluster analysis tutorial. All previous steps work fine. Interestingly, if I remove group '5' from the list of groups it works. Also, this error only happens with the `wilcoxon` method, not with `t-test`.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/530
https://github.com/scverse/scanpy/issues/530:1617,safety,test,test,1617,"Math domain error in rank_genes_groups function; I get the error below when trying to run the following:. `>>> sc.tl.rank_genes_groups(adata, 'louvain', groups=['5','16','19','30'], reference='0', method='wilcoxon')`. ```bash. C:\Users\myuser\Anaconda3\lib\site-packages\scanpy\tools\_rank_genes_groups.py:298: RuntimeWarning: overflow encountered in long_scalars. (n_active * m_active * (n_active + m_active + 1) / 12)). ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-160-dd19114ff660> in <module>. 1 #adata.obs['groups'] = ['group 1'= ['0'], 'group 2'= ['5','16','19','30']]. ----> 2 sc.tl.rank_genes_groups(adata, 'louvain', groups=['5','16','19','30'], reference='0', method='wilcoxon') # wilcoxon-rank-sum/mann-whitney u test, the default of Seurat. ~\Anaconda3\lib\site-packages\scanpy\tools\_rank_genes_groups.py in rank_genes_groups(adata, groupby, use_raw, groups, reference, n_genes, rankby_abs, key_added, copy, method, corr_method, **kwds). 296 . 297 scores = (scores - (n_active * (n_active + m_active + 1) / 2)) / sqrt(. --> 298 (n_active * m_active * (n_active + m_active + 1) / 12)). 299 scores[np.isnan(scores)] = 0. 300 pvals = 2 * stats.distributions.norm.sf(np.abs(scores)). ValueError: math domain error. ```. Here `adata` is real data from our lab, not the tutorial data. Have been trying to replicate the cluster analysis tutorial. All previous steps work fine. Interestingly, if I remove group '5' from the list of groups it works. Also, this error only happens with the `wilcoxon` method, not with `t-test`.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/530
https://github.com/scverse/scanpy/issues/530:510,testability,Trace,Traceback,510,"Math domain error in rank_genes_groups function; I get the error below when trying to run the following:. `>>> sc.tl.rank_genes_groups(adata, 'louvain', groups=['5','16','19','30'], reference='0', method='wilcoxon')`. ```bash. C:\Users\myuser\Anaconda3\lib\site-packages\scanpy\tools\_rank_genes_groups.py:298: RuntimeWarning: overflow encountered in long_scalars. (n_active * m_active * (n_active + m_active + 1) / 12)). ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-160-dd19114ff660> in <module>. 1 #adata.obs['groups'] = ['group 1'= ['0'], 'group 2'= ['5','16','19','30']]. ----> 2 sc.tl.rank_genes_groups(adata, 'louvain', groups=['5','16','19','30'], reference='0', method='wilcoxon') # wilcoxon-rank-sum/mann-whitney u test, the default of Seurat. ~\Anaconda3\lib\site-packages\scanpy\tools\_rank_genes_groups.py in rank_genes_groups(adata, groupby, use_raw, groups, reference, n_genes, rankby_abs, key_added, copy, method, corr_method, **kwds). 296 . 297 scores = (scores - (n_active * (n_active + m_active + 1) / 2)) / sqrt(. --> 298 (n_active * m_active * (n_active + m_active + 1) / 12)). 299 scores[np.isnan(scores)] = 0. 300 pvals = 2 * stats.distributions.norm.sf(np.abs(scores)). ValueError: math domain error. ```. Here `adata` is real data from our lab, not the tutorial data. Have been trying to replicate the cluster analysis tutorial. All previous steps work fine. Interestingly, if I remove group '5' from the list of groups it works. Also, this error only happens with the `wilcoxon` method, not with `t-test`.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/530
https://github.com/scverse/scanpy/issues/530:817,testability,test,test,817,"Math domain error in rank_genes_groups function; I get the error below when trying to run the following:. `>>> sc.tl.rank_genes_groups(adata, 'louvain', groups=['5','16','19','30'], reference='0', method='wilcoxon')`. ```bash. C:\Users\myuser\Anaconda3\lib\site-packages\scanpy\tools\_rank_genes_groups.py:298: RuntimeWarning: overflow encountered in long_scalars. (n_active * m_active * (n_active + m_active + 1) / 12)). ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-160-dd19114ff660> in <module>. 1 #adata.obs['groups'] = ['group 1'= ['0'], 'group 2'= ['5','16','19','30']]. ----> 2 sc.tl.rank_genes_groups(adata, 'louvain', groups=['5','16','19','30'], reference='0', method='wilcoxon') # wilcoxon-rank-sum/mann-whitney u test, the default of Seurat. ~\Anaconda3\lib\site-packages\scanpy\tools\_rank_genes_groups.py in rank_genes_groups(adata, groupby, use_raw, groups, reference, n_genes, rankby_abs, key_added, copy, method, corr_method, **kwds). 296 . 297 scores = (scores - (n_active * (n_active + m_active + 1) / 2)) / sqrt(. --> 298 (n_active * m_active * (n_active + m_active + 1) / 12)). 299 scores[np.isnan(scores)] = 0. 300 pvals = 2 * stats.distributions.norm.sf(np.abs(scores)). ValueError: math domain error. ```. Here `adata` is real data from our lab, not the tutorial data. Have been trying to replicate the cluster analysis tutorial. All previous steps work fine. Interestingly, if I remove group '5' from the list of groups it works. Also, this error only happens with the `wilcoxon` method, not with `t-test`.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/530
https://github.com/scverse/scanpy/issues/530:1617,testability,test,test,1617,"Math domain error in rank_genes_groups function; I get the error below when trying to run the following:. `>>> sc.tl.rank_genes_groups(adata, 'louvain', groups=['5','16','19','30'], reference='0', method='wilcoxon')`. ```bash. C:\Users\myuser\Anaconda3\lib\site-packages\scanpy\tools\_rank_genes_groups.py:298: RuntimeWarning: overflow encountered in long_scalars. (n_active * m_active * (n_active + m_active + 1) / 12)). ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-160-dd19114ff660> in <module>. 1 #adata.obs['groups'] = ['group 1'= ['0'], 'group 2'= ['5','16','19','30']]. ----> 2 sc.tl.rank_genes_groups(adata, 'louvain', groups=['5','16','19','30'], reference='0', method='wilcoxon') # wilcoxon-rank-sum/mann-whitney u test, the default of Seurat. ~\Anaconda3\lib\site-packages\scanpy\tools\_rank_genes_groups.py in rank_genes_groups(adata, groupby, use_raw, groups, reference, n_genes, rankby_abs, key_added, copy, method, corr_method, **kwds). 296 . 297 scores = (scores - (n_active * (n_active + m_active + 1) / 2)) / sqrt(. --> 298 (n_active * m_active * (n_active + m_active + 1) / 12)). 299 scores[np.isnan(scores)] = 0. 300 pvals = 2 * stats.distributions.norm.sf(np.abs(scores)). ValueError: math domain error. ```. Here `adata` is real data from our lab, not the tutorial data. Have been trying to replicate the cluster analysis tutorial. All previous steps work fine. Interestingly, if I remove group '5' from the list of groups it works. Also, this error only happens with the `wilcoxon` method, not with `t-test`.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/530
https://github.com/scverse/scanpy/issues/530:12,usability,error,error,12,"Math domain error in rank_genes_groups function; I get the error below when trying to run the following:. `>>> sc.tl.rank_genes_groups(adata, 'louvain', groups=['5','16','19','30'], reference='0', method='wilcoxon')`. ```bash. C:\Users\myuser\Anaconda3\lib\site-packages\scanpy\tools\_rank_genes_groups.py:298: RuntimeWarning: overflow encountered in long_scalars. (n_active * m_active * (n_active + m_active + 1) / 12)). ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-160-dd19114ff660> in <module>. 1 #adata.obs['groups'] = ['group 1'= ['0'], 'group 2'= ['5','16','19','30']]. ----> 2 sc.tl.rank_genes_groups(adata, 'louvain', groups=['5','16','19','30'], reference='0', method='wilcoxon') # wilcoxon-rank-sum/mann-whitney u test, the default of Seurat. ~\Anaconda3\lib\site-packages\scanpy\tools\_rank_genes_groups.py in rank_genes_groups(adata, groupby, use_raw, groups, reference, n_genes, rankby_abs, key_added, copy, method, corr_method, **kwds). 296 . 297 scores = (scores - (n_active * (n_active + m_active + 1) / 2)) / sqrt(. --> 298 (n_active * m_active * (n_active + m_active + 1) / 12)). 299 scores[np.isnan(scores)] = 0. 300 pvals = 2 * stats.distributions.norm.sf(np.abs(scores)). ValueError: math domain error. ```. Here `adata` is real data from our lab, not the tutorial data. Have been trying to replicate the cluster analysis tutorial. All previous steps work fine. Interestingly, if I remove group '5' from the list of groups it works. Also, this error only happens with the `wilcoxon` method, not with `t-test`.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/530
https://github.com/scverse/scanpy/issues/530:59,usability,error,error,59,"Math domain error in rank_genes_groups function; I get the error below when trying to run the following:. `>>> sc.tl.rank_genes_groups(adata, 'louvain', groups=['5','16','19','30'], reference='0', method='wilcoxon')`. ```bash. C:\Users\myuser\Anaconda3\lib\site-packages\scanpy\tools\_rank_genes_groups.py:298: RuntimeWarning: overflow encountered in long_scalars. (n_active * m_active * (n_active + m_active + 1) / 12)). ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-160-dd19114ff660> in <module>. 1 #adata.obs['groups'] = ['group 1'= ['0'], 'group 2'= ['5','16','19','30']]. ----> 2 sc.tl.rank_genes_groups(adata, 'louvain', groups=['5','16','19','30'], reference='0', method='wilcoxon') # wilcoxon-rank-sum/mann-whitney u test, the default of Seurat. ~\Anaconda3\lib\site-packages\scanpy\tools\_rank_genes_groups.py in rank_genes_groups(adata, groupby, use_raw, groups, reference, n_genes, rankby_abs, key_added, copy, method, corr_method, **kwds). 296 . 297 scores = (scores - (n_active * (n_active + m_active + 1) / 2)) / sqrt(. --> 298 (n_active * m_active * (n_active + m_active + 1) / 12)). 299 scores[np.isnan(scores)] = 0. 300 pvals = 2 * stats.distributions.norm.sf(np.abs(scores)). ValueError: math domain error. ```. Here `adata` is real data from our lab, not the tutorial data. Have been trying to replicate the cluster analysis tutorial. All previous steps work fine. Interestingly, if I remove group '5' from the list of groups it works. Also, this error only happens with the `wilcoxon` method, not with `t-test`.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/530
https://github.com/scverse/scanpy/issues/530:230,usability,User,Users,230,"Math domain error in rank_genes_groups function; I get the error below when trying to run the following:. `>>> sc.tl.rank_genes_groups(adata, 'louvain', groups=['5','16','19','30'], reference='0', method='wilcoxon')`. ```bash. C:\Users\myuser\Anaconda3\lib\site-packages\scanpy\tools\_rank_genes_groups.py:298: RuntimeWarning: overflow encountered in long_scalars. (n_active * m_active * (n_active + m_active + 1) / 12)). ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-160-dd19114ff660> in <module>. 1 #adata.obs['groups'] = ['group 1'= ['0'], 'group 2'= ['5','16','19','30']]. ----> 2 sc.tl.rank_genes_groups(adata, 'louvain', groups=['5','16','19','30'], reference='0', method='wilcoxon') # wilcoxon-rank-sum/mann-whitney u test, the default of Seurat. ~\Anaconda3\lib\site-packages\scanpy\tools\_rank_genes_groups.py in rank_genes_groups(adata, groupby, use_raw, groups, reference, n_genes, rankby_abs, key_added, copy, method, corr_method, **kwds). 296 . 297 scores = (scores - (n_active * (n_active + m_active + 1) / 2)) / sqrt(. --> 298 (n_active * m_active * (n_active + m_active + 1) / 12)). 299 scores[np.isnan(scores)] = 0. 300 pvals = 2 * stats.distributions.norm.sf(np.abs(scores)). ValueError: math domain error. ```. Here `adata` is real data from our lab, not the tutorial data. Have been trying to replicate the cluster analysis tutorial. All previous steps work fine. Interestingly, if I remove group '5' from the list of groups it works. Also, this error only happens with the `wilcoxon` method, not with `t-test`.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/530
https://github.com/scverse/scanpy/issues/530:278,usability,tool,tools,278,"Math domain error in rank_genes_groups function; I get the error below when trying to run the following:. `>>> sc.tl.rank_genes_groups(adata, 'louvain', groups=['5','16','19','30'], reference='0', method='wilcoxon')`. ```bash. C:\Users\myuser\Anaconda3\lib\site-packages\scanpy\tools\_rank_genes_groups.py:298: RuntimeWarning: overflow encountered in long_scalars. (n_active * m_active * (n_active + m_active + 1) / 12)). ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-160-dd19114ff660> in <module>. 1 #adata.obs['groups'] = ['group 1'= ['0'], 'group 2'= ['5','16','19','30']]. ----> 2 sc.tl.rank_genes_groups(adata, 'louvain', groups=['5','16','19','30'], reference='0', method='wilcoxon') # wilcoxon-rank-sum/mann-whitney u test, the default of Seurat. ~\Anaconda3\lib\site-packages\scanpy\tools\_rank_genes_groups.py in rank_genes_groups(adata, groupby, use_raw, groups, reference, n_genes, rankby_abs, key_added, copy, method, corr_method, **kwds). 296 . 297 scores = (scores - (n_active * (n_active + m_active + 1) / 2)) / sqrt(. --> 298 (n_active * m_active * (n_active + m_active + 1) / 12)). 299 scores[np.isnan(scores)] = 0. 300 pvals = 2 * stats.distributions.norm.sf(np.abs(scores)). ValueError: math domain error. ```. Here `adata` is real data from our lab, not the tutorial data. Have been trying to replicate the cluster analysis tutorial. All previous steps work fine. Interestingly, if I remove group '5' from the list of groups it works. Also, this error only happens with the `wilcoxon` method, not with `t-test`.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/530
https://github.com/scverse/scanpy/issues/530:554,usability,input,input-,554,"Math domain error in rank_genes_groups function; I get the error below when trying to run the following:. `>>> sc.tl.rank_genes_groups(adata, 'louvain', groups=['5','16','19','30'], reference='0', method='wilcoxon')`. ```bash. C:\Users\myuser\Anaconda3\lib\site-packages\scanpy\tools\_rank_genes_groups.py:298: RuntimeWarning: overflow encountered in long_scalars. (n_active * m_active * (n_active + m_active + 1) / 12)). ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-160-dd19114ff660> in <module>. 1 #adata.obs['groups'] = ['group 1'= ['0'], 'group 2'= ['5','16','19','30']]. ----> 2 sc.tl.rank_genes_groups(adata, 'louvain', groups=['5','16','19','30'], reference='0', method='wilcoxon') # wilcoxon-rank-sum/mann-whitney u test, the default of Seurat. ~\Anaconda3\lib\site-packages\scanpy\tools\_rank_genes_groups.py in rank_genes_groups(adata, groupby, use_raw, groups, reference, n_genes, rankby_abs, key_added, copy, method, corr_method, **kwds). 296 . 297 scores = (scores - (n_active * (n_active + m_active + 1) / 2)) / sqrt(. --> 298 (n_active * m_active * (n_active + m_active + 1) / 12)). 299 scores[np.isnan(scores)] = 0. 300 pvals = 2 * stats.distributions.norm.sf(np.abs(scores)). ValueError: math domain error. ```. Here `adata` is real data from our lab, not the tutorial data. Have been trying to replicate the cluster analysis tutorial. All previous steps work fine. Interestingly, if I remove group '5' from the list of groups it works. Also, this error only happens with the `wilcoxon` method, not with `t-test`.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/530
https://github.com/scverse/scanpy/issues/530:883,usability,tool,tools,883,"Math domain error in rank_genes_groups function; I get the error below when trying to run the following:. `>>> sc.tl.rank_genes_groups(adata, 'louvain', groups=['5','16','19','30'], reference='0', method='wilcoxon')`. ```bash. C:\Users\myuser\Anaconda3\lib\site-packages\scanpy\tools\_rank_genes_groups.py:298: RuntimeWarning: overflow encountered in long_scalars. (n_active * m_active * (n_active + m_active + 1) / 12)). ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-160-dd19114ff660> in <module>. 1 #adata.obs['groups'] = ['group 1'= ['0'], 'group 2'= ['5','16','19','30']]. ----> 2 sc.tl.rank_genes_groups(adata, 'louvain', groups=['5','16','19','30'], reference='0', method='wilcoxon') # wilcoxon-rank-sum/mann-whitney u test, the default of Seurat. ~\Anaconda3\lib\site-packages\scanpy\tools\_rank_genes_groups.py in rank_genes_groups(adata, groupby, use_raw, groups, reference, n_genes, rankby_abs, key_added, copy, method, corr_method, **kwds). 296 . 297 scores = (scores - (n_active * (n_active + m_active + 1) / 2)) / sqrt(. --> 298 (n_active * m_active * (n_active + m_active + 1) / 12)). 299 scores[np.isnan(scores)] = 0. 300 pvals = 2 * stats.distributions.norm.sf(np.abs(scores)). ValueError: math domain error. ```. Here `adata` is real data from our lab, not the tutorial data. Have been trying to replicate the cluster analysis tutorial. All previous steps work fine. Interestingly, if I remove group '5' from the list of groups it works. Also, this error only happens with the `wilcoxon` method, not with `t-test`.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/530
https://github.com/scverse/scanpy/issues/530:1310,usability,error,error,1310,"Math domain error in rank_genes_groups function; I get the error below when trying to run the following:. `>>> sc.tl.rank_genes_groups(adata, 'louvain', groups=['5','16','19','30'], reference='0', method='wilcoxon')`. ```bash. C:\Users\myuser\Anaconda3\lib\site-packages\scanpy\tools\_rank_genes_groups.py:298: RuntimeWarning: overflow encountered in long_scalars. (n_active * m_active * (n_active + m_active + 1) / 12)). ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-160-dd19114ff660> in <module>. 1 #adata.obs['groups'] = ['group 1'= ['0'], 'group 2'= ['5','16','19','30']]. ----> 2 sc.tl.rank_genes_groups(adata, 'louvain', groups=['5','16','19','30'], reference='0', method='wilcoxon') # wilcoxon-rank-sum/mann-whitney u test, the default of Seurat. ~\Anaconda3\lib\site-packages\scanpy\tools\_rank_genes_groups.py in rank_genes_groups(adata, groupby, use_raw, groups, reference, n_genes, rankby_abs, key_added, copy, method, corr_method, **kwds). 296 . 297 scores = (scores - (n_active * (n_active + m_active + 1) / 2)) / sqrt(. --> 298 (n_active * m_active * (n_active + m_active + 1) / 12)). 299 scores[np.isnan(scores)] = 0. 300 pvals = 2 * stats.distributions.norm.sf(np.abs(scores)). ValueError: math domain error. ```. Here `adata` is real data from our lab, not the tutorial data. Have been trying to replicate the cluster analysis tutorial. All previous steps work fine. Interestingly, if I remove group '5' from the list of groups it works. Also, this error only happens with the `wilcoxon` method, not with `t-test`.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/530
https://github.com/scverse/scanpy/issues/530:1558,usability,error,error,1558,"Math domain error in rank_genes_groups function; I get the error below when trying to run the following:. `>>> sc.tl.rank_genes_groups(adata, 'louvain', groups=['5','16','19','30'], reference='0', method='wilcoxon')`. ```bash. C:\Users\myuser\Anaconda3\lib\site-packages\scanpy\tools\_rank_genes_groups.py:298: RuntimeWarning: overflow encountered in long_scalars. (n_active * m_active * (n_active + m_active + 1) / 12)). ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-160-dd19114ff660> in <module>. 1 #adata.obs['groups'] = ['group 1'= ['0'], 'group 2'= ['5','16','19','30']]. ----> 2 sc.tl.rank_genes_groups(adata, 'louvain', groups=['5','16','19','30'], reference='0', method='wilcoxon') # wilcoxon-rank-sum/mann-whitney u test, the default of Seurat. ~\Anaconda3\lib\site-packages\scanpy\tools\_rank_genes_groups.py in rank_genes_groups(adata, groupby, use_raw, groups, reference, n_genes, rankby_abs, key_added, copy, method, corr_method, **kwds). 296 . 297 scores = (scores - (n_active * (n_active + m_active + 1) / 2)) / sqrt(. --> 298 (n_active * m_active * (n_active + m_active + 1) / 12)). 299 scores[np.isnan(scores)] = 0. 300 pvals = 2 * stats.distributions.norm.sf(np.abs(scores)). ValueError: math domain error. ```. Here `adata` is real data from our lab, not the tutorial data. Have been trying to replicate the cluster analysis tutorial. All previous steps work fine. Interestingly, if I remove group '5' from the list of groups it works. Also, this error only happens with the `wilcoxon` method, not with `t-test`.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/530
https://github.com/scverse/scanpy/issues/531:11,energy efficiency,current,current,11,"Flaws with current approaches for normalization, PCA, and feature selection?; I just saw this biorxiv manuscript and I was wondering if there are any ways to easily incorporate multinomial methods in scanpy. [Feature Selection and Dimension Reduction for Single Cell RNA-Seq based on a Multinomial Model](https://www.biorxiv.org/content/10.1101/574574v1).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/531
https://github.com/scverse/scanpy/issues/531:241,energy efficiency,Reduc,Reduction,241,"Flaws with current approaches for normalization, PCA, and feature selection?; I just saw this biorxiv manuscript and I was wondering if there are any ways to easily incorporate multinomial methods in scanpy. [Feature Selection and Dimension Reduction for Single Cell RNA-Seq based on a Multinomial Model](https://www.biorxiv.org/content/10.1101/574574v1).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/531
https://github.com/scverse/scanpy/issues/531:298,energy efficiency,Model,Model,298,"Flaws with current approaches for normalization, PCA, and feature selection?; I just saw this biorxiv manuscript and I was wondering if there are any ways to easily incorporate multinomial methods in scanpy. [Feature Selection and Dimension Reduction for Single Cell RNA-Seq based on a Multinomial Model](https://www.biorxiv.org/content/10.1101/574574v1).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/531
https://github.com/scverse/scanpy/issues/531:329,performance,content,content,329,"Flaws with current approaches for normalization, PCA, and feature selection?; I just saw this biorxiv manuscript and I was wondering if there are any ways to easily incorporate multinomial methods in scanpy. [Feature Selection and Dimension Reduction for Single Cell RNA-Seq based on a Multinomial Model](https://www.biorxiv.org/content/10.1101/574574v1).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/531
https://github.com/scverse/scanpy/issues/531:298,security,Model,Model,298,"Flaws with current approaches for normalization, PCA, and feature selection?; I just saw this biorxiv manuscript and I was wondering if there are any ways to easily incorporate multinomial methods in scanpy. [Feature Selection and Dimension Reduction for Single Cell RNA-Seq based on a Multinomial Model](https://www.biorxiv.org/content/10.1101/574574v1).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/531
https://github.com/scverse/scanpy/issues/532:203,modifiability,paramet,parameter,203,"Plotting the expressions of several markers on a single UMAP plot; Hi,. Would it be possible to plot several markers on a single UMAP plot instead of having a plot for each of them separately? The color parameter of the pl.umap function takes the markers as a list of strings and plots them individually. . Thanks,. Parisa",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/532
https://github.com/scverse/scanpy/issues/534:8,availability,Error,Error,8,"Getting Error Variable names are not unique when using .read_10x_h5 function; Good Evening,. My goal here is to get either a Gene barcode or dense matrix from a .h5 file from 10x. I'm currently trying to use the .read_10x_h5() function to help me achieve this. To my understanding I just need to input the file name into the function. When I run my code (See below), I get an error stating that ""Variable names are not unique. To make them unique, call `.var_names_make_unique`."" From the documentation I don't see a way that I can call .var_names_make_unique(). Is there some preprocessing that I'm missing? ```python. user_input = input(""Enter the path of your file: ""). def convert_h5_to_adata(filename):. filename = str(filename). if os.access(filename, os.R_OK):. sc.read_10x_h5(filename). return. convert_h5_to_adata(user_input). ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/534
https://github.com/scverse/scanpy/issues/534:376,availability,error,error,376,"Getting Error Variable names are not unique when using .read_10x_h5 function; Good Evening,. My goal here is to get either a Gene barcode or dense matrix from a .h5 file from 10x. I'm currently trying to use the .read_10x_h5() function to help me achieve this. To my understanding I just need to input the file name into the function. When I run my code (See below), I get an error stating that ""Variable names are not unique. To make them unique, call `.var_names_make_unique`."" From the documentation I don't see a way that I can call .var_names_make_unique(). Is there some preprocessing that I'm missing? ```python. user_input = input(""Enter the path of your file: ""). def convert_h5_to_adata(filename):. filename = str(filename). if os.access(filename, os.R_OK):. sc.read_10x_h5(filename). return. convert_h5_to_adata(user_input). ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/534
https://github.com/scverse/scanpy/issues/534:184,energy efficiency,current,currently,184,"Getting Error Variable names are not unique when using .read_10x_h5 function; Good Evening,. My goal here is to get either a Gene barcode or dense matrix from a .h5 file from 10x. I'm currently trying to use the .read_10x_h5() function to help me achieve this. To my understanding I just need to input the file name into the function. When I run my code (See below), I get an error stating that ""Variable names are not unique. To make them unique, call `.var_names_make_unique`."" From the documentation I don't see a way that I can call .var_names_make_unique(). Is there some preprocessing that I'm missing? ```python. user_input = input(""Enter the path of your file: ""). def convert_h5_to_adata(filename):. filename = str(filename). if os.access(filename, os.R_OK):. sc.read_10x_h5(filename). return. convert_h5_to_adata(user_input). ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/534
https://github.com/scverse/scanpy/issues/534:14,modifiability,Variab,Variable,14,"Getting Error Variable names are not unique when using .read_10x_h5 function; Good Evening,. My goal here is to get either a Gene barcode or dense matrix from a .h5 file from 10x. I'm currently trying to use the .read_10x_h5() function to help me achieve this. To my understanding I just need to input the file name into the function. When I run my code (See below), I get an error stating that ""Variable names are not unique. To make them unique, call `.var_names_make_unique`."" From the documentation I don't see a way that I can call .var_names_make_unique(). Is there some preprocessing that I'm missing? ```python. user_input = input(""Enter the path of your file: ""). def convert_h5_to_adata(filename):. filename = str(filename). if os.access(filename, os.R_OK):. sc.read_10x_h5(filename). return. convert_h5_to_adata(user_input). ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/534
https://github.com/scverse/scanpy/issues/534:396,modifiability,Variab,Variable,396,"Getting Error Variable names are not unique when using .read_10x_h5 function; Good Evening,. My goal here is to get either a Gene barcode or dense matrix from a .h5 file from 10x. I'm currently trying to use the .read_10x_h5() function to help me achieve this. To my understanding I just need to input the file name into the function. When I run my code (See below), I get an error stating that ""Variable names are not unique. To make them unique, call `.var_names_make_unique`."" From the documentation I don't see a way that I can call .var_names_make_unique(). Is there some preprocessing that I'm missing? ```python. user_input = input(""Enter the path of your file: ""). def convert_h5_to_adata(filename):. filename = str(filename). if os.access(filename, os.R_OK):. sc.read_10x_h5(filename). return. convert_h5_to_adata(user_input). ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/534
https://github.com/scverse/scanpy/issues/534:8,performance,Error,Error,8,"Getting Error Variable names are not unique when using .read_10x_h5 function; Good Evening,. My goal here is to get either a Gene barcode or dense matrix from a .h5 file from 10x. I'm currently trying to use the .read_10x_h5() function to help me achieve this. To my understanding I just need to input the file name into the function. When I run my code (See below), I get an error stating that ""Variable names are not unique. To make them unique, call `.var_names_make_unique`."" From the documentation I don't see a way that I can call .var_names_make_unique(). Is there some preprocessing that I'm missing? ```python. user_input = input(""Enter the path of your file: ""). def convert_h5_to_adata(filename):. filename = str(filename). if os.access(filename, os.R_OK):. sc.read_10x_h5(filename). return. convert_h5_to_adata(user_input). ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/534
https://github.com/scverse/scanpy/issues/534:376,performance,error,error,376,"Getting Error Variable names are not unique when using .read_10x_h5 function; Good Evening,. My goal here is to get either a Gene barcode or dense matrix from a .h5 file from 10x. I'm currently trying to use the .read_10x_h5() function to help me achieve this. To my understanding I just need to input the file name into the function. When I run my code (See below), I get an error stating that ""Variable names are not unique. To make them unique, call `.var_names_make_unique`."" From the documentation I don't see a way that I can call .var_names_make_unique(). Is there some preprocessing that I'm missing? ```python. user_input = input(""Enter the path of your file: ""). def convert_h5_to_adata(filename):. filename = str(filename). if os.access(filename, os.R_OK):. sc.read_10x_h5(filename). return. convert_h5_to_adata(user_input). ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/534
https://github.com/scverse/scanpy/issues/534:8,safety,Error,Error,8,"Getting Error Variable names are not unique when using .read_10x_h5 function; Good Evening,. My goal here is to get either a Gene barcode or dense matrix from a .h5 file from 10x. I'm currently trying to use the .read_10x_h5() function to help me achieve this. To my understanding I just need to input the file name into the function. When I run my code (See below), I get an error stating that ""Variable names are not unique. To make them unique, call `.var_names_make_unique`."" From the documentation I don't see a way that I can call .var_names_make_unique(). Is there some preprocessing that I'm missing? ```python. user_input = input(""Enter the path of your file: ""). def convert_h5_to_adata(filename):. filename = str(filename). if os.access(filename, os.R_OK):. sc.read_10x_h5(filename). return. convert_h5_to_adata(user_input). ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/534
https://github.com/scverse/scanpy/issues/534:296,safety,input,input,296,"Getting Error Variable names are not unique when using .read_10x_h5 function; Good Evening,. My goal here is to get either a Gene barcode or dense matrix from a .h5 file from 10x. I'm currently trying to use the .read_10x_h5() function to help me achieve this. To my understanding I just need to input the file name into the function. When I run my code (See below), I get an error stating that ""Variable names are not unique. To make them unique, call `.var_names_make_unique`."" From the documentation I don't see a way that I can call .var_names_make_unique(). Is there some preprocessing that I'm missing? ```python. user_input = input(""Enter the path of your file: ""). def convert_h5_to_adata(filename):. filename = str(filename). if os.access(filename, os.R_OK):. sc.read_10x_h5(filename). return. convert_h5_to_adata(user_input). ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/534
https://github.com/scverse/scanpy/issues/534:376,safety,error,error,376,"Getting Error Variable names are not unique when using .read_10x_h5 function; Good Evening,. My goal here is to get either a Gene barcode or dense matrix from a .h5 file from 10x. I'm currently trying to use the .read_10x_h5() function to help me achieve this. To my understanding I just need to input the file name into the function. When I run my code (See below), I get an error stating that ""Variable names are not unique. To make them unique, call `.var_names_make_unique`."" From the documentation I don't see a way that I can call .var_names_make_unique(). Is there some preprocessing that I'm missing? ```python. user_input = input(""Enter the path of your file: ""). def convert_h5_to_adata(filename):. filename = str(filename). if os.access(filename, os.R_OK):. sc.read_10x_h5(filename). return. convert_h5_to_adata(user_input). ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/534
https://github.com/scverse/scanpy/issues/534:633,safety,input,input,633,"Getting Error Variable names are not unique when using .read_10x_h5 function; Good Evening,. My goal here is to get either a Gene barcode or dense matrix from a .h5 file from 10x. I'm currently trying to use the .read_10x_h5() function to help me achieve this. To my understanding I just need to input the file name into the function. When I run my code (See below), I get an error stating that ""Variable names are not unique. To make them unique, call `.var_names_make_unique`."" From the documentation I don't see a way that I can call .var_names_make_unique(). Is there some preprocessing that I'm missing? ```python. user_input = input(""Enter the path of your file: ""). def convert_h5_to_adata(filename):. filename = str(filename). if os.access(filename, os.R_OK):. sc.read_10x_h5(filename). return. convert_h5_to_adata(user_input). ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/534
https://github.com/scverse/scanpy/issues/534:741,security,access,access,741,"Getting Error Variable names are not unique when using .read_10x_h5 function; Good Evening,. My goal here is to get either a Gene barcode or dense matrix from a .h5 file from 10x. I'm currently trying to use the .read_10x_h5() function to help me achieve this. To my understanding I just need to input the file name into the function. When I run my code (See below), I get an error stating that ""Variable names are not unique. To make them unique, call `.var_names_make_unique`."" From the documentation I don't see a way that I can call .var_names_make_unique(). Is there some preprocessing that I'm missing? ```python. user_input = input(""Enter the path of your file: ""). def convert_h5_to_adata(filename):. filename = str(filename). if os.access(filename, os.R_OK):. sc.read_10x_h5(filename). return. convert_h5_to_adata(user_input). ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/534
https://github.com/scverse/scanpy/issues/534:267,testability,understand,understanding,267,"Getting Error Variable names are not unique when using .read_10x_h5 function; Good Evening,. My goal here is to get either a Gene barcode or dense matrix from a .h5 file from 10x. I'm currently trying to use the .read_10x_h5() function to help me achieve this. To my understanding I just need to input the file name into the function. When I run my code (See below), I get an error stating that ""Variable names are not unique. To make them unique, call `.var_names_make_unique`."" From the documentation I don't see a way that I can call .var_names_make_unique(). Is there some preprocessing that I'm missing? ```python. user_input = input(""Enter the path of your file: ""). def convert_h5_to_adata(filename):. filename = str(filename). if os.access(filename, os.R_OK):. sc.read_10x_h5(filename). return. convert_h5_to_adata(user_input). ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/534
https://github.com/scverse/scanpy/issues/534:8,usability,Error,Error,8,"Getting Error Variable names are not unique when using .read_10x_h5 function; Good Evening,. My goal here is to get either a Gene barcode or dense matrix from a .h5 file from 10x. I'm currently trying to use the .read_10x_h5() function to help me achieve this. To my understanding I just need to input the file name into the function. When I run my code (See below), I get an error stating that ""Variable names are not unique. To make them unique, call `.var_names_make_unique`."" From the documentation I don't see a way that I can call .var_names_make_unique(). Is there some preprocessing that I'm missing? ```python. user_input = input(""Enter the path of your file: ""). def convert_h5_to_adata(filename):. filename = str(filename). if os.access(filename, os.R_OK):. sc.read_10x_h5(filename). return. convert_h5_to_adata(user_input). ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/534
https://github.com/scverse/scanpy/issues/534:239,usability,help,help,239,"Getting Error Variable names are not unique when using .read_10x_h5 function; Good Evening,. My goal here is to get either a Gene barcode or dense matrix from a .h5 file from 10x. I'm currently trying to use the .read_10x_h5() function to help me achieve this. To my understanding I just need to input the file name into the function. When I run my code (See below), I get an error stating that ""Variable names are not unique. To make them unique, call `.var_names_make_unique`."" From the documentation I don't see a way that I can call .var_names_make_unique(). Is there some preprocessing that I'm missing? ```python. user_input = input(""Enter the path of your file: ""). def convert_h5_to_adata(filename):. filename = str(filename). if os.access(filename, os.R_OK):. sc.read_10x_h5(filename). return. convert_h5_to_adata(user_input). ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/534
https://github.com/scverse/scanpy/issues/534:296,usability,input,input,296,"Getting Error Variable names are not unique when using .read_10x_h5 function; Good Evening,. My goal here is to get either a Gene barcode or dense matrix from a .h5 file from 10x. I'm currently trying to use the .read_10x_h5() function to help me achieve this. To my understanding I just need to input the file name into the function. When I run my code (See below), I get an error stating that ""Variable names are not unique. To make them unique, call `.var_names_make_unique`."" From the documentation I don't see a way that I can call .var_names_make_unique(). Is there some preprocessing that I'm missing? ```python. user_input = input(""Enter the path of your file: ""). def convert_h5_to_adata(filename):. filename = str(filename). if os.access(filename, os.R_OK):. sc.read_10x_h5(filename). return. convert_h5_to_adata(user_input). ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/534
https://github.com/scverse/scanpy/issues/534:376,usability,error,error,376,"Getting Error Variable names are not unique when using .read_10x_h5 function; Good Evening,. My goal here is to get either a Gene barcode or dense matrix from a .h5 file from 10x. I'm currently trying to use the .read_10x_h5() function to help me achieve this. To my understanding I just need to input the file name into the function. When I run my code (See below), I get an error stating that ""Variable names are not unique. To make them unique, call `.var_names_make_unique`."" From the documentation I don't see a way that I can call .var_names_make_unique(). Is there some preprocessing that I'm missing? ```python. user_input = input(""Enter the path of your file: ""). def convert_h5_to_adata(filename):. filename = str(filename). if os.access(filename, os.R_OK):. sc.read_10x_h5(filename). return. convert_h5_to_adata(user_input). ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/534
https://github.com/scverse/scanpy/issues/534:489,usability,document,documentation,489,"Getting Error Variable names are not unique when using .read_10x_h5 function; Good Evening,. My goal here is to get either a Gene barcode or dense matrix from a .h5 file from 10x. I'm currently trying to use the .read_10x_h5() function to help me achieve this. To my understanding I just need to input the file name into the function. When I run my code (See below), I get an error stating that ""Variable names are not unique. To make them unique, call `.var_names_make_unique`."" From the documentation I don't see a way that I can call .var_names_make_unique(). Is there some preprocessing that I'm missing? ```python. user_input = input(""Enter the path of your file: ""). def convert_h5_to_adata(filename):. filename = str(filename). if os.access(filename, os.R_OK):. sc.read_10x_h5(filename). return. convert_h5_to_adata(user_input). ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/534
https://github.com/scverse/scanpy/issues/534:633,usability,input,input,633,"Getting Error Variable names are not unique when using .read_10x_h5 function; Good Evening,. My goal here is to get either a Gene barcode or dense matrix from a .h5 file from 10x. I'm currently trying to use the .read_10x_h5() function to help me achieve this. To my understanding I just need to input the file name into the function. When I run my code (See below), I get an error stating that ""Variable names are not unique. To make them unique, call `.var_names_make_unique`."" From the documentation I don't see a way that I can call .var_names_make_unique(). Is there some preprocessing that I'm missing? ```python. user_input = input(""Enter the path of your file: ""). def convert_h5_to_adata(filename):. filename = str(filename). if os.access(filename, os.R_OK):. sc.read_10x_h5(filename). return. convert_h5_to_adata(user_input). ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/534
https://github.com/scverse/scanpy/issues/535:374,integrability,wrap,wraps,374,"Argument tab completion for sc.pl.umap-like functions?; It'd be nice if some level of tab completion of argument names could be maintained for functions like `sc.pl.umap`. This came up recently in #455, and I find myself frequently misspelling (mostly mis-pluralizing) argument names like `color/ colors` and `gene_name/ gene_names`. I've given this a shot using `functools.wraps`, but no luck yet. Any ideas on if we could do this @flying-sheep?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/535
https://github.com/scverse/scanpy/issues/535:128,modifiability,maintain,maintained,128,"Argument tab completion for sc.pl.umap-like functions?; It'd be nice if some level of tab completion of argument names could be maintained for functions like `sc.pl.umap`. This came up recently in #455, and I find myself frequently misspelling (mostly mis-pluralizing) argument names like `color/ colors` and `gene_name/ gene_names`. I've given this a shot using `functools.wraps`, but no luck yet. Any ideas on if we could do this @flying-sheep?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/535
https://github.com/scverse/scanpy/issues/535:13,safety,compl,completion,13,"Argument tab completion for sc.pl.umap-like functions?; It'd be nice if some level of tab completion of argument names could be maintained for functions like `sc.pl.umap`. This came up recently in #455, and I find myself frequently misspelling (mostly mis-pluralizing) argument names like `color/ colors` and `gene_name/ gene_names`. I've given this a shot using `functools.wraps`, but no luck yet. Any ideas on if we could do this @flying-sheep?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/535
https://github.com/scverse/scanpy/issues/535:90,safety,compl,completion,90,"Argument tab completion for sc.pl.umap-like functions?; It'd be nice if some level of tab completion of argument names could be maintained for functions like `sc.pl.umap`. This came up recently in #455, and I find myself frequently misspelling (mostly mis-pluralizing) argument names like `color/ colors` and `gene_name/ gene_names`. I've given this a shot using `functools.wraps`, but no luck yet. Any ideas on if we could do this @flying-sheep?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/535
https://github.com/scverse/scanpy/issues/535:128,safety,maintain,maintained,128,"Argument tab completion for sc.pl.umap-like functions?; It'd be nice if some level of tab completion of argument names could be maintained for functions like `sc.pl.umap`. This came up recently in #455, and I find myself frequently misspelling (mostly mis-pluralizing) argument names like `color/ colors` and `gene_name/ gene_names`. I've given this a shot using `functools.wraps`, but no luck yet. Any ideas on if we could do this @flying-sheep?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/535
https://github.com/scverse/scanpy/issues/535:13,security,compl,completion,13,"Argument tab completion for sc.pl.umap-like functions?; It'd be nice if some level of tab completion of argument names could be maintained for functions like `sc.pl.umap`. This came up recently in #455, and I find myself frequently misspelling (mostly mis-pluralizing) argument names like `color/ colors` and `gene_name/ gene_names`. I've given this a shot using `functools.wraps`, but no luck yet. Any ideas on if we could do this @flying-sheep?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/535
https://github.com/scverse/scanpy/issues/535:90,security,compl,completion,90,"Argument tab completion for sc.pl.umap-like functions?; It'd be nice if some level of tab completion of argument names could be maintained for functions like `sc.pl.umap`. This came up recently in #455, and I find myself frequently misspelling (mostly mis-pluralizing) argument names like `color/ colors` and `gene_name/ gene_names`. I've given this a shot using `functools.wraps`, but no luck yet. Any ideas on if we could do this @flying-sheep?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/535
https://github.com/scverse/scanpy/issues/536:119,availability,avail,available,119,"Scatter plot of numerical observation with missing data; I have a dataset for which I have an observation that is only available for some cells. When I make a scatter plot that I color code for this observation not all cells are plotted:. ```python. import random. import scanpy as sc. adata = sc.datasets.blobs(). adata.obs['property'] = 630 * [float(""nan"")] + 10 * [1] . sc.tl.pca(adata). sc.pl.pca(adata, color='property', size=50). ```. While this should plot 10 cells it only shows one cell:. ![image](https://user-images.githubusercontent.com/7300030/54540172-caa1c700-4997-11e9-946e-01c1e04dd2d2.png). I can get the plot I want by filtering cells first:. ```python. sc.pl.pca(adata[adata.obs['property'] == 1], color='property', size=50). ```. ![image](https://user-images.githubusercontent.com/7300030/54540221-e60cd200-4997-11e9-9b53-e9917bd01c59.png). Would you agree that scanpy should plot all cells that have a valid observation?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/536
https://github.com/scverse/scanpy/issues/536:26,deployability,observ,observation,26,"Scatter plot of numerical observation with missing data; I have a dataset for which I have an observation that is only available for some cells. When I make a scatter plot that I color code for this observation not all cells are plotted:. ```python. import random. import scanpy as sc. adata = sc.datasets.blobs(). adata.obs['property'] = 630 * [float(""nan"")] + 10 * [1] . sc.tl.pca(adata). sc.pl.pca(adata, color='property', size=50). ```. While this should plot 10 cells it only shows one cell:. ![image](https://user-images.githubusercontent.com/7300030/54540172-caa1c700-4997-11e9-946e-01c1e04dd2d2.png). I can get the plot I want by filtering cells first:. ```python. sc.pl.pca(adata[adata.obs['property'] == 1], color='property', size=50). ```. ![image](https://user-images.githubusercontent.com/7300030/54540221-e60cd200-4997-11e9-9b53-e9917bd01c59.png). Would you agree that scanpy should plot all cells that have a valid observation?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/536
https://github.com/scverse/scanpy/issues/536:94,deployability,observ,observation,94,"Scatter plot of numerical observation with missing data; I have a dataset for which I have an observation that is only available for some cells. When I make a scatter plot that I color code for this observation not all cells are plotted:. ```python. import random. import scanpy as sc. adata = sc.datasets.blobs(). adata.obs['property'] = 630 * [float(""nan"")] + 10 * [1] . sc.tl.pca(adata). sc.pl.pca(adata, color='property', size=50). ```. While this should plot 10 cells it only shows one cell:. ![image](https://user-images.githubusercontent.com/7300030/54540172-caa1c700-4997-11e9-946e-01c1e04dd2d2.png). I can get the plot I want by filtering cells first:. ```python. sc.pl.pca(adata[adata.obs['property'] == 1], color='property', size=50). ```. ![image](https://user-images.githubusercontent.com/7300030/54540221-e60cd200-4997-11e9-9b53-e9917bd01c59.png). Would you agree that scanpy should plot all cells that have a valid observation?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/536
https://github.com/scverse/scanpy/issues/536:199,deployability,observ,observation,199,"Scatter plot of numerical observation with missing data; I have a dataset for which I have an observation that is only available for some cells. When I make a scatter plot that I color code for this observation not all cells are plotted:. ```python. import random. import scanpy as sc. adata = sc.datasets.blobs(). adata.obs['property'] = 630 * [float(""nan"")] + 10 * [1] . sc.tl.pca(adata). sc.pl.pca(adata, color='property', size=50). ```. While this should plot 10 cells it only shows one cell:. ![image](https://user-images.githubusercontent.com/7300030/54540172-caa1c700-4997-11e9-946e-01c1e04dd2d2.png). I can get the plot I want by filtering cells first:. ```python. sc.pl.pca(adata[adata.obs['property'] == 1], color='property', size=50). ```. ![image](https://user-images.githubusercontent.com/7300030/54540221-e60cd200-4997-11e9-9b53-e9917bd01c59.png). Would you agree that scanpy should plot all cells that have a valid observation?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/536
https://github.com/scverse/scanpy/issues/536:930,deployability,observ,observation,930,"Scatter plot of numerical observation with missing data; I have a dataset for which I have an observation that is only available for some cells. When I make a scatter plot that I color code for this observation not all cells are plotted:. ```python. import random. import scanpy as sc. adata = sc.datasets.blobs(). adata.obs['property'] = 630 * [float(""nan"")] + 10 * [1] . sc.tl.pca(adata). sc.pl.pca(adata, color='property', size=50). ```. While this should plot 10 cells it only shows one cell:. ![image](https://user-images.githubusercontent.com/7300030/54540172-caa1c700-4997-11e9-946e-01c1e04dd2d2.png). I can get the plot I want by filtering cells first:. ```python. sc.pl.pca(adata[adata.obs['property'] == 1], color='property', size=50). ```. ![image](https://user-images.githubusercontent.com/7300030/54540221-e60cd200-4997-11e9-9b53-e9917bd01c59.png). Would you agree that scanpy should plot all cells that have a valid observation?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/536
https://github.com/scverse/scanpy/issues/536:638,integrability,filter,filtering,638,"Scatter plot of numerical observation with missing data; I have a dataset for which I have an observation that is only available for some cells. When I make a scatter plot that I color code for this observation not all cells are plotted:. ```python. import random. import scanpy as sc. adata = sc.datasets.blobs(). adata.obs['property'] = 630 * [float(""nan"")] + 10 * [1] . sc.tl.pca(adata). sc.pl.pca(adata, color='property', size=50). ```. While this should plot 10 cells it only shows one cell:. ![image](https://user-images.githubusercontent.com/7300030/54540172-caa1c700-4997-11e9-946e-01c1e04dd2d2.png). I can get the plot I want by filtering cells first:. ```python. sc.pl.pca(adata[adata.obs['property'] == 1], color='property', size=50). ```. ![image](https://user-images.githubusercontent.com/7300030/54540221-e60cd200-4997-11e9-9b53-e9917bd01c59.png). Would you agree that scanpy should plot all cells that have a valid observation?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/536
https://github.com/scverse/scanpy/issues/536:119,reliability,availab,available,119,"Scatter plot of numerical observation with missing data; I have a dataset for which I have an observation that is only available for some cells. When I make a scatter plot that I color code for this observation not all cells are plotted:. ```python. import random. import scanpy as sc. adata = sc.datasets.blobs(). adata.obs['property'] = 630 * [float(""nan"")] + 10 * [1] . sc.tl.pca(adata). sc.pl.pca(adata, color='property', size=50). ```. While this should plot 10 cells it only shows one cell:. ![image](https://user-images.githubusercontent.com/7300030/54540172-caa1c700-4997-11e9-946e-01c1e04dd2d2.png). I can get the plot I want by filtering cells first:. ```python. sc.pl.pca(adata[adata.obs['property'] == 1], color='property', size=50). ```. ![image](https://user-images.githubusercontent.com/7300030/54540221-e60cd200-4997-11e9-9b53-e9917bd01c59.png). Would you agree that scanpy should plot all cells that have a valid observation?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/536
https://github.com/scverse/scanpy/issues/536:119,safety,avail,available,119,"Scatter plot of numerical observation with missing data; I have a dataset for which I have an observation that is only available for some cells. When I make a scatter plot that I color code for this observation not all cells are plotted:. ```python. import random. import scanpy as sc. adata = sc.datasets.blobs(). adata.obs['property'] = 630 * [float(""nan"")] + 10 * [1] . sc.tl.pca(adata). sc.pl.pca(adata, color='property', size=50). ```. While this should plot 10 cells it only shows one cell:. ![image](https://user-images.githubusercontent.com/7300030/54540172-caa1c700-4997-11e9-946e-01c1e04dd2d2.png). I can get the plot I want by filtering cells first:. ```python. sc.pl.pca(adata[adata.obs['property'] == 1], color='property', size=50). ```. ![image](https://user-images.githubusercontent.com/7300030/54540221-e60cd200-4997-11e9-9b53-e9917bd01c59.png). Would you agree that scanpy should plot all cells that have a valid observation?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/536
https://github.com/scverse/scanpy/issues/536:924,safety,valid,valid,924,"Scatter plot of numerical observation with missing data; I have a dataset for which I have an observation that is only available for some cells. When I make a scatter plot that I color code for this observation not all cells are plotted:. ```python. import random. import scanpy as sc. adata = sc.datasets.blobs(). adata.obs['property'] = 630 * [float(""nan"")] + 10 * [1] . sc.tl.pca(adata). sc.pl.pca(adata, color='property', size=50). ```. While this should plot 10 cells it only shows one cell:. ![image](https://user-images.githubusercontent.com/7300030/54540172-caa1c700-4997-11e9-946e-01c1e04dd2d2.png). I can get the plot I want by filtering cells first:. ```python. sc.pl.pca(adata[adata.obs['property'] == 1], color='property', size=50). ```. ![image](https://user-images.githubusercontent.com/7300030/54540221-e60cd200-4997-11e9-9b53-e9917bd01c59.png). Would you agree that scanpy should plot all cells that have a valid observation?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/536
https://github.com/scverse/scanpy/issues/536:119,security,availab,available,119,"Scatter plot of numerical observation with missing data; I have a dataset for which I have an observation that is only available for some cells. When I make a scatter plot that I color code for this observation not all cells are plotted:. ```python. import random. import scanpy as sc. adata = sc.datasets.blobs(). adata.obs['property'] = 630 * [float(""nan"")] + 10 * [1] . sc.tl.pca(adata). sc.pl.pca(adata, color='property', size=50). ```. While this should plot 10 cells it only shows one cell:. ![image](https://user-images.githubusercontent.com/7300030/54540172-caa1c700-4997-11e9-946e-01c1e04dd2d2.png). I can get the plot I want by filtering cells first:. ```python. sc.pl.pca(adata[adata.obs['property'] == 1], color='property', size=50). ```. ![image](https://user-images.githubusercontent.com/7300030/54540221-e60cd200-4997-11e9-9b53-e9917bd01c59.png). Would you agree that scanpy should plot all cells that have a valid observation?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/536
https://github.com/scverse/scanpy/issues/536:26,testability,observ,observation,26,"Scatter plot of numerical observation with missing data; I have a dataset for which I have an observation that is only available for some cells. When I make a scatter plot that I color code for this observation not all cells are plotted:. ```python. import random. import scanpy as sc. adata = sc.datasets.blobs(). adata.obs['property'] = 630 * [float(""nan"")] + 10 * [1] . sc.tl.pca(adata). sc.pl.pca(adata, color='property', size=50). ```. While this should plot 10 cells it only shows one cell:. ![image](https://user-images.githubusercontent.com/7300030/54540172-caa1c700-4997-11e9-946e-01c1e04dd2d2.png). I can get the plot I want by filtering cells first:. ```python. sc.pl.pca(adata[adata.obs['property'] == 1], color='property', size=50). ```. ![image](https://user-images.githubusercontent.com/7300030/54540221-e60cd200-4997-11e9-9b53-e9917bd01c59.png). Would you agree that scanpy should plot all cells that have a valid observation?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/536
https://github.com/scverse/scanpy/issues/536:94,testability,observ,observation,94,"Scatter plot of numerical observation with missing data; I have a dataset for which I have an observation that is only available for some cells. When I make a scatter plot that I color code for this observation not all cells are plotted:. ```python. import random. import scanpy as sc. adata = sc.datasets.blobs(). adata.obs['property'] = 630 * [float(""nan"")] + 10 * [1] . sc.tl.pca(adata). sc.pl.pca(adata, color='property', size=50). ```. While this should plot 10 cells it only shows one cell:. ![image](https://user-images.githubusercontent.com/7300030/54540172-caa1c700-4997-11e9-946e-01c1e04dd2d2.png). I can get the plot I want by filtering cells first:. ```python. sc.pl.pca(adata[adata.obs['property'] == 1], color='property', size=50). ```. ![image](https://user-images.githubusercontent.com/7300030/54540221-e60cd200-4997-11e9-9b53-e9917bd01c59.png). Would you agree that scanpy should plot all cells that have a valid observation?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/536
https://github.com/scverse/scanpy/issues/536:199,testability,observ,observation,199,"Scatter plot of numerical observation with missing data; I have a dataset for which I have an observation that is only available for some cells. When I make a scatter plot that I color code for this observation not all cells are plotted:. ```python. import random. import scanpy as sc. adata = sc.datasets.blobs(). adata.obs['property'] = 630 * [float(""nan"")] + 10 * [1] . sc.tl.pca(adata). sc.pl.pca(adata, color='property', size=50). ```. While this should plot 10 cells it only shows one cell:. ![image](https://user-images.githubusercontent.com/7300030/54540172-caa1c700-4997-11e9-946e-01c1e04dd2d2.png). I can get the plot I want by filtering cells first:. ```python. sc.pl.pca(adata[adata.obs['property'] == 1], color='property', size=50). ```. ![image](https://user-images.githubusercontent.com/7300030/54540221-e60cd200-4997-11e9-9b53-e9917bd01c59.png). Would you agree that scanpy should plot all cells that have a valid observation?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/536
https://github.com/scverse/scanpy/issues/536:930,testability,observ,observation,930,"Scatter plot of numerical observation with missing data; I have a dataset for which I have an observation that is only available for some cells. When I make a scatter plot that I color code for this observation not all cells are plotted:. ```python. import random. import scanpy as sc. adata = sc.datasets.blobs(). adata.obs['property'] = 630 * [float(""nan"")] + 10 * [1] . sc.tl.pca(adata). sc.pl.pca(adata, color='property', size=50). ```. While this should plot 10 cells it only shows one cell:. ![image](https://user-images.githubusercontent.com/7300030/54540172-caa1c700-4997-11e9-946e-01c1e04dd2d2.png). I can get the plot I want by filtering cells first:. ```python. sc.pl.pca(adata[adata.obs['property'] == 1], color='property', size=50). ```. ![image](https://user-images.githubusercontent.com/7300030/54540221-e60cd200-4997-11e9-9b53-e9917bd01c59.png). Would you agree that scanpy should plot all cells that have a valid observation?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/536
https://github.com/scverse/scanpy/issues/536:515,usability,user,user-images,515,"Scatter plot of numerical observation with missing data; I have a dataset for which I have an observation that is only available for some cells. When I make a scatter plot that I color code for this observation not all cells are plotted:. ```python. import random. import scanpy as sc. adata = sc.datasets.blobs(). adata.obs['property'] = 630 * [float(""nan"")] + 10 * [1] . sc.tl.pca(adata). sc.pl.pca(adata, color='property', size=50). ```. While this should plot 10 cells it only shows one cell:. ![image](https://user-images.githubusercontent.com/7300030/54540172-caa1c700-4997-11e9-946e-01c1e04dd2d2.png). I can get the plot I want by filtering cells first:. ```python. sc.pl.pca(adata[adata.obs['property'] == 1], color='property', size=50). ```. ![image](https://user-images.githubusercontent.com/7300030/54540221-e60cd200-4997-11e9-9b53-e9917bd01c59.png). Would you agree that scanpy should plot all cells that have a valid observation?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/536
https://github.com/scverse/scanpy/issues/536:768,usability,user,user-images,768,"Scatter plot of numerical observation with missing data; I have a dataset for which I have an observation that is only available for some cells. When I make a scatter plot that I color code for this observation not all cells are plotted:. ```python. import random. import scanpy as sc. adata = sc.datasets.blobs(). adata.obs['property'] = 630 * [float(""nan"")] + 10 * [1] . sc.tl.pca(adata). sc.pl.pca(adata, color='property', size=50). ```. While this should plot 10 cells it only shows one cell:. ![image](https://user-images.githubusercontent.com/7300030/54540172-caa1c700-4997-11e9-946e-01c1e04dd2d2.png). I can get the plot I want by filtering cells first:. ```python. sc.pl.pca(adata[adata.obs['property'] == 1], color='property', size=50). ```. ![image](https://user-images.githubusercontent.com/7300030/54540221-e60cd200-4997-11e9-9b53-e9917bd01c59.png). Would you agree that scanpy should plot all cells that have a valid observation?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/536
https://github.com/scverse/scanpy/issues/537:0,deployability,Updat,Updating,0,Updating scanpy; I see that some functions have been deprecated and replaced by new ones. What is the simplest way to update Scanpy using conda?,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/537
https://github.com/scverse/scanpy/issues/537:118,deployability,updat,update,118,Updating scanpy; I see that some functions have been deprecated and replaced by new ones. What is the simplest way to update Scanpy using conda?,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/537
https://github.com/scverse/scanpy/issues/537:0,safety,Updat,Updating,0,Updating scanpy; I see that some functions have been deprecated and replaced by new ones. What is the simplest way to update Scanpy using conda?,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/537
https://github.com/scverse/scanpy/issues/537:118,safety,updat,update,118,Updating scanpy; I see that some functions have been deprecated and replaced by new ones. What is the simplest way to update Scanpy using conda?,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/537
https://github.com/scverse/scanpy/issues/537:0,security,Updat,Updating,0,Updating scanpy; I see that some functions have been deprecated and replaced by new ones. What is the simplest way to update Scanpy using conda?,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/537
https://github.com/scverse/scanpy/issues/537:118,security,updat,update,118,Updating scanpy; I see that some functions have been deprecated and replaced by new ones. What is the simplest way to update Scanpy using conda?,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/537
https://github.com/scverse/scanpy/issues/537:102,testability,simpl,simplest,102,Updating scanpy; I see that some functions have been deprecated and replaced by new ones. What is the simplest way to update Scanpy using conda?,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/537
https://github.com/scverse/scanpy/issues/537:102,usability,simpl,simplest,102,Updating scanpy; I see that some functions have been deprecated and replaced by new ones. What is the simplest way to update Scanpy using conda?,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/537
https://github.com/scverse/scanpy/pull/538:471,deployability,scale,scaled,471,"Add an explicit expression threshold to dotplot; This also adds an option to calculate the mean only over the cells expressing given genes. I think these options give users more flexibility to binarize expression. For example, for z-score normalized data, a combination of `expression_threshold=-1` and `use_raw=False` binarizes the expression using a cutoff value corresponding to one sd away from the mean. Also the `obs_bool = obs_tidy.astype(bool)` was erroneous for scaled data without the .raw since both positive and negative values were mapped to True. .",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/538
https://github.com/scverse/scanpy/pull/538:471,energy efficiency,scale,scaled,471,"Add an explicit expression threshold to dotplot; This also adds an option to calculate the mean only over the cells expressing given genes. I think these options give users more flexibility to binarize expression. For example, for z-score normalized data, a combination of `expression_threshold=-1` and `use_raw=False` binarizes the expression using a cutoff value corresponding to one sd away from the mean. Also the `obs_bool = obs_tidy.astype(bool)` was erroneous for scaled data without the .raw since both positive and negative values were mapped to True. .",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/538
https://github.com/scverse/scanpy/pull/538:471,modifiability,scal,scaled,471,"Add an explicit expression threshold to dotplot; This also adds an option to calculate the mean only over the cells expressing given genes. I think these options give users more flexibility to binarize expression. For example, for z-score normalized data, a combination of `expression_threshold=-1` and `use_raw=False` binarizes the expression using a cutoff value corresponding to one sd away from the mean. Also the `obs_bool = obs_tidy.astype(bool)` was erroneous for scaled data without the .raw since both positive and negative values were mapped to True. .",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/538
https://github.com/scverse/scanpy/pull/538:471,performance,scale,scaled,471,"Add an explicit expression threshold to dotplot; This also adds an option to calculate the mean only over the cells expressing given genes. I think these options give users more flexibility to binarize expression. For example, for z-score normalized data, a combination of `expression_threshold=-1` and `use_raw=False` binarizes the expression using a cutoff value corresponding to one sd away from the mean. Also the `obs_bool = obs_tidy.astype(bool)` was erroneous for scaled data without the .raw since both positive and negative values were mapped to True. .",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/538
https://github.com/scverse/scanpy/pull/538:167,usability,user,users,167,"Add an explicit expression threshold to dotplot; This also adds an option to calculate the mean only over the cells expressing given genes. I think these options give users more flexibility to binarize expression. For example, for z-score normalized data, a combination of `expression_threshold=-1` and `use_raw=False` binarizes the expression using a cutoff value corresponding to one sd away from the mean. Also the `obs_bool = obs_tidy.astype(bool)` was erroneous for scaled data without the .raw since both positive and negative values were mapped to True. .",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/538
https://github.com/scverse/scanpy/pull/539:104,deployability,API,API,104,"Enable autocompletion for scatterplot functions; Fixes #535. Sorry for the messy diff, I had to put the API functions after `plot_scatter` since the decorator is called at import time and needs `plot_scatter` to be defined beforehand. This also fixes the docs for `pca` and adds parameter docs for `arrows_kwds` and `return_fig`.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/539
https://github.com/scverse/scanpy/pull/539:104,integrability,API,API,104,"Enable autocompletion for scatterplot functions; Fixes #535. Sorry for the messy diff, I had to put the API functions after `plot_scatter` since the decorator is called at import time and needs `plot_scatter` to be defined beforehand. This also fixes the docs for `pca` and adds parameter docs for `arrows_kwds` and `return_fig`.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/539
https://github.com/scverse/scanpy/pull/539:104,interoperability,API,API,104,"Enable autocompletion for scatterplot functions; Fixes #535. Sorry for the messy diff, I had to put the API functions after `plot_scatter` since the decorator is called at import time and needs `plot_scatter` to be defined beforehand. This also fixes the docs for `pca` and adds parameter docs for `arrows_kwds` and `return_fig`.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/539
https://github.com/scverse/scanpy/pull/539:149,modifiability,deco,decorator,149,"Enable autocompletion for scatterplot functions; Fixes #535. Sorry for the messy diff, I had to put the API functions after `plot_scatter` since the decorator is called at import time and needs `plot_scatter` to be defined beforehand. This also fixes the docs for `pca` and adds parameter docs for `arrows_kwds` and `return_fig`.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/539
https://github.com/scverse/scanpy/pull/539:279,modifiability,paramet,parameter,279,"Enable autocompletion for scatterplot functions; Fixes #535. Sorry for the messy diff, I had to put the API functions after `plot_scatter` since the decorator is called at import time and needs `plot_scatter` to be defined beforehand. This also fixes the docs for `pca` and adds parameter docs for `arrows_kwds` and `return_fig`.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/539
https://github.com/scverse/scanpy/pull/539:179,performance,time,time,179,"Enable autocompletion for scatterplot functions; Fixes #535. Sorry for the messy diff, I had to put the API functions after `plot_scatter` since the decorator is called at import time and needs `plot_scatter` to be defined beforehand. This also fixes the docs for `pca` and adds parameter docs for `arrows_kwds` and `return_fig`.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/539
https://github.com/scverse/scanpy/pull/540:43,deployability,contain,contains,43,fix #536 ; fix a problem when data to plot contains NaNs. The fix is to return a numpy array instead of a pandas.Series,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/540
https://github.com/scverse/scanpy/issues/542:362,availability,slo,slow,362,Suggestion: Start a gitter.im chat room; Many bio data analysis packages hosted on GitHub will have a Gitter.im chat room with a direct link in the readme. This is very helpful for newcomers with questions that may not qualify as an issue but can be quickly answered by the package contributors or just powerusers. I recommend scanpy start a gitter. It may help slow the posting of new issues and allow faster triaging and help.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/542
https://github.com/scverse/scanpy/issues/542:303,energy efficiency,power,powerusers,303,Suggestion: Start a gitter.im chat room; Many bio data analysis packages hosted on GitHub will have a Gitter.im chat room with a direct link in the readme. This is very helpful for newcomers with questions that may not qualify as an issue but can be quickly answered by the package contributors or just powerusers. I recommend scanpy start a gitter. It may help slow the posting of new issues and allow faster triaging and help.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/542
https://github.com/scverse/scanpy/issues/542:64,modifiability,pac,packages,64,Suggestion: Start a gitter.im chat room; Many bio data analysis packages hosted on GitHub will have a Gitter.im chat room with a direct link in the readme. This is very helpful for newcomers with questions that may not qualify as an issue but can be quickly answered by the package contributors or just powerusers. I recommend scanpy start a gitter. It may help slow the posting of new issues and allow faster triaging and help.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/542
https://github.com/scverse/scanpy/issues/542:274,modifiability,pac,package,274,Suggestion: Start a gitter.im chat room; Many bio data analysis packages hosted on GitHub will have a Gitter.im chat room with a direct link in the readme. This is very helpful for newcomers with questions that may not qualify as an issue but can be quickly answered by the package contributors or just powerusers. I recommend scanpy start a gitter. It may help slow the posting of new issues and allow faster triaging and help.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/542
https://github.com/scverse/scanpy/issues/542:362,reliability,slo,slow,362,Suggestion: Start a gitter.im chat room; Many bio data analysis packages hosted on GitHub will have a Gitter.im chat room with a direct link in the readme. This is very helpful for newcomers with questions that may not qualify as an issue but can be quickly answered by the package contributors or just powerusers. I recommend scanpy start a gitter. It may help slow the posting of new issues and allow faster triaging and help.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/542
https://github.com/scverse/scanpy/issues/542:410,security,triag,triaging,410,Suggestion: Start a gitter.im chat room; Many bio data analysis packages hosted on GitHub will have a Gitter.im chat room with a direct link in the readme. This is very helpful for newcomers with questions that may not qualify as an issue but can be quickly answered by the package contributors or just powerusers. I recommend scanpy start a gitter. It may help slow the posting of new issues and allow faster triaging and help.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/542
https://github.com/scverse/scanpy/issues/542:169,usability,help,helpful,169,Suggestion: Start a gitter.im chat room; Many bio data analysis packages hosted on GitHub will have a Gitter.im chat room with a direct link in the readme. This is very helpful for newcomers with questions that may not qualify as an issue but can be quickly answered by the package contributors or just powerusers. I recommend scanpy start a gitter. It may help slow the posting of new issues and allow faster triaging and help.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/542
https://github.com/scverse/scanpy/issues/542:357,usability,help,help,357,Suggestion: Start a gitter.im chat room; Many bio data analysis packages hosted on GitHub will have a Gitter.im chat room with a direct link in the readme. This is very helpful for newcomers with questions that may not qualify as an issue but can be quickly answered by the package contributors or just powerusers. I recommend scanpy start a gitter. It may help slow the posting of new issues and allow faster triaging and help.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/542
https://github.com/scverse/scanpy/issues/542:423,usability,help,help,423,Suggestion: Start a gitter.im chat room; Many bio data analysis packages hosted on GitHub will have a Gitter.im chat room with a direct link in the readme. This is very helpful for newcomers with questions that may not qualify as an issue but can be quickly answered by the package contributors or just powerusers. I recommend scanpy start a gitter. It may help slow the posting of new issues and allow faster triaging and help.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/542
https://github.com/scverse/scanpy/pull/543:274,deployability,version,version,274,Density calculation and plotting; This pull request is for calculating and plotting cell densities on an embedded representation. This is especially useful together with an `.obs` covariate to calculate and visualize cell densities over conditions. Code is adapted from raw version by @sophietr . Still work in progress...,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/543
https://github.com/scverse/scanpy/pull/543:257,energy efficiency,adapt,adapted,257,Density calculation and plotting; This pull request is for calculating and plotting cell densities on an embedded representation. This is especially useful together with an `.obs` covariate to calculate and visualize cell densities over conditions. Code is adapted from raw version by @sophietr . Still work in progress...,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/543
https://github.com/scverse/scanpy/pull/543:257,integrability,adapt,adapted,257,Density calculation and plotting; This pull request is for calculating and plotting cell densities on an embedded representation. This is especially useful together with an `.obs` covariate to calculate and visualize cell densities over conditions. Code is adapted from raw version by @sophietr . Still work in progress...,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/543
https://github.com/scverse/scanpy/pull/543:274,integrability,version,version,274,Density calculation and plotting; This pull request is for calculating and plotting cell densities on an embedded representation. This is especially useful together with an `.obs` covariate to calculate and visualize cell densities over conditions. Code is adapted from raw version by @sophietr . Still work in progress...,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/543
https://github.com/scverse/scanpy/pull/543:257,interoperability,adapt,adapted,257,Density calculation and plotting; This pull request is for calculating and plotting cell densities on an embedded representation. This is especially useful together with an `.obs` covariate to calculate and visualize cell densities over conditions. Code is adapted from raw version by @sophietr . Still work in progress...,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/543
https://github.com/scverse/scanpy/pull/543:257,modifiability,adapt,adapted,257,Density calculation and plotting; This pull request is for calculating and plotting cell densities on an embedded representation. This is especially useful together with an `.obs` covariate to calculate and visualize cell densities over conditions. Code is adapted from raw version by @sophietr . Still work in progress...,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/543
https://github.com/scverse/scanpy/pull/543:274,modifiability,version,version,274,Density calculation and plotting; This pull request is for calculating and plotting cell densities on an embedded representation. This is especially useful together with an `.obs` covariate to calculate and visualize cell densities over conditions. Code is adapted from raw version by @sophietr . Still work in progress...,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/543
https://github.com/scverse/scanpy/pull/543:207,usability,visual,visualize,207,Density calculation and plotting; This pull request is for calculating and plotting cell densities on an embedded representation. This is especially useful together with an `.obs` covariate to calculate and visualize cell densities over conditions. Code is adapted from raw version by @sophietr . Still work in progress...,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/543
https://github.com/scverse/scanpy/pull/543:311,usability,progress,progress,311,Density calculation and plotting; This pull request is for calculating and plotting cell densities on an embedded representation. This is especially useful together with an `.obs` covariate to calculate and visualize cell densities over conditions. Code is adapted from raw version by @sophietr . Still work in progress...,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/543
https://github.com/scverse/scanpy/issues/544:257,availability,error,error,257,"Issue using new features in the visualizing marker genes tutorial; Hello, I am trying to use the visualize marker genes tutorial to make some plots. I am importing scanpy in the new way (import scanpy as sc) as suggested in the tutorial but I am getting an error message:. AttributeError Traceback (most recent call last). <ipython-input-5-dfc1e4d9ed06> in <module>(). ----> 1 ax = sc.pl.correlation_matrix(adata, 'cell_types'). AttributeError: module 'scanpy.plotting' has no attribute 'correlation_matrix'. Here are the versions of all the packages I am using:. scanpy==1.4 anndata==0.6.17 numpy==1.16.0 scipy==1.2.0 pandas==0.23.4 scikit-learn==0.20.2 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 . Am I missing something ? Thanks.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/544
https://github.com/scverse/scanpy/issues/544:358,deployability,modul,module,358,"Issue using new features in the visualizing marker genes tutorial; Hello, I am trying to use the visualize marker genes tutorial to make some plots. I am importing scanpy in the new way (import scanpy as sc) as suggested in the tutorial but I am getting an error message:. AttributeError Traceback (most recent call last). <ipython-input-5-dfc1e4d9ed06> in <module>(). ----> 1 ax = sc.pl.correlation_matrix(adata, 'cell_types'). AttributeError: module 'scanpy.plotting' has no attribute 'correlation_matrix'. Here are the versions of all the packages I am using:. scanpy==1.4 anndata==0.6.17 numpy==1.16.0 scipy==1.2.0 pandas==0.23.4 scikit-learn==0.20.2 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 . Am I missing something ? Thanks.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/544
https://github.com/scverse/scanpy/issues/544:445,deployability,modul,module,445,"Issue using new features in the visualizing marker genes tutorial; Hello, I am trying to use the visualize marker genes tutorial to make some plots. I am importing scanpy in the new way (import scanpy as sc) as suggested in the tutorial but I am getting an error message:. AttributeError Traceback (most recent call last). <ipython-input-5-dfc1e4d9ed06> in <module>(). ----> 1 ax = sc.pl.correlation_matrix(adata, 'cell_types'). AttributeError: module 'scanpy.plotting' has no attribute 'correlation_matrix'. Here are the versions of all the packages I am using:. scanpy==1.4 anndata==0.6.17 numpy==1.16.0 scipy==1.2.0 pandas==0.23.4 scikit-learn==0.20.2 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 . Am I missing something ? Thanks.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/544
https://github.com/scverse/scanpy/issues/544:522,deployability,version,versions,522,"Issue using new features in the visualizing marker genes tutorial; Hello, I am trying to use the visualize marker genes tutorial to make some plots. I am importing scanpy in the new way (import scanpy as sc) as suggested in the tutorial but I am getting an error message:. AttributeError Traceback (most recent call last). <ipython-input-5-dfc1e4d9ed06> in <module>(). ----> 1 ax = sc.pl.correlation_matrix(adata, 'cell_types'). AttributeError: module 'scanpy.plotting' has no attribute 'correlation_matrix'. Here are the versions of all the packages I am using:. scanpy==1.4 anndata==0.6.17 numpy==1.16.0 scipy==1.2.0 pandas==0.23.4 scikit-learn==0.20.2 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 . Am I missing something ? Thanks.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/544
https://github.com/scverse/scanpy/issues/544:263,integrability,messag,message,263,"Issue using new features in the visualizing marker genes tutorial; Hello, I am trying to use the visualize marker genes tutorial to make some plots. I am importing scanpy in the new way (import scanpy as sc) as suggested in the tutorial but I am getting an error message:. AttributeError Traceback (most recent call last). <ipython-input-5-dfc1e4d9ed06> in <module>(). ----> 1 ax = sc.pl.correlation_matrix(adata, 'cell_types'). AttributeError: module 'scanpy.plotting' has no attribute 'correlation_matrix'. Here are the versions of all the packages I am using:. scanpy==1.4 anndata==0.6.17 numpy==1.16.0 scipy==1.2.0 pandas==0.23.4 scikit-learn==0.20.2 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 . Am I missing something ? Thanks.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/544
https://github.com/scverse/scanpy/issues/544:522,integrability,version,versions,522,"Issue using new features in the visualizing marker genes tutorial; Hello, I am trying to use the visualize marker genes tutorial to make some plots. I am importing scanpy in the new way (import scanpy as sc) as suggested in the tutorial but I am getting an error message:. AttributeError Traceback (most recent call last). <ipython-input-5-dfc1e4d9ed06> in <module>(). ----> 1 ax = sc.pl.correlation_matrix(adata, 'cell_types'). AttributeError: module 'scanpy.plotting' has no attribute 'correlation_matrix'. Here are the versions of all the packages I am using:. scanpy==1.4 anndata==0.6.17 numpy==1.16.0 scipy==1.2.0 pandas==0.23.4 scikit-learn==0.20.2 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 . Am I missing something ? Thanks.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/544
https://github.com/scverse/scanpy/issues/544:263,interoperability,messag,message,263,"Issue using new features in the visualizing marker genes tutorial; Hello, I am trying to use the visualize marker genes tutorial to make some plots. I am importing scanpy in the new way (import scanpy as sc) as suggested in the tutorial but I am getting an error message:. AttributeError Traceback (most recent call last). <ipython-input-5-dfc1e4d9ed06> in <module>(). ----> 1 ax = sc.pl.correlation_matrix(adata, 'cell_types'). AttributeError: module 'scanpy.plotting' has no attribute 'correlation_matrix'. Here are the versions of all the packages I am using:. scanpy==1.4 anndata==0.6.17 numpy==1.16.0 scipy==1.2.0 pandas==0.23.4 scikit-learn==0.20.2 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 . Am I missing something ? Thanks.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/544
https://github.com/scverse/scanpy/issues/544:358,modifiability,modul,module,358,"Issue using new features in the visualizing marker genes tutorial; Hello, I am trying to use the visualize marker genes tutorial to make some plots. I am importing scanpy in the new way (import scanpy as sc) as suggested in the tutorial but I am getting an error message:. AttributeError Traceback (most recent call last). <ipython-input-5-dfc1e4d9ed06> in <module>(). ----> 1 ax = sc.pl.correlation_matrix(adata, 'cell_types'). AttributeError: module 'scanpy.plotting' has no attribute 'correlation_matrix'. Here are the versions of all the packages I am using:. scanpy==1.4 anndata==0.6.17 numpy==1.16.0 scipy==1.2.0 pandas==0.23.4 scikit-learn==0.20.2 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 . Am I missing something ? Thanks.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/544
https://github.com/scverse/scanpy/issues/544:445,modifiability,modul,module,445,"Issue using new features in the visualizing marker genes tutorial; Hello, I am trying to use the visualize marker genes tutorial to make some plots. I am importing scanpy in the new way (import scanpy as sc) as suggested in the tutorial but I am getting an error message:. AttributeError Traceback (most recent call last). <ipython-input-5-dfc1e4d9ed06> in <module>(). ----> 1 ax = sc.pl.correlation_matrix(adata, 'cell_types'). AttributeError: module 'scanpy.plotting' has no attribute 'correlation_matrix'. Here are the versions of all the packages I am using:. scanpy==1.4 anndata==0.6.17 numpy==1.16.0 scipy==1.2.0 pandas==0.23.4 scikit-learn==0.20.2 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 . Am I missing something ? Thanks.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/544
https://github.com/scverse/scanpy/issues/544:522,modifiability,version,versions,522,"Issue using new features in the visualizing marker genes tutorial; Hello, I am trying to use the visualize marker genes tutorial to make some plots. I am importing scanpy in the new way (import scanpy as sc) as suggested in the tutorial but I am getting an error message:. AttributeError Traceback (most recent call last). <ipython-input-5-dfc1e4d9ed06> in <module>(). ----> 1 ax = sc.pl.correlation_matrix(adata, 'cell_types'). AttributeError: module 'scanpy.plotting' has no attribute 'correlation_matrix'. Here are the versions of all the packages I am using:. scanpy==1.4 anndata==0.6.17 numpy==1.16.0 scipy==1.2.0 pandas==0.23.4 scikit-learn==0.20.2 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 . Am I missing something ? Thanks.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/544
https://github.com/scverse/scanpy/issues/544:542,modifiability,pac,packages,542,"Issue using new features in the visualizing marker genes tutorial; Hello, I am trying to use the visualize marker genes tutorial to make some plots. I am importing scanpy in the new way (import scanpy as sc) as suggested in the tutorial but I am getting an error message:. AttributeError Traceback (most recent call last). <ipython-input-5-dfc1e4d9ed06> in <module>(). ----> 1 ax = sc.pl.correlation_matrix(adata, 'cell_types'). AttributeError: module 'scanpy.plotting' has no attribute 'correlation_matrix'. Here are the versions of all the packages I am using:. scanpy==1.4 anndata==0.6.17 numpy==1.16.0 scipy==1.2.0 pandas==0.23.4 scikit-learn==0.20.2 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 . Am I missing something ? Thanks.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/544
https://github.com/scverse/scanpy/issues/544:257,performance,error,error,257,"Issue using new features in the visualizing marker genes tutorial; Hello, I am trying to use the visualize marker genes tutorial to make some plots. I am importing scanpy in the new way (import scanpy as sc) as suggested in the tutorial but I am getting an error message:. AttributeError Traceback (most recent call last). <ipython-input-5-dfc1e4d9ed06> in <module>(). ----> 1 ax = sc.pl.correlation_matrix(adata, 'cell_types'). AttributeError: module 'scanpy.plotting' has no attribute 'correlation_matrix'. Here are the versions of all the packages I am using:. scanpy==1.4 anndata==0.6.17 numpy==1.16.0 scipy==1.2.0 pandas==0.23.4 scikit-learn==0.20.2 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 . Am I missing something ? Thanks.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/544
https://github.com/scverse/scanpy/issues/544:257,safety,error,error,257,"Issue using new features in the visualizing marker genes tutorial; Hello, I am trying to use the visualize marker genes tutorial to make some plots. I am importing scanpy in the new way (import scanpy as sc) as suggested in the tutorial but I am getting an error message:. AttributeError Traceback (most recent call last). <ipython-input-5-dfc1e4d9ed06> in <module>(). ----> 1 ax = sc.pl.correlation_matrix(adata, 'cell_types'). AttributeError: module 'scanpy.plotting' has no attribute 'correlation_matrix'. Here are the versions of all the packages I am using:. scanpy==1.4 anndata==0.6.17 numpy==1.16.0 scipy==1.2.0 pandas==0.23.4 scikit-learn==0.20.2 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 . Am I missing something ? Thanks.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/544
https://github.com/scverse/scanpy/issues/544:332,safety,input,input-,332,"Issue using new features in the visualizing marker genes tutorial; Hello, I am trying to use the visualize marker genes tutorial to make some plots. I am importing scanpy in the new way (import scanpy as sc) as suggested in the tutorial but I am getting an error message:. AttributeError Traceback (most recent call last). <ipython-input-5-dfc1e4d9ed06> in <module>(). ----> 1 ax = sc.pl.correlation_matrix(adata, 'cell_types'). AttributeError: module 'scanpy.plotting' has no attribute 'correlation_matrix'. Here are the versions of all the packages I am using:. scanpy==1.4 anndata==0.6.17 numpy==1.16.0 scipy==1.2.0 pandas==0.23.4 scikit-learn==0.20.2 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 . Am I missing something ? Thanks.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/544
https://github.com/scverse/scanpy/issues/544:358,safety,modul,module,358,"Issue using new features in the visualizing marker genes tutorial; Hello, I am trying to use the visualize marker genes tutorial to make some plots. I am importing scanpy in the new way (import scanpy as sc) as suggested in the tutorial but I am getting an error message:. AttributeError Traceback (most recent call last). <ipython-input-5-dfc1e4d9ed06> in <module>(). ----> 1 ax = sc.pl.correlation_matrix(adata, 'cell_types'). AttributeError: module 'scanpy.plotting' has no attribute 'correlation_matrix'. Here are the versions of all the packages I am using:. scanpy==1.4 anndata==0.6.17 numpy==1.16.0 scipy==1.2.0 pandas==0.23.4 scikit-learn==0.20.2 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 . Am I missing something ? Thanks.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/544
https://github.com/scverse/scanpy/issues/544:445,safety,modul,module,445,"Issue using new features in the visualizing marker genes tutorial; Hello, I am trying to use the visualize marker genes tutorial to make some plots. I am importing scanpy in the new way (import scanpy as sc) as suggested in the tutorial but I am getting an error message:. AttributeError Traceback (most recent call last). <ipython-input-5-dfc1e4d9ed06> in <module>(). ----> 1 ax = sc.pl.correlation_matrix(adata, 'cell_types'). AttributeError: module 'scanpy.plotting' has no attribute 'correlation_matrix'. Here are the versions of all the packages I am using:. scanpy==1.4 anndata==0.6.17 numpy==1.16.0 scipy==1.2.0 pandas==0.23.4 scikit-learn==0.20.2 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 . Am I missing something ? Thanks.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/544
https://github.com/scverse/scanpy/issues/544:288,testability,Trace,Traceback,288,"Issue using new features in the visualizing marker genes tutorial; Hello, I am trying to use the visualize marker genes tutorial to make some plots. I am importing scanpy in the new way (import scanpy as sc) as suggested in the tutorial but I am getting an error message:. AttributeError Traceback (most recent call last). <ipython-input-5-dfc1e4d9ed06> in <module>(). ----> 1 ax = sc.pl.correlation_matrix(adata, 'cell_types'). AttributeError: module 'scanpy.plotting' has no attribute 'correlation_matrix'. Here are the versions of all the packages I am using:. scanpy==1.4 anndata==0.6.17 numpy==1.16.0 scipy==1.2.0 pandas==0.23.4 scikit-learn==0.20.2 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 . Am I missing something ? Thanks.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/544
https://github.com/scverse/scanpy/issues/544:32,usability,visual,visualizing,32,"Issue using new features in the visualizing marker genes tutorial; Hello, I am trying to use the visualize marker genes tutorial to make some plots. I am importing scanpy in the new way (import scanpy as sc) as suggested in the tutorial but I am getting an error message:. AttributeError Traceback (most recent call last). <ipython-input-5-dfc1e4d9ed06> in <module>(). ----> 1 ax = sc.pl.correlation_matrix(adata, 'cell_types'). AttributeError: module 'scanpy.plotting' has no attribute 'correlation_matrix'. Here are the versions of all the packages I am using:. scanpy==1.4 anndata==0.6.17 numpy==1.16.0 scipy==1.2.0 pandas==0.23.4 scikit-learn==0.20.2 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 . Am I missing something ? Thanks.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/544
https://github.com/scverse/scanpy/issues/544:97,usability,visual,visualize,97,"Issue using new features in the visualizing marker genes tutorial; Hello, I am trying to use the visualize marker genes tutorial to make some plots. I am importing scanpy in the new way (import scanpy as sc) as suggested in the tutorial but I am getting an error message:. AttributeError Traceback (most recent call last). <ipython-input-5-dfc1e4d9ed06> in <module>(). ----> 1 ax = sc.pl.correlation_matrix(adata, 'cell_types'). AttributeError: module 'scanpy.plotting' has no attribute 'correlation_matrix'. Here are the versions of all the packages I am using:. scanpy==1.4 anndata==0.6.17 numpy==1.16.0 scipy==1.2.0 pandas==0.23.4 scikit-learn==0.20.2 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 . Am I missing something ? Thanks.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/544
https://github.com/scverse/scanpy/issues/544:257,usability,error,error,257,"Issue using new features in the visualizing marker genes tutorial; Hello, I am trying to use the visualize marker genes tutorial to make some plots. I am importing scanpy in the new way (import scanpy as sc) as suggested in the tutorial but I am getting an error message:. AttributeError Traceback (most recent call last). <ipython-input-5-dfc1e4d9ed06> in <module>(). ----> 1 ax = sc.pl.correlation_matrix(adata, 'cell_types'). AttributeError: module 'scanpy.plotting' has no attribute 'correlation_matrix'. Here are the versions of all the packages I am using:. scanpy==1.4 anndata==0.6.17 numpy==1.16.0 scipy==1.2.0 pandas==0.23.4 scikit-learn==0.20.2 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 . Am I missing something ? Thanks.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/544
https://github.com/scverse/scanpy/issues/544:332,usability,input,input-,332,"Issue using new features in the visualizing marker genes tutorial; Hello, I am trying to use the visualize marker genes tutorial to make some plots. I am importing scanpy in the new way (import scanpy as sc) as suggested in the tutorial but I am getting an error message:. AttributeError Traceback (most recent call last). <ipython-input-5-dfc1e4d9ed06> in <module>(). ----> 1 ax = sc.pl.correlation_matrix(adata, 'cell_types'). AttributeError: module 'scanpy.plotting' has no attribute 'correlation_matrix'. Here are the versions of all the packages I am using:. scanpy==1.4 anndata==0.6.17 numpy==1.16.0 scipy==1.2.0 pandas==0.23.4 scikit-learn==0.20.2 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 . Am I missing something ? Thanks.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/544
https://github.com/scverse/scanpy/issues/544:641,usability,learn,learn,641,"Issue using new features in the visualizing marker genes tutorial; Hello, I am trying to use the visualize marker genes tutorial to make some plots. I am importing scanpy in the new way (import scanpy as sc) as suggested in the tutorial but I am getting an error message:. AttributeError Traceback (most recent call last). <ipython-input-5-dfc1e4d9ed06> in <module>(). ----> 1 ax = sc.pl.correlation_matrix(adata, 'cell_types'). AttributeError: module 'scanpy.plotting' has no attribute 'correlation_matrix'. Here are the versions of all the packages I am using:. scanpy==1.4 anndata==0.6.17 numpy==1.16.0 scipy==1.2.0 pandas==0.23.4 scikit-learn==0.20.2 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 . Am I missing something ? Thanks.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/544
https://github.com/scverse/scanpy/issues/545:252,availability,state,states,252,"heatmap does not have return_fig parameter; Hi,. the heatmap function treats return_fig as a kwarg parameter and raises ""AttributeError: Unknown property return_fig"". Does heatmap not actually support returning the figure even though the documentation states that function has this parameter? If so, how do I go about saving my heatmaps? Thanks.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/545
https://github.com/scverse/scanpy/issues/545:0,energy efficiency,heat,heatmap,0,"heatmap does not have return_fig parameter; Hi,. the heatmap function treats return_fig as a kwarg parameter and raises ""AttributeError: Unknown property return_fig"". Does heatmap not actually support returning the figure even though the documentation states that function has this parameter? If so, how do I go about saving my heatmaps? Thanks.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/545
https://github.com/scverse/scanpy/issues/545:53,energy efficiency,heat,heatmap,53,"heatmap does not have return_fig parameter; Hi,. the heatmap function treats return_fig as a kwarg parameter and raises ""AttributeError: Unknown property return_fig"". Does heatmap not actually support returning the figure even though the documentation states that function has this parameter? If so, how do I go about saving my heatmaps? Thanks.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/545
https://github.com/scverse/scanpy/issues/545:172,energy efficiency,heat,heatmap,172,"heatmap does not have return_fig parameter; Hi,. the heatmap function treats return_fig as a kwarg parameter and raises ""AttributeError: Unknown property return_fig"". Does heatmap not actually support returning the figure even though the documentation states that function has this parameter? If so, how do I go about saving my heatmaps? Thanks.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/545
https://github.com/scverse/scanpy/issues/545:328,energy efficiency,heat,heatmaps,328,"heatmap does not have return_fig parameter; Hi,. the heatmap function treats return_fig as a kwarg parameter and raises ""AttributeError: Unknown property return_fig"". Does heatmap not actually support returning the figure even though the documentation states that function has this parameter? If so, how do I go about saving my heatmaps? Thanks.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/545
https://github.com/scverse/scanpy/issues/545:252,integrability,state,states,252,"heatmap does not have return_fig parameter; Hi,. the heatmap function treats return_fig as a kwarg parameter and raises ""AttributeError: Unknown property return_fig"". Does heatmap not actually support returning the figure even though the documentation states that function has this parameter? If so, how do I go about saving my heatmaps? Thanks.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/545
https://github.com/scverse/scanpy/issues/545:33,modifiability,paramet,parameter,33,"heatmap does not have return_fig parameter; Hi,. the heatmap function treats return_fig as a kwarg parameter and raises ""AttributeError: Unknown property return_fig"". Does heatmap not actually support returning the figure even though the documentation states that function has this parameter? If so, how do I go about saving my heatmaps? Thanks.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/545
https://github.com/scverse/scanpy/issues/545:99,modifiability,paramet,parameter,99,"heatmap does not have return_fig parameter; Hi,. the heatmap function treats return_fig as a kwarg parameter and raises ""AttributeError: Unknown property return_fig"". Does heatmap not actually support returning the figure even though the documentation states that function has this parameter? If so, how do I go about saving my heatmaps? Thanks.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/545
https://github.com/scverse/scanpy/issues/545:282,modifiability,paramet,parameter,282,"heatmap does not have return_fig parameter; Hi,. the heatmap function treats return_fig as a kwarg parameter and raises ""AttributeError: Unknown property return_fig"". Does heatmap not actually support returning the figure even though the documentation states that function has this parameter? If so, how do I go about saving my heatmaps? Thanks.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/545
https://github.com/scverse/scanpy/issues/545:8,reliability,doe,does,8,"heatmap does not have return_fig parameter; Hi,. the heatmap function treats return_fig as a kwarg parameter and raises ""AttributeError: Unknown property return_fig"". Does heatmap not actually support returning the figure even though the documentation states that function has this parameter? If so, how do I go about saving my heatmaps? Thanks.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/545
https://github.com/scverse/scanpy/issues/545:167,reliability,Doe,Does,167,"heatmap does not have return_fig parameter; Hi,. the heatmap function treats return_fig as a kwarg parameter and raises ""AttributeError: Unknown property return_fig"". Does heatmap not actually support returning the figure even though the documentation states that function has this parameter? If so, how do I go about saving my heatmaps? Thanks.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/545
https://github.com/scverse/scanpy/issues/545:193,usability,support,support,193,"heatmap does not have return_fig parameter; Hi,. the heatmap function treats return_fig as a kwarg parameter and raises ""AttributeError: Unknown property return_fig"". Does heatmap not actually support returning the figure even though the documentation states that function has this parameter? If so, how do I go about saving my heatmaps? Thanks.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/545
https://github.com/scverse/scanpy/issues/545:238,usability,document,documentation,238,"heatmap does not have return_fig parameter; Hi,. the heatmap function treats return_fig as a kwarg parameter and raises ""AttributeError: Unknown property return_fig"". Does heatmap not actually support returning the figure even though the documentation states that function has this parameter? If so, how do I go about saving my heatmaps? Thanks.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/545
https://github.com/scverse/scanpy/pull/546:7,availability,error,error,7,fix an error when figsize is given;,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/546
https://github.com/scverse/scanpy/pull/546:7,performance,error,error,7,fix an error when figsize is given;,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/546
https://github.com/scverse/scanpy/pull/546:7,safety,error,error,7,fix an error when figsize is given;,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/546
https://github.com/scverse/scanpy/pull/546:7,usability,error,error,7,fix an error when figsize is given;,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/546
https://github.com/scverse/scanpy/issues/547:950,deployability,modul,module,950,"sc.datasets.moignard15() raises ValueError; ```python. import scanpy as sc. sc.datasets.moignard15(). ```. Output. ```python. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/pandas/io/parsers.py in _cast_types(self, values, cast_type, column). 1807 values = astype_nansafe(values, cast_type,. -> 1808 copy=True, skipna=True). 1809 except ValueError:. ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/pandas/core/dtypes/cast.py in astype_nansafe(arr, dtype, copy, skipna). 701 # Explicit copy, or required since NumPy can't view from / to object. --> 702 return arr.astype(dtype, copy=True). 703 . ValueError: could not convert string to float: '4SFGA6_247'. During handling of the above exception, another exception occurred:. ValueError Traceback (most recent call last). <ipython-input-3-bf986d1f9b8c> in <module>. 1 import scanpy as sc. ----> 2 sc.datasets.moignard15(). ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/scanpy/datasets/__init__.py in moignard15(). 104 filename = 'data/moignard15/nbt.3154-S3.xlsx'. 105 backup_url = 'http://www.nature.com/nbt/journal/v33/n3/extref/nbt.3154-S3.xlsx'. --> 106 adata = sc.read(filename, sheet='dCt_values.txt', cache=True, backup_url=backup_url). 107 # filter out 4 genes as in Haghverdi et al. (2016). 108 gene_subset = ~np.in1d(adata.var_names, ['Eif2b1', 'Mrpl19', 'Polr2a', 'Ubc']). ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/scanpy/readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs). 76 return _read(filename, backed=backed, sheet=sheet, ext=ext,. 77 delimiter=delimiter, first_column_names=first_column_names,. ---> 78 backup_url=backup_url, cache=cache, **kwargs). 79 # generate filename and read to dict. 80 filekey = filename. ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/scanpy/readwrite.py in _read(",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/547
https://github.com/scverse/scanpy/issues/547:6046,deployability,log,logging,6046,"dtype, true_values, false_values, skiprows, nrows, na_values, verbose, parse_dates, date_parser, thousands, comment, skipfooter, convert_float, mangle_dupe_cols, **kwds). 601 **kwds). 602 . --> 603 output[asheetname] = parser.read(nrows=nrows). 604 . 605 if not squeeze or isinstance(output[asheetname], DataFrame):. ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/pandas/io/parsers.py in read(self, nrows). 1137 def read(self, nrows=None):. 1138 nrows = _validate_integer('nrows', nrows). -> 1139 ret = self._engine.read(nrows). 1140 . 1141 # May alter columns / col_dict. ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/pandas/io/parsers.py in read(self, rows). 2419 columns, data = self._do_date_conversions(columns, data). 2420 . -> 2421 data = self._convert_data(data). 2422 index, columns = self._make_index(data, alldata, columns, indexnamerow). 2423 . ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/pandas/io/parsers.py in _convert_data(self, data). 2485 return self._convert_to_ndarrays(data, clean_na_values,. 2486 clean_na_fvalues, self.verbose,. -> 2487 clean_conv, clean_dtypes). 2488 . 2489 def _infer_columns(self):. ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/pandas/io/parsers.py in _convert_to_ndarrays(self, dct, na_values, na_fvalues, verbose, converters, dtypes). 1703 # invalid input to is_bool_dtype. 1704 pass. -> 1705 cvals = self._cast_types(cvals, cast_type, c). 1706 . 1707 result[c] = cvals. ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/pandas/io/parsers.py in _cast_types(self, values, cast_type, column). 1809 except ValueError:. 1810 raise ValueError(""Unable to convert column %s to "". -> 1811 ""type %s"" % (column, cast_type)). 1812 return values. 1813 . ValueError: Unable to convert column Cell to type float32. ```. ```python. sc.logging.print_versions(). scanpy==1.4 anndata==0.6.18 numpy==1.15.4 scipy==1.2.0 pandas==0.24.2 scikit-learn==0.20.2 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 . ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/547
https://github.com/scverse/scanpy/issues/547:549,energy efficiency,core,core,549,"sc.datasets.moignard15() raises ValueError; ```python. import scanpy as sc. sc.datasets.moignard15(). ```. Output. ```python. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/pandas/io/parsers.py in _cast_types(self, values, cast_type, column). 1807 values = astype_nansafe(values, cast_type,. -> 1808 copy=True, skipna=True). 1809 except ValueError:. ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/pandas/core/dtypes/cast.py in astype_nansafe(arr, dtype, copy, skipna). 701 # Explicit copy, or required since NumPy can't view from / to object. --> 702 return arr.astype(dtype, copy=True). 703 . ValueError: could not convert string to float: '4SFGA6_247'. During handling of the above exception, another exception occurred:. ValueError Traceback (most recent call last). <ipython-input-3-bf986d1f9b8c> in <module>. 1 import scanpy as sc. ----> 2 sc.datasets.moignard15(). ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/scanpy/datasets/__init__.py in moignard15(). 104 filename = 'data/moignard15/nbt.3154-S3.xlsx'. 105 backup_url = 'http://www.nature.com/nbt/journal/v33/n3/extref/nbt.3154-S3.xlsx'. --> 106 adata = sc.read(filename, sheet='dCt_values.txt', cache=True, backup_url=backup_url). 107 # filter out 4 genes as in Haghverdi et al. (2016). 108 gene_subset = ~np.in1d(adata.var_names, ['Eif2b1', 'Mrpl19', 'Polr2a', 'Ubc']). ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/scanpy/readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs). 76 return _read(filename, backed=backed, sheet=sheet, ext=ext,. 77 delimiter=delimiter, first_column_names=first_column_names,. ---> 78 backup_url=backup_url, cache=cache, **kwargs). 79 # generate filename and read to dict. 80 filekey = filename. ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/scanpy/readwrite.py in _read(",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/547
https://github.com/scverse/scanpy/issues/547:1355,integrability,filter,filter,1355,"ast_type, column). 1807 values = astype_nansafe(values, cast_type,. -> 1808 copy=True, skipna=True). 1809 except ValueError:. ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/pandas/core/dtypes/cast.py in astype_nansafe(arr, dtype, copy, skipna). 701 # Explicit copy, or required since NumPy can't view from / to object. --> 702 return arr.astype(dtype, copy=True). 703 . ValueError: could not convert string to float: '4SFGA6_247'. During handling of the above exception, another exception occurred:. ValueError Traceback (most recent call last). <ipython-input-3-bf986d1f9b8c> in <module>. 1 import scanpy as sc. ----> 2 sc.datasets.moignard15(). ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/scanpy/datasets/__init__.py in moignard15(). 104 filename = 'data/moignard15/nbt.3154-S3.xlsx'. 105 backup_url = 'http://www.nature.com/nbt/journal/v33/n3/extref/nbt.3154-S3.xlsx'. --> 106 adata = sc.read(filename, sheet='dCt_values.txt', cache=True, backup_url=backup_url). 107 # filter out 4 genes as in Haghverdi et al. (2016). 108 gene_subset = ~np.in1d(adata.var_names, ['Eif2b1', 'Mrpl19', 'Polr2a', 'Ubc']). ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/scanpy/readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs). 76 return _read(filename, backed=backed, sheet=sheet, ext=ext,. 77 delimiter=delimiter, first_column_names=first_column_names,. ---> 78 backup_url=backup_url, cache=cache, **kwargs). 79 # generate filename and read to dict. 80 filekey = filename. ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/scanpy/readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs). 458 'Provide `sheet` parameter when reading \'.xlsx\' files.'). 459 else:. --> 460 adata = read_excel(filename, sheet). 461 elif ext in {'mtx', 'mtx.gz'}:. 462 adata = read_mtx(filename). ~/miniconda3/envs/spols190117/lib/python3.6/site-packag",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/547
https://github.com/scverse/scanpy/issues/547:2741,integrability,wrap,wrapper,2741,"limiter, first_column_names=first_column_names,. ---> 78 backup_url=backup_url, cache=cache, **kwargs). 79 # generate filename and read to dict. 80 filekey = filename. ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/scanpy/readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs). 458 'Provide `sheet` parameter when reading \'.xlsx\' files.'). 459 else:. --> 460 adata = read_excel(filename, sheet). 461 elif ext in {'mtx', 'mtx.gz'}:. 462 adata = read_mtx(filename). ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/anndata/readwrite/read.py in read_excel(filename, sheet, dtype). 59 # rely on pandas for reading an excel file. 60 from pandas import read_excel. ---> 61 df = read_excel(fspath(filename), sheet, dtype=dtype). 62 X = df.values[:, 1:]. 63 row = {'row_names': df.iloc[:, 0].values.astype(str)}. ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/pandas/util/_decorators.py in wrapper(*args, **kwargs). 186 else:. 187 kwargs[new_arg_name] = new_arg_value. --> 188 return func(*args, **kwargs). 189 return wrapper. 190 return _deprecate_kwarg. ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/pandas/util/_decorators.py in wrapper(*args, **kwargs). 186 else:. 187 kwargs[new_arg_name] = new_arg_value. --> 188 return func(*args, **kwargs). 189 return wrapper. 190 return _deprecate_kwarg. ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/pandas/io/excel.py in read_excel(io, sheet_name, header, names, index_col, parse_cols, usecols, squeeze, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, verbose, parse_dates, date_parser, thousands, comment, skip_footer, skipfooter, convert_float, mangle_dupe_cols, **kwds). 373 convert_float=convert_float,. 374 mangle_dupe_cols=mangle_dupe_cols,. --> 375 **kwds). 376 . 377 . ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/pandas/io/excel.py in parse(self, sheet",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/547
https://github.com/scverse/scanpy/issues/547:2869,integrability,wrap,wrapper,2869,"nd read to dict. 80 filekey = filename. ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/scanpy/readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs). 458 'Provide `sheet` parameter when reading \'.xlsx\' files.'). 459 else:. --> 460 adata = read_excel(filename, sheet). 461 elif ext in {'mtx', 'mtx.gz'}:. 462 adata = read_mtx(filename). ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/anndata/readwrite/read.py in read_excel(filename, sheet, dtype). 59 # rely on pandas for reading an excel file. 60 from pandas import read_excel. ---> 61 df = read_excel(fspath(filename), sheet, dtype=dtype). 62 X = df.values[:, 1:]. 63 row = {'row_names': df.iloc[:, 0].values.astype(str)}. ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/pandas/util/_decorators.py in wrapper(*args, **kwargs). 186 else:. 187 kwargs[new_arg_name] = new_arg_value. --> 188 return func(*args, **kwargs). 189 return wrapper. 190 return _deprecate_kwarg. ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/pandas/util/_decorators.py in wrapper(*args, **kwargs). 186 else:. 187 kwargs[new_arg_name] = new_arg_value. --> 188 return func(*args, **kwargs). 189 return wrapper. 190 return _deprecate_kwarg. ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/pandas/io/excel.py in read_excel(io, sheet_name, header, names, index_col, parse_cols, usecols, squeeze, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, verbose, parse_dates, date_parser, thousands, comment, skip_footer, skipfooter, convert_float, mangle_dupe_cols, **kwds). 373 convert_float=convert_float,. 374 mangle_dupe_cols=mangle_dupe_cols,. --> 375 **kwds). 376 . 377 . ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/pandas/io/excel.py in parse(self, sheet_name, header, names, index_col, usecols, squeeze, converters, true_values, false_values, skiprows, nrows, na_values, parse_date",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/547
https://github.com/scverse/scanpy/issues/547:2995,integrability,wrap,wrapper,2995,"(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs). 458 'Provide `sheet` parameter when reading \'.xlsx\' files.'). 459 else:. --> 460 adata = read_excel(filename, sheet). 461 elif ext in {'mtx', 'mtx.gz'}:. 462 adata = read_mtx(filename). ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/anndata/readwrite/read.py in read_excel(filename, sheet, dtype). 59 # rely on pandas for reading an excel file. 60 from pandas import read_excel. ---> 61 df = read_excel(fspath(filename), sheet, dtype=dtype). 62 X = df.values[:, 1:]. 63 row = {'row_names': df.iloc[:, 0].values.astype(str)}. ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/pandas/util/_decorators.py in wrapper(*args, **kwargs). 186 else:. 187 kwargs[new_arg_name] = new_arg_value. --> 188 return func(*args, **kwargs). 189 return wrapper. 190 return _deprecate_kwarg. ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/pandas/util/_decorators.py in wrapper(*args, **kwargs). 186 else:. 187 kwargs[new_arg_name] = new_arg_value. --> 188 return func(*args, **kwargs). 189 return wrapper. 190 return _deprecate_kwarg. ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/pandas/io/excel.py in read_excel(io, sheet_name, header, names, index_col, parse_cols, usecols, squeeze, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, verbose, parse_dates, date_parser, thousands, comment, skip_footer, skipfooter, convert_float, mangle_dupe_cols, **kwds). 373 convert_float=convert_float,. 374 mangle_dupe_cols=mangle_dupe_cols,. --> 375 **kwds). 376 . 377 . ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/pandas/io/excel.py in parse(self, sheet_name, header, names, index_col, usecols, squeeze, converters, true_values, false_values, skiprows, nrows, na_values, parse_dates, date_parser, thousands, comment, skipfooter, convert_float, mangle_dupe_cols, **kwds). 716 convert_float=convert_float,. 71",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/547
https://github.com/scverse/scanpy/issues/547:3123,integrability,wrap,wrapper,3123," `sheet` parameter when reading \'.xlsx\' files.'). 459 else:. --> 460 adata = read_excel(filename, sheet). 461 elif ext in {'mtx', 'mtx.gz'}:. 462 adata = read_mtx(filename). ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/anndata/readwrite/read.py in read_excel(filename, sheet, dtype). 59 # rely on pandas for reading an excel file. 60 from pandas import read_excel. ---> 61 df = read_excel(fspath(filename), sheet, dtype=dtype). 62 X = df.values[:, 1:]. 63 row = {'row_names': df.iloc[:, 0].values.astype(str)}. ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/pandas/util/_decorators.py in wrapper(*args, **kwargs). 186 else:. 187 kwargs[new_arg_name] = new_arg_value. --> 188 return func(*args, **kwargs). 189 return wrapper. 190 return _deprecate_kwarg. ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/pandas/util/_decorators.py in wrapper(*args, **kwargs). 186 else:. 187 kwargs[new_arg_name] = new_arg_value. --> 188 return func(*args, **kwargs). 189 return wrapper. 190 return _deprecate_kwarg. ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/pandas/io/excel.py in read_excel(io, sheet_name, header, names, index_col, parse_cols, usecols, squeeze, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, verbose, parse_dates, date_parser, thousands, comment, skip_footer, skipfooter, convert_float, mangle_dupe_cols, **kwds). 373 convert_float=convert_float,. 374 mangle_dupe_cols=mangle_dupe_cols,. --> 375 **kwds). 376 . 377 . ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/pandas/io/excel.py in parse(self, sheet_name, header, names, index_col, usecols, squeeze, converters, true_values, false_values, skiprows, nrows, na_values, parse_dates, date_parser, thousands, comment, skipfooter, convert_float, mangle_dupe_cols, **kwds). 716 convert_float=convert_float,. 717 mangle_dupe_cols=mangle_dupe_cols,. --> 718 **kwds). 719 . 720 @property. ~/miniconda3/envs/spols190117/lib/python3.6/site-pac",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/547
https://github.com/scverse/scanpy/issues/547:2741,interoperability,wrapper,wrapper,2741,"limiter, first_column_names=first_column_names,. ---> 78 backup_url=backup_url, cache=cache, **kwargs). 79 # generate filename and read to dict. 80 filekey = filename. ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/scanpy/readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs). 458 'Provide `sheet` parameter when reading \'.xlsx\' files.'). 459 else:. --> 460 adata = read_excel(filename, sheet). 461 elif ext in {'mtx', 'mtx.gz'}:. 462 adata = read_mtx(filename). ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/anndata/readwrite/read.py in read_excel(filename, sheet, dtype). 59 # rely on pandas for reading an excel file. 60 from pandas import read_excel. ---> 61 df = read_excel(fspath(filename), sheet, dtype=dtype). 62 X = df.values[:, 1:]. 63 row = {'row_names': df.iloc[:, 0].values.astype(str)}. ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/pandas/util/_decorators.py in wrapper(*args, **kwargs). 186 else:. 187 kwargs[new_arg_name] = new_arg_value. --> 188 return func(*args, **kwargs). 189 return wrapper. 190 return _deprecate_kwarg. ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/pandas/util/_decorators.py in wrapper(*args, **kwargs). 186 else:. 187 kwargs[new_arg_name] = new_arg_value. --> 188 return func(*args, **kwargs). 189 return wrapper. 190 return _deprecate_kwarg. ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/pandas/io/excel.py in read_excel(io, sheet_name, header, names, index_col, parse_cols, usecols, squeeze, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, verbose, parse_dates, date_parser, thousands, comment, skip_footer, skipfooter, convert_float, mangle_dupe_cols, **kwds). 373 convert_float=convert_float,. 374 mangle_dupe_cols=mangle_dupe_cols,. --> 375 **kwds). 376 . 377 . ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/pandas/io/excel.py in parse(self, sheet",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/547
https://github.com/scverse/scanpy/issues/547:2869,interoperability,wrapper,wrapper,2869,"nd read to dict. 80 filekey = filename. ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/scanpy/readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs). 458 'Provide `sheet` parameter when reading \'.xlsx\' files.'). 459 else:. --> 460 adata = read_excel(filename, sheet). 461 elif ext in {'mtx', 'mtx.gz'}:. 462 adata = read_mtx(filename). ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/anndata/readwrite/read.py in read_excel(filename, sheet, dtype). 59 # rely on pandas for reading an excel file. 60 from pandas import read_excel. ---> 61 df = read_excel(fspath(filename), sheet, dtype=dtype). 62 X = df.values[:, 1:]. 63 row = {'row_names': df.iloc[:, 0].values.astype(str)}. ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/pandas/util/_decorators.py in wrapper(*args, **kwargs). 186 else:. 187 kwargs[new_arg_name] = new_arg_value. --> 188 return func(*args, **kwargs). 189 return wrapper. 190 return _deprecate_kwarg. ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/pandas/util/_decorators.py in wrapper(*args, **kwargs). 186 else:. 187 kwargs[new_arg_name] = new_arg_value. --> 188 return func(*args, **kwargs). 189 return wrapper. 190 return _deprecate_kwarg. ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/pandas/io/excel.py in read_excel(io, sheet_name, header, names, index_col, parse_cols, usecols, squeeze, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, verbose, parse_dates, date_parser, thousands, comment, skip_footer, skipfooter, convert_float, mangle_dupe_cols, **kwds). 373 convert_float=convert_float,. 374 mangle_dupe_cols=mangle_dupe_cols,. --> 375 **kwds). 376 . 377 . ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/pandas/io/excel.py in parse(self, sheet_name, header, names, index_col, usecols, squeeze, converters, true_values, false_values, skiprows, nrows, na_values, parse_date",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/547
https://github.com/scverse/scanpy/issues/547:2995,interoperability,wrapper,wrapper,2995,"(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs). 458 'Provide `sheet` parameter when reading \'.xlsx\' files.'). 459 else:. --> 460 adata = read_excel(filename, sheet). 461 elif ext in {'mtx', 'mtx.gz'}:. 462 adata = read_mtx(filename). ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/anndata/readwrite/read.py in read_excel(filename, sheet, dtype). 59 # rely on pandas for reading an excel file. 60 from pandas import read_excel. ---> 61 df = read_excel(fspath(filename), sheet, dtype=dtype). 62 X = df.values[:, 1:]. 63 row = {'row_names': df.iloc[:, 0].values.astype(str)}. ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/pandas/util/_decorators.py in wrapper(*args, **kwargs). 186 else:. 187 kwargs[new_arg_name] = new_arg_value. --> 188 return func(*args, **kwargs). 189 return wrapper. 190 return _deprecate_kwarg. ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/pandas/util/_decorators.py in wrapper(*args, **kwargs). 186 else:. 187 kwargs[new_arg_name] = new_arg_value. --> 188 return func(*args, **kwargs). 189 return wrapper. 190 return _deprecate_kwarg. ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/pandas/io/excel.py in read_excel(io, sheet_name, header, names, index_col, parse_cols, usecols, squeeze, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, verbose, parse_dates, date_parser, thousands, comment, skip_footer, skipfooter, convert_float, mangle_dupe_cols, **kwds). 373 convert_float=convert_float,. 374 mangle_dupe_cols=mangle_dupe_cols,. --> 375 **kwds). 376 . 377 . ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/pandas/io/excel.py in parse(self, sheet_name, header, names, index_col, usecols, squeeze, converters, true_values, false_values, skiprows, nrows, na_values, parse_dates, date_parser, thousands, comment, skipfooter, convert_float, mangle_dupe_cols, **kwds). 716 convert_float=convert_float,. 71",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/547
https://github.com/scverse/scanpy/issues/547:3123,interoperability,wrapper,wrapper,3123," `sheet` parameter when reading \'.xlsx\' files.'). 459 else:. --> 460 adata = read_excel(filename, sheet). 461 elif ext in {'mtx', 'mtx.gz'}:. 462 adata = read_mtx(filename). ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/anndata/readwrite/read.py in read_excel(filename, sheet, dtype). 59 # rely on pandas for reading an excel file. 60 from pandas import read_excel. ---> 61 df = read_excel(fspath(filename), sheet, dtype=dtype). 62 X = df.values[:, 1:]. 63 row = {'row_names': df.iloc[:, 0].values.astype(str)}. ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/pandas/util/_decorators.py in wrapper(*args, **kwargs). 186 else:. 187 kwargs[new_arg_name] = new_arg_value. --> 188 return func(*args, **kwargs). 189 return wrapper. 190 return _deprecate_kwarg. ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/pandas/util/_decorators.py in wrapper(*args, **kwargs). 186 else:. 187 kwargs[new_arg_name] = new_arg_value. --> 188 return func(*args, **kwargs). 189 return wrapper. 190 return _deprecate_kwarg. ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/pandas/io/excel.py in read_excel(io, sheet_name, header, names, index_col, parse_cols, usecols, squeeze, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, verbose, parse_dates, date_parser, thousands, comment, skip_footer, skipfooter, convert_float, mangle_dupe_cols, **kwds). 373 convert_float=convert_float,. 374 mangle_dupe_cols=mangle_dupe_cols,. --> 375 **kwds). 376 . 377 . ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/pandas/io/excel.py in parse(self, sheet_name, header, names, index_col, usecols, squeeze, converters, true_values, false_values, skiprows, nrows, na_values, parse_dates, date_parser, thousands, comment, skipfooter, convert_float, mangle_dupe_cols, **kwds). 716 convert_float=convert_float,. 717 mangle_dupe_cols=mangle_dupe_cols,. --> 718 **kwds). 719 . 720 @property. ~/miniconda3/envs/spols190117/lib/python3.6/site-pac",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/547
https://github.com/scverse/scanpy/issues/547:298,modifiability,pac,packages,298,"sc.datasets.moignard15() raises ValueError; ```python. import scanpy as sc. sc.datasets.moignard15(). ```. Output. ```python. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/pandas/io/parsers.py in _cast_types(self, values, cast_type, column). 1807 values = astype_nansafe(values, cast_type,. -> 1808 copy=True, skipna=True). 1809 except ValueError:. ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/pandas/core/dtypes/cast.py in astype_nansafe(arr, dtype, copy, skipna). 701 # Explicit copy, or required since NumPy can't view from / to object. --> 702 return arr.astype(dtype, copy=True). 703 . ValueError: could not convert string to float: '4SFGA6_247'. During handling of the above exception, another exception occurred:. ValueError Traceback (most recent call last). <ipython-input-3-bf986d1f9b8c> in <module>. 1 import scanpy as sc. ----> 2 sc.datasets.moignard15(). ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/scanpy/datasets/__init__.py in moignard15(). 104 filename = 'data/moignard15/nbt.3154-S3.xlsx'. 105 backup_url = 'http://www.nature.com/nbt/journal/v33/n3/extref/nbt.3154-S3.xlsx'. --> 106 adata = sc.read(filename, sheet='dCt_values.txt', cache=True, backup_url=backup_url). 107 # filter out 4 genes as in Haghverdi et al. (2016). 108 gene_subset = ~np.in1d(adata.var_names, ['Eif2b1', 'Mrpl19', 'Polr2a', 'Ubc']). ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/scanpy/readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs). 76 return _read(filename, backed=backed, sheet=sheet, ext=ext,. 77 delimiter=delimiter, first_column_names=first_column_names,. ---> 78 backup_url=backup_url, cache=cache, **kwargs). 79 # generate filename and read to dict. 80 filekey = filename. ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/scanpy/readwrite.py in _read(",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/547
https://github.com/scverse/scanpy/issues/547:533,modifiability,pac,packages,533,"sc.datasets.moignard15() raises ValueError; ```python. import scanpy as sc. sc.datasets.moignard15(). ```. Output. ```python. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/pandas/io/parsers.py in _cast_types(self, values, cast_type, column). 1807 values = astype_nansafe(values, cast_type,. -> 1808 copy=True, skipna=True). 1809 except ValueError:. ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/pandas/core/dtypes/cast.py in astype_nansafe(arr, dtype, copy, skipna). 701 # Explicit copy, or required since NumPy can't view from / to object. --> 702 return arr.astype(dtype, copy=True). 703 . ValueError: could not convert string to float: '4SFGA6_247'. During handling of the above exception, another exception occurred:. ValueError Traceback (most recent call last). <ipython-input-3-bf986d1f9b8c> in <module>. 1 import scanpy as sc. ----> 2 sc.datasets.moignard15(). ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/scanpy/datasets/__init__.py in moignard15(). 104 filename = 'data/moignard15/nbt.3154-S3.xlsx'. 105 backup_url = 'http://www.nature.com/nbt/journal/v33/n3/extref/nbt.3154-S3.xlsx'. --> 106 adata = sc.read(filename, sheet='dCt_values.txt', cache=True, backup_url=backup_url). 107 # filter out 4 genes as in Haghverdi et al. (2016). 108 gene_subset = ~np.in1d(adata.var_names, ['Eif2b1', 'Mrpl19', 'Polr2a', 'Ubc']). ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/scanpy/readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs). 76 return _read(filename, backed=backed, sheet=sheet, ext=ext,. 77 delimiter=delimiter, first_column_names=first_column_names,. ---> 78 backup_url=backup_url, cache=cache, **kwargs). 79 # generate filename and read to dict. 80 filekey = filename. ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/scanpy/readwrite.py in _read(",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/547
https://github.com/scverse/scanpy/issues/547:950,modifiability,modul,module,950,"sc.datasets.moignard15() raises ValueError; ```python. import scanpy as sc. sc.datasets.moignard15(). ```. Output. ```python. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/pandas/io/parsers.py in _cast_types(self, values, cast_type, column). 1807 values = astype_nansafe(values, cast_type,. -> 1808 copy=True, skipna=True). 1809 except ValueError:. ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/pandas/core/dtypes/cast.py in astype_nansafe(arr, dtype, copy, skipna). 701 # Explicit copy, or required since NumPy can't view from / to object. --> 702 return arr.astype(dtype, copy=True). 703 . ValueError: could not convert string to float: '4SFGA6_247'. During handling of the above exception, another exception occurred:. ValueError Traceback (most recent call last). <ipython-input-3-bf986d1f9b8c> in <module>. 1 import scanpy as sc. ----> 2 sc.datasets.moignard15(). ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/scanpy/datasets/__init__.py in moignard15(). 104 filename = 'data/moignard15/nbt.3154-S3.xlsx'. 105 backup_url = 'http://www.nature.com/nbt/journal/v33/n3/extref/nbt.3154-S3.xlsx'. --> 106 adata = sc.read(filename, sheet='dCt_values.txt', cache=True, backup_url=backup_url). 107 # filter out 4 genes as in Haghverdi et al. (2016). 108 gene_subset = ~np.in1d(adata.var_names, ['Eif2b1', 'Mrpl19', 'Polr2a', 'Ubc']). ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/scanpy/readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs). 76 return _read(filename, backed=backed, sheet=sheet, ext=ext,. 77 delimiter=delimiter, first_column_names=first_column_names,. ---> 78 backup_url=backup_url, cache=cache, **kwargs). 79 # generate filename and read to dict. 80 filekey = filename. ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/scanpy/readwrite.py in _read(",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/547
https://github.com/scverse/scanpy/issues/547:1065,modifiability,pac,packages,1065,"as sc. sc.datasets.moignard15(). ```. Output. ```python. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/pandas/io/parsers.py in _cast_types(self, values, cast_type, column). 1807 values = astype_nansafe(values, cast_type,. -> 1808 copy=True, skipna=True). 1809 except ValueError:. ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/pandas/core/dtypes/cast.py in astype_nansafe(arr, dtype, copy, skipna). 701 # Explicit copy, or required since NumPy can't view from / to object. --> 702 return arr.astype(dtype, copy=True). 703 . ValueError: could not convert string to float: '4SFGA6_247'. During handling of the above exception, another exception occurred:. ValueError Traceback (most recent call last). <ipython-input-3-bf986d1f9b8c> in <module>. 1 import scanpy as sc. ----> 2 sc.datasets.moignard15(). ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/scanpy/datasets/__init__.py in moignard15(). 104 filename = 'data/moignard15/nbt.3154-S3.xlsx'. 105 backup_url = 'http://www.nature.com/nbt/journal/v33/n3/extref/nbt.3154-S3.xlsx'. --> 106 adata = sc.read(filename, sheet='dCt_values.txt', cache=True, backup_url=backup_url). 107 # filter out 4 genes as in Haghverdi et al. (2016). 108 gene_subset = ~np.in1d(adata.var_names, ['Eif2b1', 'Mrpl19', 'Polr2a', 'Ubc']). ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/scanpy/readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs). 76 return _read(filename, backed=backed, sheet=sheet, ext=ext,. 77 delimiter=delimiter, first_column_names=first_column_names,. ---> 78 backup_url=backup_url, cache=cache, **kwargs). 79 # generate filename and read to dict. 80 filekey = filename. ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/scanpy/readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_u",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/547
https://github.com/scverse/scanpy/issues/547:1538,modifiability,pac,packages,1538,"pandas/core/dtypes/cast.py in astype_nansafe(arr, dtype, copy, skipna). 701 # Explicit copy, or required since NumPy can't view from / to object. --> 702 return arr.astype(dtype, copy=True). 703 . ValueError: could not convert string to float: '4SFGA6_247'. During handling of the above exception, another exception occurred:. ValueError Traceback (most recent call last). <ipython-input-3-bf986d1f9b8c> in <module>. 1 import scanpy as sc. ----> 2 sc.datasets.moignard15(). ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/scanpy/datasets/__init__.py in moignard15(). 104 filename = 'data/moignard15/nbt.3154-S3.xlsx'. 105 backup_url = 'http://www.nature.com/nbt/journal/v33/n3/extref/nbt.3154-S3.xlsx'. --> 106 adata = sc.read(filename, sheet='dCt_values.txt', cache=True, backup_url=backup_url). 107 # filter out 4 genes as in Haghverdi et al. (2016). 108 gene_subset = ~np.in1d(adata.var_names, ['Eif2b1', 'Mrpl19', 'Polr2a', 'Ubc']). ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/scanpy/readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs). 76 return _read(filename, backed=backed, sheet=sheet, ext=ext,. 77 delimiter=delimiter, first_column_names=first_column_names,. ---> 78 backup_url=backup_url, cache=cache, **kwargs). 79 # generate filename and read to dict. 80 filekey = filename. ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/scanpy/readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs). 458 'Provide `sheet` parameter when reading \'.xlsx\' files.'). 459 else:. --> 460 adata = read_excel(filename, sheet). 461 elif ext in {'mtx', 'mtx.gz'}:. 462 adata = read_mtx(filename). ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/anndata/readwrite/read.py in read_excel(filename, sheet, dtype). 59 # rely on pandas for reading an excel file. 60 from pandas import read_excel. ---> 61 df = read_excel(fspath(file",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/547
https://github.com/scverse/scanpy/issues/547:1962,modifiability,pac,packages,1962,"t scanpy as sc. ----> 2 sc.datasets.moignard15(). ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/scanpy/datasets/__init__.py in moignard15(). 104 filename = 'data/moignard15/nbt.3154-S3.xlsx'. 105 backup_url = 'http://www.nature.com/nbt/journal/v33/n3/extref/nbt.3154-S3.xlsx'. --> 106 adata = sc.read(filename, sheet='dCt_values.txt', cache=True, backup_url=backup_url). 107 # filter out 4 genes as in Haghverdi et al. (2016). 108 gene_subset = ~np.in1d(adata.var_names, ['Eif2b1', 'Mrpl19', 'Polr2a', 'Ubc']). ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/scanpy/readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs). 76 return _read(filename, backed=backed, sheet=sheet, ext=ext,. 77 delimiter=delimiter, first_column_names=first_column_names,. ---> 78 backup_url=backup_url, cache=cache, **kwargs). 79 # generate filename and read to dict. 80 filekey = filename. ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/scanpy/readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs). 458 'Provide `sheet` parameter when reading \'.xlsx\' files.'). 459 else:. --> 460 adata = read_excel(filename, sheet). 461 elif ext in {'mtx', 'mtx.gz'}:. 462 adata = read_mtx(filename). ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/anndata/readwrite/read.py in read_excel(filename, sheet, dtype). 59 # rely on pandas for reading an excel file. 60 from pandas import read_excel. ---> 61 df = read_excel(fspath(filename), sheet, dtype=dtype). 62 X = df.values[:, 1:]. 63 row = {'row_names': df.iloc[:, 0].values.astype(str)}. ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/pandas/util/_decorators.py in wrapper(*args, **kwargs). 186 else:. 187 kwargs[new_arg_name] = new_arg_value. --> 188 return func(*args, **kwargs). 189 return wrapper. 190 return _deprecate_kwarg. ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/p",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/547
https://github.com/scverse/scanpy/issues/547:2136,modifiability,paramet,parameter,2136,"oignard15/nbt.3154-S3.xlsx'. 105 backup_url = 'http://www.nature.com/nbt/journal/v33/n3/extref/nbt.3154-S3.xlsx'. --> 106 adata = sc.read(filename, sheet='dCt_values.txt', cache=True, backup_url=backup_url). 107 # filter out 4 genes as in Haghverdi et al. (2016). 108 gene_subset = ~np.in1d(adata.var_names, ['Eif2b1', 'Mrpl19', 'Polr2a', 'Ubc']). ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/scanpy/readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs). 76 return _read(filename, backed=backed, sheet=sheet, ext=ext,. 77 delimiter=delimiter, first_column_names=first_column_names,. ---> 78 backup_url=backup_url, cache=cache, **kwargs). 79 # generate filename and read to dict. 80 filekey = filename. ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/scanpy/readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs). 458 'Provide `sheet` parameter when reading \'.xlsx\' files.'). 459 else:. --> 460 adata = read_excel(filename, sheet). 461 elif ext in {'mtx', 'mtx.gz'}:. 462 adata = read_mtx(filename). ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/anndata/readwrite/read.py in read_excel(filename, sheet, dtype). 59 # rely on pandas for reading an excel file. 60 from pandas import read_excel. ---> 61 df = read_excel(fspath(filename), sheet, dtype=dtype). 62 X = df.values[:, 1:]. 63 row = {'row_names': df.iloc[:, 0].values.astype(str)}. ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/pandas/util/_decorators.py in wrapper(*args, **kwargs). 186 else:. 187 kwargs[new_arg_name] = new_arg_value. --> 188 return func(*args, **kwargs). 189 return wrapper. 190 return _deprecate_kwarg. ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/pandas/util/_decorators.py in wrapper(*args, **kwargs). 186 else:. 187 kwargs[new_arg_name] = new_arg_value. --> 188 return func(*args, **kwargs). 189 return wrapper. 190 retur",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/547
https://github.com/scverse/scanpy/issues/547:2352,modifiability,pac,packages,2352,"ilter out 4 genes as in Haghverdi et al. (2016). 108 gene_subset = ~np.in1d(adata.var_names, ['Eif2b1', 'Mrpl19', 'Polr2a', 'Ubc']). ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/scanpy/readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs). 76 return _read(filename, backed=backed, sheet=sheet, ext=ext,. 77 delimiter=delimiter, first_column_names=first_column_names,. ---> 78 backup_url=backup_url, cache=cache, **kwargs). 79 # generate filename and read to dict. 80 filekey = filename. ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/scanpy/readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs). 458 'Provide `sheet` parameter when reading \'.xlsx\' files.'). 459 else:. --> 460 adata = read_excel(filename, sheet). 461 elif ext in {'mtx', 'mtx.gz'}:. 462 adata = read_mtx(filename). ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/anndata/readwrite/read.py in read_excel(filename, sheet, dtype). 59 # rely on pandas for reading an excel file. 60 from pandas import read_excel. ---> 61 df = read_excel(fspath(filename), sheet, dtype=dtype). 62 X = df.values[:, 1:]. 63 row = {'row_names': df.iloc[:, 0].values.astype(str)}. ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/pandas/util/_decorators.py in wrapper(*args, **kwargs). 186 else:. 187 kwargs[new_arg_name] = new_arg_value. --> 188 return func(*args, **kwargs). 189 return wrapper. 190 return _deprecate_kwarg. ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/pandas/util/_decorators.py in wrapper(*args, **kwargs). 186 else:. 187 kwargs[new_arg_name] = new_arg_value. --> 188 return func(*args, **kwargs). 189 return wrapper. 190 return _deprecate_kwarg. ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/pandas/io/excel.py in read_excel(io, sheet_name, header, names, index_col, parse_cols, usecols, squeeze, dtype, engine, converters, true_",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/547
https://github.com/scverse/scanpy/issues/547:2702,modifiability,pac,packages,2702," sheet=sheet, ext=ext,. 77 delimiter=delimiter, first_column_names=first_column_names,. ---> 78 backup_url=backup_url, cache=cache, **kwargs). 79 # generate filename and read to dict. 80 filekey = filename. ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/scanpy/readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs). 458 'Provide `sheet` parameter when reading \'.xlsx\' files.'). 459 else:. --> 460 adata = read_excel(filename, sheet). 461 elif ext in {'mtx', 'mtx.gz'}:. 462 adata = read_mtx(filename). ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/anndata/readwrite/read.py in read_excel(filename, sheet, dtype). 59 # rely on pandas for reading an excel file. 60 from pandas import read_excel. ---> 61 df = read_excel(fspath(filename), sheet, dtype=dtype). 62 X = df.values[:, 1:]. 63 row = {'row_names': df.iloc[:, 0].values.astype(str)}. ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/pandas/util/_decorators.py in wrapper(*args, **kwargs). 186 else:. 187 kwargs[new_arg_name] = new_arg_value. --> 188 return func(*args, **kwargs). 189 return wrapper. 190 return _deprecate_kwarg. ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/pandas/util/_decorators.py in wrapper(*args, **kwargs). 186 else:. 187 kwargs[new_arg_name] = new_arg_value. --> 188 return func(*args, **kwargs). 189 return wrapper. 190 return _deprecate_kwarg. ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/pandas/io/excel.py in read_excel(io, sheet_name, header, names, index_col, parse_cols, usecols, squeeze, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, verbose, parse_dates, date_parser, thousands, comment, skip_footer, skipfooter, convert_float, mangle_dupe_cols, **kwds). 373 convert_float=convert_float,. 374 mangle_dupe_cols=mangle_dupe_cols,. --> 375 **kwds). 376 . 377 . ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/547
https://github.com/scverse/scanpy/issues/547:2956,modifiability,pac,packages,2956,"e-packages/scanpy/readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs). 458 'Provide `sheet` parameter when reading \'.xlsx\' files.'). 459 else:. --> 460 adata = read_excel(filename, sheet). 461 elif ext in {'mtx', 'mtx.gz'}:. 462 adata = read_mtx(filename). ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/anndata/readwrite/read.py in read_excel(filename, sheet, dtype). 59 # rely on pandas for reading an excel file. 60 from pandas import read_excel. ---> 61 df = read_excel(fspath(filename), sheet, dtype=dtype). 62 X = df.values[:, 1:]. 63 row = {'row_names': df.iloc[:, 0].values.astype(str)}. ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/pandas/util/_decorators.py in wrapper(*args, **kwargs). 186 else:. 187 kwargs[new_arg_name] = new_arg_value. --> 188 return func(*args, **kwargs). 189 return wrapper. 190 return _deprecate_kwarg. ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/pandas/util/_decorators.py in wrapper(*args, **kwargs). 186 else:. 187 kwargs[new_arg_name] = new_arg_value. --> 188 return func(*args, **kwargs). 189 return wrapper. 190 return _deprecate_kwarg. ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/pandas/io/excel.py in read_excel(io, sheet_name, header, names, index_col, parse_cols, usecols, squeeze, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, verbose, parse_dates, date_parser, thousands, comment, skip_footer, skipfooter, convert_float, mangle_dupe_cols, **kwds). 373 convert_float=convert_float,. 374 mangle_dupe_cols=mangle_dupe_cols,. --> 375 **kwds). 376 . 377 . ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/pandas/io/excel.py in parse(self, sheet_name, header, names, index_col, usecols, squeeze, converters, true_values, false_values, skiprows, nrows, na_values, parse_dates, date_parser, thousands, comment, skipfooter, convert_float, mangle_dupe_cols, **kwds",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/547
https://github.com/scverse/scanpy/issues/547:3210,modifiability,pac,packages,3210,"el(filename, sheet). 461 elif ext in {'mtx', 'mtx.gz'}:. 462 adata = read_mtx(filename). ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/anndata/readwrite/read.py in read_excel(filename, sheet, dtype). 59 # rely on pandas for reading an excel file. 60 from pandas import read_excel. ---> 61 df = read_excel(fspath(filename), sheet, dtype=dtype). 62 X = df.values[:, 1:]. 63 row = {'row_names': df.iloc[:, 0].values.astype(str)}. ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/pandas/util/_decorators.py in wrapper(*args, **kwargs). 186 else:. 187 kwargs[new_arg_name] = new_arg_value. --> 188 return func(*args, **kwargs). 189 return wrapper. 190 return _deprecate_kwarg. ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/pandas/util/_decorators.py in wrapper(*args, **kwargs). 186 else:. 187 kwargs[new_arg_name] = new_arg_value. --> 188 return func(*args, **kwargs). 189 return wrapper. 190 return _deprecate_kwarg. ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/pandas/io/excel.py in read_excel(io, sheet_name, header, names, index_col, parse_cols, usecols, squeeze, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, verbose, parse_dates, date_parser, thousands, comment, skip_footer, skipfooter, convert_float, mangle_dupe_cols, **kwds). 373 convert_float=convert_float,. 374 mangle_dupe_cols=mangle_dupe_cols,. --> 375 **kwds). 376 . 377 . ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/pandas/io/excel.py in parse(self, sheet_name, header, names, index_col, usecols, squeeze, converters, true_values, false_values, skiprows, nrows, na_values, parse_dates, date_parser, thousands, comment, skipfooter, convert_float, mangle_dupe_cols, **kwds). 716 convert_float=convert_float,. 717 mangle_dupe_cols=mangle_dupe_cols,. --> 718 **kwds). 719 . 720 @property. ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/pandas/io/excel.py in parse(self, sheet_name, header, names, index_col, usecols, ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/547
https://github.com/scverse/scanpy/issues/547:3697,modifiability,pac,packages,3697,"-packages/pandas/util/_decorators.py in wrapper(*args, **kwargs). 186 else:. 187 kwargs[new_arg_name] = new_arg_value. --> 188 return func(*args, **kwargs). 189 return wrapper. 190 return _deprecate_kwarg. ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/pandas/util/_decorators.py in wrapper(*args, **kwargs). 186 else:. 187 kwargs[new_arg_name] = new_arg_value. --> 188 return func(*args, **kwargs). 189 return wrapper. 190 return _deprecate_kwarg. ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/pandas/io/excel.py in read_excel(io, sheet_name, header, names, index_col, parse_cols, usecols, squeeze, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, verbose, parse_dates, date_parser, thousands, comment, skip_footer, skipfooter, convert_float, mangle_dupe_cols, **kwds). 373 convert_float=convert_float,. 374 mangle_dupe_cols=mangle_dupe_cols,. --> 375 **kwds). 376 . 377 . ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/pandas/io/excel.py in parse(self, sheet_name, header, names, index_col, usecols, squeeze, converters, true_values, false_values, skiprows, nrows, na_values, parse_dates, date_parser, thousands, comment, skipfooter, convert_float, mangle_dupe_cols, **kwds). 716 convert_float=convert_float,. 717 mangle_dupe_cols=mangle_dupe_cols,. --> 718 **kwds). 719 . 720 @property. ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/pandas/io/excel.py in parse(self, sheet_name, header, names, index_col, usecols, squeeze, dtype, true_values, false_values, skiprows, nrows, na_values, verbose, parse_dates, date_parser, thousands, comment, skipfooter, convert_float, mangle_dupe_cols, **kwds). 601 **kwds). 602 . --> 603 output[asheetname] = parser.read(nrows=nrows). 604 . 605 if not squeeze or isinstance(output[asheetname], DataFrame):. ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/pandas/io/parsers.py in read(self, nrows). 1137 def read(self, nrows=None):. 1138 nrows = _validate_int",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/547
https://github.com/scverse/scanpy/issues/547:4124,modifiability,pac,packages,4124,"er. 190 return _deprecate_kwarg. ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/pandas/io/excel.py in read_excel(io, sheet_name, header, names, index_col, parse_cols, usecols, squeeze, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, verbose, parse_dates, date_parser, thousands, comment, skip_footer, skipfooter, convert_float, mangle_dupe_cols, **kwds). 373 convert_float=convert_float,. 374 mangle_dupe_cols=mangle_dupe_cols,. --> 375 **kwds). 376 . 377 . ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/pandas/io/excel.py in parse(self, sheet_name, header, names, index_col, usecols, squeeze, converters, true_values, false_values, skiprows, nrows, na_values, parse_dates, date_parser, thousands, comment, skipfooter, convert_float, mangle_dupe_cols, **kwds). 716 convert_float=convert_float,. 717 mangle_dupe_cols=mangle_dupe_cols,. --> 718 **kwds). 719 . 720 @property. ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/pandas/io/excel.py in parse(self, sheet_name, header, names, index_col, usecols, squeeze, dtype, true_values, false_values, skiprows, nrows, na_values, verbose, parse_dates, date_parser, thousands, comment, skipfooter, convert_float, mangle_dupe_cols, **kwds). 601 **kwds). 602 . --> 603 output[asheetname] = parser.read(nrows=nrows). 604 . 605 if not squeeze or isinstance(output[asheetname], DataFrame):. ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/pandas/io/parsers.py in read(self, nrows). 1137 def read(self, nrows=None):. 1138 nrows = _validate_integer('nrows', nrows). -> 1139 ret = self._engine.read(nrows). 1140 . 1141 # May alter columns / col_dict. ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/pandas/io/parsers.py in read(self, rows). 2419 columns, data = self._do_date_conversions(columns, data). 2420 . -> 2421 data = self._convert_data(data). 2422 index, columns = self._make_index(data, alldata, columns, indexnamerow). 2423 . ~/miniconda3/envs/spols19",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/547
https://github.com/scverse/scanpy/issues/547:4589,modifiability,pac,packages,4589,"e_cols=mangle_dupe_cols,. --> 375 **kwds). 376 . 377 . ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/pandas/io/excel.py in parse(self, sheet_name, header, names, index_col, usecols, squeeze, converters, true_values, false_values, skiprows, nrows, na_values, parse_dates, date_parser, thousands, comment, skipfooter, convert_float, mangle_dupe_cols, **kwds). 716 convert_float=convert_float,. 717 mangle_dupe_cols=mangle_dupe_cols,. --> 718 **kwds). 719 . 720 @property. ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/pandas/io/excel.py in parse(self, sheet_name, header, names, index_col, usecols, squeeze, dtype, true_values, false_values, skiprows, nrows, na_values, verbose, parse_dates, date_parser, thousands, comment, skipfooter, convert_float, mangle_dupe_cols, **kwds). 601 **kwds). 602 . --> 603 output[asheetname] = parser.read(nrows=nrows). 604 . 605 if not squeeze or isinstance(output[asheetname], DataFrame):. ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/pandas/io/parsers.py in read(self, nrows). 1137 def read(self, nrows=None):. 1138 nrows = _validate_integer('nrows', nrows). -> 1139 ret = self._engine.read(nrows). 1140 . 1141 # May alter columns / col_dict. ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/pandas/io/parsers.py in read(self, rows). 2419 columns, data = self._do_date_conversions(columns, data). 2420 . -> 2421 data = self._convert_data(data). 2422 index, columns = self._make_index(data, alldata, columns, indexnamerow). 2423 . ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/pandas/io/parsers.py in _convert_data(self, data). 2485 return self._convert_to_ndarrays(data, clean_na_values,. 2486 clean_na_fvalues, self.verbose,. -> 2487 clean_conv, clean_dtypes). 2488 . 2489 def _infer_columns(self):. ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/pandas/io/parsers.py in _convert_to_ndarrays(self, dct, na_values, na_fvalues, verbose, converters, dtypes). 1703 # invalid input to is_bool_dtype. 1",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/547
https://github.com/scverse/scanpy/issues/547:4856,modifiability,pac,packages,4856,"s, parse_dates, date_parser, thousands, comment, skipfooter, convert_float, mangle_dupe_cols, **kwds). 716 convert_float=convert_float,. 717 mangle_dupe_cols=mangle_dupe_cols,. --> 718 **kwds). 719 . 720 @property. ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/pandas/io/excel.py in parse(self, sheet_name, header, names, index_col, usecols, squeeze, dtype, true_values, false_values, skiprows, nrows, na_values, verbose, parse_dates, date_parser, thousands, comment, skipfooter, convert_float, mangle_dupe_cols, **kwds). 601 **kwds). 602 . --> 603 output[asheetname] = parser.read(nrows=nrows). 604 . 605 if not squeeze or isinstance(output[asheetname], DataFrame):. ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/pandas/io/parsers.py in read(self, nrows). 1137 def read(self, nrows=None):. 1138 nrows = _validate_integer('nrows', nrows). -> 1139 ret = self._engine.read(nrows). 1140 . 1141 # May alter columns / col_dict. ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/pandas/io/parsers.py in read(self, rows). 2419 columns, data = self._do_date_conversions(columns, data). 2420 . -> 2421 data = self._convert_data(data). 2422 index, columns = self._make_index(data, alldata, columns, indexnamerow). 2423 . ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/pandas/io/parsers.py in _convert_data(self, data). 2485 return self._convert_to_ndarrays(data, clean_na_values,. 2486 clean_na_fvalues, self.verbose,. -> 2487 clean_conv, clean_dtypes). 2488 . 2489 def _infer_columns(self):. ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/pandas/io/parsers.py in _convert_to_ndarrays(self, dct, na_values, na_fvalues, verbose, converters, dtypes). 1703 # invalid input to is_bool_dtype. 1704 pass. -> 1705 cvals = self._cast_types(cvals, cast_type, c). 1706 . 1707 result[c] = cvals. ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/pandas/io/parsers.py in _cast_types(self, values, cast_type, column). 1809 except ValueError:. 1810 raise ValueEr",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/547
https://github.com/scverse/scanpy/issues/547:5152,modifiability,pac,packages,5152,"arse(self, sheet_name, header, names, index_col, usecols, squeeze, dtype, true_values, false_values, skiprows, nrows, na_values, verbose, parse_dates, date_parser, thousands, comment, skipfooter, convert_float, mangle_dupe_cols, **kwds). 601 **kwds). 602 . --> 603 output[asheetname] = parser.read(nrows=nrows). 604 . 605 if not squeeze or isinstance(output[asheetname], DataFrame):. ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/pandas/io/parsers.py in read(self, nrows). 1137 def read(self, nrows=None):. 1138 nrows = _validate_integer('nrows', nrows). -> 1139 ret = self._engine.read(nrows). 1140 . 1141 # May alter columns / col_dict. ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/pandas/io/parsers.py in read(self, rows). 2419 columns, data = self._do_date_conversions(columns, data). 2420 . -> 2421 data = self._convert_data(data). 2422 index, columns = self._make_index(data, alldata, columns, indexnamerow). 2423 . ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/pandas/io/parsers.py in _convert_data(self, data). 2485 return self._convert_to_ndarrays(data, clean_na_values,. 2486 clean_na_fvalues, self.verbose,. -> 2487 clean_conv, clean_dtypes). 2488 . 2489 def _infer_columns(self):. ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/pandas/io/parsers.py in _convert_to_ndarrays(self, dct, na_values, na_fvalues, verbose, converters, dtypes). 1703 # invalid input to is_bool_dtype. 1704 pass. -> 1705 cvals = self._cast_types(cvals, cast_type, c). 1706 . 1707 result[c] = cvals. ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/pandas/io/parsers.py in _cast_types(self, values, cast_type, column). 1809 except ValueError:. 1810 raise ValueError(""Unable to convert column %s to "". -> 1811 ""type %s"" % (column, cast_type)). 1812 return values. 1813 . ValueError: Unable to convert column Cell to type float32. ```. ```python. sc.logging.print_versions(). scanpy==1.4 anndata==0.6.18 numpy==1.15.4 scipy==1.2.0 pandas==0.24.2 scikit-learn==",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/547
https://github.com/scverse/scanpy/issues/547:5435,modifiability,pac,packages,5435,"dtype, true_values, false_values, skiprows, nrows, na_values, verbose, parse_dates, date_parser, thousands, comment, skipfooter, convert_float, mangle_dupe_cols, **kwds). 601 **kwds). 602 . --> 603 output[asheetname] = parser.read(nrows=nrows). 604 . 605 if not squeeze or isinstance(output[asheetname], DataFrame):. ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/pandas/io/parsers.py in read(self, nrows). 1137 def read(self, nrows=None):. 1138 nrows = _validate_integer('nrows', nrows). -> 1139 ret = self._engine.read(nrows). 1140 . 1141 # May alter columns / col_dict. ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/pandas/io/parsers.py in read(self, rows). 2419 columns, data = self._do_date_conversions(columns, data). 2420 . -> 2421 data = self._convert_data(data). 2422 index, columns = self._make_index(data, alldata, columns, indexnamerow). 2423 . ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/pandas/io/parsers.py in _convert_data(self, data). 2485 return self._convert_to_ndarrays(data, clean_na_values,. 2486 clean_na_fvalues, self.verbose,. -> 2487 clean_conv, clean_dtypes). 2488 . 2489 def _infer_columns(self):. ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/pandas/io/parsers.py in _convert_to_ndarrays(self, dct, na_values, na_fvalues, verbose, converters, dtypes). 1703 # invalid input to is_bool_dtype. 1704 pass. -> 1705 cvals = self._cast_types(cvals, cast_type, c). 1706 . 1707 result[c] = cvals. ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/pandas/io/parsers.py in _cast_types(self, values, cast_type, column). 1809 except ValueError:. 1810 raise ValueError(""Unable to convert column %s to "". -> 1811 ""type %s"" % (column, cast_type)). 1812 return values. 1813 . ValueError: Unable to convert column Cell to type float32. ```. ```python. sc.logging.print_versions(). scanpy==1.4 anndata==0.6.18 numpy==1.15.4 scipy==1.2.0 pandas==0.24.2 scikit-learn==0.20.2 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 . ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/547
https://github.com/scverse/scanpy/issues/547:5738,modifiability,pac,packages,5738,"dtype, true_values, false_values, skiprows, nrows, na_values, verbose, parse_dates, date_parser, thousands, comment, skipfooter, convert_float, mangle_dupe_cols, **kwds). 601 **kwds). 602 . --> 603 output[asheetname] = parser.read(nrows=nrows). 604 . 605 if not squeeze or isinstance(output[asheetname], DataFrame):. ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/pandas/io/parsers.py in read(self, nrows). 1137 def read(self, nrows=None):. 1138 nrows = _validate_integer('nrows', nrows). -> 1139 ret = self._engine.read(nrows). 1140 . 1141 # May alter columns / col_dict. ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/pandas/io/parsers.py in read(self, rows). 2419 columns, data = self._do_date_conversions(columns, data). 2420 . -> 2421 data = self._convert_data(data). 2422 index, columns = self._make_index(data, alldata, columns, indexnamerow). 2423 . ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/pandas/io/parsers.py in _convert_data(self, data). 2485 return self._convert_to_ndarrays(data, clean_na_values,. 2486 clean_na_fvalues, self.verbose,. -> 2487 clean_conv, clean_dtypes). 2488 . 2489 def _infer_columns(self):. ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/pandas/io/parsers.py in _convert_to_ndarrays(self, dct, na_values, na_fvalues, verbose, converters, dtypes). 1703 # invalid input to is_bool_dtype. 1704 pass. -> 1705 cvals = self._cast_types(cvals, cast_type, c). 1706 . 1707 result[c] = cvals. ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/pandas/io/parsers.py in _cast_types(self, values, cast_type, column). 1809 except ValueError:. 1810 raise ValueError(""Unable to convert column %s to "". -> 1811 ""type %s"" % (column, cast_type)). 1812 return values. 1813 . ValueError: Unable to convert column Cell to type float32. ```. ```python. sc.logging.print_versions(). scanpy==1.4 anndata==0.6.18 numpy==1.15.4 scipy==1.2.0 pandas==0.24.2 scikit-learn==0.20.2 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 . ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/547
https://github.com/scverse/scanpy/issues/547:1313,performance,cach,cache,1313,"/parsers.py in _cast_types(self, values, cast_type, column). 1807 values = astype_nansafe(values, cast_type,. -> 1808 copy=True, skipna=True). 1809 except ValueError:. ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/pandas/core/dtypes/cast.py in astype_nansafe(arr, dtype, copy, skipna). 701 # Explicit copy, or required since NumPy can't view from / to object. --> 702 return arr.astype(dtype, copy=True). 703 . ValueError: could not convert string to float: '4SFGA6_247'. During handling of the above exception, another exception occurred:. ValueError Traceback (most recent call last). <ipython-input-3-bf986d1f9b8c> in <module>. 1 import scanpy as sc. ----> 2 sc.datasets.moignard15(). ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/scanpy/datasets/__init__.py in moignard15(). 104 filename = 'data/moignard15/nbt.3154-S3.xlsx'. 105 backup_url = 'http://www.nature.com/nbt/journal/v33/n3/extref/nbt.3154-S3.xlsx'. --> 106 adata = sc.read(filename, sheet='dCt_values.txt', cache=True, backup_url=backup_url). 107 # filter out 4 genes as in Haghverdi et al. (2016). 108 gene_subset = ~np.in1d(adata.var_names, ['Eif2b1', 'Mrpl19', 'Polr2a', 'Ubc']). ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/scanpy/readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs). 76 return _read(filename, backed=backed, sheet=sheet, ext=ext,. 77 delimiter=delimiter, first_column_names=first_column_names,. ---> 78 backup_url=backup_url, cache=cache, **kwargs). 79 # generate filename and read to dict. 80 filekey = filename. ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/scanpy/readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs). 458 'Provide `sheet` parameter when reading \'.xlsx\' files.'). 459 else:. --> 460 adata = read_excel(filename, sheet). 461 elif ext in {'mtx', 'mtx.gz'}:. 462 adata = read_mtx(filename). ~/miniconda3/",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/547
https://github.com/scverse/scanpy/issues/547:1648,performance,cach,cache,1648,"e NumPy can't view from / to object. --> 702 return arr.astype(dtype, copy=True). 703 . ValueError: could not convert string to float: '4SFGA6_247'. During handling of the above exception, another exception occurred:. ValueError Traceback (most recent call last). <ipython-input-3-bf986d1f9b8c> in <module>. 1 import scanpy as sc. ----> 2 sc.datasets.moignard15(). ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/scanpy/datasets/__init__.py in moignard15(). 104 filename = 'data/moignard15/nbt.3154-S3.xlsx'. 105 backup_url = 'http://www.nature.com/nbt/journal/v33/n3/extref/nbt.3154-S3.xlsx'. --> 106 adata = sc.read(filename, sheet='dCt_values.txt', cache=True, backup_url=backup_url). 107 # filter out 4 genes as in Haghverdi et al. (2016). 108 gene_subset = ~np.in1d(adata.var_names, ['Eif2b1', 'Mrpl19', 'Polr2a', 'Ubc']). ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/scanpy/readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs). 76 return _read(filename, backed=backed, sheet=sheet, ext=ext,. 77 delimiter=delimiter, first_column_names=first_column_names,. ---> 78 backup_url=backup_url, cache=cache, **kwargs). 79 # generate filename and read to dict. 80 filekey = filename. ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/scanpy/readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs). 458 'Provide `sheet` parameter when reading \'.xlsx\' files.'). 459 else:. --> 460 adata = read_excel(filename, sheet). 461 elif ext in {'mtx', 'mtx.gz'}:. 462 adata = read_mtx(filename). ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/anndata/readwrite/read.py in read_excel(filename, sheet, dtype). 59 # rely on pandas for reading an excel file. 60 from pandas import read_excel. ---> 61 df = read_excel(fspath(filename), sheet, dtype=dtype). 62 X = df.values[:, 1:]. 63 row = {'row_names': df.iloc[:, 0].values.astype(str)}",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/547
https://github.com/scverse/scanpy/issues/547:1825,performance,cach,cache,1825," exception, another exception occurred:. ValueError Traceback (most recent call last). <ipython-input-3-bf986d1f9b8c> in <module>. 1 import scanpy as sc. ----> 2 sc.datasets.moignard15(). ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/scanpy/datasets/__init__.py in moignard15(). 104 filename = 'data/moignard15/nbt.3154-S3.xlsx'. 105 backup_url = 'http://www.nature.com/nbt/journal/v33/n3/extref/nbt.3154-S3.xlsx'. --> 106 adata = sc.read(filename, sheet='dCt_values.txt', cache=True, backup_url=backup_url). 107 # filter out 4 genes as in Haghverdi et al. (2016). 108 gene_subset = ~np.in1d(adata.var_names, ['Eif2b1', 'Mrpl19', 'Polr2a', 'Ubc']). ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/scanpy/readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs). 76 return _read(filename, backed=backed, sheet=sheet, ext=ext,. 77 delimiter=delimiter, first_column_names=first_column_names,. ---> 78 backup_url=backup_url, cache=cache, **kwargs). 79 # generate filename and read to dict. 80 filekey = filename. ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/scanpy/readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs). 458 'Provide `sheet` parameter when reading \'.xlsx\' files.'). 459 else:. --> 460 adata = read_excel(filename, sheet). 461 elif ext in {'mtx', 'mtx.gz'}:. 462 adata = read_mtx(filename). ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/anndata/readwrite/read.py in read_excel(filename, sheet, dtype). 59 # rely on pandas for reading an excel file. 60 from pandas import read_excel. ---> 61 df = read_excel(fspath(filename), sheet, dtype=dtype). 62 X = df.values[:, 1:]. 63 row = {'row_names': df.iloc[:, 0].values.astype(str)}. ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/pandas/util/_decorators.py in wrapper(*args, **kwargs). 186 else:. 187 kwargs[new_arg_name] = new_arg_value. --> 188 ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/547
https://github.com/scverse/scanpy/issues/547:1831,performance,cach,cache,1831,"tion, another exception occurred:. ValueError Traceback (most recent call last). <ipython-input-3-bf986d1f9b8c> in <module>. 1 import scanpy as sc. ----> 2 sc.datasets.moignard15(). ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/scanpy/datasets/__init__.py in moignard15(). 104 filename = 'data/moignard15/nbt.3154-S3.xlsx'. 105 backup_url = 'http://www.nature.com/nbt/journal/v33/n3/extref/nbt.3154-S3.xlsx'. --> 106 adata = sc.read(filename, sheet='dCt_values.txt', cache=True, backup_url=backup_url). 107 # filter out 4 genes as in Haghverdi et al. (2016). 108 gene_subset = ~np.in1d(adata.var_names, ['Eif2b1', 'Mrpl19', 'Polr2a', 'Ubc']). ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/scanpy/readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs). 76 return _read(filename, backed=backed, sheet=sheet, ext=ext,. 77 delimiter=delimiter, first_column_names=first_column_names,. ---> 78 backup_url=backup_url, cache=cache, **kwargs). 79 # generate filename and read to dict. 80 filekey = filename. ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/scanpy/readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs). 458 'Provide `sheet` parameter when reading \'.xlsx\' files.'). 459 else:. --> 460 adata = read_excel(filename, sheet). 461 elif ext in {'mtx', 'mtx.gz'}:. 462 adata = read_mtx(filename). ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/anndata/readwrite/read.py in read_excel(filename, sheet, dtype). 59 # rely on pandas for reading an excel file. 60 from pandas import read_excel. ---> 61 df = read_excel(fspath(filename), sheet, dtype=dtype). 62 X = df.values[:, 1:]. 63 row = {'row_names': df.iloc[:, 0].values.astype(str)}. ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/pandas/util/_decorators.py in wrapper(*args, **kwargs). 186 else:. 187 kwargs[new_arg_name] = new_arg_value. --> 188 return",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/547
https://github.com/scverse/scanpy/issues/547:2073,performance,cach,cache,2073,"anpy/datasets/__init__.py in moignard15(). 104 filename = 'data/moignard15/nbt.3154-S3.xlsx'. 105 backup_url = 'http://www.nature.com/nbt/journal/v33/n3/extref/nbt.3154-S3.xlsx'. --> 106 adata = sc.read(filename, sheet='dCt_values.txt', cache=True, backup_url=backup_url). 107 # filter out 4 genes as in Haghverdi et al. (2016). 108 gene_subset = ~np.in1d(adata.var_names, ['Eif2b1', 'Mrpl19', 'Polr2a', 'Ubc']). ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/scanpy/readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs). 76 return _read(filename, backed=backed, sheet=sheet, ext=ext,. 77 delimiter=delimiter, first_column_names=first_column_names,. ---> 78 backup_url=backup_url, cache=cache, **kwargs). 79 # generate filename and read to dict. 80 filekey = filename. ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/scanpy/readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs). 458 'Provide `sheet` parameter when reading \'.xlsx\' files.'). 459 else:. --> 460 adata = read_excel(filename, sheet). 461 elif ext in {'mtx', 'mtx.gz'}:. 462 adata = read_mtx(filename). ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/anndata/readwrite/read.py in read_excel(filename, sheet, dtype). 59 # rely on pandas for reading an excel file. 60 from pandas import read_excel. ---> 61 df = read_excel(fspath(filename), sheet, dtype=dtype). 62 X = df.values[:, 1:]. 63 row = {'row_names': df.iloc[:, 0].values.astype(str)}. ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/pandas/util/_decorators.py in wrapper(*args, **kwargs). 186 else:. 187 kwargs[new_arg_name] = new_arg_value. --> 188 return func(*args, **kwargs). 189 return wrapper. 190 return _deprecate_kwarg. ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/pandas/util/_decorators.py in wrapper(*args, **kwargs). 186 else:. 187 kwargs[new_arg_name] = new_arg_value. --",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/547
https://github.com/scverse/scanpy/issues/547:464,safety,except,except,464,"sc.datasets.moignard15() raises ValueError; ```python. import scanpy as sc. sc.datasets.moignard15(). ```. Output. ```python. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/pandas/io/parsers.py in _cast_types(self, values, cast_type, column). 1807 values = astype_nansafe(values, cast_type,. -> 1808 copy=True, skipna=True). 1809 except ValueError:. ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/pandas/core/dtypes/cast.py in astype_nansafe(arr, dtype, copy, skipna). 701 # Explicit copy, or required since NumPy can't view from / to object. --> 702 return arr.astype(dtype, copy=True). 703 . ValueError: could not convert string to float: '4SFGA6_247'. During handling of the above exception, another exception occurred:. ValueError Traceback (most recent call last). <ipython-input-3-bf986d1f9b8c> in <module>. 1 import scanpy as sc. ----> 2 sc.datasets.moignard15(). ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/scanpy/datasets/__init__.py in moignard15(). 104 filename = 'data/moignard15/nbt.3154-S3.xlsx'. 105 backup_url = 'http://www.nature.com/nbt/journal/v33/n3/extref/nbt.3154-S3.xlsx'. --> 106 adata = sc.read(filename, sheet='dCt_values.txt', cache=True, backup_url=backup_url). 107 # filter out 4 genes as in Haghverdi et al. (2016). 108 gene_subset = ~np.in1d(adata.var_names, ['Eif2b1', 'Mrpl19', 'Polr2a', 'Ubc']). ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/scanpy/readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs). 76 return _read(filename, backed=backed, sheet=sheet, ext=ext,. 77 delimiter=delimiter, first_column_names=first_column_names,. ---> 78 backup_url=backup_url, cache=cache, **kwargs). 79 # generate filename and read to dict. 80 filekey = filename. ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/scanpy/readwrite.py in _read(",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/547
https://github.com/scverse/scanpy/issues/547:829,safety,except,exception,829,"sc.datasets.moignard15() raises ValueError; ```python. import scanpy as sc. sc.datasets.moignard15(). ```. Output. ```python. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/pandas/io/parsers.py in _cast_types(self, values, cast_type, column). 1807 values = astype_nansafe(values, cast_type,. -> 1808 copy=True, skipna=True). 1809 except ValueError:. ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/pandas/core/dtypes/cast.py in astype_nansafe(arr, dtype, copy, skipna). 701 # Explicit copy, or required since NumPy can't view from / to object. --> 702 return arr.astype(dtype, copy=True). 703 . ValueError: could not convert string to float: '4SFGA6_247'. During handling of the above exception, another exception occurred:. ValueError Traceback (most recent call last). <ipython-input-3-bf986d1f9b8c> in <module>. 1 import scanpy as sc. ----> 2 sc.datasets.moignard15(). ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/scanpy/datasets/__init__.py in moignard15(). 104 filename = 'data/moignard15/nbt.3154-S3.xlsx'. 105 backup_url = 'http://www.nature.com/nbt/journal/v33/n3/extref/nbt.3154-S3.xlsx'. --> 106 adata = sc.read(filename, sheet='dCt_values.txt', cache=True, backup_url=backup_url). 107 # filter out 4 genes as in Haghverdi et al. (2016). 108 gene_subset = ~np.in1d(adata.var_names, ['Eif2b1', 'Mrpl19', 'Polr2a', 'Ubc']). ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/scanpy/readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs). 76 return _read(filename, backed=backed, sheet=sheet, ext=ext,. 77 delimiter=delimiter, first_column_names=first_column_names,. ---> 78 backup_url=backup_url, cache=cache, **kwargs). 79 # generate filename and read to dict. 80 filekey = filename. ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/scanpy/readwrite.py in _read(",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/547
https://github.com/scverse/scanpy/issues/547:848,safety,except,exception,848,"sc.datasets.moignard15() raises ValueError; ```python. import scanpy as sc. sc.datasets.moignard15(). ```. Output. ```python. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/pandas/io/parsers.py in _cast_types(self, values, cast_type, column). 1807 values = astype_nansafe(values, cast_type,. -> 1808 copy=True, skipna=True). 1809 except ValueError:. ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/pandas/core/dtypes/cast.py in astype_nansafe(arr, dtype, copy, skipna). 701 # Explicit copy, or required since NumPy can't view from / to object. --> 702 return arr.astype(dtype, copy=True). 703 . ValueError: could not convert string to float: '4SFGA6_247'. During handling of the above exception, another exception occurred:. ValueError Traceback (most recent call last). <ipython-input-3-bf986d1f9b8c> in <module>. 1 import scanpy as sc. ----> 2 sc.datasets.moignard15(). ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/scanpy/datasets/__init__.py in moignard15(). 104 filename = 'data/moignard15/nbt.3154-S3.xlsx'. 105 backup_url = 'http://www.nature.com/nbt/journal/v33/n3/extref/nbt.3154-S3.xlsx'. --> 106 adata = sc.read(filename, sheet='dCt_values.txt', cache=True, backup_url=backup_url). 107 # filter out 4 genes as in Haghverdi et al. (2016). 108 gene_subset = ~np.in1d(adata.var_names, ['Eif2b1', 'Mrpl19', 'Polr2a', 'Ubc']). ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/scanpy/readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs). 76 return _read(filename, backed=backed, sheet=sheet, ext=ext,. 77 delimiter=delimiter, first_column_names=first_column_names,. ---> 78 backup_url=backup_url, cache=cache, **kwargs). 79 # generate filename and read to dict. 80 filekey = filename. ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/scanpy/readwrite.py in _read(",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/547
https://github.com/scverse/scanpy/issues/547:924,safety,input,input-,924,"sc.datasets.moignard15() raises ValueError; ```python. import scanpy as sc. sc.datasets.moignard15(). ```. Output. ```python. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/pandas/io/parsers.py in _cast_types(self, values, cast_type, column). 1807 values = astype_nansafe(values, cast_type,. -> 1808 copy=True, skipna=True). 1809 except ValueError:. ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/pandas/core/dtypes/cast.py in astype_nansafe(arr, dtype, copy, skipna). 701 # Explicit copy, or required since NumPy can't view from / to object. --> 702 return arr.astype(dtype, copy=True). 703 . ValueError: could not convert string to float: '4SFGA6_247'. During handling of the above exception, another exception occurred:. ValueError Traceback (most recent call last). <ipython-input-3-bf986d1f9b8c> in <module>. 1 import scanpy as sc. ----> 2 sc.datasets.moignard15(). ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/scanpy/datasets/__init__.py in moignard15(). 104 filename = 'data/moignard15/nbt.3154-S3.xlsx'. 105 backup_url = 'http://www.nature.com/nbt/journal/v33/n3/extref/nbt.3154-S3.xlsx'. --> 106 adata = sc.read(filename, sheet='dCt_values.txt', cache=True, backup_url=backup_url). 107 # filter out 4 genes as in Haghverdi et al. (2016). 108 gene_subset = ~np.in1d(adata.var_names, ['Eif2b1', 'Mrpl19', 'Polr2a', 'Ubc']). ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/scanpy/readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs). 76 return _read(filename, backed=backed, sheet=sheet, ext=ext,. 77 delimiter=delimiter, first_column_names=first_column_names,. ---> 78 backup_url=backup_url, cache=cache, **kwargs). 79 # generate filename and read to dict. 80 filekey = filename. ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/scanpy/readwrite.py in _read(",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/547
https://github.com/scverse/scanpy/issues/547:950,safety,modul,module,950,"sc.datasets.moignard15() raises ValueError; ```python. import scanpy as sc. sc.datasets.moignard15(). ```. Output. ```python. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/pandas/io/parsers.py in _cast_types(self, values, cast_type, column). 1807 values = astype_nansafe(values, cast_type,. -> 1808 copy=True, skipna=True). 1809 except ValueError:. ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/pandas/core/dtypes/cast.py in astype_nansafe(arr, dtype, copy, skipna). 701 # Explicit copy, or required since NumPy can't view from / to object. --> 702 return arr.astype(dtype, copy=True). 703 . ValueError: could not convert string to float: '4SFGA6_247'. During handling of the above exception, another exception occurred:. ValueError Traceback (most recent call last). <ipython-input-3-bf986d1f9b8c> in <module>. 1 import scanpy as sc. ----> 2 sc.datasets.moignard15(). ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/scanpy/datasets/__init__.py in moignard15(). 104 filename = 'data/moignard15/nbt.3154-S3.xlsx'. 105 backup_url = 'http://www.nature.com/nbt/journal/v33/n3/extref/nbt.3154-S3.xlsx'. --> 106 adata = sc.read(filename, sheet='dCt_values.txt', cache=True, backup_url=backup_url). 107 # filter out 4 genes as in Haghverdi et al. (2016). 108 gene_subset = ~np.in1d(adata.var_names, ['Eif2b1', 'Mrpl19', 'Polr2a', 'Ubc']). ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/scanpy/readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs). 76 return _read(filename, backed=backed, sheet=sheet, ext=ext,. 77 delimiter=delimiter, first_column_names=first_column_names,. ---> 78 backup_url=backup_url, cache=cache, **kwargs). 79 # generate filename and read to dict. 80 filekey = filename. ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/scanpy/readwrite.py in _read(",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/547
https://github.com/scverse/scanpy/issues/547:5568,safety,input,input,5568,"dtype, true_values, false_values, skiprows, nrows, na_values, verbose, parse_dates, date_parser, thousands, comment, skipfooter, convert_float, mangle_dupe_cols, **kwds). 601 **kwds). 602 . --> 603 output[asheetname] = parser.read(nrows=nrows). 604 . 605 if not squeeze or isinstance(output[asheetname], DataFrame):. ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/pandas/io/parsers.py in read(self, nrows). 1137 def read(self, nrows=None):. 1138 nrows = _validate_integer('nrows', nrows). -> 1139 ret = self._engine.read(nrows). 1140 . 1141 # May alter columns / col_dict. ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/pandas/io/parsers.py in read(self, rows). 2419 columns, data = self._do_date_conversions(columns, data). 2420 . -> 2421 data = self._convert_data(data). 2422 index, columns = self._make_index(data, alldata, columns, indexnamerow). 2423 . ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/pandas/io/parsers.py in _convert_data(self, data). 2485 return self._convert_to_ndarrays(data, clean_na_values,. 2486 clean_na_fvalues, self.verbose,. -> 2487 clean_conv, clean_dtypes). 2488 . 2489 def _infer_columns(self):. ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/pandas/io/parsers.py in _convert_to_ndarrays(self, dct, na_values, na_fvalues, verbose, converters, dtypes). 1703 # invalid input to is_bool_dtype. 1704 pass. -> 1705 cvals = self._cast_types(cvals, cast_type, c). 1706 . 1707 result[c] = cvals. ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/pandas/io/parsers.py in _cast_types(self, values, cast_type, column). 1809 except ValueError:. 1810 raise ValueError(""Unable to convert column %s to "". -> 1811 ""type %s"" % (column, cast_type)). 1812 return values. 1813 . ValueError: Unable to convert column Cell to type float32. ```. ```python. sc.logging.print_versions(). scanpy==1.4 anndata==0.6.18 numpy==1.15.4 scipy==1.2.0 pandas==0.24.2 scikit-learn==0.20.2 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 . ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/547
https://github.com/scverse/scanpy/issues/547:5822,safety,except,except,5822,"dtype, true_values, false_values, skiprows, nrows, na_values, verbose, parse_dates, date_parser, thousands, comment, skipfooter, convert_float, mangle_dupe_cols, **kwds). 601 **kwds). 602 . --> 603 output[asheetname] = parser.read(nrows=nrows). 604 . 605 if not squeeze or isinstance(output[asheetname], DataFrame):. ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/pandas/io/parsers.py in read(self, nrows). 1137 def read(self, nrows=None):. 1138 nrows = _validate_integer('nrows', nrows). -> 1139 ret = self._engine.read(nrows). 1140 . 1141 # May alter columns / col_dict. ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/pandas/io/parsers.py in read(self, rows). 2419 columns, data = self._do_date_conversions(columns, data). 2420 . -> 2421 data = self._convert_data(data). 2422 index, columns = self._make_index(data, alldata, columns, indexnamerow). 2423 . ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/pandas/io/parsers.py in _convert_data(self, data). 2485 return self._convert_to_ndarrays(data, clean_na_values,. 2486 clean_na_fvalues, self.verbose,. -> 2487 clean_conv, clean_dtypes). 2488 . 2489 def _infer_columns(self):. ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/pandas/io/parsers.py in _convert_to_ndarrays(self, dct, na_values, na_fvalues, verbose, converters, dtypes). 1703 # invalid input to is_bool_dtype. 1704 pass. -> 1705 cvals = self._cast_types(cvals, cast_type, c). 1706 . 1707 result[c] = cvals. ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/pandas/io/parsers.py in _cast_types(self, values, cast_type, column). 1809 except ValueError:. 1810 raise ValueError(""Unable to convert column %s to "". -> 1811 ""type %s"" % (column, cast_type)). 1812 return values. 1813 . ValueError: Unable to convert column Cell to type float32. ```. ```python. sc.logging.print_versions(). scanpy==1.4 anndata==0.6.18 numpy==1.15.4 scipy==1.2.0 pandas==0.24.2 scikit-learn==0.20.2 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 . ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/547
https://github.com/scverse/scanpy/issues/547:6046,safety,log,logging,6046,"dtype, true_values, false_values, skiprows, nrows, na_values, verbose, parse_dates, date_parser, thousands, comment, skipfooter, convert_float, mangle_dupe_cols, **kwds). 601 **kwds). 602 . --> 603 output[asheetname] = parser.read(nrows=nrows). 604 . 605 if not squeeze or isinstance(output[asheetname], DataFrame):. ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/pandas/io/parsers.py in read(self, nrows). 1137 def read(self, nrows=None):. 1138 nrows = _validate_integer('nrows', nrows). -> 1139 ret = self._engine.read(nrows). 1140 . 1141 # May alter columns / col_dict. ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/pandas/io/parsers.py in read(self, rows). 2419 columns, data = self._do_date_conversions(columns, data). 2420 . -> 2421 data = self._convert_data(data). 2422 index, columns = self._make_index(data, alldata, columns, indexnamerow). 2423 . ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/pandas/io/parsers.py in _convert_data(self, data). 2485 return self._convert_to_ndarrays(data, clean_na_values,. 2486 clean_na_fvalues, self.verbose,. -> 2487 clean_conv, clean_dtypes). 2488 . 2489 def _infer_columns(self):. ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/pandas/io/parsers.py in _convert_to_ndarrays(self, dct, na_values, na_fvalues, verbose, converters, dtypes). 1703 # invalid input to is_bool_dtype. 1704 pass. -> 1705 cvals = self._cast_types(cvals, cast_type, c). 1706 . 1707 result[c] = cvals. ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/pandas/io/parsers.py in _cast_types(self, values, cast_type, column). 1809 except ValueError:. 1810 raise ValueError(""Unable to convert column %s to "". -> 1811 ""type %s"" % (column, cast_type)). 1812 return values. 1813 . ValueError: Unable to convert column Cell to type float32. ```. ```python. sc.logging.print_versions(). scanpy==1.4 anndata==0.6.18 numpy==1.15.4 scipy==1.2.0 pandas==0.24.2 scikit-learn==0.20.2 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 . ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/547
https://github.com/scverse/scanpy/issues/547:6046,security,log,logging,6046,"dtype, true_values, false_values, skiprows, nrows, na_values, verbose, parse_dates, date_parser, thousands, comment, skipfooter, convert_float, mangle_dupe_cols, **kwds). 601 **kwds). 602 . --> 603 output[asheetname] = parser.read(nrows=nrows). 604 . 605 if not squeeze or isinstance(output[asheetname], DataFrame):. ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/pandas/io/parsers.py in read(self, nrows). 1137 def read(self, nrows=None):. 1138 nrows = _validate_integer('nrows', nrows). -> 1139 ret = self._engine.read(nrows). 1140 . 1141 # May alter columns / col_dict. ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/pandas/io/parsers.py in read(self, rows). 2419 columns, data = self._do_date_conversions(columns, data). 2420 . -> 2421 data = self._convert_data(data). 2422 index, columns = self._make_index(data, alldata, columns, indexnamerow). 2423 . ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/pandas/io/parsers.py in _convert_data(self, data). 2485 return self._convert_to_ndarrays(data, clean_na_values,. 2486 clean_na_fvalues, self.verbose,. -> 2487 clean_conv, clean_dtypes). 2488 . 2489 def _infer_columns(self):. ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/pandas/io/parsers.py in _convert_to_ndarrays(self, dct, na_values, na_fvalues, verbose, converters, dtypes). 1703 # invalid input to is_bool_dtype. 1704 pass. -> 1705 cvals = self._cast_types(cvals, cast_type, c). 1706 . 1707 result[c] = cvals. ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/pandas/io/parsers.py in _cast_types(self, values, cast_type, column). 1809 except ValueError:. 1810 raise ValueError(""Unable to convert column %s to "". -> 1811 ""type %s"" % (column, cast_type)). 1812 return values. 1813 . ValueError: Unable to convert column Cell to type float32. ```. ```python. sc.logging.print_versions(). scanpy==1.4 anndata==0.6.18 numpy==1.15.4 scipy==1.2.0 pandas==0.24.2 scikit-learn==0.20.2 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 . ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/547
https://github.com/scverse/scanpy/issues/547:214,testability,Trace,Traceback,214,"sc.datasets.moignard15() raises ValueError; ```python. import scanpy as sc. sc.datasets.moignard15(). ```. Output. ```python. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/pandas/io/parsers.py in _cast_types(self, values, cast_type, column). 1807 values = astype_nansafe(values, cast_type,. -> 1808 copy=True, skipna=True). 1809 except ValueError:. ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/pandas/core/dtypes/cast.py in astype_nansafe(arr, dtype, copy, skipna). 701 # Explicit copy, or required since NumPy can't view from / to object. --> 702 return arr.astype(dtype, copy=True). 703 . ValueError: could not convert string to float: '4SFGA6_247'. During handling of the above exception, another exception occurred:. ValueError Traceback (most recent call last). <ipython-input-3-bf986d1f9b8c> in <module>. 1 import scanpy as sc. ----> 2 sc.datasets.moignard15(). ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/scanpy/datasets/__init__.py in moignard15(). 104 filename = 'data/moignard15/nbt.3154-S3.xlsx'. 105 backup_url = 'http://www.nature.com/nbt/journal/v33/n3/extref/nbt.3154-S3.xlsx'. --> 106 adata = sc.read(filename, sheet='dCt_values.txt', cache=True, backup_url=backup_url). 107 # filter out 4 genes as in Haghverdi et al. (2016). 108 gene_subset = ~np.in1d(adata.var_names, ['Eif2b1', 'Mrpl19', 'Polr2a', 'Ubc']). ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/scanpy/readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs). 76 return _read(filename, backed=backed, sheet=sheet, ext=ext,. 77 delimiter=delimiter, first_column_names=first_column_names,. ---> 78 backup_url=backup_url, cache=cache, **kwargs). 79 # generate filename and read to dict. 80 filekey = filename. ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/scanpy/readwrite.py in _read(",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/547
https://github.com/scverse/scanpy/issues/547:880,testability,Trace,Traceback,880,"sc.datasets.moignard15() raises ValueError; ```python. import scanpy as sc. sc.datasets.moignard15(). ```. Output. ```python. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/pandas/io/parsers.py in _cast_types(self, values, cast_type, column). 1807 values = astype_nansafe(values, cast_type,. -> 1808 copy=True, skipna=True). 1809 except ValueError:. ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/pandas/core/dtypes/cast.py in astype_nansafe(arr, dtype, copy, skipna). 701 # Explicit copy, or required since NumPy can't view from / to object. --> 702 return arr.astype(dtype, copy=True). 703 . ValueError: could not convert string to float: '4SFGA6_247'. During handling of the above exception, another exception occurred:. ValueError Traceback (most recent call last). <ipython-input-3-bf986d1f9b8c> in <module>. 1 import scanpy as sc. ----> 2 sc.datasets.moignard15(). ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/scanpy/datasets/__init__.py in moignard15(). 104 filename = 'data/moignard15/nbt.3154-S3.xlsx'. 105 backup_url = 'http://www.nature.com/nbt/journal/v33/n3/extref/nbt.3154-S3.xlsx'. --> 106 adata = sc.read(filename, sheet='dCt_values.txt', cache=True, backup_url=backup_url). 107 # filter out 4 genes as in Haghverdi et al. (2016). 108 gene_subset = ~np.in1d(adata.var_names, ['Eif2b1', 'Mrpl19', 'Polr2a', 'Ubc']). ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/scanpy/readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs). 76 return _read(filename, backed=backed, sheet=sheet, ext=ext,. 77 delimiter=delimiter, first_column_names=first_column_names,. ---> 78 backup_url=backup_url, cache=cache, **kwargs). 79 # generate filename and read to dict. 80 filekey = filename. ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/scanpy/readwrite.py in _read(",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/547
https://github.com/scverse/scanpy/issues/547:6046,testability,log,logging,6046,"dtype, true_values, false_values, skiprows, nrows, na_values, verbose, parse_dates, date_parser, thousands, comment, skipfooter, convert_float, mangle_dupe_cols, **kwds). 601 **kwds). 602 . --> 603 output[asheetname] = parser.read(nrows=nrows). 604 . 605 if not squeeze or isinstance(output[asheetname], DataFrame):. ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/pandas/io/parsers.py in read(self, nrows). 1137 def read(self, nrows=None):. 1138 nrows = _validate_integer('nrows', nrows). -> 1139 ret = self._engine.read(nrows). 1140 . 1141 # May alter columns / col_dict. ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/pandas/io/parsers.py in read(self, rows). 2419 columns, data = self._do_date_conversions(columns, data). 2420 . -> 2421 data = self._convert_data(data). 2422 index, columns = self._make_index(data, alldata, columns, indexnamerow). 2423 . ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/pandas/io/parsers.py in _convert_data(self, data). 2485 return self._convert_to_ndarrays(data, clean_na_values,. 2486 clean_na_fvalues, self.verbose,. -> 2487 clean_conv, clean_dtypes). 2488 . 2489 def _infer_columns(self):. ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/pandas/io/parsers.py in _convert_to_ndarrays(self, dct, na_values, na_fvalues, verbose, converters, dtypes). 1703 # invalid input to is_bool_dtype. 1704 pass. -> 1705 cvals = self._cast_types(cvals, cast_type, c). 1706 . 1707 result[c] = cvals. ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/pandas/io/parsers.py in _cast_types(self, values, cast_type, column). 1809 except ValueError:. 1810 raise ValueError(""Unable to convert column %s to "". -> 1811 ""type %s"" % (column, cast_type)). 1812 return values. 1813 . ValueError: Unable to convert column Cell to type float32. ```. ```python. sc.logging.print_versions(). scanpy==1.4 anndata==0.6.18 numpy==1.15.4 scipy==1.2.0 pandas==0.24.2 scikit-learn==0.20.2 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 . ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/547
https://github.com/scverse/scanpy/issues/547:924,usability,input,input-,924,"sc.datasets.moignard15() raises ValueError; ```python. import scanpy as sc. sc.datasets.moignard15(). ```. Output. ```python. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/pandas/io/parsers.py in _cast_types(self, values, cast_type, column). 1807 values = astype_nansafe(values, cast_type,. -> 1808 copy=True, skipna=True). 1809 except ValueError:. ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/pandas/core/dtypes/cast.py in astype_nansafe(arr, dtype, copy, skipna). 701 # Explicit copy, or required since NumPy can't view from / to object. --> 702 return arr.astype(dtype, copy=True). 703 . ValueError: could not convert string to float: '4SFGA6_247'. During handling of the above exception, another exception occurred:. ValueError Traceback (most recent call last). <ipython-input-3-bf986d1f9b8c> in <module>. 1 import scanpy as sc. ----> 2 sc.datasets.moignard15(). ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/scanpy/datasets/__init__.py in moignard15(). 104 filename = 'data/moignard15/nbt.3154-S3.xlsx'. 105 backup_url = 'http://www.nature.com/nbt/journal/v33/n3/extref/nbt.3154-S3.xlsx'. --> 106 adata = sc.read(filename, sheet='dCt_values.txt', cache=True, backup_url=backup_url). 107 # filter out 4 genes as in Haghverdi et al. (2016). 108 gene_subset = ~np.in1d(adata.var_names, ['Eif2b1', 'Mrpl19', 'Polr2a', 'Ubc']). ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/scanpy/readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs). 76 return _read(filename, backed=backed, sheet=sheet, ext=ext,. 77 delimiter=delimiter, first_column_names=first_column_names,. ---> 78 backup_url=backup_url, cache=cache, **kwargs). 79 # generate filename and read to dict. 80 filekey = filename. ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/scanpy/readwrite.py in _read(",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/547
https://github.com/scverse/scanpy/issues/547:5568,usability,input,input,5568,"dtype, true_values, false_values, skiprows, nrows, na_values, verbose, parse_dates, date_parser, thousands, comment, skipfooter, convert_float, mangle_dupe_cols, **kwds). 601 **kwds). 602 . --> 603 output[asheetname] = parser.read(nrows=nrows). 604 . 605 if not squeeze or isinstance(output[asheetname], DataFrame):. ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/pandas/io/parsers.py in read(self, nrows). 1137 def read(self, nrows=None):. 1138 nrows = _validate_integer('nrows', nrows). -> 1139 ret = self._engine.read(nrows). 1140 . 1141 # May alter columns / col_dict. ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/pandas/io/parsers.py in read(self, rows). 2419 columns, data = self._do_date_conversions(columns, data). 2420 . -> 2421 data = self._convert_data(data). 2422 index, columns = self._make_index(data, alldata, columns, indexnamerow). 2423 . ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/pandas/io/parsers.py in _convert_data(self, data). 2485 return self._convert_to_ndarrays(data, clean_na_values,. 2486 clean_na_fvalues, self.verbose,. -> 2487 clean_conv, clean_dtypes). 2488 . 2489 def _infer_columns(self):. ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/pandas/io/parsers.py in _convert_to_ndarrays(self, dct, na_values, na_fvalues, verbose, converters, dtypes). 1703 # invalid input to is_bool_dtype. 1704 pass. -> 1705 cvals = self._cast_types(cvals, cast_type, c). 1706 . 1707 result[c] = cvals. ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/pandas/io/parsers.py in _cast_types(self, values, cast_type, column). 1809 except ValueError:. 1810 raise ValueError(""Unable to convert column %s to "". -> 1811 ""type %s"" % (column, cast_type)). 1812 return values. 1813 . ValueError: Unable to convert column Cell to type float32. ```. ```python. sc.logging.print_versions(). scanpy==1.4 anndata==0.6.18 numpy==1.15.4 scipy==1.2.0 pandas==0.24.2 scikit-learn==0.20.2 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 . ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/547
https://github.com/scverse/scanpy/issues/547:6149,usability,learn,learn,6149,"dtype, true_values, false_values, skiprows, nrows, na_values, verbose, parse_dates, date_parser, thousands, comment, skipfooter, convert_float, mangle_dupe_cols, **kwds). 601 **kwds). 602 . --> 603 output[asheetname] = parser.read(nrows=nrows). 604 . 605 if not squeeze or isinstance(output[asheetname], DataFrame):. ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/pandas/io/parsers.py in read(self, nrows). 1137 def read(self, nrows=None):. 1138 nrows = _validate_integer('nrows', nrows). -> 1139 ret = self._engine.read(nrows). 1140 . 1141 # May alter columns / col_dict. ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/pandas/io/parsers.py in read(self, rows). 2419 columns, data = self._do_date_conversions(columns, data). 2420 . -> 2421 data = self._convert_data(data). 2422 index, columns = self._make_index(data, alldata, columns, indexnamerow). 2423 . ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/pandas/io/parsers.py in _convert_data(self, data). 2485 return self._convert_to_ndarrays(data, clean_na_values,. 2486 clean_na_fvalues, self.verbose,. -> 2487 clean_conv, clean_dtypes). 2488 . 2489 def _infer_columns(self):. ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/pandas/io/parsers.py in _convert_to_ndarrays(self, dct, na_values, na_fvalues, verbose, converters, dtypes). 1703 # invalid input to is_bool_dtype. 1704 pass. -> 1705 cvals = self._cast_types(cvals, cast_type, c). 1706 . 1707 result[c] = cvals. ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/pandas/io/parsers.py in _cast_types(self, values, cast_type, column). 1809 except ValueError:. 1810 raise ValueError(""Unable to convert column %s to "". -> 1811 ""type %s"" % (column, cast_type)). 1812 return values. 1813 . ValueError: Unable to convert column Cell to type float32. ```. ```python. sc.logging.print_versions(). scanpy==1.4 anndata==0.6.18 numpy==1.15.4 scipy==1.2.0 pandas==0.24.2 scikit-learn==0.20.2 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 . ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/547
https://github.com/scverse/scanpy/issues/548:125,availability,cluster,clusters,125,"sc.pl.heatmap cannot show different colors for more than 20 cell types; I plot heatmap for normalized gene expression for 34 clusters. The code is as below:. sc.pl.heatmap(adata, marker_genes, use_raw=False, swap_axes=True, show=False, show_gene_labels=True, save=ofile, groupby='louvain', dendrogram=True). However, there are only 20 colors for the cluster bars. How can I deal with this issue? ![heatmap](https://user-images.githubusercontent.com/4293164/54787042-eeb41100-4c00-11e9-9c5a-fd6bf6f82650.png).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/548
https://github.com/scverse/scanpy/issues/548:350,availability,cluster,cluster,350,"sc.pl.heatmap cannot show different colors for more than 20 cell types; I plot heatmap for normalized gene expression for 34 clusters. The code is as below:. sc.pl.heatmap(adata, marker_genes, use_raw=False, swap_axes=True, show=False, show_gene_labels=True, save=ofile, groupby='louvain', dendrogram=True). However, there are only 20 colors for the cluster bars. How can I deal with this issue? ![heatmap](https://user-images.githubusercontent.com/4293164/54787042-eeb41100-4c00-11e9-9c5a-fd6bf6f82650.png).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/548
https://github.com/scverse/scanpy/issues/548:125,deployability,cluster,clusters,125,"sc.pl.heatmap cannot show different colors for more than 20 cell types; I plot heatmap for normalized gene expression for 34 clusters. The code is as below:. sc.pl.heatmap(adata, marker_genes, use_raw=False, swap_axes=True, show=False, show_gene_labels=True, save=ofile, groupby='louvain', dendrogram=True). However, there are only 20 colors for the cluster bars. How can I deal with this issue? ![heatmap](https://user-images.githubusercontent.com/4293164/54787042-eeb41100-4c00-11e9-9c5a-fd6bf6f82650.png).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/548
https://github.com/scverse/scanpy/issues/548:350,deployability,cluster,cluster,350,"sc.pl.heatmap cannot show different colors for more than 20 cell types; I plot heatmap for normalized gene expression for 34 clusters. The code is as below:. sc.pl.heatmap(adata, marker_genes, use_raw=False, swap_axes=True, show=False, show_gene_labels=True, save=ofile, groupby='louvain', dendrogram=True). However, there are only 20 colors for the cluster bars. How can I deal with this issue? ![heatmap](https://user-images.githubusercontent.com/4293164/54787042-eeb41100-4c00-11e9-9c5a-fd6bf6f82650.png).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/548
https://github.com/scverse/scanpy/issues/548:6,energy efficiency,heat,heatmap,6,"sc.pl.heatmap cannot show different colors for more than 20 cell types; I plot heatmap for normalized gene expression for 34 clusters. The code is as below:. sc.pl.heatmap(adata, marker_genes, use_raw=False, swap_axes=True, show=False, show_gene_labels=True, save=ofile, groupby='louvain', dendrogram=True). However, there are only 20 colors for the cluster bars. How can I deal with this issue? ![heatmap](https://user-images.githubusercontent.com/4293164/54787042-eeb41100-4c00-11e9-9c5a-fd6bf6f82650.png).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/548
https://github.com/scverse/scanpy/issues/548:79,energy efficiency,heat,heatmap,79,"sc.pl.heatmap cannot show different colors for more than 20 cell types; I plot heatmap for normalized gene expression for 34 clusters. The code is as below:. sc.pl.heatmap(adata, marker_genes, use_raw=False, swap_axes=True, show=False, show_gene_labels=True, save=ofile, groupby='louvain', dendrogram=True). However, there are only 20 colors for the cluster bars. How can I deal with this issue? ![heatmap](https://user-images.githubusercontent.com/4293164/54787042-eeb41100-4c00-11e9-9c5a-fd6bf6f82650.png).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/548
https://github.com/scverse/scanpy/issues/548:164,energy efficiency,heat,heatmap,164,"sc.pl.heatmap cannot show different colors for more than 20 cell types; I plot heatmap for normalized gene expression for 34 clusters. The code is as below:. sc.pl.heatmap(adata, marker_genes, use_raw=False, swap_axes=True, show=False, show_gene_labels=True, save=ofile, groupby='louvain', dendrogram=True). However, there are only 20 colors for the cluster bars. How can I deal with this issue? ![heatmap](https://user-images.githubusercontent.com/4293164/54787042-eeb41100-4c00-11e9-9c5a-fd6bf6f82650.png).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/548
https://github.com/scverse/scanpy/issues/548:398,energy efficiency,heat,heatmap,398,"sc.pl.heatmap cannot show different colors for more than 20 cell types; I plot heatmap for normalized gene expression for 34 clusters. The code is as below:. sc.pl.heatmap(adata, marker_genes, use_raw=False, swap_axes=True, show=False, show_gene_labels=True, save=ofile, groupby='louvain', dendrogram=True). However, there are only 20 colors for the cluster bars. How can I deal with this issue? ![heatmap](https://user-images.githubusercontent.com/4293164/54787042-eeb41100-4c00-11e9-9c5a-fd6bf6f82650.png).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/548
https://github.com/scverse/scanpy/issues/548:415,usability,user,user-images,415,"sc.pl.heatmap cannot show different colors for more than 20 cell types; I plot heatmap for normalized gene expression for 34 clusters. The code is as below:. sc.pl.heatmap(adata, marker_genes, use_raw=False, swap_axes=True, show=False, show_gene_labels=True, save=ofile, groupby='louvain', dendrogram=True). However, there are only 20 colors for the cluster bars. How can I deal with this issue? ![heatmap](https://user-images.githubusercontent.com/4293164/54787042-eeb41100-4c00-11e9-9c5a-fd6bf6f82650.png).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/548
https://github.com/scverse/scanpy/pull/549:460,availability,avail,available,460,"Marker gene overlap scoring function; Introduces a function to calculate marker gene overlaps between a reference set of marker genes provided as a dictionary, and data-derived marker genes as calculated by `sc.tl.rank_genes_groups()`. Currently implemented overlap functions are: overlap counts (with row or column normalization), overlap coefficient, and jaccard index. Still to do:. - write a test. - finish documentation. - allow p-value thresholding when available. - allow using top X marker genes rather than all calculated markers. - test that it works properly...",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/549
https://github.com/scverse/scanpy/pull/549:236,energy efficiency,Current,Currently,236,"Marker gene overlap scoring function; Introduces a function to calculate marker gene overlaps between a reference set of marker genes provided as a dictionary, and data-derived marker genes as calculated by `sc.tl.rank_genes_groups()`. Currently implemented overlap functions are: overlap counts (with row or column normalization), overlap coefficient, and jaccard index. Still to do:. - write a test. - finish documentation. - allow p-value thresholding when available. - allow using top X marker genes rather than all calculated markers. - test that it works properly...",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/549
https://github.com/scverse/scanpy/pull/549:460,reliability,availab,available,460,"Marker gene overlap scoring function; Introduces a function to calculate marker gene overlaps between a reference set of marker genes provided as a dictionary, and data-derived marker genes as calculated by `sc.tl.rank_genes_groups()`. Currently implemented overlap functions are: overlap counts (with row or column normalization), overlap coefficient, and jaccard index. Still to do:. - write a test. - finish documentation. - allow p-value thresholding when available. - allow using top X marker genes rather than all calculated markers. - test that it works properly...",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/549
https://github.com/scverse/scanpy/pull/549:396,safety,test,test,396,"Marker gene overlap scoring function; Introduces a function to calculate marker gene overlaps between a reference set of marker genes provided as a dictionary, and data-derived marker genes as calculated by `sc.tl.rank_genes_groups()`. Currently implemented overlap functions are: overlap counts (with row or column normalization), overlap coefficient, and jaccard index. Still to do:. - write a test. - finish documentation. - allow p-value thresholding when available. - allow using top X marker genes rather than all calculated markers. - test that it works properly...",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/549
https://github.com/scverse/scanpy/pull/549:460,safety,avail,available,460,"Marker gene overlap scoring function; Introduces a function to calculate marker gene overlaps between a reference set of marker genes provided as a dictionary, and data-derived marker genes as calculated by `sc.tl.rank_genes_groups()`. Currently implemented overlap functions are: overlap counts (with row or column normalization), overlap coefficient, and jaccard index. Still to do:. - write a test. - finish documentation. - allow p-value thresholding when available. - allow using top X marker genes rather than all calculated markers. - test that it works properly...",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/549
https://github.com/scverse/scanpy/pull/549:542,safety,test,test,542,"Marker gene overlap scoring function; Introduces a function to calculate marker gene overlaps between a reference set of marker genes provided as a dictionary, and data-derived marker genes as calculated by `sc.tl.rank_genes_groups()`. Currently implemented overlap functions are: overlap counts (with row or column normalization), overlap coefficient, and jaccard index. Still to do:. - write a test. - finish documentation. - allow p-value thresholding when available. - allow using top X marker genes rather than all calculated markers. - test that it works properly...",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/549
https://github.com/scverse/scanpy/pull/549:460,security,availab,available,460,"Marker gene overlap scoring function; Introduces a function to calculate marker gene overlaps between a reference set of marker genes provided as a dictionary, and data-derived marker genes as calculated by `sc.tl.rank_genes_groups()`. Currently implemented overlap functions are: overlap counts (with row or column normalization), overlap coefficient, and jaccard index. Still to do:. - write a test. - finish documentation. - allow p-value thresholding when available. - allow using top X marker genes rather than all calculated markers. - test that it works properly...",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/549
https://github.com/scverse/scanpy/pull/549:396,testability,test,test,396,"Marker gene overlap scoring function; Introduces a function to calculate marker gene overlaps between a reference set of marker genes provided as a dictionary, and data-derived marker genes as calculated by `sc.tl.rank_genes_groups()`. Currently implemented overlap functions are: overlap counts (with row or column normalization), overlap coefficient, and jaccard index. Still to do:. - write a test. - finish documentation. - allow p-value thresholding when available. - allow using top X marker genes rather than all calculated markers. - test that it works properly...",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/549
https://github.com/scverse/scanpy/pull/549:542,testability,test,test,542,"Marker gene overlap scoring function; Introduces a function to calculate marker gene overlaps between a reference set of marker genes provided as a dictionary, and data-derived marker genes as calculated by `sc.tl.rank_genes_groups()`. Currently implemented overlap functions are: overlap counts (with row or column normalization), overlap coefficient, and jaccard index. Still to do:. - write a test. - finish documentation. - allow p-value thresholding when available. - allow using top X marker genes rather than all calculated markers. - test that it works properly...",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/549
https://github.com/scverse/scanpy/pull/549:411,usability,document,documentation,411,"Marker gene overlap scoring function; Introduces a function to calculate marker gene overlaps between a reference set of marker genes provided as a dictionary, and data-derived marker genes as calculated by `sc.tl.rank_genes_groups()`. Currently implemented overlap functions are: overlap counts (with row or column normalization), overlap coefficient, and jaccard index. Still to do:. - write a test. - finish documentation. - allow p-value thresholding when available. - allow using top X marker genes rather than all calculated markers. - test that it works properly...",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/549
https://github.com/scverse/scanpy/issues/550:127,availability,cluster,clusters,127,"Heatmap does not respect Ordered Categorical; I am trying to plot a heatmap to visualize genes that peak at different temporal clusters. I am passing the `groupby` parameter as the original leiden label transformed into an Ordered Categorical. However, the `pl.heatmap` API displays the clusters in alphabetic order. My code and the relative results follows:. ```python. # Order of the lineage. lin = ('0', '3', '1', '2', '6', '4'). # Genes to plot. top = ['Malat1', 'Ctdnep1', 'Atxn10', 'Fhl2', 'Etf1']. # Reordering leiden labels using Categorical data type. adata.obs['leiden_res0.6'] = adata.obs['leiden_res0.6'].cat.reorder_categories(list(lin), ordered=True). # Plotting. sc.pl.heatmap(adata, var_names=top, groupby='leiden_res0.6', cmap='viridis'). ```. ![pt](https://user-images.githubusercontent.com/697622/54848355-72c8d000-4cb7-11e9-9f87-beb632140ce8.png)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/550
https://github.com/scverse/scanpy/issues/550:287,availability,cluster,clusters,287,"Heatmap does not respect Ordered Categorical; I am trying to plot a heatmap to visualize genes that peak at different temporal clusters. I am passing the `groupby` parameter as the original leiden label transformed into an Ordered Categorical. However, the `pl.heatmap` API displays the clusters in alphabetic order. My code and the relative results follows:. ```python. # Order of the lineage. lin = ('0', '3', '1', '2', '6', '4'). # Genes to plot. top = ['Malat1', 'Ctdnep1', 'Atxn10', 'Fhl2', 'Etf1']. # Reordering leiden labels using Categorical data type. adata.obs['leiden_res0.6'] = adata.obs['leiden_res0.6'].cat.reorder_categories(list(lin), ordered=True). # Plotting. sc.pl.heatmap(adata, var_names=top, groupby='leiden_res0.6', cmap='viridis'). ```. ![pt](https://user-images.githubusercontent.com/697622/54848355-72c8d000-4cb7-11e9-9f87-beb632140ce8.png)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/550
https://github.com/scverse/scanpy/issues/550:127,deployability,cluster,clusters,127,"Heatmap does not respect Ordered Categorical; I am trying to plot a heatmap to visualize genes that peak at different temporal clusters. I am passing the `groupby` parameter as the original leiden label transformed into an Ordered Categorical. However, the `pl.heatmap` API displays the clusters in alphabetic order. My code and the relative results follows:. ```python. # Order of the lineage. lin = ('0', '3', '1', '2', '6', '4'). # Genes to plot. top = ['Malat1', 'Ctdnep1', 'Atxn10', 'Fhl2', 'Etf1']. # Reordering leiden labels using Categorical data type. adata.obs['leiden_res0.6'] = adata.obs['leiden_res0.6'].cat.reorder_categories(list(lin), ordered=True). # Plotting. sc.pl.heatmap(adata, var_names=top, groupby='leiden_res0.6', cmap='viridis'). ```. ![pt](https://user-images.githubusercontent.com/697622/54848355-72c8d000-4cb7-11e9-9f87-beb632140ce8.png)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/550
https://github.com/scverse/scanpy/issues/550:270,deployability,API,API,270,"Heatmap does not respect Ordered Categorical; I am trying to plot a heatmap to visualize genes that peak at different temporal clusters. I am passing the `groupby` parameter as the original leiden label transformed into an Ordered Categorical. However, the `pl.heatmap` API displays the clusters in alphabetic order. My code and the relative results follows:. ```python. # Order of the lineage. lin = ('0', '3', '1', '2', '6', '4'). # Genes to plot. top = ['Malat1', 'Ctdnep1', 'Atxn10', 'Fhl2', 'Etf1']. # Reordering leiden labels using Categorical data type. adata.obs['leiden_res0.6'] = adata.obs['leiden_res0.6'].cat.reorder_categories(list(lin), ordered=True). # Plotting. sc.pl.heatmap(adata, var_names=top, groupby='leiden_res0.6', cmap='viridis'). ```. ![pt](https://user-images.githubusercontent.com/697622/54848355-72c8d000-4cb7-11e9-9f87-beb632140ce8.png)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/550
https://github.com/scverse/scanpy/issues/550:287,deployability,cluster,clusters,287,"Heatmap does not respect Ordered Categorical; I am trying to plot a heatmap to visualize genes that peak at different temporal clusters. I am passing the `groupby` parameter as the original leiden label transformed into an Ordered Categorical. However, the `pl.heatmap` API displays the clusters in alphabetic order. My code and the relative results follows:. ```python. # Order of the lineage. lin = ('0', '3', '1', '2', '6', '4'). # Genes to plot. top = ['Malat1', 'Ctdnep1', 'Atxn10', 'Fhl2', 'Etf1']. # Reordering leiden labels using Categorical data type. adata.obs['leiden_res0.6'] = adata.obs['leiden_res0.6'].cat.reorder_categories(list(lin), ordered=True). # Plotting. sc.pl.heatmap(adata, var_names=top, groupby='leiden_res0.6', cmap='viridis'). ```. ![pt](https://user-images.githubusercontent.com/697622/54848355-72c8d000-4cb7-11e9-9f87-beb632140ce8.png)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/550
https://github.com/scverse/scanpy/issues/550:0,energy efficiency,Heat,Heatmap,0,"Heatmap does not respect Ordered Categorical; I am trying to plot a heatmap to visualize genes that peak at different temporal clusters. I am passing the `groupby` parameter as the original leiden label transformed into an Ordered Categorical. However, the `pl.heatmap` API displays the clusters in alphabetic order. My code and the relative results follows:. ```python. # Order of the lineage. lin = ('0', '3', '1', '2', '6', '4'). # Genes to plot. top = ['Malat1', 'Ctdnep1', 'Atxn10', 'Fhl2', 'Etf1']. # Reordering leiden labels using Categorical data type. adata.obs['leiden_res0.6'] = adata.obs['leiden_res0.6'].cat.reorder_categories(list(lin), ordered=True). # Plotting. sc.pl.heatmap(adata, var_names=top, groupby='leiden_res0.6', cmap='viridis'). ```. ![pt](https://user-images.githubusercontent.com/697622/54848355-72c8d000-4cb7-11e9-9f87-beb632140ce8.png)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/550
https://github.com/scverse/scanpy/issues/550:68,energy efficiency,heat,heatmap,68,"Heatmap does not respect Ordered Categorical; I am trying to plot a heatmap to visualize genes that peak at different temporal clusters. I am passing the `groupby` parameter as the original leiden label transformed into an Ordered Categorical. However, the `pl.heatmap` API displays the clusters in alphabetic order. My code and the relative results follows:. ```python. # Order of the lineage. lin = ('0', '3', '1', '2', '6', '4'). # Genes to plot. top = ['Malat1', 'Ctdnep1', 'Atxn10', 'Fhl2', 'Etf1']. # Reordering leiden labels using Categorical data type. adata.obs['leiden_res0.6'] = adata.obs['leiden_res0.6'].cat.reorder_categories(list(lin), ordered=True). # Plotting. sc.pl.heatmap(adata, var_names=top, groupby='leiden_res0.6', cmap='viridis'). ```. ![pt](https://user-images.githubusercontent.com/697622/54848355-72c8d000-4cb7-11e9-9f87-beb632140ce8.png)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/550
https://github.com/scverse/scanpy/issues/550:261,energy efficiency,heat,heatmap,261,"Heatmap does not respect Ordered Categorical; I am trying to plot a heatmap to visualize genes that peak at different temporal clusters. I am passing the `groupby` parameter as the original leiden label transformed into an Ordered Categorical. However, the `pl.heatmap` API displays the clusters in alphabetic order. My code and the relative results follows:. ```python. # Order of the lineage. lin = ('0', '3', '1', '2', '6', '4'). # Genes to plot. top = ['Malat1', 'Ctdnep1', 'Atxn10', 'Fhl2', 'Etf1']. # Reordering leiden labels using Categorical data type. adata.obs['leiden_res0.6'] = adata.obs['leiden_res0.6'].cat.reorder_categories(list(lin), ordered=True). # Plotting. sc.pl.heatmap(adata, var_names=top, groupby='leiden_res0.6', cmap='viridis'). ```. ![pt](https://user-images.githubusercontent.com/697622/54848355-72c8d000-4cb7-11e9-9f87-beb632140ce8.png)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/550
https://github.com/scverse/scanpy/issues/550:684,energy efficiency,heat,heatmap,684,"Heatmap does not respect Ordered Categorical; I am trying to plot a heatmap to visualize genes that peak at different temporal clusters. I am passing the `groupby` parameter as the original leiden label transformed into an Ordered Categorical. However, the `pl.heatmap` API displays the clusters in alphabetic order. My code and the relative results follows:. ```python. # Order of the lineage. lin = ('0', '3', '1', '2', '6', '4'). # Genes to plot. top = ['Malat1', 'Ctdnep1', 'Atxn10', 'Fhl2', 'Etf1']. # Reordering leiden labels using Categorical data type. adata.obs['leiden_res0.6'] = adata.obs['leiden_res0.6'].cat.reorder_categories(list(lin), ordered=True). # Plotting. sc.pl.heatmap(adata, var_names=top, groupby='leiden_res0.6', cmap='viridis'). ```. ![pt](https://user-images.githubusercontent.com/697622/54848355-72c8d000-4cb7-11e9-9f87-beb632140ce8.png)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/550
https://github.com/scverse/scanpy/issues/550:203,integrability,transform,transformed,203,"Heatmap does not respect Ordered Categorical; I am trying to plot a heatmap to visualize genes that peak at different temporal clusters. I am passing the `groupby` parameter as the original leiden label transformed into an Ordered Categorical. However, the `pl.heatmap` API displays the clusters in alphabetic order. My code and the relative results follows:. ```python. # Order of the lineage. lin = ('0', '3', '1', '2', '6', '4'). # Genes to plot. top = ['Malat1', 'Ctdnep1', 'Atxn10', 'Fhl2', 'Etf1']. # Reordering leiden labels using Categorical data type. adata.obs['leiden_res0.6'] = adata.obs['leiden_res0.6'].cat.reorder_categories(list(lin), ordered=True). # Plotting. sc.pl.heatmap(adata, var_names=top, groupby='leiden_res0.6', cmap='viridis'). ```. ![pt](https://user-images.githubusercontent.com/697622/54848355-72c8d000-4cb7-11e9-9f87-beb632140ce8.png)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/550
https://github.com/scverse/scanpy/issues/550:270,integrability,API,API,270,"Heatmap does not respect Ordered Categorical; I am trying to plot a heatmap to visualize genes that peak at different temporal clusters. I am passing the `groupby` parameter as the original leiden label transformed into an Ordered Categorical. However, the `pl.heatmap` API displays the clusters in alphabetic order. My code and the relative results follows:. ```python. # Order of the lineage. lin = ('0', '3', '1', '2', '6', '4'). # Genes to plot. top = ['Malat1', 'Ctdnep1', 'Atxn10', 'Fhl2', 'Etf1']. # Reordering leiden labels using Categorical data type. adata.obs['leiden_res0.6'] = adata.obs['leiden_res0.6'].cat.reorder_categories(list(lin), ordered=True). # Plotting. sc.pl.heatmap(adata, var_names=top, groupby='leiden_res0.6', cmap='viridis'). ```. ![pt](https://user-images.githubusercontent.com/697622/54848355-72c8d000-4cb7-11e9-9f87-beb632140ce8.png)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/550
https://github.com/scverse/scanpy/issues/550:203,interoperability,transform,transformed,203,"Heatmap does not respect Ordered Categorical; I am trying to plot a heatmap to visualize genes that peak at different temporal clusters. I am passing the `groupby` parameter as the original leiden label transformed into an Ordered Categorical. However, the `pl.heatmap` API displays the clusters in alphabetic order. My code and the relative results follows:. ```python. # Order of the lineage. lin = ('0', '3', '1', '2', '6', '4'). # Genes to plot. top = ['Malat1', 'Ctdnep1', 'Atxn10', 'Fhl2', 'Etf1']. # Reordering leiden labels using Categorical data type. adata.obs['leiden_res0.6'] = adata.obs['leiden_res0.6'].cat.reorder_categories(list(lin), ordered=True). # Plotting. sc.pl.heatmap(adata, var_names=top, groupby='leiden_res0.6', cmap='viridis'). ```. ![pt](https://user-images.githubusercontent.com/697622/54848355-72c8d000-4cb7-11e9-9f87-beb632140ce8.png)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/550
https://github.com/scverse/scanpy/issues/550:270,interoperability,API,API,270,"Heatmap does not respect Ordered Categorical; I am trying to plot a heatmap to visualize genes that peak at different temporal clusters. I am passing the `groupby` parameter as the original leiden label transformed into an Ordered Categorical. However, the `pl.heatmap` API displays the clusters in alphabetic order. My code and the relative results follows:. ```python. # Order of the lineage. lin = ('0', '3', '1', '2', '6', '4'). # Genes to plot. top = ['Malat1', 'Ctdnep1', 'Atxn10', 'Fhl2', 'Etf1']. # Reordering leiden labels using Categorical data type. adata.obs['leiden_res0.6'] = adata.obs['leiden_res0.6'].cat.reorder_categories(list(lin), ordered=True). # Plotting. sc.pl.heatmap(adata, var_names=top, groupby='leiden_res0.6', cmap='viridis'). ```. ![pt](https://user-images.githubusercontent.com/697622/54848355-72c8d000-4cb7-11e9-9f87-beb632140ce8.png)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/550
https://github.com/scverse/scanpy/issues/550:164,modifiability,paramet,parameter,164,"Heatmap does not respect Ordered Categorical; I am trying to plot a heatmap to visualize genes that peak at different temporal clusters. I am passing the `groupby` parameter as the original leiden label transformed into an Ordered Categorical. However, the `pl.heatmap` API displays the clusters in alphabetic order. My code and the relative results follows:. ```python. # Order of the lineage. lin = ('0', '3', '1', '2', '6', '4'). # Genes to plot. top = ['Malat1', 'Ctdnep1', 'Atxn10', 'Fhl2', 'Etf1']. # Reordering leiden labels using Categorical data type. adata.obs['leiden_res0.6'] = adata.obs['leiden_res0.6'].cat.reorder_categories(list(lin), ordered=True). # Plotting. sc.pl.heatmap(adata, var_names=top, groupby='leiden_res0.6', cmap='viridis'). ```. ![pt](https://user-images.githubusercontent.com/697622/54848355-72c8d000-4cb7-11e9-9f87-beb632140ce8.png)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/550
https://github.com/scverse/scanpy/issues/550:8,reliability,doe,does,8,"Heatmap does not respect Ordered Categorical; I am trying to plot a heatmap to visualize genes that peak at different temporal clusters. I am passing the `groupby` parameter as the original leiden label transformed into an Ordered Categorical. However, the `pl.heatmap` API displays the clusters in alphabetic order. My code and the relative results follows:. ```python. # Order of the lineage. lin = ('0', '3', '1', '2', '6', '4'). # Genes to plot. top = ['Malat1', 'Ctdnep1', 'Atxn10', 'Fhl2', 'Etf1']. # Reordering leiden labels using Categorical data type. adata.obs['leiden_res0.6'] = adata.obs['leiden_res0.6'].cat.reorder_categories(list(lin), ordered=True). # Plotting. sc.pl.heatmap(adata, var_names=top, groupby='leiden_res0.6', cmap='viridis'). ```. ![pt](https://user-images.githubusercontent.com/697622/54848355-72c8d000-4cb7-11e9-9f87-beb632140ce8.png)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/550
https://github.com/scverse/scanpy/issues/550:386,security,lineag,lineage,386,"Heatmap does not respect Ordered Categorical; I am trying to plot a heatmap to visualize genes that peak at different temporal clusters. I am passing the `groupby` parameter as the original leiden label transformed into an Ordered Categorical. However, the `pl.heatmap` API displays the clusters in alphabetic order. My code and the relative results follows:. ```python. # Order of the lineage. lin = ('0', '3', '1', '2', '6', '4'). # Genes to plot. top = ['Malat1', 'Ctdnep1', 'Atxn10', 'Fhl2', 'Etf1']. # Reordering leiden labels using Categorical data type. adata.obs['leiden_res0.6'] = adata.obs['leiden_res0.6'].cat.reorder_categories(list(lin), ordered=True). # Plotting. sc.pl.heatmap(adata, var_names=top, groupby='leiden_res0.6', cmap='viridis'). ```. ![pt](https://user-images.githubusercontent.com/697622/54848355-72c8d000-4cb7-11e9-9f87-beb632140ce8.png)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/550
https://github.com/scverse/scanpy/issues/550:79,usability,visual,visualize,79,"Heatmap does not respect Ordered Categorical; I am trying to plot a heatmap to visualize genes that peak at different temporal clusters. I am passing the `groupby` parameter as the original leiden label transformed into an Ordered Categorical. However, the `pl.heatmap` API displays the clusters in alphabetic order. My code and the relative results follows:. ```python. # Order of the lineage. lin = ('0', '3', '1', '2', '6', '4'). # Genes to plot. top = ['Malat1', 'Ctdnep1', 'Atxn10', 'Fhl2', 'Etf1']. # Reordering leiden labels using Categorical data type. adata.obs['leiden_res0.6'] = adata.obs['leiden_res0.6'].cat.reorder_categories(list(lin), ordered=True). # Plotting. sc.pl.heatmap(adata, var_names=top, groupby='leiden_res0.6', cmap='viridis'). ```. ![pt](https://user-images.githubusercontent.com/697622/54848355-72c8d000-4cb7-11e9-9f87-beb632140ce8.png)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/550
https://github.com/scverse/scanpy/issues/550:775,usability,user,user-images,775,"Heatmap does not respect Ordered Categorical; I am trying to plot a heatmap to visualize genes that peak at different temporal clusters. I am passing the `groupby` parameter as the original leiden label transformed into an Ordered Categorical. However, the `pl.heatmap` API displays the clusters in alphabetic order. My code and the relative results follows:. ```python. # Order of the lineage. lin = ('0', '3', '1', '2', '6', '4'). # Genes to plot. top = ['Malat1', 'Ctdnep1', 'Atxn10', 'Fhl2', 'Etf1']. # Reordering leiden labels using Categorical data type. adata.obs['leiden_res0.6'] = adata.obs['leiden_res0.6'].cat.reorder_categories(list(lin), ordered=True). # Plotting. sc.pl.heatmap(adata, var_names=top, groupby='leiden_res0.6', cmap='viridis'). ```. ![pt](https://user-images.githubusercontent.com/697622/54848355-72c8d000-4cb7-11e9-9f87-beb632140ce8.png)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/550
https://github.com/scverse/scanpy/pull/551:119,deployability,version,version,119,[WIP] Make log1p work with integers/ output of downsample_counts; Addressing #435 . I've opened this with a not-pretty version. I think this function could be simplified a lot by not requiring it to work on arrays. I'm willing to leave it ugly for now. Left to resolve: return type of `downsample_counts`.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/551
https://github.com/scverse/scanpy/pull/551:119,integrability,version,version,119,[WIP] Make log1p work with integers/ output of downsample_counts; Addressing #435 . I've opened this with a not-pretty version. I think this function could be simplified a lot by not requiring it to work on arrays. I'm willing to leave it ugly for now. Left to resolve: return type of `downsample_counts`.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/551
https://github.com/scverse/scanpy/pull/551:119,modifiability,version,version,119,[WIP] Make log1p work with integers/ output of downsample_counts; Addressing #435 . I've opened this with a not-pretty version. I think this function could be simplified a lot by not requiring it to work on arrays. I'm willing to leave it ugly for now. Left to resolve: return type of `downsample_counts`.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/551
https://github.com/scverse/scanpy/pull/551:159,testability,simpl,simplified,159,[WIP] Make log1p work with integers/ output of downsample_counts; Addressing #435 . I've opened this with a not-pretty version. I think this function could be simplified a lot by not requiring it to work on arrays. I'm willing to leave it ugly for now. Left to resolve: return type of `downsample_counts`.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/551
https://github.com/scverse/scanpy/pull/551:159,usability,simpl,simplified,159,[WIP] Make log1p work with integers/ output of downsample_counts; Addressing #435 . I've opened this with a not-pretty version. I think this function could be simplified a lot by not requiring it to work on arrays. I'm willing to leave it ugly for now. Left to resolve: return type of `downsample_counts`.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/551
https://github.com/scverse/scanpy/pull/552:48,deployability,log,logg,48,Replace stdout and stderr prints in ComBat with logg.info;,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/552
https://github.com/scverse/scanpy/pull/552:48,safety,log,logg,48,Replace stdout and stderr prints in ComBat with logg.info;,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/552
https://github.com/scverse/scanpy/pull/552:48,security,log,logg,48,Replace stdout and stderr prints in ComBat with logg.info;,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/552
https://github.com/scverse/scanpy/pull/552:48,testability,log,logg,48,Replace stdout and stderr prints in ComBat with logg.info;,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/552
https://github.com/scverse/scanpy/pull/553:96,interoperability,share,share-link,96,Added gitter; added gitter link. [scanpyhelp](https://gitter.im/scanpyhelp/community?utm_source=share-link&utm_medium=link&utm_campaign=share-link). I created a gitter.im scanpy community chat room for users to more rapidly ask questions that are more appropriate to ask in a forum-style rather than filing issues,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/553
https://github.com/scverse/scanpy/pull/553:136,interoperability,share,share-link,136,Added gitter; added gitter link. [scanpyhelp](https://gitter.im/scanpyhelp/community?utm_source=share-link&utm_medium=link&utm_campaign=share-link). I created a gitter.im scanpy community chat room for users to more rapidly ask questions that are more appropriate to ask in a forum-style rather than filing issues,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/553
https://github.com/scverse/scanpy/pull/553:202,usability,user,users,202,Added gitter; added gitter link. [scanpyhelp](https://gitter.im/scanpyhelp/community?utm_source=share-link&utm_medium=link&utm_campaign=share-link). I created a gitter.im scanpy community chat room for users to more rapidly ask questions that are more appropriate to ask in a forum-style rather than filing issues,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/553
https://github.com/scverse/scanpy/pull/555:0,modifiability,Layer,Layers,0,"Layers to scatter; This PR adds the argument `layer` to the different scatter plot functions like `.pl.umap`, `pl.tsne` etc. This partially addresses #458",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/555
https://github.com/scverse/scanpy/pull/555:46,modifiability,layer,layer,46,"Layers to scatter; This PR adds the argument `layer` to the different scatter plot functions like `.pl.umap`, `pl.tsne` etc. This partially addresses #458",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/555
https://github.com/scverse/scanpy/pull/556:33,availability,redund,redundant,33,Some style fixes; Every piece of redundant code we delete is one we don’t have to maintain.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/556
https://github.com/scverse/scanpy/pull/556:33,deployability,redundan,redundant,33,Some style fixes; Every piece of redundant code we delete is one we don’t have to maintain.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/556
https://github.com/scverse/scanpy/pull/556:82,modifiability,maintain,maintain,82,Some style fixes; Every piece of redundant code we delete is one we don’t have to maintain.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/556
https://github.com/scverse/scanpy/pull/556:33,reliability,redundan,redundant,33,Some style fixes; Every piece of redundant code we delete is one we don’t have to maintain.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/556
https://github.com/scverse/scanpy/pull/556:33,safety,redund,redundant,33,Some style fixes; Every piece of redundant code we delete is one we don’t have to maintain.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/556
https://github.com/scverse/scanpy/pull/556:82,safety,maintain,maintain,82,Some style fixes; Every piece of redundant code we delete is one we don’t have to maintain.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/556
https://github.com/scverse/scanpy/issues/558:71,availability,down,downloader,71,"Where should `sc.datasets` put data?; I'm adding that expression atlas downloader now (#489), and wondering where the files should go. `pbmc68k_reduced` and `toggleswitch` put the datasets relative to where scanpy is installed (via `__file__`). All other functions place the data relative to where the python process was started. While I like not storing the same files all over a filesystem, I'm not sure in the `scanpy` installation directory is the right place to be storing data. Thoughts?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:158,deployability,toggl,toggleswitch,158,"Where should `sc.datasets` put data?; I'm adding that expression atlas downloader now (#489), and wondering where the files should go. `pbmc68k_reduced` and `toggleswitch` put the datasets relative to where scanpy is installed (via `__file__`). All other functions place the data relative to where the python process was started. While I like not storing the same files all over a filesystem, I'm not sure in the `scanpy` installation directory is the right place to be storing data. Thoughts?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:217,deployability,instal,installed,217,"Where should `sc.datasets` put data?; I'm adding that expression atlas downloader now (#489), and wondering where the files should go. `pbmc68k_reduced` and `toggleswitch` put the datasets relative to where scanpy is installed (via `__file__`). All other functions place the data relative to where the python process was started. While I like not storing the same files all over a filesystem, I'm not sure in the `scanpy` installation directory is the right place to be storing data. Thoughts?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/558:422,deployability,instal,installation,422,"Where should `sc.datasets` put data?; I'm adding that expression atlas downloader now (#489), and wondering where the files should go. `pbmc68k_reduced` and `toggleswitch` put the datasets relative to where scanpy is installed (via `__file__`). All other functions place the data relative to where the python process was started. While I like not storing the same files all over a filesystem, I'm not sure in the `scanpy` installation directory is the right place to be storing data. Thoughts?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558
https://github.com/scverse/scanpy/issues/559:445,availability,error,error,445,"matrixplot - standard_scale issue; Hi @fidelram ,. When I try to use . ```. plt = sc.pl.matrixplot(adata, marker_genes, groupby='louvain'). ```. I get this heatmap. . ![louv1](https://user-images.githubusercontent.com/11874103/54995344-d2c8ba80-4fc6-11e9-84fe-4f659915293d.png). But as soon as I add ```standard_scale='var'```:. ```. plt = sc.pl.matrixplot(adata, marker_genes, groupby='louvain', standard_scale='var'). ```. I get the following error. ```. ---------------------------------------------------------------------------. AttributeError Traceback (most recent call last). <ipython-input-24-4ac38158d4d0> in <module>. ----> 1 plt = sc.pl.matrixplot(adata, marker_genes, groupby='louvain', standard_scale='var'). [...]/lib/python3.6/site-packages/scanpy/plotting/_anndata.py in matrixplot(adata, var_names, groupby, use_raw, log, num_categories, figsize, dendrogram, var_group_positions, var_group_labels, var_group_rotation, layer, swap_axes, show, save, **kwds). 1683 _plot_dendrogram(dendro_ax, adata, ticks=y_ticks). 1684 . -> 1685 pc = matrix_ax.pcolor(mean_obs, edgecolor='gray', **kwds). 1686 . 1687 # invert y axis to show categories ordered from top to bottom. [...]/lib/python3.6/site-packages/matplotlib/__init__.py in inner(ax, data, *args, **kwargs). 1803 ""the Matplotlib list!)"" % (label_namer, func.__name__),. 1804 RuntimeWarning, stacklevel=2). -> 1805 return func(ax, *args, **kwargs). 1806 . 1807 inner.__doc__ = _add_data_doc(inner.__doc__,. [...]/lib/python3.6/site-packages/matplotlib/axes/_axes.py in pcolor(self, alpha, norm, cmap, vmin, vmax, *args, **kwargs). 5762 kwargs.setdefault('snap', False). 5763 . -> 5764 collection = mcoll.PolyCollection(verts, **kwargs). 5765 . 5766 collection.set_alpha(alpha). [...]/lib/python3.6/site-packages/matplotlib/collections.py in __init__(self, verts, sizes, closed, **kwargs). 931 %(Collection)s. 932 """""". --> 933 Collection.__init__(self, **kwargs). 934 self.set_sizes(sizes). 935 self.set_verts(verts, closed). [...]/lib/",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/559
https://github.com/scverse/scanpy/issues/559:620,deployability,modul,module,620,"matrixplot - standard_scale issue; Hi @fidelram ,. When I try to use . ```. plt = sc.pl.matrixplot(adata, marker_genes, groupby='louvain'). ```. I get this heatmap. . ![louv1](https://user-images.githubusercontent.com/11874103/54995344-d2c8ba80-4fc6-11e9-84fe-4f659915293d.png). But as soon as I add ```standard_scale='var'```:. ```. plt = sc.pl.matrixplot(adata, marker_genes, groupby='louvain', standard_scale='var'). ```. I get the following error. ```. ---------------------------------------------------------------------------. AttributeError Traceback (most recent call last). <ipython-input-24-4ac38158d4d0> in <module>. ----> 1 plt = sc.pl.matrixplot(adata, marker_genes, groupby='louvain', standard_scale='var'). [...]/lib/python3.6/site-packages/scanpy/plotting/_anndata.py in matrixplot(adata, var_names, groupby, use_raw, log, num_categories, figsize, dendrogram, var_group_positions, var_group_labels, var_group_rotation, layer, swap_axes, show, save, **kwds). 1683 _plot_dendrogram(dendro_ax, adata, ticks=y_ticks). 1684 . -> 1685 pc = matrix_ax.pcolor(mean_obs, edgecolor='gray', **kwds). 1686 . 1687 # invert y axis to show categories ordered from top to bottom. [...]/lib/python3.6/site-packages/matplotlib/__init__.py in inner(ax, data, *args, **kwargs). 1803 ""the Matplotlib list!)"" % (label_namer, func.__name__),. 1804 RuntimeWarning, stacklevel=2). -> 1805 return func(ax, *args, **kwargs). 1806 . 1807 inner.__doc__ = _add_data_doc(inner.__doc__,. [...]/lib/python3.6/site-packages/matplotlib/axes/_axes.py in pcolor(self, alpha, norm, cmap, vmin, vmax, *args, **kwargs). 5762 kwargs.setdefault('snap', False). 5763 . -> 5764 collection = mcoll.PolyCollection(verts, **kwargs). 5765 . 5766 collection.set_alpha(alpha). [...]/lib/python3.6/site-packages/matplotlib/collections.py in __init__(self, verts, sizes, closed, **kwargs). 931 %(Collection)s. 932 """""". --> 933 Collection.__init__(self, **kwargs). 934 self.set_sizes(sizes). 935 self.set_verts(verts, closed). [...]/lib/",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/559
https://github.com/scverse/scanpy/issues/559:835,deployability,log,log,835,"matrixplot - standard_scale issue; Hi @fidelram ,. When I try to use . ```. plt = sc.pl.matrixplot(adata, marker_genes, groupby='louvain'). ```. I get this heatmap. . ![louv1](https://user-images.githubusercontent.com/11874103/54995344-d2c8ba80-4fc6-11e9-84fe-4f659915293d.png). But as soon as I add ```standard_scale='var'```:. ```. plt = sc.pl.matrixplot(adata, marker_genes, groupby='louvain', standard_scale='var'). ```. I get the following error. ```. ---------------------------------------------------------------------------. AttributeError Traceback (most recent call last). <ipython-input-24-4ac38158d4d0> in <module>. ----> 1 plt = sc.pl.matrixplot(adata, marker_genes, groupby='louvain', standard_scale='var'). [...]/lib/python3.6/site-packages/scanpy/plotting/_anndata.py in matrixplot(adata, var_names, groupby, use_raw, log, num_categories, figsize, dendrogram, var_group_positions, var_group_labels, var_group_rotation, layer, swap_axes, show, save, **kwds). 1683 _plot_dendrogram(dendro_ax, adata, ticks=y_ticks). 1684 . -> 1685 pc = matrix_ax.pcolor(mean_obs, edgecolor='gray', **kwds). 1686 . 1687 # invert y axis to show categories ordered from top to bottom. [...]/lib/python3.6/site-packages/matplotlib/__init__.py in inner(ax, data, *args, **kwargs). 1803 ""the Matplotlib list!)"" % (label_namer, func.__name__),. 1804 RuntimeWarning, stacklevel=2). -> 1805 return func(ax, *args, **kwargs). 1806 . 1807 inner.__doc__ = _add_data_doc(inner.__doc__,. [...]/lib/python3.6/site-packages/matplotlib/axes/_axes.py in pcolor(self, alpha, norm, cmap, vmin, vmax, *args, **kwargs). 5762 kwargs.setdefault('snap', False). 5763 . -> 5764 collection = mcoll.PolyCollection(verts, **kwargs). 5765 . 5766 collection.set_alpha(alpha). [...]/lib/python3.6/site-packages/matplotlib/collections.py in __init__(self, verts, sizes, closed, **kwargs). 931 %(Collection)s. 932 """""". --> 933 Collection.__init__(self, **kwargs). 934 self.set_sizes(sizes). 935 self.set_verts(verts, closed). [...]/lib/",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/559
https://github.com/scverse/scanpy/issues/559:1357,deployability,stack,stacklevel,1357,", marker_genes, groupby='louvain', standard_scale='var'). ```. I get the following error. ```. ---------------------------------------------------------------------------. AttributeError Traceback (most recent call last). <ipython-input-24-4ac38158d4d0> in <module>. ----> 1 plt = sc.pl.matrixplot(adata, marker_genes, groupby='louvain', standard_scale='var'). [...]/lib/python3.6/site-packages/scanpy/plotting/_anndata.py in matrixplot(adata, var_names, groupby, use_raw, log, num_categories, figsize, dendrogram, var_group_positions, var_group_labels, var_group_rotation, layer, swap_axes, show, save, **kwds). 1683 _plot_dendrogram(dendro_ax, adata, ticks=y_ticks). 1684 . -> 1685 pc = matrix_ax.pcolor(mean_obs, edgecolor='gray', **kwds). 1686 . 1687 # invert y axis to show categories ordered from top to bottom. [...]/lib/python3.6/site-packages/matplotlib/__init__.py in inner(ax, data, *args, **kwargs). 1803 ""the Matplotlib list!)"" % (label_namer, func.__name__),. 1804 RuntimeWarning, stacklevel=2). -> 1805 return func(ax, *args, **kwargs). 1806 . 1807 inner.__doc__ = _add_data_doc(inner.__doc__,. [...]/lib/python3.6/site-packages/matplotlib/axes/_axes.py in pcolor(self, alpha, norm, cmap, vmin, vmax, *args, **kwargs). 5762 kwargs.setdefault('snap', False). 5763 . -> 5764 collection = mcoll.PolyCollection(verts, **kwargs). 5765 . 5766 collection.set_alpha(alpha). [...]/lib/python3.6/site-packages/matplotlib/collections.py in __init__(self, verts, sizes, closed, **kwargs). 931 %(Collection)s. 932 """""". --> 933 Collection.__init__(self, **kwargs). 934 self.set_sizes(sizes). 935 self.set_verts(verts, closed). [...]/lib/python3.6/site-packages/matplotlib/collections.py in __init__(self, edgecolors, facecolors, linewidths, linestyles, capstyle, joinstyle, antialiaseds, offsets, transOffset, norm, cmap, pickradius, hatch, urls, offset_position, zorder, **kwargs). 164 . 165 self._path_effects = None. --> 166 self.update(kwargs). 167 self._paths = None. 168 . [...]/lib/python3.6/",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/559
https://github.com/scverse/scanpy/issues/559:2296,deployability,updat,update,2296,"!)"" % (label_namer, func.__name__),. 1804 RuntimeWarning, stacklevel=2). -> 1805 return func(ax, *args, **kwargs). 1806 . 1807 inner.__doc__ = _add_data_doc(inner.__doc__,. [...]/lib/python3.6/site-packages/matplotlib/axes/_axes.py in pcolor(self, alpha, norm, cmap, vmin, vmax, *args, **kwargs). 5762 kwargs.setdefault('snap', False). 5763 . -> 5764 collection = mcoll.PolyCollection(verts, **kwargs). 5765 . 5766 collection.set_alpha(alpha). [...]/lib/python3.6/site-packages/matplotlib/collections.py in __init__(self, verts, sizes, closed, **kwargs). 931 %(Collection)s. 932 """""". --> 933 Collection.__init__(self, **kwargs). 934 self.set_sizes(sizes). 935 self.set_verts(verts, closed). [...]/lib/python3.6/site-packages/matplotlib/collections.py in __init__(self, edgecolors, facecolors, linewidths, linestyles, capstyle, joinstyle, antialiaseds, offsets, transOffset, norm, cmap, pickradius, hatch, urls, offset_position, zorder, **kwargs). 164 . 165 self._path_effects = None. --> 166 self.update(kwargs). 167 self._paths = None. 168 . [...]/lib/python3.6/site-packages/matplotlib/artist.py in update(self, props). 914 . 915 with cbook._setattr_cm(self, eventson=False):. --> 916 ret = [_update_property(self, k, v) for k, v in props.items()]. 917 . 918 if len(ret):. [...]/lib/python3.6/site-packages/matplotlib/artist.py in <listcomp>(.0). 914 . 915 with cbook._setattr_cm(self, eventson=False):. --> 916 ret = [_update_property(self, k, v) for k, v in props.items()]. 917 . 918 if len(ret):. [...]/lib/python3.6/site-packages/matplotlib/artist.py in _update_property(self, k, v). 910 func = getattr(self, 'set_' + k, None). 911 if not callable(func):. --> 912 raise AttributeError('Unknown property %s' % k). 913 return func(v). 914 . AttributeError: Unknown property standard_scale. ```. Any idea of why I'm getting this? . Package info:. ```. scanpy==1.4 anndata==0.6.18 numpy==1.16.2 scipy==1.2.0 pandas==0.24.1 scikit-learn==0.20.3 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/559
https://github.com/scverse/scanpy/issues/559:2400,deployability,updat,update,2400,"**kwargs). 1806 . 1807 inner.__doc__ = _add_data_doc(inner.__doc__,. [...]/lib/python3.6/site-packages/matplotlib/axes/_axes.py in pcolor(self, alpha, norm, cmap, vmin, vmax, *args, **kwargs). 5762 kwargs.setdefault('snap', False). 5763 . -> 5764 collection = mcoll.PolyCollection(verts, **kwargs). 5765 . 5766 collection.set_alpha(alpha). [...]/lib/python3.6/site-packages/matplotlib/collections.py in __init__(self, verts, sizes, closed, **kwargs). 931 %(Collection)s. 932 """""". --> 933 Collection.__init__(self, **kwargs). 934 self.set_sizes(sizes). 935 self.set_verts(verts, closed). [...]/lib/python3.6/site-packages/matplotlib/collections.py in __init__(self, edgecolors, facecolors, linewidths, linestyles, capstyle, joinstyle, antialiaseds, offsets, transOffset, norm, cmap, pickradius, hatch, urls, offset_position, zorder, **kwargs). 164 . 165 self._path_effects = None. --> 166 self.update(kwargs). 167 self._paths = None. 168 . [...]/lib/python3.6/site-packages/matplotlib/artist.py in update(self, props). 914 . 915 with cbook._setattr_cm(self, eventson=False):. --> 916 ret = [_update_property(self, k, v) for k, v in props.items()]. 917 . 918 if len(ret):. [...]/lib/python3.6/site-packages/matplotlib/artist.py in <listcomp>(.0). 914 . 915 with cbook._setattr_cm(self, eventson=False):. --> 916 ret = [_update_property(self, k, v) for k, v in props.items()]. 917 . 918 if len(ret):. [...]/lib/python3.6/site-packages/matplotlib/artist.py in _update_property(self, k, v). 910 func = getattr(self, 'set_' + k, None). 911 if not callable(func):. --> 912 raise AttributeError('Unknown property %s' % k). 913 return func(v). 914 . AttributeError: Unknown property standard_scale. ```. Any idea of why I'm getting this? . Package info:. ```. scanpy==1.4 anndata==0.6.18 numpy==1.16.2 scipy==1.2.0 pandas==0.24.1 scikit-learn==0.20.3 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 . ```. Thank you! PS: this happens also when I just use the example data as in [here](https://scanpy-tu",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/559
https://github.com/scverse/scanpy/issues/559:156,energy efficiency,heat,heatmap,156,"matrixplot - standard_scale issue; Hi @fidelram ,. When I try to use . ```. plt = sc.pl.matrixplot(adata, marker_genes, groupby='louvain'). ```. I get this heatmap. . ![louv1](https://user-images.githubusercontent.com/11874103/54995344-d2c8ba80-4fc6-11e9-84fe-4f659915293d.png). But as soon as I add ```standard_scale='var'```:. ```. plt = sc.pl.matrixplot(adata, marker_genes, groupby='louvain', standard_scale='var'). ```. I get the following error. ```. ---------------------------------------------------------------------------. AttributeError Traceback (most recent call last). <ipython-input-24-4ac38158d4d0> in <module>. ----> 1 plt = sc.pl.matrixplot(adata, marker_genes, groupby='louvain', standard_scale='var'). [...]/lib/python3.6/site-packages/scanpy/plotting/_anndata.py in matrixplot(adata, var_names, groupby, use_raw, log, num_categories, figsize, dendrogram, var_group_positions, var_group_labels, var_group_rotation, layer, swap_axes, show, save, **kwds). 1683 _plot_dendrogram(dendro_ax, adata, ticks=y_ticks). 1684 . -> 1685 pc = matrix_ax.pcolor(mean_obs, edgecolor='gray', **kwds). 1686 . 1687 # invert y axis to show categories ordered from top to bottom. [...]/lib/python3.6/site-packages/matplotlib/__init__.py in inner(ax, data, *args, **kwargs). 1803 ""the Matplotlib list!)"" % (label_namer, func.__name__),. 1804 RuntimeWarning, stacklevel=2). -> 1805 return func(ax, *args, **kwargs). 1806 . 1807 inner.__doc__ = _add_data_doc(inner.__doc__,. [...]/lib/python3.6/site-packages/matplotlib/axes/_axes.py in pcolor(self, alpha, norm, cmap, vmin, vmax, *args, **kwargs). 5762 kwargs.setdefault('snap', False). 5763 . -> 5764 collection = mcoll.PolyCollection(verts, **kwargs). 5765 . 5766 collection.set_alpha(alpha). [...]/lib/python3.6/site-packages/matplotlib/collections.py in __init__(self, verts, sizes, closed, **kwargs). 931 %(Collection)s. 932 """""". --> 933 Collection.__init__(self, **kwargs). 934 self.set_sizes(sizes). 935 self.set_verts(verts, closed). [...]/lib/",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/559
https://github.com/scverse/scanpy/issues/559:2460,integrability,event,eventson,2460,"doc__,. [...]/lib/python3.6/site-packages/matplotlib/axes/_axes.py in pcolor(self, alpha, norm, cmap, vmin, vmax, *args, **kwargs). 5762 kwargs.setdefault('snap', False). 5763 . -> 5764 collection = mcoll.PolyCollection(verts, **kwargs). 5765 . 5766 collection.set_alpha(alpha). [...]/lib/python3.6/site-packages/matplotlib/collections.py in __init__(self, verts, sizes, closed, **kwargs). 931 %(Collection)s. 932 """""". --> 933 Collection.__init__(self, **kwargs). 934 self.set_sizes(sizes). 935 self.set_verts(verts, closed). [...]/lib/python3.6/site-packages/matplotlib/collections.py in __init__(self, edgecolors, facecolors, linewidths, linestyles, capstyle, joinstyle, antialiaseds, offsets, transOffset, norm, cmap, pickradius, hatch, urls, offset_position, zorder, **kwargs). 164 . 165 self._path_effects = None. --> 166 self.update(kwargs). 167 self._paths = None. 168 . [...]/lib/python3.6/site-packages/matplotlib/artist.py in update(self, props). 914 . 915 with cbook._setattr_cm(self, eventson=False):. --> 916 ret = [_update_property(self, k, v) for k, v in props.items()]. 917 . 918 if len(ret):. [...]/lib/python3.6/site-packages/matplotlib/artist.py in <listcomp>(.0). 914 . 915 with cbook._setattr_cm(self, eventson=False):. --> 916 ret = [_update_property(self, k, v) for k, v in props.items()]. 917 . 918 if len(ret):. [...]/lib/python3.6/site-packages/matplotlib/artist.py in _update_property(self, k, v). 910 func = getattr(self, 'set_' + k, None). 911 if not callable(func):. --> 912 raise AttributeError('Unknown property %s' % k). 913 return func(v). 914 . AttributeError: Unknown property standard_scale. ```. Any idea of why I'm getting this? . Package info:. ```. scanpy==1.4 anndata==0.6.18 numpy==1.16.2 scipy==1.2.0 pandas==0.24.1 scikit-learn==0.20.3 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 . ```. Thank you! PS: this happens also when I just use the example data as in [here](https://scanpy-tutorials.readthedocs.io/en/latest/visualizing-marker-genes.htm",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/559
https://github.com/scverse/scanpy/issues/559:2687,integrability,event,eventson,2687,"__,. [...]/lib/python3.6/site-packages/matplotlib/axes/_axes.py in pcolor(self, alpha, norm, cmap, vmin, vmax, *args, **kwargs). 5762 kwargs.setdefault('snap', False). 5763 . -> 5764 collection = mcoll.PolyCollection(verts, **kwargs). 5765 . 5766 collection.set_alpha(alpha). [...]/lib/python3.6/site-packages/matplotlib/collections.py in __init__(self, verts, sizes, closed, **kwargs). 931 %(Collection)s. 932 """""". --> 933 Collection.__init__(self, **kwargs). 934 self.set_sizes(sizes). 935 self.set_verts(verts, closed). [...]/lib/python3.6/site-packages/matplotlib/collections.py in __init__(self, edgecolors, facecolors, linewidths, linestyles, capstyle, joinstyle, antialiaseds, offsets, transOffset, norm, cmap, pickradius, hatch, urls, offset_position, zorder, **kwargs). 164 . 165 self._path_effects = None. --> 166 self.update(kwargs). 167 self._paths = None. 168 . [...]/lib/python3.6/site-packages/matplotlib/artist.py in update(self, props). 914 . 915 with cbook._setattr_cm(self, eventson=False):. --> 916 ret = [_update_property(self, k, v) for k, v in props.items()]. 917 . 918 if len(ret):. [...]/lib/python3.6/site-packages/matplotlib/artist.py in <listcomp>(.0). 914 . 915 with cbook._setattr_cm(self, eventson=False):. --> 916 ret = [_update_property(self, k, v) for k, v in props.items()]. 917 . 918 if len(ret):. [...]/lib/python3.6/site-packages/matplotlib/artist.py in _update_property(self, k, v). 910 func = getattr(self, 'set_' + k, None). 911 if not callable(func):. --> 912 raise AttributeError('Unknown property %s' % k). 913 return func(v). 914 . AttributeError: Unknown property standard_scale. ```. Any idea of why I'm getting this? . Package info:. ```. scanpy==1.4 anndata==0.6.18 numpy==1.16.2 scipy==1.2.0 pandas==0.24.1 scikit-learn==0.20.3 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 . ```. Thank you! PS: this happens also when I just use the example data as in [here](https://scanpy-tutorials.readthedocs.io/en/latest/visualizing-marker-genes.html).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/559
https://github.com/scverse/scanpy/issues/559:620,modifiability,modul,module,620,"matrixplot - standard_scale issue; Hi @fidelram ,. When I try to use . ```. plt = sc.pl.matrixplot(adata, marker_genes, groupby='louvain'). ```. I get this heatmap. . ![louv1](https://user-images.githubusercontent.com/11874103/54995344-d2c8ba80-4fc6-11e9-84fe-4f659915293d.png). But as soon as I add ```standard_scale='var'```:. ```. plt = sc.pl.matrixplot(adata, marker_genes, groupby='louvain', standard_scale='var'). ```. I get the following error. ```. ---------------------------------------------------------------------------. AttributeError Traceback (most recent call last). <ipython-input-24-4ac38158d4d0> in <module>. ----> 1 plt = sc.pl.matrixplot(adata, marker_genes, groupby='louvain', standard_scale='var'). [...]/lib/python3.6/site-packages/scanpy/plotting/_anndata.py in matrixplot(adata, var_names, groupby, use_raw, log, num_categories, figsize, dendrogram, var_group_positions, var_group_labels, var_group_rotation, layer, swap_axes, show, save, **kwds). 1683 _plot_dendrogram(dendro_ax, adata, ticks=y_ticks). 1684 . -> 1685 pc = matrix_ax.pcolor(mean_obs, edgecolor='gray', **kwds). 1686 . 1687 # invert y axis to show categories ordered from top to bottom. [...]/lib/python3.6/site-packages/matplotlib/__init__.py in inner(ax, data, *args, **kwargs). 1803 ""the Matplotlib list!)"" % (label_namer, func.__name__),. 1804 RuntimeWarning, stacklevel=2). -> 1805 return func(ax, *args, **kwargs). 1806 . 1807 inner.__doc__ = _add_data_doc(inner.__doc__,. [...]/lib/python3.6/site-packages/matplotlib/axes/_axes.py in pcolor(self, alpha, norm, cmap, vmin, vmax, *args, **kwargs). 5762 kwargs.setdefault('snap', False). 5763 . -> 5764 collection = mcoll.PolyCollection(verts, **kwargs). 5765 . 5766 collection.set_alpha(alpha). [...]/lib/python3.6/site-packages/matplotlib/collections.py in __init__(self, verts, sizes, closed, **kwargs). 931 %(Collection)s. 932 """""". --> 933 Collection.__init__(self, **kwargs). 934 self.set_sizes(sizes). 935 self.set_verts(verts, closed). [...]/lib/",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/559
https://github.com/scverse/scanpy/issues/559:748,modifiability,pac,packages,748,"matrixplot - standard_scale issue; Hi @fidelram ,. When I try to use . ```. plt = sc.pl.matrixplot(adata, marker_genes, groupby='louvain'). ```. I get this heatmap. . ![louv1](https://user-images.githubusercontent.com/11874103/54995344-d2c8ba80-4fc6-11e9-84fe-4f659915293d.png). But as soon as I add ```standard_scale='var'```:. ```. plt = sc.pl.matrixplot(adata, marker_genes, groupby='louvain', standard_scale='var'). ```. I get the following error. ```. ---------------------------------------------------------------------------. AttributeError Traceback (most recent call last). <ipython-input-24-4ac38158d4d0> in <module>. ----> 1 plt = sc.pl.matrixplot(adata, marker_genes, groupby='louvain', standard_scale='var'). [...]/lib/python3.6/site-packages/scanpy/plotting/_anndata.py in matrixplot(adata, var_names, groupby, use_raw, log, num_categories, figsize, dendrogram, var_group_positions, var_group_labels, var_group_rotation, layer, swap_axes, show, save, **kwds). 1683 _plot_dendrogram(dendro_ax, adata, ticks=y_ticks). 1684 . -> 1685 pc = matrix_ax.pcolor(mean_obs, edgecolor='gray', **kwds). 1686 . 1687 # invert y axis to show categories ordered from top to bottom. [...]/lib/python3.6/site-packages/matplotlib/__init__.py in inner(ax, data, *args, **kwargs). 1803 ""the Matplotlib list!)"" % (label_namer, func.__name__),. 1804 RuntimeWarning, stacklevel=2). -> 1805 return func(ax, *args, **kwargs). 1806 . 1807 inner.__doc__ = _add_data_doc(inner.__doc__,. [...]/lib/python3.6/site-packages/matplotlib/axes/_axes.py in pcolor(self, alpha, norm, cmap, vmin, vmax, *args, **kwargs). 5762 kwargs.setdefault('snap', False). 5763 . -> 5764 collection = mcoll.PolyCollection(verts, **kwargs). 5765 . 5766 collection.set_alpha(alpha). [...]/lib/python3.6/site-packages/matplotlib/collections.py in __init__(self, verts, sizes, closed, **kwargs). 931 %(Collection)s. 932 """""". --> 933 Collection.__init__(self, **kwargs). 934 self.set_sizes(sizes). 935 self.set_verts(verts, closed). [...]/lib/",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/559
https://github.com/scverse/scanpy/issues/559:936,modifiability,layer,layer,936,"matrixplot - standard_scale issue; Hi @fidelram ,. When I try to use . ```. plt = sc.pl.matrixplot(adata, marker_genes, groupby='louvain'). ```. I get this heatmap. . ![louv1](https://user-images.githubusercontent.com/11874103/54995344-d2c8ba80-4fc6-11e9-84fe-4f659915293d.png). But as soon as I add ```standard_scale='var'```:. ```. plt = sc.pl.matrixplot(adata, marker_genes, groupby='louvain', standard_scale='var'). ```. I get the following error. ```. ---------------------------------------------------------------------------. AttributeError Traceback (most recent call last). <ipython-input-24-4ac38158d4d0> in <module>. ----> 1 plt = sc.pl.matrixplot(adata, marker_genes, groupby='louvain', standard_scale='var'). [...]/lib/python3.6/site-packages/scanpy/plotting/_anndata.py in matrixplot(adata, var_names, groupby, use_raw, log, num_categories, figsize, dendrogram, var_group_positions, var_group_labels, var_group_rotation, layer, swap_axes, show, save, **kwds). 1683 _plot_dendrogram(dendro_ax, adata, ticks=y_ticks). 1684 . -> 1685 pc = matrix_ax.pcolor(mean_obs, edgecolor='gray', **kwds). 1686 . 1687 # invert y axis to show categories ordered from top to bottom. [...]/lib/python3.6/site-packages/matplotlib/__init__.py in inner(ax, data, *args, **kwargs). 1803 ""the Matplotlib list!)"" % (label_namer, func.__name__),. 1804 RuntimeWarning, stacklevel=2). -> 1805 return func(ax, *args, **kwargs). 1806 . 1807 inner.__doc__ = _add_data_doc(inner.__doc__,. [...]/lib/python3.6/site-packages/matplotlib/axes/_axes.py in pcolor(self, alpha, norm, cmap, vmin, vmax, *args, **kwargs). 5762 kwargs.setdefault('snap', False). 5763 . -> 5764 collection = mcoll.PolyCollection(verts, **kwargs). 5765 . 5766 collection.set_alpha(alpha). [...]/lib/python3.6/site-packages/matplotlib/collections.py in __init__(self, verts, sizes, closed, **kwargs). 931 %(Collection)s. 932 """""". --> 933 Collection.__init__(self, **kwargs). 934 self.set_sizes(sizes). 935 self.set_verts(verts, closed). [...]/lib/",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/559
https://github.com/scverse/scanpy/issues/559:1205,modifiability,pac,packages,1205,"tent.com/11874103/54995344-d2c8ba80-4fc6-11e9-84fe-4f659915293d.png). But as soon as I add ```standard_scale='var'```:. ```. plt = sc.pl.matrixplot(adata, marker_genes, groupby='louvain', standard_scale='var'). ```. I get the following error. ```. ---------------------------------------------------------------------------. AttributeError Traceback (most recent call last). <ipython-input-24-4ac38158d4d0> in <module>. ----> 1 plt = sc.pl.matrixplot(adata, marker_genes, groupby='louvain', standard_scale='var'). [...]/lib/python3.6/site-packages/scanpy/plotting/_anndata.py in matrixplot(adata, var_names, groupby, use_raw, log, num_categories, figsize, dendrogram, var_group_positions, var_group_labels, var_group_rotation, layer, swap_axes, show, save, **kwds). 1683 _plot_dendrogram(dendro_ax, adata, ticks=y_ticks). 1684 . -> 1685 pc = matrix_ax.pcolor(mean_obs, edgecolor='gray', **kwds). 1686 . 1687 # invert y axis to show categories ordered from top to bottom. [...]/lib/python3.6/site-packages/matplotlib/__init__.py in inner(ax, data, *args, **kwargs). 1803 ""the Matplotlib list!)"" % (label_namer, func.__name__),. 1804 RuntimeWarning, stacklevel=2). -> 1805 return func(ax, *args, **kwargs). 1806 . 1807 inner.__doc__ = _add_data_doc(inner.__doc__,. [...]/lib/python3.6/site-packages/matplotlib/axes/_axes.py in pcolor(self, alpha, norm, cmap, vmin, vmax, *args, **kwargs). 5762 kwargs.setdefault('snap', False). 5763 . -> 5764 collection = mcoll.PolyCollection(verts, **kwargs). 5765 . 5766 collection.set_alpha(alpha). [...]/lib/python3.6/site-packages/matplotlib/collections.py in __init__(self, verts, sizes, closed, **kwargs). 931 %(Collection)s. 932 """""". --> 933 Collection.__init__(self, **kwargs). 934 self.set_sizes(sizes). 935 self.set_verts(verts, closed). [...]/lib/python3.6/site-packages/matplotlib/collections.py in __init__(self, edgecolors, facecolors, linewidths, linestyles, capstyle, joinstyle, antialiaseds, offsets, transOffset, norm, cmap, pickradius, hatch, urls,",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/559
https://github.com/scverse/scanpy/issues/559:1497,modifiability,pac,packages,1497,"-------------------------------. AttributeError Traceback (most recent call last). <ipython-input-24-4ac38158d4d0> in <module>. ----> 1 plt = sc.pl.matrixplot(adata, marker_genes, groupby='louvain', standard_scale='var'). [...]/lib/python3.6/site-packages/scanpy/plotting/_anndata.py in matrixplot(adata, var_names, groupby, use_raw, log, num_categories, figsize, dendrogram, var_group_positions, var_group_labels, var_group_rotation, layer, swap_axes, show, save, **kwds). 1683 _plot_dendrogram(dendro_ax, adata, ticks=y_ticks). 1684 . -> 1685 pc = matrix_ax.pcolor(mean_obs, edgecolor='gray', **kwds). 1686 . 1687 # invert y axis to show categories ordered from top to bottom. [...]/lib/python3.6/site-packages/matplotlib/__init__.py in inner(ax, data, *args, **kwargs). 1803 ""the Matplotlib list!)"" % (label_namer, func.__name__),. 1804 RuntimeWarning, stacklevel=2). -> 1805 return func(ax, *args, **kwargs). 1806 . 1807 inner.__doc__ = _add_data_doc(inner.__doc__,. [...]/lib/python3.6/site-packages/matplotlib/axes/_axes.py in pcolor(self, alpha, norm, cmap, vmin, vmax, *args, **kwargs). 5762 kwargs.setdefault('snap', False). 5763 . -> 5764 collection = mcoll.PolyCollection(verts, **kwargs). 5765 . 5766 collection.set_alpha(alpha). [...]/lib/python3.6/site-packages/matplotlib/collections.py in __init__(self, verts, sizes, closed, **kwargs). 931 %(Collection)s. 932 """""". --> 933 Collection.__init__(self, **kwargs). 934 self.set_sizes(sizes). 935 self.set_verts(verts, closed). [...]/lib/python3.6/site-packages/matplotlib/collections.py in __init__(self, edgecolors, facecolors, linewidths, linestyles, capstyle, joinstyle, antialiaseds, offsets, transOffset, norm, cmap, pickradius, hatch, urls, offset_position, zorder, **kwargs). 164 . 165 self._path_effects = None. --> 166 self.update(kwargs). 167 self._paths = None. 168 . [...]/lib/python3.6/site-packages/matplotlib/artist.py in update(self, props). 914 . 915 with cbook._setattr_cm(self, eventson=False):. --> 916 ret = [_update_",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/559
https://github.com/scverse/scanpy/issues/559:1768,modifiability,pac,packages,1768,"/_anndata.py in matrixplot(adata, var_names, groupby, use_raw, log, num_categories, figsize, dendrogram, var_group_positions, var_group_labels, var_group_rotation, layer, swap_axes, show, save, **kwds). 1683 _plot_dendrogram(dendro_ax, adata, ticks=y_ticks). 1684 . -> 1685 pc = matrix_ax.pcolor(mean_obs, edgecolor='gray', **kwds). 1686 . 1687 # invert y axis to show categories ordered from top to bottom. [...]/lib/python3.6/site-packages/matplotlib/__init__.py in inner(ax, data, *args, **kwargs). 1803 ""the Matplotlib list!)"" % (label_namer, func.__name__),. 1804 RuntimeWarning, stacklevel=2). -> 1805 return func(ax, *args, **kwargs). 1806 . 1807 inner.__doc__ = _add_data_doc(inner.__doc__,. [...]/lib/python3.6/site-packages/matplotlib/axes/_axes.py in pcolor(self, alpha, norm, cmap, vmin, vmax, *args, **kwargs). 5762 kwargs.setdefault('snap', False). 5763 . -> 5764 collection = mcoll.PolyCollection(verts, **kwargs). 5765 . 5766 collection.set_alpha(alpha). [...]/lib/python3.6/site-packages/matplotlib/collections.py in __init__(self, verts, sizes, closed, **kwargs). 931 %(Collection)s. 932 """""". --> 933 Collection.__init__(self, **kwargs). 934 self.set_sizes(sizes). 935 self.set_verts(verts, closed). [...]/lib/python3.6/site-packages/matplotlib/collections.py in __init__(self, edgecolors, facecolors, linewidths, linestyles, capstyle, joinstyle, antialiaseds, offsets, transOffset, norm, cmap, pickradius, hatch, urls, offset_position, zorder, **kwargs). 164 . 165 self._path_effects = None. --> 166 self.update(kwargs). 167 self._paths = None. 168 . [...]/lib/python3.6/site-packages/matplotlib/artist.py in update(self, props). 914 . 915 with cbook._setattr_cm(self, eventson=False):. --> 916 ret = [_update_property(self, k, v) for k, v in props.items()]. 917 . 918 if len(ret):. [...]/lib/python3.6/site-packages/matplotlib/artist.py in <listcomp>(.0). 914 . 915 with cbook._setattr_cm(self, eventson=False):. --> 916 ret = [_update_property(self, k, v) for k, v in props.items",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/559
https://github.com/scverse/scanpy/issues/559:2015,modifiability,pac,packages,2015,"s=y_ticks). 1684 . -> 1685 pc = matrix_ax.pcolor(mean_obs, edgecolor='gray', **kwds). 1686 . 1687 # invert y axis to show categories ordered from top to bottom. [...]/lib/python3.6/site-packages/matplotlib/__init__.py in inner(ax, data, *args, **kwargs). 1803 ""the Matplotlib list!)"" % (label_namer, func.__name__),. 1804 RuntimeWarning, stacklevel=2). -> 1805 return func(ax, *args, **kwargs). 1806 . 1807 inner.__doc__ = _add_data_doc(inner.__doc__,. [...]/lib/python3.6/site-packages/matplotlib/axes/_axes.py in pcolor(self, alpha, norm, cmap, vmin, vmax, *args, **kwargs). 5762 kwargs.setdefault('snap', False). 5763 . -> 5764 collection = mcoll.PolyCollection(verts, **kwargs). 5765 . 5766 collection.set_alpha(alpha). [...]/lib/python3.6/site-packages/matplotlib/collections.py in __init__(self, verts, sizes, closed, **kwargs). 931 %(Collection)s. 932 """""". --> 933 Collection.__init__(self, **kwargs). 934 self.set_sizes(sizes). 935 self.set_verts(verts, closed). [...]/lib/python3.6/site-packages/matplotlib/collections.py in __init__(self, edgecolors, facecolors, linewidths, linestyles, capstyle, joinstyle, antialiaseds, offsets, transOffset, norm, cmap, pickradius, hatch, urls, offset_position, zorder, **kwargs). 164 . 165 self._path_effects = None. --> 166 self.update(kwargs). 167 self._paths = None. 168 . [...]/lib/python3.6/site-packages/matplotlib/artist.py in update(self, props). 914 . 915 with cbook._setattr_cm(self, eventson=False):. --> 916 ret = [_update_property(self, k, v) for k, v in props.items()]. 917 . 918 if len(ret):. [...]/lib/python3.6/site-packages/matplotlib/artist.py in <listcomp>(.0). 914 . 915 with cbook._setattr_cm(self, eventson=False):. --> 916 ret = [_update_property(self, k, v) for k, v in props.items()]. 917 . 918 if len(ret):. [...]/lib/python3.6/site-packages/matplotlib/artist.py in _update_property(self, k, v). 910 func = getattr(self, 'set_' + k, None). 911 if not callable(func):. --> 912 raise AttributeError('Unknown property %s' % k). 9",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/559
https://github.com/scverse/scanpy/issues/559:2367,modifiability,pac,packages,2367," -> 1805 return func(ax, *args, **kwargs). 1806 . 1807 inner.__doc__ = _add_data_doc(inner.__doc__,. [...]/lib/python3.6/site-packages/matplotlib/axes/_axes.py in pcolor(self, alpha, norm, cmap, vmin, vmax, *args, **kwargs). 5762 kwargs.setdefault('snap', False). 5763 . -> 5764 collection = mcoll.PolyCollection(verts, **kwargs). 5765 . 5766 collection.set_alpha(alpha). [...]/lib/python3.6/site-packages/matplotlib/collections.py in __init__(self, verts, sizes, closed, **kwargs). 931 %(Collection)s. 932 """""". --> 933 Collection.__init__(self, **kwargs). 934 self.set_sizes(sizes). 935 self.set_verts(verts, closed). [...]/lib/python3.6/site-packages/matplotlib/collections.py in __init__(self, edgecolors, facecolors, linewidths, linestyles, capstyle, joinstyle, antialiaseds, offsets, transOffset, norm, cmap, pickradius, hatch, urls, offset_position, zorder, **kwargs). 164 . 165 self._path_effects = None. --> 166 self.update(kwargs). 167 self._paths = None. 168 . [...]/lib/python3.6/site-packages/matplotlib/artist.py in update(self, props). 914 . 915 with cbook._setattr_cm(self, eventson=False):. --> 916 ret = [_update_property(self, k, v) for k, v in props.items()]. 917 . 918 if len(ret):. [...]/lib/python3.6/site-packages/matplotlib/artist.py in <listcomp>(.0). 914 . 915 with cbook._setattr_cm(self, eventson=False):. --> 916 ret = [_update_property(self, k, v) for k, v in props.items()]. 917 . 918 if len(ret):. [...]/lib/python3.6/site-packages/matplotlib/artist.py in _update_property(self, k, v). 910 func = getattr(self, 'set_' + k, None). 911 if not callable(func):. --> 912 raise AttributeError('Unknown property %s' % k). 913 return func(v). 914 . AttributeError: Unknown property standard_scale. ```. Any idea of why I'm getting this? . Package info:. ```. scanpy==1.4 anndata==0.6.18 numpy==1.16.2 scipy==1.2.0 pandas==0.24.1 scikit-learn==0.20.3 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 . ```. Thank you! PS: this happens also when I just use the example dat",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/559
https://github.com/scverse/scanpy/issues/559:2599,modifiability,pac,packages,2599,"__,. [...]/lib/python3.6/site-packages/matplotlib/axes/_axes.py in pcolor(self, alpha, norm, cmap, vmin, vmax, *args, **kwargs). 5762 kwargs.setdefault('snap', False). 5763 . -> 5764 collection = mcoll.PolyCollection(verts, **kwargs). 5765 . 5766 collection.set_alpha(alpha). [...]/lib/python3.6/site-packages/matplotlib/collections.py in __init__(self, verts, sizes, closed, **kwargs). 931 %(Collection)s. 932 """""". --> 933 Collection.__init__(self, **kwargs). 934 self.set_sizes(sizes). 935 self.set_verts(verts, closed). [...]/lib/python3.6/site-packages/matplotlib/collections.py in __init__(self, edgecolors, facecolors, linewidths, linestyles, capstyle, joinstyle, antialiaseds, offsets, transOffset, norm, cmap, pickradius, hatch, urls, offset_position, zorder, **kwargs). 164 . 165 self._path_effects = None. --> 166 self.update(kwargs). 167 self._paths = None. 168 . [...]/lib/python3.6/site-packages/matplotlib/artist.py in update(self, props). 914 . 915 with cbook._setattr_cm(self, eventson=False):. --> 916 ret = [_update_property(self, k, v) for k, v in props.items()]. 917 . 918 if len(ret):. [...]/lib/python3.6/site-packages/matplotlib/artist.py in <listcomp>(.0). 914 . 915 with cbook._setattr_cm(self, eventson=False):. --> 916 ret = [_update_property(self, k, v) for k, v in props.items()]. 917 . 918 if len(ret):. [...]/lib/python3.6/site-packages/matplotlib/artist.py in _update_property(self, k, v). 910 func = getattr(self, 'set_' + k, None). 911 if not callable(func):. --> 912 raise AttributeError('Unknown property %s' % k). 913 return func(v). 914 . AttributeError: Unknown property standard_scale. ```. Any idea of why I'm getting this? . Package info:. ```. scanpy==1.4 anndata==0.6.18 numpy==1.16.2 scipy==1.2.0 pandas==0.24.1 scikit-learn==0.20.3 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 . ```. Thank you! PS: this happens also when I just use the example data as in [here](https://scanpy-tutorials.readthedocs.io/en/latest/visualizing-marker-genes.html).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/559
https://github.com/scverse/scanpy/issues/559:2826,modifiability,pac,packages,2826,"__,. [...]/lib/python3.6/site-packages/matplotlib/axes/_axes.py in pcolor(self, alpha, norm, cmap, vmin, vmax, *args, **kwargs). 5762 kwargs.setdefault('snap', False). 5763 . -> 5764 collection = mcoll.PolyCollection(verts, **kwargs). 5765 . 5766 collection.set_alpha(alpha). [...]/lib/python3.6/site-packages/matplotlib/collections.py in __init__(self, verts, sizes, closed, **kwargs). 931 %(Collection)s. 932 """""". --> 933 Collection.__init__(self, **kwargs). 934 self.set_sizes(sizes). 935 self.set_verts(verts, closed). [...]/lib/python3.6/site-packages/matplotlib/collections.py in __init__(self, edgecolors, facecolors, linewidths, linestyles, capstyle, joinstyle, antialiaseds, offsets, transOffset, norm, cmap, pickradius, hatch, urls, offset_position, zorder, **kwargs). 164 . 165 self._path_effects = None. --> 166 self.update(kwargs). 167 self._paths = None. 168 . [...]/lib/python3.6/site-packages/matplotlib/artist.py in update(self, props). 914 . 915 with cbook._setattr_cm(self, eventson=False):. --> 916 ret = [_update_property(self, k, v) for k, v in props.items()]. 917 . 918 if len(ret):. [...]/lib/python3.6/site-packages/matplotlib/artist.py in <listcomp>(.0). 914 . 915 with cbook._setattr_cm(self, eventson=False):. --> 916 ret = [_update_property(self, k, v) for k, v in props.items()]. 917 . 918 if len(ret):. [...]/lib/python3.6/site-packages/matplotlib/artist.py in _update_property(self, k, v). 910 func = getattr(self, 'set_' + k, None). 911 if not callable(func):. --> 912 raise AttributeError('Unknown property %s' % k). 913 return func(v). 914 . AttributeError: Unknown property standard_scale. ```. Any idea of why I'm getting this? . Package info:. ```. scanpy==1.4 anndata==0.6.18 numpy==1.16.2 scipy==1.2.0 pandas==0.24.1 scikit-learn==0.20.3 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 . ```. Thank you! PS: this happens also when I just use the example data as in [here](https://scanpy-tutorials.readthedocs.io/en/latest/visualizing-marker-genes.html).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/559
https://github.com/scverse/scanpy/issues/559:3134,modifiability,Pac,Package,3134,"__,. [...]/lib/python3.6/site-packages/matplotlib/axes/_axes.py in pcolor(self, alpha, norm, cmap, vmin, vmax, *args, **kwargs). 5762 kwargs.setdefault('snap', False). 5763 . -> 5764 collection = mcoll.PolyCollection(verts, **kwargs). 5765 . 5766 collection.set_alpha(alpha). [...]/lib/python3.6/site-packages/matplotlib/collections.py in __init__(self, verts, sizes, closed, **kwargs). 931 %(Collection)s. 932 """""". --> 933 Collection.__init__(self, **kwargs). 934 self.set_sizes(sizes). 935 self.set_verts(verts, closed). [...]/lib/python3.6/site-packages/matplotlib/collections.py in __init__(self, edgecolors, facecolors, linewidths, linestyles, capstyle, joinstyle, antialiaseds, offsets, transOffset, norm, cmap, pickradius, hatch, urls, offset_position, zorder, **kwargs). 164 . 165 self._path_effects = None. --> 166 self.update(kwargs). 167 self._paths = None. 168 . [...]/lib/python3.6/site-packages/matplotlib/artist.py in update(self, props). 914 . 915 with cbook._setattr_cm(self, eventson=False):. --> 916 ret = [_update_property(self, k, v) for k, v in props.items()]. 917 . 918 if len(ret):. [...]/lib/python3.6/site-packages/matplotlib/artist.py in <listcomp>(.0). 914 . 915 with cbook._setattr_cm(self, eventson=False):. --> 916 ret = [_update_property(self, k, v) for k, v in props.items()]. 917 . 918 if len(ret):. [...]/lib/python3.6/site-packages/matplotlib/artist.py in _update_property(self, k, v). 910 func = getattr(self, 'set_' + k, None). 911 if not callable(func):. --> 912 raise AttributeError('Unknown property %s' % k). 913 return func(v). 914 . AttributeError: Unknown property standard_scale. ```. Any idea of why I'm getting this? . Package info:. ```. scanpy==1.4 anndata==0.6.18 numpy==1.16.2 scipy==1.2.0 pandas==0.24.1 scikit-learn==0.20.3 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 . ```. Thank you! PS: this happens also when I just use the example data as in [here](https://scanpy-tutorials.readthedocs.io/en/latest/visualizing-marker-genes.html).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/559
https://github.com/scverse/scanpy/issues/559:445,performance,error,error,445,"matrixplot - standard_scale issue; Hi @fidelram ,. When I try to use . ```. plt = sc.pl.matrixplot(adata, marker_genes, groupby='louvain'). ```. I get this heatmap. . ![louv1](https://user-images.githubusercontent.com/11874103/54995344-d2c8ba80-4fc6-11e9-84fe-4f659915293d.png). But as soon as I add ```standard_scale='var'```:. ```. plt = sc.pl.matrixplot(adata, marker_genes, groupby='louvain', standard_scale='var'). ```. I get the following error. ```. ---------------------------------------------------------------------------. AttributeError Traceback (most recent call last). <ipython-input-24-4ac38158d4d0> in <module>. ----> 1 plt = sc.pl.matrixplot(adata, marker_genes, groupby='louvain', standard_scale='var'). [...]/lib/python3.6/site-packages/scanpy/plotting/_anndata.py in matrixplot(adata, var_names, groupby, use_raw, log, num_categories, figsize, dendrogram, var_group_positions, var_group_labels, var_group_rotation, layer, swap_axes, show, save, **kwds). 1683 _plot_dendrogram(dendro_ax, adata, ticks=y_ticks). 1684 . -> 1685 pc = matrix_ax.pcolor(mean_obs, edgecolor='gray', **kwds). 1686 . 1687 # invert y axis to show categories ordered from top to bottom. [...]/lib/python3.6/site-packages/matplotlib/__init__.py in inner(ax, data, *args, **kwargs). 1803 ""the Matplotlib list!)"" % (label_namer, func.__name__),. 1804 RuntimeWarning, stacklevel=2). -> 1805 return func(ax, *args, **kwargs). 1806 . 1807 inner.__doc__ = _add_data_doc(inner.__doc__,. [...]/lib/python3.6/site-packages/matplotlib/axes/_axes.py in pcolor(self, alpha, norm, cmap, vmin, vmax, *args, **kwargs). 5762 kwargs.setdefault('snap', False). 5763 . -> 5764 collection = mcoll.PolyCollection(verts, **kwargs). 5765 . 5766 collection.set_alpha(alpha). [...]/lib/python3.6/site-packages/matplotlib/collections.py in __init__(self, verts, sizes, closed, **kwargs). 931 %(Collection)s. 932 """""". --> 933 Collection.__init__(self, **kwargs). 934 self.set_sizes(sizes). 935 self.set_verts(verts, closed). [...]/lib/",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/559
https://github.com/scverse/scanpy/issues/559:445,safety,error,error,445,"matrixplot - standard_scale issue; Hi @fidelram ,. When I try to use . ```. plt = sc.pl.matrixplot(adata, marker_genes, groupby='louvain'). ```. I get this heatmap. . ![louv1](https://user-images.githubusercontent.com/11874103/54995344-d2c8ba80-4fc6-11e9-84fe-4f659915293d.png). But as soon as I add ```standard_scale='var'```:. ```. plt = sc.pl.matrixplot(adata, marker_genes, groupby='louvain', standard_scale='var'). ```. I get the following error. ```. ---------------------------------------------------------------------------. AttributeError Traceback (most recent call last). <ipython-input-24-4ac38158d4d0> in <module>. ----> 1 plt = sc.pl.matrixplot(adata, marker_genes, groupby='louvain', standard_scale='var'). [...]/lib/python3.6/site-packages/scanpy/plotting/_anndata.py in matrixplot(adata, var_names, groupby, use_raw, log, num_categories, figsize, dendrogram, var_group_positions, var_group_labels, var_group_rotation, layer, swap_axes, show, save, **kwds). 1683 _plot_dendrogram(dendro_ax, adata, ticks=y_ticks). 1684 . -> 1685 pc = matrix_ax.pcolor(mean_obs, edgecolor='gray', **kwds). 1686 . 1687 # invert y axis to show categories ordered from top to bottom. [...]/lib/python3.6/site-packages/matplotlib/__init__.py in inner(ax, data, *args, **kwargs). 1803 ""the Matplotlib list!)"" % (label_namer, func.__name__),. 1804 RuntimeWarning, stacklevel=2). -> 1805 return func(ax, *args, **kwargs). 1806 . 1807 inner.__doc__ = _add_data_doc(inner.__doc__,. [...]/lib/python3.6/site-packages/matplotlib/axes/_axes.py in pcolor(self, alpha, norm, cmap, vmin, vmax, *args, **kwargs). 5762 kwargs.setdefault('snap', False). 5763 . -> 5764 collection = mcoll.PolyCollection(verts, **kwargs). 5765 . 5766 collection.set_alpha(alpha). [...]/lib/python3.6/site-packages/matplotlib/collections.py in __init__(self, verts, sizes, closed, **kwargs). 931 %(Collection)s. 932 """""". --> 933 Collection.__init__(self, **kwargs). 934 self.set_sizes(sizes). 935 self.set_verts(verts, closed). [...]/lib/",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/559
https://github.com/scverse/scanpy/issues/559:593,safety,input,input-,593,"matrixplot - standard_scale issue; Hi @fidelram ,. When I try to use . ```. plt = sc.pl.matrixplot(adata, marker_genes, groupby='louvain'). ```. I get this heatmap. . ![louv1](https://user-images.githubusercontent.com/11874103/54995344-d2c8ba80-4fc6-11e9-84fe-4f659915293d.png). But as soon as I add ```standard_scale='var'```:. ```. plt = sc.pl.matrixplot(adata, marker_genes, groupby='louvain', standard_scale='var'). ```. I get the following error. ```. ---------------------------------------------------------------------------. AttributeError Traceback (most recent call last). <ipython-input-24-4ac38158d4d0> in <module>. ----> 1 plt = sc.pl.matrixplot(adata, marker_genes, groupby='louvain', standard_scale='var'). [...]/lib/python3.6/site-packages/scanpy/plotting/_anndata.py in matrixplot(adata, var_names, groupby, use_raw, log, num_categories, figsize, dendrogram, var_group_positions, var_group_labels, var_group_rotation, layer, swap_axes, show, save, **kwds). 1683 _plot_dendrogram(dendro_ax, adata, ticks=y_ticks). 1684 . -> 1685 pc = matrix_ax.pcolor(mean_obs, edgecolor='gray', **kwds). 1686 . 1687 # invert y axis to show categories ordered from top to bottom. [...]/lib/python3.6/site-packages/matplotlib/__init__.py in inner(ax, data, *args, **kwargs). 1803 ""the Matplotlib list!)"" % (label_namer, func.__name__),. 1804 RuntimeWarning, stacklevel=2). -> 1805 return func(ax, *args, **kwargs). 1806 . 1807 inner.__doc__ = _add_data_doc(inner.__doc__,. [...]/lib/python3.6/site-packages/matplotlib/axes/_axes.py in pcolor(self, alpha, norm, cmap, vmin, vmax, *args, **kwargs). 5762 kwargs.setdefault('snap', False). 5763 . -> 5764 collection = mcoll.PolyCollection(verts, **kwargs). 5765 . 5766 collection.set_alpha(alpha). [...]/lib/python3.6/site-packages/matplotlib/collections.py in __init__(self, verts, sizes, closed, **kwargs). 931 %(Collection)s. 932 """""". --> 933 Collection.__init__(self, **kwargs). 934 self.set_sizes(sizes). 935 self.set_verts(verts, closed). [...]/lib/",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/559
https://github.com/scverse/scanpy/issues/559:620,safety,modul,module,620,"matrixplot - standard_scale issue; Hi @fidelram ,. When I try to use . ```. plt = sc.pl.matrixplot(adata, marker_genes, groupby='louvain'). ```. I get this heatmap. . ![louv1](https://user-images.githubusercontent.com/11874103/54995344-d2c8ba80-4fc6-11e9-84fe-4f659915293d.png). But as soon as I add ```standard_scale='var'```:. ```. plt = sc.pl.matrixplot(adata, marker_genes, groupby='louvain', standard_scale='var'). ```. I get the following error. ```. ---------------------------------------------------------------------------. AttributeError Traceback (most recent call last). <ipython-input-24-4ac38158d4d0> in <module>. ----> 1 plt = sc.pl.matrixplot(adata, marker_genes, groupby='louvain', standard_scale='var'). [...]/lib/python3.6/site-packages/scanpy/plotting/_anndata.py in matrixplot(adata, var_names, groupby, use_raw, log, num_categories, figsize, dendrogram, var_group_positions, var_group_labels, var_group_rotation, layer, swap_axes, show, save, **kwds). 1683 _plot_dendrogram(dendro_ax, adata, ticks=y_ticks). 1684 . -> 1685 pc = matrix_ax.pcolor(mean_obs, edgecolor='gray', **kwds). 1686 . 1687 # invert y axis to show categories ordered from top to bottom. [...]/lib/python3.6/site-packages/matplotlib/__init__.py in inner(ax, data, *args, **kwargs). 1803 ""the Matplotlib list!)"" % (label_namer, func.__name__),. 1804 RuntimeWarning, stacklevel=2). -> 1805 return func(ax, *args, **kwargs). 1806 . 1807 inner.__doc__ = _add_data_doc(inner.__doc__,. [...]/lib/python3.6/site-packages/matplotlib/axes/_axes.py in pcolor(self, alpha, norm, cmap, vmin, vmax, *args, **kwargs). 5762 kwargs.setdefault('snap', False). 5763 . -> 5764 collection = mcoll.PolyCollection(verts, **kwargs). 5765 . 5766 collection.set_alpha(alpha). [...]/lib/python3.6/site-packages/matplotlib/collections.py in __init__(self, verts, sizes, closed, **kwargs). 931 %(Collection)s. 932 """""". --> 933 Collection.__init__(self, **kwargs). 934 self.set_sizes(sizes). 935 self.set_verts(verts, closed). [...]/lib/",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/559
https://github.com/scverse/scanpy/issues/559:835,safety,log,log,835,"matrixplot - standard_scale issue; Hi @fidelram ,. When I try to use . ```. plt = sc.pl.matrixplot(adata, marker_genes, groupby='louvain'). ```. I get this heatmap. . ![louv1](https://user-images.githubusercontent.com/11874103/54995344-d2c8ba80-4fc6-11e9-84fe-4f659915293d.png). But as soon as I add ```standard_scale='var'```:. ```. plt = sc.pl.matrixplot(adata, marker_genes, groupby='louvain', standard_scale='var'). ```. I get the following error. ```. ---------------------------------------------------------------------------. AttributeError Traceback (most recent call last). <ipython-input-24-4ac38158d4d0> in <module>. ----> 1 plt = sc.pl.matrixplot(adata, marker_genes, groupby='louvain', standard_scale='var'). [...]/lib/python3.6/site-packages/scanpy/plotting/_anndata.py in matrixplot(adata, var_names, groupby, use_raw, log, num_categories, figsize, dendrogram, var_group_positions, var_group_labels, var_group_rotation, layer, swap_axes, show, save, **kwds). 1683 _plot_dendrogram(dendro_ax, adata, ticks=y_ticks). 1684 . -> 1685 pc = matrix_ax.pcolor(mean_obs, edgecolor='gray', **kwds). 1686 . 1687 # invert y axis to show categories ordered from top to bottom. [...]/lib/python3.6/site-packages/matplotlib/__init__.py in inner(ax, data, *args, **kwargs). 1803 ""the Matplotlib list!)"" % (label_namer, func.__name__),. 1804 RuntimeWarning, stacklevel=2). -> 1805 return func(ax, *args, **kwargs). 1806 . 1807 inner.__doc__ = _add_data_doc(inner.__doc__,. [...]/lib/python3.6/site-packages/matplotlib/axes/_axes.py in pcolor(self, alpha, norm, cmap, vmin, vmax, *args, **kwargs). 5762 kwargs.setdefault('snap', False). 5763 . -> 5764 collection = mcoll.PolyCollection(verts, **kwargs). 5765 . 5766 collection.set_alpha(alpha). [...]/lib/python3.6/site-packages/matplotlib/collections.py in __init__(self, verts, sizes, closed, **kwargs). 931 %(Collection)s. 932 """""". --> 933 Collection.__init__(self, **kwargs). 934 self.set_sizes(sizes). 935 self.set_verts(verts, closed). [...]/lib/",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/559
https://github.com/scverse/scanpy/issues/559:2296,safety,updat,update,2296,"!)"" % (label_namer, func.__name__),. 1804 RuntimeWarning, stacklevel=2). -> 1805 return func(ax, *args, **kwargs). 1806 . 1807 inner.__doc__ = _add_data_doc(inner.__doc__,. [...]/lib/python3.6/site-packages/matplotlib/axes/_axes.py in pcolor(self, alpha, norm, cmap, vmin, vmax, *args, **kwargs). 5762 kwargs.setdefault('snap', False). 5763 . -> 5764 collection = mcoll.PolyCollection(verts, **kwargs). 5765 . 5766 collection.set_alpha(alpha). [...]/lib/python3.6/site-packages/matplotlib/collections.py in __init__(self, verts, sizes, closed, **kwargs). 931 %(Collection)s. 932 """""". --> 933 Collection.__init__(self, **kwargs). 934 self.set_sizes(sizes). 935 self.set_verts(verts, closed). [...]/lib/python3.6/site-packages/matplotlib/collections.py in __init__(self, edgecolors, facecolors, linewidths, linestyles, capstyle, joinstyle, antialiaseds, offsets, transOffset, norm, cmap, pickradius, hatch, urls, offset_position, zorder, **kwargs). 164 . 165 self._path_effects = None. --> 166 self.update(kwargs). 167 self._paths = None. 168 . [...]/lib/python3.6/site-packages/matplotlib/artist.py in update(self, props). 914 . 915 with cbook._setattr_cm(self, eventson=False):. --> 916 ret = [_update_property(self, k, v) for k, v in props.items()]. 917 . 918 if len(ret):. [...]/lib/python3.6/site-packages/matplotlib/artist.py in <listcomp>(.0). 914 . 915 with cbook._setattr_cm(self, eventson=False):. --> 916 ret = [_update_property(self, k, v) for k, v in props.items()]. 917 . 918 if len(ret):. [...]/lib/python3.6/site-packages/matplotlib/artist.py in _update_property(self, k, v). 910 func = getattr(self, 'set_' + k, None). 911 if not callable(func):. --> 912 raise AttributeError('Unknown property %s' % k). 913 return func(v). 914 . AttributeError: Unknown property standard_scale. ```. Any idea of why I'm getting this? . Package info:. ```. scanpy==1.4 anndata==0.6.18 numpy==1.16.2 scipy==1.2.0 pandas==0.24.1 scikit-learn==0.20.3 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/559
https://github.com/scverse/scanpy/issues/559:2400,safety,updat,update,2400,"**kwargs). 1806 . 1807 inner.__doc__ = _add_data_doc(inner.__doc__,. [...]/lib/python3.6/site-packages/matplotlib/axes/_axes.py in pcolor(self, alpha, norm, cmap, vmin, vmax, *args, **kwargs). 5762 kwargs.setdefault('snap', False). 5763 . -> 5764 collection = mcoll.PolyCollection(verts, **kwargs). 5765 . 5766 collection.set_alpha(alpha). [...]/lib/python3.6/site-packages/matplotlib/collections.py in __init__(self, verts, sizes, closed, **kwargs). 931 %(Collection)s. 932 """""". --> 933 Collection.__init__(self, **kwargs). 934 self.set_sizes(sizes). 935 self.set_verts(verts, closed). [...]/lib/python3.6/site-packages/matplotlib/collections.py in __init__(self, edgecolors, facecolors, linewidths, linestyles, capstyle, joinstyle, antialiaseds, offsets, transOffset, norm, cmap, pickradius, hatch, urls, offset_position, zorder, **kwargs). 164 . 165 self._path_effects = None. --> 166 self.update(kwargs). 167 self._paths = None. 168 . [...]/lib/python3.6/site-packages/matplotlib/artist.py in update(self, props). 914 . 915 with cbook._setattr_cm(self, eventson=False):. --> 916 ret = [_update_property(self, k, v) for k, v in props.items()]. 917 . 918 if len(ret):. [...]/lib/python3.6/site-packages/matplotlib/artist.py in <listcomp>(.0). 914 . 915 with cbook._setattr_cm(self, eventson=False):. --> 916 ret = [_update_property(self, k, v) for k, v in props.items()]. 917 . 918 if len(ret):. [...]/lib/python3.6/site-packages/matplotlib/artist.py in _update_property(self, k, v). 910 func = getattr(self, 'set_' + k, None). 911 if not callable(func):. --> 912 raise AttributeError('Unknown property %s' % k). 913 return func(v). 914 . AttributeError: Unknown property standard_scale. ```. Any idea of why I'm getting this? . Package info:. ```. scanpy==1.4 anndata==0.6.18 numpy==1.16.2 scipy==1.2.0 pandas==0.24.1 scikit-learn==0.20.3 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 . ```. Thank you! PS: this happens also when I just use the example data as in [here](https://scanpy-tu",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/559
https://github.com/scverse/scanpy/issues/559:835,security,log,log,835,"matrixplot - standard_scale issue; Hi @fidelram ,. When I try to use . ```. plt = sc.pl.matrixplot(adata, marker_genes, groupby='louvain'). ```. I get this heatmap. . ![louv1](https://user-images.githubusercontent.com/11874103/54995344-d2c8ba80-4fc6-11e9-84fe-4f659915293d.png). But as soon as I add ```standard_scale='var'```:. ```. plt = sc.pl.matrixplot(adata, marker_genes, groupby='louvain', standard_scale='var'). ```. I get the following error. ```. ---------------------------------------------------------------------------. AttributeError Traceback (most recent call last). <ipython-input-24-4ac38158d4d0> in <module>. ----> 1 plt = sc.pl.matrixplot(adata, marker_genes, groupby='louvain', standard_scale='var'). [...]/lib/python3.6/site-packages/scanpy/plotting/_anndata.py in matrixplot(adata, var_names, groupby, use_raw, log, num_categories, figsize, dendrogram, var_group_positions, var_group_labels, var_group_rotation, layer, swap_axes, show, save, **kwds). 1683 _plot_dendrogram(dendro_ax, adata, ticks=y_ticks). 1684 . -> 1685 pc = matrix_ax.pcolor(mean_obs, edgecolor='gray', **kwds). 1686 . 1687 # invert y axis to show categories ordered from top to bottom. [...]/lib/python3.6/site-packages/matplotlib/__init__.py in inner(ax, data, *args, **kwargs). 1803 ""the Matplotlib list!)"" % (label_namer, func.__name__),. 1804 RuntimeWarning, stacklevel=2). -> 1805 return func(ax, *args, **kwargs). 1806 . 1807 inner.__doc__ = _add_data_doc(inner.__doc__,. [...]/lib/python3.6/site-packages/matplotlib/axes/_axes.py in pcolor(self, alpha, norm, cmap, vmin, vmax, *args, **kwargs). 5762 kwargs.setdefault('snap', False). 5763 . -> 5764 collection = mcoll.PolyCollection(verts, **kwargs). 5765 . 5766 collection.set_alpha(alpha). [...]/lib/python3.6/site-packages/matplotlib/collections.py in __init__(self, verts, sizes, closed, **kwargs). 931 %(Collection)s. 932 """""". --> 933 Collection.__init__(self, **kwargs). 934 self.set_sizes(sizes). 935 self.set_verts(verts, closed). [...]/lib/",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/559
https://github.com/scverse/scanpy/issues/559:2296,security,updat,update,2296,"!)"" % (label_namer, func.__name__),. 1804 RuntimeWarning, stacklevel=2). -> 1805 return func(ax, *args, **kwargs). 1806 . 1807 inner.__doc__ = _add_data_doc(inner.__doc__,. [...]/lib/python3.6/site-packages/matplotlib/axes/_axes.py in pcolor(self, alpha, norm, cmap, vmin, vmax, *args, **kwargs). 5762 kwargs.setdefault('snap', False). 5763 . -> 5764 collection = mcoll.PolyCollection(verts, **kwargs). 5765 . 5766 collection.set_alpha(alpha). [...]/lib/python3.6/site-packages/matplotlib/collections.py in __init__(self, verts, sizes, closed, **kwargs). 931 %(Collection)s. 932 """""". --> 933 Collection.__init__(self, **kwargs). 934 self.set_sizes(sizes). 935 self.set_verts(verts, closed). [...]/lib/python3.6/site-packages/matplotlib/collections.py in __init__(self, edgecolors, facecolors, linewidths, linestyles, capstyle, joinstyle, antialiaseds, offsets, transOffset, norm, cmap, pickradius, hatch, urls, offset_position, zorder, **kwargs). 164 . 165 self._path_effects = None. --> 166 self.update(kwargs). 167 self._paths = None. 168 . [...]/lib/python3.6/site-packages/matplotlib/artist.py in update(self, props). 914 . 915 with cbook._setattr_cm(self, eventson=False):. --> 916 ret = [_update_property(self, k, v) for k, v in props.items()]. 917 . 918 if len(ret):. [...]/lib/python3.6/site-packages/matplotlib/artist.py in <listcomp>(.0). 914 . 915 with cbook._setattr_cm(self, eventson=False):. --> 916 ret = [_update_property(self, k, v) for k, v in props.items()]. 917 . 918 if len(ret):. [...]/lib/python3.6/site-packages/matplotlib/artist.py in _update_property(self, k, v). 910 func = getattr(self, 'set_' + k, None). 911 if not callable(func):. --> 912 raise AttributeError('Unknown property %s' % k). 913 return func(v). 914 . AttributeError: Unknown property standard_scale. ```. Any idea of why I'm getting this? . Package info:. ```. scanpy==1.4 anndata==0.6.18 numpy==1.16.2 scipy==1.2.0 pandas==0.24.1 scikit-learn==0.20.3 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/559
https://github.com/scverse/scanpy/issues/559:2400,security,updat,update,2400,"**kwargs). 1806 . 1807 inner.__doc__ = _add_data_doc(inner.__doc__,. [...]/lib/python3.6/site-packages/matplotlib/axes/_axes.py in pcolor(self, alpha, norm, cmap, vmin, vmax, *args, **kwargs). 5762 kwargs.setdefault('snap', False). 5763 . -> 5764 collection = mcoll.PolyCollection(verts, **kwargs). 5765 . 5766 collection.set_alpha(alpha). [...]/lib/python3.6/site-packages/matplotlib/collections.py in __init__(self, verts, sizes, closed, **kwargs). 931 %(Collection)s. 932 """""". --> 933 Collection.__init__(self, **kwargs). 934 self.set_sizes(sizes). 935 self.set_verts(verts, closed). [...]/lib/python3.6/site-packages/matplotlib/collections.py in __init__(self, edgecolors, facecolors, linewidths, linestyles, capstyle, joinstyle, antialiaseds, offsets, transOffset, norm, cmap, pickradius, hatch, urls, offset_position, zorder, **kwargs). 164 . 165 self._path_effects = None. --> 166 self.update(kwargs). 167 self._paths = None. 168 . [...]/lib/python3.6/site-packages/matplotlib/artist.py in update(self, props). 914 . 915 with cbook._setattr_cm(self, eventson=False):. --> 916 ret = [_update_property(self, k, v) for k, v in props.items()]. 917 . 918 if len(ret):. [...]/lib/python3.6/site-packages/matplotlib/artist.py in <listcomp>(.0). 914 . 915 with cbook._setattr_cm(self, eventson=False):. --> 916 ret = [_update_property(self, k, v) for k, v in props.items()]. 917 . 918 if len(ret):. [...]/lib/python3.6/site-packages/matplotlib/artist.py in _update_property(self, k, v). 910 func = getattr(self, 'set_' + k, None). 911 if not callable(func):. --> 912 raise AttributeError('Unknown property %s' % k). 913 return func(v). 914 . AttributeError: Unknown property standard_scale. ```. Any idea of why I'm getting this? . Package info:. ```. scanpy==1.4 anndata==0.6.18 numpy==1.16.2 scipy==1.2.0 pandas==0.24.1 scikit-learn==0.20.3 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 . ```. Thank you! PS: this happens also when I just use the example data as in [here](https://scanpy-tu",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/559
https://github.com/scverse/scanpy/issues/559:549,testability,Trace,Traceback,549,"matrixplot - standard_scale issue; Hi @fidelram ,. When I try to use . ```. plt = sc.pl.matrixplot(adata, marker_genes, groupby='louvain'). ```. I get this heatmap. . ![louv1](https://user-images.githubusercontent.com/11874103/54995344-d2c8ba80-4fc6-11e9-84fe-4f659915293d.png). But as soon as I add ```standard_scale='var'```:. ```. plt = sc.pl.matrixplot(adata, marker_genes, groupby='louvain', standard_scale='var'). ```. I get the following error. ```. ---------------------------------------------------------------------------. AttributeError Traceback (most recent call last). <ipython-input-24-4ac38158d4d0> in <module>. ----> 1 plt = sc.pl.matrixplot(adata, marker_genes, groupby='louvain', standard_scale='var'). [...]/lib/python3.6/site-packages/scanpy/plotting/_anndata.py in matrixplot(adata, var_names, groupby, use_raw, log, num_categories, figsize, dendrogram, var_group_positions, var_group_labels, var_group_rotation, layer, swap_axes, show, save, **kwds). 1683 _plot_dendrogram(dendro_ax, adata, ticks=y_ticks). 1684 . -> 1685 pc = matrix_ax.pcolor(mean_obs, edgecolor='gray', **kwds). 1686 . 1687 # invert y axis to show categories ordered from top to bottom. [...]/lib/python3.6/site-packages/matplotlib/__init__.py in inner(ax, data, *args, **kwargs). 1803 ""the Matplotlib list!)"" % (label_namer, func.__name__),. 1804 RuntimeWarning, stacklevel=2). -> 1805 return func(ax, *args, **kwargs). 1806 . 1807 inner.__doc__ = _add_data_doc(inner.__doc__,. [...]/lib/python3.6/site-packages/matplotlib/axes/_axes.py in pcolor(self, alpha, norm, cmap, vmin, vmax, *args, **kwargs). 5762 kwargs.setdefault('snap', False). 5763 . -> 5764 collection = mcoll.PolyCollection(verts, **kwargs). 5765 . 5766 collection.set_alpha(alpha). [...]/lib/python3.6/site-packages/matplotlib/collections.py in __init__(self, verts, sizes, closed, **kwargs). 931 %(Collection)s. 932 """""". --> 933 Collection.__init__(self, **kwargs). 934 self.set_sizes(sizes). 935 self.set_verts(verts, closed). [...]/lib/",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/559
https://github.com/scverse/scanpy/issues/559:835,testability,log,log,835,"matrixplot - standard_scale issue; Hi @fidelram ,. When I try to use . ```. plt = sc.pl.matrixplot(adata, marker_genes, groupby='louvain'). ```. I get this heatmap. . ![louv1](https://user-images.githubusercontent.com/11874103/54995344-d2c8ba80-4fc6-11e9-84fe-4f659915293d.png). But as soon as I add ```standard_scale='var'```:. ```. plt = sc.pl.matrixplot(adata, marker_genes, groupby='louvain', standard_scale='var'). ```. I get the following error. ```. ---------------------------------------------------------------------------. AttributeError Traceback (most recent call last). <ipython-input-24-4ac38158d4d0> in <module>. ----> 1 plt = sc.pl.matrixplot(adata, marker_genes, groupby='louvain', standard_scale='var'). [...]/lib/python3.6/site-packages/scanpy/plotting/_anndata.py in matrixplot(adata, var_names, groupby, use_raw, log, num_categories, figsize, dendrogram, var_group_positions, var_group_labels, var_group_rotation, layer, swap_axes, show, save, **kwds). 1683 _plot_dendrogram(dendro_ax, adata, ticks=y_ticks). 1684 . -> 1685 pc = matrix_ax.pcolor(mean_obs, edgecolor='gray', **kwds). 1686 . 1687 # invert y axis to show categories ordered from top to bottom. [...]/lib/python3.6/site-packages/matplotlib/__init__.py in inner(ax, data, *args, **kwargs). 1803 ""the Matplotlib list!)"" % (label_namer, func.__name__),. 1804 RuntimeWarning, stacklevel=2). -> 1805 return func(ax, *args, **kwargs). 1806 . 1807 inner.__doc__ = _add_data_doc(inner.__doc__,. [...]/lib/python3.6/site-packages/matplotlib/axes/_axes.py in pcolor(self, alpha, norm, cmap, vmin, vmax, *args, **kwargs). 5762 kwargs.setdefault('snap', False). 5763 . -> 5764 collection = mcoll.PolyCollection(verts, **kwargs). 5765 . 5766 collection.set_alpha(alpha). [...]/lib/python3.6/site-packages/matplotlib/collections.py in __init__(self, verts, sizes, closed, **kwargs). 931 %(Collection)s. 932 """""". --> 933 Collection.__init__(self, **kwargs). 934 self.set_sizes(sizes). 935 self.set_verts(verts, closed). [...]/lib/",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/559
https://github.com/scverse/scanpy/issues/559:184,usability,user,user-images,184,"matrixplot - standard_scale issue; Hi @fidelram ,. When I try to use . ```. plt = sc.pl.matrixplot(adata, marker_genes, groupby='louvain'). ```. I get this heatmap. . ![louv1](https://user-images.githubusercontent.com/11874103/54995344-d2c8ba80-4fc6-11e9-84fe-4f659915293d.png). But as soon as I add ```standard_scale='var'```:. ```. plt = sc.pl.matrixplot(adata, marker_genes, groupby='louvain', standard_scale='var'). ```. I get the following error. ```. ---------------------------------------------------------------------------. AttributeError Traceback (most recent call last). <ipython-input-24-4ac38158d4d0> in <module>. ----> 1 plt = sc.pl.matrixplot(adata, marker_genes, groupby='louvain', standard_scale='var'). [...]/lib/python3.6/site-packages/scanpy/plotting/_anndata.py in matrixplot(adata, var_names, groupby, use_raw, log, num_categories, figsize, dendrogram, var_group_positions, var_group_labels, var_group_rotation, layer, swap_axes, show, save, **kwds). 1683 _plot_dendrogram(dendro_ax, adata, ticks=y_ticks). 1684 . -> 1685 pc = matrix_ax.pcolor(mean_obs, edgecolor='gray', **kwds). 1686 . 1687 # invert y axis to show categories ordered from top to bottom. [...]/lib/python3.6/site-packages/matplotlib/__init__.py in inner(ax, data, *args, **kwargs). 1803 ""the Matplotlib list!)"" % (label_namer, func.__name__),. 1804 RuntimeWarning, stacklevel=2). -> 1805 return func(ax, *args, **kwargs). 1806 . 1807 inner.__doc__ = _add_data_doc(inner.__doc__,. [...]/lib/python3.6/site-packages/matplotlib/axes/_axes.py in pcolor(self, alpha, norm, cmap, vmin, vmax, *args, **kwargs). 5762 kwargs.setdefault('snap', False). 5763 . -> 5764 collection = mcoll.PolyCollection(verts, **kwargs). 5765 . 5766 collection.set_alpha(alpha). [...]/lib/python3.6/site-packages/matplotlib/collections.py in __init__(self, verts, sizes, closed, **kwargs). 931 %(Collection)s. 932 """""". --> 933 Collection.__init__(self, **kwargs). 934 self.set_sizes(sizes). 935 self.set_verts(verts, closed). [...]/lib/",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/559
https://github.com/scverse/scanpy/issues/559:445,usability,error,error,445,"matrixplot - standard_scale issue; Hi @fidelram ,. When I try to use . ```. plt = sc.pl.matrixplot(adata, marker_genes, groupby='louvain'). ```. I get this heatmap. . ![louv1](https://user-images.githubusercontent.com/11874103/54995344-d2c8ba80-4fc6-11e9-84fe-4f659915293d.png). But as soon as I add ```standard_scale='var'```:. ```. plt = sc.pl.matrixplot(adata, marker_genes, groupby='louvain', standard_scale='var'). ```. I get the following error. ```. ---------------------------------------------------------------------------. AttributeError Traceback (most recent call last). <ipython-input-24-4ac38158d4d0> in <module>. ----> 1 plt = sc.pl.matrixplot(adata, marker_genes, groupby='louvain', standard_scale='var'). [...]/lib/python3.6/site-packages/scanpy/plotting/_anndata.py in matrixplot(adata, var_names, groupby, use_raw, log, num_categories, figsize, dendrogram, var_group_positions, var_group_labels, var_group_rotation, layer, swap_axes, show, save, **kwds). 1683 _plot_dendrogram(dendro_ax, adata, ticks=y_ticks). 1684 . -> 1685 pc = matrix_ax.pcolor(mean_obs, edgecolor='gray', **kwds). 1686 . 1687 # invert y axis to show categories ordered from top to bottom. [...]/lib/python3.6/site-packages/matplotlib/__init__.py in inner(ax, data, *args, **kwargs). 1803 ""the Matplotlib list!)"" % (label_namer, func.__name__),. 1804 RuntimeWarning, stacklevel=2). -> 1805 return func(ax, *args, **kwargs). 1806 . 1807 inner.__doc__ = _add_data_doc(inner.__doc__,. [...]/lib/python3.6/site-packages/matplotlib/axes/_axes.py in pcolor(self, alpha, norm, cmap, vmin, vmax, *args, **kwargs). 5762 kwargs.setdefault('snap', False). 5763 . -> 5764 collection = mcoll.PolyCollection(verts, **kwargs). 5765 . 5766 collection.set_alpha(alpha). [...]/lib/python3.6/site-packages/matplotlib/collections.py in __init__(self, verts, sizes, closed, **kwargs). 931 %(Collection)s. 932 """""". --> 933 Collection.__init__(self, **kwargs). 934 self.set_sizes(sizes). 935 self.set_verts(verts, closed). [...]/lib/",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/559
https://github.com/scverse/scanpy/issues/559:593,usability,input,input-,593,"matrixplot - standard_scale issue; Hi @fidelram ,. When I try to use . ```. plt = sc.pl.matrixplot(adata, marker_genes, groupby='louvain'). ```. I get this heatmap. . ![louv1](https://user-images.githubusercontent.com/11874103/54995344-d2c8ba80-4fc6-11e9-84fe-4f659915293d.png). But as soon as I add ```standard_scale='var'```:. ```. plt = sc.pl.matrixplot(adata, marker_genes, groupby='louvain', standard_scale='var'). ```. I get the following error. ```. ---------------------------------------------------------------------------. AttributeError Traceback (most recent call last). <ipython-input-24-4ac38158d4d0> in <module>. ----> 1 plt = sc.pl.matrixplot(adata, marker_genes, groupby='louvain', standard_scale='var'). [...]/lib/python3.6/site-packages/scanpy/plotting/_anndata.py in matrixplot(adata, var_names, groupby, use_raw, log, num_categories, figsize, dendrogram, var_group_positions, var_group_labels, var_group_rotation, layer, swap_axes, show, save, **kwds). 1683 _plot_dendrogram(dendro_ax, adata, ticks=y_ticks). 1684 . -> 1685 pc = matrix_ax.pcolor(mean_obs, edgecolor='gray', **kwds). 1686 . 1687 # invert y axis to show categories ordered from top to bottom. [...]/lib/python3.6/site-packages/matplotlib/__init__.py in inner(ax, data, *args, **kwargs). 1803 ""the Matplotlib list!)"" % (label_namer, func.__name__),. 1804 RuntimeWarning, stacklevel=2). -> 1805 return func(ax, *args, **kwargs). 1806 . 1807 inner.__doc__ = _add_data_doc(inner.__doc__,. [...]/lib/python3.6/site-packages/matplotlib/axes/_axes.py in pcolor(self, alpha, norm, cmap, vmin, vmax, *args, **kwargs). 5762 kwargs.setdefault('snap', False). 5763 . -> 5764 collection = mcoll.PolyCollection(verts, **kwargs). 5765 . 5766 collection.set_alpha(alpha). [...]/lib/python3.6/site-packages/matplotlib/collections.py in __init__(self, verts, sizes, closed, **kwargs). 931 %(Collection)s. 932 """""". --> 933 Collection.__init__(self, **kwargs). 934 self.set_sizes(sizes). 935 self.set_verts(verts, closed). [...]/lib/",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/559
https://github.com/scverse/scanpy/issues/559:1835,usability,close,closed,1835,", num_categories, figsize, dendrogram, var_group_positions, var_group_labels, var_group_rotation, layer, swap_axes, show, save, **kwds). 1683 _plot_dendrogram(dendro_ax, adata, ticks=y_ticks). 1684 . -> 1685 pc = matrix_ax.pcolor(mean_obs, edgecolor='gray', **kwds). 1686 . 1687 # invert y axis to show categories ordered from top to bottom. [...]/lib/python3.6/site-packages/matplotlib/__init__.py in inner(ax, data, *args, **kwargs). 1803 ""the Matplotlib list!)"" % (label_namer, func.__name__),. 1804 RuntimeWarning, stacklevel=2). -> 1805 return func(ax, *args, **kwargs). 1806 . 1807 inner.__doc__ = _add_data_doc(inner.__doc__,. [...]/lib/python3.6/site-packages/matplotlib/axes/_axes.py in pcolor(self, alpha, norm, cmap, vmin, vmax, *args, **kwargs). 5762 kwargs.setdefault('snap', False). 5763 . -> 5764 collection = mcoll.PolyCollection(verts, **kwargs). 5765 . 5766 collection.set_alpha(alpha). [...]/lib/python3.6/site-packages/matplotlib/collections.py in __init__(self, verts, sizes, closed, **kwargs). 931 %(Collection)s. 932 """""". --> 933 Collection.__init__(self, **kwargs). 934 self.set_sizes(sizes). 935 self.set_verts(verts, closed). [...]/lib/python3.6/site-packages/matplotlib/collections.py in __init__(self, edgecolors, facecolors, linewidths, linestyles, capstyle, joinstyle, antialiaseds, offsets, transOffset, norm, cmap, pickradius, hatch, urls, offset_position, zorder, **kwargs). 164 . 165 self._path_effects = None. --> 166 self.update(kwargs). 167 self._paths = None. 168 . [...]/lib/python3.6/site-packages/matplotlib/artist.py in update(self, props). 914 . 915 with cbook._setattr_cm(self, eventson=False):. --> 916 ret = [_update_property(self, k, v) for k, v in props.items()]. 917 . 918 if len(ret):. [...]/lib/python3.6/site-packages/matplotlib/artist.py in <listcomp>(.0). 914 . 915 with cbook._setattr_cm(self, eventson=False):. --> 916 ret = [_update_property(self, k, v) for k, v in props.items()]. 917 . 918 if len(ret):. [...]/lib/python3.6/site-packages/mat",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/559
https://github.com/scverse/scanpy/issues/559:1981,usability,close,closed,1981,"t_dendrogram(dendro_ax, adata, ticks=y_ticks). 1684 . -> 1685 pc = matrix_ax.pcolor(mean_obs, edgecolor='gray', **kwds). 1686 . 1687 # invert y axis to show categories ordered from top to bottom. [...]/lib/python3.6/site-packages/matplotlib/__init__.py in inner(ax, data, *args, **kwargs). 1803 ""the Matplotlib list!)"" % (label_namer, func.__name__),. 1804 RuntimeWarning, stacklevel=2). -> 1805 return func(ax, *args, **kwargs). 1806 . 1807 inner.__doc__ = _add_data_doc(inner.__doc__,. [...]/lib/python3.6/site-packages/matplotlib/axes/_axes.py in pcolor(self, alpha, norm, cmap, vmin, vmax, *args, **kwargs). 5762 kwargs.setdefault('snap', False). 5763 . -> 5764 collection = mcoll.PolyCollection(verts, **kwargs). 5765 . 5766 collection.set_alpha(alpha). [...]/lib/python3.6/site-packages/matplotlib/collections.py in __init__(self, verts, sizes, closed, **kwargs). 931 %(Collection)s. 932 """""". --> 933 Collection.__init__(self, **kwargs). 934 self.set_sizes(sizes). 935 self.set_verts(verts, closed). [...]/lib/python3.6/site-packages/matplotlib/collections.py in __init__(self, edgecolors, facecolors, linewidths, linestyles, capstyle, joinstyle, antialiaseds, offsets, transOffset, norm, cmap, pickradius, hatch, urls, offset_position, zorder, **kwargs). 164 . 165 self._path_effects = None. --> 166 self.update(kwargs). 167 self._paths = None. 168 . [...]/lib/python3.6/site-packages/matplotlib/artist.py in update(self, props). 914 . 915 with cbook._setattr_cm(self, eventson=False):. --> 916 ret = [_update_property(self, k, v) for k, v in props.items()]. 917 . 918 if len(ret):. [...]/lib/python3.6/site-packages/matplotlib/artist.py in <listcomp>(.0). 914 . 915 with cbook._setattr_cm(self, eventson=False):. --> 916 ret = [_update_property(self, k, v) for k, v in props.items()]. 917 . 918 if len(ret):. [...]/lib/python3.6/site-packages/matplotlib/artist.py in _update_property(self, k, v). 910 func = getattr(self, 'set_' + k, None). 911 if not callable(func):. --> 912 raise Attribute",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/559
https://github.com/scverse/scanpy/issues/559:3231,usability,learn,learn,3231,"__,. [...]/lib/python3.6/site-packages/matplotlib/axes/_axes.py in pcolor(self, alpha, norm, cmap, vmin, vmax, *args, **kwargs). 5762 kwargs.setdefault('snap', False). 5763 . -> 5764 collection = mcoll.PolyCollection(verts, **kwargs). 5765 . 5766 collection.set_alpha(alpha). [...]/lib/python3.6/site-packages/matplotlib/collections.py in __init__(self, verts, sizes, closed, **kwargs). 931 %(Collection)s. 932 """""". --> 933 Collection.__init__(self, **kwargs). 934 self.set_sizes(sizes). 935 self.set_verts(verts, closed). [...]/lib/python3.6/site-packages/matplotlib/collections.py in __init__(self, edgecolors, facecolors, linewidths, linestyles, capstyle, joinstyle, antialiaseds, offsets, transOffset, norm, cmap, pickradius, hatch, urls, offset_position, zorder, **kwargs). 164 . 165 self._path_effects = None. --> 166 self.update(kwargs). 167 self._paths = None. 168 . [...]/lib/python3.6/site-packages/matplotlib/artist.py in update(self, props). 914 . 915 with cbook._setattr_cm(self, eventson=False):. --> 916 ret = [_update_property(self, k, v) for k, v in props.items()]. 917 . 918 if len(ret):. [...]/lib/python3.6/site-packages/matplotlib/artist.py in <listcomp>(.0). 914 . 915 with cbook._setattr_cm(self, eventson=False):. --> 916 ret = [_update_property(self, k, v) for k, v in props.items()]. 917 . 918 if len(ret):. [...]/lib/python3.6/site-packages/matplotlib/artist.py in _update_property(self, k, v). 910 func = getattr(self, 'set_' + k, None). 911 if not callable(func):. --> 912 raise AttributeError('Unknown property %s' % k). 913 return func(v). 914 . AttributeError: Unknown property standard_scale. ```. Any idea of why I'm getting this? . Package info:. ```. scanpy==1.4 anndata==0.6.18 numpy==1.16.2 scipy==1.2.0 pandas==0.24.1 scikit-learn==0.20.3 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 . ```. Thank you! PS: this happens also when I just use the example data as in [here](https://scanpy-tutorials.readthedocs.io/en/latest/visualizing-marker-genes.html).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/559
https://github.com/scverse/scanpy/issues/559:3436,usability,visual,visualizing-marker-genes,3436,"__,. [...]/lib/python3.6/site-packages/matplotlib/axes/_axes.py in pcolor(self, alpha, norm, cmap, vmin, vmax, *args, **kwargs). 5762 kwargs.setdefault('snap', False). 5763 . -> 5764 collection = mcoll.PolyCollection(verts, **kwargs). 5765 . 5766 collection.set_alpha(alpha). [...]/lib/python3.6/site-packages/matplotlib/collections.py in __init__(self, verts, sizes, closed, **kwargs). 931 %(Collection)s. 932 """""". --> 933 Collection.__init__(self, **kwargs). 934 self.set_sizes(sizes). 935 self.set_verts(verts, closed). [...]/lib/python3.6/site-packages/matplotlib/collections.py in __init__(self, edgecolors, facecolors, linewidths, linestyles, capstyle, joinstyle, antialiaseds, offsets, transOffset, norm, cmap, pickradius, hatch, urls, offset_position, zorder, **kwargs). 164 . 165 self._path_effects = None. --> 166 self.update(kwargs). 167 self._paths = None. 168 . [...]/lib/python3.6/site-packages/matplotlib/artist.py in update(self, props). 914 . 915 with cbook._setattr_cm(self, eventson=False):. --> 916 ret = [_update_property(self, k, v) for k, v in props.items()]. 917 . 918 if len(ret):. [...]/lib/python3.6/site-packages/matplotlib/artist.py in <listcomp>(.0). 914 . 915 with cbook._setattr_cm(self, eventson=False):. --> 916 ret = [_update_property(self, k, v) for k, v in props.items()]. 917 . 918 if len(ret):. [...]/lib/python3.6/site-packages/matplotlib/artist.py in _update_property(self, k, v). 910 func = getattr(self, 'set_' + k, None). 911 if not callable(func):. --> 912 raise AttributeError('Unknown property %s' % k). 913 return func(v). 914 . AttributeError: Unknown property standard_scale. ```. Any idea of why I'm getting this? . Package info:. ```. scanpy==1.4 anndata==0.6.18 numpy==1.16.2 scipy==1.2.0 pandas==0.24.1 scikit-learn==0.20.3 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 . ```. Thank you! PS: this happens also when I just use the example data as in [here](https://scanpy-tutorials.readthedocs.io/en/latest/visualizing-marker-genes.html).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/559
https://github.com/scverse/scanpy/issues/560:66,availability,down,down,66,"pip installation - missing tools?; Hi there,. I was trying do dig down to understand the problem in #559 , and I found out that in my ```plotting/_anndata.py``` [these lines](https://github.com/theislab/scanpy/blob/f33924011f7d0a7924fada933e1a20d7b5ceaac3/scanpy/plotting/_anndata.py#L828-L837) and all the ones related to ```standard_scale``` are missing. So I created a new conda environment and tried to install a new version of scanpy, but this did not solve the issue (i.e. the problem is not with my old environment) as these lines are still missing. . When I tried to replace the file and re-run my heatmap I got a different error:. ```. ---------------------------------------------------------------------------. ModuleNotFoundError Traceback (most recent call last). <ipython-input-5-49e0357ed731> in <module>. ----> 1 sc.pl.matrixplot(pbmc, marker_genes, groupby='bulk_labels', dendrogram=True, standard_scale='var'). /anaconda3/envs/test/lib/python3.6/site-packages/scanpy/plotting/_anndata.py in matrixplot(adata, var_names, groupby, use_raw, log, num_categories, figsize, dendrogram, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, standard_scale, swap_axes, show, save, **kwds). 1644 var_names=var_names,. 1645 var_group_labels=var_group_labels,. -> 1646 var_group_positions=var_group_positions). 1647 . 1648 var_group_labels = dendro_data['var_group_labels']. /anaconda3/envs/test/lib/python3.6/site-packages/scanpy/plotting/_anndata.py in _reorder_categories_after_dendrogram(adata, groupby, dendrogram, var_names, var_group_labels, var_group_positions). 2332 """""". 2333 . -> 2334 key = _get_dendrogram_key(adata, dendrogram, groupby). 2335 . 2336 dendro_info = adata.uns[key]. /anaconda3/envs/test/lib/python3.6/site-packages/scanpy/plotting/_anndata.py in _get_dendrogram_key(adata, dendrogram_key, groupby). 2406 . 2407 if dendrogram_key not in adata.uns:. -> 2408 from ..tools._dendrogram import dendrogram. 2409 logg.warn(""dendrogram data not foun",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/560
https://github.com/scverse/scanpy/issues/560:632,availability,error,error,632,"pip installation - missing tools?; Hi there,. I was trying do dig down to understand the problem in #559 , and I found out that in my ```plotting/_anndata.py``` [these lines](https://github.com/theislab/scanpy/blob/f33924011f7d0a7924fada933e1a20d7b5ceaac3/scanpy/plotting/_anndata.py#L828-L837) and all the ones related to ```standard_scale``` are missing. So I created a new conda environment and tried to install a new version of scanpy, but this did not solve the issue (i.e. the problem is not with my old environment) as these lines are still missing. . When I tried to replace the file and re-run my heatmap I got a different error:. ```. ---------------------------------------------------------------------------. ModuleNotFoundError Traceback (most recent call last). <ipython-input-5-49e0357ed731> in <module>. ----> 1 sc.pl.matrixplot(pbmc, marker_genes, groupby='bulk_labels', dendrogram=True, standard_scale='var'). /anaconda3/envs/test/lib/python3.6/site-packages/scanpy/plotting/_anndata.py in matrixplot(adata, var_names, groupby, use_raw, log, num_categories, figsize, dendrogram, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, standard_scale, swap_axes, show, save, **kwds). 1644 var_names=var_names,. 1645 var_group_labels=var_group_labels,. -> 1646 var_group_positions=var_group_positions). 1647 . 1648 var_group_labels = dendro_data['var_group_labels']. /anaconda3/envs/test/lib/python3.6/site-packages/scanpy/plotting/_anndata.py in _reorder_categories_after_dendrogram(adata, groupby, dendrogram, var_names, var_group_labels, var_group_positions). 2332 """""". 2333 . -> 2334 key = _get_dendrogram_key(adata, dendrogram, groupby). 2335 . 2336 dendro_info = adata.uns[key]. /anaconda3/envs/test/lib/python3.6/site-packages/scanpy/plotting/_anndata.py in _get_dendrogram_key(adata, dendrogram_key, groupby). 2406 . 2407 if dendrogram_key not in adata.uns:. -> 2408 from ..tools._dendrogram import dendrogram. 2409 logg.warn(""dendrogram data not foun",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/560
https://github.com/scverse/scanpy/issues/560:4,deployability,instal,installation,4,"pip installation - missing tools?; Hi there,. I was trying do dig down to understand the problem in #559 , and I found out that in my ```plotting/_anndata.py``` [these lines](https://github.com/theislab/scanpy/blob/f33924011f7d0a7924fada933e1a20d7b5ceaac3/scanpy/plotting/_anndata.py#L828-L837) and all the ones related to ```standard_scale``` are missing. So I created a new conda environment and tried to install a new version of scanpy, but this did not solve the issue (i.e. the problem is not with my old environment) as these lines are still missing. . When I tried to replace the file and re-run my heatmap I got a different error:. ```. ---------------------------------------------------------------------------. ModuleNotFoundError Traceback (most recent call last). <ipython-input-5-49e0357ed731> in <module>. ----> 1 sc.pl.matrixplot(pbmc, marker_genes, groupby='bulk_labels', dendrogram=True, standard_scale='var'). /anaconda3/envs/test/lib/python3.6/site-packages/scanpy/plotting/_anndata.py in matrixplot(adata, var_names, groupby, use_raw, log, num_categories, figsize, dendrogram, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, standard_scale, swap_axes, show, save, **kwds). 1644 var_names=var_names,. 1645 var_group_labels=var_group_labels,. -> 1646 var_group_positions=var_group_positions). 1647 . 1648 var_group_labels = dendro_data['var_group_labels']. /anaconda3/envs/test/lib/python3.6/site-packages/scanpy/plotting/_anndata.py in _reorder_categories_after_dendrogram(adata, groupby, dendrogram, var_names, var_group_labels, var_group_positions). 2332 """""". 2333 . -> 2334 key = _get_dendrogram_key(adata, dendrogram, groupby). 2335 . 2336 dendro_info = adata.uns[key]. /anaconda3/envs/test/lib/python3.6/site-packages/scanpy/plotting/_anndata.py in _get_dendrogram_key(adata, dendrogram_key, groupby). 2406 . 2407 if dendrogram_key not in adata.uns:. -> 2408 from ..tools._dendrogram import dendrogram. 2409 logg.warn(""dendrogram data not foun",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/560
https://github.com/scverse/scanpy/issues/560:407,deployability,instal,install,407,"pip installation - missing tools?; Hi there,. I was trying do dig down to understand the problem in #559 , and I found out that in my ```plotting/_anndata.py``` [these lines](https://github.com/theislab/scanpy/blob/f33924011f7d0a7924fada933e1a20d7b5ceaac3/scanpy/plotting/_anndata.py#L828-L837) and all the ones related to ```standard_scale``` are missing. So I created a new conda environment and tried to install a new version of scanpy, but this did not solve the issue (i.e. the problem is not with my old environment) as these lines are still missing. . When I tried to replace the file and re-run my heatmap I got a different error:. ```. ---------------------------------------------------------------------------. ModuleNotFoundError Traceback (most recent call last). <ipython-input-5-49e0357ed731> in <module>. ----> 1 sc.pl.matrixplot(pbmc, marker_genes, groupby='bulk_labels', dendrogram=True, standard_scale='var'). /anaconda3/envs/test/lib/python3.6/site-packages/scanpy/plotting/_anndata.py in matrixplot(adata, var_names, groupby, use_raw, log, num_categories, figsize, dendrogram, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, standard_scale, swap_axes, show, save, **kwds). 1644 var_names=var_names,. 1645 var_group_labels=var_group_labels,. -> 1646 var_group_positions=var_group_positions). 1647 . 1648 var_group_labels = dendro_data['var_group_labels']. /anaconda3/envs/test/lib/python3.6/site-packages/scanpy/plotting/_anndata.py in _reorder_categories_after_dendrogram(adata, groupby, dendrogram, var_names, var_group_labels, var_group_positions). 2332 """""". 2333 . -> 2334 key = _get_dendrogram_key(adata, dendrogram, groupby). 2335 . 2336 dendro_info = adata.uns[key]. /anaconda3/envs/test/lib/python3.6/site-packages/scanpy/plotting/_anndata.py in _get_dendrogram_key(adata, dendrogram_key, groupby). 2406 . 2407 if dendrogram_key not in adata.uns:. -> 2408 from ..tools._dendrogram import dendrogram. 2409 logg.warn(""dendrogram data not foun",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/560
https://github.com/scverse/scanpy/issues/560:421,deployability,version,version,421,"pip installation - missing tools?; Hi there,. I was trying do dig down to understand the problem in #559 , and I found out that in my ```plotting/_anndata.py``` [these lines](https://github.com/theislab/scanpy/blob/f33924011f7d0a7924fada933e1a20d7b5ceaac3/scanpy/plotting/_anndata.py#L828-L837) and all the ones related to ```standard_scale``` are missing. So I created a new conda environment and tried to install a new version of scanpy, but this did not solve the issue (i.e. the problem is not with my old environment) as these lines are still missing. . When I tried to replace the file and re-run my heatmap I got a different error:. ```. ---------------------------------------------------------------------------. ModuleNotFoundError Traceback (most recent call last). <ipython-input-5-49e0357ed731> in <module>. ----> 1 sc.pl.matrixplot(pbmc, marker_genes, groupby='bulk_labels', dendrogram=True, standard_scale='var'). /anaconda3/envs/test/lib/python3.6/site-packages/scanpy/plotting/_anndata.py in matrixplot(adata, var_names, groupby, use_raw, log, num_categories, figsize, dendrogram, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, standard_scale, swap_axes, show, save, **kwds). 1644 var_names=var_names,. 1645 var_group_labels=var_group_labels,. -> 1646 var_group_positions=var_group_positions). 1647 . 1648 var_group_labels = dendro_data['var_group_labels']. /anaconda3/envs/test/lib/python3.6/site-packages/scanpy/plotting/_anndata.py in _reorder_categories_after_dendrogram(adata, groupby, dendrogram, var_names, var_group_labels, var_group_positions). 2332 """""". 2333 . -> 2334 key = _get_dendrogram_key(adata, dendrogram, groupby). 2335 . 2336 dendro_info = adata.uns[key]. /anaconda3/envs/test/lib/python3.6/site-packages/scanpy/plotting/_anndata.py in _get_dendrogram_key(adata, dendrogram_key, groupby). 2406 . 2407 if dendrogram_key not in adata.uns:. -> 2408 from ..tools._dendrogram import dendrogram. 2409 logg.warn(""dendrogram data not foun",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/560
https://github.com/scverse/scanpy/issues/560:722,deployability,Modul,ModuleNotFoundError,722,"pip installation - missing tools?; Hi there,. I was trying do dig down to understand the problem in #559 , and I found out that in my ```plotting/_anndata.py``` [these lines](https://github.com/theislab/scanpy/blob/f33924011f7d0a7924fada933e1a20d7b5ceaac3/scanpy/plotting/_anndata.py#L828-L837) and all the ones related to ```standard_scale``` are missing. So I created a new conda environment and tried to install a new version of scanpy, but this did not solve the issue (i.e. the problem is not with my old environment) as these lines are still missing. . When I tried to replace the file and re-run my heatmap I got a different error:. ```. ---------------------------------------------------------------------------. ModuleNotFoundError Traceback (most recent call last). <ipython-input-5-49e0357ed731> in <module>. ----> 1 sc.pl.matrixplot(pbmc, marker_genes, groupby='bulk_labels', dendrogram=True, standard_scale='var'). /anaconda3/envs/test/lib/python3.6/site-packages/scanpy/plotting/_anndata.py in matrixplot(adata, var_names, groupby, use_raw, log, num_categories, figsize, dendrogram, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, standard_scale, swap_axes, show, save, **kwds). 1644 var_names=var_names,. 1645 var_group_labels=var_group_labels,. -> 1646 var_group_positions=var_group_positions). 1647 . 1648 var_group_labels = dendro_data['var_group_labels']. /anaconda3/envs/test/lib/python3.6/site-packages/scanpy/plotting/_anndata.py in _reorder_categories_after_dendrogram(adata, groupby, dendrogram, var_names, var_group_labels, var_group_positions). 2332 """""". 2333 . -> 2334 key = _get_dendrogram_key(adata, dendrogram, groupby). 2335 . 2336 dendro_info = adata.uns[key]. /anaconda3/envs/test/lib/python3.6/site-packages/scanpy/plotting/_anndata.py in _get_dendrogram_key(adata, dendrogram_key, groupby). 2406 . 2407 if dendrogram_key not in adata.uns:. -> 2408 from ..tools._dendrogram import dendrogram. 2409 logg.warn(""dendrogram data not foun",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/560
https://github.com/scverse/scanpy/issues/560:812,deployability,modul,module,812,"pip installation - missing tools?; Hi there,. I was trying do dig down to understand the problem in #559 , and I found out that in my ```plotting/_anndata.py``` [these lines](https://github.com/theislab/scanpy/blob/f33924011f7d0a7924fada933e1a20d7b5ceaac3/scanpy/plotting/_anndata.py#L828-L837) and all the ones related to ```standard_scale``` are missing. So I created a new conda environment and tried to install a new version of scanpy, but this did not solve the issue (i.e. the problem is not with my old environment) as these lines are still missing. . When I tried to replace the file and re-run my heatmap I got a different error:. ```. ---------------------------------------------------------------------------. ModuleNotFoundError Traceback (most recent call last). <ipython-input-5-49e0357ed731> in <module>. ----> 1 sc.pl.matrixplot(pbmc, marker_genes, groupby='bulk_labels', dendrogram=True, standard_scale='var'). /anaconda3/envs/test/lib/python3.6/site-packages/scanpy/plotting/_anndata.py in matrixplot(adata, var_names, groupby, use_raw, log, num_categories, figsize, dendrogram, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, standard_scale, swap_axes, show, save, **kwds). 1644 var_names=var_names,. 1645 var_group_labels=var_group_labels,. -> 1646 var_group_positions=var_group_positions). 1647 . 1648 var_group_labels = dendro_data['var_group_labels']. /anaconda3/envs/test/lib/python3.6/site-packages/scanpy/plotting/_anndata.py in _reorder_categories_after_dendrogram(adata, groupby, dendrogram, var_names, var_group_labels, var_group_positions). 2332 """""". 2333 . -> 2334 key = _get_dendrogram_key(adata, dendrogram, groupby). 2335 . 2336 dendro_info = adata.uns[key]. /anaconda3/envs/test/lib/python3.6/site-packages/scanpy/plotting/_anndata.py in _get_dendrogram_key(adata, dendrogram_key, groupby). 2406 . 2407 if dendrogram_key not in adata.uns:. -> 2408 from ..tools._dendrogram import dendrogram. 2409 logg.warn(""dendrogram data not foun",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/560
https://github.com/scverse/scanpy/issues/560:1056,deployability,log,log,1056," do dig down to understand the problem in #559 , and I found out that in my ```plotting/_anndata.py``` [these lines](https://github.com/theislab/scanpy/blob/f33924011f7d0a7924fada933e1a20d7b5ceaac3/scanpy/plotting/_anndata.py#L828-L837) and all the ones related to ```standard_scale``` are missing. So I created a new conda environment and tried to install a new version of scanpy, but this did not solve the issue (i.e. the problem is not with my old environment) as these lines are still missing. . When I tried to replace the file and re-run my heatmap I got a different error:. ```. ---------------------------------------------------------------------------. ModuleNotFoundError Traceback (most recent call last). <ipython-input-5-49e0357ed731> in <module>. ----> 1 sc.pl.matrixplot(pbmc, marker_genes, groupby='bulk_labels', dendrogram=True, standard_scale='var'). /anaconda3/envs/test/lib/python3.6/site-packages/scanpy/plotting/_anndata.py in matrixplot(adata, var_names, groupby, use_raw, log, num_categories, figsize, dendrogram, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, standard_scale, swap_axes, show, save, **kwds). 1644 var_names=var_names,. 1645 var_group_labels=var_group_labels,. -> 1646 var_group_positions=var_group_positions). 1647 . 1648 var_group_labels = dendro_data['var_group_labels']. /anaconda3/envs/test/lib/python3.6/site-packages/scanpy/plotting/_anndata.py in _reorder_categories_after_dendrogram(adata, groupby, dendrogram, var_names, var_group_labels, var_group_positions). 2332 """""". 2333 . -> 2334 key = _get_dendrogram_key(adata, dendrogram, groupby). 2335 . 2336 dendro_info = adata.uns[key]. /anaconda3/envs/test/lib/python3.6/site-packages/scanpy/plotting/_anndata.py in _get_dendrogram_key(adata, dendrogram_key, groupby). 2406 . 2407 if dendrogram_key not in adata.uns:. -> 2408 from ..tools._dendrogram import dendrogram. 2409 logg.warn(""dendrogram data not found (using key={}). Running `sc.tl.dendrogram` "". 2410 ""with",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/560
https://github.com/scverse/scanpy/issues/560:1965,deployability,log,logg,1965,"o ```standard_scale``` are missing. So I created a new conda environment and tried to install a new version of scanpy, but this did not solve the issue (i.e. the problem is not with my old environment) as these lines are still missing. . When I tried to replace the file and re-run my heatmap I got a different error:. ```. ---------------------------------------------------------------------------. ModuleNotFoundError Traceback (most recent call last). <ipython-input-5-49e0357ed731> in <module>. ----> 1 sc.pl.matrixplot(pbmc, marker_genes, groupby='bulk_labels', dendrogram=True, standard_scale='var'). /anaconda3/envs/test/lib/python3.6/site-packages/scanpy/plotting/_anndata.py in matrixplot(adata, var_names, groupby, use_raw, log, num_categories, figsize, dendrogram, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, standard_scale, swap_axes, show, save, **kwds). 1644 var_names=var_names,. 1645 var_group_labels=var_group_labels,. -> 1646 var_group_positions=var_group_positions). 1647 . 1648 var_group_labels = dendro_data['var_group_labels']. /anaconda3/envs/test/lib/python3.6/site-packages/scanpy/plotting/_anndata.py in _reorder_categories_after_dendrogram(adata, groupby, dendrogram, var_names, var_group_labels, var_group_positions). 2332 """""". 2333 . -> 2334 key = _get_dendrogram_key(adata, dendrogram, groupby). 2335 . 2336 dendro_info = adata.uns[key]. /anaconda3/envs/test/lib/python3.6/site-packages/scanpy/plotting/_anndata.py in _get_dendrogram_key(adata, dendrogram_key, groupby). 2406 . 2407 if dendrogram_key not in adata.uns:. -> 2408 from ..tools._dendrogram import dendrogram. 2409 logg.warn(""dendrogram data not found (using key={}). Running `sc.tl.dendrogram` "". 2410 ""with default parameters. For fine tuning it is recommended to run `sc.tl.dendrogram` "". ModuleNotFoundError: No module named 'scanpy.tools._dendrogram'. ```. And indeed, in my scanpy this dendrogram tool is missing. . Is the PyPi version missing something? Thank you!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/560
https://github.com/scverse/scanpy/issues/560:2142,deployability,Modul,ModuleNotFoundError,2142,"o ```standard_scale``` are missing. So I created a new conda environment and tried to install a new version of scanpy, but this did not solve the issue (i.e. the problem is not with my old environment) as these lines are still missing. . When I tried to replace the file and re-run my heatmap I got a different error:. ```. ---------------------------------------------------------------------------. ModuleNotFoundError Traceback (most recent call last). <ipython-input-5-49e0357ed731> in <module>. ----> 1 sc.pl.matrixplot(pbmc, marker_genes, groupby='bulk_labels', dendrogram=True, standard_scale='var'). /anaconda3/envs/test/lib/python3.6/site-packages/scanpy/plotting/_anndata.py in matrixplot(adata, var_names, groupby, use_raw, log, num_categories, figsize, dendrogram, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, standard_scale, swap_axes, show, save, **kwds). 1644 var_names=var_names,. 1645 var_group_labels=var_group_labels,. -> 1646 var_group_positions=var_group_positions). 1647 . 1648 var_group_labels = dendro_data['var_group_labels']. /anaconda3/envs/test/lib/python3.6/site-packages/scanpy/plotting/_anndata.py in _reorder_categories_after_dendrogram(adata, groupby, dendrogram, var_names, var_group_labels, var_group_positions). 2332 """""". 2333 . -> 2334 key = _get_dendrogram_key(adata, dendrogram, groupby). 2335 . 2336 dendro_info = adata.uns[key]. /anaconda3/envs/test/lib/python3.6/site-packages/scanpy/plotting/_anndata.py in _get_dendrogram_key(adata, dendrogram_key, groupby). 2406 . 2407 if dendrogram_key not in adata.uns:. -> 2408 from ..tools._dendrogram import dendrogram. 2409 logg.warn(""dendrogram data not found (using key={}). Running `sc.tl.dendrogram` "". 2410 ""with default parameters. For fine tuning it is recommended to run `sc.tl.dendrogram` "". ModuleNotFoundError: No module named 'scanpy.tools._dendrogram'. ```. And indeed, in my scanpy this dendrogram tool is missing. . Is the PyPi version missing something? Thank you!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/560
https://github.com/scverse/scanpy/issues/560:2166,deployability,modul,module,2166,"o ```standard_scale``` are missing. So I created a new conda environment and tried to install a new version of scanpy, but this did not solve the issue (i.e. the problem is not with my old environment) as these lines are still missing. . When I tried to replace the file and re-run my heatmap I got a different error:. ```. ---------------------------------------------------------------------------. ModuleNotFoundError Traceback (most recent call last). <ipython-input-5-49e0357ed731> in <module>. ----> 1 sc.pl.matrixplot(pbmc, marker_genes, groupby='bulk_labels', dendrogram=True, standard_scale='var'). /anaconda3/envs/test/lib/python3.6/site-packages/scanpy/plotting/_anndata.py in matrixplot(adata, var_names, groupby, use_raw, log, num_categories, figsize, dendrogram, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, standard_scale, swap_axes, show, save, **kwds). 1644 var_names=var_names,. 1645 var_group_labels=var_group_labels,. -> 1646 var_group_positions=var_group_positions). 1647 . 1648 var_group_labels = dendro_data['var_group_labels']. /anaconda3/envs/test/lib/python3.6/site-packages/scanpy/plotting/_anndata.py in _reorder_categories_after_dendrogram(adata, groupby, dendrogram, var_names, var_group_labels, var_group_positions). 2332 """""". 2333 . -> 2334 key = _get_dendrogram_key(adata, dendrogram, groupby). 2335 . 2336 dendro_info = adata.uns[key]. /anaconda3/envs/test/lib/python3.6/site-packages/scanpy/plotting/_anndata.py in _get_dendrogram_key(adata, dendrogram_key, groupby). 2406 . 2407 if dendrogram_key not in adata.uns:. -> 2408 from ..tools._dendrogram import dendrogram. 2409 logg.warn(""dendrogram data not found (using key={}). Running `sc.tl.dendrogram` "". 2410 ""with default parameters. For fine tuning it is recommended to run `sc.tl.dendrogram` "". ModuleNotFoundError: No module named 'scanpy.tools._dendrogram'. ```. And indeed, in my scanpy this dendrogram tool is missing. . Is the PyPi version missing something? Thank you!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/560
https://github.com/scverse/scanpy/issues/560:2284,deployability,version,version,2284,"o ```standard_scale``` are missing. So I created a new conda environment and tried to install a new version of scanpy, but this did not solve the issue (i.e. the problem is not with my old environment) as these lines are still missing. . When I tried to replace the file and re-run my heatmap I got a different error:. ```. ---------------------------------------------------------------------------. ModuleNotFoundError Traceback (most recent call last). <ipython-input-5-49e0357ed731> in <module>. ----> 1 sc.pl.matrixplot(pbmc, marker_genes, groupby='bulk_labels', dendrogram=True, standard_scale='var'). /anaconda3/envs/test/lib/python3.6/site-packages/scanpy/plotting/_anndata.py in matrixplot(adata, var_names, groupby, use_raw, log, num_categories, figsize, dendrogram, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, standard_scale, swap_axes, show, save, **kwds). 1644 var_names=var_names,. 1645 var_group_labels=var_group_labels,. -> 1646 var_group_positions=var_group_positions). 1647 . 1648 var_group_labels = dendro_data['var_group_labels']. /anaconda3/envs/test/lib/python3.6/site-packages/scanpy/plotting/_anndata.py in _reorder_categories_after_dendrogram(adata, groupby, dendrogram, var_names, var_group_labels, var_group_positions). 2332 """""". 2333 . -> 2334 key = _get_dendrogram_key(adata, dendrogram, groupby). 2335 . 2336 dendro_info = adata.uns[key]. /anaconda3/envs/test/lib/python3.6/site-packages/scanpy/plotting/_anndata.py in _get_dendrogram_key(adata, dendrogram_key, groupby). 2406 . 2407 if dendrogram_key not in adata.uns:. -> 2408 from ..tools._dendrogram import dendrogram. 2409 logg.warn(""dendrogram data not found (using key={}). Running `sc.tl.dendrogram` "". 2410 ""with default parameters. For fine tuning it is recommended to run `sc.tl.dendrogram` "". ModuleNotFoundError: No module named 'scanpy.tools._dendrogram'. ```. And indeed, in my scanpy this dendrogram tool is missing. . Is the PyPi version missing something? Thank you!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/560
https://github.com/scverse/scanpy/issues/560:606,energy efficiency,heat,heatmap,606,"pip installation - missing tools?; Hi there,. I was trying do dig down to understand the problem in #559 , and I found out that in my ```plotting/_anndata.py``` [these lines](https://github.com/theislab/scanpy/blob/f33924011f7d0a7924fada933e1a20d7b5ceaac3/scanpy/plotting/_anndata.py#L828-L837) and all the ones related to ```standard_scale``` are missing. So I created a new conda environment and tried to install a new version of scanpy, but this did not solve the issue (i.e. the problem is not with my old environment) as these lines are still missing. . When I tried to replace the file and re-run my heatmap I got a different error:. ```. ---------------------------------------------------------------------------. ModuleNotFoundError Traceback (most recent call last). <ipython-input-5-49e0357ed731> in <module>. ----> 1 sc.pl.matrixplot(pbmc, marker_genes, groupby='bulk_labels', dendrogram=True, standard_scale='var'). /anaconda3/envs/test/lib/python3.6/site-packages/scanpy/plotting/_anndata.py in matrixplot(adata, var_names, groupby, use_raw, log, num_categories, figsize, dendrogram, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, standard_scale, swap_axes, show, save, **kwds). 1644 var_names=var_names,. 1645 var_group_labels=var_group_labels,. -> 1646 var_group_positions=var_group_positions). 1647 . 1648 var_group_labels = dendro_data['var_group_labels']. /anaconda3/envs/test/lib/python3.6/site-packages/scanpy/plotting/_anndata.py in _reorder_categories_after_dendrogram(adata, groupby, dendrogram, var_names, var_group_labels, var_group_positions). 2332 """""". 2333 . -> 2334 key = _get_dendrogram_key(adata, dendrogram, groupby). 2335 . 2336 dendro_info = adata.uns[key]. /anaconda3/envs/test/lib/python3.6/site-packages/scanpy/plotting/_anndata.py in _get_dendrogram_key(adata, dendrogram_key, groupby). 2406 . 2407 if dendrogram_key not in adata.uns:. -> 2408 from ..tools._dendrogram import dendrogram. 2409 logg.warn(""dendrogram data not foun",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/560
https://github.com/scverse/scanpy/issues/560:421,integrability,version,version,421,"pip installation - missing tools?; Hi there,. I was trying do dig down to understand the problem in #559 , and I found out that in my ```plotting/_anndata.py``` [these lines](https://github.com/theislab/scanpy/blob/f33924011f7d0a7924fada933e1a20d7b5ceaac3/scanpy/plotting/_anndata.py#L828-L837) and all the ones related to ```standard_scale``` are missing. So I created a new conda environment and tried to install a new version of scanpy, but this did not solve the issue (i.e. the problem is not with my old environment) as these lines are still missing. . When I tried to replace the file and re-run my heatmap I got a different error:. ```. ---------------------------------------------------------------------------. ModuleNotFoundError Traceback (most recent call last). <ipython-input-5-49e0357ed731> in <module>. ----> 1 sc.pl.matrixplot(pbmc, marker_genes, groupby='bulk_labels', dendrogram=True, standard_scale='var'). /anaconda3/envs/test/lib/python3.6/site-packages/scanpy/plotting/_anndata.py in matrixplot(adata, var_names, groupby, use_raw, log, num_categories, figsize, dendrogram, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, standard_scale, swap_axes, show, save, **kwds). 1644 var_names=var_names,. 1645 var_group_labels=var_group_labels,. -> 1646 var_group_positions=var_group_positions). 1647 . 1648 var_group_labels = dendro_data['var_group_labels']. /anaconda3/envs/test/lib/python3.6/site-packages/scanpy/plotting/_anndata.py in _reorder_categories_after_dendrogram(adata, groupby, dendrogram, var_names, var_group_labels, var_group_positions). 2332 """""". 2333 . -> 2334 key = _get_dendrogram_key(adata, dendrogram, groupby). 2335 . 2336 dendro_info = adata.uns[key]. /anaconda3/envs/test/lib/python3.6/site-packages/scanpy/plotting/_anndata.py in _get_dendrogram_key(adata, dendrogram_key, groupby). 2406 . 2407 if dendrogram_key not in adata.uns:. -> 2408 from ..tools._dendrogram import dendrogram. 2409 logg.warn(""dendrogram data not foun",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/560
https://github.com/scverse/scanpy/issues/560:2284,integrability,version,version,2284,"o ```standard_scale``` are missing. So I created a new conda environment and tried to install a new version of scanpy, but this did not solve the issue (i.e. the problem is not with my old environment) as these lines are still missing. . When I tried to replace the file and re-run my heatmap I got a different error:. ```. ---------------------------------------------------------------------------. ModuleNotFoundError Traceback (most recent call last). <ipython-input-5-49e0357ed731> in <module>. ----> 1 sc.pl.matrixplot(pbmc, marker_genes, groupby='bulk_labels', dendrogram=True, standard_scale='var'). /anaconda3/envs/test/lib/python3.6/site-packages/scanpy/plotting/_anndata.py in matrixplot(adata, var_names, groupby, use_raw, log, num_categories, figsize, dendrogram, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, standard_scale, swap_axes, show, save, **kwds). 1644 var_names=var_names,. 1645 var_group_labels=var_group_labels,. -> 1646 var_group_positions=var_group_positions). 1647 . 1648 var_group_labels = dendro_data['var_group_labels']. /anaconda3/envs/test/lib/python3.6/site-packages/scanpy/plotting/_anndata.py in _reorder_categories_after_dendrogram(adata, groupby, dendrogram, var_names, var_group_labels, var_group_positions). 2332 """""". 2333 . -> 2334 key = _get_dendrogram_key(adata, dendrogram, groupby). 2335 . 2336 dendro_info = adata.uns[key]. /anaconda3/envs/test/lib/python3.6/site-packages/scanpy/plotting/_anndata.py in _get_dendrogram_key(adata, dendrogram_key, groupby). 2406 . 2407 if dendrogram_key not in adata.uns:. -> 2408 from ..tools._dendrogram import dendrogram. 2409 logg.warn(""dendrogram data not found (using key={}). Running `sc.tl.dendrogram` "". 2410 ""with default parameters. For fine tuning it is recommended to run `sc.tl.dendrogram` "". ModuleNotFoundError: No module named 'scanpy.tools._dendrogram'. ```. And indeed, in my scanpy this dendrogram tool is missing. . Is the PyPi version missing something? Thank you!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/560
https://github.com/scverse/scanpy/issues/560:421,modifiability,version,version,421,"pip installation - missing tools?; Hi there,. I was trying do dig down to understand the problem in #559 , and I found out that in my ```plotting/_anndata.py``` [these lines](https://github.com/theislab/scanpy/blob/f33924011f7d0a7924fada933e1a20d7b5ceaac3/scanpy/plotting/_anndata.py#L828-L837) and all the ones related to ```standard_scale``` are missing. So I created a new conda environment and tried to install a new version of scanpy, but this did not solve the issue (i.e. the problem is not with my old environment) as these lines are still missing. . When I tried to replace the file and re-run my heatmap I got a different error:. ```. ---------------------------------------------------------------------------. ModuleNotFoundError Traceback (most recent call last). <ipython-input-5-49e0357ed731> in <module>. ----> 1 sc.pl.matrixplot(pbmc, marker_genes, groupby='bulk_labels', dendrogram=True, standard_scale='var'). /anaconda3/envs/test/lib/python3.6/site-packages/scanpy/plotting/_anndata.py in matrixplot(adata, var_names, groupby, use_raw, log, num_categories, figsize, dendrogram, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, standard_scale, swap_axes, show, save, **kwds). 1644 var_names=var_names,. 1645 var_group_labels=var_group_labels,. -> 1646 var_group_positions=var_group_positions). 1647 . 1648 var_group_labels = dendro_data['var_group_labels']. /anaconda3/envs/test/lib/python3.6/site-packages/scanpy/plotting/_anndata.py in _reorder_categories_after_dendrogram(adata, groupby, dendrogram, var_names, var_group_labels, var_group_positions). 2332 """""". 2333 . -> 2334 key = _get_dendrogram_key(adata, dendrogram, groupby). 2335 . 2336 dendro_info = adata.uns[key]. /anaconda3/envs/test/lib/python3.6/site-packages/scanpy/plotting/_anndata.py in _get_dendrogram_key(adata, dendrogram_key, groupby). 2406 . 2407 if dendrogram_key not in adata.uns:. -> 2408 from ..tools._dendrogram import dendrogram. 2409 logg.warn(""dendrogram data not foun",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/560
https://github.com/scverse/scanpy/issues/560:722,modifiability,Modul,ModuleNotFoundError,722,"pip installation - missing tools?; Hi there,. I was trying do dig down to understand the problem in #559 , and I found out that in my ```plotting/_anndata.py``` [these lines](https://github.com/theislab/scanpy/blob/f33924011f7d0a7924fada933e1a20d7b5ceaac3/scanpy/plotting/_anndata.py#L828-L837) and all the ones related to ```standard_scale``` are missing. So I created a new conda environment and tried to install a new version of scanpy, but this did not solve the issue (i.e. the problem is not with my old environment) as these lines are still missing. . When I tried to replace the file and re-run my heatmap I got a different error:. ```. ---------------------------------------------------------------------------. ModuleNotFoundError Traceback (most recent call last). <ipython-input-5-49e0357ed731> in <module>. ----> 1 sc.pl.matrixplot(pbmc, marker_genes, groupby='bulk_labels', dendrogram=True, standard_scale='var'). /anaconda3/envs/test/lib/python3.6/site-packages/scanpy/plotting/_anndata.py in matrixplot(adata, var_names, groupby, use_raw, log, num_categories, figsize, dendrogram, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, standard_scale, swap_axes, show, save, **kwds). 1644 var_names=var_names,. 1645 var_group_labels=var_group_labels,. -> 1646 var_group_positions=var_group_positions). 1647 . 1648 var_group_labels = dendro_data['var_group_labels']. /anaconda3/envs/test/lib/python3.6/site-packages/scanpy/plotting/_anndata.py in _reorder_categories_after_dendrogram(adata, groupby, dendrogram, var_names, var_group_labels, var_group_positions). 2332 """""". 2333 . -> 2334 key = _get_dendrogram_key(adata, dendrogram, groupby). 2335 . 2336 dendro_info = adata.uns[key]. /anaconda3/envs/test/lib/python3.6/site-packages/scanpy/plotting/_anndata.py in _get_dendrogram_key(adata, dendrogram_key, groupby). 2406 . 2407 if dendrogram_key not in adata.uns:. -> 2408 from ..tools._dendrogram import dendrogram. 2409 logg.warn(""dendrogram data not foun",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/560
https://github.com/scverse/scanpy/issues/560:812,modifiability,modul,module,812,"pip installation - missing tools?; Hi there,. I was trying do dig down to understand the problem in #559 , and I found out that in my ```plotting/_anndata.py``` [these lines](https://github.com/theislab/scanpy/blob/f33924011f7d0a7924fada933e1a20d7b5ceaac3/scanpy/plotting/_anndata.py#L828-L837) and all the ones related to ```standard_scale``` are missing. So I created a new conda environment and tried to install a new version of scanpy, but this did not solve the issue (i.e. the problem is not with my old environment) as these lines are still missing. . When I tried to replace the file and re-run my heatmap I got a different error:. ```. ---------------------------------------------------------------------------. ModuleNotFoundError Traceback (most recent call last). <ipython-input-5-49e0357ed731> in <module>. ----> 1 sc.pl.matrixplot(pbmc, marker_genes, groupby='bulk_labels', dendrogram=True, standard_scale='var'). /anaconda3/envs/test/lib/python3.6/site-packages/scanpy/plotting/_anndata.py in matrixplot(adata, var_names, groupby, use_raw, log, num_categories, figsize, dendrogram, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, standard_scale, swap_axes, show, save, **kwds). 1644 var_names=var_names,. 1645 var_group_labels=var_group_labels,. -> 1646 var_group_positions=var_group_positions). 1647 . 1648 var_group_labels = dendro_data['var_group_labels']. /anaconda3/envs/test/lib/python3.6/site-packages/scanpy/plotting/_anndata.py in _reorder_categories_after_dendrogram(adata, groupby, dendrogram, var_names, var_group_labels, var_group_positions). 2332 """""". 2333 . -> 2334 key = _get_dendrogram_key(adata, dendrogram, groupby). 2335 . 2336 dendro_info = adata.uns[key]. /anaconda3/envs/test/lib/python3.6/site-packages/scanpy/plotting/_anndata.py in _get_dendrogram_key(adata, dendrogram_key, groupby). 2406 . 2407 if dendrogram_key not in adata.uns:. -> 2408 from ..tools._dendrogram import dendrogram. 2409 logg.warn(""dendrogram data not foun",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/560
https://github.com/scverse/scanpy/issues/560:969,modifiability,pac,packages,969,"pip installation - missing tools?; Hi there,. I was trying do dig down to understand the problem in #559 , and I found out that in my ```plotting/_anndata.py``` [these lines](https://github.com/theislab/scanpy/blob/f33924011f7d0a7924fada933e1a20d7b5ceaac3/scanpy/plotting/_anndata.py#L828-L837) and all the ones related to ```standard_scale``` are missing. So I created a new conda environment and tried to install a new version of scanpy, but this did not solve the issue (i.e. the problem is not with my old environment) as these lines are still missing. . When I tried to replace the file and re-run my heatmap I got a different error:. ```. ---------------------------------------------------------------------------. ModuleNotFoundError Traceback (most recent call last). <ipython-input-5-49e0357ed731> in <module>. ----> 1 sc.pl.matrixplot(pbmc, marker_genes, groupby='bulk_labels', dendrogram=True, standard_scale='var'). /anaconda3/envs/test/lib/python3.6/site-packages/scanpy/plotting/_anndata.py in matrixplot(adata, var_names, groupby, use_raw, log, num_categories, figsize, dendrogram, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, standard_scale, swap_axes, show, save, **kwds). 1644 var_names=var_names,. 1645 var_group_labels=var_group_labels,. -> 1646 var_group_positions=var_group_positions). 1647 . 1648 var_group_labels = dendro_data['var_group_labels']. /anaconda3/envs/test/lib/python3.6/site-packages/scanpy/plotting/_anndata.py in _reorder_categories_after_dendrogram(adata, groupby, dendrogram, var_names, var_group_labels, var_group_positions). 2332 """""". 2333 . -> 2334 key = _get_dendrogram_key(adata, dendrogram, groupby). 2335 . 2336 dendro_info = adata.uns[key]. /anaconda3/envs/test/lib/python3.6/site-packages/scanpy/plotting/_anndata.py in _get_dendrogram_key(adata, dendrogram_key, groupby). 2406 . 2407 if dendrogram_key not in adata.uns:. -> 2408 from ..tools._dendrogram import dendrogram. 2409 logg.warn(""dendrogram data not foun",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/560
https://github.com/scverse/scanpy/issues/560:1171,modifiability,layer,layer,1171,"(https://github.com/theislab/scanpy/blob/f33924011f7d0a7924fada933e1a20d7b5ceaac3/scanpy/plotting/_anndata.py#L828-L837) and all the ones related to ```standard_scale``` are missing. So I created a new conda environment and tried to install a new version of scanpy, but this did not solve the issue (i.e. the problem is not with my old environment) as these lines are still missing. . When I tried to replace the file and re-run my heatmap I got a different error:. ```. ---------------------------------------------------------------------------. ModuleNotFoundError Traceback (most recent call last). <ipython-input-5-49e0357ed731> in <module>. ----> 1 sc.pl.matrixplot(pbmc, marker_genes, groupby='bulk_labels', dendrogram=True, standard_scale='var'). /anaconda3/envs/test/lib/python3.6/site-packages/scanpy/plotting/_anndata.py in matrixplot(adata, var_names, groupby, use_raw, log, num_categories, figsize, dendrogram, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, standard_scale, swap_axes, show, save, **kwds). 1644 var_names=var_names,. 1645 var_group_labels=var_group_labels,. -> 1646 var_group_positions=var_group_positions). 1647 . 1648 var_group_labels = dendro_data['var_group_labels']. /anaconda3/envs/test/lib/python3.6/site-packages/scanpy/plotting/_anndata.py in _reorder_categories_after_dendrogram(adata, groupby, dendrogram, var_names, var_group_labels, var_group_positions). 2332 """""". 2333 . -> 2334 key = _get_dendrogram_key(adata, dendrogram, groupby). 2335 . 2336 dendro_info = adata.uns[key]. /anaconda3/envs/test/lib/python3.6/site-packages/scanpy/plotting/_anndata.py in _get_dendrogram_key(adata, dendrogram_key, groupby). 2406 . 2407 if dendrogram_key not in adata.uns:. -> 2408 from ..tools._dendrogram import dendrogram. 2409 logg.warn(""dendrogram data not found (using key={}). Running `sc.tl.dendrogram` "". 2410 ""with default parameters. For fine tuning it is recommended to run `sc.tl.dendrogram` "". ModuleNotFoundError: No module n",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/560
https://github.com/scverse/scanpy/issues/560:1448,modifiability,pac,packages,1448,"o ```standard_scale``` are missing. So I created a new conda environment and tried to install a new version of scanpy, but this did not solve the issue (i.e. the problem is not with my old environment) as these lines are still missing. . When I tried to replace the file and re-run my heatmap I got a different error:. ```. ---------------------------------------------------------------------------. ModuleNotFoundError Traceback (most recent call last). <ipython-input-5-49e0357ed731> in <module>. ----> 1 sc.pl.matrixplot(pbmc, marker_genes, groupby='bulk_labels', dendrogram=True, standard_scale='var'). /anaconda3/envs/test/lib/python3.6/site-packages/scanpy/plotting/_anndata.py in matrixplot(adata, var_names, groupby, use_raw, log, num_categories, figsize, dendrogram, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, standard_scale, swap_axes, show, save, **kwds). 1644 var_names=var_names,. 1645 var_group_labels=var_group_labels,. -> 1646 var_group_positions=var_group_positions). 1647 . 1648 var_group_labels = dendro_data['var_group_labels']. /anaconda3/envs/test/lib/python3.6/site-packages/scanpy/plotting/_anndata.py in _reorder_categories_after_dendrogram(adata, groupby, dendrogram, var_names, var_group_labels, var_group_positions). 2332 """""". 2333 . -> 2334 key = _get_dendrogram_key(adata, dendrogram, groupby). 2335 . 2336 dendro_info = adata.uns[key]. /anaconda3/envs/test/lib/python3.6/site-packages/scanpy/plotting/_anndata.py in _get_dendrogram_key(adata, dendrogram_key, groupby). 2406 . 2407 if dendrogram_key not in adata.uns:. -> 2408 from ..tools._dendrogram import dendrogram. 2409 logg.warn(""dendrogram data not found (using key={}). Running `sc.tl.dendrogram` "". 2410 ""with default parameters. For fine tuning it is recommended to run `sc.tl.dendrogram` "". ModuleNotFoundError: No module named 'scanpy.tools._dendrogram'. ```. And indeed, in my scanpy this dendrogram tool is missing. . Is the PyPi version missing something? Thank you!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/560
https://github.com/scverse/scanpy/issues/560:1766,modifiability,pac,packages,1766,"o ```standard_scale``` are missing. So I created a new conda environment and tried to install a new version of scanpy, but this did not solve the issue (i.e. the problem is not with my old environment) as these lines are still missing. . When I tried to replace the file and re-run my heatmap I got a different error:. ```. ---------------------------------------------------------------------------. ModuleNotFoundError Traceback (most recent call last). <ipython-input-5-49e0357ed731> in <module>. ----> 1 sc.pl.matrixplot(pbmc, marker_genes, groupby='bulk_labels', dendrogram=True, standard_scale='var'). /anaconda3/envs/test/lib/python3.6/site-packages/scanpy/plotting/_anndata.py in matrixplot(adata, var_names, groupby, use_raw, log, num_categories, figsize, dendrogram, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, standard_scale, swap_axes, show, save, **kwds). 1644 var_names=var_names,. 1645 var_group_labels=var_group_labels,. -> 1646 var_group_positions=var_group_positions). 1647 . 1648 var_group_labels = dendro_data['var_group_labels']. /anaconda3/envs/test/lib/python3.6/site-packages/scanpy/plotting/_anndata.py in _reorder_categories_after_dendrogram(adata, groupby, dendrogram, var_names, var_group_labels, var_group_positions). 2332 """""". 2333 . -> 2334 key = _get_dendrogram_key(adata, dendrogram, groupby). 2335 . 2336 dendro_info = adata.uns[key]. /anaconda3/envs/test/lib/python3.6/site-packages/scanpy/plotting/_anndata.py in _get_dendrogram_key(adata, dendrogram_key, groupby). 2406 . 2407 if dendrogram_key not in adata.uns:. -> 2408 from ..tools._dendrogram import dendrogram. 2409 logg.warn(""dendrogram data not found (using key={}). Running `sc.tl.dendrogram` "". 2410 ""with default parameters. For fine tuning it is recommended to run `sc.tl.dendrogram` "". ModuleNotFoundError: No module named 'scanpy.tools._dendrogram'. ```. And indeed, in my scanpy this dendrogram tool is missing. . Is the PyPi version missing something? Thank you!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/560
https://github.com/scverse/scanpy/issues/560:2067,modifiability,paramet,parameters,2067,"o ```standard_scale``` are missing. So I created a new conda environment and tried to install a new version of scanpy, but this did not solve the issue (i.e. the problem is not with my old environment) as these lines are still missing. . When I tried to replace the file and re-run my heatmap I got a different error:. ```. ---------------------------------------------------------------------------. ModuleNotFoundError Traceback (most recent call last). <ipython-input-5-49e0357ed731> in <module>. ----> 1 sc.pl.matrixplot(pbmc, marker_genes, groupby='bulk_labels', dendrogram=True, standard_scale='var'). /anaconda3/envs/test/lib/python3.6/site-packages/scanpy/plotting/_anndata.py in matrixplot(adata, var_names, groupby, use_raw, log, num_categories, figsize, dendrogram, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, standard_scale, swap_axes, show, save, **kwds). 1644 var_names=var_names,. 1645 var_group_labels=var_group_labels,. -> 1646 var_group_positions=var_group_positions). 1647 . 1648 var_group_labels = dendro_data['var_group_labels']. /anaconda3/envs/test/lib/python3.6/site-packages/scanpy/plotting/_anndata.py in _reorder_categories_after_dendrogram(adata, groupby, dendrogram, var_names, var_group_labels, var_group_positions). 2332 """""". 2333 . -> 2334 key = _get_dendrogram_key(adata, dendrogram, groupby). 2335 . 2336 dendro_info = adata.uns[key]. /anaconda3/envs/test/lib/python3.6/site-packages/scanpy/plotting/_anndata.py in _get_dendrogram_key(adata, dendrogram_key, groupby). 2406 . 2407 if dendrogram_key not in adata.uns:. -> 2408 from ..tools._dendrogram import dendrogram. 2409 logg.warn(""dendrogram data not found (using key={}). Running `sc.tl.dendrogram` "". 2410 ""with default parameters. For fine tuning it is recommended to run `sc.tl.dendrogram` "". ModuleNotFoundError: No module named 'scanpy.tools._dendrogram'. ```. And indeed, in my scanpy this dendrogram tool is missing. . Is the PyPi version missing something? Thank you!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/560
https://github.com/scverse/scanpy/issues/560:2142,modifiability,Modul,ModuleNotFoundError,2142,"o ```standard_scale``` are missing. So I created a new conda environment and tried to install a new version of scanpy, but this did not solve the issue (i.e. the problem is not with my old environment) as these lines are still missing. . When I tried to replace the file and re-run my heatmap I got a different error:. ```. ---------------------------------------------------------------------------. ModuleNotFoundError Traceback (most recent call last). <ipython-input-5-49e0357ed731> in <module>. ----> 1 sc.pl.matrixplot(pbmc, marker_genes, groupby='bulk_labels', dendrogram=True, standard_scale='var'). /anaconda3/envs/test/lib/python3.6/site-packages/scanpy/plotting/_anndata.py in matrixplot(adata, var_names, groupby, use_raw, log, num_categories, figsize, dendrogram, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, standard_scale, swap_axes, show, save, **kwds). 1644 var_names=var_names,. 1645 var_group_labels=var_group_labels,. -> 1646 var_group_positions=var_group_positions). 1647 . 1648 var_group_labels = dendro_data['var_group_labels']. /anaconda3/envs/test/lib/python3.6/site-packages/scanpy/plotting/_anndata.py in _reorder_categories_after_dendrogram(adata, groupby, dendrogram, var_names, var_group_labels, var_group_positions). 2332 """""". 2333 . -> 2334 key = _get_dendrogram_key(adata, dendrogram, groupby). 2335 . 2336 dendro_info = adata.uns[key]. /anaconda3/envs/test/lib/python3.6/site-packages/scanpy/plotting/_anndata.py in _get_dendrogram_key(adata, dendrogram_key, groupby). 2406 . 2407 if dendrogram_key not in adata.uns:. -> 2408 from ..tools._dendrogram import dendrogram. 2409 logg.warn(""dendrogram data not found (using key={}). Running `sc.tl.dendrogram` "". 2410 ""with default parameters. For fine tuning it is recommended to run `sc.tl.dendrogram` "". ModuleNotFoundError: No module named 'scanpy.tools._dendrogram'. ```. And indeed, in my scanpy this dendrogram tool is missing. . Is the PyPi version missing something? Thank you!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/560
https://github.com/scverse/scanpy/issues/560:2166,modifiability,modul,module,2166,"o ```standard_scale``` are missing. So I created a new conda environment and tried to install a new version of scanpy, but this did not solve the issue (i.e. the problem is not with my old environment) as these lines are still missing. . When I tried to replace the file and re-run my heatmap I got a different error:. ```. ---------------------------------------------------------------------------. ModuleNotFoundError Traceback (most recent call last). <ipython-input-5-49e0357ed731> in <module>. ----> 1 sc.pl.matrixplot(pbmc, marker_genes, groupby='bulk_labels', dendrogram=True, standard_scale='var'). /anaconda3/envs/test/lib/python3.6/site-packages/scanpy/plotting/_anndata.py in matrixplot(adata, var_names, groupby, use_raw, log, num_categories, figsize, dendrogram, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, standard_scale, swap_axes, show, save, **kwds). 1644 var_names=var_names,. 1645 var_group_labels=var_group_labels,. -> 1646 var_group_positions=var_group_positions). 1647 . 1648 var_group_labels = dendro_data['var_group_labels']. /anaconda3/envs/test/lib/python3.6/site-packages/scanpy/plotting/_anndata.py in _reorder_categories_after_dendrogram(adata, groupby, dendrogram, var_names, var_group_labels, var_group_positions). 2332 """""". 2333 . -> 2334 key = _get_dendrogram_key(adata, dendrogram, groupby). 2335 . 2336 dendro_info = adata.uns[key]. /anaconda3/envs/test/lib/python3.6/site-packages/scanpy/plotting/_anndata.py in _get_dendrogram_key(adata, dendrogram_key, groupby). 2406 . 2407 if dendrogram_key not in adata.uns:. -> 2408 from ..tools._dendrogram import dendrogram. 2409 logg.warn(""dendrogram data not found (using key={}). Running `sc.tl.dendrogram` "". 2410 ""with default parameters. For fine tuning it is recommended to run `sc.tl.dendrogram` "". ModuleNotFoundError: No module named 'scanpy.tools._dendrogram'. ```. And indeed, in my scanpy this dendrogram tool is missing. . Is the PyPi version missing something? Thank you!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/560
https://github.com/scverse/scanpy/issues/560:2284,modifiability,version,version,2284,"o ```standard_scale``` are missing. So I created a new conda environment and tried to install a new version of scanpy, but this did not solve the issue (i.e. the problem is not with my old environment) as these lines are still missing. . When I tried to replace the file and re-run my heatmap I got a different error:. ```. ---------------------------------------------------------------------------. ModuleNotFoundError Traceback (most recent call last). <ipython-input-5-49e0357ed731> in <module>. ----> 1 sc.pl.matrixplot(pbmc, marker_genes, groupby='bulk_labels', dendrogram=True, standard_scale='var'). /anaconda3/envs/test/lib/python3.6/site-packages/scanpy/plotting/_anndata.py in matrixplot(adata, var_names, groupby, use_raw, log, num_categories, figsize, dendrogram, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, standard_scale, swap_axes, show, save, **kwds). 1644 var_names=var_names,. 1645 var_group_labels=var_group_labels,. -> 1646 var_group_positions=var_group_positions). 1647 . 1648 var_group_labels = dendro_data['var_group_labels']. /anaconda3/envs/test/lib/python3.6/site-packages/scanpy/plotting/_anndata.py in _reorder_categories_after_dendrogram(adata, groupby, dendrogram, var_names, var_group_labels, var_group_positions). 2332 """""". 2333 . -> 2334 key = _get_dendrogram_key(adata, dendrogram, groupby). 2335 . 2336 dendro_info = adata.uns[key]. /anaconda3/envs/test/lib/python3.6/site-packages/scanpy/plotting/_anndata.py in _get_dendrogram_key(adata, dendrogram_key, groupby). 2406 . 2407 if dendrogram_key not in adata.uns:. -> 2408 from ..tools._dendrogram import dendrogram. 2409 logg.warn(""dendrogram data not found (using key={}). Running `sc.tl.dendrogram` "". 2410 ""with default parameters. For fine tuning it is recommended to run `sc.tl.dendrogram` "". ModuleNotFoundError: No module named 'scanpy.tools._dendrogram'. ```. And indeed, in my scanpy this dendrogram tool is missing. . Is the PyPi version missing something? Thank you!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/560
https://github.com/scverse/scanpy/issues/560:632,performance,error,error,632,"pip installation - missing tools?; Hi there,. I was trying do dig down to understand the problem in #559 , and I found out that in my ```plotting/_anndata.py``` [these lines](https://github.com/theislab/scanpy/blob/f33924011f7d0a7924fada933e1a20d7b5ceaac3/scanpy/plotting/_anndata.py#L828-L837) and all the ones related to ```standard_scale``` are missing. So I created a new conda environment and tried to install a new version of scanpy, but this did not solve the issue (i.e. the problem is not with my old environment) as these lines are still missing. . When I tried to replace the file and re-run my heatmap I got a different error:. ```. ---------------------------------------------------------------------------. ModuleNotFoundError Traceback (most recent call last). <ipython-input-5-49e0357ed731> in <module>. ----> 1 sc.pl.matrixplot(pbmc, marker_genes, groupby='bulk_labels', dendrogram=True, standard_scale='var'). /anaconda3/envs/test/lib/python3.6/site-packages/scanpy/plotting/_anndata.py in matrixplot(adata, var_names, groupby, use_raw, log, num_categories, figsize, dendrogram, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, standard_scale, swap_axes, show, save, **kwds). 1644 var_names=var_names,. 1645 var_group_labels=var_group_labels,. -> 1646 var_group_positions=var_group_positions). 1647 . 1648 var_group_labels = dendro_data['var_group_labels']. /anaconda3/envs/test/lib/python3.6/site-packages/scanpy/plotting/_anndata.py in _reorder_categories_after_dendrogram(adata, groupby, dendrogram, var_names, var_group_labels, var_group_positions). 2332 """""". 2333 . -> 2334 key = _get_dendrogram_key(adata, dendrogram, groupby). 2335 . 2336 dendro_info = adata.uns[key]. /anaconda3/envs/test/lib/python3.6/site-packages/scanpy/plotting/_anndata.py in _get_dendrogram_key(adata, dendrogram_key, groupby). 2406 . 2407 if dendrogram_key not in adata.uns:. -> 2408 from ..tools._dendrogram import dendrogram. 2409 logg.warn(""dendrogram data not foun",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/560
https://github.com/scverse/scanpy/issues/560:632,safety,error,error,632,"pip installation - missing tools?; Hi there,. I was trying do dig down to understand the problem in #559 , and I found out that in my ```plotting/_anndata.py``` [these lines](https://github.com/theislab/scanpy/blob/f33924011f7d0a7924fada933e1a20d7b5ceaac3/scanpy/plotting/_anndata.py#L828-L837) and all the ones related to ```standard_scale``` are missing. So I created a new conda environment and tried to install a new version of scanpy, but this did not solve the issue (i.e. the problem is not with my old environment) as these lines are still missing. . When I tried to replace the file and re-run my heatmap I got a different error:. ```. ---------------------------------------------------------------------------. ModuleNotFoundError Traceback (most recent call last). <ipython-input-5-49e0357ed731> in <module>. ----> 1 sc.pl.matrixplot(pbmc, marker_genes, groupby='bulk_labels', dendrogram=True, standard_scale='var'). /anaconda3/envs/test/lib/python3.6/site-packages/scanpy/plotting/_anndata.py in matrixplot(adata, var_names, groupby, use_raw, log, num_categories, figsize, dendrogram, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, standard_scale, swap_axes, show, save, **kwds). 1644 var_names=var_names,. 1645 var_group_labels=var_group_labels,. -> 1646 var_group_positions=var_group_positions). 1647 . 1648 var_group_labels = dendro_data['var_group_labels']. /anaconda3/envs/test/lib/python3.6/site-packages/scanpy/plotting/_anndata.py in _reorder_categories_after_dendrogram(adata, groupby, dendrogram, var_names, var_group_labels, var_group_positions). 2332 """""". 2333 . -> 2334 key = _get_dendrogram_key(adata, dendrogram, groupby). 2335 . 2336 dendro_info = adata.uns[key]. /anaconda3/envs/test/lib/python3.6/site-packages/scanpy/plotting/_anndata.py in _get_dendrogram_key(adata, dendrogram_key, groupby). 2406 . 2407 if dendrogram_key not in adata.uns:. -> 2408 from ..tools._dendrogram import dendrogram. 2409 logg.warn(""dendrogram data not foun",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/560
https://github.com/scverse/scanpy/issues/560:722,safety,Modul,ModuleNotFoundError,722,"pip installation - missing tools?; Hi there,. I was trying do dig down to understand the problem in #559 , and I found out that in my ```plotting/_anndata.py``` [these lines](https://github.com/theislab/scanpy/blob/f33924011f7d0a7924fada933e1a20d7b5ceaac3/scanpy/plotting/_anndata.py#L828-L837) and all the ones related to ```standard_scale``` are missing. So I created a new conda environment and tried to install a new version of scanpy, but this did not solve the issue (i.e. the problem is not with my old environment) as these lines are still missing. . When I tried to replace the file and re-run my heatmap I got a different error:. ```. ---------------------------------------------------------------------------. ModuleNotFoundError Traceback (most recent call last). <ipython-input-5-49e0357ed731> in <module>. ----> 1 sc.pl.matrixplot(pbmc, marker_genes, groupby='bulk_labels', dendrogram=True, standard_scale='var'). /anaconda3/envs/test/lib/python3.6/site-packages/scanpy/plotting/_anndata.py in matrixplot(adata, var_names, groupby, use_raw, log, num_categories, figsize, dendrogram, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, standard_scale, swap_axes, show, save, **kwds). 1644 var_names=var_names,. 1645 var_group_labels=var_group_labels,. -> 1646 var_group_positions=var_group_positions). 1647 . 1648 var_group_labels = dendro_data['var_group_labels']. /anaconda3/envs/test/lib/python3.6/site-packages/scanpy/plotting/_anndata.py in _reorder_categories_after_dendrogram(adata, groupby, dendrogram, var_names, var_group_labels, var_group_positions). 2332 """""". 2333 . -> 2334 key = _get_dendrogram_key(adata, dendrogram, groupby). 2335 . 2336 dendro_info = adata.uns[key]. /anaconda3/envs/test/lib/python3.6/site-packages/scanpy/plotting/_anndata.py in _get_dendrogram_key(adata, dendrogram_key, groupby). 2406 . 2407 if dendrogram_key not in adata.uns:. -> 2408 from ..tools._dendrogram import dendrogram. 2409 logg.warn(""dendrogram data not foun",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/560
https://github.com/scverse/scanpy/issues/560:786,safety,input,input-,786,"pip installation - missing tools?; Hi there,. I was trying do dig down to understand the problem in #559 , and I found out that in my ```plotting/_anndata.py``` [these lines](https://github.com/theislab/scanpy/blob/f33924011f7d0a7924fada933e1a20d7b5ceaac3/scanpy/plotting/_anndata.py#L828-L837) and all the ones related to ```standard_scale``` are missing. So I created a new conda environment and tried to install a new version of scanpy, but this did not solve the issue (i.e. the problem is not with my old environment) as these lines are still missing. . When I tried to replace the file and re-run my heatmap I got a different error:. ```. ---------------------------------------------------------------------------. ModuleNotFoundError Traceback (most recent call last). <ipython-input-5-49e0357ed731> in <module>. ----> 1 sc.pl.matrixplot(pbmc, marker_genes, groupby='bulk_labels', dendrogram=True, standard_scale='var'). /anaconda3/envs/test/lib/python3.6/site-packages/scanpy/plotting/_anndata.py in matrixplot(adata, var_names, groupby, use_raw, log, num_categories, figsize, dendrogram, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, standard_scale, swap_axes, show, save, **kwds). 1644 var_names=var_names,. 1645 var_group_labels=var_group_labels,. -> 1646 var_group_positions=var_group_positions). 1647 . 1648 var_group_labels = dendro_data['var_group_labels']. /anaconda3/envs/test/lib/python3.6/site-packages/scanpy/plotting/_anndata.py in _reorder_categories_after_dendrogram(adata, groupby, dendrogram, var_names, var_group_labels, var_group_positions). 2332 """""". 2333 . -> 2334 key = _get_dendrogram_key(adata, dendrogram, groupby). 2335 . 2336 dendro_info = adata.uns[key]. /anaconda3/envs/test/lib/python3.6/site-packages/scanpy/plotting/_anndata.py in _get_dendrogram_key(adata, dendrogram_key, groupby). 2406 . 2407 if dendrogram_key not in adata.uns:. -> 2408 from ..tools._dendrogram import dendrogram. 2409 logg.warn(""dendrogram data not foun",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/560
https://github.com/scverse/scanpy/issues/560:812,safety,modul,module,812,"pip installation - missing tools?; Hi there,. I was trying do dig down to understand the problem in #559 , and I found out that in my ```plotting/_anndata.py``` [these lines](https://github.com/theislab/scanpy/blob/f33924011f7d0a7924fada933e1a20d7b5ceaac3/scanpy/plotting/_anndata.py#L828-L837) and all the ones related to ```standard_scale``` are missing. So I created a new conda environment and tried to install a new version of scanpy, but this did not solve the issue (i.e. the problem is not with my old environment) as these lines are still missing. . When I tried to replace the file and re-run my heatmap I got a different error:. ```. ---------------------------------------------------------------------------. ModuleNotFoundError Traceback (most recent call last). <ipython-input-5-49e0357ed731> in <module>. ----> 1 sc.pl.matrixplot(pbmc, marker_genes, groupby='bulk_labels', dendrogram=True, standard_scale='var'). /anaconda3/envs/test/lib/python3.6/site-packages/scanpy/plotting/_anndata.py in matrixplot(adata, var_names, groupby, use_raw, log, num_categories, figsize, dendrogram, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, standard_scale, swap_axes, show, save, **kwds). 1644 var_names=var_names,. 1645 var_group_labels=var_group_labels,. -> 1646 var_group_positions=var_group_positions). 1647 . 1648 var_group_labels = dendro_data['var_group_labels']. /anaconda3/envs/test/lib/python3.6/site-packages/scanpy/plotting/_anndata.py in _reorder_categories_after_dendrogram(adata, groupby, dendrogram, var_names, var_group_labels, var_group_positions). 2332 """""". 2333 . -> 2334 key = _get_dendrogram_key(adata, dendrogram, groupby). 2335 . 2336 dendro_info = adata.uns[key]. /anaconda3/envs/test/lib/python3.6/site-packages/scanpy/plotting/_anndata.py in _get_dendrogram_key(adata, dendrogram_key, groupby). 2406 . 2407 if dendrogram_key not in adata.uns:. -> 2408 from ..tools._dendrogram import dendrogram. 2409 logg.warn(""dendrogram data not foun",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/560
https://github.com/scverse/scanpy/issues/560:945,safety,test,test,945,"pip installation - missing tools?; Hi there,. I was trying do dig down to understand the problem in #559 , and I found out that in my ```plotting/_anndata.py``` [these lines](https://github.com/theislab/scanpy/blob/f33924011f7d0a7924fada933e1a20d7b5ceaac3/scanpy/plotting/_anndata.py#L828-L837) and all the ones related to ```standard_scale``` are missing. So I created a new conda environment and tried to install a new version of scanpy, but this did not solve the issue (i.e. the problem is not with my old environment) as these lines are still missing. . When I tried to replace the file and re-run my heatmap I got a different error:. ```. ---------------------------------------------------------------------------. ModuleNotFoundError Traceback (most recent call last). <ipython-input-5-49e0357ed731> in <module>. ----> 1 sc.pl.matrixplot(pbmc, marker_genes, groupby='bulk_labels', dendrogram=True, standard_scale='var'). /anaconda3/envs/test/lib/python3.6/site-packages/scanpy/plotting/_anndata.py in matrixplot(adata, var_names, groupby, use_raw, log, num_categories, figsize, dendrogram, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, standard_scale, swap_axes, show, save, **kwds). 1644 var_names=var_names,. 1645 var_group_labels=var_group_labels,. -> 1646 var_group_positions=var_group_positions). 1647 . 1648 var_group_labels = dendro_data['var_group_labels']. /anaconda3/envs/test/lib/python3.6/site-packages/scanpy/plotting/_anndata.py in _reorder_categories_after_dendrogram(adata, groupby, dendrogram, var_names, var_group_labels, var_group_positions). 2332 """""". 2333 . -> 2334 key = _get_dendrogram_key(adata, dendrogram, groupby). 2335 . 2336 dendro_info = adata.uns[key]. /anaconda3/envs/test/lib/python3.6/site-packages/scanpy/plotting/_anndata.py in _get_dendrogram_key(adata, dendrogram_key, groupby). 2406 . 2407 if dendrogram_key not in adata.uns:. -> 2408 from ..tools._dendrogram import dendrogram. 2409 logg.warn(""dendrogram data not foun",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/560
https://github.com/scverse/scanpy/issues/560:1056,safety,log,log,1056," do dig down to understand the problem in #559 , and I found out that in my ```plotting/_anndata.py``` [these lines](https://github.com/theislab/scanpy/blob/f33924011f7d0a7924fada933e1a20d7b5ceaac3/scanpy/plotting/_anndata.py#L828-L837) and all the ones related to ```standard_scale``` are missing. So I created a new conda environment and tried to install a new version of scanpy, but this did not solve the issue (i.e. the problem is not with my old environment) as these lines are still missing. . When I tried to replace the file and re-run my heatmap I got a different error:. ```. ---------------------------------------------------------------------------. ModuleNotFoundError Traceback (most recent call last). <ipython-input-5-49e0357ed731> in <module>. ----> 1 sc.pl.matrixplot(pbmc, marker_genes, groupby='bulk_labels', dendrogram=True, standard_scale='var'). /anaconda3/envs/test/lib/python3.6/site-packages/scanpy/plotting/_anndata.py in matrixplot(adata, var_names, groupby, use_raw, log, num_categories, figsize, dendrogram, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, standard_scale, swap_axes, show, save, **kwds). 1644 var_names=var_names,. 1645 var_group_labels=var_group_labels,. -> 1646 var_group_positions=var_group_positions). 1647 . 1648 var_group_labels = dendro_data['var_group_labels']. /anaconda3/envs/test/lib/python3.6/site-packages/scanpy/plotting/_anndata.py in _reorder_categories_after_dendrogram(adata, groupby, dendrogram, var_names, var_group_labels, var_group_positions). 2332 """""". 2333 . -> 2334 key = _get_dendrogram_key(adata, dendrogram, groupby). 2335 . 2336 dendro_info = adata.uns[key]. /anaconda3/envs/test/lib/python3.6/site-packages/scanpy/plotting/_anndata.py in _get_dendrogram_key(adata, dendrogram_key, groupby). 2406 . 2407 if dendrogram_key not in adata.uns:. -> 2408 from ..tools._dendrogram import dendrogram. 2409 logg.warn(""dendrogram data not found (using key={}). Running `sc.tl.dendrogram` "". 2410 ""with",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/560
https://github.com/scverse/scanpy/issues/560:1424,safety,test,test,1424,"o ```standard_scale``` are missing. So I created a new conda environment and tried to install a new version of scanpy, but this did not solve the issue (i.e. the problem is not with my old environment) as these lines are still missing. . When I tried to replace the file and re-run my heatmap I got a different error:. ```. ---------------------------------------------------------------------------. ModuleNotFoundError Traceback (most recent call last). <ipython-input-5-49e0357ed731> in <module>. ----> 1 sc.pl.matrixplot(pbmc, marker_genes, groupby='bulk_labels', dendrogram=True, standard_scale='var'). /anaconda3/envs/test/lib/python3.6/site-packages/scanpy/plotting/_anndata.py in matrixplot(adata, var_names, groupby, use_raw, log, num_categories, figsize, dendrogram, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, standard_scale, swap_axes, show, save, **kwds). 1644 var_names=var_names,. 1645 var_group_labels=var_group_labels,. -> 1646 var_group_positions=var_group_positions). 1647 . 1648 var_group_labels = dendro_data['var_group_labels']. /anaconda3/envs/test/lib/python3.6/site-packages/scanpy/plotting/_anndata.py in _reorder_categories_after_dendrogram(adata, groupby, dendrogram, var_names, var_group_labels, var_group_positions). 2332 """""". 2333 . -> 2334 key = _get_dendrogram_key(adata, dendrogram, groupby). 2335 . 2336 dendro_info = adata.uns[key]. /anaconda3/envs/test/lib/python3.6/site-packages/scanpy/plotting/_anndata.py in _get_dendrogram_key(adata, dendrogram_key, groupby). 2406 . 2407 if dendrogram_key not in adata.uns:. -> 2408 from ..tools._dendrogram import dendrogram. 2409 logg.warn(""dendrogram data not found (using key={}). Running `sc.tl.dendrogram` "". 2410 ""with default parameters. For fine tuning it is recommended to run `sc.tl.dendrogram` "". ModuleNotFoundError: No module named 'scanpy.tools._dendrogram'. ```. And indeed, in my scanpy this dendrogram tool is missing. . Is the PyPi version missing something? Thank you!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/560
https://github.com/scverse/scanpy/issues/560:1742,safety,test,test,1742,"o ```standard_scale``` are missing. So I created a new conda environment and tried to install a new version of scanpy, but this did not solve the issue (i.e. the problem is not with my old environment) as these lines are still missing. . When I tried to replace the file and re-run my heatmap I got a different error:. ```. ---------------------------------------------------------------------------. ModuleNotFoundError Traceback (most recent call last). <ipython-input-5-49e0357ed731> in <module>. ----> 1 sc.pl.matrixplot(pbmc, marker_genes, groupby='bulk_labels', dendrogram=True, standard_scale='var'). /anaconda3/envs/test/lib/python3.6/site-packages/scanpy/plotting/_anndata.py in matrixplot(adata, var_names, groupby, use_raw, log, num_categories, figsize, dendrogram, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, standard_scale, swap_axes, show, save, **kwds). 1644 var_names=var_names,. 1645 var_group_labels=var_group_labels,. -> 1646 var_group_positions=var_group_positions). 1647 . 1648 var_group_labels = dendro_data['var_group_labels']. /anaconda3/envs/test/lib/python3.6/site-packages/scanpy/plotting/_anndata.py in _reorder_categories_after_dendrogram(adata, groupby, dendrogram, var_names, var_group_labels, var_group_positions). 2332 """""". 2333 . -> 2334 key = _get_dendrogram_key(adata, dendrogram, groupby). 2335 . 2336 dendro_info = adata.uns[key]. /anaconda3/envs/test/lib/python3.6/site-packages/scanpy/plotting/_anndata.py in _get_dendrogram_key(adata, dendrogram_key, groupby). 2406 . 2407 if dendrogram_key not in adata.uns:. -> 2408 from ..tools._dendrogram import dendrogram. 2409 logg.warn(""dendrogram data not found (using key={}). Running `sc.tl.dendrogram` "". 2410 ""with default parameters. For fine tuning it is recommended to run `sc.tl.dendrogram` "". ModuleNotFoundError: No module named 'scanpy.tools._dendrogram'. ```. And indeed, in my scanpy this dendrogram tool is missing. . Is the PyPi version missing something? Thank you!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/560
https://github.com/scverse/scanpy/issues/560:1965,safety,log,logg,1965,"o ```standard_scale``` are missing. So I created a new conda environment and tried to install a new version of scanpy, but this did not solve the issue (i.e. the problem is not with my old environment) as these lines are still missing. . When I tried to replace the file and re-run my heatmap I got a different error:. ```. ---------------------------------------------------------------------------. ModuleNotFoundError Traceback (most recent call last). <ipython-input-5-49e0357ed731> in <module>. ----> 1 sc.pl.matrixplot(pbmc, marker_genes, groupby='bulk_labels', dendrogram=True, standard_scale='var'). /anaconda3/envs/test/lib/python3.6/site-packages/scanpy/plotting/_anndata.py in matrixplot(adata, var_names, groupby, use_raw, log, num_categories, figsize, dendrogram, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, standard_scale, swap_axes, show, save, **kwds). 1644 var_names=var_names,. 1645 var_group_labels=var_group_labels,. -> 1646 var_group_positions=var_group_positions). 1647 . 1648 var_group_labels = dendro_data['var_group_labels']. /anaconda3/envs/test/lib/python3.6/site-packages/scanpy/plotting/_anndata.py in _reorder_categories_after_dendrogram(adata, groupby, dendrogram, var_names, var_group_labels, var_group_positions). 2332 """""". 2333 . -> 2334 key = _get_dendrogram_key(adata, dendrogram, groupby). 2335 . 2336 dendro_info = adata.uns[key]. /anaconda3/envs/test/lib/python3.6/site-packages/scanpy/plotting/_anndata.py in _get_dendrogram_key(adata, dendrogram_key, groupby). 2406 . 2407 if dendrogram_key not in adata.uns:. -> 2408 from ..tools._dendrogram import dendrogram. 2409 logg.warn(""dendrogram data not found (using key={}). Running `sc.tl.dendrogram` "". 2410 ""with default parameters. For fine tuning it is recommended to run `sc.tl.dendrogram` "". ModuleNotFoundError: No module named 'scanpy.tools._dendrogram'. ```. And indeed, in my scanpy this dendrogram tool is missing. . Is the PyPi version missing something? Thank you!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/560
https://github.com/scverse/scanpy/issues/560:2142,safety,Modul,ModuleNotFoundError,2142,"o ```standard_scale``` are missing. So I created a new conda environment and tried to install a new version of scanpy, but this did not solve the issue (i.e. the problem is not with my old environment) as these lines are still missing. . When I tried to replace the file and re-run my heatmap I got a different error:. ```. ---------------------------------------------------------------------------. ModuleNotFoundError Traceback (most recent call last). <ipython-input-5-49e0357ed731> in <module>. ----> 1 sc.pl.matrixplot(pbmc, marker_genes, groupby='bulk_labels', dendrogram=True, standard_scale='var'). /anaconda3/envs/test/lib/python3.6/site-packages/scanpy/plotting/_anndata.py in matrixplot(adata, var_names, groupby, use_raw, log, num_categories, figsize, dendrogram, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, standard_scale, swap_axes, show, save, **kwds). 1644 var_names=var_names,. 1645 var_group_labels=var_group_labels,. -> 1646 var_group_positions=var_group_positions). 1647 . 1648 var_group_labels = dendro_data['var_group_labels']. /anaconda3/envs/test/lib/python3.6/site-packages/scanpy/plotting/_anndata.py in _reorder_categories_after_dendrogram(adata, groupby, dendrogram, var_names, var_group_labels, var_group_positions). 2332 """""". 2333 . -> 2334 key = _get_dendrogram_key(adata, dendrogram, groupby). 2335 . 2336 dendro_info = adata.uns[key]. /anaconda3/envs/test/lib/python3.6/site-packages/scanpy/plotting/_anndata.py in _get_dendrogram_key(adata, dendrogram_key, groupby). 2406 . 2407 if dendrogram_key not in adata.uns:. -> 2408 from ..tools._dendrogram import dendrogram. 2409 logg.warn(""dendrogram data not found (using key={}). Running `sc.tl.dendrogram` "". 2410 ""with default parameters. For fine tuning it is recommended to run `sc.tl.dendrogram` "". ModuleNotFoundError: No module named 'scanpy.tools._dendrogram'. ```. And indeed, in my scanpy this dendrogram tool is missing. . Is the PyPi version missing something? Thank you!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/560
https://github.com/scverse/scanpy/issues/560:2166,safety,modul,module,2166,"o ```standard_scale``` are missing. So I created a new conda environment and tried to install a new version of scanpy, but this did not solve the issue (i.e. the problem is not with my old environment) as these lines are still missing. . When I tried to replace the file and re-run my heatmap I got a different error:. ```. ---------------------------------------------------------------------------. ModuleNotFoundError Traceback (most recent call last). <ipython-input-5-49e0357ed731> in <module>. ----> 1 sc.pl.matrixplot(pbmc, marker_genes, groupby='bulk_labels', dendrogram=True, standard_scale='var'). /anaconda3/envs/test/lib/python3.6/site-packages/scanpy/plotting/_anndata.py in matrixplot(adata, var_names, groupby, use_raw, log, num_categories, figsize, dendrogram, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, standard_scale, swap_axes, show, save, **kwds). 1644 var_names=var_names,. 1645 var_group_labels=var_group_labels,. -> 1646 var_group_positions=var_group_positions). 1647 . 1648 var_group_labels = dendro_data['var_group_labels']. /anaconda3/envs/test/lib/python3.6/site-packages/scanpy/plotting/_anndata.py in _reorder_categories_after_dendrogram(adata, groupby, dendrogram, var_names, var_group_labels, var_group_positions). 2332 """""". 2333 . -> 2334 key = _get_dendrogram_key(adata, dendrogram, groupby). 2335 . 2336 dendro_info = adata.uns[key]. /anaconda3/envs/test/lib/python3.6/site-packages/scanpy/plotting/_anndata.py in _get_dendrogram_key(adata, dendrogram_key, groupby). 2406 . 2407 if dendrogram_key not in adata.uns:. -> 2408 from ..tools._dendrogram import dendrogram. 2409 logg.warn(""dendrogram data not found (using key={}). Running `sc.tl.dendrogram` "". 2410 ""with default parameters. For fine tuning it is recommended to run `sc.tl.dendrogram` "". ModuleNotFoundError: No module named 'scanpy.tools._dendrogram'. ```. And indeed, in my scanpy this dendrogram tool is missing. . Is the PyPi version missing something? Thank you!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/560
https://github.com/scverse/scanpy/issues/560:1056,security,log,log,1056," do dig down to understand the problem in #559 , and I found out that in my ```plotting/_anndata.py``` [these lines](https://github.com/theislab/scanpy/blob/f33924011f7d0a7924fada933e1a20d7b5ceaac3/scanpy/plotting/_anndata.py#L828-L837) and all the ones related to ```standard_scale``` are missing. So I created a new conda environment and tried to install a new version of scanpy, but this did not solve the issue (i.e. the problem is not with my old environment) as these lines are still missing. . When I tried to replace the file and re-run my heatmap I got a different error:. ```. ---------------------------------------------------------------------------. ModuleNotFoundError Traceback (most recent call last). <ipython-input-5-49e0357ed731> in <module>. ----> 1 sc.pl.matrixplot(pbmc, marker_genes, groupby='bulk_labels', dendrogram=True, standard_scale='var'). /anaconda3/envs/test/lib/python3.6/site-packages/scanpy/plotting/_anndata.py in matrixplot(adata, var_names, groupby, use_raw, log, num_categories, figsize, dendrogram, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, standard_scale, swap_axes, show, save, **kwds). 1644 var_names=var_names,. 1645 var_group_labels=var_group_labels,. -> 1646 var_group_positions=var_group_positions). 1647 . 1648 var_group_labels = dendro_data['var_group_labels']. /anaconda3/envs/test/lib/python3.6/site-packages/scanpy/plotting/_anndata.py in _reorder_categories_after_dendrogram(adata, groupby, dendrogram, var_names, var_group_labels, var_group_positions). 2332 """""". 2333 . -> 2334 key = _get_dendrogram_key(adata, dendrogram, groupby). 2335 . 2336 dendro_info = adata.uns[key]. /anaconda3/envs/test/lib/python3.6/site-packages/scanpy/plotting/_anndata.py in _get_dendrogram_key(adata, dendrogram_key, groupby). 2406 . 2407 if dendrogram_key not in adata.uns:. -> 2408 from ..tools._dendrogram import dendrogram. 2409 logg.warn(""dendrogram data not found (using key={}). Running `sc.tl.dendrogram` "". 2410 ""with",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/560
https://github.com/scverse/scanpy/issues/560:1965,security,log,logg,1965,"o ```standard_scale``` are missing. So I created a new conda environment and tried to install a new version of scanpy, but this did not solve the issue (i.e. the problem is not with my old environment) as these lines are still missing. . When I tried to replace the file and re-run my heatmap I got a different error:. ```. ---------------------------------------------------------------------------. ModuleNotFoundError Traceback (most recent call last). <ipython-input-5-49e0357ed731> in <module>. ----> 1 sc.pl.matrixplot(pbmc, marker_genes, groupby='bulk_labels', dendrogram=True, standard_scale='var'). /anaconda3/envs/test/lib/python3.6/site-packages/scanpy/plotting/_anndata.py in matrixplot(adata, var_names, groupby, use_raw, log, num_categories, figsize, dendrogram, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, standard_scale, swap_axes, show, save, **kwds). 1644 var_names=var_names,. 1645 var_group_labels=var_group_labels,. -> 1646 var_group_positions=var_group_positions). 1647 . 1648 var_group_labels = dendro_data['var_group_labels']. /anaconda3/envs/test/lib/python3.6/site-packages/scanpy/plotting/_anndata.py in _reorder_categories_after_dendrogram(adata, groupby, dendrogram, var_names, var_group_labels, var_group_positions). 2332 """""". 2333 . -> 2334 key = _get_dendrogram_key(adata, dendrogram, groupby). 2335 . 2336 dendro_info = adata.uns[key]. /anaconda3/envs/test/lib/python3.6/site-packages/scanpy/plotting/_anndata.py in _get_dendrogram_key(adata, dendrogram_key, groupby). 2406 . 2407 if dendrogram_key not in adata.uns:. -> 2408 from ..tools._dendrogram import dendrogram. 2409 logg.warn(""dendrogram data not found (using key={}). Running `sc.tl.dendrogram` "". 2410 ""with default parameters. For fine tuning it is recommended to run `sc.tl.dendrogram` "". ModuleNotFoundError: No module named 'scanpy.tools._dendrogram'. ```. And indeed, in my scanpy this dendrogram tool is missing. . Is the PyPi version missing something? Thank you!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/560
https://github.com/scverse/scanpy/issues/560:74,testability,understand,understand,74,"pip installation - missing tools?; Hi there,. I was trying do dig down to understand the problem in #559 , and I found out that in my ```plotting/_anndata.py``` [these lines](https://github.com/theislab/scanpy/blob/f33924011f7d0a7924fada933e1a20d7b5ceaac3/scanpy/plotting/_anndata.py#L828-L837) and all the ones related to ```standard_scale``` are missing. So I created a new conda environment and tried to install a new version of scanpy, but this did not solve the issue (i.e. the problem is not with my old environment) as these lines are still missing. . When I tried to replace the file and re-run my heatmap I got a different error:. ```. ---------------------------------------------------------------------------. ModuleNotFoundError Traceback (most recent call last). <ipython-input-5-49e0357ed731> in <module>. ----> 1 sc.pl.matrixplot(pbmc, marker_genes, groupby='bulk_labels', dendrogram=True, standard_scale='var'). /anaconda3/envs/test/lib/python3.6/site-packages/scanpy/plotting/_anndata.py in matrixplot(adata, var_names, groupby, use_raw, log, num_categories, figsize, dendrogram, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, standard_scale, swap_axes, show, save, **kwds). 1644 var_names=var_names,. 1645 var_group_labels=var_group_labels,. -> 1646 var_group_positions=var_group_positions). 1647 . 1648 var_group_labels = dendro_data['var_group_labels']. /anaconda3/envs/test/lib/python3.6/site-packages/scanpy/plotting/_anndata.py in _reorder_categories_after_dendrogram(adata, groupby, dendrogram, var_names, var_group_labels, var_group_positions). 2332 """""". 2333 . -> 2334 key = _get_dendrogram_key(adata, dendrogram, groupby). 2335 . 2336 dendro_info = adata.uns[key]. /anaconda3/envs/test/lib/python3.6/site-packages/scanpy/plotting/_anndata.py in _get_dendrogram_key(adata, dendrogram_key, groupby). 2406 . 2407 if dendrogram_key not in adata.uns:. -> 2408 from ..tools._dendrogram import dendrogram. 2409 logg.warn(""dendrogram data not foun",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/560
https://github.com/scverse/scanpy/issues/560:742,testability,Trace,Traceback,742,"pip installation - missing tools?; Hi there,. I was trying do dig down to understand the problem in #559 , and I found out that in my ```plotting/_anndata.py``` [these lines](https://github.com/theislab/scanpy/blob/f33924011f7d0a7924fada933e1a20d7b5ceaac3/scanpy/plotting/_anndata.py#L828-L837) and all the ones related to ```standard_scale``` are missing. So I created a new conda environment and tried to install a new version of scanpy, but this did not solve the issue (i.e. the problem is not with my old environment) as these lines are still missing. . When I tried to replace the file and re-run my heatmap I got a different error:. ```. ---------------------------------------------------------------------------. ModuleNotFoundError Traceback (most recent call last). <ipython-input-5-49e0357ed731> in <module>. ----> 1 sc.pl.matrixplot(pbmc, marker_genes, groupby='bulk_labels', dendrogram=True, standard_scale='var'). /anaconda3/envs/test/lib/python3.6/site-packages/scanpy/plotting/_anndata.py in matrixplot(adata, var_names, groupby, use_raw, log, num_categories, figsize, dendrogram, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, standard_scale, swap_axes, show, save, **kwds). 1644 var_names=var_names,. 1645 var_group_labels=var_group_labels,. -> 1646 var_group_positions=var_group_positions). 1647 . 1648 var_group_labels = dendro_data['var_group_labels']. /anaconda3/envs/test/lib/python3.6/site-packages/scanpy/plotting/_anndata.py in _reorder_categories_after_dendrogram(adata, groupby, dendrogram, var_names, var_group_labels, var_group_positions). 2332 """""". 2333 . -> 2334 key = _get_dendrogram_key(adata, dendrogram, groupby). 2335 . 2336 dendro_info = adata.uns[key]. /anaconda3/envs/test/lib/python3.6/site-packages/scanpy/plotting/_anndata.py in _get_dendrogram_key(adata, dendrogram_key, groupby). 2406 . 2407 if dendrogram_key not in adata.uns:. -> 2408 from ..tools._dendrogram import dendrogram. 2409 logg.warn(""dendrogram data not foun",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/560
https://github.com/scverse/scanpy/issues/560:945,testability,test,test,945,"pip installation - missing tools?; Hi there,. I was trying do dig down to understand the problem in #559 , and I found out that in my ```plotting/_anndata.py``` [these lines](https://github.com/theislab/scanpy/blob/f33924011f7d0a7924fada933e1a20d7b5ceaac3/scanpy/plotting/_anndata.py#L828-L837) and all the ones related to ```standard_scale``` are missing. So I created a new conda environment and tried to install a new version of scanpy, but this did not solve the issue (i.e. the problem is not with my old environment) as these lines are still missing. . When I tried to replace the file and re-run my heatmap I got a different error:. ```. ---------------------------------------------------------------------------. ModuleNotFoundError Traceback (most recent call last). <ipython-input-5-49e0357ed731> in <module>. ----> 1 sc.pl.matrixplot(pbmc, marker_genes, groupby='bulk_labels', dendrogram=True, standard_scale='var'). /anaconda3/envs/test/lib/python3.6/site-packages/scanpy/plotting/_anndata.py in matrixplot(adata, var_names, groupby, use_raw, log, num_categories, figsize, dendrogram, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, standard_scale, swap_axes, show, save, **kwds). 1644 var_names=var_names,. 1645 var_group_labels=var_group_labels,. -> 1646 var_group_positions=var_group_positions). 1647 . 1648 var_group_labels = dendro_data['var_group_labels']. /anaconda3/envs/test/lib/python3.6/site-packages/scanpy/plotting/_anndata.py in _reorder_categories_after_dendrogram(adata, groupby, dendrogram, var_names, var_group_labels, var_group_positions). 2332 """""". 2333 . -> 2334 key = _get_dendrogram_key(adata, dendrogram, groupby). 2335 . 2336 dendro_info = adata.uns[key]. /anaconda3/envs/test/lib/python3.6/site-packages/scanpy/plotting/_anndata.py in _get_dendrogram_key(adata, dendrogram_key, groupby). 2406 . 2407 if dendrogram_key not in adata.uns:. -> 2408 from ..tools._dendrogram import dendrogram. 2409 logg.warn(""dendrogram data not foun",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/560
https://github.com/scverse/scanpy/issues/560:1056,testability,log,log,1056," do dig down to understand the problem in #559 , and I found out that in my ```plotting/_anndata.py``` [these lines](https://github.com/theislab/scanpy/blob/f33924011f7d0a7924fada933e1a20d7b5ceaac3/scanpy/plotting/_anndata.py#L828-L837) and all the ones related to ```standard_scale``` are missing. So I created a new conda environment and tried to install a new version of scanpy, but this did not solve the issue (i.e. the problem is not with my old environment) as these lines are still missing. . When I tried to replace the file and re-run my heatmap I got a different error:. ```. ---------------------------------------------------------------------------. ModuleNotFoundError Traceback (most recent call last). <ipython-input-5-49e0357ed731> in <module>. ----> 1 sc.pl.matrixplot(pbmc, marker_genes, groupby='bulk_labels', dendrogram=True, standard_scale='var'). /anaconda3/envs/test/lib/python3.6/site-packages/scanpy/plotting/_anndata.py in matrixplot(adata, var_names, groupby, use_raw, log, num_categories, figsize, dendrogram, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, standard_scale, swap_axes, show, save, **kwds). 1644 var_names=var_names,. 1645 var_group_labels=var_group_labels,. -> 1646 var_group_positions=var_group_positions). 1647 . 1648 var_group_labels = dendro_data['var_group_labels']. /anaconda3/envs/test/lib/python3.6/site-packages/scanpy/plotting/_anndata.py in _reorder_categories_after_dendrogram(adata, groupby, dendrogram, var_names, var_group_labels, var_group_positions). 2332 """""". 2333 . -> 2334 key = _get_dendrogram_key(adata, dendrogram, groupby). 2335 . 2336 dendro_info = adata.uns[key]. /anaconda3/envs/test/lib/python3.6/site-packages/scanpy/plotting/_anndata.py in _get_dendrogram_key(adata, dendrogram_key, groupby). 2406 . 2407 if dendrogram_key not in adata.uns:. -> 2408 from ..tools._dendrogram import dendrogram. 2409 logg.warn(""dendrogram data not found (using key={}). Running `sc.tl.dendrogram` "". 2410 ""with",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/560
https://github.com/scverse/scanpy/issues/560:1424,testability,test,test,1424,"o ```standard_scale``` are missing. So I created a new conda environment and tried to install a new version of scanpy, but this did not solve the issue (i.e. the problem is not with my old environment) as these lines are still missing. . When I tried to replace the file and re-run my heatmap I got a different error:. ```. ---------------------------------------------------------------------------. ModuleNotFoundError Traceback (most recent call last). <ipython-input-5-49e0357ed731> in <module>. ----> 1 sc.pl.matrixplot(pbmc, marker_genes, groupby='bulk_labels', dendrogram=True, standard_scale='var'). /anaconda3/envs/test/lib/python3.6/site-packages/scanpy/plotting/_anndata.py in matrixplot(adata, var_names, groupby, use_raw, log, num_categories, figsize, dendrogram, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, standard_scale, swap_axes, show, save, **kwds). 1644 var_names=var_names,. 1645 var_group_labels=var_group_labels,. -> 1646 var_group_positions=var_group_positions). 1647 . 1648 var_group_labels = dendro_data['var_group_labels']. /anaconda3/envs/test/lib/python3.6/site-packages/scanpy/plotting/_anndata.py in _reorder_categories_after_dendrogram(adata, groupby, dendrogram, var_names, var_group_labels, var_group_positions). 2332 """""". 2333 . -> 2334 key = _get_dendrogram_key(adata, dendrogram, groupby). 2335 . 2336 dendro_info = adata.uns[key]. /anaconda3/envs/test/lib/python3.6/site-packages/scanpy/plotting/_anndata.py in _get_dendrogram_key(adata, dendrogram_key, groupby). 2406 . 2407 if dendrogram_key not in adata.uns:. -> 2408 from ..tools._dendrogram import dendrogram. 2409 logg.warn(""dendrogram data not found (using key={}). Running `sc.tl.dendrogram` "". 2410 ""with default parameters. For fine tuning it is recommended to run `sc.tl.dendrogram` "". ModuleNotFoundError: No module named 'scanpy.tools._dendrogram'. ```. And indeed, in my scanpy this dendrogram tool is missing. . Is the PyPi version missing something? Thank you!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/560
https://github.com/scverse/scanpy/issues/560:1742,testability,test,test,1742,"o ```standard_scale``` are missing. So I created a new conda environment and tried to install a new version of scanpy, but this did not solve the issue (i.e. the problem is not with my old environment) as these lines are still missing. . When I tried to replace the file and re-run my heatmap I got a different error:. ```. ---------------------------------------------------------------------------. ModuleNotFoundError Traceback (most recent call last). <ipython-input-5-49e0357ed731> in <module>. ----> 1 sc.pl.matrixplot(pbmc, marker_genes, groupby='bulk_labels', dendrogram=True, standard_scale='var'). /anaconda3/envs/test/lib/python3.6/site-packages/scanpy/plotting/_anndata.py in matrixplot(adata, var_names, groupby, use_raw, log, num_categories, figsize, dendrogram, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, standard_scale, swap_axes, show, save, **kwds). 1644 var_names=var_names,. 1645 var_group_labels=var_group_labels,. -> 1646 var_group_positions=var_group_positions). 1647 . 1648 var_group_labels = dendro_data['var_group_labels']. /anaconda3/envs/test/lib/python3.6/site-packages/scanpy/plotting/_anndata.py in _reorder_categories_after_dendrogram(adata, groupby, dendrogram, var_names, var_group_labels, var_group_positions). 2332 """""". 2333 . -> 2334 key = _get_dendrogram_key(adata, dendrogram, groupby). 2335 . 2336 dendro_info = adata.uns[key]. /anaconda3/envs/test/lib/python3.6/site-packages/scanpy/plotting/_anndata.py in _get_dendrogram_key(adata, dendrogram_key, groupby). 2406 . 2407 if dendrogram_key not in adata.uns:. -> 2408 from ..tools._dendrogram import dendrogram. 2409 logg.warn(""dendrogram data not found (using key={}). Running `sc.tl.dendrogram` "". 2410 ""with default parameters. For fine tuning it is recommended to run `sc.tl.dendrogram` "". ModuleNotFoundError: No module named 'scanpy.tools._dendrogram'. ```. And indeed, in my scanpy this dendrogram tool is missing. . Is the PyPi version missing something? Thank you!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/560
https://github.com/scverse/scanpy/issues/560:1965,testability,log,logg,1965,"o ```standard_scale``` are missing. So I created a new conda environment and tried to install a new version of scanpy, but this did not solve the issue (i.e. the problem is not with my old environment) as these lines are still missing. . When I tried to replace the file and re-run my heatmap I got a different error:. ```. ---------------------------------------------------------------------------. ModuleNotFoundError Traceback (most recent call last). <ipython-input-5-49e0357ed731> in <module>. ----> 1 sc.pl.matrixplot(pbmc, marker_genes, groupby='bulk_labels', dendrogram=True, standard_scale='var'). /anaconda3/envs/test/lib/python3.6/site-packages/scanpy/plotting/_anndata.py in matrixplot(adata, var_names, groupby, use_raw, log, num_categories, figsize, dendrogram, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, standard_scale, swap_axes, show, save, **kwds). 1644 var_names=var_names,. 1645 var_group_labels=var_group_labels,. -> 1646 var_group_positions=var_group_positions). 1647 . 1648 var_group_labels = dendro_data['var_group_labels']. /anaconda3/envs/test/lib/python3.6/site-packages/scanpy/plotting/_anndata.py in _reorder_categories_after_dendrogram(adata, groupby, dendrogram, var_names, var_group_labels, var_group_positions). 2332 """""". 2333 . -> 2334 key = _get_dendrogram_key(adata, dendrogram, groupby). 2335 . 2336 dendro_info = adata.uns[key]. /anaconda3/envs/test/lib/python3.6/site-packages/scanpy/plotting/_anndata.py in _get_dendrogram_key(adata, dendrogram_key, groupby). 2406 . 2407 if dendrogram_key not in adata.uns:. -> 2408 from ..tools._dendrogram import dendrogram. 2409 logg.warn(""dendrogram data not found (using key={}). Running `sc.tl.dendrogram` "". 2410 ""with default parameters. For fine tuning it is recommended to run `sc.tl.dendrogram` "". ModuleNotFoundError: No module named 'scanpy.tools._dendrogram'. ```. And indeed, in my scanpy this dendrogram tool is missing. . Is the PyPi version missing something? Thank you!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/560
https://github.com/scverse/scanpy/issues/560:27,usability,tool,tools,27,"pip installation - missing tools?; Hi there,. I was trying do dig down to understand the problem in #559 , and I found out that in my ```plotting/_anndata.py``` [these lines](https://github.com/theislab/scanpy/blob/f33924011f7d0a7924fada933e1a20d7b5ceaac3/scanpy/plotting/_anndata.py#L828-L837) and all the ones related to ```standard_scale``` are missing. So I created a new conda environment and tried to install a new version of scanpy, but this did not solve the issue (i.e. the problem is not with my old environment) as these lines are still missing. . When I tried to replace the file and re-run my heatmap I got a different error:. ```. ---------------------------------------------------------------------------. ModuleNotFoundError Traceback (most recent call last). <ipython-input-5-49e0357ed731> in <module>. ----> 1 sc.pl.matrixplot(pbmc, marker_genes, groupby='bulk_labels', dendrogram=True, standard_scale='var'). /anaconda3/envs/test/lib/python3.6/site-packages/scanpy/plotting/_anndata.py in matrixplot(adata, var_names, groupby, use_raw, log, num_categories, figsize, dendrogram, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, standard_scale, swap_axes, show, save, **kwds). 1644 var_names=var_names,. 1645 var_group_labels=var_group_labels,. -> 1646 var_group_positions=var_group_positions). 1647 . 1648 var_group_labels = dendro_data['var_group_labels']. /anaconda3/envs/test/lib/python3.6/site-packages/scanpy/plotting/_anndata.py in _reorder_categories_after_dendrogram(adata, groupby, dendrogram, var_names, var_group_labels, var_group_positions). 2332 """""". 2333 . -> 2334 key = _get_dendrogram_key(adata, dendrogram, groupby). 2335 . 2336 dendro_info = adata.uns[key]. /anaconda3/envs/test/lib/python3.6/site-packages/scanpy/plotting/_anndata.py in _get_dendrogram_key(adata, dendrogram_key, groupby). 2406 . 2407 if dendrogram_key not in adata.uns:. -> 2408 from ..tools._dendrogram import dendrogram. 2409 logg.warn(""dendrogram data not foun",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/560
https://github.com/scverse/scanpy/issues/560:632,usability,error,error,632,"pip installation - missing tools?; Hi there,. I was trying do dig down to understand the problem in #559 , and I found out that in my ```plotting/_anndata.py``` [these lines](https://github.com/theislab/scanpy/blob/f33924011f7d0a7924fada933e1a20d7b5ceaac3/scanpy/plotting/_anndata.py#L828-L837) and all the ones related to ```standard_scale``` are missing. So I created a new conda environment and tried to install a new version of scanpy, but this did not solve the issue (i.e. the problem is not with my old environment) as these lines are still missing. . When I tried to replace the file and re-run my heatmap I got a different error:. ```. ---------------------------------------------------------------------------. ModuleNotFoundError Traceback (most recent call last). <ipython-input-5-49e0357ed731> in <module>. ----> 1 sc.pl.matrixplot(pbmc, marker_genes, groupby='bulk_labels', dendrogram=True, standard_scale='var'). /anaconda3/envs/test/lib/python3.6/site-packages/scanpy/plotting/_anndata.py in matrixplot(adata, var_names, groupby, use_raw, log, num_categories, figsize, dendrogram, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, standard_scale, swap_axes, show, save, **kwds). 1644 var_names=var_names,. 1645 var_group_labels=var_group_labels,. -> 1646 var_group_positions=var_group_positions). 1647 . 1648 var_group_labels = dendro_data['var_group_labels']. /anaconda3/envs/test/lib/python3.6/site-packages/scanpy/plotting/_anndata.py in _reorder_categories_after_dendrogram(adata, groupby, dendrogram, var_names, var_group_labels, var_group_positions). 2332 """""". 2333 . -> 2334 key = _get_dendrogram_key(adata, dendrogram, groupby). 2335 . 2336 dendro_info = adata.uns[key]. /anaconda3/envs/test/lib/python3.6/site-packages/scanpy/plotting/_anndata.py in _get_dendrogram_key(adata, dendrogram_key, groupby). 2406 . 2407 if dendrogram_key not in adata.uns:. -> 2408 from ..tools._dendrogram import dendrogram. 2409 logg.warn(""dendrogram data not foun",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/560
https://github.com/scverse/scanpy/issues/560:786,usability,input,input-,786,"pip installation - missing tools?; Hi there,. I was trying do dig down to understand the problem in #559 , and I found out that in my ```plotting/_anndata.py``` [these lines](https://github.com/theislab/scanpy/blob/f33924011f7d0a7924fada933e1a20d7b5ceaac3/scanpy/plotting/_anndata.py#L828-L837) and all the ones related to ```standard_scale``` are missing. So I created a new conda environment and tried to install a new version of scanpy, but this did not solve the issue (i.e. the problem is not with my old environment) as these lines are still missing. . When I tried to replace the file and re-run my heatmap I got a different error:. ```. ---------------------------------------------------------------------------. ModuleNotFoundError Traceback (most recent call last). <ipython-input-5-49e0357ed731> in <module>. ----> 1 sc.pl.matrixplot(pbmc, marker_genes, groupby='bulk_labels', dendrogram=True, standard_scale='var'). /anaconda3/envs/test/lib/python3.6/site-packages/scanpy/plotting/_anndata.py in matrixplot(adata, var_names, groupby, use_raw, log, num_categories, figsize, dendrogram, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, standard_scale, swap_axes, show, save, **kwds). 1644 var_names=var_names,. 1645 var_group_labels=var_group_labels,. -> 1646 var_group_positions=var_group_positions). 1647 . 1648 var_group_labels = dendro_data['var_group_labels']. /anaconda3/envs/test/lib/python3.6/site-packages/scanpy/plotting/_anndata.py in _reorder_categories_after_dendrogram(adata, groupby, dendrogram, var_names, var_group_labels, var_group_positions). 2332 """""". 2333 . -> 2334 key = _get_dendrogram_key(adata, dendrogram, groupby). 2335 . 2336 dendro_info = adata.uns[key]. /anaconda3/envs/test/lib/python3.6/site-packages/scanpy/plotting/_anndata.py in _get_dendrogram_key(adata, dendrogram_key, groupby). 2406 . 2407 if dendrogram_key not in adata.uns:. -> 2408 from ..tools._dendrogram import dendrogram. 2409 logg.warn(""dendrogram data not foun",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/560
https://github.com/scverse/scanpy/issues/560:1923,usability,tool,tools,1923,"o ```standard_scale``` are missing. So I created a new conda environment and tried to install a new version of scanpy, but this did not solve the issue (i.e. the problem is not with my old environment) as these lines are still missing. . When I tried to replace the file and re-run my heatmap I got a different error:. ```. ---------------------------------------------------------------------------. ModuleNotFoundError Traceback (most recent call last). <ipython-input-5-49e0357ed731> in <module>. ----> 1 sc.pl.matrixplot(pbmc, marker_genes, groupby='bulk_labels', dendrogram=True, standard_scale='var'). /anaconda3/envs/test/lib/python3.6/site-packages/scanpy/plotting/_anndata.py in matrixplot(adata, var_names, groupby, use_raw, log, num_categories, figsize, dendrogram, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, standard_scale, swap_axes, show, save, **kwds). 1644 var_names=var_names,. 1645 var_group_labels=var_group_labels,. -> 1646 var_group_positions=var_group_positions). 1647 . 1648 var_group_labels = dendro_data['var_group_labels']. /anaconda3/envs/test/lib/python3.6/site-packages/scanpy/plotting/_anndata.py in _reorder_categories_after_dendrogram(adata, groupby, dendrogram, var_names, var_group_labels, var_group_positions). 2332 """""". 2333 . -> 2334 key = _get_dendrogram_key(adata, dendrogram, groupby). 2335 . 2336 dendro_info = adata.uns[key]. /anaconda3/envs/test/lib/python3.6/site-packages/scanpy/plotting/_anndata.py in _get_dendrogram_key(adata, dendrogram_key, groupby). 2406 . 2407 if dendrogram_key not in adata.uns:. -> 2408 from ..tools._dendrogram import dendrogram. 2409 logg.warn(""dendrogram data not found (using key={}). Running `sc.tl.dendrogram` "". 2410 ""with default parameters. For fine tuning it is recommended to run `sc.tl.dendrogram` "". ModuleNotFoundError: No module named 'scanpy.tools._dendrogram'. ```. And indeed, in my scanpy this dendrogram tool is missing. . Is the PyPi version missing something? Thank you!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/560
https://github.com/scverse/scanpy/issues/560:2187,usability,tool,tools,2187,"o ```standard_scale``` are missing. So I created a new conda environment and tried to install a new version of scanpy, but this did not solve the issue (i.e. the problem is not with my old environment) as these lines are still missing. . When I tried to replace the file and re-run my heatmap I got a different error:. ```. ---------------------------------------------------------------------------. ModuleNotFoundError Traceback (most recent call last). <ipython-input-5-49e0357ed731> in <module>. ----> 1 sc.pl.matrixplot(pbmc, marker_genes, groupby='bulk_labels', dendrogram=True, standard_scale='var'). /anaconda3/envs/test/lib/python3.6/site-packages/scanpy/plotting/_anndata.py in matrixplot(adata, var_names, groupby, use_raw, log, num_categories, figsize, dendrogram, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, standard_scale, swap_axes, show, save, **kwds). 1644 var_names=var_names,. 1645 var_group_labels=var_group_labels,. -> 1646 var_group_positions=var_group_positions). 1647 . 1648 var_group_labels = dendro_data['var_group_labels']. /anaconda3/envs/test/lib/python3.6/site-packages/scanpy/plotting/_anndata.py in _reorder_categories_after_dendrogram(adata, groupby, dendrogram, var_names, var_group_labels, var_group_positions). 2332 """""". 2333 . -> 2334 key = _get_dendrogram_key(adata, dendrogram, groupby). 2335 . 2336 dendro_info = adata.uns[key]. /anaconda3/envs/test/lib/python3.6/site-packages/scanpy/plotting/_anndata.py in _get_dendrogram_key(adata, dendrogram_key, groupby). 2406 . 2407 if dendrogram_key not in adata.uns:. -> 2408 from ..tools._dendrogram import dendrogram. 2409 logg.warn(""dendrogram data not found (using key={}). Running `sc.tl.dendrogram` "". 2410 ""with default parameters. For fine tuning it is recommended to run `sc.tl.dendrogram` "". ModuleNotFoundError: No module named 'scanpy.tools._dendrogram'. ```. And indeed, in my scanpy this dendrogram tool is missing. . Is the PyPi version missing something? Thank you!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/560
https://github.com/scverse/scanpy/issues/560:2253,usability,tool,tool,2253,"o ```standard_scale``` are missing. So I created a new conda environment and tried to install a new version of scanpy, but this did not solve the issue (i.e. the problem is not with my old environment) as these lines are still missing. . When I tried to replace the file and re-run my heatmap I got a different error:. ```. ---------------------------------------------------------------------------. ModuleNotFoundError Traceback (most recent call last). <ipython-input-5-49e0357ed731> in <module>. ----> 1 sc.pl.matrixplot(pbmc, marker_genes, groupby='bulk_labels', dendrogram=True, standard_scale='var'). /anaconda3/envs/test/lib/python3.6/site-packages/scanpy/plotting/_anndata.py in matrixplot(adata, var_names, groupby, use_raw, log, num_categories, figsize, dendrogram, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, standard_scale, swap_axes, show, save, **kwds). 1644 var_names=var_names,. 1645 var_group_labels=var_group_labels,. -> 1646 var_group_positions=var_group_positions). 1647 . 1648 var_group_labels = dendro_data['var_group_labels']. /anaconda3/envs/test/lib/python3.6/site-packages/scanpy/plotting/_anndata.py in _reorder_categories_after_dendrogram(adata, groupby, dendrogram, var_names, var_group_labels, var_group_positions). 2332 """""". 2333 . -> 2334 key = _get_dendrogram_key(adata, dendrogram, groupby). 2335 . 2336 dendro_info = adata.uns[key]. /anaconda3/envs/test/lib/python3.6/site-packages/scanpy/plotting/_anndata.py in _get_dendrogram_key(adata, dendrogram_key, groupby). 2406 . 2407 if dendrogram_key not in adata.uns:. -> 2408 from ..tools._dendrogram import dendrogram. 2409 logg.warn(""dendrogram data not found (using key={}). Running `sc.tl.dendrogram` "". 2410 ""with default parameters. For fine tuning it is recommended to run `sc.tl.dendrogram` "". ModuleNotFoundError: No module named 'scanpy.tools._dendrogram'. ```. And indeed, in my scanpy this dendrogram tool is missing. . Is the PyPi version missing something? Thank you!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/560
https://github.com/scverse/scanpy/issues/561:182,deployability,instal,installed,182,"Cannot import scanpy in jupyter notebook with python 3.5.2; Hello,. I'm having troubles with importing scanpy in my jupyter notebook. I am working on a server which has python 3.5.2 installed, and unfortunately I cannot update it. I am getting this issue:. ```pytb. import scanpy as sc. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-111-0074c9bc0b31> in <module>(). ----> 1 import scanpy as sc. ~/.local/lib/python3.5/site-packages/scanpy/__init__.py in <module>(). 29 . 30 # the actual API. ---> 31 from . import tools as tl. 32 from . import preprocessing as pp. 33 from . import plotting as pl. ~/.local/lib/python3.5/site-packages/scanpy/tools/__init__.py in <module>(). 8 from ._rank_genes_groups import rank_genes_groups. 9 from ._dpt import dpt. ---> 10 from ._leiden import leiden. 11 from ._louvain import louvain. 12 from ._sim import sim. ~/.local/lib/python3.5/site-packages/scanpy/tools/_leiden.py in <module>(). 29 n_iterations: int = -1,. 30 partition_type: Optional[Type[MutableVertexPartition]] = None,. ---> 31 copy: bool = False,. 32 **partition_kwargs. 33 ) -> Optional[AnnData]:. /usr/lib/python3.5/typing.py in __getitem__(self, arg). 647 def __getitem__(self, arg):. 648 arg = _type_check(arg, ""Optional[t] requires a single type.""). --> 649 return Union[arg, type(None)]. 650 . 651 . /usr/lib/python3.5/typing.py in __getitem__(self, parameters). 550 parameters = (parameters,). 551 return self.__class__(self.__name__, self.__bases__,. --> 552 dict(self.__dict__), parameters, _root=True). 553 . 554 def __eq__(self, other):. /usr/lib/python3.5/typing.py in __new__(cls, name, bases, namespace, parameters, _root). 510 continue. 511 if any(isinstance(t2, type) and issubclass(t1, t2). --> 512 for t2 in all_params - {t1} if not isinstance(t2, TypeVar)):. 513 all_params.remove(t1). 514 # It's not a union if there's only one type left. /usr/lib/python3.5/typing.py in <genexpr>(.0). 5",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/561
https://github.com/scverse/scanpy/issues/561:220,deployability,updat,update,220,"Cannot import scanpy in jupyter notebook with python 3.5.2; Hello,. I'm having troubles with importing scanpy in my jupyter notebook. I am working on a server which has python 3.5.2 installed, and unfortunately I cannot update it. I am getting this issue:. ```pytb. import scanpy as sc. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-111-0074c9bc0b31> in <module>(). ----> 1 import scanpy as sc. ~/.local/lib/python3.5/site-packages/scanpy/__init__.py in <module>(). 29 . 30 # the actual API. ---> 31 from . import tools as tl. 32 from . import preprocessing as pp. 33 from . import plotting as pl. ~/.local/lib/python3.5/site-packages/scanpy/tools/__init__.py in <module>(). 8 from ._rank_genes_groups import rank_genes_groups. 9 from ._dpt import dpt. ---> 10 from ._leiden import leiden. 11 from ._louvain import louvain. 12 from ._sim import sim. ~/.local/lib/python3.5/site-packages/scanpy/tools/_leiden.py in <module>(). 29 n_iterations: int = -1,. 30 partition_type: Optional[Type[MutableVertexPartition]] = None,. ---> 31 copy: bool = False,. 32 **partition_kwargs. 33 ) -> Optional[AnnData]:. /usr/lib/python3.5/typing.py in __getitem__(self, arg). 647 def __getitem__(self, arg):. 648 arg = _type_check(arg, ""Optional[t] requires a single type.""). --> 649 return Union[arg, type(None)]. 650 . 651 . /usr/lib/python3.5/typing.py in __getitem__(self, parameters). 550 parameters = (parameters,). 551 return self.__class__(self.__name__, self.__bases__,. --> 552 dict(self.__dict__), parameters, _root=True). 553 . 554 def __eq__(self, other):. /usr/lib/python3.5/typing.py in __new__(cls, name, bases, namespace, parameters, _root). 510 continue. 511 if any(isinstance(t2, type) and issubclass(t1, t2). --> 512 for t2 in all_params - {t1} if not isinstance(t2, TypeVar)):. 513 all_params.remove(t1). 514 # It's not a union if there's only one type left. /usr/lib/python3.5/typing.py in <genexpr>(.0). 5",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/561
https://github.com/scverse/scanpy/issues/561:446,deployability,modul,module,446,"Cannot import scanpy in jupyter notebook with python 3.5.2; Hello,. I'm having troubles with importing scanpy in my jupyter notebook. I am working on a server which has python 3.5.2 installed, and unfortunately I cannot update it. I am getting this issue:. ```pytb. import scanpy as sc. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-111-0074c9bc0b31> in <module>(). ----> 1 import scanpy as sc. ~/.local/lib/python3.5/site-packages/scanpy/__init__.py in <module>(). 29 . 30 # the actual API. ---> 31 from . import tools as tl. 32 from . import preprocessing as pp. 33 from . import plotting as pl. ~/.local/lib/python3.5/site-packages/scanpy/tools/__init__.py in <module>(). 8 from ._rank_genes_groups import rank_genes_groups. 9 from ._dpt import dpt. ---> 10 from ._leiden import leiden. 11 from ._louvain import louvain. 12 from ._sim import sim. ~/.local/lib/python3.5/site-packages/scanpy/tools/_leiden.py in <module>(). 29 n_iterations: int = -1,. 30 partition_type: Optional[Type[MutableVertexPartition]] = None,. ---> 31 copy: bool = False,. 32 **partition_kwargs. 33 ) -> Optional[AnnData]:. /usr/lib/python3.5/typing.py in __getitem__(self, arg). 647 def __getitem__(self, arg):. 648 arg = _type_check(arg, ""Optional[t] requires a single type.""). --> 649 return Union[arg, type(None)]. 650 . 651 . /usr/lib/python3.5/typing.py in __getitem__(self, parameters). 550 parameters = (parameters,). 551 return self.__class__(self.__name__, self.__bases__,. --> 552 dict(self.__dict__), parameters, _root=True). 553 . 554 def __eq__(self, other):. /usr/lib/python3.5/typing.py in __new__(cls, name, bases, namespace, parameters, _root). 510 continue. 511 if any(isinstance(t2, type) and issubclass(t1, t2). --> 512 for t2 in all_params - {t1} if not isinstance(t2, TypeVar)):. 513 all_params.remove(t1). 514 # It's not a union if there's only one type left. /usr/lib/python3.5/typing.py in <genexpr>(.0). 5",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/561
https://github.com/scverse/scanpy/issues/561:546,deployability,modul,module,546,"Cannot import scanpy in jupyter notebook with python 3.5.2; Hello,. I'm having troubles with importing scanpy in my jupyter notebook. I am working on a server which has python 3.5.2 installed, and unfortunately I cannot update it. I am getting this issue:. ```pytb. import scanpy as sc. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-111-0074c9bc0b31> in <module>(). ----> 1 import scanpy as sc. ~/.local/lib/python3.5/site-packages/scanpy/__init__.py in <module>(). 29 . 30 # the actual API. ---> 31 from . import tools as tl. 32 from . import preprocessing as pp. 33 from . import plotting as pl. ~/.local/lib/python3.5/site-packages/scanpy/tools/__init__.py in <module>(). 8 from ._rank_genes_groups import rank_genes_groups. 9 from ._dpt import dpt. ---> 10 from ._leiden import leiden. 11 from ._louvain import louvain. 12 from ._sim import sim. ~/.local/lib/python3.5/site-packages/scanpy/tools/_leiden.py in <module>(). 29 n_iterations: int = -1,. 30 partition_type: Optional[Type[MutableVertexPartition]] = None,. ---> 31 copy: bool = False,. 32 **partition_kwargs. 33 ) -> Optional[AnnData]:. /usr/lib/python3.5/typing.py in __getitem__(self, arg). 647 def __getitem__(self, arg):. 648 arg = _type_check(arg, ""Optional[t] requires a single type.""). --> 649 return Union[arg, type(None)]. 650 . 651 . /usr/lib/python3.5/typing.py in __getitem__(self, parameters). 550 parameters = (parameters,). 551 return self.__class__(self.__name__, self.__bases__,. --> 552 dict(self.__dict__), parameters, _root=True). 553 . 554 def __eq__(self, other):. /usr/lib/python3.5/typing.py in __new__(cls, name, bases, namespace, parameters, _root). 510 continue. 511 if any(isinstance(t2, type) and issubclass(t1, t2). --> 512 for t2 in all_params - {t1} if not isinstance(t2, TypeVar)):. 513 all_params.remove(t1). 514 # It's not a union if there's only one type left. /usr/lib/python3.5/typing.py in <genexpr>(.0). 5",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/561
https://github.com/scverse/scanpy/issues/561:578,deployability,API,API,578,"Cannot import scanpy in jupyter notebook with python 3.5.2; Hello,. I'm having troubles with importing scanpy in my jupyter notebook. I am working on a server which has python 3.5.2 installed, and unfortunately I cannot update it. I am getting this issue:. ```pytb. import scanpy as sc. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-111-0074c9bc0b31> in <module>(). ----> 1 import scanpy as sc. ~/.local/lib/python3.5/site-packages/scanpy/__init__.py in <module>(). 29 . 30 # the actual API. ---> 31 from . import tools as tl. 32 from . import preprocessing as pp. 33 from . import plotting as pl. ~/.local/lib/python3.5/site-packages/scanpy/tools/__init__.py in <module>(). 8 from ._rank_genes_groups import rank_genes_groups. 9 from ._dpt import dpt. ---> 10 from ._leiden import leiden. 11 from ._louvain import louvain. 12 from ._sim import sim. ~/.local/lib/python3.5/site-packages/scanpy/tools/_leiden.py in <module>(). 29 n_iterations: int = -1,. 30 partition_type: Optional[Type[MutableVertexPartition]] = None,. ---> 31 copy: bool = False,. 32 **partition_kwargs. 33 ) -> Optional[AnnData]:. /usr/lib/python3.5/typing.py in __getitem__(self, arg). 647 def __getitem__(self, arg):. 648 arg = _type_check(arg, ""Optional[t] requires a single type.""). --> 649 return Union[arg, type(None)]. 650 . 651 . /usr/lib/python3.5/typing.py in __getitem__(self, parameters). 550 parameters = (parameters,). 551 return self.__class__(self.__name__, self.__bases__,. --> 552 dict(self.__dict__), parameters, _root=True). 553 . 554 def __eq__(self, other):. /usr/lib/python3.5/typing.py in __new__(cls, name, bases, namespace, parameters, _root). 510 continue. 511 if any(isinstance(t2, type) and issubclass(t1, t2). --> 512 for t2 in all_params - {t1} if not isinstance(t2, TypeVar)):. 513 all_params.remove(t1). 514 # It's not a union if there's only one type left. /usr/lib/python3.5/typing.py in <genexpr>(.0). 5",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/561
https://github.com/scverse/scanpy/issues/561:755,deployability,modul,module,755,"Cannot import scanpy in jupyter notebook with python 3.5.2; Hello,. I'm having troubles with importing scanpy in my jupyter notebook. I am working on a server which has python 3.5.2 installed, and unfortunately I cannot update it. I am getting this issue:. ```pytb. import scanpy as sc. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-111-0074c9bc0b31> in <module>(). ----> 1 import scanpy as sc. ~/.local/lib/python3.5/site-packages/scanpy/__init__.py in <module>(). 29 . 30 # the actual API. ---> 31 from . import tools as tl. 32 from . import preprocessing as pp. 33 from . import plotting as pl. ~/.local/lib/python3.5/site-packages/scanpy/tools/__init__.py in <module>(). 8 from ._rank_genes_groups import rank_genes_groups. 9 from ._dpt import dpt. ---> 10 from ._leiden import leiden. 11 from ._louvain import louvain. 12 from ._sim import sim. ~/.local/lib/python3.5/site-packages/scanpy/tools/_leiden.py in <module>(). 29 n_iterations: int = -1,. 30 partition_type: Optional[Type[MutableVertexPartition]] = None,. ---> 31 copy: bool = False,. 32 **partition_kwargs. 33 ) -> Optional[AnnData]:. /usr/lib/python3.5/typing.py in __getitem__(self, arg). 647 def __getitem__(self, arg):. 648 arg = _type_check(arg, ""Optional[t] requires a single type.""). --> 649 return Union[arg, type(None)]. 650 . 651 . /usr/lib/python3.5/typing.py in __getitem__(self, parameters). 550 parameters = (parameters,). 551 return self.__class__(self.__name__, self.__bases__,. --> 552 dict(self.__dict__), parameters, _root=True). 553 . 554 def __eq__(self, other):. /usr/lib/python3.5/typing.py in __new__(cls, name, bases, namespace, parameters, _root). 510 continue. 511 if any(isinstance(t2, type) and issubclass(t1, t2). --> 512 for t2 in all_params - {t1} if not isinstance(t2, TypeVar)):. 513 all_params.remove(t1). 514 # It's not a union if there's only one type left. /usr/lib/python3.5/typing.py in <genexpr>(.0). 5",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/561
https://github.com/scverse/scanpy/issues/561:1006,deployability,modul,module,1006,"port scanpy in jupyter notebook with python 3.5.2; Hello,. I'm having troubles with importing scanpy in my jupyter notebook. I am working on a server which has python 3.5.2 installed, and unfortunately I cannot update it. I am getting this issue:. ```pytb. import scanpy as sc. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-111-0074c9bc0b31> in <module>(). ----> 1 import scanpy as sc. ~/.local/lib/python3.5/site-packages/scanpy/__init__.py in <module>(). 29 . 30 # the actual API. ---> 31 from . import tools as tl. 32 from . import preprocessing as pp. 33 from . import plotting as pl. ~/.local/lib/python3.5/site-packages/scanpy/tools/__init__.py in <module>(). 8 from ._rank_genes_groups import rank_genes_groups. 9 from ._dpt import dpt. ---> 10 from ._leiden import leiden. 11 from ._louvain import louvain. 12 from ._sim import sim. ~/.local/lib/python3.5/site-packages/scanpy/tools/_leiden.py in <module>(). 29 n_iterations: int = -1,. 30 partition_type: Optional[Type[MutableVertexPartition]] = None,. ---> 31 copy: bool = False,. 32 **partition_kwargs. 33 ) -> Optional[AnnData]:. /usr/lib/python3.5/typing.py in __getitem__(self, arg). 647 def __getitem__(self, arg):. 648 arg = _type_check(arg, ""Optional[t] requires a single type.""). --> 649 return Union[arg, type(None)]. 650 . 651 . /usr/lib/python3.5/typing.py in __getitem__(self, parameters). 550 parameters = (parameters,). 551 return self.__class__(self.__name__, self.__bases__,. --> 552 dict(self.__dict__), parameters, _root=True). 553 . 554 def __eq__(self, other):. /usr/lib/python3.5/typing.py in __new__(cls, name, bases, namespace, parameters, _root). 510 continue. 511 if any(isinstance(t2, type) and issubclass(t1, t2). --> 512 for t2 in all_params - {t1} if not isinstance(t2, TypeVar)):. 513 all_params.remove(t1). 514 # It's not a union if there's only one type left. /usr/lib/python3.5/typing.py in <genexpr>(.0). 510 contin",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/561
https://github.com/scverse/scanpy/issues/561:1735,deployability,continu,continue,1735,"__init__.py in <module>(). 8 from ._rank_genes_groups import rank_genes_groups. 9 from ._dpt import dpt. ---> 10 from ._leiden import leiden. 11 from ._louvain import louvain. 12 from ._sim import sim. ~/.local/lib/python3.5/site-packages/scanpy/tools/_leiden.py in <module>(). 29 n_iterations: int = -1,. 30 partition_type: Optional[Type[MutableVertexPartition]] = None,. ---> 31 copy: bool = False,. 32 **partition_kwargs. 33 ) -> Optional[AnnData]:. /usr/lib/python3.5/typing.py in __getitem__(self, arg). 647 def __getitem__(self, arg):. 648 arg = _type_check(arg, ""Optional[t] requires a single type.""). --> 649 return Union[arg, type(None)]. 650 . 651 . /usr/lib/python3.5/typing.py in __getitem__(self, parameters). 550 parameters = (parameters,). 551 return self.__class__(self.__name__, self.__bases__,. --> 552 dict(self.__dict__), parameters, _root=True). 553 . 554 def __eq__(self, other):. /usr/lib/python3.5/typing.py in __new__(cls, name, bases, namespace, parameters, _root). 510 continue. 511 if any(isinstance(t2, type) and issubclass(t1, t2). --> 512 for t2 in all_params - {t1} if not isinstance(t2, TypeVar)):. 513 all_params.remove(t1). 514 # It's not a union if there's only one type left. /usr/lib/python3.5/typing.py in <genexpr>(.0). 510 continue. 511 if any(isinstance(t2, type) and issubclass(t1, t2). --> 512 for t2 in all_params - {t1} if not isinstance(t2, TypeVar)):. 513 all_params.remove(t1). 514 # It's not a union if there's only one type left. /usr/lib/python3.5/typing.py in __subclasscheck__(self, cls). 1075 return True. 1076 # If we break out of the loop, the superclass gets a chance. -> 1077 if super().__subclasscheck__(cls):. 1078 return True. 1079 if self.__extra__ is None or isinstance(cls, GenericMeta):. /usr/lib/python3.5/abc.py in __subclasscheck__(cls, subclass). 223 return True. 224 # Check if it's a subclass of a subclass (recursive). --> 225 for scls in cls.__subclasses__():. 226 if issubclass(subclass, scls):. 227 cls._abc_cache.add(subcla",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/561
https://github.com/scverse/scanpy/issues/561:2003,deployability,continu,continue,2003,"odule>(). 29 n_iterations: int = -1,. 30 partition_type: Optional[Type[MutableVertexPartition]] = None,. ---> 31 copy: bool = False,. 32 **partition_kwargs. 33 ) -> Optional[AnnData]:. /usr/lib/python3.5/typing.py in __getitem__(self, arg). 647 def __getitem__(self, arg):. 648 arg = _type_check(arg, ""Optional[t] requires a single type.""). --> 649 return Union[arg, type(None)]. 650 . 651 . /usr/lib/python3.5/typing.py in __getitem__(self, parameters). 550 parameters = (parameters,). 551 return self.__class__(self.__name__, self.__bases__,. --> 552 dict(self.__dict__), parameters, _root=True). 553 . 554 def __eq__(self, other):. /usr/lib/python3.5/typing.py in __new__(cls, name, bases, namespace, parameters, _root). 510 continue. 511 if any(isinstance(t2, type) and issubclass(t1, t2). --> 512 for t2 in all_params - {t1} if not isinstance(t2, TypeVar)):. 513 all_params.remove(t1). 514 # It's not a union if there's only one type left. /usr/lib/python3.5/typing.py in <genexpr>(.0). 510 continue. 511 if any(isinstance(t2, type) and issubclass(t1, t2). --> 512 for t2 in all_params - {t1} if not isinstance(t2, TypeVar)):. 513 all_params.remove(t1). 514 # It's not a union if there's only one type left. /usr/lib/python3.5/typing.py in __subclasscheck__(self, cls). 1075 return True. 1076 # If we break out of the loop, the superclass gets a chance. -> 1077 if super().__subclasscheck__(cls):. 1078 return True. 1079 if self.__extra__ is None or isinstance(cls, GenericMeta):. /usr/lib/python3.5/abc.py in __subclasscheck__(cls, subclass). 223 return True. 224 # Check if it's a subclass of a subclass (recursive). --> 225 for scls in cls.__subclasses__():. 226 if issubclass(subclass, scls):. 227 cls._abc_cache.add(subclass). TypeError: descriptor '__subclasses__' of 'type' object needs an argument. ```. I am also getting the same issue with import scanpy.api as sc. I believe this is a common issue with python 3.5.2 but unfortunately I cannot change the python version. Thank you so mu",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/561
https://github.com/scverse/scanpy/issues/561:2876,deployability,api,api,2876,". 29 n_iterations: int = -1,. 30 partition_type: Optional[Type[MutableVertexPartition]] = None,. ---> 31 copy: bool = False,. 32 **partition_kwargs. 33 ) -> Optional[AnnData]:. /usr/lib/python3.5/typing.py in __getitem__(self, arg). 647 def __getitem__(self, arg):. 648 arg = _type_check(arg, ""Optional[t] requires a single type.""). --> 649 return Union[arg, type(None)]. 650 . 651 . /usr/lib/python3.5/typing.py in __getitem__(self, parameters). 550 parameters = (parameters,). 551 return self.__class__(self.__name__, self.__bases__,. --> 552 dict(self.__dict__), parameters, _root=True). 553 . 554 def __eq__(self, other):. /usr/lib/python3.5/typing.py in __new__(cls, name, bases, namespace, parameters, _root). 510 continue. 511 if any(isinstance(t2, type) and issubclass(t1, t2). --> 512 for t2 in all_params - {t1} if not isinstance(t2, TypeVar)):. 513 all_params.remove(t1). 514 # It's not a union if there's only one type left. /usr/lib/python3.5/typing.py in <genexpr>(.0). 510 continue. 511 if any(isinstance(t2, type) and issubclass(t1, t2). --> 512 for t2 in all_params - {t1} if not isinstance(t2, TypeVar)):. 513 all_params.remove(t1). 514 # It's not a union if there's only one type left. /usr/lib/python3.5/typing.py in __subclasscheck__(self, cls). 1075 return True. 1076 # If we break out of the loop, the superclass gets a chance. -> 1077 if super().__subclasscheck__(cls):. 1078 return True. 1079 if self.__extra__ is None or isinstance(cls, GenericMeta):. /usr/lib/python3.5/abc.py in __subclasscheck__(cls, subclass). 223 return True. 224 # Check if it's a subclass of a subclass (recursive). --> 225 for scls in cls.__subclasses__():. 226 if issubclass(subclass, scls):. 227 cls._abc_cache.add(subclass). TypeError: descriptor '__subclasses__' of 'type' object needs an argument. ```. I am also getting the same issue with import scanpy.api as sc. I believe this is a common issue with python 3.5.2 but unfortunately I cannot change the python version. Thank you so much! Olga",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/561
https://github.com/scverse/scanpy/issues/561:2983,deployability,version,version,2983,". 29 n_iterations: int = -1,. 30 partition_type: Optional[Type[MutableVertexPartition]] = None,. ---> 31 copy: bool = False,. 32 **partition_kwargs. 33 ) -> Optional[AnnData]:. /usr/lib/python3.5/typing.py in __getitem__(self, arg). 647 def __getitem__(self, arg):. 648 arg = _type_check(arg, ""Optional[t] requires a single type.""). --> 649 return Union[arg, type(None)]. 650 . 651 . /usr/lib/python3.5/typing.py in __getitem__(self, parameters). 550 parameters = (parameters,). 551 return self.__class__(self.__name__, self.__bases__,. --> 552 dict(self.__dict__), parameters, _root=True). 553 . 554 def __eq__(self, other):. /usr/lib/python3.5/typing.py in __new__(cls, name, bases, namespace, parameters, _root). 510 continue. 511 if any(isinstance(t2, type) and issubclass(t1, t2). --> 512 for t2 in all_params - {t1} if not isinstance(t2, TypeVar)):. 513 all_params.remove(t1). 514 # It's not a union if there's only one type left. /usr/lib/python3.5/typing.py in <genexpr>(.0). 510 continue. 511 if any(isinstance(t2, type) and issubclass(t1, t2). --> 512 for t2 in all_params - {t1} if not isinstance(t2, TypeVar)):. 513 all_params.remove(t1). 514 # It's not a union if there's only one type left. /usr/lib/python3.5/typing.py in __subclasscheck__(self, cls). 1075 return True. 1076 # If we break out of the loop, the superclass gets a chance. -> 1077 if super().__subclasscheck__(cls):. 1078 return True. 1079 if self.__extra__ is None or isinstance(cls, GenericMeta):. /usr/lib/python3.5/abc.py in __subclasscheck__(cls, subclass). 223 return True. 224 # Check if it's a subclass of a subclass (recursive). --> 225 for scls in cls.__subclasses__():. 226 if issubclass(subclass, scls):. 227 cls._abc_cache.add(subclass). TypeError: descriptor '__subclasses__' of 'type' object needs an argument. ```. I am also getting the same issue with import scanpy.api as sc. I believe this is a common issue with python 3.5.2 but unfortunately I cannot change the python version. Thank you so much! Olga",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/561
https://github.com/scverse/scanpy/issues/561:578,integrability,API,API,578,"Cannot import scanpy in jupyter notebook with python 3.5.2; Hello,. I'm having troubles with importing scanpy in my jupyter notebook. I am working on a server which has python 3.5.2 installed, and unfortunately I cannot update it. I am getting this issue:. ```pytb. import scanpy as sc. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-111-0074c9bc0b31> in <module>(). ----> 1 import scanpy as sc. ~/.local/lib/python3.5/site-packages/scanpy/__init__.py in <module>(). 29 . 30 # the actual API. ---> 31 from . import tools as tl. 32 from . import preprocessing as pp. 33 from . import plotting as pl. ~/.local/lib/python3.5/site-packages/scanpy/tools/__init__.py in <module>(). 8 from ._rank_genes_groups import rank_genes_groups. 9 from ._dpt import dpt. ---> 10 from ._leiden import leiden. 11 from ._louvain import louvain. 12 from ._sim import sim. ~/.local/lib/python3.5/site-packages/scanpy/tools/_leiden.py in <module>(). 29 n_iterations: int = -1,. 30 partition_type: Optional[Type[MutableVertexPartition]] = None,. ---> 31 copy: bool = False,. 32 **partition_kwargs. 33 ) -> Optional[AnnData]:. /usr/lib/python3.5/typing.py in __getitem__(self, arg). 647 def __getitem__(self, arg):. 648 arg = _type_check(arg, ""Optional[t] requires a single type.""). --> 649 return Union[arg, type(None)]. 650 . 651 . /usr/lib/python3.5/typing.py in __getitem__(self, parameters). 550 parameters = (parameters,). 551 return self.__class__(self.__name__, self.__bases__,. --> 552 dict(self.__dict__), parameters, _root=True). 553 . 554 def __eq__(self, other):. /usr/lib/python3.5/typing.py in __new__(cls, name, bases, namespace, parameters, _root). 510 continue. 511 if any(isinstance(t2, type) and issubclass(t1, t2). --> 512 for t2 in all_params - {t1} if not isinstance(t2, TypeVar)):. 513 all_params.remove(t1). 514 # It's not a union if there's only one type left. /usr/lib/python3.5/typing.py in <genexpr>(.0). 5",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/561
https://github.com/scverse/scanpy/issues/561:2545,integrability,sub,subclass,2545,". 29 n_iterations: int = -1,. 30 partition_type: Optional[Type[MutableVertexPartition]] = None,. ---> 31 copy: bool = False,. 32 **partition_kwargs. 33 ) -> Optional[AnnData]:. /usr/lib/python3.5/typing.py in __getitem__(self, arg). 647 def __getitem__(self, arg):. 648 arg = _type_check(arg, ""Optional[t] requires a single type.""). --> 649 return Union[arg, type(None)]. 650 . 651 . /usr/lib/python3.5/typing.py in __getitem__(self, parameters). 550 parameters = (parameters,). 551 return self.__class__(self.__name__, self.__bases__,. --> 552 dict(self.__dict__), parameters, _root=True). 553 . 554 def __eq__(self, other):. /usr/lib/python3.5/typing.py in __new__(cls, name, bases, namespace, parameters, _root). 510 continue. 511 if any(isinstance(t2, type) and issubclass(t1, t2). --> 512 for t2 in all_params - {t1} if not isinstance(t2, TypeVar)):. 513 all_params.remove(t1). 514 # It's not a union if there's only one type left. /usr/lib/python3.5/typing.py in <genexpr>(.0). 510 continue. 511 if any(isinstance(t2, type) and issubclass(t1, t2). --> 512 for t2 in all_params - {t1} if not isinstance(t2, TypeVar)):. 513 all_params.remove(t1). 514 # It's not a union if there's only one type left. /usr/lib/python3.5/typing.py in __subclasscheck__(self, cls). 1075 return True. 1076 # If we break out of the loop, the superclass gets a chance. -> 1077 if super().__subclasscheck__(cls):. 1078 return True. 1079 if self.__extra__ is None or isinstance(cls, GenericMeta):. /usr/lib/python3.5/abc.py in __subclasscheck__(cls, subclass). 223 return True. 224 # Check if it's a subclass of a subclass (recursive). --> 225 for scls in cls.__subclasses__():. 226 if issubclass(subclass, scls):. 227 cls._abc_cache.add(subclass). TypeError: descriptor '__subclasses__' of 'type' object needs an argument. ```. I am also getting the same issue with import scanpy.api as sc. I believe this is a common issue with python 3.5.2 but unfortunately I cannot change the python version. Thank you so much! Olga",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/561
https://github.com/scverse/scanpy/issues/561:2595,integrability,sub,subclass,2595,". 29 n_iterations: int = -1,. 30 partition_type: Optional[Type[MutableVertexPartition]] = None,. ---> 31 copy: bool = False,. 32 **partition_kwargs. 33 ) -> Optional[AnnData]:. /usr/lib/python3.5/typing.py in __getitem__(self, arg). 647 def __getitem__(self, arg):. 648 arg = _type_check(arg, ""Optional[t] requires a single type.""). --> 649 return Union[arg, type(None)]. 650 . 651 . /usr/lib/python3.5/typing.py in __getitem__(self, parameters). 550 parameters = (parameters,). 551 return self.__class__(self.__name__, self.__bases__,. --> 552 dict(self.__dict__), parameters, _root=True). 553 . 554 def __eq__(self, other):. /usr/lib/python3.5/typing.py in __new__(cls, name, bases, namespace, parameters, _root). 510 continue. 511 if any(isinstance(t2, type) and issubclass(t1, t2). --> 512 for t2 in all_params - {t1} if not isinstance(t2, TypeVar)):. 513 all_params.remove(t1). 514 # It's not a union if there's only one type left. /usr/lib/python3.5/typing.py in <genexpr>(.0). 510 continue. 511 if any(isinstance(t2, type) and issubclass(t1, t2). --> 512 for t2 in all_params - {t1} if not isinstance(t2, TypeVar)):. 513 all_params.remove(t1). 514 # It's not a union if there's only one type left. /usr/lib/python3.5/typing.py in __subclasscheck__(self, cls). 1075 return True. 1076 # If we break out of the loop, the superclass gets a chance. -> 1077 if super().__subclasscheck__(cls):. 1078 return True. 1079 if self.__extra__ is None or isinstance(cls, GenericMeta):. /usr/lib/python3.5/abc.py in __subclasscheck__(cls, subclass). 223 return True. 224 # Check if it's a subclass of a subclass (recursive). --> 225 for scls in cls.__subclasses__():. 226 if issubclass(subclass, scls):. 227 cls._abc_cache.add(subclass). TypeError: descriptor '__subclasses__' of 'type' object needs an argument. ```. I am also getting the same issue with import scanpy.api as sc. I believe this is a common issue with python 3.5.2 but unfortunately I cannot change the python version. Thank you so much! Olga",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/561
https://github.com/scverse/scanpy/issues/561:2609,integrability,sub,subclass,2609,". 29 n_iterations: int = -1,. 30 partition_type: Optional[Type[MutableVertexPartition]] = None,. ---> 31 copy: bool = False,. 32 **partition_kwargs. 33 ) -> Optional[AnnData]:. /usr/lib/python3.5/typing.py in __getitem__(self, arg). 647 def __getitem__(self, arg):. 648 arg = _type_check(arg, ""Optional[t] requires a single type.""). --> 649 return Union[arg, type(None)]. 650 . 651 . /usr/lib/python3.5/typing.py in __getitem__(self, parameters). 550 parameters = (parameters,). 551 return self.__class__(self.__name__, self.__bases__,. --> 552 dict(self.__dict__), parameters, _root=True). 553 . 554 def __eq__(self, other):. /usr/lib/python3.5/typing.py in __new__(cls, name, bases, namespace, parameters, _root). 510 continue. 511 if any(isinstance(t2, type) and issubclass(t1, t2). --> 512 for t2 in all_params - {t1} if not isinstance(t2, TypeVar)):. 513 all_params.remove(t1). 514 # It's not a union if there's only one type left. /usr/lib/python3.5/typing.py in <genexpr>(.0). 510 continue. 511 if any(isinstance(t2, type) and issubclass(t1, t2). --> 512 for t2 in all_params - {t1} if not isinstance(t2, TypeVar)):. 513 all_params.remove(t1). 514 # It's not a union if there's only one type left. /usr/lib/python3.5/typing.py in __subclasscheck__(self, cls). 1075 return True. 1076 # If we break out of the loop, the superclass gets a chance. -> 1077 if super().__subclasscheck__(cls):. 1078 return True. 1079 if self.__extra__ is None or isinstance(cls, GenericMeta):. /usr/lib/python3.5/abc.py in __subclasscheck__(cls, subclass). 223 return True. 224 # Check if it's a subclass of a subclass (recursive). --> 225 for scls in cls.__subclasses__():. 226 if issubclass(subclass, scls):. 227 cls._abc_cache.add(subclass). TypeError: descriptor '__subclasses__' of 'type' object needs an argument. ```. I am also getting the same issue with import scanpy.api as sc. I believe this is a common issue with python 3.5.2 but unfortunately I cannot change the python version. Thank you so much! Olga",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/561
https://github.com/scverse/scanpy/issues/561:2692,integrability,sub,subclass,2692,". 29 n_iterations: int = -1,. 30 partition_type: Optional[Type[MutableVertexPartition]] = None,. ---> 31 copy: bool = False,. 32 **partition_kwargs. 33 ) -> Optional[AnnData]:. /usr/lib/python3.5/typing.py in __getitem__(self, arg). 647 def __getitem__(self, arg):. 648 arg = _type_check(arg, ""Optional[t] requires a single type.""). --> 649 return Union[arg, type(None)]. 650 . 651 . /usr/lib/python3.5/typing.py in __getitem__(self, parameters). 550 parameters = (parameters,). 551 return self.__class__(self.__name__, self.__bases__,. --> 552 dict(self.__dict__), parameters, _root=True). 553 . 554 def __eq__(self, other):. /usr/lib/python3.5/typing.py in __new__(cls, name, bases, namespace, parameters, _root). 510 continue. 511 if any(isinstance(t2, type) and issubclass(t1, t2). --> 512 for t2 in all_params - {t1} if not isinstance(t2, TypeVar)):. 513 all_params.remove(t1). 514 # It's not a union if there's only one type left. /usr/lib/python3.5/typing.py in <genexpr>(.0). 510 continue. 511 if any(isinstance(t2, type) and issubclass(t1, t2). --> 512 for t2 in all_params - {t1} if not isinstance(t2, TypeVar)):. 513 all_params.remove(t1). 514 # It's not a union if there's only one type left. /usr/lib/python3.5/typing.py in __subclasscheck__(self, cls). 1075 return True. 1076 # If we break out of the loop, the superclass gets a chance. -> 1077 if super().__subclasscheck__(cls):. 1078 return True. 1079 if self.__extra__ is None or isinstance(cls, GenericMeta):. /usr/lib/python3.5/abc.py in __subclasscheck__(cls, subclass). 223 return True. 224 # Check if it's a subclass of a subclass (recursive). --> 225 for scls in cls.__subclasses__():. 226 if issubclass(subclass, scls):. 227 cls._abc_cache.add(subclass). TypeError: descriptor '__subclasses__' of 'type' object needs an argument. ```. I am also getting the same issue with import scanpy.api as sc. I believe this is a common issue with python 3.5.2 but unfortunately I cannot change the python version. Thank you so much! Olga",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/561
https://github.com/scverse/scanpy/issues/561:2733,integrability,sub,subclass,2733,". 29 n_iterations: int = -1,. 30 partition_type: Optional[Type[MutableVertexPartition]] = None,. ---> 31 copy: bool = False,. 32 **partition_kwargs. 33 ) -> Optional[AnnData]:. /usr/lib/python3.5/typing.py in __getitem__(self, arg). 647 def __getitem__(self, arg):. 648 arg = _type_check(arg, ""Optional[t] requires a single type.""). --> 649 return Union[arg, type(None)]. 650 . 651 . /usr/lib/python3.5/typing.py in __getitem__(self, parameters). 550 parameters = (parameters,). 551 return self.__class__(self.__name__, self.__bases__,. --> 552 dict(self.__dict__), parameters, _root=True). 553 . 554 def __eq__(self, other):. /usr/lib/python3.5/typing.py in __new__(cls, name, bases, namespace, parameters, _root). 510 continue. 511 if any(isinstance(t2, type) and issubclass(t1, t2). --> 512 for t2 in all_params - {t1} if not isinstance(t2, TypeVar)):. 513 all_params.remove(t1). 514 # It's not a union if there's only one type left. /usr/lib/python3.5/typing.py in <genexpr>(.0). 510 continue. 511 if any(isinstance(t2, type) and issubclass(t1, t2). --> 512 for t2 in all_params - {t1} if not isinstance(t2, TypeVar)):. 513 all_params.remove(t1). 514 # It's not a union if there's only one type left. /usr/lib/python3.5/typing.py in __subclasscheck__(self, cls). 1075 return True. 1076 # If we break out of the loop, the superclass gets a chance. -> 1077 if super().__subclasscheck__(cls):. 1078 return True. 1079 if self.__extra__ is None or isinstance(cls, GenericMeta):. /usr/lib/python3.5/abc.py in __subclasscheck__(cls, subclass). 223 return True. 224 # Check if it's a subclass of a subclass (recursive). --> 225 for scls in cls.__subclasses__():. 226 if issubclass(subclass, scls):. 227 cls._abc_cache.add(subclass). TypeError: descriptor '__subclasses__' of 'type' object needs an argument. ```. I am also getting the same issue with import scanpy.api as sc. I believe this is a common issue with python 3.5.2 but unfortunately I cannot change the python version. Thank you so much! Olga",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/561
https://github.com/scverse/scanpy/issues/561:2876,integrability,api,api,2876,". 29 n_iterations: int = -1,. 30 partition_type: Optional[Type[MutableVertexPartition]] = None,. ---> 31 copy: bool = False,. 32 **partition_kwargs. 33 ) -> Optional[AnnData]:. /usr/lib/python3.5/typing.py in __getitem__(self, arg). 647 def __getitem__(self, arg):. 648 arg = _type_check(arg, ""Optional[t] requires a single type.""). --> 649 return Union[arg, type(None)]. 650 . 651 . /usr/lib/python3.5/typing.py in __getitem__(self, parameters). 550 parameters = (parameters,). 551 return self.__class__(self.__name__, self.__bases__,. --> 552 dict(self.__dict__), parameters, _root=True). 553 . 554 def __eq__(self, other):. /usr/lib/python3.5/typing.py in __new__(cls, name, bases, namespace, parameters, _root). 510 continue. 511 if any(isinstance(t2, type) and issubclass(t1, t2). --> 512 for t2 in all_params - {t1} if not isinstance(t2, TypeVar)):. 513 all_params.remove(t1). 514 # It's not a union if there's only one type left. /usr/lib/python3.5/typing.py in <genexpr>(.0). 510 continue. 511 if any(isinstance(t2, type) and issubclass(t1, t2). --> 512 for t2 in all_params - {t1} if not isinstance(t2, TypeVar)):. 513 all_params.remove(t1). 514 # It's not a union if there's only one type left. /usr/lib/python3.5/typing.py in __subclasscheck__(self, cls). 1075 return True. 1076 # If we break out of the loop, the superclass gets a chance. -> 1077 if super().__subclasscheck__(cls):. 1078 return True. 1079 if self.__extra__ is None or isinstance(cls, GenericMeta):. /usr/lib/python3.5/abc.py in __subclasscheck__(cls, subclass). 223 return True. 224 # Check if it's a subclass of a subclass (recursive). --> 225 for scls in cls.__subclasses__():. 226 if issubclass(subclass, scls):. 227 cls._abc_cache.add(subclass). TypeError: descriptor '__subclasses__' of 'type' object needs an argument. ```. I am also getting the same issue with import scanpy.api as sc. I believe this is a common issue with python 3.5.2 but unfortunately I cannot change the python version. Thank you so much! Olga",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/561
https://github.com/scverse/scanpy/issues/561:2983,integrability,version,version,2983,". 29 n_iterations: int = -1,. 30 partition_type: Optional[Type[MutableVertexPartition]] = None,. ---> 31 copy: bool = False,. 32 **partition_kwargs. 33 ) -> Optional[AnnData]:. /usr/lib/python3.5/typing.py in __getitem__(self, arg). 647 def __getitem__(self, arg):. 648 arg = _type_check(arg, ""Optional[t] requires a single type.""). --> 649 return Union[arg, type(None)]. 650 . 651 . /usr/lib/python3.5/typing.py in __getitem__(self, parameters). 550 parameters = (parameters,). 551 return self.__class__(self.__name__, self.__bases__,. --> 552 dict(self.__dict__), parameters, _root=True). 553 . 554 def __eq__(self, other):. /usr/lib/python3.5/typing.py in __new__(cls, name, bases, namespace, parameters, _root). 510 continue. 511 if any(isinstance(t2, type) and issubclass(t1, t2). --> 512 for t2 in all_params - {t1} if not isinstance(t2, TypeVar)):. 513 all_params.remove(t1). 514 # It's not a union if there's only one type left. /usr/lib/python3.5/typing.py in <genexpr>(.0). 510 continue. 511 if any(isinstance(t2, type) and issubclass(t1, t2). --> 512 for t2 in all_params - {t1} if not isinstance(t2, TypeVar)):. 513 all_params.remove(t1). 514 # It's not a union if there's only one type left. /usr/lib/python3.5/typing.py in __subclasscheck__(self, cls). 1075 return True. 1076 # If we break out of the loop, the superclass gets a chance. -> 1077 if super().__subclasscheck__(cls):. 1078 return True. 1079 if self.__extra__ is None or isinstance(cls, GenericMeta):. /usr/lib/python3.5/abc.py in __subclasscheck__(cls, subclass). 223 return True. 224 # Check if it's a subclass of a subclass (recursive). --> 225 for scls in cls.__subclasses__():. 226 if issubclass(subclass, scls):. 227 cls._abc_cache.add(subclass). TypeError: descriptor '__subclasses__' of 'type' object needs an argument. ```. I am also getting the same issue with import scanpy.api as sc. I believe this is a common issue with python 3.5.2 but unfortunately I cannot change the python version. Thank you so much! Olga",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/561
https://github.com/scverse/scanpy/issues/561:578,interoperability,API,API,578,"Cannot import scanpy in jupyter notebook with python 3.5.2; Hello,. I'm having troubles with importing scanpy in my jupyter notebook. I am working on a server which has python 3.5.2 installed, and unfortunately I cannot update it. I am getting this issue:. ```pytb. import scanpy as sc. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-111-0074c9bc0b31> in <module>(). ----> 1 import scanpy as sc. ~/.local/lib/python3.5/site-packages/scanpy/__init__.py in <module>(). 29 . 30 # the actual API. ---> 31 from . import tools as tl. 32 from . import preprocessing as pp. 33 from . import plotting as pl. ~/.local/lib/python3.5/site-packages/scanpy/tools/__init__.py in <module>(). 8 from ._rank_genes_groups import rank_genes_groups. 9 from ._dpt import dpt. ---> 10 from ._leiden import leiden. 11 from ._louvain import louvain. 12 from ._sim import sim. ~/.local/lib/python3.5/site-packages/scanpy/tools/_leiden.py in <module>(). 29 n_iterations: int = -1,. 30 partition_type: Optional[Type[MutableVertexPartition]] = None,. ---> 31 copy: bool = False,. 32 **partition_kwargs. 33 ) -> Optional[AnnData]:. /usr/lib/python3.5/typing.py in __getitem__(self, arg). 647 def __getitem__(self, arg):. 648 arg = _type_check(arg, ""Optional[t] requires a single type.""). --> 649 return Union[arg, type(None)]. 650 . 651 . /usr/lib/python3.5/typing.py in __getitem__(self, parameters). 550 parameters = (parameters,). 551 return self.__class__(self.__name__, self.__bases__,. --> 552 dict(self.__dict__), parameters, _root=True). 553 . 554 def __eq__(self, other):. /usr/lib/python3.5/typing.py in __new__(cls, name, bases, namespace, parameters, _root). 510 continue. 511 if any(isinstance(t2, type) and issubclass(t1, t2). --> 512 for t2 in all_params - {t1} if not isinstance(t2, TypeVar)):. 513 all_params.remove(t1). 514 # It's not a union if there's only one type left. /usr/lib/python3.5/typing.py in <genexpr>(.0). 5",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/561
https://github.com/scverse/scanpy/issues/561:2876,interoperability,api,api,2876,". 29 n_iterations: int = -1,. 30 partition_type: Optional[Type[MutableVertexPartition]] = None,. ---> 31 copy: bool = False,. 32 **partition_kwargs. 33 ) -> Optional[AnnData]:. /usr/lib/python3.5/typing.py in __getitem__(self, arg). 647 def __getitem__(self, arg):. 648 arg = _type_check(arg, ""Optional[t] requires a single type.""). --> 649 return Union[arg, type(None)]. 650 . 651 . /usr/lib/python3.5/typing.py in __getitem__(self, parameters). 550 parameters = (parameters,). 551 return self.__class__(self.__name__, self.__bases__,. --> 552 dict(self.__dict__), parameters, _root=True). 553 . 554 def __eq__(self, other):. /usr/lib/python3.5/typing.py in __new__(cls, name, bases, namespace, parameters, _root). 510 continue. 511 if any(isinstance(t2, type) and issubclass(t1, t2). --> 512 for t2 in all_params - {t1} if not isinstance(t2, TypeVar)):. 513 all_params.remove(t1). 514 # It's not a union if there's only one type left. /usr/lib/python3.5/typing.py in <genexpr>(.0). 510 continue. 511 if any(isinstance(t2, type) and issubclass(t1, t2). --> 512 for t2 in all_params - {t1} if not isinstance(t2, TypeVar)):. 513 all_params.remove(t1). 514 # It's not a union if there's only one type left. /usr/lib/python3.5/typing.py in __subclasscheck__(self, cls). 1075 return True. 1076 # If we break out of the loop, the superclass gets a chance. -> 1077 if super().__subclasscheck__(cls):. 1078 return True. 1079 if self.__extra__ is None or isinstance(cls, GenericMeta):. /usr/lib/python3.5/abc.py in __subclasscheck__(cls, subclass). 223 return True. 224 # Check if it's a subclass of a subclass (recursive). --> 225 for scls in cls.__subclasses__():. 226 if issubclass(subclass, scls):. 227 cls._abc_cache.add(subclass). TypeError: descriptor '__subclasses__' of 'type' object needs an argument. ```. I am also getting the same issue with import scanpy.api as sc. I believe this is a common issue with python 3.5.2 but unfortunately I cannot change the python version. Thank you so much! Olga",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/561
https://github.com/scverse/scanpy/issues/561:446,modifiability,modul,module,446,"Cannot import scanpy in jupyter notebook with python 3.5.2; Hello,. I'm having troubles with importing scanpy in my jupyter notebook. I am working on a server which has python 3.5.2 installed, and unfortunately I cannot update it. I am getting this issue:. ```pytb. import scanpy as sc. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-111-0074c9bc0b31> in <module>(). ----> 1 import scanpy as sc. ~/.local/lib/python3.5/site-packages/scanpy/__init__.py in <module>(). 29 . 30 # the actual API. ---> 31 from . import tools as tl. 32 from . import preprocessing as pp. 33 from . import plotting as pl. ~/.local/lib/python3.5/site-packages/scanpy/tools/__init__.py in <module>(). 8 from ._rank_genes_groups import rank_genes_groups. 9 from ._dpt import dpt. ---> 10 from ._leiden import leiden. 11 from ._louvain import louvain. 12 from ._sim import sim. ~/.local/lib/python3.5/site-packages/scanpy/tools/_leiden.py in <module>(). 29 n_iterations: int = -1,. 30 partition_type: Optional[Type[MutableVertexPartition]] = None,. ---> 31 copy: bool = False,. 32 **partition_kwargs. 33 ) -> Optional[AnnData]:. /usr/lib/python3.5/typing.py in __getitem__(self, arg). 647 def __getitem__(self, arg):. 648 arg = _type_check(arg, ""Optional[t] requires a single type.""). --> 649 return Union[arg, type(None)]. 650 . 651 . /usr/lib/python3.5/typing.py in __getitem__(self, parameters). 550 parameters = (parameters,). 551 return self.__class__(self.__name__, self.__bases__,. --> 552 dict(self.__dict__), parameters, _root=True). 553 . 554 def __eq__(self, other):. /usr/lib/python3.5/typing.py in __new__(cls, name, bases, namespace, parameters, _root). 510 continue. 511 if any(isinstance(t2, type) and issubclass(t1, t2). --> 512 for t2 in all_params - {t1} if not isinstance(t2, TypeVar)):. 513 all_params.remove(t1). 514 # It's not a union if there's only one type left. /usr/lib/python3.5/typing.py in <genexpr>(.0). 5",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/561
https://github.com/scverse/scanpy/issues/561:514,modifiability,pac,packages,514,"Cannot import scanpy in jupyter notebook with python 3.5.2; Hello,. I'm having troubles with importing scanpy in my jupyter notebook. I am working on a server which has python 3.5.2 installed, and unfortunately I cannot update it. I am getting this issue:. ```pytb. import scanpy as sc. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-111-0074c9bc0b31> in <module>(). ----> 1 import scanpy as sc. ~/.local/lib/python3.5/site-packages/scanpy/__init__.py in <module>(). 29 . 30 # the actual API. ---> 31 from . import tools as tl. 32 from . import preprocessing as pp. 33 from . import plotting as pl. ~/.local/lib/python3.5/site-packages/scanpy/tools/__init__.py in <module>(). 8 from ._rank_genes_groups import rank_genes_groups. 9 from ._dpt import dpt. ---> 10 from ._leiden import leiden. 11 from ._louvain import louvain. 12 from ._sim import sim. ~/.local/lib/python3.5/site-packages/scanpy/tools/_leiden.py in <module>(). 29 n_iterations: int = -1,. 30 partition_type: Optional[Type[MutableVertexPartition]] = None,. ---> 31 copy: bool = False,. 32 **partition_kwargs. 33 ) -> Optional[AnnData]:. /usr/lib/python3.5/typing.py in __getitem__(self, arg). 647 def __getitem__(self, arg):. 648 arg = _type_check(arg, ""Optional[t] requires a single type.""). --> 649 return Union[arg, type(None)]. 650 . 651 . /usr/lib/python3.5/typing.py in __getitem__(self, parameters). 550 parameters = (parameters,). 551 return self.__class__(self.__name__, self.__bases__,. --> 552 dict(self.__dict__), parameters, _root=True). 553 . 554 def __eq__(self, other):. /usr/lib/python3.5/typing.py in __new__(cls, name, bases, namespace, parameters, _root). 510 continue. 511 if any(isinstance(t2, type) and issubclass(t1, t2). --> 512 for t2 in all_params - {t1} if not isinstance(t2, TypeVar)):. 513 all_params.remove(t1). 514 # It's not a union if there's only one type left. /usr/lib/python3.5/typing.py in <genexpr>(.0). 5",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/561
https://github.com/scverse/scanpy/issues/561:546,modifiability,modul,module,546,"Cannot import scanpy in jupyter notebook with python 3.5.2; Hello,. I'm having troubles with importing scanpy in my jupyter notebook. I am working on a server which has python 3.5.2 installed, and unfortunately I cannot update it. I am getting this issue:. ```pytb. import scanpy as sc. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-111-0074c9bc0b31> in <module>(). ----> 1 import scanpy as sc. ~/.local/lib/python3.5/site-packages/scanpy/__init__.py in <module>(). 29 . 30 # the actual API. ---> 31 from . import tools as tl. 32 from . import preprocessing as pp. 33 from . import plotting as pl. ~/.local/lib/python3.5/site-packages/scanpy/tools/__init__.py in <module>(). 8 from ._rank_genes_groups import rank_genes_groups. 9 from ._dpt import dpt. ---> 10 from ._leiden import leiden. 11 from ._louvain import louvain. 12 from ._sim import sim. ~/.local/lib/python3.5/site-packages/scanpy/tools/_leiden.py in <module>(). 29 n_iterations: int = -1,. 30 partition_type: Optional[Type[MutableVertexPartition]] = None,. ---> 31 copy: bool = False,. 32 **partition_kwargs. 33 ) -> Optional[AnnData]:. /usr/lib/python3.5/typing.py in __getitem__(self, arg). 647 def __getitem__(self, arg):. 648 arg = _type_check(arg, ""Optional[t] requires a single type.""). --> 649 return Union[arg, type(None)]. 650 . 651 . /usr/lib/python3.5/typing.py in __getitem__(self, parameters). 550 parameters = (parameters,). 551 return self.__class__(self.__name__, self.__bases__,. --> 552 dict(self.__dict__), parameters, _root=True). 553 . 554 def __eq__(self, other):. /usr/lib/python3.5/typing.py in __new__(cls, name, bases, namespace, parameters, _root). 510 continue. 511 if any(isinstance(t2, type) and issubclass(t1, t2). --> 512 for t2 in all_params - {t1} if not isinstance(t2, TypeVar)):. 513 all_params.remove(t1). 514 # It's not a union if there's only one type left. /usr/lib/python3.5/typing.py in <genexpr>(.0). 5",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/561
https://github.com/scverse/scanpy/issues/561:717,modifiability,pac,packages,717,"Cannot import scanpy in jupyter notebook with python 3.5.2; Hello,. I'm having troubles with importing scanpy in my jupyter notebook. I am working on a server which has python 3.5.2 installed, and unfortunately I cannot update it. I am getting this issue:. ```pytb. import scanpy as sc. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-111-0074c9bc0b31> in <module>(). ----> 1 import scanpy as sc. ~/.local/lib/python3.5/site-packages/scanpy/__init__.py in <module>(). 29 . 30 # the actual API. ---> 31 from . import tools as tl. 32 from . import preprocessing as pp. 33 from . import plotting as pl. ~/.local/lib/python3.5/site-packages/scanpy/tools/__init__.py in <module>(). 8 from ._rank_genes_groups import rank_genes_groups. 9 from ._dpt import dpt. ---> 10 from ._leiden import leiden. 11 from ._louvain import louvain. 12 from ._sim import sim. ~/.local/lib/python3.5/site-packages/scanpy/tools/_leiden.py in <module>(). 29 n_iterations: int = -1,. 30 partition_type: Optional[Type[MutableVertexPartition]] = None,. ---> 31 copy: bool = False,. 32 **partition_kwargs. 33 ) -> Optional[AnnData]:. /usr/lib/python3.5/typing.py in __getitem__(self, arg). 647 def __getitem__(self, arg):. 648 arg = _type_check(arg, ""Optional[t] requires a single type.""). --> 649 return Union[arg, type(None)]. 650 . 651 . /usr/lib/python3.5/typing.py in __getitem__(self, parameters). 550 parameters = (parameters,). 551 return self.__class__(self.__name__, self.__bases__,. --> 552 dict(self.__dict__), parameters, _root=True). 553 . 554 def __eq__(self, other):. /usr/lib/python3.5/typing.py in __new__(cls, name, bases, namespace, parameters, _root). 510 continue. 511 if any(isinstance(t2, type) and issubclass(t1, t2). --> 512 for t2 in all_params - {t1} if not isinstance(t2, TypeVar)):. 513 all_params.remove(t1). 514 # It's not a union if there's only one type left. /usr/lib/python3.5/typing.py in <genexpr>(.0). 5",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/561
https://github.com/scverse/scanpy/issues/561:755,modifiability,modul,module,755,"Cannot import scanpy in jupyter notebook with python 3.5.2; Hello,. I'm having troubles with importing scanpy in my jupyter notebook. I am working on a server which has python 3.5.2 installed, and unfortunately I cannot update it. I am getting this issue:. ```pytb. import scanpy as sc. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-111-0074c9bc0b31> in <module>(). ----> 1 import scanpy as sc. ~/.local/lib/python3.5/site-packages/scanpy/__init__.py in <module>(). 29 . 30 # the actual API. ---> 31 from . import tools as tl. 32 from . import preprocessing as pp. 33 from . import plotting as pl. ~/.local/lib/python3.5/site-packages/scanpy/tools/__init__.py in <module>(). 8 from ._rank_genes_groups import rank_genes_groups. 9 from ._dpt import dpt. ---> 10 from ._leiden import leiden. 11 from ._louvain import louvain. 12 from ._sim import sim. ~/.local/lib/python3.5/site-packages/scanpy/tools/_leiden.py in <module>(). 29 n_iterations: int = -1,. 30 partition_type: Optional[Type[MutableVertexPartition]] = None,. ---> 31 copy: bool = False,. 32 **partition_kwargs. 33 ) -> Optional[AnnData]:. /usr/lib/python3.5/typing.py in __getitem__(self, arg). 647 def __getitem__(self, arg):. 648 arg = _type_check(arg, ""Optional[t] requires a single type.""). --> 649 return Union[arg, type(None)]. 650 . 651 . /usr/lib/python3.5/typing.py in __getitem__(self, parameters). 550 parameters = (parameters,). 551 return self.__class__(self.__name__, self.__bases__,. --> 552 dict(self.__dict__), parameters, _root=True). 553 . 554 def __eq__(self, other):. /usr/lib/python3.5/typing.py in __new__(cls, name, bases, namespace, parameters, _root). 510 continue. 511 if any(isinstance(t2, type) and issubclass(t1, t2). --> 512 for t2 in all_params - {t1} if not isinstance(t2, TypeVar)):. 513 all_params.remove(t1). 514 # It's not a union if there's only one type left. /usr/lib/python3.5/typing.py in <genexpr>(.0). 5",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/561
https://github.com/scverse/scanpy/issues/561:969,modifiability,pac,packages,969,"Cannot import scanpy in jupyter notebook with python 3.5.2; Hello,. I'm having troubles with importing scanpy in my jupyter notebook. I am working on a server which has python 3.5.2 installed, and unfortunately I cannot update it. I am getting this issue:. ```pytb. import scanpy as sc. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-111-0074c9bc0b31> in <module>(). ----> 1 import scanpy as sc. ~/.local/lib/python3.5/site-packages/scanpy/__init__.py in <module>(). 29 . 30 # the actual API. ---> 31 from . import tools as tl. 32 from . import preprocessing as pp. 33 from . import plotting as pl. ~/.local/lib/python3.5/site-packages/scanpy/tools/__init__.py in <module>(). 8 from ._rank_genes_groups import rank_genes_groups. 9 from ._dpt import dpt. ---> 10 from ._leiden import leiden. 11 from ._louvain import louvain. 12 from ._sim import sim. ~/.local/lib/python3.5/site-packages/scanpy/tools/_leiden.py in <module>(). 29 n_iterations: int = -1,. 30 partition_type: Optional[Type[MutableVertexPartition]] = None,. ---> 31 copy: bool = False,. 32 **partition_kwargs. 33 ) -> Optional[AnnData]:. /usr/lib/python3.5/typing.py in __getitem__(self, arg). 647 def __getitem__(self, arg):. 648 arg = _type_check(arg, ""Optional[t] requires a single type.""). --> 649 return Union[arg, type(None)]. 650 . 651 . /usr/lib/python3.5/typing.py in __getitem__(self, parameters). 550 parameters = (parameters,). 551 return self.__class__(self.__name__, self.__bases__,. --> 552 dict(self.__dict__), parameters, _root=True). 553 . 554 def __eq__(self, other):. /usr/lib/python3.5/typing.py in __new__(cls, name, bases, namespace, parameters, _root). 510 continue. 511 if any(isinstance(t2, type) and issubclass(t1, t2). --> 512 for t2 in all_params - {t1} if not isinstance(t2, TypeVar)):. 513 all_params.remove(t1). 514 # It's not a union if there's only one type left. /usr/lib/python3.5/typing.py in <genexpr>(.0). 5",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/561
https://github.com/scverse/scanpy/issues/561:1006,modifiability,modul,module,1006,"port scanpy in jupyter notebook with python 3.5.2; Hello,. I'm having troubles with importing scanpy in my jupyter notebook. I am working on a server which has python 3.5.2 installed, and unfortunately I cannot update it. I am getting this issue:. ```pytb. import scanpy as sc. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-111-0074c9bc0b31> in <module>(). ----> 1 import scanpy as sc. ~/.local/lib/python3.5/site-packages/scanpy/__init__.py in <module>(). 29 . 30 # the actual API. ---> 31 from . import tools as tl. 32 from . import preprocessing as pp. 33 from . import plotting as pl. ~/.local/lib/python3.5/site-packages/scanpy/tools/__init__.py in <module>(). 8 from ._rank_genes_groups import rank_genes_groups. 9 from ._dpt import dpt. ---> 10 from ._leiden import leiden. 11 from ._louvain import louvain. 12 from ._sim import sim. ~/.local/lib/python3.5/site-packages/scanpy/tools/_leiden.py in <module>(). 29 n_iterations: int = -1,. 30 partition_type: Optional[Type[MutableVertexPartition]] = None,. ---> 31 copy: bool = False,. 32 **partition_kwargs. 33 ) -> Optional[AnnData]:. /usr/lib/python3.5/typing.py in __getitem__(self, arg). 647 def __getitem__(self, arg):. 648 arg = _type_check(arg, ""Optional[t] requires a single type.""). --> 649 return Union[arg, type(None)]. 650 . 651 . /usr/lib/python3.5/typing.py in __getitem__(self, parameters). 550 parameters = (parameters,). 551 return self.__class__(self.__name__, self.__bases__,. --> 552 dict(self.__dict__), parameters, _root=True). 553 . 554 def __eq__(self, other):. /usr/lib/python3.5/typing.py in __new__(cls, name, bases, namespace, parameters, _root). 510 continue. 511 if any(isinstance(t2, type) and issubclass(t1, t2). --> 512 for t2 in all_params - {t1} if not isinstance(t2, TypeVar)):. 513 all_params.remove(t1). 514 # It's not a union if there's only one type left. /usr/lib/python3.5/typing.py in <genexpr>(.0). 510 contin",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/561
https://github.com/scverse/scanpy/issues/561:1449,modifiability,paramet,parameters,1449,"). ----> 1 import scanpy as sc. ~/.local/lib/python3.5/site-packages/scanpy/__init__.py in <module>(). 29 . 30 # the actual API. ---> 31 from . import tools as tl. 32 from . import preprocessing as pp. 33 from . import plotting as pl. ~/.local/lib/python3.5/site-packages/scanpy/tools/__init__.py in <module>(). 8 from ._rank_genes_groups import rank_genes_groups. 9 from ._dpt import dpt. ---> 10 from ._leiden import leiden. 11 from ._louvain import louvain. 12 from ._sim import sim. ~/.local/lib/python3.5/site-packages/scanpy/tools/_leiden.py in <module>(). 29 n_iterations: int = -1,. 30 partition_type: Optional[Type[MutableVertexPartition]] = None,. ---> 31 copy: bool = False,. 32 **partition_kwargs. 33 ) -> Optional[AnnData]:. /usr/lib/python3.5/typing.py in __getitem__(self, arg). 647 def __getitem__(self, arg):. 648 arg = _type_check(arg, ""Optional[t] requires a single type.""). --> 649 return Union[arg, type(None)]. 650 . 651 . /usr/lib/python3.5/typing.py in __getitem__(self, parameters). 550 parameters = (parameters,). 551 return self.__class__(self.__name__, self.__bases__,. --> 552 dict(self.__dict__), parameters, _root=True). 553 . 554 def __eq__(self, other):. /usr/lib/python3.5/typing.py in __new__(cls, name, bases, namespace, parameters, _root). 510 continue. 511 if any(isinstance(t2, type) and issubclass(t1, t2). --> 512 for t2 in all_params - {t1} if not isinstance(t2, TypeVar)):. 513 all_params.remove(t1). 514 # It's not a union if there's only one type left. /usr/lib/python3.5/typing.py in <genexpr>(.0). 510 continue. 511 if any(isinstance(t2, type) and issubclass(t1, t2). --> 512 for t2 in all_params - {t1} if not isinstance(t2, TypeVar)):. 513 all_params.remove(t1). 514 # It's not a union if there's only one type left. /usr/lib/python3.5/typing.py in __subclasscheck__(self, cls). 1075 return True. 1076 # If we break out of the loop, the superclass gets a chance. -> 1077 if super().__subclasscheck__(cls):. 1078 return True. 1079 if self.__extra__ is ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/561
https://github.com/scverse/scanpy/issues/561:1466,modifiability,paramet,parameters,1466," scanpy as sc. ~/.local/lib/python3.5/site-packages/scanpy/__init__.py in <module>(). 29 . 30 # the actual API. ---> 31 from . import tools as tl. 32 from . import preprocessing as pp. 33 from . import plotting as pl. ~/.local/lib/python3.5/site-packages/scanpy/tools/__init__.py in <module>(). 8 from ._rank_genes_groups import rank_genes_groups. 9 from ._dpt import dpt. ---> 10 from ._leiden import leiden. 11 from ._louvain import louvain. 12 from ._sim import sim. ~/.local/lib/python3.5/site-packages/scanpy/tools/_leiden.py in <module>(). 29 n_iterations: int = -1,. 30 partition_type: Optional[Type[MutableVertexPartition]] = None,. ---> 31 copy: bool = False,. 32 **partition_kwargs. 33 ) -> Optional[AnnData]:. /usr/lib/python3.5/typing.py in __getitem__(self, arg). 647 def __getitem__(self, arg):. 648 arg = _type_check(arg, ""Optional[t] requires a single type.""). --> 649 return Union[arg, type(None)]. 650 . 651 . /usr/lib/python3.5/typing.py in __getitem__(self, parameters). 550 parameters = (parameters,). 551 return self.__class__(self.__name__, self.__bases__,. --> 552 dict(self.__dict__), parameters, _root=True). 553 . 554 def __eq__(self, other):. /usr/lib/python3.5/typing.py in __new__(cls, name, bases, namespace, parameters, _root). 510 continue. 511 if any(isinstance(t2, type) and issubclass(t1, t2). --> 512 for t2 in all_params - {t1} if not isinstance(t2, TypeVar)):. 513 all_params.remove(t1). 514 # It's not a union if there's only one type left. /usr/lib/python3.5/typing.py in <genexpr>(.0). 510 continue. 511 if any(isinstance(t2, type) and issubclass(t1, t2). --> 512 for t2 in all_params - {t1} if not isinstance(t2, TypeVar)):. 513 all_params.remove(t1). 514 # It's not a union if there's only one type left. /usr/lib/python3.5/typing.py in __subclasscheck__(self, cls). 1075 return True. 1076 # If we break out of the loop, the superclass gets a chance. -> 1077 if super().__subclasscheck__(cls):. 1078 return True. 1079 if self.__extra__ is None or isinstanc",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/561
https://github.com/scverse/scanpy/issues/561:1480,modifiability,paramet,parameters,1480," ~/.local/lib/python3.5/site-packages/scanpy/__init__.py in <module>(). 29 . 30 # the actual API. ---> 31 from . import tools as tl. 32 from . import preprocessing as pp. 33 from . import plotting as pl. ~/.local/lib/python3.5/site-packages/scanpy/tools/__init__.py in <module>(). 8 from ._rank_genes_groups import rank_genes_groups. 9 from ._dpt import dpt. ---> 10 from ._leiden import leiden. 11 from ._louvain import louvain. 12 from ._sim import sim. ~/.local/lib/python3.5/site-packages/scanpy/tools/_leiden.py in <module>(). 29 n_iterations: int = -1,. 30 partition_type: Optional[Type[MutableVertexPartition]] = None,. ---> 31 copy: bool = False,. 32 **partition_kwargs. 33 ) -> Optional[AnnData]:. /usr/lib/python3.5/typing.py in __getitem__(self, arg). 647 def __getitem__(self, arg):. 648 arg = _type_check(arg, ""Optional[t] requires a single type.""). --> 649 return Union[arg, type(None)]. 650 . 651 . /usr/lib/python3.5/typing.py in __getitem__(self, parameters). 550 parameters = (parameters,). 551 return self.__class__(self.__name__, self.__bases__,. --> 552 dict(self.__dict__), parameters, _root=True). 553 . 554 def __eq__(self, other):. /usr/lib/python3.5/typing.py in __new__(cls, name, bases, namespace, parameters, _root). 510 continue. 511 if any(isinstance(t2, type) and issubclass(t1, t2). --> 512 for t2 in all_params - {t1} if not isinstance(t2, TypeVar)):. 513 all_params.remove(t1). 514 # It's not a union if there's only one type left. /usr/lib/python3.5/typing.py in <genexpr>(.0). 510 continue. 511 if any(isinstance(t2, type) and issubclass(t1, t2). --> 512 for t2 in all_params - {t1} if not isinstance(t2, TypeVar)):. 513 all_params.remove(t1). 514 # It's not a union if there's only one type left. /usr/lib/python3.5/typing.py in __subclasscheck__(self, cls). 1075 return True. 1076 # If we break out of the loop, the superclass gets a chance. -> 1077 if super().__subclasscheck__(cls):. 1078 return True. 1079 if self.__extra__ is None or isinstance(cls, Generic",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/561
https://github.com/scverse/scanpy/issues/561:1581,modifiability,paramet,parameters,1581,"> 31 from . import tools as tl. 32 from . import preprocessing as pp. 33 from . import plotting as pl. ~/.local/lib/python3.5/site-packages/scanpy/tools/__init__.py in <module>(). 8 from ._rank_genes_groups import rank_genes_groups. 9 from ._dpt import dpt. ---> 10 from ._leiden import leiden. 11 from ._louvain import louvain. 12 from ._sim import sim. ~/.local/lib/python3.5/site-packages/scanpy/tools/_leiden.py in <module>(). 29 n_iterations: int = -1,. 30 partition_type: Optional[Type[MutableVertexPartition]] = None,. ---> 31 copy: bool = False,. 32 **partition_kwargs. 33 ) -> Optional[AnnData]:. /usr/lib/python3.5/typing.py in __getitem__(self, arg). 647 def __getitem__(self, arg):. 648 arg = _type_check(arg, ""Optional[t] requires a single type.""). --> 649 return Union[arg, type(None)]. 650 . 651 . /usr/lib/python3.5/typing.py in __getitem__(self, parameters). 550 parameters = (parameters,). 551 return self.__class__(self.__name__, self.__bases__,. --> 552 dict(self.__dict__), parameters, _root=True). 553 . 554 def __eq__(self, other):. /usr/lib/python3.5/typing.py in __new__(cls, name, bases, namespace, parameters, _root). 510 continue. 511 if any(isinstance(t2, type) and issubclass(t1, t2). --> 512 for t2 in all_params - {t1} if not isinstance(t2, TypeVar)):. 513 all_params.remove(t1). 514 # It's not a union if there's only one type left. /usr/lib/python3.5/typing.py in <genexpr>(.0). 510 continue. 511 if any(isinstance(t2, type) and issubclass(t1, t2). --> 512 for t2 in all_params - {t1} if not isinstance(t2, TypeVar)):. 513 all_params.remove(t1). 514 # It's not a union if there's only one type left. /usr/lib/python3.5/typing.py in __subclasscheck__(self, cls). 1075 return True. 1076 # If we break out of the loop, the superclass gets a chance. -> 1077 if super().__subclasscheck__(cls):. 1078 return True. 1079 if self.__extra__ is None or isinstance(cls, GenericMeta):. /usr/lib/python3.5/abc.py in __subclasscheck__(cls, subclass). 223 return True. 224 # Check i",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/561
https://github.com/scverse/scanpy/issues/561:1711,modifiability,paramet,parameters,1711,"-packages/scanpy/tools/__init__.py in <module>(). 8 from ._rank_genes_groups import rank_genes_groups. 9 from ._dpt import dpt. ---> 10 from ._leiden import leiden. 11 from ._louvain import louvain. 12 from ._sim import sim. ~/.local/lib/python3.5/site-packages/scanpy/tools/_leiden.py in <module>(). 29 n_iterations: int = -1,. 30 partition_type: Optional[Type[MutableVertexPartition]] = None,. ---> 31 copy: bool = False,. 32 **partition_kwargs. 33 ) -> Optional[AnnData]:. /usr/lib/python3.5/typing.py in __getitem__(self, arg). 647 def __getitem__(self, arg):. 648 arg = _type_check(arg, ""Optional[t] requires a single type.""). --> 649 return Union[arg, type(None)]. 650 . 651 . /usr/lib/python3.5/typing.py in __getitem__(self, parameters). 550 parameters = (parameters,). 551 return self.__class__(self.__name__, self.__bases__,. --> 552 dict(self.__dict__), parameters, _root=True). 553 . 554 def __eq__(self, other):. /usr/lib/python3.5/typing.py in __new__(cls, name, bases, namespace, parameters, _root). 510 continue. 511 if any(isinstance(t2, type) and issubclass(t1, t2). --> 512 for t2 in all_params - {t1} if not isinstance(t2, TypeVar)):. 513 all_params.remove(t1). 514 # It's not a union if there's only one type left. /usr/lib/python3.5/typing.py in <genexpr>(.0). 510 continue. 511 if any(isinstance(t2, type) and issubclass(t1, t2). --> 512 for t2 in all_params - {t1} if not isinstance(t2, TypeVar)):. 513 all_params.remove(t1). 514 # It's not a union if there's only one type left. /usr/lib/python3.5/typing.py in __subclasscheck__(self, cls). 1075 return True. 1076 # If we break out of the loop, the superclass gets a chance. -> 1077 if super().__subclasscheck__(cls):. 1078 return True. 1079 if self.__extra__ is None or isinstance(cls, GenericMeta):. /usr/lib/python3.5/abc.py in __subclasscheck__(cls, subclass). 223 return True. 224 # Check if it's a subclass of a subclass (recursive). --> 225 for scls in cls.__subclasses__():. 226 if issubclass(subclass, scls):. 227 cl",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/561
https://github.com/scverse/scanpy/issues/561:2983,modifiability,version,version,2983,". 29 n_iterations: int = -1,. 30 partition_type: Optional[Type[MutableVertexPartition]] = None,. ---> 31 copy: bool = False,. 32 **partition_kwargs. 33 ) -> Optional[AnnData]:. /usr/lib/python3.5/typing.py in __getitem__(self, arg). 647 def __getitem__(self, arg):. 648 arg = _type_check(arg, ""Optional[t] requires a single type.""). --> 649 return Union[arg, type(None)]. 650 . 651 . /usr/lib/python3.5/typing.py in __getitem__(self, parameters). 550 parameters = (parameters,). 551 return self.__class__(self.__name__, self.__bases__,. --> 552 dict(self.__dict__), parameters, _root=True). 553 . 554 def __eq__(self, other):. /usr/lib/python3.5/typing.py in __new__(cls, name, bases, namespace, parameters, _root). 510 continue. 511 if any(isinstance(t2, type) and issubclass(t1, t2). --> 512 for t2 in all_params - {t1} if not isinstance(t2, TypeVar)):. 513 all_params.remove(t1). 514 # It's not a union if there's only one type left. /usr/lib/python3.5/typing.py in <genexpr>(.0). 510 continue. 511 if any(isinstance(t2, type) and issubclass(t1, t2). --> 512 for t2 in all_params - {t1} if not isinstance(t2, TypeVar)):. 513 all_params.remove(t1). 514 # It's not a union if there's only one type left. /usr/lib/python3.5/typing.py in __subclasscheck__(self, cls). 1075 return True. 1076 # If we break out of the loop, the superclass gets a chance. -> 1077 if super().__subclasscheck__(cls):. 1078 return True. 1079 if self.__extra__ is None or isinstance(cls, GenericMeta):. /usr/lib/python3.5/abc.py in __subclasscheck__(cls, subclass). 223 return True. 224 # Check if it's a subclass of a subclass (recursive). --> 225 for scls in cls.__subclasses__():. 226 if issubclass(subclass, scls):. 227 cls._abc_cache.add(subclass). TypeError: descriptor '__subclasses__' of 'type' object needs an argument. ```. I am also getting the same issue with import scanpy.api as sc. I believe this is a common issue with python 3.5.2 but unfortunately I cannot change the python version. Thank you so much! Olga",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/561
https://github.com/scverse/scanpy/issues/561:220,safety,updat,update,220,"Cannot import scanpy in jupyter notebook with python 3.5.2; Hello,. I'm having troubles with importing scanpy in my jupyter notebook. I am working on a server which has python 3.5.2 installed, and unfortunately I cannot update it. I am getting this issue:. ```pytb. import scanpy as sc. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-111-0074c9bc0b31> in <module>(). ----> 1 import scanpy as sc. ~/.local/lib/python3.5/site-packages/scanpy/__init__.py in <module>(). 29 . 30 # the actual API. ---> 31 from . import tools as tl. 32 from . import preprocessing as pp. 33 from . import plotting as pl. ~/.local/lib/python3.5/site-packages/scanpy/tools/__init__.py in <module>(). 8 from ._rank_genes_groups import rank_genes_groups. 9 from ._dpt import dpt. ---> 10 from ._leiden import leiden. 11 from ._louvain import louvain. 12 from ._sim import sim. ~/.local/lib/python3.5/site-packages/scanpy/tools/_leiden.py in <module>(). 29 n_iterations: int = -1,. 30 partition_type: Optional[Type[MutableVertexPartition]] = None,. ---> 31 copy: bool = False,. 32 **partition_kwargs. 33 ) -> Optional[AnnData]:. /usr/lib/python3.5/typing.py in __getitem__(self, arg). 647 def __getitem__(self, arg):. 648 arg = _type_check(arg, ""Optional[t] requires a single type.""). --> 649 return Union[arg, type(None)]. 650 . 651 . /usr/lib/python3.5/typing.py in __getitem__(self, parameters). 550 parameters = (parameters,). 551 return self.__class__(self.__name__, self.__bases__,. --> 552 dict(self.__dict__), parameters, _root=True). 553 . 554 def __eq__(self, other):. /usr/lib/python3.5/typing.py in __new__(cls, name, bases, namespace, parameters, _root). 510 continue. 511 if any(isinstance(t2, type) and issubclass(t1, t2). --> 512 for t2 in all_params - {t1} if not isinstance(t2, TypeVar)):. 513 all_params.remove(t1). 514 # It's not a union if there's only one type left. /usr/lib/python3.5/typing.py in <genexpr>(.0). 5",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/561
https://github.com/scverse/scanpy/issues/561:418,safety,input,input-,418,"Cannot import scanpy in jupyter notebook with python 3.5.2; Hello,. I'm having troubles with importing scanpy in my jupyter notebook. I am working on a server which has python 3.5.2 installed, and unfortunately I cannot update it. I am getting this issue:. ```pytb. import scanpy as sc. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-111-0074c9bc0b31> in <module>(). ----> 1 import scanpy as sc. ~/.local/lib/python3.5/site-packages/scanpy/__init__.py in <module>(). 29 . 30 # the actual API. ---> 31 from . import tools as tl. 32 from . import preprocessing as pp. 33 from . import plotting as pl. ~/.local/lib/python3.5/site-packages/scanpy/tools/__init__.py in <module>(). 8 from ._rank_genes_groups import rank_genes_groups. 9 from ._dpt import dpt. ---> 10 from ._leiden import leiden. 11 from ._louvain import louvain. 12 from ._sim import sim. ~/.local/lib/python3.5/site-packages/scanpy/tools/_leiden.py in <module>(). 29 n_iterations: int = -1,. 30 partition_type: Optional[Type[MutableVertexPartition]] = None,. ---> 31 copy: bool = False,. 32 **partition_kwargs. 33 ) -> Optional[AnnData]:. /usr/lib/python3.5/typing.py in __getitem__(self, arg). 647 def __getitem__(self, arg):. 648 arg = _type_check(arg, ""Optional[t] requires a single type.""). --> 649 return Union[arg, type(None)]. 650 . 651 . /usr/lib/python3.5/typing.py in __getitem__(self, parameters). 550 parameters = (parameters,). 551 return self.__class__(self.__name__, self.__bases__,. --> 552 dict(self.__dict__), parameters, _root=True). 553 . 554 def __eq__(self, other):. /usr/lib/python3.5/typing.py in __new__(cls, name, bases, namespace, parameters, _root). 510 continue. 511 if any(isinstance(t2, type) and issubclass(t1, t2). --> 512 for t2 in all_params - {t1} if not isinstance(t2, TypeVar)):. 513 all_params.remove(t1). 514 # It's not a union if there's only one type left. /usr/lib/python3.5/typing.py in <genexpr>(.0). 5",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/561
https://github.com/scverse/scanpy/issues/561:446,safety,modul,module,446,"Cannot import scanpy in jupyter notebook with python 3.5.2; Hello,. I'm having troubles with importing scanpy in my jupyter notebook. I am working on a server which has python 3.5.2 installed, and unfortunately I cannot update it. I am getting this issue:. ```pytb. import scanpy as sc. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-111-0074c9bc0b31> in <module>(). ----> 1 import scanpy as sc. ~/.local/lib/python3.5/site-packages/scanpy/__init__.py in <module>(). 29 . 30 # the actual API. ---> 31 from . import tools as tl. 32 from . import preprocessing as pp. 33 from . import plotting as pl. ~/.local/lib/python3.5/site-packages/scanpy/tools/__init__.py in <module>(). 8 from ._rank_genes_groups import rank_genes_groups. 9 from ._dpt import dpt. ---> 10 from ._leiden import leiden. 11 from ._louvain import louvain. 12 from ._sim import sim. ~/.local/lib/python3.5/site-packages/scanpy/tools/_leiden.py in <module>(). 29 n_iterations: int = -1,. 30 partition_type: Optional[Type[MutableVertexPartition]] = None,. ---> 31 copy: bool = False,. 32 **partition_kwargs. 33 ) -> Optional[AnnData]:. /usr/lib/python3.5/typing.py in __getitem__(self, arg). 647 def __getitem__(self, arg):. 648 arg = _type_check(arg, ""Optional[t] requires a single type.""). --> 649 return Union[arg, type(None)]. 650 . 651 . /usr/lib/python3.5/typing.py in __getitem__(self, parameters). 550 parameters = (parameters,). 551 return self.__class__(self.__name__, self.__bases__,. --> 552 dict(self.__dict__), parameters, _root=True). 553 . 554 def __eq__(self, other):. /usr/lib/python3.5/typing.py in __new__(cls, name, bases, namespace, parameters, _root). 510 continue. 511 if any(isinstance(t2, type) and issubclass(t1, t2). --> 512 for t2 in all_params - {t1} if not isinstance(t2, TypeVar)):. 513 all_params.remove(t1). 514 # It's not a union if there's only one type left. /usr/lib/python3.5/typing.py in <genexpr>(.0). 5",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/561
https://github.com/scverse/scanpy/issues/561:546,safety,modul,module,546,"Cannot import scanpy in jupyter notebook with python 3.5.2; Hello,. I'm having troubles with importing scanpy in my jupyter notebook. I am working on a server which has python 3.5.2 installed, and unfortunately I cannot update it. I am getting this issue:. ```pytb. import scanpy as sc. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-111-0074c9bc0b31> in <module>(). ----> 1 import scanpy as sc. ~/.local/lib/python3.5/site-packages/scanpy/__init__.py in <module>(). 29 . 30 # the actual API. ---> 31 from . import tools as tl. 32 from . import preprocessing as pp. 33 from . import plotting as pl. ~/.local/lib/python3.5/site-packages/scanpy/tools/__init__.py in <module>(). 8 from ._rank_genes_groups import rank_genes_groups. 9 from ._dpt import dpt. ---> 10 from ._leiden import leiden. 11 from ._louvain import louvain. 12 from ._sim import sim. ~/.local/lib/python3.5/site-packages/scanpy/tools/_leiden.py in <module>(). 29 n_iterations: int = -1,. 30 partition_type: Optional[Type[MutableVertexPartition]] = None,. ---> 31 copy: bool = False,. 32 **partition_kwargs. 33 ) -> Optional[AnnData]:. /usr/lib/python3.5/typing.py in __getitem__(self, arg). 647 def __getitem__(self, arg):. 648 arg = _type_check(arg, ""Optional[t] requires a single type.""). --> 649 return Union[arg, type(None)]. 650 . 651 . /usr/lib/python3.5/typing.py in __getitem__(self, parameters). 550 parameters = (parameters,). 551 return self.__class__(self.__name__, self.__bases__,. --> 552 dict(self.__dict__), parameters, _root=True). 553 . 554 def __eq__(self, other):. /usr/lib/python3.5/typing.py in __new__(cls, name, bases, namespace, parameters, _root). 510 continue. 511 if any(isinstance(t2, type) and issubclass(t1, t2). --> 512 for t2 in all_params - {t1} if not isinstance(t2, TypeVar)):. 513 all_params.remove(t1). 514 # It's not a union if there's only one type left. /usr/lib/python3.5/typing.py in <genexpr>(.0). 5",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/561
https://github.com/scverse/scanpy/issues/561:755,safety,modul,module,755,"Cannot import scanpy in jupyter notebook with python 3.5.2; Hello,. I'm having troubles with importing scanpy in my jupyter notebook. I am working on a server which has python 3.5.2 installed, and unfortunately I cannot update it. I am getting this issue:. ```pytb. import scanpy as sc. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-111-0074c9bc0b31> in <module>(). ----> 1 import scanpy as sc. ~/.local/lib/python3.5/site-packages/scanpy/__init__.py in <module>(). 29 . 30 # the actual API. ---> 31 from . import tools as tl. 32 from . import preprocessing as pp. 33 from . import plotting as pl. ~/.local/lib/python3.5/site-packages/scanpy/tools/__init__.py in <module>(). 8 from ._rank_genes_groups import rank_genes_groups. 9 from ._dpt import dpt. ---> 10 from ._leiden import leiden. 11 from ._louvain import louvain. 12 from ._sim import sim. ~/.local/lib/python3.5/site-packages/scanpy/tools/_leiden.py in <module>(). 29 n_iterations: int = -1,. 30 partition_type: Optional[Type[MutableVertexPartition]] = None,. ---> 31 copy: bool = False,. 32 **partition_kwargs. 33 ) -> Optional[AnnData]:. /usr/lib/python3.5/typing.py in __getitem__(self, arg). 647 def __getitem__(self, arg):. 648 arg = _type_check(arg, ""Optional[t] requires a single type.""). --> 649 return Union[arg, type(None)]. 650 . 651 . /usr/lib/python3.5/typing.py in __getitem__(self, parameters). 550 parameters = (parameters,). 551 return self.__class__(self.__name__, self.__bases__,. --> 552 dict(self.__dict__), parameters, _root=True). 553 . 554 def __eq__(self, other):. /usr/lib/python3.5/typing.py in __new__(cls, name, bases, namespace, parameters, _root). 510 continue. 511 if any(isinstance(t2, type) and issubclass(t1, t2). --> 512 for t2 in all_params - {t1} if not isinstance(t2, TypeVar)):. 513 all_params.remove(t1). 514 # It's not a union if there's only one type left. /usr/lib/python3.5/typing.py in <genexpr>(.0). 5",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/561
https://github.com/scverse/scanpy/issues/561:1006,safety,modul,module,1006,"port scanpy in jupyter notebook with python 3.5.2; Hello,. I'm having troubles with importing scanpy in my jupyter notebook. I am working on a server which has python 3.5.2 installed, and unfortunately I cannot update it. I am getting this issue:. ```pytb. import scanpy as sc. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-111-0074c9bc0b31> in <module>(). ----> 1 import scanpy as sc. ~/.local/lib/python3.5/site-packages/scanpy/__init__.py in <module>(). 29 . 30 # the actual API. ---> 31 from . import tools as tl. 32 from . import preprocessing as pp. 33 from . import plotting as pl. ~/.local/lib/python3.5/site-packages/scanpy/tools/__init__.py in <module>(). 8 from ._rank_genes_groups import rank_genes_groups. 9 from ._dpt import dpt. ---> 10 from ._leiden import leiden. 11 from ._louvain import louvain. 12 from ._sim import sim. ~/.local/lib/python3.5/site-packages/scanpy/tools/_leiden.py in <module>(). 29 n_iterations: int = -1,. 30 partition_type: Optional[Type[MutableVertexPartition]] = None,. ---> 31 copy: bool = False,. 32 **partition_kwargs. 33 ) -> Optional[AnnData]:. /usr/lib/python3.5/typing.py in __getitem__(self, arg). 647 def __getitem__(self, arg):. 648 arg = _type_check(arg, ""Optional[t] requires a single type.""). --> 649 return Union[arg, type(None)]. 650 . 651 . /usr/lib/python3.5/typing.py in __getitem__(self, parameters). 550 parameters = (parameters,). 551 return self.__class__(self.__name__, self.__bases__,. --> 552 dict(self.__dict__), parameters, _root=True). 553 . 554 def __eq__(self, other):. /usr/lib/python3.5/typing.py in __new__(cls, name, bases, namespace, parameters, _root). 510 continue. 511 if any(isinstance(t2, type) and issubclass(t1, t2). --> 512 for t2 in all_params - {t1} if not isinstance(t2, TypeVar)):. 513 all_params.remove(t1). 514 # It's not a union if there's only one type left. /usr/lib/python3.5/typing.py in <genexpr>(.0). 510 contin",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/561
https://github.com/scverse/scanpy/issues/561:220,security,updat,update,220,"Cannot import scanpy in jupyter notebook with python 3.5.2; Hello,. I'm having troubles with importing scanpy in my jupyter notebook. I am working on a server which has python 3.5.2 installed, and unfortunately I cannot update it. I am getting this issue:. ```pytb. import scanpy as sc. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-111-0074c9bc0b31> in <module>(). ----> 1 import scanpy as sc. ~/.local/lib/python3.5/site-packages/scanpy/__init__.py in <module>(). 29 . 30 # the actual API. ---> 31 from . import tools as tl. 32 from . import preprocessing as pp. 33 from . import plotting as pl. ~/.local/lib/python3.5/site-packages/scanpy/tools/__init__.py in <module>(). 8 from ._rank_genes_groups import rank_genes_groups. 9 from ._dpt import dpt. ---> 10 from ._leiden import leiden. 11 from ._louvain import louvain. 12 from ._sim import sim. ~/.local/lib/python3.5/site-packages/scanpy/tools/_leiden.py in <module>(). 29 n_iterations: int = -1,. 30 partition_type: Optional[Type[MutableVertexPartition]] = None,. ---> 31 copy: bool = False,. 32 **partition_kwargs. 33 ) -> Optional[AnnData]:. /usr/lib/python3.5/typing.py in __getitem__(self, arg). 647 def __getitem__(self, arg):. 648 arg = _type_check(arg, ""Optional[t] requires a single type.""). --> 649 return Union[arg, type(None)]. 650 . 651 . /usr/lib/python3.5/typing.py in __getitem__(self, parameters). 550 parameters = (parameters,). 551 return self.__class__(self.__name__, self.__bases__,. --> 552 dict(self.__dict__), parameters, _root=True). 553 . 554 def __eq__(self, other):. /usr/lib/python3.5/typing.py in __new__(cls, name, bases, namespace, parameters, _root). 510 continue. 511 if any(isinstance(t2, type) and issubclass(t1, t2). --> 512 for t2 in all_params - {t1} if not isinstance(t2, TypeVar)):. 513 all_params.remove(t1). 514 # It's not a union if there's only one type left. /usr/lib/python3.5/typing.py in <genexpr>(.0). 5",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/561
https://github.com/scverse/scanpy/issues/561:374,testability,Trace,Traceback,374,"Cannot import scanpy in jupyter notebook with python 3.5.2; Hello,. I'm having troubles with importing scanpy in my jupyter notebook. I am working on a server which has python 3.5.2 installed, and unfortunately I cannot update it. I am getting this issue:. ```pytb. import scanpy as sc. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-111-0074c9bc0b31> in <module>(). ----> 1 import scanpy as sc. ~/.local/lib/python3.5/site-packages/scanpy/__init__.py in <module>(). 29 . 30 # the actual API. ---> 31 from . import tools as tl. 32 from . import preprocessing as pp. 33 from . import plotting as pl. ~/.local/lib/python3.5/site-packages/scanpy/tools/__init__.py in <module>(). 8 from ._rank_genes_groups import rank_genes_groups. 9 from ._dpt import dpt. ---> 10 from ._leiden import leiden. 11 from ._louvain import louvain. 12 from ._sim import sim. ~/.local/lib/python3.5/site-packages/scanpy/tools/_leiden.py in <module>(). 29 n_iterations: int = -1,. 30 partition_type: Optional[Type[MutableVertexPartition]] = None,. ---> 31 copy: bool = False,. 32 **partition_kwargs. 33 ) -> Optional[AnnData]:. /usr/lib/python3.5/typing.py in __getitem__(self, arg). 647 def __getitem__(self, arg):. 648 arg = _type_check(arg, ""Optional[t] requires a single type.""). --> 649 return Union[arg, type(None)]. 650 . 651 . /usr/lib/python3.5/typing.py in __getitem__(self, parameters). 550 parameters = (parameters,). 551 return self.__class__(self.__name__, self.__bases__,. --> 552 dict(self.__dict__), parameters, _root=True). 553 . 554 def __eq__(self, other):. /usr/lib/python3.5/typing.py in __new__(cls, name, bases, namespace, parameters, _root). 510 continue. 511 if any(isinstance(t2, type) and issubclass(t1, t2). --> 512 for t2 in all_params - {t1} if not isinstance(t2, TypeVar)):. 513 all_params.remove(t1). 514 # It's not a union if there's only one type left. /usr/lib/python3.5/typing.py in <genexpr>(.0). 5",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/561
https://github.com/scverse/scanpy/issues/561:418,usability,input,input-,418,"Cannot import scanpy in jupyter notebook with python 3.5.2; Hello,. I'm having troubles with importing scanpy in my jupyter notebook. I am working on a server which has python 3.5.2 installed, and unfortunately I cannot update it. I am getting this issue:. ```pytb. import scanpy as sc. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-111-0074c9bc0b31> in <module>(). ----> 1 import scanpy as sc. ~/.local/lib/python3.5/site-packages/scanpy/__init__.py in <module>(). 29 . 30 # the actual API. ---> 31 from . import tools as tl. 32 from . import preprocessing as pp. 33 from . import plotting as pl. ~/.local/lib/python3.5/site-packages/scanpy/tools/__init__.py in <module>(). 8 from ._rank_genes_groups import rank_genes_groups. 9 from ._dpt import dpt. ---> 10 from ._leiden import leiden. 11 from ._louvain import louvain. 12 from ._sim import sim. ~/.local/lib/python3.5/site-packages/scanpy/tools/_leiden.py in <module>(). 29 n_iterations: int = -1,. 30 partition_type: Optional[Type[MutableVertexPartition]] = None,. ---> 31 copy: bool = False,. 32 **partition_kwargs. 33 ) -> Optional[AnnData]:. /usr/lib/python3.5/typing.py in __getitem__(self, arg). 647 def __getitem__(self, arg):. 648 arg = _type_check(arg, ""Optional[t] requires a single type.""). --> 649 return Union[arg, type(None)]. 650 . 651 . /usr/lib/python3.5/typing.py in __getitem__(self, parameters). 550 parameters = (parameters,). 551 return self.__class__(self.__name__, self.__bases__,. --> 552 dict(self.__dict__), parameters, _root=True). 553 . 554 def __eq__(self, other):. /usr/lib/python3.5/typing.py in __new__(cls, name, bases, namespace, parameters, _root). 510 continue. 511 if any(isinstance(t2, type) and issubclass(t1, t2). --> 512 for t2 in all_params - {t1} if not isinstance(t2, TypeVar)):. 513 all_params.remove(t1). 514 # It's not a union if there's only one type left. /usr/lib/python3.5/typing.py in <genexpr>(.0). 5",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/561
https://github.com/scverse/scanpy/issues/561:605,usability,tool,tools,605,"Cannot import scanpy in jupyter notebook with python 3.5.2; Hello,. I'm having troubles with importing scanpy in my jupyter notebook. I am working on a server which has python 3.5.2 installed, and unfortunately I cannot update it. I am getting this issue:. ```pytb. import scanpy as sc. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-111-0074c9bc0b31> in <module>(). ----> 1 import scanpy as sc. ~/.local/lib/python3.5/site-packages/scanpy/__init__.py in <module>(). 29 . 30 # the actual API. ---> 31 from . import tools as tl. 32 from . import preprocessing as pp. 33 from . import plotting as pl. ~/.local/lib/python3.5/site-packages/scanpy/tools/__init__.py in <module>(). 8 from ._rank_genes_groups import rank_genes_groups. 9 from ._dpt import dpt. ---> 10 from ._leiden import leiden. 11 from ._louvain import louvain. 12 from ._sim import sim. ~/.local/lib/python3.5/site-packages/scanpy/tools/_leiden.py in <module>(). 29 n_iterations: int = -1,. 30 partition_type: Optional[Type[MutableVertexPartition]] = None,. ---> 31 copy: bool = False,. 32 **partition_kwargs. 33 ) -> Optional[AnnData]:. /usr/lib/python3.5/typing.py in __getitem__(self, arg). 647 def __getitem__(self, arg):. 648 arg = _type_check(arg, ""Optional[t] requires a single type.""). --> 649 return Union[arg, type(None)]. 650 . 651 . /usr/lib/python3.5/typing.py in __getitem__(self, parameters). 550 parameters = (parameters,). 551 return self.__class__(self.__name__, self.__bases__,. --> 552 dict(self.__dict__), parameters, _root=True). 553 . 554 def __eq__(self, other):. /usr/lib/python3.5/typing.py in __new__(cls, name, bases, namespace, parameters, _root). 510 continue. 511 if any(isinstance(t2, type) and issubclass(t1, t2). --> 512 for t2 in all_params - {t1} if not isinstance(t2, TypeVar)):. 513 all_params.remove(t1). 514 # It's not a union if there's only one type left. /usr/lib/python3.5/typing.py in <genexpr>(.0). 5",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/561
https://github.com/scverse/scanpy/issues/561:733,usability,tool,tools,733,"Cannot import scanpy in jupyter notebook with python 3.5.2; Hello,. I'm having troubles with importing scanpy in my jupyter notebook. I am working on a server which has python 3.5.2 installed, and unfortunately I cannot update it. I am getting this issue:. ```pytb. import scanpy as sc. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-111-0074c9bc0b31> in <module>(). ----> 1 import scanpy as sc. ~/.local/lib/python3.5/site-packages/scanpy/__init__.py in <module>(). 29 . 30 # the actual API. ---> 31 from . import tools as tl. 32 from . import preprocessing as pp. 33 from . import plotting as pl. ~/.local/lib/python3.5/site-packages/scanpy/tools/__init__.py in <module>(). 8 from ._rank_genes_groups import rank_genes_groups. 9 from ._dpt import dpt. ---> 10 from ._leiden import leiden. 11 from ._louvain import louvain. 12 from ._sim import sim. ~/.local/lib/python3.5/site-packages/scanpy/tools/_leiden.py in <module>(). 29 n_iterations: int = -1,. 30 partition_type: Optional[Type[MutableVertexPartition]] = None,. ---> 31 copy: bool = False,. 32 **partition_kwargs. 33 ) -> Optional[AnnData]:. /usr/lib/python3.5/typing.py in __getitem__(self, arg). 647 def __getitem__(self, arg):. 648 arg = _type_check(arg, ""Optional[t] requires a single type.""). --> 649 return Union[arg, type(None)]. 650 . 651 . /usr/lib/python3.5/typing.py in __getitem__(self, parameters). 550 parameters = (parameters,). 551 return self.__class__(self.__name__, self.__bases__,. --> 552 dict(self.__dict__), parameters, _root=True). 553 . 554 def __eq__(self, other):. /usr/lib/python3.5/typing.py in __new__(cls, name, bases, namespace, parameters, _root). 510 continue. 511 if any(isinstance(t2, type) and issubclass(t1, t2). --> 512 for t2 in all_params - {t1} if not isinstance(t2, TypeVar)):. 513 all_params.remove(t1). 514 # It's not a union if there's only one type left. /usr/lib/python3.5/typing.py in <genexpr>(.0). 5",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/561
https://github.com/scverse/scanpy/issues/561:985,usability,tool,tools,985,"Cannot import scanpy in jupyter notebook with python 3.5.2; Hello,. I'm having troubles with importing scanpy in my jupyter notebook. I am working on a server which has python 3.5.2 installed, and unfortunately I cannot update it. I am getting this issue:. ```pytb. import scanpy as sc. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-111-0074c9bc0b31> in <module>(). ----> 1 import scanpy as sc. ~/.local/lib/python3.5/site-packages/scanpy/__init__.py in <module>(). 29 . 30 # the actual API. ---> 31 from . import tools as tl. 32 from . import preprocessing as pp. 33 from . import plotting as pl. ~/.local/lib/python3.5/site-packages/scanpy/tools/__init__.py in <module>(). 8 from ._rank_genes_groups import rank_genes_groups. 9 from ._dpt import dpt. ---> 10 from ._leiden import leiden. 11 from ._louvain import louvain. 12 from ._sim import sim. ~/.local/lib/python3.5/site-packages/scanpy/tools/_leiden.py in <module>(). 29 n_iterations: int = -1,. 30 partition_type: Optional[Type[MutableVertexPartition]] = None,. ---> 31 copy: bool = False,. 32 **partition_kwargs. 33 ) -> Optional[AnnData]:. /usr/lib/python3.5/typing.py in __getitem__(self, arg). 647 def __getitem__(self, arg):. 648 arg = _type_check(arg, ""Optional[t] requires a single type.""). --> 649 return Union[arg, type(None)]. 650 . 651 . /usr/lib/python3.5/typing.py in __getitem__(self, parameters). 550 parameters = (parameters,). 551 return self.__class__(self.__name__, self.__bases__,. --> 552 dict(self.__dict__), parameters, _root=True). 553 . 554 def __eq__(self, other):. /usr/lib/python3.5/typing.py in __new__(cls, name, bases, namespace, parameters, _root). 510 continue. 511 if any(isinstance(t2, type) and issubclass(t1, t2). --> 512 for t2 in all_params - {t1} if not isinstance(t2, TypeVar)):. 513 all_params.remove(t1). 514 # It's not a union if there's only one type left. /usr/lib/python3.5/typing.py in <genexpr>(.0). 5",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/561
https://github.com/scverse/scanpy/issues/562:242,testability,simpl,simple,242,"Dotplot where sizes are proportional to p-value and the color to log2-fold change?; @fidelram, as discussed today, could we adopt `pl.rank_genes_groups_dotplot` so that it reads this information from `.uns['rank_genes_groups']`? Maybe just a simple switch? Or having arguments `color` and `size` be a choice from a selection {`pvals`, `pvals_adj`, `log2FC`, `expression`, `frac-genes-expressed`}. .",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562
https://github.com/scverse/scanpy/issues/562:242,usability,simpl,simple,242,"Dotplot where sizes are proportional to p-value and the color to log2-fold change?; @fidelram, as discussed today, could we adopt `pl.rank_genes_groups_dotplot` so that it reads this information from `.uns['rank_genes_groups']`? Maybe just a simple switch? Or having arguments `color` and `size` be a choice from a selection {`pvals`, `pvals_adj`, `log2FC`, `expression`, `frac-genes-expressed`}. .",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562
https://github.com/scverse/scanpy/issues/563:4453,availability,operat,operating,4453,"ocal\conda\conda\envs\Scanpy\lib\os.py in makedirs(name, mode, exist_ok). 208 if head and tail and not path.exists(head):. 209 try:. --> 210 makedirs(head, mode, exist_ok). 211 except FileExistsError:. 212 # Defeats race condition when another thread created the path. ~\AppData\Local\conda\conda\envs\Scanpy\lib\os.py in makedirs(name, mode, exist_ok). 208 if head and tail and not path.exists(head):. 209 try:. --> 210 makedirs(head, mode, exist_ok). 211 except FileExistsError:. 212 # Defeats race condition when another thread created the path. ~\AppData\Local\conda\conda\envs\Scanpy\lib\os.py in makedirs(name, mode, exist_ok). 208 if head and tail and not path.exists(head):. 209 try:. --> 210 makedirs(head, mode, exist_ok). 211 except FileExistsError:. 212 # Defeats race condition when another thread created the path. ~\AppData\Local\conda\conda\envs\Scanpy\lib\os.py in makedirs(name, mode, exist_ok). 208 if head and tail and not path.exists(head):. 209 try:. --> 210 makedirs(head, mode, exist_ok). 211 except FileExistsError:. 212 # Defeats race condition when another thread created the path. ~\AppData\Local\conda\conda\envs\Scanpy\lib\os.py in makedirs(name, mode, exist_ok). 208 if head and tail and not path.exists(head):. 209 try:. --> 210 makedirs(head, mode, exist_ok). 211 except FileExistsError:. 212 # Defeats race condition when another thread created the path. ~\AppData\Local\conda\conda\envs\Scanpy\lib\os.py in makedirs(name, mode, exist_ok). 218 return. 219 try:. --> 220 mkdir(name, mode). 221 except OSError:. 222 # Cannot rely on checking for EEXIST, since the operating system. OSError: [WinError 123] The filename, directory name, or volume label syntax is incorrect: './cache/C:'. ```. Looks like the directory name for writing the file is messed up ./cache/C: it is trying to use linux formatting on a Windows machine. It works with cache = FALSE but I wanna try figuring out this problem first before moving on the other parts of the analysis. Any idea? Thanks",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/563
https://github.com/scverse/scanpy/issues/563:411,deployability,modul,module,411,"OSError: [WinError 123] When importing 10x data with cache=TRUE; Hi, . I'm running Scanpy through Conda on Windows. I have an issue when I try to import a dataset and set cache = TRUE. ```pytb. ... writing an h5ad cache file to speedup reading next time. ---------------------------------------------------------------------------. OSError Traceback (most recent call last). <ipython-input-10-894335192e05> in <module>. 2 'C:\\Users\\david\\Desktop\\10x_hiv_mcherry\\aggregate\\outs\\filtered_feature_bc_matrix',. 3 var_names='gene_symbols', # use gene symbols for the variable names (variables-axis index). ----> 4 cache=True) # write a cache file for faster subsequent reading. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\readwrite.py in read_10x_mtx(path, var_names, make_unique, cache, gex_only). 244 else:. 245 adata = _read_v3_10x_mtx(path, var_names=var_names,. --> 246 make_unique=make_unique, cache=cache). 247 if not gex_only:. 248 return adata. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\readwrite.py in _read_v3_10x_mtx(path, var_names, make_unique, cache). 277 Read mex from output from Cell Ranger v3 or later versions. 278 """""". --> 279 adata = read(os.path.join(path, 'matrix.mtx.gz'), cache=cache).T # transpose the data. 280 genes = pd.read_csv(os.path.join(path, 'features.tsv.gz'), header=None, sep='\t'). 281 if var_names == 'gene_symbols':. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs). 76 return _read(filename, backed=backed, sheet=sheet, ext=ext,. 77 delimiter=delimiter, first_column_names=first_column_names,. ---> 78 backup_url=backup_url, cache=cache, **kwargs). 79 # generate filename and read to dict. 80 filekey = filename. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, supp",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/563
https://github.com/scverse/scanpy/issues/563:1167,deployability,version,versions,1167,"cache = TRUE. ```pytb. ... writing an h5ad cache file to speedup reading next time. ---------------------------------------------------------------------------. OSError Traceback (most recent call last). <ipython-input-10-894335192e05> in <module>. 2 'C:\\Users\\david\\Desktop\\10x_hiv_mcherry\\aggregate\\outs\\filtered_feature_bc_matrix',. 3 var_names='gene_symbols', # use gene symbols for the variable names (variables-axis index). ----> 4 cache=True) # write a cache file for faster subsequent reading. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\readwrite.py in read_10x_mtx(path, var_names, make_unique, cache, gex_only). 244 else:. 245 adata = _read_v3_10x_mtx(path, var_names=var_names,. --> 246 make_unique=make_unique, cache=cache). 247 if not gex_only:. 248 return adata. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\readwrite.py in _read_v3_10x_mtx(path, var_names, make_unique, cache). 277 Read mex from output from Cell Ranger v3 or later versions. 278 """""". --> 279 adata = read(os.path.join(path, 'matrix.mtx.gz'), cache=cache).T # transpose the data. 280 genes = pd.read_csv(os.path.join(path, 'features.tsv.gz'), header=None, sep='\t'). 281 if var_names == 'gene_symbols':. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs). 76 return _read(filename, backed=backed, sheet=sheet, ext=ext,. 77 delimiter=delimiter, first_column_names=first_column_names,. ---> 78 backup_url=backup_url, cache=cache, **kwargs). 79 # generate filename and read to dict. 80 filekey = filename. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs). 479 'cache file to speedup reading next time'). 480 if not os.path.exists(os.path.dirname(filename_cache)):. --> 481 os.makedirs(os.path.dir",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/563
https://github.com/scverse/scanpy/issues/563:660,integrability,sub,subsequent,660,"OSError: [WinError 123] When importing 10x data with cache=TRUE; Hi, . I'm running Scanpy through Conda on Windows. I have an issue when I try to import a dataset and set cache = TRUE. ```pytb. ... writing an h5ad cache file to speedup reading next time. ---------------------------------------------------------------------------. OSError Traceback (most recent call last). <ipython-input-10-894335192e05> in <module>. 2 'C:\\Users\\david\\Desktop\\10x_hiv_mcherry\\aggregate\\outs\\filtered_feature_bc_matrix',. 3 var_names='gene_symbols', # use gene symbols for the variable names (variables-axis index). ----> 4 cache=True) # write a cache file for faster subsequent reading. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\readwrite.py in read_10x_mtx(path, var_names, make_unique, cache, gex_only). 244 else:. 245 adata = _read_v3_10x_mtx(path, var_names=var_names,. --> 246 make_unique=make_unique, cache=cache). 247 if not gex_only:. 248 return adata. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\readwrite.py in _read_v3_10x_mtx(path, var_names, make_unique, cache). 277 Read mex from output from Cell Ranger v3 or later versions. 278 """""". --> 279 adata = read(os.path.join(path, 'matrix.mtx.gz'), cache=cache).T # transpose the data. 280 genes = pd.read_csv(os.path.join(path, 'features.tsv.gz'), header=None, sep='\t'). 281 if var_names == 'gene_symbols':. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs). 76 return _read(filename, backed=backed, sheet=sheet, ext=ext,. 77 delimiter=delimiter, first_column_names=first_column_names,. ---> 78 backup_url=backup_url, cache=cache, **kwargs). 79 # generate filename and read to dict. 80 filekey = filename. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, supp",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/563
https://github.com/scverse/scanpy/issues/563:1167,integrability,version,versions,1167,"cache = TRUE. ```pytb. ... writing an h5ad cache file to speedup reading next time. ---------------------------------------------------------------------------. OSError Traceback (most recent call last). <ipython-input-10-894335192e05> in <module>. 2 'C:\\Users\\david\\Desktop\\10x_hiv_mcherry\\aggregate\\outs\\filtered_feature_bc_matrix',. 3 var_names='gene_symbols', # use gene symbols for the variable names (variables-axis index). ----> 4 cache=True) # write a cache file for faster subsequent reading. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\readwrite.py in read_10x_mtx(path, var_names, make_unique, cache, gex_only). 244 else:. 245 adata = _read_v3_10x_mtx(path, var_names=var_names,. --> 246 make_unique=make_unique, cache=cache). 247 if not gex_only:. 248 return adata. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\readwrite.py in _read_v3_10x_mtx(path, var_names, make_unique, cache). 277 Read mex from output from Cell Ranger v3 or later versions. 278 """""". --> 279 adata = read(os.path.join(path, 'matrix.mtx.gz'), cache=cache).T # transpose the data. 280 genes = pd.read_csv(os.path.join(path, 'features.tsv.gz'), header=None, sep='\t'). 281 if var_names == 'gene_symbols':. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs). 76 return _read(filename, backed=backed, sheet=sheet, ext=ext,. 77 delimiter=delimiter, first_column_names=first_column_names,. ---> 78 backup_url=backup_url, cache=cache, **kwargs). 79 # generate filename and read to dict. 80 filekey = filename. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs). 479 'cache file to speedup reading next time'). 480 if not os.path.exists(os.path.dirname(filename_cache)):. --> 481 os.makedirs(os.path.dir",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/563
https://github.com/scverse/scanpy/issues/563:4682,interoperability,format,formatting,4682,"ocal\conda\conda\envs\Scanpy\lib\os.py in makedirs(name, mode, exist_ok). 208 if head and tail and not path.exists(head):. 209 try:. --> 210 makedirs(head, mode, exist_ok). 211 except FileExistsError:. 212 # Defeats race condition when another thread created the path. ~\AppData\Local\conda\conda\envs\Scanpy\lib\os.py in makedirs(name, mode, exist_ok). 208 if head and tail and not path.exists(head):. 209 try:. --> 210 makedirs(head, mode, exist_ok). 211 except FileExistsError:. 212 # Defeats race condition when another thread created the path. ~\AppData\Local\conda\conda\envs\Scanpy\lib\os.py in makedirs(name, mode, exist_ok). 208 if head and tail and not path.exists(head):. 209 try:. --> 210 makedirs(head, mode, exist_ok). 211 except FileExistsError:. 212 # Defeats race condition when another thread created the path. ~\AppData\Local\conda\conda\envs\Scanpy\lib\os.py in makedirs(name, mode, exist_ok). 208 if head and tail and not path.exists(head):. 209 try:. --> 210 makedirs(head, mode, exist_ok). 211 except FileExistsError:. 212 # Defeats race condition when another thread created the path. ~\AppData\Local\conda\conda\envs\Scanpy\lib\os.py in makedirs(name, mode, exist_ok). 208 if head and tail and not path.exists(head):. 209 try:. --> 210 makedirs(head, mode, exist_ok). 211 except FileExistsError:. 212 # Defeats race condition when another thread created the path. ~\AppData\Local\conda\conda\envs\Scanpy\lib\os.py in makedirs(name, mode, exist_ok). 218 return. 219 try:. --> 220 mkdir(name, mode). 221 except OSError:. 222 # Cannot rely on checking for EEXIST, since the operating system. OSError: [WinError 123] The filename, directory name, or volume label syntax is incorrect: './cache/C:'. ```. Looks like the directory name for writing the file is messed up ./cache/C: it is trying to use linux formatting on a Windows machine. It works with cache = FALSE but I wanna try figuring out this problem first before moving on the other parts of the analysis. Any idea? Thanks",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/563
https://github.com/scverse/scanpy/issues/563:411,modifiability,modul,module,411,"OSError: [WinError 123] When importing 10x data with cache=TRUE; Hi, . I'm running Scanpy through Conda on Windows. I have an issue when I try to import a dataset and set cache = TRUE. ```pytb. ... writing an h5ad cache file to speedup reading next time. ---------------------------------------------------------------------------. OSError Traceback (most recent call last). <ipython-input-10-894335192e05> in <module>. 2 'C:\\Users\\david\\Desktop\\10x_hiv_mcherry\\aggregate\\outs\\filtered_feature_bc_matrix',. 3 var_names='gene_symbols', # use gene symbols for the variable names (variables-axis index). ----> 4 cache=True) # write a cache file for faster subsequent reading. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\readwrite.py in read_10x_mtx(path, var_names, make_unique, cache, gex_only). 244 else:. 245 adata = _read_v3_10x_mtx(path, var_names=var_names,. --> 246 make_unique=make_unique, cache=cache). 247 if not gex_only:. 248 return adata. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\readwrite.py in _read_v3_10x_mtx(path, var_names, make_unique, cache). 277 Read mex from output from Cell Ranger v3 or later versions. 278 """""". --> 279 adata = read(os.path.join(path, 'matrix.mtx.gz'), cache=cache).T # transpose the data. 280 genes = pd.read_csv(os.path.join(path, 'features.tsv.gz'), header=None, sep='\t'). 281 if var_names == 'gene_symbols':. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs). 76 return _read(filename, backed=backed, sheet=sheet, ext=ext,. 77 delimiter=delimiter, first_column_names=first_column_names,. ---> 78 backup_url=backup_url, cache=cache, **kwargs). 79 # generate filename and read to dict. 80 filekey = filename. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, supp",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/563
https://github.com/scverse/scanpy/issues/563:569,modifiability,variab,variable,569,"OSError: [WinError 123] When importing 10x data with cache=TRUE; Hi, . I'm running Scanpy through Conda on Windows. I have an issue when I try to import a dataset and set cache = TRUE. ```pytb. ... writing an h5ad cache file to speedup reading next time. ---------------------------------------------------------------------------. OSError Traceback (most recent call last). <ipython-input-10-894335192e05> in <module>. 2 'C:\\Users\\david\\Desktop\\10x_hiv_mcherry\\aggregate\\outs\\filtered_feature_bc_matrix',. 3 var_names='gene_symbols', # use gene symbols for the variable names (variables-axis index). ----> 4 cache=True) # write a cache file for faster subsequent reading. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\readwrite.py in read_10x_mtx(path, var_names, make_unique, cache, gex_only). 244 else:. 245 adata = _read_v3_10x_mtx(path, var_names=var_names,. --> 246 make_unique=make_unique, cache=cache). 247 if not gex_only:. 248 return adata. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\readwrite.py in _read_v3_10x_mtx(path, var_names, make_unique, cache). 277 Read mex from output from Cell Ranger v3 or later versions. 278 """""". --> 279 adata = read(os.path.join(path, 'matrix.mtx.gz'), cache=cache).T # transpose the data. 280 genes = pd.read_csv(os.path.join(path, 'features.tsv.gz'), header=None, sep='\t'). 281 if var_names == 'gene_symbols':. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs). 76 return _read(filename, backed=backed, sheet=sheet, ext=ext,. 77 delimiter=delimiter, first_column_names=first_column_names,. ---> 78 backup_url=backup_url, cache=cache, **kwargs). 79 # generate filename and read to dict. 80 filekey = filename. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, supp",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/563
https://github.com/scverse/scanpy/issues/563:585,modifiability,variab,variables-axis,585,"OSError: [WinError 123] When importing 10x data with cache=TRUE; Hi, . I'm running Scanpy through Conda on Windows. I have an issue when I try to import a dataset and set cache = TRUE. ```pytb. ... writing an h5ad cache file to speedup reading next time. ---------------------------------------------------------------------------. OSError Traceback (most recent call last). <ipython-input-10-894335192e05> in <module>. 2 'C:\\Users\\david\\Desktop\\10x_hiv_mcherry\\aggregate\\outs\\filtered_feature_bc_matrix',. 3 var_names='gene_symbols', # use gene symbols for the variable names (variables-axis index). ----> 4 cache=True) # write a cache file for faster subsequent reading. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\readwrite.py in read_10x_mtx(path, var_names, make_unique, cache, gex_only). 244 else:. 245 adata = _read_v3_10x_mtx(path, var_names=var_names,. --> 246 make_unique=make_unique, cache=cache). 247 if not gex_only:. 248 return adata. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\readwrite.py in _read_v3_10x_mtx(path, var_names, make_unique, cache). 277 Read mex from output from Cell Ranger v3 or later versions. 278 """""". --> 279 adata = read(os.path.join(path, 'matrix.mtx.gz'), cache=cache).T # transpose the data. 280 genes = pd.read_csv(os.path.join(path, 'features.tsv.gz'), header=None, sep='\t'). 281 if var_names == 'gene_symbols':. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs). 76 return _read(filename, backed=backed, sheet=sheet, ext=ext,. 77 delimiter=delimiter, first_column_names=first_column_names,. ---> 78 backup_url=backup_url, cache=cache, **kwargs). 79 # generate filename and read to dict. 80 filekey = filename. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, supp",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/563
https://github.com/scverse/scanpy/issues/563:729,modifiability,pac,packages,729,"OSError: [WinError 123] When importing 10x data with cache=TRUE; Hi, . I'm running Scanpy through Conda on Windows. I have an issue when I try to import a dataset and set cache = TRUE. ```pytb. ... writing an h5ad cache file to speedup reading next time. ---------------------------------------------------------------------------. OSError Traceback (most recent call last). <ipython-input-10-894335192e05> in <module>. 2 'C:\\Users\\david\\Desktop\\10x_hiv_mcherry\\aggregate\\outs\\filtered_feature_bc_matrix',. 3 var_names='gene_symbols', # use gene symbols for the variable names (variables-axis index). ----> 4 cache=True) # write a cache file for faster subsequent reading. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\readwrite.py in read_10x_mtx(path, var_names, make_unique, cache, gex_only). 244 else:. 245 adata = _read_v3_10x_mtx(path, var_names=var_names,. --> 246 make_unique=make_unique, cache=cache). 247 if not gex_only:. 248 return adata. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\readwrite.py in _read_v3_10x_mtx(path, var_names, make_unique, cache). 277 Read mex from output from Cell Ranger v3 or later versions. 278 """""". --> 279 adata = read(os.path.join(path, 'matrix.mtx.gz'), cache=cache).T # transpose the data. 280 genes = pd.read_csv(os.path.join(path, 'features.tsv.gz'), header=None, sep='\t'). 281 if var_names == 'gene_symbols':. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs). 76 return _read(filename, backed=backed, sheet=sheet, ext=ext,. 77 delimiter=delimiter, first_column_names=first_column_names,. ---> 78 backup_url=backup_url, cache=cache, **kwargs). 79 # generate filename and read to dict. 80 filekey = filename. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, supp",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/563
https://github.com/scverse/scanpy/issues/563:1026,modifiability,pac,packages,1026,"mporting 10x data with cache=TRUE; Hi, . I'm running Scanpy through Conda on Windows. I have an issue when I try to import a dataset and set cache = TRUE. ```pytb. ... writing an h5ad cache file to speedup reading next time. ---------------------------------------------------------------------------. OSError Traceback (most recent call last). <ipython-input-10-894335192e05> in <module>. 2 'C:\\Users\\david\\Desktop\\10x_hiv_mcherry\\aggregate\\outs\\filtered_feature_bc_matrix',. 3 var_names='gene_symbols', # use gene symbols for the variable names (variables-axis index). ----> 4 cache=True) # write a cache file for faster subsequent reading. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\readwrite.py in read_10x_mtx(path, var_names, make_unique, cache, gex_only). 244 else:. 245 adata = _read_v3_10x_mtx(path, var_names=var_names,. --> 246 make_unique=make_unique, cache=cache). 247 if not gex_only:. 248 return adata. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\readwrite.py in _read_v3_10x_mtx(path, var_names, make_unique, cache). 277 Read mex from output from Cell Ranger v3 or later versions. 278 """""". --> 279 adata = read(os.path.join(path, 'matrix.mtx.gz'), cache=cache).T # transpose the data. 280 genes = pd.read_csv(os.path.join(path, 'features.tsv.gz'), header=None, sep='\t'). 281 if var_names == 'gene_symbols':. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs). 76 return _read(filename, backed=backed, sheet=sheet, ext=ext,. 77 delimiter=delimiter, first_column_names=first_column_names,. ---> 78 backup_url=backup_url, cache=cache, **kwargs). 79 # generate filename and read to dict. 80 filekey = filename. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/563
https://github.com/scverse/scanpy/issues/563:1167,modifiability,version,versions,1167,"cache = TRUE. ```pytb. ... writing an h5ad cache file to speedup reading next time. ---------------------------------------------------------------------------. OSError Traceback (most recent call last). <ipython-input-10-894335192e05> in <module>. 2 'C:\\Users\\david\\Desktop\\10x_hiv_mcherry\\aggregate\\outs\\filtered_feature_bc_matrix',. 3 var_names='gene_symbols', # use gene symbols for the variable names (variables-axis index). ----> 4 cache=True) # write a cache file for faster subsequent reading. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\readwrite.py in read_10x_mtx(path, var_names, make_unique, cache, gex_only). 244 else:. 245 adata = _read_v3_10x_mtx(path, var_names=var_names,. --> 246 make_unique=make_unique, cache=cache). 247 if not gex_only:. 248 return adata. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\readwrite.py in _read_v3_10x_mtx(path, var_names, make_unique, cache). 277 Read mex from output from Cell Ranger v3 or later versions. 278 """""". --> 279 adata = read(os.path.join(path, 'matrix.mtx.gz'), cache=cache).T # transpose the data. 280 genes = pd.read_csv(os.path.join(path, 'features.tsv.gz'), header=None, sep='\t'). 281 if var_names == 'gene_symbols':. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs). 76 return _read(filename, backed=backed, sheet=sheet, ext=ext,. 77 delimiter=delimiter, first_column_names=first_column_names,. ---> 78 backup_url=backup_url, cache=cache, **kwargs). 79 # generate filename and read to dict. 80 filekey = filename. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs). 479 'cache file to speedup reading next time'). 480 if not os.path.exists(os.path.dirname(filename_cache)):. --> 481 os.makedirs(os.path.dir",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/563
https://github.com/scverse/scanpy/issues/563:1454,modifiability,pac,packages,1454,"mcherry\\aggregate\\outs\\filtered_feature_bc_matrix',. 3 var_names='gene_symbols', # use gene symbols for the variable names (variables-axis index). ----> 4 cache=True) # write a cache file for faster subsequent reading. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\readwrite.py in read_10x_mtx(path, var_names, make_unique, cache, gex_only). 244 else:. 245 adata = _read_v3_10x_mtx(path, var_names=var_names,. --> 246 make_unique=make_unique, cache=cache). 247 if not gex_only:. 248 return adata. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\readwrite.py in _read_v3_10x_mtx(path, var_names, make_unique, cache). 277 Read mex from output from Cell Ranger v3 or later versions. 278 """""". --> 279 adata = read(os.path.join(path, 'matrix.mtx.gz'), cache=cache).T # transpose the data. 280 genes = pd.read_csv(os.path.join(path, 'features.tsv.gz'), header=None, sep='\t'). 281 if var_names == 'gene_symbols':. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs). 76 return _read(filename, backed=backed, sheet=sheet, ext=ext,. 77 delimiter=delimiter, first_column_names=first_column_names,. ---> 78 backup_url=backup_url, cache=cache, **kwargs). 79 # generate filename and read to dict. 80 filekey = filename. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs). 479 'cache file to speedup reading next time'). 480 if not os.path.exists(os.path.dirname(filename_cache)):. --> 481 os.makedirs(os.path.dirname(filename_cache)). 482 # write for faster reading when calling the next time. 483 adata.write(filename_cache). ~\AppData\Local\conda\conda\envs\Scanpy\lib\os.py in makedirs(name, mode, exist_ok). 208 if head and tail and not path.exists(head):. 209 try:. --> 210 makedirs(head, mode,",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/563
https://github.com/scverse/scanpy/issues/563:1878,modifiability,pac,packages,1878,"names,. --> 246 make_unique=make_unique, cache=cache). 247 if not gex_only:. 248 return adata. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\readwrite.py in _read_v3_10x_mtx(path, var_names, make_unique, cache). 277 Read mex from output from Cell Ranger v3 or later versions. 278 """""". --> 279 adata = read(os.path.join(path, 'matrix.mtx.gz'), cache=cache).T # transpose the data. 280 genes = pd.read_csv(os.path.join(path, 'features.tsv.gz'), header=None, sep='\t'). 281 if var_names == 'gene_symbols':. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs). 76 return _read(filename, backed=backed, sheet=sheet, ext=ext,. 77 delimiter=delimiter, first_column_names=first_column_names,. ---> 78 backup_url=backup_url, cache=cache, **kwargs). 79 # generate filename and read to dict. 80 filekey = filename. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs). 479 'cache file to speedup reading next time'). 480 if not os.path.exists(os.path.dirname(filename_cache)):. --> 481 os.makedirs(os.path.dirname(filename_cache)). 482 # write for faster reading when calling the next time. 483 adata.write(filename_cache). ~\AppData\Local\conda\conda\envs\Scanpy\lib\os.py in makedirs(name, mode, exist_ok). 208 if head and tail and not path.exists(head):. 209 try:. --> 210 makedirs(head, mode, exist_ok). 211 except FileExistsError:. 212 # Defeats race condition when another thread created the path. ~\AppData\Local\conda\conda\envs\Scanpy\lib\os.py in makedirs(name, mode, exist_ok). 208 if head and tail and not path.exists(head):. 209 try:. --> 210 makedirs(head, mode, exist_ok). 211 except FileExistsError:. 212 # Defeats race condition when another thread created the path. ~\AppData\Local\conda\conda\envs\Sca",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/563
https://github.com/scverse/scanpy/issues/563:53,performance,cach,cache,53,"OSError: [WinError 123] When importing 10x data with cache=TRUE; Hi, . I'm running Scanpy through Conda on Windows. I have an issue when I try to import a dataset and set cache = TRUE. ```pytb. ... writing an h5ad cache file to speedup reading next time. ---------------------------------------------------------------------------. OSError Traceback (most recent call last). <ipython-input-10-894335192e05> in <module>. 2 'C:\\Users\\david\\Desktop\\10x_hiv_mcherry\\aggregate\\outs\\filtered_feature_bc_matrix',. 3 var_names='gene_symbols', # use gene symbols for the variable names (variables-axis index). ----> 4 cache=True) # write a cache file for faster subsequent reading. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\readwrite.py in read_10x_mtx(path, var_names, make_unique, cache, gex_only). 244 else:. 245 adata = _read_v3_10x_mtx(path, var_names=var_names,. --> 246 make_unique=make_unique, cache=cache). 247 if not gex_only:. 248 return adata. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\readwrite.py in _read_v3_10x_mtx(path, var_names, make_unique, cache). 277 Read mex from output from Cell Ranger v3 or later versions. 278 """""". --> 279 adata = read(os.path.join(path, 'matrix.mtx.gz'), cache=cache).T # transpose the data. 280 genes = pd.read_csv(os.path.join(path, 'features.tsv.gz'), header=None, sep='\t'). 281 if var_names == 'gene_symbols':. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs). 76 return _read(filename, backed=backed, sheet=sheet, ext=ext,. 77 delimiter=delimiter, first_column_names=first_column_names,. ---> 78 backup_url=backup_url, cache=cache, **kwargs). 79 # generate filename and read to dict. 80 filekey = filename. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, supp",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/563
https://github.com/scverse/scanpy/issues/563:171,performance,cach,cache,171,"OSError: [WinError 123] When importing 10x data with cache=TRUE; Hi, . I'm running Scanpy through Conda on Windows. I have an issue when I try to import a dataset and set cache = TRUE. ```pytb. ... writing an h5ad cache file to speedup reading next time. ---------------------------------------------------------------------------. OSError Traceback (most recent call last). <ipython-input-10-894335192e05> in <module>. 2 'C:\\Users\\david\\Desktop\\10x_hiv_mcherry\\aggregate\\outs\\filtered_feature_bc_matrix',. 3 var_names='gene_symbols', # use gene symbols for the variable names (variables-axis index). ----> 4 cache=True) # write a cache file for faster subsequent reading. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\readwrite.py in read_10x_mtx(path, var_names, make_unique, cache, gex_only). 244 else:. 245 adata = _read_v3_10x_mtx(path, var_names=var_names,. --> 246 make_unique=make_unique, cache=cache). 247 if not gex_only:. 248 return adata. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\readwrite.py in _read_v3_10x_mtx(path, var_names, make_unique, cache). 277 Read mex from output from Cell Ranger v3 or later versions. 278 """""". --> 279 adata = read(os.path.join(path, 'matrix.mtx.gz'), cache=cache).T # transpose the data. 280 genes = pd.read_csv(os.path.join(path, 'features.tsv.gz'), header=None, sep='\t'). 281 if var_names == 'gene_symbols':. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs). 76 return _read(filename, backed=backed, sheet=sheet, ext=ext,. 77 delimiter=delimiter, first_column_names=first_column_names,. ---> 78 backup_url=backup_url, cache=cache, **kwargs). 79 # generate filename and read to dict. 80 filekey = filename. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, supp",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/563
https://github.com/scverse/scanpy/issues/563:214,performance,cach,cache,214,"OSError: [WinError 123] When importing 10x data with cache=TRUE; Hi, . I'm running Scanpy through Conda on Windows. I have an issue when I try to import a dataset and set cache = TRUE. ```pytb. ... writing an h5ad cache file to speedup reading next time. ---------------------------------------------------------------------------. OSError Traceback (most recent call last). <ipython-input-10-894335192e05> in <module>. 2 'C:\\Users\\david\\Desktop\\10x_hiv_mcherry\\aggregate\\outs\\filtered_feature_bc_matrix',. 3 var_names='gene_symbols', # use gene symbols for the variable names (variables-axis index). ----> 4 cache=True) # write a cache file for faster subsequent reading. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\readwrite.py in read_10x_mtx(path, var_names, make_unique, cache, gex_only). 244 else:. 245 adata = _read_v3_10x_mtx(path, var_names=var_names,. --> 246 make_unique=make_unique, cache=cache). 247 if not gex_only:. 248 return adata. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\readwrite.py in _read_v3_10x_mtx(path, var_names, make_unique, cache). 277 Read mex from output from Cell Ranger v3 or later versions. 278 """""". --> 279 adata = read(os.path.join(path, 'matrix.mtx.gz'), cache=cache).T # transpose the data. 280 genes = pd.read_csv(os.path.join(path, 'features.tsv.gz'), header=None, sep='\t'). 281 if var_names == 'gene_symbols':. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs). 76 return _read(filename, backed=backed, sheet=sheet, ext=ext,. 77 delimiter=delimiter, first_column_names=first_column_names,. ---> 78 backup_url=backup_url, cache=cache, **kwargs). 79 # generate filename and read to dict. 80 filekey = filename. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, supp",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/563
https://github.com/scverse/scanpy/issues/563:249,performance,time,time,249,"OSError: [WinError 123] When importing 10x data with cache=TRUE; Hi, . I'm running Scanpy through Conda on Windows. I have an issue when I try to import a dataset and set cache = TRUE. ```pytb. ... writing an h5ad cache file to speedup reading next time. ---------------------------------------------------------------------------. OSError Traceback (most recent call last). <ipython-input-10-894335192e05> in <module>. 2 'C:\\Users\\david\\Desktop\\10x_hiv_mcherry\\aggregate\\outs\\filtered_feature_bc_matrix',. 3 var_names='gene_symbols', # use gene symbols for the variable names (variables-axis index). ----> 4 cache=True) # write a cache file for faster subsequent reading. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\readwrite.py in read_10x_mtx(path, var_names, make_unique, cache, gex_only). 244 else:. 245 adata = _read_v3_10x_mtx(path, var_names=var_names,. --> 246 make_unique=make_unique, cache=cache). 247 if not gex_only:. 248 return adata. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\readwrite.py in _read_v3_10x_mtx(path, var_names, make_unique, cache). 277 Read mex from output from Cell Ranger v3 or later versions. 278 """""". --> 279 adata = read(os.path.join(path, 'matrix.mtx.gz'), cache=cache).T # transpose the data. 280 genes = pd.read_csv(os.path.join(path, 'features.tsv.gz'), header=None, sep='\t'). 281 if var_names == 'gene_symbols':. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs). 76 return _read(filename, backed=backed, sheet=sheet, ext=ext,. 77 delimiter=delimiter, first_column_names=first_column_names,. ---> 78 backup_url=backup_url, cache=cache, **kwargs). 79 # generate filename and read to dict. 80 filekey = filename. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, supp",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/563
https://github.com/scverse/scanpy/issues/563:616,performance,cach,cache,616,"OSError: [WinError 123] When importing 10x data with cache=TRUE; Hi, . I'm running Scanpy through Conda on Windows. I have an issue when I try to import a dataset and set cache = TRUE. ```pytb. ... writing an h5ad cache file to speedup reading next time. ---------------------------------------------------------------------------. OSError Traceback (most recent call last). <ipython-input-10-894335192e05> in <module>. 2 'C:\\Users\\david\\Desktop\\10x_hiv_mcherry\\aggregate\\outs\\filtered_feature_bc_matrix',. 3 var_names='gene_symbols', # use gene symbols for the variable names (variables-axis index). ----> 4 cache=True) # write a cache file for faster subsequent reading. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\readwrite.py in read_10x_mtx(path, var_names, make_unique, cache, gex_only). 244 else:. 245 adata = _read_v3_10x_mtx(path, var_names=var_names,. --> 246 make_unique=make_unique, cache=cache). 247 if not gex_only:. 248 return adata. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\readwrite.py in _read_v3_10x_mtx(path, var_names, make_unique, cache). 277 Read mex from output from Cell Ranger v3 or later versions. 278 """""". --> 279 adata = read(os.path.join(path, 'matrix.mtx.gz'), cache=cache).T # transpose the data. 280 genes = pd.read_csv(os.path.join(path, 'features.tsv.gz'), header=None, sep='\t'). 281 if var_names == 'gene_symbols':. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs). 76 return _read(filename, backed=backed, sheet=sheet, ext=ext,. 77 delimiter=delimiter, first_column_names=first_column_names,. ---> 78 backup_url=backup_url, cache=cache, **kwargs). 79 # generate filename and read to dict. 80 filekey = filename. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, supp",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/563
https://github.com/scverse/scanpy/issues/563:638,performance,cach,cache,638,"OSError: [WinError 123] When importing 10x data with cache=TRUE; Hi, . I'm running Scanpy through Conda on Windows. I have an issue when I try to import a dataset and set cache = TRUE. ```pytb. ... writing an h5ad cache file to speedup reading next time. ---------------------------------------------------------------------------. OSError Traceback (most recent call last). <ipython-input-10-894335192e05> in <module>. 2 'C:\\Users\\david\\Desktop\\10x_hiv_mcherry\\aggregate\\outs\\filtered_feature_bc_matrix',. 3 var_names='gene_symbols', # use gene symbols for the variable names (variables-axis index). ----> 4 cache=True) # write a cache file for faster subsequent reading. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\readwrite.py in read_10x_mtx(path, var_names, make_unique, cache, gex_only). 244 else:. 245 adata = _read_v3_10x_mtx(path, var_names=var_names,. --> 246 make_unique=make_unique, cache=cache). 247 if not gex_only:. 248 return adata. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\readwrite.py in _read_v3_10x_mtx(path, var_names, make_unique, cache). 277 Read mex from output from Cell Ranger v3 or later versions. 278 """""". --> 279 adata = read(os.path.join(path, 'matrix.mtx.gz'), cache=cache).T # transpose the data. 280 genes = pd.read_csv(os.path.join(path, 'features.tsv.gz'), header=None, sep='\t'). 281 if var_names == 'gene_symbols':. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs). 76 return _read(filename, backed=backed, sheet=sheet, ext=ext,. 77 delimiter=delimiter, first_column_names=first_column_names,. ---> 78 backup_url=backup_url, cache=cache, **kwargs). 79 # generate filename and read to dict. 80 filekey = filename. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, supp",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/563
https://github.com/scverse/scanpy/issues/563:804,performance,cach,cache,804,"OSError: [WinError 123] When importing 10x data with cache=TRUE; Hi, . I'm running Scanpy through Conda on Windows. I have an issue when I try to import a dataset and set cache = TRUE. ```pytb. ... writing an h5ad cache file to speedup reading next time. ---------------------------------------------------------------------------. OSError Traceback (most recent call last). <ipython-input-10-894335192e05> in <module>. 2 'C:\\Users\\david\\Desktop\\10x_hiv_mcherry\\aggregate\\outs\\filtered_feature_bc_matrix',. 3 var_names='gene_symbols', # use gene symbols for the variable names (variables-axis index). ----> 4 cache=True) # write a cache file for faster subsequent reading. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\readwrite.py in read_10x_mtx(path, var_names, make_unique, cache, gex_only). 244 else:. 245 adata = _read_v3_10x_mtx(path, var_names=var_names,. --> 246 make_unique=make_unique, cache=cache). 247 if not gex_only:. 248 return adata. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\readwrite.py in _read_v3_10x_mtx(path, var_names, make_unique, cache). 277 Read mex from output from Cell Ranger v3 or later versions. 278 """""". --> 279 adata = read(os.path.join(path, 'matrix.mtx.gz'), cache=cache).T # transpose the data. 280 genes = pd.read_csv(os.path.join(path, 'features.tsv.gz'), header=None, sep='\t'). 281 if var_names == 'gene_symbols':. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs). 76 return _read(filename, backed=backed, sheet=sheet, ext=ext,. 77 delimiter=delimiter, first_column_names=first_column_names,. ---> 78 backup_url=backup_url, cache=cache, **kwargs). 79 # generate filename and read to dict. 80 filekey = filename. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, supp",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/563
https://github.com/scverse/scanpy/issues/563:923,performance,cach,cache,923,"OSError: [WinError 123] When importing 10x data with cache=TRUE; Hi, . I'm running Scanpy through Conda on Windows. I have an issue when I try to import a dataset and set cache = TRUE. ```pytb. ... writing an h5ad cache file to speedup reading next time. ---------------------------------------------------------------------------. OSError Traceback (most recent call last). <ipython-input-10-894335192e05> in <module>. 2 'C:\\Users\\david\\Desktop\\10x_hiv_mcherry\\aggregate\\outs\\filtered_feature_bc_matrix',. 3 var_names='gene_symbols', # use gene symbols for the variable names (variables-axis index). ----> 4 cache=True) # write a cache file for faster subsequent reading. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\readwrite.py in read_10x_mtx(path, var_names, make_unique, cache, gex_only). 244 else:. 245 adata = _read_v3_10x_mtx(path, var_names=var_names,. --> 246 make_unique=make_unique, cache=cache). 247 if not gex_only:. 248 return adata. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\readwrite.py in _read_v3_10x_mtx(path, var_names, make_unique, cache). 277 Read mex from output from Cell Ranger v3 or later versions. 278 """""". --> 279 adata = read(os.path.join(path, 'matrix.mtx.gz'), cache=cache).T # transpose the data. 280 genes = pd.read_csv(os.path.join(path, 'features.tsv.gz'), header=None, sep='\t'). 281 if var_names == 'gene_symbols':. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs). 76 return _read(filename, backed=backed, sheet=sheet, ext=ext,. 77 delimiter=delimiter, first_column_names=first_column_names,. ---> 78 backup_url=backup_url, cache=cache, **kwargs). 79 # generate filename and read to dict. 80 filekey = filename. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, supp",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/563
https://github.com/scverse/scanpy/issues/563:929,performance,cach,cache,929,"OSError: [WinError 123] When importing 10x data with cache=TRUE; Hi, . I'm running Scanpy through Conda on Windows. I have an issue when I try to import a dataset and set cache = TRUE. ```pytb. ... writing an h5ad cache file to speedup reading next time. ---------------------------------------------------------------------------. OSError Traceback (most recent call last). <ipython-input-10-894335192e05> in <module>. 2 'C:\\Users\\david\\Desktop\\10x_hiv_mcherry\\aggregate\\outs\\filtered_feature_bc_matrix',. 3 var_names='gene_symbols', # use gene symbols for the variable names (variables-axis index). ----> 4 cache=True) # write a cache file for faster subsequent reading. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\readwrite.py in read_10x_mtx(path, var_names, make_unique, cache, gex_only). 244 else:. 245 adata = _read_v3_10x_mtx(path, var_names=var_names,. --> 246 make_unique=make_unique, cache=cache). 247 if not gex_only:. 248 return adata. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\readwrite.py in _read_v3_10x_mtx(path, var_names, make_unique, cache). 277 Read mex from output from Cell Ranger v3 or later versions. 278 """""". --> 279 adata = read(os.path.join(path, 'matrix.mtx.gz'), cache=cache).T # transpose the data. 280 genes = pd.read_csv(os.path.join(path, 'features.tsv.gz'), header=None, sep='\t'). 281 if var_names == 'gene_symbols':. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs). 76 return _read(filename, backed=backed, sheet=sheet, ext=ext,. 77 delimiter=delimiter, first_column_names=first_column_names,. ---> 78 backup_url=backup_url, cache=cache, **kwargs). 79 # generate filename and read to dict. 80 filekey = filename. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, supp",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/563
https://github.com/scverse/scanpy/issues/563:1105,performance,cach,cache,1105,"indows. I have an issue when I try to import a dataset and set cache = TRUE. ```pytb. ... writing an h5ad cache file to speedup reading next time. ---------------------------------------------------------------------------. OSError Traceback (most recent call last). <ipython-input-10-894335192e05> in <module>. 2 'C:\\Users\\david\\Desktop\\10x_hiv_mcherry\\aggregate\\outs\\filtered_feature_bc_matrix',. 3 var_names='gene_symbols', # use gene symbols for the variable names (variables-axis index). ----> 4 cache=True) # write a cache file for faster subsequent reading. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\readwrite.py in read_10x_mtx(path, var_names, make_unique, cache, gex_only). 244 else:. 245 adata = _read_v3_10x_mtx(path, var_names=var_names,. --> 246 make_unique=make_unique, cache=cache). 247 if not gex_only:. 248 return adata. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\readwrite.py in _read_v3_10x_mtx(path, var_names, make_unique, cache). 277 Read mex from output from Cell Ranger v3 or later versions. 278 """""". --> 279 adata = read(os.path.join(path, 'matrix.mtx.gz'), cache=cache).T # transpose the data. 280 genes = pd.read_csv(os.path.join(path, 'features.tsv.gz'), header=None, sep='\t'). 281 if var_names == 'gene_symbols':. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs). 76 return _read(filename, backed=backed, sheet=sheet, ext=ext,. 77 delimiter=delimiter, first_column_names=first_column_names,. ---> 78 backup_url=backup_url, cache=cache, **kwargs). 79 # generate filename and read to dict. 80 filekey = filename. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs). 479 'cache file to speedup reading next time'). 480 if not os.path.exists(os.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/563
https://github.com/scverse/scanpy/issues/563:1244,performance,cach,cache,1244,"t time. ---------------------------------------------------------------------------. OSError Traceback (most recent call last). <ipython-input-10-894335192e05> in <module>. 2 'C:\\Users\\david\\Desktop\\10x_hiv_mcherry\\aggregate\\outs\\filtered_feature_bc_matrix',. 3 var_names='gene_symbols', # use gene symbols for the variable names (variables-axis index). ----> 4 cache=True) # write a cache file for faster subsequent reading. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\readwrite.py in read_10x_mtx(path, var_names, make_unique, cache, gex_only). 244 else:. 245 adata = _read_v3_10x_mtx(path, var_names=var_names,. --> 246 make_unique=make_unique, cache=cache). 247 if not gex_only:. 248 return adata. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\readwrite.py in _read_v3_10x_mtx(path, var_names, make_unique, cache). 277 Read mex from output from Cell Ranger v3 or later versions. 278 """""". --> 279 adata = read(os.path.join(path, 'matrix.mtx.gz'), cache=cache).T # transpose the data. 280 genes = pd.read_csv(os.path.join(path, 'features.tsv.gz'), header=None, sep='\t'). 281 if var_names == 'gene_symbols':. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs). 76 return _read(filename, backed=backed, sheet=sheet, ext=ext,. 77 delimiter=delimiter, first_column_names=first_column_names,. ---> 78 backup_url=backup_url, cache=cache, **kwargs). 79 # generate filename and read to dict. 80 filekey = filename. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs). 479 'cache file to speedup reading next time'). 480 if not os.path.exists(os.path.dirname(filename_cache)):. --> 481 os.makedirs(os.path.dirname(filename_cache)). 482 # write for faster reading when calling the next ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/563
https://github.com/scverse/scanpy/issues/563:1250,performance,cach,cache,1250,". ---------------------------------------------------------------------------. OSError Traceback (most recent call last). <ipython-input-10-894335192e05> in <module>. 2 'C:\\Users\\david\\Desktop\\10x_hiv_mcherry\\aggregate\\outs\\filtered_feature_bc_matrix',. 3 var_names='gene_symbols', # use gene symbols for the variable names (variables-axis index). ----> 4 cache=True) # write a cache file for faster subsequent reading. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\readwrite.py in read_10x_mtx(path, var_names, make_unique, cache, gex_only). 244 else:. 245 adata = _read_v3_10x_mtx(path, var_names=var_names,. --> 246 make_unique=make_unique, cache=cache). 247 if not gex_only:. 248 return adata. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\readwrite.py in _read_v3_10x_mtx(path, var_names, make_unique, cache). 277 Read mex from output from Cell Ranger v3 or later versions. 278 """""". --> 279 adata = read(os.path.join(path, 'matrix.mtx.gz'), cache=cache).T # transpose the data. 280 genes = pd.read_csv(os.path.join(path, 'features.tsv.gz'), header=None, sep='\t'). 281 if var_names == 'gene_symbols':. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs). 76 return _read(filename, backed=backed, sheet=sheet, ext=ext,. 77 delimiter=delimiter, first_column_names=first_column_names,. ---> 78 backup_url=backup_url, cache=cache, **kwargs). 79 # generate filename and read to dict. 80 filekey = filename. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs). 479 'cache file to speedup reading next time'). 480 if not os.path.exists(os.path.dirname(filename_cache)):. --> 481 os.makedirs(os.path.dirname(filename_cache)). 482 # write for faster reading when calling the next time. ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/563
https://github.com/scverse/scanpy/issues/563:1564,performance,cach,cache,1564,"e variable names (variables-axis index). ----> 4 cache=True) # write a cache file for faster subsequent reading. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\readwrite.py in read_10x_mtx(path, var_names, make_unique, cache, gex_only). 244 else:. 245 adata = _read_v3_10x_mtx(path, var_names=var_names,. --> 246 make_unique=make_unique, cache=cache). 247 if not gex_only:. 248 return adata. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\readwrite.py in _read_v3_10x_mtx(path, var_names, make_unique, cache). 277 Read mex from output from Cell Ranger v3 or later versions. 278 """""". --> 279 adata = read(os.path.join(path, 'matrix.mtx.gz'), cache=cache).T # transpose the data. 280 genes = pd.read_csv(os.path.join(path, 'features.tsv.gz'), header=None, sep='\t'). 281 if var_names == 'gene_symbols':. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs). 76 return _read(filename, backed=backed, sheet=sheet, ext=ext,. 77 delimiter=delimiter, first_column_names=first_column_names,. ---> 78 backup_url=backup_url, cache=cache, **kwargs). 79 # generate filename and read to dict. 80 filekey = filename. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs). 479 'cache file to speedup reading next time'). 480 if not os.path.exists(os.path.dirname(filename_cache)):. --> 481 os.makedirs(os.path.dirname(filename_cache)). 482 # write for faster reading when calling the next time. 483 adata.write(filename_cache). ~\AppData\Local\conda\conda\envs\Scanpy\lib\os.py in makedirs(name, mode, exist_ok). 208 if head and tail and not path.exists(head):. 209 try:. --> 210 makedirs(head, mode, exist_ok). 211 except FileExistsError:. 212 # Defeats race condition when another thread created the path. ~",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/563
https://github.com/scverse/scanpy/issues/563:1741,performance,cach,cache,1741,"\readwrite.py in read_10x_mtx(path, var_names, make_unique, cache, gex_only). 244 else:. 245 adata = _read_v3_10x_mtx(path, var_names=var_names,. --> 246 make_unique=make_unique, cache=cache). 247 if not gex_only:. 248 return adata. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\readwrite.py in _read_v3_10x_mtx(path, var_names, make_unique, cache). 277 Read mex from output from Cell Ranger v3 or later versions. 278 """""". --> 279 adata = read(os.path.join(path, 'matrix.mtx.gz'), cache=cache).T # transpose the data. 280 genes = pd.read_csv(os.path.join(path, 'features.tsv.gz'), header=None, sep='\t'). 281 if var_names == 'gene_symbols':. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs). 76 return _read(filename, backed=backed, sheet=sheet, ext=ext,. 77 delimiter=delimiter, first_column_names=first_column_names,. ---> 78 backup_url=backup_url, cache=cache, **kwargs). 79 # generate filename and read to dict. 80 filekey = filename. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs). 479 'cache file to speedup reading next time'). 480 if not os.path.exists(os.path.dirname(filename_cache)):. --> 481 os.makedirs(os.path.dirname(filename_cache)). 482 # write for faster reading when calling the next time. 483 adata.write(filename_cache). ~\AppData\Local\conda\conda\envs\Scanpy\lib\os.py in makedirs(name, mode, exist_ok). 208 if head and tail and not path.exists(head):. 209 try:. --> 210 makedirs(head, mode, exist_ok). 211 except FileExistsError:. 212 # Defeats race condition when another thread created the path. ~\AppData\Local\conda\conda\envs\Scanpy\lib\os.py in makedirs(name, mode, exist_ok). 208 if head and tail and not path.exists(head):. 209 try:. --> 210 makedirs(head, mode, exist",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/563
https://github.com/scverse/scanpy/issues/563:1747,performance,cach,cache,1747,"rite.py in read_10x_mtx(path, var_names, make_unique, cache, gex_only). 244 else:. 245 adata = _read_v3_10x_mtx(path, var_names=var_names,. --> 246 make_unique=make_unique, cache=cache). 247 if not gex_only:. 248 return adata. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\readwrite.py in _read_v3_10x_mtx(path, var_names, make_unique, cache). 277 Read mex from output from Cell Ranger v3 or later versions. 278 """""". --> 279 adata = read(os.path.join(path, 'matrix.mtx.gz'), cache=cache).T # transpose the data. 280 genes = pd.read_csv(os.path.join(path, 'features.tsv.gz'), header=None, sep='\t'). 281 if var_names == 'gene_symbols':. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs). 76 return _read(filename, backed=backed, sheet=sheet, ext=ext,. 77 delimiter=delimiter, first_column_names=first_column_names,. ---> 78 backup_url=backup_url, cache=cache, **kwargs). 79 # generate filename and read to dict. 80 filekey = filename. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs). 479 'cache file to speedup reading next time'). 480 if not os.path.exists(os.path.dirname(filename_cache)):. --> 481 os.makedirs(os.path.dirname(filename_cache)). 482 # write for faster reading when calling the next time. 483 adata.write(filename_cache). ~\AppData\Local\conda\conda\envs\Scanpy\lib\os.py in makedirs(name, mode, exist_ok). 208 if head and tail and not path.exists(head):. 209 try:. --> 210 makedirs(head, mode, exist_ok). 211 except FileExistsError:. 212 # Defeats race condition when another thread created the path. ~\AppData\Local\conda\conda\envs\Scanpy\lib\os.py in makedirs(name, mode, exist_ok). 208 if head and tail and not path.exists(head):. 209 try:. --> 210 makedirs(head, mode, exist_ok). ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/563
https://github.com/scverse/scanpy/issues/563:1989,performance,cach,cache,1989,"\conda\conda\envs\Scanpy\lib\site-packages\scanpy\readwrite.py in _read_v3_10x_mtx(path, var_names, make_unique, cache). 277 Read mex from output from Cell Ranger v3 or later versions. 278 """""". --> 279 adata = read(os.path.join(path, 'matrix.mtx.gz'), cache=cache).T # transpose the data. 280 genes = pd.read_csv(os.path.join(path, 'features.tsv.gz'), header=None, sep='\t'). 281 if var_names == 'gene_symbols':. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs). 76 return _read(filename, backed=backed, sheet=sheet, ext=ext,. 77 delimiter=delimiter, first_column_names=first_column_names,. ---> 78 backup_url=backup_url, cache=cache, **kwargs). 79 # generate filename and read to dict. 80 filekey = filename. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs). 479 'cache file to speedup reading next time'). 480 if not os.path.exists(os.path.dirname(filename_cache)):. --> 481 os.makedirs(os.path.dirname(filename_cache)). 482 # write for faster reading when calling the next time. 483 adata.write(filename_cache). ~\AppData\Local\conda\conda\envs\Scanpy\lib\os.py in makedirs(name, mode, exist_ok). 208 if head and tail and not path.exists(head):. 209 try:. --> 210 makedirs(head, mode, exist_ok). 211 except FileExistsError:. 212 # Defeats race condition when another thread created the path. ~\AppData\Local\conda\conda\envs\Scanpy\lib\os.py in makedirs(name, mode, exist_ok). 208 if head and tail and not path.exists(head):. 209 try:. --> 210 makedirs(head, mode, exist_ok). 211 except FileExistsError:. 212 # Defeats race condition when another thread created the path. ~\AppData\Local\conda\conda\envs\Scanpy\lib\os.py in makedirs(name, mode, exist_ok). 208 if head and tail and not path.exists(head):. 209 try:. --",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/563
https://github.com/scverse/scanpy/issues/563:2036,performance,cach,cache,2036,"py\readwrite.py in _read_v3_10x_mtx(path, var_names, make_unique, cache). 277 Read mex from output from Cell Ranger v3 or later versions. 278 """""". --> 279 adata = read(os.path.join(path, 'matrix.mtx.gz'), cache=cache).T # transpose the data. 280 genes = pd.read_csv(os.path.join(path, 'features.tsv.gz'), header=None, sep='\t'). 281 if var_names == 'gene_symbols':. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs). 76 return _read(filename, backed=backed, sheet=sheet, ext=ext,. 77 delimiter=delimiter, first_column_names=first_column_names,. ---> 78 backup_url=backup_url, cache=cache, **kwargs). 79 # generate filename and read to dict. 80 filekey = filename. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs). 479 'cache file to speedup reading next time'). 480 if not os.path.exists(os.path.dirname(filename_cache)):. --> 481 os.makedirs(os.path.dirname(filename_cache)). 482 # write for faster reading when calling the next time. 483 adata.write(filename_cache). ~\AppData\Local\conda\conda\envs\Scanpy\lib\os.py in makedirs(name, mode, exist_ok). 208 if head and tail and not path.exists(head):. 209 try:. --> 210 makedirs(head, mode, exist_ok). 211 except FileExistsError:. 212 # Defeats race condition when another thread created the path. ~\AppData\Local\conda\conda\envs\Scanpy\lib\os.py in makedirs(name, mode, exist_ok). 208 if head and tail and not path.exists(head):. 209 try:. --> 210 makedirs(head, mode, exist_ok). 211 except FileExistsError:. 212 # Defeats race condition when another thread created the path. ~\AppData\Local\conda\conda\envs\Scanpy\lib\os.py in makedirs(name, mode, exist_ok). 208 if head and tail and not path.exists(head):. 209 try:. --> 210 makedirs(head, mode, exist_ok). 211 excep",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/563
https://github.com/scverse/scanpy/issues/563:2071,performance,time,time,2071,"x(path, var_names, make_unique, cache). 277 Read mex from output from Cell Ranger v3 or later versions. 278 """""". --> 279 adata = read(os.path.join(path, 'matrix.mtx.gz'), cache=cache).T # transpose the data. 280 genes = pd.read_csv(os.path.join(path, 'features.tsv.gz'), header=None, sep='\t'). 281 if var_names == 'gene_symbols':. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs). 76 return _read(filename, backed=backed, sheet=sheet, ext=ext,. 77 delimiter=delimiter, first_column_names=first_column_names,. ---> 78 backup_url=backup_url, cache=cache, **kwargs). 79 # generate filename and read to dict. 80 filekey = filename. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs). 479 'cache file to speedup reading next time'). 480 if not os.path.exists(os.path.dirname(filename_cache)):. --> 481 os.makedirs(os.path.dirname(filename_cache)). 482 # write for faster reading when calling the next time. 483 adata.write(filename_cache). ~\AppData\Local\conda\conda\envs\Scanpy\lib\os.py in makedirs(name, mode, exist_ok). 208 if head and tail and not path.exists(head):. 209 try:. --> 210 makedirs(head, mode, exist_ok). 211 except FileExistsError:. 212 # Defeats race condition when another thread created the path. ~\AppData\Local\conda\conda\envs\Scanpy\lib\os.py in makedirs(name, mode, exist_ok). 208 if head and tail and not path.exists(head):. 209 try:. --> 210 makedirs(head, mode, exist_ok). 211 except FileExistsError:. 212 # Defeats race condition when another thread created the path. ~\AppData\Local\conda\conda\envs\Scanpy\lib\os.py in makedirs(name, mode, exist_ok). 208 if head and tail and not path.exists(head):. 209 try:. --> 210 makedirs(head, mode, exist_ok). 211 except FileExistsError:. 212 # Defeats ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/563
https://github.com/scverse/scanpy/issues/563:2247,performance,time,time,2247,"=cache).T # transpose the data. 280 genes = pd.read_csv(os.path.join(path, 'features.tsv.gz'), header=None, sep='\t'). 281 if var_names == 'gene_symbols':. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs). 76 return _read(filename, backed=backed, sheet=sheet, ext=ext,. 77 delimiter=delimiter, first_column_names=first_column_names,. ---> 78 backup_url=backup_url, cache=cache, **kwargs). 79 # generate filename and read to dict. 80 filekey = filename. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs). 479 'cache file to speedup reading next time'). 480 if not os.path.exists(os.path.dirname(filename_cache)):. --> 481 os.makedirs(os.path.dirname(filename_cache)). 482 # write for faster reading when calling the next time. 483 adata.write(filename_cache). ~\AppData\Local\conda\conda\envs\Scanpy\lib\os.py in makedirs(name, mode, exist_ok). 208 if head and tail and not path.exists(head):. 209 try:. --> 210 makedirs(head, mode, exist_ok). 211 except FileExistsError:. 212 # Defeats race condition when another thread created the path. ~\AppData\Local\conda\conda\envs\Scanpy\lib\os.py in makedirs(name, mode, exist_ok). 208 if head and tail and not path.exists(head):. 209 try:. --> 210 makedirs(head, mode, exist_ok). 211 except FileExistsError:. 212 # Defeats race condition when another thread created the path. ~\AppData\Local\conda\conda\envs\Scanpy\lib\os.py in makedirs(name, mode, exist_ok). 208 if head and tail and not path.exists(head):. 209 try:. --> 210 makedirs(head, mode, exist_ok). 211 except FileExistsError:. 212 # Defeats race condition when another thread created the path. ~\AppData\Local\conda\conda\envs\Scanpy\lib\os.py in makedirs(name, mode, exist_ok). 208 if head and tail and not path.exis",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/563
https://github.com/scverse/scanpy/issues/563:4565,performance,cach,cache,4565,"ocal\conda\conda\envs\Scanpy\lib\os.py in makedirs(name, mode, exist_ok). 208 if head and tail and not path.exists(head):. 209 try:. --> 210 makedirs(head, mode, exist_ok). 211 except FileExistsError:. 212 # Defeats race condition when another thread created the path. ~\AppData\Local\conda\conda\envs\Scanpy\lib\os.py in makedirs(name, mode, exist_ok). 208 if head and tail and not path.exists(head):. 209 try:. --> 210 makedirs(head, mode, exist_ok). 211 except FileExistsError:. 212 # Defeats race condition when another thread created the path. ~\AppData\Local\conda\conda\envs\Scanpy\lib\os.py in makedirs(name, mode, exist_ok). 208 if head and tail and not path.exists(head):. 209 try:. --> 210 makedirs(head, mode, exist_ok). 211 except FileExistsError:. 212 # Defeats race condition when another thread created the path. ~\AppData\Local\conda\conda\envs\Scanpy\lib\os.py in makedirs(name, mode, exist_ok). 208 if head and tail and not path.exists(head):. 209 try:. --> 210 makedirs(head, mode, exist_ok). 211 except FileExistsError:. 212 # Defeats race condition when another thread created the path. ~\AppData\Local\conda\conda\envs\Scanpy\lib\os.py in makedirs(name, mode, exist_ok). 208 if head and tail and not path.exists(head):. 209 try:. --> 210 makedirs(head, mode, exist_ok). 211 except FileExistsError:. 212 # Defeats race condition when another thread created the path. ~\AppData\Local\conda\conda\envs\Scanpy\lib\os.py in makedirs(name, mode, exist_ok). 218 return. 219 try:. --> 220 mkdir(name, mode). 221 except OSError:. 222 # Cannot rely on checking for EEXIST, since the operating system. OSError: [WinError 123] The filename, directory name, or volume label syntax is incorrect: './cache/C:'. ```. Looks like the directory name for writing the file is messed up ./cache/C: it is trying to use linux formatting on a Windows machine. It works with cache = FALSE but I wanna try figuring out this problem first before moving on the other parts of the analysis. Any idea? Thanks",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/563
https://github.com/scverse/scanpy/issues/563:4647,performance,cach,cache,4647,"ocal\conda\conda\envs\Scanpy\lib\os.py in makedirs(name, mode, exist_ok). 208 if head and tail and not path.exists(head):. 209 try:. --> 210 makedirs(head, mode, exist_ok). 211 except FileExistsError:. 212 # Defeats race condition when another thread created the path. ~\AppData\Local\conda\conda\envs\Scanpy\lib\os.py in makedirs(name, mode, exist_ok). 208 if head and tail and not path.exists(head):. 209 try:. --> 210 makedirs(head, mode, exist_ok). 211 except FileExistsError:. 212 # Defeats race condition when another thread created the path. ~\AppData\Local\conda\conda\envs\Scanpy\lib\os.py in makedirs(name, mode, exist_ok). 208 if head and tail and not path.exists(head):. 209 try:. --> 210 makedirs(head, mode, exist_ok). 211 except FileExistsError:. 212 # Defeats race condition when another thread created the path. ~\AppData\Local\conda\conda\envs\Scanpy\lib\os.py in makedirs(name, mode, exist_ok). 208 if head and tail and not path.exists(head):. 209 try:. --> 210 makedirs(head, mode, exist_ok). 211 except FileExistsError:. 212 # Defeats race condition when another thread created the path. ~\AppData\Local\conda\conda\envs\Scanpy\lib\os.py in makedirs(name, mode, exist_ok). 208 if head and tail and not path.exists(head):. 209 try:. --> 210 makedirs(head, mode, exist_ok). 211 except FileExistsError:. 212 # Defeats race condition when another thread created the path. ~\AppData\Local\conda\conda\envs\Scanpy\lib\os.py in makedirs(name, mode, exist_ok). 218 return. 219 try:. --> 220 mkdir(name, mode). 221 except OSError:. 222 # Cannot rely on checking for EEXIST, since the operating system. OSError: [WinError 123] The filename, directory name, or volume label syntax is incorrect: './cache/C:'. ```. Looks like the directory name for writing the file is messed up ./cache/C: it is trying to use linux formatting on a Windows machine. It works with cache = FALSE but I wanna try figuring out this problem first before moving on the other parts of the analysis. Any idea? Thanks",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/563
https://github.com/scverse/scanpy/issues/563:4729,performance,cach,cache,4729,"ocal\conda\conda\envs\Scanpy\lib\os.py in makedirs(name, mode, exist_ok). 208 if head and tail and not path.exists(head):. 209 try:. --> 210 makedirs(head, mode, exist_ok). 211 except FileExistsError:. 212 # Defeats race condition when another thread created the path. ~\AppData\Local\conda\conda\envs\Scanpy\lib\os.py in makedirs(name, mode, exist_ok). 208 if head and tail and not path.exists(head):. 209 try:. --> 210 makedirs(head, mode, exist_ok). 211 except FileExistsError:. 212 # Defeats race condition when another thread created the path. ~\AppData\Local\conda\conda\envs\Scanpy\lib\os.py in makedirs(name, mode, exist_ok). 208 if head and tail and not path.exists(head):. 209 try:. --> 210 makedirs(head, mode, exist_ok). 211 except FileExistsError:. 212 # Defeats race condition when another thread created the path. ~\AppData\Local\conda\conda\envs\Scanpy\lib\os.py in makedirs(name, mode, exist_ok). 208 if head and tail and not path.exists(head):. 209 try:. --> 210 makedirs(head, mode, exist_ok). 211 except FileExistsError:. 212 # Defeats race condition when another thread created the path. ~\AppData\Local\conda\conda\envs\Scanpy\lib\os.py in makedirs(name, mode, exist_ok). 208 if head and tail and not path.exists(head):. 209 try:. --> 210 makedirs(head, mode, exist_ok). 211 except FileExistsError:. 212 # Defeats race condition when another thread created the path. ~\AppData\Local\conda\conda\envs\Scanpy\lib\os.py in makedirs(name, mode, exist_ok). 218 return. 219 try:. --> 220 mkdir(name, mode). 221 except OSError:. 222 # Cannot rely on checking for EEXIST, since the operating system. OSError: [WinError 123] The filename, directory name, or volume label syntax is incorrect: './cache/C:'. ```. Looks like the directory name for writing the file is messed up ./cache/C: it is trying to use linux formatting on a Windows machine. It works with cache = FALSE but I wanna try figuring out this problem first before moving on the other parts of the analysis. Any idea? Thanks",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/563
https://github.com/scverse/scanpy/issues/563:384,safety,input,input-,384,"OSError: [WinError 123] When importing 10x data with cache=TRUE; Hi, . I'm running Scanpy through Conda on Windows. I have an issue when I try to import a dataset and set cache = TRUE. ```pytb. ... writing an h5ad cache file to speedup reading next time. ---------------------------------------------------------------------------. OSError Traceback (most recent call last). <ipython-input-10-894335192e05> in <module>. 2 'C:\\Users\\david\\Desktop\\10x_hiv_mcherry\\aggregate\\outs\\filtered_feature_bc_matrix',. 3 var_names='gene_symbols', # use gene symbols for the variable names (variables-axis index). ----> 4 cache=True) # write a cache file for faster subsequent reading. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\readwrite.py in read_10x_mtx(path, var_names, make_unique, cache, gex_only). 244 else:. 245 adata = _read_v3_10x_mtx(path, var_names=var_names,. --> 246 make_unique=make_unique, cache=cache). 247 if not gex_only:. 248 return adata. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\readwrite.py in _read_v3_10x_mtx(path, var_names, make_unique, cache). 277 Read mex from output from Cell Ranger v3 or later versions. 278 """""". --> 279 adata = read(os.path.join(path, 'matrix.mtx.gz'), cache=cache).T # transpose the data. 280 genes = pd.read_csv(os.path.join(path, 'features.tsv.gz'), header=None, sep='\t'). 281 if var_names == 'gene_symbols':. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs). 76 return _read(filename, backed=backed, sheet=sheet, ext=ext,. 77 delimiter=delimiter, first_column_names=first_column_names,. ---> 78 backup_url=backup_url, cache=cache, **kwargs). 79 # generate filename and read to dict. 80 filekey = filename. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, supp",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/563
https://github.com/scverse/scanpy/issues/563:411,safety,modul,module,411,"OSError: [WinError 123] When importing 10x data with cache=TRUE; Hi, . I'm running Scanpy through Conda on Windows. I have an issue when I try to import a dataset and set cache = TRUE. ```pytb. ... writing an h5ad cache file to speedup reading next time. ---------------------------------------------------------------------------. OSError Traceback (most recent call last). <ipython-input-10-894335192e05> in <module>. 2 'C:\\Users\\david\\Desktop\\10x_hiv_mcherry\\aggregate\\outs\\filtered_feature_bc_matrix',. 3 var_names='gene_symbols', # use gene symbols for the variable names (variables-axis index). ----> 4 cache=True) # write a cache file for faster subsequent reading. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\readwrite.py in read_10x_mtx(path, var_names, make_unique, cache, gex_only). 244 else:. 245 adata = _read_v3_10x_mtx(path, var_names=var_names,. --> 246 make_unique=make_unique, cache=cache). 247 if not gex_only:. 248 return adata. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\readwrite.py in _read_v3_10x_mtx(path, var_names, make_unique, cache). 277 Read mex from output from Cell Ranger v3 or later versions. 278 """""". --> 279 adata = read(os.path.join(path, 'matrix.mtx.gz'), cache=cache).T # transpose the data. 280 genes = pd.read_csv(os.path.join(path, 'features.tsv.gz'), header=None, sep='\t'). 281 if var_names == 'gene_symbols':. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs). 76 return _read(filename, backed=backed, sheet=sheet, ext=ext,. 77 delimiter=delimiter, first_column_names=first_column_names,. ---> 78 backup_url=backup_url, cache=cache, **kwargs). 79 # generate filename and read to dict. 80 filekey = filename. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, supp",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/563
https://github.com/scverse/scanpy/issues/563:2474,safety,except,except,2474,"te.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs). 76 return _read(filename, backed=backed, sheet=sheet, ext=ext,. 77 delimiter=delimiter, first_column_names=first_column_names,. ---> 78 backup_url=backup_url, cache=cache, **kwargs). 79 # generate filename and read to dict. 80 filekey = filename. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs). 479 'cache file to speedup reading next time'). 480 if not os.path.exists(os.path.dirname(filename_cache)):. --> 481 os.makedirs(os.path.dirname(filename_cache)). 482 # write for faster reading when calling the next time. 483 adata.write(filename_cache). ~\AppData\Local\conda\conda\envs\Scanpy\lib\os.py in makedirs(name, mode, exist_ok). 208 if head and tail and not path.exists(head):. 209 try:. --> 210 makedirs(head, mode, exist_ok). 211 except FileExistsError:. 212 # Defeats race condition when another thread created the path. ~\AppData\Local\conda\conda\envs\Scanpy\lib\os.py in makedirs(name, mode, exist_ok). 208 if head and tail and not path.exists(head):. 209 try:. --> 210 makedirs(head, mode, exist_ok). 211 except FileExistsError:. 212 # Defeats race condition when another thread created the path. ~\AppData\Local\conda\conda\envs\Scanpy\lib\os.py in makedirs(name, mode, exist_ok). 208 if head and tail and not path.exists(head):. 209 try:. --> 210 makedirs(head, mode, exist_ok). 211 except FileExistsError:. 212 # Defeats race condition when another thread created the path. ~\AppData\Local\conda\conda\envs\Scanpy\lib\os.py in makedirs(name, mode, exist_ok). 208 if head and tail and not path.exists(head):. 209 try:. --> 210 makedirs(head, mode, exist_ok). 211 except FileExistsError:. 212 # Defeats race condition when another thread created the path. ~\AppData\Local\conda\conda\envs\Scanpy\lib\os.py in makedirs(name, mod",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/563
https://github.com/scverse/scanpy/issues/563:2754,safety,except,except,2754,"wargs). 79 # generate filename and read to dict. 80 filekey = filename. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs). 479 'cache file to speedup reading next time'). 480 if not os.path.exists(os.path.dirname(filename_cache)):. --> 481 os.makedirs(os.path.dirname(filename_cache)). 482 # write for faster reading when calling the next time. 483 adata.write(filename_cache). ~\AppData\Local\conda\conda\envs\Scanpy\lib\os.py in makedirs(name, mode, exist_ok). 208 if head and tail and not path.exists(head):. 209 try:. --> 210 makedirs(head, mode, exist_ok). 211 except FileExistsError:. 212 # Defeats race condition when another thread created the path. ~\AppData\Local\conda\conda\envs\Scanpy\lib\os.py in makedirs(name, mode, exist_ok). 208 if head and tail and not path.exists(head):. 209 try:. --> 210 makedirs(head, mode, exist_ok). 211 except FileExistsError:. 212 # Defeats race condition when another thread created the path. ~\AppData\Local\conda\conda\envs\Scanpy\lib\os.py in makedirs(name, mode, exist_ok). 208 if head and tail and not path.exists(head):. 209 try:. --> 210 makedirs(head, mode, exist_ok). 211 except FileExistsError:. 212 # Defeats race condition when another thread created the path. ~\AppData\Local\conda\conda\envs\Scanpy\lib\os.py in makedirs(name, mode, exist_ok). 208 if head and tail and not path.exists(head):. 209 try:. --> 210 makedirs(head, mode, exist_ok). 211 except FileExistsError:. 212 # Defeats race condition when another thread created the path. ~\AppData\Local\conda\conda\envs\Scanpy\lib\os.py in makedirs(name, mode, exist_ok). 208 if head and tail and not path.exists(head):. 209 try:. --> 210 makedirs(head, mode, exist_ok). 211 except FileExistsError:. 212 # Defeats race condition when another thread created the path. ~\AppData\Local\conda\conda\envs\Scanpy\lib\os.py in makedirs(name, mod",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/563
https://github.com/scverse/scanpy/issues/563:3034,safety,except,except,3034,"ache file to speedup reading next time'). 480 if not os.path.exists(os.path.dirname(filename_cache)):. --> 481 os.makedirs(os.path.dirname(filename_cache)). 482 # write for faster reading when calling the next time. 483 adata.write(filename_cache). ~\AppData\Local\conda\conda\envs\Scanpy\lib\os.py in makedirs(name, mode, exist_ok). 208 if head and tail and not path.exists(head):. 209 try:. --> 210 makedirs(head, mode, exist_ok). 211 except FileExistsError:. 212 # Defeats race condition when another thread created the path. ~\AppData\Local\conda\conda\envs\Scanpy\lib\os.py in makedirs(name, mode, exist_ok). 208 if head and tail and not path.exists(head):. 209 try:. --> 210 makedirs(head, mode, exist_ok). 211 except FileExistsError:. 212 # Defeats race condition when another thread created the path. ~\AppData\Local\conda\conda\envs\Scanpy\lib\os.py in makedirs(name, mode, exist_ok). 208 if head and tail and not path.exists(head):. 209 try:. --> 210 makedirs(head, mode, exist_ok). 211 except FileExistsError:. 212 # Defeats race condition when another thread created the path. ~\AppData\Local\conda\conda\envs\Scanpy\lib\os.py in makedirs(name, mode, exist_ok). 208 if head and tail and not path.exists(head):. 209 try:. --> 210 makedirs(head, mode, exist_ok). 211 except FileExistsError:. 212 # Defeats race condition when another thread created the path. ~\AppData\Local\conda\conda\envs\Scanpy\lib\os.py in makedirs(name, mode, exist_ok). 208 if head and tail and not path.exists(head):. 209 try:. --> 210 makedirs(head, mode, exist_ok). 211 except FileExistsError:. 212 # Defeats race condition when another thread created the path. ~\AppData\Local\conda\conda\envs\Scanpy\lib\os.py in makedirs(name, mode, exist_ok). 208 if head and tail and not path.exists(head):. 209 try:. --> 210 makedirs(head, mode, exist_ok). 211 except FileExistsError:. 212 # Defeats race condition when another thread created the path. ~\AppData\Local\conda\conda\envs\Scanpy\lib\os.py in makedirs(name, mod",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/563
https://github.com/scverse/scanpy/issues/563:3314,safety,except,except,3314,"s\Scanpy\lib\os.py in makedirs(name, mode, exist_ok). 208 if head and tail and not path.exists(head):. 209 try:. --> 210 makedirs(head, mode, exist_ok). 211 except FileExistsError:. 212 # Defeats race condition when another thread created the path. ~\AppData\Local\conda\conda\envs\Scanpy\lib\os.py in makedirs(name, mode, exist_ok). 208 if head and tail and not path.exists(head):. 209 try:. --> 210 makedirs(head, mode, exist_ok). 211 except FileExistsError:. 212 # Defeats race condition when another thread created the path. ~\AppData\Local\conda\conda\envs\Scanpy\lib\os.py in makedirs(name, mode, exist_ok). 208 if head and tail and not path.exists(head):. 209 try:. --> 210 makedirs(head, mode, exist_ok). 211 except FileExistsError:. 212 # Defeats race condition when another thread created the path. ~\AppData\Local\conda\conda\envs\Scanpy\lib\os.py in makedirs(name, mode, exist_ok). 208 if head and tail and not path.exists(head):. 209 try:. --> 210 makedirs(head, mode, exist_ok). 211 except FileExistsError:. 212 # Defeats race condition when another thread created the path. ~\AppData\Local\conda\conda\envs\Scanpy\lib\os.py in makedirs(name, mode, exist_ok). 208 if head and tail and not path.exists(head):. 209 try:. --> 210 makedirs(head, mode, exist_ok). 211 except FileExistsError:. 212 # Defeats race condition when another thread created the path. ~\AppData\Local\conda\conda\envs\Scanpy\lib\os.py in makedirs(name, mode, exist_ok). 208 if head and tail and not path.exists(head):. 209 try:. --> 210 makedirs(head, mode, exist_ok). 211 except FileExistsError:. 212 # Defeats race condition when another thread created the path. ~\AppData\Local\conda\conda\envs\Scanpy\lib\os.py in makedirs(name, mode, exist_ok). 208 if head and tail and not path.exists(head):. 209 try:. --> 210 makedirs(head, mode, exist_ok). 211 except FileExistsError:. 212 # Defeats race condition when another thread created the path. ~\AppData\Local\conda\conda\envs\Scanpy\lib\os.py in makedirs(name, mod",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/563
https://github.com/scverse/scanpy/issues/563:3594,safety,except,except,3594,"s\Scanpy\lib\os.py in makedirs(name, mode, exist_ok). 208 if head and tail and not path.exists(head):. 209 try:. --> 210 makedirs(head, mode, exist_ok). 211 except FileExistsError:. 212 # Defeats race condition when another thread created the path. ~\AppData\Local\conda\conda\envs\Scanpy\lib\os.py in makedirs(name, mode, exist_ok). 208 if head and tail and not path.exists(head):. 209 try:. --> 210 makedirs(head, mode, exist_ok). 211 except FileExistsError:. 212 # Defeats race condition when another thread created the path. ~\AppData\Local\conda\conda\envs\Scanpy\lib\os.py in makedirs(name, mode, exist_ok). 208 if head and tail and not path.exists(head):. 209 try:. --> 210 makedirs(head, mode, exist_ok). 211 except FileExistsError:. 212 # Defeats race condition when another thread created the path. ~\AppData\Local\conda\conda\envs\Scanpy\lib\os.py in makedirs(name, mode, exist_ok). 208 if head and tail and not path.exists(head):. 209 try:. --> 210 makedirs(head, mode, exist_ok). 211 except FileExistsError:. 212 # Defeats race condition when another thread created the path. ~\AppData\Local\conda\conda\envs\Scanpy\lib\os.py in makedirs(name, mode, exist_ok). 208 if head and tail and not path.exists(head):. 209 try:. --> 210 makedirs(head, mode, exist_ok). 211 except FileExistsError:. 212 # Defeats race condition when another thread created the path. ~\AppData\Local\conda\conda\envs\Scanpy\lib\os.py in makedirs(name, mode, exist_ok). 208 if head and tail and not path.exists(head):. 209 try:. --> 210 makedirs(head, mode, exist_ok). 211 except FileExistsError:. 212 # Defeats race condition when another thread created the path. ~\AppData\Local\conda\conda\envs\Scanpy\lib\os.py in makedirs(name, mode, exist_ok). 218 return. 219 try:. --> 220 mkdir(name, mode). 221 except OSError:. 222 # Cannot rely on checking for EEXIST, since the operating system. OSError: [WinError 123] The filename, directory name, or volume label syntax is incorrect: './cache/C:'. ```. Looks like the d",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/563
https://github.com/scverse/scanpy/issues/563:3874,safety,except,except,3874,"ocal\conda\conda\envs\Scanpy\lib\os.py in makedirs(name, mode, exist_ok). 208 if head and tail and not path.exists(head):. 209 try:. --> 210 makedirs(head, mode, exist_ok). 211 except FileExistsError:. 212 # Defeats race condition when another thread created the path. ~\AppData\Local\conda\conda\envs\Scanpy\lib\os.py in makedirs(name, mode, exist_ok). 208 if head and tail and not path.exists(head):. 209 try:. --> 210 makedirs(head, mode, exist_ok). 211 except FileExistsError:. 212 # Defeats race condition when another thread created the path. ~\AppData\Local\conda\conda\envs\Scanpy\lib\os.py in makedirs(name, mode, exist_ok). 208 if head and tail and not path.exists(head):. 209 try:. --> 210 makedirs(head, mode, exist_ok). 211 except FileExistsError:. 212 # Defeats race condition when another thread created the path. ~\AppData\Local\conda\conda\envs\Scanpy\lib\os.py in makedirs(name, mode, exist_ok). 208 if head and tail and not path.exists(head):. 209 try:. --> 210 makedirs(head, mode, exist_ok). 211 except FileExistsError:. 212 # Defeats race condition when another thread created the path. ~\AppData\Local\conda\conda\envs\Scanpy\lib\os.py in makedirs(name, mode, exist_ok). 208 if head and tail and not path.exists(head):. 209 try:. --> 210 makedirs(head, mode, exist_ok). 211 except FileExistsError:. 212 # Defeats race condition when another thread created the path. ~\AppData\Local\conda\conda\envs\Scanpy\lib\os.py in makedirs(name, mode, exist_ok). 218 return. 219 try:. --> 220 mkdir(name, mode). 221 except OSError:. 222 # Cannot rely on checking for EEXIST, since the operating system. OSError: [WinError 123] The filename, directory name, or volume label syntax is incorrect: './cache/C:'. ```. Looks like the directory name for writing the file is messed up ./cache/C: it is trying to use linux formatting on a Windows machine. It works with cache = FALSE but I wanna try figuring out this problem first before moving on the other parts of the analysis. Any idea? Thanks",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/563
https://github.com/scverse/scanpy/issues/563:4154,safety,except,except,4154,"ocal\conda\conda\envs\Scanpy\lib\os.py in makedirs(name, mode, exist_ok). 208 if head and tail and not path.exists(head):. 209 try:. --> 210 makedirs(head, mode, exist_ok). 211 except FileExistsError:. 212 # Defeats race condition when another thread created the path. ~\AppData\Local\conda\conda\envs\Scanpy\lib\os.py in makedirs(name, mode, exist_ok). 208 if head and tail and not path.exists(head):. 209 try:. --> 210 makedirs(head, mode, exist_ok). 211 except FileExistsError:. 212 # Defeats race condition when another thread created the path. ~\AppData\Local\conda\conda\envs\Scanpy\lib\os.py in makedirs(name, mode, exist_ok). 208 if head and tail and not path.exists(head):. 209 try:. --> 210 makedirs(head, mode, exist_ok). 211 except FileExistsError:. 212 # Defeats race condition when another thread created the path. ~\AppData\Local\conda\conda\envs\Scanpy\lib\os.py in makedirs(name, mode, exist_ok). 208 if head and tail and not path.exists(head):. 209 try:. --> 210 makedirs(head, mode, exist_ok). 211 except FileExistsError:. 212 # Defeats race condition when another thread created the path. ~\AppData\Local\conda\conda\envs\Scanpy\lib\os.py in makedirs(name, mode, exist_ok). 208 if head and tail and not path.exists(head):. 209 try:. --> 210 makedirs(head, mode, exist_ok). 211 except FileExistsError:. 212 # Defeats race condition when another thread created the path. ~\AppData\Local\conda\conda\envs\Scanpy\lib\os.py in makedirs(name, mode, exist_ok). 218 return. 219 try:. --> 220 mkdir(name, mode). 221 except OSError:. 222 # Cannot rely on checking for EEXIST, since the operating system. OSError: [WinError 123] The filename, directory name, or volume label syntax is incorrect: './cache/C:'. ```. Looks like the directory name for writing the file is messed up ./cache/C: it is trying to use linux formatting on a Windows machine. It works with cache = FALSE but I wanna try figuring out this problem first before moving on the other parts of the analysis. Any idea? Thanks",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/563
https://github.com/scverse/scanpy/issues/563:4384,safety,except,except,4384,"ocal\conda\conda\envs\Scanpy\lib\os.py in makedirs(name, mode, exist_ok). 208 if head and tail and not path.exists(head):. 209 try:. --> 210 makedirs(head, mode, exist_ok). 211 except FileExistsError:. 212 # Defeats race condition when another thread created the path. ~\AppData\Local\conda\conda\envs\Scanpy\lib\os.py in makedirs(name, mode, exist_ok). 208 if head and tail and not path.exists(head):. 209 try:. --> 210 makedirs(head, mode, exist_ok). 211 except FileExistsError:. 212 # Defeats race condition when another thread created the path. ~\AppData\Local\conda\conda\envs\Scanpy\lib\os.py in makedirs(name, mode, exist_ok). 208 if head and tail and not path.exists(head):. 209 try:. --> 210 makedirs(head, mode, exist_ok). 211 except FileExistsError:. 212 # Defeats race condition when another thread created the path. ~\AppData\Local\conda\conda\envs\Scanpy\lib\os.py in makedirs(name, mode, exist_ok). 208 if head and tail and not path.exists(head):. 209 try:. --> 210 makedirs(head, mode, exist_ok). 211 except FileExistsError:. 212 # Defeats race condition when another thread created the path. ~\AppData\Local\conda\conda\envs\Scanpy\lib\os.py in makedirs(name, mode, exist_ok). 208 if head and tail and not path.exists(head):. 209 try:. --> 210 makedirs(head, mode, exist_ok). 211 except FileExistsError:. 212 # Defeats race condition when another thread created the path. ~\AppData\Local\conda\conda\envs\Scanpy\lib\os.py in makedirs(name, mode, exist_ok). 218 return. 219 try:. --> 220 mkdir(name, mode). 221 except OSError:. 222 # Cannot rely on checking for EEXIST, since the operating system. OSError: [WinError 123] The filename, directory name, or volume label syntax is incorrect: './cache/C:'. ```. Looks like the directory name for writing the file is messed up ./cache/C: it is trying to use linux formatting on a Windows machine. It works with cache = FALSE but I wanna try figuring out this problem first before moving on the other parts of the analysis. Any idea? Thanks",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/563
https://github.com/scverse/scanpy/issues/563:340,testability,Trace,Traceback,340,"OSError: [WinError 123] When importing 10x data with cache=TRUE; Hi, . I'm running Scanpy through Conda on Windows. I have an issue when I try to import a dataset and set cache = TRUE. ```pytb. ... writing an h5ad cache file to speedup reading next time. ---------------------------------------------------------------------------. OSError Traceback (most recent call last). <ipython-input-10-894335192e05> in <module>. 2 'C:\\Users\\david\\Desktop\\10x_hiv_mcherry\\aggregate\\outs\\filtered_feature_bc_matrix',. 3 var_names='gene_symbols', # use gene symbols for the variable names (variables-axis index). ----> 4 cache=True) # write a cache file for faster subsequent reading. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\readwrite.py in read_10x_mtx(path, var_names, make_unique, cache, gex_only). 244 else:. 245 adata = _read_v3_10x_mtx(path, var_names=var_names,. --> 246 make_unique=make_unique, cache=cache). 247 if not gex_only:. 248 return adata. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\readwrite.py in _read_v3_10x_mtx(path, var_names, make_unique, cache). 277 Read mex from output from Cell Ranger v3 or later versions. 278 """""". --> 279 adata = read(os.path.join(path, 'matrix.mtx.gz'), cache=cache).T # transpose the data. 280 genes = pd.read_csv(os.path.join(path, 'features.tsv.gz'), header=None, sep='\t'). 281 if var_names == 'gene_symbols':. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs). 76 return _read(filename, backed=backed, sheet=sheet, ext=ext,. 77 delimiter=delimiter, first_column_names=first_column_names,. ---> 78 backup_url=backup_url, cache=cache, **kwargs). 79 # generate filename and read to dict. 80 filekey = filename. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, supp",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/563
https://github.com/scverse/scanpy/issues/563:384,usability,input,input-,384,"OSError: [WinError 123] When importing 10x data with cache=TRUE; Hi, . I'm running Scanpy through Conda on Windows. I have an issue when I try to import a dataset and set cache = TRUE. ```pytb. ... writing an h5ad cache file to speedup reading next time. ---------------------------------------------------------------------------. OSError Traceback (most recent call last). <ipython-input-10-894335192e05> in <module>. 2 'C:\\Users\\david\\Desktop\\10x_hiv_mcherry\\aggregate\\outs\\filtered_feature_bc_matrix',. 3 var_names='gene_symbols', # use gene symbols for the variable names (variables-axis index). ----> 4 cache=True) # write a cache file for faster subsequent reading. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\readwrite.py in read_10x_mtx(path, var_names, make_unique, cache, gex_only). 244 else:. 245 adata = _read_v3_10x_mtx(path, var_names=var_names,. --> 246 make_unique=make_unique, cache=cache). 247 if not gex_only:. 248 return adata. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\readwrite.py in _read_v3_10x_mtx(path, var_names, make_unique, cache). 277 Read mex from output from Cell Ranger v3 or later versions. 278 """""". --> 279 adata = read(os.path.join(path, 'matrix.mtx.gz'), cache=cache).T # transpose the data. 280 genes = pd.read_csv(os.path.join(path, 'features.tsv.gz'), header=None, sep='\t'). 281 if var_names == 'gene_symbols':. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs). 76 return _read(filename, backed=backed, sheet=sheet, ext=ext,. 77 delimiter=delimiter, first_column_names=first_column_names,. ---> 78 backup_url=backup_url, cache=cache, **kwargs). 79 # generate filename and read to dict. 80 filekey = filename. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, supp",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/563
https://github.com/scverse/scanpy/issues/563:427,usability,User,Users,427,"OSError: [WinError 123] When importing 10x data with cache=TRUE; Hi, . I'm running Scanpy through Conda on Windows. I have an issue when I try to import a dataset and set cache = TRUE. ```pytb. ... writing an h5ad cache file to speedup reading next time. ---------------------------------------------------------------------------. OSError Traceback (most recent call last). <ipython-input-10-894335192e05> in <module>. 2 'C:\\Users\\david\\Desktop\\10x_hiv_mcherry\\aggregate\\outs\\filtered_feature_bc_matrix',. 3 var_names='gene_symbols', # use gene symbols for the variable names (variables-axis index). ----> 4 cache=True) # write a cache file for faster subsequent reading. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\readwrite.py in read_10x_mtx(path, var_names, make_unique, cache, gex_only). 244 else:. 245 adata = _read_v3_10x_mtx(path, var_names=var_names,. --> 246 make_unique=make_unique, cache=cache). 247 if not gex_only:. 248 return adata. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\readwrite.py in _read_v3_10x_mtx(path, var_names, make_unique, cache). 277 Read mex from output from Cell Ranger v3 or later versions. 278 """""". --> 279 adata = read(os.path.join(path, 'matrix.mtx.gz'), cache=cache).T # transpose the data. 280 genes = pd.read_csv(os.path.join(path, 'features.tsv.gz'), header=None, sep='\t'). 281 if var_names == 'gene_symbols':. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs). 76 return _read(filename, backed=backed, sheet=sheet, ext=ext,. 77 delimiter=delimiter, first_column_names=first_column_names,. ---> 78 backup_url=backup_url, cache=cache, **kwargs). 79 # generate filename and read to dict. 80 filekey = filename. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, supp",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/563
https://github.com/scverse/scanpy/pull/564:43,availability,error,error,43,Use pathlib in readwrite module; It’s less error prone and a nicer API. Fixes #563,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/564
https://github.com/scverse/scanpy/pull/564:25,deployability,modul,module,25,Use pathlib in readwrite module; It’s less error prone and a nicer API. Fixes #563,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/564
https://github.com/scverse/scanpy/pull/564:67,deployability,API,API,67,Use pathlib in readwrite module; It’s less error prone and a nicer API. Fixes #563,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/564
https://github.com/scverse/scanpy/pull/564:67,integrability,API,API,67,Use pathlib in readwrite module; It’s less error prone and a nicer API. Fixes #563,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/564
https://github.com/scverse/scanpy/pull/564:67,interoperability,API,API,67,Use pathlib in readwrite module; It’s less error prone and a nicer API. Fixes #563,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/564
https://github.com/scverse/scanpy/pull/564:25,modifiability,modul,module,25,Use pathlib in readwrite module; It’s less error prone and a nicer API. Fixes #563,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/564
https://github.com/scverse/scanpy/pull/564:43,performance,error,error,43,Use pathlib in readwrite module; It’s less error prone and a nicer API. Fixes #563,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/564
https://github.com/scverse/scanpy/pull/564:25,safety,modul,module,25,Use pathlib in readwrite module; It’s less error prone and a nicer API. Fixes #563,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/564
https://github.com/scverse/scanpy/pull/564:43,safety,error,error,43,Use pathlib in readwrite module; It’s less error prone and a nicer API. Fixes #563,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/564
https://github.com/scverse/scanpy/pull/564:43,usability,error,error,43,Use pathlib in readwrite module; It’s less error prone and a nicer API. Fixes #563,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/564
https://github.com/scverse/scanpy/issues/565:228,availability,error,error,228,"standard_scale = 'var' AttributeError; Hi, . I have an issue with the standard_scale ='var' function. Whenever I try to make any plot and scaling the data from 0 to 1 with the standard_scale = 'var' function I get the following error:. ```. ---------------------------------------------------------------------------. AttributeError Traceback (most recent call last). <ipython-input-432-bef389f3fd99> in <module>. ----> 1 gs = sc.pl.matrixplot(adata, marker_genes, groupby='louvain', dendrogram=True, standard_scale='var'). ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\plotting\_anndata.py in matrixplot(adata, var_names, groupby, use_raw, log, num_categories, figsize, dendrogram, var_group_positions, var_group_labels, var_group_rotation, layer, swap_axes, show, save, **kwds). 1683 _plot_dendrogram(dendro_ax, adata, ticks=y_ticks). 1684 . -> 1685 pc = matrix_ax.pcolor(mean_obs, edgecolor='gray', **kwds). 1686 . 1687 # invert y axis to show categories ordered from top to bottom. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\matplotlib\__init__.py in inner(ax, data, *args, **kwargs). 1808 ""the Matplotlib list!)"" % (label_namer, func.__name__),. 1809 RuntimeWarning, stacklevel=2). -> 1810 return func(ax, *args, **kwargs). 1811 . 1812 inner.__doc__ = _add_data_doc(inner.__doc__,. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\matplotlib\axes\_axes.py in pcolor(self, alpha, norm, cmap, vmin, vmax, *args, **kwargs). 5773 kwargs.setdefault('snap', False). 5774 . -> 5775 collection = mcoll.PolyCollection(verts, **kwargs). 5776 . 5777 collection.set_alpha(alpha). ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\matplotlib\collections.py in __init__(self, verts, sizes, closed, **kwargs). 931 %(Collection)s. 932 """""". --> 933 Collection.__init__(self, **kwargs). 934 self.set_sizes(sizes). 935 self.set_verts(verts, closed). ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\matplotlib\collections.py in __init__(self, edgecolors,",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/565
https://github.com/scverse/scanpy/issues/565:405,deployability,modul,module,405,"standard_scale = 'var' AttributeError; Hi, . I have an issue with the standard_scale ='var' function. Whenever I try to make any plot and scaling the data from 0 to 1 with the standard_scale = 'var' function I get the following error:. ```. ---------------------------------------------------------------------------. AttributeError Traceback (most recent call last). <ipython-input-432-bef389f3fd99> in <module>. ----> 1 gs = sc.pl.matrixplot(adata, marker_genes, groupby='louvain', dendrogram=True, standard_scale='var'). ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\plotting\_anndata.py in matrixplot(adata, var_names, groupby, use_raw, log, num_categories, figsize, dendrogram, var_group_positions, var_group_labels, var_group_rotation, layer, swap_axes, show, save, **kwds). 1683 _plot_dendrogram(dendro_ax, adata, ticks=y_ticks). 1684 . -> 1685 pc = matrix_ax.pcolor(mean_obs, edgecolor='gray', **kwds). 1686 . 1687 # invert y axis to show categories ordered from top to bottom. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\matplotlib\__init__.py in inner(ax, data, *args, **kwargs). 1808 ""the Matplotlib list!)"" % (label_namer, func.__name__),. 1809 RuntimeWarning, stacklevel=2). -> 1810 return func(ax, *args, **kwargs). 1811 . 1812 inner.__doc__ = _add_data_doc(inner.__doc__,. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\matplotlib\axes\_axes.py in pcolor(self, alpha, norm, cmap, vmin, vmax, *args, **kwargs). 5773 kwargs.setdefault('snap', False). 5774 . -> 5775 collection = mcoll.PolyCollection(verts, **kwargs). 5776 . 5777 collection.set_alpha(alpha). ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\matplotlib\collections.py in __init__(self, verts, sizes, closed, **kwargs). 931 %(Collection)s. 932 """""". --> 933 Collection.__init__(self, **kwargs). 934 self.set_sizes(sizes). 935 self.set_verts(verts, closed). ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\matplotlib\collections.py in __init__(self, edgecolors,",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/565
https://github.com/scverse/scanpy/issues/565:660,deployability,log,log,660,"standard_scale = 'var' AttributeError; Hi, . I have an issue with the standard_scale ='var' function. Whenever I try to make any plot and scaling the data from 0 to 1 with the standard_scale = 'var' function I get the following error:. ```. ---------------------------------------------------------------------------. AttributeError Traceback (most recent call last). <ipython-input-432-bef389f3fd99> in <module>. ----> 1 gs = sc.pl.matrixplot(adata, marker_genes, groupby='louvain', dendrogram=True, standard_scale='var'). ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\plotting\_anndata.py in matrixplot(adata, var_names, groupby, use_raw, log, num_categories, figsize, dendrogram, var_group_positions, var_group_labels, var_group_rotation, layer, swap_axes, show, save, **kwds). 1683 _plot_dendrogram(dendro_ax, adata, ticks=y_ticks). 1684 . -> 1685 pc = matrix_ax.pcolor(mean_obs, edgecolor='gray', **kwds). 1686 . 1687 # invert y axis to show categories ordered from top to bottom. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\matplotlib\__init__.py in inner(ax, data, *args, **kwargs). 1808 ""the Matplotlib list!)"" % (label_namer, func.__name__),. 1809 RuntimeWarning, stacklevel=2). -> 1810 return func(ax, *args, **kwargs). 1811 . 1812 inner.__doc__ = _add_data_doc(inner.__doc__,. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\matplotlib\axes\_axes.py in pcolor(self, alpha, norm, cmap, vmin, vmax, *args, **kwargs). 5773 kwargs.setdefault('snap', False). 5774 . -> 5775 collection = mcoll.PolyCollection(verts, **kwargs). 5776 . 5777 collection.set_alpha(alpha). ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\matplotlib\collections.py in __init__(self, verts, sizes, closed, **kwargs). 931 %(Collection)s. 932 """""". --> 933 Collection.__init__(self, **kwargs). 934 self.set_sizes(sizes). 935 self.set_verts(verts, closed). ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\matplotlib\collections.py in __init__(self, edgecolors,",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/565
https://github.com/scverse/scanpy/issues/565:1206,deployability,stack,stacklevel,1206,"et the following error:. ```. ---------------------------------------------------------------------------. AttributeError Traceback (most recent call last). <ipython-input-432-bef389f3fd99> in <module>. ----> 1 gs = sc.pl.matrixplot(adata, marker_genes, groupby='louvain', dendrogram=True, standard_scale='var'). ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\plotting\_anndata.py in matrixplot(adata, var_names, groupby, use_raw, log, num_categories, figsize, dendrogram, var_group_positions, var_group_labels, var_group_rotation, layer, swap_axes, show, save, **kwds). 1683 _plot_dendrogram(dendro_ax, adata, ticks=y_ticks). 1684 . -> 1685 pc = matrix_ax.pcolor(mean_obs, edgecolor='gray', **kwds). 1686 . 1687 # invert y axis to show categories ordered from top to bottom. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\matplotlib\__init__.py in inner(ax, data, *args, **kwargs). 1808 ""the Matplotlib list!)"" % (label_namer, func.__name__),. 1809 RuntimeWarning, stacklevel=2). -> 1810 return func(ax, *args, **kwargs). 1811 . 1812 inner.__doc__ = _add_data_doc(inner.__doc__,. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\matplotlib\axes\_axes.py in pcolor(self, alpha, norm, cmap, vmin, vmax, *args, **kwargs). 5773 kwargs.setdefault('snap', False). 5774 . -> 5775 collection = mcoll.PolyCollection(verts, **kwargs). 5776 . 5777 collection.set_alpha(alpha). ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\matplotlib\collections.py in __init__(self, verts, sizes, closed, **kwargs). 931 %(Collection)s. 932 """""". --> 933 Collection.__init__(self, **kwargs). 934 self.set_sizes(sizes). 935 self.set_verts(verts, closed). ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\matplotlib\collections.py in __init__(self, edgecolors, facecolors, linewidths, linestyles, capstyle, joinstyle, antialiaseds, offsets, transOffset, norm, cmap, pickradius, hatch, urls, offset_position, zorder, **kwargs). 164 . 165 self._path_effects = None. --> 166",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/565
https://github.com/scverse/scanpy/issues/565:2217,deployability,updat,update,2217,"Matplotlib list!)"" % (label_namer, func.__name__),. 1809 RuntimeWarning, stacklevel=2). -> 1810 return func(ax, *args, **kwargs). 1811 . 1812 inner.__doc__ = _add_data_doc(inner.__doc__,. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\matplotlib\axes\_axes.py in pcolor(self, alpha, norm, cmap, vmin, vmax, *args, **kwargs). 5773 kwargs.setdefault('snap', False). 5774 . -> 5775 collection = mcoll.PolyCollection(verts, **kwargs). 5776 . 5777 collection.set_alpha(alpha). ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\matplotlib\collections.py in __init__(self, verts, sizes, closed, **kwargs). 931 %(Collection)s. 932 """""". --> 933 Collection.__init__(self, **kwargs). 934 self.set_sizes(sizes). 935 self.set_verts(verts, closed). ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\matplotlib\collections.py in __init__(self, edgecolors, facecolors, linewidths, linestyles, capstyle, joinstyle, antialiaseds, offsets, transOffset, norm, cmap, pickradius, hatch, urls, offset_position, zorder, **kwargs). 164 . 165 self._path_effects = None. --> 166 self.update(kwargs). 167 self._paths = None. 168 . ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\matplotlib\artist.py in update(self, props). 914 . 915 with cbook._setattr_cm(self, eventson=False):. --> 916 ret = [_update_property(self, k, v) for k, v in props.items()]. 917 . 918 if len(ret):. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\matplotlib\artist.py in <listcomp>(.0). 914 . 915 with cbook._setattr_cm(self, eventson=False):. --> 916 ret = [_update_property(self, k, v) for k, v in props.items()]. 917 . 918 if len(ret):. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\matplotlib\artist.py in _update_property(self, k, v). 910 func = getattr(self, 'set_' + k, None). 911 if not callable(func):. --> 912 raise AttributeError('Unknown property %s' % k). 913 return func(v). 914 . AttributeError: Unknown property standard_scale. ```. Any idea of what I'm missing here? Thanks.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/565
https://github.com/scverse/scanpy/issues/565:2345,deployability,updat,update,2345,"Matplotlib list!)"" % (label_namer, func.__name__),. 1809 RuntimeWarning, stacklevel=2). -> 1810 return func(ax, *args, **kwargs). 1811 . 1812 inner.__doc__ = _add_data_doc(inner.__doc__,. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\matplotlib\axes\_axes.py in pcolor(self, alpha, norm, cmap, vmin, vmax, *args, **kwargs). 5773 kwargs.setdefault('snap', False). 5774 . -> 5775 collection = mcoll.PolyCollection(verts, **kwargs). 5776 . 5777 collection.set_alpha(alpha). ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\matplotlib\collections.py in __init__(self, verts, sizes, closed, **kwargs). 931 %(Collection)s. 932 """""". --> 933 Collection.__init__(self, **kwargs). 934 self.set_sizes(sizes). 935 self.set_verts(verts, closed). ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\matplotlib\collections.py in __init__(self, edgecolors, facecolors, linewidths, linestyles, capstyle, joinstyle, antialiaseds, offsets, transOffset, norm, cmap, pickradius, hatch, urls, offset_position, zorder, **kwargs). 164 . 165 self._path_effects = None. --> 166 self.update(kwargs). 167 self._paths = None. 168 . ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\matplotlib\artist.py in update(self, props). 914 . 915 with cbook._setattr_cm(self, eventson=False):. --> 916 ret = [_update_property(self, k, v) for k, v in props.items()]. 917 . 918 if len(ret):. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\matplotlib\artist.py in <listcomp>(.0). 914 . 915 with cbook._setattr_cm(self, eventson=False):. --> 916 ret = [_update_property(self, k, v) for k, v in props.items()]. 917 . 918 if len(ret):. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\matplotlib\artist.py in _update_property(self, k, v). 910 func = getattr(self, 'set_' + k, None). 911 if not callable(func):. --> 912 raise AttributeError('Unknown property %s' % k). 913 return func(v). 914 . AttributeError: Unknown property standard_scale. ```. Any idea of what I'm missing here? Thanks.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/565
https://github.com/scverse/scanpy/issues/565:2405,integrability,event,eventson,2405,"Matplotlib list!)"" % (label_namer, func.__name__),. 1809 RuntimeWarning, stacklevel=2). -> 1810 return func(ax, *args, **kwargs). 1811 . 1812 inner.__doc__ = _add_data_doc(inner.__doc__,. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\matplotlib\axes\_axes.py in pcolor(self, alpha, norm, cmap, vmin, vmax, *args, **kwargs). 5773 kwargs.setdefault('snap', False). 5774 . -> 5775 collection = mcoll.PolyCollection(verts, **kwargs). 5776 . 5777 collection.set_alpha(alpha). ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\matplotlib\collections.py in __init__(self, verts, sizes, closed, **kwargs). 931 %(Collection)s. 932 """""". --> 933 Collection.__init__(self, **kwargs). 934 self.set_sizes(sizes). 935 self.set_verts(verts, closed). ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\matplotlib\collections.py in __init__(self, edgecolors, facecolors, linewidths, linestyles, capstyle, joinstyle, antialiaseds, offsets, transOffset, norm, cmap, pickradius, hatch, urls, offset_position, zorder, **kwargs). 164 . 165 self._path_effects = None. --> 166 self.update(kwargs). 167 self._paths = None. 168 . ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\matplotlib\artist.py in update(self, props). 914 . 915 with cbook._setattr_cm(self, eventson=False):. --> 916 ret = [_update_property(self, k, v) for k, v in props.items()]. 917 . 918 if len(ret):. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\matplotlib\artist.py in <listcomp>(.0). 914 . 915 with cbook._setattr_cm(self, eventson=False):. --> 916 ret = [_update_property(self, k, v) for k, v in props.items()]. 917 . 918 if len(ret):. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\matplotlib\artist.py in _update_property(self, k, v). 910 func = getattr(self, 'set_' + k, None). 911 if not callable(func):. --> 912 raise AttributeError('Unknown property %s' % k). 913 return func(v). 914 . AttributeError: Unknown property standard_scale. ```. Any idea of what I'm missing here? Thanks.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/565
https://github.com/scverse/scanpy/issues/565:2656,integrability,event,eventson,2656,"Matplotlib list!)"" % (label_namer, func.__name__),. 1809 RuntimeWarning, stacklevel=2). -> 1810 return func(ax, *args, **kwargs). 1811 . 1812 inner.__doc__ = _add_data_doc(inner.__doc__,. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\matplotlib\axes\_axes.py in pcolor(self, alpha, norm, cmap, vmin, vmax, *args, **kwargs). 5773 kwargs.setdefault('snap', False). 5774 . -> 5775 collection = mcoll.PolyCollection(verts, **kwargs). 5776 . 5777 collection.set_alpha(alpha). ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\matplotlib\collections.py in __init__(self, verts, sizes, closed, **kwargs). 931 %(Collection)s. 932 """""". --> 933 Collection.__init__(self, **kwargs). 934 self.set_sizes(sizes). 935 self.set_verts(verts, closed). ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\matplotlib\collections.py in __init__(self, edgecolors, facecolors, linewidths, linestyles, capstyle, joinstyle, antialiaseds, offsets, transOffset, norm, cmap, pickradius, hatch, urls, offset_position, zorder, **kwargs). 164 . 165 self._path_effects = None. --> 166 self.update(kwargs). 167 self._paths = None. 168 . ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\matplotlib\artist.py in update(self, props). 914 . 915 with cbook._setattr_cm(self, eventson=False):. --> 916 ret = [_update_property(self, k, v) for k, v in props.items()]. 917 . 918 if len(ret):. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\matplotlib\artist.py in <listcomp>(.0). 914 . 915 with cbook._setattr_cm(self, eventson=False):. --> 916 ret = [_update_property(self, k, v) for k, v in props.items()]. 917 . 918 if len(ret):. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\matplotlib\artist.py in _update_property(self, k, v). 910 func = getattr(self, 'set_' + k, None). 911 if not callable(func):. --> 912 raise AttributeError('Unknown property %s' % k). 913 return func(v). 914 . AttributeError: Unknown property standard_scale. ```. Any idea of what I'm missing here? Thanks.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/565
https://github.com/scverse/scanpy/issues/565:138,modifiability,scal,scaling,138,"standard_scale = 'var' AttributeError; Hi, . I have an issue with the standard_scale ='var' function. Whenever I try to make any plot and scaling the data from 0 to 1 with the standard_scale = 'var' function I get the following error:. ```. ---------------------------------------------------------------------------. AttributeError Traceback (most recent call last). <ipython-input-432-bef389f3fd99> in <module>. ----> 1 gs = sc.pl.matrixplot(adata, marker_genes, groupby='louvain', dendrogram=True, standard_scale='var'). ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\plotting\_anndata.py in matrixplot(adata, var_names, groupby, use_raw, log, num_categories, figsize, dendrogram, var_group_positions, var_group_labels, var_group_rotation, layer, swap_axes, show, save, **kwds). 1683 _plot_dendrogram(dendro_ax, adata, ticks=y_ticks). 1684 . -> 1685 pc = matrix_ax.pcolor(mean_obs, edgecolor='gray', **kwds). 1686 . 1687 # invert y axis to show categories ordered from top to bottom. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\matplotlib\__init__.py in inner(ax, data, *args, **kwargs). 1808 ""the Matplotlib list!)"" % (label_namer, func.__name__),. 1809 RuntimeWarning, stacklevel=2). -> 1810 return func(ax, *args, **kwargs). 1811 . 1812 inner.__doc__ = _add_data_doc(inner.__doc__,. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\matplotlib\axes\_axes.py in pcolor(self, alpha, norm, cmap, vmin, vmax, *args, **kwargs). 5773 kwargs.setdefault('snap', False). 5774 . -> 5775 collection = mcoll.PolyCollection(verts, **kwargs). 5776 . 5777 collection.set_alpha(alpha). ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\matplotlib\collections.py in __init__(self, verts, sizes, closed, **kwargs). 931 %(Collection)s. 932 """""". --> 933 Collection.__init__(self, **kwargs). 934 self.set_sizes(sizes). 935 self.set_verts(verts, closed). ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\matplotlib\collections.py in __init__(self, edgecolors,",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/565
https://github.com/scverse/scanpy/issues/565:405,modifiability,modul,module,405,"standard_scale = 'var' AttributeError; Hi, . I have an issue with the standard_scale ='var' function. Whenever I try to make any plot and scaling the data from 0 to 1 with the standard_scale = 'var' function I get the following error:. ```. ---------------------------------------------------------------------------. AttributeError Traceback (most recent call last). <ipython-input-432-bef389f3fd99> in <module>. ----> 1 gs = sc.pl.matrixplot(adata, marker_genes, groupby='louvain', dendrogram=True, standard_scale='var'). ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\plotting\_anndata.py in matrixplot(adata, var_names, groupby, use_raw, log, num_categories, figsize, dendrogram, var_group_positions, var_group_labels, var_group_rotation, layer, swap_axes, show, save, **kwds). 1683 _plot_dendrogram(dendro_ax, adata, ticks=y_ticks). 1684 . -> 1685 pc = matrix_ax.pcolor(mean_obs, edgecolor='gray', **kwds). 1686 . 1687 # invert y axis to show categories ordered from top to bottom. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\matplotlib\__init__.py in inner(ax, data, *args, **kwargs). 1808 ""the Matplotlib list!)"" % (label_namer, func.__name__),. 1809 RuntimeWarning, stacklevel=2). -> 1810 return func(ax, *args, **kwargs). 1811 . 1812 inner.__doc__ = _add_data_doc(inner.__doc__,. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\matplotlib\axes\_axes.py in pcolor(self, alpha, norm, cmap, vmin, vmax, *args, **kwargs). 5773 kwargs.setdefault('snap', False). 5774 . -> 5775 collection = mcoll.PolyCollection(verts, **kwargs). 5776 . 5777 collection.set_alpha(alpha). ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\matplotlib\collections.py in __init__(self, verts, sizes, closed, **kwargs). 931 %(Collection)s. 932 """""". --> 933 Collection.__init__(self, **kwargs). 934 self.set_sizes(sizes). 935 self.set_verts(verts, closed). ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\matplotlib\collections.py in __init__(self, edgecolors,",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/565
https://github.com/scverse/scanpy/issues/565:573,modifiability,pac,packages,573,"standard_scale = 'var' AttributeError; Hi, . I have an issue with the standard_scale ='var' function. Whenever I try to make any plot and scaling the data from 0 to 1 with the standard_scale = 'var' function I get the following error:. ```. ---------------------------------------------------------------------------. AttributeError Traceback (most recent call last). <ipython-input-432-bef389f3fd99> in <module>. ----> 1 gs = sc.pl.matrixplot(adata, marker_genes, groupby='louvain', dendrogram=True, standard_scale='var'). ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\plotting\_anndata.py in matrixplot(adata, var_names, groupby, use_raw, log, num_categories, figsize, dendrogram, var_group_positions, var_group_labels, var_group_rotation, layer, swap_axes, show, save, **kwds). 1683 _plot_dendrogram(dendro_ax, adata, ticks=y_ticks). 1684 . -> 1685 pc = matrix_ax.pcolor(mean_obs, edgecolor='gray', **kwds). 1686 . 1687 # invert y axis to show categories ordered from top to bottom. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\matplotlib\__init__.py in inner(ax, data, *args, **kwargs). 1808 ""the Matplotlib list!)"" % (label_namer, func.__name__),. 1809 RuntimeWarning, stacklevel=2). -> 1810 return func(ax, *args, **kwargs). 1811 . 1812 inner.__doc__ = _add_data_doc(inner.__doc__,. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\matplotlib\axes\_axes.py in pcolor(self, alpha, norm, cmap, vmin, vmax, *args, **kwargs). 5773 kwargs.setdefault('snap', False). 5774 . -> 5775 collection = mcoll.PolyCollection(verts, **kwargs). 5776 . 5777 collection.set_alpha(alpha). ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\matplotlib\collections.py in __init__(self, verts, sizes, closed, **kwargs). 931 %(Collection)s. 932 """""". --> 933 Collection.__init__(self, **kwargs). 934 self.set_sizes(sizes). 935 self.set_verts(verts, closed). ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\matplotlib\collections.py in __init__(self, edgecolors,",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/565
https://github.com/scverse/scanpy/issues/565:761,modifiability,layer,layer,761,"standard_scale = 'var' AttributeError; Hi, . I have an issue with the standard_scale ='var' function. Whenever I try to make any plot and scaling the data from 0 to 1 with the standard_scale = 'var' function I get the following error:. ```. ---------------------------------------------------------------------------. AttributeError Traceback (most recent call last). <ipython-input-432-bef389f3fd99> in <module>. ----> 1 gs = sc.pl.matrixplot(adata, marker_genes, groupby='louvain', dendrogram=True, standard_scale='var'). ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\plotting\_anndata.py in matrixplot(adata, var_names, groupby, use_raw, log, num_categories, figsize, dendrogram, var_group_positions, var_group_labels, var_group_rotation, layer, swap_axes, show, save, **kwds). 1683 _plot_dendrogram(dendro_ax, adata, ticks=y_ticks). 1684 . -> 1685 pc = matrix_ax.pcolor(mean_obs, edgecolor='gray', **kwds). 1686 . 1687 # invert y axis to show categories ordered from top to bottom. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\matplotlib\__init__.py in inner(ax, data, *args, **kwargs). 1808 ""the Matplotlib list!)"" % (label_namer, func.__name__),. 1809 RuntimeWarning, stacklevel=2). -> 1810 return func(ax, *args, **kwargs). 1811 . 1812 inner.__doc__ = _add_data_doc(inner.__doc__,. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\matplotlib\axes\_axes.py in pcolor(self, alpha, norm, cmap, vmin, vmax, *args, **kwargs). 5773 kwargs.setdefault('snap', False). 5774 . -> 5775 collection = mcoll.PolyCollection(verts, **kwargs). 5776 . 5777 collection.set_alpha(alpha). ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\matplotlib\collections.py in __init__(self, verts, sizes, closed, **kwargs). 931 %(Collection)s. 932 """""". --> 933 Collection.__init__(self, **kwargs). 934 self.set_sizes(sizes). 935 self.set_verts(verts, closed). ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\matplotlib\collections.py in __init__(self, edgecolors,",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/565
https://github.com/scverse/scanpy/issues/565:1054,modifiability,pac,packages,1054,"ue with the standard_scale ='var' function. Whenever I try to make any plot and scaling the data from 0 to 1 with the standard_scale = 'var' function I get the following error:. ```. ---------------------------------------------------------------------------. AttributeError Traceback (most recent call last). <ipython-input-432-bef389f3fd99> in <module>. ----> 1 gs = sc.pl.matrixplot(adata, marker_genes, groupby='louvain', dendrogram=True, standard_scale='var'). ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\plotting\_anndata.py in matrixplot(adata, var_names, groupby, use_raw, log, num_categories, figsize, dendrogram, var_group_positions, var_group_labels, var_group_rotation, layer, swap_axes, show, save, **kwds). 1683 _plot_dendrogram(dendro_ax, adata, ticks=y_ticks). 1684 . -> 1685 pc = matrix_ax.pcolor(mean_obs, edgecolor='gray', **kwds). 1686 . 1687 # invert y axis to show categories ordered from top to bottom. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\matplotlib\__init__.py in inner(ax, data, *args, **kwargs). 1808 ""the Matplotlib list!)"" % (label_namer, func.__name__),. 1809 RuntimeWarning, stacklevel=2). -> 1810 return func(ax, *args, **kwargs). 1811 . 1812 inner.__doc__ = _add_data_doc(inner.__doc__,. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\matplotlib\axes\_axes.py in pcolor(self, alpha, norm, cmap, vmin, vmax, *args, **kwargs). 5773 kwargs.setdefault('snap', False). 5774 . -> 5775 collection = mcoll.PolyCollection(verts, **kwargs). 5776 . 5777 collection.set_alpha(alpha). ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\matplotlib\collections.py in __init__(self, verts, sizes, closed, **kwargs). 931 %(Collection)s. 932 """""". --> 933 Collection.__init__(self, **kwargs). 934 self.set_sizes(sizes). 935 self.set_verts(verts, closed). ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\matplotlib\collections.py in __init__(self, edgecolors, facecolors, linewidths, linestyles, capstyle, joinstyle, ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/565
https://github.com/scverse/scanpy/issues/565:1370,modifiability,pac,packages,1370,"on-input-432-bef389f3fd99> in <module>. ----> 1 gs = sc.pl.matrixplot(adata, marker_genes, groupby='louvain', dendrogram=True, standard_scale='var'). ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\plotting\_anndata.py in matrixplot(adata, var_names, groupby, use_raw, log, num_categories, figsize, dendrogram, var_group_positions, var_group_labels, var_group_rotation, layer, swap_axes, show, save, **kwds). 1683 _plot_dendrogram(dendro_ax, adata, ticks=y_ticks). 1684 . -> 1685 pc = matrix_ax.pcolor(mean_obs, edgecolor='gray', **kwds). 1686 . 1687 # invert y axis to show categories ordered from top to bottom. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\matplotlib\__init__.py in inner(ax, data, *args, **kwargs). 1808 ""the Matplotlib list!)"" % (label_namer, func.__name__),. 1809 RuntimeWarning, stacklevel=2). -> 1810 return func(ax, *args, **kwargs). 1811 . 1812 inner.__doc__ = _add_data_doc(inner.__doc__,. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\matplotlib\axes\_axes.py in pcolor(self, alpha, norm, cmap, vmin, vmax, *args, **kwargs). 5773 kwargs.setdefault('snap', False). 5774 . -> 5775 collection = mcoll.PolyCollection(verts, **kwargs). 5776 . 5777 collection.set_alpha(alpha). ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\matplotlib\collections.py in __init__(self, verts, sizes, closed, **kwargs). 931 %(Collection)s. 932 """""". --> 933 Collection.__init__(self, **kwargs). 934 self.set_sizes(sizes). 935 self.set_verts(verts, closed). ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\matplotlib\collections.py in __init__(self, edgecolors, facecolors, linewidths, linestyles, capstyle, joinstyle, antialiaseds, offsets, transOffset, norm, cmap, pickradius, hatch, urls, offset_position, zorder, **kwargs). 164 . 165 self._path_effects = None. --> 166 self.update(kwargs). 167 self._paths = None. 168 . ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\matplotlib\artist.py in update(self, props). 914 . 91",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/565
https://github.com/scverse/scanpy/issues/565:1665,modifiability,pac,packages,1665,"categories, figsize, dendrogram, var_group_positions, var_group_labels, var_group_rotation, layer, swap_axes, show, save, **kwds). 1683 _plot_dendrogram(dendro_ax, adata, ticks=y_ticks). 1684 . -> 1685 pc = matrix_ax.pcolor(mean_obs, edgecolor='gray', **kwds). 1686 . 1687 # invert y axis to show categories ordered from top to bottom. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\matplotlib\__init__.py in inner(ax, data, *args, **kwargs). 1808 ""the Matplotlib list!)"" % (label_namer, func.__name__),. 1809 RuntimeWarning, stacklevel=2). -> 1810 return func(ax, *args, **kwargs). 1811 . 1812 inner.__doc__ = _add_data_doc(inner.__doc__,. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\matplotlib\axes\_axes.py in pcolor(self, alpha, norm, cmap, vmin, vmax, *args, **kwargs). 5773 kwargs.setdefault('snap', False). 5774 . -> 5775 collection = mcoll.PolyCollection(verts, **kwargs). 5776 . 5777 collection.set_alpha(alpha). ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\matplotlib\collections.py in __init__(self, verts, sizes, closed, **kwargs). 931 %(Collection)s. 932 """""". --> 933 Collection.__init__(self, **kwargs). 934 self.set_sizes(sizes). 935 self.set_verts(verts, closed). ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\matplotlib\collections.py in __init__(self, edgecolors, facecolors, linewidths, linestyles, capstyle, joinstyle, antialiaseds, offsets, transOffset, norm, cmap, pickradius, hatch, urls, offset_position, zorder, **kwargs). 164 . 165 self._path_effects = None. --> 166 self.update(kwargs). 167 self._paths = None. 168 . ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\matplotlib\artist.py in update(self, props). 914 . 915 with cbook._setattr_cm(self, eventson=False):. --> 916 ret = [_update_property(self, k, v) for k, v in props.items()]. 917 . 918 if len(ret):. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\matplotlib\artist.py in <listcomp>(.0). 914 . 915 with cbook._setattr_cm(self, eventson=Fals",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/565
https://github.com/scverse/scanpy/issues/565:1936,modifiability,pac,packages,1936,"7 # invert y axis to show categories ordered from top to bottom. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\matplotlib\__init__.py in inner(ax, data, *args, **kwargs). 1808 ""the Matplotlib list!)"" % (label_namer, func.__name__),. 1809 RuntimeWarning, stacklevel=2). -> 1810 return func(ax, *args, **kwargs). 1811 . 1812 inner.__doc__ = _add_data_doc(inner.__doc__,. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\matplotlib\axes\_axes.py in pcolor(self, alpha, norm, cmap, vmin, vmax, *args, **kwargs). 5773 kwargs.setdefault('snap', False). 5774 . -> 5775 collection = mcoll.PolyCollection(verts, **kwargs). 5776 . 5777 collection.set_alpha(alpha). ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\matplotlib\collections.py in __init__(self, verts, sizes, closed, **kwargs). 931 %(Collection)s. 932 """""". --> 933 Collection.__init__(self, **kwargs). 934 self.set_sizes(sizes). 935 self.set_verts(verts, closed). ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\matplotlib\collections.py in __init__(self, edgecolors, facecolors, linewidths, linestyles, capstyle, joinstyle, antialiaseds, offsets, transOffset, norm, cmap, pickradius, hatch, urls, offset_position, zorder, **kwargs). 164 . 165 self._path_effects = None. --> 166 self.update(kwargs). 167 self._paths = None. 168 . ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\matplotlib\artist.py in update(self, props). 914 . 915 with cbook._setattr_cm(self, eventson=False):. --> 916 ret = [_update_property(self, k, v) for k, v in props.items()]. 917 . 918 if len(ret):. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\matplotlib\artist.py in <listcomp>(.0). 914 . 915 with cbook._setattr_cm(self, eventson=False):. --> 916 ret = [_update_property(self, k, v) for k, v in props.items()]. 917 . 918 if len(ret):. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\matplotlib\artist.py in _update_property(self, k, v). 910 func = getattr(self, 'set_' + k, None). 911 if not cal",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/565
https://github.com/scverse/scanpy/issues/565:2312,modifiability,pac,packages,2312,"Matplotlib list!)"" % (label_namer, func.__name__),. 1809 RuntimeWarning, stacklevel=2). -> 1810 return func(ax, *args, **kwargs). 1811 . 1812 inner.__doc__ = _add_data_doc(inner.__doc__,. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\matplotlib\axes\_axes.py in pcolor(self, alpha, norm, cmap, vmin, vmax, *args, **kwargs). 5773 kwargs.setdefault('snap', False). 5774 . -> 5775 collection = mcoll.PolyCollection(verts, **kwargs). 5776 . 5777 collection.set_alpha(alpha). ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\matplotlib\collections.py in __init__(self, verts, sizes, closed, **kwargs). 931 %(Collection)s. 932 """""". --> 933 Collection.__init__(self, **kwargs). 934 self.set_sizes(sizes). 935 self.set_verts(verts, closed). ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\matplotlib\collections.py in __init__(self, edgecolors, facecolors, linewidths, linestyles, capstyle, joinstyle, antialiaseds, offsets, transOffset, norm, cmap, pickradius, hatch, urls, offset_position, zorder, **kwargs). 164 . 165 self._path_effects = None. --> 166 self.update(kwargs). 167 self._paths = None. 168 . ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\matplotlib\artist.py in update(self, props). 914 . 915 with cbook._setattr_cm(self, eventson=False):. --> 916 ret = [_update_property(self, k, v) for k, v in props.items()]. 917 . 918 if len(ret):. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\matplotlib\artist.py in <listcomp>(.0). 914 . 915 with cbook._setattr_cm(self, eventson=False):. --> 916 ret = [_update_property(self, k, v) for k, v in props.items()]. 917 . 918 if len(ret):. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\matplotlib\artist.py in _update_property(self, k, v). 910 func = getattr(self, 'set_' + k, None). 911 if not callable(func):. --> 912 raise AttributeError('Unknown property %s' % k). 913 return func(v). 914 . AttributeError: Unknown property standard_scale. ```. Any idea of what I'm missing here? Thanks.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/565
https://github.com/scverse/scanpy/issues/565:2568,modifiability,pac,packages,2568,"Matplotlib list!)"" % (label_namer, func.__name__),. 1809 RuntimeWarning, stacklevel=2). -> 1810 return func(ax, *args, **kwargs). 1811 . 1812 inner.__doc__ = _add_data_doc(inner.__doc__,. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\matplotlib\axes\_axes.py in pcolor(self, alpha, norm, cmap, vmin, vmax, *args, **kwargs). 5773 kwargs.setdefault('snap', False). 5774 . -> 5775 collection = mcoll.PolyCollection(verts, **kwargs). 5776 . 5777 collection.set_alpha(alpha). ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\matplotlib\collections.py in __init__(self, verts, sizes, closed, **kwargs). 931 %(Collection)s. 932 """""". --> 933 Collection.__init__(self, **kwargs). 934 self.set_sizes(sizes). 935 self.set_verts(verts, closed). ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\matplotlib\collections.py in __init__(self, edgecolors, facecolors, linewidths, linestyles, capstyle, joinstyle, antialiaseds, offsets, transOffset, norm, cmap, pickradius, hatch, urls, offset_position, zorder, **kwargs). 164 . 165 self._path_effects = None. --> 166 self.update(kwargs). 167 self._paths = None. 168 . ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\matplotlib\artist.py in update(self, props). 914 . 915 with cbook._setattr_cm(self, eventson=False):. --> 916 ret = [_update_property(self, k, v) for k, v in props.items()]. 917 . 918 if len(ret):. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\matplotlib\artist.py in <listcomp>(.0). 914 . 915 with cbook._setattr_cm(self, eventson=False):. --> 916 ret = [_update_property(self, k, v) for k, v in props.items()]. 917 . 918 if len(ret):. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\matplotlib\artist.py in _update_property(self, k, v). 910 func = getattr(self, 'set_' + k, None). 911 if not callable(func):. --> 912 raise AttributeError('Unknown property %s' % k). 913 return func(v). 914 . AttributeError: Unknown property standard_scale. ```. Any idea of what I'm missing here? Thanks.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/565
https://github.com/scverse/scanpy/issues/565:2819,modifiability,pac,packages,2819,"Matplotlib list!)"" % (label_namer, func.__name__),. 1809 RuntimeWarning, stacklevel=2). -> 1810 return func(ax, *args, **kwargs). 1811 . 1812 inner.__doc__ = _add_data_doc(inner.__doc__,. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\matplotlib\axes\_axes.py in pcolor(self, alpha, norm, cmap, vmin, vmax, *args, **kwargs). 5773 kwargs.setdefault('snap', False). 5774 . -> 5775 collection = mcoll.PolyCollection(verts, **kwargs). 5776 . 5777 collection.set_alpha(alpha). ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\matplotlib\collections.py in __init__(self, verts, sizes, closed, **kwargs). 931 %(Collection)s. 932 """""". --> 933 Collection.__init__(self, **kwargs). 934 self.set_sizes(sizes). 935 self.set_verts(verts, closed). ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\matplotlib\collections.py in __init__(self, edgecolors, facecolors, linewidths, linestyles, capstyle, joinstyle, antialiaseds, offsets, transOffset, norm, cmap, pickradius, hatch, urls, offset_position, zorder, **kwargs). 164 . 165 self._path_effects = None. --> 166 self.update(kwargs). 167 self._paths = None. 168 . ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\matplotlib\artist.py in update(self, props). 914 . 915 with cbook._setattr_cm(self, eventson=False):. --> 916 ret = [_update_property(self, k, v) for k, v in props.items()]. 917 . 918 if len(ret):. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\matplotlib\artist.py in <listcomp>(.0). 914 . 915 with cbook._setattr_cm(self, eventson=False):. --> 916 ret = [_update_property(self, k, v) for k, v in props.items()]. 917 . 918 if len(ret):. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\matplotlib\artist.py in _update_property(self, k, v). 910 func = getattr(self, 'set_' + k, None). 911 if not callable(func):. --> 912 raise AttributeError('Unknown property %s' % k). 913 return func(v). 914 . AttributeError: Unknown property standard_scale. ```. Any idea of what I'm missing here? Thanks.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/565
https://github.com/scverse/scanpy/issues/565:228,performance,error,error,228,"standard_scale = 'var' AttributeError; Hi, . I have an issue with the standard_scale ='var' function. Whenever I try to make any plot and scaling the data from 0 to 1 with the standard_scale = 'var' function I get the following error:. ```. ---------------------------------------------------------------------------. AttributeError Traceback (most recent call last). <ipython-input-432-bef389f3fd99> in <module>. ----> 1 gs = sc.pl.matrixplot(adata, marker_genes, groupby='louvain', dendrogram=True, standard_scale='var'). ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\plotting\_anndata.py in matrixplot(adata, var_names, groupby, use_raw, log, num_categories, figsize, dendrogram, var_group_positions, var_group_labels, var_group_rotation, layer, swap_axes, show, save, **kwds). 1683 _plot_dendrogram(dendro_ax, adata, ticks=y_ticks). 1684 . -> 1685 pc = matrix_ax.pcolor(mean_obs, edgecolor='gray', **kwds). 1686 . 1687 # invert y axis to show categories ordered from top to bottom. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\matplotlib\__init__.py in inner(ax, data, *args, **kwargs). 1808 ""the Matplotlib list!)"" % (label_namer, func.__name__),. 1809 RuntimeWarning, stacklevel=2). -> 1810 return func(ax, *args, **kwargs). 1811 . 1812 inner.__doc__ = _add_data_doc(inner.__doc__,. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\matplotlib\axes\_axes.py in pcolor(self, alpha, norm, cmap, vmin, vmax, *args, **kwargs). 5773 kwargs.setdefault('snap', False). 5774 . -> 5775 collection = mcoll.PolyCollection(verts, **kwargs). 5776 . 5777 collection.set_alpha(alpha). ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\matplotlib\collections.py in __init__(self, verts, sizes, closed, **kwargs). 931 %(Collection)s. 932 """""". --> 933 Collection.__init__(self, **kwargs). 934 self.set_sizes(sizes). 935 self.set_verts(verts, closed). ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\matplotlib\collections.py in __init__(self, edgecolors,",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/565
https://github.com/scverse/scanpy/issues/565:228,safety,error,error,228,"standard_scale = 'var' AttributeError; Hi, . I have an issue with the standard_scale ='var' function. Whenever I try to make any plot and scaling the data from 0 to 1 with the standard_scale = 'var' function I get the following error:. ```. ---------------------------------------------------------------------------. AttributeError Traceback (most recent call last). <ipython-input-432-bef389f3fd99> in <module>. ----> 1 gs = sc.pl.matrixplot(adata, marker_genes, groupby='louvain', dendrogram=True, standard_scale='var'). ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\plotting\_anndata.py in matrixplot(adata, var_names, groupby, use_raw, log, num_categories, figsize, dendrogram, var_group_positions, var_group_labels, var_group_rotation, layer, swap_axes, show, save, **kwds). 1683 _plot_dendrogram(dendro_ax, adata, ticks=y_ticks). 1684 . -> 1685 pc = matrix_ax.pcolor(mean_obs, edgecolor='gray', **kwds). 1686 . 1687 # invert y axis to show categories ordered from top to bottom. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\matplotlib\__init__.py in inner(ax, data, *args, **kwargs). 1808 ""the Matplotlib list!)"" % (label_namer, func.__name__),. 1809 RuntimeWarning, stacklevel=2). -> 1810 return func(ax, *args, **kwargs). 1811 . 1812 inner.__doc__ = _add_data_doc(inner.__doc__,. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\matplotlib\axes\_axes.py in pcolor(self, alpha, norm, cmap, vmin, vmax, *args, **kwargs). 5773 kwargs.setdefault('snap', False). 5774 . -> 5775 collection = mcoll.PolyCollection(verts, **kwargs). 5776 . 5777 collection.set_alpha(alpha). ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\matplotlib\collections.py in __init__(self, verts, sizes, closed, **kwargs). 931 %(Collection)s. 932 """""". --> 933 Collection.__init__(self, **kwargs). 934 self.set_sizes(sizes). 935 self.set_verts(verts, closed). ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\matplotlib\collections.py in __init__(self, edgecolors,",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/565
https://github.com/scverse/scanpy/issues/565:377,safety,input,input-,377,"standard_scale = 'var' AttributeError; Hi, . I have an issue with the standard_scale ='var' function. Whenever I try to make any plot and scaling the data from 0 to 1 with the standard_scale = 'var' function I get the following error:. ```. ---------------------------------------------------------------------------. AttributeError Traceback (most recent call last). <ipython-input-432-bef389f3fd99> in <module>. ----> 1 gs = sc.pl.matrixplot(adata, marker_genes, groupby='louvain', dendrogram=True, standard_scale='var'). ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\plotting\_anndata.py in matrixplot(adata, var_names, groupby, use_raw, log, num_categories, figsize, dendrogram, var_group_positions, var_group_labels, var_group_rotation, layer, swap_axes, show, save, **kwds). 1683 _plot_dendrogram(dendro_ax, adata, ticks=y_ticks). 1684 . -> 1685 pc = matrix_ax.pcolor(mean_obs, edgecolor='gray', **kwds). 1686 . 1687 # invert y axis to show categories ordered from top to bottom. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\matplotlib\__init__.py in inner(ax, data, *args, **kwargs). 1808 ""the Matplotlib list!)"" % (label_namer, func.__name__),. 1809 RuntimeWarning, stacklevel=2). -> 1810 return func(ax, *args, **kwargs). 1811 . 1812 inner.__doc__ = _add_data_doc(inner.__doc__,. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\matplotlib\axes\_axes.py in pcolor(self, alpha, norm, cmap, vmin, vmax, *args, **kwargs). 5773 kwargs.setdefault('snap', False). 5774 . -> 5775 collection = mcoll.PolyCollection(verts, **kwargs). 5776 . 5777 collection.set_alpha(alpha). ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\matplotlib\collections.py in __init__(self, verts, sizes, closed, **kwargs). 931 %(Collection)s. 932 """""". --> 933 Collection.__init__(self, **kwargs). 934 self.set_sizes(sizes). 935 self.set_verts(verts, closed). ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\matplotlib\collections.py in __init__(self, edgecolors,",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/565
https://github.com/scverse/scanpy/issues/565:405,safety,modul,module,405,"standard_scale = 'var' AttributeError; Hi, . I have an issue with the standard_scale ='var' function. Whenever I try to make any plot and scaling the data from 0 to 1 with the standard_scale = 'var' function I get the following error:. ```. ---------------------------------------------------------------------------. AttributeError Traceback (most recent call last). <ipython-input-432-bef389f3fd99> in <module>. ----> 1 gs = sc.pl.matrixplot(adata, marker_genes, groupby='louvain', dendrogram=True, standard_scale='var'). ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\plotting\_anndata.py in matrixplot(adata, var_names, groupby, use_raw, log, num_categories, figsize, dendrogram, var_group_positions, var_group_labels, var_group_rotation, layer, swap_axes, show, save, **kwds). 1683 _plot_dendrogram(dendro_ax, adata, ticks=y_ticks). 1684 . -> 1685 pc = matrix_ax.pcolor(mean_obs, edgecolor='gray', **kwds). 1686 . 1687 # invert y axis to show categories ordered from top to bottom. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\matplotlib\__init__.py in inner(ax, data, *args, **kwargs). 1808 ""the Matplotlib list!)"" % (label_namer, func.__name__),. 1809 RuntimeWarning, stacklevel=2). -> 1810 return func(ax, *args, **kwargs). 1811 . 1812 inner.__doc__ = _add_data_doc(inner.__doc__,. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\matplotlib\axes\_axes.py in pcolor(self, alpha, norm, cmap, vmin, vmax, *args, **kwargs). 5773 kwargs.setdefault('snap', False). 5774 . -> 5775 collection = mcoll.PolyCollection(verts, **kwargs). 5776 . 5777 collection.set_alpha(alpha). ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\matplotlib\collections.py in __init__(self, verts, sizes, closed, **kwargs). 931 %(Collection)s. 932 """""". --> 933 Collection.__init__(self, **kwargs). 934 self.set_sizes(sizes). 935 self.set_verts(verts, closed). ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\matplotlib\collections.py in __init__(self, edgecolors,",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/565
https://github.com/scverse/scanpy/issues/565:660,safety,log,log,660,"standard_scale = 'var' AttributeError; Hi, . I have an issue with the standard_scale ='var' function. Whenever I try to make any plot and scaling the data from 0 to 1 with the standard_scale = 'var' function I get the following error:. ```. ---------------------------------------------------------------------------. AttributeError Traceback (most recent call last). <ipython-input-432-bef389f3fd99> in <module>. ----> 1 gs = sc.pl.matrixplot(adata, marker_genes, groupby='louvain', dendrogram=True, standard_scale='var'). ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\plotting\_anndata.py in matrixplot(adata, var_names, groupby, use_raw, log, num_categories, figsize, dendrogram, var_group_positions, var_group_labels, var_group_rotation, layer, swap_axes, show, save, **kwds). 1683 _plot_dendrogram(dendro_ax, adata, ticks=y_ticks). 1684 . -> 1685 pc = matrix_ax.pcolor(mean_obs, edgecolor='gray', **kwds). 1686 . 1687 # invert y axis to show categories ordered from top to bottom. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\matplotlib\__init__.py in inner(ax, data, *args, **kwargs). 1808 ""the Matplotlib list!)"" % (label_namer, func.__name__),. 1809 RuntimeWarning, stacklevel=2). -> 1810 return func(ax, *args, **kwargs). 1811 . 1812 inner.__doc__ = _add_data_doc(inner.__doc__,. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\matplotlib\axes\_axes.py in pcolor(self, alpha, norm, cmap, vmin, vmax, *args, **kwargs). 5773 kwargs.setdefault('snap', False). 5774 . -> 5775 collection = mcoll.PolyCollection(verts, **kwargs). 5776 . 5777 collection.set_alpha(alpha). ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\matplotlib\collections.py in __init__(self, verts, sizes, closed, **kwargs). 931 %(Collection)s. 932 """""". --> 933 Collection.__init__(self, **kwargs). 934 self.set_sizes(sizes). 935 self.set_verts(verts, closed). ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\matplotlib\collections.py in __init__(self, edgecolors,",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/565
https://github.com/scverse/scanpy/issues/565:2217,safety,updat,update,2217,"Matplotlib list!)"" % (label_namer, func.__name__),. 1809 RuntimeWarning, stacklevel=2). -> 1810 return func(ax, *args, **kwargs). 1811 . 1812 inner.__doc__ = _add_data_doc(inner.__doc__,. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\matplotlib\axes\_axes.py in pcolor(self, alpha, norm, cmap, vmin, vmax, *args, **kwargs). 5773 kwargs.setdefault('snap', False). 5774 . -> 5775 collection = mcoll.PolyCollection(verts, **kwargs). 5776 . 5777 collection.set_alpha(alpha). ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\matplotlib\collections.py in __init__(self, verts, sizes, closed, **kwargs). 931 %(Collection)s. 932 """""". --> 933 Collection.__init__(self, **kwargs). 934 self.set_sizes(sizes). 935 self.set_verts(verts, closed). ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\matplotlib\collections.py in __init__(self, edgecolors, facecolors, linewidths, linestyles, capstyle, joinstyle, antialiaseds, offsets, transOffset, norm, cmap, pickradius, hatch, urls, offset_position, zorder, **kwargs). 164 . 165 self._path_effects = None. --> 166 self.update(kwargs). 167 self._paths = None. 168 . ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\matplotlib\artist.py in update(self, props). 914 . 915 with cbook._setattr_cm(self, eventson=False):. --> 916 ret = [_update_property(self, k, v) for k, v in props.items()]. 917 . 918 if len(ret):. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\matplotlib\artist.py in <listcomp>(.0). 914 . 915 with cbook._setattr_cm(self, eventson=False):. --> 916 ret = [_update_property(self, k, v) for k, v in props.items()]. 917 . 918 if len(ret):. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\matplotlib\artist.py in _update_property(self, k, v). 910 func = getattr(self, 'set_' + k, None). 911 if not callable(func):. --> 912 raise AttributeError('Unknown property %s' % k). 913 return func(v). 914 . AttributeError: Unknown property standard_scale. ```. Any idea of what I'm missing here? Thanks.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/565
https://github.com/scverse/scanpy/issues/565:2345,safety,updat,update,2345,"Matplotlib list!)"" % (label_namer, func.__name__),. 1809 RuntimeWarning, stacklevel=2). -> 1810 return func(ax, *args, **kwargs). 1811 . 1812 inner.__doc__ = _add_data_doc(inner.__doc__,. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\matplotlib\axes\_axes.py in pcolor(self, alpha, norm, cmap, vmin, vmax, *args, **kwargs). 5773 kwargs.setdefault('snap', False). 5774 . -> 5775 collection = mcoll.PolyCollection(verts, **kwargs). 5776 . 5777 collection.set_alpha(alpha). ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\matplotlib\collections.py in __init__(self, verts, sizes, closed, **kwargs). 931 %(Collection)s. 932 """""". --> 933 Collection.__init__(self, **kwargs). 934 self.set_sizes(sizes). 935 self.set_verts(verts, closed). ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\matplotlib\collections.py in __init__(self, edgecolors, facecolors, linewidths, linestyles, capstyle, joinstyle, antialiaseds, offsets, transOffset, norm, cmap, pickradius, hatch, urls, offset_position, zorder, **kwargs). 164 . 165 self._path_effects = None. --> 166 self.update(kwargs). 167 self._paths = None. 168 . ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\matplotlib\artist.py in update(self, props). 914 . 915 with cbook._setattr_cm(self, eventson=False):. --> 916 ret = [_update_property(self, k, v) for k, v in props.items()]. 917 . 918 if len(ret):. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\matplotlib\artist.py in <listcomp>(.0). 914 . 915 with cbook._setattr_cm(self, eventson=False):. --> 916 ret = [_update_property(self, k, v) for k, v in props.items()]. 917 . 918 if len(ret):. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\matplotlib\artist.py in _update_property(self, k, v). 910 func = getattr(self, 'set_' + k, None). 911 if not callable(func):. --> 912 raise AttributeError('Unknown property %s' % k). 913 return func(v). 914 . AttributeError: Unknown property standard_scale. ```. Any idea of what I'm missing here? Thanks.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/565
https://github.com/scverse/scanpy/issues/565:660,security,log,log,660,"standard_scale = 'var' AttributeError; Hi, . I have an issue with the standard_scale ='var' function. Whenever I try to make any plot and scaling the data from 0 to 1 with the standard_scale = 'var' function I get the following error:. ```. ---------------------------------------------------------------------------. AttributeError Traceback (most recent call last). <ipython-input-432-bef389f3fd99> in <module>. ----> 1 gs = sc.pl.matrixplot(adata, marker_genes, groupby='louvain', dendrogram=True, standard_scale='var'). ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\plotting\_anndata.py in matrixplot(adata, var_names, groupby, use_raw, log, num_categories, figsize, dendrogram, var_group_positions, var_group_labels, var_group_rotation, layer, swap_axes, show, save, **kwds). 1683 _plot_dendrogram(dendro_ax, adata, ticks=y_ticks). 1684 . -> 1685 pc = matrix_ax.pcolor(mean_obs, edgecolor='gray', **kwds). 1686 . 1687 # invert y axis to show categories ordered from top to bottom. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\matplotlib\__init__.py in inner(ax, data, *args, **kwargs). 1808 ""the Matplotlib list!)"" % (label_namer, func.__name__),. 1809 RuntimeWarning, stacklevel=2). -> 1810 return func(ax, *args, **kwargs). 1811 . 1812 inner.__doc__ = _add_data_doc(inner.__doc__,. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\matplotlib\axes\_axes.py in pcolor(self, alpha, norm, cmap, vmin, vmax, *args, **kwargs). 5773 kwargs.setdefault('snap', False). 5774 . -> 5775 collection = mcoll.PolyCollection(verts, **kwargs). 5776 . 5777 collection.set_alpha(alpha). ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\matplotlib\collections.py in __init__(self, verts, sizes, closed, **kwargs). 931 %(Collection)s. 932 """""". --> 933 Collection.__init__(self, **kwargs). 934 self.set_sizes(sizes). 935 self.set_verts(verts, closed). ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\matplotlib\collections.py in __init__(self, edgecolors,",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/565
https://github.com/scverse/scanpy/issues/565:2217,security,updat,update,2217,"Matplotlib list!)"" % (label_namer, func.__name__),. 1809 RuntimeWarning, stacklevel=2). -> 1810 return func(ax, *args, **kwargs). 1811 . 1812 inner.__doc__ = _add_data_doc(inner.__doc__,. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\matplotlib\axes\_axes.py in pcolor(self, alpha, norm, cmap, vmin, vmax, *args, **kwargs). 5773 kwargs.setdefault('snap', False). 5774 . -> 5775 collection = mcoll.PolyCollection(verts, **kwargs). 5776 . 5777 collection.set_alpha(alpha). ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\matplotlib\collections.py in __init__(self, verts, sizes, closed, **kwargs). 931 %(Collection)s. 932 """""". --> 933 Collection.__init__(self, **kwargs). 934 self.set_sizes(sizes). 935 self.set_verts(verts, closed). ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\matplotlib\collections.py in __init__(self, edgecolors, facecolors, linewidths, linestyles, capstyle, joinstyle, antialiaseds, offsets, transOffset, norm, cmap, pickradius, hatch, urls, offset_position, zorder, **kwargs). 164 . 165 self._path_effects = None. --> 166 self.update(kwargs). 167 self._paths = None. 168 . ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\matplotlib\artist.py in update(self, props). 914 . 915 with cbook._setattr_cm(self, eventson=False):. --> 916 ret = [_update_property(self, k, v) for k, v in props.items()]. 917 . 918 if len(ret):. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\matplotlib\artist.py in <listcomp>(.0). 914 . 915 with cbook._setattr_cm(self, eventson=False):. --> 916 ret = [_update_property(self, k, v) for k, v in props.items()]. 917 . 918 if len(ret):. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\matplotlib\artist.py in _update_property(self, k, v). 910 func = getattr(self, 'set_' + k, None). 911 if not callable(func):. --> 912 raise AttributeError('Unknown property %s' % k). 913 return func(v). 914 . AttributeError: Unknown property standard_scale. ```. Any idea of what I'm missing here? Thanks.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/565
https://github.com/scverse/scanpy/issues/565:2345,security,updat,update,2345,"Matplotlib list!)"" % (label_namer, func.__name__),. 1809 RuntimeWarning, stacklevel=2). -> 1810 return func(ax, *args, **kwargs). 1811 . 1812 inner.__doc__ = _add_data_doc(inner.__doc__,. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\matplotlib\axes\_axes.py in pcolor(self, alpha, norm, cmap, vmin, vmax, *args, **kwargs). 5773 kwargs.setdefault('snap', False). 5774 . -> 5775 collection = mcoll.PolyCollection(verts, **kwargs). 5776 . 5777 collection.set_alpha(alpha). ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\matplotlib\collections.py in __init__(self, verts, sizes, closed, **kwargs). 931 %(Collection)s. 932 """""". --> 933 Collection.__init__(self, **kwargs). 934 self.set_sizes(sizes). 935 self.set_verts(verts, closed). ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\matplotlib\collections.py in __init__(self, edgecolors, facecolors, linewidths, linestyles, capstyle, joinstyle, antialiaseds, offsets, transOffset, norm, cmap, pickradius, hatch, urls, offset_position, zorder, **kwargs). 164 . 165 self._path_effects = None. --> 166 self.update(kwargs). 167 self._paths = None. 168 . ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\matplotlib\artist.py in update(self, props). 914 . 915 with cbook._setattr_cm(self, eventson=False):. --> 916 ret = [_update_property(self, k, v) for k, v in props.items()]. 917 . 918 if len(ret):. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\matplotlib\artist.py in <listcomp>(.0). 914 . 915 with cbook._setattr_cm(self, eventson=False):. --> 916 ret = [_update_property(self, k, v) for k, v in props.items()]. 917 . 918 if len(ret):. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\matplotlib\artist.py in _update_property(self, k, v). 910 func = getattr(self, 'set_' + k, None). 911 if not callable(func):. --> 912 raise AttributeError('Unknown property %s' % k). 913 return func(v). 914 . AttributeError: Unknown property standard_scale. ```. Any idea of what I'm missing here? Thanks.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/565
https://github.com/scverse/scanpy/issues/565:333,testability,Trace,Traceback,333,"standard_scale = 'var' AttributeError; Hi, . I have an issue with the standard_scale ='var' function. Whenever I try to make any plot and scaling the data from 0 to 1 with the standard_scale = 'var' function I get the following error:. ```. ---------------------------------------------------------------------------. AttributeError Traceback (most recent call last). <ipython-input-432-bef389f3fd99> in <module>. ----> 1 gs = sc.pl.matrixplot(adata, marker_genes, groupby='louvain', dendrogram=True, standard_scale='var'). ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\plotting\_anndata.py in matrixplot(adata, var_names, groupby, use_raw, log, num_categories, figsize, dendrogram, var_group_positions, var_group_labels, var_group_rotation, layer, swap_axes, show, save, **kwds). 1683 _plot_dendrogram(dendro_ax, adata, ticks=y_ticks). 1684 . -> 1685 pc = matrix_ax.pcolor(mean_obs, edgecolor='gray', **kwds). 1686 . 1687 # invert y axis to show categories ordered from top to bottom. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\matplotlib\__init__.py in inner(ax, data, *args, **kwargs). 1808 ""the Matplotlib list!)"" % (label_namer, func.__name__),. 1809 RuntimeWarning, stacklevel=2). -> 1810 return func(ax, *args, **kwargs). 1811 . 1812 inner.__doc__ = _add_data_doc(inner.__doc__,. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\matplotlib\axes\_axes.py in pcolor(self, alpha, norm, cmap, vmin, vmax, *args, **kwargs). 5773 kwargs.setdefault('snap', False). 5774 . -> 5775 collection = mcoll.PolyCollection(verts, **kwargs). 5776 . 5777 collection.set_alpha(alpha). ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\matplotlib\collections.py in __init__(self, verts, sizes, closed, **kwargs). 931 %(Collection)s. 932 """""". --> 933 Collection.__init__(self, **kwargs). 934 self.set_sizes(sizes). 935 self.set_verts(verts, closed). ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\matplotlib\collections.py in __init__(self, edgecolors,",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/565
https://github.com/scverse/scanpy/issues/565:660,testability,log,log,660,"standard_scale = 'var' AttributeError; Hi, . I have an issue with the standard_scale ='var' function. Whenever I try to make any plot and scaling the data from 0 to 1 with the standard_scale = 'var' function I get the following error:. ```. ---------------------------------------------------------------------------. AttributeError Traceback (most recent call last). <ipython-input-432-bef389f3fd99> in <module>. ----> 1 gs = sc.pl.matrixplot(adata, marker_genes, groupby='louvain', dendrogram=True, standard_scale='var'). ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\plotting\_anndata.py in matrixplot(adata, var_names, groupby, use_raw, log, num_categories, figsize, dendrogram, var_group_positions, var_group_labels, var_group_rotation, layer, swap_axes, show, save, **kwds). 1683 _plot_dendrogram(dendro_ax, adata, ticks=y_ticks). 1684 . -> 1685 pc = matrix_ax.pcolor(mean_obs, edgecolor='gray', **kwds). 1686 . 1687 # invert y axis to show categories ordered from top to bottom. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\matplotlib\__init__.py in inner(ax, data, *args, **kwargs). 1808 ""the Matplotlib list!)"" % (label_namer, func.__name__),. 1809 RuntimeWarning, stacklevel=2). -> 1810 return func(ax, *args, **kwargs). 1811 . 1812 inner.__doc__ = _add_data_doc(inner.__doc__,. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\matplotlib\axes\_axes.py in pcolor(self, alpha, norm, cmap, vmin, vmax, *args, **kwargs). 5773 kwargs.setdefault('snap', False). 5774 . -> 5775 collection = mcoll.PolyCollection(verts, **kwargs). 5776 . 5777 collection.set_alpha(alpha). ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\matplotlib\collections.py in __init__(self, verts, sizes, closed, **kwargs). 931 %(Collection)s. 932 """""". --> 933 Collection.__init__(self, **kwargs). 934 self.set_sizes(sizes). 935 self.set_verts(verts, closed). ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\matplotlib\collections.py in __init__(self, edgecolors,",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/565
https://github.com/scverse/scanpy/issues/565:228,usability,error,error,228,"standard_scale = 'var' AttributeError; Hi, . I have an issue with the standard_scale ='var' function. Whenever I try to make any plot and scaling the data from 0 to 1 with the standard_scale = 'var' function I get the following error:. ```. ---------------------------------------------------------------------------. AttributeError Traceback (most recent call last). <ipython-input-432-bef389f3fd99> in <module>. ----> 1 gs = sc.pl.matrixplot(adata, marker_genes, groupby='louvain', dendrogram=True, standard_scale='var'). ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\plotting\_anndata.py in matrixplot(adata, var_names, groupby, use_raw, log, num_categories, figsize, dendrogram, var_group_positions, var_group_labels, var_group_rotation, layer, swap_axes, show, save, **kwds). 1683 _plot_dendrogram(dendro_ax, adata, ticks=y_ticks). 1684 . -> 1685 pc = matrix_ax.pcolor(mean_obs, edgecolor='gray', **kwds). 1686 . 1687 # invert y axis to show categories ordered from top to bottom. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\matplotlib\__init__.py in inner(ax, data, *args, **kwargs). 1808 ""the Matplotlib list!)"" % (label_namer, func.__name__),. 1809 RuntimeWarning, stacklevel=2). -> 1810 return func(ax, *args, **kwargs). 1811 . 1812 inner.__doc__ = _add_data_doc(inner.__doc__,. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\matplotlib\axes\_axes.py in pcolor(self, alpha, norm, cmap, vmin, vmax, *args, **kwargs). 5773 kwargs.setdefault('snap', False). 5774 . -> 5775 collection = mcoll.PolyCollection(verts, **kwargs). 5776 . 5777 collection.set_alpha(alpha). ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\matplotlib\collections.py in __init__(self, verts, sizes, closed, **kwargs). 931 %(Collection)s. 932 """""". --> 933 Collection.__init__(self, **kwargs). 934 self.set_sizes(sizes). 935 self.set_verts(verts, closed). ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\matplotlib\collections.py in __init__(self, edgecolors,",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/565
https://github.com/scverse/scanpy/issues/565:377,usability,input,input-,377,"standard_scale = 'var' AttributeError; Hi, . I have an issue with the standard_scale ='var' function. Whenever I try to make any plot and scaling the data from 0 to 1 with the standard_scale = 'var' function I get the following error:. ```. ---------------------------------------------------------------------------. AttributeError Traceback (most recent call last). <ipython-input-432-bef389f3fd99> in <module>. ----> 1 gs = sc.pl.matrixplot(adata, marker_genes, groupby='louvain', dendrogram=True, standard_scale='var'). ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\plotting\_anndata.py in matrixplot(adata, var_names, groupby, use_raw, log, num_categories, figsize, dendrogram, var_group_positions, var_group_labels, var_group_rotation, layer, swap_axes, show, save, **kwds). 1683 _plot_dendrogram(dendro_ax, adata, ticks=y_ticks). 1684 . -> 1685 pc = matrix_ax.pcolor(mean_obs, edgecolor='gray', **kwds). 1686 . 1687 # invert y axis to show categories ordered from top to bottom. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\matplotlib\__init__.py in inner(ax, data, *args, **kwargs). 1808 ""the Matplotlib list!)"" % (label_namer, func.__name__),. 1809 RuntimeWarning, stacklevel=2). -> 1810 return func(ax, *args, **kwargs). 1811 . 1812 inner.__doc__ = _add_data_doc(inner.__doc__,. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\matplotlib\axes\_axes.py in pcolor(self, alpha, norm, cmap, vmin, vmax, *args, **kwargs). 5773 kwargs.setdefault('snap', False). 5774 . -> 5775 collection = mcoll.PolyCollection(verts, **kwargs). 5776 . 5777 collection.set_alpha(alpha). ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\matplotlib\collections.py in __init__(self, verts, sizes, closed, **kwargs). 931 %(Collection)s. 932 """""". --> 933 Collection.__init__(self, **kwargs). 934 self.set_sizes(sizes). 935 self.set_verts(verts, closed). ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\matplotlib\collections.py in __init__(self, edgecolors,",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/565
https://github.com/scverse/scanpy/issues/565:1732,usability,close,closed,1732,"bels, var_group_rotation, layer, swap_axes, show, save, **kwds). 1683 _plot_dendrogram(dendro_ax, adata, ticks=y_ticks). 1684 . -> 1685 pc = matrix_ax.pcolor(mean_obs, edgecolor='gray', **kwds). 1686 . 1687 # invert y axis to show categories ordered from top to bottom. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\matplotlib\__init__.py in inner(ax, data, *args, **kwargs). 1808 ""the Matplotlib list!)"" % (label_namer, func.__name__),. 1809 RuntimeWarning, stacklevel=2). -> 1810 return func(ax, *args, **kwargs). 1811 . 1812 inner.__doc__ = _add_data_doc(inner.__doc__,. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\matplotlib\axes\_axes.py in pcolor(self, alpha, norm, cmap, vmin, vmax, *args, **kwargs). 5773 kwargs.setdefault('snap', False). 5774 . -> 5775 collection = mcoll.PolyCollection(verts, **kwargs). 5776 . 5777 collection.set_alpha(alpha). ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\matplotlib\collections.py in __init__(self, verts, sizes, closed, **kwargs). 931 %(Collection)s. 932 """""". --> 933 Collection.__init__(self, **kwargs). 934 self.set_sizes(sizes). 935 self.set_verts(verts, closed). ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\matplotlib\collections.py in __init__(self, edgecolors, facecolors, linewidths, linestyles, capstyle, joinstyle, antialiaseds, offsets, transOffset, norm, cmap, pickradius, hatch, urls, offset_position, zorder, **kwargs). 164 . 165 self._path_effects = None. --> 166 self.update(kwargs). 167 self._paths = None. 168 . ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\matplotlib\artist.py in update(self, props). 914 . 915 with cbook._setattr_cm(self, eventson=False):. --> 916 ret = [_update_property(self, k, v) for k, v in props.items()]. 917 . 918 if len(ret):. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\matplotlib\artist.py in <listcomp>(.0). 914 . 915 with cbook._setattr_cm(self, eventson=False):. --> 916 ret = [_update_property(self, k, v) for k, v in props",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/565
https://github.com/scverse/scanpy/issues/565:1878,usability,close,closed,1878,"x_ax.pcolor(mean_obs, edgecolor='gray', **kwds). 1686 . 1687 # invert y axis to show categories ordered from top to bottom. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\matplotlib\__init__.py in inner(ax, data, *args, **kwargs). 1808 ""the Matplotlib list!)"" % (label_namer, func.__name__),. 1809 RuntimeWarning, stacklevel=2). -> 1810 return func(ax, *args, **kwargs). 1811 . 1812 inner.__doc__ = _add_data_doc(inner.__doc__,. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\matplotlib\axes\_axes.py in pcolor(self, alpha, norm, cmap, vmin, vmax, *args, **kwargs). 5773 kwargs.setdefault('snap', False). 5774 . -> 5775 collection = mcoll.PolyCollection(verts, **kwargs). 5776 . 5777 collection.set_alpha(alpha). ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\matplotlib\collections.py in __init__(self, verts, sizes, closed, **kwargs). 931 %(Collection)s. 932 """""". --> 933 Collection.__init__(self, **kwargs). 934 self.set_sizes(sizes). 935 self.set_verts(verts, closed). ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\matplotlib\collections.py in __init__(self, edgecolors, facecolors, linewidths, linestyles, capstyle, joinstyle, antialiaseds, offsets, transOffset, norm, cmap, pickradius, hatch, urls, offset_position, zorder, **kwargs). 164 . 165 self._path_effects = None. --> 166 self.update(kwargs). 167 self._paths = None. 168 . ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\matplotlib\artist.py in update(self, props). 914 . 915 with cbook._setattr_cm(self, eventson=False):. --> 916 ret = [_update_property(self, k, v) for k, v in props.items()]. 917 . 918 if len(ret):. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\matplotlib\artist.py in <listcomp>(.0). 914 . 915 with cbook._setattr_cm(self, eventson=False):. --> 916 ret = [_update_property(self, k, v) for k, v in props.items()]. 917 . 918 if len(ret):. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\matplotlib\artist.py in _update_property(self, k, v).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/565
https://github.com/scverse/scanpy/issues/566:12,availability,error,error,12,"Math domain error when using the Wilcoxon rank-sum; Another error I get and have no idea how to solve is when using the Wilcoxon rank-sum for testing for differential gene expression:. `sc.tl.rank_genes_groups(adata, 'louvain', method='wilcoxon'). sc.pl.rank_genes_groups(adata, n_genes=25, sharey=False)`. ```. ranking genes. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-385-c2fa7bb8ea8d> in <module>. ----> 1 sc.tl.rank_genes_groups(adata, 'louvain', method='wilcoxon'). 2 sc.pl.rank_genes_groups(adata, n_genes=25, sharey=False). ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\tools\_rank_genes_groups.py in rank_genes_groups(adata, groupby, use_raw, groups, reference, n_genes, rankby_abs, key_added, copy, method, corr_method, **kwds). 352 . 353 scores[imask, :] = (scores[imask, :] - (ns[imask] * (n_cells + 1) / 2)) / sqrt(. --> 354 (ns[imask] * (n_cells - ns[imask]) * (n_cells + 1) / 12)). 355 scores[np.isnan(scores)] = 0. 356 pvals = 2 * stats.distributions.norm.sf(np.abs(scores[imask,:])). ValueError: math domain error. ```. The logistic regression and t-test work fine. I guess it is related to my data....",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/566
https://github.com/scverse/scanpy/issues/566:60,availability,error,error,60,"Math domain error when using the Wilcoxon rank-sum; Another error I get and have no idea how to solve is when using the Wilcoxon rank-sum for testing for differential gene expression:. `sc.tl.rank_genes_groups(adata, 'louvain', method='wilcoxon'). sc.pl.rank_genes_groups(adata, n_genes=25, sharey=False)`. ```. ranking genes. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-385-c2fa7bb8ea8d> in <module>. ----> 1 sc.tl.rank_genes_groups(adata, 'louvain', method='wilcoxon'). 2 sc.pl.rank_genes_groups(adata, n_genes=25, sharey=False). ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\tools\_rank_genes_groups.py in rank_genes_groups(adata, groupby, use_raw, groups, reference, n_genes, rankby_abs, key_added, copy, method, corr_method, **kwds). 352 . 353 scores[imask, :] = (scores[imask, :] - (ns[imask] * (n_cells + 1) / 2)) / sqrt(. --> 354 (ns[imask] * (n_cells - ns[imask]) * (n_cells + 1) / 12)). 355 scores[np.isnan(scores)] = 0. 356 pvals = 2 * stats.distributions.norm.sf(np.abs(scores[imask,:])). ValueError: math domain error. ```. The logistic regression and t-test work fine. I guess it is related to my data....",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/566
https://github.com/scverse/scanpy/issues/566:1138,availability,error,error,1138,"Math domain error when using the Wilcoxon rank-sum; Another error I get and have no idea how to solve is when using the Wilcoxon rank-sum for testing for differential gene expression:. `sc.tl.rank_genes_groups(adata, 'louvain', method='wilcoxon'). sc.pl.rank_genes_groups(adata, n_genes=25, sharey=False)`. ```. ranking genes. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-385-c2fa7bb8ea8d> in <module>. ----> 1 sc.tl.rank_genes_groups(adata, 'louvain', method='wilcoxon'). 2 sc.pl.rank_genes_groups(adata, n_genes=25, sharey=False). ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\tools\_rank_genes_groups.py in rank_genes_groups(adata, groupby, use_raw, groups, reference, n_genes, rankby_abs, key_added, copy, method, corr_method, **kwds). 352 . 353 scores[imask, :] = (scores[imask, :] - (ns[imask] * (n_cells + 1) / 2)) / sqrt(. --> 354 (ns[imask] * (n_cells - ns[imask]) * (n_cells + 1) / 12)). 355 scores[np.isnan(scores)] = 0. 356 pvals = 2 * stats.distributions.norm.sf(np.abs(scores[imask,:])). ValueError: math domain error. ```. The logistic regression and t-test work fine. I guess it is related to my data....",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/566
https://github.com/scverse/scanpy/issues/566:487,deployability,modul,module,487,"Math domain error when using the Wilcoxon rank-sum; Another error I get and have no idea how to solve is when using the Wilcoxon rank-sum for testing for differential gene expression:. `sc.tl.rank_genes_groups(adata, 'louvain', method='wilcoxon'). sc.pl.rank_genes_groups(adata, n_genes=25, sharey=False)`. ```. ranking genes. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-385-c2fa7bb8ea8d> in <module>. ----> 1 sc.tl.rank_genes_groups(adata, 'louvain', method='wilcoxon'). 2 sc.pl.rank_genes_groups(adata, n_genes=25, sharey=False). ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\tools\_rank_genes_groups.py in rank_genes_groups(adata, groupby, use_raw, groups, reference, n_genes, rankby_abs, key_added, copy, method, corr_method, **kwds). 352 . 353 scores[imask, :] = (scores[imask, :] - (ns[imask] * (n_cells + 1) / 2)) / sqrt(. --> 354 (ns[imask] * (n_cells - ns[imask]) * (n_cells + 1) / 12)). 355 scores[np.isnan(scores)] = 0. 356 pvals = 2 * stats.distributions.norm.sf(np.abs(scores[imask,:])). ValueError: math domain error. ```. The logistic regression and t-test work fine. I guess it is related to my data....",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/566
https://github.com/scverse/scanpy/issues/566:1154,deployability,log,logistic,1154,"Math domain error when using the Wilcoxon rank-sum; Another error I get and have no idea how to solve is when using the Wilcoxon rank-sum for testing for differential gene expression:. `sc.tl.rank_genes_groups(adata, 'louvain', method='wilcoxon'). sc.pl.rank_genes_groups(adata, n_genes=25, sharey=False)`. ```. ranking genes. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-385-c2fa7bb8ea8d> in <module>. ----> 1 sc.tl.rank_genes_groups(adata, 'louvain', method='wilcoxon'). 2 sc.pl.rank_genes_groups(adata, n_genes=25, sharey=False). ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\tools\_rank_genes_groups.py in rank_genes_groups(adata, groupby, use_raw, groups, reference, n_genes, rankby_abs, key_added, copy, method, corr_method, **kwds). 352 . 353 scores[imask, :] = (scores[imask, :] - (ns[imask] * (n_cells + 1) / 2)) / sqrt(. --> 354 (ns[imask] * (n_cells - ns[imask]) * (n_cells + 1) / 12)). 355 scores[np.isnan(scores)] = 0. 356 pvals = 2 * stats.distributions.norm.sf(np.abs(scores[imask,:])). ValueError: math domain error. ```. The logistic regression and t-test work fine. I guess it is related to my data....",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/566
https://github.com/scverse/scanpy/issues/566:291,interoperability,share,sharey,291,"Math domain error when using the Wilcoxon rank-sum; Another error I get and have no idea how to solve is when using the Wilcoxon rank-sum for testing for differential gene expression:. `sc.tl.rank_genes_groups(adata, 'louvain', method='wilcoxon'). sc.pl.rank_genes_groups(adata, n_genes=25, sharey=False)`. ```. ranking genes. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-385-c2fa7bb8ea8d> in <module>. ----> 1 sc.tl.rank_genes_groups(adata, 'louvain', method='wilcoxon'). 2 sc.pl.rank_genes_groups(adata, n_genes=25, sharey=False). ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\tools\_rank_genes_groups.py in rank_genes_groups(adata, groupby, use_raw, groups, reference, n_genes, rankby_abs, key_added, copy, method, corr_method, **kwds). 352 . 353 scores[imask, :] = (scores[imask, :] - (ns[imask] * (n_cells + 1) / 2)) / sqrt(. --> 354 (ns[imask] * (n_cells - ns[imask]) * (n_cells + 1) / 12)). 355 scores[np.isnan(scores)] = 0. 356 pvals = 2 * stats.distributions.norm.sf(np.abs(scores[imask,:])). ValueError: math domain error. ```. The logistic regression and t-test work fine. I guess it is related to my data....",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/566
https://github.com/scverse/scanpy/issues/566:611,interoperability,share,sharey,611,"Math domain error when using the Wilcoxon rank-sum; Another error I get and have no idea how to solve is when using the Wilcoxon rank-sum for testing for differential gene expression:. `sc.tl.rank_genes_groups(adata, 'louvain', method='wilcoxon'). sc.pl.rank_genes_groups(adata, n_genes=25, sharey=False)`. ```. ranking genes. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-385-c2fa7bb8ea8d> in <module>. ----> 1 sc.tl.rank_genes_groups(adata, 'louvain', method='wilcoxon'). 2 sc.pl.rank_genes_groups(adata, n_genes=25, sharey=False). ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\tools\_rank_genes_groups.py in rank_genes_groups(adata, groupby, use_raw, groups, reference, n_genes, rankby_abs, key_added, copy, method, corr_method, **kwds). 352 . 353 scores[imask, :] = (scores[imask, :] - (ns[imask] * (n_cells + 1) / 2)) / sqrt(. --> 354 (ns[imask] * (n_cells - ns[imask]) * (n_cells + 1) / 12)). 355 scores[np.isnan(scores)] = 0. 356 pvals = 2 * stats.distributions.norm.sf(np.abs(scores[imask,:])). ValueError: math domain error. ```. The logistic regression and t-test work fine. I guess it is related to my data....",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/566
https://github.com/scverse/scanpy/issues/566:1066,interoperability,distribut,distributions,1066,"Math domain error when using the Wilcoxon rank-sum; Another error I get and have no idea how to solve is when using the Wilcoxon rank-sum for testing for differential gene expression:. `sc.tl.rank_genes_groups(adata, 'louvain', method='wilcoxon'). sc.pl.rank_genes_groups(adata, n_genes=25, sharey=False)`. ```. ranking genes. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-385-c2fa7bb8ea8d> in <module>. ----> 1 sc.tl.rank_genes_groups(adata, 'louvain', method='wilcoxon'). 2 sc.pl.rank_genes_groups(adata, n_genes=25, sharey=False). ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\tools\_rank_genes_groups.py in rank_genes_groups(adata, groupby, use_raw, groups, reference, n_genes, rankby_abs, key_added, copy, method, corr_method, **kwds). 352 . 353 scores[imask, :] = (scores[imask, :] - (ns[imask] * (n_cells + 1) / 2)) / sqrt(. --> 354 (ns[imask] * (n_cells - ns[imask]) * (n_cells + 1) / 12)). 355 scores[np.isnan(scores)] = 0. 356 pvals = 2 * stats.distributions.norm.sf(np.abs(scores[imask,:])). ValueError: math domain error. ```. The logistic regression and t-test work fine. I guess it is related to my data....",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/566
https://github.com/scverse/scanpy/issues/566:487,modifiability,modul,module,487,"Math domain error when using the Wilcoxon rank-sum; Another error I get and have no idea how to solve is when using the Wilcoxon rank-sum for testing for differential gene expression:. `sc.tl.rank_genes_groups(adata, 'louvain', method='wilcoxon'). sc.pl.rank_genes_groups(adata, n_genes=25, sharey=False)`. ```. ranking genes. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-385-c2fa7bb8ea8d> in <module>. ----> 1 sc.tl.rank_genes_groups(adata, 'louvain', method='wilcoxon'). 2 sc.pl.rank_genes_groups(adata, n_genes=25, sharey=False). ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\tools\_rank_genes_groups.py in rank_genes_groups(adata, groupby, use_raw, groups, reference, n_genes, rankby_abs, key_added, copy, method, corr_method, **kwds). 352 . 353 scores[imask, :] = (scores[imask, :] - (ns[imask] * (n_cells + 1) / 2)) / sqrt(. --> 354 (ns[imask] * (n_cells - ns[imask]) * (n_cells + 1) / 12)). 355 scores[np.isnan(scores)] = 0. 356 pvals = 2 * stats.distributions.norm.sf(np.abs(scores[imask,:])). ValueError: math domain error. ```. The logistic regression and t-test work fine. I guess it is related to my data....",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/566
https://github.com/scverse/scanpy/issues/566:675,modifiability,pac,packages,675,"Math domain error when using the Wilcoxon rank-sum; Another error I get and have no idea how to solve is when using the Wilcoxon rank-sum for testing for differential gene expression:. `sc.tl.rank_genes_groups(adata, 'louvain', method='wilcoxon'). sc.pl.rank_genes_groups(adata, n_genes=25, sharey=False)`. ```. ranking genes. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-385-c2fa7bb8ea8d> in <module>. ----> 1 sc.tl.rank_genes_groups(adata, 'louvain', method='wilcoxon'). 2 sc.pl.rank_genes_groups(adata, n_genes=25, sharey=False). ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\tools\_rank_genes_groups.py in rank_genes_groups(adata, groupby, use_raw, groups, reference, n_genes, rankby_abs, key_added, copy, method, corr_method, **kwds). 352 . 353 scores[imask, :] = (scores[imask, :] - (ns[imask] * (n_cells + 1) / 2)) / sqrt(. --> 354 (ns[imask] * (n_cells - ns[imask]) * (n_cells + 1) / 12)). 355 scores[np.isnan(scores)] = 0. 356 pvals = 2 * stats.distributions.norm.sf(np.abs(scores[imask,:])). ValueError: math domain error. ```. The logistic regression and t-test work fine. I guess it is related to my data....",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/566
https://github.com/scverse/scanpy/issues/566:12,performance,error,error,12,"Math domain error when using the Wilcoxon rank-sum; Another error I get and have no idea how to solve is when using the Wilcoxon rank-sum for testing for differential gene expression:. `sc.tl.rank_genes_groups(adata, 'louvain', method='wilcoxon'). sc.pl.rank_genes_groups(adata, n_genes=25, sharey=False)`. ```. ranking genes. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-385-c2fa7bb8ea8d> in <module>. ----> 1 sc.tl.rank_genes_groups(adata, 'louvain', method='wilcoxon'). 2 sc.pl.rank_genes_groups(adata, n_genes=25, sharey=False). ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\tools\_rank_genes_groups.py in rank_genes_groups(adata, groupby, use_raw, groups, reference, n_genes, rankby_abs, key_added, copy, method, corr_method, **kwds). 352 . 353 scores[imask, :] = (scores[imask, :] - (ns[imask] * (n_cells + 1) / 2)) / sqrt(. --> 354 (ns[imask] * (n_cells - ns[imask]) * (n_cells + 1) / 12)). 355 scores[np.isnan(scores)] = 0. 356 pvals = 2 * stats.distributions.norm.sf(np.abs(scores[imask,:])). ValueError: math domain error. ```. The logistic regression and t-test work fine. I guess it is related to my data....",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/566
https://github.com/scverse/scanpy/issues/566:60,performance,error,error,60,"Math domain error when using the Wilcoxon rank-sum; Another error I get and have no idea how to solve is when using the Wilcoxon rank-sum for testing for differential gene expression:. `sc.tl.rank_genes_groups(adata, 'louvain', method='wilcoxon'). sc.pl.rank_genes_groups(adata, n_genes=25, sharey=False)`. ```. ranking genes. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-385-c2fa7bb8ea8d> in <module>. ----> 1 sc.tl.rank_genes_groups(adata, 'louvain', method='wilcoxon'). 2 sc.pl.rank_genes_groups(adata, n_genes=25, sharey=False). ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\tools\_rank_genes_groups.py in rank_genes_groups(adata, groupby, use_raw, groups, reference, n_genes, rankby_abs, key_added, copy, method, corr_method, **kwds). 352 . 353 scores[imask, :] = (scores[imask, :] - (ns[imask] * (n_cells + 1) / 2)) / sqrt(. --> 354 (ns[imask] * (n_cells - ns[imask]) * (n_cells + 1) / 12)). 355 scores[np.isnan(scores)] = 0. 356 pvals = 2 * stats.distributions.norm.sf(np.abs(scores[imask,:])). ValueError: math domain error. ```. The logistic regression and t-test work fine. I guess it is related to my data....",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/566
https://github.com/scverse/scanpy/issues/566:1138,performance,error,error,1138,"Math domain error when using the Wilcoxon rank-sum; Another error I get and have no idea how to solve is when using the Wilcoxon rank-sum for testing for differential gene expression:. `sc.tl.rank_genes_groups(adata, 'louvain', method='wilcoxon'). sc.pl.rank_genes_groups(adata, n_genes=25, sharey=False)`. ```. ranking genes. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-385-c2fa7bb8ea8d> in <module>. ----> 1 sc.tl.rank_genes_groups(adata, 'louvain', method='wilcoxon'). 2 sc.pl.rank_genes_groups(adata, n_genes=25, sharey=False). ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\tools\_rank_genes_groups.py in rank_genes_groups(adata, groupby, use_raw, groups, reference, n_genes, rankby_abs, key_added, copy, method, corr_method, **kwds). 352 . 353 scores[imask, :] = (scores[imask, :] - (ns[imask] * (n_cells + 1) / 2)) / sqrt(. --> 354 (ns[imask] * (n_cells - ns[imask]) * (n_cells + 1) / 12)). 355 scores[np.isnan(scores)] = 0. 356 pvals = 2 * stats.distributions.norm.sf(np.abs(scores[imask,:])). ValueError: math domain error. ```. The logistic regression and t-test work fine. I guess it is related to my data....",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/566
https://github.com/scverse/scanpy/issues/566:12,safety,error,error,12,"Math domain error when using the Wilcoxon rank-sum; Another error I get and have no idea how to solve is when using the Wilcoxon rank-sum for testing for differential gene expression:. `sc.tl.rank_genes_groups(adata, 'louvain', method='wilcoxon'). sc.pl.rank_genes_groups(adata, n_genes=25, sharey=False)`. ```. ranking genes. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-385-c2fa7bb8ea8d> in <module>. ----> 1 sc.tl.rank_genes_groups(adata, 'louvain', method='wilcoxon'). 2 sc.pl.rank_genes_groups(adata, n_genes=25, sharey=False). ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\tools\_rank_genes_groups.py in rank_genes_groups(adata, groupby, use_raw, groups, reference, n_genes, rankby_abs, key_added, copy, method, corr_method, **kwds). 352 . 353 scores[imask, :] = (scores[imask, :] - (ns[imask] * (n_cells + 1) / 2)) / sqrt(. --> 354 (ns[imask] * (n_cells - ns[imask]) * (n_cells + 1) / 12)). 355 scores[np.isnan(scores)] = 0. 356 pvals = 2 * stats.distributions.norm.sf(np.abs(scores[imask,:])). ValueError: math domain error. ```. The logistic regression and t-test work fine. I guess it is related to my data....",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/566
https://github.com/scverse/scanpy/issues/566:60,safety,error,error,60,"Math domain error when using the Wilcoxon rank-sum; Another error I get and have no idea how to solve is when using the Wilcoxon rank-sum for testing for differential gene expression:. `sc.tl.rank_genes_groups(adata, 'louvain', method='wilcoxon'). sc.pl.rank_genes_groups(adata, n_genes=25, sharey=False)`. ```. ranking genes. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-385-c2fa7bb8ea8d> in <module>. ----> 1 sc.tl.rank_genes_groups(adata, 'louvain', method='wilcoxon'). 2 sc.pl.rank_genes_groups(adata, n_genes=25, sharey=False). ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\tools\_rank_genes_groups.py in rank_genes_groups(adata, groupby, use_raw, groups, reference, n_genes, rankby_abs, key_added, copy, method, corr_method, **kwds). 352 . 353 scores[imask, :] = (scores[imask, :] - (ns[imask] * (n_cells + 1) / 2)) / sqrt(. --> 354 (ns[imask] * (n_cells - ns[imask]) * (n_cells + 1) / 12)). 355 scores[np.isnan(scores)] = 0. 356 pvals = 2 * stats.distributions.norm.sf(np.abs(scores[imask,:])). ValueError: math domain error. ```. The logistic regression and t-test work fine. I guess it is related to my data....",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/566
https://github.com/scverse/scanpy/issues/566:142,safety,test,testing,142,"Math domain error when using the Wilcoxon rank-sum; Another error I get and have no idea how to solve is when using the Wilcoxon rank-sum for testing for differential gene expression:. `sc.tl.rank_genes_groups(adata, 'louvain', method='wilcoxon'). sc.pl.rank_genes_groups(adata, n_genes=25, sharey=False)`. ```. ranking genes. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-385-c2fa7bb8ea8d> in <module>. ----> 1 sc.tl.rank_genes_groups(adata, 'louvain', method='wilcoxon'). 2 sc.pl.rank_genes_groups(adata, n_genes=25, sharey=False). ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\tools\_rank_genes_groups.py in rank_genes_groups(adata, groupby, use_raw, groups, reference, n_genes, rankby_abs, key_added, copy, method, corr_method, **kwds). 352 . 353 scores[imask, :] = (scores[imask, :] - (ns[imask] * (n_cells + 1) / 2)) / sqrt(. --> 354 (ns[imask] * (n_cells - ns[imask]) * (n_cells + 1) / 12)). 355 scores[np.isnan(scores)] = 0. 356 pvals = 2 * stats.distributions.norm.sf(np.abs(scores[imask,:])). ValueError: math domain error. ```. The logistic regression and t-test work fine. I guess it is related to my data....",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/566
https://github.com/scverse/scanpy/issues/566:459,safety,input,input-,459,"Math domain error when using the Wilcoxon rank-sum; Another error I get and have no idea how to solve is when using the Wilcoxon rank-sum for testing for differential gene expression:. `sc.tl.rank_genes_groups(adata, 'louvain', method='wilcoxon'). sc.pl.rank_genes_groups(adata, n_genes=25, sharey=False)`. ```. ranking genes. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-385-c2fa7bb8ea8d> in <module>. ----> 1 sc.tl.rank_genes_groups(adata, 'louvain', method='wilcoxon'). 2 sc.pl.rank_genes_groups(adata, n_genes=25, sharey=False). ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\tools\_rank_genes_groups.py in rank_genes_groups(adata, groupby, use_raw, groups, reference, n_genes, rankby_abs, key_added, copy, method, corr_method, **kwds). 352 . 353 scores[imask, :] = (scores[imask, :] - (ns[imask] * (n_cells + 1) / 2)) / sqrt(. --> 354 (ns[imask] * (n_cells - ns[imask]) * (n_cells + 1) / 12)). 355 scores[np.isnan(scores)] = 0. 356 pvals = 2 * stats.distributions.norm.sf(np.abs(scores[imask,:])). ValueError: math domain error. ```. The logistic regression and t-test work fine. I guess it is related to my data....",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/566
https://github.com/scverse/scanpy/issues/566:487,safety,modul,module,487,"Math domain error when using the Wilcoxon rank-sum; Another error I get and have no idea how to solve is when using the Wilcoxon rank-sum for testing for differential gene expression:. `sc.tl.rank_genes_groups(adata, 'louvain', method='wilcoxon'). sc.pl.rank_genes_groups(adata, n_genes=25, sharey=False)`. ```. ranking genes. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-385-c2fa7bb8ea8d> in <module>. ----> 1 sc.tl.rank_genes_groups(adata, 'louvain', method='wilcoxon'). 2 sc.pl.rank_genes_groups(adata, n_genes=25, sharey=False). ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\tools\_rank_genes_groups.py in rank_genes_groups(adata, groupby, use_raw, groups, reference, n_genes, rankby_abs, key_added, copy, method, corr_method, **kwds). 352 . 353 scores[imask, :] = (scores[imask, :] - (ns[imask] * (n_cells + 1) / 2)) / sqrt(. --> 354 (ns[imask] * (n_cells - ns[imask]) * (n_cells + 1) / 12)). 355 scores[np.isnan(scores)] = 0. 356 pvals = 2 * stats.distributions.norm.sf(np.abs(scores[imask,:])). ValueError: math domain error. ```. The logistic regression and t-test work fine. I guess it is related to my data....",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/566
https://github.com/scverse/scanpy/issues/566:1138,safety,error,error,1138,"Math domain error when using the Wilcoxon rank-sum; Another error I get and have no idea how to solve is when using the Wilcoxon rank-sum for testing for differential gene expression:. `sc.tl.rank_genes_groups(adata, 'louvain', method='wilcoxon'). sc.pl.rank_genes_groups(adata, n_genes=25, sharey=False)`. ```. ranking genes. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-385-c2fa7bb8ea8d> in <module>. ----> 1 sc.tl.rank_genes_groups(adata, 'louvain', method='wilcoxon'). 2 sc.pl.rank_genes_groups(adata, n_genes=25, sharey=False). ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\tools\_rank_genes_groups.py in rank_genes_groups(adata, groupby, use_raw, groups, reference, n_genes, rankby_abs, key_added, copy, method, corr_method, **kwds). 352 . 353 scores[imask, :] = (scores[imask, :] - (ns[imask] * (n_cells + 1) / 2)) / sqrt(. --> 354 (ns[imask] * (n_cells - ns[imask]) * (n_cells + 1) / 12)). 355 scores[np.isnan(scores)] = 0. 356 pvals = 2 * stats.distributions.norm.sf(np.abs(scores[imask,:])). ValueError: math domain error. ```. The logistic regression and t-test work fine. I guess it is related to my data....",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/566
https://github.com/scverse/scanpy/issues/566:1154,safety,log,logistic,1154,"Math domain error when using the Wilcoxon rank-sum; Another error I get and have no idea how to solve is when using the Wilcoxon rank-sum for testing for differential gene expression:. `sc.tl.rank_genes_groups(adata, 'louvain', method='wilcoxon'). sc.pl.rank_genes_groups(adata, n_genes=25, sharey=False)`. ```. ranking genes. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-385-c2fa7bb8ea8d> in <module>. ----> 1 sc.tl.rank_genes_groups(adata, 'louvain', method='wilcoxon'). 2 sc.pl.rank_genes_groups(adata, n_genes=25, sharey=False). ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\tools\_rank_genes_groups.py in rank_genes_groups(adata, groupby, use_raw, groups, reference, n_genes, rankby_abs, key_added, copy, method, corr_method, **kwds). 352 . 353 scores[imask, :] = (scores[imask, :] - (ns[imask] * (n_cells + 1) / 2)) / sqrt(. --> 354 (ns[imask] * (n_cells - ns[imask]) * (n_cells + 1) / 12)). 355 scores[np.isnan(scores)] = 0. 356 pvals = 2 * stats.distributions.norm.sf(np.abs(scores[imask,:])). ValueError: math domain error. ```. The logistic regression and t-test work fine. I guess it is related to my data....",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/566
https://github.com/scverse/scanpy/issues/566:1180,safety,test,test,1180,"Math domain error when using the Wilcoxon rank-sum; Another error I get and have no idea how to solve is when using the Wilcoxon rank-sum for testing for differential gene expression:. `sc.tl.rank_genes_groups(adata, 'louvain', method='wilcoxon'). sc.pl.rank_genes_groups(adata, n_genes=25, sharey=False)`. ```. ranking genes. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-385-c2fa7bb8ea8d> in <module>. ----> 1 sc.tl.rank_genes_groups(adata, 'louvain', method='wilcoxon'). 2 sc.pl.rank_genes_groups(adata, n_genes=25, sharey=False). ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\tools\_rank_genes_groups.py in rank_genes_groups(adata, groupby, use_raw, groups, reference, n_genes, rankby_abs, key_added, copy, method, corr_method, **kwds). 352 . 353 scores[imask, :] = (scores[imask, :] - (ns[imask] * (n_cells + 1) / 2)) / sqrt(. --> 354 (ns[imask] * (n_cells - ns[imask]) * (n_cells + 1) / 12)). 355 scores[np.isnan(scores)] = 0. 356 pvals = 2 * stats.distributions.norm.sf(np.abs(scores[imask,:])). ValueError: math domain error. ```. The logistic regression and t-test work fine. I guess it is related to my data....",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/566
https://github.com/scverse/scanpy/issues/566:1154,security,log,logistic,1154,"Math domain error when using the Wilcoxon rank-sum; Another error I get and have no idea how to solve is when using the Wilcoxon rank-sum for testing for differential gene expression:. `sc.tl.rank_genes_groups(adata, 'louvain', method='wilcoxon'). sc.pl.rank_genes_groups(adata, n_genes=25, sharey=False)`. ```. ranking genes. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-385-c2fa7bb8ea8d> in <module>. ----> 1 sc.tl.rank_genes_groups(adata, 'louvain', method='wilcoxon'). 2 sc.pl.rank_genes_groups(adata, n_genes=25, sharey=False). ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\tools\_rank_genes_groups.py in rank_genes_groups(adata, groupby, use_raw, groups, reference, n_genes, rankby_abs, key_added, copy, method, corr_method, **kwds). 352 . 353 scores[imask, :] = (scores[imask, :] - (ns[imask] * (n_cells + 1) / 2)) / sqrt(. --> 354 (ns[imask] * (n_cells - ns[imask]) * (n_cells + 1) / 12)). 355 scores[np.isnan(scores)] = 0. 356 pvals = 2 * stats.distributions.norm.sf(np.abs(scores[imask,:])). ValueError: math domain error. ```. The logistic regression and t-test work fine. I guess it is related to my data....",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/566
https://github.com/scverse/scanpy/issues/566:142,testability,test,testing,142,"Math domain error when using the Wilcoxon rank-sum; Another error I get and have no idea how to solve is when using the Wilcoxon rank-sum for testing for differential gene expression:. `sc.tl.rank_genes_groups(adata, 'louvain', method='wilcoxon'). sc.pl.rank_genes_groups(adata, n_genes=25, sharey=False)`. ```. ranking genes. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-385-c2fa7bb8ea8d> in <module>. ----> 1 sc.tl.rank_genes_groups(adata, 'louvain', method='wilcoxon'). 2 sc.pl.rank_genes_groups(adata, n_genes=25, sharey=False). ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\tools\_rank_genes_groups.py in rank_genes_groups(adata, groupby, use_raw, groups, reference, n_genes, rankby_abs, key_added, copy, method, corr_method, **kwds). 352 . 353 scores[imask, :] = (scores[imask, :] - (ns[imask] * (n_cells + 1) / 2)) / sqrt(. --> 354 (ns[imask] * (n_cells - ns[imask]) * (n_cells + 1) / 12)). 355 scores[np.isnan(scores)] = 0. 356 pvals = 2 * stats.distributions.norm.sf(np.abs(scores[imask,:])). ValueError: math domain error. ```. The logistic regression and t-test work fine. I guess it is related to my data....",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/566
https://github.com/scverse/scanpy/issues/566:415,testability,Trace,Traceback,415,"Math domain error when using the Wilcoxon rank-sum; Another error I get and have no idea how to solve is when using the Wilcoxon rank-sum for testing for differential gene expression:. `sc.tl.rank_genes_groups(adata, 'louvain', method='wilcoxon'). sc.pl.rank_genes_groups(adata, n_genes=25, sharey=False)`. ```. ranking genes. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-385-c2fa7bb8ea8d> in <module>. ----> 1 sc.tl.rank_genes_groups(adata, 'louvain', method='wilcoxon'). 2 sc.pl.rank_genes_groups(adata, n_genes=25, sharey=False). ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\tools\_rank_genes_groups.py in rank_genes_groups(adata, groupby, use_raw, groups, reference, n_genes, rankby_abs, key_added, copy, method, corr_method, **kwds). 352 . 353 scores[imask, :] = (scores[imask, :] - (ns[imask] * (n_cells + 1) / 2)) / sqrt(. --> 354 (ns[imask] * (n_cells - ns[imask]) * (n_cells + 1) / 12)). 355 scores[np.isnan(scores)] = 0. 356 pvals = 2 * stats.distributions.norm.sf(np.abs(scores[imask,:])). ValueError: math domain error. ```. The logistic regression and t-test work fine. I guess it is related to my data....",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/566
https://github.com/scverse/scanpy/issues/566:1154,testability,log,logistic,1154,"Math domain error when using the Wilcoxon rank-sum; Another error I get and have no idea how to solve is when using the Wilcoxon rank-sum for testing for differential gene expression:. `sc.tl.rank_genes_groups(adata, 'louvain', method='wilcoxon'). sc.pl.rank_genes_groups(adata, n_genes=25, sharey=False)`. ```. ranking genes. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-385-c2fa7bb8ea8d> in <module>. ----> 1 sc.tl.rank_genes_groups(adata, 'louvain', method='wilcoxon'). 2 sc.pl.rank_genes_groups(adata, n_genes=25, sharey=False). ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\tools\_rank_genes_groups.py in rank_genes_groups(adata, groupby, use_raw, groups, reference, n_genes, rankby_abs, key_added, copy, method, corr_method, **kwds). 352 . 353 scores[imask, :] = (scores[imask, :] - (ns[imask] * (n_cells + 1) / 2)) / sqrt(. --> 354 (ns[imask] * (n_cells - ns[imask]) * (n_cells + 1) / 12)). 355 scores[np.isnan(scores)] = 0. 356 pvals = 2 * stats.distributions.norm.sf(np.abs(scores[imask,:])). ValueError: math domain error. ```. The logistic regression and t-test work fine. I guess it is related to my data....",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/566
https://github.com/scverse/scanpy/issues/566:1163,testability,regress,regression,1163,"Math domain error when using the Wilcoxon rank-sum; Another error I get and have no idea how to solve is when using the Wilcoxon rank-sum for testing for differential gene expression:. `sc.tl.rank_genes_groups(adata, 'louvain', method='wilcoxon'). sc.pl.rank_genes_groups(adata, n_genes=25, sharey=False)`. ```. ranking genes. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-385-c2fa7bb8ea8d> in <module>. ----> 1 sc.tl.rank_genes_groups(adata, 'louvain', method='wilcoxon'). 2 sc.pl.rank_genes_groups(adata, n_genes=25, sharey=False). ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\tools\_rank_genes_groups.py in rank_genes_groups(adata, groupby, use_raw, groups, reference, n_genes, rankby_abs, key_added, copy, method, corr_method, **kwds). 352 . 353 scores[imask, :] = (scores[imask, :] - (ns[imask] * (n_cells + 1) / 2)) / sqrt(. --> 354 (ns[imask] * (n_cells - ns[imask]) * (n_cells + 1) / 12)). 355 scores[np.isnan(scores)] = 0. 356 pvals = 2 * stats.distributions.norm.sf(np.abs(scores[imask,:])). ValueError: math domain error. ```. The logistic regression and t-test work fine. I guess it is related to my data....",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/566
https://github.com/scverse/scanpy/issues/566:1180,testability,test,test,1180,"Math domain error when using the Wilcoxon rank-sum; Another error I get and have no idea how to solve is when using the Wilcoxon rank-sum for testing for differential gene expression:. `sc.tl.rank_genes_groups(adata, 'louvain', method='wilcoxon'). sc.pl.rank_genes_groups(adata, n_genes=25, sharey=False)`. ```. ranking genes. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-385-c2fa7bb8ea8d> in <module>. ----> 1 sc.tl.rank_genes_groups(adata, 'louvain', method='wilcoxon'). 2 sc.pl.rank_genes_groups(adata, n_genes=25, sharey=False). ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\tools\_rank_genes_groups.py in rank_genes_groups(adata, groupby, use_raw, groups, reference, n_genes, rankby_abs, key_added, copy, method, corr_method, **kwds). 352 . 353 scores[imask, :] = (scores[imask, :] - (ns[imask] * (n_cells + 1) / 2)) / sqrt(. --> 354 (ns[imask] * (n_cells - ns[imask]) * (n_cells + 1) / 12)). 355 scores[np.isnan(scores)] = 0. 356 pvals = 2 * stats.distributions.norm.sf(np.abs(scores[imask,:])). ValueError: math domain error. ```. The logistic regression and t-test work fine. I guess it is related to my data....",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/566
https://github.com/scverse/scanpy/issues/566:12,usability,error,error,12,"Math domain error when using the Wilcoxon rank-sum; Another error I get and have no idea how to solve is when using the Wilcoxon rank-sum for testing for differential gene expression:. `sc.tl.rank_genes_groups(adata, 'louvain', method='wilcoxon'). sc.pl.rank_genes_groups(adata, n_genes=25, sharey=False)`. ```. ranking genes. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-385-c2fa7bb8ea8d> in <module>. ----> 1 sc.tl.rank_genes_groups(adata, 'louvain', method='wilcoxon'). 2 sc.pl.rank_genes_groups(adata, n_genes=25, sharey=False). ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\tools\_rank_genes_groups.py in rank_genes_groups(adata, groupby, use_raw, groups, reference, n_genes, rankby_abs, key_added, copy, method, corr_method, **kwds). 352 . 353 scores[imask, :] = (scores[imask, :] - (ns[imask] * (n_cells + 1) / 2)) / sqrt(. --> 354 (ns[imask] * (n_cells - ns[imask]) * (n_cells + 1) / 12)). 355 scores[np.isnan(scores)] = 0. 356 pvals = 2 * stats.distributions.norm.sf(np.abs(scores[imask,:])). ValueError: math domain error. ```. The logistic regression and t-test work fine. I guess it is related to my data....",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/566
https://github.com/scverse/scanpy/issues/566:60,usability,error,error,60,"Math domain error when using the Wilcoxon rank-sum; Another error I get and have no idea how to solve is when using the Wilcoxon rank-sum for testing for differential gene expression:. `sc.tl.rank_genes_groups(adata, 'louvain', method='wilcoxon'). sc.pl.rank_genes_groups(adata, n_genes=25, sharey=False)`. ```. ranking genes. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-385-c2fa7bb8ea8d> in <module>. ----> 1 sc.tl.rank_genes_groups(adata, 'louvain', method='wilcoxon'). 2 sc.pl.rank_genes_groups(adata, n_genes=25, sharey=False). ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\tools\_rank_genes_groups.py in rank_genes_groups(adata, groupby, use_raw, groups, reference, n_genes, rankby_abs, key_added, copy, method, corr_method, **kwds). 352 . 353 scores[imask, :] = (scores[imask, :] - (ns[imask] * (n_cells + 1) / 2)) / sqrt(. --> 354 (ns[imask] * (n_cells - ns[imask]) * (n_cells + 1) / 12)). 355 scores[np.isnan(scores)] = 0. 356 pvals = 2 * stats.distributions.norm.sf(np.abs(scores[imask,:])). ValueError: math domain error. ```. The logistic regression and t-test work fine. I guess it is related to my data....",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/566
https://github.com/scverse/scanpy/issues/566:459,usability,input,input-,459,"Math domain error when using the Wilcoxon rank-sum; Another error I get and have no idea how to solve is when using the Wilcoxon rank-sum for testing for differential gene expression:. `sc.tl.rank_genes_groups(adata, 'louvain', method='wilcoxon'). sc.pl.rank_genes_groups(adata, n_genes=25, sharey=False)`. ```. ranking genes. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-385-c2fa7bb8ea8d> in <module>. ----> 1 sc.tl.rank_genes_groups(adata, 'louvain', method='wilcoxon'). 2 sc.pl.rank_genes_groups(adata, n_genes=25, sharey=False). ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\tools\_rank_genes_groups.py in rank_genes_groups(adata, groupby, use_raw, groups, reference, n_genes, rankby_abs, key_added, copy, method, corr_method, **kwds). 352 . 353 scores[imask, :] = (scores[imask, :] - (ns[imask] * (n_cells + 1) / 2)) / sqrt(. --> 354 (ns[imask] * (n_cells - ns[imask]) * (n_cells + 1) / 12)). 355 scores[np.isnan(scores)] = 0. 356 pvals = 2 * stats.distributions.norm.sf(np.abs(scores[imask,:])). ValueError: math domain error. ```. The logistic regression and t-test work fine. I guess it is related to my data....",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/566
https://github.com/scverse/scanpy/issues/566:691,usability,tool,tools,691,"Math domain error when using the Wilcoxon rank-sum; Another error I get and have no idea how to solve is when using the Wilcoxon rank-sum for testing for differential gene expression:. `sc.tl.rank_genes_groups(adata, 'louvain', method='wilcoxon'). sc.pl.rank_genes_groups(adata, n_genes=25, sharey=False)`. ```. ranking genes. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-385-c2fa7bb8ea8d> in <module>. ----> 1 sc.tl.rank_genes_groups(adata, 'louvain', method='wilcoxon'). 2 sc.pl.rank_genes_groups(adata, n_genes=25, sharey=False). ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\tools\_rank_genes_groups.py in rank_genes_groups(adata, groupby, use_raw, groups, reference, n_genes, rankby_abs, key_added, copy, method, corr_method, **kwds). 352 . 353 scores[imask, :] = (scores[imask, :] - (ns[imask] * (n_cells + 1) / 2)) / sqrt(. --> 354 (ns[imask] * (n_cells - ns[imask]) * (n_cells + 1) / 12)). 355 scores[np.isnan(scores)] = 0. 356 pvals = 2 * stats.distributions.norm.sf(np.abs(scores[imask,:])). ValueError: math domain error. ```. The logistic regression and t-test work fine. I guess it is related to my data....",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/566
https://github.com/scverse/scanpy/issues/566:1138,usability,error,error,1138,"Math domain error when using the Wilcoxon rank-sum; Another error I get and have no idea how to solve is when using the Wilcoxon rank-sum for testing for differential gene expression:. `sc.tl.rank_genes_groups(adata, 'louvain', method='wilcoxon'). sc.pl.rank_genes_groups(adata, n_genes=25, sharey=False)`. ```. ranking genes. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-385-c2fa7bb8ea8d> in <module>. ----> 1 sc.tl.rank_genes_groups(adata, 'louvain', method='wilcoxon'). 2 sc.pl.rank_genes_groups(adata, n_genes=25, sharey=False). ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\tools\_rank_genes_groups.py in rank_genes_groups(adata, groupby, use_raw, groups, reference, n_genes, rankby_abs, key_added, copy, method, corr_method, **kwds). 352 . 353 scores[imask, :] = (scores[imask, :] - (ns[imask] * (n_cells + 1) / 2)) / sqrt(. --> 354 (ns[imask] * (n_cells - ns[imask]) * (n_cells + 1) / 12)). 355 scores[np.isnan(scores)] = 0. 356 pvals = 2 * stats.distributions.norm.sf(np.abs(scores[imask,:])). ValueError: math domain error. ```. The logistic regression and t-test work fine. I guess it is related to my data....",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/566
https://github.com/scverse/scanpy/issues/567:17,availability,error,error,17,"macOS matplotlib error; 2019-03-28 12:33:23.060 python[7387:245759] -[NSApplication _setup:]: unrecognized selector sent to instance 0x7fb4fdc7cfb0. 2019-03-28 12:33:23.065 python[7387:245759] *** Terminating app due to uncaught exception 'NSInvalidArgumentException', reason: '-[NSApplication _setup:]: unrecognized selector sent to instance 0x7fb4fdc7cfb0'. *** First throw call stack:. (. 	0 CoreFoundation 0x00007fff44fd3ded __exceptionPreprocess + 256. 	1 libobjc.A.dylib 0x00007fff7109b720 objc_exception_throw + 48. 	2 CoreFoundation 0x00007fff45051195 -[NSObject(NSObject) __retain_OA] + 0. 	3 CoreFoundation 0x00007fff44f75a60 ___forwarding___ + 1486. 	4 CoreFoundation 0x00007fff44f75408 _CF_forwarding_prep_0 + 120. 	5 libtk8.6.dylib 0x0000001a278cc31d TkpInit + 413. 	6 libtk8.6.dylib 0x0000001a2782417e Initialize + 2622. 	7 _tkinter.cpython-37m-darwin.so 0x0000001a2764ca0f _tkinter_create + 1183. 	8 python 0x000000010a42d8b6 _PyMethodDef_RawFastCallKeywords + 230. 	9 python 0x000000010a56db47 call_function + 247. 	10 python 0x000000010a56b684 _PyEval_EvalFrameDefault + 45284. 	11 python 0x000000010a55f2d2 _PyEval_EvalCodeWithName + 418. 	12 python 0x000000010a42c577 _PyFunction_FastCallDict + 231. 	13 python 0x000000010a4b00b1 slot_tp_init + 193. 	14 python 0x000000010a4ba091 type_call + 241. 	15 python 0x000000010a42d283 _PyObject_FastCallKeywords + 179. 	16 python 0x000000010a56dbe4 call_function + 404. 	17 python 0x000000010a56b778 _PyEval_EvalFrameDefault + 45528. 	18 python 0x000000010a42d078 function_code_fastcall + 120. 	19 python 0x000000010a56dafe call_function + 174. 	20 python 0x000000010a56b684 _PyEval_EvalFrameDefault + 45284. 	21 python 0x000000010a55f2d2 _PyEval_EvalCodeWithName + 418. 	22 python 0x000000010a42c577 _PyFunction_FastCallDict + 231. 	23 python 0x000000010a4304a2 method_call + 130. 	24 python 0x000000010a42def2 PyObject_Call + 130. 	25 python 0x000000010a56b8cf _PyEval_EvalFrameDefault + 45871. 	26 python 0x000000010a55f2d2 _PyEval_Eval",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/567
https://github.com/scverse/scanpy/issues/567:381,deployability,stack,stack,381,"macOS matplotlib error; 2019-03-28 12:33:23.060 python[7387:245759] -[NSApplication _setup:]: unrecognized selector sent to instance 0x7fb4fdc7cfb0. 2019-03-28 12:33:23.065 python[7387:245759] *** Terminating app due to uncaught exception 'NSInvalidArgumentException', reason: '-[NSApplication _setup:]: unrecognized selector sent to instance 0x7fb4fdc7cfb0'. *** First throw call stack:. (. 	0 CoreFoundation 0x00007fff44fd3ded __exceptionPreprocess + 256. 	1 libobjc.A.dylib 0x00007fff7109b720 objc_exception_throw + 48. 	2 CoreFoundation 0x00007fff45051195 -[NSObject(NSObject) __retain_OA] + 0. 	3 CoreFoundation 0x00007fff44f75a60 ___forwarding___ + 1486. 	4 CoreFoundation 0x00007fff44f75408 _CF_forwarding_prep_0 + 120. 	5 libtk8.6.dylib 0x0000001a278cc31d TkpInit + 413. 	6 libtk8.6.dylib 0x0000001a2782417e Initialize + 2622. 	7 _tkinter.cpython-37m-darwin.so 0x0000001a2764ca0f _tkinter_create + 1183. 	8 python 0x000000010a42d8b6 _PyMethodDef_RawFastCallKeywords + 230. 	9 python 0x000000010a56db47 call_function + 247. 	10 python 0x000000010a56b684 _PyEval_EvalFrameDefault + 45284. 	11 python 0x000000010a55f2d2 _PyEval_EvalCodeWithName + 418. 	12 python 0x000000010a42c577 _PyFunction_FastCallDict + 231. 	13 python 0x000000010a4b00b1 slot_tp_init + 193. 	14 python 0x000000010a4ba091 type_call + 241. 	15 python 0x000000010a42d283 _PyObject_FastCallKeywords + 179. 	16 python 0x000000010a56dbe4 call_function + 404. 	17 python 0x000000010a56b778 _PyEval_EvalFrameDefault + 45528. 	18 python 0x000000010a42d078 function_code_fastcall + 120. 	19 python 0x000000010a56dafe call_function + 174. 	20 python 0x000000010a56b684 _PyEval_EvalFrameDefault + 45284. 	21 python 0x000000010a55f2d2 _PyEval_EvalCodeWithName + 418. 	22 python 0x000000010a42c577 _PyFunction_FastCallDict + 231. 	23 python 0x000000010a4304a2 method_call + 130. 	24 python 0x000000010a42def2 PyObject_Call + 130. 	25 python 0x000000010a56b8cf _PyEval_EvalFrameDefault + 45871. 	26 python 0x000000010a55f2d2 _PyEval_Eval",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/567
https://github.com/scverse/scanpy/issues/567:395,energy efficiency,Core,CoreFoundation,395,"macOS matplotlib error; 2019-03-28 12:33:23.060 python[7387:245759] -[NSApplication _setup:]: unrecognized selector sent to instance 0x7fb4fdc7cfb0. 2019-03-28 12:33:23.065 python[7387:245759] *** Terminating app due to uncaught exception 'NSInvalidArgumentException', reason: '-[NSApplication _setup:]: unrecognized selector sent to instance 0x7fb4fdc7cfb0'. *** First throw call stack:. (. 	0 CoreFoundation 0x00007fff44fd3ded __exceptionPreprocess + 256. 	1 libobjc.A.dylib 0x00007fff7109b720 objc_exception_throw + 48. 	2 CoreFoundation 0x00007fff45051195 -[NSObject(NSObject) __retain_OA] + 0. 	3 CoreFoundation 0x00007fff44f75a60 ___forwarding___ + 1486. 	4 CoreFoundation 0x00007fff44f75408 _CF_forwarding_prep_0 + 120. 	5 libtk8.6.dylib 0x0000001a278cc31d TkpInit + 413. 	6 libtk8.6.dylib 0x0000001a2782417e Initialize + 2622. 	7 _tkinter.cpython-37m-darwin.so 0x0000001a2764ca0f _tkinter_create + 1183. 	8 python 0x000000010a42d8b6 _PyMethodDef_RawFastCallKeywords + 230. 	9 python 0x000000010a56db47 call_function + 247. 	10 python 0x000000010a56b684 _PyEval_EvalFrameDefault + 45284. 	11 python 0x000000010a55f2d2 _PyEval_EvalCodeWithName + 418. 	12 python 0x000000010a42c577 _PyFunction_FastCallDict + 231. 	13 python 0x000000010a4b00b1 slot_tp_init + 193. 	14 python 0x000000010a4ba091 type_call + 241. 	15 python 0x000000010a42d283 _PyObject_FastCallKeywords + 179. 	16 python 0x000000010a56dbe4 call_function + 404. 	17 python 0x000000010a56b778 _PyEval_EvalFrameDefault + 45528. 	18 python 0x000000010a42d078 function_code_fastcall + 120. 	19 python 0x000000010a56dafe call_function + 174. 	20 python 0x000000010a56b684 _PyEval_EvalFrameDefault + 45284. 	21 python 0x000000010a55f2d2 _PyEval_EvalCodeWithName + 418. 	22 python 0x000000010a42c577 _PyFunction_FastCallDict + 231. 	23 python 0x000000010a4304a2 method_call + 130. 	24 python 0x000000010a42def2 PyObject_Call + 130. 	25 python 0x000000010a56b8cf _PyEval_EvalFrameDefault + 45871. 	26 python 0x000000010a55f2d2 _PyEval_Eval",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/567
https://github.com/scverse/scanpy/issues/567:526,energy efficiency,Core,CoreFoundation,526,"macOS matplotlib error; 2019-03-28 12:33:23.060 python[7387:245759] -[NSApplication _setup:]: unrecognized selector sent to instance 0x7fb4fdc7cfb0. 2019-03-28 12:33:23.065 python[7387:245759] *** Terminating app due to uncaught exception 'NSInvalidArgumentException', reason: '-[NSApplication _setup:]: unrecognized selector sent to instance 0x7fb4fdc7cfb0'. *** First throw call stack:. (. 	0 CoreFoundation 0x00007fff44fd3ded __exceptionPreprocess + 256. 	1 libobjc.A.dylib 0x00007fff7109b720 objc_exception_throw + 48. 	2 CoreFoundation 0x00007fff45051195 -[NSObject(NSObject) __retain_OA] + 0. 	3 CoreFoundation 0x00007fff44f75a60 ___forwarding___ + 1486. 	4 CoreFoundation 0x00007fff44f75408 _CF_forwarding_prep_0 + 120. 	5 libtk8.6.dylib 0x0000001a278cc31d TkpInit + 413. 	6 libtk8.6.dylib 0x0000001a2782417e Initialize + 2622. 	7 _tkinter.cpython-37m-darwin.so 0x0000001a2764ca0f _tkinter_create + 1183. 	8 python 0x000000010a42d8b6 _PyMethodDef_RawFastCallKeywords + 230. 	9 python 0x000000010a56db47 call_function + 247. 	10 python 0x000000010a56b684 _PyEval_EvalFrameDefault + 45284. 	11 python 0x000000010a55f2d2 _PyEval_EvalCodeWithName + 418. 	12 python 0x000000010a42c577 _PyFunction_FastCallDict + 231. 	13 python 0x000000010a4b00b1 slot_tp_init + 193. 	14 python 0x000000010a4ba091 type_call + 241. 	15 python 0x000000010a42d283 _PyObject_FastCallKeywords + 179. 	16 python 0x000000010a56dbe4 call_function + 404. 	17 python 0x000000010a56b778 _PyEval_EvalFrameDefault + 45528. 	18 python 0x000000010a42d078 function_code_fastcall + 120. 	19 python 0x000000010a56dafe call_function + 174. 	20 python 0x000000010a56b684 _PyEval_EvalFrameDefault + 45284. 	21 python 0x000000010a55f2d2 _PyEval_EvalCodeWithName + 418. 	22 python 0x000000010a42c577 _PyFunction_FastCallDict + 231. 	23 python 0x000000010a4304a2 method_call + 130. 	24 python 0x000000010a42def2 PyObject_Call + 130. 	25 python 0x000000010a56b8cf _PyEval_EvalFrameDefault + 45871. 	26 python 0x000000010a55f2d2 _PyEval_Eval",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/567
https://github.com/scverse/scanpy/issues/567:602,energy efficiency,Core,CoreFoundation,602,"macOS matplotlib error; 2019-03-28 12:33:23.060 python[7387:245759] -[NSApplication _setup:]: unrecognized selector sent to instance 0x7fb4fdc7cfb0. 2019-03-28 12:33:23.065 python[7387:245759] *** Terminating app due to uncaught exception 'NSInvalidArgumentException', reason: '-[NSApplication _setup:]: unrecognized selector sent to instance 0x7fb4fdc7cfb0'. *** First throw call stack:. (. 	0 CoreFoundation 0x00007fff44fd3ded __exceptionPreprocess + 256. 	1 libobjc.A.dylib 0x00007fff7109b720 objc_exception_throw + 48. 	2 CoreFoundation 0x00007fff45051195 -[NSObject(NSObject) __retain_OA] + 0. 	3 CoreFoundation 0x00007fff44f75a60 ___forwarding___ + 1486. 	4 CoreFoundation 0x00007fff44f75408 _CF_forwarding_prep_0 + 120. 	5 libtk8.6.dylib 0x0000001a278cc31d TkpInit + 413. 	6 libtk8.6.dylib 0x0000001a2782417e Initialize + 2622. 	7 _tkinter.cpython-37m-darwin.so 0x0000001a2764ca0f _tkinter_create + 1183. 	8 python 0x000000010a42d8b6 _PyMethodDef_RawFastCallKeywords + 230. 	9 python 0x000000010a56db47 call_function + 247. 	10 python 0x000000010a56b684 _PyEval_EvalFrameDefault + 45284. 	11 python 0x000000010a55f2d2 _PyEval_EvalCodeWithName + 418. 	12 python 0x000000010a42c577 _PyFunction_FastCallDict + 231. 	13 python 0x000000010a4b00b1 slot_tp_init + 193. 	14 python 0x000000010a4ba091 type_call + 241. 	15 python 0x000000010a42d283 _PyObject_FastCallKeywords + 179. 	16 python 0x000000010a56dbe4 call_function + 404. 	17 python 0x000000010a56b778 _PyEval_EvalFrameDefault + 45528. 	18 python 0x000000010a42d078 function_code_fastcall + 120. 	19 python 0x000000010a56dafe call_function + 174. 	20 python 0x000000010a56b684 _PyEval_EvalFrameDefault + 45284. 	21 python 0x000000010a55f2d2 _PyEval_EvalCodeWithName + 418. 	22 python 0x000000010a42c577 _PyFunction_FastCallDict + 231. 	23 python 0x000000010a4304a2 method_call + 130. 	24 python 0x000000010a42def2 PyObject_Call + 130. 	25 python 0x000000010a56b8cf _PyEval_EvalFrameDefault + 45871. 	26 python 0x000000010a55f2d2 _PyEval_Eval",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/567
https://github.com/scverse/scanpy/issues/567:664,energy efficiency,Core,CoreFoundation,664,"macOS matplotlib error; 2019-03-28 12:33:23.060 python[7387:245759] -[NSApplication _setup:]: unrecognized selector sent to instance 0x7fb4fdc7cfb0. 2019-03-28 12:33:23.065 python[7387:245759] *** Terminating app due to uncaught exception 'NSInvalidArgumentException', reason: '-[NSApplication _setup:]: unrecognized selector sent to instance 0x7fb4fdc7cfb0'. *** First throw call stack:. (. 	0 CoreFoundation 0x00007fff44fd3ded __exceptionPreprocess + 256. 	1 libobjc.A.dylib 0x00007fff7109b720 objc_exception_throw + 48. 	2 CoreFoundation 0x00007fff45051195 -[NSObject(NSObject) __retain_OA] + 0. 	3 CoreFoundation 0x00007fff44f75a60 ___forwarding___ + 1486. 	4 CoreFoundation 0x00007fff44f75408 _CF_forwarding_prep_0 + 120. 	5 libtk8.6.dylib 0x0000001a278cc31d TkpInit + 413. 	6 libtk8.6.dylib 0x0000001a2782417e Initialize + 2622. 	7 _tkinter.cpython-37m-darwin.so 0x0000001a2764ca0f _tkinter_create + 1183. 	8 python 0x000000010a42d8b6 _PyMethodDef_RawFastCallKeywords + 230. 	9 python 0x000000010a56db47 call_function + 247. 	10 python 0x000000010a56b684 _PyEval_EvalFrameDefault + 45284. 	11 python 0x000000010a55f2d2 _PyEval_EvalCodeWithName + 418. 	12 python 0x000000010a42c577 _PyFunction_FastCallDict + 231. 	13 python 0x000000010a4b00b1 slot_tp_init + 193. 	14 python 0x000000010a4ba091 type_call + 241. 	15 python 0x000000010a42d283 _PyObject_FastCallKeywords + 179. 	16 python 0x000000010a56dbe4 call_function + 404. 	17 python 0x000000010a56b778 _PyEval_EvalFrameDefault + 45528. 	18 python 0x000000010a42d078 function_code_fastcall + 120. 	19 python 0x000000010a56dafe call_function + 174. 	20 python 0x000000010a56b684 _PyEval_EvalFrameDefault + 45284. 	21 python 0x000000010a55f2d2 _PyEval_EvalCodeWithName + 418. 	22 python 0x000000010a42c577 _PyFunction_FastCallDict + 231. 	23 python 0x000000010a4304a2 method_call + 130. 	24 python 0x000000010a42def2 PyObject_Call + 130. 	25 python 0x000000010a56b8cf _PyEval_EvalFrameDefault + 45871. 	26 python 0x000000010a55f2d2 _PyEval_Eval",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/567
https://github.com/scverse/scanpy/issues/567:17,performance,error,error,17,"macOS matplotlib error; 2019-03-28 12:33:23.060 python[7387:245759] -[NSApplication _setup:]: unrecognized selector sent to instance 0x7fb4fdc7cfb0. 2019-03-28 12:33:23.065 python[7387:245759] *** Terminating app due to uncaught exception 'NSInvalidArgumentException', reason: '-[NSApplication _setup:]: unrecognized selector sent to instance 0x7fb4fdc7cfb0'. *** First throw call stack:. (. 	0 CoreFoundation 0x00007fff44fd3ded __exceptionPreprocess + 256. 	1 libobjc.A.dylib 0x00007fff7109b720 objc_exception_throw + 48. 	2 CoreFoundation 0x00007fff45051195 -[NSObject(NSObject) __retain_OA] + 0. 	3 CoreFoundation 0x00007fff44f75a60 ___forwarding___ + 1486. 	4 CoreFoundation 0x00007fff44f75408 _CF_forwarding_prep_0 + 120. 	5 libtk8.6.dylib 0x0000001a278cc31d TkpInit + 413. 	6 libtk8.6.dylib 0x0000001a2782417e Initialize + 2622. 	7 _tkinter.cpython-37m-darwin.so 0x0000001a2764ca0f _tkinter_create + 1183. 	8 python 0x000000010a42d8b6 _PyMethodDef_RawFastCallKeywords + 230. 	9 python 0x000000010a56db47 call_function + 247. 	10 python 0x000000010a56b684 _PyEval_EvalFrameDefault + 45284. 	11 python 0x000000010a55f2d2 _PyEval_EvalCodeWithName + 418. 	12 python 0x000000010a42c577 _PyFunction_FastCallDict + 231. 	13 python 0x000000010a4b00b1 slot_tp_init + 193. 	14 python 0x000000010a4ba091 type_call + 241. 	15 python 0x000000010a42d283 _PyObject_FastCallKeywords + 179. 	16 python 0x000000010a56dbe4 call_function + 404. 	17 python 0x000000010a56b778 _PyEval_EvalFrameDefault + 45528. 	18 python 0x000000010a42d078 function_code_fastcall + 120. 	19 python 0x000000010a56dafe call_function + 174. 	20 python 0x000000010a56b684 _PyEval_EvalFrameDefault + 45284. 	21 python 0x000000010a55f2d2 _PyEval_EvalCodeWithName + 418. 	22 python 0x000000010a42c577 _PyFunction_FastCallDict + 231. 	23 python 0x000000010a4304a2 method_call + 130. 	24 python 0x000000010a42def2 PyObject_Call + 130. 	25 python 0x000000010a56b8cf _PyEval_EvalFrameDefault + 45871. 	26 python 0x000000010a55f2d2 _PyEval_Eval",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/567
https://github.com/scverse/scanpy/issues/567:17,safety,error,error,17,"macOS matplotlib error; 2019-03-28 12:33:23.060 python[7387:245759] -[NSApplication _setup:]: unrecognized selector sent to instance 0x7fb4fdc7cfb0. 2019-03-28 12:33:23.065 python[7387:245759] *** Terminating app due to uncaught exception 'NSInvalidArgumentException', reason: '-[NSApplication _setup:]: unrecognized selector sent to instance 0x7fb4fdc7cfb0'. *** First throw call stack:. (. 	0 CoreFoundation 0x00007fff44fd3ded __exceptionPreprocess + 256. 	1 libobjc.A.dylib 0x00007fff7109b720 objc_exception_throw + 48. 	2 CoreFoundation 0x00007fff45051195 -[NSObject(NSObject) __retain_OA] + 0. 	3 CoreFoundation 0x00007fff44f75a60 ___forwarding___ + 1486. 	4 CoreFoundation 0x00007fff44f75408 _CF_forwarding_prep_0 + 120. 	5 libtk8.6.dylib 0x0000001a278cc31d TkpInit + 413. 	6 libtk8.6.dylib 0x0000001a2782417e Initialize + 2622. 	7 _tkinter.cpython-37m-darwin.so 0x0000001a2764ca0f _tkinter_create + 1183. 	8 python 0x000000010a42d8b6 _PyMethodDef_RawFastCallKeywords + 230. 	9 python 0x000000010a56db47 call_function + 247. 	10 python 0x000000010a56b684 _PyEval_EvalFrameDefault + 45284. 	11 python 0x000000010a55f2d2 _PyEval_EvalCodeWithName + 418. 	12 python 0x000000010a42c577 _PyFunction_FastCallDict + 231. 	13 python 0x000000010a4b00b1 slot_tp_init + 193. 	14 python 0x000000010a4ba091 type_call + 241. 	15 python 0x000000010a42d283 _PyObject_FastCallKeywords + 179. 	16 python 0x000000010a56dbe4 call_function + 404. 	17 python 0x000000010a56b778 _PyEval_EvalFrameDefault + 45528. 	18 python 0x000000010a42d078 function_code_fastcall + 120. 	19 python 0x000000010a56dafe call_function + 174. 	20 python 0x000000010a56b684 _PyEval_EvalFrameDefault + 45284. 	21 python 0x000000010a55f2d2 _PyEval_EvalCodeWithName + 418. 	22 python 0x000000010a42c577 _PyFunction_FastCallDict + 231. 	23 python 0x000000010a4304a2 method_call + 130. 	24 python 0x000000010a42def2 PyObject_Call + 130. 	25 python 0x000000010a56b8cf _PyEval_EvalFrameDefault + 45871. 	26 python 0x000000010a55f2d2 _PyEval_Eval",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/567
https://github.com/scverse/scanpy/issues/567:229,safety,except,exception,229,"macOS matplotlib error; 2019-03-28 12:33:23.060 python[7387:245759] -[NSApplication _setup:]: unrecognized selector sent to instance 0x7fb4fdc7cfb0. 2019-03-28 12:33:23.065 python[7387:245759] *** Terminating app due to uncaught exception 'NSInvalidArgumentException', reason: '-[NSApplication _setup:]: unrecognized selector sent to instance 0x7fb4fdc7cfb0'. *** First throw call stack:. (. 	0 CoreFoundation 0x00007fff44fd3ded __exceptionPreprocess + 256. 	1 libobjc.A.dylib 0x00007fff7109b720 objc_exception_throw + 48. 	2 CoreFoundation 0x00007fff45051195 -[NSObject(NSObject) __retain_OA] + 0. 	3 CoreFoundation 0x00007fff44f75a60 ___forwarding___ + 1486. 	4 CoreFoundation 0x00007fff44f75408 _CF_forwarding_prep_0 + 120. 	5 libtk8.6.dylib 0x0000001a278cc31d TkpInit + 413. 	6 libtk8.6.dylib 0x0000001a2782417e Initialize + 2622. 	7 _tkinter.cpython-37m-darwin.so 0x0000001a2764ca0f _tkinter_create + 1183. 	8 python 0x000000010a42d8b6 _PyMethodDef_RawFastCallKeywords + 230. 	9 python 0x000000010a56db47 call_function + 247. 	10 python 0x000000010a56b684 _PyEval_EvalFrameDefault + 45284. 	11 python 0x000000010a55f2d2 _PyEval_EvalCodeWithName + 418. 	12 python 0x000000010a42c577 _PyFunction_FastCallDict + 231. 	13 python 0x000000010a4b00b1 slot_tp_init + 193. 	14 python 0x000000010a4ba091 type_call + 241. 	15 python 0x000000010a42d283 _PyObject_FastCallKeywords + 179. 	16 python 0x000000010a56dbe4 call_function + 404. 	17 python 0x000000010a56b778 _PyEval_EvalFrameDefault + 45528. 	18 python 0x000000010a42d078 function_code_fastcall + 120. 	19 python 0x000000010a56dafe call_function + 174. 	20 python 0x000000010a56b684 _PyEval_EvalFrameDefault + 45284. 	21 python 0x000000010a55f2d2 _PyEval_EvalCodeWithName + 418. 	22 python 0x000000010a42c577 _PyFunction_FastCallDict + 231. 	23 python 0x000000010a4304a2 method_call + 130. 	24 python 0x000000010a42def2 PyObject_Call + 130. 	25 python 0x000000010a56b8cf _PyEval_EvalFrameDefault + 45871. 	26 python 0x000000010a55f2d2 _PyEval_Eval",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/567
https://github.com/scverse/scanpy/issues/567:3377,safety,except,exception,3377,on + 404. 	17 python 0x000000010a56b778 _PyEval_EvalFrameDefault + 45528. 	18 python 0x000000010a42d078 function_code_fastcall + 120. 	19 python 0x000000010a56dafe call_function + 174. 	20 python 0x000000010a56b684 _PyEval_EvalFrameDefault + 45284. 	21 python 0x000000010a55f2d2 _PyEval_EvalCodeWithName + 418. 	22 python 0x000000010a42c577 _PyFunction_FastCallDict + 231. 	23 python 0x000000010a4304a2 method_call + 130. 	24 python 0x000000010a42def2 PyObject_Call + 130. 	25 python 0x000000010a56b8cf _PyEval_EvalFrameDefault + 45871. 	26 python 0x000000010a55f2d2 _PyEval_EvalCodeWithName + 418. 	27 python 0x000000010a42c577 _PyFunction_FastCallDict + 231. 	28 python 0x000000010a56b8cf _PyEval_EvalFrameDefault + 45871. 	29 python 0x000000010a55f2d2 _PyEval_EvalCodeWithName + 418. 	30 python 0x000000010a42c577 _PyFunction_FastCallDict + 231. 	31 python 0x000000010a56b8cf _PyEval_EvalFrameDefault + 45871. 	32 python 0x000000010a55f2d2 _PyEval_EvalCodeWithName + 418. 	33 python 0x000000010a42c577 _PyFunction_FastCallDict + 231. 	34 python 0x000000010a4b00b1 slot_tp_init + 193. 	35 python 0x000000010a4ba091 type_call + 241. 	36 python 0x000000010a42d283 _PyObject_FastCallKeywords + 179. 	37 python 0x000000010a56dbe4 call_function + 404. 	38 python 0x000000010a56b778 _PyEval_EvalFrameDefault + 45528. 	39 python 0x000000010a55f2d2 _PyEval_EvalCodeWithName + 418. 	40 python 0x000000010a42d783 _PyFunction_FastCallKeywords + 195. 	41 python 0x000000010a56dafe call_function + 174. 	42 python 0x000000010a56b778 _PyEval_EvalFrameDefault + 45528. 	43 python 0x000000010a55f2d2 _PyEval_EvalCodeWithName + 418. 	44 python 0x000000010a5c3c40 PyRun_FileExFlags + 256. 	45 python 0x000000010a5c30b7 PyRun_SimpleFileExFlags + 391. 	46 python 0x000000010a5ef7ec pymain_main + 9564. 	47 python 0x000000010a3ffa9d main + 125. 	48 libdyld.dylib 0x00007fff72169ed9 start + 1. 	49 ??? 0x0000000000000002 0x0 + 2. ). libc++abi.dylib: terminating with uncaught exception of type NSException. Abort trap: 6,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/567
https://github.com/scverse/scanpy/issues/567:17,usability,error,error,17,"macOS matplotlib error; 2019-03-28 12:33:23.060 python[7387:245759] -[NSApplication _setup:]: unrecognized selector sent to instance 0x7fb4fdc7cfb0. 2019-03-28 12:33:23.065 python[7387:245759] *** Terminating app due to uncaught exception 'NSInvalidArgumentException', reason: '-[NSApplication _setup:]: unrecognized selector sent to instance 0x7fb4fdc7cfb0'. *** First throw call stack:. (. 	0 CoreFoundation 0x00007fff44fd3ded __exceptionPreprocess + 256. 	1 libobjc.A.dylib 0x00007fff7109b720 objc_exception_throw + 48. 	2 CoreFoundation 0x00007fff45051195 -[NSObject(NSObject) __retain_OA] + 0. 	3 CoreFoundation 0x00007fff44f75a60 ___forwarding___ + 1486. 	4 CoreFoundation 0x00007fff44f75408 _CF_forwarding_prep_0 + 120. 	5 libtk8.6.dylib 0x0000001a278cc31d TkpInit + 413. 	6 libtk8.6.dylib 0x0000001a2782417e Initialize + 2622. 	7 _tkinter.cpython-37m-darwin.so 0x0000001a2764ca0f _tkinter_create + 1183. 	8 python 0x000000010a42d8b6 _PyMethodDef_RawFastCallKeywords + 230. 	9 python 0x000000010a56db47 call_function + 247. 	10 python 0x000000010a56b684 _PyEval_EvalFrameDefault + 45284. 	11 python 0x000000010a55f2d2 _PyEval_EvalCodeWithName + 418. 	12 python 0x000000010a42c577 _PyFunction_FastCallDict + 231. 	13 python 0x000000010a4b00b1 slot_tp_init + 193. 	14 python 0x000000010a4ba091 type_call + 241. 	15 python 0x000000010a42d283 _PyObject_FastCallKeywords + 179. 	16 python 0x000000010a56dbe4 call_function + 404. 	17 python 0x000000010a56b778 _PyEval_EvalFrameDefault + 45528. 	18 python 0x000000010a42d078 function_code_fastcall + 120. 	19 python 0x000000010a56dafe call_function + 174. 	20 python 0x000000010a56b684 _PyEval_EvalFrameDefault + 45284. 	21 python 0x000000010a55f2d2 _PyEval_EvalCodeWithName + 418. 	22 python 0x000000010a42c577 _PyFunction_FastCallDict + 231. 	23 python 0x000000010a4304a2 method_call + 130. 	24 python 0x000000010a42def2 PyObject_Call + 130. 	25 python 0x000000010a56b8cf _PyEval_EvalFrameDefault + 45871. 	26 python 0x000000010a55f2d2 _PyEval_Eval",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/567
https://github.com/scverse/scanpy/pull/568:87,deployability,updat,updating,87,Add issues topic to contributing guidelines; Saw a couple duplicate issues and figured updating the contributing guidelines could help with that. Anything I forgot to mention?,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/568
https://github.com/scverse/scanpy/pull/568:11,integrability,topic,topic,11,Add issues topic to contributing guidelines; Saw a couple duplicate issues and figured updating the contributing guidelines could help with that. Anything I forgot to mention?,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/568
https://github.com/scverse/scanpy/pull/568:51,integrability,coupl,couple,51,Add issues topic to contributing guidelines; Saw a couple duplicate issues and figured updating the contributing guidelines could help with that. Anything I forgot to mention?,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/568
