id,quality_attribute,keyword,matched_word,match_idx,sentence,source,author,repo,version,wiki,url
https://github.com/google/deepvariant/issues/770:946,modifiability,Pac,PacBio,946,"Hi @Luosanmu,. For your filtering question. We don't directly calculate many of the statistics in the INFO field for variant calling. We observe that the most effective way to adjust filters is to use the Genotype Quality (GQ) property or the Phred Likelihood (PL) fields (GQ is mathematically derived from PL). Do you want to increase sensitivity? The best way is likely to post-process the VCF to extract the low-confidence REF calls using this field. For the question - why does DeepVariant make a call that differs from GATK. For any single call, it's difficult to say the exact reasons. Sometimes, looking at the reads and the reference in the region can give clues about why a call would be made. If you have an IGV screenshot showing the region it might be informative. DeepVariant does seem very confident that there isn't a variant here. . It would also be helpful to know something about the sequencing and prep. Is this Illumina data? PacBio data? Is this a PCR-free prep, or does it include PCR? things like that. Thank you,. Andrew.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/770
https://github.com/google/deepvariant/issues/770:477,reliability,doe,does,477,"Hi @Luosanmu,. For your filtering question. We don't directly calculate many of the statistics in the INFO field for variant calling. We observe that the most effective way to adjust filters is to use the Genotype Quality (GQ) property or the Phred Likelihood (PL) fields (GQ is mathematically derived from PL). Do you want to increase sensitivity? The best way is likely to post-process the VCF to extract the low-confidence REF calls using this field. For the question - why does DeepVariant make a call that differs from GATK. For any single call, it's difficult to say the exact reasons. Sometimes, looking at the reads and the reference in the region can give clues about why a call would be made. If you have an IGV screenshot showing the region it might be informative. DeepVariant does seem very confident that there isn't a variant here. . It would also be helpful to know something about the sequencing and prep. Is this Illumina data? PacBio data? Is this a PCR-free prep, or does it include PCR? things like that. Thank you,. Andrew.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/770
https://github.com/google/deepvariant/issues/770:789,reliability,doe,does,789,"Hi @Luosanmu,. For your filtering question. We don't directly calculate many of the statistics in the INFO field for variant calling. We observe that the most effective way to adjust filters is to use the Genotype Quality (GQ) property or the Phred Likelihood (PL) fields (GQ is mathematically derived from PL). Do you want to increase sensitivity? The best way is likely to post-process the VCF to extract the low-confidence REF calls using this field. For the question - why does DeepVariant make a call that differs from GATK. For any single call, it's difficult to say the exact reasons. Sometimes, looking at the reads and the reference in the region can give clues about why a call would be made. If you have an IGV screenshot showing the region it might be informative. DeepVariant does seem very confident that there isn't a variant here. . It would also be helpful to know something about the sequencing and prep. Is this Illumina data? PacBio data? Is this a PCR-free prep, or does it include PCR? things like that. Thank you,. Andrew.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/770
https://github.com/google/deepvariant/issues/770:987,reliability,doe,does,987,"Hi @Luosanmu,. For your filtering question. We don't directly calculate many of the statistics in the INFO field for variant calling. We observe that the most effective way to adjust filters is to use the Genotype Quality (GQ) property or the Phred Likelihood (PL) fields (GQ is mathematically derived from PL). Do you want to increase sensitivity? The best way is likely to post-process the VCF to extract the low-confidence REF calls using this field. For the question - why does DeepVariant make a call that differs from GATK. For any single call, it's difficult to say the exact reasons. Sometimes, looking at the reads and the reference in the region can give clues about why a call would be made. If you have an IGV screenshot showing the region it might be informative. DeepVariant does seem very confident that there isn't a variant here. . It would also be helpful to know something about the sequencing and prep. Is this Illumina data? PacBio data? Is this a PCR-free prep, or does it include PCR? things like that. Thank you,. Andrew.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/770
https://github.com/google/deepvariant/issues/770:137,testability,observ,observe,137,"Hi @Luosanmu,. For your filtering question. We don't directly calculate many of the statistics in the INFO field for variant calling. We observe that the most effective way to adjust filters is to use the Genotype Quality (GQ) property or the Phred Likelihood (PL) fields (GQ is mathematically derived from PL). Do you want to increase sensitivity? The best way is likely to post-process the VCF to extract the low-confidence REF calls using this field. For the question - why does DeepVariant make a call that differs from GATK. For any single call, it's difficult to say the exact reasons. Sometimes, looking at the reads and the reference in the region can give clues about why a call would be made. If you have an IGV screenshot showing the region it might be informative. DeepVariant does seem very confident that there isn't a variant here. . It would also be helpful to know something about the sequencing and prep. Is this Illumina data? PacBio data? Is this a PCR-free prep, or does it include PCR? things like that. Thank you,. Andrew.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/770
https://github.com/google/deepvariant/issues/770:159,usability,effectiv,effective,159,"Hi @Luosanmu,. For your filtering question. We don't directly calculate many of the statistics in the INFO field for variant calling. We observe that the most effective way to adjust filters is to use the Genotype Quality (GQ) property or the Phred Likelihood (PL) fields (GQ is mathematically derived from PL). Do you want to increase sensitivity? The best way is likely to post-process the VCF to extract the low-confidence REF calls using this field. For the question - why does DeepVariant make a call that differs from GATK. For any single call, it's difficult to say the exact reasons. Sometimes, looking at the reads and the reference in the region can give clues about why a call would be made. If you have an IGV screenshot showing the region it might be informative. DeepVariant does seem very confident that there isn't a variant here. . It would also be helpful to know something about the sequencing and prep. Is this Illumina data? PacBio data? Is this a PCR-free prep, or does it include PCR? things like that. Thank you,. Andrew.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/770
https://github.com/google/deepvariant/issues/770:866,usability,help,helpful,866,"Hi @Luosanmu,. For your filtering question. We don't directly calculate many of the statistics in the INFO field for variant calling. We observe that the most effective way to adjust filters is to use the Genotype Quality (GQ) property or the Phred Likelihood (PL) fields (GQ is mathematically derived from PL). Do you want to increase sensitivity? The best way is likely to post-process the VCF to extract the low-confidence REF calls using this field. For the question - why does DeepVariant make a call that differs from GATK. For any single call, it's difficult to say the exact reasons. Sometimes, looking at the reads and the reference in the region can give clues about why a call would be made. If you have an IGV screenshot showing the region it might be informative. DeepVariant does seem very confident that there isn't a variant here. . It would also be helpful to know something about the sequencing and prep. Is this Illumina data? PacBio data? Is this a PCR-free prep, or does it include PCR? things like that. Thank you,. Andrew.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/770
https://github.com/google/deepvariant/issues/770:655,availability,error,error,655,"Thank you @Luosanmu . I see. From your image, I think I understand why DeepVariant would make a REF call here. The variant in question is a 1-bp extension of a homopolymer (10A -> 11A). Homopolymers are generally difficult to sequence through. The number of reference-supporting reads are 47 and alternate-supporting reads are 10 (~16%), which is far from the typically-expected 50% if the position is heterozygous. DeepVariant's model has to weigh which probability is more likely: that this is a real HET event and the random sampling of the alleles causes the observations to be skewed as far as 16%, or is there a sufficiently recurring 1bp insertion error during sequencing that explains these insertions at this ratio. Presumably, over the bulk of DeepVariant's training, when it has seen similar situations, in more cases these are insertion errors. Now, whether that is what is truly going on in your sample, it's difficult for me as a human to say.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/770
https://github.com/google/deepvariant/issues/770:849,availability,error,errors,849,"Thank you @Luosanmu . I see. From your image, I think I understand why DeepVariant would make a REF call here. The variant in question is a 1-bp extension of a homopolymer (10A -> 11A). Homopolymers are generally difficult to sequence through. The number of reference-supporting reads are 47 and alternate-supporting reads are 10 (~16%), which is far from the typically-expected 50% if the position is heterozygous. DeepVariant's model has to weigh which probability is more likely: that this is a real HET event and the random sampling of the alleles causes the observations to be skewed as far as 16%, or is there a sufficiently recurring 1bp insertion error during sequencing that explains these insertions at this ratio. Presumably, over the bulk of DeepVariant's training, when it has seen similar situations, in more cases these are insertion errors. Now, whether that is what is truly going on in your sample, it's difficult for me as a human to say.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/770
https://github.com/google/deepvariant/issues/770:563,deployability,observ,observations,563,"Thank you @Luosanmu . I see. From your image, I think I understand why DeepVariant would make a REF call here. The variant in question is a 1-bp extension of a homopolymer (10A -> 11A). Homopolymers are generally difficult to sequence through. The number of reference-supporting reads are 47 and alternate-supporting reads are 10 (~16%), which is far from the typically-expected 50% if the position is heterozygous. DeepVariant's model has to weigh which probability is more likely: that this is a real HET event and the random sampling of the alleles causes the observations to be skewed as far as 16%, or is there a sufficiently recurring 1bp insertion error during sequencing that explains these insertions at this ratio. Presumably, over the bulk of DeepVariant's training, when it has seen similar situations, in more cases these are insertion errors. Now, whether that is what is truly going on in your sample, it's difficult for me as a human to say.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/770
https://github.com/google/deepvariant/issues/770:430,energy efficiency,model,model,430,"Thank you @Luosanmu . I see. From your image, I think I understand why DeepVariant would make a REF call here. The variant in question is a 1-bp extension of a homopolymer (10A -> 11A). Homopolymers are generally difficult to sequence through. The number of reference-supporting reads are 47 and alternate-supporting reads are 10 (~16%), which is far from the typically-expected 50% if the position is heterozygous. DeepVariant's model has to weigh which probability is more likely: that this is a real HET event and the random sampling of the alleles causes the observations to be skewed as far as 16%, or is there a sufficiently recurring 1bp insertion error during sequencing that explains these insertions at this ratio. Presumably, over the bulk of DeepVariant's training, when it has seen similar situations, in more cases these are insertion errors. Now, whether that is what is truly going on in your sample, it's difficult for me as a human to say.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/770
https://github.com/google/deepvariant/issues/770:507,integrability,event,event,507,"Thank you @Luosanmu . I see. From your image, I think I understand why DeepVariant would make a REF call here. The variant in question is a 1-bp extension of a homopolymer (10A -> 11A). Homopolymers are generally difficult to sequence through. The number of reference-supporting reads are 47 and alternate-supporting reads are 10 (~16%), which is far from the typically-expected 50% if the position is heterozygous. DeepVariant's model has to weigh which probability is more likely: that this is a real HET event and the random sampling of the alleles causes the observations to be skewed as far as 16%, or is there a sufficiently recurring 1bp insertion error during sequencing that explains these insertions at this ratio. Presumably, over the bulk of DeepVariant's training, when it has seen similar situations, in more cases these are insertion errors. Now, whether that is what is truly going on in your sample, it's difficult for me as a human to say.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/770
https://github.com/google/deepvariant/issues/770:145,modifiability,extens,extension,145,"Thank you @Luosanmu . I see. From your image, I think I understand why DeepVariant would make a REF call here. The variant in question is a 1-bp extension of a homopolymer (10A -> 11A). Homopolymers are generally difficult to sequence through. The number of reference-supporting reads are 47 and alternate-supporting reads are 10 (~16%), which is far from the typically-expected 50% if the position is heterozygous. DeepVariant's model has to weigh which probability is more likely: that this is a real HET event and the random sampling of the alleles causes the observations to be skewed as far as 16%, or is there a sufficiently recurring 1bp insertion error during sequencing that explains these insertions at this ratio. Presumably, over the bulk of DeepVariant's training, when it has seen similar situations, in more cases these are insertion errors. Now, whether that is what is truly going on in your sample, it's difficult for me as a human to say.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/770
https://github.com/google/deepvariant/issues/770:655,performance,error,error,655,"Thank you @Luosanmu . I see. From your image, I think I understand why DeepVariant would make a REF call here. The variant in question is a 1-bp extension of a homopolymer (10A -> 11A). Homopolymers are generally difficult to sequence through. The number of reference-supporting reads are 47 and alternate-supporting reads are 10 (~16%), which is far from the typically-expected 50% if the position is heterozygous. DeepVariant's model has to weigh which probability is more likely: that this is a real HET event and the random sampling of the alleles causes the observations to be skewed as far as 16%, or is there a sufficiently recurring 1bp insertion error during sequencing that explains these insertions at this ratio. Presumably, over the bulk of DeepVariant's training, when it has seen similar situations, in more cases these are insertion errors. Now, whether that is what is truly going on in your sample, it's difficult for me as a human to say.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/770
https://github.com/google/deepvariant/issues/770:849,performance,error,errors,849,"Thank you @Luosanmu . I see. From your image, I think I understand why DeepVariant would make a REF call here. The variant in question is a 1-bp extension of a homopolymer (10A -> 11A). Homopolymers are generally difficult to sequence through. The number of reference-supporting reads are 47 and alternate-supporting reads are 10 (~16%), which is far from the typically-expected 50% if the position is heterozygous. DeepVariant's model has to weigh which probability is more likely: that this is a real HET event and the random sampling of the alleles causes the observations to be skewed as far as 16%, or is there a sufficiently recurring 1bp insertion error during sequencing that explains these insertions at this ratio. Presumably, over the bulk of DeepVariant's training, when it has seen similar situations, in more cases these are insertion errors. Now, whether that is what is truly going on in your sample, it's difficult for me as a human to say.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/770
https://github.com/google/deepvariant/issues/770:655,safety,error,error,655,"Thank you @Luosanmu . I see. From your image, I think I understand why DeepVariant would make a REF call here. The variant in question is a 1-bp extension of a homopolymer (10A -> 11A). Homopolymers are generally difficult to sequence through. The number of reference-supporting reads are 47 and alternate-supporting reads are 10 (~16%), which is far from the typically-expected 50% if the position is heterozygous. DeepVariant's model has to weigh which probability is more likely: that this is a real HET event and the random sampling of the alleles causes the observations to be skewed as far as 16%, or is there a sufficiently recurring 1bp insertion error during sequencing that explains these insertions at this ratio. Presumably, over the bulk of DeepVariant's training, when it has seen similar situations, in more cases these are insertion errors. Now, whether that is what is truly going on in your sample, it's difficult for me as a human to say.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/770
https://github.com/google/deepvariant/issues/770:849,safety,error,errors,849,"Thank you @Luosanmu . I see. From your image, I think I understand why DeepVariant would make a REF call here. The variant in question is a 1-bp extension of a homopolymer (10A -> 11A). Homopolymers are generally difficult to sequence through. The number of reference-supporting reads are 47 and alternate-supporting reads are 10 (~16%), which is far from the typically-expected 50% if the position is heterozygous. DeepVariant's model has to weigh which probability is more likely: that this is a real HET event and the random sampling of the alleles causes the observations to be skewed as far as 16%, or is there a sufficiently recurring 1bp insertion error during sequencing that explains these insertions at this ratio. Presumably, over the bulk of DeepVariant's training, when it has seen similar situations, in more cases these are insertion errors. Now, whether that is what is truly going on in your sample, it's difficult for me as a human to say.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/770
https://github.com/google/deepvariant/issues/770:430,security,model,model,430,"Thank you @Luosanmu . I see. From your image, I think I understand why DeepVariant would make a REF call here. The variant in question is a 1-bp extension of a homopolymer (10A -> 11A). Homopolymers are generally difficult to sequence through. The number of reference-supporting reads are 47 and alternate-supporting reads are 10 (~16%), which is far from the typically-expected 50% if the position is heterozygous. DeepVariant's model has to weigh which probability is more likely: that this is a real HET event and the random sampling of the alleles causes the observations to be skewed as far as 16%, or is there a sufficiently recurring 1bp insertion error during sequencing that explains these insertions at this ratio. Presumably, over the bulk of DeepVariant's training, when it has seen similar situations, in more cases these are insertion errors. Now, whether that is what is truly going on in your sample, it's difficult for me as a human to say.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/770
https://github.com/google/deepvariant/issues/770:56,testability,understand,understand,56,"Thank you @Luosanmu . I see. From your image, I think I understand why DeepVariant would make a REF call here. The variant in question is a 1-bp extension of a homopolymer (10A -> 11A). Homopolymers are generally difficult to sequence through. The number of reference-supporting reads are 47 and alternate-supporting reads are 10 (~16%), which is far from the typically-expected 50% if the position is heterozygous. DeepVariant's model has to weigh which probability is more likely: that this is a real HET event and the random sampling of the alleles causes the observations to be skewed as far as 16%, or is there a sufficiently recurring 1bp insertion error during sequencing that explains these insertions at this ratio. Presumably, over the bulk of DeepVariant's training, when it has seen similar situations, in more cases these are insertion errors. Now, whether that is what is truly going on in your sample, it's difficult for me as a human to say.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/770
https://github.com/google/deepvariant/issues/770:563,testability,observ,observations,563,"Thank you @Luosanmu . I see. From your image, I think I understand why DeepVariant would make a REF call here. The variant in question is a 1-bp extension of a homopolymer (10A -> 11A). Homopolymers are generally difficult to sequence through. The number of reference-supporting reads are 47 and alternate-supporting reads are 10 (~16%), which is far from the typically-expected 50% if the position is heterozygous. DeepVariant's model has to weigh which probability is more likely: that this is a real HET event and the random sampling of the alleles causes the observations to be skewed as far as 16%, or is there a sufficiently recurring 1bp insertion error during sequencing that explains these insertions at this ratio. Presumably, over the bulk of DeepVariant's training, when it has seen similar situations, in more cases these are insertion errors. Now, whether that is what is truly going on in your sample, it's difficult for me as a human to say.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/770
https://github.com/google/deepvariant/issues/770:268,usability,support,supporting,268,"Thank you @Luosanmu . I see. From your image, I think I understand why DeepVariant would make a REF call here. The variant in question is a 1-bp extension of a homopolymer (10A -> 11A). Homopolymers are generally difficult to sequence through. The number of reference-supporting reads are 47 and alternate-supporting reads are 10 (~16%), which is far from the typically-expected 50% if the position is heterozygous. DeepVariant's model has to weigh which probability is more likely: that this is a real HET event and the random sampling of the alleles causes the observations to be skewed as far as 16%, or is there a sufficiently recurring 1bp insertion error during sequencing that explains these insertions at this ratio. Presumably, over the bulk of DeepVariant's training, when it has seen similar situations, in more cases these are insertion errors. Now, whether that is what is truly going on in your sample, it's difficult for me as a human to say.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/770
https://github.com/google/deepvariant/issues/770:306,usability,support,supporting,306,"Thank you @Luosanmu . I see. From your image, I think I understand why DeepVariant would make a REF call here. The variant in question is a 1-bp extension of a homopolymer (10A -> 11A). Homopolymers are generally difficult to sequence through. The number of reference-supporting reads are 47 and alternate-supporting reads are 10 (~16%), which is far from the typically-expected 50% if the position is heterozygous. DeepVariant's model has to weigh which probability is more likely: that this is a real HET event and the random sampling of the alleles causes the observations to be skewed as far as 16%, or is there a sufficiently recurring 1bp insertion error during sequencing that explains these insertions at this ratio. Presumably, over the bulk of DeepVariant's training, when it has seen similar situations, in more cases these are insertion errors. Now, whether that is what is truly going on in your sample, it's difficult for me as a human to say.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/770
https://github.com/google/deepvariant/issues/770:655,usability,error,error,655,"Thank you @Luosanmu . I see. From your image, I think I understand why DeepVariant would make a REF call here. The variant in question is a 1-bp extension of a homopolymer (10A -> 11A). Homopolymers are generally difficult to sequence through. The number of reference-supporting reads are 47 and alternate-supporting reads are 10 (~16%), which is far from the typically-expected 50% if the position is heterozygous. DeepVariant's model has to weigh which probability is more likely: that this is a real HET event and the random sampling of the alleles causes the observations to be skewed as far as 16%, or is there a sufficiently recurring 1bp insertion error during sequencing that explains these insertions at this ratio. Presumably, over the bulk of DeepVariant's training, when it has seen similar situations, in more cases these are insertion errors. Now, whether that is what is truly going on in your sample, it's difficult for me as a human to say.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/770
https://github.com/google/deepvariant/issues/770:849,usability,error,errors,849,"Thank you @Luosanmu . I see. From your image, I think I understand why DeepVariant would make a REF call here. The variant in question is a 1-bp extension of a homopolymer (10A -> 11A). Homopolymers are generally difficult to sequence through. The number of reference-supporting reads are 47 and alternate-supporting reads are 10 (~16%), which is far from the typically-expected 50% if the position is heterozygous. DeepVariant's model has to weigh which probability is more likely: that this is a real HET event and the random sampling of the alleles causes the observations to be skewed as far as 16%, or is there a sufficiently recurring 1bp insertion error during sequencing that explains these insertions at this ratio. Presumably, over the bulk of DeepVariant's training, when it has seen similar situations, in more cases these are insertion errors. Now, whether that is what is truly going on in your sample, it's difficult for me as a human to say.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/770
https://github.com/google/deepvariant/issues/770:54,usability,close,close,54,"Hi @Luosanmu ,. Due to inactivity on this issue, I'll close it. Please feel free to follow up if you have more questions.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/770
https://github.com/google/deepvariant/issues/771:67,deployability,instal,installation,67,"Hello @helizabeth1103,. dv_constants.py is the part of DeepVariant installation. . You may take a look at https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md#notes-on-singularity for more details on DeepVariant with Singularity. You may also check https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-training-case-study.md that shows how to run make_examples from docker.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/771
https://github.com/google/deepvariant/issues/771:60,usability,close,close,60,"Hi @helizabeth1103 ,. Due to inactivity on this issue, I'll close it. Please feel free to follow up if you have more questions.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/771
https://github.com/google/deepvariant/issues/772:16,energy efficiency,Current,Currently,16,"hi @gneedle1 ,. Currently it is not possible to have the barcodes output in the VCF. Thank you for the request.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/772
https://github.com/google/deepvariant/issues/772:21,integrability,sub,subset,21,Would it be valid to subset the original BAM by barcode and run each subset individually?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/772
https://github.com/google/deepvariant/issues/772:69,integrability,sub,subset,69,Would it be valid to subset the original BAM by barcode and run each subset individually?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/772
https://github.com/google/deepvariant/issues/772:12,safety,valid,valid,12,Would it be valid to subset the original BAM by barcode and run each subset individually?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/772
https://github.com/google/deepvariant/issues/772:373,availability,toler,tolerance,373,"Hi @gneedle1 . It depends on the type of experiment. If the barcodes are the same sample and you are trying to get at some other specific property (e.g. cell type or preparation), then it's a question of sequencing coverage. If you will have enough coverage to make good quality calls within the reads of a single barcode (something like at least 15x-20x depending on your tolerance for errors), then subsetting by barcode could be reasonable. If you have less coverage, then the effects of reducing coverage will likely be much larger than whatever effect you are trying to detect. . If the barcodes separate different samples (i.e. those with different germline DNA), then the correct thing is to separate by barcode. I would need a little more information about the nature of the samples and what you are looking for to give you a more direct opinion.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/772
https://github.com/google/deepvariant/issues/772:387,availability,error,errors,387,"Hi @gneedle1 . It depends on the type of experiment. If the barcodes are the same sample and you are trying to get at some other specific property (e.g. cell type or preparation), then it's a question of sequencing coverage. If you will have enough coverage to make good quality calls within the reads of a single barcode (something like at least 15x-20x depending on your tolerance for errors), then subsetting by barcode could be reasonable. If you have less coverage, then the effects of reducing coverage will likely be much larger than whatever effect you are trying to detect. . If the barcodes separate different samples (i.e. those with different germline DNA), then the correct thing is to separate by barcode. I would need a little more information about the nature of the samples and what you are looking for to give you a more direct opinion.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/772
https://github.com/google/deepvariant/issues/772:18,deployability,depend,depends,18,"Hi @gneedle1 . It depends on the type of experiment. If the barcodes are the same sample and you are trying to get at some other specific property (e.g. cell type or preparation), then it's a question of sequencing coverage. If you will have enough coverage to make good quality calls within the reads of a single barcode (something like at least 15x-20x depending on your tolerance for errors), then subsetting by barcode could be reasonable. If you have less coverage, then the effects of reducing coverage will likely be much larger than whatever effect you are trying to detect. . If the barcodes separate different samples (i.e. those with different germline DNA), then the correct thing is to separate by barcode. I would need a little more information about the nature of the samples and what you are looking for to give you a more direct opinion.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/772
https://github.com/google/deepvariant/issues/772:355,deployability,depend,depending,355,"Hi @gneedle1 . It depends on the type of experiment. If the barcodes are the same sample and you are trying to get at some other specific property (e.g. cell type or preparation), then it's a question of sequencing coverage. If you will have enough coverage to make good quality calls within the reads of a single barcode (something like at least 15x-20x depending on your tolerance for errors), then subsetting by barcode could be reasonable. If you have less coverage, then the effects of reducing coverage will likely be much larger than whatever effect you are trying to detect. . If the barcodes separate different samples (i.e. those with different germline DNA), then the correct thing is to separate by barcode. I would need a little more information about the nature of the samples and what you are looking for to give you a more direct opinion.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/772
https://github.com/google/deepvariant/issues/772:491,energy efficiency,reduc,reducing,491,"Hi @gneedle1 . It depends on the type of experiment. If the barcodes are the same sample and you are trying to get at some other specific property (e.g. cell type or preparation), then it's a question of sequencing coverage. If you will have enough coverage to make good quality calls within the reads of a single barcode (something like at least 15x-20x depending on your tolerance for errors), then subsetting by barcode could be reasonable. If you have less coverage, then the effects of reducing coverage will likely be much larger than whatever effect you are trying to detect. . If the barcodes separate different samples (i.e. those with different germline DNA), then the correct thing is to separate by barcode. I would need a little more information about the nature of the samples and what you are looking for to give you a more direct opinion.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/772
https://github.com/google/deepvariant/issues/772:18,integrability,depend,depends,18,"Hi @gneedle1 . It depends on the type of experiment. If the barcodes are the same sample and you are trying to get at some other specific property (e.g. cell type or preparation), then it's a question of sequencing coverage. If you will have enough coverage to make good quality calls within the reads of a single barcode (something like at least 15x-20x depending on your tolerance for errors), then subsetting by barcode could be reasonable. If you have less coverage, then the effects of reducing coverage will likely be much larger than whatever effect you are trying to detect. . If the barcodes separate different samples (i.e. those with different germline DNA), then the correct thing is to separate by barcode. I would need a little more information about the nature of the samples and what you are looking for to give you a more direct opinion.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/772
https://github.com/google/deepvariant/issues/772:355,integrability,depend,depending,355,"Hi @gneedle1 . It depends on the type of experiment. If the barcodes are the same sample and you are trying to get at some other specific property (e.g. cell type or preparation), then it's a question of sequencing coverage. If you will have enough coverage to make good quality calls within the reads of a single barcode (something like at least 15x-20x depending on your tolerance for errors), then subsetting by barcode could be reasonable. If you have less coverage, then the effects of reducing coverage will likely be much larger than whatever effect you are trying to detect. . If the barcodes separate different samples (i.e. those with different germline DNA), then the correct thing is to separate by barcode. I would need a little more information about the nature of the samples and what you are looking for to give you a more direct opinion.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/772
https://github.com/google/deepvariant/issues/772:401,integrability,sub,subsetting,401,"Hi @gneedle1 . It depends on the type of experiment. If the barcodes are the same sample and you are trying to get at some other specific property (e.g. cell type or preparation), then it's a question of sequencing coverage. If you will have enough coverage to make good quality calls within the reads of a single barcode (something like at least 15x-20x depending on your tolerance for errors), then subsetting by barcode could be reasonable. If you have less coverage, then the effects of reducing coverage will likely be much larger than whatever effect you are trying to detect. . If the barcodes separate different samples (i.e. those with different germline DNA), then the correct thing is to separate by barcode. I would need a little more information about the nature of the samples and what you are looking for to give you a more direct opinion.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/772
https://github.com/google/deepvariant/issues/772:129,interoperability,specif,specific,129,"Hi @gneedle1 . It depends on the type of experiment. If the barcodes are the same sample and you are trying to get at some other specific property (e.g. cell type or preparation), then it's a question of sequencing coverage. If you will have enough coverage to make good quality calls within the reads of a single barcode (something like at least 15x-20x depending on your tolerance for errors), then subsetting by barcode could be reasonable. If you have less coverage, then the effects of reducing coverage will likely be much larger than whatever effect you are trying to detect. . If the barcodes separate different samples (i.e. those with different germline DNA), then the correct thing is to separate by barcode. I would need a little more information about the nature of the samples and what you are looking for to give you a more direct opinion.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/772
https://github.com/google/deepvariant/issues/772:18,modifiability,depend,depends,18,"Hi @gneedle1 . It depends on the type of experiment. If the barcodes are the same sample and you are trying to get at some other specific property (e.g. cell type or preparation), then it's a question of sequencing coverage. If you will have enough coverage to make good quality calls within the reads of a single barcode (something like at least 15x-20x depending on your tolerance for errors), then subsetting by barcode could be reasonable. If you have less coverage, then the effects of reducing coverage will likely be much larger than whatever effect you are trying to detect. . If the barcodes separate different samples (i.e. those with different germline DNA), then the correct thing is to separate by barcode. I would need a little more information about the nature of the samples and what you are looking for to give you a more direct opinion.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/772
https://github.com/google/deepvariant/issues/772:355,modifiability,depend,depending,355,"Hi @gneedle1 . It depends on the type of experiment. If the barcodes are the same sample and you are trying to get at some other specific property (e.g. cell type or preparation), then it's a question of sequencing coverage. If you will have enough coverage to make good quality calls within the reads of a single barcode (something like at least 15x-20x depending on your tolerance for errors), then subsetting by barcode could be reasonable. If you have less coverage, then the effects of reducing coverage will likely be much larger than whatever effect you are trying to detect. . If the barcodes separate different samples (i.e. those with different germline DNA), then the correct thing is to separate by barcode. I would need a little more information about the nature of the samples and what you are looking for to give you a more direct opinion.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/772
https://github.com/google/deepvariant/issues/772:387,performance,error,errors,387,"Hi @gneedle1 . It depends on the type of experiment. If the barcodes are the same sample and you are trying to get at some other specific property (e.g. cell type or preparation), then it's a question of sequencing coverage. If you will have enough coverage to make good quality calls within the reads of a single barcode (something like at least 15x-20x depending on your tolerance for errors), then subsetting by barcode could be reasonable. If you have less coverage, then the effects of reducing coverage will likely be much larger than whatever effect you are trying to detect. . If the barcodes separate different samples (i.e. those with different germline DNA), then the correct thing is to separate by barcode. I would need a little more information about the nature of the samples and what you are looking for to give you a more direct opinion.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/772
https://github.com/google/deepvariant/issues/772:373,reliability,toleran,tolerance,373,"Hi @gneedle1 . It depends on the type of experiment. If the barcodes are the same sample and you are trying to get at some other specific property (e.g. cell type or preparation), then it's a question of sequencing coverage. If you will have enough coverage to make good quality calls within the reads of a single barcode (something like at least 15x-20x depending on your tolerance for errors), then subsetting by barcode could be reasonable. If you have less coverage, then the effects of reducing coverage will likely be much larger than whatever effect you are trying to detect. . If the barcodes separate different samples (i.e. those with different germline DNA), then the correct thing is to separate by barcode. I would need a little more information about the nature of the samples and what you are looking for to give you a more direct opinion.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/772
https://github.com/google/deepvariant/issues/772:18,safety,depend,depends,18,"Hi @gneedle1 . It depends on the type of experiment. If the barcodes are the same sample and you are trying to get at some other specific property (e.g. cell type or preparation), then it's a question of sequencing coverage. If you will have enough coverage to make good quality calls within the reads of a single barcode (something like at least 15x-20x depending on your tolerance for errors), then subsetting by barcode could be reasonable. If you have less coverage, then the effects of reducing coverage will likely be much larger than whatever effect you are trying to detect. . If the barcodes separate different samples (i.e. those with different germline DNA), then the correct thing is to separate by barcode. I would need a little more information about the nature of the samples and what you are looking for to give you a more direct opinion.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/772
https://github.com/google/deepvariant/issues/772:355,safety,depend,depending,355,"Hi @gneedle1 . It depends on the type of experiment. If the barcodes are the same sample and you are trying to get at some other specific property (e.g. cell type or preparation), then it's a question of sequencing coverage. If you will have enough coverage to make good quality calls within the reads of a single barcode (something like at least 15x-20x depending on your tolerance for errors), then subsetting by barcode could be reasonable. If you have less coverage, then the effects of reducing coverage will likely be much larger than whatever effect you are trying to detect. . If the barcodes separate different samples (i.e. those with different germline DNA), then the correct thing is to separate by barcode. I would need a little more information about the nature of the samples and what you are looking for to give you a more direct opinion.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/772
https://github.com/google/deepvariant/issues/772:387,safety,error,errors,387,"Hi @gneedle1 . It depends on the type of experiment. If the barcodes are the same sample and you are trying to get at some other specific property (e.g. cell type or preparation), then it's a question of sequencing coverage. If you will have enough coverage to make good quality calls within the reads of a single barcode (something like at least 15x-20x depending on your tolerance for errors), then subsetting by barcode could be reasonable. If you have less coverage, then the effects of reducing coverage will likely be much larger than whatever effect you are trying to detect. . If the barcodes separate different samples (i.e. those with different germline DNA), then the correct thing is to separate by barcode. I would need a little more information about the nature of the samples and what you are looking for to give you a more direct opinion.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/772
https://github.com/google/deepvariant/issues/772:575,safety,detect,detect,575,"Hi @gneedle1 . It depends on the type of experiment. If the barcodes are the same sample and you are trying to get at some other specific property (e.g. cell type or preparation), then it's a question of sequencing coverage. If you will have enough coverage to make good quality calls within the reads of a single barcode (something like at least 15x-20x depending on your tolerance for errors), then subsetting by barcode could be reasonable. If you have less coverage, then the effects of reducing coverage will likely be much larger than whatever effect you are trying to detect. . If the barcodes separate different samples (i.e. those with different germline DNA), then the correct thing is to separate by barcode. I would need a little more information about the nature of the samples and what you are looking for to give you a more direct opinion.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/772
https://github.com/google/deepvariant/issues/772:575,security,detect,detect,575,"Hi @gneedle1 . It depends on the type of experiment. If the barcodes are the same sample and you are trying to get at some other specific property (e.g. cell type or preparation), then it's a question of sequencing coverage. If you will have enough coverage to make good quality calls within the reads of a single barcode (something like at least 15x-20x depending on your tolerance for errors), then subsetting by barcode could be reasonable. If you have less coverage, then the effects of reducing coverage will likely be much larger than whatever effect you are trying to detect. . If the barcodes separate different samples (i.e. those with different germline DNA), then the correct thing is to separate by barcode. I would need a little more information about the nature of the samples and what you are looking for to give you a more direct opinion.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/772
https://github.com/google/deepvariant/issues/772:18,testability,depend,depends,18,"Hi @gneedle1 . It depends on the type of experiment. If the barcodes are the same sample and you are trying to get at some other specific property (e.g. cell type or preparation), then it's a question of sequencing coverage. If you will have enough coverage to make good quality calls within the reads of a single barcode (something like at least 15x-20x depending on your tolerance for errors), then subsetting by barcode could be reasonable. If you have less coverage, then the effects of reducing coverage will likely be much larger than whatever effect you are trying to detect. . If the barcodes separate different samples (i.e. those with different germline DNA), then the correct thing is to separate by barcode. I would need a little more information about the nature of the samples and what you are looking for to give you a more direct opinion.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/772
https://github.com/google/deepvariant/issues/772:215,testability,coverag,coverage,215,"Hi @gneedle1 . It depends on the type of experiment. If the barcodes are the same sample and you are trying to get at some other specific property (e.g. cell type or preparation), then it's a question of sequencing coverage. If you will have enough coverage to make good quality calls within the reads of a single barcode (something like at least 15x-20x depending on your tolerance for errors), then subsetting by barcode could be reasonable. If you have less coverage, then the effects of reducing coverage will likely be much larger than whatever effect you are trying to detect. . If the barcodes separate different samples (i.e. those with different germline DNA), then the correct thing is to separate by barcode. I would need a little more information about the nature of the samples and what you are looking for to give you a more direct opinion.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/772
https://github.com/google/deepvariant/issues/772:249,testability,coverag,coverage,249,"Hi @gneedle1 . It depends on the type of experiment. If the barcodes are the same sample and you are trying to get at some other specific property (e.g. cell type or preparation), then it's a question of sequencing coverage. If you will have enough coverage to make good quality calls within the reads of a single barcode (something like at least 15x-20x depending on your tolerance for errors), then subsetting by barcode could be reasonable. If you have less coverage, then the effects of reducing coverage will likely be much larger than whatever effect you are trying to detect. . If the barcodes separate different samples (i.e. those with different germline DNA), then the correct thing is to separate by barcode. I would need a little more information about the nature of the samples and what you are looking for to give you a more direct opinion.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/772
https://github.com/google/deepvariant/issues/772:355,testability,depend,depending,355,"Hi @gneedle1 . It depends on the type of experiment. If the barcodes are the same sample and you are trying to get at some other specific property (e.g. cell type or preparation), then it's a question of sequencing coverage. If you will have enough coverage to make good quality calls within the reads of a single barcode (something like at least 15x-20x depending on your tolerance for errors), then subsetting by barcode could be reasonable. If you have less coverage, then the effects of reducing coverage will likely be much larger than whatever effect you are trying to detect. . If the barcodes separate different samples (i.e. those with different germline DNA), then the correct thing is to separate by barcode. I would need a little more information about the nature of the samples and what you are looking for to give you a more direct opinion.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/772
https://github.com/google/deepvariant/issues/772:461,testability,coverag,coverage,461,"Hi @gneedle1 . It depends on the type of experiment. If the barcodes are the same sample and you are trying to get at some other specific property (e.g. cell type or preparation), then it's a question of sequencing coverage. If you will have enough coverage to make good quality calls within the reads of a single barcode (something like at least 15x-20x depending on your tolerance for errors), then subsetting by barcode could be reasonable. If you have less coverage, then the effects of reducing coverage will likely be much larger than whatever effect you are trying to detect. . If the barcodes separate different samples (i.e. those with different germline DNA), then the correct thing is to separate by barcode. I would need a little more information about the nature of the samples and what you are looking for to give you a more direct opinion.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/772
https://github.com/google/deepvariant/issues/772:500,testability,coverag,coverage,500,"Hi @gneedle1 . It depends on the type of experiment. If the barcodes are the same sample and you are trying to get at some other specific property (e.g. cell type or preparation), then it's a question of sequencing coverage. If you will have enough coverage to make good quality calls within the reads of a single barcode (something like at least 15x-20x depending on your tolerance for errors), then subsetting by barcode could be reasonable. If you have less coverage, then the effects of reducing coverage will likely be much larger than whatever effect you are trying to detect. . If the barcodes separate different samples (i.e. those with different germline DNA), then the correct thing is to separate by barcode. I would need a little more information about the nature of the samples and what you are looking for to give you a more direct opinion.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/772
https://github.com/google/deepvariant/issues/772:387,usability,error,errors,387,"Hi @gneedle1 . It depends on the type of experiment. If the barcodes are the same sample and you are trying to get at some other specific property (e.g. cell type or preparation), then it's a question of sequencing coverage. If you will have enough coverage to make good quality calls within the reads of a single barcode (something like at least 15x-20x depending on your tolerance for errors), then subsetting by barcode could be reasonable. If you have less coverage, then the effects of reducing coverage will likely be much larger than whatever effect you are trying to detect. . If the barcodes separate different samples (i.e. those with different germline DNA), then the correct thing is to separate by barcode. I would need a little more information about the nature of the samples and what you are looking for to give you a more direct opinion.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/772
https://github.com/google/deepvariant/issues/772:414,deployability,build,build,414,The experimental setup is roughly:. 1. Hairy cell leukemia cells were isolated from the blood of patients. 2. Single-cell cDNAs were synthesized and barcoded by 10X Genomics platform (cDNAs of each cell are barcoded individually). 3. Direct RNA seq will be run for the single-cell cDNAs from one patient on the flow cell of ONT sequencer. 4. The goal is to call mutations for the cDNA in individual cells and then build single-cell phylogeny based on mutations.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/772
https://github.com/google/deepvariant/issues/772:174,interoperability,platform,platform,174,The experimental setup is roughly:. 1. Hairy cell leukemia cells were isolated from the blood of patients. 2. Single-cell cDNAs were synthesized and barcoded by 10X Genomics platform (cDNAs of each cell are barcoded individually). 3. Direct RNA seq will be run for the single-cell cDNAs from one patient on the flow cell of ONT sequencer. 4. The goal is to call mutations for the cDNA in individual cells and then build single-cell phylogeny based on mutations.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/772
https://github.com/google/deepvariant/issues/772:70,safety,isol,isolated,70,The experimental setup is roughly:. 1. Hairy cell leukemia cells were isolated from the blood of patients. 2. Single-cell cDNAs were synthesized and barcoded by 10X Genomics platform (cDNAs of each cell are barcoded individually). 3. Direct RNA seq will be run for the single-cell cDNAs from one patient on the flow cell of ONT sequencer. 4. The goal is to call mutations for the cDNA in individual cells and then build single-cell phylogeny based on mutations.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/772
https://github.com/google/deepvariant/issues/772:70,security,iso,isolated,70,The experimental setup is roughly:. 1. Hairy cell leukemia cells were isolated from the blood of patients. 2. Single-cell cDNAs were synthesized and barcoded by 10X Genomics platform (cDNAs of each cell are barcoded individually). 3. Direct RNA seq will be run for the single-cell cDNAs from one patient on the flow cell of ONT sequencer. 4. The goal is to call mutations for the cDNA in individual cells and then build single-cell phylogeny based on mutations.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/772
https://github.com/google/deepvariant/issues/772:70,testability,isol,isolated,70,The experimental setup is roughly:. 1. Hairy cell leukemia cells were isolated from the blood of patients. 2. Single-cell cDNAs were synthesized and barcoded by 10X Genomics platform (cDNAs of each cell are barcoded individually). 3. Direct RNA seq will be run for the single-cell cDNAs from one patient on the flow cell of ONT sequencer. 4. The goal is to call mutations for the cDNA in individual cells and then build single-cell phylogeny based on mutations.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/772
https://github.com/google/deepvariant/issues/772:54,interoperability,specif,specific,54,"I see, so in this case, each barcode corresponds to a specific cell. If you have sufficient coverage, you can split by barcode and make the call, it's really just a question of coverage. For your purpose, you can split by barcodes.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/772
https://github.com/google/deepvariant/issues/772:92,testability,coverag,coverage,92,"I see, so in this case, each barcode corresponds to a specific cell. If you have sufficient coverage, you can split by barcode and make the call, it's really just a question of coverage. For your purpose, you can split by barcodes.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/772
https://github.com/google/deepvariant/issues/772:177,testability,coverag,coverage,177,"I see, so in this case, each barcode corresponds to a specific cell. If you have sufficient coverage, you can split by barcode and make the call, it's really just a question of coverage. For your purpose, you can split by barcodes.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/772
https://github.com/google/deepvariant/issues/772:59,usability,close,close,59,Thanks @AndrewCarroll for the follow-up answer. I will now close this issue.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/772
https://github.com/google/deepvariant/issues/773:38,usability,confirm,confirm,38,"Thank you for your suggestion, I will confirm again！",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/773
https://github.com/google/deepvariant/issues/774:286,deployability,log,logs,286,"@Ge-Lab if possible, please structure your issue using code fences. See [this guide](https://docs.github.com/en/get-started/writing-on-github/working-with-advanced-formatting/creating-and-highlighting-code-blocks) for details. It will make it easier to read and understand if you place logs inside code blocks, for example. [This guide](https://docs.sylabs.io/guides/3.5/user-guide/gpu.html) suggests a few things to try. Interestingly, it apperas you should set `CUDA_VISIBLE_DEVICES=0` within the container itself, but `SINGULARITYENV_CUDA_VISIBLE_DEVICES=0` outside of the container. Were you setting the variable appropriately, within the container or outside of it? Additionally, since you have two GPUs you will want to set this variable to 0,1. Do you have the `nvidia-container-cli` installed as suggested on the support page? .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/774
https://github.com/google/deepvariant/issues/774:499,deployability,contain,container,499,"@Ge-Lab if possible, please structure your issue using code fences. See [this guide](https://docs.github.com/en/get-started/writing-on-github/working-with-advanced-formatting/creating-and-highlighting-code-blocks) for details. It will make it easier to read and understand if you place logs inside code blocks, for example. [This guide](https://docs.sylabs.io/guides/3.5/user-guide/gpu.html) suggests a few things to try. Interestingly, it apperas you should set `CUDA_VISIBLE_DEVICES=0` within the container itself, but `SINGULARITYENV_CUDA_VISIBLE_DEVICES=0` outside of the container. Were you setting the variable appropriately, within the container or outside of it? Additionally, since you have two GPUs you will want to set this variable to 0,1. Do you have the `nvidia-container-cli` installed as suggested on the support page? .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/774
https://github.com/google/deepvariant/issues/774:576,deployability,contain,container,576,"@Ge-Lab if possible, please structure your issue using code fences. See [this guide](https://docs.github.com/en/get-started/writing-on-github/working-with-advanced-formatting/creating-and-highlighting-code-blocks) for details. It will make it easier to read and understand if you place logs inside code blocks, for example. [This guide](https://docs.sylabs.io/guides/3.5/user-guide/gpu.html) suggests a few things to try. Interestingly, it apperas you should set `CUDA_VISIBLE_DEVICES=0` within the container itself, but `SINGULARITYENV_CUDA_VISIBLE_DEVICES=0` outside of the container. Were you setting the variable appropriately, within the container or outside of it? Additionally, since you have two GPUs you will want to set this variable to 0,1. Do you have the `nvidia-container-cli` installed as suggested on the support page? .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/774
https://github.com/google/deepvariant/issues/774:643,deployability,contain,container,643,"@Ge-Lab if possible, please structure your issue using code fences. See [this guide](https://docs.github.com/en/get-started/writing-on-github/working-with-advanced-formatting/creating-and-highlighting-code-blocks) for details. It will make it easier to read and understand if you place logs inside code blocks, for example. [This guide](https://docs.sylabs.io/guides/3.5/user-guide/gpu.html) suggests a few things to try. Interestingly, it apperas you should set `CUDA_VISIBLE_DEVICES=0` within the container itself, but `SINGULARITYENV_CUDA_VISIBLE_DEVICES=0` outside of the container. Were you setting the variable appropriately, within the container or outside of it? Additionally, since you have two GPUs you will want to set this variable to 0,1. Do you have the `nvidia-container-cli` installed as suggested on the support page? .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/774
https://github.com/google/deepvariant/issues/774:776,deployability,contain,container-cli,776,"@Ge-Lab if possible, please structure your issue using code fences. See [this guide](https://docs.github.com/en/get-started/writing-on-github/working-with-advanced-formatting/creating-and-highlighting-code-blocks) for details. It will make it easier to read and understand if you place logs inside code blocks, for example. [This guide](https://docs.sylabs.io/guides/3.5/user-guide/gpu.html) suggests a few things to try. Interestingly, it apperas you should set `CUDA_VISIBLE_DEVICES=0` within the container itself, but `SINGULARITYENV_CUDA_VISIBLE_DEVICES=0` outside of the container. Were you setting the variable appropriately, within the container or outside of it? Additionally, since you have two GPUs you will want to set this variable to 0,1. Do you have the `nvidia-container-cli` installed as suggested on the support page? .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/774
https://github.com/google/deepvariant/issues/774:791,deployability,instal,installed,791,"@Ge-Lab if possible, please structure your issue using code fences. See [this guide](https://docs.github.com/en/get-started/writing-on-github/working-with-advanced-formatting/creating-and-highlighting-code-blocks) for details. It will make it easier to read and understand if you place logs inside code blocks, for example. [This guide](https://docs.sylabs.io/guides/3.5/user-guide/gpu.html) suggests a few things to try. Interestingly, it apperas you should set `CUDA_VISIBLE_DEVICES=0` within the container itself, but `SINGULARITYENV_CUDA_VISIBLE_DEVICES=0` outside of the container. Were you setting the variable appropriately, within the container or outside of it? Additionally, since you have two GPUs you will want to set this variable to 0,1. Do you have the `nvidia-container-cli` installed as suggested on the support page? .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/774
https://github.com/google/deepvariant/issues/774:382,energy efficiency,gpu,gpu,382,"@Ge-Lab if possible, please structure your issue using code fences. See [this guide](https://docs.github.com/en/get-started/writing-on-github/working-with-advanced-formatting/creating-and-highlighting-code-blocks) for details. It will make it easier to read and understand if you place logs inside code blocks, for example. [This guide](https://docs.sylabs.io/guides/3.5/user-guide/gpu.html) suggests a few things to try. Interestingly, it apperas you should set `CUDA_VISIBLE_DEVICES=0` within the container itself, but `SINGULARITYENV_CUDA_VISIBLE_DEVICES=0` outside of the container. Were you setting the variable appropriately, within the container or outside of it? Additionally, since you have two GPUs you will want to set this variable to 0,1. Do you have the `nvidia-container-cli` installed as suggested on the support page? .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/774
https://github.com/google/deepvariant/issues/774:704,energy efficiency,GPU,GPUs,704,"@Ge-Lab if possible, please structure your issue using code fences. See [this guide](https://docs.github.com/en/get-started/writing-on-github/working-with-advanced-formatting/creating-and-highlighting-code-blocks) for details. It will make it easier to read and understand if you place logs inside code blocks, for example. [This guide](https://docs.sylabs.io/guides/3.5/user-guide/gpu.html) suggests a few things to try. Interestingly, it apperas you should set `CUDA_VISIBLE_DEVICES=0` within the container itself, but `SINGULARITYENV_CUDA_VISIBLE_DEVICES=0` outside of the container. Were you setting the variable appropriately, within the container or outside of it? Additionally, since you have two GPUs you will want to set this variable to 0,1. Do you have the `nvidia-container-cli` installed as suggested on the support page? .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/774
https://github.com/google/deepvariant/issues/774:164,interoperability,format,formatting,164,"@Ge-Lab if possible, please structure your issue using code fences. See [this guide](https://docs.github.com/en/get-started/writing-on-github/working-with-advanced-formatting/creating-and-highlighting-code-blocks) for details. It will make it easier to read and understand if you place logs inside code blocks, for example. [This guide](https://docs.sylabs.io/guides/3.5/user-guide/gpu.html) suggests a few things to try. Interestingly, it apperas you should set `CUDA_VISIBLE_DEVICES=0` within the container itself, but `SINGULARITYENV_CUDA_VISIBLE_DEVICES=0` outside of the container. Were you setting the variable appropriately, within the container or outside of it? Additionally, since you have two GPUs you will want to set this variable to 0,1. Do you have the `nvidia-container-cli` installed as suggested on the support page? .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/774
https://github.com/google/deepvariant/issues/774:608,modifiability,variab,variable,608,"@Ge-Lab if possible, please structure your issue using code fences. See [this guide](https://docs.github.com/en/get-started/writing-on-github/working-with-advanced-formatting/creating-and-highlighting-code-blocks) for details. It will make it easier to read and understand if you place logs inside code blocks, for example. [This guide](https://docs.sylabs.io/guides/3.5/user-guide/gpu.html) suggests a few things to try. Interestingly, it apperas you should set `CUDA_VISIBLE_DEVICES=0` within the container itself, but `SINGULARITYENV_CUDA_VISIBLE_DEVICES=0` outside of the container. Were you setting the variable appropriately, within the container or outside of it? Additionally, since you have two GPUs you will want to set this variable to 0,1. Do you have the `nvidia-container-cli` installed as suggested on the support page? .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/774
https://github.com/google/deepvariant/issues/774:735,modifiability,variab,variable,735,"@Ge-Lab if possible, please structure your issue using code fences. See [this guide](https://docs.github.com/en/get-started/writing-on-github/working-with-advanced-formatting/creating-and-highlighting-code-blocks) for details. It will make it easier to read and understand if you place logs inside code blocks, for example. [This guide](https://docs.sylabs.io/guides/3.5/user-guide/gpu.html) suggests a few things to try. Interestingly, it apperas you should set `CUDA_VISIBLE_DEVICES=0` within the container itself, but `SINGULARITYENV_CUDA_VISIBLE_DEVICES=0` outside of the container. Were you setting the variable appropriately, within the container or outside of it? Additionally, since you have two GPUs you will want to set this variable to 0,1. Do you have the `nvidia-container-cli` installed as suggested on the support page? .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/774
https://github.com/google/deepvariant/issues/774:382,performance,gpu,gpu,382,"@Ge-Lab if possible, please structure your issue using code fences. See [this guide](https://docs.github.com/en/get-started/writing-on-github/working-with-advanced-formatting/creating-and-highlighting-code-blocks) for details. It will make it easier to read and understand if you place logs inside code blocks, for example. [This guide](https://docs.sylabs.io/guides/3.5/user-guide/gpu.html) suggests a few things to try. Interestingly, it apperas you should set `CUDA_VISIBLE_DEVICES=0` within the container itself, but `SINGULARITYENV_CUDA_VISIBLE_DEVICES=0` outside of the container. Were you setting the variable appropriately, within the container or outside of it? Additionally, since you have two GPUs you will want to set this variable to 0,1. Do you have the `nvidia-container-cli` installed as suggested on the support page? .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/774
https://github.com/google/deepvariant/issues/774:704,performance,GPU,GPUs,704,"@Ge-Lab if possible, please structure your issue using code fences. See [this guide](https://docs.github.com/en/get-started/writing-on-github/working-with-advanced-formatting/creating-and-highlighting-code-blocks) for details. It will make it easier to read and understand if you place logs inside code blocks, for example. [This guide](https://docs.sylabs.io/guides/3.5/user-guide/gpu.html) suggests a few things to try. Interestingly, it apperas you should set `CUDA_VISIBLE_DEVICES=0` within the container itself, but `SINGULARITYENV_CUDA_VISIBLE_DEVICES=0` outside of the container. Were you setting the variable appropriately, within the container or outside of it? Additionally, since you have two GPUs you will want to set this variable to 0,1. Do you have the `nvidia-container-cli` installed as suggested on the support page? .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/774
https://github.com/google/deepvariant/issues/774:286,safety,log,logs,286,"@Ge-Lab if possible, please structure your issue using code fences. See [this guide](https://docs.github.com/en/get-started/writing-on-github/working-with-advanced-formatting/creating-and-highlighting-code-blocks) for details. It will make it easier to read and understand if you place logs inside code blocks, for example. [This guide](https://docs.sylabs.io/guides/3.5/user-guide/gpu.html) suggests a few things to try. Interestingly, it apperas you should set `CUDA_VISIBLE_DEVICES=0` within the container itself, but `SINGULARITYENV_CUDA_VISIBLE_DEVICES=0` outside of the container. Were you setting the variable appropriately, within the container or outside of it? Additionally, since you have two GPUs you will want to set this variable to 0,1. Do you have the `nvidia-container-cli` installed as suggested on the support page? .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/774
https://github.com/google/deepvariant/issues/774:286,security,log,logs,286,"@Ge-Lab if possible, please structure your issue using code fences. See [this guide](https://docs.github.com/en/get-started/writing-on-github/working-with-advanced-formatting/creating-and-highlighting-code-blocks) for details. It will make it easier to read and understand if you place logs inside code blocks, for example. [This guide](https://docs.sylabs.io/guides/3.5/user-guide/gpu.html) suggests a few things to try. Interestingly, it apperas you should set `CUDA_VISIBLE_DEVICES=0` within the container itself, but `SINGULARITYENV_CUDA_VISIBLE_DEVICES=0` outside of the container. Were you setting the variable appropriately, within the container or outside of it? Additionally, since you have two GPUs you will want to set this variable to 0,1. Do you have the `nvidia-container-cli` installed as suggested on the support page? .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/774
https://github.com/google/deepvariant/issues/774:262,testability,understand,understand,262,"@Ge-Lab if possible, please structure your issue using code fences. See [this guide](https://docs.github.com/en/get-started/writing-on-github/working-with-advanced-formatting/creating-and-highlighting-code-blocks) for details. It will make it easier to read and understand if you place logs inside code blocks, for example. [This guide](https://docs.sylabs.io/guides/3.5/user-guide/gpu.html) suggests a few things to try. Interestingly, it apperas you should set `CUDA_VISIBLE_DEVICES=0` within the container itself, but `SINGULARITYENV_CUDA_VISIBLE_DEVICES=0` outside of the container. Were you setting the variable appropriately, within the container or outside of it? Additionally, since you have two GPUs you will want to set this variable to 0,1. Do you have the `nvidia-container-cli` installed as suggested on the support page? .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/774
https://github.com/google/deepvariant/issues/774:286,testability,log,logs,286,"@Ge-Lab if possible, please structure your issue using code fences. See [this guide](https://docs.github.com/en/get-started/writing-on-github/working-with-advanced-formatting/creating-and-highlighting-code-blocks) for details. It will make it easier to read and understand if you place logs inside code blocks, for example. [This guide](https://docs.sylabs.io/guides/3.5/user-guide/gpu.html) suggests a few things to try. Interestingly, it apperas you should set `CUDA_VISIBLE_DEVICES=0` within the container itself, but `SINGULARITYENV_CUDA_VISIBLE_DEVICES=0` outside of the container. Were you setting the variable appropriately, within the container or outside of it? Additionally, since you have two GPUs you will want to set this variable to 0,1. Do you have the `nvidia-container-cli` installed as suggested on the support page? .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/774
https://github.com/google/deepvariant/issues/774:78,usability,guid,guide,78,"@Ge-Lab if possible, please structure your issue using code fences. See [this guide](https://docs.github.com/en/get-started/writing-on-github/working-with-advanced-formatting/creating-and-highlighting-code-blocks) for details. It will make it easier to read and understand if you place logs inside code blocks, for example. [This guide](https://docs.sylabs.io/guides/3.5/user-guide/gpu.html) suggests a few things to try. Interestingly, it apperas you should set `CUDA_VISIBLE_DEVICES=0` within the container itself, but `SINGULARITYENV_CUDA_VISIBLE_DEVICES=0` outside of the container. Were you setting the variable appropriately, within the container or outside of it? Additionally, since you have two GPUs you will want to set this variable to 0,1. Do you have the `nvidia-container-cli` installed as suggested on the support page? .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/774
https://github.com/google/deepvariant/issues/774:330,usability,guid,guide,330,"@Ge-Lab if possible, please structure your issue using code fences. See [this guide](https://docs.github.com/en/get-started/writing-on-github/working-with-advanced-formatting/creating-and-highlighting-code-blocks) for details. It will make it easier to read and understand if you place logs inside code blocks, for example. [This guide](https://docs.sylabs.io/guides/3.5/user-guide/gpu.html) suggests a few things to try. Interestingly, it apperas you should set `CUDA_VISIBLE_DEVICES=0` within the container itself, but `SINGULARITYENV_CUDA_VISIBLE_DEVICES=0` outside of the container. Were you setting the variable appropriately, within the container or outside of it? Additionally, since you have two GPUs you will want to set this variable to 0,1. Do you have the `nvidia-container-cli` installed as suggested on the support page? .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/774
https://github.com/google/deepvariant/issues/774:360,usability,guid,guides,360,"@Ge-Lab if possible, please structure your issue using code fences. See [this guide](https://docs.github.com/en/get-started/writing-on-github/working-with-advanced-formatting/creating-and-highlighting-code-blocks) for details. It will make it easier to read and understand if you place logs inside code blocks, for example. [This guide](https://docs.sylabs.io/guides/3.5/user-guide/gpu.html) suggests a few things to try. Interestingly, it apperas you should set `CUDA_VISIBLE_DEVICES=0` within the container itself, but `SINGULARITYENV_CUDA_VISIBLE_DEVICES=0` outside of the container. Were you setting the variable appropriately, within the container or outside of it? Additionally, since you have two GPUs you will want to set this variable to 0,1. Do you have the `nvidia-container-cli` installed as suggested on the support page? .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/774
https://github.com/google/deepvariant/issues/774:371,usability,user,user-guide,371,"@Ge-Lab if possible, please structure your issue using code fences. See [this guide](https://docs.github.com/en/get-started/writing-on-github/working-with-advanced-formatting/creating-and-highlighting-code-blocks) for details. It will make it easier to read and understand if you place logs inside code blocks, for example. [This guide](https://docs.sylabs.io/guides/3.5/user-guide/gpu.html) suggests a few things to try. Interestingly, it apperas you should set `CUDA_VISIBLE_DEVICES=0` within the container itself, but `SINGULARITYENV_CUDA_VISIBLE_DEVICES=0` outside of the container. Were you setting the variable appropriately, within the container or outside of it? Additionally, since you have two GPUs you will want to set this variable to 0,1. Do you have the `nvidia-container-cli` installed as suggested on the support page? .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/774
https://github.com/google/deepvariant/issues/774:821,usability,support,support,821,"@Ge-Lab if possible, please structure your issue using code fences. See [this guide](https://docs.github.com/en/get-started/writing-on-github/working-with-advanced-formatting/creating-and-highlighting-code-blocks) for details. It will make it easier to read and understand if you place logs inside code blocks, for example. [This guide](https://docs.sylabs.io/guides/3.5/user-guide/gpu.html) suggests a few things to try. Interestingly, it apperas you should set `CUDA_VISIBLE_DEVICES=0` within the container itself, but `SINGULARITYENV_CUDA_VISIBLE_DEVICES=0` outside of the container. Were you setting the variable appropriately, within the container or outside of it? Additionally, since you have two GPUs you will want to set this variable to 0,1. Do you have the `nvidia-container-cli` installed as suggested on the support page? .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/774
https://github.com/google/deepvariant/issues/774:41,deployability,updat,updated,41,"Hi Daniele,. Thanks for your response. I updated the post as you suggested. . How can I set ```CUDA_VISIBLE_DEVICES=0``` within the container and ```SINGULARITYENV_CUDA_VISIBLE_DEVICES=0``` outside of the container? I just typed ```export CUDA_VISIBLE_DEVICES=0``` in the terminal and then typed the apptainer command to run. . I do have ```nvidia-container-cli```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/774
https://github.com/google/deepvariant/issues/774:132,deployability,contain,container,132,"Hi Daniele,. Thanks for your response. I updated the post as you suggested. . How can I set ```CUDA_VISIBLE_DEVICES=0``` within the container and ```SINGULARITYENV_CUDA_VISIBLE_DEVICES=0``` outside of the container? I just typed ```export CUDA_VISIBLE_DEVICES=0``` in the terminal and then typed the apptainer command to run. . I do have ```nvidia-container-cli```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/774
https://github.com/google/deepvariant/issues/774:205,deployability,contain,container,205,"Hi Daniele,. Thanks for your response. I updated the post as you suggested. . How can I set ```CUDA_VISIBLE_DEVICES=0``` within the container and ```SINGULARITYENV_CUDA_VISIBLE_DEVICES=0``` outside of the container? I just typed ```export CUDA_VISIBLE_DEVICES=0``` in the terminal and then typed the apptainer command to run. . I do have ```nvidia-container-cli```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/774
https://github.com/google/deepvariant/issues/774:348,deployability,contain,container-cli,348,"Hi Daniele,. Thanks for your response. I updated the post as you suggested. . How can I set ```CUDA_VISIBLE_DEVICES=0``` within the container and ```SINGULARITYENV_CUDA_VISIBLE_DEVICES=0``` outside of the container? I just typed ```export CUDA_VISIBLE_DEVICES=0``` in the terminal and then typed the apptainer command to run. . I do have ```nvidia-container-cli```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/774
https://github.com/google/deepvariant/issues/774:41,safety,updat,updated,41,"Hi Daniele,. Thanks for your response. I updated the post as you suggested. . How can I set ```CUDA_VISIBLE_DEVICES=0``` within the container and ```SINGULARITYENV_CUDA_VISIBLE_DEVICES=0``` outside of the container? I just typed ```export CUDA_VISIBLE_DEVICES=0``` in the terminal and then typed the apptainer command to run. . I do have ```nvidia-container-cli```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/774
https://github.com/google/deepvariant/issues/774:41,security,updat,updated,41,"Hi Daniele,. Thanks for your response. I updated the post as you suggested. . How can I set ```CUDA_VISIBLE_DEVICES=0``` within the container and ```SINGULARITYENV_CUDA_VISIBLE_DEVICES=0``` outside of the container? I just typed ```export CUDA_VISIBLE_DEVICES=0``` in the terminal and then typed the apptainer command to run. . I do have ```nvidia-container-cli```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/774
https://github.com/google/deepvariant/issues/774:310,usability,command,command,310,"Hi Daniele,. Thanks for your response. I updated the post as you suggested. . How can I set ```CUDA_VISIBLE_DEVICES=0``` within the container and ```SINGULARITYENV_CUDA_VISIBLE_DEVICES=0``` outside of the container? I just typed ```export CUDA_VISIBLE_DEVICES=0``` in the terminal and then typed the apptainer command to run. . I do have ```nvidia-container-cli```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/774
https://github.com/google/deepvariant/issues/774:97,deployability,contain,container,97,@Ge-Lab first lets try setting `export SINGULARITYENV_CUDA_VISIBLE_DEVICES=0` before you run the container since that is easier. Give that a try and let me know if you are still having issues.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/774
https://github.com/google/deepvariant/issues/774:67,usability,close,close,67,"Hi @Ge-Lab , this has been inactive for more than a month, so I'll close. But please feel free to reopen with more details now if you want to follow up!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/774
https://github.com/google/deepvariant/issues/775:304,energy efficiency,optim,optimize,304,"Hi @AndrewCarroll When i ran variant calling on the same bam with other tool Im getting more variants than while running deepvariant. Also, some of the important variants are missed in the final output in the deepvariant. I tried 1.4.0 and i'm getting the same output. Let me know if there is any way to optimize the parameter or the code I'm trying is correct.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/775
https://github.com/google/deepvariant/issues/775:317,modifiability,paramet,parameter,317,"Hi @AndrewCarroll When i ran variant calling on the same bam with other tool Im getting more variants than while running deepvariant. Also, some of the important variants are missed in the final output in the deepvariant. I tried 1.4.0 and i'm getting the same output. Let me know if there is any way to optimize the parameter or the code I'm trying is correct.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/775
https://github.com/google/deepvariant/issues/775:304,performance,optimiz,optimize,304,"Hi @AndrewCarroll When i ran variant calling on the same bam with other tool Im getting more variants than while running deepvariant. Also, some of the important variants are missed in the final output in the deepvariant. I tried 1.4.0 and i'm getting the same output. Let me know if there is any way to optimize the parameter or the code I'm trying is correct.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/775
https://github.com/google/deepvariant/issues/775:72,usability,tool,tool,72,"Hi @AndrewCarroll When i ran variant calling on the same bam with other tool Im getting more variants than while running deepvariant. Also, some of the important variants are missed in the final output in the deepvariant. I tried 1.4.0 and i'm getting the same output. Let me know if there is any way to optimize the parameter or the code I'm trying is correct.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/775
https://github.com/google/deepvariant/issues/775:192,deployability,observ,observation,192,"Hi @NIBIL401 . I don't see any other specific issues in your command. Without knowing more about the specific types of differences, it's difficult to give advice on what might be missing. One observation that we do have is that DeepVariant has learned not to call RNA editing events as variants. These are post-transcription changes to the RNA sequence. Those edits appear as A->G and T->C in sequencing data. To give more advice beyond this, I think I would need to know more about the sequencing (the most ideal would be to have some a BAM file or snippet with a variant call not being made that we can diagnose why). Thank you,. Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/775
https://github.com/google/deepvariant/issues/775:276,integrability,event,events,276,"Hi @NIBIL401 . I don't see any other specific issues in your command. Without knowing more about the specific types of differences, it's difficult to give advice on what might be missing. One observation that we do have is that DeepVariant has learned not to call RNA editing events as variants. These are post-transcription changes to the RNA sequence. Those edits appear as A->G and T->C in sequencing data. To give more advice beyond this, I think I would need to know more about the sequencing (the most ideal would be to have some a BAM file or snippet with a variant call not being made that we can diagnose why). Thank you,. Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/775
https://github.com/google/deepvariant/issues/775:37,interoperability,specif,specific,37,"Hi @NIBIL401 . I don't see any other specific issues in your command. Without knowing more about the specific types of differences, it's difficult to give advice on what might be missing. One observation that we do have is that DeepVariant has learned not to call RNA editing events as variants. These are post-transcription changes to the RNA sequence. Those edits appear as A->G and T->C in sequencing data. To give more advice beyond this, I think I would need to know more about the sequencing (the most ideal would be to have some a BAM file or snippet with a variant call not being made that we can diagnose why). Thank you,. Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/775
https://github.com/google/deepvariant/issues/775:101,interoperability,specif,specific,101,"Hi @NIBIL401 . I don't see any other specific issues in your command. Without knowing more about the specific types of differences, it's difficult to give advice on what might be missing. One observation that we do have is that DeepVariant has learned not to call RNA editing events as variants. These are post-transcription changes to the RNA sequence. Those edits appear as A->G and T->C in sequencing data. To give more advice beyond this, I think I would need to know more about the sequencing (the most ideal would be to have some a BAM file or snippet with a variant call not being made that we can diagnose why). Thank you,. Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/775
https://github.com/google/deepvariant/issues/775:605,reliability,diagno,diagnose,605,"Hi @NIBIL401 . I don't see any other specific issues in your command. Without knowing more about the specific types of differences, it's difficult to give advice on what might be missing. One observation that we do have is that DeepVariant has learned not to call RNA editing events as variants. These are post-transcription changes to the RNA sequence. Those edits appear as A->G and T->C in sequencing data. To give more advice beyond this, I think I would need to know more about the sequencing (the most ideal would be to have some a BAM file or snippet with a variant call not being made that we can diagnose why). Thank you,. Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/775
https://github.com/google/deepvariant/issues/775:192,testability,observ,observation,192,"Hi @NIBIL401 . I don't see any other specific issues in your command. Without knowing more about the specific types of differences, it's difficult to give advice on what might be missing. One observation that we do have is that DeepVariant has learned not to call RNA editing events as variants. These are post-transcription changes to the RNA sequence. Those edits appear as A->G and T->C in sequencing data. To give more advice beyond this, I think I would need to know more about the sequencing (the most ideal would be to have some a BAM file or snippet with a variant call not being made that we can diagnose why). Thank you,. Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/775
https://github.com/google/deepvariant/issues/775:605,testability,diagno,diagnose,605,"Hi @NIBIL401 . I don't see any other specific issues in your command. Without knowing more about the specific types of differences, it's difficult to give advice on what might be missing. One observation that we do have is that DeepVariant has learned not to call RNA editing events as variants. These are post-transcription changes to the RNA sequence. Those edits appear as A->G and T->C in sequencing data. To give more advice beyond this, I think I would need to know more about the sequencing (the most ideal would be to have some a BAM file or snippet with a variant call not being made that we can diagnose why). Thank you,. Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/775
https://github.com/google/deepvariant/issues/775:61,usability,command,command,61,"Hi @NIBIL401 . I don't see any other specific issues in your command. Without knowing more about the specific types of differences, it's difficult to give advice on what might be missing. One observation that we do have is that DeepVariant has learned not to call RNA editing events as variants. These are post-transcription changes to the RNA sequence. Those edits appear as A->G and T->C in sequencing data. To give more advice beyond this, I think I would need to know more about the sequencing (the most ideal would be to have some a BAM file or snippet with a variant call not being made that we can diagnose why). Thank you,. Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/775
https://github.com/google/deepvariant/issues/775:244,usability,learn,learned,244,"Hi @NIBIL401 . I don't see any other specific issues in your command. Without knowing more about the specific types of differences, it's difficult to give advice on what might be missing. One observation that we do have is that DeepVariant has learned not to call RNA editing events as variants. These are post-transcription changes to the RNA sequence. Those edits appear as A->G and T->C in sequencing data. To give more advice beyond this, I think I would need to know more about the sequencing (the most ideal would be to have some a BAM file or snippet with a variant call not being made that we can diagnose why). Thank you,. Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/775
https://github.com/google/deepvariant/issues/775:197,interoperability,share,share,197,"Hi @NIBIL401 . I'm sorry, but without taking a look at the BAM file and the variants called or not called, it's quite difficult to say the reason why a variant would be missing. If you are able to share a snippet of it with an example, we can take a look. For chimeric reads, do you mean secondary/supplementary read alignments?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/775
https://github.com/google/deepvariant/issues/775:182,integrability,pub,public,182,"Hi @NIBIL401 , as @AndrewCarroll mentioned, it's hard for us to help determine the reason if we can't have a reproducible setup. If you can provide a similar reproducible setup with public data, that will be great! Meanwhile, please read https://github.com/google/deepvariant/blob/r1.6/docs/FAQ.md#why-does-deepvariant-not-call-a-specific-variant-in-my-data to see if any of the topics there might apply. For now, I'll close this issue, but please do feel free to reopen this bug with more information to help us debug!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/775
https://github.com/google/deepvariant/issues/775:379,integrability,topic,topics,379,"Hi @NIBIL401 , as @AndrewCarroll mentioned, it's hard for us to help determine the reason if we can't have a reproducible setup. If you can provide a similar reproducible setup with public data, that will be great! Meanwhile, please read https://github.com/google/deepvariant/blob/r1.6/docs/FAQ.md#why-does-deepvariant-not-call-a-specific-variant-in-my-data to see if any of the topics there might apply. For now, I'll close this issue, but please do feel free to reopen this bug with more information to help us debug!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/775
https://github.com/google/deepvariant/issues/775:330,interoperability,specif,specific-variant-in-my-data,330,"Hi @NIBIL401 , as @AndrewCarroll mentioned, it's hard for us to help determine the reason if we can't have a reproducible setup. If you can provide a similar reproducible setup with public data, that will be great! Meanwhile, please read https://github.com/google/deepvariant/blob/r1.6/docs/FAQ.md#why-does-deepvariant-not-call-a-specific-variant-in-my-data to see if any of the topics there might apply. For now, I'll close this issue, but please do feel free to reopen this bug with more information to help us debug!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/775
https://github.com/google/deepvariant/issues/775:302,reliability,doe,does-deepvariant-not-call-a-specific-variant-in-my-data,302,"Hi @NIBIL401 , as @AndrewCarroll mentioned, it's hard for us to help determine the reason if we can't have a reproducible setup. If you can provide a similar reproducible setup with public data, that will be great! Meanwhile, please read https://github.com/google/deepvariant/blob/r1.6/docs/FAQ.md#why-does-deepvariant-not-call-a-specific-variant-in-my-data to see if any of the topics there might apply. For now, I'll close this issue, but please do feel free to reopen this bug with more information to help us debug!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/775
https://github.com/google/deepvariant/issues/775:64,usability,help,help,64,"Hi @NIBIL401 , as @AndrewCarroll mentioned, it's hard for us to help determine the reason if we can't have a reproducible setup. If you can provide a similar reproducible setup with public data, that will be great! Meanwhile, please read https://github.com/google/deepvariant/blob/r1.6/docs/FAQ.md#why-does-deepvariant-not-call-a-specific-variant-in-my-data to see if any of the topics there might apply. For now, I'll close this issue, but please do feel free to reopen this bug with more information to help us debug!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/775
https://github.com/google/deepvariant/issues/775:419,usability,close,close,419,"Hi @NIBIL401 , as @AndrewCarroll mentioned, it's hard for us to help determine the reason if we can't have a reproducible setup. If you can provide a similar reproducible setup with public data, that will be great! Meanwhile, please read https://github.com/google/deepvariant/blob/r1.6/docs/FAQ.md#why-does-deepvariant-not-call-a-specific-variant-in-my-data to see if any of the topics there might apply. For now, I'll close this issue, but please do feel free to reopen this bug with more information to help us debug!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/775
https://github.com/google/deepvariant/issues/775:505,usability,help,help,505,"Hi @NIBIL401 , as @AndrewCarroll mentioned, it's hard for us to help determine the reason if we can't have a reproducible setup. If you can provide a similar reproducible setup with public data, that will be great! Meanwhile, please read https://github.com/google/deepvariant/blob/r1.6/docs/FAQ.md#why-does-deepvariant-not-call-a-specific-variant-in-my-data to see if any of the topics there might apply. For now, I'll close this issue, but please do feel free to reopen this bug with more information to help us debug!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/775
https://github.com/google/deepvariant/issues/776:254,deployability,observ,observing,254,"@ErinKinghorn somewhat confusingly, shard specifies are 0-based for the first number (shard index) and 1-indexed for the second (count). So your first shard is `00000-of-00032` and your last shard is `00031-of-00032`. Can you confirm that you indeed are observing only 31 output files? The message you report here looks normal - and the warning should not make a difference.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/776
https://github.com/google/deepvariant/issues/776:290,integrability,messag,message,290,"@ErinKinghorn somewhat confusingly, shard specifies are 0-based for the first number (shard index) and 1-indexed for the second (count). So your first shard is `00000-of-00032` and your last shard is `00031-of-00032`. Can you confirm that you indeed are observing only 31 output files? The message you report here looks normal - and the warning should not make a difference.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/776
https://github.com/google/deepvariant/issues/776:42,interoperability,specif,specifies,42,"@ErinKinghorn somewhat confusingly, shard specifies are 0-based for the first number (shard index) and 1-indexed for the second (count). So your first shard is `00000-of-00032` and your last shard is `00031-of-00032`. Can you confirm that you indeed are observing only 31 output files? The message you report here looks normal - and the warning should not make a difference.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/776
https://github.com/google/deepvariant/issues/776:290,interoperability,messag,message,290,"@ErinKinghorn somewhat confusingly, shard specifies are 0-based for the first number (shard index) and 1-indexed for the second (count). So your first shard is `00000-of-00032` and your last shard is `00031-of-00032`. Can you confirm that you indeed are observing only 31 output files? The message you report here looks normal - and the warning should not make a difference.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/776
https://github.com/google/deepvariant/issues/776:254,testability,observ,observing,254,"@ErinKinghorn somewhat confusingly, shard specifies are 0-based for the first number (shard index) and 1-indexed for the second (count). So your first shard is `00000-of-00032` and your last shard is `00031-of-00032`. Can you confirm that you indeed are observing only 31 output files? The message you report here looks normal - and the warning should not make a difference.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/776
https://github.com/google/deepvariant/issues/776:226,usability,confirm,confirm,226,"@ErinKinghorn somewhat confusingly, shard specifies are 0-based for the first number (shard index) and 1-indexed for the second (count). So your first shard is `00000-of-00032` and your last shard is `00031-of-00032`. Can you confirm that you indeed are observing only 31 output files? The message you report here looks normal - and the warning should not make a difference.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/776
https://github.com/google/deepvariant/issues/776:264,energy efficiency,gpu,gpu,264,"@ErinKinghorn ,. Once you follow @danielecook 's suggestion to see if the files look good. If you in fact find the files look normal, can you please test this docker:. ```bash. docker pull google/deepvariant:CL602468145. docker pull google/deepvariant:CL602468145-gpu. ```. It seems like this issue is related to a this issue: https://github.com/google/deepvariant/issues/769",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/776
https://github.com/google/deepvariant/issues/776:264,performance,gpu,gpu,264,"@ErinKinghorn ,. Once you follow @danielecook 's suggestion to see if the files look good. If you in fact find the files look normal, can you please test this docker:. ```bash. docker pull google/deepvariant:CL602468145. docker pull google/deepvariant:CL602468145-gpu. ```. It seems like this issue is related to a this issue: https://github.com/google/deepvariant/issues/769",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/776
https://github.com/google/deepvariant/issues/776:149,safety,test,test,149,"@ErinKinghorn ,. Once you follow @danielecook 's suggestion to see if the files look good. If you in fact find the files look normal, can you please test this docker:. ```bash. docker pull google/deepvariant:CL602468145. docker pull google/deepvariant:CL602468145-gpu. ```. It seems like this issue is related to a this issue: https://github.com/google/deepvariant/issues/769",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/776
https://github.com/google/deepvariant/issues/776:149,testability,test,test,149,"@ErinKinghorn ,. Once you follow @danielecook 's suggestion to see if the files look good. If you in fact find the files look normal, can you please test this docker:. ```bash. docker pull google/deepvariant:CL602468145. docker pull google/deepvariant:CL602468145-gpu. ```. It seems like this issue is related to a this issue: https://github.com/google/deepvariant/issues/769",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/776
https://github.com/google/deepvariant/issues/776:301,deployability,log,log,301,"Thank you for the feedback! @danielecook I can confirm that the files in the tmp directory do look to be normal as you described above. @kishwarshafin, I tested the docker that you suggested and now it seems that Deepvariant did not run at all (vcfs and gvcfs are empty). [deepvarrun_b37_MND_G33.1kei.log](https://github.com/google/deepvariant/files/14458499/deepvarrun_b37_MND_G33.1kei.log). . I have attached a log file for one of the samples, so you can see what happened.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/776
https://github.com/google/deepvariant/issues/776:387,deployability,log,log,387,"Thank you for the feedback! @danielecook I can confirm that the files in the tmp directory do look to be normal as you described above. @kishwarshafin, I tested the docker that you suggested and now it seems that Deepvariant did not run at all (vcfs and gvcfs are empty). [deepvarrun_b37_MND_G33.1kei.log](https://github.com/google/deepvariant/files/14458499/deepvarrun_b37_MND_G33.1kei.log). . I have attached a log file for one of the samples, so you can see what happened.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/776
https://github.com/google/deepvariant/issues/776:413,deployability,log,log,413,"Thank you for the feedback! @danielecook I can confirm that the files in the tmp directory do look to be normal as you described above. @kishwarshafin, I tested the docker that you suggested and now it seems that Deepvariant did not run at all (vcfs and gvcfs are empty). [deepvarrun_b37_MND_G33.1kei.log](https://github.com/google/deepvariant/files/14458499/deepvarrun_b37_MND_G33.1kei.log). . I have attached a log file for one of the samples, so you can see what happened.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/776
https://github.com/google/deepvariant/issues/776:154,safety,test,tested,154,"Thank you for the feedback! @danielecook I can confirm that the files in the tmp directory do look to be normal as you described above. @kishwarshafin, I tested the docker that you suggested and now it seems that Deepvariant did not run at all (vcfs and gvcfs are empty). [deepvarrun_b37_MND_G33.1kei.log](https://github.com/google/deepvariant/files/14458499/deepvarrun_b37_MND_G33.1kei.log). . I have attached a log file for one of the samples, so you can see what happened.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/776
https://github.com/google/deepvariant/issues/776:301,safety,log,log,301,"Thank you for the feedback! @danielecook I can confirm that the files in the tmp directory do look to be normal as you described above. @kishwarshafin, I tested the docker that you suggested and now it seems that Deepvariant did not run at all (vcfs and gvcfs are empty). [deepvarrun_b37_MND_G33.1kei.log](https://github.com/google/deepvariant/files/14458499/deepvarrun_b37_MND_G33.1kei.log). . I have attached a log file for one of the samples, so you can see what happened.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/776
https://github.com/google/deepvariant/issues/776:387,safety,log,log,387,"Thank you for the feedback! @danielecook I can confirm that the files in the tmp directory do look to be normal as you described above. @kishwarshafin, I tested the docker that you suggested and now it seems that Deepvariant did not run at all (vcfs and gvcfs are empty). [deepvarrun_b37_MND_G33.1kei.log](https://github.com/google/deepvariant/files/14458499/deepvarrun_b37_MND_G33.1kei.log). . I have attached a log file for one of the samples, so you can see what happened.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/776
https://github.com/google/deepvariant/issues/776:413,safety,log,log,413,"Thank you for the feedback! @danielecook I can confirm that the files in the tmp directory do look to be normal as you described above. @kishwarshafin, I tested the docker that you suggested and now it seems that Deepvariant did not run at all (vcfs and gvcfs are empty). [deepvarrun_b37_MND_G33.1kei.log](https://github.com/google/deepvariant/files/14458499/deepvarrun_b37_MND_G33.1kei.log). . I have attached a log file for one of the samples, so you can see what happened.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/776
https://github.com/google/deepvariant/issues/776:301,security,log,log,301,"Thank you for the feedback! @danielecook I can confirm that the files in the tmp directory do look to be normal as you described above. @kishwarshafin, I tested the docker that you suggested and now it seems that Deepvariant did not run at all (vcfs and gvcfs are empty). [deepvarrun_b37_MND_G33.1kei.log](https://github.com/google/deepvariant/files/14458499/deepvarrun_b37_MND_G33.1kei.log). . I have attached a log file for one of the samples, so you can see what happened.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/776
https://github.com/google/deepvariant/issues/776:387,security,log,log,387,"Thank you for the feedback! @danielecook I can confirm that the files in the tmp directory do look to be normal as you described above. @kishwarshafin, I tested the docker that you suggested and now it seems that Deepvariant did not run at all (vcfs and gvcfs are empty). [deepvarrun_b37_MND_G33.1kei.log](https://github.com/google/deepvariant/files/14458499/deepvarrun_b37_MND_G33.1kei.log). . I have attached a log file for one of the samples, so you can see what happened.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/776
https://github.com/google/deepvariant/issues/776:413,security,log,log,413,"Thank you for the feedback! @danielecook I can confirm that the files in the tmp directory do look to be normal as you described above. @kishwarshafin, I tested the docker that you suggested and now it seems that Deepvariant did not run at all (vcfs and gvcfs are empty). [deepvarrun_b37_MND_G33.1kei.log](https://github.com/google/deepvariant/files/14458499/deepvarrun_b37_MND_G33.1kei.log). . I have attached a log file for one of the samples, so you can see what happened.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/776
https://github.com/google/deepvariant/issues/776:154,testability,test,tested,154,"Thank you for the feedback! @danielecook I can confirm that the files in the tmp directory do look to be normal as you described above. @kishwarshafin, I tested the docker that you suggested and now it seems that Deepvariant did not run at all (vcfs and gvcfs are empty). [deepvarrun_b37_MND_G33.1kei.log](https://github.com/google/deepvariant/files/14458499/deepvarrun_b37_MND_G33.1kei.log). . I have attached a log file for one of the samples, so you can see what happened.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/776
https://github.com/google/deepvariant/issues/776:301,testability,log,log,301,"Thank you for the feedback! @danielecook I can confirm that the files in the tmp directory do look to be normal as you described above. @kishwarshafin, I tested the docker that you suggested and now it seems that Deepvariant did not run at all (vcfs and gvcfs are empty). [deepvarrun_b37_MND_G33.1kei.log](https://github.com/google/deepvariant/files/14458499/deepvarrun_b37_MND_G33.1kei.log). . I have attached a log file for one of the samples, so you can see what happened.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/776
https://github.com/google/deepvariant/issues/776:387,testability,log,log,387,"Thank you for the feedback! @danielecook I can confirm that the files in the tmp directory do look to be normal as you described above. @kishwarshafin, I tested the docker that you suggested and now it seems that Deepvariant did not run at all (vcfs and gvcfs are empty). [deepvarrun_b37_MND_G33.1kei.log](https://github.com/google/deepvariant/files/14458499/deepvarrun_b37_MND_G33.1kei.log). . I have attached a log file for one of the samples, so you can see what happened.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/776
https://github.com/google/deepvariant/issues/776:413,testability,log,log,413,"Thank you for the feedback! @danielecook I can confirm that the files in the tmp directory do look to be normal as you described above. @kishwarshafin, I tested the docker that you suggested and now it seems that Deepvariant did not run at all (vcfs and gvcfs are empty). [deepvarrun_b37_MND_G33.1kei.log](https://github.com/google/deepvariant/files/14458499/deepvarrun_b37_MND_G33.1kei.log). . I have attached a log file for one of the samples, so you can see what happened.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/776
https://github.com/google/deepvariant/issues/776:18,usability,feedback,feedback,18,"Thank you for the feedback! @danielecook I can confirm that the files in the tmp directory do look to be normal as you described above. @kishwarshafin, I tested the docker that you suggested and now it seems that Deepvariant did not run at all (vcfs and gvcfs are empty). [deepvarrun_b37_MND_G33.1kei.log](https://github.com/google/deepvariant/files/14458499/deepvarrun_b37_MND_G33.1kei.log). . I have attached a log file for one of the samples, so you can see what happened.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/776
https://github.com/google/deepvariant/issues/776:47,usability,confirm,confirm,47,"Thank you for the feedback! @danielecook I can confirm that the files in the tmp directory do look to be normal as you described above. @kishwarshafin, I tested the docker that you suggested and now it seems that Deepvariant did not run at all (vcfs and gvcfs are empty). [deepvarrun_b37_MND_G33.1kei.log](https://github.com/google/deepvariant/files/14458499/deepvarrun_b37_MND_G33.1kei.log). . I have attached a log file for one of the samples, so you can see what happened.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/776
https://github.com/google/deepvariant/issues/776:20,availability,error,error,20,I have picked up an error in the original cram files for the 4 samples that did not work. Thank you for the help!,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/776
https://github.com/google/deepvariant/issues/776:20,performance,error,error,20,I have picked up an error in the original cram files for the 4 samples that did not work. Thank you for the help!,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/776
https://github.com/google/deepvariant/issues/776:20,safety,error,error,20,I have picked up an error in the original cram files for the 4 samples that did not work. Thank you for the help!,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/776
https://github.com/google/deepvariant/issues/776:20,usability,error,error,20,I have picked up an error in the original cram files for the 4 samples that did not work. Thank you for the help!,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/776
https://github.com/google/deepvariant/issues/776:108,usability,help,help,108,I have picked up an error in the original cram files for the 4 samples that did not work. Thank you for the help!,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/776
https://github.com/google/deepvariant/issues/776:235,deployability,releas,release,235,"Hi @ErinKinghorn , if I understand your latest comment, you meant that you were able to get them to work now? If so, I'll close this. (But if I misunderstood, please reopen with more questions!). @kishwarshafin will plan to do a 1.6.1 release to fix the issue above (and will officially publish a Docker). Thanks for helping us test!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/776
https://github.com/google/deepvariant/issues/776:287,integrability,pub,publish,287,"Hi @ErinKinghorn , if I understand your latest comment, you meant that you were able to get them to work now? If so, I'll close this. (But if I misunderstood, please reopen with more questions!). @kishwarshafin will plan to do a 1.6.1 release to fix the issue above (and will officially publish a Docker). Thanks for helping us test!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/776
https://github.com/google/deepvariant/issues/776:328,safety,test,test,328,"Hi @ErinKinghorn , if I understand your latest comment, you meant that you were able to get them to work now? If so, I'll close this. (But if I misunderstood, please reopen with more questions!). @kishwarshafin will plan to do a 1.6.1 release to fix the issue above (and will officially publish a Docker). Thanks for helping us test!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/776
https://github.com/google/deepvariant/issues/776:24,testability,understand,understand,24,"Hi @ErinKinghorn , if I understand your latest comment, you meant that you were able to get them to work now? If so, I'll close this. (But if I misunderstood, please reopen with more questions!). @kishwarshafin will plan to do a 1.6.1 release to fix the issue above (and will officially publish a Docker). Thanks for helping us test!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/776
https://github.com/google/deepvariant/issues/776:216,testability,plan,plan,216,"Hi @ErinKinghorn , if I understand your latest comment, you meant that you were able to get them to work now? If so, I'll close this. (But if I misunderstood, please reopen with more questions!). @kishwarshafin will plan to do a 1.6.1 release to fix the issue above (and will officially publish a Docker). Thanks for helping us test!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/776
https://github.com/google/deepvariant/issues/776:328,testability,test,test,328,"Hi @ErinKinghorn , if I understand your latest comment, you meant that you were able to get them to work now? If so, I'll close this. (But if I misunderstood, please reopen with more questions!). @kishwarshafin will plan to do a 1.6.1 release to fix the issue above (and will officially publish a Docker). Thanks for helping us test!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/776
https://github.com/google/deepvariant/issues/776:122,usability,close,close,122,"Hi @ErinKinghorn , if I understand your latest comment, you meant that you were able to get them to work now? If so, I'll close this. (But if I misunderstood, please reopen with more questions!). @kishwarshafin will plan to do a 1.6.1 release to fix the issue above (and will officially publish a Docker). Thanks for helping us test!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/776
https://github.com/google/deepvariant/issues/776:317,usability,help,helping,317,"Hi @ErinKinghorn , if I understand your latest comment, you meant that you were able to get them to work now? If so, I'll close this. (But if I misunderstood, please reopen with more questions!). @kishwarshafin will plan to do a 1.6.1 release to fix the issue above (and will officially publish a Docker). Thanks for helping us test!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/776
https://github.com/google/deepvariant/issues/777:245,availability,down,downloaded,245,@MetteBoge can you confirm that you have the following files under a model directory? ```. ├── model. │ ├── model.ckpt.data-00000-of-00001. │ ├── model.ckpt.index. │ └── model.ckpt.meta. ```. You will want to make sure the model files have been downloaded to the model directory:. ```bash. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.data-00000-of-00001 > model/model.ckpt.data-00000-of-00001. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.example_info.json > model/model.ckpt.example_info.json. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.index > model/model.ckpt.index. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.meta > model/model.ckpt.meta. ```,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/777
https://github.com/google/deepvariant/issues/777:69,energy efficiency,model,model,69,@MetteBoge can you confirm that you have the following files under a model directory? ```. ├── model. │ ├── model.ckpt.data-00000-of-00001. │ ├── model.ckpt.index. │ └── model.ckpt.meta. ```. You will want to make sure the model files have been downloaded to the model directory:. ```bash. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.data-00000-of-00001 > model/model.ckpt.data-00000-of-00001. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.example_info.json > model/model.ckpt.example_info.json. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.index > model/model.ckpt.index. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.meta > model/model.ckpt.meta. ```,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/777
https://github.com/google/deepvariant/issues/777:95,energy efficiency,model,model,95,@MetteBoge can you confirm that you have the following files under a model directory? ```. ├── model. │ ├── model.ckpt.data-00000-of-00001. │ ├── model.ckpt.index. │ └── model.ckpt.meta. ```. You will want to make sure the model files have been downloaded to the model directory:. ```bash. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.data-00000-of-00001 > model/model.ckpt.data-00000-of-00001. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.example_info.json > model/model.ckpt.example_info.json. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.index > model/model.ckpt.index. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.meta > model/model.ckpt.meta. ```,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/777
https://github.com/google/deepvariant/issues/777:108,energy efficiency,model,model,108,@MetteBoge can you confirm that you have the following files under a model directory? ```. ├── model. │ ├── model.ckpt.data-00000-of-00001. │ ├── model.ckpt.index. │ └── model.ckpt.meta. ```. You will want to make sure the model files have been downloaded to the model directory:. ```bash. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.data-00000-of-00001 > model/model.ckpt.data-00000-of-00001. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.example_info.json > model/model.ckpt.example_info.json. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.index > model/model.ckpt.index. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.meta > model/model.ckpt.meta. ```,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/777
https://github.com/google/deepvariant/issues/777:146,energy efficiency,model,model,146,@MetteBoge can you confirm that you have the following files under a model directory? ```. ├── model. │ ├── model.ckpt.data-00000-of-00001. │ ├── model.ckpt.index. │ └── model.ckpt.meta. ```. You will want to make sure the model files have been downloaded to the model directory:. ```bash. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.data-00000-of-00001 > model/model.ckpt.data-00000-of-00001. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.example_info.json > model/model.ckpt.example_info.json. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.index > model/model.ckpt.index. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.meta > model/model.ckpt.meta. ```,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/777
https://github.com/google/deepvariant/issues/777:170,energy efficiency,model,model,170,@MetteBoge can you confirm that you have the following files under a model directory? ```. ├── model. │ ├── model.ckpt.data-00000-of-00001. │ ├── model.ckpt.index. │ └── model.ckpt.meta. ```. You will want to make sure the model files have been downloaded to the model directory:. ```bash. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.data-00000-of-00001 > model/model.ckpt.data-00000-of-00001. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.example_info.json > model/model.ckpt.example_info.json. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.index > model/model.ckpt.index. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.meta > model/model.ckpt.meta. ```,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/777
https://github.com/google/deepvariant/issues/777:223,energy efficiency,model,model,223,@MetteBoge can you confirm that you have the following files under a model directory? ```. ├── model. │ ├── model.ckpt.data-00000-of-00001. │ ├── model.ckpt.index. │ └── model.ckpt.meta. ```. You will want to make sure the model files have been downloaded to the model directory:. ```bash. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.data-00000-of-00001 > model/model.ckpt.data-00000-of-00001. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.example_info.json > model/model.ckpt.example_info.json. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.index > model/model.ckpt.index. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.meta > model/model.ckpt.meta. ```,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/777
https://github.com/google/deepvariant/issues/777:263,energy efficiency,model,model,263,@MetteBoge can you confirm that you have the following files under a model directory? ```. ├── model. │ ├── model.ckpt.data-00000-of-00001. │ ├── model.ckpt.index. │ └── model.ckpt.meta. ```. You will want to make sure the model files have been downloaded to the model directory:. ```bash. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.data-00000-of-00001 > model/model.ckpt.data-00000-of-00001. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.example_info.json > model/model.ckpt.example_info.json. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.index > model/model.ckpt.index. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.meta > model/model.ckpt.meta. ```,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/777
https://github.com/google/deepvariant/issues/777:338,energy efficiency,model,models,338,@MetteBoge can you confirm that you have the following files under a model directory? ```. ├── model. │ ├── model.ckpt.data-00000-of-00001. │ ├── model.ckpt.index. │ └── model.ckpt.meta. ```. You will want to make sure the model files have been downloaded to the model directory:. ```bash. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.data-00000-of-00001 > model/model.ckpt.data-00000-of-00001. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.example_info.json > model/model.ckpt.example_info.json. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.index > model/model.ckpt.index. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.meta > model/model.ckpt.meta. ```,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/777
https://github.com/google/deepvariant/issues/777:415,energy efficiency,model,model,415,@MetteBoge can you confirm that you have the following files under a model directory? ```. ├── model. │ ├── model.ckpt.data-00000-of-00001. │ ├── model.ckpt.index. │ └── model.ckpt.meta. ```. You will want to make sure the model files have been downloaded to the model directory:. ```bash. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.data-00000-of-00001 > model/model.ckpt.data-00000-of-00001. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.example_info.json > model/model.ckpt.example_info.json. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.index > model/model.ckpt.index. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.meta > model/model.ckpt.meta. ```,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/777
https://github.com/google/deepvariant/issues/777:448,energy efficiency,model,model,448,@MetteBoge can you confirm that you have the following files under a model directory? ```. ├── model. │ ├── model.ckpt.data-00000-of-00001. │ ├── model.ckpt.index. │ └── model.ckpt.meta. ```. You will want to make sure the model files have been downloaded to the model directory:. ```bash. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.data-00000-of-00001 > model/model.ckpt.data-00000-of-00001. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.example_info.json > model/model.ckpt.example_info.json. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.index > model/model.ckpt.index. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.meta > model/model.ckpt.meta. ```,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/777
https://github.com/google/deepvariant/issues/777:454,energy efficiency,model,model,454,@MetteBoge can you confirm that you have the following files under a model directory? ```. ├── model. │ ├── model.ckpt.data-00000-of-00001. │ ├── model.ckpt.index. │ └── model.ckpt.meta. ```. You will want to make sure the model files have been downloaded to the model directory:. ```bash. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.data-00000-of-00001 > model/model.ckpt.data-00000-of-00001. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.example_info.json > model/model.ckpt.example_info.json. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.index > model/model.ckpt.index. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.meta > model/model.ckpt.meta. ```,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/777
https://github.com/google/deepvariant/issues/777:534,energy efficiency,model,models,534,@MetteBoge can you confirm that you have the following files under a model directory? ```. ├── model. │ ├── model.ckpt.data-00000-of-00001. │ ├── model.ckpt.index. │ └── model.ckpt.meta. ```. You will want to make sure the model files have been downloaded to the model directory:. ```bash. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.data-00000-of-00001 > model/model.ckpt.data-00000-of-00001. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.example_info.json > model/model.ckpt.example_info.json. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.index > model/model.ckpt.index. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.meta > model/model.ckpt.meta. ```,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/777
https://github.com/google/deepvariant/issues/777:611,energy efficiency,model,model,611,@MetteBoge can you confirm that you have the following files under a model directory? ```. ├── model. │ ├── model.ckpt.data-00000-of-00001. │ ├── model.ckpt.index. │ └── model.ckpt.meta. ```. You will want to make sure the model files have been downloaded to the model directory:. ```bash. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.data-00000-of-00001 > model/model.ckpt.data-00000-of-00001. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.example_info.json > model/model.ckpt.example_info.json. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.index > model/model.ckpt.index. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.meta > model/model.ckpt.meta. ```,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/777
https://github.com/google/deepvariant/issues/777:642,energy efficiency,model,model,642,@MetteBoge can you confirm that you have the following files under a model directory? ```. ├── model. │ ├── model.ckpt.data-00000-of-00001. │ ├── model.ckpt.index. │ └── model.ckpt.meta. ```. You will want to make sure the model files have been downloaded to the model directory:. ```bash. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.data-00000-of-00001 > model/model.ckpt.data-00000-of-00001. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.example_info.json > model/model.ckpt.example_info.json. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.index > model/model.ckpt.index. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.meta > model/model.ckpt.meta. ```,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/777
https://github.com/google/deepvariant/issues/777:648,energy efficiency,model,model,648,@MetteBoge can you confirm that you have the following files under a model directory? ```. ├── model. │ ├── model.ckpt.data-00000-of-00001. │ ├── model.ckpt.index. │ └── model.ckpt.meta. ```. You will want to make sure the model files have been downloaded to the model directory:. ```bash. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.data-00000-of-00001 > model/model.ckpt.data-00000-of-00001. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.example_info.json > model/model.ckpt.example_info.json. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.index > model/model.ckpt.index. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.meta > model/model.ckpt.meta. ```,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/777
https://github.com/google/deepvariant/issues/777:726,energy efficiency,model,models,726,@MetteBoge can you confirm that you have the following files under a model directory? ```. ├── model. │ ├── model.ckpt.data-00000-of-00001. │ ├── model.ckpt.index. │ └── model.ckpt.meta. ```. You will want to make sure the model files have been downloaded to the model directory:. ```bash. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.data-00000-of-00001 > model/model.ckpt.data-00000-of-00001. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.example_info.json > model/model.ckpt.example_info.json. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.index > model/model.ckpt.index. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.meta > model/model.ckpt.meta. ```,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/777
https://github.com/google/deepvariant/issues/777:803,energy efficiency,model,model,803,@MetteBoge can you confirm that you have the following files under a model directory? ```. ├── model. │ ├── model.ckpt.data-00000-of-00001. │ ├── model.ckpt.index. │ └── model.ckpt.meta. ```. You will want to make sure the model files have been downloaded to the model directory:. ```bash. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.data-00000-of-00001 > model/model.ckpt.data-00000-of-00001. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.example_info.json > model/model.ckpt.example_info.json. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.index > model/model.ckpt.index. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.meta > model/model.ckpt.meta. ```,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/777
https://github.com/google/deepvariant/issues/777:822,energy efficiency,model,model,822,@MetteBoge can you confirm that you have the following files under a model directory? ```. ├── model. │ ├── model.ckpt.data-00000-of-00001. │ ├── model.ckpt.index. │ └── model.ckpt.meta. ```. You will want to make sure the model files have been downloaded to the model directory:. ```bash. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.data-00000-of-00001 > model/model.ckpt.data-00000-of-00001. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.example_info.json > model/model.ckpt.example_info.json. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.index > model/model.ckpt.index. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.meta > model/model.ckpt.meta. ```,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/777
https://github.com/google/deepvariant/issues/777:828,energy efficiency,model,model,828,@MetteBoge can you confirm that you have the following files under a model directory? ```. ├── model. │ ├── model.ckpt.data-00000-of-00001. │ ├── model.ckpt.index. │ └── model.ckpt.meta. ```. You will want to make sure the model files have been downloaded to the model directory:. ```bash. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.data-00000-of-00001 > model/model.ckpt.data-00000-of-00001. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.example_info.json > model/model.ckpt.example_info.json. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.index > model/model.ckpt.index. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.meta > model/model.ckpt.meta. ```,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/777
https://github.com/google/deepvariant/issues/777:894,energy efficiency,model,models,894,@MetteBoge can you confirm that you have the following files under a model directory? ```. ├── model. │ ├── model.ckpt.data-00000-of-00001. │ ├── model.ckpt.index. │ └── model.ckpt.meta. ```. You will want to make sure the model files have been downloaded to the model directory:. ```bash. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.data-00000-of-00001 > model/model.ckpt.data-00000-of-00001. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.example_info.json > model/model.ckpt.example_info.json. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.index > model/model.ckpt.index. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.meta > model/model.ckpt.meta. ```,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/777
https://github.com/google/deepvariant/issues/777:971,energy efficiency,model,model,971,@MetteBoge can you confirm that you have the following files under a model directory? ```. ├── model. │ ├── model.ckpt.data-00000-of-00001. │ ├── model.ckpt.index. │ └── model.ckpt.meta. ```. You will want to make sure the model files have been downloaded to the model directory:. ```bash. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.data-00000-of-00001 > model/model.ckpt.data-00000-of-00001. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.example_info.json > model/model.ckpt.example_info.json. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.index > model/model.ckpt.index. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.meta > model/model.ckpt.meta. ```,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/777
https://github.com/google/deepvariant/issues/777:989,energy efficiency,model,model,989,@MetteBoge can you confirm that you have the following files under a model directory? ```. ├── model. │ ├── model.ckpt.data-00000-of-00001. │ ├── model.ckpt.index. │ └── model.ckpt.meta. ```. You will want to make sure the model files have been downloaded to the model directory:. ```bash. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.data-00000-of-00001 > model/model.ckpt.data-00000-of-00001. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.example_info.json > model/model.ckpt.example_info.json. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.index > model/model.ckpt.index. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.meta > model/model.ckpt.meta. ```,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/777
https://github.com/google/deepvariant/issues/777:995,energy efficiency,model,model,995,@MetteBoge can you confirm that you have the following files under a model directory? ```. ├── model. │ ├── model.ckpt.data-00000-of-00001. │ ├── model.ckpt.index. │ └── model.ckpt.meta. ```. You will want to make sure the model files have been downloaded to the model directory:. ```bash. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.data-00000-of-00001 > model/model.ckpt.data-00000-of-00001. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.example_info.json > model/model.ckpt.example_info.json. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.index > model/model.ckpt.index. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.meta > model/model.ckpt.meta. ```,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/777
https://github.com/google/deepvariant/issues/777:69,security,model,model,69,@MetteBoge can you confirm that you have the following files under a model directory? ```. ├── model. │ ├── model.ckpt.data-00000-of-00001. │ ├── model.ckpt.index. │ └── model.ckpt.meta. ```. You will want to make sure the model files have been downloaded to the model directory:. ```bash. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.data-00000-of-00001 > model/model.ckpt.data-00000-of-00001. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.example_info.json > model/model.ckpt.example_info.json. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.index > model/model.ckpt.index. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.meta > model/model.ckpt.meta. ```,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/777
https://github.com/google/deepvariant/issues/777:95,security,model,model,95,@MetteBoge can you confirm that you have the following files under a model directory? ```. ├── model. │ ├── model.ckpt.data-00000-of-00001. │ ├── model.ckpt.index. │ └── model.ckpt.meta. ```. You will want to make sure the model files have been downloaded to the model directory:. ```bash. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.data-00000-of-00001 > model/model.ckpt.data-00000-of-00001. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.example_info.json > model/model.ckpt.example_info.json. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.index > model/model.ckpt.index. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.meta > model/model.ckpt.meta. ```,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/777
https://github.com/google/deepvariant/issues/777:108,security,model,model,108,@MetteBoge can you confirm that you have the following files under a model directory? ```. ├── model. │ ├── model.ckpt.data-00000-of-00001. │ ├── model.ckpt.index. │ └── model.ckpt.meta. ```. You will want to make sure the model files have been downloaded to the model directory:. ```bash. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.data-00000-of-00001 > model/model.ckpt.data-00000-of-00001. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.example_info.json > model/model.ckpt.example_info.json. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.index > model/model.ckpt.index. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.meta > model/model.ckpt.meta. ```,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/777
https://github.com/google/deepvariant/issues/777:146,security,model,model,146,@MetteBoge can you confirm that you have the following files under a model directory? ```. ├── model. │ ├── model.ckpt.data-00000-of-00001. │ ├── model.ckpt.index. │ └── model.ckpt.meta. ```. You will want to make sure the model files have been downloaded to the model directory:. ```bash. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.data-00000-of-00001 > model/model.ckpt.data-00000-of-00001. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.example_info.json > model/model.ckpt.example_info.json. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.index > model/model.ckpt.index. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.meta > model/model.ckpt.meta. ```,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/777
https://github.com/google/deepvariant/issues/777:170,security,model,model,170,@MetteBoge can you confirm that you have the following files under a model directory? ```. ├── model. │ ├── model.ckpt.data-00000-of-00001. │ ├── model.ckpt.index. │ └── model.ckpt.meta. ```. You will want to make sure the model files have been downloaded to the model directory:. ```bash. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.data-00000-of-00001 > model/model.ckpt.data-00000-of-00001. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.example_info.json > model/model.ckpt.example_info.json. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.index > model/model.ckpt.index. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.meta > model/model.ckpt.meta. ```,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/777
https://github.com/google/deepvariant/issues/777:223,security,model,model,223,@MetteBoge can you confirm that you have the following files under a model directory? ```. ├── model. │ ├── model.ckpt.data-00000-of-00001. │ ├── model.ckpt.index. │ └── model.ckpt.meta. ```. You will want to make sure the model files have been downloaded to the model directory:. ```bash. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.data-00000-of-00001 > model/model.ckpt.data-00000-of-00001. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.example_info.json > model/model.ckpt.example_info.json. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.index > model/model.ckpt.index. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.meta > model/model.ckpt.meta. ```,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/777
https://github.com/google/deepvariant/issues/777:263,security,model,model,263,@MetteBoge can you confirm that you have the following files under a model directory? ```. ├── model. │ ├── model.ckpt.data-00000-of-00001. │ ├── model.ckpt.index. │ └── model.ckpt.meta. ```. You will want to make sure the model files have been downloaded to the model directory:. ```bash. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.data-00000-of-00001 > model/model.ckpt.data-00000-of-00001. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.example_info.json > model/model.ckpt.example_info.json. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.index > model/model.ckpt.index. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.meta > model/model.ckpt.meta. ```,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/777
https://github.com/google/deepvariant/issues/777:338,security,model,models,338,@MetteBoge can you confirm that you have the following files under a model directory? ```. ├── model. │ ├── model.ckpt.data-00000-of-00001. │ ├── model.ckpt.index. │ └── model.ckpt.meta. ```. You will want to make sure the model files have been downloaded to the model directory:. ```bash. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.data-00000-of-00001 > model/model.ckpt.data-00000-of-00001. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.example_info.json > model/model.ckpt.example_info.json. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.index > model/model.ckpt.index. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.meta > model/model.ckpt.meta. ```,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/777
https://github.com/google/deepvariant/issues/777:415,security,model,model,415,@MetteBoge can you confirm that you have the following files under a model directory? ```. ├── model. │ ├── model.ckpt.data-00000-of-00001. │ ├── model.ckpt.index. │ └── model.ckpt.meta. ```. You will want to make sure the model files have been downloaded to the model directory:. ```bash. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.data-00000-of-00001 > model/model.ckpt.data-00000-of-00001. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.example_info.json > model/model.ckpt.example_info.json. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.index > model/model.ckpt.index. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.meta > model/model.ckpt.meta. ```,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/777
https://github.com/google/deepvariant/issues/777:448,security,model,model,448,@MetteBoge can you confirm that you have the following files under a model directory? ```. ├── model. │ ├── model.ckpt.data-00000-of-00001. │ ├── model.ckpt.index. │ └── model.ckpt.meta. ```. You will want to make sure the model files have been downloaded to the model directory:. ```bash. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.data-00000-of-00001 > model/model.ckpt.data-00000-of-00001. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.example_info.json > model/model.ckpt.example_info.json. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.index > model/model.ckpt.index. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.meta > model/model.ckpt.meta. ```,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/777
https://github.com/google/deepvariant/issues/777:454,security,model,model,454,@MetteBoge can you confirm that you have the following files under a model directory? ```. ├── model. │ ├── model.ckpt.data-00000-of-00001. │ ├── model.ckpt.index. │ └── model.ckpt.meta. ```. You will want to make sure the model files have been downloaded to the model directory:. ```bash. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.data-00000-of-00001 > model/model.ckpt.data-00000-of-00001. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.example_info.json > model/model.ckpt.example_info.json. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.index > model/model.ckpt.index. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.meta > model/model.ckpt.meta. ```,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/777
https://github.com/google/deepvariant/issues/777:534,security,model,models,534,@MetteBoge can you confirm that you have the following files under a model directory? ```. ├── model. │ ├── model.ckpt.data-00000-of-00001. │ ├── model.ckpt.index. │ └── model.ckpt.meta. ```. You will want to make sure the model files have been downloaded to the model directory:. ```bash. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.data-00000-of-00001 > model/model.ckpt.data-00000-of-00001. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.example_info.json > model/model.ckpt.example_info.json. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.index > model/model.ckpt.index. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.meta > model/model.ckpt.meta. ```,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/777
https://github.com/google/deepvariant/issues/777:611,security,model,model,611,@MetteBoge can you confirm that you have the following files under a model directory? ```. ├── model. │ ├── model.ckpt.data-00000-of-00001. │ ├── model.ckpt.index. │ └── model.ckpt.meta. ```. You will want to make sure the model files have been downloaded to the model directory:. ```bash. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.data-00000-of-00001 > model/model.ckpt.data-00000-of-00001. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.example_info.json > model/model.ckpt.example_info.json. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.index > model/model.ckpt.index. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.meta > model/model.ckpt.meta. ```,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/777
https://github.com/google/deepvariant/issues/777:642,security,model,model,642,@MetteBoge can you confirm that you have the following files under a model directory? ```. ├── model. │ ├── model.ckpt.data-00000-of-00001. │ ├── model.ckpt.index. │ └── model.ckpt.meta. ```. You will want to make sure the model files have been downloaded to the model directory:. ```bash. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.data-00000-of-00001 > model/model.ckpt.data-00000-of-00001. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.example_info.json > model/model.ckpt.example_info.json. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.index > model/model.ckpt.index. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.meta > model/model.ckpt.meta. ```,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/777
https://github.com/google/deepvariant/issues/777:648,security,model,model,648,@MetteBoge can you confirm that you have the following files under a model directory? ```. ├── model. │ ├── model.ckpt.data-00000-of-00001. │ ├── model.ckpt.index. │ └── model.ckpt.meta. ```. You will want to make sure the model files have been downloaded to the model directory:. ```bash. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.data-00000-of-00001 > model/model.ckpt.data-00000-of-00001. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.example_info.json > model/model.ckpt.example_info.json. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.index > model/model.ckpt.index. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.meta > model/model.ckpt.meta. ```,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/777
https://github.com/google/deepvariant/issues/777:726,security,model,models,726,@MetteBoge can you confirm that you have the following files under a model directory? ```. ├── model. │ ├── model.ckpt.data-00000-of-00001. │ ├── model.ckpt.index. │ └── model.ckpt.meta. ```. You will want to make sure the model files have been downloaded to the model directory:. ```bash. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.data-00000-of-00001 > model/model.ckpt.data-00000-of-00001. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.example_info.json > model/model.ckpt.example_info.json. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.index > model/model.ckpt.index. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.meta > model/model.ckpt.meta. ```,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/777
https://github.com/google/deepvariant/issues/777:803,security,model,model,803,@MetteBoge can you confirm that you have the following files under a model directory? ```. ├── model. │ ├── model.ckpt.data-00000-of-00001. │ ├── model.ckpt.index. │ └── model.ckpt.meta. ```. You will want to make sure the model files have been downloaded to the model directory:. ```bash. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.data-00000-of-00001 > model/model.ckpt.data-00000-of-00001. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.example_info.json > model/model.ckpt.example_info.json. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.index > model/model.ckpt.index. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.meta > model/model.ckpt.meta. ```,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/777
https://github.com/google/deepvariant/issues/777:822,security,model,model,822,@MetteBoge can you confirm that you have the following files under a model directory? ```. ├── model. │ ├── model.ckpt.data-00000-of-00001. │ ├── model.ckpt.index. │ └── model.ckpt.meta. ```. You will want to make sure the model files have been downloaded to the model directory:. ```bash. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.data-00000-of-00001 > model/model.ckpt.data-00000-of-00001. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.example_info.json > model/model.ckpt.example_info.json. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.index > model/model.ckpt.index. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.meta > model/model.ckpt.meta. ```,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/777
https://github.com/google/deepvariant/issues/777:828,security,model,model,828,@MetteBoge can you confirm that you have the following files under a model directory? ```. ├── model. │ ├── model.ckpt.data-00000-of-00001. │ ├── model.ckpt.index. │ └── model.ckpt.meta. ```. You will want to make sure the model files have been downloaded to the model directory:. ```bash. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.data-00000-of-00001 > model/model.ckpt.data-00000-of-00001. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.example_info.json > model/model.ckpt.example_info.json. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.index > model/model.ckpt.index. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.meta > model/model.ckpt.meta. ```,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/777
https://github.com/google/deepvariant/issues/777:894,security,model,models,894,@MetteBoge can you confirm that you have the following files under a model directory? ```. ├── model. │ ├── model.ckpt.data-00000-of-00001. │ ├── model.ckpt.index. │ └── model.ckpt.meta. ```. You will want to make sure the model files have been downloaded to the model directory:. ```bash. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.data-00000-of-00001 > model/model.ckpt.data-00000-of-00001. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.example_info.json > model/model.ckpt.example_info.json. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.index > model/model.ckpt.index. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.meta > model/model.ckpt.meta. ```,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/777
https://github.com/google/deepvariant/issues/777:971,security,model,model,971,@MetteBoge can you confirm that you have the following files under a model directory? ```. ├── model. │ ├── model.ckpt.data-00000-of-00001. │ ├── model.ckpt.index. │ └── model.ckpt.meta. ```. You will want to make sure the model files have been downloaded to the model directory:. ```bash. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.data-00000-of-00001 > model/model.ckpt.data-00000-of-00001. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.example_info.json > model/model.ckpt.example_info.json. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.index > model/model.ckpt.index. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.meta > model/model.ckpt.meta. ```,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/777
https://github.com/google/deepvariant/issues/777:989,security,model,model,989,@MetteBoge can you confirm that you have the following files under a model directory? ```. ├── model. │ ├── model.ckpt.data-00000-of-00001. │ ├── model.ckpt.index. │ └── model.ckpt.meta. ```. You will want to make sure the model files have been downloaded to the model directory:. ```bash. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.data-00000-of-00001 > model/model.ckpt.data-00000-of-00001. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.example_info.json > model/model.ckpt.example_info.json. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.index > model/model.ckpt.index. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.meta > model/model.ckpt.meta. ```,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/777
https://github.com/google/deepvariant/issues/777:995,security,model,model,995,@MetteBoge can you confirm that you have the following files under a model directory? ```. ├── model. │ ├── model.ckpt.data-00000-of-00001. │ ├── model.ckpt.index. │ └── model.ckpt.meta. ```. You will want to make sure the model files have been downloaded to the model directory:. ```bash. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.data-00000-of-00001 > model/model.ckpt.data-00000-of-00001. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.example_info.json > model/model.ckpt.example_info.json. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.index > model/model.ckpt.index. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.meta > model/model.ckpt.meta. ```,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/777
https://github.com/google/deepvariant/issues/777:19,usability,confirm,confirm,19,@MetteBoge can you confirm that you have the following files under a model directory? ```. ├── model. │ ├── model.ckpt.data-00000-of-00001. │ ├── model.ckpt.index. │ └── model.ckpt.meta. ```. You will want to make sure the model files have been downloaded to the model directory:. ```bash. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.data-00000-of-00001 > model/model.ckpt.data-00000-of-00001. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.example_info.json > model/model.ckpt.example_info.json. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.index > model/model.ckpt.index. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.meta > model/model.ckpt.meta. ```,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/777
https://github.com/google/deepvariant/issues/777:39,energy efficiency,model,models,39,"Ah of course, thanks. They are in /opt/models/wes/ instead. It seems to be working now.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/777
https://github.com/google/deepvariant/issues/777:39,security,model,models,39,"Ah of course, thanks. They are in /opt/models/wes/ instead. It seems to be working now.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/777
https://github.com/google/deepvariant/issues/777:33,energy efficiency,model,model,33,@MetteBoge those are whole-exome model files. You'll want to make sure you are pointing to the RNA-seq model.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/777
https://github.com/google/deepvariant/issues/777:103,energy efficiency,model,model,103,@MetteBoge those are whole-exome model files. You'll want to make sure you are pointing to the RNA-seq model.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/777
https://github.com/google/deepvariant/issues/777:33,security,model,model,33,@MetteBoge those are whole-exome model files. You'll want to make sure you are pointing to the RNA-seq model.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/777
https://github.com/google/deepvariant/issues/777:103,security,model,model,103,@MetteBoge those are whole-exome model files. You'll want to make sure you are pointing to the RNA-seq model.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/777
https://github.com/google/deepvariant/issues/777:15,availability,down,downloaded,15,"Makes sense. I downloaded the files now, and it works with the RNAseq model. Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/777
https://github.com/google/deepvariant/issues/777:70,energy efficiency,model,model,70,"Makes sense. I downloaded the files now, and it works with the RNAseq model. Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/777
https://github.com/google/deepvariant/issues/777:70,security,model,model,70,"Makes sense. I downloaded the files now, and it works with the RNAseq model. Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/777
https://github.com/google/deepvariant/issues/778:273,deployability,observ,observe,273,"Hi @WeiYang-BAI . I would recommend using GLnexus to merge gVCFs of GATK and DeepVariant. GLnexus has been optimized for both GATK and DeepVariant outputs. There are different presets for GLnexus, to combine multiple methods I would recommeng using unfiltered settings. We observe that the GATK joint genotyper doesn't seem to handle DeepVariant gVCFs well, and the accuracy is much lower after using GATK on those.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/778
https://github.com/google/deepvariant/issues/778:107,energy efficiency,optim,optimized,107,"Hi @WeiYang-BAI . I would recommend using GLnexus to merge gVCFs of GATK and DeepVariant. GLnexus has been optimized for both GATK and DeepVariant outputs. There are different presets for GLnexus, to combine multiple methods I would recommeng using unfiltered settings. We observe that the GATK joint genotyper doesn't seem to handle DeepVariant gVCFs well, and the accuracy is much lower after using GATK on those.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/778
https://github.com/google/deepvariant/issues/778:107,performance,optimiz,optimized,107,"Hi @WeiYang-BAI . I would recommend using GLnexus to merge gVCFs of GATK and DeepVariant. GLnexus has been optimized for both GATK and DeepVariant outputs. There are different presets for GLnexus, to combine multiple methods I would recommeng using unfiltered settings. We observe that the GATK joint genotyper doesn't seem to handle DeepVariant gVCFs well, and the accuracy is much lower after using GATK on those.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/778
https://github.com/google/deepvariant/issues/778:311,reliability,doe,doesn,311,"Hi @WeiYang-BAI . I would recommend using GLnexus to merge gVCFs of GATK and DeepVariant. GLnexus has been optimized for both GATK and DeepVariant outputs. There are different presets for GLnexus, to combine multiple methods I would recommeng using unfiltered settings. We observe that the GATK joint genotyper doesn't seem to handle DeepVariant gVCFs well, and the accuracy is much lower after using GATK on those.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/778
https://github.com/google/deepvariant/issues/778:273,testability,observ,observe,273,"Hi @WeiYang-BAI . I would recommend using GLnexus to merge gVCFs of GATK and DeepVariant. GLnexus has been optimized for both GATK and DeepVariant outputs. There are different presets for GLnexus, to combine multiple methods I would recommeng using unfiltered settings. We observe that the GATK joint genotyper doesn't seem to handle DeepVariant gVCFs well, and the accuracy is much lower after using GATK on those.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/778
https://github.com/google/deepvariant/issues/778:452,deployability,log,log,452,"Hi @AndrewCarroll ,. I followed the instructions to merge gvcf file into a final vcf via GLnexus with the default parameters like this:. `singularity exec glnexus.sif glnexus_cli --config DeepVariantWGS $gvcf_path/*.gvcf.gz > ${output_bcf}`. But it only output 62409 SNPs in the final vcf file (pepper.merged.glnexus.vcf.gz 6.8M), there are 5 input gvcf files (each of one is about 11GB, the sample is from the whole genome of pig). . the below is the log from GLnexus. > INFO: Convert SIF file to sandbox... > WARNING: underlay of /etc/localtime required more than 50 (77) bind mounts. > [71420] [2024-04-03 09:10:42.182] [GLnexus] [info] glnexus_cli release v1.4.1-0-g68e25e5 Aug 13 2021. > [71420] [2024-04-03 09:10:42.182] [GLnexus] [info] detected jemalloc 5.2.1-0-gea6b3e973b477b8061e0076bb257dbd7f3faa756. > [71420] [2024-04-03 09:10:42.183] [GLnexus] [info] Loading config preset DeepVariantWGS. > [71420] [2024-04-03 09:10:42.190] [GLnexus] [info] config:. > unifier_config:. > drop_filtered: false. > min_allele_copy_number: 1. > min_AQ1: 10. > min_AQ2: 10. > min_GQ: 0. > max_alleles_per_site: 32. > monoallelic_sites_for_lost_alleles: true. > preference: common. > genotyper_config:. > revise_genotypes: true. > min_assumed_allele_frequency: 9.99999975e-05. > snv_prior_calibration: 0.600000024. > indel_prior_calibration: 0.449999988. > required_dp: 0. > allow_partial_data: true. > allele_dp_format: AD. > ref_dp_format: MIN_DP. > output_residuals: false. > more_PL: true. > squeeze: false. > trim_uncalled_alleles: true. > top_two_half_calls: false. > output_format: BCF. > liftover_fields:. > - {orig_names: [MIN_DP, DP], name: DP, description: ""##FORMAT=<ID=DP,Number=1,Type=Integer,Description=\""Approximate read depth (reads with MQ=255 or with bad mates are filtered)\"">"", type: int, number: basic, default_type: missing, count: 1, combi_method: min, ignore_non_variants: true}. > - {orig_names: [AD], name: AD, description: ""##FORMAT=<ID=AD,Number=R,Type=Integer,Description=\""Al",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/778
https://github.com/google/deepvariant/issues/778:652,deployability,releas,release,652,"Hi @AndrewCarroll ,. I followed the instructions to merge gvcf file into a final vcf via GLnexus with the default parameters like this:. `singularity exec glnexus.sif glnexus_cli --config DeepVariantWGS $gvcf_path/*.gvcf.gz > ${output_bcf}`. But it only output 62409 SNPs in the final vcf file (pepper.merged.glnexus.vcf.gz 6.8M), there are 5 input gvcf files (each of one is about 11GB, the sample is from the whole genome of pig). . the below is the log from GLnexus. > INFO: Convert SIF file to sandbox... > WARNING: underlay of /etc/localtime required more than 50 (77) bind mounts. > [71420] [2024-04-03 09:10:42.182] [GLnexus] [info] glnexus_cli release v1.4.1-0-g68e25e5 Aug 13 2021. > [71420] [2024-04-03 09:10:42.182] [GLnexus] [info] detected jemalloc 5.2.1-0-gea6b3e973b477b8061e0076bb257dbd7f3faa756. > [71420] [2024-04-03 09:10:42.183] [GLnexus] [info] Loading config preset DeepVariantWGS. > [71420] [2024-04-03 09:10:42.190] [GLnexus] [info] config:. > unifier_config:. > drop_filtered: false. > min_allele_copy_number: 1. > min_AQ1: 10. > min_AQ2: 10. > min_GQ: 0. > max_alleles_per_site: 32. > monoallelic_sites_for_lost_alleles: true. > preference: common. > genotyper_config:. > revise_genotypes: true. > min_assumed_allele_frequency: 9.99999975e-05. > snv_prior_calibration: 0.600000024. > indel_prior_calibration: 0.449999988. > required_dp: 0. > allow_partial_data: true. > allele_dp_format: AD. > ref_dp_format: MIN_DP. > output_residuals: false. > more_PL: true. > squeeze: false. > trim_uncalled_alleles: true. > top_two_half_calls: false. > output_format: BCF. > liftover_fields:. > - {orig_names: [MIN_DP, DP], name: DP, description: ""##FORMAT=<ID=DP,Number=1,Type=Integer,Description=\""Approximate read depth (reads with MQ=255 or with bad mates are filtered)\"">"", type: int, number: basic, default_type: missing, count: 1, combi_method: min, ignore_non_variants: true}. > - {orig_names: [AD], name: AD, description: ""##FORMAT=<ID=AD,Number=R,Type=Integer,Description=\""Al",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/778
https://github.com/google/deepvariant/issues/778:2504,deployability,scale,scaled,2504,"trim_uncalled_alleles: true. > top_two_half_calls: false. > output_format: BCF. > liftover_fields:. > - {orig_names: [MIN_DP, DP], name: DP, description: ""##FORMAT=<ID=DP,Number=1,Type=Integer,Description=\""Approximate read depth (reads with MQ=255 or with bad mates are filtered)\"">"", type: int, number: basic, default_type: missing, count: 1, combi_method: min, ignore_non_variants: true}. > - {orig_names: [AD], name: AD, description: ""##FORMAT=<ID=AD,Number=R,Type=Integer,Description=\""Allelic depths for the ref and alt alleles in the order listed\"">"", type: int, number: alleles, default_type: zero, count: 0, combi_method: min, ignore_non_variants: false}. > - {orig_names: [GQ], name: GQ, description: ""##FORMAT=<ID=GQ,Number=1,Type=Integer,Description=\""Genotype Quality\"">"", type: int, number: basic, default_type: missing, count: 1, combi_method: min, ignore_non_variants: true}. > - {orig_names: [PL], name: PL, description: ""##FORMAT=<ID=PL,Number=G,Type=Integer,Description=\""Phred-scaled genotype Likelihoods\"">"", type: int, number: genotype, default_type: missing, count: 0, combi_method: missing, ignore_non_variants: true}. > [71420] [2024-04-03 09:10:42.191] [GLnexus] [info] config CRC32C = 2932316105. > [71420] [2024-04-03 09:10:42.191] [GLnexus] [info] init database, exemplar_vcf=/public/home/zenglingsen/01.data/01.ONT_data/01.ONT_20X_fastq_SNP_calling/03.pepper/01.gvcf/gvcf_file/AW.new.excluded.mnps.gvcf.gz. > [71420] [2024-04-03 09:10:42.611] [GLnexus] [info] Initialized GLnexus database in GLnexus.DB. > [71420] [2024-04-03 09:10:42.611] [GLnexus] [info] bucket size: 30000. > [71420] [2024-04-03 09:10:42.612] [GLnexus] [info] contigs: NC_010443.5 NC_010444.4 NC_010445.4 NC_010446.5 NC_010447.5 NC_010448.4 NC_010449.5 NC_010450.4 NC_010451.4 NC_010452.4 NC_010453.5 NC_010454.4 NC_010455.5 NC_010456.5 NC_010457.5 NC_010458.4 NC_010459.5 NC_010460.4 NC_010461.5 NC_010462.3 NW_018084777.1 NW_018084778.1 NW_018084779.1 NW_018084780.1 NW_018084781.1 NW_018084782.1 N",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/778
https://github.com/google/deepvariant/issues/778:866,energy efficiency,Load,Loading,866,"Hi @AndrewCarroll ,. I followed the instructions to merge gvcf file into a final vcf via GLnexus with the default parameters like this:. `singularity exec glnexus.sif glnexus_cli --config DeepVariantWGS $gvcf_path/*.gvcf.gz > ${output_bcf}`. But it only output 62409 SNPs in the final vcf file (pepper.merged.glnexus.vcf.gz 6.8M), there are 5 input gvcf files (each of one is about 11GB, the sample is from the whole genome of pig). . the below is the log from GLnexus. > INFO: Convert SIF file to sandbox... > WARNING: underlay of /etc/localtime required more than 50 (77) bind mounts. > [71420] [2024-04-03 09:10:42.182] [GLnexus] [info] glnexus_cli release v1.4.1-0-g68e25e5 Aug 13 2021. > [71420] [2024-04-03 09:10:42.182] [GLnexus] [info] detected jemalloc 5.2.1-0-gea6b3e973b477b8061e0076bb257dbd7f3faa756. > [71420] [2024-04-03 09:10:42.183] [GLnexus] [info] Loading config preset DeepVariantWGS. > [71420] [2024-04-03 09:10:42.190] [GLnexus] [info] config:. > unifier_config:. > drop_filtered: false. > min_allele_copy_number: 1. > min_AQ1: 10. > min_AQ2: 10. > min_GQ: 0. > max_alleles_per_site: 32. > monoallelic_sites_for_lost_alleles: true. > preference: common. > genotyper_config:. > revise_genotypes: true. > min_assumed_allele_frequency: 9.99999975e-05. > snv_prior_calibration: 0.600000024. > indel_prior_calibration: 0.449999988. > required_dp: 0. > allow_partial_data: true. > allele_dp_format: AD. > ref_dp_format: MIN_DP. > output_residuals: false. > more_PL: true. > squeeze: false. > trim_uncalled_alleles: true. > top_two_half_calls: false. > output_format: BCF. > liftover_fields:. > - {orig_names: [MIN_DP, DP], name: DP, description: ""##FORMAT=<ID=DP,Number=1,Type=Integer,Description=\""Approximate read depth (reads with MQ=255 or with bad mates are filtered)\"">"", type: int, number: basic, default_type: missing, count: 1, combi_method: min, ignore_non_variants: true}. > - {orig_names: [AD], name: AD, description: ""##FORMAT=<ID=AD,Number=R,Type=Integer,Description=\""Al",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/778
https://github.com/google/deepvariant/issues/778:2504,energy efficiency,scale,scaled,2504,"trim_uncalled_alleles: true. > top_two_half_calls: false. > output_format: BCF. > liftover_fields:. > - {orig_names: [MIN_DP, DP], name: DP, description: ""##FORMAT=<ID=DP,Number=1,Type=Integer,Description=\""Approximate read depth (reads with MQ=255 or with bad mates are filtered)\"">"", type: int, number: basic, default_type: missing, count: 1, combi_method: min, ignore_non_variants: true}. > - {orig_names: [AD], name: AD, description: ""##FORMAT=<ID=AD,Number=R,Type=Integer,Description=\""Allelic depths for the ref and alt alleles in the order listed\"">"", type: int, number: alleles, default_type: zero, count: 0, combi_method: min, ignore_non_variants: false}. > - {orig_names: [GQ], name: GQ, description: ""##FORMAT=<ID=GQ,Number=1,Type=Integer,Description=\""Genotype Quality\"">"", type: int, number: basic, default_type: missing, count: 1, combi_method: min, ignore_non_variants: true}. > - {orig_names: [PL], name: PL, description: ""##FORMAT=<ID=PL,Number=G,Type=Integer,Description=\""Phred-scaled genotype Likelihoods\"">"", type: int, number: genotype, default_type: missing, count: 0, combi_method: missing, ignore_non_variants: true}. > [71420] [2024-04-03 09:10:42.191] [GLnexus] [info] config CRC32C = 2932316105. > [71420] [2024-04-03 09:10:42.191] [GLnexus] [info] init database, exemplar_vcf=/public/home/zenglingsen/01.data/01.ONT_data/01.ONT_20X_fastq_SNP_calling/03.pepper/01.gvcf/gvcf_file/AW.new.excluded.mnps.gvcf.gz. > [71420] [2024-04-03 09:10:42.611] [GLnexus] [info] Initialized GLnexus database in GLnexus.DB. > [71420] [2024-04-03 09:10:42.611] [GLnexus] [info] bucket size: 30000. > [71420] [2024-04-03 09:10:42.612] [GLnexus] [info] contigs: NC_010443.5 NC_010444.4 NC_010445.4 NC_010446.5 NC_010447.5 NC_010448.4 NC_010449.5 NC_010450.4 NC_010451.4 NC_010452.4 NC_010453.5 NC_010454.4 NC_010455.5 NC_010456.5 NC_010457.5 NC_010458.4 NC_010459.5 NC_010460.4 NC_010461.5 NC_010462.3 NW_018084777.1 NW_018084778.1 NW_018084779.1 NW_018084780.1 NW_018084781.1 NW_018084782.1 N",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/778
https://github.com/google/deepvariant/issues/778:12457,energy efficiency,load,load,12457,"018085313.1 NW_018085314.1 NW_018085315.1 NW_018085316.1 NW_018085317.1 NW_018085318.1 NW_018085319.1 NW_018085320.1 NW_018085321.1 NW_018085322.1 NW_018085323.1 NW_018085324.1 NW_018085325.1 NW_018085326.1 NW_018085327.1 NW_018085328.1 NW_018085329.1 NW_018085330.1 NW_018085331.1 NW_018085332.1 NW_018085333.1 NW_018085334.1 NW_018085335.1 NW_018085336.1 NW_018085337.1 NW_018085338.1 NW_018085339.1 NW_018085340.1 NW_018085341.1 NW_018085342.1 NW_018085343.1 NW_018085344.1 NW_018085345.1 NW_018085346.1 NW_018085347.1 NW_018085348.1 NW_018085349.1 NW_018085350.1 NW_018085351.1 NW_018085352.1 NW_018085353.1 NW_018085354.1 NW_018085355.1 NW_018085356.1 NW_018085357.1 NW_018085358.1 NW_018085359.1 NW_018085360.1 NW_018085361.1 NW_018085362.1 NW_018085363.1 NW_018085364.1 NW_018085365.1 NW_018085366.1 NW_018085367.1 NW_018085368.1 NC_000845.1. > [71420] [2024-04-03 09:10:42.642] [GLnexus] [info] db_get_contigs GLnexus.DB. > [71420] [2024-04-03 09:10:42.789] [GLnexus] [info] Beginning bulk load with no range filter. > [71420] [2024-04-03 10:09:37.111] [GLnexus] [info] Loaded 5 datasets with 5 samples; 846365851832 bytes in 8659464665 BCF records (882 duplicate) in 414215 buckets. Bucket max 2856376 bytes, 28997 records. 0 BCF records skipped due to caller-specific exceptions. > [71420] [2024-04-03 10:09:37.141] [GLnexus] [info] Created sample set *@5. > [71420] [2024-04-03 10:09:37.142] [GLnexus] [info] Flushing database... > [71420] [2024-04-03 10:11:17.432] [GLnexus] [info] Bulk load complete! > [71420] [2024-04-03 10:11:17.482] [GLnexus] [warning] Processing full length of 613 contigs, as no --bed was provided. Providing a BED file with regions of interest, if applicable, can speed this up. > [71420] [2024-04-03 10:11:17.509] [GLnexus] [info] found sample set *@5. > [71420] [2024-04-03 10:11:17.509] [GLnexus] [info] discovering alleles in 613 range(s) on 126 threads. > [71420] [2024-04-03 10:17:12.989] [GLnexus] [info] discovered 3689057 alleles. > [71420] [2024-04-03 1",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/778
https://github.com/google/deepvariant/issues/778:12537,energy efficiency,Load,Loaded,12537,"318.1 NW_018085319.1 NW_018085320.1 NW_018085321.1 NW_018085322.1 NW_018085323.1 NW_018085324.1 NW_018085325.1 NW_018085326.1 NW_018085327.1 NW_018085328.1 NW_018085329.1 NW_018085330.1 NW_018085331.1 NW_018085332.1 NW_018085333.1 NW_018085334.1 NW_018085335.1 NW_018085336.1 NW_018085337.1 NW_018085338.1 NW_018085339.1 NW_018085340.1 NW_018085341.1 NW_018085342.1 NW_018085343.1 NW_018085344.1 NW_018085345.1 NW_018085346.1 NW_018085347.1 NW_018085348.1 NW_018085349.1 NW_018085350.1 NW_018085351.1 NW_018085352.1 NW_018085353.1 NW_018085354.1 NW_018085355.1 NW_018085356.1 NW_018085357.1 NW_018085358.1 NW_018085359.1 NW_018085360.1 NW_018085361.1 NW_018085362.1 NW_018085363.1 NW_018085364.1 NW_018085365.1 NW_018085366.1 NW_018085367.1 NW_018085368.1 NC_000845.1. > [71420] [2024-04-03 09:10:42.642] [GLnexus] [info] db_get_contigs GLnexus.DB. > [71420] [2024-04-03 09:10:42.789] [GLnexus] [info] Beginning bulk load with no range filter. > [71420] [2024-04-03 10:09:37.111] [GLnexus] [info] Loaded 5 datasets with 5 samples; 846365851832 bytes in 8659464665 BCF records (882 duplicate) in 414215 buckets. Bucket max 2856376 bytes, 28997 records. 0 BCF records skipped due to caller-specific exceptions. > [71420] [2024-04-03 10:09:37.141] [GLnexus] [info] Created sample set *@5. > [71420] [2024-04-03 10:09:37.142] [GLnexus] [info] Flushing database... > [71420] [2024-04-03 10:11:17.432] [GLnexus] [info] Bulk load complete! > [71420] [2024-04-03 10:11:17.482] [GLnexus] [warning] Processing full length of 613 contigs, as no --bed was provided. Providing a BED file with regions of interest, if applicable, can speed this up. > [71420] [2024-04-03 10:11:17.509] [GLnexus] [info] found sample set *@5. > [71420] [2024-04-03 10:11:17.509] [GLnexus] [info] discovering alleles in 613 range(s) on 126 threads. > [71420] [2024-04-03 10:17:12.989] [GLnexus] [info] discovered 3689057 alleles. > [71420] [2024-04-03 10:17:15.093] [GLnexus] [info] unified to 159191 sites cleanly with 159684 ALT all",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/778
https://github.com/google/deepvariant/issues/778:12958,energy efficiency,load,load,12958,"346.1 NW_018085347.1 NW_018085348.1 NW_018085349.1 NW_018085350.1 NW_018085351.1 NW_018085352.1 NW_018085353.1 NW_018085354.1 NW_018085355.1 NW_018085356.1 NW_018085357.1 NW_018085358.1 NW_018085359.1 NW_018085360.1 NW_018085361.1 NW_018085362.1 NW_018085363.1 NW_018085364.1 NW_018085365.1 NW_018085366.1 NW_018085367.1 NW_018085368.1 NC_000845.1. > [71420] [2024-04-03 09:10:42.642] [GLnexus] [info] db_get_contigs GLnexus.DB. > [71420] [2024-04-03 09:10:42.789] [GLnexus] [info] Beginning bulk load with no range filter. > [71420] [2024-04-03 10:09:37.111] [GLnexus] [info] Loaded 5 datasets with 5 samples; 846365851832 bytes in 8659464665 BCF records (882 duplicate) in 414215 buckets. Bucket max 2856376 bytes, 28997 records. 0 BCF records skipped due to caller-specific exceptions. > [71420] [2024-04-03 10:09:37.141] [GLnexus] [info] Created sample set *@5. > [71420] [2024-04-03 10:09:37.142] [GLnexus] [info] Flushing database... > [71420] [2024-04-03 10:11:17.432] [GLnexus] [info] Bulk load complete! > [71420] [2024-04-03 10:11:17.482] [GLnexus] [warning] Processing full length of 613 contigs, as no --bed was provided. Providing a BED file with regions of interest, if applicable, can speed this up. > [71420] [2024-04-03 10:11:17.509] [GLnexus] [info] found sample set *@5. > [71420] [2024-04-03 10:11:17.509] [GLnexus] [info] discovering alleles in 613 range(s) on 126 threads. > [71420] [2024-04-03 10:17:12.989] [GLnexus] [info] discovered 3689057 alleles. > [71420] [2024-04-03 10:17:15.093] [GLnexus] [info] unified to 159191 sites cleanly with 159684 ALT alleles. 1 ALT alleles were additionally included in monoallelic sites and 1704795 were filtered out on quality thresholds. > [71420] [2024-04-03 10:17:15.093] [GLnexus] [info] Finishing database compaction... > [71420] [2024-04-03 10:17:17.832] [GLnexus] [info] genotyping 159191 sites; sample set = *@5 mem_budget = 0 threads = 128. > [71420] [2024-04-03 10:19:30.901] [GLnexus] [info] genotyping complete! > [71420] [202",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/778
https://github.com/google/deepvariant/issues/778:1778,integrability,filter,filtered,1778,"77b8061e0076bb257dbd7f3faa756. > [71420] [2024-04-03 09:10:42.183] [GLnexus] [info] Loading config preset DeepVariantWGS. > [71420] [2024-04-03 09:10:42.190] [GLnexus] [info] config:. > unifier_config:. > drop_filtered: false. > min_allele_copy_number: 1. > min_AQ1: 10. > min_AQ2: 10. > min_GQ: 0. > max_alleles_per_site: 32. > monoallelic_sites_for_lost_alleles: true. > preference: common. > genotyper_config:. > revise_genotypes: true. > min_assumed_allele_frequency: 9.99999975e-05. > snv_prior_calibration: 0.600000024. > indel_prior_calibration: 0.449999988. > required_dp: 0. > allow_partial_data: true. > allele_dp_format: AD. > ref_dp_format: MIN_DP. > output_residuals: false. > more_PL: true. > squeeze: false. > trim_uncalled_alleles: true. > top_two_half_calls: false. > output_format: BCF. > liftover_fields:. > - {orig_names: [MIN_DP, DP], name: DP, description: ""##FORMAT=<ID=DP,Number=1,Type=Integer,Description=\""Approximate read depth (reads with MQ=255 or with bad mates are filtered)\"">"", type: int, number: basic, default_type: missing, count: 1, combi_method: min, ignore_non_variants: true}. > - {orig_names: [AD], name: AD, description: ""##FORMAT=<ID=AD,Number=R,Type=Integer,Description=\""Allelic depths for the ref and alt alleles in the order listed\"">"", type: int, number: alleles, default_type: zero, count: 0, combi_method: min, ignore_non_variants: false}. > - {orig_names: [GQ], name: GQ, description: ""##FORMAT=<ID=GQ,Number=1,Type=Integer,Description=\""Genotype Quality\"">"", type: int, number: basic, default_type: missing, count: 1, combi_method: min, ignore_non_variants: true}. > - {orig_names: [PL], name: PL, description: ""##FORMAT=<ID=PL,Number=G,Type=Integer,Description=\""Phred-scaled genotype Likelihoods\"">"", type: int, number: genotype, default_type: missing, count: 0, combi_method: missing, ignore_non_variants: true}. > [71420] [2024-04-03 09:10:42.191] [GLnexus] [info] config CRC32C = 2932316105. > [71420] [2024-04-03 09:10:42.191] [GLnexus] [info",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/778
https://github.com/google/deepvariant/issues/778:2813,integrability,pub,public,2813,"c, default_type: missing, count: 1, combi_method: min, ignore_non_variants: true}. > - {orig_names: [AD], name: AD, description: ""##FORMAT=<ID=AD,Number=R,Type=Integer,Description=\""Allelic depths for the ref and alt alleles in the order listed\"">"", type: int, number: alleles, default_type: zero, count: 0, combi_method: min, ignore_non_variants: false}. > - {orig_names: [GQ], name: GQ, description: ""##FORMAT=<ID=GQ,Number=1,Type=Integer,Description=\""Genotype Quality\"">"", type: int, number: basic, default_type: missing, count: 1, combi_method: min, ignore_non_variants: true}. > - {orig_names: [PL], name: PL, description: ""##FORMAT=<ID=PL,Number=G,Type=Integer,Description=\""Phred-scaled genotype Likelihoods\"">"", type: int, number: genotype, default_type: missing, count: 0, combi_method: missing, ignore_non_variants: true}. > [71420] [2024-04-03 09:10:42.191] [GLnexus] [info] config CRC32C = 2932316105. > [71420] [2024-04-03 09:10:42.191] [GLnexus] [info] init database, exemplar_vcf=/public/home/zenglingsen/01.data/01.ONT_data/01.ONT_20X_fastq_SNP_calling/03.pepper/01.gvcf/gvcf_file/AW.new.excluded.mnps.gvcf.gz. > [71420] [2024-04-03 09:10:42.611] [GLnexus] [info] Initialized GLnexus database in GLnexus.DB. > [71420] [2024-04-03 09:10:42.611] [GLnexus] [info] bucket size: 30000. > [71420] [2024-04-03 09:10:42.612] [GLnexus] [info] contigs: NC_010443.5 NC_010444.4 NC_010445.4 NC_010446.5 NC_010447.5 NC_010448.4 NC_010449.5 NC_010450.4 NC_010451.4 NC_010452.4 NC_010453.5 NC_010454.4 NC_010455.5 NC_010456.5 NC_010457.5 NC_010458.4 NC_010459.5 NC_010460.4 NC_010461.5 NC_010462.3 NW_018084777.1 NW_018084778.1 NW_018084779.1 NW_018084780.1 NW_018084781.1 NW_018084782.1 NW_018084783.1 NW_018084784.1 NW_018084785.1 NW_018084786.1 NW_018084787.1 NW_018084788.1 NW_018084789.1 NW_018084790.1 NW_018084791.1 NW_018084792.1 NW_018084793.1 NW_018084794.1 NW_018084795.1 NW_018084796.1 NW_018084797.1 NW_018084798.1 NW_018084799.1 NW_018084800.1 NW_018084801.1 NW_018084802.1 NW_0180848",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/778
https://github.com/google/deepvariant/issues/778:12476,integrability,filter,filter,12476,"5314.1 NW_018085315.1 NW_018085316.1 NW_018085317.1 NW_018085318.1 NW_018085319.1 NW_018085320.1 NW_018085321.1 NW_018085322.1 NW_018085323.1 NW_018085324.1 NW_018085325.1 NW_018085326.1 NW_018085327.1 NW_018085328.1 NW_018085329.1 NW_018085330.1 NW_018085331.1 NW_018085332.1 NW_018085333.1 NW_018085334.1 NW_018085335.1 NW_018085336.1 NW_018085337.1 NW_018085338.1 NW_018085339.1 NW_018085340.1 NW_018085341.1 NW_018085342.1 NW_018085343.1 NW_018085344.1 NW_018085345.1 NW_018085346.1 NW_018085347.1 NW_018085348.1 NW_018085349.1 NW_018085350.1 NW_018085351.1 NW_018085352.1 NW_018085353.1 NW_018085354.1 NW_018085355.1 NW_018085356.1 NW_018085357.1 NW_018085358.1 NW_018085359.1 NW_018085360.1 NW_018085361.1 NW_018085362.1 NW_018085363.1 NW_018085364.1 NW_018085365.1 NW_018085366.1 NW_018085367.1 NW_018085368.1 NC_000845.1. > [71420] [2024-04-03 09:10:42.642] [GLnexus] [info] db_get_contigs GLnexus.DB. > [71420] [2024-04-03 09:10:42.789] [GLnexus] [info] Beginning bulk load with no range filter. > [71420] [2024-04-03 10:09:37.111] [GLnexus] [info] Loaded 5 datasets with 5 samples; 846365851832 bytes in 8659464665 BCF records (882 duplicate) in 414215 buckets. Bucket max 2856376 bytes, 28997 records. 0 BCF records skipped due to caller-specific exceptions. > [71420] [2024-04-03 10:09:37.141] [GLnexus] [info] Created sample set *@5. > [71420] [2024-04-03 10:09:37.142] [GLnexus] [info] Flushing database... > [71420] [2024-04-03 10:11:17.432] [GLnexus] [info] Bulk load complete! > [71420] [2024-04-03 10:11:17.482] [GLnexus] [warning] Processing full length of 613 contigs, as no --bed was provided. Providing a BED file with regions of interest, if applicable, can speed this up. > [71420] [2024-04-03 10:11:17.509] [GLnexus] [info] found sample set *@5. > [71420] [2024-04-03 10:11:17.509] [GLnexus] [info] discovering alleles in 613 range(s) on 126 threads. > [71420] [2024-04-03 10:17:12.989] [GLnexus] [info] discovered 3689057 alleles. > [71420] [2024-04-03 10:17:15.093] [GLnexu",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/778
https://github.com/google/deepvariant/issues/778:13303,integrability,discover,discovering,13303,"5361.1 NW_018085362.1 NW_018085363.1 NW_018085364.1 NW_018085365.1 NW_018085366.1 NW_018085367.1 NW_018085368.1 NC_000845.1. > [71420] [2024-04-03 09:10:42.642] [GLnexus] [info] db_get_contigs GLnexus.DB. > [71420] [2024-04-03 09:10:42.789] [GLnexus] [info] Beginning bulk load with no range filter. > [71420] [2024-04-03 10:09:37.111] [GLnexus] [info] Loaded 5 datasets with 5 samples; 846365851832 bytes in 8659464665 BCF records (882 duplicate) in 414215 buckets. Bucket max 2856376 bytes, 28997 records. 0 BCF records skipped due to caller-specific exceptions. > [71420] [2024-04-03 10:09:37.141] [GLnexus] [info] Created sample set *@5. > [71420] [2024-04-03 10:09:37.142] [GLnexus] [info] Flushing database... > [71420] [2024-04-03 10:11:17.432] [GLnexus] [info] Bulk load complete! > [71420] [2024-04-03 10:11:17.482] [GLnexus] [warning] Processing full length of 613 contigs, as no --bed was provided. Providing a BED file with regions of interest, if applicable, can speed this up. > [71420] [2024-04-03 10:11:17.509] [GLnexus] [info] found sample set *@5. > [71420] [2024-04-03 10:11:17.509] [GLnexus] [info] discovering alleles in 613 range(s) on 126 threads. > [71420] [2024-04-03 10:17:12.989] [GLnexus] [info] discovered 3689057 alleles. > [71420] [2024-04-03 10:17:15.093] [GLnexus] [info] unified to 159191 sites cleanly with 159684 ALT alleles. 1 ALT alleles were additionally included in monoallelic sites and 1704795 were filtered out on quality thresholds. > [71420] [2024-04-03 10:17:15.093] [GLnexus] [info] Finishing database compaction... > [71420] [2024-04-03 10:17:17.832] [GLnexus] [info] genotyping 159191 sites; sample set = *@5 mem_budget = 0 threads = 128. > [71420] [2024-04-03 10:19:30.901] [GLnexus] [info] genotyping complete! > [71420] [2024-04-03 10:19:30.917] [GLnexus] [info] worker threads were cumulatively stalled for 169810ms. > [71420] [2024-04-03 10:19:30.917] [GLnexus] [info] Num BCF records read 5557229 query hits 1178413. > INFO: Cleaning up image...",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/778
https://github.com/google/deepvariant/issues/778:13408,integrability,discover,discovered,13408,"5361.1 NW_018085362.1 NW_018085363.1 NW_018085364.1 NW_018085365.1 NW_018085366.1 NW_018085367.1 NW_018085368.1 NC_000845.1. > [71420] [2024-04-03 09:10:42.642] [GLnexus] [info] db_get_contigs GLnexus.DB. > [71420] [2024-04-03 09:10:42.789] [GLnexus] [info] Beginning bulk load with no range filter. > [71420] [2024-04-03 10:09:37.111] [GLnexus] [info] Loaded 5 datasets with 5 samples; 846365851832 bytes in 8659464665 BCF records (882 duplicate) in 414215 buckets. Bucket max 2856376 bytes, 28997 records. 0 BCF records skipped due to caller-specific exceptions. > [71420] [2024-04-03 10:09:37.141] [GLnexus] [info] Created sample set *@5. > [71420] [2024-04-03 10:09:37.142] [GLnexus] [info] Flushing database... > [71420] [2024-04-03 10:11:17.432] [GLnexus] [info] Bulk load complete! > [71420] [2024-04-03 10:11:17.482] [GLnexus] [warning] Processing full length of 613 contigs, as no --bed was provided. Providing a BED file with regions of interest, if applicable, can speed this up. > [71420] [2024-04-03 10:11:17.509] [GLnexus] [info] found sample set *@5. > [71420] [2024-04-03 10:11:17.509] [GLnexus] [info] discovering alleles in 613 range(s) on 126 threads. > [71420] [2024-04-03 10:17:12.989] [GLnexus] [info] discovered 3689057 alleles. > [71420] [2024-04-03 10:17:15.093] [GLnexus] [info] unified to 159191 sites cleanly with 159684 ALT alleles. 1 ALT alleles were additionally included in monoallelic sites and 1704795 were filtered out on quality thresholds. > [71420] [2024-04-03 10:17:15.093] [GLnexus] [info] Finishing database compaction... > [71420] [2024-04-03 10:17:17.832] [GLnexus] [info] genotyping 159191 sites; sample set = *@5 mem_budget = 0 threads = 128. > [71420] [2024-04-03 10:19:30.901] [GLnexus] [info] genotyping complete! > [71420] [2024-04-03 10:19:30.917] [GLnexus] [info] worker threads were cumulatively stalled for 169810ms. > [71420] [2024-04-03 10:19:30.917] [GLnexus] [info] Num BCF records read 5557229 query hits 1178413. > INFO: Cleaning up image...",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/778
https://github.com/google/deepvariant/issues/778:13625,integrability,filter,filtered,13625,"5361.1 NW_018085362.1 NW_018085363.1 NW_018085364.1 NW_018085365.1 NW_018085366.1 NW_018085367.1 NW_018085368.1 NC_000845.1. > [71420] [2024-04-03 09:10:42.642] [GLnexus] [info] db_get_contigs GLnexus.DB. > [71420] [2024-04-03 09:10:42.789] [GLnexus] [info] Beginning bulk load with no range filter. > [71420] [2024-04-03 10:09:37.111] [GLnexus] [info] Loaded 5 datasets with 5 samples; 846365851832 bytes in 8659464665 BCF records (882 duplicate) in 414215 buckets. Bucket max 2856376 bytes, 28997 records. 0 BCF records skipped due to caller-specific exceptions. > [71420] [2024-04-03 10:09:37.141] [GLnexus] [info] Created sample set *@5. > [71420] [2024-04-03 10:09:37.142] [GLnexus] [info] Flushing database... > [71420] [2024-04-03 10:11:17.432] [GLnexus] [info] Bulk load complete! > [71420] [2024-04-03 10:11:17.482] [GLnexus] [warning] Processing full length of 613 contigs, as no --bed was provided. Providing a BED file with regions of interest, if applicable, can speed this up. > [71420] [2024-04-03 10:11:17.509] [GLnexus] [info] found sample set *@5. > [71420] [2024-04-03 10:11:17.509] [GLnexus] [info] discovering alleles in 613 range(s) on 126 threads. > [71420] [2024-04-03 10:17:12.989] [GLnexus] [info] discovered 3689057 alleles. > [71420] [2024-04-03 10:17:15.093] [GLnexus] [info] unified to 159191 sites cleanly with 159684 ALT alleles. 1 ALT alleles were additionally included in monoallelic sites and 1704795 were filtered out on quality thresholds. > [71420] [2024-04-03 10:17:15.093] [GLnexus] [info] Finishing database compaction... > [71420] [2024-04-03 10:17:17.832] [GLnexus] [info] genotyping 159191 sites; sample set = *@5 mem_budget = 0 threads = 128. > [71420] [2024-04-03 10:19:30.901] [GLnexus] [info] genotyping complete! > [71420] [2024-04-03 10:19:30.917] [GLnexus] [info] worker threads were cumulatively stalled for 169810ms. > [71420] [2024-04-03 10:19:30.917] [GLnexus] [info] Num BCF records read 5557229 query hits 1178413. > INFO: Cleaning up image...",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/778
https://github.com/google/deepvariant/issues/778:574,interoperability,bind,bind,574,"Hi @AndrewCarroll ,. I followed the instructions to merge gvcf file into a final vcf via GLnexus with the default parameters like this:. `singularity exec glnexus.sif glnexus_cli --config DeepVariantWGS $gvcf_path/*.gvcf.gz > ${output_bcf}`. But it only output 62409 SNPs in the final vcf file (pepper.merged.glnexus.vcf.gz 6.8M), there are 5 input gvcf files (each of one is about 11GB, the sample is from the whole genome of pig). . the below is the log from GLnexus. > INFO: Convert SIF file to sandbox... > WARNING: underlay of /etc/localtime required more than 50 (77) bind mounts. > [71420] [2024-04-03 09:10:42.182] [GLnexus] [info] glnexus_cli release v1.4.1-0-g68e25e5 Aug 13 2021. > [71420] [2024-04-03 09:10:42.182] [GLnexus] [info] detected jemalloc 5.2.1-0-gea6b3e973b477b8061e0076bb257dbd7f3faa756. > [71420] [2024-04-03 09:10:42.183] [GLnexus] [info] Loading config preset DeepVariantWGS. > [71420] [2024-04-03 09:10:42.190] [GLnexus] [info] config:. > unifier_config:. > drop_filtered: false. > min_allele_copy_number: 1. > min_AQ1: 10. > min_AQ2: 10. > min_GQ: 0. > max_alleles_per_site: 32. > monoallelic_sites_for_lost_alleles: true. > preference: common. > genotyper_config:. > revise_genotypes: true. > min_assumed_allele_frequency: 9.99999975e-05. > snv_prior_calibration: 0.600000024. > indel_prior_calibration: 0.449999988. > required_dp: 0. > allow_partial_data: true. > allele_dp_format: AD. > ref_dp_format: MIN_DP. > output_residuals: false. > more_PL: true. > squeeze: false. > trim_uncalled_alleles: true. > top_two_half_calls: false. > output_format: BCF. > liftover_fields:. > - {orig_names: [MIN_DP, DP], name: DP, description: ""##FORMAT=<ID=DP,Number=1,Type=Integer,Description=\""Approximate read depth (reads with MQ=255 or with bad mates are filtered)\"">"", type: int, number: basic, default_type: missing, count: 1, combi_method: min, ignore_non_variants: true}. > - {orig_names: [AD], name: AD, description: ""##FORMAT=<ID=AD,Number=R,Type=Integer,Description=\""Al",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/778
https://github.com/google/deepvariant/issues/778:1664,interoperability,FORMAT,FORMAT,1664,"0-g68e25e5 Aug 13 2021. > [71420] [2024-04-03 09:10:42.182] [GLnexus] [info] detected jemalloc 5.2.1-0-gea6b3e973b477b8061e0076bb257dbd7f3faa756. > [71420] [2024-04-03 09:10:42.183] [GLnexus] [info] Loading config preset DeepVariantWGS. > [71420] [2024-04-03 09:10:42.190] [GLnexus] [info] config:. > unifier_config:. > drop_filtered: false. > min_allele_copy_number: 1. > min_AQ1: 10. > min_AQ2: 10. > min_GQ: 0. > max_alleles_per_site: 32. > monoallelic_sites_for_lost_alleles: true. > preference: common. > genotyper_config:. > revise_genotypes: true. > min_assumed_allele_frequency: 9.99999975e-05. > snv_prior_calibration: 0.600000024. > indel_prior_calibration: 0.449999988. > required_dp: 0. > allow_partial_data: true. > allele_dp_format: AD. > ref_dp_format: MIN_DP. > output_residuals: false. > more_PL: true. > squeeze: false. > trim_uncalled_alleles: true. > top_two_half_calls: false. > output_format: BCF. > liftover_fields:. > - {orig_names: [MIN_DP, DP], name: DP, description: ""##FORMAT=<ID=DP,Number=1,Type=Integer,Description=\""Approximate read depth (reads with MQ=255 or with bad mates are filtered)\"">"", type: int, number: basic, default_type: missing, count: 1, combi_method: min, ignore_non_variants: true}. > - {orig_names: [AD], name: AD, description: ""##FORMAT=<ID=AD,Number=R,Type=Integer,Description=\""Allelic depths for the ref and alt alleles in the order listed\"">"", type: int, number: alleles, default_type: zero, count: 0, combi_method: min, ignore_non_variants: false}. > - {orig_names: [GQ], name: GQ, description: ""##FORMAT=<ID=GQ,Number=1,Type=Integer,Description=\""Genotype Quality\"">"", type: int, number: basic, default_type: missing, count: 1, combi_method: min, ignore_non_variants: true}. > - {orig_names: [PL], name: PL, description: ""##FORMAT=<ID=PL,Number=G,Type=Integer,Description=\""Phred-scaled genotype Likelihoods\"">"", type: int, number: genotype, default_type: missing, count: 0, combi_method: missing, ignore_non_variants: true}. > [71420] [2024-0",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/778
https://github.com/google/deepvariant/issues/778:1948,interoperability,FORMAT,FORMAT,1948,"info] config:. > unifier_config:. > drop_filtered: false. > min_allele_copy_number: 1. > min_AQ1: 10. > min_AQ2: 10. > min_GQ: 0. > max_alleles_per_site: 32. > monoallelic_sites_for_lost_alleles: true. > preference: common. > genotyper_config:. > revise_genotypes: true. > min_assumed_allele_frequency: 9.99999975e-05. > snv_prior_calibration: 0.600000024. > indel_prior_calibration: 0.449999988. > required_dp: 0. > allow_partial_data: true. > allele_dp_format: AD. > ref_dp_format: MIN_DP. > output_residuals: false. > more_PL: true. > squeeze: false. > trim_uncalled_alleles: true. > top_two_half_calls: false. > output_format: BCF. > liftover_fields:. > - {orig_names: [MIN_DP, DP], name: DP, description: ""##FORMAT=<ID=DP,Number=1,Type=Integer,Description=\""Approximate read depth (reads with MQ=255 or with bad mates are filtered)\"">"", type: int, number: basic, default_type: missing, count: 1, combi_method: min, ignore_non_variants: true}. > - {orig_names: [AD], name: AD, description: ""##FORMAT=<ID=AD,Number=R,Type=Integer,Description=\""Allelic depths for the ref and alt alleles in the order listed\"">"", type: int, number: alleles, default_type: zero, count: 0, combi_method: min, ignore_non_variants: false}. > - {orig_names: [GQ], name: GQ, description: ""##FORMAT=<ID=GQ,Number=1,Type=Integer,Description=\""Genotype Quality\"">"", type: int, number: basic, default_type: missing, count: 1, combi_method: min, ignore_non_variants: true}. > - {orig_names: [PL], name: PL, description: ""##FORMAT=<ID=PL,Number=G,Type=Integer,Description=\""Phred-scaled genotype Likelihoods\"">"", type: int, number: genotype, default_type: missing, count: 0, combi_method: missing, ignore_non_variants: true}. > [71420] [2024-04-03 09:10:42.191] [GLnexus] [info] config CRC32C = 2932316105. > [71420] [2024-04-03 09:10:42.191] [GLnexus] [info] init database, exemplar_vcf=/public/home/zenglingsen/01.data/01.ONT_data/01.ONT_20X_fastq_SNP_calling/03.pepper/01.gvcf/gvcf_file/AW.new.excluded.mnps.gvcf.gz. > [7142",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/778
https://github.com/google/deepvariant/issues/778:2221,interoperability,FORMAT,FORMAT,2221,"min_assumed_allele_frequency: 9.99999975e-05. > snv_prior_calibration: 0.600000024. > indel_prior_calibration: 0.449999988. > required_dp: 0. > allow_partial_data: true. > allele_dp_format: AD. > ref_dp_format: MIN_DP. > output_residuals: false. > more_PL: true. > squeeze: false. > trim_uncalled_alleles: true. > top_two_half_calls: false. > output_format: BCF. > liftover_fields:. > - {orig_names: [MIN_DP, DP], name: DP, description: ""##FORMAT=<ID=DP,Number=1,Type=Integer,Description=\""Approximate read depth (reads with MQ=255 or with bad mates are filtered)\"">"", type: int, number: basic, default_type: missing, count: 1, combi_method: min, ignore_non_variants: true}. > - {orig_names: [AD], name: AD, description: ""##FORMAT=<ID=AD,Number=R,Type=Integer,Description=\""Allelic depths for the ref and alt alleles in the order listed\"">"", type: int, number: alleles, default_type: zero, count: 0, combi_method: min, ignore_non_variants: false}. > - {orig_names: [GQ], name: GQ, description: ""##FORMAT=<ID=GQ,Number=1,Type=Integer,Description=\""Genotype Quality\"">"", type: int, number: basic, default_type: missing, count: 1, combi_method: min, ignore_non_variants: true}. > - {orig_names: [PL], name: PL, description: ""##FORMAT=<ID=PL,Number=G,Type=Integer,Description=\""Phred-scaled genotype Likelihoods\"">"", type: int, number: genotype, default_type: missing, count: 0, combi_method: missing, ignore_non_variants: true}. > [71420] [2024-04-03 09:10:42.191] [GLnexus] [info] config CRC32C = 2932316105. > [71420] [2024-04-03 09:10:42.191] [GLnexus] [info] init database, exemplar_vcf=/public/home/zenglingsen/01.data/01.ONT_data/01.ONT_20X_fastq_SNP_calling/03.pepper/01.gvcf/gvcf_file/AW.new.excluded.mnps.gvcf.gz. > [71420] [2024-04-03 09:10:42.611] [GLnexus] [info] Initialized GLnexus database in GLnexus.DB. > [71420] [2024-04-03 09:10:42.611] [GLnexus] [info] bucket size: 30000. > [71420] [2024-04-03 09:10:42.612] [GLnexus] [info] contigs: NC_010443.5 NC_010444.4 NC_010445.4 NC_010446.5 ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/778
https://github.com/google/deepvariant/issues/778:2448,interoperability,FORMAT,FORMAT,2448,"_residuals: false. > more_PL: true. > squeeze: false. > trim_uncalled_alleles: true. > top_two_half_calls: false. > output_format: BCF. > liftover_fields:. > - {orig_names: [MIN_DP, DP], name: DP, description: ""##FORMAT=<ID=DP,Number=1,Type=Integer,Description=\""Approximate read depth (reads with MQ=255 or with bad mates are filtered)\"">"", type: int, number: basic, default_type: missing, count: 1, combi_method: min, ignore_non_variants: true}. > - {orig_names: [AD], name: AD, description: ""##FORMAT=<ID=AD,Number=R,Type=Integer,Description=\""Allelic depths for the ref and alt alleles in the order listed\"">"", type: int, number: alleles, default_type: zero, count: 0, combi_method: min, ignore_non_variants: false}. > - {orig_names: [GQ], name: GQ, description: ""##FORMAT=<ID=GQ,Number=1,Type=Integer,Description=\""Genotype Quality\"">"", type: int, number: basic, default_type: missing, count: 1, combi_method: min, ignore_non_variants: true}. > - {orig_names: [PL], name: PL, description: ""##FORMAT=<ID=PL,Number=G,Type=Integer,Description=\""Phred-scaled genotype Likelihoods\"">"", type: int, number: genotype, default_type: missing, count: 0, combi_method: missing, ignore_non_variants: true}. > [71420] [2024-04-03 09:10:42.191] [GLnexus] [info] config CRC32C = 2932316105. > [71420] [2024-04-03 09:10:42.191] [GLnexus] [info] init database, exemplar_vcf=/public/home/zenglingsen/01.data/01.ONT_data/01.ONT_20X_fastq_SNP_calling/03.pepper/01.gvcf/gvcf_file/AW.new.excluded.mnps.gvcf.gz. > [71420] [2024-04-03 09:10:42.611] [GLnexus] [info] Initialized GLnexus database in GLnexus.DB. > [71420] [2024-04-03 09:10:42.611] [GLnexus] [info] bucket size: 30000. > [71420] [2024-04-03 09:10:42.612] [GLnexus] [info] contigs: NC_010443.5 NC_010444.4 NC_010445.4 NC_010446.5 NC_010447.5 NC_010448.4 NC_010449.5 NC_010450.4 NC_010451.4 NC_010452.4 NC_010453.5 NC_010454.4 NC_010455.5 NC_010456.5 NC_010457.5 NC_010458.4 NC_010459.5 NC_010460.4 NC_010461.5 NC_010462.3 NW_018084777.1 NW_018084778.1 NW_01",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/778
https://github.com/google/deepvariant/issues/778:12728,interoperability,specif,specific,12728,"085331.1 NW_018085332.1 NW_018085333.1 NW_018085334.1 NW_018085335.1 NW_018085336.1 NW_018085337.1 NW_018085338.1 NW_018085339.1 NW_018085340.1 NW_018085341.1 NW_018085342.1 NW_018085343.1 NW_018085344.1 NW_018085345.1 NW_018085346.1 NW_018085347.1 NW_018085348.1 NW_018085349.1 NW_018085350.1 NW_018085351.1 NW_018085352.1 NW_018085353.1 NW_018085354.1 NW_018085355.1 NW_018085356.1 NW_018085357.1 NW_018085358.1 NW_018085359.1 NW_018085360.1 NW_018085361.1 NW_018085362.1 NW_018085363.1 NW_018085364.1 NW_018085365.1 NW_018085366.1 NW_018085367.1 NW_018085368.1 NC_000845.1. > [71420] [2024-04-03 09:10:42.642] [GLnexus] [info] db_get_contigs GLnexus.DB. > [71420] [2024-04-03 09:10:42.789] [GLnexus] [info] Beginning bulk load with no range filter. > [71420] [2024-04-03 10:09:37.111] [GLnexus] [info] Loaded 5 datasets with 5 samples; 846365851832 bytes in 8659464665 BCF records (882 duplicate) in 414215 buckets. Bucket max 2856376 bytes, 28997 records. 0 BCF records skipped due to caller-specific exceptions. > [71420] [2024-04-03 10:09:37.141] [GLnexus] [info] Created sample set *@5. > [71420] [2024-04-03 10:09:37.142] [GLnexus] [info] Flushing database... > [71420] [2024-04-03 10:11:17.432] [GLnexus] [info] Bulk load complete! > [71420] [2024-04-03 10:11:17.482] [GLnexus] [warning] Processing full length of 613 contigs, as no --bed was provided. Providing a BED file with regions of interest, if applicable, can speed this up. > [71420] [2024-04-03 10:11:17.509] [GLnexus] [info] found sample set *@5. > [71420] [2024-04-03 10:11:17.509] [GLnexus] [info] discovering alleles in 613 range(s) on 126 threads. > [71420] [2024-04-03 10:17:12.989] [GLnexus] [info] discovered 3689057 alleles. > [71420] [2024-04-03 10:17:15.093] [GLnexus] [info] unified to 159191 sites cleanly with 159684 ALT alleles. 1 ALT alleles were additionally included in monoallelic sites and 1704795 were filtered out on quality thresholds. > [71420] [2024-04-03 10:17:15.093] [GLnexus] [info] Finishing database",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/778
https://github.com/google/deepvariant/issues/778:13303,interoperability,discover,discovering,13303,"5361.1 NW_018085362.1 NW_018085363.1 NW_018085364.1 NW_018085365.1 NW_018085366.1 NW_018085367.1 NW_018085368.1 NC_000845.1. > [71420] [2024-04-03 09:10:42.642] [GLnexus] [info] db_get_contigs GLnexus.DB. > [71420] [2024-04-03 09:10:42.789] [GLnexus] [info] Beginning bulk load with no range filter. > [71420] [2024-04-03 10:09:37.111] [GLnexus] [info] Loaded 5 datasets with 5 samples; 846365851832 bytes in 8659464665 BCF records (882 duplicate) in 414215 buckets. Bucket max 2856376 bytes, 28997 records. 0 BCF records skipped due to caller-specific exceptions. > [71420] [2024-04-03 10:09:37.141] [GLnexus] [info] Created sample set *@5. > [71420] [2024-04-03 10:09:37.142] [GLnexus] [info] Flushing database... > [71420] [2024-04-03 10:11:17.432] [GLnexus] [info] Bulk load complete! > [71420] [2024-04-03 10:11:17.482] [GLnexus] [warning] Processing full length of 613 contigs, as no --bed was provided. Providing a BED file with regions of interest, if applicable, can speed this up. > [71420] [2024-04-03 10:11:17.509] [GLnexus] [info] found sample set *@5. > [71420] [2024-04-03 10:11:17.509] [GLnexus] [info] discovering alleles in 613 range(s) on 126 threads. > [71420] [2024-04-03 10:17:12.989] [GLnexus] [info] discovered 3689057 alleles. > [71420] [2024-04-03 10:17:15.093] [GLnexus] [info] unified to 159191 sites cleanly with 159684 ALT alleles. 1 ALT alleles were additionally included in monoallelic sites and 1704795 were filtered out on quality thresholds. > [71420] [2024-04-03 10:17:15.093] [GLnexus] [info] Finishing database compaction... > [71420] [2024-04-03 10:17:17.832] [GLnexus] [info] genotyping 159191 sites; sample set = *@5 mem_budget = 0 threads = 128. > [71420] [2024-04-03 10:19:30.901] [GLnexus] [info] genotyping complete! > [71420] [2024-04-03 10:19:30.917] [GLnexus] [info] worker threads were cumulatively stalled for 169810ms. > [71420] [2024-04-03 10:19:30.917] [GLnexus] [info] Num BCF records read 5557229 query hits 1178413. > INFO: Cleaning up image...",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/778
https://github.com/google/deepvariant/issues/778:13408,interoperability,discover,discovered,13408,"5361.1 NW_018085362.1 NW_018085363.1 NW_018085364.1 NW_018085365.1 NW_018085366.1 NW_018085367.1 NW_018085368.1 NC_000845.1. > [71420] [2024-04-03 09:10:42.642] [GLnexus] [info] db_get_contigs GLnexus.DB. > [71420] [2024-04-03 09:10:42.789] [GLnexus] [info] Beginning bulk load with no range filter. > [71420] [2024-04-03 10:09:37.111] [GLnexus] [info] Loaded 5 datasets with 5 samples; 846365851832 bytes in 8659464665 BCF records (882 duplicate) in 414215 buckets. Bucket max 2856376 bytes, 28997 records. 0 BCF records skipped due to caller-specific exceptions. > [71420] [2024-04-03 10:09:37.141] [GLnexus] [info] Created sample set *@5. > [71420] [2024-04-03 10:09:37.142] [GLnexus] [info] Flushing database... > [71420] [2024-04-03 10:11:17.432] [GLnexus] [info] Bulk load complete! > [71420] [2024-04-03 10:11:17.482] [GLnexus] [warning] Processing full length of 613 contigs, as no --bed was provided. Providing a BED file with regions of interest, if applicable, can speed this up. > [71420] [2024-04-03 10:11:17.509] [GLnexus] [info] found sample set *@5. > [71420] [2024-04-03 10:11:17.509] [GLnexus] [info] discovering alleles in 613 range(s) on 126 threads. > [71420] [2024-04-03 10:17:12.989] [GLnexus] [info] discovered 3689057 alleles. > [71420] [2024-04-03 10:17:15.093] [GLnexus] [info] unified to 159191 sites cleanly with 159684 ALT alleles. 1 ALT alleles were additionally included in monoallelic sites and 1704795 were filtered out on quality thresholds. > [71420] [2024-04-03 10:17:15.093] [GLnexus] [info] Finishing database compaction... > [71420] [2024-04-03 10:17:17.832] [GLnexus] [info] genotyping 159191 sites; sample set = *@5 mem_budget = 0 threads = 128. > [71420] [2024-04-03 10:19:30.901] [GLnexus] [info] genotyping complete! > [71420] [2024-04-03 10:19:30.917] [GLnexus] [info] worker threads were cumulatively stalled for 169810ms. > [71420] [2024-04-03 10:19:30.917] [GLnexus] [info] Num BCF records read 5557229 query hits 1178413. > INFO: Cleaning up image...",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/778
https://github.com/google/deepvariant/issues/778:114,modifiability,paramet,parameters,114,"Hi @AndrewCarroll ,. I followed the instructions to merge gvcf file into a final vcf via GLnexus with the default parameters like this:. `singularity exec glnexus.sif glnexus_cli --config DeepVariantWGS $gvcf_path/*.gvcf.gz > ${output_bcf}`. But it only output 62409 SNPs in the final vcf file (pepper.merged.glnexus.vcf.gz 6.8M), there are 5 input gvcf files (each of one is about 11GB, the sample is from the whole genome of pig). . the below is the log from GLnexus. > INFO: Convert SIF file to sandbox... > WARNING: underlay of /etc/localtime required more than 50 (77) bind mounts. > [71420] [2024-04-03 09:10:42.182] [GLnexus] [info] glnexus_cli release v1.4.1-0-g68e25e5 Aug 13 2021. > [71420] [2024-04-03 09:10:42.182] [GLnexus] [info] detected jemalloc 5.2.1-0-gea6b3e973b477b8061e0076bb257dbd7f3faa756. > [71420] [2024-04-03 09:10:42.183] [GLnexus] [info] Loading config preset DeepVariantWGS. > [71420] [2024-04-03 09:10:42.190] [GLnexus] [info] config:. > unifier_config:. > drop_filtered: false. > min_allele_copy_number: 1. > min_AQ1: 10. > min_AQ2: 10. > min_GQ: 0. > max_alleles_per_site: 32. > monoallelic_sites_for_lost_alleles: true. > preference: common. > genotyper_config:. > revise_genotypes: true. > min_assumed_allele_frequency: 9.99999975e-05. > snv_prior_calibration: 0.600000024. > indel_prior_calibration: 0.449999988. > required_dp: 0. > allow_partial_data: true. > allele_dp_format: AD. > ref_dp_format: MIN_DP. > output_residuals: false. > more_PL: true. > squeeze: false. > trim_uncalled_alleles: true. > top_two_half_calls: false. > output_format: BCF. > liftover_fields:. > - {orig_names: [MIN_DP, DP], name: DP, description: ""##FORMAT=<ID=DP,Number=1,Type=Integer,Description=\""Approximate read depth (reads with MQ=255 or with bad mates are filtered)\"">"", type: int, number: basic, default_type: missing, count: 1, combi_method: min, ignore_non_variants: true}. > - {orig_names: [AD], name: AD, description: ""##FORMAT=<ID=AD,Number=R,Type=Integer,Description=\""Al",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/778
https://github.com/google/deepvariant/issues/778:574,modifiability,bind,bind,574,"Hi @AndrewCarroll ,. I followed the instructions to merge gvcf file into a final vcf via GLnexus with the default parameters like this:. `singularity exec glnexus.sif glnexus_cli --config DeepVariantWGS $gvcf_path/*.gvcf.gz > ${output_bcf}`. But it only output 62409 SNPs in the final vcf file (pepper.merged.glnexus.vcf.gz 6.8M), there are 5 input gvcf files (each of one is about 11GB, the sample is from the whole genome of pig). . the below is the log from GLnexus. > INFO: Convert SIF file to sandbox... > WARNING: underlay of /etc/localtime required more than 50 (77) bind mounts. > [71420] [2024-04-03 09:10:42.182] [GLnexus] [info] glnexus_cli release v1.4.1-0-g68e25e5 Aug 13 2021. > [71420] [2024-04-03 09:10:42.182] [GLnexus] [info] detected jemalloc 5.2.1-0-gea6b3e973b477b8061e0076bb257dbd7f3faa756. > [71420] [2024-04-03 09:10:42.183] [GLnexus] [info] Loading config preset DeepVariantWGS. > [71420] [2024-04-03 09:10:42.190] [GLnexus] [info] config:. > unifier_config:. > drop_filtered: false. > min_allele_copy_number: 1. > min_AQ1: 10. > min_AQ2: 10. > min_GQ: 0. > max_alleles_per_site: 32. > monoallelic_sites_for_lost_alleles: true. > preference: common. > genotyper_config:. > revise_genotypes: true. > min_assumed_allele_frequency: 9.99999975e-05. > snv_prior_calibration: 0.600000024. > indel_prior_calibration: 0.449999988. > required_dp: 0. > allow_partial_data: true. > allele_dp_format: AD. > ref_dp_format: MIN_DP. > output_residuals: false. > more_PL: true. > squeeze: false. > trim_uncalled_alleles: true. > top_two_half_calls: false. > output_format: BCF. > liftover_fields:. > - {orig_names: [MIN_DP, DP], name: DP, description: ""##FORMAT=<ID=DP,Number=1,Type=Integer,Description=\""Approximate read depth (reads with MQ=255 or with bad mates are filtered)\"">"", type: int, number: basic, default_type: missing, count: 1, combi_method: min, ignore_non_variants: true}. > - {orig_names: [AD], name: AD, description: ""##FORMAT=<ID=AD,Number=R,Type=Integer,Description=\""Al",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/778
https://github.com/google/deepvariant/issues/778:2504,modifiability,scal,scaled,2504,"trim_uncalled_alleles: true. > top_two_half_calls: false. > output_format: BCF. > liftover_fields:. > - {orig_names: [MIN_DP, DP], name: DP, description: ""##FORMAT=<ID=DP,Number=1,Type=Integer,Description=\""Approximate read depth (reads with MQ=255 or with bad mates are filtered)\"">"", type: int, number: basic, default_type: missing, count: 1, combi_method: min, ignore_non_variants: true}. > - {orig_names: [AD], name: AD, description: ""##FORMAT=<ID=AD,Number=R,Type=Integer,Description=\""Allelic depths for the ref and alt alleles in the order listed\"">"", type: int, number: alleles, default_type: zero, count: 0, combi_method: min, ignore_non_variants: false}. > - {orig_names: [GQ], name: GQ, description: ""##FORMAT=<ID=GQ,Number=1,Type=Integer,Description=\""Genotype Quality\"">"", type: int, number: basic, default_type: missing, count: 1, combi_method: min, ignore_non_variants: true}. > - {orig_names: [PL], name: PL, description: ""##FORMAT=<ID=PL,Number=G,Type=Integer,Description=\""Phred-scaled genotype Likelihoods\"">"", type: int, number: genotype, default_type: missing, count: 0, combi_method: missing, ignore_non_variants: true}. > [71420] [2024-04-03 09:10:42.191] [GLnexus] [info] config CRC32C = 2932316105. > [71420] [2024-04-03 09:10:42.191] [GLnexus] [info] init database, exemplar_vcf=/public/home/zenglingsen/01.data/01.ONT_data/01.ONT_20X_fastq_SNP_calling/03.pepper/01.gvcf/gvcf_file/AW.new.excluded.mnps.gvcf.gz. > [71420] [2024-04-03 09:10:42.611] [GLnexus] [info] Initialized GLnexus database in GLnexus.DB. > [71420] [2024-04-03 09:10:42.611] [GLnexus] [info] bucket size: 30000. > [71420] [2024-04-03 09:10:42.612] [GLnexus] [info] contigs: NC_010443.5 NC_010444.4 NC_010445.4 NC_010446.5 NC_010447.5 NC_010448.4 NC_010449.5 NC_010450.4 NC_010451.4 NC_010452.4 NC_010453.5 NC_010454.4 NC_010455.5 NC_010456.5 NC_010457.5 NC_010458.4 NC_010459.5 NC_010460.4 NC_010461.5 NC_010462.3 NW_018084777.1 NW_018084778.1 NW_018084779.1 NW_018084780.1 NW_018084781.1 NW_018084782.1 N",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/778
https://github.com/google/deepvariant/issues/778:866,performance,Load,Loading,866,"Hi @AndrewCarroll ,. I followed the instructions to merge gvcf file into a final vcf via GLnexus with the default parameters like this:. `singularity exec glnexus.sif glnexus_cli --config DeepVariantWGS $gvcf_path/*.gvcf.gz > ${output_bcf}`. But it only output 62409 SNPs in the final vcf file (pepper.merged.glnexus.vcf.gz 6.8M), there are 5 input gvcf files (each of one is about 11GB, the sample is from the whole genome of pig). . the below is the log from GLnexus. > INFO: Convert SIF file to sandbox... > WARNING: underlay of /etc/localtime required more than 50 (77) bind mounts. > [71420] [2024-04-03 09:10:42.182] [GLnexus] [info] glnexus_cli release v1.4.1-0-g68e25e5 Aug 13 2021. > [71420] [2024-04-03 09:10:42.182] [GLnexus] [info] detected jemalloc 5.2.1-0-gea6b3e973b477b8061e0076bb257dbd7f3faa756. > [71420] [2024-04-03 09:10:42.183] [GLnexus] [info] Loading config preset DeepVariantWGS. > [71420] [2024-04-03 09:10:42.190] [GLnexus] [info] config:. > unifier_config:. > drop_filtered: false. > min_allele_copy_number: 1. > min_AQ1: 10. > min_AQ2: 10. > min_GQ: 0. > max_alleles_per_site: 32. > monoallelic_sites_for_lost_alleles: true. > preference: common. > genotyper_config:. > revise_genotypes: true. > min_assumed_allele_frequency: 9.99999975e-05. > snv_prior_calibration: 0.600000024. > indel_prior_calibration: 0.449999988. > required_dp: 0. > allow_partial_data: true. > allele_dp_format: AD. > ref_dp_format: MIN_DP. > output_residuals: false. > more_PL: true. > squeeze: false. > trim_uncalled_alleles: true. > top_two_half_calls: false. > output_format: BCF. > liftover_fields:. > - {orig_names: [MIN_DP, DP], name: DP, description: ""##FORMAT=<ID=DP,Number=1,Type=Integer,Description=\""Approximate read depth (reads with MQ=255 or with bad mates are filtered)\"">"", type: int, number: basic, default_type: missing, count: 1, combi_method: min, ignore_non_variants: true}. > - {orig_names: [AD], name: AD, description: ""##FORMAT=<ID=AD,Number=R,Type=Integer,Description=\""Al",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/778
https://github.com/google/deepvariant/issues/778:2504,performance,scale,scaled,2504,"trim_uncalled_alleles: true. > top_two_half_calls: false. > output_format: BCF. > liftover_fields:. > - {orig_names: [MIN_DP, DP], name: DP, description: ""##FORMAT=<ID=DP,Number=1,Type=Integer,Description=\""Approximate read depth (reads with MQ=255 or with bad mates are filtered)\"">"", type: int, number: basic, default_type: missing, count: 1, combi_method: min, ignore_non_variants: true}. > - {orig_names: [AD], name: AD, description: ""##FORMAT=<ID=AD,Number=R,Type=Integer,Description=\""Allelic depths for the ref and alt alleles in the order listed\"">"", type: int, number: alleles, default_type: zero, count: 0, combi_method: min, ignore_non_variants: false}. > - {orig_names: [GQ], name: GQ, description: ""##FORMAT=<ID=GQ,Number=1,Type=Integer,Description=\""Genotype Quality\"">"", type: int, number: basic, default_type: missing, count: 1, combi_method: min, ignore_non_variants: true}. > - {orig_names: [PL], name: PL, description: ""##FORMAT=<ID=PL,Number=G,Type=Integer,Description=\""Phred-scaled genotype Likelihoods\"">"", type: int, number: genotype, default_type: missing, count: 0, combi_method: missing, ignore_non_variants: true}. > [71420] [2024-04-03 09:10:42.191] [GLnexus] [info] config CRC32C = 2932316105. > [71420] [2024-04-03 09:10:42.191] [GLnexus] [info] init database, exemplar_vcf=/public/home/zenglingsen/01.data/01.ONT_data/01.ONT_20X_fastq_SNP_calling/03.pepper/01.gvcf/gvcf_file/AW.new.excluded.mnps.gvcf.gz. > [71420] [2024-04-03 09:10:42.611] [GLnexus] [info] Initialized GLnexus database in GLnexus.DB. > [71420] [2024-04-03 09:10:42.611] [GLnexus] [info] bucket size: 30000. > [71420] [2024-04-03 09:10:42.612] [GLnexus] [info] contigs: NC_010443.5 NC_010444.4 NC_010445.4 NC_010446.5 NC_010447.5 NC_010448.4 NC_010449.5 NC_010450.4 NC_010451.4 NC_010452.4 NC_010453.5 NC_010454.4 NC_010455.5 NC_010456.5 NC_010457.5 NC_010458.4 NC_010459.5 NC_010460.4 NC_010461.5 NC_010462.3 NW_018084777.1 NW_018084778.1 NW_018084779.1 NW_018084780.1 NW_018084781.1 NW_018084782.1 N",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/778
https://github.com/google/deepvariant/issues/778:12457,performance,load,load,12457,"018085313.1 NW_018085314.1 NW_018085315.1 NW_018085316.1 NW_018085317.1 NW_018085318.1 NW_018085319.1 NW_018085320.1 NW_018085321.1 NW_018085322.1 NW_018085323.1 NW_018085324.1 NW_018085325.1 NW_018085326.1 NW_018085327.1 NW_018085328.1 NW_018085329.1 NW_018085330.1 NW_018085331.1 NW_018085332.1 NW_018085333.1 NW_018085334.1 NW_018085335.1 NW_018085336.1 NW_018085337.1 NW_018085338.1 NW_018085339.1 NW_018085340.1 NW_018085341.1 NW_018085342.1 NW_018085343.1 NW_018085344.1 NW_018085345.1 NW_018085346.1 NW_018085347.1 NW_018085348.1 NW_018085349.1 NW_018085350.1 NW_018085351.1 NW_018085352.1 NW_018085353.1 NW_018085354.1 NW_018085355.1 NW_018085356.1 NW_018085357.1 NW_018085358.1 NW_018085359.1 NW_018085360.1 NW_018085361.1 NW_018085362.1 NW_018085363.1 NW_018085364.1 NW_018085365.1 NW_018085366.1 NW_018085367.1 NW_018085368.1 NC_000845.1. > [71420] [2024-04-03 09:10:42.642] [GLnexus] [info] db_get_contigs GLnexus.DB. > [71420] [2024-04-03 09:10:42.789] [GLnexus] [info] Beginning bulk load with no range filter. > [71420] [2024-04-03 10:09:37.111] [GLnexus] [info] Loaded 5 datasets with 5 samples; 846365851832 bytes in 8659464665 BCF records (882 duplicate) in 414215 buckets. Bucket max 2856376 bytes, 28997 records. 0 BCF records skipped due to caller-specific exceptions. > [71420] [2024-04-03 10:09:37.141] [GLnexus] [info] Created sample set *@5. > [71420] [2024-04-03 10:09:37.142] [GLnexus] [info] Flushing database... > [71420] [2024-04-03 10:11:17.432] [GLnexus] [info] Bulk load complete! > [71420] [2024-04-03 10:11:17.482] [GLnexus] [warning] Processing full length of 613 contigs, as no --bed was provided. Providing a BED file with regions of interest, if applicable, can speed this up. > [71420] [2024-04-03 10:11:17.509] [GLnexus] [info] found sample set *@5. > [71420] [2024-04-03 10:11:17.509] [GLnexus] [info] discovering alleles in 613 range(s) on 126 threads. > [71420] [2024-04-03 10:17:12.989] [GLnexus] [info] discovered 3689057 alleles. > [71420] [2024-04-03 1",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/778
https://github.com/google/deepvariant/issues/778:12537,performance,Load,Loaded,12537,"318.1 NW_018085319.1 NW_018085320.1 NW_018085321.1 NW_018085322.1 NW_018085323.1 NW_018085324.1 NW_018085325.1 NW_018085326.1 NW_018085327.1 NW_018085328.1 NW_018085329.1 NW_018085330.1 NW_018085331.1 NW_018085332.1 NW_018085333.1 NW_018085334.1 NW_018085335.1 NW_018085336.1 NW_018085337.1 NW_018085338.1 NW_018085339.1 NW_018085340.1 NW_018085341.1 NW_018085342.1 NW_018085343.1 NW_018085344.1 NW_018085345.1 NW_018085346.1 NW_018085347.1 NW_018085348.1 NW_018085349.1 NW_018085350.1 NW_018085351.1 NW_018085352.1 NW_018085353.1 NW_018085354.1 NW_018085355.1 NW_018085356.1 NW_018085357.1 NW_018085358.1 NW_018085359.1 NW_018085360.1 NW_018085361.1 NW_018085362.1 NW_018085363.1 NW_018085364.1 NW_018085365.1 NW_018085366.1 NW_018085367.1 NW_018085368.1 NC_000845.1. > [71420] [2024-04-03 09:10:42.642] [GLnexus] [info] db_get_contigs GLnexus.DB. > [71420] [2024-04-03 09:10:42.789] [GLnexus] [info] Beginning bulk load with no range filter. > [71420] [2024-04-03 10:09:37.111] [GLnexus] [info] Loaded 5 datasets with 5 samples; 846365851832 bytes in 8659464665 BCF records (882 duplicate) in 414215 buckets. Bucket max 2856376 bytes, 28997 records. 0 BCF records skipped due to caller-specific exceptions. > [71420] [2024-04-03 10:09:37.141] [GLnexus] [info] Created sample set *@5. > [71420] [2024-04-03 10:09:37.142] [GLnexus] [info] Flushing database... > [71420] [2024-04-03 10:11:17.432] [GLnexus] [info] Bulk load complete! > [71420] [2024-04-03 10:11:17.482] [GLnexus] [warning] Processing full length of 613 contigs, as no --bed was provided. Providing a BED file with regions of interest, if applicable, can speed this up. > [71420] [2024-04-03 10:11:17.509] [GLnexus] [info] found sample set *@5. > [71420] [2024-04-03 10:11:17.509] [GLnexus] [info] discovering alleles in 613 range(s) on 126 threads. > [71420] [2024-04-03 10:17:12.989] [GLnexus] [info] discovered 3689057 alleles. > [71420] [2024-04-03 10:17:15.093] [GLnexus] [info] unified to 159191 sites cleanly with 159684 ALT all",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/778
https://github.com/google/deepvariant/issues/778:12958,performance,load,load,12958,"346.1 NW_018085347.1 NW_018085348.1 NW_018085349.1 NW_018085350.1 NW_018085351.1 NW_018085352.1 NW_018085353.1 NW_018085354.1 NW_018085355.1 NW_018085356.1 NW_018085357.1 NW_018085358.1 NW_018085359.1 NW_018085360.1 NW_018085361.1 NW_018085362.1 NW_018085363.1 NW_018085364.1 NW_018085365.1 NW_018085366.1 NW_018085367.1 NW_018085368.1 NC_000845.1. > [71420] [2024-04-03 09:10:42.642] [GLnexus] [info] db_get_contigs GLnexus.DB. > [71420] [2024-04-03 09:10:42.789] [GLnexus] [info] Beginning bulk load with no range filter. > [71420] [2024-04-03 10:09:37.111] [GLnexus] [info] Loaded 5 datasets with 5 samples; 846365851832 bytes in 8659464665 BCF records (882 duplicate) in 414215 buckets. Bucket max 2856376 bytes, 28997 records. 0 BCF records skipped due to caller-specific exceptions. > [71420] [2024-04-03 10:09:37.141] [GLnexus] [info] Created sample set *@5. > [71420] [2024-04-03 10:09:37.142] [GLnexus] [info] Flushing database... > [71420] [2024-04-03 10:11:17.432] [GLnexus] [info] Bulk load complete! > [71420] [2024-04-03 10:11:17.482] [GLnexus] [warning] Processing full length of 613 contigs, as no --bed was provided. Providing a BED file with regions of interest, if applicable, can speed this up. > [71420] [2024-04-03 10:11:17.509] [GLnexus] [info] found sample set *@5. > [71420] [2024-04-03 10:11:17.509] [GLnexus] [info] discovering alleles in 613 range(s) on 126 threads. > [71420] [2024-04-03 10:17:12.989] [GLnexus] [info] discovered 3689057 alleles. > [71420] [2024-04-03 10:17:15.093] [GLnexus] [info] unified to 159191 sites cleanly with 159684 ALT alleles. 1 ALT alleles were additionally included in monoallelic sites and 1704795 were filtered out on quality thresholds. > [71420] [2024-04-03 10:17:15.093] [GLnexus] [info] Finishing database compaction... > [71420] [2024-04-03 10:17:17.832] [GLnexus] [info] genotyping 159191 sites; sample set = *@5 mem_budget = 0 threads = 128. > [71420] [2024-04-03 10:19:30.901] [GLnexus] [info] genotyping complete! > [71420] [202",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/778
https://github.com/google/deepvariant/issues/778:343,safety,input,input,343,"Hi @AndrewCarroll ,. I followed the instructions to merge gvcf file into a final vcf via GLnexus with the default parameters like this:. `singularity exec glnexus.sif glnexus_cli --config DeepVariantWGS $gvcf_path/*.gvcf.gz > ${output_bcf}`. But it only output 62409 SNPs in the final vcf file (pepper.merged.glnexus.vcf.gz 6.8M), there are 5 input gvcf files (each of one is about 11GB, the sample is from the whole genome of pig). . the below is the log from GLnexus. > INFO: Convert SIF file to sandbox... > WARNING: underlay of /etc/localtime required more than 50 (77) bind mounts. > [71420] [2024-04-03 09:10:42.182] [GLnexus] [info] glnexus_cli release v1.4.1-0-g68e25e5 Aug 13 2021. > [71420] [2024-04-03 09:10:42.182] [GLnexus] [info] detected jemalloc 5.2.1-0-gea6b3e973b477b8061e0076bb257dbd7f3faa756. > [71420] [2024-04-03 09:10:42.183] [GLnexus] [info] Loading config preset DeepVariantWGS. > [71420] [2024-04-03 09:10:42.190] [GLnexus] [info] config:. > unifier_config:. > drop_filtered: false. > min_allele_copy_number: 1. > min_AQ1: 10. > min_AQ2: 10. > min_GQ: 0. > max_alleles_per_site: 32. > monoallelic_sites_for_lost_alleles: true. > preference: common. > genotyper_config:. > revise_genotypes: true. > min_assumed_allele_frequency: 9.99999975e-05. > snv_prior_calibration: 0.600000024. > indel_prior_calibration: 0.449999988. > required_dp: 0. > allow_partial_data: true. > allele_dp_format: AD. > ref_dp_format: MIN_DP. > output_residuals: false. > more_PL: true. > squeeze: false. > trim_uncalled_alleles: true. > top_two_half_calls: false. > output_format: BCF. > liftover_fields:. > - {orig_names: [MIN_DP, DP], name: DP, description: ""##FORMAT=<ID=DP,Number=1,Type=Integer,Description=\""Approximate read depth (reads with MQ=255 or with bad mates are filtered)\"">"", type: int, number: basic, default_type: missing, count: 1, combi_method: min, ignore_non_variants: true}. > - {orig_names: [AD], name: AD, description: ""##FORMAT=<ID=AD,Number=R,Type=Integer,Description=\""Al",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/778
https://github.com/google/deepvariant/issues/778:452,safety,log,log,452,"Hi @AndrewCarroll ,. I followed the instructions to merge gvcf file into a final vcf via GLnexus with the default parameters like this:. `singularity exec glnexus.sif glnexus_cli --config DeepVariantWGS $gvcf_path/*.gvcf.gz > ${output_bcf}`. But it only output 62409 SNPs in the final vcf file (pepper.merged.glnexus.vcf.gz 6.8M), there are 5 input gvcf files (each of one is about 11GB, the sample is from the whole genome of pig). . the below is the log from GLnexus. > INFO: Convert SIF file to sandbox... > WARNING: underlay of /etc/localtime required more than 50 (77) bind mounts. > [71420] [2024-04-03 09:10:42.182] [GLnexus] [info] glnexus_cli release v1.4.1-0-g68e25e5 Aug 13 2021. > [71420] [2024-04-03 09:10:42.182] [GLnexus] [info] detected jemalloc 5.2.1-0-gea6b3e973b477b8061e0076bb257dbd7f3faa756. > [71420] [2024-04-03 09:10:42.183] [GLnexus] [info] Loading config preset DeepVariantWGS. > [71420] [2024-04-03 09:10:42.190] [GLnexus] [info] config:. > unifier_config:. > drop_filtered: false. > min_allele_copy_number: 1. > min_AQ1: 10. > min_AQ2: 10. > min_GQ: 0. > max_alleles_per_site: 32. > monoallelic_sites_for_lost_alleles: true. > preference: common. > genotyper_config:. > revise_genotypes: true. > min_assumed_allele_frequency: 9.99999975e-05. > snv_prior_calibration: 0.600000024. > indel_prior_calibration: 0.449999988. > required_dp: 0. > allow_partial_data: true. > allele_dp_format: AD. > ref_dp_format: MIN_DP. > output_residuals: false. > more_PL: true. > squeeze: false. > trim_uncalled_alleles: true. > top_two_half_calls: false. > output_format: BCF. > liftover_fields:. > - {orig_names: [MIN_DP, DP], name: DP, description: ""##FORMAT=<ID=DP,Number=1,Type=Integer,Description=\""Approximate read depth (reads with MQ=255 or with bad mates are filtered)\"">"", type: int, number: basic, default_type: missing, count: 1, combi_method: min, ignore_non_variants: true}. > - {orig_names: [AD], name: AD, description: ""##FORMAT=<ID=AD,Number=R,Type=Integer,Description=\""Al",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/778
https://github.com/google/deepvariant/issues/778:744,safety,detect,detected,744,"Hi @AndrewCarroll ,. I followed the instructions to merge gvcf file into a final vcf via GLnexus with the default parameters like this:. `singularity exec glnexus.sif glnexus_cli --config DeepVariantWGS $gvcf_path/*.gvcf.gz > ${output_bcf}`. But it only output 62409 SNPs in the final vcf file (pepper.merged.glnexus.vcf.gz 6.8M), there are 5 input gvcf files (each of one is about 11GB, the sample is from the whole genome of pig). . the below is the log from GLnexus. > INFO: Convert SIF file to sandbox... > WARNING: underlay of /etc/localtime required more than 50 (77) bind mounts. > [71420] [2024-04-03 09:10:42.182] [GLnexus] [info] glnexus_cli release v1.4.1-0-g68e25e5 Aug 13 2021. > [71420] [2024-04-03 09:10:42.182] [GLnexus] [info] detected jemalloc 5.2.1-0-gea6b3e973b477b8061e0076bb257dbd7f3faa756. > [71420] [2024-04-03 09:10:42.183] [GLnexus] [info] Loading config preset DeepVariantWGS. > [71420] [2024-04-03 09:10:42.190] [GLnexus] [info] config:. > unifier_config:. > drop_filtered: false. > min_allele_copy_number: 1. > min_AQ1: 10. > min_AQ2: 10. > min_GQ: 0. > max_alleles_per_site: 32. > monoallelic_sites_for_lost_alleles: true. > preference: common. > genotyper_config:. > revise_genotypes: true. > min_assumed_allele_frequency: 9.99999975e-05. > snv_prior_calibration: 0.600000024. > indel_prior_calibration: 0.449999988. > required_dp: 0. > allow_partial_data: true. > allele_dp_format: AD. > ref_dp_format: MIN_DP. > output_residuals: false. > more_PL: true. > squeeze: false. > trim_uncalled_alleles: true. > top_two_half_calls: false. > output_format: BCF. > liftover_fields:. > - {orig_names: [MIN_DP, DP], name: DP, description: ""##FORMAT=<ID=DP,Number=1,Type=Integer,Description=\""Approximate read depth (reads with MQ=255 or with bad mates are filtered)\"">"", type: int, number: basic, default_type: missing, count: 1, combi_method: min, ignore_non_variants: true}. > - {orig_names: [AD], name: AD, description: ""##FORMAT=<ID=AD,Number=R,Type=Integer,Description=\""Al",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/778
https://github.com/google/deepvariant/issues/778:12737,safety,except,exceptions,12737,"W_018085332.1 NW_018085333.1 NW_018085334.1 NW_018085335.1 NW_018085336.1 NW_018085337.1 NW_018085338.1 NW_018085339.1 NW_018085340.1 NW_018085341.1 NW_018085342.1 NW_018085343.1 NW_018085344.1 NW_018085345.1 NW_018085346.1 NW_018085347.1 NW_018085348.1 NW_018085349.1 NW_018085350.1 NW_018085351.1 NW_018085352.1 NW_018085353.1 NW_018085354.1 NW_018085355.1 NW_018085356.1 NW_018085357.1 NW_018085358.1 NW_018085359.1 NW_018085360.1 NW_018085361.1 NW_018085362.1 NW_018085363.1 NW_018085364.1 NW_018085365.1 NW_018085366.1 NW_018085367.1 NW_018085368.1 NC_000845.1. > [71420] [2024-04-03 09:10:42.642] [GLnexus] [info] db_get_contigs GLnexus.DB. > [71420] [2024-04-03 09:10:42.789] [GLnexus] [info] Beginning bulk load with no range filter. > [71420] [2024-04-03 10:09:37.111] [GLnexus] [info] Loaded 5 datasets with 5 samples; 846365851832 bytes in 8659464665 BCF records (882 duplicate) in 414215 buckets. Bucket max 2856376 bytes, 28997 records. 0 BCF records skipped due to caller-specific exceptions. > [71420] [2024-04-03 10:09:37.141] [GLnexus] [info] Created sample set *@5. > [71420] [2024-04-03 10:09:37.142] [GLnexus] [info] Flushing database... > [71420] [2024-04-03 10:11:17.432] [GLnexus] [info] Bulk load complete! > [71420] [2024-04-03 10:11:17.482] [GLnexus] [warning] Processing full length of 613 contigs, as no --bed was provided. Providing a BED file with regions of interest, if applicable, can speed this up. > [71420] [2024-04-03 10:11:17.509] [GLnexus] [info] found sample set *@5. > [71420] [2024-04-03 10:11:17.509] [GLnexus] [info] discovering alleles in 613 range(s) on 126 threads. > [71420] [2024-04-03 10:17:12.989] [GLnexus] [info] discovered 3689057 alleles. > [71420] [2024-04-03 10:17:15.093] [GLnexus] [info] unified to 159191 sites cleanly with 159684 ALT alleles. 1 ALT alleles were additionally included in monoallelic sites and 1704795 were filtered out on quality thresholds. > [71420] [2024-04-03 10:17:15.093] [GLnexus] [info] Finishing database compactio",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/778
https://github.com/google/deepvariant/issues/778:12963,safety,compl,complete,12963,"W_018085347.1 NW_018085348.1 NW_018085349.1 NW_018085350.1 NW_018085351.1 NW_018085352.1 NW_018085353.1 NW_018085354.1 NW_018085355.1 NW_018085356.1 NW_018085357.1 NW_018085358.1 NW_018085359.1 NW_018085360.1 NW_018085361.1 NW_018085362.1 NW_018085363.1 NW_018085364.1 NW_018085365.1 NW_018085366.1 NW_018085367.1 NW_018085368.1 NC_000845.1. > [71420] [2024-04-03 09:10:42.642] [GLnexus] [info] db_get_contigs GLnexus.DB. > [71420] [2024-04-03 09:10:42.789] [GLnexus] [info] Beginning bulk load with no range filter. > [71420] [2024-04-03 10:09:37.111] [GLnexus] [info] Loaded 5 datasets with 5 samples; 846365851832 bytes in 8659464665 BCF records (882 duplicate) in 414215 buckets. Bucket max 2856376 bytes, 28997 records. 0 BCF records skipped due to caller-specific exceptions. > [71420] [2024-04-03 10:09:37.141] [GLnexus] [info] Created sample set *@5. > [71420] [2024-04-03 10:09:37.142] [GLnexus] [info] Flushing database... > [71420] [2024-04-03 10:11:17.432] [GLnexus] [info] Bulk load complete! > [71420] [2024-04-03 10:11:17.482] [GLnexus] [warning] Processing full length of 613 contigs, as no --bed was provided. Providing a BED file with regions of interest, if applicable, can speed this up. > [71420] [2024-04-03 10:11:17.509] [GLnexus] [info] found sample set *@5. > [71420] [2024-04-03 10:11:17.509] [GLnexus] [info] discovering alleles in 613 range(s) on 126 threads. > [71420] [2024-04-03 10:17:12.989] [GLnexus] [info] discovered 3689057 alleles. > [71420] [2024-04-03 10:17:15.093] [GLnexus] [info] unified to 159191 sites cleanly with 159684 ALT alleles. 1 ALT alleles were additionally included in monoallelic sites and 1704795 were filtered out on quality thresholds. > [71420] [2024-04-03 10:17:15.093] [GLnexus] [info] Finishing database compaction... > [71420] [2024-04-03 10:17:17.832] [GLnexus] [info] genotyping 159191 sites; sample set = *@5 mem_budget = 0 threads = 128. > [71420] [2024-04-03 10:19:30.901] [GLnexus] [info] genotyping complete! > [71420] [2024-04-03",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/778
https://github.com/google/deepvariant/issues/778:13936,safety,compl,complete,13936,"5361.1 NW_018085362.1 NW_018085363.1 NW_018085364.1 NW_018085365.1 NW_018085366.1 NW_018085367.1 NW_018085368.1 NC_000845.1. > [71420] [2024-04-03 09:10:42.642] [GLnexus] [info] db_get_contigs GLnexus.DB. > [71420] [2024-04-03 09:10:42.789] [GLnexus] [info] Beginning bulk load with no range filter. > [71420] [2024-04-03 10:09:37.111] [GLnexus] [info] Loaded 5 datasets with 5 samples; 846365851832 bytes in 8659464665 BCF records (882 duplicate) in 414215 buckets. Bucket max 2856376 bytes, 28997 records. 0 BCF records skipped due to caller-specific exceptions. > [71420] [2024-04-03 10:09:37.141] [GLnexus] [info] Created sample set *@5. > [71420] [2024-04-03 10:09:37.142] [GLnexus] [info] Flushing database... > [71420] [2024-04-03 10:11:17.432] [GLnexus] [info] Bulk load complete! > [71420] [2024-04-03 10:11:17.482] [GLnexus] [warning] Processing full length of 613 contigs, as no --bed was provided. Providing a BED file with regions of interest, if applicable, can speed this up. > [71420] [2024-04-03 10:11:17.509] [GLnexus] [info] found sample set *@5. > [71420] [2024-04-03 10:11:17.509] [GLnexus] [info] discovering alleles in 613 range(s) on 126 threads. > [71420] [2024-04-03 10:17:12.989] [GLnexus] [info] discovered 3689057 alleles. > [71420] [2024-04-03 10:17:15.093] [GLnexus] [info] unified to 159191 sites cleanly with 159684 ALT alleles. 1 ALT alleles were additionally included in monoallelic sites and 1704795 were filtered out on quality thresholds. > [71420] [2024-04-03 10:17:15.093] [GLnexus] [info] Finishing database compaction... > [71420] [2024-04-03 10:17:17.832] [GLnexus] [info] genotyping 159191 sites; sample set = *@5 mem_budget = 0 threads = 128. > [71420] [2024-04-03 10:19:30.901] [GLnexus] [info] genotyping complete! > [71420] [2024-04-03 10:19:30.917] [GLnexus] [info] worker threads were cumulatively stalled for 169810ms. > [71420] [2024-04-03 10:19:30.917] [GLnexus] [info] Num BCF records read 5557229 query hits 1178413. > INFO: Cleaning up image...",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/778
https://github.com/google/deepvariant/issues/778:452,security,log,log,452,"Hi @AndrewCarroll ,. I followed the instructions to merge gvcf file into a final vcf via GLnexus with the default parameters like this:. `singularity exec glnexus.sif glnexus_cli --config DeepVariantWGS $gvcf_path/*.gvcf.gz > ${output_bcf}`. But it only output 62409 SNPs in the final vcf file (pepper.merged.glnexus.vcf.gz 6.8M), there are 5 input gvcf files (each of one is about 11GB, the sample is from the whole genome of pig). . the below is the log from GLnexus. > INFO: Convert SIF file to sandbox... > WARNING: underlay of /etc/localtime required more than 50 (77) bind mounts. > [71420] [2024-04-03 09:10:42.182] [GLnexus] [info] glnexus_cli release v1.4.1-0-g68e25e5 Aug 13 2021. > [71420] [2024-04-03 09:10:42.182] [GLnexus] [info] detected jemalloc 5.2.1-0-gea6b3e973b477b8061e0076bb257dbd7f3faa756. > [71420] [2024-04-03 09:10:42.183] [GLnexus] [info] Loading config preset DeepVariantWGS. > [71420] [2024-04-03 09:10:42.190] [GLnexus] [info] config:. > unifier_config:. > drop_filtered: false. > min_allele_copy_number: 1. > min_AQ1: 10. > min_AQ2: 10. > min_GQ: 0. > max_alleles_per_site: 32. > monoallelic_sites_for_lost_alleles: true. > preference: common. > genotyper_config:. > revise_genotypes: true. > min_assumed_allele_frequency: 9.99999975e-05. > snv_prior_calibration: 0.600000024. > indel_prior_calibration: 0.449999988. > required_dp: 0. > allow_partial_data: true. > allele_dp_format: AD. > ref_dp_format: MIN_DP. > output_residuals: false. > more_PL: true. > squeeze: false. > trim_uncalled_alleles: true. > top_two_half_calls: false. > output_format: BCF. > liftover_fields:. > - {orig_names: [MIN_DP, DP], name: DP, description: ""##FORMAT=<ID=DP,Number=1,Type=Integer,Description=\""Approximate read depth (reads with MQ=255 or with bad mates are filtered)\"">"", type: int, number: basic, default_type: missing, count: 1, combi_method: min, ignore_non_variants: true}. > - {orig_names: [AD], name: AD, description: ""##FORMAT=<ID=AD,Number=R,Type=Integer,Description=\""Al",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/778
https://github.com/google/deepvariant/issues/778:498,security,sandbox,sandbox,498,"Hi @AndrewCarroll ,. I followed the instructions to merge gvcf file into a final vcf via GLnexus with the default parameters like this:. `singularity exec glnexus.sif glnexus_cli --config DeepVariantWGS $gvcf_path/*.gvcf.gz > ${output_bcf}`. But it only output 62409 SNPs in the final vcf file (pepper.merged.glnexus.vcf.gz 6.8M), there are 5 input gvcf files (each of one is about 11GB, the sample is from the whole genome of pig). . the below is the log from GLnexus. > INFO: Convert SIF file to sandbox... > WARNING: underlay of /etc/localtime required more than 50 (77) bind mounts. > [71420] [2024-04-03 09:10:42.182] [GLnexus] [info] glnexus_cli release v1.4.1-0-g68e25e5 Aug 13 2021. > [71420] [2024-04-03 09:10:42.182] [GLnexus] [info] detected jemalloc 5.2.1-0-gea6b3e973b477b8061e0076bb257dbd7f3faa756. > [71420] [2024-04-03 09:10:42.183] [GLnexus] [info] Loading config preset DeepVariantWGS. > [71420] [2024-04-03 09:10:42.190] [GLnexus] [info] config:. > unifier_config:. > drop_filtered: false. > min_allele_copy_number: 1. > min_AQ1: 10. > min_AQ2: 10. > min_GQ: 0. > max_alleles_per_site: 32. > monoallelic_sites_for_lost_alleles: true. > preference: common. > genotyper_config:. > revise_genotypes: true. > min_assumed_allele_frequency: 9.99999975e-05. > snv_prior_calibration: 0.600000024. > indel_prior_calibration: 0.449999988. > required_dp: 0. > allow_partial_data: true. > allele_dp_format: AD. > ref_dp_format: MIN_DP. > output_residuals: false. > more_PL: true. > squeeze: false. > trim_uncalled_alleles: true. > top_two_half_calls: false. > output_format: BCF. > liftover_fields:. > - {orig_names: [MIN_DP, DP], name: DP, description: ""##FORMAT=<ID=DP,Number=1,Type=Integer,Description=\""Approximate read depth (reads with MQ=255 or with bad mates are filtered)\"">"", type: int, number: basic, default_type: missing, count: 1, combi_method: min, ignore_non_variants: true}. > - {orig_names: [AD], name: AD, description: ""##FORMAT=<ID=AD,Number=R,Type=Integer,Description=\""Al",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/778
https://github.com/google/deepvariant/issues/778:744,security,detect,detected,744,"Hi @AndrewCarroll ,. I followed the instructions to merge gvcf file into a final vcf via GLnexus with the default parameters like this:. `singularity exec glnexus.sif glnexus_cli --config DeepVariantWGS $gvcf_path/*.gvcf.gz > ${output_bcf}`. But it only output 62409 SNPs in the final vcf file (pepper.merged.glnexus.vcf.gz 6.8M), there are 5 input gvcf files (each of one is about 11GB, the sample is from the whole genome of pig). . the below is the log from GLnexus. > INFO: Convert SIF file to sandbox... > WARNING: underlay of /etc/localtime required more than 50 (77) bind mounts. > [71420] [2024-04-03 09:10:42.182] [GLnexus] [info] glnexus_cli release v1.4.1-0-g68e25e5 Aug 13 2021. > [71420] [2024-04-03 09:10:42.182] [GLnexus] [info] detected jemalloc 5.2.1-0-gea6b3e973b477b8061e0076bb257dbd7f3faa756. > [71420] [2024-04-03 09:10:42.183] [GLnexus] [info] Loading config preset DeepVariantWGS. > [71420] [2024-04-03 09:10:42.190] [GLnexus] [info] config:. > unifier_config:. > drop_filtered: false. > min_allele_copy_number: 1. > min_AQ1: 10. > min_AQ2: 10. > min_GQ: 0. > max_alleles_per_site: 32. > monoallelic_sites_for_lost_alleles: true. > preference: common. > genotyper_config:. > revise_genotypes: true. > min_assumed_allele_frequency: 9.99999975e-05. > snv_prior_calibration: 0.600000024. > indel_prior_calibration: 0.449999988. > required_dp: 0. > allow_partial_data: true. > allele_dp_format: AD. > ref_dp_format: MIN_DP. > output_residuals: false. > more_PL: true. > squeeze: false. > trim_uncalled_alleles: true. > top_two_half_calls: false. > output_format: BCF. > liftover_fields:. > - {orig_names: [MIN_DP, DP], name: DP, description: ""##FORMAT=<ID=DP,Number=1,Type=Integer,Description=\""Approximate read depth (reads with MQ=255 or with bad mates are filtered)\"">"", type: int, number: basic, default_type: missing, count: 1, combi_method: min, ignore_non_variants: true}. > - {orig_names: [AD], name: AD, description: ""##FORMAT=<ID=AD,Number=R,Type=Integer,Description=\""Al",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/778
https://github.com/google/deepvariant/issues/778:12963,security,compl,complete,12963,"W_018085347.1 NW_018085348.1 NW_018085349.1 NW_018085350.1 NW_018085351.1 NW_018085352.1 NW_018085353.1 NW_018085354.1 NW_018085355.1 NW_018085356.1 NW_018085357.1 NW_018085358.1 NW_018085359.1 NW_018085360.1 NW_018085361.1 NW_018085362.1 NW_018085363.1 NW_018085364.1 NW_018085365.1 NW_018085366.1 NW_018085367.1 NW_018085368.1 NC_000845.1. > [71420] [2024-04-03 09:10:42.642] [GLnexus] [info] db_get_contigs GLnexus.DB. > [71420] [2024-04-03 09:10:42.789] [GLnexus] [info] Beginning bulk load with no range filter. > [71420] [2024-04-03 10:09:37.111] [GLnexus] [info] Loaded 5 datasets with 5 samples; 846365851832 bytes in 8659464665 BCF records (882 duplicate) in 414215 buckets. Bucket max 2856376 bytes, 28997 records. 0 BCF records skipped due to caller-specific exceptions. > [71420] [2024-04-03 10:09:37.141] [GLnexus] [info] Created sample set *@5. > [71420] [2024-04-03 10:09:37.142] [GLnexus] [info] Flushing database... > [71420] [2024-04-03 10:11:17.432] [GLnexus] [info] Bulk load complete! > [71420] [2024-04-03 10:11:17.482] [GLnexus] [warning] Processing full length of 613 contigs, as no --bed was provided. Providing a BED file with regions of interest, if applicable, can speed this up. > [71420] [2024-04-03 10:11:17.509] [GLnexus] [info] found sample set *@5. > [71420] [2024-04-03 10:11:17.509] [GLnexus] [info] discovering alleles in 613 range(s) on 126 threads. > [71420] [2024-04-03 10:17:12.989] [GLnexus] [info] discovered 3689057 alleles. > [71420] [2024-04-03 10:17:15.093] [GLnexus] [info] unified to 159191 sites cleanly with 159684 ALT alleles. 1 ALT alleles were additionally included in monoallelic sites and 1704795 were filtered out on quality thresholds. > [71420] [2024-04-03 10:17:15.093] [GLnexus] [info] Finishing database compaction... > [71420] [2024-04-03 10:17:17.832] [GLnexus] [info] genotyping 159191 sites; sample set = *@5 mem_budget = 0 threads = 128. > [71420] [2024-04-03 10:19:30.901] [GLnexus] [info] genotyping complete! > [71420] [2024-04-03",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/778
https://github.com/google/deepvariant/issues/778:13936,security,compl,complete,13936,"5361.1 NW_018085362.1 NW_018085363.1 NW_018085364.1 NW_018085365.1 NW_018085366.1 NW_018085367.1 NW_018085368.1 NC_000845.1. > [71420] [2024-04-03 09:10:42.642] [GLnexus] [info] db_get_contigs GLnexus.DB. > [71420] [2024-04-03 09:10:42.789] [GLnexus] [info] Beginning bulk load with no range filter. > [71420] [2024-04-03 10:09:37.111] [GLnexus] [info] Loaded 5 datasets with 5 samples; 846365851832 bytes in 8659464665 BCF records (882 duplicate) in 414215 buckets. Bucket max 2856376 bytes, 28997 records. 0 BCF records skipped due to caller-specific exceptions. > [71420] [2024-04-03 10:09:37.141] [GLnexus] [info] Created sample set *@5. > [71420] [2024-04-03 10:09:37.142] [GLnexus] [info] Flushing database... > [71420] [2024-04-03 10:11:17.432] [GLnexus] [info] Bulk load complete! > [71420] [2024-04-03 10:11:17.482] [GLnexus] [warning] Processing full length of 613 contigs, as no --bed was provided. Providing a BED file with regions of interest, if applicable, can speed this up. > [71420] [2024-04-03 10:11:17.509] [GLnexus] [info] found sample set *@5. > [71420] [2024-04-03 10:11:17.509] [GLnexus] [info] discovering alleles in 613 range(s) on 126 threads. > [71420] [2024-04-03 10:17:12.989] [GLnexus] [info] discovered 3689057 alleles. > [71420] [2024-04-03 10:17:15.093] [GLnexus] [info] unified to 159191 sites cleanly with 159684 ALT alleles. 1 ALT alleles were additionally included in monoallelic sites and 1704795 were filtered out on quality thresholds. > [71420] [2024-04-03 10:17:15.093] [GLnexus] [info] Finishing database compaction... > [71420] [2024-04-03 10:17:17.832] [GLnexus] [info] genotyping 159191 sites; sample set = *@5 mem_budget = 0 threads = 128. > [71420] [2024-04-03 10:19:30.901] [GLnexus] [info] genotyping complete! > [71420] [2024-04-03 10:19:30.917] [GLnexus] [info] worker threads were cumulatively stalled for 169810ms. > [71420] [2024-04-03 10:19:30.917] [GLnexus] [info] Num BCF records read 5557229 query hits 1178413. > INFO: Cleaning up image...",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/778
https://github.com/google/deepvariant/issues/778:452,testability,log,log,452,"Hi @AndrewCarroll ,. I followed the instructions to merge gvcf file into a final vcf via GLnexus with the default parameters like this:. `singularity exec glnexus.sif glnexus_cli --config DeepVariantWGS $gvcf_path/*.gvcf.gz > ${output_bcf}`. But it only output 62409 SNPs in the final vcf file (pepper.merged.glnexus.vcf.gz 6.8M), there are 5 input gvcf files (each of one is about 11GB, the sample is from the whole genome of pig). . the below is the log from GLnexus. > INFO: Convert SIF file to sandbox... > WARNING: underlay of /etc/localtime required more than 50 (77) bind mounts. > [71420] [2024-04-03 09:10:42.182] [GLnexus] [info] glnexus_cli release v1.4.1-0-g68e25e5 Aug 13 2021. > [71420] [2024-04-03 09:10:42.182] [GLnexus] [info] detected jemalloc 5.2.1-0-gea6b3e973b477b8061e0076bb257dbd7f3faa756. > [71420] [2024-04-03 09:10:42.183] [GLnexus] [info] Loading config preset DeepVariantWGS. > [71420] [2024-04-03 09:10:42.190] [GLnexus] [info] config:. > unifier_config:. > drop_filtered: false. > min_allele_copy_number: 1. > min_AQ1: 10. > min_AQ2: 10. > min_GQ: 0. > max_alleles_per_site: 32. > monoallelic_sites_for_lost_alleles: true. > preference: common. > genotyper_config:. > revise_genotypes: true. > min_assumed_allele_frequency: 9.99999975e-05. > snv_prior_calibration: 0.600000024. > indel_prior_calibration: 0.449999988. > required_dp: 0. > allow_partial_data: true. > allele_dp_format: AD. > ref_dp_format: MIN_DP. > output_residuals: false. > more_PL: true. > squeeze: false. > trim_uncalled_alleles: true. > top_two_half_calls: false. > output_format: BCF. > liftover_fields:. > - {orig_names: [MIN_DP, DP], name: DP, description: ""##FORMAT=<ID=DP,Number=1,Type=Integer,Description=\""Approximate read depth (reads with MQ=255 or with bad mates are filtered)\"">"", type: int, number: basic, default_type: missing, count: 1, combi_method: min, ignore_non_variants: true}. > - {orig_names: [AD], name: AD, description: ""##FORMAT=<ID=AD,Number=R,Type=Integer,Description=\""Al",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/778
https://github.com/google/deepvariant/issues/778:343,usability,input,input,343,"Hi @AndrewCarroll ,. I followed the instructions to merge gvcf file into a final vcf via GLnexus with the default parameters like this:. `singularity exec glnexus.sif glnexus_cli --config DeepVariantWGS $gvcf_path/*.gvcf.gz > ${output_bcf}`. But it only output 62409 SNPs in the final vcf file (pepper.merged.glnexus.vcf.gz 6.8M), there are 5 input gvcf files (each of one is about 11GB, the sample is from the whole genome of pig). . the below is the log from GLnexus. > INFO: Convert SIF file to sandbox... > WARNING: underlay of /etc/localtime required more than 50 (77) bind mounts. > [71420] [2024-04-03 09:10:42.182] [GLnexus] [info] glnexus_cli release v1.4.1-0-g68e25e5 Aug 13 2021. > [71420] [2024-04-03 09:10:42.182] [GLnexus] [info] detected jemalloc 5.2.1-0-gea6b3e973b477b8061e0076bb257dbd7f3faa756. > [71420] [2024-04-03 09:10:42.183] [GLnexus] [info] Loading config preset DeepVariantWGS. > [71420] [2024-04-03 09:10:42.190] [GLnexus] [info] config:. > unifier_config:. > drop_filtered: false. > min_allele_copy_number: 1. > min_AQ1: 10. > min_AQ2: 10. > min_GQ: 0. > max_alleles_per_site: 32. > monoallelic_sites_for_lost_alleles: true. > preference: common. > genotyper_config:. > revise_genotypes: true. > min_assumed_allele_frequency: 9.99999975e-05. > snv_prior_calibration: 0.600000024. > indel_prior_calibration: 0.449999988. > required_dp: 0. > allow_partial_data: true. > allele_dp_format: AD. > ref_dp_format: MIN_DP. > output_residuals: false. > more_PL: true. > squeeze: false. > trim_uncalled_alleles: true. > top_two_half_calls: false. > output_format: BCF. > liftover_fields:. > - {orig_names: [MIN_DP, DP], name: DP, description: ""##FORMAT=<ID=DP,Number=1,Type=Integer,Description=\""Approximate read depth (reads with MQ=255 or with bad mates are filtered)\"">"", type: int, number: basic, default_type: missing, count: 1, combi_method: min, ignore_non_variants: true}. > - {orig_names: [AD], name: AD, description: ""##FORMAT=<ID=AD,Number=R,Type=Integer,Description=\""Al",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/778
https://github.com/google/deepvariant/issues/778:1155,usability,prefer,preference,1155,"us.sif glnexus_cli --config DeepVariantWGS $gvcf_path/*.gvcf.gz > ${output_bcf}`. But it only output 62409 SNPs in the final vcf file (pepper.merged.glnexus.vcf.gz 6.8M), there are 5 input gvcf files (each of one is about 11GB, the sample is from the whole genome of pig). . the below is the log from GLnexus. > INFO: Convert SIF file to sandbox... > WARNING: underlay of /etc/localtime required more than 50 (77) bind mounts. > [71420] [2024-04-03 09:10:42.182] [GLnexus] [info] glnexus_cli release v1.4.1-0-g68e25e5 Aug 13 2021. > [71420] [2024-04-03 09:10:42.182] [GLnexus] [info] detected jemalloc 5.2.1-0-gea6b3e973b477b8061e0076bb257dbd7f3faa756. > [71420] [2024-04-03 09:10:42.183] [GLnexus] [info] Loading config preset DeepVariantWGS. > [71420] [2024-04-03 09:10:42.190] [GLnexus] [info] config:. > unifier_config:. > drop_filtered: false. > min_allele_copy_number: 1. > min_AQ1: 10. > min_AQ2: 10. > min_GQ: 0. > max_alleles_per_site: 32. > monoallelic_sites_for_lost_alleles: true. > preference: common. > genotyper_config:. > revise_genotypes: true. > min_assumed_allele_frequency: 9.99999975e-05. > snv_prior_calibration: 0.600000024. > indel_prior_calibration: 0.449999988. > required_dp: 0. > allow_partial_data: true. > allele_dp_format: AD. > ref_dp_format: MIN_DP. > output_residuals: false. > more_PL: true. > squeeze: false. > trim_uncalled_alleles: true. > top_two_half_calls: false. > output_format: BCF. > liftover_fields:. > - {orig_names: [MIN_DP, DP], name: DP, description: ""##FORMAT=<ID=DP,Number=1,Type=Integer,Description=\""Approximate read depth (reads with MQ=255 or with bad mates are filtered)\"">"", type: int, number: basic, default_type: missing, count: 1, combi_method: min, ignore_non_variants: true}. > - {orig_names: [AD], name: AD, description: ""##FORMAT=<ID=AD,Number=R,Type=Integer,Description=\""Allelic depths for the ref and alt alleles in the order listed\"">"", type: int, number: alleles, default_type: zero, count: 0, combi_method: min, ignore_non_varian",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/778
https://github.com/google/deepvariant/issues/778:13303,usability,discov,discovering,13303,"5361.1 NW_018085362.1 NW_018085363.1 NW_018085364.1 NW_018085365.1 NW_018085366.1 NW_018085367.1 NW_018085368.1 NC_000845.1. > [71420] [2024-04-03 09:10:42.642] [GLnexus] [info] db_get_contigs GLnexus.DB. > [71420] [2024-04-03 09:10:42.789] [GLnexus] [info] Beginning bulk load with no range filter. > [71420] [2024-04-03 10:09:37.111] [GLnexus] [info] Loaded 5 datasets with 5 samples; 846365851832 bytes in 8659464665 BCF records (882 duplicate) in 414215 buckets. Bucket max 2856376 bytes, 28997 records. 0 BCF records skipped due to caller-specific exceptions. > [71420] [2024-04-03 10:09:37.141] [GLnexus] [info] Created sample set *@5. > [71420] [2024-04-03 10:09:37.142] [GLnexus] [info] Flushing database... > [71420] [2024-04-03 10:11:17.432] [GLnexus] [info] Bulk load complete! > [71420] [2024-04-03 10:11:17.482] [GLnexus] [warning] Processing full length of 613 contigs, as no --bed was provided. Providing a BED file with regions of interest, if applicable, can speed this up. > [71420] [2024-04-03 10:11:17.509] [GLnexus] [info] found sample set *@5. > [71420] [2024-04-03 10:11:17.509] [GLnexus] [info] discovering alleles in 613 range(s) on 126 threads. > [71420] [2024-04-03 10:17:12.989] [GLnexus] [info] discovered 3689057 alleles. > [71420] [2024-04-03 10:17:15.093] [GLnexus] [info] unified to 159191 sites cleanly with 159684 ALT alleles. 1 ALT alleles were additionally included in monoallelic sites and 1704795 were filtered out on quality thresholds. > [71420] [2024-04-03 10:17:15.093] [GLnexus] [info] Finishing database compaction... > [71420] [2024-04-03 10:17:17.832] [GLnexus] [info] genotyping 159191 sites; sample set = *@5 mem_budget = 0 threads = 128. > [71420] [2024-04-03 10:19:30.901] [GLnexus] [info] genotyping complete! > [71420] [2024-04-03 10:19:30.917] [GLnexus] [info] worker threads were cumulatively stalled for 169810ms. > [71420] [2024-04-03 10:19:30.917] [GLnexus] [info] Num BCF records read 5557229 query hits 1178413. > INFO: Cleaning up image...",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/778
https://github.com/google/deepvariant/issues/778:13408,usability,discov,discovered,13408,"5361.1 NW_018085362.1 NW_018085363.1 NW_018085364.1 NW_018085365.1 NW_018085366.1 NW_018085367.1 NW_018085368.1 NC_000845.1. > [71420] [2024-04-03 09:10:42.642] [GLnexus] [info] db_get_contigs GLnexus.DB. > [71420] [2024-04-03 09:10:42.789] [GLnexus] [info] Beginning bulk load with no range filter. > [71420] [2024-04-03 10:09:37.111] [GLnexus] [info] Loaded 5 datasets with 5 samples; 846365851832 bytes in 8659464665 BCF records (882 duplicate) in 414215 buckets. Bucket max 2856376 bytes, 28997 records. 0 BCF records skipped due to caller-specific exceptions. > [71420] [2024-04-03 10:09:37.141] [GLnexus] [info] Created sample set *@5. > [71420] [2024-04-03 10:09:37.142] [GLnexus] [info] Flushing database... > [71420] [2024-04-03 10:11:17.432] [GLnexus] [info] Bulk load complete! > [71420] [2024-04-03 10:11:17.482] [GLnexus] [warning] Processing full length of 613 contigs, as no --bed was provided. Providing a BED file with regions of interest, if applicable, can speed this up. > [71420] [2024-04-03 10:11:17.509] [GLnexus] [info] found sample set *@5. > [71420] [2024-04-03 10:11:17.509] [GLnexus] [info] discovering alleles in 613 range(s) on 126 threads. > [71420] [2024-04-03 10:17:12.989] [GLnexus] [info] discovered 3689057 alleles. > [71420] [2024-04-03 10:17:15.093] [GLnexus] [info] unified to 159191 sites cleanly with 159684 ALT alleles. 1 ALT alleles were additionally included in monoallelic sites and 1704795 were filtered out on quality thresholds. > [71420] [2024-04-03 10:17:15.093] [GLnexus] [info] Finishing database compaction... > [71420] [2024-04-03 10:17:17.832] [GLnexus] [info] genotyping 159191 sites; sample set = *@5 mem_budget = 0 threads = 128. > [71420] [2024-04-03 10:19:30.901] [GLnexus] [info] genotyping complete! > [71420] [2024-04-03 10:19:30.917] [GLnexus] [info] worker threads were cumulatively stalled for 169810ms. > [71420] [2024-04-03 10:19:30.917] [GLnexus] [info] Num BCF records read 5557229 query hits 1178413. > INFO: Cleaning up image...",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/778
https://github.com/google/deepvariant/issues/778:174,availability,recov,recover,174,"Hi @Modernism-01 . For ONT data can you try the merge set DeepVariant_unfiltered. The presets for DeepVariantWGS were determined based on Illumina WGS. I hope that will help recover ONT variants that are too aggressively filtered. If this is not the case, you could please report back here. Thank you,. Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/778
https://github.com/google/deepvariant/issues/778:174,deployability,recov,recover,174,"Hi @Modernism-01 . For ONT data can you try the merge set DeepVariant_unfiltered. The presets for DeepVariantWGS were determined based on Illumina WGS. I hope that will help recover ONT variants that are too aggressively filtered. If this is not the case, you could please report back here. Thank you,. Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/778
https://github.com/google/deepvariant/issues/778:221,integrability,filter,filtered,221,"Hi @Modernism-01 . For ONT data can you try the merge set DeepVariant_unfiltered. The presets for DeepVariantWGS were determined based on Illumina WGS. I hope that will help recover ONT variants that are too aggressively filtered. If this is not the case, you could please report back here. Thank you,. Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/778
https://github.com/google/deepvariant/issues/778:174,reliability,recov,recover,174,"Hi @Modernism-01 . For ONT data can you try the merge set DeepVariant_unfiltered. The presets for DeepVariantWGS were determined based on Illumina WGS. I hope that will help recover ONT variants that are too aggressively filtered. If this is not the case, you could please report back here. Thank you,. Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/778
https://github.com/google/deepvariant/issues/778:174,safety,recov,recover,174,"Hi @Modernism-01 . For ONT data can you try the merge set DeepVariant_unfiltered. The presets for DeepVariantWGS were determined based on Illumina WGS. I hope that will help recover ONT variants that are too aggressively filtered. If this is not the case, you could please report back here. Thank you,. Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/778
https://github.com/google/deepvariant/issues/778:174,security,recov,recover,174,"Hi @Modernism-01 . For ONT data can you try the merge set DeepVariant_unfiltered. The presets for DeepVariantWGS were determined based on Illumina WGS. I hope that will help recover ONT variants that are too aggressively filtered. If this is not the case, you could please report back here. Thank you,. Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/778
https://github.com/google/deepvariant/issues/778:169,usability,help,help,169,"Hi @Modernism-01 . For ONT data can you try the merge set DeepVariant_unfiltered. The presets for DeepVariantWGS were determined based on Illumina WGS. I hope that will help recover ONT variants that are too aggressively filtered. If this is not the case, you could please report back here. Thank you,. Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/778
https://github.com/google/deepvariant/issues/779:87,deployability,pipelin,pipeline-based,87,"Hi @francois-lecoquierre ,. There are a few ways you can find de novo mutations:. 1) A pipeline-based approach can be found here: https://github.com/shlokanegi/denovo_snps. 2) You can use [rtgtools](https://github.com/RealTimeGenomics/rtg-tools) to isolate de novo variants. Here's an example command for the HG002 trio:. ```bash. rtg vcfmerge HG004.vcf.gz HG003.vcf.gz HG002.vcf.gz \. --add-header ""##PEDIGREE=<Child=HG002,Mother=HG003,Father=HG004>"" \. --add-header ""##SAMPLE<ID=HG002,Sex-MALE>"" \. --output HG002_trio.vcf.gz. rtg mendelian -t /path/to/ref.sdf --input HG002_trio.vcf.gz \. --lenient --output-inconsistent trio-non-mendelian.vcf.gz. ```. You can generate the SDF by running:. ```bash. rtg format -o /path/to/ref.sdf /path/to/ref.fasta. ```. This suggestion is from [biostars](https://www.biostars.org/p/329022/) that also lists few other solutions.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/779
https://github.com/google/deepvariant/issues/779:87,integrability,pipelin,pipeline-based,87,"Hi @francois-lecoquierre ,. There are a few ways you can find de novo mutations:. 1) A pipeline-based approach can be found here: https://github.com/shlokanegi/denovo_snps. 2) You can use [rtgtools](https://github.com/RealTimeGenomics/rtg-tools) to isolate de novo variants. Here's an example command for the HG002 trio:. ```bash. rtg vcfmerge HG004.vcf.gz HG003.vcf.gz HG002.vcf.gz \. --add-header ""##PEDIGREE=<Child=HG002,Mother=HG003,Father=HG004>"" \. --add-header ""##SAMPLE<ID=HG002,Sex-MALE>"" \. --output HG002_trio.vcf.gz. rtg mendelian -t /path/to/ref.sdf --input HG002_trio.vcf.gz \. --lenient --output-inconsistent trio-non-mendelian.vcf.gz. ```. You can generate the SDF by running:. ```bash. rtg format -o /path/to/ref.sdf /path/to/ref.fasta. ```. This suggestion is from [biostars](https://www.biostars.org/p/329022/) that also lists few other solutions.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/779
https://github.com/google/deepvariant/issues/779:707,interoperability,format,format,707,"Hi @francois-lecoquierre ,. There are a few ways you can find de novo mutations:. 1) A pipeline-based approach can be found here: https://github.com/shlokanegi/denovo_snps. 2) You can use [rtgtools](https://github.com/RealTimeGenomics/rtg-tools) to isolate de novo variants. Here's an example command for the HG002 trio:. ```bash. rtg vcfmerge HG004.vcf.gz HG003.vcf.gz HG002.vcf.gz \. --add-header ""##PEDIGREE=<Child=HG002,Mother=HG003,Father=HG004>"" \. --add-header ""##SAMPLE<ID=HG002,Sex-MALE>"" \. --output HG002_trio.vcf.gz. rtg mendelian -t /path/to/ref.sdf --input HG002_trio.vcf.gz \. --lenient --output-inconsistent trio-non-mendelian.vcf.gz. ```. You can generate the SDF by running:. ```bash. rtg format -o /path/to/ref.sdf /path/to/ref.fasta. ```. This suggestion is from [biostars](https://www.biostars.org/p/329022/) that also lists few other solutions.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/779
https://github.com/google/deepvariant/issues/779:249,safety,isol,isolate,249,"Hi @francois-lecoquierre ,. There are a few ways you can find de novo mutations:. 1) A pipeline-based approach can be found here: https://github.com/shlokanegi/denovo_snps. 2) You can use [rtgtools](https://github.com/RealTimeGenomics/rtg-tools) to isolate de novo variants. Here's an example command for the HG002 trio:. ```bash. rtg vcfmerge HG004.vcf.gz HG003.vcf.gz HG002.vcf.gz \. --add-header ""##PEDIGREE=<Child=HG002,Mother=HG003,Father=HG004>"" \. --add-header ""##SAMPLE<ID=HG002,Sex-MALE>"" \. --output HG002_trio.vcf.gz. rtg mendelian -t /path/to/ref.sdf --input HG002_trio.vcf.gz \. --lenient --output-inconsistent trio-non-mendelian.vcf.gz. ```. You can generate the SDF by running:. ```bash. rtg format -o /path/to/ref.sdf /path/to/ref.fasta. ```. This suggestion is from [biostars](https://www.biostars.org/p/329022/) that also lists few other solutions.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/779
https://github.com/google/deepvariant/issues/779:565,safety,input,input,565,"Hi @francois-lecoquierre ,. There are a few ways you can find de novo mutations:. 1) A pipeline-based approach can be found here: https://github.com/shlokanegi/denovo_snps. 2) You can use [rtgtools](https://github.com/RealTimeGenomics/rtg-tools) to isolate de novo variants. Here's an example command for the HG002 trio:. ```bash. rtg vcfmerge HG004.vcf.gz HG003.vcf.gz HG002.vcf.gz \. --add-header ""##PEDIGREE=<Child=HG002,Mother=HG003,Father=HG004>"" \. --add-header ""##SAMPLE<ID=HG002,Sex-MALE>"" \. --output HG002_trio.vcf.gz. rtg mendelian -t /path/to/ref.sdf --input HG002_trio.vcf.gz \. --lenient --output-inconsistent trio-non-mendelian.vcf.gz. ```. You can generate the SDF by running:. ```bash. rtg format -o /path/to/ref.sdf /path/to/ref.fasta. ```. This suggestion is from [biostars](https://www.biostars.org/p/329022/) that also lists few other solutions.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/779
https://github.com/google/deepvariant/issues/779:249,security,iso,isolate,249,"Hi @francois-lecoquierre ,. There are a few ways you can find de novo mutations:. 1) A pipeline-based approach can be found here: https://github.com/shlokanegi/denovo_snps. 2) You can use [rtgtools](https://github.com/RealTimeGenomics/rtg-tools) to isolate de novo variants. Here's an example command for the HG002 trio:. ```bash. rtg vcfmerge HG004.vcf.gz HG003.vcf.gz HG002.vcf.gz \. --add-header ""##PEDIGREE=<Child=HG002,Mother=HG003,Father=HG004>"" \. --add-header ""##SAMPLE<ID=HG002,Sex-MALE>"" \. --output HG002_trio.vcf.gz. rtg mendelian -t /path/to/ref.sdf --input HG002_trio.vcf.gz \. --lenient --output-inconsistent trio-non-mendelian.vcf.gz. ```. You can generate the SDF by running:. ```bash. rtg format -o /path/to/ref.sdf /path/to/ref.fasta. ```. This suggestion is from [biostars](https://www.biostars.org/p/329022/) that also lists few other solutions.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/779
https://github.com/google/deepvariant/issues/779:249,testability,isol,isolate,249,"Hi @francois-lecoquierre ,. There are a few ways you can find de novo mutations:. 1) A pipeline-based approach can be found here: https://github.com/shlokanegi/denovo_snps. 2) You can use [rtgtools](https://github.com/RealTimeGenomics/rtg-tools) to isolate de novo variants. Here's an example command for the HG002 trio:. ```bash. rtg vcfmerge HG004.vcf.gz HG003.vcf.gz HG002.vcf.gz \. --add-header ""##PEDIGREE=<Child=HG002,Mother=HG003,Father=HG004>"" \. --add-header ""##SAMPLE<ID=HG002,Sex-MALE>"" \. --output HG002_trio.vcf.gz. rtg mendelian -t /path/to/ref.sdf --input HG002_trio.vcf.gz \. --lenient --output-inconsistent trio-non-mendelian.vcf.gz. ```. You can generate the SDF by running:. ```bash. rtg format -o /path/to/ref.sdf /path/to/ref.fasta. ```. This suggestion is from [biostars](https://www.biostars.org/p/329022/) that also lists few other solutions.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/779
https://github.com/google/deepvariant/issues/779:239,usability,tool,tools,239,"Hi @francois-lecoquierre ,. There are a few ways you can find de novo mutations:. 1) A pipeline-based approach can be found here: https://github.com/shlokanegi/denovo_snps. 2) You can use [rtgtools](https://github.com/RealTimeGenomics/rtg-tools) to isolate de novo variants. Here's an example command for the HG002 trio:. ```bash. rtg vcfmerge HG004.vcf.gz HG003.vcf.gz HG002.vcf.gz \. --add-header ""##PEDIGREE=<Child=HG002,Mother=HG003,Father=HG004>"" \. --add-header ""##SAMPLE<ID=HG002,Sex-MALE>"" \. --output HG002_trio.vcf.gz. rtg mendelian -t /path/to/ref.sdf --input HG002_trio.vcf.gz \. --lenient --output-inconsistent trio-non-mendelian.vcf.gz. ```. You can generate the SDF by running:. ```bash. rtg format -o /path/to/ref.sdf /path/to/ref.fasta. ```. This suggestion is from [biostars](https://www.biostars.org/p/329022/) that also lists few other solutions.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/779
https://github.com/google/deepvariant/issues/779:293,usability,command,command,293,"Hi @francois-lecoquierre ,. There are a few ways you can find de novo mutations:. 1) A pipeline-based approach can be found here: https://github.com/shlokanegi/denovo_snps. 2) You can use [rtgtools](https://github.com/RealTimeGenomics/rtg-tools) to isolate de novo variants. Here's an example command for the HG002 trio:. ```bash. rtg vcfmerge HG004.vcf.gz HG003.vcf.gz HG002.vcf.gz \. --add-header ""##PEDIGREE=<Child=HG002,Mother=HG003,Father=HG004>"" \. --add-header ""##SAMPLE<ID=HG002,Sex-MALE>"" \. --output HG002_trio.vcf.gz. rtg mendelian -t /path/to/ref.sdf --input HG002_trio.vcf.gz \. --lenient --output-inconsistent trio-non-mendelian.vcf.gz. ```. You can generate the SDF by running:. ```bash. rtg format -o /path/to/ref.sdf /path/to/ref.fasta. ```. This suggestion is from [biostars](https://www.biostars.org/p/329022/) that also lists few other solutions.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/779
https://github.com/google/deepvariant/issues/779:565,usability,input,input,565,"Hi @francois-lecoquierre ,. There are a few ways you can find de novo mutations:. 1) A pipeline-based approach can be found here: https://github.com/shlokanegi/denovo_snps. 2) You can use [rtgtools](https://github.com/RealTimeGenomics/rtg-tools) to isolate de novo variants. Here's an example command for the HG002 trio:. ```bash. rtg vcfmerge HG004.vcf.gz HG003.vcf.gz HG002.vcf.gz \. --add-header ""##PEDIGREE=<Child=HG002,Mother=HG003,Father=HG004>"" \. --add-header ""##SAMPLE<ID=HG002,Sex-MALE>"" \. --output HG002_trio.vcf.gz. rtg mendelian -t /path/to/ref.sdf --input HG002_trio.vcf.gz \. --lenient --output-inconsistent trio-non-mendelian.vcf.gz. ```. You can generate the SDF by running:. ```bash. rtg format -o /path/to/ref.sdf /path/to/ref.fasta. ```. This suggestion is from [biostars](https://www.biostars.org/p/329022/) that also lists few other solutions.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/779
https://github.com/google/deepvariant/issues/779:5,usability,close,close,5,I'll close this. Feel free to open if you have more questions.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/779
https://github.com/google/deepvariant/issues/779:11,testability,simpl,simple,11,"There is a simple typo in the `rtg vcfmerge` step above, it should `=` after `Sex`. `--add-header ""##SAMPLE<ID=HG002,Sex=MALE>"" \`",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/779
https://github.com/google/deepvariant/issues/779:11,usability,simpl,simple,11,"There is a simple typo in the `rtg vcfmerge` step above, it should `=` after `Sex`. `--add-header ""##SAMPLE<ID=HG002,Sex=MALE>"" \`",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/779
https://github.com/google/deepvariant/issues/781:24,energy efficiency,model,models,24,"@malonzm1 generally the models should work with alternative references, but we recommend aligning with STAR as this is what we trained on.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/781
https://github.com/google/deepvariant/issues/781:24,security,model,models,24,"@malonzm1 generally the models should work with alternative references, but we recommend aligning with STAR as this is what we trained on.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/781
https://github.com/google/deepvariant/issues/782:14,deployability,version,version,14,It works with version 1.4.0.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/782
https://github.com/google/deepvariant/issues/782:14,integrability,version,version,14,It works with version 1.4.0.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/782
https://github.com/google/deepvariant/issues/782:14,modifiability,version,version,14,It works with version 1.4.0.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/782
https://github.com/google/deepvariant/issues/782:306,energy efficiency,model,model,306,"@malonzm1 it could be that your external environment is problematic. Can you try adding the `--cleanenv ` flag? e.g. . ```. singularity run --cleanenv -B /scratch \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --customized_model=${MODEL_DIR}/model.ckpt \. --ref=""${FASTA_DIR}""/genome.fa \. --reads=""${READ_DIR}""/SRR6006655Aligned.sortedByCoord.out.bam \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --make_examples_extra_args=""split_skip_reads=true,channels=''"" \. --num_shards=${NCPU} #\ **How many cores the `make_examples` step uses. Change it to the number of CPU cores you have.** . ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/782
https://github.com/google/deepvariant/issues/782:686,energy efficiency,core,cores,686,"@malonzm1 it could be that your external environment is problematic. Can you try adding the `--cleanenv ` flag? e.g. . ```. singularity run --cleanenv -B /scratch \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --customized_model=${MODEL_DIR}/model.ckpt \. --ref=""${FASTA_DIR}""/genome.fa \. --reads=""${READ_DIR}""/SRR6006655Aligned.sortedByCoord.out.bam \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --make_examples_extra_args=""split_skip_reads=true,channels=''"" \. --num_shards=${NCPU} #\ **How many cores the `make_examples` step uses. Change it to the number of CPU cores you have.** . ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/782
https://github.com/google/deepvariant/issues/782:750,energy efficiency,CPU,CPU,750,"@malonzm1 it could be that your external environment is problematic. Can you try adding the `--cleanenv ` flag? e.g. . ```. singularity run --cleanenv -B /scratch \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --customized_model=${MODEL_DIR}/model.ckpt \. --ref=""${FASTA_DIR}""/genome.fa \. --reads=""${READ_DIR}""/SRR6006655Aligned.sortedByCoord.out.bam \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --make_examples_extra_args=""split_skip_reads=true,channels=''"" \. --num_shards=${NCPU} #\ **How many cores the `make_examples` step uses. Change it to the number of CPU cores you have.** . ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/782
https://github.com/google/deepvariant/issues/782:754,energy efficiency,core,cores,754,"@malonzm1 it could be that your external environment is problematic. Can you try adding the `--cleanenv ` flag? e.g. . ```. singularity run --cleanenv -B /scratch \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --customized_model=${MODEL_DIR}/model.ckpt \. --ref=""${FASTA_DIR}""/genome.fa \. --reads=""${READ_DIR}""/SRR6006655Aligned.sortedByCoord.out.bam \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --make_examples_extra_args=""split_skip_reads=true,channels=''"" \. --num_shards=${NCPU} #\ **How many cores the `make_examples` step uses. Change it to the number of CPU cores you have.** . ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/782
https://github.com/google/deepvariant/issues/782:750,performance,CPU,CPU,750,"@malonzm1 it could be that your external environment is problematic. Can you try adding the `--cleanenv ` flag? e.g. . ```. singularity run --cleanenv -B /scratch \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --customized_model=${MODEL_DIR}/model.ckpt \. --ref=""${FASTA_DIR}""/genome.fa \. --reads=""${READ_DIR}""/SRR6006655Aligned.sortedByCoord.out.bam \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --make_examples_extra_args=""split_skip_reads=true,channels=''"" \. --num_shards=${NCPU} #\ **How many cores the `make_examples` step uses. Change it to the number of CPU cores you have.** . ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/782
https://github.com/google/deepvariant/issues/782:306,security,model,model,306,"@malonzm1 it could be that your external environment is problematic. Can you try adding the `--cleanenv ` flag? e.g. . ```. singularity run --cleanenv -B /scratch \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --customized_model=${MODEL_DIR}/model.ckpt \. --ref=""${FASTA_DIR}""/genome.fa \. --reads=""${READ_DIR}""/SRR6006655Aligned.sortedByCoord.out.bam \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --make_examples_extra_args=""split_skip_reads=true,channels=''"" \. --num_shards=${NCPU} #\ **How many cores the `make_examples` step uses. Change it to the number of CPU cores you have.** . ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/782
https://github.com/google/deepvariant/issues/782:20,availability,error,error,20,"After searching the error in our GitHub repo, it seems like https://github.com/google/deepvariant/issues/746 and https://github.com/google/deepvariant/issues/640 can be relevant. @malonzm1 you can check your numpy version, and see if you can update it?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/782
https://github.com/google/deepvariant/issues/782:214,deployability,version,version,214,"After searching the error in our GitHub repo, it seems like https://github.com/google/deepvariant/issues/746 and https://github.com/google/deepvariant/issues/640 can be relevant. @malonzm1 you can check your numpy version, and see if you can update it?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/782
https://github.com/google/deepvariant/issues/782:242,deployability,updat,update,242,"After searching the error in our GitHub repo, it seems like https://github.com/google/deepvariant/issues/746 and https://github.com/google/deepvariant/issues/640 can be relevant. @malonzm1 you can check your numpy version, and see if you can update it?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/782
https://github.com/google/deepvariant/issues/782:214,integrability,version,version,214,"After searching the error in our GitHub repo, it seems like https://github.com/google/deepvariant/issues/746 and https://github.com/google/deepvariant/issues/640 can be relevant. @malonzm1 you can check your numpy version, and see if you can update it?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/782
https://github.com/google/deepvariant/issues/782:214,modifiability,version,version,214,"After searching the error in our GitHub repo, it seems like https://github.com/google/deepvariant/issues/746 and https://github.com/google/deepvariant/issues/640 can be relevant. @malonzm1 you can check your numpy version, and see if you can update it?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/782
https://github.com/google/deepvariant/issues/782:20,performance,error,error,20,"After searching the error in our GitHub repo, it seems like https://github.com/google/deepvariant/issues/746 and https://github.com/google/deepvariant/issues/640 can be relevant. @malonzm1 you can check your numpy version, and see if you can update it?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/782
https://github.com/google/deepvariant/issues/782:20,safety,error,error,20,"After searching the error in our GitHub repo, it seems like https://github.com/google/deepvariant/issues/746 and https://github.com/google/deepvariant/issues/640 can be relevant. @malonzm1 you can check your numpy version, and see if you can update it?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/782
https://github.com/google/deepvariant/issues/782:242,safety,updat,update,242,"After searching the error in our GitHub repo, it seems like https://github.com/google/deepvariant/issues/746 and https://github.com/google/deepvariant/issues/640 can be relevant. @malonzm1 you can check your numpy version, and see if you can update it?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/782
https://github.com/google/deepvariant/issues/782:242,security,updat,update,242,"After searching the error in our GitHub repo, it seems like https://github.com/google/deepvariant/issues/746 and https://github.com/google/deepvariant/issues/640 can be relevant. @malonzm1 you can check your numpy version, and see if you can update it?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/782
https://github.com/google/deepvariant/issues/782:20,usability,error,error,20,"After searching the error in our GitHub repo, it seems like https://github.com/google/deepvariant/issues/746 and https://github.com/google/deepvariant/issues/640 can be relevant. @malonzm1 you can check your numpy version, and see if you can update it?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/782
https://github.com/google/deepvariant/issues/782:22,usability,close,close,22,"Hi @malonzm1 , I will close this issue for now. Please feel free re-open if you need further assistance on this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/782
https://github.com/google/deepvariant/issues/783:107,deployability,contain,container,107,@malonzm1 which bash file are you referring to? You could try using absolute paths to both the singularity container and the bash script and see if that works.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/783
https://github.com/google/deepvariant/issues/783:102,usability,help,help,102,"Hi @malonzm1 , please let us know if you can clarify your setup a bit more. I am also not sure how to help with your question here.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/783
https://github.com/google/deepvariant/issues/783:19,usability,close,close,19,"@malonzm1 , I will close this bug. Please feel free to re-open if you need further assistance.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/783
https://github.com/google/deepvariant/issues/783:34,availability,error,error,34,Has there been a solution to this error? I'm seeing the same thing. I'm running deepvariant:1.6.1-gpu in a WDL workflow. It runs without error when I use it interactively but throws the error inside the WDL task.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/783
https://github.com/google/deepvariant/issues/783:137,availability,error,error,137,Has there been a solution to this error? I'm seeing the same thing. I'm running deepvariant:1.6.1-gpu in a WDL workflow. It runs without error when I use it interactively but throws the error inside the WDL task.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/783
https://github.com/google/deepvariant/issues/783:186,availability,error,error,186,Has there been a solution to this error? I'm seeing the same thing. I'm running deepvariant:1.6.1-gpu in a WDL workflow. It runs without error when I use it interactively but throws the error inside the WDL task.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/783
https://github.com/google/deepvariant/issues/783:98,energy efficiency,gpu,gpu,98,Has there been a solution to this error? I'm seeing the same thing. I'm running deepvariant:1.6.1-gpu in a WDL workflow. It runs without error when I use it interactively but throws the error inside the WDL task.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/783
https://github.com/google/deepvariant/issues/783:34,performance,error,error,34,Has there been a solution to this error? I'm seeing the same thing. I'm running deepvariant:1.6.1-gpu in a WDL workflow. It runs without error when I use it interactively but throws the error inside the WDL task.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/783
https://github.com/google/deepvariant/issues/783:98,performance,gpu,gpu,98,Has there been a solution to this error? I'm seeing the same thing. I'm running deepvariant:1.6.1-gpu in a WDL workflow. It runs without error when I use it interactively but throws the error inside the WDL task.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/783
https://github.com/google/deepvariant/issues/783:137,performance,error,error,137,Has there been a solution to this error? I'm seeing the same thing. I'm running deepvariant:1.6.1-gpu in a WDL workflow. It runs without error when I use it interactively but throws the error inside the WDL task.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/783
https://github.com/google/deepvariant/issues/783:186,performance,error,error,186,Has there been a solution to this error? I'm seeing the same thing. I'm running deepvariant:1.6.1-gpu in a WDL workflow. It runs without error when I use it interactively but throws the error inside the WDL task.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/783
https://github.com/google/deepvariant/issues/783:34,safety,error,error,34,Has there been a solution to this error? I'm seeing the same thing. I'm running deepvariant:1.6.1-gpu in a WDL workflow. It runs without error when I use it interactively but throws the error inside the WDL task.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/783
https://github.com/google/deepvariant/issues/783:137,safety,error,error,137,Has there been a solution to this error? I'm seeing the same thing. I'm running deepvariant:1.6.1-gpu in a WDL workflow. It runs without error when I use it interactively but throws the error inside the WDL task.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/783
https://github.com/google/deepvariant/issues/783:186,safety,error,error,186,Has there been a solution to this error? I'm seeing the same thing. I'm running deepvariant:1.6.1-gpu in a WDL workflow. It runs without error when I use it interactively but throws the error inside the WDL task.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/783
https://github.com/google/deepvariant/issues/783:34,usability,error,error,34,Has there been a solution to this error? I'm seeing the same thing. I'm running deepvariant:1.6.1-gpu in a WDL workflow. It runs without error when I use it interactively but throws the error inside the WDL task.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/783
https://github.com/google/deepvariant/issues/783:111,usability,workflow,workflow,111,Has there been a solution to this error? I'm seeing the same thing. I'm running deepvariant:1.6.1-gpu in a WDL workflow. It runs without error when I use it interactively but throws the error inside the WDL task.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/783
https://github.com/google/deepvariant/issues/783:137,usability,error,error,137,Has there been a solution to this error? I'm seeing the same thing. I'm running deepvariant:1.6.1-gpu in a WDL workflow. It runs without error when I use it interactively but throws the error inside the WDL task.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/783
https://github.com/google/deepvariant/issues/783:157,usability,interact,interactively,157,Has there been a solution to this error? I'm seeing the same thing. I'm running deepvariant:1.6.1-gpu in a WDL workflow. It runs without error when I use it interactively but throws the error inside the WDL task.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/783
https://github.com/google/deepvariant/issues/783:186,usability,error,error,186,Has there been a solution to this error? I'm seeing the same thing. I'm running deepvariant:1.6.1-gpu in a WDL workflow. It runs without error when I use it interactively but throws the error inside the WDL task.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/783
https://github.com/google/deepvariant/issues/783:48,deployability,log,logs,48,"@mccafj02,. Can you please send the command and logs for details? This issue is not related to GPU.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/783
https://github.com/google/deepvariant/issues/783:95,energy efficiency,GPU,GPU,95,"@mccafj02,. Can you please send the command and logs for details? This issue is not related to GPU.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/783
https://github.com/google/deepvariant/issues/783:95,performance,GPU,GPU,95,"@mccafj02,. Can you please send the command and logs for details? This issue is not related to GPU.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/783
https://github.com/google/deepvariant/issues/783:48,safety,log,logs,48,"@mccafj02,. Can you please send the command and logs for details? This issue is not related to GPU.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/783
https://github.com/google/deepvariant/issues/783:48,security,log,logs,48,"@mccafj02,. Can you please send the command and logs for details? This issue is not related to GPU.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/783
https://github.com/google/deepvariant/issues/783:48,testability,log,logs,48,"@mccafj02,. Can you please send the command and logs for details? This issue is not related to GPU.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/783
https://github.com/google/deepvariant/issues/783:36,usability,command,command,36,"@mccafj02,. Can you please send the command and logs for details? This issue is not related to GPU.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/783
https://github.com/google/deepvariant/issues/783:55,deployability,log,log,55,"Thanks for the quick response. This is the command and log file. [stderr.log](https://github.com/google/deepvariant/files/14670380/stderr.log). ```. /opt/deepvariant/bin/run_deepvariant \. --model_type=~{model} \ . --ref= ~{ref_fasta} \. --reads= ~{align_bam} \. --make_examples_extra_args=~{MAKE_EXAMPLE_ARGS} \. --output_vcf= ~{sampleID}.vcf.gz \. --output_gvcf= ~{output_file_name} \. --num_shards= ~{cpu_per_task} \. --haploid_contigs=""chrX,chrY"" \ . --par_regions_bed= ~{par_bed} \ . --postprocess_variants_extra_args=~{POSTPROCESS_VARIANTS_ARGS} \. --regions= ~{regions}```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/783
https://github.com/google/deepvariant/issues/783:73,deployability,log,log,73,"Thanks for the quick response. This is the command and log file. [stderr.log](https://github.com/google/deepvariant/files/14670380/stderr.log). ```. /opt/deepvariant/bin/run_deepvariant \. --model_type=~{model} \ . --ref= ~{ref_fasta} \. --reads= ~{align_bam} \. --make_examples_extra_args=~{MAKE_EXAMPLE_ARGS} \. --output_vcf= ~{sampleID}.vcf.gz \. --output_gvcf= ~{output_file_name} \. --num_shards= ~{cpu_per_task} \. --haploid_contigs=""chrX,chrY"" \ . --par_regions_bed= ~{par_bed} \ . --postprocess_variants_extra_args=~{POSTPROCESS_VARIANTS_ARGS} \. --regions= ~{regions}```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/783
https://github.com/google/deepvariant/issues/783:138,deployability,log,log,138,"Thanks for the quick response. This is the command and log file. [stderr.log](https://github.com/google/deepvariant/files/14670380/stderr.log). ```. /opt/deepvariant/bin/run_deepvariant \. --model_type=~{model} \ . --ref= ~{ref_fasta} \. --reads= ~{align_bam} \. --make_examples_extra_args=~{MAKE_EXAMPLE_ARGS} \. --output_vcf= ~{sampleID}.vcf.gz \. --output_gvcf= ~{output_file_name} \. --num_shards= ~{cpu_per_task} \. --haploid_contigs=""chrX,chrY"" \ . --par_regions_bed= ~{par_bed} \ . --postprocess_variants_extra_args=~{POSTPROCESS_VARIANTS_ARGS} \. --regions= ~{regions}```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/783
https://github.com/google/deepvariant/issues/783:204,energy efficiency,model,model,204,"Thanks for the quick response. This is the command and log file. [stderr.log](https://github.com/google/deepvariant/files/14670380/stderr.log). ```. /opt/deepvariant/bin/run_deepvariant \. --model_type=~{model} \ . --ref= ~{ref_fasta} \. --reads= ~{align_bam} \. --make_examples_extra_args=~{MAKE_EXAMPLE_ARGS} \. --output_vcf= ~{sampleID}.vcf.gz \. --output_gvcf= ~{output_file_name} \. --num_shards= ~{cpu_per_task} \. --haploid_contigs=""chrX,chrY"" \ . --par_regions_bed= ~{par_bed} \ . --postprocess_variants_extra_args=~{POSTPROCESS_VARIANTS_ARGS} \. --regions= ~{regions}```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/783
https://github.com/google/deepvariant/issues/783:55,safety,log,log,55,"Thanks for the quick response. This is the command and log file. [stderr.log](https://github.com/google/deepvariant/files/14670380/stderr.log). ```. /opt/deepvariant/bin/run_deepvariant \. --model_type=~{model} \ . --ref= ~{ref_fasta} \. --reads= ~{align_bam} \. --make_examples_extra_args=~{MAKE_EXAMPLE_ARGS} \. --output_vcf= ~{sampleID}.vcf.gz \. --output_gvcf= ~{output_file_name} \. --num_shards= ~{cpu_per_task} \. --haploid_contigs=""chrX,chrY"" \ . --par_regions_bed= ~{par_bed} \ . --postprocess_variants_extra_args=~{POSTPROCESS_VARIANTS_ARGS} \. --regions= ~{regions}```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/783
https://github.com/google/deepvariant/issues/783:73,safety,log,log,73,"Thanks for the quick response. This is the command and log file. [stderr.log](https://github.com/google/deepvariant/files/14670380/stderr.log). ```. /opt/deepvariant/bin/run_deepvariant \. --model_type=~{model} \ . --ref= ~{ref_fasta} \. --reads= ~{align_bam} \. --make_examples_extra_args=~{MAKE_EXAMPLE_ARGS} \. --output_vcf= ~{sampleID}.vcf.gz \. --output_gvcf= ~{output_file_name} \. --num_shards= ~{cpu_per_task} \. --haploid_contigs=""chrX,chrY"" \ . --par_regions_bed= ~{par_bed} \ . --postprocess_variants_extra_args=~{POSTPROCESS_VARIANTS_ARGS} \. --regions= ~{regions}```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/783
https://github.com/google/deepvariant/issues/783:138,safety,log,log,138,"Thanks for the quick response. This is the command and log file. [stderr.log](https://github.com/google/deepvariant/files/14670380/stderr.log). ```. /opt/deepvariant/bin/run_deepvariant \. --model_type=~{model} \ . --ref= ~{ref_fasta} \. --reads= ~{align_bam} \. --make_examples_extra_args=~{MAKE_EXAMPLE_ARGS} \. --output_vcf= ~{sampleID}.vcf.gz \. --output_gvcf= ~{output_file_name} \. --num_shards= ~{cpu_per_task} \. --haploid_contigs=""chrX,chrY"" \ . --par_regions_bed= ~{par_bed} \ . --postprocess_variants_extra_args=~{POSTPROCESS_VARIANTS_ARGS} \. --regions= ~{regions}```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/783
https://github.com/google/deepvariant/issues/783:55,security,log,log,55,"Thanks for the quick response. This is the command and log file. [stderr.log](https://github.com/google/deepvariant/files/14670380/stderr.log). ```. /opt/deepvariant/bin/run_deepvariant \. --model_type=~{model} \ . --ref= ~{ref_fasta} \. --reads= ~{align_bam} \. --make_examples_extra_args=~{MAKE_EXAMPLE_ARGS} \. --output_vcf= ~{sampleID}.vcf.gz \. --output_gvcf= ~{output_file_name} \. --num_shards= ~{cpu_per_task} \. --haploid_contigs=""chrX,chrY"" \ . --par_regions_bed= ~{par_bed} \ . --postprocess_variants_extra_args=~{POSTPROCESS_VARIANTS_ARGS} \. --regions= ~{regions}```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/783
https://github.com/google/deepvariant/issues/783:73,security,log,log,73,"Thanks for the quick response. This is the command and log file. [stderr.log](https://github.com/google/deepvariant/files/14670380/stderr.log). ```. /opt/deepvariant/bin/run_deepvariant \. --model_type=~{model} \ . --ref= ~{ref_fasta} \. --reads= ~{align_bam} \. --make_examples_extra_args=~{MAKE_EXAMPLE_ARGS} \. --output_vcf= ~{sampleID}.vcf.gz \. --output_gvcf= ~{output_file_name} \. --num_shards= ~{cpu_per_task} \. --haploid_contigs=""chrX,chrY"" \ . --par_regions_bed= ~{par_bed} \ . --postprocess_variants_extra_args=~{POSTPROCESS_VARIANTS_ARGS} \. --regions= ~{regions}```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/783
https://github.com/google/deepvariant/issues/783:138,security,log,log,138,"Thanks for the quick response. This is the command and log file. [stderr.log](https://github.com/google/deepvariant/files/14670380/stderr.log). ```. /opt/deepvariant/bin/run_deepvariant \. --model_type=~{model} \ . --ref= ~{ref_fasta} \. --reads= ~{align_bam} \. --make_examples_extra_args=~{MAKE_EXAMPLE_ARGS} \. --output_vcf= ~{sampleID}.vcf.gz \. --output_gvcf= ~{output_file_name} \. --num_shards= ~{cpu_per_task} \. --haploid_contigs=""chrX,chrY"" \ . --par_regions_bed= ~{par_bed} \ . --postprocess_variants_extra_args=~{POSTPROCESS_VARIANTS_ARGS} \. --regions= ~{regions}```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/783
https://github.com/google/deepvariant/issues/783:204,security,model,model,204,"Thanks for the quick response. This is the command and log file. [stderr.log](https://github.com/google/deepvariant/files/14670380/stderr.log). ```. /opt/deepvariant/bin/run_deepvariant \. --model_type=~{model} \ . --ref= ~{ref_fasta} \. --reads= ~{align_bam} \. --make_examples_extra_args=~{MAKE_EXAMPLE_ARGS} \. --output_vcf= ~{sampleID}.vcf.gz \. --output_gvcf= ~{output_file_name} \. --num_shards= ~{cpu_per_task} \. --haploid_contigs=""chrX,chrY"" \ . --par_regions_bed= ~{par_bed} \ . --postprocess_variants_extra_args=~{POSTPROCESS_VARIANTS_ARGS} \. --regions= ~{regions}```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/783
https://github.com/google/deepvariant/issues/783:55,testability,log,log,55,"Thanks for the quick response. This is the command and log file. [stderr.log](https://github.com/google/deepvariant/files/14670380/stderr.log). ```. /opt/deepvariant/bin/run_deepvariant \. --model_type=~{model} \ . --ref= ~{ref_fasta} \. --reads= ~{align_bam} \. --make_examples_extra_args=~{MAKE_EXAMPLE_ARGS} \. --output_vcf= ~{sampleID}.vcf.gz \. --output_gvcf= ~{output_file_name} \. --num_shards= ~{cpu_per_task} \. --haploid_contigs=""chrX,chrY"" \ . --par_regions_bed= ~{par_bed} \ . --postprocess_variants_extra_args=~{POSTPROCESS_VARIANTS_ARGS} \. --regions= ~{regions}```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/783
https://github.com/google/deepvariant/issues/783:73,testability,log,log,73,"Thanks for the quick response. This is the command and log file. [stderr.log](https://github.com/google/deepvariant/files/14670380/stderr.log). ```. /opt/deepvariant/bin/run_deepvariant \. --model_type=~{model} \ . --ref= ~{ref_fasta} \. --reads= ~{align_bam} \. --make_examples_extra_args=~{MAKE_EXAMPLE_ARGS} \. --output_vcf= ~{sampleID}.vcf.gz \. --output_gvcf= ~{output_file_name} \. --num_shards= ~{cpu_per_task} \. --haploid_contigs=""chrX,chrY"" \ . --par_regions_bed= ~{par_bed} \ . --postprocess_variants_extra_args=~{POSTPROCESS_VARIANTS_ARGS} \. --regions= ~{regions}```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/783
https://github.com/google/deepvariant/issues/783:138,testability,log,log,138,"Thanks for the quick response. This is the command and log file. [stderr.log](https://github.com/google/deepvariant/files/14670380/stderr.log). ```. /opt/deepvariant/bin/run_deepvariant \. --model_type=~{model} \ . --ref= ~{ref_fasta} \. --reads= ~{align_bam} \. --make_examples_extra_args=~{MAKE_EXAMPLE_ARGS} \. --output_vcf= ~{sampleID}.vcf.gz \. --output_gvcf= ~{output_file_name} \. --num_shards= ~{cpu_per_task} \. --haploid_contigs=""chrX,chrY"" \ . --par_regions_bed= ~{par_bed} \ . --postprocess_variants_extra_args=~{POSTPROCESS_VARIANTS_ARGS} \. --regions= ~{regions}```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/783
https://github.com/google/deepvariant/issues/783:43,usability,command,command,43,"Thanks for the quick response. This is the command and log file. [stderr.log](https://github.com/google/deepvariant/files/14670380/stderr.log). ```. /opt/deepvariant/bin/run_deepvariant \. --model_type=~{model} \ . --ref= ~{ref_fasta} \. --reads= ~{align_bam} \. --make_examples_extra_args=~{MAKE_EXAMPLE_ARGS} \. --output_vcf= ~{sampleID}.vcf.gz \. --output_gvcf= ~{output_file_name} \. --num_shards= ~{cpu_per_task} \. --haploid_contigs=""chrX,chrY"" \ . --par_regions_bed= ~{par_bed} \ . --postprocess_variants_extra_args=~{POSTPROCESS_VARIANTS_ARGS} \. --regions= ~{regions}```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/783
https://github.com/google/deepvariant/issues/783:15,availability,error,error,15,"please see the error has:. ```bash. --ref is required. Pass --helpshort or --helpfull to see help on flags. ```. Meaning your variable ~{ref_fasta} is either NULL or set to a value that is not accessible to the program. Can you try absolute path to make sure the files are being passed correctly? Also, please run the quick-start locally to make sure the program is running correctly. The quick-start can be found here: https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-quick-start.md",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/783
https://github.com/google/deepvariant/issues/783:126,modifiability,variab,variable,126,"please see the error has:. ```bash. --ref is required. Pass --helpshort or --helpfull to see help on flags. ```. Meaning your variable ~{ref_fasta} is either NULL or set to a value that is not accessible to the program. Can you try absolute path to make sure the files are being passed correctly? Also, please run the quick-start locally to make sure the program is running correctly. The quick-start can be found here: https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-quick-start.md",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/783
https://github.com/google/deepvariant/issues/783:15,performance,error,error,15,"please see the error has:. ```bash. --ref is required. Pass --helpshort or --helpfull to see help on flags. ```. Meaning your variable ~{ref_fasta} is either NULL or set to a value that is not accessible to the program. Can you try absolute path to make sure the files are being passed correctly? Also, please run the quick-start locally to make sure the program is running correctly. The quick-start can be found here: https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-quick-start.md",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/783
https://github.com/google/deepvariant/issues/783:15,safety,error,error,15,"please see the error has:. ```bash. --ref is required. Pass --helpshort or --helpfull to see help on flags. ```. Meaning your variable ~{ref_fasta} is either NULL or set to a value that is not accessible to the program. Can you try absolute path to make sure the files are being passed correctly? Also, please run the quick-start locally to make sure the program is running correctly. The quick-start can be found here: https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-quick-start.md",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/783
https://github.com/google/deepvariant/issues/783:193,security,access,accessible,193,"please see the error has:. ```bash. --ref is required. Pass --helpshort or --helpfull to see help on flags. ```. Meaning your variable ~{ref_fasta} is either NULL or set to a value that is not accessible to the program. Can you try absolute path to make sure the files are being passed correctly? Also, please run the quick-start locally to make sure the program is running correctly. The quick-start can be found here: https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-quick-start.md",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/783
https://github.com/google/deepvariant/issues/783:15,usability,error,error,15,"please see the error has:. ```bash. --ref is required. Pass --helpshort or --helpfull to see help on flags. ```. Meaning your variable ~{ref_fasta} is either NULL or set to a value that is not accessible to the program. Can you try absolute path to make sure the files are being passed correctly? Also, please run the quick-start locally to make sure the program is running correctly. The quick-start can be found here: https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-quick-start.md",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/783
https://github.com/google/deepvariant/issues/783:62,usability,help,helpshort,62,"please see the error has:. ```bash. --ref is required. Pass --helpshort or --helpfull to see help on flags. ```. Meaning your variable ~{ref_fasta} is either NULL or set to a value that is not accessible to the program. Can you try absolute path to make sure the files are being passed correctly? Also, please run the quick-start locally to make sure the program is running correctly. The quick-start can be found here: https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-quick-start.md",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/783
https://github.com/google/deepvariant/issues/783:77,usability,help,helpfull,77,"please see the error has:. ```bash. --ref is required. Pass --helpshort or --helpfull to see help on flags. ```. Meaning your variable ~{ref_fasta} is either NULL or set to a value that is not accessible to the program. Can you try absolute path to make sure the files are being passed correctly? Also, please run the quick-start locally to make sure the program is running correctly. The quick-start can be found here: https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-quick-start.md",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/783
https://github.com/google/deepvariant/issues/783:93,usability,help,help,93,"please see the error has:. ```bash. --ref is required. Pass --helpshort or --helpfull to see help on flags. ```. Meaning your variable ~{ref_fasta} is either NULL or set to a value that is not accessible to the program. Can you try absolute path to make sure the files are being passed correctly? Also, please run the quick-start locally to make sure the program is running correctly. The quick-start can be found here: https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-quick-start.md",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/783
https://github.com/google/deepvariant/issues/783:70,usability,close,close,70,"Hi @mccafj02 ,. This issue has been inactive for a few weeks, so I'll close it. Please feel free to reopen if you want to follow up with more details after the last suggestion. Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/783
https://github.com/google/deepvariant/issues/784:18,reliability,Doe,Does,18,Hi @danielecook . Does this method have a tumor-only mode?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/784
https://github.com/google/deepvariant/issues/784:27,deployability,releas,release,27,Currently - no. But we may release a tumor only mode in the future.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/784
https://github.com/google/deepvariant/issues/784:0,energy efficiency,Current,Currently,0,Currently - no. But we may release a tumor only mode in the future.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/784
https://github.com/google/deepvariant/issues/785:124,energy efficiency,model,model,124,"Hi YM, . Please use ONT_R104. If you have older ONT data, you can consider https://github.com/kishwarshafin/pepper. Our WGS model is for short-read data, for example Illumina or Element. You can also see our Quick Start tutorials for examples. Let me know if you have more questions!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/785
https://github.com/google/deepvariant/issues/785:124,security,model,model,124,"Hi YM, . Please use ONT_R104. If you have older ONT data, you can consider https://github.com/kishwarshafin/pepper. Our WGS model is for short-read data, for example Illumina or Element. You can also see our Quick Start tutorials for examples. Let me know if you have more questions!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/785
https://github.com/google/deepvariant/issues/786:128,safety,input,input,128,"Hi @pichuan . Thank you for your attention to this issue! . There is no bug or issue in DeepVariant. The problem lies within my input file, which I haven't fully understand yet. However, it works using other dataset. . I will close this issue, . Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/786
https://github.com/google/deepvariant/issues/786:162,testability,understand,understand,162,"Hi @pichuan . Thank you for your attention to this issue! . There is no bug or issue in DeepVariant. The problem lies within my input file, which I haven't fully understand yet. However, it works using other dataset. . I will close this issue, . Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/786
https://github.com/google/deepvariant/issues/786:128,usability,input,input,128,"Hi @pichuan . Thank you for your attention to this issue! . There is no bug or issue in DeepVariant. The problem lies within my input file, which I haven't fully understand yet. However, it works using other dataset. . I will close this issue, . Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/786
https://github.com/google/deepvariant/issues/786:226,usability,close,close,226,"Hi @pichuan . Thank you for your attention to this issue! . There is no bug or issue in DeepVariant. The problem lies within my input file, which I haven't fully understand yet. However, it works using other dataset. . I will close this issue, . Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/786
https://github.com/google/deepvariant/pull/787:19,deployability,updat,updated,19,Closing this as we updated the 1.6.1 branch separately.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/787
https://github.com/google/deepvariant/pull/787:19,safety,updat,updated,19,Closing this as we updated the 1.6.1 branch separately.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/787
https://github.com/google/deepvariant/pull/787:19,security,updat,updated,19,Closing this as we updated the 1.6.1 branch separately.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/787
https://github.com/google/deepvariant/issues/788:21,deployability,version,version,21,"Hi, up to the latest version (1.6), we've been excluding HG003 and chr20-22 from the training data. Building on that assumption, if you train a model (warmstarting our model) that also exclude those data from training, you can be sure that the model would never have seen those data. And you're correct that it's still good to be aware that HG002 is the son of HG003. So it depends on what hypothesis you're testing, you'll want to think about that factor too.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/788
https://github.com/google/deepvariant/issues/788:100,deployability,Build,Building,100,"Hi, up to the latest version (1.6), we've been excluding HG003 and chr20-22 from the training data. Building on that assumption, if you train a model (warmstarting our model) that also exclude those data from training, you can be sure that the model would never have seen those data. And you're correct that it's still good to be aware that HG002 is the son of HG003. So it depends on what hypothesis you're testing, you'll want to think about that factor too.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/788
https://github.com/google/deepvariant/issues/788:374,deployability,depend,depends,374,"Hi, up to the latest version (1.6), we've been excluding HG003 and chr20-22 from the training data. Building on that assumption, if you train a model (warmstarting our model) that also exclude those data from training, you can be sure that the model would never have seen those data. And you're correct that it's still good to be aware that HG002 is the son of HG003. So it depends on what hypothesis you're testing, you'll want to think about that factor too.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/788
https://github.com/google/deepvariant/issues/788:144,energy efficiency,model,model,144,"Hi, up to the latest version (1.6), we've been excluding HG003 and chr20-22 from the training data. Building on that assumption, if you train a model (warmstarting our model) that also exclude those data from training, you can be sure that the model would never have seen those data. And you're correct that it's still good to be aware that HG002 is the son of HG003. So it depends on what hypothesis you're testing, you'll want to think about that factor too.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/788
https://github.com/google/deepvariant/issues/788:168,energy efficiency,model,model,168,"Hi, up to the latest version (1.6), we've been excluding HG003 and chr20-22 from the training data. Building on that assumption, if you train a model (warmstarting our model) that also exclude those data from training, you can be sure that the model would never have seen those data. And you're correct that it's still good to be aware that HG002 is the son of HG003. So it depends on what hypothesis you're testing, you'll want to think about that factor too.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/788
https://github.com/google/deepvariant/issues/788:244,energy efficiency,model,model,244,"Hi, up to the latest version (1.6), we've been excluding HG003 and chr20-22 from the training data. Building on that assumption, if you train a model (warmstarting our model) that also exclude those data from training, you can be sure that the model would never have seen those data. And you're correct that it's still good to be aware that HG002 is the son of HG003. So it depends on what hypothesis you're testing, you'll want to think about that factor too.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/788
https://github.com/google/deepvariant/issues/788:21,integrability,version,version,21,"Hi, up to the latest version (1.6), we've been excluding HG003 and chr20-22 from the training data. Building on that assumption, if you train a model (warmstarting our model) that also exclude those data from training, you can be sure that the model would never have seen those data. And you're correct that it's still good to be aware that HG002 is the son of HG003. So it depends on what hypothesis you're testing, you'll want to think about that factor too.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/788
https://github.com/google/deepvariant/issues/788:374,integrability,depend,depends,374,"Hi, up to the latest version (1.6), we've been excluding HG003 and chr20-22 from the training data. Building on that assumption, if you train a model (warmstarting our model) that also exclude those data from training, you can be sure that the model would never have seen those data. And you're correct that it's still good to be aware that HG002 is the son of HG003. So it depends on what hypothesis you're testing, you'll want to think about that factor too.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/788
https://github.com/google/deepvariant/issues/788:21,modifiability,version,version,21,"Hi, up to the latest version (1.6), we've been excluding HG003 and chr20-22 from the training data. Building on that assumption, if you train a model (warmstarting our model) that also exclude those data from training, you can be sure that the model would never have seen those data. And you're correct that it's still good to be aware that HG002 is the son of HG003. So it depends on what hypothesis you're testing, you'll want to think about that factor too.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/788
https://github.com/google/deepvariant/issues/788:374,modifiability,depend,depends,374,"Hi, up to the latest version (1.6), we've been excluding HG003 and chr20-22 from the training data. Building on that assumption, if you train a model (warmstarting our model) that also exclude those data from training, you can be sure that the model would never have seen those data. And you're correct that it's still good to be aware that HG002 is the son of HG003. So it depends on what hypothesis you're testing, you'll want to think about that factor too.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/788
https://github.com/google/deepvariant/issues/788:374,safety,depend,depends,374,"Hi, up to the latest version (1.6), we've been excluding HG003 and chr20-22 from the training data. Building on that assumption, if you train a model (warmstarting our model) that also exclude those data from training, you can be sure that the model would never have seen those data. And you're correct that it's still good to be aware that HG002 is the son of HG003. So it depends on what hypothesis you're testing, you'll want to think about that factor too.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/788
https://github.com/google/deepvariant/issues/788:408,safety,test,testing,408,"Hi, up to the latest version (1.6), we've been excluding HG003 and chr20-22 from the training data. Building on that assumption, if you train a model (warmstarting our model) that also exclude those data from training, you can be sure that the model would never have seen those data. And you're correct that it's still good to be aware that HG002 is the son of HG003. So it depends on what hypothesis you're testing, you'll want to think about that factor too.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/788
https://github.com/google/deepvariant/issues/788:144,security,model,model,144,"Hi, up to the latest version (1.6), we've been excluding HG003 and chr20-22 from the training data. Building on that assumption, if you train a model (warmstarting our model) that also exclude those data from training, you can be sure that the model would never have seen those data. And you're correct that it's still good to be aware that HG002 is the son of HG003. So it depends on what hypothesis you're testing, you'll want to think about that factor too.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/788
https://github.com/google/deepvariant/issues/788:168,security,model,model,168,"Hi, up to the latest version (1.6), we've been excluding HG003 and chr20-22 from the training data. Building on that assumption, if you train a model (warmstarting our model) that also exclude those data from training, you can be sure that the model would never have seen those data. And you're correct that it's still good to be aware that HG002 is the son of HG003. So it depends on what hypothesis you're testing, you'll want to think about that factor too.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/788
https://github.com/google/deepvariant/issues/788:244,security,model,model,244,"Hi, up to the latest version (1.6), we've been excluding HG003 and chr20-22 from the training data. Building on that assumption, if you train a model (warmstarting our model) that also exclude those data from training, you can be sure that the model would never have seen those data. And you're correct that it's still good to be aware that HG002 is the son of HG003. So it depends on what hypothesis you're testing, you'll want to think about that factor too.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/788
https://github.com/google/deepvariant/issues/788:374,testability,depend,depends,374,"Hi, up to the latest version (1.6), we've been excluding HG003 and chr20-22 from the training data. Building on that assumption, if you train a model (warmstarting our model) that also exclude those data from training, you can be sure that the model would never have seen those data. And you're correct that it's still good to be aware that HG002 is the son of HG003. So it depends on what hypothesis you're testing, you'll want to think about that factor too.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/788
https://github.com/google/deepvariant/issues/788:408,testability,test,testing,408,"Hi, up to the latest version (1.6), we've been excluding HG003 and chr20-22 from the training data. Building on that assumption, if you train a model (warmstarting our model) that also exclude those data from training, you can be sure that the model would never have seen those data. And you're correct that it's still good to be aware that HG002 is the son of HG003. So it depends on what hypothesis you're testing, you'll want to think about that factor too.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/788
https://github.com/google/deepvariant/issues/789:27,safety,test,test,27,Thank you @carsonhh ! I'll test this and make a change accordingly.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/789
https://github.com/google/deepvariant/issues/789:27,testability,test,test,27,Thank you @carsonhh ! I'll test this and make a change accordingly.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/789
https://github.com/google/deepvariant/issues/789:103,deployability,version,version,103,"Note you will also have to revert a couple of lines referred to in run-preset.sh to match the tensorrt version. This is the text in run-preset.sh, you should probably revert to the way it was in 1.5.0 for this section:. . ```. # In v8.6.1, the libs got moved to tensorrt_libs:. # https://docs.nvidia.com/deeplearning/tensorrt/release-notes/index.html#rel-8-6-1. sudo ln -sf ""${TENSORRT_PATH}_libs/libnvinfer.so.8"" ""${TENSORRT_PATH}_libs/libnvinfer.so.7"". sudo ln -sf ""${TENSORRT_PATH}_libs/libnvinfer_plugin.so.8"" ""${TENSORRT_PATH}_libs/libnvinfer_plugin.so.7"". ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/789
https://github.com/google/deepvariant/issues/789:326,deployability,releas,release-notes,326,"Note you will also have to revert a couple of lines referred to in run-preset.sh to match the tensorrt version. This is the text in run-preset.sh, you should probably revert to the way it was in 1.5.0 for this section:. . ```. # In v8.6.1, the libs got moved to tensorrt_libs:. # https://docs.nvidia.com/deeplearning/tensorrt/release-notes/index.html#rel-8-6-1. sudo ln -sf ""${TENSORRT_PATH}_libs/libnvinfer.so.8"" ""${TENSORRT_PATH}_libs/libnvinfer.so.7"". sudo ln -sf ""${TENSORRT_PATH}_libs/libnvinfer_plugin.so.8"" ""${TENSORRT_PATH}_libs/libnvinfer_plugin.so.7"". ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/789
https://github.com/google/deepvariant/issues/789:36,integrability,coupl,couple,36,"Note you will also have to revert a couple of lines referred to in run-preset.sh to match the tensorrt version. This is the text in run-preset.sh, you should probably revert to the way it was in 1.5.0 for this section:. . ```. # In v8.6.1, the libs got moved to tensorrt_libs:. # https://docs.nvidia.com/deeplearning/tensorrt/release-notes/index.html#rel-8-6-1. sudo ln -sf ""${TENSORRT_PATH}_libs/libnvinfer.so.8"" ""${TENSORRT_PATH}_libs/libnvinfer.so.7"". sudo ln -sf ""${TENSORRT_PATH}_libs/libnvinfer_plugin.so.8"" ""${TENSORRT_PATH}_libs/libnvinfer_plugin.so.7"". ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/789
https://github.com/google/deepvariant/issues/789:103,integrability,version,version,103,"Note you will also have to revert a couple of lines referred to in run-preset.sh to match the tensorrt version. This is the text in run-preset.sh, you should probably revert to the way it was in 1.5.0 for this section:. . ```. # In v8.6.1, the libs got moved to tensorrt_libs:. # https://docs.nvidia.com/deeplearning/tensorrt/release-notes/index.html#rel-8-6-1. sudo ln -sf ""${TENSORRT_PATH}_libs/libnvinfer.so.8"" ""${TENSORRT_PATH}_libs/libnvinfer.so.7"". sudo ln -sf ""${TENSORRT_PATH}_libs/libnvinfer_plugin.so.8"" ""${TENSORRT_PATH}_libs/libnvinfer_plugin.so.7"". ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/789
https://github.com/google/deepvariant/issues/789:36,modifiability,coupl,couple,36,"Note you will also have to revert a couple of lines referred to in run-preset.sh to match the tensorrt version. This is the text in run-preset.sh, you should probably revert to the way it was in 1.5.0 for this section:. . ```. # In v8.6.1, the libs got moved to tensorrt_libs:. # https://docs.nvidia.com/deeplearning/tensorrt/release-notes/index.html#rel-8-6-1. sudo ln -sf ""${TENSORRT_PATH}_libs/libnvinfer.so.8"" ""${TENSORRT_PATH}_libs/libnvinfer.so.7"". sudo ln -sf ""${TENSORRT_PATH}_libs/libnvinfer_plugin.so.8"" ""${TENSORRT_PATH}_libs/libnvinfer_plugin.so.7"". ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/789
https://github.com/google/deepvariant/issues/789:103,modifiability,version,version,103,"Note you will also have to revert a couple of lines referred to in run-preset.sh to match the tensorrt version. This is the text in run-preset.sh, you should probably revert to the way it was in 1.5.0 for this section:. . ```. # In v8.6.1, the libs got moved to tensorrt_libs:. # https://docs.nvidia.com/deeplearning/tensorrt/release-notes/index.html#rel-8-6-1. sudo ln -sf ""${TENSORRT_PATH}_libs/libnvinfer.so.8"" ""${TENSORRT_PATH}_libs/libnvinfer.so.7"". sudo ln -sf ""${TENSORRT_PATH}_libs/libnvinfer_plugin.so.8"" ""${TENSORRT_PATH}_libs/libnvinfer_plugin.so.7"". ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/789
https://github.com/google/deepvariant/issues/789:36,testability,coupl,couple,36,"Note you will also have to revert a couple of lines referred to in run-preset.sh to match the tensorrt version. This is the text in run-preset.sh, you should probably revert to the way it was in 1.5.0 for this section:. . ```. # In v8.6.1, the libs got moved to tensorrt_libs:. # https://docs.nvidia.com/deeplearning/tensorrt/release-notes/index.html#rel-8-6-1. sudo ln -sf ""${TENSORRT_PATH}_libs/libnvinfer.so.8"" ""${TENSORRT_PATH}_libs/libnvinfer.so.7"". sudo ln -sf ""${TENSORRT_PATH}_libs/libnvinfer_plugin.so.8"" ""${TENSORRT_PATH}_libs/libnvinfer_plugin.so.7"". ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/789
https://github.com/google/deepvariant/issues/789:63,deployability,instal,install,63,"Here is my diff for run-preset.sh edits:. ```. 274c274. < pip3 install ""${PIP_ARGS[@]}"" tensorrt==8.5.3.1. ---. > pip3 install ""${PIP_ARGS[@]}"" nvidia-tensorrt. 276c276. < pip3 show tensorrt. ---. > pip3 show nvidia-tensorrt. 278,285c278,282. < ## In v8.6.1, the libs got moved to tensorrt_libs:. < ## https://docs.nvidia.com/deeplearning/tensorrt/release-notes/index.html#rel-8-6-1. < ##sudo ln -sf ""${TENSORRT_PATH}_libs/libnvinfer.so.8"" ""${TENSORRT_PATH}_libs/libnvinfer.so.7"". < ##sudo ln -sf ""${TENSORRT_PATH}_libs/libnvinfer_plugin.so.8"" ""${TENSORRT_PATH}_libs/libnvinfer_plugin.so.7"". < ##export LD_LIBRARY_PATH=""${LD_LIBRARY_PATH-}:${TENSORRT_PATH}_libs"". < sudo ln -sf ""${TENSORRT_PATH}/libnvinfer.so.8"" ""${TENSORRT_PATH}/libnvinfer.so.7"". < sudo ln -sf ""${TENSORRT_PATH}/libnvinfer_plugin.so.8"" ""${TENSORRT_PATH}/libnvinfer_plugin.so.7"". < export LD_LIBRARY_PATH=""${LD_LIBRARY_PATH-}:${TENSORRT_PATH}"". ---. > # In v8.6.1, the libs got moved to tensorrt_libs:. > # https://docs.nvidia.com/deeplearning/tensorrt/release-notes/index.html#rel-8-6-1. > sudo ln -sf ""${TENSORRT_PATH}_libs/libnvinfer.so.8"" ""${TENSORRT_PATH}_libs/libnvinfer.so.7"". > sudo ln -sf ""${TENSORRT_PATH}_libs/libnvinfer_plugin.so.8"" ""${TENSORRT_PATH}_libs/libnvinfer_plugin.so.7"". > export LD_LIBRARY_PATH=""${LD_LIBRARY_PATH-}:${TENSORRT_PATH}_libs"". 292,295c289,290. < ##sudo ln -sf ""${TENSORRT_PATH}_libs/libnvinfer.so.7"" /usr/local/nvidia/lib/libnvinfer.so.7. < ##sudo ln -sf ""${TENSORRT_PATH}_libs/libnvinfer_plugin.so.7"" /usr/local/nvidia/lib/libnvinfer_plugin.so.7. < sudo ln -sf ""${TENSORRT_PATH}/libnvinfer.so.7"" /usr/local/nvidia/lib/libnvinfer.so.7. < sudo ln -sf ""${TENSORRT_PATH}/libnvinfer_plugin.so.7"" /usr/local/nvidia/lib/libnvinfer_plugin.so.7. ---. > sudo ln -sf ""${TENSORRT_PATH}_libs/libnvinfer.so.7"" /usr/local/nvidia/lib/libnvinfer.so.7. > sudo ln -sf ""${TENSORRT_PATH}_libs/libnvinfer_plugin.so.7"" /usr/local/nvidia/lib/libnvinfer_plugin.so.7. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/789
https://github.com/google/deepvariant/issues/789:119,deployability,instal,install,119,"Here is my diff for run-preset.sh edits:. ```. 274c274. < pip3 install ""${PIP_ARGS[@]}"" tensorrt==8.5.3.1. ---. > pip3 install ""${PIP_ARGS[@]}"" nvidia-tensorrt. 276c276. < pip3 show tensorrt. ---. > pip3 show nvidia-tensorrt. 278,285c278,282. < ## In v8.6.1, the libs got moved to tensorrt_libs:. < ## https://docs.nvidia.com/deeplearning/tensorrt/release-notes/index.html#rel-8-6-1. < ##sudo ln -sf ""${TENSORRT_PATH}_libs/libnvinfer.so.8"" ""${TENSORRT_PATH}_libs/libnvinfer.so.7"". < ##sudo ln -sf ""${TENSORRT_PATH}_libs/libnvinfer_plugin.so.8"" ""${TENSORRT_PATH}_libs/libnvinfer_plugin.so.7"". < ##export LD_LIBRARY_PATH=""${LD_LIBRARY_PATH-}:${TENSORRT_PATH}_libs"". < sudo ln -sf ""${TENSORRT_PATH}/libnvinfer.so.8"" ""${TENSORRT_PATH}/libnvinfer.so.7"". < sudo ln -sf ""${TENSORRT_PATH}/libnvinfer_plugin.so.8"" ""${TENSORRT_PATH}/libnvinfer_plugin.so.7"". < export LD_LIBRARY_PATH=""${LD_LIBRARY_PATH-}:${TENSORRT_PATH}"". ---. > # In v8.6.1, the libs got moved to tensorrt_libs:. > # https://docs.nvidia.com/deeplearning/tensorrt/release-notes/index.html#rel-8-6-1. > sudo ln -sf ""${TENSORRT_PATH}_libs/libnvinfer.so.8"" ""${TENSORRT_PATH}_libs/libnvinfer.so.7"". > sudo ln -sf ""${TENSORRT_PATH}_libs/libnvinfer_plugin.so.8"" ""${TENSORRT_PATH}_libs/libnvinfer_plugin.so.7"". > export LD_LIBRARY_PATH=""${LD_LIBRARY_PATH-}:${TENSORRT_PATH}_libs"". 292,295c289,290. < ##sudo ln -sf ""${TENSORRT_PATH}_libs/libnvinfer.so.7"" /usr/local/nvidia/lib/libnvinfer.so.7. < ##sudo ln -sf ""${TENSORRT_PATH}_libs/libnvinfer_plugin.so.7"" /usr/local/nvidia/lib/libnvinfer_plugin.so.7. < sudo ln -sf ""${TENSORRT_PATH}/libnvinfer.so.7"" /usr/local/nvidia/lib/libnvinfer.so.7. < sudo ln -sf ""${TENSORRT_PATH}/libnvinfer_plugin.so.7"" /usr/local/nvidia/lib/libnvinfer_plugin.so.7. ---. > sudo ln -sf ""${TENSORRT_PATH}_libs/libnvinfer.so.7"" /usr/local/nvidia/lib/libnvinfer.so.7. > sudo ln -sf ""${TENSORRT_PATH}_libs/libnvinfer_plugin.so.7"" /usr/local/nvidia/lib/libnvinfer_plugin.so.7. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/789
https://github.com/google/deepvariant/issues/789:348,deployability,releas,release-notes,348,"Here is my diff for run-preset.sh edits:. ```. 274c274. < pip3 install ""${PIP_ARGS[@]}"" tensorrt==8.5.3.1. ---. > pip3 install ""${PIP_ARGS[@]}"" nvidia-tensorrt. 276c276. < pip3 show tensorrt. ---. > pip3 show nvidia-tensorrt. 278,285c278,282. < ## In v8.6.1, the libs got moved to tensorrt_libs:. < ## https://docs.nvidia.com/deeplearning/tensorrt/release-notes/index.html#rel-8-6-1. < ##sudo ln -sf ""${TENSORRT_PATH}_libs/libnvinfer.so.8"" ""${TENSORRT_PATH}_libs/libnvinfer.so.7"". < ##sudo ln -sf ""${TENSORRT_PATH}_libs/libnvinfer_plugin.so.8"" ""${TENSORRT_PATH}_libs/libnvinfer_plugin.so.7"". < ##export LD_LIBRARY_PATH=""${LD_LIBRARY_PATH-}:${TENSORRT_PATH}_libs"". < sudo ln -sf ""${TENSORRT_PATH}/libnvinfer.so.8"" ""${TENSORRT_PATH}/libnvinfer.so.7"". < sudo ln -sf ""${TENSORRT_PATH}/libnvinfer_plugin.so.8"" ""${TENSORRT_PATH}/libnvinfer_plugin.so.7"". < export LD_LIBRARY_PATH=""${LD_LIBRARY_PATH-}:${TENSORRT_PATH}"". ---. > # In v8.6.1, the libs got moved to tensorrt_libs:. > # https://docs.nvidia.com/deeplearning/tensorrt/release-notes/index.html#rel-8-6-1. > sudo ln -sf ""${TENSORRT_PATH}_libs/libnvinfer.so.8"" ""${TENSORRT_PATH}_libs/libnvinfer.so.7"". > sudo ln -sf ""${TENSORRT_PATH}_libs/libnvinfer_plugin.so.8"" ""${TENSORRT_PATH}_libs/libnvinfer_plugin.so.7"". > export LD_LIBRARY_PATH=""${LD_LIBRARY_PATH-}:${TENSORRT_PATH}_libs"". 292,295c289,290. < ##sudo ln -sf ""${TENSORRT_PATH}_libs/libnvinfer.so.7"" /usr/local/nvidia/lib/libnvinfer.so.7. < ##sudo ln -sf ""${TENSORRT_PATH}_libs/libnvinfer_plugin.so.7"" /usr/local/nvidia/lib/libnvinfer_plugin.so.7. < sudo ln -sf ""${TENSORRT_PATH}/libnvinfer.so.7"" /usr/local/nvidia/lib/libnvinfer.so.7. < sudo ln -sf ""${TENSORRT_PATH}/libnvinfer_plugin.so.7"" /usr/local/nvidia/lib/libnvinfer_plugin.so.7. ---. > sudo ln -sf ""${TENSORRT_PATH}_libs/libnvinfer.so.7"" /usr/local/nvidia/lib/libnvinfer.so.7. > sudo ln -sf ""${TENSORRT_PATH}_libs/libnvinfer_plugin.so.7"" /usr/local/nvidia/lib/libnvinfer_plugin.so.7. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/789
https://github.com/google/deepvariant/issues/789:1021,deployability,releas,release-notes,1021,"Here is my diff for run-preset.sh edits:. ```. 274c274. < pip3 install ""${PIP_ARGS[@]}"" tensorrt==8.5.3.1. ---. > pip3 install ""${PIP_ARGS[@]}"" nvidia-tensorrt. 276c276. < pip3 show tensorrt. ---. > pip3 show nvidia-tensorrt. 278,285c278,282. < ## In v8.6.1, the libs got moved to tensorrt_libs:. < ## https://docs.nvidia.com/deeplearning/tensorrt/release-notes/index.html#rel-8-6-1. < ##sudo ln -sf ""${TENSORRT_PATH}_libs/libnvinfer.so.8"" ""${TENSORRT_PATH}_libs/libnvinfer.so.7"". < ##sudo ln -sf ""${TENSORRT_PATH}_libs/libnvinfer_plugin.so.8"" ""${TENSORRT_PATH}_libs/libnvinfer_plugin.so.7"". < ##export LD_LIBRARY_PATH=""${LD_LIBRARY_PATH-}:${TENSORRT_PATH}_libs"". < sudo ln -sf ""${TENSORRT_PATH}/libnvinfer.so.8"" ""${TENSORRT_PATH}/libnvinfer.so.7"". < sudo ln -sf ""${TENSORRT_PATH}/libnvinfer_plugin.so.8"" ""${TENSORRT_PATH}/libnvinfer_plugin.so.7"". < export LD_LIBRARY_PATH=""${LD_LIBRARY_PATH-}:${TENSORRT_PATH}"". ---. > # In v8.6.1, the libs got moved to tensorrt_libs:. > # https://docs.nvidia.com/deeplearning/tensorrt/release-notes/index.html#rel-8-6-1. > sudo ln -sf ""${TENSORRT_PATH}_libs/libnvinfer.so.8"" ""${TENSORRT_PATH}_libs/libnvinfer.so.7"". > sudo ln -sf ""${TENSORRT_PATH}_libs/libnvinfer_plugin.so.8"" ""${TENSORRT_PATH}_libs/libnvinfer_plugin.so.7"". > export LD_LIBRARY_PATH=""${LD_LIBRARY_PATH-}:${TENSORRT_PATH}_libs"". 292,295c289,290. < ##sudo ln -sf ""${TENSORRT_PATH}_libs/libnvinfer.so.7"" /usr/local/nvidia/lib/libnvinfer.so.7. < ##sudo ln -sf ""${TENSORRT_PATH}_libs/libnvinfer_plugin.so.7"" /usr/local/nvidia/lib/libnvinfer_plugin.so.7. < sudo ln -sf ""${TENSORRT_PATH}/libnvinfer.so.7"" /usr/local/nvidia/lib/libnvinfer.so.7. < sudo ln -sf ""${TENSORRT_PATH}/libnvinfer_plugin.so.7"" /usr/local/nvidia/lib/libnvinfer_plugin.so.7. ---. > sudo ln -sf ""${TENSORRT_PATH}_libs/libnvinfer.so.7"" /usr/local/nvidia/lib/libnvinfer.so.7. > sudo ln -sf ""${TENSORRT_PATH}_libs/libnvinfer_plugin.so.7"" /usr/local/nvidia/lib/libnvinfer_plugin.so.7. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/789
https://github.com/google/deepvariant/issues/789:57,deployability,releas,release,57,"Thanks @carsonhh . I'll plan to fix this before our next release (1.7.0). We have a small fix recently (1.6.1) which won't have this updated yet. For the 1.7.0 release, I'm also working on updating Python version to 3.10 (and use Ubuntu 22.04 as base image). I'll try your change with those changes and see if it works! I have filed an internal issue to track, so I'll close this. Thanks again for your suggestion!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/789
https://github.com/google/deepvariant/issues/789:133,deployability,updat,updated,133,"Thanks @carsonhh . I'll plan to fix this before our next release (1.7.0). We have a small fix recently (1.6.1) which won't have this updated yet. For the 1.7.0 release, I'm also working on updating Python version to 3.10 (and use Ubuntu 22.04 as base image). I'll try your change with those changes and see if it works! I have filed an internal issue to track, so I'll close this. Thanks again for your suggestion!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/789
https://github.com/google/deepvariant/issues/789:160,deployability,releas,release,160,"Thanks @carsonhh . I'll plan to fix this before our next release (1.7.0). We have a small fix recently (1.6.1) which won't have this updated yet. For the 1.7.0 release, I'm also working on updating Python version to 3.10 (and use Ubuntu 22.04 as base image). I'll try your change with those changes and see if it works! I have filed an internal issue to track, so I'll close this. Thanks again for your suggestion!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/789
https://github.com/google/deepvariant/issues/789:189,deployability,updat,updating,189,"Thanks @carsonhh . I'll plan to fix this before our next release (1.7.0). We have a small fix recently (1.6.1) which won't have this updated yet. For the 1.7.0 release, I'm also working on updating Python version to 3.10 (and use Ubuntu 22.04 as base image). I'll try your change with those changes and see if it works! I have filed an internal issue to track, so I'll close this. Thanks again for your suggestion!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/789
https://github.com/google/deepvariant/issues/789:205,deployability,version,version,205,"Thanks @carsonhh . I'll plan to fix this before our next release (1.7.0). We have a small fix recently (1.6.1) which won't have this updated yet. For the 1.7.0 release, I'm also working on updating Python version to 3.10 (and use Ubuntu 22.04 as base image). I'll try your change with those changes and see if it works! I have filed an internal issue to track, so I'll close this. Thanks again for your suggestion!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/789
https://github.com/google/deepvariant/issues/789:205,integrability,version,version,205,"Thanks @carsonhh . I'll plan to fix this before our next release (1.7.0). We have a small fix recently (1.6.1) which won't have this updated yet. For the 1.7.0 release, I'm also working on updating Python version to 3.10 (and use Ubuntu 22.04 as base image). I'll try your change with those changes and see if it works! I have filed an internal issue to track, so I'll close this. Thanks again for your suggestion!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/789
https://github.com/google/deepvariant/issues/789:205,modifiability,version,version,205,"Thanks @carsonhh . I'll plan to fix this before our next release (1.7.0). We have a small fix recently (1.6.1) which won't have this updated yet. For the 1.7.0 release, I'm also working on updating Python version to 3.10 (and use Ubuntu 22.04 as base image). I'll try your change with those changes and see if it works! I have filed an internal issue to track, so I'll close this. Thanks again for your suggestion!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/789
https://github.com/google/deepvariant/issues/789:133,safety,updat,updated,133,"Thanks @carsonhh . I'll plan to fix this before our next release (1.7.0). We have a small fix recently (1.6.1) which won't have this updated yet. For the 1.7.0 release, I'm also working on updating Python version to 3.10 (and use Ubuntu 22.04 as base image). I'll try your change with those changes and see if it works! I have filed an internal issue to track, so I'll close this. Thanks again for your suggestion!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/789
https://github.com/google/deepvariant/issues/789:189,safety,updat,updating,189,"Thanks @carsonhh . I'll plan to fix this before our next release (1.7.0). We have a small fix recently (1.6.1) which won't have this updated yet. For the 1.7.0 release, I'm also working on updating Python version to 3.10 (and use Ubuntu 22.04 as base image). I'll try your change with those changes and see if it works! I have filed an internal issue to track, so I'll close this. Thanks again for your suggestion!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/789
https://github.com/google/deepvariant/issues/789:133,security,updat,updated,133,"Thanks @carsonhh . I'll plan to fix this before our next release (1.7.0). We have a small fix recently (1.6.1) which won't have this updated yet. For the 1.7.0 release, I'm also working on updating Python version to 3.10 (and use Ubuntu 22.04 as base image). I'll try your change with those changes and see if it works! I have filed an internal issue to track, so I'll close this. Thanks again for your suggestion!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/789
https://github.com/google/deepvariant/issues/789:189,security,updat,updating,189,"Thanks @carsonhh . I'll plan to fix this before our next release (1.7.0). We have a small fix recently (1.6.1) which won't have this updated yet. For the 1.7.0 release, I'm also working on updating Python version to 3.10 (and use Ubuntu 22.04 as base image). I'll try your change with those changes and see if it works! I have filed an internal issue to track, so I'll close this. Thanks again for your suggestion!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/789
https://github.com/google/deepvariant/issues/789:24,testability,plan,plan,24,"Thanks @carsonhh . I'll plan to fix this before our next release (1.7.0). We have a small fix recently (1.6.1) which won't have this updated yet. For the 1.7.0 release, I'm also working on updating Python version to 3.10 (and use Ubuntu 22.04 as base image). I'll try your change with those changes and see if it works! I have filed an internal issue to track, so I'll close this. Thanks again for your suggestion!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/789
https://github.com/google/deepvariant/issues/789:369,usability,close,close,369,"Thanks @carsonhh . I'll plan to fix this before our next release (1.7.0). We have a small fix recently (1.6.1) which won't have this updated yet. For the 1.7.0 release, I'm also working on updating Python version to 3.10 (and use Ubuntu 22.04 as base image). I'll try your change with those changes and see if it works! I have filed an internal issue to track, so I'll close this. Thanks again for your suggestion!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/789
https://github.com/google/deepvariant/issues/790:929,deployability,build,building,929,"Hi @rickymagner ,. If you want to learn more about channels, you can take look at this blog post:. https://google.github.io/deepvariant/posts/2022-06-09-adding-custom-channels/. Two things to mention:. 1. This would be an advanced use of the DeepVariant codebase, both from the research and engineering aspects. We can try to answer your questions, but can't guarantee that we'll have bandwidth or knowledge to support your use case all the way. 2. Since r1.6, we've made quite a lot of refactoring regarding the channels internal implementation. These new code will come out (with documentation) in a future r1.7, but right now it's not quite ready yet. That said, if you have a specific use case now, it's worth looking at the blog post and the r1.6 to try to implement your own channel! If you have any findings or questions, feel free to share or ask here. We can try our best to answer. I've always enjoyed seeing our users building on top of DeepVariant!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/790
https://github.com/google/deepvariant/issues/790:680,interoperability,specif,specific,680,"Hi @rickymagner ,. If you want to learn more about channels, you can take look at this blog post:. https://google.github.io/deepvariant/posts/2022-06-09-adding-custom-channels/. Two things to mention:. 1. This would be an advanced use of the DeepVariant codebase, both from the research and engineering aspects. We can try to answer your questions, but can't guarantee that we'll have bandwidth or knowledge to support your use case all the way. 2. Since r1.6, we've made quite a lot of refactoring regarding the channels internal implementation. These new code will come out (with documentation) in a future r1.7, but right now it's not quite ready yet. That said, if you have a specific use case now, it's worth looking at the blog post and the r1.6 to try to implement your own channel! If you have any findings or questions, feel free to share or ask here. We can try our best to answer. I've always enjoyed seeing our users building on top of DeepVariant!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/790
https://github.com/google/deepvariant/issues/790:842,interoperability,share,share,842,"Hi @rickymagner ,. If you want to learn more about channels, you can take look at this blog post:. https://google.github.io/deepvariant/posts/2022-06-09-adding-custom-channels/. Two things to mention:. 1. This would be an advanced use of the DeepVariant codebase, both from the research and engineering aspects. We can try to answer your questions, but can't guarantee that we'll have bandwidth or knowledge to support your use case all the way. 2. Since r1.6, we've made quite a lot of refactoring regarding the channels internal implementation. These new code will come out (with documentation) in a future r1.7, but right now it's not quite ready yet. That said, if you have a specific use case now, it's worth looking at the blog post and the r1.6 to try to implement your own channel! If you have any findings or questions, feel free to share or ask here. We can try our best to answer. I've always enjoyed seeing our users building on top of DeepVariant!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/790
https://github.com/google/deepvariant/issues/790:487,modifiability,refact,refactoring,487,"Hi @rickymagner ,. If you want to learn more about channels, you can take look at this blog post:. https://google.github.io/deepvariant/posts/2022-06-09-adding-custom-channels/. Two things to mention:. 1. This would be an advanced use of the DeepVariant codebase, both from the research and engineering aspects. We can try to answer your questions, but can't guarantee that we'll have bandwidth or knowledge to support your use case all the way. 2. Since r1.6, we've made quite a lot of refactoring regarding the channels internal implementation. These new code will come out (with documentation) in a future r1.7, but right now it's not quite ready yet. That said, if you have a specific use case now, it's worth looking at the blog post and the r1.6 to try to implement your own channel! If you have any findings or questions, feel free to share or ask here. We can try our best to answer. I've always enjoyed seeing our users building on top of DeepVariant!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/790
https://github.com/google/deepvariant/issues/790:487,performance,refactor,refactoring,487,"Hi @rickymagner ,. If you want to learn more about channels, you can take look at this blog post:. https://google.github.io/deepvariant/posts/2022-06-09-adding-custom-channels/. Two things to mention:. 1. This would be an advanced use of the DeepVariant codebase, both from the research and engineering aspects. We can try to answer your questions, but can't guarantee that we'll have bandwidth or knowledge to support your use case all the way. 2. Since r1.6, we've made quite a lot of refactoring regarding the channels internal implementation. These new code will come out (with documentation) in a future r1.7, but right now it's not quite ready yet. That said, if you have a specific use case now, it's worth looking at the blog post and the r1.6 to try to implement your own channel! If you have any findings or questions, feel free to share or ask here. We can try our best to answer. I've always enjoyed seeing our users building on top of DeepVariant!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/790
https://github.com/google/deepvariant/issues/790:34,usability,learn,learn,34,"Hi @rickymagner ,. If you want to learn more about channels, you can take look at this blog post:. https://google.github.io/deepvariant/posts/2022-06-09-adding-custom-channels/. Two things to mention:. 1. This would be an advanced use of the DeepVariant codebase, both from the research and engineering aspects. We can try to answer your questions, but can't guarantee that we'll have bandwidth or knowledge to support your use case all the way. 2. Since r1.6, we've made quite a lot of refactoring regarding the channels internal implementation. These new code will come out (with documentation) in a future r1.7, but right now it's not quite ready yet. That said, if you have a specific use case now, it's worth looking at the blog post and the r1.6 to try to implement your own channel! If you have any findings or questions, feel free to share or ask here. We can try our best to answer. I've always enjoyed seeing our users building on top of DeepVariant!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/790
https://github.com/google/deepvariant/issues/790:160,usability,custom,custom-channels,160,"Hi @rickymagner ,. If you want to learn more about channels, you can take look at this blog post:. https://google.github.io/deepvariant/posts/2022-06-09-adding-custom-channels/. Two things to mention:. 1. This would be an advanced use of the DeepVariant codebase, both from the research and engineering aspects. We can try to answer your questions, but can't guarantee that we'll have bandwidth or knowledge to support your use case all the way. 2. Since r1.6, we've made quite a lot of refactoring regarding the channels internal implementation. These new code will come out (with documentation) in a future r1.7, but right now it's not quite ready yet. That said, if you have a specific use case now, it's worth looking at the blog post and the r1.6 to try to implement your own channel! If you have any findings or questions, feel free to share or ask here. We can try our best to answer. I've always enjoyed seeing our users building on top of DeepVariant!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/790
https://github.com/google/deepvariant/issues/790:411,usability,support,support,411,"Hi @rickymagner ,. If you want to learn more about channels, you can take look at this blog post:. https://google.github.io/deepvariant/posts/2022-06-09-adding-custom-channels/. Two things to mention:. 1. This would be an advanced use of the DeepVariant codebase, both from the research and engineering aspects. We can try to answer your questions, but can't guarantee that we'll have bandwidth or knowledge to support your use case all the way. 2. Since r1.6, we've made quite a lot of refactoring regarding the channels internal implementation. These new code will come out (with documentation) in a future r1.7, but right now it's not quite ready yet. That said, if you have a specific use case now, it's worth looking at the blog post and the r1.6 to try to implement your own channel! If you have any findings or questions, feel free to share or ask here. We can try our best to answer. I've always enjoyed seeing our users building on top of DeepVariant!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/790
https://github.com/google/deepvariant/issues/790:582,usability,document,documentation,582,"Hi @rickymagner ,. If you want to learn more about channels, you can take look at this blog post:. https://google.github.io/deepvariant/posts/2022-06-09-adding-custom-channels/. Two things to mention:. 1. This would be an advanced use of the DeepVariant codebase, both from the research and engineering aspects. We can try to answer your questions, but can't guarantee that we'll have bandwidth or knowledge to support your use case all the way. 2. Since r1.6, we've made quite a lot of refactoring regarding the channels internal implementation. These new code will come out (with documentation) in a future r1.7, but right now it's not quite ready yet. That said, if you have a specific use case now, it's worth looking at the blog post and the r1.6 to try to implement your own channel! If you have any findings or questions, feel free to share or ask here. We can try our best to answer. I've always enjoyed seeing our users building on top of DeepVariant!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/790
https://github.com/google/deepvariant/issues/790:923,usability,user,users,923,"Hi @rickymagner ,. If you want to learn more about channels, you can take look at this blog post:. https://google.github.io/deepvariant/posts/2022-06-09-adding-custom-channels/. Two things to mention:. 1. This would be an advanced use of the DeepVariant codebase, both from the research and engineering aspects. We can try to answer your questions, but can't guarantee that we'll have bandwidth or knowledge to support your use case all the way. 2. Since r1.6, we've made quite a lot of refactoring regarding the channels internal implementation. These new code will come out (with documentation) in a future r1.7, but right now it's not quite ready yet. That said, if you have a specific use case now, it's worth looking at the blog post and the r1.6 to try to implement your own channel! If you have any findings or questions, feel free to share or ask here. We can try our best to answer. I've always enjoyed seeing our users building on top of DeepVariant!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/790
https://github.com/google/deepvariant/issues/790:57,usability,help,helpful,57,"Hi @pichuan, thanks for the reply! The blog post is very helpful. I'm excited to hear that 1.7 might have more support for this type of advanced use case. I hope I'll be able to give 1.6 a try before then. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/790
https://github.com/google/deepvariant/issues/790:111,usability,support,support,111,"Hi @pichuan, thanks for the reply! The blog post is very helpful. I'm excited to hear that 1.7 might have more support for this type of advanced use case. I hope I'll be able to give 1.6 a try before then. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/790
https://github.com/google/deepvariant/issues/791:623,availability,sli,slight,623,"> Do you recommend read trimming before alignment using tools such as fastp? No - we don't perform read trimming and in fact, this can hurt model performance (because we trained on untrimmed data). We observed this when running our models on processed with Opossum (which performs trimming):. <img width=""1167"" alt=""image"" src=""https://github.com/google/deepvariant/assets/1536935/4e0cd335-0818-4cec-ae0c-ef294e98da39"">. > I can see that this is the repository for the human reference genome, which DeepVariant recommends. You can use a reference with or without alt contigs. With ALT contigs we have previously observed a slight increase in error rates with other models. I have not investigated whether they have an impact with our RNA-seq model. __Note:__ I have updated this after speaking with my teammates regarding this question.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/791
https://github.com/google/deepvariant/issues/791:642,availability,error,error,642,"> Do you recommend read trimming before alignment using tools such as fastp? No - we don't perform read trimming and in fact, this can hurt model performance (because we trained on untrimmed data). We observed this when running our models on processed with Opossum (which performs trimming):. <img width=""1167"" alt=""image"" src=""https://github.com/google/deepvariant/assets/1536935/4e0cd335-0818-4cec-ae0c-ef294e98da39"">. > I can see that this is the repository for the human reference genome, which DeepVariant recommends. You can use a reference with or without alt contigs. With ALT contigs we have previously observed a slight increase in error rates with other models. I have not investigated whether they have an impact with our RNA-seq model. __Note:__ I have updated this after speaking with my teammates regarding this question.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/791
https://github.com/google/deepvariant/issues/791:201,deployability,observ,observed,201,"> Do you recommend read trimming before alignment using tools such as fastp? No - we don't perform read trimming and in fact, this can hurt model performance (because we trained on untrimmed data). We observed this when running our models on processed with Opossum (which performs trimming):. <img width=""1167"" alt=""image"" src=""https://github.com/google/deepvariant/assets/1536935/4e0cd335-0818-4cec-ae0c-ef294e98da39"">. > I can see that this is the repository for the human reference genome, which DeepVariant recommends. You can use a reference with or without alt contigs. With ALT contigs we have previously observed a slight increase in error rates with other models. I have not investigated whether they have an impact with our RNA-seq model. __Note:__ I have updated this after speaking with my teammates regarding this question.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/791
https://github.com/google/deepvariant/issues/791:612,deployability,observ,observed,612,"> Do you recommend read trimming before alignment using tools such as fastp? No - we don't perform read trimming and in fact, this can hurt model performance (because we trained on untrimmed data). We observed this when running our models on processed with Opossum (which performs trimming):. <img width=""1167"" alt=""image"" src=""https://github.com/google/deepvariant/assets/1536935/4e0cd335-0818-4cec-ae0c-ef294e98da39"">. > I can see that this is the repository for the human reference genome, which DeepVariant recommends. You can use a reference with or without alt contigs. With ALT contigs we have previously observed a slight increase in error rates with other models. I have not investigated whether they have an impact with our RNA-seq model. __Note:__ I have updated this after speaking with my teammates regarding this question.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/791
https://github.com/google/deepvariant/issues/791:766,deployability,updat,updated,766,"> Do you recommend read trimming before alignment using tools such as fastp? No - we don't perform read trimming and in fact, this can hurt model performance (because we trained on untrimmed data). We observed this when running our models on processed with Opossum (which performs trimming):. <img width=""1167"" alt=""image"" src=""https://github.com/google/deepvariant/assets/1536935/4e0cd335-0818-4cec-ae0c-ef294e98da39"">. > I can see that this is the repository for the human reference genome, which DeepVariant recommends. You can use a reference with or without alt contigs. With ALT contigs we have previously observed a slight increase in error rates with other models. I have not investigated whether they have an impact with our RNA-seq model. __Note:__ I have updated this after speaking with my teammates regarding this question.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/791
https://github.com/google/deepvariant/issues/791:140,energy efficiency,model,model,140,"> Do you recommend read trimming before alignment using tools such as fastp? No - we don't perform read trimming and in fact, this can hurt model performance (because we trained on untrimmed data). We observed this when running our models on processed with Opossum (which performs trimming):. <img width=""1167"" alt=""image"" src=""https://github.com/google/deepvariant/assets/1536935/4e0cd335-0818-4cec-ae0c-ef294e98da39"">. > I can see that this is the repository for the human reference genome, which DeepVariant recommends. You can use a reference with or without alt contigs. With ALT contigs we have previously observed a slight increase in error rates with other models. I have not investigated whether they have an impact with our RNA-seq model. __Note:__ I have updated this after speaking with my teammates regarding this question.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/791
https://github.com/google/deepvariant/issues/791:232,energy efficiency,model,models,232,"> Do you recommend read trimming before alignment using tools such as fastp? No - we don't perform read trimming and in fact, this can hurt model performance (because we trained on untrimmed data). We observed this when running our models on processed with Opossum (which performs trimming):. <img width=""1167"" alt=""image"" src=""https://github.com/google/deepvariant/assets/1536935/4e0cd335-0818-4cec-ae0c-ef294e98da39"">. > I can see that this is the repository for the human reference genome, which DeepVariant recommends. You can use a reference with or without alt contigs. With ALT contigs we have previously observed a slight increase in error rates with other models. I have not investigated whether they have an impact with our RNA-seq model. __Note:__ I have updated this after speaking with my teammates regarding this question.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/791
https://github.com/google/deepvariant/issues/791:665,energy efficiency,model,models,665,"> Do you recommend read trimming before alignment using tools such as fastp? No - we don't perform read trimming and in fact, this can hurt model performance (because we trained on untrimmed data). We observed this when running our models on processed with Opossum (which performs trimming):. <img width=""1167"" alt=""image"" src=""https://github.com/google/deepvariant/assets/1536935/4e0cd335-0818-4cec-ae0c-ef294e98da39"">. > I can see that this is the repository for the human reference genome, which DeepVariant recommends. You can use a reference with or without alt contigs. With ALT contigs we have previously observed a slight increase in error rates with other models. I have not investigated whether they have an impact with our RNA-seq model. __Note:__ I have updated this after speaking with my teammates regarding this question.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/791
https://github.com/google/deepvariant/issues/791:742,energy efficiency,model,model,742,"> Do you recommend read trimming before alignment using tools such as fastp? No - we don't perform read trimming and in fact, this can hurt model performance (because we trained on untrimmed data). We observed this when running our models on processed with Opossum (which performs trimming):. <img width=""1167"" alt=""image"" src=""https://github.com/google/deepvariant/assets/1536935/4e0cd335-0818-4cec-ae0c-ef294e98da39"">. > I can see that this is the repository for the human reference genome, which DeepVariant recommends. You can use a reference with or without alt contigs. With ALT contigs we have previously observed a slight increase in error rates with other models. I have not investigated whether they have an impact with our RNA-seq model. __Note:__ I have updated this after speaking with my teammates regarding this question.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/791
https://github.com/google/deepvariant/issues/791:450,integrability,repositor,repository,450,"> Do you recommend read trimming before alignment using tools such as fastp? No - we don't perform read trimming and in fact, this can hurt model performance (because we trained on untrimmed data). We observed this when running our models on processed with Opossum (which performs trimming):. <img width=""1167"" alt=""image"" src=""https://github.com/google/deepvariant/assets/1536935/4e0cd335-0818-4cec-ae0c-ef294e98da39"">. > I can see that this is the repository for the human reference genome, which DeepVariant recommends. You can use a reference with or without alt contigs. With ALT contigs we have previously observed a slight increase in error rates with other models. I have not investigated whether they have an impact with our RNA-seq model. __Note:__ I have updated this after speaking with my teammates regarding this question.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/791
https://github.com/google/deepvariant/issues/791:450,interoperability,repositor,repository,450,"> Do you recommend read trimming before alignment using tools such as fastp? No - we don't perform read trimming and in fact, this can hurt model performance (because we trained on untrimmed data). We observed this when running our models on processed with Opossum (which performs trimming):. <img width=""1167"" alt=""image"" src=""https://github.com/google/deepvariant/assets/1536935/4e0cd335-0818-4cec-ae0c-ef294e98da39"">. > I can see that this is the repository for the human reference genome, which DeepVariant recommends. You can use a reference with or without alt contigs. With ALT contigs we have previously observed a slight increase in error rates with other models. I have not investigated whether they have an impact with our RNA-seq model. __Note:__ I have updated this after speaking with my teammates regarding this question.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/791
https://github.com/google/deepvariant/issues/791:91,performance,perform,perform,91,"> Do you recommend read trimming before alignment using tools such as fastp? No - we don't perform read trimming and in fact, this can hurt model performance (because we trained on untrimmed data). We observed this when running our models on processed with Opossum (which performs trimming):. <img width=""1167"" alt=""image"" src=""https://github.com/google/deepvariant/assets/1536935/4e0cd335-0818-4cec-ae0c-ef294e98da39"">. > I can see that this is the repository for the human reference genome, which DeepVariant recommends. You can use a reference with or without alt contigs. With ALT contigs we have previously observed a slight increase in error rates with other models. I have not investigated whether they have an impact with our RNA-seq model. __Note:__ I have updated this after speaking with my teammates regarding this question.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/791
https://github.com/google/deepvariant/issues/791:146,performance,perform,performance,146,"> Do you recommend read trimming before alignment using tools such as fastp? No - we don't perform read trimming and in fact, this can hurt model performance (because we trained on untrimmed data). We observed this when running our models on processed with Opossum (which performs trimming):. <img width=""1167"" alt=""image"" src=""https://github.com/google/deepvariant/assets/1536935/4e0cd335-0818-4cec-ae0c-ef294e98da39"">. > I can see that this is the repository for the human reference genome, which DeepVariant recommends. You can use a reference with or without alt contigs. With ALT contigs we have previously observed a slight increase in error rates with other models. I have not investigated whether they have an impact with our RNA-seq model. __Note:__ I have updated this after speaking with my teammates regarding this question.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/791
https://github.com/google/deepvariant/issues/791:272,performance,perform,performs,272,"> Do you recommend read trimming before alignment using tools such as fastp? No - we don't perform read trimming and in fact, this can hurt model performance (because we trained on untrimmed data). We observed this when running our models on processed with Opossum (which performs trimming):. <img width=""1167"" alt=""image"" src=""https://github.com/google/deepvariant/assets/1536935/4e0cd335-0818-4cec-ae0c-ef294e98da39"">. > I can see that this is the repository for the human reference genome, which DeepVariant recommends. You can use a reference with or without alt contigs. With ALT contigs we have previously observed a slight increase in error rates with other models. I have not investigated whether they have an impact with our RNA-seq model. __Note:__ I have updated this after speaking with my teammates regarding this question.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/791
https://github.com/google/deepvariant/issues/791:642,performance,error,error,642,"> Do you recommend read trimming before alignment using tools such as fastp? No - we don't perform read trimming and in fact, this can hurt model performance (because we trained on untrimmed data). We observed this when running our models on processed with Opossum (which performs trimming):. <img width=""1167"" alt=""image"" src=""https://github.com/google/deepvariant/assets/1536935/4e0cd335-0818-4cec-ae0c-ef294e98da39"">. > I can see that this is the repository for the human reference genome, which DeepVariant recommends. You can use a reference with or without alt contigs. With ALT contigs we have previously observed a slight increase in error rates with other models. I have not investigated whether they have an impact with our RNA-seq model. __Note:__ I have updated this after speaking with my teammates regarding this question.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/791
https://github.com/google/deepvariant/issues/791:623,reliability,sli,slight,623,"> Do you recommend read trimming before alignment using tools such as fastp? No - we don't perform read trimming and in fact, this can hurt model performance (because we trained on untrimmed data). We observed this when running our models on processed with Opossum (which performs trimming):. <img width=""1167"" alt=""image"" src=""https://github.com/google/deepvariant/assets/1536935/4e0cd335-0818-4cec-ae0c-ef294e98da39"">. > I can see that this is the repository for the human reference genome, which DeepVariant recommends. You can use a reference with or without alt contigs. With ALT contigs we have previously observed a slight increase in error rates with other models. I have not investigated whether they have an impact with our RNA-seq model. __Note:__ I have updated this after speaking with my teammates regarding this question.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/791
https://github.com/google/deepvariant/issues/791:642,safety,error,error,642,"> Do you recommend read trimming before alignment using tools such as fastp? No - we don't perform read trimming and in fact, this can hurt model performance (because we trained on untrimmed data). We observed this when running our models on processed with Opossum (which performs trimming):. <img width=""1167"" alt=""image"" src=""https://github.com/google/deepvariant/assets/1536935/4e0cd335-0818-4cec-ae0c-ef294e98da39"">. > I can see that this is the repository for the human reference genome, which DeepVariant recommends. You can use a reference with or without alt contigs. With ALT contigs we have previously observed a slight increase in error rates with other models. I have not investigated whether they have an impact with our RNA-seq model. __Note:__ I have updated this after speaking with my teammates regarding this question.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/791
https://github.com/google/deepvariant/issues/791:766,safety,updat,updated,766,"> Do you recommend read trimming before alignment using tools such as fastp? No - we don't perform read trimming and in fact, this can hurt model performance (because we trained on untrimmed data). We observed this when running our models on processed with Opossum (which performs trimming):. <img width=""1167"" alt=""image"" src=""https://github.com/google/deepvariant/assets/1536935/4e0cd335-0818-4cec-ae0c-ef294e98da39"">. > I can see that this is the repository for the human reference genome, which DeepVariant recommends. You can use a reference with or without alt contigs. With ALT contigs we have previously observed a slight increase in error rates with other models. I have not investigated whether they have an impact with our RNA-seq model. __Note:__ I have updated this after speaking with my teammates regarding this question.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/791
https://github.com/google/deepvariant/issues/791:140,security,model,model,140,"> Do you recommend read trimming before alignment using tools such as fastp? No - we don't perform read trimming and in fact, this can hurt model performance (because we trained on untrimmed data). We observed this when running our models on processed with Opossum (which performs trimming):. <img width=""1167"" alt=""image"" src=""https://github.com/google/deepvariant/assets/1536935/4e0cd335-0818-4cec-ae0c-ef294e98da39"">. > I can see that this is the repository for the human reference genome, which DeepVariant recommends. You can use a reference with or without alt contigs. With ALT contigs we have previously observed a slight increase in error rates with other models. I have not investigated whether they have an impact with our RNA-seq model. __Note:__ I have updated this after speaking with my teammates regarding this question.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/791
https://github.com/google/deepvariant/issues/791:232,security,model,models,232,"> Do you recommend read trimming before alignment using tools such as fastp? No - we don't perform read trimming and in fact, this can hurt model performance (because we trained on untrimmed data). We observed this when running our models on processed with Opossum (which performs trimming):. <img width=""1167"" alt=""image"" src=""https://github.com/google/deepvariant/assets/1536935/4e0cd335-0818-4cec-ae0c-ef294e98da39"">. > I can see that this is the repository for the human reference genome, which DeepVariant recommends. You can use a reference with or without alt contigs. With ALT contigs we have previously observed a slight increase in error rates with other models. I have not investigated whether they have an impact with our RNA-seq model. __Note:__ I have updated this after speaking with my teammates regarding this question.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/791
https://github.com/google/deepvariant/issues/791:665,security,model,models,665,"> Do you recommend read trimming before alignment using tools such as fastp? No - we don't perform read trimming and in fact, this can hurt model performance (because we trained on untrimmed data). We observed this when running our models on processed with Opossum (which performs trimming):. <img width=""1167"" alt=""image"" src=""https://github.com/google/deepvariant/assets/1536935/4e0cd335-0818-4cec-ae0c-ef294e98da39"">. > I can see that this is the repository for the human reference genome, which DeepVariant recommends. You can use a reference with or without alt contigs. With ALT contigs we have previously observed a slight increase in error rates with other models. I have not investigated whether they have an impact with our RNA-seq model. __Note:__ I have updated this after speaking with my teammates regarding this question.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/791
https://github.com/google/deepvariant/issues/791:742,security,model,model,742,"> Do you recommend read trimming before alignment using tools such as fastp? No - we don't perform read trimming and in fact, this can hurt model performance (because we trained on untrimmed data). We observed this when running our models on processed with Opossum (which performs trimming):. <img width=""1167"" alt=""image"" src=""https://github.com/google/deepvariant/assets/1536935/4e0cd335-0818-4cec-ae0c-ef294e98da39"">. > I can see that this is the repository for the human reference genome, which DeepVariant recommends. You can use a reference with or without alt contigs. With ALT contigs we have previously observed a slight increase in error rates with other models. I have not investigated whether they have an impact with our RNA-seq model. __Note:__ I have updated this after speaking with my teammates regarding this question.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/791
https://github.com/google/deepvariant/issues/791:766,security,updat,updated,766,"> Do you recommend read trimming before alignment using tools such as fastp? No - we don't perform read trimming and in fact, this can hurt model performance (because we trained on untrimmed data). We observed this when running our models on processed with Opossum (which performs trimming):. <img width=""1167"" alt=""image"" src=""https://github.com/google/deepvariant/assets/1536935/4e0cd335-0818-4cec-ae0c-ef294e98da39"">. > I can see that this is the repository for the human reference genome, which DeepVariant recommends. You can use a reference with or without alt contigs. With ALT contigs we have previously observed a slight increase in error rates with other models. I have not investigated whether they have an impact with our RNA-seq model. __Note:__ I have updated this after speaking with my teammates regarding this question.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/791
https://github.com/google/deepvariant/issues/791:802,security,team,teammates,802,"> Do you recommend read trimming before alignment using tools such as fastp? No - we don't perform read trimming and in fact, this can hurt model performance (because we trained on untrimmed data). We observed this when running our models on processed with Opossum (which performs trimming):. <img width=""1167"" alt=""image"" src=""https://github.com/google/deepvariant/assets/1536935/4e0cd335-0818-4cec-ae0c-ef294e98da39"">. > I can see that this is the repository for the human reference genome, which DeepVariant recommends. You can use a reference with or without alt contigs. With ALT contigs we have previously observed a slight increase in error rates with other models. I have not investigated whether they have an impact with our RNA-seq model. __Note:__ I have updated this after speaking with my teammates regarding this question.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/791
https://github.com/google/deepvariant/issues/791:201,testability,observ,observed,201,"> Do you recommend read trimming before alignment using tools such as fastp? No - we don't perform read trimming and in fact, this can hurt model performance (because we trained on untrimmed data). We observed this when running our models on processed with Opossum (which performs trimming):. <img width=""1167"" alt=""image"" src=""https://github.com/google/deepvariant/assets/1536935/4e0cd335-0818-4cec-ae0c-ef294e98da39"">. > I can see that this is the repository for the human reference genome, which DeepVariant recommends. You can use a reference with or without alt contigs. With ALT contigs we have previously observed a slight increase in error rates with other models. I have not investigated whether they have an impact with our RNA-seq model. __Note:__ I have updated this after speaking with my teammates regarding this question.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/791
https://github.com/google/deepvariant/issues/791:612,testability,observ,observed,612,"> Do you recommend read trimming before alignment using tools such as fastp? No - we don't perform read trimming and in fact, this can hurt model performance (because we trained on untrimmed data). We observed this when running our models on processed with Opossum (which performs trimming):. <img width=""1167"" alt=""image"" src=""https://github.com/google/deepvariant/assets/1536935/4e0cd335-0818-4cec-ae0c-ef294e98da39"">. > I can see that this is the repository for the human reference genome, which DeepVariant recommends. You can use a reference with or without alt contigs. With ALT contigs we have previously observed a slight increase in error rates with other models. I have not investigated whether they have an impact with our RNA-seq model. __Note:__ I have updated this after speaking with my teammates regarding this question.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/791
https://github.com/google/deepvariant/issues/791:56,usability,tool,tools,56,"> Do you recommend read trimming before alignment using tools such as fastp? No - we don't perform read trimming and in fact, this can hurt model performance (because we trained on untrimmed data). We observed this when running our models on processed with Opossum (which performs trimming):. <img width=""1167"" alt=""image"" src=""https://github.com/google/deepvariant/assets/1536935/4e0cd335-0818-4cec-ae0c-ef294e98da39"">. > I can see that this is the repository for the human reference genome, which DeepVariant recommends. You can use a reference with or without alt contigs. With ALT contigs we have previously observed a slight increase in error rates with other models. I have not investigated whether they have an impact with our RNA-seq model. __Note:__ I have updated this after speaking with my teammates regarding this question.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/791
https://github.com/google/deepvariant/issues/791:91,usability,perform,perform,91,"> Do you recommend read trimming before alignment using tools such as fastp? No - we don't perform read trimming and in fact, this can hurt model performance (because we trained on untrimmed data). We observed this when running our models on processed with Opossum (which performs trimming):. <img width=""1167"" alt=""image"" src=""https://github.com/google/deepvariant/assets/1536935/4e0cd335-0818-4cec-ae0c-ef294e98da39"">. > I can see that this is the repository for the human reference genome, which DeepVariant recommends. You can use a reference with or without alt contigs. With ALT contigs we have previously observed a slight increase in error rates with other models. I have not investigated whether they have an impact with our RNA-seq model. __Note:__ I have updated this after speaking with my teammates regarding this question.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/791
https://github.com/google/deepvariant/issues/791:146,usability,perform,performance,146,"> Do you recommend read trimming before alignment using tools such as fastp? No - we don't perform read trimming and in fact, this can hurt model performance (because we trained on untrimmed data). We observed this when running our models on processed with Opossum (which performs trimming):. <img width=""1167"" alt=""image"" src=""https://github.com/google/deepvariant/assets/1536935/4e0cd335-0818-4cec-ae0c-ef294e98da39"">. > I can see that this is the repository for the human reference genome, which DeepVariant recommends. You can use a reference with or without alt contigs. With ALT contigs we have previously observed a slight increase in error rates with other models. I have not investigated whether they have an impact with our RNA-seq model. __Note:__ I have updated this after speaking with my teammates regarding this question.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/791
https://github.com/google/deepvariant/issues/791:272,usability,perform,performs,272,"> Do you recommend read trimming before alignment using tools such as fastp? No - we don't perform read trimming and in fact, this can hurt model performance (because we trained on untrimmed data). We observed this when running our models on processed with Opossum (which performs trimming):. <img width=""1167"" alt=""image"" src=""https://github.com/google/deepvariant/assets/1536935/4e0cd335-0818-4cec-ae0c-ef294e98da39"">. > I can see that this is the repository for the human reference genome, which DeepVariant recommends. You can use a reference with or without alt contigs. With ALT contigs we have previously observed a slight increase in error rates with other models. I have not investigated whether they have an impact with our RNA-seq model. __Note:__ I have updated this after speaking with my teammates regarding this question.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/791
https://github.com/google/deepvariant/issues/791:642,usability,error,error,642,"> Do you recommend read trimming before alignment using tools such as fastp? No - we don't perform read trimming and in fact, this can hurt model performance (because we trained on untrimmed data). We observed this when running our models on processed with Opossum (which performs trimming):. <img width=""1167"" alt=""image"" src=""https://github.com/google/deepvariant/assets/1536935/4e0cd335-0818-4cec-ae0c-ef294e98da39"">. > I can see that this is the repository for the human reference genome, which DeepVariant recommends. You can use a reference with or without alt contigs. With ALT contigs we have previously observed a slight increase in error rates with other models. I have not investigated whether they have an impact with our RNA-seq model. __Note:__ I have updated this after speaking with my teammates regarding this question.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/791
https://github.com/google/deepvariant/issues/791:75,interoperability,share,shared,75,@malonzm1 . I think that this should do fine. Can be found from the link I shared. GCA_000001405.15_GRCh38_full_analysis_set.refseq_annotation.gtf.gz .,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/791
https://github.com/google/deepvariant/issues/791:56,usability,close,close,56,"Hi @j0ntt , this has been inactive for a month, so I'll close it. If there are remaining questions, please feel free to reopen with more details.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/791
https://github.com/google/deepvariant/issues/792:67,energy efficiency,current,currently,67,"Hi @rickymagner ,. to share my own experience, I work at Google so currently use an Google internal IDE. And, I actually don't use autocomplete myself. During my earlier times at Google, I'm actually an emacs user (and I used `emacs -nw` a lot). I have used VScode for smaller projects externally. And many years ago in a startup, I used Sublime Text when I wrote in JS. But, in all cases I don't really use the IDE features very much. So I might not be the best person to ask. It's outside our support scope to provide suggestions for IDE. And of all the engineerings teams I've been on, people seem to have different preferences anyway. I'll leave this open a bit to see if any other readers want to share your own setup :)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/792
https://github.com/google/deepvariant/issues/792:338,integrability,Sub,Sublime,338,"Hi @rickymagner ,. to share my own experience, I work at Google so currently use an Google internal IDE. And, I actually don't use autocomplete myself. During my earlier times at Google, I'm actually an emacs user (and I used `emacs -nw` a lot). I have used VScode for smaller projects externally. And many years ago in a startup, I used Sublime Text when I wrote in JS. But, in all cases I don't really use the IDE features very much. So I might not be the best person to ask. It's outside our support scope to provide suggestions for IDE. And of all the engineerings teams I've been on, people seem to have different preferences anyway. I'll leave this open a bit to see if any other readers want to share your own setup :)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/792
https://github.com/google/deepvariant/issues/792:22,interoperability,share,share,22,"Hi @rickymagner ,. to share my own experience, I work at Google so currently use an Google internal IDE. And, I actually don't use autocomplete myself. During my earlier times at Google, I'm actually an emacs user (and I used `emacs -nw` a lot). I have used VScode for smaller projects externally. And many years ago in a startup, I used Sublime Text when I wrote in JS. But, in all cases I don't really use the IDE features very much. So I might not be the best person to ask. It's outside our support scope to provide suggestions for IDE. And of all the engineerings teams I've been on, people seem to have different preferences anyway. I'll leave this open a bit to see if any other readers want to share your own setup :)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/792
https://github.com/google/deepvariant/issues/792:702,interoperability,share,share,702,"Hi @rickymagner ,. to share my own experience, I work at Google so currently use an Google internal IDE. And, I actually don't use autocomplete myself. During my earlier times at Google, I'm actually an emacs user (and I used `emacs -nw` a lot). I have used VScode for smaller projects externally. And many years ago in a startup, I used Sublime Text when I wrote in JS. But, in all cases I don't really use the IDE features very much. So I might not be the best person to ask. It's outside our support scope to provide suggestions for IDE. And of all the engineerings teams I've been on, people seem to have different preferences anyway. I'll leave this open a bit to see if any other readers want to share your own setup :)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/792
https://github.com/google/deepvariant/issues/792:170,performance,time,times,170,"Hi @rickymagner ,. to share my own experience, I work at Google so currently use an Google internal IDE. And, I actually don't use autocomplete myself. During my earlier times at Google, I'm actually an emacs user (and I used `emacs -nw` a lot). I have used VScode for smaller projects externally. And many years ago in a startup, I used Sublime Text when I wrote in JS. But, in all cases I don't really use the IDE features very much. So I might not be the best person to ask. It's outside our support scope to provide suggestions for IDE. And of all the engineerings teams I've been on, people seem to have different preferences anyway. I'll leave this open a bit to see if any other readers want to share your own setup :)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/792
https://github.com/google/deepvariant/issues/792:569,security,team,teams,569,"Hi @rickymagner ,. to share my own experience, I work at Google so currently use an Google internal IDE. And, I actually don't use autocomplete myself. During my earlier times at Google, I'm actually an emacs user (and I used `emacs -nw` a lot). I have used VScode for smaller projects externally. And many years ago in a startup, I used Sublime Text when I wrote in JS. But, in all cases I don't really use the IDE features very much. So I might not be the best person to ask. It's outside our support scope to provide suggestions for IDE. And of all the engineerings teams I've been on, people seem to have different preferences anyway. I'll leave this open a bit to see if any other readers want to share your own setup :)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/792
https://github.com/google/deepvariant/issues/792:35,usability,experien,experience,35,"Hi @rickymagner ,. to share my own experience, I work at Google so currently use an Google internal IDE. And, I actually don't use autocomplete myself. During my earlier times at Google, I'm actually an emacs user (and I used `emacs -nw` a lot). I have used VScode for smaller projects externally. And many years ago in a startup, I used Sublime Text when I wrote in JS. But, in all cases I don't really use the IDE features very much. So I might not be the best person to ask. It's outside our support scope to provide suggestions for IDE. And of all the engineerings teams I've been on, people seem to have different preferences anyway. I'll leave this open a bit to see if any other readers want to share your own setup :)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/792
https://github.com/google/deepvariant/issues/792:209,usability,user,user,209,"Hi @rickymagner ,. to share my own experience, I work at Google so currently use an Google internal IDE. And, I actually don't use autocomplete myself. During my earlier times at Google, I'm actually an emacs user (and I used `emacs -nw` a lot). I have used VScode for smaller projects externally. And many years ago in a startup, I used Sublime Text when I wrote in JS. But, in all cases I don't really use the IDE features very much. So I might not be the best person to ask. It's outside our support scope to provide suggestions for IDE. And of all the engineerings teams I've been on, people seem to have different preferences anyway. I'll leave this open a bit to see if any other readers want to share your own setup :)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/792
https://github.com/google/deepvariant/issues/792:463,usability,person,person,463,"Hi @rickymagner ,. to share my own experience, I work at Google so currently use an Google internal IDE. And, I actually don't use autocomplete myself. During my earlier times at Google, I'm actually an emacs user (and I used `emacs -nw` a lot). I have used VScode for smaller projects externally. And many years ago in a startup, I used Sublime Text when I wrote in JS. But, in all cases I don't really use the IDE features very much. So I might not be the best person to ask. It's outside our support scope to provide suggestions for IDE. And of all the engineerings teams I've been on, people seem to have different preferences anyway. I'll leave this open a bit to see if any other readers want to share your own setup :)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/792
https://github.com/google/deepvariant/issues/792:495,usability,support,support,495,"Hi @rickymagner ,. to share my own experience, I work at Google so currently use an Google internal IDE. And, I actually don't use autocomplete myself. During my earlier times at Google, I'm actually an emacs user (and I used `emacs -nw` a lot). I have used VScode for smaller projects externally. And many years ago in a startup, I used Sublime Text when I wrote in JS. But, in all cases I don't really use the IDE features very much. So I might not be the best person to ask. It's outside our support scope to provide suggestions for IDE. And of all the engineerings teams I've been on, people seem to have different preferences anyway. I'll leave this open a bit to see if any other readers want to share your own setup :)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/792
https://github.com/google/deepvariant/issues/792:619,usability,prefer,preferences,619,"Hi @rickymagner ,. to share my own experience, I work at Google so currently use an Google internal IDE. And, I actually don't use autocomplete myself. During my earlier times at Google, I'm actually an emacs user (and I used `emacs -nw` a lot). I have used VScode for smaller projects externally. And many years ago in a startup, I used Sublime Text when I wrote in JS. But, in all cases I don't really use the IDE features very much. So I might not be the best person to ask. It's outside our support scope to provide suggestions for IDE. And of all the engineerings teams I've been on, people seem to have different preferences anyway. I'll leave this open a bit to see if any other readers want to share your own setup :)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/792
https://github.com/google/deepvariant/issues/792:121,deployability,instal,install,121,"hi @rickymagner ,. In projects like these, I used to prefer CLion for an IDE. It has many customizable features. You can install the python extension for python support. You can also add some autocomplete extensions (at least last year it was the case). However, it's mostly personal preference in most cases. Hope you find something that works for you.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/792
https://github.com/google/deepvariant/issues/792:140,modifiability,extens,extension,140,"hi @rickymagner ,. In projects like these, I used to prefer CLion for an IDE. It has many customizable features. You can install the python extension for python support. You can also add some autocomplete extensions (at least last year it was the case). However, it's mostly personal preference in most cases. Hope you find something that works for you.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/792
https://github.com/google/deepvariant/issues/792:205,modifiability,extens,extensions,205,"hi @rickymagner ,. In projects like these, I used to prefer CLion for an IDE. It has many customizable features. You can install the python extension for python support. You can also add some autocomplete extensions (at least last year it was the case). However, it's mostly personal preference in most cases. Hope you find something that works for you.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/792
https://github.com/google/deepvariant/issues/792:53,usability,prefer,prefer,53,"hi @rickymagner ,. In projects like these, I used to prefer CLion for an IDE. It has many customizable features. You can install the python extension for python support. You can also add some autocomplete extensions (at least last year it was the case). However, it's mostly personal preference in most cases. Hope you find something that works for you.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/792
https://github.com/google/deepvariant/issues/792:90,usability,custom,customizable,90,"hi @rickymagner ,. In projects like these, I used to prefer CLion for an IDE. It has many customizable features. You can install the python extension for python support. You can also add some autocomplete extensions (at least last year it was the case). However, it's mostly personal preference in most cases. Hope you find something that works for you.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/792
https://github.com/google/deepvariant/issues/792:161,usability,support,support,161,"hi @rickymagner ,. In projects like these, I used to prefer CLion for an IDE. It has many customizable features. You can install the python extension for python support. You can also add some autocomplete extensions (at least last year it was the case). However, it's mostly personal preference in most cases. Hope you find something that works for you.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/792
https://github.com/google/deepvariant/issues/792:275,usability,person,personal,275,"hi @rickymagner ,. In projects like these, I used to prefer CLion for an IDE. It has many customizable features. You can install the python extension for python support. You can also add some autocomplete extensions (at least last year it was the case). However, it's mostly personal preference in most cases. Hope you find something that works for you.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/792
https://github.com/google/deepvariant/issues/792:284,usability,prefer,preference,284,"hi @rickymagner ,. In projects like these, I used to prefer CLion for an IDE. It has many customizable features. You can install the python extension for python support. You can also add some autocomplete extensions (at least last year it was the case). However, it's mostly personal preference in most cases. Hope you find something that works for you.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/792
https://github.com/google/deepvariant/issues/792:115,availability,avail,available,115,"DeepVariant is built with bazel. Internally we have an add-on in CLion to import bazel project. If this plug-in is available you can try to do that. Bazel project is in the file named ""BUILD"" which exists in all sub-directories. If importing bazel project is not available then you may need to create your project from scratch in your IDE and then add all source files.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/792
https://github.com/google/deepvariant/issues/792:263,availability,avail,available,263,"DeepVariant is built with bazel. Internally we have an add-on in CLion to import bazel project. If this plug-in is available you can try to do that. Bazel project is in the file named ""BUILD"" which exists in all sub-directories. If importing bazel project is not available then you may need to create your project from scratch in your IDE and then add all source files.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/792
https://github.com/google/deepvariant/issues/792:185,deployability,BUILD,BUILD,185,"DeepVariant is built with bazel. Internally we have an add-on in CLion to import bazel project. If this plug-in is available you can try to do that. Bazel project is in the file named ""BUILD"" which exists in all sub-directories. If importing bazel project is not available then you may need to create your project from scratch in your IDE and then add all source files.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/792
https://github.com/google/deepvariant/issues/792:104,integrability,plug-in,plug-in,104,"DeepVariant is built with bazel. Internally we have an add-on in CLion to import bazel project. If this plug-in is available you can try to do that. Bazel project is in the file named ""BUILD"" which exists in all sub-directories. If importing bazel project is not available then you may need to create your project from scratch in your IDE and then add all source files.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/792
https://github.com/google/deepvariant/issues/792:212,integrability,sub,sub-directories,212,"DeepVariant is built with bazel. Internally we have an add-on in CLion to import bazel project. If this plug-in is available you can try to do that. Bazel project is in the file named ""BUILD"" which exists in all sub-directories. If importing bazel project is not available then you may need to create your project from scratch in your IDE and then add all source files.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/792
https://github.com/google/deepvariant/issues/792:104,interoperability,plug,plug-in,104,"DeepVariant is built with bazel. Internally we have an add-on in CLion to import bazel project. If this plug-in is available you can try to do that. Bazel project is in the file named ""BUILD"" which exists in all sub-directories. If importing bazel project is not available then you may need to create your project from scratch in your IDE and then add all source files.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/792
https://github.com/google/deepvariant/issues/792:115,reliability,availab,available,115,"DeepVariant is built with bazel. Internally we have an add-on in CLion to import bazel project. If this plug-in is available you can try to do that. Bazel project is in the file named ""BUILD"" which exists in all sub-directories. If importing bazel project is not available then you may need to create your project from scratch in your IDE and then add all source files.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/792
https://github.com/google/deepvariant/issues/792:263,reliability,availab,available,263,"DeepVariant is built with bazel. Internally we have an add-on in CLion to import bazel project. If this plug-in is available you can try to do that. Bazel project is in the file named ""BUILD"" which exists in all sub-directories. If importing bazel project is not available then you may need to create your project from scratch in your IDE and then add all source files.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/792
https://github.com/google/deepvariant/issues/792:115,safety,avail,available,115,"DeepVariant is built with bazel. Internally we have an add-on in CLion to import bazel project. If this plug-in is available you can try to do that. Bazel project is in the file named ""BUILD"" which exists in all sub-directories. If importing bazel project is not available then you may need to create your project from scratch in your IDE and then add all source files.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/792
https://github.com/google/deepvariant/issues/792:263,safety,avail,available,263,"DeepVariant is built with bazel. Internally we have an add-on in CLion to import bazel project. If this plug-in is available you can try to do that. Bazel project is in the file named ""BUILD"" which exists in all sub-directories. If importing bazel project is not available then you may need to create your project from scratch in your IDE and then add all source files.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/792
https://github.com/google/deepvariant/issues/792:115,security,availab,available,115,"DeepVariant is built with bazel. Internally we have an add-on in CLion to import bazel project. If this plug-in is available you can try to do that. Bazel project is in the file named ""BUILD"" which exists in all sub-directories. If importing bazel project is not available then you may need to create your project from scratch in your IDE and then add all source files.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/792
https://github.com/google/deepvariant/issues/792:263,security,availab,available,263,"DeepVariant is built with bazel. Internally we have an add-on in CLion to import bazel project. If this plug-in is available you can try to do that. Bazel project is in the file named ""BUILD"" which exists in all sub-directories. If importing bazel project is not available then you may need to create your project from scratch in your IDE and then add all source files.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/792
https://github.com/google/deepvariant/issues/792:243,deployability,depend,dependencies,243,Thanks for sharing your thoughts! I was able to get some syntax highlighting to work in VSCode (though not autocomplete beyond local variables). I might try out CLion and set it up to be configured with Bazel so that it might be more aware of dependencies for autocompleting. Thanks!,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/792
https://github.com/google/deepvariant/issues/792:187,integrability,configur,configured,187,Thanks for sharing your thoughts! I was able to get some syntax highlighting to work in VSCode (though not autocomplete beyond local variables). I might try out CLion and set it up to be configured with Bazel so that it might be more aware of dependencies for autocompleting. Thanks!,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/792
https://github.com/google/deepvariant/issues/792:243,integrability,depend,dependencies,243,Thanks for sharing your thoughts! I was able to get some syntax highlighting to work in VSCode (though not autocomplete beyond local variables). I might try out CLion and set it up to be configured with Bazel so that it might be more aware of dependencies for autocompleting. Thanks!,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/792
https://github.com/google/deepvariant/issues/792:133,modifiability,variab,variables,133,Thanks for sharing your thoughts! I was able to get some syntax highlighting to work in VSCode (though not autocomplete beyond local variables). I might try out CLion and set it up to be configured with Bazel so that it might be more aware of dependencies for autocompleting. Thanks!,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/792
https://github.com/google/deepvariant/issues/792:187,modifiability,configur,configured,187,Thanks for sharing your thoughts! I was able to get some syntax highlighting to work in VSCode (though not autocomplete beyond local variables). I might try out CLion and set it up to be configured with Bazel so that it might be more aware of dependencies for autocompleting. Thanks!,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/792
https://github.com/google/deepvariant/issues/792:243,modifiability,depend,dependencies,243,Thanks for sharing your thoughts! I was able to get some syntax highlighting to work in VSCode (though not autocomplete beyond local variables). I might try out CLion and set it up to be configured with Bazel so that it might be more aware of dependencies for autocompleting. Thanks!,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/792
https://github.com/google/deepvariant/issues/792:243,safety,depend,dependencies,243,Thanks for sharing your thoughts! I was able to get some syntax highlighting to work in VSCode (though not autocomplete beyond local variables). I might try out CLion and set it up to be configured with Bazel so that it might be more aware of dependencies for autocompleting. Thanks!,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/792
https://github.com/google/deepvariant/issues/792:187,security,configur,configured,187,Thanks for sharing your thoughts! I was able to get some syntax highlighting to work in VSCode (though not autocomplete beyond local variables). I might try out CLion and set it up to be configured with Bazel so that it might be more aware of dependencies for autocompleting. Thanks!,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/792
https://github.com/google/deepvariant/issues/792:243,testability,depend,dependencies,243,Thanks for sharing your thoughts! I was able to get some syntax highlighting to work in VSCode (though not autocomplete beyond local variables). I might try out CLion and set it up to be configured with Bazel so that it might be more aware of dependencies for autocompleting. Thanks!,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/792
https://github.com/google/deepvariant/issues/793:409,availability,down,downloaded,409,"> Hello,. > . > I'm very new to model training and honestly, coding, so thank you for your patience! I'm trying to run my own samples following along with the [advanced training case study](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md). I've reached the stage where I need to locally shuffle the training examples using the shuffle_tfrecords_beam.py script. > . > I downloaded the latest version of tensorflow (2.15) and was initially getting an error that apache beam was not being recognized, and realized that beam did not install because its latest version (2.54) was incompatible with the current version of numpy (1.26) that was being imported. I uninstalled that new version of numpy in tensorflow and installed an older version that would be compatible (1.24.4), and then was able to install apache beam (2.54). However, now I'm getting even more errors (see below). Do you have any advice on which versions of everything I should make sure to have installed correctly before running the shuffle script? Any guidance is very much appreciated. > . > Not so much a question but I want to confirm my understanding of the pipeline from the tutorial, as again I am very new to this. First step is to run deepvariant make_examples in training mode to create training and validation sets. In the documents, these are individual chromosomes, but in theory these could be whole individuals or multiple individuals, is that correct? And then make_examples in training mode should be run multiple times independently for training and validation sets? If for example, I used Chromosome 1 for my training set and Chromosome 2 for my validation set, should those repeated runs be made on different chromosomes, or the same chromosomes? Then finally, once everything is shuffled, run model_train and model_eval. Let me reply to this part first:. ""First step is to run deepvariant make_examples in training mode to create training and validation sets. In the docume",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:489,availability,error,error,489,"> Hello,. > . > I'm very new to model training and honestly, coding, so thank you for your patience! I'm trying to run my own samples following along with the [advanced training case study](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md). I've reached the stage where I need to locally shuffle the training examples using the shuffle_tfrecords_beam.py script. > . > I downloaded the latest version of tensorflow (2.15) and was initially getting an error that apache beam was not being recognized, and realized that beam did not install because its latest version (2.54) was incompatible with the current version of numpy (1.26) that was being imported. I uninstalled that new version of numpy in tensorflow and installed an older version that would be compatible (1.24.4), and then was able to install apache beam (2.54). However, now I'm getting even more errors (see below). Do you have any advice on which versions of everything I should make sure to have installed correctly before running the shuffle script? Any guidance is very much appreciated. > . > Not so much a question but I want to confirm my understanding of the pipeline from the tutorial, as again I am very new to this. First step is to run deepvariant make_examples in training mode to create training and validation sets. In the documents, these are individual chromosomes, but in theory these could be whole individuals or multiple individuals, is that correct? And then make_examples in training mode should be run multiple times independently for training and validation sets? If for example, I used Chromosome 1 for my training set and Chromosome 2 for my validation set, should those repeated runs be made on different chromosomes, or the same chromosomes? Then finally, once everything is shuffled, run model_train and model_eval. Let me reply to this part first:. ""First step is to run deepvariant make_examples in training mode to create training and validation sets. In the docume",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:898,availability,error,errors,898,"> Hello,. > . > I'm very new to model training and honestly, coding, so thank you for your patience! I'm trying to run my own samples following along with the [advanced training case study](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md). I've reached the stage where I need to locally shuffle the training examples using the shuffle_tfrecords_beam.py script. > . > I downloaded the latest version of tensorflow (2.15) and was initially getting an error that apache beam was not being recognized, and realized that beam did not install because its latest version (2.54) was incompatible with the current version of numpy (1.26) that was being imported. I uninstalled that new version of numpy in tensorflow and installed an older version that would be compatible (1.24.4), and then was able to install apache beam (2.54). However, now I'm getting even more errors (see below). Do you have any advice on which versions of everything I should make sure to have installed correctly before running the shuffle script? Any guidance is very much appreciated. > . > Not so much a question but I want to confirm my understanding of the pipeline from the tutorial, as again I am very new to this. First step is to run deepvariant make_examples in training mode to create training and validation sets. In the documents, these are individual chromosomes, but in theory these could be whole individuals or multiple individuals, is that correct? And then make_examples in training mode should be run multiple times independently for training and validation sets? If for example, I used Chromosome 1 for my training set and Chromosome 2 for my validation set, should those repeated runs be made on different chromosomes, or the same chromosomes? Then finally, once everything is shuffled, run model_train and model_eval. Let me reply to this part first:. ""First step is to run deepvariant make_examples in training mode to create training and validation sets. In the docume",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:3476,availability,consist,consistent,3476," same chromosomes? Then finally, once everything is shuffled, run model_train and model_eval. Let me reply to this part first:. ""First step is to run deepvariant make_examples in training mode to create training and validation sets. In the documents, these are individual chromosomes, but in theory these could be whole individuals or multiple individuals, is that correct?"" --> Yes that's correct. ""If for example, I used Chromosome 1 for my training set and Chromosome 2 for my validation set, should those repeated runs be made on different chromosomes, or the same chromosomes?"". If you want Chromosome 1 for your training set, and Chromosome 2 for your validation set, you'll run make_examples twice. One run to generate the training set with chr1, the other run to generate validation set as chr2. Note that in both runs, you'll run make_examples with the `--mode training` flag. This can be be a bit confusing, but `--mode training` in make_examples just means that we will create examples with truth labels. And you will need truth labels for your training set and validation set, . > . > Thank you very much for your time, and if these questions are answered clearly in a doc already, then I apologize and would appreciate being directed there. > . > Best, Haley. > . I'll separately look at the dependency issue. I'll plan to repeat what is documented in https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-training-case-study.md on a clean machine and see if the dependencies still work. Sometimes when we documented it, things worked, but later on some underlying dependencies might have shifted. This is actually why we packaged our variant calling in Docker to make sure the versions are more consistent. But we haven't done so for our shuffling code in the training tutorial. Anyway, let me plan to walk through https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-training-case-study.md myself and see if it still works. I'll document my steps here later.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:297,deployability,stage,stage,297,"> Hello,. > . > I'm very new to model training and honestly, coding, so thank you for your patience! I'm trying to run my own samples following along with the [advanced training case study](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md). I've reached the stage where I need to locally shuffle the training examples using the shuffle_tfrecords_beam.py script. > . > I downloaded the latest version of tensorflow (2.15) and was initially getting an error that apache beam was not being recognized, and realized that beam did not install because its latest version (2.54) was incompatible with the current version of numpy (1.26) that was being imported. I uninstalled that new version of numpy in tensorflow and installed an older version that would be compatible (1.24.4), and then was able to install apache beam (2.54). However, now I'm getting even more errors (see below). Do you have any advice on which versions of everything I should make sure to have installed correctly before running the shuffle script? Any guidance is very much appreciated. > . > Not so much a question but I want to confirm my understanding of the pipeline from the tutorial, as again I am very new to this. First step is to run deepvariant make_examples in training mode to create training and validation sets. In the documents, these are individual chromosomes, but in theory these could be whole individuals or multiple individuals, is that correct? And then make_examples in training mode should be run multiple times independently for training and validation sets? If for example, I used Chromosome 1 for my training set and Chromosome 2 for my validation set, should those repeated runs be made on different chromosomes, or the same chromosomes? Then finally, once everything is shuffled, run model_train and model_eval. Let me reply to this part first:. ""First step is to run deepvariant make_examples in training mode to create training and validation sets. In the docume",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:431,deployability,version,version,431,"> Hello,. > . > I'm very new to model training and honestly, coding, so thank you for your patience! I'm trying to run my own samples following along with the [advanced training case study](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md). I've reached the stage where I need to locally shuffle the training examples using the shuffle_tfrecords_beam.py script. > . > I downloaded the latest version of tensorflow (2.15) and was initially getting an error that apache beam was not being recognized, and realized that beam did not install because its latest version (2.54) was incompatible with the current version of numpy (1.26) that was being imported. I uninstalled that new version of numpy in tensorflow and installed an older version that would be compatible (1.24.4), and then was able to install apache beam (2.54). However, now I'm getting even more errors (see below). Do you have any advice on which versions of everything I should make sure to have installed correctly before running the shuffle script? Any guidance is very much appreciated. > . > Not so much a question but I want to confirm my understanding of the pipeline from the tutorial, as again I am very new to this. First step is to run deepvariant make_examples in training mode to create training and validation sets. In the documents, these are individual chromosomes, but in theory these could be whole individuals or multiple individuals, is that correct? And then make_examples in training mode should be run multiple times independently for training and validation sets? If for example, I used Chromosome 1 for my training set and Chromosome 2 for my validation set, should those repeated runs be made on different chromosomes, or the same chromosomes? Then finally, once everything is shuffled, run model_train and model_eval. Let me reply to this part first:. ""First step is to run deepvariant make_examples in training mode to create training and validation sets. In the docume",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:569,deployability,instal,install,569,"> Hello,. > . > I'm very new to model training and honestly, coding, so thank you for your patience! I'm trying to run my own samples following along with the [advanced training case study](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md). I've reached the stage where I need to locally shuffle the training examples using the shuffle_tfrecords_beam.py script. > . > I downloaded the latest version of tensorflow (2.15) and was initially getting an error that apache beam was not being recognized, and realized that beam did not install because its latest version (2.54) was incompatible with the current version of numpy (1.26) that was being imported. I uninstalled that new version of numpy in tensorflow and installed an older version that would be compatible (1.24.4), and then was able to install apache beam (2.54). However, now I'm getting even more errors (see below). Do you have any advice on which versions of everything I should make sure to have installed correctly before running the shuffle script? Any guidance is very much appreciated. > . > Not so much a question but I want to confirm my understanding of the pipeline from the tutorial, as again I am very new to this. First step is to run deepvariant make_examples in training mode to create training and validation sets. In the documents, these are individual chromosomes, but in theory these could be whole individuals or multiple individuals, is that correct? And then make_examples in training mode should be run multiple times independently for training and validation sets? If for example, I used Chromosome 1 for my training set and Chromosome 2 for my validation set, should those repeated runs be made on different chromosomes, or the same chromosomes? Then finally, once everything is shuffled, run model_train and model_eval. Let me reply to this part first:. ""First step is to run deepvariant make_examples in training mode to create training and validation sets. In the docume",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:596,deployability,version,version,596,"> Hello,. > . > I'm very new to model training and honestly, coding, so thank you for your patience! I'm trying to run my own samples following along with the [advanced training case study](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md). I've reached the stage where I need to locally shuffle the training examples using the shuffle_tfrecords_beam.py script. > . > I downloaded the latest version of tensorflow (2.15) and was initially getting an error that apache beam was not being recognized, and realized that beam did not install because its latest version (2.54) was incompatible with the current version of numpy (1.26) that was being imported. I uninstalled that new version of numpy in tensorflow and installed an older version that would be compatible (1.24.4), and then was able to install apache beam (2.54). However, now I'm getting even more errors (see below). Do you have any advice on which versions of everything I should make sure to have installed correctly before running the shuffle script? Any guidance is very much appreciated. > . > Not so much a question but I want to confirm my understanding of the pipeline from the tutorial, as again I am very new to this. First step is to run deepvariant make_examples in training mode to create training and validation sets. In the documents, these are individual chromosomes, but in theory these could be whole individuals or multiple individuals, is that correct? And then make_examples in training mode should be run multiple times independently for training and validation sets? If for example, I used Chromosome 1 for my training set and Chromosome 2 for my validation set, should those repeated runs be made on different chromosomes, or the same chromosomes? Then finally, once everything is shuffled, run model_train and model_eval. Let me reply to this part first:. ""First step is to run deepvariant make_examples in training mode to create training and validation sets. In the docume",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:645,deployability,version,version,645,"> Hello,. > . > I'm very new to model training and honestly, coding, so thank you for your patience! I'm trying to run my own samples following along with the [advanced training case study](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md). I've reached the stage where I need to locally shuffle the training examples using the shuffle_tfrecords_beam.py script. > . > I downloaded the latest version of tensorflow (2.15) and was initially getting an error that apache beam was not being recognized, and realized that beam did not install because its latest version (2.54) was incompatible with the current version of numpy (1.26) that was being imported. I uninstalled that new version of numpy in tensorflow and installed an older version that would be compatible (1.24.4), and then was able to install apache beam (2.54). However, now I'm getting even more errors (see below). Do you have any advice on which versions of everything I should make sure to have installed correctly before running the shuffle script? Any guidance is very much appreciated. > . > Not so much a question but I want to confirm my understanding of the pipeline from the tutorial, as again I am very new to this. First step is to run deepvariant make_examples in training mode to create training and validation sets. In the documents, these are individual chromosomes, but in theory these could be whole individuals or multiple individuals, is that correct? And then make_examples in training mode should be run multiple times independently for training and validation sets? If for example, I used Chromosome 1 for my training set and Chromosome 2 for my validation set, should those repeated runs be made on different chromosomes, or the same chromosomes? Then finally, once everything is shuffled, run model_train and model_eval. Let me reply to this part first:. ""First step is to run deepvariant make_examples in training mode to create training and validation sets. In the docume",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:717,deployability,version,version,717,"> Hello,. > . > I'm very new to model training and honestly, coding, so thank you for your patience! I'm trying to run my own samples following along with the [advanced training case study](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md). I've reached the stage where I need to locally shuffle the training examples using the shuffle_tfrecords_beam.py script. > . > I downloaded the latest version of tensorflow (2.15) and was initially getting an error that apache beam was not being recognized, and realized that beam did not install because its latest version (2.54) was incompatible with the current version of numpy (1.26) that was being imported. I uninstalled that new version of numpy in tensorflow and installed an older version that would be compatible (1.24.4), and then was able to install apache beam (2.54). However, now I'm getting even more errors (see below). Do you have any advice on which versions of everything I should make sure to have installed correctly before running the shuffle script? Any guidance is very much appreciated. > . > Not so much a question but I want to confirm my understanding of the pipeline from the tutorial, as again I am very new to this. First step is to run deepvariant make_examples in training mode to create training and validation sets. In the documents, these are individual chromosomes, but in theory these could be whole individuals or multiple individuals, is that correct? And then make_examples in training mode should be run multiple times independently for training and validation sets? If for example, I used Chromosome 1 for my training set and Chromosome 2 for my validation set, should those repeated runs be made on different chromosomes, or the same chromosomes? Then finally, once everything is shuffled, run model_train and model_eval. Let me reply to this part first:. ""First step is to run deepvariant make_examples in training mode to create training and validation sets. In the docume",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:752,deployability,instal,installed,752,"> Hello,. > . > I'm very new to model training and honestly, coding, so thank you for your patience! I'm trying to run my own samples following along with the [advanced training case study](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md). I've reached the stage where I need to locally shuffle the training examples using the shuffle_tfrecords_beam.py script. > . > I downloaded the latest version of tensorflow (2.15) and was initially getting an error that apache beam was not being recognized, and realized that beam did not install because its latest version (2.54) was incompatible with the current version of numpy (1.26) that was being imported. I uninstalled that new version of numpy in tensorflow and installed an older version that would be compatible (1.24.4), and then was able to install apache beam (2.54). However, now I'm getting even more errors (see below). Do you have any advice on which versions of everything I should make sure to have installed correctly before running the shuffle script? Any guidance is very much appreciated. > . > Not so much a question but I want to confirm my understanding of the pipeline from the tutorial, as again I am very new to this. First step is to run deepvariant make_examples in training mode to create training and validation sets. In the documents, these are individual chromosomes, but in theory these could be whole individuals or multiple individuals, is that correct? And then make_examples in training mode should be run multiple times independently for training and validation sets? If for example, I used Chromosome 1 for my training set and Chromosome 2 for my validation set, should those repeated runs be made on different chromosomes, or the same chromosomes? Then finally, once everything is shuffled, run model_train and model_eval. Let me reply to this part first:. ""First step is to run deepvariant make_examples in training mode to create training and validation sets. In the docume",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:771,deployability,version,version,771,"> Hello,. > . > I'm very new to model training and honestly, coding, so thank you for your patience! I'm trying to run my own samples following along with the [advanced training case study](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md). I've reached the stage where I need to locally shuffle the training examples using the shuffle_tfrecords_beam.py script. > . > I downloaded the latest version of tensorflow (2.15) and was initially getting an error that apache beam was not being recognized, and realized that beam did not install because its latest version (2.54) was incompatible with the current version of numpy (1.26) that was being imported. I uninstalled that new version of numpy in tensorflow and installed an older version that would be compatible (1.24.4), and then was able to install apache beam (2.54). However, now I'm getting even more errors (see below). Do you have any advice on which versions of everything I should make sure to have installed correctly before running the shuffle script? Any guidance is very much appreciated. > . > Not so much a question but I want to confirm my understanding of the pipeline from the tutorial, as again I am very new to this. First step is to run deepvariant make_examples in training mode to create training and validation sets. In the documents, these are individual chromosomes, but in theory these could be whole individuals or multiple individuals, is that correct? And then make_examples in training mode should be run multiple times independently for training and validation sets? If for example, I used Chromosome 1 for my training set and Chromosome 2 for my validation set, should those repeated runs be made on different chromosomes, or the same chromosomes? Then finally, once everything is shuffled, run model_train and model_eval. Let me reply to this part first:. ""First step is to run deepvariant make_examples in training mode to create training and validation sets. In the docume",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:835,deployability,instal,install,835,"> Hello,. > . > I'm very new to model training and honestly, coding, so thank you for your patience! I'm trying to run my own samples following along with the [advanced training case study](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md). I've reached the stage where I need to locally shuffle the training examples using the shuffle_tfrecords_beam.py script. > . > I downloaded the latest version of tensorflow (2.15) and was initially getting an error that apache beam was not being recognized, and realized that beam did not install because its latest version (2.54) was incompatible with the current version of numpy (1.26) that was being imported. I uninstalled that new version of numpy in tensorflow and installed an older version that would be compatible (1.24.4), and then was able to install apache beam (2.54). However, now I'm getting even more errors (see below). Do you have any advice on which versions of everything I should make sure to have installed correctly before running the shuffle script? Any guidance is very much appreciated. > . > Not so much a question but I want to confirm my understanding of the pipeline from the tutorial, as again I am very new to this. First step is to run deepvariant make_examples in training mode to create training and validation sets. In the documents, these are individual chromosomes, but in theory these could be whole individuals or multiple individuals, is that correct? And then make_examples in training mode should be run multiple times independently for training and validation sets? If for example, I used Chromosome 1 for my training set and Chromosome 2 for my validation set, should those repeated runs be made on different chromosomes, or the same chromosomes? Then finally, once everything is shuffled, run model_train and model_eval. Let me reply to this part first:. ""First step is to run deepvariant make_examples in training mode to create training and validation sets. In the docume",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:950,deployability,version,versions,950,"> Hello,. > . > I'm very new to model training and honestly, coding, so thank you for your patience! I'm trying to run my own samples following along with the [advanced training case study](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md). I've reached the stage where I need to locally shuffle the training examples using the shuffle_tfrecords_beam.py script. > . > I downloaded the latest version of tensorflow (2.15) and was initially getting an error that apache beam was not being recognized, and realized that beam did not install because its latest version (2.54) was incompatible with the current version of numpy (1.26) that was being imported. I uninstalled that new version of numpy in tensorflow and installed an older version that would be compatible (1.24.4), and then was able to install apache beam (2.54). However, now I'm getting even more errors (see below). Do you have any advice on which versions of everything I should make sure to have installed correctly before running the shuffle script? Any guidance is very much appreciated. > . > Not so much a question but I want to confirm my understanding of the pipeline from the tutorial, as again I am very new to this. First step is to run deepvariant make_examples in training mode to create training and validation sets. In the documents, these are individual chromosomes, but in theory these could be whole individuals or multiple individuals, is that correct? And then make_examples in training mode should be run multiple times independently for training and validation sets? If for example, I used Chromosome 1 for my training set and Chromosome 2 for my validation set, should those repeated runs be made on different chromosomes, or the same chromosomes? Then finally, once everything is shuffled, run model_train and model_eval. Let me reply to this part first:. ""First step is to run deepvariant make_examples in training mode to create training and validation sets. In the docume",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:1000,deployability,instal,installed,1000,"lo,. > . > I'm very new to model training and honestly, coding, so thank you for your patience! I'm trying to run my own samples following along with the [advanced training case study](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md). I've reached the stage where I need to locally shuffle the training examples using the shuffle_tfrecords_beam.py script. > . > I downloaded the latest version of tensorflow (2.15) and was initially getting an error that apache beam was not being recognized, and realized that beam did not install because its latest version (2.54) was incompatible with the current version of numpy (1.26) that was being imported. I uninstalled that new version of numpy in tensorflow and installed an older version that would be compatible (1.24.4), and then was able to install apache beam (2.54). However, now I'm getting even more errors (see below). Do you have any advice on which versions of everything I should make sure to have installed correctly before running the shuffle script? Any guidance is very much appreciated. > . > Not so much a question but I want to confirm my understanding of the pipeline from the tutorial, as again I am very new to this. First step is to run deepvariant make_examples in training mode to create training and validation sets. In the documents, these are individual chromosomes, but in theory these could be whole individuals or multiple individuals, is that correct? And then make_examples in training mode should be run multiple times independently for training and validation sets? If for example, I used Chromosome 1 for my training set and Chromosome 2 for my validation set, should those repeated runs be made on different chromosomes, or the same chromosomes? Then finally, once everything is shuffled, run model_train and model_eval. Let me reply to this part first:. ""First step is to run deepvariant make_examples in training mode to create training and validation sets. In the documents, ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:1169,deployability,pipelin,pipeline,1169,"ning case study](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md). I've reached the stage where I need to locally shuffle the training examples using the shuffle_tfrecords_beam.py script. > . > I downloaded the latest version of tensorflow (2.15) and was initially getting an error that apache beam was not being recognized, and realized that beam did not install because its latest version (2.54) was incompatible with the current version of numpy (1.26) that was being imported. I uninstalled that new version of numpy in tensorflow and installed an older version that would be compatible (1.24.4), and then was able to install apache beam (2.54). However, now I'm getting even more errors (see below). Do you have any advice on which versions of everything I should make sure to have installed correctly before running the shuffle script? Any guidance is very much appreciated. > . > Not so much a question but I want to confirm my understanding of the pipeline from the tutorial, as again I am very new to this. First step is to run deepvariant make_examples in training mode to create training and validation sets. In the documents, these are individual chromosomes, but in theory these could be whole individuals or multiple individuals, is that correct? And then make_examples in training mode should be run multiple times independently for training and validation sets? If for example, I used Chromosome 1 for my training set and Chromosome 2 for my validation set, should those repeated runs be made on different chromosomes, or the same chromosomes? Then finally, once everything is shuffled, run model_train and model_eval. Let me reply to this part first:. ""First step is to run deepvariant make_examples in training mode to create training and validation sets. In the documents, these are individual chromosomes, but in theory these could be whole individuals or multiple individuals, is that correct?"" --> Yes that's correct. ""If for example, I u",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:3059,deployability,depend,dependency,3059," same chromosomes? Then finally, once everything is shuffled, run model_train and model_eval. Let me reply to this part first:. ""First step is to run deepvariant make_examples in training mode to create training and validation sets. In the documents, these are individual chromosomes, but in theory these could be whole individuals or multiple individuals, is that correct?"" --> Yes that's correct. ""If for example, I used Chromosome 1 for my training set and Chromosome 2 for my validation set, should those repeated runs be made on different chromosomes, or the same chromosomes?"". If you want Chromosome 1 for your training set, and Chromosome 2 for your validation set, you'll run make_examples twice. One run to generate the training set with chr1, the other run to generate validation set as chr2. Note that in both runs, you'll run make_examples with the `--mode training` flag. This can be be a bit confusing, but `--mode training` in make_examples just means that we will create examples with truth labels. And you will need truth labels for your training set and validation set, . > . > Thank you very much for your time, and if these questions are answered clearly in a doc already, then I apologize and would appreciate being directed there. > . > Best, Haley. > . I'll separately look at the dependency issue. I'll plan to repeat what is documented in https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-training-case-study.md on a clean machine and see if the dependencies still work. Sometimes when we documented it, things worked, but later on some underlying dependencies might have shifted. This is actually why we packaged our variant calling in Docker to make sure the versions are more consistent. But we haven't done so for our shuffling code in the training tutorial. Anyway, let me plan to walk through https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-training-case-study.md myself and see if it still works. I'll document my steps here later.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:3243,deployability,depend,dependencies,3243," same chromosomes? Then finally, once everything is shuffled, run model_train and model_eval. Let me reply to this part first:. ""First step is to run deepvariant make_examples in training mode to create training and validation sets. In the documents, these are individual chromosomes, but in theory these could be whole individuals or multiple individuals, is that correct?"" --> Yes that's correct. ""If for example, I used Chromosome 1 for my training set and Chromosome 2 for my validation set, should those repeated runs be made on different chromosomes, or the same chromosomes?"". If you want Chromosome 1 for your training set, and Chromosome 2 for your validation set, you'll run make_examples twice. One run to generate the training set with chr1, the other run to generate validation set as chr2. Note that in both runs, you'll run make_examples with the `--mode training` flag. This can be be a bit confusing, but `--mode training` in make_examples just means that we will create examples with truth labels. And you will need truth labels for your training set and validation set, . > . > Thank you very much for your time, and if these questions are answered clearly in a doc already, then I apologize and would appreciate being directed there. > . > Best, Haley. > . I'll separately look at the dependency issue. I'll plan to repeat what is documented in https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-training-case-study.md on a clean machine and see if the dependencies still work. Sometimes when we documented it, things worked, but later on some underlying dependencies might have shifted. This is actually why we packaged our variant calling in Docker to make sure the versions are more consistent. But we haven't done so for our shuffling code in the training tutorial. Anyway, let me plan to walk through https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-training-case-study.md myself and see if it still works. I'll document my steps here later.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:3345,deployability,depend,dependencies,3345," same chromosomes? Then finally, once everything is shuffled, run model_train and model_eval. Let me reply to this part first:. ""First step is to run deepvariant make_examples in training mode to create training and validation sets. In the documents, these are individual chromosomes, but in theory these could be whole individuals or multiple individuals, is that correct?"" --> Yes that's correct. ""If for example, I used Chromosome 1 for my training set and Chromosome 2 for my validation set, should those repeated runs be made on different chromosomes, or the same chromosomes?"". If you want Chromosome 1 for your training set, and Chromosome 2 for your validation set, you'll run make_examples twice. One run to generate the training set with chr1, the other run to generate validation set as chr2. Note that in both runs, you'll run make_examples with the `--mode training` flag. This can be be a bit confusing, but `--mode training` in make_examples just means that we will create examples with truth labels. And you will need truth labels for your training set and validation set, . > . > Thank you very much for your time, and if these questions are answered clearly in a doc already, then I apologize and would appreciate being directed there. > . > Best, Haley. > . I'll separately look at the dependency issue. I'll plan to repeat what is documented in https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-training-case-study.md on a clean machine and see if the dependencies still work. Sometimes when we documented it, things worked, but later on some underlying dependencies might have shifted. This is actually why we packaged our variant calling in Docker to make sure the versions are more consistent. But we haven't done so for our shuffling code in the training tutorial. Anyway, let me plan to walk through https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-training-case-study.md myself and see if it still works. I'll document my steps here later.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:3458,deployability,version,versions,3458," same chromosomes? Then finally, once everything is shuffled, run model_train and model_eval. Let me reply to this part first:. ""First step is to run deepvariant make_examples in training mode to create training and validation sets. In the documents, these are individual chromosomes, but in theory these could be whole individuals or multiple individuals, is that correct?"" --> Yes that's correct. ""If for example, I used Chromosome 1 for my training set and Chromosome 2 for my validation set, should those repeated runs be made on different chromosomes, or the same chromosomes?"". If you want Chromosome 1 for your training set, and Chromosome 2 for your validation set, you'll run make_examples twice. One run to generate the training set with chr1, the other run to generate validation set as chr2. Note that in both runs, you'll run make_examples with the `--mode training` flag. This can be be a bit confusing, but `--mode training` in make_examples just means that we will create examples with truth labels. And you will need truth labels for your training set and validation set, . > . > Thank you very much for your time, and if these questions are answered clearly in a doc already, then I apologize and would appreciate being directed there. > . > Best, Haley. > . I'll separately look at the dependency issue. I'll plan to repeat what is documented in https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-training-case-study.md on a clean machine and see if the dependencies still work. Sometimes when we documented it, things worked, but later on some underlying dependencies might have shifted. This is actually why we packaged our variant calling in Docker to make sure the versions are more consistent. But we haven't done so for our shuffling code in the training tutorial. Anyway, let me plan to walk through https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-training-case-study.md myself and see if it still works. I'll document my steps here later.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:32,energy efficiency,model,model,32,"> Hello,. > . > I'm very new to model training and honestly, coding, so thank you for your patience! I'm trying to run my own samples following along with the [advanced training case study](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md). I've reached the stage where I need to locally shuffle the training examples using the shuffle_tfrecords_beam.py script. > . > I downloaded the latest version of tensorflow (2.15) and was initially getting an error that apache beam was not being recognized, and realized that beam did not install because its latest version (2.54) was incompatible with the current version of numpy (1.26) that was being imported. I uninstalled that new version of numpy in tensorflow and installed an older version that would be compatible (1.24.4), and then was able to install apache beam (2.54). However, now I'm getting even more errors (see below). Do you have any advice on which versions of everything I should make sure to have installed correctly before running the shuffle script? Any guidance is very much appreciated. > . > Not so much a question but I want to confirm my understanding of the pipeline from the tutorial, as again I am very new to this. First step is to run deepvariant make_examples in training mode to create training and validation sets. In the documents, these are individual chromosomes, but in theory these could be whole individuals or multiple individuals, is that correct? And then make_examples in training mode should be run multiple times independently for training and validation sets? If for example, I used Chromosome 1 for my training set and Chromosome 2 for my validation set, should those repeated runs be made on different chromosomes, or the same chromosomes? Then finally, once everything is shuffled, run model_train and model_eval. Let me reply to this part first:. ""First step is to run deepvariant make_examples in training mode to create training and validation sets. In the docume",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:637,energy efficiency,current,current,637,"> Hello,. > . > I'm very new to model training and honestly, coding, so thank you for your patience! I'm trying to run my own samples following along with the [advanced training case study](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md). I've reached the stage where I need to locally shuffle the training examples using the shuffle_tfrecords_beam.py script. > . > I downloaded the latest version of tensorflow (2.15) and was initially getting an error that apache beam was not being recognized, and realized that beam did not install because its latest version (2.54) was incompatible with the current version of numpy (1.26) that was being imported. I uninstalled that new version of numpy in tensorflow and installed an older version that would be compatible (1.24.4), and then was able to install apache beam (2.54). However, now I'm getting even more errors (see below). Do you have any advice on which versions of everything I should make sure to have installed correctly before running the shuffle script? Any guidance is very much appreciated. > . > Not so much a question but I want to confirm my understanding of the pipeline from the tutorial, as again I am very new to this. First step is to run deepvariant make_examples in training mode to create training and validation sets. In the documents, these are individual chromosomes, but in theory these could be whole individuals or multiple individuals, is that correct? And then make_examples in training mode should be run multiple times independently for training and validation sets? If for example, I used Chromosome 1 for my training set and Chromosome 2 for my validation set, should those repeated runs be made on different chromosomes, or the same chromosomes? Then finally, once everything is shuffled, run model_train and model_eval. Let me reply to this part first:. ""First step is to run deepvariant make_examples in training mode to create training and validation sets. In the docume",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:431,integrability,version,version,431,"> Hello,. > . > I'm very new to model training and honestly, coding, so thank you for your patience! I'm trying to run my own samples following along with the [advanced training case study](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md). I've reached the stage where I need to locally shuffle the training examples using the shuffle_tfrecords_beam.py script. > . > I downloaded the latest version of tensorflow (2.15) and was initially getting an error that apache beam was not being recognized, and realized that beam did not install because its latest version (2.54) was incompatible with the current version of numpy (1.26) that was being imported. I uninstalled that new version of numpy in tensorflow and installed an older version that would be compatible (1.24.4), and then was able to install apache beam (2.54). However, now I'm getting even more errors (see below). Do you have any advice on which versions of everything I should make sure to have installed correctly before running the shuffle script? Any guidance is very much appreciated. > . > Not so much a question but I want to confirm my understanding of the pipeline from the tutorial, as again I am very new to this. First step is to run deepvariant make_examples in training mode to create training and validation sets. In the documents, these are individual chromosomes, but in theory these could be whole individuals or multiple individuals, is that correct? And then make_examples in training mode should be run multiple times independently for training and validation sets? If for example, I used Chromosome 1 for my training set and Chromosome 2 for my validation set, should those repeated runs be made on different chromosomes, or the same chromosomes? Then finally, once everything is shuffled, run model_train and model_eval. Let me reply to this part first:. ""First step is to run deepvariant make_examples in training mode to create training and validation sets. In the docume",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:596,integrability,version,version,596,"> Hello,. > . > I'm very new to model training and honestly, coding, so thank you for your patience! I'm trying to run my own samples following along with the [advanced training case study](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md). I've reached the stage where I need to locally shuffle the training examples using the shuffle_tfrecords_beam.py script. > . > I downloaded the latest version of tensorflow (2.15) and was initially getting an error that apache beam was not being recognized, and realized that beam did not install because its latest version (2.54) was incompatible with the current version of numpy (1.26) that was being imported. I uninstalled that new version of numpy in tensorflow and installed an older version that would be compatible (1.24.4), and then was able to install apache beam (2.54). However, now I'm getting even more errors (see below). Do you have any advice on which versions of everything I should make sure to have installed correctly before running the shuffle script? Any guidance is very much appreciated. > . > Not so much a question but I want to confirm my understanding of the pipeline from the tutorial, as again I am very new to this. First step is to run deepvariant make_examples in training mode to create training and validation sets. In the documents, these are individual chromosomes, but in theory these could be whole individuals or multiple individuals, is that correct? And then make_examples in training mode should be run multiple times independently for training and validation sets? If for example, I used Chromosome 1 for my training set and Chromosome 2 for my validation set, should those repeated runs be made on different chromosomes, or the same chromosomes? Then finally, once everything is shuffled, run model_train and model_eval. Let me reply to this part first:. ""First step is to run deepvariant make_examples in training mode to create training and validation sets. In the docume",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:645,integrability,version,version,645,"> Hello,. > . > I'm very new to model training and honestly, coding, so thank you for your patience! I'm trying to run my own samples following along with the [advanced training case study](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md). I've reached the stage where I need to locally shuffle the training examples using the shuffle_tfrecords_beam.py script. > . > I downloaded the latest version of tensorflow (2.15) and was initially getting an error that apache beam was not being recognized, and realized that beam did not install because its latest version (2.54) was incompatible with the current version of numpy (1.26) that was being imported. I uninstalled that new version of numpy in tensorflow and installed an older version that would be compatible (1.24.4), and then was able to install apache beam (2.54). However, now I'm getting even more errors (see below). Do you have any advice on which versions of everything I should make sure to have installed correctly before running the shuffle script? Any guidance is very much appreciated. > . > Not so much a question but I want to confirm my understanding of the pipeline from the tutorial, as again I am very new to this. First step is to run deepvariant make_examples in training mode to create training and validation sets. In the documents, these are individual chromosomes, but in theory these could be whole individuals or multiple individuals, is that correct? And then make_examples in training mode should be run multiple times independently for training and validation sets? If for example, I used Chromosome 1 for my training set and Chromosome 2 for my validation set, should those repeated runs be made on different chromosomes, or the same chromosomes? Then finally, once everything is shuffled, run model_train and model_eval. Let me reply to this part first:. ""First step is to run deepvariant make_examples in training mode to create training and validation sets. In the docume",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:717,integrability,version,version,717,"> Hello,. > . > I'm very new to model training and honestly, coding, so thank you for your patience! I'm trying to run my own samples following along with the [advanced training case study](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md). I've reached the stage where I need to locally shuffle the training examples using the shuffle_tfrecords_beam.py script. > . > I downloaded the latest version of tensorflow (2.15) and was initially getting an error that apache beam was not being recognized, and realized that beam did not install because its latest version (2.54) was incompatible with the current version of numpy (1.26) that was being imported. I uninstalled that new version of numpy in tensorflow and installed an older version that would be compatible (1.24.4), and then was able to install apache beam (2.54). However, now I'm getting even more errors (see below). Do you have any advice on which versions of everything I should make sure to have installed correctly before running the shuffle script? Any guidance is very much appreciated. > . > Not so much a question but I want to confirm my understanding of the pipeline from the tutorial, as again I am very new to this. First step is to run deepvariant make_examples in training mode to create training and validation sets. In the documents, these are individual chromosomes, but in theory these could be whole individuals or multiple individuals, is that correct? And then make_examples in training mode should be run multiple times independently for training and validation sets? If for example, I used Chromosome 1 for my training set and Chromosome 2 for my validation set, should those repeated runs be made on different chromosomes, or the same chromosomes? Then finally, once everything is shuffled, run model_train and model_eval. Let me reply to this part first:. ""First step is to run deepvariant make_examples in training mode to create training and validation sets. In the docume",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:771,integrability,version,version,771,"> Hello,. > . > I'm very new to model training and honestly, coding, so thank you for your patience! I'm trying to run my own samples following along with the [advanced training case study](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md). I've reached the stage where I need to locally shuffle the training examples using the shuffle_tfrecords_beam.py script. > . > I downloaded the latest version of tensorflow (2.15) and was initially getting an error that apache beam was not being recognized, and realized that beam did not install because its latest version (2.54) was incompatible with the current version of numpy (1.26) that was being imported. I uninstalled that new version of numpy in tensorflow and installed an older version that would be compatible (1.24.4), and then was able to install apache beam (2.54). However, now I'm getting even more errors (see below). Do you have any advice on which versions of everything I should make sure to have installed correctly before running the shuffle script? Any guidance is very much appreciated. > . > Not so much a question but I want to confirm my understanding of the pipeline from the tutorial, as again I am very new to this. First step is to run deepvariant make_examples in training mode to create training and validation sets. In the documents, these are individual chromosomes, but in theory these could be whole individuals or multiple individuals, is that correct? And then make_examples in training mode should be run multiple times independently for training and validation sets? If for example, I used Chromosome 1 for my training set and Chromosome 2 for my validation set, should those repeated runs be made on different chromosomes, or the same chromosomes? Then finally, once everything is shuffled, run model_train and model_eval. Let me reply to this part first:. ""First step is to run deepvariant make_examples in training mode to create training and validation sets. In the docume",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:950,integrability,version,versions,950,"> Hello,. > . > I'm very new to model training and honestly, coding, so thank you for your patience! I'm trying to run my own samples following along with the [advanced training case study](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md). I've reached the stage where I need to locally shuffle the training examples using the shuffle_tfrecords_beam.py script. > . > I downloaded the latest version of tensorflow (2.15) and was initially getting an error that apache beam was not being recognized, and realized that beam did not install because its latest version (2.54) was incompatible with the current version of numpy (1.26) that was being imported. I uninstalled that new version of numpy in tensorflow and installed an older version that would be compatible (1.24.4), and then was able to install apache beam (2.54). However, now I'm getting even more errors (see below). Do you have any advice on which versions of everything I should make sure to have installed correctly before running the shuffle script? Any guidance is very much appreciated. > . > Not so much a question but I want to confirm my understanding of the pipeline from the tutorial, as again I am very new to this. First step is to run deepvariant make_examples in training mode to create training and validation sets. In the documents, these are individual chromosomes, but in theory these could be whole individuals or multiple individuals, is that correct? And then make_examples in training mode should be run multiple times independently for training and validation sets? If for example, I used Chromosome 1 for my training set and Chromosome 2 for my validation set, should those repeated runs be made on different chromosomes, or the same chromosomes? Then finally, once everything is shuffled, run model_train and model_eval. Let me reply to this part first:. ""First step is to run deepvariant make_examples in training mode to create training and validation sets. In the docume",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:1169,integrability,pipelin,pipeline,1169,"ning case study](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md). I've reached the stage where I need to locally shuffle the training examples using the shuffle_tfrecords_beam.py script. > . > I downloaded the latest version of tensorflow (2.15) and was initially getting an error that apache beam was not being recognized, and realized that beam did not install because its latest version (2.54) was incompatible with the current version of numpy (1.26) that was being imported. I uninstalled that new version of numpy in tensorflow and installed an older version that would be compatible (1.24.4), and then was able to install apache beam (2.54). However, now I'm getting even more errors (see below). Do you have any advice on which versions of everything I should make sure to have installed correctly before running the shuffle script? Any guidance is very much appreciated. > . > Not so much a question but I want to confirm my understanding of the pipeline from the tutorial, as again I am very new to this. First step is to run deepvariant make_examples in training mode to create training and validation sets. In the documents, these are individual chromosomes, but in theory these could be whole individuals or multiple individuals, is that correct? And then make_examples in training mode should be run multiple times independently for training and validation sets? If for example, I used Chromosome 1 for my training set and Chromosome 2 for my validation set, should those repeated runs be made on different chromosomes, or the same chromosomes? Then finally, once everything is shuffled, run model_train and model_eval. Let me reply to this part first:. ""First step is to run deepvariant make_examples in training mode to create training and validation sets. In the documents, these are individual chromosomes, but in theory these could be whole individuals or multiple individuals, is that correct?"" --> Yes that's correct. ""If for example, I u",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:3059,integrability,depend,dependency,3059," same chromosomes? Then finally, once everything is shuffled, run model_train and model_eval. Let me reply to this part first:. ""First step is to run deepvariant make_examples in training mode to create training and validation sets. In the documents, these are individual chromosomes, but in theory these could be whole individuals or multiple individuals, is that correct?"" --> Yes that's correct. ""If for example, I used Chromosome 1 for my training set and Chromosome 2 for my validation set, should those repeated runs be made on different chromosomes, or the same chromosomes?"". If you want Chromosome 1 for your training set, and Chromosome 2 for your validation set, you'll run make_examples twice. One run to generate the training set with chr1, the other run to generate validation set as chr2. Note that in both runs, you'll run make_examples with the `--mode training` flag. This can be be a bit confusing, but `--mode training` in make_examples just means that we will create examples with truth labels. And you will need truth labels for your training set and validation set, . > . > Thank you very much for your time, and if these questions are answered clearly in a doc already, then I apologize and would appreciate being directed there. > . > Best, Haley. > . I'll separately look at the dependency issue. I'll plan to repeat what is documented in https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-training-case-study.md on a clean machine and see if the dependencies still work. Sometimes when we documented it, things worked, but later on some underlying dependencies might have shifted. This is actually why we packaged our variant calling in Docker to make sure the versions are more consistent. But we haven't done so for our shuffling code in the training tutorial. Anyway, let me plan to walk through https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-training-case-study.md myself and see if it still works. I'll document my steps here later.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:3243,integrability,depend,dependencies,3243," same chromosomes? Then finally, once everything is shuffled, run model_train and model_eval. Let me reply to this part first:. ""First step is to run deepvariant make_examples in training mode to create training and validation sets. In the documents, these are individual chromosomes, but in theory these could be whole individuals or multiple individuals, is that correct?"" --> Yes that's correct. ""If for example, I used Chromosome 1 for my training set and Chromosome 2 for my validation set, should those repeated runs be made on different chromosomes, or the same chromosomes?"". If you want Chromosome 1 for your training set, and Chromosome 2 for your validation set, you'll run make_examples twice. One run to generate the training set with chr1, the other run to generate validation set as chr2. Note that in both runs, you'll run make_examples with the `--mode training` flag. This can be be a bit confusing, but `--mode training` in make_examples just means that we will create examples with truth labels. And you will need truth labels for your training set and validation set, . > . > Thank you very much for your time, and if these questions are answered clearly in a doc already, then I apologize and would appreciate being directed there. > . > Best, Haley. > . I'll separately look at the dependency issue. I'll plan to repeat what is documented in https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-training-case-study.md on a clean machine and see if the dependencies still work. Sometimes when we documented it, things worked, but later on some underlying dependencies might have shifted. This is actually why we packaged our variant calling in Docker to make sure the versions are more consistent. But we haven't done so for our shuffling code in the training tutorial. Anyway, let me plan to walk through https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-training-case-study.md myself and see if it still works. I'll document my steps here later.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:3345,integrability,depend,dependencies,3345," same chromosomes? Then finally, once everything is shuffled, run model_train and model_eval. Let me reply to this part first:. ""First step is to run deepvariant make_examples in training mode to create training and validation sets. In the documents, these are individual chromosomes, but in theory these could be whole individuals or multiple individuals, is that correct?"" --> Yes that's correct. ""If for example, I used Chromosome 1 for my training set and Chromosome 2 for my validation set, should those repeated runs be made on different chromosomes, or the same chromosomes?"". If you want Chromosome 1 for your training set, and Chromosome 2 for your validation set, you'll run make_examples twice. One run to generate the training set with chr1, the other run to generate validation set as chr2. Note that in both runs, you'll run make_examples with the `--mode training` flag. This can be be a bit confusing, but `--mode training` in make_examples just means that we will create examples with truth labels. And you will need truth labels for your training set and validation set, . > . > Thank you very much for your time, and if these questions are answered clearly in a doc already, then I apologize and would appreciate being directed there. > . > Best, Haley. > . I'll separately look at the dependency issue. I'll plan to repeat what is documented in https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-training-case-study.md on a clean machine and see if the dependencies still work. Sometimes when we documented it, things worked, but later on some underlying dependencies might have shifted. This is actually why we packaged our variant calling in Docker to make sure the versions are more consistent. But we haven't done so for our shuffling code in the training tutorial. Anyway, let me plan to walk through https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-training-case-study.md myself and see if it still works. I'll document my steps here later.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:3458,integrability,version,versions,3458," same chromosomes? Then finally, once everything is shuffled, run model_train and model_eval. Let me reply to this part first:. ""First step is to run deepvariant make_examples in training mode to create training and validation sets. In the documents, these are individual chromosomes, but in theory these could be whole individuals or multiple individuals, is that correct?"" --> Yes that's correct. ""If for example, I used Chromosome 1 for my training set and Chromosome 2 for my validation set, should those repeated runs be made on different chromosomes, or the same chromosomes?"". If you want Chromosome 1 for your training set, and Chromosome 2 for your validation set, you'll run make_examples twice. One run to generate the training set with chr1, the other run to generate validation set as chr2. Note that in both runs, you'll run make_examples with the `--mode training` flag. This can be be a bit confusing, but `--mode training` in make_examples just means that we will create examples with truth labels. And you will need truth labels for your training set and validation set, . > . > Thank you very much for your time, and if these questions are answered clearly in a doc already, then I apologize and would appreciate being directed there. > . > Best, Haley. > . I'll separately look at the dependency issue. I'll plan to repeat what is documented in https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-training-case-study.md on a clean machine and see if the dependencies still work. Sometimes when we documented it, things worked, but later on some underlying dependencies might have shifted. This is actually why we packaged our variant calling in Docker to make sure the versions are more consistent. But we haven't done so for our shuffling code in the training tutorial. Anyway, let me plan to walk through https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-training-case-study.md myself and see if it still works. I'll document my steps here later.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:615,interoperability,incompatib,incompatible,615,"> Hello,. > . > I'm very new to model training and honestly, coding, so thank you for your patience! I'm trying to run my own samples following along with the [advanced training case study](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md). I've reached the stage where I need to locally shuffle the training examples using the shuffle_tfrecords_beam.py script. > . > I downloaded the latest version of tensorflow (2.15) and was initially getting an error that apache beam was not being recognized, and realized that beam did not install because its latest version (2.54) was incompatible with the current version of numpy (1.26) that was being imported. I uninstalled that new version of numpy in tensorflow and installed an older version that would be compatible (1.24.4), and then was able to install apache beam (2.54). However, now I'm getting even more errors (see below). Do you have any advice on which versions of everything I should make sure to have installed correctly before running the shuffle script? Any guidance is very much appreciated. > . > Not so much a question but I want to confirm my understanding of the pipeline from the tutorial, as again I am very new to this. First step is to run deepvariant make_examples in training mode to create training and validation sets. In the documents, these are individual chromosomes, but in theory these could be whole individuals or multiple individuals, is that correct? And then make_examples in training mode should be run multiple times independently for training and validation sets? If for example, I used Chromosome 1 for my training set and Chromosome 2 for my validation set, should those repeated runs be made on different chromosomes, or the same chromosomes? Then finally, once everything is shuffled, run model_train and model_eval. Let me reply to this part first:. ""First step is to run deepvariant make_examples in training mode to create training and validation sets. In the docume",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:793,interoperability,compatib,compatible,793,"> Hello,. > . > I'm very new to model training and honestly, coding, so thank you for your patience! I'm trying to run my own samples following along with the [advanced training case study](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md). I've reached the stage where I need to locally shuffle the training examples using the shuffle_tfrecords_beam.py script. > . > I downloaded the latest version of tensorflow (2.15) and was initially getting an error that apache beam was not being recognized, and realized that beam did not install because its latest version (2.54) was incompatible with the current version of numpy (1.26) that was being imported. I uninstalled that new version of numpy in tensorflow and installed an older version that would be compatible (1.24.4), and then was able to install apache beam (2.54). However, now I'm getting even more errors (see below). Do you have any advice on which versions of everything I should make sure to have installed correctly before running the shuffle script? Any guidance is very much appreciated. > . > Not so much a question but I want to confirm my understanding of the pipeline from the tutorial, as again I am very new to this. First step is to run deepvariant make_examples in training mode to create training and validation sets. In the documents, these are individual chromosomes, but in theory these could be whole individuals or multiple individuals, is that correct? And then make_examples in training mode should be run multiple times independently for training and validation sets? If for example, I used Chromosome 1 for my training set and Chromosome 2 for my validation set, should those repeated runs be made on different chromosomes, or the same chromosomes? Then finally, once everything is shuffled, run model_train and model_eval. Let me reply to this part first:. ""First step is to run deepvariant make_examples in training mode to create training and validation sets. In the docume",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:431,modifiability,version,version,431,"> Hello,. > . > I'm very new to model training and honestly, coding, so thank you for your patience! I'm trying to run my own samples following along with the [advanced training case study](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md). I've reached the stage where I need to locally shuffle the training examples using the shuffle_tfrecords_beam.py script. > . > I downloaded the latest version of tensorflow (2.15) and was initially getting an error that apache beam was not being recognized, and realized that beam did not install because its latest version (2.54) was incompatible with the current version of numpy (1.26) that was being imported. I uninstalled that new version of numpy in tensorflow and installed an older version that would be compatible (1.24.4), and then was able to install apache beam (2.54). However, now I'm getting even more errors (see below). Do you have any advice on which versions of everything I should make sure to have installed correctly before running the shuffle script? Any guidance is very much appreciated. > . > Not so much a question but I want to confirm my understanding of the pipeline from the tutorial, as again I am very new to this. First step is to run deepvariant make_examples in training mode to create training and validation sets. In the documents, these are individual chromosomes, but in theory these could be whole individuals or multiple individuals, is that correct? And then make_examples in training mode should be run multiple times independently for training and validation sets? If for example, I used Chromosome 1 for my training set and Chromosome 2 for my validation set, should those repeated runs be made on different chromosomes, or the same chromosomes? Then finally, once everything is shuffled, run model_train and model_eval. Let me reply to this part first:. ""First step is to run deepvariant make_examples in training mode to create training and validation sets. In the docume",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:596,modifiability,version,version,596,"> Hello,. > . > I'm very new to model training and honestly, coding, so thank you for your patience! I'm trying to run my own samples following along with the [advanced training case study](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md). I've reached the stage where I need to locally shuffle the training examples using the shuffle_tfrecords_beam.py script. > . > I downloaded the latest version of tensorflow (2.15) and was initially getting an error that apache beam was not being recognized, and realized that beam did not install because its latest version (2.54) was incompatible with the current version of numpy (1.26) that was being imported. I uninstalled that new version of numpy in tensorflow and installed an older version that would be compatible (1.24.4), and then was able to install apache beam (2.54). However, now I'm getting even more errors (see below). Do you have any advice on which versions of everything I should make sure to have installed correctly before running the shuffle script? Any guidance is very much appreciated. > . > Not so much a question but I want to confirm my understanding of the pipeline from the tutorial, as again I am very new to this. First step is to run deepvariant make_examples in training mode to create training and validation sets. In the documents, these are individual chromosomes, but in theory these could be whole individuals or multiple individuals, is that correct? And then make_examples in training mode should be run multiple times independently for training and validation sets? If for example, I used Chromosome 1 for my training set and Chromosome 2 for my validation set, should those repeated runs be made on different chromosomes, or the same chromosomes? Then finally, once everything is shuffled, run model_train and model_eval. Let me reply to this part first:. ""First step is to run deepvariant make_examples in training mode to create training and validation sets. In the docume",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:645,modifiability,version,version,645,"> Hello,. > . > I'm very new to model training and honestly, coding, so thank you for your patience! I'm trying to run my own samples following along with the [advanced training case study](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md). I've reached the stage where I need to locally shuffle the training examples using the shuffle_tfrecords_beam.py script. > . > I downloaded the latest version of tensorflow (2.15) and was initially getting an error that apache beam was not being recognized, and realized that beam did not install because its latest version (2.54) was incompatible with the current version of numpy (1.26) that was being imported. I uninstalled that new version of numpy in tensorflow and installed an older version that would be compatible (1.24.4), and then was able to install apache beam (2.54). However, now I'm getting even more errors (see below). Do you have any advice on which versions of everything I should make sure to have installed correctly before running the shuffle script? Any guidance is very much appreciated. > . > Not so much a question but I want to confirm my understanding of the pipeline from the tutorial, as again I am very new to this. First step is to run deepvariant make_examples in training mode to create training and validation sets. In the documents, these are individual chromosomes, but in theory these could be whole individuals or multiple individuals, is that correct? And then make_examples in training mode should be run multiple times independently for training and validation sets? If for example, I used Chromosome 1 for my training set and Chromosome 2 for my validation set, should those repeated runs be made on different chromosomes, or the same chromosomes? Then finally, once everything is shuffled, run model_train and model_eval. Let me reply to this part first:. ""First step is to run deepvariant make_examples in training mode to create training and validation sets. In the docume",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:717,modifiability,version,version,717,"> Hello,. > . > I'm very new to model training and honestly, coding, so thank you for your patience! I'm trying to run my own samples following along with the [advanced training case study](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md). I've reached the stage where I need to locally shuffle the training examples using the shuffle_tfrecords_beam.py script. > . > I downloaded the latest version of tensorflow (2.15) and was initially getting an error that apache beam was not being recognized, and realized that beam did not install because its latest version (2.54) was incompatible with the current version of numpy (1.26) that was being imported. I uninstalled that new version of numpy in tensorflow and installed an older version that would be compatible (1.24.4), and then was able to install apache beam (2.54). However, now I'm getting even more errors (see below). Do you have any advice on which versions of everything I should make sure to have installed correctly before running the shuffle script? Any guidance is very much appreciated. > . > Not so much a question but I want to confirm my understanding of the pipeline from the tutorial, as again I am very new to this. First step is to run deepvariant make_examples in training mode to create training and validation sets. In the documents, these are individual chromosomes, but in theory these could be whole individuals or multiple individuals, is that correct? And then make_examples in training mode should be run multiple times independently for training and validation sets? If for example, I used Chromosome 1 for my training set and Chromosome 2 for my validation set, should those repeated runs be made on different chromosomes, or the same chromosomes? Then finally, once everything is shuffled, run model_train and model_eval. Let me reply to this part first:. ""First step is to run deepvariant make_examples in training mode to create training and validation sets. In the docume",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:771,modifiability,version,version,771,"> Hello,. > . > I'm very new to model training and honestly, coding, so thank you for your patience! I'm trying to run my own samples following along with the [advanced training case study](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md). I've reached the stage where I need to locally shuffle the training examples using the shuffle_tfrecords_beam.py script. > . > I downloaded the latest version of tensorflow (2.15) and was initially getting an error that apache beam was not being recognized, and realized that beam did not install because its latest version (2.54) was incompatible with the current version of numpy (1.26) that was being imported. I uninstalled that new version of numpy in tensorflow and installed an older version that would be compatible (1.24.4), and then was able to install apache beam (2.54). However, now I'm getting even more errors (see below). Do you have any advice on which versions of everything I should make sure to have installed correctly before running the shuffle script? Any guidance is very much appreciated. > . > Not so much a question but I want to confirm my understanding of the pipeline from the tutorial, as again I am very new to this. First step is to run deepvariant make_examples in training mode to create training and validation sets. In the documents, these are individual chromosomes, but in theory these could be whole individuals or multiple individuals, is that correct? And then make_examples in training mode should be run multiple times independently for training and validation sets? If for example, I used Chromosome 1 for my training set and Chromosome 2 for my validation set, should those repeated runs be made on different chromosomes, or the same chromosomes? Then finally, once everything is shuffled, run model_train and model_eval. Let me reply to this part first:. ""First step is to run deepvariant make_examples in training mode to create training and validation sets. In the docume",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:950,modifiability,version,versions,950,"> Hello,. > . > I'm very new to model training and honestly, coding, so thank you for your patience! I'm trying to run my own samples following along with the [advanced training case study](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md). I've reached the stage where I need to locally shuffle the training examples using the shuffle_tfrecords_beam.py script. > . > I downloaded the latest version of tensorflow (2.15) and was initially getting an error that apache beam was not being recognized, and realized that beam did not install because its latest version (2.54) was incompatible with the current version of numpy (1.26) that was being imported. I uninstalled that new version of numpy in tensorflow and installed an older version that would be compatible (1.24.4), and then was able to install apache beam (2.54). However, now I'm getting even more errors (see below). Do you have any advice on which versions of everything I should make sure to have installed correctly before running the shuffle script? Any guidance is very much appreciated. > . > Not so much a question but I want to confirm my understanding of the pipeline from the tutorial, as again I am very new to this. First step is to run deepvariant make_examples in training mode to create training and validation sets. In the documents, these are individual chromosomes, but in theory these could be whole individuals or multiple individuals, is that correct? And then make_examples in training mode should be run multiple times independently for training and validation sets? If for example, I used Chromosome 1 for my training set and Chromosome 2 for my validation set, should those repeated runs be made on different chromosomes, or the same chromosomes? Then finally, once everything is shuffled, run model_train and model_eval. Let me reply to this part first:. ""First step is to run deepvariant make_examples in training mode to create training and validation sets. In the docume",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:3059,modifiability,depend,dependency,3059," same chromosomes? Then finally, once everything is shuffled, run model_train and model_eval. Let me reply to this part first:. ""First step is to run deepvariant make_examples in training mode to create training and validation sets. In the documents, these are individual chromosomes, but in theory these could be whole individuals or multiple individuals, is that correct?"" --> Yes that's correct. ""If for example, I used Chromosome 1 for my training set and Chromosome 2 for my validation set, should those repeated runs be made on different chromosomes, or the same chromosomes?"". If you want Chromosome 1 for your training set, and Chromosome 2 for your validation set, you'll run make_examples twice. One run to generate the training set with chr1, the other run to generate validation set as chr2. Note that in both runs, you'll run make_examples with the `--mode training` flag. This can be be a bit confusing, but `--mode training` in make_examples just means that we will create examples with truth labels. And you will need truth labels for your training set and validation set, . > . > Thank you very much for your time, and if these questions are answered clearly in a doc already, then I apologize and would appreciate being directed there. > . > Best, Haley. > . I'll separately look at the dependency issue. I'll plan to repeat what is documented in https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-training-case-study.md on a clean machine and see if the dependencies still work. Sometimes when we documented it, things worked, but later on some underlying dependencies might have shifted. This is actually why we packaged our variant calling in Docker to make sure the versions are more consistent. But we haven't done so for our shuffling code in the training tutorial. Anyway, let me plan to walk through https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-training-case-study.md myself and see if it still works. I'll document my steps here later.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:3243,modifiability,depend,dependencies,3243," same chromosomes? Then finally, once everything is shuffled, run model_train and model_eval. Let me reply to this part first:. ""First step is to run deepvariant make_examples in training mode to create training and validation sets. In the documents, these are individual chromosomes, but in theory these could be whole individuals or multiple individuals, is that correct?"" --> Yes that's correct. ""If for example, I used Chromosome 1 for my training set and Chromosome 2 for my validation set, should those repeated runs be made on different chromosomes, or the same chromosomes?"". If you want Chromosome 1 for your training set, and Chromosome 2 for your validation set, you'll run make_examples twice. One run to generate the training set with chr1, the other run to generate validation set as chr2. Note that in both runs, you'll run make_examples with the `--mode training` flag. This can be be a bit confusing, but `--mode training` in make_examples just means that we will create examples with truth labels. And you will need truth labels for your training set and validation set, . > . > Thank you very much for your time, and if these questions are answered clearly in a doc already, then I apologize and would appreciate being directed there. > . > Best, Haley. > . I'll separately look at the dependency issue. I'll plan to repeat what is documented in https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-training-case-study.md on a clean machine and see if the dependencies still work. Sometimes when we documented it, things worked, but later on some underlying dependencies might have shifted. This is actually why we packaged our variant calling in Docker to make sure the versions are more consistent. But we haven't done so for our shuffling code in the training tutorial. Anyway, let me plan to walk through https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-training-case-study.md myself and see if it still works. I'll document my steps here later.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:3345,modifiability,depend,dependencies,3345," same chromosomes? Then finally, once everything is shuffled, run model_train and model_eval. Let me reply to this part first:. ""First step is to run deepvariant make_examples in training mode to create training and validation sets. In the documents, these are individual chromosomes, but in theory these could be whole individuals or multiple individuals, is that correct?"" --> Yes that's correct. ""If for example, I used Chromosome 1 for my training set and Chromosome 2 for my validation set, should those repeated runs be made on different chromosomes, or the same chromosomes?"". If you want Chromosome 1 for your training set, and Chromosome 2 for your validation set, you'll run make_examples twice. One run to generate the training set with chr1, the other run to generate validation set as chr2. Note that in both runs, you'll run make_examples with the `--mode training` flag. This can be be a bit confusing, but `--mode training` in make_examples just means that we will create examples with truth labels. And you will need truth labels for your training set and validation set, . > . > Thank you very much for your time, and if these questions are answered clearly in a doc already, then I apologize and would appreciate being directed there. > . > Best, Haley. > . I'll separately look at the dependency issue. I'll plan to repeat what is documented in https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-training-case-study.md on a clean machine and see if the dependencies still work. Sometimes when we documented it, things worked, but later on some underlying dependencies might have shifted. This is actually why we packaged our variant calling in Docker to make sure the versions are more consistent. But we haven't done so for our shuffling code in the training tutorial. Anyway, let me plan to walk through https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-training-case-study.md myself and see if it still works. I'll document my steps here later.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:3402,modifiability,pac,packaged,3402," same chromosomes? Then finally, once everything is shuffled, run model_train and model_eval. Let me reply to this part first:. ""First step is to run deepvariant make_examples in training mode to create training and validation sets. In the documents, these are individual chromosomes, but in theory these could be whole individuals or multiple individuals, is that correct?"" --> Yes that's correct. ""If for example, I used Chromosome 1 for my training set and Chromosome 2 for my validation set, should those repeated runs be made on different chromosomes, or the same chromosomes?"". If you want Chromosome 1 for your training set, and Chromosome 2 for your validation set, you'll run make_examples twice. One run to generate the training set with chr1, the other run to generate validation set as chr2. Note that in both runs, you'll run make_examples with the `--mode training` flag. This can be be a bit confusing, but `--mode training` in make_examples just means that we will create examples with truth labels. And you will need truth labels for your training set and validation set, . > . > Thank you very much for your time, and if these questions are answered clearly in a doc already, then I apologize and would appreciate being directed there. > . > Best, Haley. > . I'll separately look at the dependency issue. I'll plan to repeat what is documented in https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-training-case-study.md on a clean machine and see if the dependencies still work. Sometimes when we documented it, things worked, but later on some underlying dependencies might have shifted. This is actually why we packaged our variant calling in Docker to make sure the versions are more consistent. But we haven't done so for our shuffling code in the training tutorial. Anyway, let me plan to walk through https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-training-case-study.md myself and see if it still works. I'll document my steps here later.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:3458,modifiability,version,versions,3458," same chromosomes? Then finally, once everything is shuffled, run model_train and model_eval. Let me reply to this part first:. ""First step is to run deepvariant make_examples in training mode to create training and validation sets. In the documents, these are individual chromosomes, but in theory these could be whole individuals or multiple individuals, is that correct?"" --> Yes that's correct. ""If for example, I used Chromosome 1 for my training set and Chromosome 2 for my validation set, should those repeated runs be made on different chromosomes, or the same chromosomes?"". If you want Chromosome 1 for your training set, and Chromosome 2 for your validation set, you'll run make_examples twice. One run to generate the training set with chr1, the other run to generate validation set as chr2. Note that in both runs, you'll run make_examples with the `--mode training` flag. This can be be a bit confusing, but `--mode training` in make_examples just means that we will create examples with truth labels. And you will need truth labels for your training set and validation set, . > . > Thank you very much for your time, and if these questions are answered clearly in a doc already, then I apologize and would appreciate being directed there. > . > Best, Haley. > . I'll separately look at the dependency issue. I'll plan to repeat what is documented in https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-training-case-study.md on a clean machine and see if the dependencies still work. Sometimes when we documented it, things worked, but later on some underlying dependencies might have shifted. This is actually why we packaged our variant calling in Docker to make sure the versions are more consistent. But we haven't done so for our shuffling code in the training tutorial. Anyway, let me plan to walk through https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-training-case-study.md myself and see if it still works. I'll document my steps here later.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:489,performance,error,error,489,"> Hello,. > . > I'm very new to model training and honestly, coding, so thank you for your patience! I'm trying to run my own samples following along with the [advanced training case study](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md). I've reached the stage where I need to locally shuffle the training examples using the shuffle_tfrecords_beam.py script. > . > I downloaded the latest version of tensorflow (2.15) and was initially getting an error that apache beam was not being recognized, and realized that beam did not install because its latest version (2.54) was incompatible with the current version of numpy (1.26) that was being imported. I uninstalled that new version of numpy in tensorflow and installed an older version that would be compatible (1.24.4), and then was able to install apache beam (2.54). However, now I'm getting even more errors (see below). Do you have any advice on which versions of everything I should make sure to have installed correctly before running the shuffle script? Any guidance is very much appreciated. > . > Not so much a question but I want to confirm my understanding of the pipeline from the tutorial, as again I am very new to this. First step is to run deepvariant make_examples in training mode to create training and validation sets. In the documents, these are individual chromosomes, but in theory these could be whole individuals or multiple individuals, is that correct? And then make_examples in training mode should be run multiple times independently for training and validation sets? If for example, I used Chromosome 1 for my training set and Chromosome 2 for my validation set, should those repeated runs be made on different chromosomes, or the same chromosomes? Then finally, once everything is shuffled, run model_train and model_eval. Let me reply to this part first:. ""First step is to run deepvariant make_examples in training mode to create training and validation sets. In the docume",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:898,performance,error,errors,898,"> Hello,. > . > I'm very new to model training and honestly, coding, so thank you for your patience! I'm trying to run my own samples following along with the [advanced training case study](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md). I've reached the stage where I need to locally shuffle the training examples using the shuffle_tfrecords_beam.py script. > . > I downloaded the latest version of tensorflow (2.15) and was initially getting an error that apache beam was not being recognized, and realized that beam did not install because its latest version (2.54) was incompatible with the current version of numpy (1.26) that was being imported. I uninstalled that new version of numpy in tensorflow and installed an older version that would be compatible (1.24.4), and then was able to install apache beam (2.54). However, now I'm getting even more errors (see below). Do you have any advice on which versions of everything I should make sure to have installed correctly before running the shuffle script? Any guidance is very much appreciated. > . > Not so much a question but I want to confirm my understanding of the pipeline from the tutorial, as again I am very new to this. First step is to run deepvariant make_examples in training mode to create training and validation sets. In the documents, these are individual chromosomes, but in theory these could be whole individuals or multiple individuals, is that correct? And then make_examples in training mode should be run multiple times independently for training and validation sets? If for example, I used Chromosome 1 for my training set and Chromosome 2 for my validation set, should those repeated runs be made on different chromosomes, or the same chromosomes? Then finally, once everything is shuffled, run model_train and model_eval. Let me reply to this part first:. ""First step is to run deepvariant make_examples in training mode to create training and validation sets. In the docume",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:1537,performance,time,times,1537,"d realized that beam did not install because its latest version (2.54) was incompatible with the current version of numpy (1.26) that was being imported. I uninstalled that new version of numpy in tensorflow and installed an older version that would be compatible (1.24.4), and then was able to install apache beam (2.54). However, now I'm getting even more errors (see below). Do you have any advice on which versions of everything I should make sure to have installed correctly before running the shuffle script? Any guidance is very much appreciated. > . > Not so much a question but I want to confirm my understanding of the pipeline from the tutorial, as again I am very new to this. First step is to run deepvariant make_examples in training mode to create training and validation sets. In the documents, these are individual chromosomes, but in theory these could be whole individuals or multiple individuals, is that correct? And then make_examples in training mode should be run multiple times independently for training and validation sets? If for example, I used Chromosome 1 for my training set and Chromosome 2 for my validation set, should those repeated runs be made on different chromosomes, or the same chromosomes? Then finally, once everything is shuffled, run model_train and model_eval. Let me reply to this part first:. ""First step is to run deepvariant make_examples in training mode to create training and validation sets. In the documents, these are individual chromosomes, but in theory these could be whole individuals or multiple individuals, is that correct?"" --> Yes that's correct. ""If for example, I used Chromosome 1 for my training set and Chromosome 2 for my validation set, should those repeated runs be made on different chromosomes, or the same chromosomes?"". If you want Chromosome 1 for your training set, and Chromosome 2 for your validation set, you'll run make_examples twice. One run to generate the training set with chr1, the other run to generate valida",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:2880,performance,time,time,2880," same chromosomes? Then finally, once everything is shuffled, run model_train and model_eval. Let me reply to this part first:. ""First step is to run deepvariant make_examples in training mode to create training and validation sets. In the documents, these are individual chromosomes, but in theory these could be whole individuals or multiple individuals, is that correct?"" --> Yes that's correct. ""If for example, I used Chromosome 1 for my training set and Chromosome 2 for my validation set, should those repeated runs be made on different chromosomes, or the same chromosomes?"". If you want Chromosome 1 for your training set, and Chromosome 2 for your validation set, you'll run make_examples twice. One run to generate the training set with chr1, the other run to generate validation set as chr2. Note that in both runs, you'll run make_examples with the `--mode training` flag. This can be be a bit confusing, but `--mode training` in make_examples just means that we will create examples with truth labels. And you will need truth labels for your training set and validation set, . > . > Thank you very much for your time, and if these questions are answered clearly in a doc already, then I apologize and would appreciate being directed there. > . > Best, Haley. > . I'll separately look at the dependency issue. I'll plan to repeat what is documented in https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-training-case-study.md on a clean machine and see if the dependencies still work. Sometimes when we documented it, things worked, but later on some underlying dependencies might have shifted. This is actually why we packaged our variant calling in Docker to make sure the versions are more consistent. But we haven't done so for our shuffling code in the training tutorial. Anyway, let me plan to walk through https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-training-case-study.md myself and see if it still works. I'll document my steps here later.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:489,safety,error,error,489,"> Hello,. > . > I'm very new to model training and honestly, coding, so thank you for your patience! I'm trying to run my own samples following along with the [advanced training case study](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md). I've reached the stage where I need to locally shuffle the training examples using the shuffle_tfrecords_beam.py script. > . > I downloaded the latest version of tensorflow (2.15) and was initially getting an error that apache beam was not being recognized, and realized that beam did not install because its latest version (2.54) was incompatible with the current version of numpy (1.26) that was being imported. I uninstalled that new version of numpy in tensorflow and installed an older version that would be compatible (1.24.4), and then was able to install apache beam (2.54). However, now I'm getting even more errors (see below). Do you have any advice on which versions of everything I should make sure to have installed correctly before running the shuffle script? Any guidance is very much appreciated. > . > Not so much a question but I want to confirm my understanding of the pipeline from the tutorial, as again I am very new to this. First step is to run deepvariant make_examples in training mode to create training and validation sets. In the documents, these are individual chromosomes, but in theory these could be whole individuals or multiple individuals, is that correct? And then make_examples in training mode should be run multiple times independently for training and validation sets? If for example, I used Chromosome 1 for my training set and Chromosome 2 for my validation set, should those repeated runs be made on different chromosomes, or the same chromosomes? Then finally, once everything is shuffled, run model_train and model_eval. Let me reply to this part first:. ""First step is to run deepvariant make_examples in training mode to create training and validation sets. In the docume",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:898,safety,error,errors,898,"> Hello,. > . > I'm very new to model training and honestly, coding, so thank you for your patience! I'm trying to run my own samples following along with the [advanced training case study](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md). I've reached the stage where I need to locally shuffle the training examples using the shuffle_tfrecords_beam.py script. > . > I downloaded the latest version of tensorflow (2.15) and was initially getting an error that apache beam was not being recognized, and realized that beam did not install because its latest version (2.54) was incompatible with the current version of numpy (1.26) that was being imported. I uninstalled that new version of numpy in tensorflow and installed an older version that would be compatible (1.24.4), and then was able to install apache beam (2.54). However, now I'm getting even more errors (see below). Do you have any advice on which versions of everything I should make sure to have installed correctly before running the shuffle script? Any guidance is very much appreciated. > . > Not so much a question but I want to confirm my understanding of the pipeline from the tutorial, as again I am very new to this. First step is to run deepvariant make_examples in training mode to create training and validation sets. In the documents, these are individual chromosomes, but in theory these could be whole individuals or multiple individuals, is that correct? And then make_examples in training mode should be run multiple times independently for training and validation sets? If for example, I used Chromosome 1 for my training set and Chromosome 2 for my validation set, should those repeated runs be made on different chromosomes, or the same chromosomes? Then finally, once everything is shuffled, run model_train and model_eval. Let me reply to this part first:. ""First step is to run deepvariant make_examples in training mode to create training and validation sets. In the docume",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:1316,safety,valid,validation,1316,"cally shuffle the training examples using the shuffle_tfrecords_beam.py script. > . > I downloaded the latest version of tensorflow (2.15) and was initially getting an error that apache beam was not being recognized, and realized that beam did not install because its latest version (2.54) was incompatible with the current version of numpy (1.26) that was being imported. I uninstalled that new version of numpy in tensorflow and installed an older version that would be compatible (1.24.4), and then was able to install apache beam (2.54). However, now I'm getting even more errors (see below). Do you have any advice on which versions of everything I should make sure to have installed correctly before running the shuffle script? Any guidance is very much appreciated. > . > Not so much a question but I want to confirm my understanding of the pipeline from the tutorial, as again I am very new to this. First step is to run deepvariant make_examples in training mode to create training and validation sets. In the documents, these are individual chromosomes, but in theory these could be whole individuals or multiple individuals, is that correct? And then make_examples in training mode should be run multiple times independently for training and validation sets? If for example, I used Chromosome 1 for my training set and Chromosome 2 for my validation set, should those repeated runs be made on different chromosomes, or the same chromosomes? Then finally, once everything is shuffled, run model_train and model_eval. Let me reply to this part first:. ""First step is to run deepvariant make_examples in training mode to create training and validation sets. In the documents, these are individual chromosomes, but in theory these could be whole individuals or multiple individuals, is that correct?"" --> Yes that's correct. ""If for example, I used Chromosome 1 for my training set and Chromosome 2 for my validation set, should those repeated runs be made on different chromosomes, or the sam",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:1574,safety,valid,validation,1574,"cause its latest version (2.54) was incompatible with the current version of numpy (1.26) that was being imported. I uninstalled that new version of numpy in tensorflow and installed an older version that would be compatible (1.24.4), and then was able to install apache beam (2.54). However, now I'm getting even more errors (see below). Do you have any advice on which versions of everything I should make sure to have installed correctly before running the shuffle script? Any guidance is very much appreciated. > . > Not so much a question but I want to confirm my understanding of the pipeline from the tutorial, as again I am very new to this. First step is to run deepvariant make_examples in training mode to create training and validation sets. In the documents, these are individual chromosomes, but in theory these could be whole individuals or multiple individuals, is that correct? And then make_examples in training mode should be run multiple times independently for training and validation sets? If for example, I used Chromosome 1 for my training set and Chromosome 2 for my validation set, should those repeated runs be made on different chromosomes, or the same chromosomes? Then finally, once everything is shuffled, run model_train and model_eval. Let me reply to this part first:. ""First step is to run deepvariant make_examples in training mode to create training and validation sets. In the documents, these are individual chromosomes, but in theory these could be whole individuals or multiple individuals, is that correct?"" --> Yes that's correct. ""If for example, I used Chromosome 1 for my training set and Chromosome 2 for my validation set, should those repeated runs be made on different chromosomes, or the same chromosomes?"". If you want Chromosome 1 for your training set, and Chromosome 2 for your validation set, you'll run make_examples twice. One run to generate the training set with chr1, the other run to generate validation set as chr2. Note that in both run",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:1671,safety,valid,validation,1671,"s being imported. I uninstalled that new version of numpy in tensorflow and installed an older version that would be compatible (1.24.4), and then was able to install apache beam (2.54). However, now I'm getting even more errors (see below). Do you have any advice on which versions of everything I should make sure to have installed correctly before running the shuffle script? Any guidance is very much appreciated. > . > Not so much a question but I want to confirm my understanding of the pipeline from the tutorial, as again I am very new to this. First step is to run deepvariant make_examples in training mode to create training and validation sets. In the documents, these are individual chromosomes, but in theory these could be whole individuals or multiple individuals, is that correct? And then make_examples in training mode should be run multiple times independently for training and validation sets? If for example, I used Chromosome 1 for my training set and Chromosome 2 for my validation set, should those repeated runs be made on different chromosomes, or the same chromosomes? Then finally, once everything is shuffled, run model_train and model_eval. Let me reply to this part first:. ""First step is to run deepvariant make_examples in training mode to create training and validation sets. In the documents, these are individual chromosomes, but in theory these could be whole individuals or multiple individuals, is that correct?"" --> Yes that's correct. ""If for example, I used Chromosome 1 for my training set and Chromosome 2 for my validation set, should those repeated runs be made on different chromosomes, or the same chromosomes?"". If you want Chromosome 1 for your training set, and Chromosome 2 for your validation set, you'll run make_examples twice. One run to generate the training set with chr1, the other run to generate validation set as chr2. Note that in both runs, you'll run make_examples with the `--mode training` flag. This can be be a bit confusing, but ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:1970,safety,valid,validation,1970,"should make sure to have installed correctly before running the shuffle script? Any guidance is very much appreciated. > . > Not so much a question but I want to confirm my understanding of the pipeline from the tutorial, as again I am very new to this. First step is to run deepvariant make_examples in training mode to create training and validation sets. In the documents, these are individual chromosomes, but in theory these could be whole individuals or multiple individuals, is that correct? And then make_examples in training mode should be run multiple times independently for training and validation sets? If for example, I used Chromosome 1 for my training set and Chromosome 2 for my validation set, should those repeated runs be made on different chromosomes, or the same chromosomes? Then finally, once everything is shuffled, run model_train and model_eval. Let me reply to this part first:. ""First step is to run deepvariant make_examples in training mode to create training and validation sets. In the documents, these are individual chromosomes, but in theory these could be whole individuals or multiple individuals, is that correct?"" --> Yes that's correct. ""If for example, I used Chromosome 1 for my training set and Chromosome 2 for my validation set, should those repeated runs be made on different chromosomes, or the same chromosomes?"". If you want Chromosome 1 for your training set, and Chromosome 2 for your validation set, you'll run make_examples twice. One run to generate the training set with chr1, the other run to generate validation set as chr2. Note that in both runs, you'll run make_examples with the `--mode training` flag. This can be be a bit confusing, but `--mode training` in make_examples just means that we will create examples with truth labels. And you will need truth labels for your training set and validation set, . > . > Thank you very much for your time, and if these questions are answered clearly in a doc already, then I apologize and would ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:2234,safety,valid,validation,2234," is to run deepvariant make_examples in training mode to create training and validation sets. In the documents, these are individual chromosomes, but in theory these could be whole individuals or multiple individuals, is that correct? And then make_examples in training mode should be run multiple times independently for training and validation sets? If for example, I used Chromosome 1 for my training set and Chromosome 2 for my validation set, should those repeated runs be made on different chromosomes, or the same chromosomes? Then finally, once everything is shuffled, run model_train and model_eval. Let me reply to this part first:. ""First step is to run deepvariant make_examples in training mode to create training and validation sets. In the documents, these are individual chromosomes, but in theory these could be whole individuals or multiple individuals, is that correct?"" --> Yes that's correct. ""If for example, I used Chromosome 1 for my training set and Chromosome 2 for my validation set, should those repeated runs be made on different chromosomes, or the same chromosomes?"". If you want Chromosome 1 for your training set, and Chromosome 2 for your validation set, you'll run make_examples twice. One run to generate the training set with chr1, the other run to generate validation set as chr2. Note that in both runs, you'll run make_examples with the `--mode training` flag. This can be be a bit confusing, but `--mode training` in make_examples just means that we will create examples with truth labels. And you will need truth labels for your training set and validation set, . > . > Thank you very much for your time, and if these questions are answered clearly in a doc already, then I apologize and would appreciate being directed there. > . > Best, Haley. > . I'll separately look at the dependency issue. I'll plan to repeat what is documented in https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-training-case-study.md on a clean machine and see if ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:2412,safety,valid,validation,2412,"le individuals or multiple individuals, is that correct? And then make_examples in training mode should be run multiple times independently for training and validation sets? If for example, I used Chromosome 1 for my training set and Chromosome 2 for my validation set, should those repeated runs be made on different chromosomes, or the same chromosomes? Then finally, once everything is shuffled, run model_train and model_eval. Let me reply to this part first:. ""First step is to run deepvariant make_examples in training mode to create training and validation sets. In the documents, these are individual chromosomes, but in theory these could be whole individuals or multiple individuals, is that correct?"" --> Yes that's correct. ""If for example, I used Chromosome 1 for my training set and Chromosome 2 for my validation set, should those repeated runs be made on different chromosomes, or the same chromosomes?"". If you want Chromosome 1 for your training set, and Chromosome 2 for your validation set, you'll run make_examples twice. One run to generate the training set with chr1, the other run to generate validation set as chr2. Note that in both runs, you'll run make_examples with the `--mode training` flag. This can be be a bit confusing, but `--mode training` in make_examples just means that we will create examples with truth labels. And you will need truth labels for your training set and validation set, . > . > Thank you very much for your time, and if these questions are answered clearly in a doc already, then I apologize and would appreciate being directed there. > . > Best, Haley. > . I'll separately look at the dependency issue. I'll plan to repeat what is documented in https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-training-case-study.md on a clean machine and see if the dependencies still work. Sometimes when we documented it, things worked, but later on some underlying dependencies might have shifted. This is actually why we packaged our va",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:2534,safety,valid,validation,2534,"mes independently for training and validation sets? If for example, I used Chromosome 1 for my training set and Chromosome 2 for my validation set, should those repeated runs be made on different chromosomes, or the same chromosomes? Then finally, once everything is shuffled, run model_train and model_eval. Let me reply to this part first:. ""First step is to run deepvariant make_examples in training mode to create training and validation sets. In the documents, these are individual chromosomes, but in theory these could be whole individuals or multiple individuals, is that correct?"" --> Yes that's correct. ""If for example, I used Chromosome 1 for my training set and Chromosome 2 for my validation set, should those repeated runs be made on different chromosomes, or the same chromosomes?"". If you want Chromosome 1 for your training set, and Chromosome 2 for your validation set, you'll run make_examples twice. One run to generate the training set with chr1, the other run to generate validation set as chr2. Note that in both runs, you'll run make_examples with the `--mode training` flag. This can be be a bit confusing, but `--mode training` in make_examples just means that we will create examples with truth labels. And you will need truth labels for your training set and validation set, . > . > Thank you very much for your time, and if these questions are answered clearly in a doc already, then I apologize and would appreciate being directed there. > . > Best, Haley. > . I'll separately look at the dependency issue. I'll plan to repeat what is documented in https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-training-case-study.md on a clean machine and see if the dependencies still work. Sometimes when we documented it, things worked, but later on some underlying dependencies might have shifted. This is actually why we packaged our variant calling in Docker to make sure the versions are more consistent. But we haven't done so for our shuffling code in th",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:2827,safety,valid,validation,2827," same chromosomes? Then finally, once everything is shuffled, run model_train and model_eval. Let me reply to this part first:. ""First step is to run deepvariant make_examples in training mode to create training and validation sets. In the documents, these are individual chromosomes, but in theory these could be whole individuals or multiple individuals, is that correct?"" --> Yes that's correct. ""If for example, I used Chromosome 1 for my training set and Chromosome 2 for my validation set, should those repeated runs be made on different chromosomes, or the same chromosomes?"". If you want Chromosome 1 for your training set, and Chromosome 2 for your validation set, you'll run make_examples twice. One run to generate the training set with chr1, the other run to generate validation set as chr2. Note that in both runs, you'll run make_examples with the `--mode training` flag. This can be be a bit confusing, but `--mode training` in make_examples just means that we will create examples with truth labels. And you will need truth labels for your training set and validation set, . > . > Thank you very much for your time, and if these questions are answered clearly in a doc already, then I apologize and would appreciate being directed there. > . > Best, Haley. > . I'll separately look at the dependency issue. I'll plan to repeat what is documented in https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-training-case-study.md on a clean machine and see if the dependencies still work. Sometimes when we documented it, things worked, but later on some underlying dependencies might have shifted. This is actually why we packaged our variant calling in Docker to make sure the versions are more consistent. But we haven't done so for our shuffling code in the training tutorial. Anyway, let me plan to walk through https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-training-case-study.md myself and see if it still works. I'll document my steps here later.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:3059,safety,depend,dependency,3059," same chromosomes? Then finally, once everything is shuffled, run model_train and model_eval. Let me reply to this part first:. ""First step is to run deepvariant make_examples in training mode to create training and validation sets. In the documents, these are individual chromosomes, but in theory these could be whole individuals or multiple individuals, is that correct?"" --> Yes that's correct. ""If for example, I used Chromosome 1 for my training set and Chromosome 2 for my validation set, should those repeated runs be made on different chromosomes, or the same chromosomes?"". If you want Chromosome 1 for your training set, and Chromosome 2 for your validation set, you'll run make_examples twice. One run to generate the training set with chr1, the other run to generate validation set as chr2. Note that in both runs, you'll run make_examples with the `--mode training` flag. This can be be a bit confusing, but `--mode training` in make_examples just means that we will create examples with truth labels. And you will need truth labels for your training set and validation set, . > . > Thank you very much for your time, and if these questions are answered clearly in a doc already, then I apologize and would appreciate being directed there. > . > Best, Haley. > . I'll separately look at the dependency issue. I'll plan to repeat what is documented in https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-training-case-study.md on a clean machine and see if the dependencies still work. Sometimes when we documented it, things worked, but later on some underlying dependencies might have shifted. This is actually why we packaged our variant calling in Docker to make sure the versions are more consistent. But we haven't done so for our shuffling code in the training tutorial. Anyway, let me plan to walk through https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-training-case-study.md myself and see if it still works. I'll document my steps here later.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:3243,safety,depend,dependencies,3243," same chromosomes? Then finally, once everything is shuffled, run model_train and model_eval. Let me reply to this part first:. ""First step is to run deepvariant make_examples in training mode to create training and validation sets. In the documents, these are individual chromosomes, but in theory these could be whole individuals or multiple individuals, is that correct?"" --> Yes that's correct. ""If for example, I used Chromosome 1 for my training set and Chromosome 2 for my validation set, should those repeated runs be made on different chromosomes, or the same chromosomes?"". If you want Chromosome 1 for your training set, and Chromosome 2 for your validation set, you'll run make_examples twice. One run to generate the training set with chr1, the other run to generate validation set as chr2. Note that in both runs, you'll run make_examples with the `--mode training` flag. This can be be a bit confusing, but `--mode training` in make_examples just means that we will create examples with truth labels. And you will need truth labels for your training set and validation set, . > . > Thank you very much for your time, and if these questions are answered clearly in a doc already, then I apologize and would appreciate being directed there. > . > Best, Haley. > . I'll separately look at the dependency issue. I'll plan to repeat what is documented in https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-training-case-study.md on a clean machine and see if the dependencies still work. Sometimes when we documented it, things worked, but later on some underlying dependencies might have shifted. This is actually why we packaged our variant calling in Docker to make sure the versions are more consistent. But we haven't done so for our shuffling code in the training tutorial. Anyway, let me plan to walk through https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-training-case-study.md myself and see if it still works. I'll document my steps here later.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:3345,safety,depend,dependencies,3345," same chromosomes? Then finally, once everything is shuffled, run model_train and model_eval. Let me reply to this part first:. ""First step is to run deepvariant make_examples in training mode to create training and validation sets. In the documents, these are individual chromosomes, but in theory these could be whole individuals or multiple individuals, is that correct?"" --> Yes that's correct. ""If for example, I used Chromosome 1 for my training set and Chromosome 2 for my validation set, should those repeated runs be made on different chromosomes, or the same chromosomes?"". If you want Chromosome 1 for your training set, and Chromosome 2 for your validation set, you'll run make_examples twice. One run to generate the training set with chr1, the other run to generate validation set as chr2. Note that in both runs, you'll run make_examples with the `--mode training` flag. This can be be a bit confusing, but `--mode training` in make_examples just means that we will create examples with truth labels. And you will need truth labels for your training set and validation set, . > . > Thank you very much for your time, and if these questions are answered clearly in a doc already, then I apologize and would appreciate being directed there. > . > Best, Haley. > . I'll separately look at the dependency issue. I'll plan to repeat what is documented in https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-training-case-study.md on a clean machine and see if the dependencies still work. Sometimes when we documented it, things worked, but later on some underlying dependencies might have shifted. This is actually why we packaged our variant calling in Docker to make sure the versions are more consistent. But we haven't done so for our shuffling code in the training tutorial. Anyway, let me plan to walk through https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-training-case-study.md myself and see if it still works. I'll document my steps here later.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:32,security,model,model,32,"> Hello,. > . > I'm very new to model training and honestly, coding, so thank you for your patience! I'm trying to run my own samples following along with the [advanced training case study](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md). I've reached the stage where I need to locally shuffle the training examples using the shuffle_tfrecords_beam.py script. > . > I downloaded the latest version of tensorflow (2.15) and was initially getting an error that apache beam was not being recognized, and realized that beam did not install because its latest version (2.54) was incompatible with the current version of numpy (1.26) that was being imported. I uninstalled that new version of numpy in tensorflow and installed an older version that would be compatible (1.24.4), and then was able to install apache beam (2.54). However, now I'm getting even more errors (see below). Do you have any advice on which versions of everything I should make sure to have installed correctly before running the shuffle script? Any guidance is very much appreciated. > . > Not so much a question but I want to confirm my understanding of the pipeline from the tutorial, as again I am very new to this. First step is to run deepvariant make_examples in training mode to create training and validation sets. In the documents, these are individual chromosomes, but in theory these could be whole individuals or multiple individuals, is that correct? And then make_examples in training mode should be run multiple times independently for training and validation sets? If for example, I used Chromosome 1 for my training set and Chromosome 2 for my validation set, should those repeated runs be made on different chromosomes, or the same chromosomes? Then finally, once everything is shuffled, run model_train and model_eval. Let me reply to this part first:. ""First step is to run deepvariant make_examples in training mode to create training and validation sets. In the docume",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:1316,security,validat,validation,1316,"cally shuffle the training examples using the shuffle_tfrecords_beam.py script. > . > I downloaded the latest version of tensorflow (2.15) and was initially getting an error that apache beam was not being recognized, and realized that beam did not install because its latest version (2.54) was incompatible with the current version of numpy (1.26) that was being imported. I uninstalled that new version of numpy in tensorflow and installed an older version that would be compatible (1.24.4), and then was able to install apache beam (2.54). However, now I'm getting even more errors (see below). Do you have any advice on which versions of everything I should make sure to have installed correctly before running the shuffle script? Any guidance is very much appreciated. > . > Not so much a question but I want to confirm my understanding of the pipeline from the tutorial, as again I am very new to this. First step is to run deepvariant make_examples in training mode to create training and validation sets. In the documents, these are individual chromosomes, but in theory these could be whole individuals or multiple individuals, is that correct? And then make_examples in training mode should be run multiple times independently for training and validation sets? If for example, I used Chromosome 1 for my training set and Chromosome 2 for my validation set, should those repeated runs be made on different chromosomes, or the same chromosomes? Then finally, once everything is shuffled, run model_train and model_eval. Let me reply to this part first:. ""First step is to run deepvariant make_examples in training mode to create training and validation sets. In the documents, these are individual chromosomes, but in theory these could be whole individuals or multiple individuals, is that correct?"" --> Yes that's correct. ""If for example, I used Chromosome 1 for my training set and Chromosome 2 for my validation set, should those repeated runs be made on different chromosomes, or the sam",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:1574,security,validat,validation,1574,"cause its latest version (2.54) was incompatible with the current version of numpy (1.26) that was being imported. I uninstalled that new version of numpy in tensorflow and installed an older version that would be compatible (1.24.4), and then was able to install apache beam (2.54). However, now I'm getting even more errors (see below). Do you have any advice on which versions of everything I should make sure to have installed correctly before running the shuffle script? Any guidance is very much appreciated. > . > Not so much a question but I want to confirm my understanding of the pipeline from the tutorial, as again I am very new to this. First step is to run deepvariant make_examples in training mode to create training and validation sets. In the documents, these are individual chromosomes, but in theory these could be whole individuals or multiple individuals, is that correct? And then make_examples in training mode should be run multiple times independently for training and validation sets? If for example, I used Chromosome 1 for my training set and Chromosome 2 for my validation set, should those repeated runs be made on different chromosomes, or the same chromosomes? Then finally, once everything is shuffled, run model_train and model_eval. Let me reply to this part first:. ""First step is to run deepvariant make_examples in training mode to create training and validation sets. In the documents, these are individual chromosomes, but in theory these could be whole individuals or multiple individuals, is that correct?"" --> Yes that's correct. ""If for example, I used Chromosome 1 for my training set and Chromosome 2 for my validation set, should those repeated runs be made on different chromosomes, or the same chromosomes?"". If you want Chromosome 1 for your training set, and Chromosome 2 for your validation set, you'll run make_examples twice. One run to generate the training set with chr1, the other run to generate validation set as chr2. Note that in both run",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:1671,security,validat,validation,1671,"s being imported. I uninstalled that new version of numpy in tensorflow and installed an older version that would be compatible (1.24.4), and then was able to install apache beam (2.54). However, now I'm getting even more errors (see below). Do you have any advice on which versions of everything I should make sure to have installed correctly before running the shuffle script? Any guidance is very much appreciated. > . > Not so much a question but I want to confirm my understanding of the pipeline from the tutorial, as again I am very new to this. First step is to run deepvariant make_examples in training mode to create training and validation sets. In the documents, these are individual chromosomes, but in theory these could be whole individuals or multiple individuals, is that correct? And then make_examples in training mode should be run multiple times independently for training and validation sets? If for example, I used Chromosome 1 for my training set and Chromosome 2 for my validation set, should those repeated runs be made on different chromosomes, or the same chromosomes? Then finally, once everything is shuffled, run model_train and model_eval. Let me reply to this part first:. ""First step is to run deepvariant make_examples in training mode to create training and validation sets. In the documents, these are individual chromosomes, but in theory these could be whole individuals or multiple individuals, is that correct?"" --> Yes that's correct. ""If for example, I used Chromosome 1 for my training set and Chromosome 2 for my validation set, should those repeated runs be made on different chromosomes, or the same chromosomes?"". If you want Chromosome 1 for your training set, and Chromosome 2 for your validation set, you'll run make_examples twice. One run to generate the training set with chr1, the other run to generate validation set as chr2. Note that in both runs, you'll run make_examples with the `--mode training` flag. This can be be a bit confusing, but ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:1970,security,validat,validation,1970,"should make sure to have installed correctly before running the shuffle script? Any guidance is very much appreciated. > . > Not so much a question but I want to confirm my understanding of the pipeline from the tutorial, as again I am very new to this. First step is to run deepvariant make_examples in training mode to create training and validation sets. In the documents, these are individual chromosomes, but in theory these could be whole individuals or multiple individuals, is that correct? And then make_examples in training mode should be run multiple times independently for training and validation sets? If for example, I used Chromosome 1 for my training set and Chromosome 2 for my validation set, should those repeated runs be made on different chromosomes, or the same chromosomes? Then finally, once everything is shuffled, run model_train and model_eval. Let me reply to this part first:. ""First step is to run deepvariant make_examples in training mode to create training and validation sets. In the documents, these are individual chromosomes, but in theory these could be whole individuals or multiple individuals, is that correct?"" --> Yes that's correct. ""If for example, I used Chromosome 1 for my training set and Chromosome 2 for my validation set, should those repeated runs be made on different chromosomes, or the same chromosomes?"". If you want Chromosome 1 for your training set, and Chromosome 2 for your validation set, you'll run make_examples twice. One run to generate the training set with chr1, the other run to generate validation set as chr2. Note that in both runs, you'll run make_examples with the `--mode training` flag. This can be be a bit confusing, but `--mode training` in make_examples just means that we will create examples with truth labels. And you will need truth labels for your training set and validation set, . > . > Thank you very much for your time, and if these questions are answered clearly in a doc already, then I apologize and would ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:2234,security,validat,validation,2234," is to run deepvariant make_examples in training mode to create training and validation sets. In the documents, these are individual chromosomes, but in theory these could be whole individuals or multiple individuals, is that correct? And then make_examples in training mode should be run multiple times independently for training and validation sets? If for example, I used Chromosome 1 for my training set and Chromosome 2 for my validation set, should those repeated runs be made on different chromosomes, or the same chromosomes? Then finally, once everything is shuffled, run model_train and model_eval. Let me reply to this part first:. ""First step is to run deepvariant make_examples in training mode to create training and validation sets. In the documents, these are individual chromosomes, but in theory these could be whole individuals or multiple individuals, is that correct?"" --> Yes that's correct. ""If for example, I used Chromosome 1 for my training set and Chromosome 2 for my validation set, should those repeated runs be made on different chromosomes, or the same chromosomes?"". If you want Chromosome 1 for your training set, and Chromosome 2 for your validation set, you'll run make_examples twice. One run to generate the training set with chr1, the other run to generate validation set as chr2. Note that in both runs, you'll run make_examples with the `--mode training` flag. This can be be a bit confusing, but `--mode training` in make_examples just means that we will create examples with truth labels. And you will need truth labels for your training set and validation set, . > . > Thank you very much for your time, and if these questions are answered clearly in a doc already, then I apologize and would appreciate being directed there. > . > Best, Haley. > . I'll separately look at the dependency issue. I'll plan to repeat what is documented in https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-training-case-study.md on a clean machine and see if ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:2412,security,validat,validation,2412,"le individuals or multiple individuals, is that correct? And then make_examples in training mode should be run multiple times independently for training and validation sets? If for example, I used Chromosome 1 for my training set and Chromosome 2 for my validation set, should those repeated runs be made on different chromosomes, or the same chromosomes? Then finally, once everything is shuffled, run model_train and model_eval. Let me reply to this part first:. ""First step is to run deepvariant make_examples in training mode to create training and validation sets. In the documents, these are individual chromosomes, but in theory these could be whole individuals or multiple individuals, is that correct?"" --> Yes that's correct. ""If for example, I used Chromosome 1 for my training set and Chromosome 2 for my validation set, should those repeated runs be made on different chromosomes, or the same chromosomes?"". If you want Chromosome 1 for your training set, and Chromosome 2 for your validation set, you'll run make_examples twice. One run to generate the training set with chr1, the other run to generate validation set as chr2. Note that in both runs, you'll run make_examples with the `--mode training` flag. This can be be a bit confusing, but `--mode training` in make_examples just means that we will create examples with truth labels. And you will need truth labels for your training set and validation set, . > . > Thank you very much for your time, and if these questions are answered clearly in a doc already, then I apologize and would appreciate being directed there. > . > Best, Haley. > . I'll separately look at the dependency issue. I'll plan to repeat what is documented in https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-training-case-study.md on a clean machine and see if the dependencies still work. Sometimes when we documented it, things worked, but later on some underlying dependencies might have shifted. This is actually why we packaged our va",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:2534,security,validat,validation,2534,"mes independently for training and validation sets? If for example, I used Chromosome 1 for my training set and Chromosome 2 for my validation set, should those repeated runs be made on different chromosomes, or the same chromosomes? Then finally, once everything is shuffled, run model_train and model_eval. Let me reply to this part first:. ""First step is to run deepvariant make_examples in training mode to create training and validation sets. In the documents, these are individual chromosomes, but in theory these could be whole individuals or multiple individuals, is that correct?"" --> Yes that's correct. ""If for example, I used Chromosome 1 for my training set and Chromosome 2 for my validation set, should those repeated runs be made on different chromosomes, or the same chromosomes?"". If you want Chromosome 1 for your training set, and Chromosome 2 for your validation set, you'll run make_examples twice. One run to generate the training set with chr1, the other run to generate validation set as chr2. Note that in both runs, you'll run make_examples with the `--mode training` flag. This can be be a bit confusing, but `--mode training` in make_examples just means that we will create examples with truth labels. And you will need truth labels for your training set and validation set, . > . > Thank you very much for your time, and if these questions are answered clearly in a doc already, then I apologize and would appreciate being directed there. > . > Best, Haley. > . I'll separately look at the dependency issue. I'll plan to repeat what is documented in https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-training-case-study.md on a clean machine and see if the dependencies still work. Sometimes when we documented it, things worked, but later on some underlying dependencies might have shifted. This is actually why we packaged our variant calling in Docker to make sure the versions are more consistent. But we haven't done so for our shuffling code in th",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:2827,security,validat,validation,2827," same chromosomes? Then finally, once everything is shuffled, run model_train and model_eval. Let me reply to this part first:. ""First step is to run deepvariant make_examples in training mode to create training and validation sets. In the documents, these are individual chromosomes, but in theory these could be whole individuals or multiple individuals, is that correct?"" --> Yes that's correct. ""If for example, I used Chromosome 1 for my training set and Chromosome 2 for my validation set, should those repeated runs be made on different chromosomes, or the same chromosomes?"". If you want Chromosome 1 for your training set, and Chromosome 2 for your validation set, you'll run make_examples twice. One run to generate the training set with chr1, the other run to generate validation set as chr2. Note that in both runs, you'll run make_examples with the `--mode training` flag. This can be be a bit confusing, but `--mode training` in make_examples just means that we will create examples with truth labels. And you will need truth labels for your training set and validation set, . > . > Thank you very much for your time, and if these questions are answered clearly in a doc already, then I apologize and would appreciate being directed there. > . > Best, Haley. > . I'll separately look at the dependency issue. I'll plan to repeat what is documented in https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-training-case-study.md on a clean machine and see if the dependencies still work. Sometimes when we documented it, things worked, but later on some underlying dependencies might have shifted. This is actually why we packaged our variant calling in Docker to make sure the versions are more consistent. But we haven't done so for our shuffling code in the training tutorial. Anyway, let me plan to walk through https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-training-case-study.md myself and see if it still works. I'll document my steps here later.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:1148,testability,understand,understanding,1148,"the [advanced training case study](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md). I've reached the stage where I need to locally shuffle the training examples using the shuffle_tfrecords_beam.py script. > . > I downloaded the latest version of tensorflow (2.15) and was initially getting an error that apache beam was not being recognized, and realized that beam did not install because its latest version (2.54) was incompatible with the current version of numpy (1.26) that was being imported. I uninstalled that new version of numpy in tensorflow and installed an older version that would be compatible (1.24.4), and then was able to install apache beam (2.54). However, now I'm getting even more errors (see below). Do you have any advice on which versions of everything I should make sure to have installed correctly before running the shuffle script? Any guidance is very much appreciated. > . > Not so much a question but I want to confirm my understanding of the pipeline from the tutorial, as again I am very new to this. First step is to run deepvariant make_examples in training mode to create training and validation sets. In the documents, these are individual chromosomes, but in theory these could be whole individuals or multiple individuals, is that correct? And then make_examples in training mode should be run multiple times independently for training and validation sets? If for example, I used Chromosome 1 for my training set and Chromosome 2 for my validation set, should those repeated runs be made on different chromosomes, or the same chromosomes? Then finally, once everything is shuffled, run model_train and model_eval. Let me reply to this part first:. ""First step is to run deepvariant make_examples in training mode to create training and validation sets. In the documents, these are individual chromosomes, but in theory these could be whole individuals or multiple individuals, is that correct?"" --> Yes that's correct. ""I",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:3059,testability,depend,dependency,3059," same chromosomes? Then finally, once everything is shuffled, run model_train and model_eval. Let me reply to this part first:. ""First step is to run deepvariant make_examples in training mode to create training and validation sets. In the documents, these are individual chromosomes, but in theory these could be whole individuals or multiple individuals, is that correct?"" --> Yes that's correct. ""If for example, I used Chromosome 1 for my training set and Chromosome 2 for my validation set, should those repeated runs be made on different chromosomes, or the same chromosomes?"". If you want Chromosome 1 for your training set, and Chromosome 2 for your validation set, you'll run make_examples twice. One run to generate the training set with chr1, the other run to generate validation set as chr2. Note that in both runs, you'll run make_examples with the `--mode training` flag. This can be be a bit confusing, but `--mode training` in make_examples just means that we will create examples with truth labels. And you will need truth labels for your training set and validation set, . > . > Thank you very much for your time, and if these questions are answered clearly in a doc already, then I apologize and would appreciate being directed there. > . > Best, Haley. > . I'll separately look at the dependency issue. I'll plan to repeat what is documented in https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-training-case-study.md on a clean machine and see if the dependencies still work. Sometimes when we documented it, things worked, but later on some underlying dependencies might have shifted. This is actually why we packaged our variant calling in Docker to make sure the versions are more consistent. But we haven't done so for our shuffling code in the training tutorial. Anyway, let me plan to walk through https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-training-case-study.md myself and see if it still works. I'll document my steps here later.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:3082,testability,plan,plan,3082," same chromosomes? Then finally, once everything is shuffled, run model_train and model_eval. Let me reply to this part first:. ""First step is to run deepvariant make_examples in training mode to create training and validation sets. In the documents, these are individual chromosomes, but in theory these could be whole individuals or multiple individuals, is that correct?"" --> Yes that's correct. ""If for example, I used Chromosome 1 for my training set and Chromosome 2 for my validation set, should those repeated runs be made on different chromosomes, or the same chromosomes?"". If you want Chromosome 1 for your training set, and Chromosome 2 for your validation set, you'll run make_examples twice. One run to generate the training set with chr1, the other run to generate validation set as chr2. Note that in both runs, you'll run make_examples with the `--mode training` flag. This can be be a bit confusing, but `--mode training` in make_examples just means that we will create examples with truth labels. And you will need truth labels for your training set and validation set, . > . > Thank you very much for your time, and if these questions are answered clearly in a doc already, then I apologize and would appreciate being directed there. > . > Best, Haley. > . I'll separately look at the dependency issue. I'll plan to repeat what is documented in https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-training-case-study.md on a clean machine and see if the dependencies still work. Sometimes when we documented it, things worked, but later on some underlying dependencies might have shifted. This is actually why we packaged our variant calling in Docker to make sure the versions are more consistent. But we haven't done so for our shuffling code in the training tutorial. Anyway, let me plan to walk through https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-training-case-study.md myself and see if it still works. I'll document my steps here later.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:3243,testability,depend,dependencies,3243," same chromosomes? Then finally, once everything is shuffled, run model_train and model_eval. Let me reply to this part first:. ""First step is to run deepvariant make_examples in training mode to create training and validation sets. In the documents, these are individual chromosomes, but in theory these could be whole individuals or multiple individuals, is that correct?"" --> Yes that's correct. ""If for example, I used Chromosome 1 for my training set and Chromosome 2 for my validation set, should those repeated runs be made on different chromosomes, or the same chromosomes?"". If you want Chromosome 1 for your training set, and Chromosome 2 for your validation set, you'll run make_examples twice. One run to generate the training set with chr1, the other run to generate validation set as chr2. Note that in both runs, you'll run make_examples with the `--mode training` flag. This can be be a bit confusing, but `--mode training` in make_examples just means that we will create examples with truth labels. And you will need truth labels for your training set and validation set, . > . > Thank you very much for your time, and if these questions are answered clearly in a doc already, then I apologize and would appreciate being directed there. > . > Best, Haley. > . I'll separately look at the dependency issue. I'll plan to repeat what is documented in https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-training-case-study.md on a clean machine and see if the dependencies still work. Sometimes when we documented it, things worked, but later on some underlying dependencies might have shifted. This is actually why we packaged our variant calling in Docker to make sure the versions are more consistent. But we haven't done so for our shuffling code in the training tutorial. Anyway, let me plan to walk through https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-training-case-study.md myself and see if it still works. I'll document my steps here later.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:3345,testability,depend,dependencies,3345," same chromosomes? Then finally, once everything is shuffled, run model_train and model_eval. Let me reply to this part first:. ""First step is to run deepvariant make_examples in training mode to create training and validation sets. In the documents, these are individual chromosomes, but in theory these could be whole individuals or multiple individuals, is that correct?"" --> Yes that's correct. ""If for example, I used Chromosome 1 for my training set and Chromosome 2 for my validation set, should those repeated runs be made on different chromosomes, or the same chromosomes?"". If you want Chromosome 1 for your training set, and Chromosome 2 for your validation set, you'll run make_examples twice. One run to generate the training set with chr1, the other run to generate validation set as chr2. Note that in both runs, you'll run make_examples with the `--mode training` flag. This can be be a bit confusing, but `--mode training` in make_examples just means that we will create examples with truth labels. And you will need truth labels for your training set and validation set, . > . > Thank you very much for your time, and if these questions are answered clearly in a doc already, then I apologize and would appreciate being directed there. > . > Best, Haley. > . I'll separately look at the dependency issue. I'll plan to repeat what is documented in https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-training-case-study.md on a clean machine and see if the dependencies still work. Sometimes when we documented it, things worked, but later on some underlying dependencies might have shifted. This is actually why we packaged our variant calling in Docker to make sure the versions are more consistent. But we haven't done so for our shuffling code in the training tutorial. Anyway, let me plan to walk through https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-training-case-study.md myself and see if it still works. I'll document my steps here later.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:3575,testability,plan,plan,3575," same chromosomes? Then finally, once everything is shuffled, run model_train and model_eval. Let me reply to this part first:. ""First step is to run deepvariant make_examples in training mode to create training and validation sets. In the documents, these are individual chromosomes, but in theory these could be whole individuals or multiple individuals, is that correct?"" --> Yes that's correct. ""If for example, I used Chromosome 1 for my training set and Chromosome 2 for my validation set, should those repeated runs be made on different chromosomes, or the same chromosomes?"". If you want Chromosome 1 for your training set, and Chromosome 2 for your validation set, you'll run make_examples twice. One run to generate the training set with chr1, the other run to generate validation set as chr2. Note that in both runs, you'll run make_examples with the `--mode training` flag. This can be be a bit confusing, but `--mode training` in make_examples just means that we will create examples with truth labels. And you will need truth labels for your training set and validation set, . > . > Thank you very much for your time, and if these questions are answered clearly in a doc already, then I apologize and would appreciate being directed there. > . > Best, Haley. > . I'll separately look at the dependency issue. I'll plan to repeat what is documented in https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-training-case-study.md on a clean machine and see if the dependencies still work. Sometimes when we documented it, things worked, but later on some underlying dependencies might have shifted. This is actually why we packaged our variant calling in Docker to make sure the versions are more consistent. But we haven't done so for our shuffling code in the training tutorial. Anyway, let me plan to walk through https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-training-case-study.md myself and see if it still works. I'll document my steps here later.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:489,usability,error,error,489,"> Hello,. > . > I'm very new to model training and honestly, coding, so thank you for your patience! I'm trying to run my own samples following along with the [advanced training case study](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md). I've reached the stage where I need to locally shuffle the training examples using the shuffle_tfrecords_beam.py script. > . > I downloaded the latest version of tensorflow (2.15) and was initially getting an error that apache beam was not being recognized, and realized that beam did not install because its latest version (2.54) was incompatible with the current version of numpy (1.26) that was being imported. I uninstalled that new version of numpy in tensorflow and installed an older version that would be compatible (1.24.4), and then was able to install apache beam (2.54). However, now I'm getting even more errors (see below). Do you have any advice on which versions of everything I should make sure to have installed correctly before running the shuffle script? Any guidance is very much appreciated. > . > Not so much a question but I want to confirm my understanding of the pipeline from the tutorial, as again I am very new to this. First step is to run deepvariant make_examples in training mode to create training and validation sets. In the documents, these are individual chromosomes, but in theory these could be whole individuals or multiple individuals, is that correct? And then make_examples in training mode should be run multiple times independently for training and validation sets? If for example, I used Chromosome 1 for my training set and Chromosome 2 for my validation set, should those repeated runs be made on different chromosomes, or the same chromosomes? Then finally, once everything is shuffled, run model_train and model_eval. Let me reply to this part first:. ""First step is to run deepvariant make_examples in training mode to create training and validation sets. In the docume",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:898,usability,error,errors,898,"> Hello,. > . > I'm very new to model training and honestly, coding, so thank you for your patience! I'm trying to run my own samples following along with the [advanced training case study](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md). I've reached the stage where I need to locally shuffle the training examples using the shuffle_tfrecords_beam.py script. > . > I downloaded the latest version of tensorflow (2.15) and was initially getting an error that apache beam was not being recognized, and realized that beam did not install because its latest version (2.54) was incompatible with the current version of numpy (1.26) that was being imported. I uninstalled that new version of numpy in tensorflow and installed an older version that would be compatible (1.24.4), and then was able to install apache beam (2.54). However, now I'm getting even more errors (see below). Do you have any advice on which versions of everything I should make sure to have installed correctly before running the shuffle script? Any guidance is very much appreciated. > . > Not so much a question but I want to confirm my understanding of the pipeline from the tutorial, as again I am very new to this. First step is to run deepvariant make_examples in training mode to create training and validation sets. In the documents, these are individual chromosomes, but in theory these could be whole individuals or multiple individuals, is that correct? And then make_examples in training mode should be run multiple times independently for training and validation sets? If for example, I used Chromosome 1 for my training set and Chromosome 2 for my validation set, should those repeated runs be made on different chromosomes, or the same chromosomes? Then finally, once everything is shuffled, run model_train and model_eval. Let me reply to this part first:. ""First step is to run deepvariant make_examples in training mode to create training and validation sets. In the docume",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:1059,usability,guidanc,guidance,1059,"ding, so thank you for your patience! I'm trying to run my own samples following along with the [advanced training case study](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md). I've reached the stage where I need to locally shuffle the training examples using the shuffle_tfrecords_beam.py script. > . > I downloaded the latest version of tensorflow (2.15) and was initially getting an error that apache beam was not being recognized, and realized that beam did not install because its latest version (2.54) was incompatible with the current version of numpy (1.26) that was being imported. I uninstalled that new version of numpy in tensorflow and installed an older version that would be compatible (1.24.4), and then was able to install apache beam (2.54). However, now I'm getting even more errors (see below). Do you have any advice on which versions of everything I should make sure to have installed correctly before running the shuffle script? Any guidance is very much appreciated. > . > Not so much a question but I want to confirm my understanding of the pipeline from the tutorial, as again I am very new to this. First step is to run deepvariant make_examples in training mode to create training and validation sets. In the documents, these are individual chromosomes, but in theory these could be whole individuals or multiple individuals, is that correct? And then make_examples in training mode should be run multiple times independently for training and validation sets? If for example, I used Chromosome 1 for my training set and Chromosome 2 for my validation set, should those repeated runs be made on different chromosomes, or the same chromosomes? Then finally, once everything is shuffled, run model_train and model_eval. Let me reply to this part first:. ""First step is to run deepvariant make_examples in training mode to create training and validation sets. In the documents, these are individual chromosomes, but in theory these coul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:1137,usability,confirm,confirm,1137,"ng along with the [advanced training case study](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md). I've reached the stage where I need to locally shuffle the training examples using the shuffle_tfrecords_beam.py script. > . > I downloaded the latest version of tensorflow (2.15) and was initially getting an error that apache beam was not being recognized, and realized that beam did not install because its latest version (2.54) was incompatible with the current version of numpy (1.26) that was being imported. I uninstalled that new version of numpy in tensorflow and installed an older version that would be compatible (1.24.4), and then was able to install apache beam (2.54). However, now I'm getting even more errors (see below). Do you have any advice on which versions of everything I should make sure to have installed correctly before running the shuffle script? Any guidance is very much appreciated. > . > Not so much a question but I want to confirm my understanding of the pipeline from the tutorial, as again I am very new to this. First step is to run deepvariant make_examples in training mode to create training and validation sets. In the documents, these are individual chromosomes, but in theory these could be whole individuals or multiple individuals, is that correct? And then make_examples in training mode should be run multiple times independently for training and validation sets? If for example, I used Chromosome 1 for my training set and Chromosome 2 for my validation set, should those repeated runs be made on different chromosomes, or the same chromosomes? Then finally, once everything is shuffled, run model_train and model_eval. Let me reply to this part first:. ""First step is to run deepvariant make_examples in training mode to create training and validation sets. In the documents, these are individual chromosomes, but in theory these could be whole individuals or multiple individuals, is that correct?"" --> Yes that",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:1340,usability,document,documents,1340,"ng examples using the shuffle_tfrecords_beam.py script. > . > I downloaded the latest version of tensorflow (2.15) and was initially getting an error that apache beam was not being recognized, and realized that beam did not install because its latest version (2.54) was incompatible with the current version of numpy (1.26) that was being imported. I uninstalled that new version of numpy in tensorflow and installed an older version that would be compatible (1.24.4), and then was able to install apache beam (2.54). However, now I'm getting even more errors (see below). Do you have any advice on which versions of everything I should make sure to have installed correctly before running the shuffle script? Any guidance is very much appreciated. > . > Not so much a question but I want to confirm my understanding of the pipeline from the tutorial, as again I am very new to this. First step is to run deepvariant make_examples in training mode to create training and validation sets. In the documents, these are individual chromosomes, but in theory these could be whole individuals or multiple individuals, is that correct? And then make_examples in training mode should be run multiple times independently for training and validation sets? If for example, I used Chromosome 1 for my training set and Chromosome 2 for my validation set, should those repeated runs be made on different chromosomes, or the same chromosomes? Then finally, once everything is shuffled, run model_train and model_eval. Let me reply to this part first:. ""First step is to run deepvariant make_examples in training mode to create training and validation sets. In the documents, these are individual chromosomes, but in theory these could be whole individuals or multiple individuals, is that correct?"" --> Yes that's correct. ""If for example, I used Chromosome 1 for my training set and Chromosome 2 for my validation set, should those repeated runs be made on different chromosomes, or the same chromosomes?"". If you ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:1994,usability,document,documents,1994," installed correctly before running the shuffle script? Any guidance is very much appreciated. > . > Not so much a question but I want to confirm my understanding of the pipeline from the tutorial, as again I am very new to this. First step is to run deepvariant make_examples in training mode to create training and validation sets. In the documents, these are individual chromosomes, but in theory these could be whole individuals or multiple individuals, is that correct? And then make_examples in training mode should be run multiple times independently for training and validation sets? If for example, I used Chromosome 1 for my training set and Chromosome 2 for my validation set, should those repeated runs be made on different chromosomes, or the same chromosomes? Then finally, once everything is shuffled, run model_train and model_eval. Let me reply to this part first:. ""First step is to run deepvariant make_examples in training mode to create training and validation sets. In the documents, these are individual chromosomes, but in theory these could be whole individuals or multiple individuals, is that correct?"" --> Yes that's correct. ""If for example, I used Chromosome 1 for my training set and Chromosome 2 for my validation set, should those repeated runs be made on different chromosomes, or the same chromosomes?"". If you want Chromosome 1 for your training set, and Chromosome 2 for your validation set, you'll run make_examples twice. One run to generate the training set with chr1, the other run to generate validation set as chr2. Note that in both runs, you'll run make_examples with the `--mode training` flag. This can be be a bit confusing, but `--mode training` in make_examples just means that we will create examples with truth labels. And you will need truth labels for your training set and validation set, . > . > Thank you very much for your time, and if these questions are answered clearly in a doc already, then I apologize and would appreciate being directe",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:2922,usability,clear,clearly,2922," same chromosomes? Then finally, once everything is shuffled, run model_train and model_eval. Let me reply to this part first:. ""First step is to run deepvariant make_examples in training mode to create training and validation sets. In the documents, these are individual chromosomes, but in theory these could be whole individuals or multiple individuals, is that correct?"" --> Yes that's correct. ""If for example, I used Chromosome 1 for my training set and Chromosome 2 for my validation set, should those repeated runs be made on different chromosomes, or the same chromosomes?"". If you want Chromosome 1 for your training set, and Chromosome 2 for your validation set, you'll run make_examples twice. One run to generate the training set with chr1, the other run to generate validation set as chr2. Note that in both runs, you'll run make_examples with the `--mode training` flag. This can be be a bit confusing, but `--mode training` in make_examples just means that we will create examples with truth labels. And you will need truth labels for your training set and validation set, . > . > Thank you very much for your time, and if these questions are answered clearly in a doc already, then I apologize and would appreciate being directed there. > . > Best, Haley. > . I'll separately look at the dependency issue. I'll plan to repeat what is documented in https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-training-case-study.md on a clean machine and see if the dependencies still work. Sometimes when we documented it, things worked, but later on some underlying dependencies might have shifted. This is actually why we packaged our variant calling in Docker to make sure the versions are more consistent. But we haven't done so for our shuffling code in the training tutorial. Anyway, let me plan to walk through https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-training-case-study.md myself and see if it still works. I'll document my steps here later.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:3105,usability,document,documented,3105," same chromosomes? Then finally, once everything is shuffled, run model_train and model_eval. Let me reply to this part first:. ""First step is to run deepvariant make_examples in training mode to create training and validation sets. In the documents, these are individual chromosomes, but in theory these could be whole individuals or multiple individuals, is that correct?"" --> Yes that's correct. ""If for example, I used Chromosome 1 for my training set and Chromosome 2 for my validation set, should those repeated runs be made on different chromosomes, or the same chromosomes?"". If you want Chromosome 1 for your training set, and Chromosome 2 for your validation set, you'll run make_examples twice. One run to generate the training set with chr1, the other run to generate validation set as chr2. Note that in both runs, you'll run make_examples with the `--mode training` flag. This can be be a bit confusing, but `--mode training` in make_examples just means that we will create examples with truth labels. And you will need truth labels for your training set and validation set, . > . > Thank you very much for your time, and if these questions are answered clearly in a doc already, then I apologize and would appreciate being directed there. > . > Best, Haley. > . I'll separately look at the dependency issue. I'll plan to repeat what is documented in https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-training-case-study.md on a clean machine and see if the dependencies still work. Sometimes when we documented it, things worked, but later on some underlying dependencies might have shifted. This is actually why we packaged our variant calling in Docker to make sure the versions are more consistent. But we haven't done so for our shuffling code in the training tutorial. Anyway, let me plan to walk through https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-training-case-study.md myself and see if it still works. I'll document my steps here later.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:3286,usability,document,documented,3286," same chromosomes? Then finally, once everything is shuffled, run model_train and model_eval. Let me reply to this part first:. ""First step is to run deepvariant make_examples in training mode to create training and validation sets. In the documents, these are individual chromosomes, but in theory these could be whole individuals or multiple individuals, is that correct?"" --> Yes that's correct. ""If for example, I used Chromosome 1 for my training set and Chromosome 2 for my validation set, should those repeated runs be made on different chromosomes, or the same chromosomes?"". If you want Chromosome 1 for your training set, and Chromosome 2 for your validation set, you'll run make_examples twice. One run to generate the training set with chr1, the other run to generate validation set as chr2. Note that in both runs, you'll run make_examples with the `--mode training` flag. This can be be a bit confusing, but `--mode training` in make_examples just means that we will create examples with truth labels. And you will need truth labels for your training set and validation set, . > . > Thank you very much for your time, and if these questions are answered clearly in a doc already, then I apologize and would appreciate being directed there. > . > Best, Haley. > . I'll separately look at the dependency issue. I'll plan to repeat what is documented in https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-training-case-study.md on a clean machine and see if the dependencies still work. Sometimes when we documented it, things worked, but later on some underlying dependencies might have shifted. This is actually why we packaged our variant calling in Docker to make sure the versions are more consistent. But we haven't done so for our shuffling code in the training tutorial. Anyway, let me plan to walk through https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-training-case-study.md myself and see if it still works. I'll document my steps here later.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:3476,usability,consist,consistent,3476," same chromosomes? Then finally, once everything is shuffled, run model_train and model_eval. Let me reply to this part first:. ""First step is to run deepvariant make_examples in training mode to create training and validation sets. In the documents, these are individual chromosomes, but in theory these could be whole individuals or multiple individuals, is that correct?"" --> Yes that's correct. ""If for example, I used Chromosome 1 for my training set and Chromosome 2 for my validation set, should those repeated runs be made on different chromosomes, or the same chromosomes?"". If you want Chromosome 1 for your training set, and Chromosome 2 for your validation set, you'll run make_examples twice. One run to generate the training set with chr1, the other run to generate validation set as chr2. Note that in both runs, you'll run make_examples with the `--mode training` flag. This can be be a bit confusing, but `--mode training` in make_examples just means that we will create examples with truth labels. And you will need truth labels for your training set and validation set, . > . > Thank you very much for your time, and if these questions are answered clearly in a doc already, then I apologize and would appreciate being directed there. > . > Best, Haley. > . I'll separately look at the dependency issue. I'll plan to repeat what is documented in https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-training-case-study.md on a clean machine and see if the dependencies still work. Sometimes when we documented it, things worked, but later on some underlying dependencies might have shifted. This is actually why we packaged our variant calling in Docker to make sure the versions are more consistent. But we haven't done so for our shuffling code in the training tutorial. Anyway, let me plan to walk through https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-training-case-study.md myself and see if it still works. I'll document my steps here later.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:3725,usability,document,document,3725," same chromosomes? Then finally, once everything is shuffled, run model_train and model_eval. Let me reply to this part first:. ""First step is to run deepvariant make_examples in training mode to create training and validation sets. In the documents, these are individual chromosomes, but in theory these could be whole individuals or multiple individuals, is that correct?"" --> Yes that's correct. ""If for example, I used Chromosome 1 for my training set and Chromosome 2 for my validation set, should those repeated runs be made on different chromosomes, or the same chromosomes?"". If you want Chromosome 1 for your training set, and Chromosome 2 for your validation set, you'll run make_examples twice. One run to generate the training set with chr1, the other run to generate validation set as chr2. Note that in both runs, you'll run make_examples with the `--mode training` flag. This can be be a bit confusing, but `--mode training` in make_examples just means that we will create examples with truth labels. And you will need truth labels for your training set and validation set, . > . > Thank you very much for your time, and if these questions are answered clearly in a doc already, then I apologize and would appreciate being directed there. > . > Best, Haley. > . I'll separately look at the dependency issue. I'll plan to repeat what is documented in https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-training-case-study.md on a clean machine and see if the dependencies still work. Sometimes when we documented it, things worked, but later on some underlying dependencies might have shifted. This is actually why we packaged our variant calling in Docker to make sure the versions are more consistent. But we haven't done so for our shuffling code in the training tutorial. Anyway, let me plan to walk through https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-training-case-study.md myself and see if it still works. I'll document my steps here later.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:275,availability,mainten,maintenance-policy,275,"# Follow https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-training-case-study.md. I got a machine:. ```bash. host=""${USER}-deepvariant-vm"". zone=""us-west1-b"". gcloud compute instances create ${host} \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --maintenance-policy ""TERMINATE"" \. --accelerator=type=nvidia-tesla-p100,count=1 \. --image-family ""ubuntu-2004-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""n1-standard-16"" \. --boot-disk-size ""300"" \. --zone ""${zone}"" \. --min-cpu-platform ""Intel Skylake"". ```. I sshed into my machine:. ```bash. gcloud compute ssh pichuan-deepvariant-vm --zone us-west1-b. ```. I ran this with my own `YOUR_PROJECT` and `OUTPUT_GCS_BUCKET` setting. Then the following is basically just copy/paste from the doc:. ```. BUCKET=""gs://deepvariant"". VERSION=""1.6.1"". DOCKER_IMAGE=""google/deepvariant:${VERSION}"". MODEL_BUCKET=""${BUCKET}/models/DeepVariant/${VERSION}/DeepVariant-inception_v3-${VERSION}+data-wgs_standard"". GCS_PRETRAINED_WGS_MODEL=""${MODEL_BUCKET}/model.ckpt"". OUTPUT_BUCKET=""${OUTPUT_GCS_BUCKET}/customized_training"". TRAINING_DIR=""${OUTPUT_BUCKET}/training_dir"". BASE=""${HOME}/training-case-study"". DATA_BUCKET=gs://deepvariant/training-case-study/BGISEQ-HG001. INPUT_DIR=""${BASE}/input"". BIN_DIR=""${INPUT_DIR}/bin"". DATA_DIR=""${INPUT_DIR}/data"". OUTPUT_DIR=""${BASE}/output"". LOG_DIR=""${OUTPUT_DIR}/logs"". SHUFFLE_SCRIPT_DIR=""${HOME}/deepvariant/tools"". REF=""${DATA_DIR}/ucsc_hg19.fa"". BAM_CHR1=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr1.bam"". BAM_CHR20=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr20.bam"". BAM_CHR21=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr21.bam"". TRUTH_VCF=""${DATA_DIR}/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_PGandRTGphasetransfer_chrs_FIXED.vcf.gz"". TRUTH_BED=""${DATA_DIR}/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_nosomaticdel_chr.bed"". N_SHARDS=16. ```. ```bash. mkdir -p ""${OUTPUT_DIR}"". mkd",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:819,deployability,VERSION,VERSION,819,"# Follow https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-training-case-study.md. I got a machine:. ```bash. host=""${USER}-deepvariant-vm"". zone=""us-west1-b"". gcloud compute instances create ${host} \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --maintenance-policy ""TERMINATE"" \. --accelerator=type=nvidia-tesla-p100,count=1 \. --image-family ""ubuntu-2004-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""n1-standard-16"" \. --boot-disk-size ""300"" \. --zone ""${zone}"" \. --min-cpu-platform ""Intel Skylake"". ```. I sshed into my machine:. ```bash. gcloud compute ssh pichuan-deepvariant-vm --zone us-west1-b. ```. I ran this with my own `YOUR_PROJECT` and `OUTPUT_GCS_BUCKET` setting. Then the following is basically just copy/paste from the doc:. ```. BUCKET=""gs://deepvariant"". VERSION=""1.6.1"". DOCKER_IMAGE=""google/deepvariant:${VERSION}"". MODEL_BUCKET=""${BUCKET}/models/DeepVariant/${VERSION}/DeepVariant-inception_v3-${VERSION}+data-wgs_standard"". GCS_PRETRAINED_WGS_MODEL=""${MODEL_BUCKET}/model.ckpt"". OUTPUT_BUCKET=""${OUTPUT_GCS_BUCKET}/customized_training"". TRAINING_DIR=""${OUTPUT_BUCKET}/training_dir"". BASE=""${HOME}/training-case-study"". DATA_BUCKET=gs://deepvariant/training-case-study/BGISEQ-HG001. INPUT_DIR=""${BASE}/input"". BIN_DIR=""${INPUT_DIR}/bin"". DATA_DIR=""${INPUT_DIR}/data"". OUTPUT_DIR=""${BASE}/output"". LOG_DIR=""${OUTPUT_DIR}/logs"". SHUFFLE_SCRIPT_DIR=""${HOME}/deepvariant/tools"". REF=""${DATA_DIR}/ucsc_hg19.fa"". BAM_CHR1=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr1.bam"". BAM_CHR20=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr20.bam"". BAM_CHR21=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr21.bam"". TRUTH_VCF=""${DATA_DIR}/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_PGandRTGphasetransfer_chrs_FIXED.vcf.gz"". TRUTH_BED=""${DATA_DIR}/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_nosomaticdel_chr.bed"". N_SHARDS=16. ```. ```bash. mkdir -p ""${OUTPUT_DIR}"". mkd",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:871,deployability,VERSION,VERSION,871,"# Follow https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-training-case-study.md. I got a machine:. ```bash. host=""${USER}-deepvariant-vm"". zone=""us-west1-b"". gcloud compute instances create ${host} \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --maintenance-policy ""TERMINATE"" \. --accelerator=type=nvidia-tesla-p100,count=1 \. --image-family ""ubuntu-2004-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""n1-standard-16"" \. --boot-disk-size ""300"" \. --zone ""${zone}"" \. --min-cpu-platform ""Intel Skylake"". ```. I sshed into my machine:. ```bash. gcloud compute ssh pichuan-deepvariant-vm --zone us-west1-b. ```. I ran this with my own `YOUR_PROJECT` and `OUTPUT_GCS_BUCKET` setting. Then the following is basically just copy/paste from the doc:. ```. BUCKET=""gs://deepvariant"". VERSION=""1.6.1"". DOCKER_IMAGE=""google/deepvariant:${VERSION}"". MODEL_BUCKET=""${BUCKET}/models/DeepVariant/${VERSION}/DeepVariant-inception_v3-${VERSION}+data-wgs_standard"". GCS_PRETRAINED_WGS_MODEL=""${MODEL_BUCKET}/model.ckpt"". OUTPUT_BUCKET=""${OUTPUT_GCS_BUCKET}/customized_training"". TRAINING_DIR=""${OUTPUT_BUCKET}/training_dir"". BASE=""${HOME}/training-case-study"". DATA_BUCKET=gs://deepvariant/training-case-study/BGISEQ-HG001. INPUT_DIR=""${BASE}/input"". BIN_DIR=""${INPUT_DIR}/bin"". DATA_DIR=""${INPUT_DIR}/data"". OUTPUT_DIR=""${BASE}/output"". LOG_DIR=""${OUTPUT_DIR}/logs"". SHUFFLE_SCRIPT_DIR=""${HOME}/deepvariant/tools"". REF=""${DATA_DIR}/ucsc_hg19.fa"". BAM_CHR1=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr1.bam"". BAM_CHR20=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr20.bam"". BAM_CHR21=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr21.bam"". TRUTH_VCF=""${DATA_DIR}/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_PGandRTGphasetransfer_chrs_FIXED.vcf.gz"". TRUTH_BED=""${DATA_DIR}/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_nosomaticdel_chr.bed"". N_SHARDS=16. ```. ```bash. mkdir -p ""${OUTPUT_DIR}"". mkd",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:927,deployability,VERSION,VERSION,927,"# Follow https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-training-case-study.md. I got a machine:. ```bash. host=""${USER}-deepvariant-vm"". zone=""us-west1-b"". gcloud compute instances create ${host} \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --maintenance-policy ""TERMINATE"" \. --accelerator=type=nvidia-tesla-p100,count=1 \. --image-family ""ubuntu-2004-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""n1-standard-16"" \. --boot-disk-size ""300"" \. --zone ""${zone}"" \. --min-cpu-platform ""Intel Skylake"". ```. I sshed into my machine:. ```bash. gcloud compute ssh pichuan-deepvariant-vm --zone us-west1-b. ```. I ran this with my own `YOUR_PROJECT` and `OUTPUT_GCS_BUCKET` setting. Then the following is basically just copy/paste from the doc:. ```. BUCKET=""gs://deepvariant"". VERSION=""1.6.1"". DOCKER_IMAGE=""google/deepvariant:${VERSION}"". MODEL_BUCKET=""${BUCKET}/models/DeepVariant/${VERSION}/DeepVariant-inception_v3-${VERSION}+data-wgs_standard"". GCS_PRETRAINED_WGS_MODEL=""${MODEL_BUCKET}/model.ckpt"". OUTPUT_BUCKET=""${OUTPUT_GCS_BUCKET}/customized_training"". TRAINING_DIR=""${OUTPUT_BUCKET}/training_dir"". BASE=""${HOME}/training-case-study"". DATA_BUCKET=gs://deepvariant/training-case-study/BGISEQ-HG001. INPUT_DIR=""${BASE}/input"". BIN_DIR=""${INPUT_DIR}/bin"". DATA_DIR=""${INPUT_DIR}/data"". OUTPUT_DIR=""${BASE}/output"". LOG_DIR=""${OUTPUT_DIR}/logs"". SHUFFLE_SCRIPT_DIR=""${HOME}/deepvariant/tools"". REF=""${DATA_DIR}/ucsc_hg19.fa"". BAM_CHR1=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr1.bam"". BAM_CHR20=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr20.bam"". BAM_CHR21=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr21.bam"". TRUTH_VCF=""${DATA_DIR}/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_PGandRTGphasetransfer_chrs_FIXED.vcf.gz"". TRUTH_BED=""${DATA_DIR}/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_nosomaticdel_chr.bed"". N_SHARDS=16. ```. ```bash. mkdir -p ""${OUTPUT_DIR}"". mkd",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:963,deployability,VERSION,VERSION,963,"# Follow https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-training-case-study.md. I got a machine:. ```bash. host=""${USER}-deepvariant-vm"". zone=""us-west1-b"". gcloud compute instances create ${host} \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --maintenance-policy ""TERMINATE"" \. --accelerator=type=nvidia-tesla-p100,count=1 \. --image-family ""ubuntu-2004-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""n1-standard-16"" \. --boot-disk-size ""300"" \. --zone ""${zone}"" \. --min-cpu-platform ""Intel Skylake"". ```. I sshed into my machine:. ```bash. gcloud compute ssh pichuan-deepvariant-vm --zone us-west1-b. ```. I ran this with my own `YOUR_PROJECT` and `OUTPUT_GCS_BUCKET` setting. Then the following is basically just copy/paste from the doc:. ```. BUCKET=""gs://deepvariant"". VERSION=""1.6.1"". DOCKER_IMAGE=""google/deepvariant:${VERSION}"". MODEL_BUCKET=""${BUCKET}/models/DeepVariant/${VERSION}/DeepVariant-inception_v3-${VERSION}+data-wgs_standard"". GCS_PRETRAINED_WGS_MODEL=""${MODEL_BUCKET}/model.ckpt"". OUTPUT_BUCKET=""${OUTPUT_GCS_BUCKET}/customized_training"". TRAINING_DIR=""${OUTPUT_BUCKET}/training_dir"". BASE=""${HOME}/training-case-study"". DATA_BUCKET=gs://deepvariant/training-case-study/BGISEQ-HG001. INPUT_DIR=""${BASE}/input"". BIN_DIR=""${INPUT_DIR}/bin"". DATA_DIR=""${INPUT_DIR}/data"". OUTPUT_DIR=""${BASE}/output"". LOG_DIR=""${OUTPUT_DIR}/logs"". SHUFFLE_SCRIPT_DIR=""${HOME}/deepvariant/tools"". REF=""${DATA_DIR}/ucsc_hg19.fa"". BAM_CHR1=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr1.bam"". BAM_CHR20=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr20.bam"". BAM_CHR21=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr21.bam"". TRUTH_VCF=""${DATA_DIR}/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_PGandRTGphasetransfer_chrs_FIXED.vcf.gz"". TRUTH_BED=""${DATA_DIR}/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_nosomaticdel_chr.bed"". N_SHARDS=16. ```. ```bash. mkdir -p ""${OUTPUT_DIR}"". mkd",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:1387,deployability,log,logs,1387," \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""n1-standard-16"" \. --boot-disk-size ""300"" \. --zone ""${zone}"" \. --min-cpu-platform ""Intel Skylake"". ```. I sshed into my machine:. ```bash. gcloud compute ssh pichuan-deepvariant-vm --zone us-west1-b. ```. I ran this with my own `YOUR_PROJECT` and `OUTPUT_GCS_BUCKET` setting. Then the following is basically just copy/paste from the doc:. ```. BUCKET=""gs://deepvariant"". VERSION=""1.6.1"". DOCKER_IMAGE=""google/deepvariant:${VERSION}"". MODEL_BUCKET=""${BUCKET}/models/DeepVariant/${VERSION}/DeepVariant-inception_v3-${VERSION}+data-wgs_standard"". GCS_PRETRAINED_WGS_MODEL=""${MODEL_BUCKET}/model.ckpt"". OUTPUT_BUCKET=""${OUTPUT_GCS_BUCKET}/customized_training"". TRAINING_DIR=""${OUTPUT_BUCKET}/training_dir"". BASE=""${HOME}/training-case-study"". DATA_BUCKET=gs://deepvariant/training-case-study/BGISEQ-HG001. INPUT_DIR=""${BASE}/input"". BIN_DIR=""${INPUT_DIR}/bin"". DATA_DIR=""${INPUT_DIR}/data"". OUTPUT_DIR=""${BASE}/output"". LOG_DIR=""${OUTPUT_DIR}/logs"". SHUFFLE_SCRIPT_DIR=""${HOME}/deepvariant/tools"". REF=""${DATA_DIR}/ucsc_hg19.fa"". BAM_CHR1=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr1.bam"". BAM_CHR20=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr20.bam"". BAM_CHR21=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr21.bam"". TRUTH_VCF=""${DATA_DIR}/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_PGandRTGphasetransfer_chrs_FIXED.vcf.gz"". TRUTH_BED=""${DATA_DIR}/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_nosomaticdel_chr.bed"". N_SHARDS=16. ```. ```bash. mkdir -p ""${OUTPUT_DIR}"". mkdir -p ""${BIN_DIR}"". mkdir -p ""${DATA_DIR}"". mkdir -p ""${LOG_DIR}"". ```. ```bash. gsutil -m cp ${DATA_BUCKET}/BGISEQ_PE100_NA12878.sorted.chr*.bam* ""${DATA_DIR}"". gsutil -m cp -r ""${DATA_BUCKET}/ucsc_hg19.fa*"" ""${DATA_DIR}"". gsutil -m cp -r ""${DATA_BUCKET}/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_*"" ""${DATA_DIR}"". ```. ```bash. sudo apt -y upd",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:2386,deployability,updat,update,2386,"gs"". SHUFFLE_SCRIPT_DIR=""${HOME}/deepvariant/tools"". REF=""${DATA_DIR}/ucsc_hg19.fa"". BAM_CHR1=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr1.bam"". BAM_CHR20=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr20.bam"". BAM_CHR21=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr21.bam"". TRUTH_VCF=""${DATA_DIR}/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_PGandRTGphasetransfer_chrs_FIXED.vcf.gz"". TRUTH_BED=""${DATA_DIR}/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_nosomaticdel_chr.bed"". N_SHARDS=16. ```. ```bash. mkdir -p ""${OUTPUT_DIR}"". mkdir -p ""${BIN_DIR}"". mkdir -p ""${DATA_DIR}"". mkdir -p ""${LOG_DIR}"". ```. ```bash. gsutil -m cp ${DATA_BUCKET}/BGISEQ_PE100_NA12878.sorted.chr*.bam* ""${DATA_DIR}"". gsutil -m cp -r ""${DATA_BUCKET}/ucsc_hg19.fa*"" ""${DATA_DIR}"". gsutil -m cp -r ""${DATA_BUCKET}/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_*"" ""${DATA_DIR}"". ```. ```bash. sudo apt -y update. sudo apt -y install parallel. curl -O https://raw.githubusercontent.com/google/deepvariant/r1.6.1/scripts/install_nvidia_docker.sh. bash -x install_nvidia_docker.sh. ```. ```bash. sudo docker pull ${DOCKER_IMAGE} # Standard CPU Docker Image. sudo docker pull ${DOCKER_IMAGE}-gpu # GPU-enabled Docker image. ```. ```bash. ( time seq 0 $((N_SHARDS-1)) | \. parallel --halt 2 --line-buffer \. sudo docker run \. -v ${HOME}:${HOME} \. ${DOCKER_IMAGE} \. make_examples \. --mode training \. --ref ""${REF}"" \. --reads ""${BAM_CHR1}"" \. --examples ""${OUTPUT_DIR}/training_set.with_label.tfrecord@${N_SHARDS}.gz"" \. --truth_variants ""${TRUTH_VCF}"" \. --confident_regions ""${TRUTH_BED}"" \. --task {} \. --regions ""'chr1'"" \. --channels ""insert_size"" \. ) 2>&1 | tee ""${LOG_DIR}/training_set.with_label.make_examples.log"". ```. This took `20m0.146s`. ```. $ cat ""${OUTPUT_DIR}/training_set.with_label.tfrecord-00000-of-00016.gz.example_info.json"". {""version"": ""1.6.0"", ""shape"": [100, 221, 7], ""channels"": [",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:2406,deployability,instal,install,2406,"IR=""${HOME}/deepvariant/tools"". REF=""${DATA_DIR}/ucsc_hg19.fa"". BAM_CHR1=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr1.bam"". BAM_CHR20=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr20.bam"". BAM_CHR21=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr21.bam"". TRUTH_VCF=""${DATA_DIR}/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_PGandRTGphasetransfer_chrs_FIXED.vcf.gz"". TRUTH_BED=""${DATA_DIR}/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_nosomaticdel_chr.bed"". N_SHARDS=16. ```. ```bash. mkdir -p ""${OUTPUT_DIR}"". mkdir -p ""${BIN_DIR}"". mkdir -p ""${DATA_DIR}"". mkdir -p ""${LOG_DIR}"". ```. ```bash. gsutil -m cp ${DATA_BUCKET}/BGISEQ_PE100_NA12878.sorted.chr*.bam* ""${DATA_DIR}"". gsutil -m cp -r ""${DATA_BUCKET}/ucsc_hg19.fa*"" ""${DATA_DIR}"". gsutil -m cp -r ""${DATA_BUCKET}/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_*"" ""${DATA_DIR}"". ```. ```bash. sudo apt -y update. sudo apt -y install parallel. curl -O https://raw.githubusercontent.com/google/deepvariant/r1.6.1/scripts/install_nvidia_docker.sh. bash -x install_nvidia_docker.sh. ```. ```bash. sudo docker pull ${DOCKER_IMAGE} # Standard CPU Docker Image. sudo docker pull ${DOCKER_IMAGE}-gpu # GPU-enabled Docker image. ```. ```bash. ( time seq 0 $((N_SHARDS-1)) | \. parallel --halt 2 --line-buffer \. sudo docker run \. -v ${HOME}:${HOME} \. ${DOCKER_IMAGE} \. make_examples \. --mode training \. --ref ""${REF}"" \. --reads ""${BAM_CHR1}"" \. --examples ""${OUTPUT_DIR}/training_set.with_label.tfrecord@${N_SHARDS}.gz"" \. --truth_variants ""${TRUTH_VCF}"" \. --confident_regions ""${TRUTH_BED}"" \. --task {} \. --regions ""'chr1'"" \. --channels ""insert_size"" \. ) 2>&1 | tee ""${LOG_DIR}/training_set.with_label.make_examples.log"". ```. This took `20m0.146s`. ```. $ cat ""${OUTPUT_DIR}/training_set.with_label.tfrecord-00000-of-00016.gz.example_info.json"". {""version"": ""1.6.0"", ""shape"": [100, 221, 7], ""channels"": [1, 2, 3, 4, 5, 6, 19]",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:3200,deployability,log,log,3200,"9.fa*"" ""${DATA_DIR}"". gsutil -m cp -r ""${DATA_BUCKET}/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_*"" ""${DATA_DIR}"". ```. ```bash. sudo apt -y update. sudo apt -y install parallel. curl -O https://raw.githubusercontent.com/google/deepvariant/r1.6.1/scripts/install_nvidia_docker.sh. bash -x install_nvidia_docker.sh. ```. ```bash. sudo docker pull ${DOCKER_IMAGE} # Standard CPU Docker Image. sudo docker pull ${DOCKER_IMAGE}-gpu # GPU-enabled Docker image. ```. ```bash. ( time seq 0 $((N_SHARDS-1)) | \. parallel --halt 2 --line-buffer \. sudo docker run \. -v ${HOME}:${HOME} \. ${DOCKER_IMAGE} \. make_examples \. --mode training \. --ref ""${REF}"" \. --reads ""${BAM_CHR1}"" \. --examples ""${OUTPUT_DIR}/training_set.with_label.tfrecord@${N_SHARDS}.gz"" \. --truth_variants ""${TRUTH_VCF}"" \. --confident_regions ""${TRUTH_BED}"" \. --task {} \. --regions ""'chr1'"" \. --channels ""insert_size"" \. ) 2>&1 | tee ""${LOG_DIR}/training_set.with_label.make_examples.log"". ```. This took `20m0.146s`. ```. $ cat ""${OUTPUT_DIR}/training_set.with_label.tfrecord-00000-of-00016.gz.example_info.json"". {""version"": ""1.6.0"", ""shape"": [100, 221, 7], ""channels"": [1, 2, 3, 4, 5, 6, 19]}. ```. ```bash. gsutil -m cp ${OUTPUT_DIR}/training_set.with_label.tfrecord-?????-of-00016.gz* \. ${OUTPUT_BUCKET}. ```. ```bash. ( time seq 0 $((N_SHARDS-1)) | \. parallel --halt 2 --line-buffer \. sudo docker run \. -v /home/${USER}:/home/${USER} \. ${DOCKER_IMAGE} \. make_examples \. --mode training \. --ref ""${REF}"" \. --reads ""${BAM_CHR21}"" \. --examples ""${OUTPUT_DIR}/validation_set.with_label.tfrecord@${N_SHARDS}.gz"" \. --truth_variants ""${TRUTH_VCF}"" \. --confident_regions ""${TRUTH_BED}"" \. --task {} \. --regions ""'chr21'"" \. --channels ""insert_size"" \. ) 2>&1 | tee ""${LOG_DIR}/validation_set.with_label.make_examples.log"". ```. This took `5m25.905s`. ```bash. gsutil -m cp ${OUTPUT_DIR}/validation_set.with_label.tfrecord-?????-of-00016.gz* \. ${OUTPUT_BUCKET}. ```. # This p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:3333,deployability,version,version,3333,"conf_*"" ""${DATA_DIR}"". ```. ```bash. sudo apt -y update. sudo apt -y install parallel. curl -O https://raw.githubusercontent.com/google/deepvariant/r1.6.1/scripts/install_nvidia_docker.sh. bash -x install_nvidia_docker.sh. ```. ```bash. sudo docker pull ${DOCKER_IMAGE} # Standard CPU Docker Image. sudo docker pull ${DOCKER_IMAGE}-gpu # GPU-enabled Docker image. ```. ```bash. ( time seq 0 $((N_SHARDS-1)) | \. parallel --halt 2 --line-buffer \. sudo docker run \. -v ${HOME}:${HOME} \. ${DOCKER_IMAGE} \. make_examples \. --mode training \. --ref ""${REF}"" \. --reads ""${BAM_CHR1}"" \. --examples ""${OUTPUT_DIR}/training_set.with_label.tfrecord@${N_SHARDS}.gz"" \. --truth_variants ""${TRUTH_VCF}"" \. --confident_regions ""${TRUTH_BED}"" \. --task {} \. --regions ""'chr1'"" \. --channels ""insert_size"" \. ) 2>&1 | tee ""${LOG_DIR}/training_set.with_label.make_examples.log"". ```. This took `20m0.146s`. ```. $ cat ""${OUTPUT_DIR}/training_set.with_label.tfrecord-00000-of-00016.gz.example_info.json"". {""version"": ""1.6.0"", ""shape"": [100, 221, 7], ""channels"": [1, 2, 3, 4, 5, 6, 19]}. ```. ```bash. gsutil -m cp ${OUTPUT_DIR}/training_set.with_label.tfrecord-?????-of-00016.gz* \. ${OUTPUT_BUCKET}. ```. ```bash. ( time seq 0 $((N_SHARDS-1)) | \. parallel --halt 2 --line-buffer \. sudo docker run \. -v /home/${USER}:/home/${USER} \. ${DOCKER_IMAGE} \. make_examples \. --mode training \. --ref ""${REF}"" \. --reads ""${BAM_CHR21}"" \. --examples ""${OUTPUT_DIR}/validation_set.with_label.tfrecord@${N_SHARDS}.gz"" \. --truth_variants ""${TRUTH_VCF}"" \. --confident_regions ""${TRUTH_BED}"" \. --task {} \. --regions ""'chr21'"" \. --channels ""insert_size"" \. ) 2>&1 | tee ""${LOG_DIR}/validation_set.with_label.make_examples.log"". ```. This took `5m25.905s`. ```bash. gsutil -m cp ${OUTPUT_DIR}/validation_set.with_label.tfrecord-?????-of-00016.gz* \. ${OUTPUT_BUCKET}. ```. # This parts starts shuffling... ```bash. sudo apt install -y python3.8-venv. # Create a virtualenv. python3 -m venv beam. # Activate the virtu",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:4044,deployability,log,log,4044,"t_regions ""${TRUTH_BED}"" \. --task {} \. --regions ""'chr1'"" \. --channels ""insert_size"" \. ) 2>&1 | tee ""${LOG_DIR}/training_set.with_label.make_examples.log"". ```. This took `20m0.146s`. ```. $ cat ""${OUTPUT_DIR}/training_set.with_label.tfrecord-00000-of-00016.gz.example_info.json"". {""version"": ""1.6.0"", ""shape"": [100, 221, 7], ""channels"": [1, 2, 3, 4, 5, 6, 19]}. ```. ```bash. gsutil -m cp ${OUTPUT_DIR}/training_set.with_label.tfrecord-?????-of-00016.gz* \. ${OUTPUT_BUCKET}. ```. ```bash. ( time seq 0 $((N_SHARDS-1)) | \. parallel --halt 2 --line-buffer \. sudo docker run \. -v /home/${USER}:/home/${USER} \. ${DOCKER_IMAGE} \. make_examples \. --mode training \. --ref ""${REF}"" \. --reads ""${BAM_CHR21}"" \. --examples ""${OUTPUT_DIR}/validation_set.with_label.tfrecord@${N_SHARDS}.gz"" \. --truth_variants ""${TRUTH_VCF}"" \. --confident_regions ""${TRUTH_BED}"" \. --task {} \. --regions ""'chr21'"" \. --channels ""insert_size"" \. ) 2>&1 | tee ""${LOG_DIR}/validation_set.with_label.make_examples.log"". ```. This took `5m25.905s`. ```bash. gsutil -m cp ${OUTPUT_DIR}/validation_set.with_label.tfrecord-?????-of-00016.gz* \. ${OUTPUT_BUCKET}. ```. # This parts starts shuffling... ```bash. sudo apt install -y python3.8-venv. # Create a virtualenv. python3 -m venv beam. # Activate the virtualenv. . beam/bin/activate. ```. ```bash. mkdir -p ${SHUFFLE_SCRIPT_DIR}. wget https://raw.githubusercontent.com/google/deepvariant/r1.6.1/tools/shuffle_tfrecords_beam.py -O ${SHUFFLE_SCRIPT_DIR}/shuffle_tfrecords_beam.py. ```. ```bash. sudo apt -y update && sudo apt -y install python3-pip. pip3 install --upgrade pip. pip3 install setuptools --upgrade. pip3 install apache_beam[gcp]==2.50.0 # 2.51.0 didn't work in my run. pip3 install tensorflow # For parsing tf.Example in shuffle_tfrecords_beam.py. ```. ```bash. time python3 ${SHUFFLE_SCRIPT_DIR}/shuffle_tfrecords_beam.py \. --project=""${YOUR_PROJECT}"" \. --input_pattern_list=""${OUTPUT_BUCKET}""/training_set.with_label.tfrecord-?????-of-00016.gz \. --",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:4245,deployability,instal,install,4245,"UTPUT_DIR}/training_set.with_label.tfrecord-00000-of-00016.gz.example_info.json"". {""version"": ""1.6.0"", ""shape"": [100, 221, 7], ""channels"": [1, 2, 3, 4, 5, 6, 19]}. ```. ```bash. gsutil -m cp ${OUTPUT_DIR}/training_set.with_label.tfrecord-?????-of-00016.gz* \. ${OUTPUT_BUCKET}. ```. ```bash. ( time seq 0 $((N_SHARDS-1)) | \. parallel --halt 2 --line-buffer \. sudo docker run \. -v /home/${USER}:/home/${USER} \. ${DOCKER_IMAGE} \. make_examples \. --mode training \. --ref ""${REF}"" \. --reads ""${BAM_CHR21}"" \. --examples ""${OUTPUT_DIR}/validation_set.with_label.tfrecord@${N_SHARDS}.gz"" \. --truth_variants ""${TRUTH_VCF}"" \. --confident_regions ""${TRUTH_BED}"" \. --task {} \. --regions ""'chr21'"" \. --channels ""insert_size"" \. ) 2>&1 | tee ""${LOG_DIR}/validation_set.with_label.make_examples.log"". ```. This took `5m25.905s`. ```bash. gsutil -m cp ${OUTPUT_DIR}/validation_set.with_label.tfrecord-?????-of-00016.gz* \. ${OUTPUT_BUCKET}. ```. # This parts starts shuffling... ```bash. sudo apt install -y python3.8-venv. # Create a virtualenv. python3 -m venv beam. # Activate the virtualenv. . beam/bin/activate. ```. ```bash. mkdir -p ${SHUFFLE_SCRIPT_DIR}. wget https://raw.githubusercontent.com/google/deepvariant/r1.6.1/tools/shuffle_tfrecords_beam.py -O ${SHUFFLE_SCRIPT_DIR}/shuffle_tfrecords_beam.py. ```. ```bash. sudo apt -y update && sudo apt -y install python3-pip. pip3 install --upgrade pip. pip3 install setuptools --upgrade. pip3 install apache_beam[gcp]==2.50.0 # 2.51.0 didn't work in my run. pip3 install tensorflow # For parsing tf.Example in shuffle_tfrecords_beam.py. ```. ```bash. time python3 ${SHUFFLE_SCRIPT_DIR}/shuffle_tfrecords_beam.py \. --project=""${YOUR_PROJECT}"" \. --input_pattern_list=""${OUTPUT_BUCKET}""/training_set.with_label.tfrecord-?????-of-00016.gz \. --output_pattern_prefix=""${OUTPUT_BUCKET}/training_set.with_label.shuffled"" \. --output_dataset_name=""HG001"" \. --output_dataset_config_pbtxt=""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt"" \. --job_n",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:4586,deployability,updat,update,4586,"t 2 --line-buffer \. sudo docker run \. -v /home/${USER}:/home/${USER} \. ${DOCKER_IMAGE} \. make_examples \. --mode training \. --ref ""${REF}"" \. --reads ""${BAM_CHR21}"" \. --examples ""${OUTPUT_DIR}/validation_set.with_label.tfrecord@${N_SHARDS}.gz"" \. --truth_variants ""${TRUTH_VCF}"" \. --confident_regions ""${TRUTH_BED}"" \. --task {} \. --regions ""'chr21'"" \. --channels ""insert_size"" \. ) 2>&1 | tee ""${LOG_DIR}/validation_set.with_label.make_examples.log"". ```. This took `5m25.905s`. ```bash. gsutil -m cp ${OUTPUT_DIR}/validation_set.with_label.tfrecord-?????-of-00016.gz* \. ${OUTPUT_BUCKET}. ```. # This parts starts shuffling... ```bash. sudo apt install -y python3.8-venv. # Create a virtualenv. python3 -m venv beam. # Activate the virtualenv. . beam/bin/activate. ```. ```bash. mkdir -p ${SHUFFLE_SCRIPT_DIR}. wget https://raw.githubusercontent.com/google/deepvariant/r1.6.1/tools/shuffle_tfrecords_beam.py -O ${SHUFFLE_SCRIPT_DIR}/shuffle_tfrecords_beam.py. ```. ```bash. sudo apt -y update && sudo apt -y install python3-pip. pip3 install --upgrade pip. pip3 install setuptools --upgrade. pip3 install apache_beam[gcp]==2.50.0 # 2.51.0 didn't work in my run. pip3 install tensorflow # For parsing tf.Example in shuffle_tfrecords_beam.py. ```. ```bash. time python3 ${SHUFFLE_SCRIPT_DIR}/shuffle_tfrecords_beam.py \. --project=""${YOUR_PROJECT}"" \. --input_pattern_list=""${OUTPUT_BUCKET}""/training_set.with_label.tfrecord-?????-of-00016.gz \. --output_pattern_prefix=""${OUTPUT_BUCKET}/training_set.with_label.shuffled"" \. --output_dataset_name=""HG001"" \. --output_dataset_config_pbtxt=""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt"" \. --job_name=shuffle-tfrecords \. --runner=DataflowRunner \. --staging_location=""${OUTPUT_BUCKET}/staging"" \. --temp_location=""${OUTPUT_BUCKET}/tempdir"" \. --save_main_session \. --region us-east1. ```. This worked for me. It took `15m32.746s`. I didn't get the numpy issue. So I decided to run `pip3 freeze` to get my current versions so you can co",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:4608,deployability,instal,install,4608,"do docker run \. -v /home/${USER}:/home/${USER} \. ${DOCKER_IMAGE} \. make_examples \. --mode training \. --ref ""${REF}"" \. --reads ""${BAM_CHR21}"" \. --examples ""${OUTPUT_DIR}/validation_set.with_label.tfrecord@${N_SHARDS}.gz"" \. --truth_variants ""${TRUTH_VCF}"" \. --confident_regions ""${TRUTH_BED}"" \. --task {} \. --regions ""'chr21'"" \. --channels ""insert_size"" \. ) 2>&1 | tee ""${LOG_DIR}/validation_set.with_label.make_examples.log"". ```. This took `5m25.905s`. ```bash. gsutil -m cp ${OUTPUT_DIR}/validation_set.with_label.tfrecord-?????-of-00016.gz* \. ${OUTPUT_BUCKET}. ```. # This parts starts shuffling... ```bash. sudo apt install -y python3.8-venv. # Create a virtualenv. python3 -m venv beam. # Activate the virtualenv. . beam/bin/activate. ```. ```bash. mkdir -p ${SHUFFLE_SCRIPT_DIR}. wget https://raw.githubusercontent.com/google/deepvariant/r1.6.1/tools/shuffle_tfrecords_beam.py -O ${SHUFFLE_SCRIPT_DIR}/shuffle_tfrecords_beam.py. ```. ```bash. sudo apt -y update && sudo apt -y install python3-pip. pip3 install --upgrade pip. pip3 install setuptools --upgrade. pip3 install apache_beam[gcp]==2.50.0 # 2.51.0 didn't work in my run. pip3 install tensorflow # For parsing tf.Example in shuffle_tfrecords_beam.py. ```. ```bash. time python3 ${SHUFFLE_SCRIPT_DIR}/shuffle_tfrecords_beam.py \. --project=""${YOUR_PROJECT}"" \. --input_pattern_list=""${OUTPUT_BUCKET}""/training_set.with_label.tfrecord-?????-of-00016.gz \. --output_pattern_prefix=""${OUTPUT_BUCKET}/training_set.with_label.shuffled"" \. --output_dataset_name=""HG001"" \. --output_dataset_config_pbtxt=""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt"" \. --job_name=shuffle-tfrecords \. --runner=DataflowRunner \. --staging_location=""${OUTPUT_BUCKET}/staging"" \. --temp_location=""${OUTPUT_BUCKET}/tempdir"" \. --save_main_session \. --region us-east1. ```. This worked for me. It took `15m32.746s`. I didn't get the numpy issue. So I decided to run `pip3 freeze` to get my current versions so you can compare:. ```bash. $ pip3",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:4634,deployability,instal,install,4634,"${USER}:/home/${USER} \. ${DOCKER_IMAGE} \. make_examples \. --mode training \. --ref ""${REF}"" \. --reads ""${BAM_CHR21}"" \. --examples ""${OUTPUT_DIR}/validation_set.with_label.tfrecord@${N_SHARDS}.gz"" \. --truth_variants ""${TRUTH_VCF}"" \. --confident_regions ""${TRUTH_BED}"" \. --task {} \. --regions ""'chr21'"" \. --channels ""insert_size"" \. ) 2>&1 | tee ""${LOG_DIR}/validation_set.with_label.make_examples.log"". ```. This took `5m25.905s`. ```bash. gsutil -m cp ${OUTPUT_DIR}/validation_set.with_label.tfrecord-?????-of-00016.gz* \. ${OUTPUT_BUCKET}. ```. # This parts starts shuffling... ```bash. sudo apt install -y python3.8-venv. # Create a virtualenv. python3 -m venv beam. # Activate the virtualenv. . beam/bin/activate. ```. ```bash. mkdir -p ${SHUFFLE_SCRIPT_DIR}. wget https://raw.githubusercontent.com/google/deepvariant/r1.6.1/tools/shuffle_tfrecords_beam.py -O ${SHUFFLE_SCRIPT_DIR}/shuffle_tfrecords_beam.py. ```. ```bash. sudo apt -y update && sudo apt -y install python3-pip. pip3 install --upgrade pip. pip3 install setuptools --upgrade. pip3 install apache_beam[gcp]==2.50.0 # 2.51.0 didn't work in my run. pip3 install tensorflow # For parsing tf.Example in shuffle_tfrecords_beam.py. ```. ```bash. time python3 ${SHUFFLE_SCRIPT_DIR}/shuffle_tfrecords_beam.py \. --project=""${YOUR_PROJECT}"" \. --input_pattern_list=""${OUTPUT_BUCKET}""/training_set.with_label.tfrecord-?????-of-00016.gz \. --output_pattern_prefix=""${OUTPUT_BUCKET}/training_set.with_label.shuffled"" \. --output_dataset_name=""HG001"" \. --output_dataset_config_pbtxt=""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt"" \. --job_name=shuffle-tfrecords \. --runner=DataflowRunner \. --staging_location=""${OUTPUT_BUCKET}/staging"" \. --temp_location=""${OUTPUT_BUCKET}/tempdir"" \. --save_main_session \. --region us-east1. ```. This worked for me. It took `15m32.746s`. I didn't get the numpy issue. So I decided to run `pip3 freeze` to get my current versions so you can compare:. ```bash. $ pip3 freeze. absl-py==2.1.0. a",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:4644,deployability,upgrad,upgrade,4644,"ome/${USER} \. ${DOCKER_IMAGE} \. make_examples \. --mode training \. --ref ""${REF}"" \. --reads ""${BAM_CHR21}"" \. --examples ""${OUTPUT_DIR}/validation_set.with_label.tfrecord@${N_SHARDS}.gz"" \. --truth_variants ""${TRUTH_VCF}"" \. --confident_regions ""${TRUTH_BED}"" \. --task {} \. --regions ""'chr21'"" \. --channels ""insert_size"" \. ) 2>&1 | tee ""${LOG_DIR}/validation_set.with_label.make_examples.log"". ```. This took `5m25.905s`. ```bash. gsutil -m cp ${OUTPUT_DIR}/validation_set.with_label.tfrecord-?????-of-00016.gz* \. ${OUTPUT_BUCKET}. ```. # This parts starts shuffling... ```bash. sudo apt install -y python3.8-venv. # Create a virtualenv. python3 -m venv beam. # Activate the virtualenv. . beam/bin/activate. ```. ```bash. mkdir -p ${SHUFFLE_SCRIPT_DIR}. wget https://raw.githubusercontent.com/google/deepvariant/r1.6.1/tools/shuffle_tfrecords_beam.py -O ${SHUFFLE_SCRIPT_DIR}/shuffle_tfrecords_beam.py. ```. ```bash. sudo apt -y update && sudo apt -y install python3-pip. pip3 install --upgrade pip. pip3 install setuptools --upgrade. pip3 install apache_beam[gcp]==2.50.0 # 2.51.0 didn't work in my run. pip3 install tensorflow # For parsing tf.Example in shuffle_tfrecords_beam.py. ```. ```bash. time python3 ${SHUFFLE_SCRIPT_DIR}/shuffle_tfrecords_beam.py \. --project=""${YOUR_PROJECT}"" \. --input_pattern_list=""${OUTPUT_BUCKET}""/training_set.with_label.tfrecord-?????-of-00016.gz \. --output_pattern_prefix=""${OUTPUT_BUCKET}/training_set.with_label.shuffled"" \. --output_dataset_name=""HG001"" \. --output_dataset_config_pbtxt=""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt"" \. --job_name=shuffle-tfrecords \. --runner=DataflowRunner \. --staging_location=""${OUTPUT_BUCKET}/staging"" \. --temp_location=""${OUTPUT_BUCKET}/tempdir"" \. --save_main_session \. --region us-east1. ```. This worked for me. It took `15m32.746s`. I didn't get the numpy issue. So I decided to run `pip3 freeze` to get my current versions so you can compare:. ```bash. $ pip3 freeze. absl-py==2.1.0. apache-beam",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:4662,deployability,instal,install,4662,"OCKER_IMAGE} \. make_examples \. --mode training \. --ref ""${REF}"" \. --reads ""${BAM_CHR21}"" \. --examples ""${OUTPUT_DIR}/validation_set.with_label.tfrecord@${N_SHARDS}.gz"" \. --truth_variants ""${TRUTH_VCF}"" \. --confident_regions ""${TRUTH_BED}"" \. --task {} \. --regions ""'chr21'"" \. --channels ""insert_size"" \. ) 2>&1 | tee ""${LOG_DIR}/validation_set.with_label.make_examples.log"". ```. This took `5m25.905s`. ```bash. gsutil -m cp ${OUTPUT_DIR}/validation_set.with_label.tfrecord-?????-of-00016.gz* \. ${OUTPUT_BUCKET}. ```. # This parts starts shuffling... ```bash. sudo apt install -y python3.8-venv. # Create a virtualenv. python3 -m venv beam. # Activate the virtualenv. . beam/bin/activate. ```. ```bash. mkdir -p ${SHUFFLE_SCRIPT_DIR}. wget https://raw.githubusercontent.com/google/deepvariant/r1.6.1/tools/shuffle_tfrecords_beam.py -O ${SHUFFLE_SCRIPT_DIR}/shuffle_tfrecords_beam.py. ```. ```bash. sudo apt -y update && sudo apt -y install python3-pip. pip3 install --upgrade pip. pip3 install setuptools --upgrade. pip3 install apache_beam[gcp]==2.50.0 # 2.51.0 didn't work in my run. pip3 install tensorflow # For parsing tf.Example in shuffle_tfrecords_beam.py. ```. ```bash. time python3 ${SHUFFLE_SCRIPT_DIR}/shuffle_tfrecords_beam.py \. --project=""${YOUR_PROJECT}"" \. --input_pattern_list=""${OUTPUT_BUCKET}""/training_set.with_label.tfrecord-?????-of-00016.gz \. --output_pattern_prefix=""${OUTPUT_BUCKET}/training_set.with_label.shuffled"" \. --output_dataset_name=""HG001"" \. --output_dataset_config_pbtxt=""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt"" \. --job_name=shuffle-tfrecords \. --runner=DataflowRunner \. --staging_location=""${OUTPUT_BUCKET}/staging"" \. --temp_location=""${OUTPUT_BUCKET}/tempdir"" \. --save_main_session \. --region us-east1. ```. This worked for me. It took `15m32.746s`. I didn't get the numpy issue. So I decided to run `pip3 freeze` to get my current versions so you can compare:. ```bash. $ pip3 freeze. absl-py==2.1.0. apache-beam==2.50.0. astunpar",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:4683,deployability,upgrad,upgrade,4683,"examples \. --mode training \. --ref ""${REF}"" \. --reads ""${BAM_CHR21}"" \. --examples ""${OUTPUT_DIR}/validation_set.with_label.tfrecord@${N_SHARDS}.gz"" \. --truth_variants ""${TRUTH_VCF}"" \. --confident_regions ""${TRUTH_BED}"" \. --task {} \. --regions ""'chr21'"" \. --channels ""insert_size"" \. ) 2>&1 | tee ""${LOG_DIR}/validation_set.with_label.make_examples.log"". ```. This took `5m25.905s`. ```bash. gsutil -m cp ${OUTPUT_DIR}/validation_set.with_label.tfrecord-?????-of-00016.gz* \. ${OUTPUT_BUCKET}. ```. # This parts starts shuffling... ```bash. sudo apt install -y python3.8-venv. # Create a virtualenv. python3 -m venv beam. # Activate the virtualenv. . beam/bin/activate. ```. ```bash. mkdir -p ${SHUFFLE_SCRIPT_DIR}. wget https://raw.githubusercontent.com/google/deepvariant/r1.6.1/tools/shuffle_tfrecords_beam.py -O ${SHUFFLE_SCRIPT_DIR}/shuffle_tfrecords_beam.py. ```. ```bash. sudo apt -y update && sudo apt -y install python3-pip. pip3 install --upgrade pip. pip3 install setuptools --upgrade. pip3 install apache_beam[gcp]==2.50.0 # 2.51.0 didn't work in my run. pip3 install tensorflow # For parsing tf.Example in shuffle_tfrecords_beam.py. ```. ```bash. time python3 ${SHUFFLE_SCRIPT_DIR}/shuffle_tfrecords_beam.py \. --project=""${YOUR_PROJECT}"" \. --input_pattern_list=""${OUTPUT_BUCKET}""/training_set.with_label.tfrecord-?????-of-00016.gz \. --output_pattern_prefix=""${OUTPUT_BUCKET}/training_set.with_label.shuffled"" \. --output_dataset_name=""HG001"" \. --output_dataset_config_pbtxt=""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt"" \. --job_name=shuffle-tfrecords \. --runner=DataflowRunner \. --staging_location=""${OUTPUT_BUCKET}/staging"" \. --temp_location=""${OUTPUT_BUCKET}/tempdir"" \. --save_main_session \. --region us-east1. ```. This worked for me. It took `15m32.746s`. I didn't get the numpy issue. So I decided to run `pip3 freeze` to get my current versions so you can compare:. ```bash. $ pip3 freeze. absl-py==2.1.0. apache-beam==2.50.0. astunparse==1.6.3. cachetools",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:4697,deployability,instal,install,4697,"mode training \. --ref ""${REF}"" \. --reads ""${BAM_CHR21}"" \. --examples ""${OUTPUT_DIR}/validation_set.with_label.tfrecord@${N_SHARDS}.gz"" \. --truth_variants ""${TRUTH_VCF}"" \. --confident_regions ""${TRUTH_BED}"" \. --task {} \. --regions ""'chr21'"" \. --channels ""insert_size"" \. ) 2>&1 | tee ""${LOG_DIR}/validation_set.with_label.make_examples.log"". ```. This took `5m25.905s`. ```bash. gsutil -m cp ${OUTPUT_DIR}/validation_set.with_label.tfrecord-?????-of-00016.gz* \. ${OUTPUT_BUCKET}. ```. # This parts starts shuffling... ```bash. sudo apt install -y python3.8-venv. # Create a virtualenv. python3 -m venv beam. # Activate the virtualenv. . beam/bin/activate. ```. ```bash. mkdir -p ${SHUFFLE_SCRIPT_DIR}. wget https://raw.githubusercontent.com/google/deepvariant/r1.6.1/tools/shuffle_tfrecords_beam.py -O ${SHUFFLE_SCRIPT_DIR}/shuffle_tfrecords_beam.py. ```. ```bash. sudo apt -y update && sudo apt -y install python3-pip. pip3 install --upgrade pip. pip3 install setuptools --upgrade. pip3 install apache_beam[gcp]==2.50.0 # 2.51.0 didn't work in my run. pip3 install tensorflow # For parsing tf.Example in shuffle_tfrecords_beam.py. ```. ```bash. time python3 ${SHUFFLE_SCRIPT_DIR}/shuffle_tfrecords_beam.py \. --project=""${YOUR_PROJECT}"" \. --input_pattern_list=""${OUTPUT_BUCKET}""/training_set.with_label.tfrecord-?????-of-00016.gz \. --output_pattern_prefix=""${OUTPUT_BUCKET}/training_set.with_label.shuffled"" \. --output_dataset_name=""HG001"" \. --output_dataset_config_pbtxt=""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt"" \. --job_name=shuffle-tfrecords \. --runner=DataflowRunner \. --staging_location=""${OUTPUT_BUCKET}/staging"" \. --temp_location=""${OUTPUT_BUCKET}/tempdir"" \. --save_main_session \. --region us-east1. ```. This worked for me. It took `15m32.746s`. I didn't get the numpy issue. So I decided to run `pip3 freeze` to get my current versions so you can compare:. ```bash. $ pip3 freeze. absl-py==2.1.0. apache-beam==2.50.0. astunparse==1.6.3. cachetools==5.3.3. certi",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:4767,deployability,instal,install,4767,"s ""${OUTPUT_DIR}/validation_set.with_label.tfrecord@${N_SHARDS}.gz"" \. --truth_variants ""${TRUTH_VCF}"" \. --confident_regions ""${TRUTH_BED}"" \. --task {} \. --regions ""'chr21'"" \. --channels ""insert_size"" \. ) 2>&1 | tee ""${LOG_DIR}/validation_set.with_label.make_examples.log"". ```. This took `5m25.905s`. ```bash. gsutil -m cp ${OUTPUT_DIR}/validation_set.with_label.tfrecord-?????-of-00016.gz* \. ${OUTPUT_BUCKET}. ```. # This parts starts shuffling... ```bash. sudo apt install -y python3.8-venv. # Create a virtualenv. python3 -m venv beam. # Activate the virtualenv. . beam/bin/activate. ```. ```bash. mkdir -p ${SHUFFLE_SCRIPT_DIR}. wget https://raw.githubusercontent.com/google/deepvariant/r1.6.1/tools/shuffle_tfrecords_beam.py -O ${SHUFFLE_SCRIPT_DIR}/shuffle_tfrecords_beam.py. ```. ```bash. sudo apt -y update && sudo apt -y install python3-pip. pip3 install --upgrade pip. pip3 install setuptools --upgrade. pip3 install apache_beam[gcp]==2.50.0 # 2.51.0 didn't work in my run. pip3 install tensorflow # For parsing tf.Example in shuffle_tfrecords_beam.py. ```. ```bash. time python3 ${SHUFFLE_SCRIPT_DIR}/shuffle_tfrecords_beam.py \. --project=""${YOUR_PROJECT}"" \. --input_pattern_list=""${OUTPUT_BUCKET}""/training_set.with_label.tfrecord-?????-of-00016.gz \. --output_pattern_prefix=""${OUTPUT_BUCKET}/training_set.with_label.shuffled"" \. --output_dataset_name=""HG001"" \. --output_dataset_config_pbtxt=""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt"" \. --job_name=shuffle-tfrecords \. --runner=DataflowRunner \. --staging_location=""${OUTPUT_BUCKET}/staging"" \. --temp_location=""${OUTPUT_BUCKET}/tempdir"" \. --save_main_session \. --region us-east1. ```. This worked for me. It took `15m32.746s`. I didn't get the numpy issue. So I decided to run `pip3 freeze` to get my current versions so you can compare:. ```bash. $ pip3 freeze. absl-py==2.1.0. apache-beam==2.50.0. astunparse==1.6.3. cachetools==5.3.3. certifi==2024.2.2. charset-normalizer==3.3.2. cloudpickle==2.2.1. crcmod==1",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:5567,deployability,version,versions,5567,"h. sudo apt -y update && sudo apt -y install python3-pip. pip3 install --upgrade pip. pip3 install setuptools --upgrade. pip3 install apache_beam[gcp]==2.50.0 # 2.51.0 didn't work in my run. pip3 install tensorflow # For parsing tf.Example in shuffle_tfrecords_beam.py. ```. ```bash. time python3 ${SHUFFLE_SCRIPT_DIR}/shuffle_tfrecords_beam.py \. --project=""${YOUR_PROJECT}"" \. --input_pattern_list=""${OUTPUT_BUCKET}""/training_set.with_label.tfrecord-?????-of-00016.gz \. --output_pattern_prefix=""${OUTPUT_BUCKET}/training_set.with_label.shuffled"" \. --output_dataset_name=""HG001"" \. --output_dataset_config_pbtxt=""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt"" \. --job_name=shuffle-tfrecords \. --runner=DataflowRunner \. --staging_location=""${OUTPUT_BUCKET}/staging"" \. --temp_location=""${OUTPUT_BUCKET}/tempdir"" \. --save_main_session \. --region us-east1. ```. This worked for me. It took `15m32.746s`. I didn't get the numpy issue. So I decided to run `pip3 freeze` to get my current versions so you can compare:. ```bash. $ pip3 freeze. absl-py==2.1.0. apache-beam==2.50.0. astunparse==1.6.3. cachetools==5.3.3. certifi==2024.2.2. charset-normalizer==3.3.2. cloudpickle==2.2.1. crcmod==1.7. Deprecated==1.2.14. dill==0.3.1.1. dnspython==2.6.1. docopt==0.6.2. fastavro==1.9.4. fasteners==0.19. flatbuffers==24.3.7. gast==0.4.0. google-api-core==2.17.1. google-apitools==0.5.31. google-auth==2.28.2. google-auth-httplib2==0.1.1. google-auth-oauthlib==1.0.0. google-cloud-aiplatform==1.44.0. google-cloud-bigquery==3.19.0. google-cloud-bigquery-storage==2.24.0. google-cloud-bigtable==2.23.0. google-cloud-core==2.4.1. google-cloud-datastore==2.19.0. google-cloud-dlp==3.16.0. google-cloud-language==2.13.3. google-cloud-pubsub==2.20.2. google-cloud-pubsublite==1.9.0. google-cloud-recommendations-ai==0.10.10. google-cloud-resource-manager==1.12.3. google-cloud-spanner==3.44.0. google-cloud-storage==2.16.0. google-cloud-videointelligence==2.13.3. google-cloud-vision==3.7.2. google-crc32",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:5918,deployability,api,api-core,5918,"roject=""${YOUR_PROJECT}"" \. --input_pattern_list=""${OUTPUT_BUCKET}""/training_set.with_label.tfrecord-?????-of-00016.gz \. --output_pattern_prefix=""${OUTPUT_BUCKET}/training_set.with_label.shuffled"" \. --output_dataset_name=""HG001"" \. --output_dataset_config_pbtxt=""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt"" \. --job_name=shuffle-tfrecords \. --runner=DataflowRunner \. --staging_location=""${OUTPUT_BUCKET}/staging"" \. --temp_location=""${OUTPUT_BUCKET}/tempdir"" \. --save_main_session \. --region us-east1. ```. This worked for me. It took `15m32.746s`. I didn't get the numpy issue. So I decided to run `pip3 freeze` to get my current versions so you can compare:. ```bash. $ pip3 freeze. absl-py==2.1.0. apache-beam==2.50.0. astunparse==1.6.3. cachetools==5.3.3. certifi==2024.2.2. charset-normalizer==3.3.2. cloudpickle==2.2.1. crcmod==1.7. Deprecated==1.2.14. dill==0.3.1.1. dnspython==2.6.1. docopt==0.6.2. fastavro==1.9.4. fasteners==0.19. flatbuffers==24.3.7. gast==0.4.0. google-api-core==2.17.1. google-apitools==0.5.31. google-auth==2.28.2. google-auth-httplib2==0.1.1. google-auth-oauthlib==1.0.0. google-cloud-aiplatform==1.44.0. google-cloud-bigquery==3.19.0. google-cloud-bigquery-storage==2.24.0. google-cloud-bigtable==2.23.0. google-cloud-core==2.4.1. google-cloud-datastore==2.19.0. google-cloud-dlp==3.16.0. google-cloud-language==2.13.3. google-cloud-pubsub==2.20.2. google-cloud-pubsublite==1.9.0. google-cloud-recommendations-ai==0.10.10. google-cloud-resource-manager==1.12.3. google-cloud-spanner==3.44.0. google-cloud-storage==2.16.0. google-cloud-videointelligence==2.13.3. google-cloud-vision==3.7.2. google-crc32c==1.5.0. google-pasta==0.2.0. google-resumable-media==2.7.0. googleapis-common-protos==1.63.0. grpc-google-iam-v1==0.13.0. grpc-interceptor==0.15.4. grpcio==1.62.1. grpcio-status==1.62.1. h5py==3.10.0. hdfs==2.7.3. httplib2==0.22.0. idna==3.6. importlib_metadata==7.0.2. keras==2.13.1. libclang==18.1.1. Markdown==3.6. MarkupSafe==2.1.5. numpy==1.24.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:5943,deployability,api,apitools,5943,"\. --input_pattern_list=""${OUTPUT_BUCKET}""/training_set.with_label.tfrecord-?????-of-00016.gz \. --output_pattern_prefix=""${OUTPUT_BUCKET}/training_set.with_label.shuffled"" \. --output_dataset_name=""HG001"" \. --output_dataset_config_pbtxt=""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt"" \. --job_name=shuffle-tfrecords \. --runner=DataflowRunner \. --staging_location=""${OUTPUT_BUCKET}/staging"" \. --temp_location=""${OUTPUT_BUCKET}/tempdir"" \. --save_main_session \. --region us-east1. ```. This worked for me. It took `15m32.746s`. I didn't get the numpy issue. So I decided to run `pip3 freeze` to get my current versions so you can compare:. ```bash. $ pip3 freeze. absl-py==2.1.0. apache-beam==2.50.0. astunparse==1.6.3. cachetools==5.3.3. certifi==2024.2.2. charset-normalizer==3.3.2. cloudpickle==2.2.1. crcmod==1.7. Deprecated==1.2.14. dill==0.3.1.1. dnspython==2.6.1. docopt==0.6.2. fastavro==1.9.4. fasteners==0.19. flatbuffers==24.3.7. gast==0.4.0. google-api-core==2.17.1. google-apitools==0.5.31. google-auth==2.28.2. google-auth-httplib2==0.1.1. google-auth-oauthlib==1.0.0. google-cloud-aiplatform==1.44.0. google-cloud-bigquery==3.19.0. google-cloud-bigquery-storage==2.24.0. google-cloud-bigtable==2.23.0. google-cloud-core==2.4.1. google-cloud-datastore==2.19.0. google-cloud-dlp==3.16.0. google-cloud-language==2.13.3. google-cloud-pubsub==2.20.2. google-cloud-pubsublite==1.9.0. google-cloud-recommendations-ai==0.10.10. google-cloud-resource-manager==1.12.3. google-cloud-spanner==3.44.0. google-cloud-storage==2.16.0. google-cloud-videointelligence==2.13.3. google-cloud-vision==3.7.2. google-crc32c==1.5.0. google-pasta==0.2.0. google-resumable-media==2.7.0. googleapis-common-protos==1.63.0. grpc-google-iam-v1==0.13.0. grpc-interceptor==0.15.4. grpcio==1.62.1. grpcio-status==1.62.1. h5py==3.10.0. hdfs==2.7.3. httplib2==0.22.0. idna==3.6. importlib_metadata==7.0.2. keras==2.13.1. libclang==18.1.1. Markdown==3.6. MarkupSafe==2.1.5. numpy==1.24.3. oauth2client==4.1.3. o",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:6405,deployability,resourc,resource-manager,6405,on \. --region us-east1. ```. This worked for me. It took `15m32.746s`. I didn't get the numpy issue. So I decided to run `pip3 freeze` to get my current versions so you can compare:. ```bash. $ pip3 freeze. absl-py==2.1.0. apache-beam==2.50.0. astunparse==1.6.3. cachetools==5.3.3. certifi==2024.2.2. charset-normalizer==3.3.2. cloudpickle==2.2.1. crcmod==1.7. Deprecated==1.2.14. dill==0.3.1.1. dnspython==2.6.1. docopt==0.6.2. fastavro==1.9.4. fasteners==0.19. flatbuffers==24.3.7. gast==0.4.0. google-api-core==2.17.1. google-apitools==0.5.31. google-auth==2.28.2. google-auth-httplib2==0.1.1. google-auth-oauthlib==1.0.0. google-cloud-aiplatform==1.44.0. google-cloud-bigquery==3.19.0. google-cloud-bigquery-storage==2.24.0. google-cloud-bigtable==2.23.0. google-cloud-core==2.4.1. google-cloud-datastore==2.19.0. google-cloud-dlp==3.16.0. google-cloud-language==2.13.3. google-cloud-pubsub==2.20.2. google-cloud-pubsublite==1.9.0. google-cloud-recommendations-ai==0.10.10. google-cloud-resource-manager==1.12.3. google-cloud-spanner==3.44.0. google-cloud-storage==2.16.0. google-cloud-videointelligence==2.13.3. google-cloud-vision==3.7.2. google-crc32c==1.5.0. google-pasta==0.2.0. google-resumable-media==2.7.0. googleapis-common-protos==1.63.0. grpc-google-iam-v1==0.13.0. grpc-interceptor==0.15.4. grpcio==1.62.1. grpcio-status==1.62.1. h5py==3.10.0. hdfs==2.7.3. httplib2==0.22.0. idna==3.6. importlib_metadata==7.0.2. keras==2.13.1. libclang==18.1.1. Markdown==3.6. MarkupSafe==2.1.5. numpy==1.24.3. oauth2client==4.1.3. oauthlib==3.2.2. objsize==0.6.1. opt-einsum==3.3.0. orjson==3.9.15. overrides==7.7.0. packaging==24.0. pkg_resources==0.0.0. proto-plus==1.23.0. protobuf==4.23.4. pyarrow==11.0.0. pyasn1==0.5.1. pyasn1-modules==0.3.0. pydot==1.4.2. pymongo==4.6.2. pyparsing==3.1.2. python-dateutil==2.9.0.post0. pytz==2024.1. regex==2023.12.25. requests==2.31.0. requests-oauthlib==1.4.0. rsa==4.9. shapely==2.0.3. six==1.16.0. sqlparse==0.4.4. tensorboard==2.13.0. tensorboard-data-,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:7148,deployability,modul,modules,7148,"rmalizer==3.3.2. cloudpickle==2.2.1. crcmod==1.7. Deprecated==1.2.14. dill==0.3.1.1. dnspython==2.6.1. docopt==0.6.2. fastavro==1.9.4. fasteners==0.19. flatbuffers==24.3.7. gast==0.4.0. google-api-core==2.17.1. google-apitools==0.5.31. google-auth==2.28.2. google-auth-httplib2==0.1.1. google-auth-oauthlib==1.0.0. google-cloud-aiplatform==1.44.0. google-cloud-bigquery==3.19.0. google-cloud-bigquery-storage==2.24.0. google-cloud-bigtable==2.23.0. google-cloud-core==2.4.1. google-cloud-datastore==2.19.0. google-cloud-dlp==3.16.0. google-cloud-language==2.13.3. google-cloud-pubsub==2.20.2. google-cloud-pubsublite==1.9.0. google-cloud-recommendations-ai==0.10.10. google-cloud-resource-manager==1.12.3. google-cloud-spanner==3.44.0. google-cloud-storage==2.16.0. google-cloud-videointelligence==2.13.3. google-cloud-vision==3.7.2. google-crc32c==1.5.0. google-pasta==0.2.0. google-resumable-media==2.7.0. googleapis-common-protos==1.63.0. grpc-google-iam-v1==0.13.0. grpc-interceptor==0.15.4. grpcio==1.62.1. grpcio-status==1.62.1. h5py==3.10.0. hdfs==2.7.3. httplib2==0.22.0. idna==3.6. importlib_metadata==7.0.2. keras==2.13.1. libclang==18.1.1. Markdown==3.6. MarkupSafe==2.1.5. numpy==1.24.3. oauth2client==4.1.3. oauthlib==3.2.2. objsize==0.6.1. opt-einsum==3.3.0. orjson==3.9.15. overrides==7.7.0. packaging==24.0. pkg_resources==0.0.0. proto-plus==1.23.0. protobuf==4.23.4. pyarrow==11.0.0. pyasn1==0.5.1. pyasn1-modules==0.3.0. pydot==1.4.2. pymongo==4.6.2. pyparsing==3.1.2. python-dateutil==2.9.0.post0. pytz==2024.1. regex==2023.12.25. requests==2.31.0. requests-oauthlib==1.4.0. rsa==4.9. shapely==2.0.3. six==1.16.0. sqlparse==0.4.4. tensorboard==2.13.0. tensorboard-data-server==0.7.2. tensorflow==2.13.1. tensorflow-estimator==2.13.0. tensorflow-io-gcs-filesystem==0.34.0. termcolor==2.4.0. typing_extensions==4.5.0. urllib3==2.2.1. Werkzeug==3.0.1. wrapt==1.16.0. zipp==3.18.1. zstandard==0.22.0. ```. ---. @helizabeth1103 , can you see which version might be different from yours?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:7687,deployability,version,version,7687,"rmalizer==3.3.2. cloudpickle==2.2.1. crcmod==1.7. Deprecated==1.2.14. dill==0.3.1.1. dnspython==2.6.1. docopt==0.6.2. fastavro==1.9.4. fasteners==0.19. flatbuffers==24.3.7. gast==0.4.0. google-api-core==2.17.1. google-apitools==0.5.31. google-auth==2.28.2. google-auth-httplib2==0.1.1. google-auth-oauthlib==1.0.0. google-cloud-aiplatform==1.44.0. google-cloud-bigquery==3.19.0. google-cloud-bigquery-storage==2.24.0. google-cloud-bigtable==2.23.0. google-cloud-core==2.4.1. google-cloud-datastore==2.19.0. google-cloud-dlp==3.16.0. google-cloud-language==2.13.3. google-cloud-pubsub==2.20.2. google-cloud-pubsublite==1.9.0. google-cloud-recommendations-ai==0.10.10. google-cloud-resource-manager==1.12.3. google-cloud-spanner==3.44.0. google-cloud-storage==2.16.0. google-cloud-videointelligence==2.13.3. google-cloud-vision==3.7.2. google-crc32c==1.5.0. google-pasta==0.2.0. google-resumable-media==2.7.0. googleapis-common-protos==1.63.0. grpc-google-iam-v1==0.13.0. grpc-interceptor==0.15.4. grpcio==1.62.1. grpcio-status==1.62.1. h5py==3.10.0. hdfs==2.7.3. httplib2==0.22.0. idna==3.6. importlib_metadata==7.0.2. keras==2.13.1. libclang==18.1.1. Markdown==3.6. MarkupSafe==2.1.5. numpy==1.24.3. oauth2client==4.1.3. oauthlib==3.2.2. objsize==0.6.1. opt-einsum==3.3.0. orjson==3.9.15. overrides==7.7.0. packaging==24.0. pkg_resources==0.0.0. proto-plus==1.23.0. protobuf==4.23.4. pyarrow==11.0.0. pyasn1==0.5.1. pyasn1-modules==0.3.0. pydot==1.4.2. pymongo==4.6.2. pyparsing==3.1.2. python-dateutil==2.9.0.post0. pytz==2024.1. regex==2023.12.25. requests==2.31.0. requests-oauthlib==1.4.0. rsa==4.9. shapely==2.0.3. six==1.16.0. sqlparse==0.4.4. tensorboard==2.13.0. tensorboard-data-server==0.7.2. tensorflow==2.13.1. tensorflow-estimator==2.13.0. tensorflow-io-gcs-filesystem==0.34.0. termcolor==2.4.0. typing_extensions==4.5.0. urllib3==2.2.1. Werkzeug==3.0.1. wrapt==1.16.0. zipp==3.18.1. zstandard==0.22.0. ```. ---. @helizabeth1103 , can you see which version might be different from yours?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:254,energy efficiency,cloud,cloud-platform,254,"# Follow https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-training-case-study.md. I got a machine:. ```bash. host=""${USER}-deepvariant-vm"". zone=""us-west1-b"". gcloud compute instances create ${host} \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --maintenance-policy ""TERMINATE"" \. --accelerator=type=nvidia-tesla-p100,count=1 \. --image-family ""ubuntu-2004-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""n1-standard-16"" \. --boot-disk-size ""300"" \. --zone ""${zone}"" \. --min-cpu-platform ""Intel Skylake"". ```. I sshed into my machine:. ```bash. gcloud compute ssh pichuan-deepvariant-vm --zone us-west1-b. ```. I ran this with my own `YOUR_PROJECT` and `OUTPUT_GCS_BUCKET` setting. Then the following is basically just copy/paste from the doc:. ```. BUCKET=""gs://deepvariant"". VERSION=""1.6.1"". DOCKER_IMAGE=""google/deepvariant:${VERSION}"". MODEL_BUCKET=""${BUCKET}/models/DeepVariant/${VERSION}/DeepVariant-inception_v3-${VERSION}+data-wgs_standard"". GCS_PRETRAINED_WGS_MODEL=""${MODEL_BUCKET}/model.ckpt"". OUTPUT_BUCKET=""${OUTPUT_GCS_BUCKET}/customized_training"". TRAINING_DIR=""${OUTPUT_BUCKET}/training_dir"". BASE=""${HOME}/training-case-study"". DATA_BUCKET=gs://deepvariant/training-case-study/BGISEQ-HG001. INPUT_DIR=""${BASE}/input"". BIN_DIR=""${INPUT_DIR}/bin"". DATA_DIR=""${INPUT_DIR}/data"". OUTPUT_DIR=""${BASE}/output"". LOG_DIR=""${OUTPUT_DIR}/logs"". SHUFFLE_SCRIPT_DIR=""${HOME}/deepvariant/tools"". REF=""${DATA_DIR}/ucsc_hg19.fa"". BAM_CHR1=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr1.bam"". BAM_CHR20=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr20.bam"". BAM_CHR21=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr21.bam"". TRUTH_VCF=""${DATA_DIR}/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_PGandRTGphasetransfer_chrs_FIXED.vcf.gz"". TRUTH_BED=""${DATA_DIR}/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_nosomaticdel_chr.bed"". N_SHARDS=16. ```. ```bash. mkdir -p ""${OUTPUT_DIR}"". mkd",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:420,energy efficiency,cloud,cloud,420,"# Follow https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-training-case-study.md. I got a machine:. ```bash. host=""${USER}-deepvariant-vm"". zone=""us-west1-b"". gcloud compute instances create ${host} \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --maintenance-policy ""TERMINATE"" \. --accelerator=type=nvidia-tesla-p100,count=1 \. --image-family ""ubuntu-2004-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""n1-standard-16"" \. --boot-disk-size ""300"" \. --zone ""${zone}"" \. --min-cpu-platform ""Intel Skylake"". ```. I sshed into my machine:. ```bash. gcloud compute ssh pichuan-deepvariant-vm --zone us-west1-b. ```. I ran this with my own `YOUR_PROJECT` and `OUTPUT_GCS_BUCKET` setting. Then the following is basically just copy/paste from the doc:. ```. BUCKET=""gs://deepvariant"". VERSION=""1.6.1"". DOCKER_IMAGE=""google/deepvariant:${VERSION}"". MODEL_BUCKET=""${BUCKET}/models/DeepVariant/${VERSION}/DeepVariant-inception_v3-${VERSION}+data-wgs_standard"". GCS_PRETRAINED_WGS_MODEL=""${MODEL_BUCKET}/model.ckpt"". OUTPUT_BUCKET=""${OUTPUT_GCS_BUCKET}/customized_training"". TRAINING_DIR=""${OUTPUT_BUCKET}/training_dir"". BASE=""${HOME}/training-case-study"". DATA_BUCKET=gs://deepvariant/training-case-study/BGISEQ-HG001. INPUT_DIR=""${BASE}/input"". BIN_DIR=""${INPUT_DIR}/bin"". DATA_DIR=""${INPUT_DIR}/data"". OUTPUT_DIR=""${BASE}/output"". LOG_DIR=""${OUTPUT_DIR}/logs"". SHUFFLE_SCRIPT_DIR=""${HOME}/deepvariant/tools"". REF=""${DATA_DIR}/ucsc_hg19.fa"". BAM_CHR1=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr1.bam"". BAM_CHR20=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr20.bam"". BAM_CHR21=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr21.bam"". TRUTH_VCF=""${DATA_DIR}/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_PGandRTGphasetransfer_chrs_FIXED.vcf.gz"". TRUTH_BED=""${DATA_DIR}/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_nosomaticdel_chr.bed"". N_SHARDS=16. ```. ```bash. mkdir -p ""${OUTPUT_DIR}"". mkd",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:517,energy efficiency,cpu,cpu-platform,517,"# Follow https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-training-case-study.md. I got a machine:. ```bash. host=""${USER}-deepvariant-vm"". zone=""us-west1-b"". gcloud compute instances create ${host} \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --maintenance-policy ""TERMINATE"" \. --accelerator=type=nvidia-tesla-p100,count=1 \. --image-family ""ubuntu-2004-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""n1-standard-16"" \. --boot-disk-size ""300"" \. --zone ""${zone}"" \. --min-cpu-platform ""Intel Skylake"". ```. I sshed into my machine:. ```bash. gcloud compute ssh pichuan-deepvariant-vm --zone us-west1-b. ```. I ran this with my own `YOUR_PROJECT` and `OUTPUT_GCS_BUCKET` setting. Then the following is basically just copy/paste from the doc:. ```. BUCKET=""gs://deepvariant"". VERSION=""1.6.1"". DOCKER_IMAGE=""google/deepvariant:${VERSION}"". MODEL_BUCKET=""${BUCKET}/models/DeepVariant/${VERSION}/DeepVariant-inception_v3-${VERSION}+data-wgs_standard"". GCS_PRETRAINED_WGS_MODEL=""${MODEL_BUCKET}/model.ckpt"". OUTPUT_BUCKET=""${OUTPUT_GCS_BUCKET}/customized_training"". TRAINING_DIR=""${OUTPUT_BUCKET}/training_dir"". BASE=""${HOME}/training-case-study"". DATA_BUCKET=gs://deepvariant/training-case-study/BGISEQ-HG001. INPUT_DIR=""${BASE}/input"". BIN_DIR=""${INPUT_DIR}/bin"". DATA_DIR=""${INPUT_DIR}/data"". OUTPUT_DIR=""${BASE}/output"". LOG_DIR=""${OUTPUT_DIR}/logs"". SHUFFLE_SCRIPT_DIR=""${HOME}/deepvariant/tools"". REF=""${DATA_DIR}/ucsc_hg19.fa"". BAM_CHR1=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr1.bam"". BAM_CHR20=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr20.bam"". BAM_CHR21=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr21.bam"". TRUTH_VCF=""${DATA_DIR}/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_PGandRTGphasetransfer_chrs_FIXED.vcf.gz"". TRUTH_BED=""${DATA_DIR}/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_nosomaticdel_chr.bed"". N_SHARDS=16. ```. ```bash. mkdir -p ""${OUTPUT_DIR}"". mkd",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:906,energy efficiency,model,models,906,"# Follow https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-training-case-study.md. I got a machine:. ```bash. host=""${USER}-deepvariant-vm"". zone=""us-west1-b"". gcloud compute instances create ${host} \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --maintenance-policy ""TERMINATE"" \. --accelerator=type=nvidia-tesla-p100,count=1 \. --image-family ""ubuntu-2004-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""n1-standard-16"" \. --boot-disk-size ""300"" \. --zone ""${zone}"" \. --min-cpu-platform ""Intel Skylake"". ```. I sshed into my machine:. ```bash. gcloud compute ssh pichuan-deepvariant-vm --zone us-west1-b. ```. I ran this with my own `YOUR_PROJECT` and `OUTPUT_GCS_BUCKET` setting. Then the following is basically just copy/paste from the doc:. ```. BUCKET=""gs://deepvariant"". VERSION=""1.6.1"". DOCKER_IMAGE=""google/deepvariant:${VERSION}"". MODEL_BUCKET=""${BUCKET}/models/DeepVariant/${VERSION}/DeepVariant-inception_v3-${VERSION}+data-wgs_standard"". GCS_PRETRAINED_WGS_MODEL=""${MODEL_BUCKET}/model.ckpt"". OUTPUT_BUCKET=""${OUTPUT_GCS_BUCKET}/customized_training"". TRAINING_DIR=""${OUTPUT_BUCKET}/training_dir"". BASE=""${HOME}/training-case-study"". DATA_BUCKET=gs://deepvariant/training-case-study/BGISEQ-HG001. INPUT_DIR=""${BASE}/input"". BIN_DIR=""${INPUT_DIR}/bin"". DATA_DIR=""${INPUT_DIR}/data"". OUTPUT_DIR=""${BASE}/output"". LOG_DIR=""${OUTPUT_DIR}/logs"". SHUFFLE_SCRIPT_DIR=""${HOME}/deepvariant/tools"". REF=""${DATA_DIR}/ucsc_hg19.fa"". BAM_CHR1=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr1.bam"". BAM_CHR20=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr20.bam"". BAM_CHR21=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr21.bam"". TRUTH_VCF=""${DATA_DIR}/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_PGandRTGphasetransfer_chrs_FIXED.vcf.gz"". TRUTH_BED=""${DATA_DIR}/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_nosomaticdel_chr.bed"". N_SHARDS=16. ```. ```bash. mkdir -p ""${OUTPUT_DIR}"". mkd",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:1034,energy efficiency,model,model,1034,"epvariant/blob/r1.6.1/docs/deepvariant-training-case-study.md. I got a machine:. ```bash. host=""${USER}-deepvariant-vm"". zone=""us-west1-b"". gcloud compute instances create ${host} \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --maintenance-policy ""TERMINATE"" \. --accelerator=type=nvidia-tesla-p100,count=1 \. --image-family ""ubuntu-2004-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""n1-standard-16"" \. --boot-disk-size ""300"" \. --zone ""${zone}"" \. --min-cpu-platform ""Intel Skylake"". ```. I sshed into my machine:. ```bash. gcloud compute ssh pichuan-deepvariant-vm --zone us-west1-b. ```. I ran this with my own `YOUR_PROJECT` and `OUTPUT_GCS_BUCKET` setting. Then the following is basically just copy/paste from the doc:. ```. BUCKET=""gs://deepvariant"". VERSION=""1.6.1"". DOCKER_IMAGE=""google/deepvariant:${VERSION}"". MODEL_BUCKET=""${BUCKET}/models/DeepVariant/${VERSION}/DeepVariant-inception_v3-${VERSION}+data-wgs_standard"". GCS_PRETRAINED_WGS_MODEL=""${MODEL_BUCKET}/model.ckpt"". OUTPUT_BUCKET=""${OUTPUT_GCS_BUCKET}/customized_training"". TRAINING_DIR=""${OUTPUT_BUCKET}/training_dir"". BASE=""${HOME}/training-case-study"". DATA_BUCKET=gs://deepvariant/training-case-study/BGISEQ-HG001. INPUT_DIR=""${BASE}/input"". BIN_DIR=""${INPUT_DIR}/bin"". DATA_DIR=""${INPUT_DIR}/data"". OUTPUT_DIR=""${BASE}/output"". LOG_DIR=""${OUTPUT_DIR}/logs"". SHUFFLE_SCRIPT_DIR=""${HOME}/deepvariant/tools"". REF=""${DATA_DIR}/ucsc_hg19.fa"". BAM_CHR1=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr1.bam"". BAM_CHR20=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr20.bam"". BAM_CHR21=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr21.bam"". TRUTH_VCF=""${DATA_DIR}/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_PGandRTGphasetransfer_chrs_FIXED.vcf.gz"". TRUTH_BED=""${DATA_DIR}/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_nosomaticdel_chr.bed"". N_SHARDS=16. ```. ```bash. mkdir -p ""${OUTPUT_DIR}"". mkdir -p ""${BIN_DIR}"". mkdir -p ""${DATA_",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:2618,energy efficiency,CPU,CPU,2618,"/BGISEQ_PE100_NA12878.sorted.chr21.bam"". TRUTH_VCF=""${DATA_DIR}/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_PGandRTGphasetransfer_chrs_FIXED.vcf.gz"". TRUTH_BED=""${DATA_DIR}/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_nosomaticdel_chr.bed"". N_SHARDS=16. ```. ```bash. mkdir -p ""${OUTPUT_DIR}"". mkdir -p ""${BIN_DIR}"". mkdir -p ""${DATA_DIR}"". mkdir -p ""${LOG_DIR}"". ```. ```bash. gsutil -m cp ${DATA_BUCKET}/BGISEQ_PE100_NA12878.sorted.chr*.bam* ""${DATA_DIR}"". gsutil -m cp -r ""${DATA_BUCKET}/ucsc_hg19.fa*"" ""${DATA_DIR}"". gsutil -m cp -r ""${DATA_BUCKET}/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_*"" ""${DATA_DIR}"". ```. ```bash. sudo apt -y update. sudo apt -y install parallel. curl -O https://raw.githubusercontent.com/google/deepvariant/r1.6.1/scripts/install_nvidia_docker.sh. bash -x install_nvidia_docker.sh. ```. ```bash. sudo docker pull ${DOCKER_IMAGE} # Standard CPU Docker Image. sudo docker pull ${DOCKER_IMAGE}-gpu # GPU-enabled Docker image. ```. ```bash. ( time seq 0 $((N_SHARDS-1)) | \. parallel --halt 2 --line-buffer \. sudo docker run \. -v ${HOME}:${HOME} \. ${DOCKER_IMAGE} \. make_examples \. --mode training \. --ref ""${REF}"" \. --reads ""${BAM_CHR1}"" \. --examples ""${OUTPUT_DIR}/training_set.with_label.tfrecord@${N_SHARDS}.gz"" \. --truth_variants ""${TRUTH_VCF}"" \. --confident_regions ""${TRUTH_BED}"" \. --task {} \. --regions ""'chr1'"" \. --channels ""insert_size"" \. ) 2>&1 | tee ""${LOG_DIR}/training_set.with_label.make_examples.log"". ```. This took `20m0.146s`. ```. $ cat ""${OUTPUT_DIR}/training_set.with_label.tfrecord-00000-of-00016.gz.example_info.json"". {""version"": ""1.6.0"", ""shape"": [100, 221, 7], ""channels"": [1, 2, 3, 4, 5, 6, 19]}. ```. ```bash. gsutil -m cp ${OUTPUT_DIR}/training_set.with_label.tfrecord-?????-of-00016.gz* \. ${OUTPUT_BUCKET}. ```. ```bash. ( time seq 0 $((N_SHARDS-1)) | \. parallel --halt 2 --line-buffer \. sudo docke",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:2669,energy efficiency,gpu,gpu,2669,"""${DATA_DIR}/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_PGandRTGphasetransfer_chrs_FIXED.vcf.gz"". TRUTH_BED=""${DATA_DIR}/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_nosomaticdel_chr.bed"". N_SHARDS=16. ```. ```bash. mkdir -p ""${OUTPUT_DIR}"". mkdir -p ""${BIN_DIR}"". mkdir -p ""${DATA_DIR}"". mkdir -p ""${LOG_DIR}"". ```. ```bash. gsutil -m cp ${DATA_BUCKET}/BGISEQ_PE100_NA12878.sorted.chr*.bam* ""${DATA_DIR}"". gsutil -m cp -r ""${DATA_BUCKET}/ucsc_hg19.fa*"" ""${DATA_DIR}"". gsutil -m cp -r ""${DATA_BUCKET}/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_*"" ""${DATA_DIR}"". ```. ```bash. sudo apt -y update. sudo apt -y install parallel. curl -O https://raw.githubusercontent.com/google/deepvariant/r1.6.1/scripts/install_nvidia_docker.sh. bash -x install_nvidia_docker.sh. ```. ```bash. sudo docker pull ${DOCKER_IMAGE} # Standard CPU Docker Image. sudo docker pull ${DOCKER_IMAGE}-gpu # GPU-enabled Docker image. ```. ```bash. ( time seq 0 $((N_SHARDS-1)) | \. parallel --halt 2 --line-buffer \. sudo docker run \. -v ${HOME}:${HOME} \. ${DOCKER_IMAGE} \. make_examples \. --mode training \. --ref ""${REF}"" \. --reads ""${BAM_CHR1}"" \. --examples ""${OUTPUT_DIR}/training_set.with_label.tfrecord@${N_SHARDS}.gz"" \. --truth_variants ""${TRUTH_VCF}"" \. --confident_regions ""${TRUTH_BED}"" \. --task {} \. --regions ""'chr1'"" \. --channels ""insert_size"" \. ) 2>&1 | tee ""${LOG_DIR}/training_set.with_label.make_examples.log"". ```. This took `20m0.146s`. ```. $ cat ""${OUTPUT_DIR}/training_set.with_label.tfrecord-00000-of-00016.gz.example_info.json"". {""version"": ""1.6.0"", ""shape"": [100, 221, 7], ""channels"": [1, 2, 3, 4, 5, 6, 19]}. ```. ```bash. gsutil -m cp ${OUTPUT_DIR}/training_set.with_label.tfrecord-?????-of-00016.gz* \. ${OUTPUT_BUCKET}. ```. ```bash. ( time seq 0 $((N_SHARDS-1)) | \. parallel --halt 2 --line-buffer \. sudo docker run \. -v /home/${USER}:/home/${USER} \. ${DOCKER",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:2675,energy efficiency,GPU,GPU-enabled,2675,"R}/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_PGandRTGphasetransfer_chrs_FIXED.vcf.gz"". TRUTH_BED=""${DATA_DIR}/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_nosomaticdel_chr.bed"". N_SHARDS=16. ```. ```bash. mkdir -p ""${OUTPUT_DIR}"". mkdir -p ""${BIN_DIR}"". mkdir -p ""${DATA_DIR}"". mkdir -p ""${LOG_DIR}"". ```. ```bash. gsutil -m cp ${DATA_BUCKET}/BGISEQ_PE100_NA12878.sorted.chr*.bam* ""${DATA_DIR}"". gsutil -m cp -r ""${DATA_BUCKET}/ucsc_hg19.fa*"" ""${DATA_DIR}"". gsutil -m cp -r ""${DATA_BUCKET}/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_*"" ""${DATA_DIR}"". ```. ```bash. sudo apt -y update. sudo apt -y install parallel. curl -O https://raw.githubusercontent.com/google/deepvariant/r1.6.1/scripts/install_nvidia_docker.sh. bash -x install_nvidia_docker.sh. ```. ```bash. sudo docker pull ${DOCKER_IMAGE} # Standard CPU Docker Image. sudo docker pull ${DOCKER_IMAGE}-gpu # GPU-enabled Docker image. ```. ```bash. ( time seq 0 $((N_SHARDS-1)) | \. parallel --halt 2 --line-buffer \. sudo docker run \. -v ${HOME}:${HOME} \. ${DOCKER_IMAGE} \. make_examples \. --mode training \. --ref ""${REF}"" \. --reads ""${BAM_CHR1}"" \. --examples ""${OUTPUT_DIR}/training_set.with_label.tfrecord@${N_SHARDS}.gz"" \. --truth_variants ""${TRUTH_VCF}"" \. --confident_regions ""${TRUTH_BED}"" \. --task {} \. --regions ""'chr1'"" \. --channels ""insert_size"" \. ) 2>&1 | tee ""${LOG_DIR}/training_set.with_label.make_examples.log"". ```. This took `20m0.146s`. ```. $ cat ""${OUTPUT_DIR}/training_set.with_label.tfrecord-00000-of-00016.gz.example_info.json"". {""version"": ""1.6.0"", ""shape"": [100, 221, 7], ""channels"": [1, 2, 3, 4, 5, 6, 19]}. ```. ```bash. gsutil -m cp ${OUTPUT_DIR}/training_set.with_label.tfrecord-?????-of-00016.gz* \. ${OUTPUT_BUCKET}. ```. ```bash. ( time seq 0 $((N_SHARDS-1)) | \. parallel --halt 2 --line-buffer \. sudo docker run \. -v /home/${USER}:/home/${USER} \. ${DOCKER_IMAGE} \.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:5559,energy efficiency,current,current,5559,". ```bash. sudo apt -y update && sudo apt -y install python3-pip. pip3 install --upgrade pip. pip3 install setuptools --upgrade. pip3 install apache_beam[gcp]==2.50.0 # 2.51.0 didn't work in my run. pip3 install tensorflow # For parsing tf.Example in shuffle_tfrecords_beam.py. ```. ```bash. time python3 ${SHUFFLE_SCRIPT_DIR}/shuffle_tfrecords_beam.py \. --project=""${YOUR_PROJECT}"" \. --input_pattern_list=""${OUTPUT_BUCKET}""/training_set.with_label.tfrecord-?????-of-00016.gz \. --output_pattern_prefix=""${OUTPUT_BUCKET}/training_set.with_label.shuffled"" \. --output_dataset_name=""HG001"" \. --output_dataset_config_pbtxt=""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt"" \. --job_name=shuffle-tfrecords \. --runner=DataflowRunner \. --staging_location=""${OUTPUT_BUCKET}/staging"" \. --temp_location=""${OUTPUT_BUCKET}/tempdir"" \. --save_main_session \. --region us-east1. ```. This worked for me. It took `15m32.746s`. I didn't get the numpy issue. So I decided to run `pip3 freeze` to get my current versions so you can compare:. ```bash. $ pip3 freeze. absl-py==2.1.0. apache-beam==2.50.0. astunparse==1.6.3. cachetools==5.3.3. certifi==2024.2.2. charset-normalizer==3.3.2. cloudpickle==2.2.1. crcmod==1.7. Deprecated==1.2.14. dill==0.3.1.1. dnspython==2.6.1. docopt==0.6.2. fastavro==1.9.4. fasteners==0.19. flatbuffers==24.3.7. gast==0.4.0. google-api-core==2.17.1. google-apitools==0.5.31. google-auth==2.28.2. google-auth-httplib2==0.1.1. google-auth-oauthlib==1.0.0. google-cloud-aiplatform==1.44.0. google-cloud-bigquery==3.19.0. google-cloud-bigquery-storage==2.24.0. google-cloud-bigtable==2.23.0. google-cloud-core==2.4.1. google-cloud-datastore==2.19.0. google-cloud-dlp==3.16.0. google-cloud-language==2.13.3. google-cloud-pubsub==2.20.2. google-cloud-pubsublite==1.9.0. google-cloud-recommendations-ai==0.10.10. google-cloud-resource-manager==1.12.3. google-cloud-spanner==3.44.0. google-cloud-storage==2.16.0. google-cloud-videointelligence==2.13.3. google-cloud-vision==3.7.2. goog",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:5742,energy efficiency,cloud,cloudpickle,5742,"rk in my run. pip3 install tensorflow # For parsing tf.Example in shuffle_tfrecords_beam.py. ```. ```bash. time python3 ${SHUFFLE_SCRIPT_DIR}/shuffle_tfrecords_beam.py \. --project=""${YOUR_PROJECT}"" \. --input_pattern_list=""${OUTPUT_BUCKET}""/training_set.with_label.tfrecord-?????-of-00016.gz \. --output_pattern_prefix=""${OUTPUT_BUCKET}/training_set.with_label.shuffled"" \. --output_dataset_name=""HG001"" \. --output_dataset_config_pbtxt=""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt"" \. --job_name=shuffle-tfrecords \. --runner=DataflowRunner \. --staging_location=""${OUTPUT_BUCKET}/staging"" \. --temp_location=""${OUTPUT_BUCKET}/tempdir"" \. --save_main_session \. --region us-east1. ```. This worked for me. It took `15m32.746s`. I didn't get the numpy issue. So I decided to run `pip3 freeze` to get my current versions so you can compare:. ```bash. $ pip3 freeze. absl-py==2.1.0. apache-beam==2.50.0. astunparse==1.6.3. cachetools==5.3.3. certifi==2024.2.2. charset-normalizer==3.3.2. cloudpickle==2.2.1. crcmod==1.7. Deprecated==1.2.14. dill==0.3.1.1. dnspython==2.6.1. docopt==0.6.2. fastavro==1.9.4. fasteners==0.19. flatbuffers==24.3.7. gast==0.4.0. google-api-core==2.17.1. google-apitools==0.5.31. google-auth==2.28.2. google-auth-httplib2==0.1.1. google-auth-oauthlib==1.0.0. google-cloud-aiplatform==1.44.0. google-cloud-bigquery==3.19.0. google-cloud-bigquery-storage==2.24.0. google-cloud-bigtable==2.23.0. google-cloud-core==2.4.1. google-cloud-datastore==2.19.0. google-cloud-dlp==3.16.0. google-cloud-language==2.13.3. google-cloud-pubsub==2.20.2. google-cloud-pubsublite==1.9.0. google-cloud-recommendations-ai==0.10.10. google-cloud-resource-manager==1.12.3. google-cloud-spanner==3.44.0. google-cloud-storage==2.16.0. google-cloud-videointelligence==2.13.3. google-cloud-vision==3.7.2. google-crc32c==1.5.0. google-pasta==0.2.0. google-resumable-media==2.7.0. googleapis-common-protos==1.63.0. grpc-google-iam-v1==0.13.0. grpc-interceptor==0.15.4. grpcio==1.62.1. grpcio-stat",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:5922,energy efficiency,core,core,5922,"ject=""${YOUR_PROJECT}"" \. --input_pattern_list=""${OUTPUT_BUCKET}""/training_set.with_label.tfrecord-?????-of-00016.gz \. --output_pattern_prefix=""${OUTPUT_BUCKET}/training_set.with_label.shuffled"" \. --output_dataset_name=""HG001"" \. --output_dataset_config_pbtxt=""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt"" \. --job_name=shuffle-tfrecords \. --runner=DataflowRunner \. --staging_location=""${OUTPUT_BUCKET}/staging"" \. --temp_location=""${OUTPUT_BUCKET}/tempdir"" \. --save_main_session \. --region us-east1. ```. This worked for me. It took `15m32.746s`. I didn't get the numpy issue. So I decided to run `pip3 freeze` to get my current versions so you can compare:. ```bash. $ pip3 freeze. absl-py==2.1.0. apache-beam==2.50.0. astunparse==1.6.3. cachetools==5.3.3. certifi==2024.2.2. charset-normalizer==3.3.2. cloudpickle==2.2.1. crcmod==1.7. Deprecated==1.2.14. dill==0.3.1.1. dnspython==2.6.1. docopt==0.6.2. fastavro==1.9.4. fasteners==0.19. flatbuffers==24.3.7. gast==0.4.0. google-api-core==2.17.1. google-apitools==0.5.31. google-auth==2.28.2. google-auth-httplib2==0.1.1. google-auth-oauthlib==1.0.0. google-cloud-aiplatform==1.44.0. google-cloud-bigquery==3.19.0. google-cloud-bigquery-storage==2.24.0. google-cloud-bigtable==2.23.0. google-cloud-core==2.4.1. google-cloud-datastore==2.19.0. google-cloud-dlp==3.16.0. google-cloud-language==2.13.3. google-cloud-pubsub==2.20.2. google-cloud-pubsublite==1.9.0. google-cloud-recommendations-ai==0.10.10. google-cloud-resource-manager==1.12.3. google-cloud-spanner==3.44.0. google-cloud-storage==2.16.0. google-cloud-videointelligence==2.13.3. google-cloud-vision==3.7.2. google-crc32c==1.5.0. google-pasta==0.2.0. google-resumable-media==2.7.0. googleapis-common-protos==1.63.0. grpc-google-iam-v1==0.13.0. grpc-interceptor==0.15.4. grpcio==1.62.1. grpcio-status==1.62.1. h5py==3.10.0. hdfs==2.7.3. httplib2==0.22.0. idna==3.6. importlib_metadata==7.0.2. keras==2.13.1. libclang==18.1.1. Markdown==3.6. MarkupSafe==2.1.5. numpy==1.24.3.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:6047,energy efficiency,cloud,cloud-aiplatform,6047,"ttern_prefix=""${OUTPUT_BUCKET}/training_set.with_label.shuffled"" \. --output_dataset_name=""HG001"" \. --output_dataset_config_pbtxt=""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt"" \. --job_name=shuffle-tfrecords \. --runner=DataflowRunner \. --staging_location=""${OUTPUT_BUCKET}/staging"" \. --temp_location=""${OUTPUT_BUCKET}/tempdir"" \. --save_main_session \. --region us-east1. ```. This worked for me. It took `15m32.746s`. I didn't get the numpy issue. So I decided to run `pip3 freeze` to get my current versions so you can compare:. ```bash. $ pip3 freeze. absl-py==2.1.0. apache-beam==2.50.0. astunparse==1.6.3. cachetools==5.3.3. certifi==2024.2.2. charset-normalizer==3.3.2. cloudpickle==2.2.1. crcmod==1.7. Deprecated==1.2.14. dill==0.3.1.1. dnspython==2.6.1. docopt==0.6.2. fastavro==1.9.4. fasteners==0.19. flatbuffers==24.3.7. gast==0.4.0. google-api-core==2.17.1. google-apitools==0.5.31. google-auth==2.28.2. google-auth-httplib2==0.1.1. google-auth-oauthlib==1.0.0. google-cloud-aiplatform==1.44.0. google-cloud-bigquery==3.19.0. google-cloud-bigquery-storage==2.24.0. google-cloud-bigtable==2.23.0. google-cloud-core==2.4.1. google-cloud-datastore==2.19.0. google-cloud-dlp==3.16.0. google-cloud-language==2.13.3. google-cloud-pubsub==2.20.2. google-cloud-pubsublite==1.9.0. google-cloud-recommendations-ai==0.10.10. google-cloud-resource-manager==1.12.3. google-cloud-spanner==3.44.0. google-cloud-storage==2.16.0. google-cloud-videointelligence==2.13.3. google-cloud-vision==3.7.2. google-crc32c==1.5.0. google-pasta==0.2.0. google-resumable-media==2.7.0. googleapis-common-protos==1.63.0. grpc-google-iam-v1==0.13.0. grpc-interceptor==0.15.4. grpcio==1.62.1. grpcio-status==1.62.1. h5py==3.10.0. hdfs==2.7.3. httplib2==0.22.0. idna==3.6. importlib_metadata==7.0.2. keras==2.13.1. libclang==18.1.1. Markdown==3.6. MarkupSafe==2.1.5. numpy==1.24.3. oauth2client==4.1.3. oauthlib==3.2.2. objsize==0.6.1. opt-einsum==3.3.0. orjson==3.9.15. overrides==7.7.0. packaging==24.0. pkg_re",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:6080,energy efficiency,cloud,cloud-bigquery,6080,"raining_set.with_label.shuffled"" \. --output_dataset_name=""HG001"" \. --output_dataset_config_pbtxt=""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt"" \. --job_name=shuffle-tfrecords \. --runner=DataflowRunner \. --staging_location=""${OUTPUT_BUCKET}/staging"" \. --temp_location=""${OUTPUT_BUCKET}/tempdir"" \. --save_main_session \. --region us-east1. ```. This worked for me. It took `15m32.746s`. I didn't get the numpy issue. So I decided to run `pip3 freeze` to get my current versions so you can compare:. ```bash. $ pip3 freeze. absl-py==2.1.0. apache-beam==2.50.0. astunparse==1.6.3. cachetools==5.3.3. certifi==2024.2.2. charset-normalizer==3.3.2. cloudpickle==2.2.1. crcmod==1.7. Deprecated==1.2.14. dill==0.3.1.1. dnspython==2.6.1. docopt==0.6.2. fastavro==1.9.4. fasteners==0.19. flatbuffers==24.3.7. gast==0.4.0. google-api-core==2.17.1. google-apitools==0.5.31. google-auth==2.28.2. google-auth-httplib2==0.1.1. google-auth-oauthlib==1.0.0. google-cloud-aiplatform==1.44.0. google-cloud-bigquery==3.19.0. google-cloud-bigquery-storage==2.24.0. google-cloud-bigtable==2.23.0. google-cloud-core==2.4.1. google-cloud-datastore==2.19.0. google-cloud-dlp==3.16.0. google-cloud-language==2.13.3. google-cloud-pubsub==2.20.2. google-cloud-pubsublite==1.9.0. google-cloud-recommendations-ai==0.10.10. google-cloud-resource-manager==1.12.3. google-cloud-spanner==3.44.0. google-cloud-storage==2.16.0. google-cloud-videointelligence==2.13.3. google-cloud-vision==3.7.2. google-crc32c==1.5.0. google-pasta==0.2.0. google-resumable-media==2.7.0. googleapis-common-protos==1.63.0. grpc-google-iam-v1==0.13.0. grpc-interceptor==0.15.4. grpcio==1.62.1. grpcio-status==1.62.1. h5py==3.10.0. hdfs==2.7.3. httplib2==0.22.0. idna==3.6. importlib_metadata==7.0.2. keras==2.13.1. libclang==18.1.1. Markdown==3.6. MarkupSafe==2.1.5. numpy==1.24.3. oauth2client==4.1.3. oauthlib==3.2.2. objsize==0.6.1. opt-einsum==3.3.0. orjson==3.9.15. overrides==7.7.0. packaging==24.0. pkg_resources==0.0.0. proto-plus==1.23",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:6111,energy efficiency,cloud,cloud-bigquery-storage,6111," --output_dataset_name=""HG001"" \. --output_dataset_config_pbtxt=""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt"" \. --job_name=shuffle-tfrecords \. --runner=DataflowRunner \. --staging_location=""${OUTPUT_BUCKET}/staging"" \. --temp_location=""${OUTPUT_BUCKET}/tempdir"" \. --save_main_session \. --region us-east1. ```. This worked for me. It took `15m32.746s`. I didn't get the numpy issue. So I decided to run `pip3 freeze` to get my current versions so you can compare:. ```bash. $ pip3 freeze. absl-py==2.1.0. apache-beam==2.50.0. astunparse==1.6.3. cachetools==5.3.3. certifi==2024.2.2. charset-normalizer==3.3.2. cloudpickle==2.2.1. crcmod==1.7. Deprecated==1.2.14. dill==0.3.1.1. dnspython==2.6.1. docopt==0.6.2. fastavro==1.9.4. fasteners==0.19. flatbuffers==24.3.7. gast==0.4.0. google-api-core==2.17.1. google-apitools==0.5.31. google-auth==2.28.2. google-auth-httplib2==0.1.1. google-auth-oauthlib==1.0.0. google-cloud-aiplatform==1.44.0. google-cloud-bigquery==3.19.0. google-cloud-bigquery-storage==2.24.0. google-cloud-bigtable==2.23.0. google-cloud-core==2.4.1. google-cloud-datastore==2.19.0. google-cloud-dlp==3.16.0. google-cloud-language==2.13.3. google-cloud-pubsub==2.20.2. google-cloud-pubsublite==1.9.0. google-cloud-recommendations-ai==0.10.10. google-cloud-resource-manager==1.12.3. google-cloud-spanner==3.44.0. google-cloud-storage==2.16.0. google-cloud-videointelligence==2.13.3. google-cloud-vision==3.7.2. google-crc32c==1.5.0. google-pasta==0.2.0. google-resumable-media==2.7.0. googleapis-common-protos==1.63.0. grpc-google-iam-v1==0.13.0. grpc-interceptor==0.15.4. grpcio==1.62.1. grpcio-status==1.62.1. h5py==3.10.0. hdfs==2.7.3. httplib2==0.22.0. idna==3.6. importlib_metadata==7.0.2. keras==2.13.1. libclang==18.1.1. Markdown==3.6. MarkupSafe==2.1.5. numpy==1.24.3. oauth2client==4.1.3. oauthlib==3.2.2. objsize==0.6.1. opt-einsum==3.3.0. orjson==3.9.15. overrides==7.7.0. packaging==24.0. pkg_resources==0.0.0. proto-plus==1.23.0. protobuf==4.23.4. pyarrow==11.0",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:6150,energy efficiency,cloud,cloud-bigtable,6150,"-output_dataset_config_pbtxt=""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt"" \. --job_name=shuffle-tfrecords \. --runner=DataflowRunner \. --staging_location=""${OUTPUT_BUCKET}/staging"" \. --temp_location=""${OUTPUT_BUCKET}/tempdir"" \. --save_main_session \. --region us-east1. ```. This worked for me. It took `15m32.746s`. I didn't get the numpy issue. So I decided to run `pip3 freeze` to get my current versions so you can compare:. ```bash. $ pip3 freeze. absl-py==2.1.0. apache-beam==2.50.0. astunparse==1.6.3. cachetools==5.3.3. certifi==2024.2.2. charset-normalizer==3.3.2. cloudpickle==2.2.1. crcmod==1.7. Deprecated==1.2.14. dill==0.3.1.1. dnspython==2.6.1. docopt==0.6.2. fastavro==1.9.4. fasteners==0.19. flatbuffers==24.3.7. gast==0.4.0. google-api-core==2.17.1. google-apitools==0.5.31. google-auth==2.28.2. google-auth-httplib2==0.1.1. google-auth-oauthlib==1.0.0. google-cloud-aiplatform==1.44.0. google-cloud-bigquery==3.19.0. google-cloud-bigquery-storage==2.24.0. google-cloud-bigtable==2.23.0. google-cloud-core==2.4.1. google-cloud-datastore==2.19.0. google-cloud-dlp==3.16.0. google-cloud-language==2.13.3. google-cloud-pubsub==2.20.2. google-cloud-pubsublite==1.9.0. google-cloud-recommendations-ai==0.10.10. google-cloud-resource-manager==1.12.3. google-cloud-spanner==3.44.0. google-cloud-storage==2.16.0. google-cloud-videointelligence==2.13.3. google-cloud-vision==3.7.2. google-crc32c==1.5.0. google-pasta==0.2.0. google-resumable-media==2.7.0. googleapis-common-protos==1.63.0. grpc-google-iam-v1==0.13.0. grpc-interceptor==0.15.4. grpcio==1.62.1. grpcio-status==1.62.1. h5py==3.10.0. hdfs==2.7.3. httplib2==0.22.0. idna==3.6. importlib_metadata==7.0.2. keras==2.13.1. libclang==18.1.1. Markdown==3.6. MarkupSafe==2.1.5. numpy==1.24.3. oauth2client==4.1.3. oauthlib==3.2.2. objsize==0.6.1. opt-einsum==3.3.0. orjson==3.9.15. overrides==7.7.0. packaging==24.0. pkg_resources==0.0.0. proto-plus==1.23.0. protobuf==4.23.4. pyarrow==11.0.0. pyasn1==0.5.1. pyasn1-modules==",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:6181,energy efficiency,cloud,cloud-core,6181,"""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt"" \. --job_name=shuffle-tfrecords \. --runner=DataflowRunner \. --staging_location=""${OUTPUT_BUCKET}/staging"" \. --temp_location=""${OUTPUT_BUCKET}/tempdir"" \. --save_main_session \. --region us-east1. ```. This worked for me. It took `15m32.746s`. I didn't get the numpy issue. So I decided to run `pip3 freeze` to get my current versions so you can compare:. ```bash. $ pip3 freeze. absl-py==2.1.0. apache-beam==2.50.0. astunparse==1.6.3. cachetools==5.3.3. certifi==2024.2.2. charset-normalizer==3.3.2. cloudpickle==2.2.1. crcmod==1.7. Deprecated==1.2.14. dill==0.3.1.1. dnspython==2.6.1. docopt==0.6.2. fastavro==1.9.4. fasteners==0.19. flatbuffers==24.3.7. gast==0.4.0. google-api-core==2.17.1. google-apitools==0.5.31. google-auth==2.28.2. google-auth-httplib2==0.1.1. google-auth-oauthlib==1.0.0. google-cloud-aiplatform==1.44.0. google-cloud-bigquery==3.19.0. google-cloud-bigquery-storage==2.24.0. google-cloud-bigtable==2.23.0. google-cloud-core==2.4.1. google-cloud-datastore==2.19.0. google-cloud-dlp==3.16.0. google-cloud-language==2.13.3. google-cloud-pubsub==2.20.2. google-cloud-pubsublite==1.9.0. google-cloud-recommendations-ai==0.10.10. google-cloud-resource-manager==1.12.3. google-cloud-spanner==3.44.0. google-cloud-storage==2.16.0. google-cloud-videointelligence==2.13.3. google-cloud-vision==3.7.2. google-crc32c==1.5.0. google-pasta==0.2.0. google-resumable-media==2.7.0. googleapis-common-protos==1.63.0. grpc-google-iam-v1==0.13.0. grpc-interceptor==0.15.4. grpcio==1.62.1. grpcio-status==1.62.1. h5py==3.10.0. hdfs==2.7.3. httplib2==0.22.0. idna==3.6. importlib_metadata==7.0.2. keras==2.13.1. libclang==18.1.1. Markdown==3.6. MarkupSafe==2.1.5. numpy==1.24.3. oauth2client==4.1.3. oauthlib==3.2.2. objsize==0.6.1. opt-einsum==3.3.0. orjson==3.9.15. overrides==7.7.0. packaging==24.0. pkg_resources==0.0.0. proto-plus==1.23.0. protobuf==4.23.4. pyarrow==11.0.0. pyasn1==0.5.1. pyasn1-modules==0.3.0. pydot==1.4.2. pymongo=",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:6207,energy efficiency,cloud,cloud-datastore,6207,"t.dataset_config.pbtxt"" \. --job_name=shuffle-tfrecords \. --runner=DataflowRunner \. --staging_location=""${OUTPUT_BUCKET}/staging"" \. --temp_location=""${OUTPUT_BUCKET}/tempdir"" \. --save_main_session \. --region us-east1. ```. This worked for me. It took `15m32.746s`. I didn't get the numpy issue. So I decided to run `pip3 freeze` to get my current versions so you can compare:. ```bash. $ pip3 freeze. absl-py==2.1.0. apache-beam==2.50.0. astunparse==1.6.3. cachetools==5.3.3. certifi==2024.2.2. charset-normalizer==3.3.2. cloudpickle==2.2.1. crcmod==1.7. Deprecated==1.2.14. dill==0.3.1.1. dnspython==2.6.1. docopt==0.6.2. fastavro==1.9.4. fasteners==0.19. flatbuffers==24.3.7. gast==0.4.0. google-api-core==2.17.1. google-apitools==0.5.31. google-auth==2.28.2. google-auth-httplib2==0.1.1. google-auth-oauthlib==1.0.0. google-cloud-aiplatform==1.44.0. google-cloud-bigquery==3.19.0. google-cloud-bigquery-storage==2.24.0. google-cloud-bigtable==2.23.0. google-cloud-core==2.4.1. google-cloud-datastore==2.19.0. google-cloud-dlp==3.16.0. google-cloud-language==2.13.3. google-cloud-pubsub==2.20.2. google-cloud-pubsublite==1.9.0. google-cloud-recommendations-ai==0.10.10. google-cloud-resource-manager==1.12.3. google-cloud-spanner==3.44.0. google-cloud-storage==2.16.0. google-cloud-videointelligence==2.13.3. google-cloud-vision==3.7.2. google-crc32c==1.5.0. google-pasta==0.2.0. google-resumable-media==2.7.0. googleapis-common-protos==1.63.0. grpc-google-iam-v1==0.13.0. grpc-interceptor==0.15.4. grpcio==1.62.1. grpcio-status==1.62.1. h5py==3.10.0. hdfs==2.7.3. httplib2==0.22.0. idna==3.6. importlib_metadata==7.0.2. keras==2.13.1. libclang==18.1.1. Markdown==3.6. MarkupSafe==2.1.5. numpy==1.24.3. oauth2client==4.1.3. oauthlib==3.2.2. objsize==0.6.1. opt-einsum==3.3.0. orjson==3.9.15. overrides==7.7.0. packaging==24.0. pkg_resources==0.0.0. proto-plus==1.23.0. protobuf==4.23.4. pyarrow==11.0.0. pyasn1==0.5.1. pyasn1-modules==0.3.0. pydot==1.4.2. pymongo==4.6.2. pyparsing==3.1.2. pyt",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:6239,energy efficiency,cloud,cloud-dlp,6239,"job_name=shuffle-tfrecords \. --runner=DataflowRunner \. --staging_location=""${OUTPUT_BUCKET}/staging"" \. --temp_location=""${OUTPUT_BUCKET}/tempdir"" \. --save_main_session \. --region us-east1. ```. This worked for me. It took `15m32.746s`. I didn't get the numpy issue. So I decided to run `pip3 freeze` to get my current versions so you can compare:. ```bash. $ pip3 freeze. absl-py==2.1.0. apache-beam==2.50.0. astunparse==1.6.3. cachetools==5.3.3. certifi==2024.2.2. charset-normalizer==3.3.2. cloudpickle==2.2.1. crcmod==1.7. Deprecated==1.2.14. dill==0.3.1.1. dnspython==2.6.1. docopt==0.6.2. fastavro==1.9.4. fasteners==0.19. flatbuffers==24.3.7. gast==0.4.0. google-api-core==2.17.1. google-apitools==0.5.31. google-auth==2.28.2. google-auth-httplib2==0.1.1. google-auth-oauthlib==1.0.0. google-cloud-aiplatform==1.44.0. google-cloud-bigquery==3.19.0. google-cloud-bigquery-storage==2.24.0. google-cloud-bigtable==2.23.0. google-cloud-core==2.4.1. google-cloud-datastore==2.19.0. google-cloud-dlp==3.16.0. google-cloud-language==2.13.3. google-cloud-pubsub==2.20.2. google-cloud-pubsublite==1.9.0. google-cloud-recommendations-ai==0.10.10. google-cloud-resource-manager==1.12.3. google-cloud-spanner==3.44.0. google-cloud-storage==2.16.0. google-cloud-videointelligence==2.13.3. google-cloud-vision==3.7.2. google-crc32c==1.5.0. google-pasta==0.2.0. google-resumable-media==2.7.0. googleapis-common-protos==1.63.0. grpc-google-iam-v1==0.13.0. grpc-interceptor==0.15.4. grpcio==1.62.1. grpcio-status==1.62.1. h5py==3.10.0. hdfs==2.7.3. httplib2==0.22.0. idna==3.6. importlib_metadata==7.0.2. keras==2.13.1. libclang==18.1.1. Markdown==3.6. MarkupSafe==2.1.5. numpy==1.24.3. oauth2client==4.1.3. oauthlib==3.2.2. objsize==0.6.1. opt-einsum==3.3.0. orjson==3.9.15. overrides==7.7.0. packaging==24.0. pkg_resources==0.0.0. proto-plus==1.23.0. protobuf==4.23.4. pyarrow==11.0.0. pyasn1==0.5.1. pyasn1-modules==0.3.0. pydot==1.4.2. pymongo==4.6.2. pyparsing==3.1.2. python-dateutil==2.9.0.post0. py",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:6265,energy efficiency,cloud,cloud-language,6265,". --runner=DataflowRunner \. --staging_location=""${OUTPUT_BUCKET}/staging"" \. --temp_location=""${OUTPUT_BUCKET}/tempdir"" \. --save_main_session \. --region us-east1. ```. This worked for me. It took `15m32.746s`. I didn't get the numpy issue. So I decided to run `pip3 freeze` to get my current versions so you can compare:. ```bash. $ pip3 freeze. absl-py==2.1.0. apache-beam==2.50.0. astunparse==1.6.3. cachetools==5.3.3. certifi==2024.2.2. charset-normalizer==3.3.2. cloudpickle==2.2.1. crcmod==1.7. Deprecated==1.2.14. dill==0.3.1.1. dnspython==2.6.1. docopt==0.6.2. fastavro==1.9.4. fasteners==0.19. flatbuffers==24.3.7. gast==0.4.0. google-api-core==2.17.1. google-apitools==0.5.31. google-auth==2.28.2. google-auth-httplib2==0.1.1. google-auth-oauthlib==1.0.0. google-cloud-aiplatform==1.44.0. google-cloud-bigquery==3.19.0. google-cloud-bigquery-storage==2.24.0. google-cloud-bigtable==2.23.0. google-cloud-core==2.4.1. google-cloud-datastore==2.19.0. google-cloud-dlp==3.16.0. google-cloud-language==2.13.3. google-cloud-pubsub==2.20.2. google-cloud-pubsublite==1.9.0. google-cloud-recommendations-ai==0.10.10. google-cloud-resource-manager==1.12.3. google-cloud-spanner==3.44.0. google-cloud-storage==2.16.0. google-cloud-videointelligence==2.13.3. google-cloud-vision==3.7.2. google-crc32c==1.5.0. google-pasta==0.2.0. google-resumable-media==2.7.0. googleapis-common-protos==1.63.0. grpc-google-iam-v1==0.13.0. grpc-interceptor==0.15.4. grpcio==1.62.1. grpcio-status==1.62.1. h5py==3.10.0. hdfs==2.7.3. httplib2==0.22.0. idna==3.6. importlib_metadata==7.0.2. keras==2.13.1. libclang==18.1.1. Markdown==3.6. MarkupSafe==2.1.5. numpy==1.24.3. oauth2client==4.1.3. oauthlib==3.2.2. objsize==0.6.1. opt-einsum==3.3.0. orjson==3.9.15. overrides==7.7.0. packaging==24.0. pkg_resources==0.0.0. proto-plus==1.23.0. protobuf==4.23.4. pyarrow==11.0.0. pyasn1==0.5.1. pyasn1-modules==0.3.0. pydot==1.4.2. pymongo==4.6.2. pyparsing==3.1.2. python-dateutil==2.9.0.post0. pytz==2024.1. regex==2023.12.2",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:6296,energy efficiency,cloud,cloud-pubsub,6296,"-staging_location=""${OUTPUT_BUCKET}/staging"" \. --temp_location=""${OUTPUT_BUCKET}/tempdir"" \. --save_main_session \. --region us-east1. ```. This worked for me. It took `15m32.746s`. I didn't get the numpy issue. So I decided to run `pip3 freeze` to get my current versions so you can compare:. ```bash. $ pip3 freeze. absl-py==2.1.0. apache-beam==2.50.0. astunparse==1.6.3. cachetools==5.3.3. certifi==2024.2.2. charset-normalizer==3.3.2. cloudpickle==2.2.1. crcmod==1.7. Deprecated==1.2.14. dill==0.3.1.1. dnspython==2.6.1. docopt==0.6.2. fastavro==1.9.4. fasteners==0.19. flatbuffers==24.3.7. gast==0.4.0. google-api-core==2.17.1. google-apitools==0.5.31. google-auth==2.28.2. google-auth-httplib2==0.1.1. google-auth-oauthlib==1.0.0. google-cloud-aiplatform==1.44.0. google-cloud-bigquery==3.19.0. google-cloud-bigquery-storage==2.24.0. google-cloud-bigtable==2.23.0. google-cloud-core==2.4.1. google-cloud-datastore==2.19.0. google-cloud-dlp==3.16.0. google-cloud-language==2.13.3. google-cloud-pubsub==2.20.2. google-cloud-pubsublite==1.9.0. google-cloud-recommendations-ai==0.10.10. google-cloud-resource-manager==1.12.3. google-cloud-spanner==3.44.0. google-cloud-storage==2.16.0. google-cloud-videointelligence==2.13.3. google-cloud-vision==3.7.2. google-crc32c==1.5.0. google-pasta==0.2.0. google-resumable-media==2.7.0. googleapis-common-protos==1.63.0. grpc-google-iam-v1==0.13.0. grpc-interceptor==0.15.4. grpcio==1.62.1. grpcio-status==1.62.1. h5py==3.10.0. hdfs==2.7.3. httplib2==0.22.0. idna==3.6. importlib_metadata==7.0.2. keras==2.13.1. libclang==18.1.1. Markdown==3.6. MarkupSafe==2.1.5. numpy==1.24.3. oauth2client==4.1.3. oauthlib==3.2.2. objsize==0.6.1. opt-einsum==3.3.0. orjson==3.9.15. overrides==7.7.0. packaging==24.0. pkg_resources==0.0.0. proto-plus==1.23.0. protobuf==4.23.4. pyarrow==11.0.0. pyasn1==0.5.1. pyasn1-modules==0.3.0. pydot==1.4.2. pymongo==4.6.2. pyparsing==3.1.2. python-dateutil==2.9.0.post0. pytz==2024.1. regex==2023.12.25. requests==2.31.0. requests-",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:6325,energy efficiency,cloud,cloud-pubsublite,6325,"KET}/staging"" \. --temp_location=""${OUTPUT_BUCKET}/tempdir"" \. --save_main_session \. --region us-east1. ```. This worked for me. It took `15m32.746s`. I didn't get the numpy issue. So I decided to run `pip3 freeze` to get my current versions so you can compare:. ```bash. $ pip3 freeze. absl-py==2.1.0. apache-beam==2.50.0. astunparse==1.6.3. cachetools==5.3.3. certifi==2024.2.2. charset-normalizer==3.3.2. cloudpickle==2.2.1. crcmod==1.7. Deprecated==1.2.14. dill==0.3.1.1. dnspython==2.6.1. docopt==0.6.2. fastavro==1.9.4. fasteners==0.19. flatbuffers==24.3.7. gast==0.4.0. google-api-core==2.17.1. google-apitools==0.5.31. google-auth==2.28.2. google-auth-httplib2==0.1.1. google-auth-oauthlib==1.0.0. google-cloud-aiplatform==1.44.0. google-cloud-bigquery==3.19.0. google-cloud-bigquery-storage==2.24.0. google-cloud-bigtable==2.23.0. google-cloud-core==2.4.1. google-cloud-datastore==2.19.0. google-cloud-dlp==3.16.0. google-cloud-language==2.13.3. google-cloud-pubsub==2.20.2. google-cloud-pubsublite==1.9.0. google-cloud-recommendations-ai==0.10.10. google-cloud-resource-manager==1.12.3. google-cloud-spanner==3.44.0. google-cloud-storage==2.16.0. google-cloud-videointelligence==2.13.3. google-cloud-vision==3.7.2. google-crc32c==1.5.0. google-pasta==0.2.0. google-resumable-media==2.7.0. googleapis-common-protos==1.63.0. grpc-google-iam-v1==0.13.0. grpc-interceptor==0.15.4. grpcio==1.62.1. grpcio-status==1.62.1. h5py==3.10.0. hdfs==2.7.3. httplib2==0.22.0. idna==3.6. importlib_metadata==7.0.2. keras==2.13.1. libclang==18.1.1. Markdown==3.6. MarkupSafe==2.1.5. numpy==1.24.3. oauth2client==4.1.3. oauthlib==3.2.2. objsize==0.6.1. opt-einsum==3.3.0. orjson==3.9.15. overrides==7.7.0. packaging==24.0. pkg_resources==0.0.0. proto-plus==1.23.0. protobuf==4.23.4. pyarrow==11.0.0. pyasn1==0.5.1. pyasn1-modules==0.3.0. pydot==1.4.2. pymongo==4.6.2. pyparsing==3.1.2. python-dateutil==2.9.0.post0. pytz==2024.1. regex==2023.12.25. requests==2.31.0. requests-oauthlib==1.4.0. rsa==4.9. shap",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:6357,energy efficiency,cloud,cloud-recommendations-ai,6357,"OUTPUT_BUCKET}/tempdir"" \. --save_main_session \. --region us-east1. ```. This worked for me. It took `15m32.746s`. I didn't get the numpy issue. So I decided to run `pip3 freeze` to get my current versions so you can compare:. ```bash. $ pip3 freeze. absl-py==2.1.0. apache-beam==2.50.0. astunparse==1.6.3. cachetools==5.3.3. certifi==2024.2.2. charset-normalizer==3.3.2. cloudpickle==2.2.1. crcmod==1.7. Deprecated==1.2.14. dill==0.3.1.1. dnspython==2.6.1. docopt==0.6.2. fastavro==1.9.4. fasteners==0.19. flatbuffers==24.3.7. gast==0.4.0. google-api-core==2.17.1. google-apitools==0.5.31. google-auth==2.28.2. google-auth-httplib2==0.1.1. google-auth-oauthlib==1.0.0. google-cloud-aiplatform==1.44.0. google-cloud-bigquery==3.19.0. google-cloud-bigquery-storage==2.24.0. google-cloud-bigtable==2.23.0. google-cloud-core==2.4.1. google-cloud-datastore==2.19.0. google-cloud-dlp==3.16.0. google-cloud-language==2.13.3. google-cloud-pubsub==2.20.2. google-cloud-pubsublite==1.9.0. google-cloud-recommendations-ai==0.10.10. google-cloud-resource-manager==1.12.3. google-cloud-spanner==3.44.0. google-cloud-storage==2.16.0. google-cloud-videointelligence==2.13.3. google-cloud-vision==3.7.2. google-crc32c==1.5.0. google-pasta==0.2.0. google-resumable-media==2.7.0. googleapis-common-protos==1.63.0. grpc-google-iam-v1==0.13.0. grpc-interceptor==0.15.4. grpcio==1.62.1. grpcio-status==1.62.1. h5py==3.10.0. hdfs==2.7.3. httplib2==0.22.0. idna==3.6. importlib_metadata==7.0.2. keras==2.13.1. libclang==18.1.1. Markdown==3.6. MarkupSafe==2.1.5. numpy==1.24.3. oauth2client==4.1.3. oauthlib==3.2.2. objsize==0.6.1. opt-einsum==3.3.0. orjson==3.9.15. overrides==7.7.0. packaging==24.0. pkg_resources==0.0.0. proto-plus==1.23.0. protobuf==4.23.4. pyarrow==11.0.0. pyasn1==0.5.1. pyasn1-modules==0.3.0. pydot==1.4.2. pymongo==4.6.2. pyparsing==3.1.2. python-dateutil==2.9.0.post0. pytz==2024.1. regex==2023.12.25. requests==2.31.0. requests-oauthlib==1.4.0. rsa==4.9. shapely==2.0.3. six==1.16.0. sqlparse==0",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:6399,energy efficiency,cloud,cloud-resource-manager,6399,ssion \. --region us-east1. ```. This worked for me. It took `15m32.746s`. I didn't get the numpy issue. So I decided to run `pip3 freeze` to get my current versions so you can compare:. ```bash. $ pip3 freeze. absl-py==2.1.0. apache-beam==2.50.0. astunparse==1.6.3. cachetools==5.3.3. certifi==2024.2.2. charset-normalizer==3.3.2. cloudpickle==2.2.1. crcmod==1.7. Deprecated==1.2.14. dill==0.3.1.1. dnspython==2.6.1. docopt==0.6.2. fastavro==1.9.4. fasteners==0.19. flatbuffers==24.3.7. gast==0.4.0. google-api-core==2.17.1. google-apitools==0.5.31. google-auth==2.28.2. google-auth-httplib2==0.1.1. google-auth-oauthlib==1.0.0. google-cloud-aiplatform==1.44.0. google-cloud-bigquery==3.19.0. google-cloud-bigquery-storage==2.24.0. google-cloud-bigtable==2.23.0. google-cloud-core==2.4.1. google-cloud-datastore==2.19.0. google-cloud-dlp==3.16.0. google-cloud-language==2.13.3. google-cloud-pubsub==2.20.2. google-cloud-pubsublite==1.9.0. google-cloud-recommendations-ai==0.10.10. google-cloud-resource-manager==1.12.3. google-cloud-spanner==3.44.0. google-cloud-storage==2.16.0. google-cloud-videointelligence==2.13.3. google-cloud-vision==3.7.2. google-crc32c==1.5.0. google-pasta==0.2.0. google-resumable-media==2.7.0. googleapis-common-protos==1.63.0. grpc-google-iam-v1==0.13.0. grpc-interceptor==0.15.4. grpcio==1.62.1. grpcio-status==1.62.1. h5py==3.10.0. hdfs==2.7.3. httplib2==0.22.0. idna==3.6. importlib_metadata==7.0.2. keras==2.13.1. libclang==18.1.1. Markdown==3.6. MarkupSafe==2.1.5. numpy==1.24.3. oauth2client==4.1.3. oauthlib==3.2.2. objsize==0.6.1. opt-einsum==3.3.0. orjson==3.9.15. overrides==7.7.0. packaging==24.0. pkg_resources==0.0.0. proto-plus==1.23.0. protobuf==4.23.4. pyarrow==11.0.0. pyasn1==0.5.1. pyasn1-modules==0.3.0. pydot==1.4.2. pymongo==4.6.2. pyparsing==3.1.2. python-dateutil==2.9.0.post0. pytz==2024.1. regex==2023.12.25. requests==2.31.0. requests-oauthlib==1.4.0. rsa==4.9. shapely==2.0.3. six==1.16.0. sqlparse==0.4.4. tensorboard==2.13.0. tensorboard-da,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:6438,energy efficiency,cloud,cloud-spanner,6438,is worked for me. It took `15m32.746s`. I didn't get the numpy issue. So I decided to run `pip3 freeze` to get my current versions so you can compare:. ```bash. $ pip3 freeze. absl-py==2.1.0. apache-beam==2.50.0. astunparse==1.6.3. cachetools==5.3.3. certifi==2024.2.2. charset-normalizer==3.3.2. cloudpickle==2.2.1. crcmod==1.7. Deprecated==1.2.14. dill==0.3.1.1. dnspython==2.6.1. docopt==0.6.2. fastavro==1.9.4. fasteners==0.19. flatbuffers==24.3.7. gast==0.4.0. google-api-core==2.17.1. google-apitools==0.5.31. google-auth==2.28.2. google-auth-httplib2==0.1.1. google-auth-oauthlib==1.0.0. google-cloud-aiplatform==1.44.0. google-cloud-bigquery==3.19.0. google-cloud-bigquery-storage==2.24.0. google-cloud-bigtable==2.23.0. google-cloud-core==2.4.1. google-cloud-datastore==2.19.0. google-cloud-dlp==3.16.0. google-cloud-language==2.13.3. google-cloud-pubsub==2.20.2. google-cloud-pubsublite==1.9.0. google-cloud-recommendations-ai==0.10.10. google-cloud-resource-manager==1.12.3. google-cloud-spanner==3.44.0. google-cloud-storage==2.16.0. google-cloud-videointelligence==2.13.3. google-cloud-vision==3.7.2. google-crc32c==1.5.0. google-pasta==0.2.0. google-resumable-media==2.7.0. googleapis-common-protos==1.63.0. grpc-google-iam-v1==0.13.0. grpc-interceptor==0.15.4. grpcio==1.62.1. grpcio-status==1.62.1. h5py==3.10.0. hdfs==2.7.3. httplib2==0.22.0. idna==3.6. importlib_metadata==7.0.2. keras==2.13.1. libclang==18.1.1. Markdown==3.6. MarkupSafe==2.1.5. numpy==1.24.3. oauth2client==4.1.3. oauthlib==3.2.2. objsize==0.6.1. opt-einsum==3.3.0. orjson==3.9.15. overrides==7.7.0. packaging==24.0. pkg_resources==0.0.0. proto-plus==1.23.0. protobuf==4.23.4. pyarrow==11.0.0. pyasn1==0.5.1. pyasn1-modules==0.3.0. pydot==1.4.2. pymongo==4.6.2. pyparsing==3.1.2. python-dateutil==2.9.0.post0. pytz==2024.1. regex==2023.12.25. requests==2.31.0. requests-oauthlib==1.4.0. rsa==4.9. shapely==2.0.3. six==1.16.0. sqlparse==0.4.4. tensorboard==2.13.0. tensorboard-data-server==0.7.2. tensorflow==2.13.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:6468,energy efficiency,cloud,cloud-storage,6468,32.746s`. I didn't get the numpy issue. So I decided to run `pip3 freeze` to get my current versions so you can compare:. ```bash. $ pip3 freeze. absl-py==2.1.0. apache-beam==2.50.0. astunparse==1.6.3. cachetools==5.3.3. certifi==2024.2.2. charset-normalizer==3.3.2. cloudpickle==2.2.1. crcmod==1.7. Deprecated==1.2.14. dill==0.3.1.1. dnspython==2.6.1. docopt==0.6.2. fastavro==1.9.4. fasteners==0.19. flatbuffers==24.3.7. gast==0.4.0. google-api-core==2.17.1. google-apitools==0.5.31. google-auth==2.28.2. google-auth-httplib2==0.1.1. google-auth-oauthlib==1.0.0. google-cloud-aiplatform==1.44.0. google-cloud-bigquery==3.19.0. google-cloud-bigquery-storage==2.24.0. google-cloud-bigtable==2.23.0. google-cloud-core==2.4.1. google-cloud-datastore==2.19.0. google-cloud-dlp==3.16.0. google-cloud-language==2.13.3. google-cloud-pubsub==2.20.2. google-cloud-pubsublite==1.9.0. google-cloud-recommendations-ai==0.10.10. google-cloud-resource-manager==1.12.3. google-cloud-spanner==3.44.0. google-cloud-storage==2.16.0. google-cloud-videointelligence==2.13.3. google-cloud-vision==3.7.2. google-crc32c==1.5.0. google-pasta==0.2.0. google-resumable-media==2.7.0. googleapis-common-protos==1.63.0. grpc-google-iam-v1==0.13.0. grpc-interceptor==0.15.4. grpcio==1.62.1. grpcio-status==1.62.1. h5py==3.10.0. hdfs==2.7.3. httplib2==0.22.0. idna==3.6. importlib_metadata==7.0.2. keras==2.13.1. libclang==18.1.1. Markdown==3.6. MarkupSafe==2.1.5. numpy==1.24.3. oauth2client==4.1.3. oauthlib==3.2.2. objsize==0.6.1. opt-einsum==3.3.0. orjson==3.9.15. overrides==7.7.0. packaging==24.0. pkg_resources==0.0.0. proto-plus==1.23.0. protobuf==4.23.4. pyarrow==11.0.0. pyasn1==0.5.1. pyasn1-modules==0.3.0. pydot==1.4.2. pymongo==4.6.2. pyparsing==3.1.2. python-dateutil==2.9.0.post0. pytz==2024.1. regex==2023.12.25. requests==2.31.0. requests-oauthlib==1.4.0. rsa==4.9. shapely==2.0.3. six==1.16.0. sqlparse==0.4.4. tensorboard==2.13.0. tensorboard-data-server==0.7.2. tensorflow==2.13.1. tensorflow-estimator==2.13.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:6498,energy efficiency,cloud,cloud-videointelligence,6498,sue. So I decided to run `pip3 freeze` to get my current versions so you can compare:. ```bash. $ pip3 freeze. absl-py==2.1.0. apache-beam==2.50.0. astunparse==1.6.3. cachetools==5.3.3. certifi==2024.2.2. charset-normalizer==3.3.2. cloudpickle==2.2.1. crcmod==1.7. Deprecated==1.2.14. dill==0.3.1.1. dnspython==2.6.1. docopt==0.6.2. fastavro==1.9.4. fasteners==0.19. flatbuffers==24.3.7. gast==0.4.0. google-api-core==2.17.1. google-apitools==0.5.31. google-auth==2.28.2. google-auth-httplib2==0.1.1. google-auth-oauthlib==1.0.0. google-cloud-aiplatform==1.44.0. google-cloud-bigquery==3.19.0. google-cloud-bigquery-storage==2.24.0. google-cloud-bigtable==2.23.0. google-cloud-core==2.4.1. google-cloud-datastore==2.19.0. google-cloud-dlp==3.16.0. google-cloud-language==2.13.3. google-cloud-pubsub==2.20.2. google-cloud-pubsublite==1.9.0. google-cloud-recommendations-ai==0.10.10. google-cloud-resource-manager==1.12.3. google-cloud-spanner==3.44.0. google-cloud-storage==2.16.0. google-cloud-videointelligence==2.13.3. google-cloud-vision==3.7.2. google-crc32c==1.5.0. google-pasta==0.2.0. google-resumable-media==2.7.0. googleapis-common-protos==1.63.0. grpc-google-iam-v1==0.13.0. grpc-interceptor==0.15.4. grpcio==1.62.1. grpcio-status==1.62.1. h5py==3.10.0. hdfs==2.7.3. httplib2==0.22.0. idna==3.6. importlib_metadata==7.0.2. keras==2.13.1. libclang==18.1.1. Markdown==3.6. MarkupSafe==2.1.5. numpy==1.24.3. oauth2client==4.1.3. oauthlib==3.2.2. objsize==0.6.1. opt-einsum==3.3.0. orjson==3.9.15. overrides==7.7.0. packaging==24.0. pkg_resources==0.0.0. proto-plus==1.23.0. protobuf==4.23.4. pyarrow==11.0.0. pyasn1==0.5.1. pyasn1-modules==0.3.0. pydot==1.4.2. pymongo==4.6.2. pyparsing==3.1.2. python-dateutil==2.9.0.post0. pytz==2024.1. regex==2023.12.25. requests==2.31.0. requests-oauthlib==1.4.0. rsa==4.9. shapely==2.0.3. six==1.16.0. sqlparse==0.4.4. tensorboard==2.13.0. tensorboard-data-server==0.7.2. tensorflow==2.13.1. tensorflow-estimator==2.13.0. tensorflow-io-gcs-filesystem==0.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:6538,energy efficiency,cloud,cloud-vision,6538,eze` to get my current versions so you can compare:. ```bash. $ pip3 freeze. absl-py==2.1.0. apache-beam==2.50.0. astunparse==1.6.3. cachetools==5.3.3. certifi==2024.2.2. charset-normalizer==3.3.2. cloudpickle==2.2.1. crcmod==1.7. Deprecated==1.2.14. dill==0.3.1.1. dnspython==2.6.1. docopt==0.6.2. fastavro==1.9.4. fasteners==0.19. flatbuffers==24.3.7. gast==0.4.0. google-api-core==2.17.1. google-apitools==0.5.31. google-auth==2.28.2. google-auth-httplib2==0.1.1. google-auth-oauthlib==1.0.0. google-cloud-aiplatform==1.44.0. google-cloud-bigquery==3.19.0. google-cloud-bigquery-storage==2.24.0. google-cloud-bigtable==2.23.0. google-cloud-core==2.4.1. google-cloud-datastore==2.19.0. google-cloud-dlp==3.16.0. google-cloud-language==2.13.3. google-cloud-pubsub==2.20.2. google-cloud-pubsublite==1.9.0. google-cloud-recommendations-ai==0.10.10. google-cloud-resource-manager==1.12.3. google-cloud-spanner==3.44.0. google-cloud-storage==2.16.0. google-cloud-videointelligence==2.13.3. google-cloud-vision==3.7.2. google-crc32c==1.5.0. google-pasta==0.2.0. google-resumable-media==2.7.0. googleapis-common-protos==1.63.0. grpc-google-iam-v1==0.13.0. grpc-interceptor==0.15.4. grpcio==1.62.1. grpcio-status==1.62.1. h5py==3.10.0. hdfs==2.7.3. httplib2==0.22.0. idna==3.6. importlib_metadata==7.0.2. keras==2.13.1. libclang==18.1.1. Markdown==3.6. MarkupSafe==2.1.5. numpy==1.24.3. oauth2client==4.1.3. oauthlib==3.2.2. objsize==0.6.1. opt-einsum==3.3.0. orjson==3.9.15. overrides==7.7.0. packaging==24.0. pkg_resources==0.0.0. proto-plus==1.23.0. protobuf==4.23.4. pyarrow==11.0.0. pyasn1==0.5.1. pyasn1-modules==0.3.0. pydot==1.4.2. pymongo==4.6.2. pyparsing==3.1.2. python-dateutil==2.9.0.post0. pytz==2024.1. regex==2023.12.25. requests==2.31.0. requests-oauthlib==1.4.0. rsa==4.9. shapely==2.0.3. six==1.16.0. sqlparse==0.4.4. tensorboard==2.13.0. tensorboard-data-server==0.7.2. tensorflow==2.13.1. tensorflow-estimator==2.13.0. tensorflow-io-gcs-filesystem==0.34.0. termcolor==2.4.0. typing_ext,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:7459,energy efficiency,estimat,estimator,7459,"rmalizer==3.3.2. cloudpickle==2.2.1. crcmod==1.7. Deprecated==1.2.14. dill==0.3.1.1. dnspython==2.6.1. docopt==0.6.2. fastavro==1.9.4. fasteners==0.19. flatbuffers==24.3.7. gast==0.4.0. google-api-core==2.17.1. google-apitools==0.5.31. google-auth==2.28.2. google-auth-httplib2==0.1.1. google-auth-oauthlib==1.0.0. google-cloud-aiplatform==1.44.0. google-cloud-bigquery==3.19.0. google-cloud-bigquery-storage==2.24.0. google-cloud-bigtable==2.23.0. google-cloud-core==2.4.1. google-cloud-datastore==2.19.0. google-cloud-dlp==3.16.0. google-cloud-language==2.13.3. google-cloud-pubsub==2.20.2. google-cloud-pubsublite==1.9.0. google-cloud-recommendations-ai==0.10.10. google-cloud-resource-manager==1.12.3. google-cloud-spanner==3.44.0. google-cloud-storage==2.16.0. google-cloud-videointelligence==2.13.3. google-cloud-vision==3.7.2. google-crc32c==1.5.0. google-pasta==0.2.0. google-resumable-media==2.7.0. googleapis-common-protos==1.63.0. grpc-google-iam-v1==0.13.0. grpc-interceptor==0.15.4. grpcio==1.62.1. grpcio-status==1.62.1. h5py==3.10.0. hdfs==2.7.3. httplib2==0.22.0. idna==3.6. importlib_metadata==7.0.2. keras==2.13.1. libclang==18.1.1. Markdown==3.6. MarkupSafe==2.1.5. numpy==1.24.3. oauth2client==4.1.3. oauthlib==3.2.2. objsize==0.6.1. opt-einsum==3.3.0. orjson==3.9.15. overrides==7.7.0. packaging==24.0. pkg_resources==0.0.0. proto-plus==1.23.0. protobuf==4.23.4. pyarrow==11.0.0. pyasn1==0.5.1. pyasn1-modules==0.3.0. pydot==1.4.2. pymongo==4.6.2. pyparsing==3.1.2. python-dateutil==2.9.0.post0. pytz==2024.1. regex==2023.12.25. requests==2.31.0. requests-oauthlib==1.4.0. rsa==4.9. shapely==2.0.3. six==1.16.0. sqlparse==0.4.4. tensorboard==2.13.0. tensorboard-data-server==0.7.2. tensorflow==2.13.1. tensorflow-estimator==2.13.0. tensorflow-io-gcs-filesystem==0.34.0. termcolor==2.4.0. typing_extensions==4.5.0. urllib3==2.2.1. Werkzeug==3.0.1. wrapt==1.16.0. zipp==3.18.1. zstandard==0.22.0. ```. ---. @helizabeth1103 , can you see which version might be different from yours?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:819,integrability,VERSION,VERSION,819,"# Follow https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-training-case-study.md. I got a machine:. ```bash. host=""${USER}-deepvariant-vm"". zone=""us-west1-b"". gcloud compute instances create ${host} \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --maintenance-policy ""TERMINATE"" \. --accelerator=type=nvidia-tesla-p100,count=1 \. --image-family ""ubuntu-2004-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""n1-standard-16"" \. --boot-disk-size ""300"" \. --zone ""${zone}"" \. --min-cpu-platform ""Intel Skylake"". ```. I sshed into my machine:. ```bash. gcloud compute ssh pichuan-deepvariant-vm --zone us-west1-b. ```. I ran this with my own `YOUR_PROJECT` and `OUTPUT_GCS_BUCKET` setting. Then the following is basically just copy/paste from the doc:. ```. BUCKET=""gs://deepvariant"". VERSION=""1.6.1"". DOCKER_IMAGE=""google/deepvariant:${VERSION}"". MODEL_BUCKET=""${BUCKET}/models/DeepVariant/${VERSION}/DeepVariant-inception_v3-${VERSION}+data-wgs_standard"". GCS_PRETRAINED_WGS_MODEL=""${MODEL_BUCKET}/model.ckpt"". OUTPUT_BUCKET=""${OUTPUT_GCS_BUCKET}/customized_training"". TRAINING_DIR=""${OUTPUT_BUCKET}/training_dir"". BASE=""${HOME}/training-case-study"". DATA_BUCKET=gs://deepvariant/training-case-study/BGISEQ-HG001. INPUT_DIR=""${BASE}/input"". BIN_DIR=""${INPUT_DIR}/bin"". DATA_DIR=""${INPUT_DIR}/data"". OUTPUT_DIR=""${BASE}/output"". LOG_DIR=""${OUTPUT_DIR}/logs"". SHUFFLE_SCRIPT_DIR=""${HOME}/deepvariant/tools"". REF=""${DATA_DIR}/ucsc_hg19.fa"". BAM_CHR1=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr1.bam"". BAM_CHR20=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr20.bam"". BAM_CHR21=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr21.bam"". TRUTH_VCF=""${DATA_DIR}/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_PGandRTGphasetransfer_chrs_FIXED.vcf.gz"". TRUTH_BED=""${DATA_DIR}/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_nosomaticdel_chr.bed"". N_SHARDS=16. ```. ```bash. mkdir -p ""${OUTPUT_DIR}"". mkd",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:871,integrability,VERSION,VERSION,871,"# Follow https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-training-case-study.md. I got a machine:. ```bash. host=""${USER}-deepvariant-vm"". zone=""us-west1-b"". gcloud compute instances create ${host} \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --maintenance-policy ""TERMINATE"" \. --accelerator=type=nvidia-tesla-p100,count=1 \. --image-family ""ubuntu-2004-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""n1-standard-16"" \. --boot-disk-size ""300"" \. --zone ""${zone}"" \. --min-cpu-platform ""Intel Skylake"". ```. I sshed into my machine:. ```bash. gcloud compute ssh pichuan-deepvariant-vm --zone us-west1-b. ```. I ran this with my own `YOUR_PROJECT` and `OUTPUT_GCS_BUCKET` setting. Then the following is basically just copy/paste from the doc:. ```. BUCKET=""gs://deepvariant"". VERSION=""1.6.1"". DOCKER_IMAGE=""google/deepvariant:${VERSION}"". MODEL_BUCKET=""${BUCKET}/models/DeepVariant/${VERSION}/DeepVariant-inception_v3-${VERSION}+data-wgs_standard"". GCS_PRETRAINED_WGS_MODEL=""${MODEL_BUCKET}/model.ckpt"". OUTPUT_BUCKET=""${OUTPUT_GCS_BUCKET}/customized_training"". TRAINING_DIR=""${OUTPUT_BUCKET}/training_dir"". BASE=""${HOME}/training-case-study"". DATA_BUCKET=gs://deepvariant/training-case-study/BGISEQ-HG001. INPUT_DIR=""${BASE}/input"". BIN_DIR=""${INPUT_DIR}/bin"". DATA_DIR=""${INPUT_DIR}/data"". OUTPUT_DIR=""${BASE}/output"". LOG_DIR=""${OUTPUT_DIR}/logs"". SHUFFLE_SCRIPT_DIR=""${HOME}/deepvariant/tools"". REF=""${DATA_DIR}/ucsc_hg19.fa"". BAM_CHR1=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr1.bam"". BAM_CHR20=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr20.bam"". BAM_CHR21=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr21.bam"". TRUTH_VCF=""${DATA_DIR}/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_PGandRTGphasetransfer_chrs_FIXED.vcf.gz"". TRUTH_BED=""${DATA_DIR}/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_nosomaticdel_chr.bed"". N_SHARDS=16. ```. ```bash. mkdir -p ""${OUTPUT_DIR}"". mkd",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:927,integrability,VERSION,VERSION,927,"# Follow https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-training-case-study.md. I got a machine:. ```bash. host=""${USER}-deepvariant-vm"". zone=""us-west1-b"". gcloud compute instances create ${host} \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --maintenance-policy ""TERMINATE"" \. --accelerator=type=nvidia-tesla-p100,count=1 \. --image-family ""ubuntu-2004-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""n1-standard-16"" \. --boot-disk-size ""300"" \. --zone ""${zone}"" \. --min-cpu-platform ""Intel Skylake"". ```. I sshed into my machine:. ```bash. gcloud compute ssh pichuan-deepvariant-vm --zone us-west1-b. ```. I ran this with my own `YOUR_PROJECT` and `OUTPUT_GCS_BUCKET` setting. Then the following is basically just copy/paste from the doc:. ```. BUCKET=""gs://deepvariant"". VERSION=""1.6.1"". DOCKER_IMAGE=""google/deepvariant:${VERSION}"". MODEL_BUCKET=""${BUCKET}/models/DeepVariant/${VERSION}/DeepVariant-inception_v3-${VERSION}+data-wgs_standard"". GCS_PRETRAINED_WGS_MODEL=""${MODEL_BUCKET}/model.ckpt"". OUTPUT_BUCKET=""${OUTPUT_GCS_BUCKET}/customized_training"". TRAINING_DIR=""${OUTPUT_BUCKET}/training_dir"". BASE=""${HOME}/training-case-study"". DATA_BUCKET=gs://deepvariant/training-case-study/BGISEQ-HG001. INPUT_DIR=""${BASE}/input"". BIN_DIR=""${INPUT_DIR}/bin"". DATA_DIR=""${INPUT_DIR}/data"". OUTPUT_DIR=""${BASE}/output"". LOG_DIR=""${OUTPUT_DIR}/logs"". SHUFFLE_SCRIPT_DIR=""${HOME}/deepvariant/tools"". REF=""${DATA_DIR}/ucsc_hg19.fa"". BAM_CHR1=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr1.bam"". BAM_CHR20=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr20.bam"". BAM_CHR21=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr21.bam"". TRUTH_VCF=""${DATA_DIR}/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_PGandRTGphasetransfer_chrs_FIXED.vcf.gz"". TRUTH_BED=""${DATA_DIR}/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_nosomaticdel_chr.bed"". N_SHARDS=16. ```. ```bash. mkdir -p ""${OUTPUT_DIR}"". mkd",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:963,integrability,VERSION,VERSION,963,"# Follow https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-training-case-study.md. I got a machine:. ```bash. host=""${USER}-deepvariant-vm"". zone=""us-west1-b"". gcloud compute instances create ${host} \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --maintenance-policy ""TERMINATE"" \. --accelerator=type=nvidia-tesla-p100,count=1 \. --image-family ""ubuntu-2004-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""n1-standard-16"" \. --boot-disk-size ""300"" \. --zone ""${zone}"" \. --min-cpu-platform ""Intel Skylake"". ```. I sshed into my machine:. ```bash. gcloud compute ssh pichuan-deepvariant-vm --zone us-west1-b. ```. I ran this with my own `YOUR_PROJECT` and `OUTPUT_GCS_BUCKET` setting. Then the following is basically just copy/paste from the doc:. ```. BUCKET=""gs://deepvariant"". VERSION=""1.6.1"". DOCKER_IMAGE=""google/deepvariant:${VERSION}"". MODEL_BUCKET=""${BUCKET}/models/DeepVariant/${VERSION}/DeepVariant-inception_v3-${VERSION}+data-wgs_standard"". GCS_PRETRAINED_WGS_MODEL=""${MODEL_BUCKET}/model.ckpt"". OUTPUT_BUCKET=""${OUTPUT_GCS_BUCKET}/customized_training"". TRAINING_DIR=""${OUTPUT_BUCKET}/training_dir"". BASE=""${HOME}/training-case-study"". DATA_BUCKET=gs://deepvariant/training-case-study/BGISEQ-HG001. INPUT_DIR=""${BASE}/input"". BIN_DIR=""${INPUT_DIR}/bin"". DATA_DIR=""${INPUT_DIR}/data"". OUTPUT_DIR=""${BASE}/output"". LOG_DIR=""${OUTPUT_DIR}/logs"". SHUFFLE_SCRIPT_DIR=""${HOME}/deepvariant/tools"". REF=""${DATA_DIR}/ucsc_hg19.fa"". BAM_CHR1=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr1.bam"". BAM_CHR20=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr20.bam"". BAM_CHR21=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr21.bam"". TRUTH_VCF=""${DATA_DIR}/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_PGandRTGphasetransfer_chrs_FIXED.vcf.gz"". TRUTH_BED=""${DATA_DIR}/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_nosomaticdel_chr.bed"". N_SHARDS=16. ```. ```bash. mkdir -p ""${OUTPUT_DIR}"". mkd",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:2774,integrability,buffer,buffer,2774,"Gphasetransfer_chrs_FIXED.vcf.gz"". TRUTH_BED=""${DATA_DIR}/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_nosomaticdel_chr.bed"". N_SHARDS=16. ```. ```bash. mkdir -p ""${OUTPUT_DIR}"". mkdir -p ""${BIN_DIR}"". mkdir -p ""${DATA_DIR}"". mkdir -p ""${LOG_DIR}"". ```. ```bash. gsutil -m cp ${DATA_BUCKET}/BGISEQ_PE100_NA12878.sorted.chr*.bam* ""${DATA_DIR}"". gsutil -m cp -r ""${DATA_BUCKET}/ucsc_hg19.fa*"" ""${DATA_DIR}"". gsutil -m cp -r ""${DATA_BUCKET}/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_*"" ""${DATA_DIR}"". ```. ```bash. sudo apt -y update. sudo apt -y install parallel. curl -O https://raw.githubusercontent.com/google/deepvariant/r1.6.1/scripts/install_nvidia_docker.sh. bash -x install_nvidia_docker.sh. ```. ```bash. sudo docker pull ${DOCKER_IMAGE} # Standard CPU Docker Image. sudo docker pull ${DOCKER_IMAGE}-gpu # GPU-enabled Docker image. ```. ```bash. ( time seq 0 $((N_SHARDS-1)) | \. parallel --halt 2 --line-buffer \. sudo docker run \. -v ${HOME}:${HOME} \. ${DOCKER_IMAGE} \. make_examples \. --mode training \. --ref ""${REF}"" \. --reads ""${BAM_CHR1}"" \. --examples ""${OUTPUT_DIR}/training_set.with_label.tfrecord@${N_SHARDS}.gz"" \. --truth_variants ""${TRUTH_VCF}"" \. --confident_regions ""${TRUTH_BED}"" \. --task {} \. --regions ""'chr1'"" \. --channels ""insert_size"" \. ) 2>&1 | tee ""${LOG_DIR}/training_set.with_label.make_examples.log"". ```. This took `20m0.146s`. ```. $ cat ""${OUTPUT_DIR}/training_set.with_label.tfrecord-00000-of-00016.gz.example_info.json"". {""version"": ""1.6.0"", ""shape"": [100, 221, 7], ""channels"": [1, 2, 3, 4, 5, 6, 19]}. ```. ```bash. gsutil -m cp ${OUTPUT_DIR}/training_set.with_label.tfrecord-?????-of-00016.gz* \. ${OUTPUT_BUCKET}. ```. ```bash. ( time seq 0 $((N_SHARDS-1)) | \. parallel --halt 2 --line-buffer \. sudo docker run \. -v /home/${USER}:/home/${USER} \. ${DOCKER_IMAGE} \. make_examples \. --mode training \. --ref ""${REF}"" \. --reads ""${BAM_CHR21}"" \. --examples ""${O",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:3333,integrability,version,version,3333,"conf_*"" ""${DATA_DIR}"". ```. ```bash. sudo apt -y update. sudo apt -y install parallel. curl -O https://raw.githubusercontent.com/google/deepvariant/r1.6.1/scripts/install_nvidia_docker.sh. bash -x install_nvidia_docker.sh. ```. ```bash. sudo docker pull ${DOCKER_IMAGE} # Standard CPU Docker Image. sudo docker pull ${DOCKER_IMAGE}-gpu # GPU-enabled Docker image. ```. ```bash. ( time seq 0 $((N_SHARDS-1)) | \. parallel --halt 2 --line-buffer \. sudo docker run \. -v ${HOME}:${HOME} \. ${DOCKER_IMAGE} \. make_examples \. --mode training \. --ref ""${REF}"" \. --reads ""${BAM_CHR1}"" \. --examples ""${OUTPUT_DIR}/training_set.with_label.tfrecord@${N_SHARDS}.gz"" \. --truth_variants ""${TRUTH_VCF}"" \. --confident_regions ""${TRUTH_BED}"" \. --task {} \. --regions ""'chr1'"" \. --channels ""insert_size"" \. ) 2>&1 | tee ""${LOG_DIR}/training_set.with_label.make_examples.log"". ```. This took `20m0.146s`. ```. $ cat ""${OUTPUT_DIR}/training_set.with_label.tfrecord-00000-of-00016.gz.example_info.json"". {""version"": ""1.6.0"", ""shape"": [100, 221, 7], ""channels"": [1, 2, 3, 4, 5, 6, 19]}. ```. ```bash. gsutil -m cp ${OUTPUT_DIR}/training_set.with_label.tfrecord-?????-of-00016.gz* \. ${OUTPUT_BUCKET}. ```. ```bash. ( time seq 0 $((N_SHARDS-1)) | \. parallel --halt 2 --line-buffer \. sudo docker run \. -v /home/${USER}:/home/${USER} \. ${DOCKER_IMAGE} \. make_examples \. --mode training \. --ref ""${REF}"" \. --reads ""${BAM_CHR21}"" \. --examples ""${OUTPUT_DIR}/validation_set.with_label.tfrecord@${N_SHARDS}.gz"" \. --truth_variants ""${TRUTH_VCF}"" \. --confident_regions ""${TRUTH_BED}"" \. --task {} \. --regions ""'chr21'"" \. --channels ""insert_size"" \. ) 2>&1 | tee ""${LOG_DIR}/validation_set.with_label.make_examples.log"". ```. This took `5m25.905s`. ```bash. gsutil -m cp ${OUTPUT_DIR}/validation_set.with_label.tfrecord-?????-of-00016.gz* \. ${OUTPUT_BUCKET}. ```. # This parts starts shuffling... ```bash. sudo apt install -y python3.8-venv. # Create a virtualenv. python3 -m venv beam. # Activate the virtu",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:3600,integrability,buffer,buffer,3600,"GE} # Standard CPU Docker Image. sudo docker pull ${DOCKER_IMAGE}-gpu # GPU-enabled Docker image. ```. ```bash. ( time seq 0 $((N_SHARDS-1)) | \. parallel --halt 2 --line-buffer \. sudo docker run \. -v ${HOME}:${HOME} \. ${DOCKER_IMAGE} \. make_examples \. --mode training \. --ref ""${REF}"" \. --reads ""${BAM_CHR1}"" \. --examples ""${OUTPUT_DIR}/training_set.with_label.tfrecord@${N_SHARDS}.gz"" \. --truth_variants ""${TRUTH_VCF}"" \. --confident_regions ""${TRUTH_BED}"" \. --task {} \. --regions ""'chr1'"" \. --channels ""insert_size"" \. ) 2>&1 | tee ""${LOG_DIR}/training_set.with_label.make_examples.log"". ```. This took `20m0.146s`. ```. $ cat ""${OUTPUT_DIR}/training_set.with_label.tfrecord-00000-of-00016.gz.example_info.json"". {""version"": ""1.6.0"", ""shape"": [100, 221, 7], ""channels"": [1, 2, 3, 4, 5, 6, 19]}. ```. ```bash. gsutil -m cp ${OUTPUT_DIR}/training_set.with_label.tfrecord-?????-of-00016.gz* \. ${OUTPUT_BUCKET}. ```. ```bash. ( time seq 0 $((N_SHARDS-1)) | \. parallel --halt 2 --line-buffer \. sudo docker run \. -v /home/${USER}:/home/${USER} \. ${DOCKER_IMAGE} \. make_examples \. --mode training \. --ref ""${REF}"" \. --reads ""${BAM_CHR21}"" \. --examples ""${OUTPUT_DIR}/validation_set.with_label.tfrecord@${N_SHARDS}.gz"" \. --truth_variants ""${TRUTH_VCF}"" \. --confident_regions ""${TRUTH_BED}"" \. --task {} \. --regions ""'chr21'"" \. --channels ""insert_size"" \. ) 2>&1 | tee ""${LOG_DIR}/validation_set.with_label.make_examples.log"". ```. This took `5m25.905s`. ```bash. gsutil -m cp ${OUTPUT_DIR}/validation_set.with_label.tfrecord-?????-of-00016.gz* \. ${OUTPUT_BUCKET}. ```. # This parts starts shuffling... ```bash. sudo apt install -y python3.8-venv. # Create a virtualenv. python3 -m venv beam. # Activate the virtualenv. . beam/bin/activate. ```. ```bash. mkdir -p ${SHUFFLE_SCRIPT_DIR}. wget https://raw.githubusercontent.com/google/deepvariant/r1.6.1/tools/shuffle_tfrecords_beam.py -O ${SHUFFLE_SCRIPT_DIR}/shuffle_tfrecords_beam.py. ```. ```bash. sudo apt -y update && sudo ap",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:5567,integrability,version,versions,5567,"h. sudo apt -y update && sudo apt -y install python3-pip. pip3 install --upgrade pip. pip3 install setuptools --upgrade. pip3 install apache_beam[gcp]==2.50.0 # 2.51.0 didn't work in my run. pip3 install tensorflow # For parsing tf.Example in shuffle_tfrecords_beam.py. ```. ```bash. time python3 ${SHUFFLE_SCRIPT_DIR}/shuffle_tfrecords_beam.py \. --project=""${YOUR_PROJECT}"" \. --input_pattern_list=""${OUTPUT_BUCKET}""/training_set.with_label.tfrecord-?????-of-00016.gz \. --output_pattern_prefix=""${OUTPUT_BUCKET}/training_set.with_label.shuffled"" \. --output_dataset_name=""HG001"" \. --output_dataset_config_pbtxt=""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt"" \. --job_name=shuffle-tfrecords \. --runner=DataflowRunner \. --staging_location=""${OUTPUT_BUCKET}/staging"" \. --temp_location=""${OUTPUT_BUCKET}/tempdir"" \. --save_main_session \. --region us-east1. ```. This worked for me. It took `15m32.746s`. I didn't get the numpy issue. So I decided to run `pip3 freeze` to get my current versions so you can compare:. ```bash. $ pip3 freeze. absl-py==2.1.0. apache-beam==2.50.0. astunparse==1.6.3. cachetools==5.3.3. certifi==2024.2.2. charset-normalizer==3.3.2. cloudpickle==2.2.1. crcmod==1.7. Deprecated==1.2.14. dill==0.3.1.1. dnspython==2.6.1. docopt==0.6.2. fastavro==1.9.4. fasteners==0.19. flatbuffers==24.3.7. gast==0.4.0. google-api-core==2.17.1. google-apitools==0.5.31. google-auth==2.28.2. google-auth-httplib2==0.1.1. google-auth-oauthlib==1.0.0. google-cloud-aiplatform==1.44.0. google-cloud-bigquery==3.19.0. google-cloud-bigquery-storage==2.24.0. google-cloud-bigtable==2.23.0. google-cloud-core==2.4.1. google-cloud-datastore==2.19.0. google-cloud-dlp==3.16.0. google-cloud-language==2.13.3. google-cloud-pubsub==2.20.2. google-cloud-pubsublite==1.9.0. google-cloud-recommendations-ai==0.10.10. google-cloud-resource-manager==1.12.3. google-cloud-spanner==3.44.0. google-cloud-storage==2.16.0. google-cloud-videointelligence==2.13.3. google-cloud-vision==3.7.2. google-crc32",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:5918,integrability,api,api-core,5918,"roject=""${YOUR_PROJECT}"" \. --input_pattern_list=""${OUTPUT_BUCKET}""/training_set.with_label.tfrecord-?????-of-00016.gz \. --output_pattern_prefix=""${OUTPUT_BUCKET}/training_set.with_label.shuffled"" \. --output_dataset_name=""HG001"" \. --output_dataset_config_pbtxt=""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt"" \. --job_name=shuffle-tfrecords \. --runner=DataflowRunner \. --staging_location=""${OUTPUT_BUCKET}/staging"" \. --temp_location=""${OUTPUT_BUCKET}/tempdir"" \. --save_main_session \. --region us-east1. ```. This worked for me. It took `15m32.746s`. I didn't get the numpy issue. So I decided to run `pip3 freeze` to get my current versions so you can compare:. ```bash. $ pip3 freeze. absl-py==2.1.0. apache-beam==2.50.0. astunparse==1.6.3. cachetools==5.3.3. certifi==2024.2.2. charset-normalizer==3.3.2. cloudpickle==2.2.1. crcmod==1.7. Deprecated==1.2.14. dill==0.3.1.1. dnspython==2.6.1. docopt==0.6.2. fastavro==1.9.4. fasteners==0.19. flatbuffers==24.3.7. gast==0.4.0. google-api-core==2.17.1. google-apitools==0.5.31. google-auth==2.28.2. google-auth-httplib2==0.1.1. google-auth-oauthlib==1.0.0. google-cloud-aiplatform==1.44.0. google-cloud-bigquery==3.19.0. google-cloud-bigquery-storage==2.24.0. google-cloud-bigtable==2.23.0. google-cloud-core==2.4.1. google-cloud-datastore==2.19.0. google-cloud-dlp==3.16.0. google-cloud-language==2.13.3. google-cloud-pubsub==2.20.2. google-cloud-pubsublite==1.9.0. google-cloud-recommendations-ai==0.10.10. google-cloud-resource-manager==1.12.3. google-cloud-spanner==3.44.0. google-cloud-storage==2.16.0. google-cloud-videointelligence==2.13.3. google-cloud-vision==3.7.2. google-crc32c==1.5.0. google-pasta==0.2.0. google-resumable-media==2.7.0. googleapis-common-protos==1.63.0. grpc-google-iam-v1==0.13.0. grpc-interceptor==0.15.4. grpcio==1.62.1. grpcio-status==1.62.1. h5py==3.10.0. hdfs==2.7.3. httplib2==0.22.0. idna==3.6. importlib_metadata==7.0.2. keras==2.13.1. libclang==18.1.1. Markdown==3.6. MarkupSafe==2.1.5. numpy==1.24.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:5943,integrability,api,apitools,5943,"\. --input_pattern_list=""${OUTPUT_BUCKET}""/training_set.with_label.tfrecord-?????-of-00016.gz \. --output_pattern_prefix=""${OUTPUT_BUCKET}/training_set.with_label.shuffled"" \. --output_dataset_name=""HG001"" \. --output_dataset_config_pbtxt=""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt"" \. --job_name=shuffle-tfrecords \. --runner=DataflowRunner \. --staging_location=""${OUTPUT_BUCKET}/staging"" \. --temp_location=""${OUTPUT_BUCKET}/tempdir"" \. --save_main_session \. --region us-east1. ```. This worked for me. It took `15m32.746s`. I didn't get the numpy issue. So I decided to run `pip3 freeze` to get my current versions so you can compare:. ```bash. $ pip3 freeze. absl-py==2.1.0. apache-beam==2.50.0. astunparse==1.6.3. cachetools==5.3.3. certifi==2024.2.2. charset-normalizer==3.3.2. cloudpickle==2.2.1. crcmod==1.7. Deprecated==1.2.14. dill==0.3.1.1. dnspython==2.6.1. docopt==0.6.2. fastavro==1.9.4. fasteners==0.19. flatbuffers==24.3.7. gast==0.4.0. google-api-core==2.17.1. google-apitools==0.5.31. google-auth==2.28.2. google-auth-httplib2==0.1.1. google-auth-oauthlib==1.0.0. google-cloud-aiplatform==1.44.0. google-cloud-bigquery==3.19.0. google-cloud-bigquery-storage==2.24.0. google-cloud-bigtable==2.23.0. google-cloud-core==2.4.1. google-cloud-datastore==2.19.0. google-cloud-dlp==3.16.0. google-cloud-language==2.13.3. google-cloud-pubsub==2.20.2. google-cloud-pubsublite==1.9.0. google-cloud-recommendations-ai==0.10.10. google-cloud-resource-manager==1.12.3. google-cloud-spanner==3.44.0. google-cloud-storage==2.16.0. google-cloud-videointelligence==2.13.3. google-cloud-vision==3.7.2. google-crc32c==1.5.0. google-pasta==0.2.0. google-resumable-media==2.7.0. googleapis-common-protos==1.63.0. grpc-google-iam-v1==0.13.0. grpc-interceptor==0.15.4. grpcio==1.62.1. grpcio-status==1.62.1. h5py==3.10.0. hdfs==2.7.3. httplib2==0.22.0. idna==3.6. importlib_metadata==7.0.2. keras==2.13.1. libclang==18.1.1. Markdown==3.6. MarkupSafe==2.1.5. numpy==1.24.3. oauth2client==4.1.3. o",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:6302,integrability,pub,pubsub,6302,"aging_location=""${OUTPUT_BUCKET}/staging"" \. --temp_location=""${OUTPUT_BUCKET}/tempdir"" \. --save_main_session \. --region us-east1. ```. This worked for me. It took `15m32.746s`. I didn't get the numpy issue. So I decided to run `pip3 freeze` to get my current versions so you can compare:. ```bash. $ pip3 freeze. absl-py==2.1.0. apache-beam==2.50.0. astunparse==1.6.3. cachetools==5.3.3. certifi==2024.2.2. charset-normalizer==3.3.2. cloudpickle==2.2.1. crcmod==1.7. Deprecated==1.2.14. dill==0.3.1.1. dnspython==2.6.1. docopt==0.6.2. fastavro==1.9.4. fasteners==0.19. flatbuffers==24.3.7. gast==0.4.0. google-api-core==2.17.1. google-apitools==0.5.31. google-auth==2.28.2. google-auth-httplib2==0.1.1. google-auth-oauthlib==1.0.0. google-cloud-aiplatform==1.44.0. google-cloud-bigquery==3.19.0. google-cloud-bigquery-storage==2.24.0. google-cloud-bigtable==2.23.0. google-cloud-core==2.4.1. google-cloud-datastore==2.19.0. google-cloud-dlp==3.16.0. google-cloud-language==2.13.3. google-cloud-pubsub==2.20.2. google-cloud-pubsublite==1.9.0. google-cloud-recommendations-ai==0.10.10. google-cloud-resource-manager==1.12.3. google-cloud-spanner==3.44.0. google-cloud-storage==2.16.0. google-cloud-videointelligence==2.13.3. google-cloud-vision==3.7.2. google-crc32c==1.5.0. google-pasta==0.2.0. google-resumable-media==2.7.0. googleapis-common-protos==1.63.0. grpc-google-iam-v1==0.13.0. grpc-interceptor==0.15.4. grpcio==1.62.1. grpcio-status==1.62.1. h5py==3.10.0. hdfs==2.7.3. httplib2==0.22.0. idna==3.6. importlib_metadata==7.0.2. keras==2.13.1. libclang==18.1.1. Markdown==3.6. MarkupSafe==2.1.5. numpy==1.24.3. oauth2client==4.1.3. oauthlib==3.2.2. objsize==0.6.1. opt-einsum==3.3.0. orjson==3.9.15. overrides==7.7.0. packaging==24.0. pkg_resources==0.0.0. proto-plus==1.23.0. protobuf==4.23.4. pyarrow==11.0.0. pyasn1==0.5.1. pyasn1-modules==0.3.0. pydot==1.4.2. pymongo==4.6.2. pyparsing==3.1.2. python-dateutil==2.9.0.post0. pytz==2024.1. regex==2023.12.25. requests==2.31.0. requests-oau",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:6331,integrability,pub,pubsublite,6331,"}/staging"" \. --temp_location=""${OUTPUT_BUCKET}/tempdir"" \. --save_main_session \. --region us-east1. ```. This worked for me. It took `15m32.746s`. I didn't get the numpy issue. So I decided to run `pip3 freeze` to get my current versions so you can compare:. ```bash. $ pip3 freeze. absl-py==2.1.0. apache-beam==2.50.0. astunparse==1.6.3. cachetools==5.3.3. certifi==2024.2.2. charset-normalizer==3.3.2. cloudpickle==2.2.1. crcmod==1.7. Deprecated==1.2.14. dill==0.3.1.1. dnspython==2.6.1. docopt==0.6.2. fastavro==1.9.4. fasteners==0.19. flatbuffers==24.3.7. gast==0.4.0. google-api-core==2.17.1. google-apitools==0.5.31. google-auth==2.28.2. google-auth-httplib2==0.1.1. google-auth-oauthlib==1.0.0. google-cloud-aiplatform==1.44.0. google-cloud-bigquery==3.19.0. google-cloud-bigquery-storage==2.24.0. google-cloud-bigtable==2.23.0. google-cloud-core==2.4.1. google-cloud-datastore==2.19.0. google-cloud-dlp==3.16.0. google-cloud-language==2.13.3. google-cloud-pubsub==2.20.2. google-cloud-pubsublite==1.9.0. google-cloud-recommendations-ai==0.10.10. google-cloud-resource-manager==1.12.3. google-cloud-spanner==3.44.0. google-cloud-storage==2.16.0. google-cloud-videointelligence==2.13.3. google-cloud-vision==3.7.2. google-crc32c==1.5.0. google-pasta==0.2.0. google-resumable-media==2.7.0. googleapis-common-protos==1.63.0. grpc-google-iam-v1==0.13.0. grpc-interceptor==0.15.4. grpcio==1.62.1. grpcio-status==1.62.1. h5py==3.10.0. hdfs==2.7.3. httplib2==0.22.0. idna==3.6. importlib_metadata==7.0.2. keras==2.13.1. libclang==18.1.1. Markdown==3.6. MarkupSafe==2.1.5. numpy==1.24.3. oauth2client==4.1.3. oauthlib==3.2.2. objsize==0.6.1. opt-einsum==3.3.0. orjson==3.9.15. overrides==7.7.0. packaging==24.0. pkg_resources==0.0.0. proto-plus==1.23.0. protobuf==4.23.4. pyarrow==11.0.0. pyasn1==0.5.1. pyasn1-modules==0.3.0. pydot==1.4.2. pymongo==4.6.2. pyparsing==3.1.2. python-dateutil==2.9.0.post0. pytz==2024.1. regex==2023.12.25. requests==2.31.0. requests-oauthlib==1.4.0. rsa==4.9. shapely",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:7593,integrability,wrap,wrapt,7593,"rmalizer==3.3.2. cloudpickle==2.2.1. crcmod==1.7. Deprecated==1.2.14. dill==0.3.1.1. dnspython==2.6.1. docopt==0.6.2. fastavro==1.9.4. fasteners==0.19. flatbuffers==24.3.7. gast==0.4.0. google-api-core==2.17.1. google-apitools==0.5.31. google-auth==2.28.2. google-auth-httplib2==0.1.1. google-auth-oauthlib==1.0.0. google-cloud-aiplatform==1.44.0. google-cloud-bigquery==3.19.0. google-cloud-bigquery-storage==2.24.0. google-cloud-bigtable==2.23.0. google-cloud-core==2.4.1. google-cloud-datastore==2.19.0. google-cloud-dlp==3.16.0. google-cloud-language==2.13.3. google-cloud-pubsub==2.20.2. google-cloud-pubsublite==1.9.0. google-cloud-recommendations-ai==0.10.10. google-cloud-resource-manager==1.12.3. google-cloud-spanner==3.44.0. google-cloud-storage==2.16.0. google-cloud-videointelligence==2.13.3. google-cloud-vision==3.7.2. google-crc32c==1.5.0. google-pasta==0.2.0. google-resumable-media==2.7.0. googleapis-common-protos==1.63.0. grpc-google-iam-v1==0.13.0. grpc-interceptor==0.15.4. grpcio==1.62.1. grpcio-status==1.62.1. h5py==3.10.0. hdfs==2.7.3. httplib2==0.22.0. idna==3.6. importlib_metadata==7.0.2. keras==2.13.1. libclang==18.1.1. Markdown==3.6. MarkupSafe==2.1.5. numpy==1.24.3. oauth2client==4.1.3. oauthlib==3.2.2. objsize==0.6.1. opt-einsum==3.3.0. orjson==3.9.15. overrides==7.7.0. packaging==24.0. pkg_resources==0.0.0. proto-plus==1.23.0. protobuf==4.23.4. pyarrow==11.0.0. pyasn1==0.5.1. pyasn1-modules==0.3.0. pydot==1.4.2. pymongo==4.6.2. pyparsing==3.1.2. python-dateutil==2.9.0.post0. pytz==2024.1. regex==2023.12.25. requests==2.31.0. requests-oauthlib==1.4.0. rsa==4.9. shapely==2.0.3. six==1.16.0. sqlparse==0.4.4. tensorboard==2.13.0. tensorboard-data-server==0.7.2. tensorflow==2.13.1. tensorflow-estimator==2.13.0. tensorflow-io-gcs-filesystem==0.34.0. termcolor==2.4.0. typing_extensions==4.5.0. urllib3==2.2.1. Werkzeug==3.0.1. wrapt==1.16.0. zipp==3.18.1. zstandard==0.22.0. ```. ---. @helizabeth1103 , can you see which version might be different from yours?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:7687,integrability,version,version,7687,"rmalizer==3.3.2. cloudpickle==2.2.1. crcmod==1.7. Deprecated==1.2.14. dill==0.3.1.1. dnspython==2.6.1. docopt==0.6.2. fastavro==1.9.4. fasteners==0.19. flatbuffers==24.3.7. gast==0.4.0. google-api-core==2.17.1. google-apitools==0.5.31. google-auth==2.28.2. google-auth-httplib2==0.1.1. google-auth-oauthlib==1.0.0. google-cloud-aiplatform==1.44.0. google-cloud-bigquery==3.19.0. google-cloud-bigquery-storage==2.24.0. google-cloud-bigtable==2.23.0. google-cloud-core==2.4.1. google-cloud-datastore==2.19.0. google-cloud-dlp==3.16.0. google-cloud-language==2.13.3. google-cloud-pubsub==2.20.2. google-cloud-pubsublite==1.9.0. google-cloud-recommendations-ai==0.10.10. google-cloud-resource-manager==1.12.3. google-cloud-spanner==3.44.0. google-cloud-storage==2.16.0. google-cloud-videointelligence==2.13.3. google-cloud-vision==3.7.2. google-crc32c==1.5.0. google-pasta==0.2.0. google-resumable-media==2.7.0. googleapis-common-protos==1.63.0. grpc-google-iam-v1==0.13.0. grpc-interceptor==0.15.4. grpcio==1.62.1. grpcio-status==1.62.1. h5py==3.10.0. hdfs==2.7.3. httplib2==0.22.0. idna==3.6. importlib_metadata==7.0.2. keras==2.13.1. libclang==18.1.1. Markdown==3.6. MarkupSafe==2.1.5. numpy==1.24.3. oauth2client==4.1.3. oauthlib==3.2.2. objsize==0.6.1. opt-einsum==3.3.0. orjson==3.9.15. overrides==7.7.0. packaging==24.0. pkg_resources==0.0.0. proto-plus==1.23.0. protobuf==4.23.4. pyarrow==11.0.0. pyasn1==0.5.1. pyasn1-modules==0.3.0. pydot==1.4.2. pymongo==4.6.2. pyparsing==3.1.2. python-dateutil==2.9.0.post0. pytz==2024.1. regex==2023.12.25. requests==2.31.0. requests-oauthlib==1.4.0. rsa==4.9. shapely==2.0.3. six==1.16.0. sqlparse==0.4.4. tensorboard==2.13.0. tensorboard-data-server==0.7.2. tensorflow==2.13.1. tensorflow-estimator==2.13.0. tensorflow-io-gcs-filesystem==0.34.0. termcolor==2.4.0. typing_extensions==4.5.0. urllib3==2.2.1. Werkzeug==3.0.1. wrapt==1.16.0. zipp==3.18.1. zstandard==0.22.0. ```. ---. @helizabeth1103 , can you see which version might be different from yours?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:260,interoperability,platform,platform,260,"# Follow https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-training-case-study.md. I got a machine:. ```bash. host=""${USER}-deepvariant-vm"". zone=""us-west1-b"". gcloud compute instances create ${host} \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --maintenance-policy ""TERMINATE"" \. --accelerator=type=nvidia-tesla-p100,count=1 \. --image-family ""ubuntu-2004-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""n1-standard-16"" \. --boot-disk-size ""300"" \. --zone ""${zone}"" \. --min-cpu-platform ""Intel Skylake"". ```. I sshed into my machine:. ```bash. gcloud compute ssh pichuan-deepvariant-vm --zone us-west1-b. ```. I ran this with my own `YOUR_PROJECT` and `OUTPUT_GCS_BUCKET` setting. Then the following is basically just copy/paste from the doc:. ```. BUCKET=""gs://deepvariant"". VERSION=""1.6.1"". DOCKER_IMAGE=""google/deepvariant:${VERSION}"". MODEL_BUCKET=""${BUCKET}/models/DeepVariant/${VERSION}/DeepVariant-inception_v3-${VERSION}+data-wgs_standard"". GCS_PRETRAINED_WGS_MODEL=""${MODEL_BUCKET}/model.ckpt"". OUTPUT_BUCKET=""${OUTPUT_GCS_BUCKET}/customized_training"". TRAINING_DIR=""${OUTPUT_BUCKET}/training_dir"". BASE=""${HOME}/training-case-study"". DATA_BUCKET=gs://deepvariant/training-case-study/BGISEQ-HG001. INPUT_DIR=""${BASE}/input"". BIN_DIR=""${INPUT_DIR}/bin"". DATA_DIR=""${INPUT_DIR}/data"". OUTPUT_DIR=""${BASE}/output"". LOG_DIR=""${OUTPUT_DIR}/logs"". SHUFFLE_SCRIPT_DIR=""${HOME}/deepvariant/tools"". REF=""${DATA_DIR}/ucsc_hg19.fa"". BAM_CHR1=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr1.bam"". BAM_CHR20=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr20.bam"". BAM_CHR21=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr21.bam"". TRUTH_VCF=""${DATA_DIR}/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_PGandRTGphasetransfer_chrs_FIXED.vcf.gz"". TRUTH_BED=""${DATA_DIR}/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_nosomaticdel_chr.bed"". N_SHARDS=16. ```. ```bash. mkdir -p ""${OUTPUT_DIR}"". mkd",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:449,interoperability,standard,standard-,449,"# Follow https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-training-case-study.md. I got a machine:. ```bash. host=""${USER}-deepvariant-vm"". zone=""us-west1-b"". gcloud compute instances create ${host} \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --maintenance-policy ""TERMINATE"" \. --accelerator=type=nvidia-tesla-p100,count=1 \. --image-family ""ubuntu-2004-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""n1-standard-16"" \. --boot-disk-size ""300"" \. --zone ""${zone}"" \. --min-cpu-platform ""Intel Skylake"". ```. I sshed into my machine:. ```bash. gcloud compute ssh pichuan-deepvariant-vm --zone us-west1-b. ```. I ran this with my own `YOUR_PROJECT` and `OUTPUT_GCS_BUCKET` setting. Then the following is basically just copy/paste from the doc:. ```. BUCKET=""gs://deepvariant"". VERSION=""1.6.1"". DOCKER_IMAGE=""google/deepvariant:${VERSION}"". MODEL_BUCKET=""${BUCKET}/models/DeepVariant/${VERSION}/DeepVariant-inception_v3-${VERSION}+data-wgs_standard"". GCS_PRETRAINED_WGS_MODEL=""${MODEL_BUCKET}/model.ckpt"". OUTPUT_BUCKET=""${OUTPUT_GCS_BUCKET}/customized_training"". TRAINING_DIR=""${OUTPUT_BUCKET}/training_dir"". BASE=""${HOME}/training-case-study"". DATA_BUCKET=gs://deepvariant/training-case-study/BGISEQ-HG001. INPUT_DIR=""${BASE}/input"". BIN_DIR=""${INPUT_DIR}/bin"". DATA_DIR=""${INPUT_DIR}/data"". OUTPUT_DIR=""${BASE}/output"". LOG_DIR=""${OUTPUT_DIR}/logs"". SHUFFLE_SCRIPT_DIR=""${HOME}/deepvariant/tools"". REF=""${DATA_DIR}/ucsc_hg19.fa"". BAM_CHR1=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr1.bam"". BAM_CHR20=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr20.bam"". BAM_CHR21=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr21.bam"". TRUTH_VCF=""${DATA_DIR}/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_PGandRTGphasetransfer_chrs_FIXED.vcf.gz"". TRUTH_BED=""${DATA_DIR}/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_nosomaticdel_chr.bed"". N_SHARDS=16. ```. ```bash. mkdir -p ""${OUTPUT_DIR}"". mkd",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:521,interoperability,platform,platform,521,"# Follow https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-training-case-study.md. I got a machine:. ```bash. host=""${USER}-deepvariant-vm"". zone=""us-west1-b"". gcloud compute instances create ${host} \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --maintenance-policy ""TERMINATE"" \. --accelerator=type=nvidia-tesla-p100,count=1 \. --image-family ""ubuntu-2004-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""n1-standard-16"" \. --boot-disk-size ""300"" \. --zone ""${zone}"" \. --min-cpu-platform ""Intel Skylake"". ```. I sshed into my machine:. ```bash. gcloud compute ssh pichuan-deepvariant-vm --zone us-west1-b. ```. I ran this with my own `YOUR_PROJECT` and `OUTPUT_GCS_BUCKET` setting. Then the following is basically just copy/paste from the doc:. ```. BUCKET=""gs://deepvariant"". VERSION=""1.6.1"". DOCKER_IMAGE=""google/deepvariant:${VERSION}"". MODEL_BUCKET=""${BUCKET}/models/DeepVariant/${VERSION}/DeepVariant-inception_v3-${VERSION}+data-wgs_standard"". GCS_PRETRAINED_WGS_MODEL=""${MODEL_BUCKET}/model.ckpt"". OUTPUT_BUCKET=""${OUTPUT_GCS_BUCKET}/customized_training"". TRAINING_DIR=""${OUTPUT_BUCKET}/training_dir"". BASE=""${HOME}/training-case-study"". DATA_BUCKET=gs://deepvariant/training-case-study/BGISEQ-HG001. INPUT_DIR=""${BASE}/input"". BIN_DIR=""${INPUT_DIR}/bin"". DATA_DIR=""${INPUT_DIR}/data"". OUTPUT_DIR=""${BASE}/output"". LOG_DIR=""${OUTPUT_DIR}/logs"". SHUFFLE_SCRIPT_DIR=""${HOME}/deepvariant/tools"". REF=""${DATA_DIR}/ucsc_hg19.fa"". BAM_CHR1=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr1.bam"". BAM_CHR20=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr20.bam"". BAM_CHR21=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr21.bam"". TRUTH_VCF=""${DATA_DIR}/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_PGandRTGphasetransfer_chrs_FIXED.vcf.gz"". TRUTH_BED=""${DATA_DIR}/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_nosomaticdel_chr.bed"". N_SHARDS=16. ```. ```bash. mkdir -p ""${OUTPUT_DIR}"". mkd",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:2609,interoperability,Standard,Standard,2609,"TA_DIR}/BGISEQ_PE100_NA12878.sorted.chr21.bam"". TRUTH_VCF=""${DATA_DIR}/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_PGandRTGphasetransfer_chrs_FIXED.vcf.gz"". TRUTH_BED=""${DATA_DIR}/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_nosomaticdel_chr.bed"". N_SHARDS=16. ```. ```bash. mkdir -p ""${OUTPUT_DIR}"". mkdir -p ""${BIN_DIR}"". mkdir -p ""${DATA_DIR}"". mkdir -p ""${LOG_DIR}"". ```. ```bash. gsutil -m cp ${DATA_BUCKET}/BGISEQ_PE100_NA12878.sorted.chr*.bam* ""${DATA_DIR}"". gsutil -m cp -r ""${DATA_BUCKET}/ucsc_hg19.fa*"" ""${DATA_DIR}"". gsutil -m cp -r ""${DATA_BUCKET}/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_*"" ""${DATA_DIR}"". ```. ```bash. sudo apt -y update. sudo apt -y install parallel. curl -O https://raw.githubusercontent.com/google/deepvariant/r1.6.1/scripts/install_nvidia_docker.sh. bash -x install_nvidia_docker.sh. ```. ```bash. sudo docker pull ${DOCKER_IMAGE} # Standard CPU Docker Image. sudo docker pull ${DOCKER_IMAGE}-gpu # GPU-enabled Docker image. ```. ```bash. ( time seq 0 $((N_SHARDS-1)) | \. parallel --halt 2 --line-buffer \. sudo docker run \. -v ${HOME}:${HOME} \. ${DOCKER_IMAGE} \. make_examples \. --mode training \. --ref ""${REF}"" \. --reads ""${BAM_CHR1}"" \. --examples ""${OUTPUT_DIR}/training_set.with_label.tfrecord@${N_SHARDS}.gz"" \. --truth_variants ""${TRUTH_VCF}"" \. --confident_regions ""${TRUTH_BED}"" \. --task {} \. --regions ""'chr1'"" \. --channels ""insert_size"" \. ) 2>&1 | tee ""${LOG_DIR}/training_set.with_label.make_examples.log"". ```. This took `20m0.146s`. ```. $ cat ""${OUTPUT_DIR}/training_set.with_label.tfrecord-00000-of-00016.gz.example_info.json"". {""version"": ""1.6.0"", ""shape"": [100, 221, 7], ""channels"": [1, 2, 3, 4, 5, 6, 19]}. ```. ```bash. gsutil -m cp ${OUTPUT_DIR}/training_set.with_label.tfrecord-?????-of-00016.gz* \. ${OUTPUT_BUCKET}. ```. ```bash. ( time seq 0 $((N_SHARDS-1)) | \. parallel --halt 2 --line-buffer \. sud",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:5918,interoperability,api,api-core,5918,"roject=""${YOUR_PROJECT}"" \. --input_pattern_list=""${OUTPUT_BUCKET}""/training_set.with_label.tfrecord-?????-of-00016.gz \. --output_pattern_prefix=""${OUTPUT_BUCKET}/training_set.with_label.shuffled"" \. --output_dataset_name=""HG001"" \. --output_dataset_config_pbtxt=""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt"" \. --job_name=shuffle-tfrecords \. --runner=DataflowRunner \. --staging_location=""${OUTPUT_BUCKET}/staging"" \. --temp_location=""${OUTPUT_BUCKET}/tempdir"" \. --save_main_session \. --region us-east1. ```. This worked for me. It took `15m32.746s`. I didn't get the numpy issue. So I decided to run `pip3 freeze` to get my current versions so you can compare:. ```bash. $ pip3 freeze. absl-py==2.1.0. apache-beam==2.50.0. astunparse==1.6.3. cachetools==5.3.3. certifi==2024.2.2. charset-normalizer==3.3.2. cloudpickle==2.2.1. crcmod==1.7. Deprecated==1.2.14. dill==0.3.1.1. dnspython==2.6.1. docopt==0.6.2. fastavro==1.9.4. fasteners==0.19. flatbuffers==24.3.7. gast==0.4.0. google-api-core==2.17.1. google-apitools==0.5.31. google-auth==2.28.2. google-auth-httplib2==0.1.1. google-auth-oauthlib==1.0.0. google-cloud-aiplatform==1.44.0. google-cloud-bigquery==3.19.0. google-cloud-bigquery-storage==2.24.0. google-cloud-bigtable==2.23.0. google-cloud-core==2.4.1. google-cloud-datastore==2.19.0. google-cloud-dlp==3.16.0. google-cloud-language==2.13.3. google-cloud-pubsub==2.20.2. google-cloud-pubsublite==1.9.0. google-cloud-recommendations-ai==0.10.10. google-cloud-resource-manager==1.12.3. google-cloud-spanner==3.44.0. google-cloud-storage==2.16.0. google-cloud-videointelligence==2.13.3. google-cloud-vision==3.7.2. google-crc32c==1.5.0. google-pasta==0.2.0. google-resumable-media==2.7.0. googleapis-common-protos==1.63.0. grpc-google-iam-v1==0.13.0. grpc-interceptor==0.15.4. grpcio==1.62.1. grpcio-status==1.62.1. h5py==3.10.0. hdfs==2.7.3. httplib2==0.22.0. idna==3.6. importlib_metadata==7.0.2. keras==2.13.1. libclang==18.1.1. Markdown==3.6. MarkupSafe==2.1.5. numpy==1.24.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:5943,interoperability,api,apitools,5943,"\. --input_pattern_list=""${OUTPUT_BUCKET}""/training_set.with_label.tfrecord-?????-of-00016.gz \. --output_pattern_prefix=""${OUTPUT_BUCKET}/training_set.with_label.shuffled"" \. --output_dataset_name=""HG001"" \. --output_dataset_config_pbtxt=""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt"" \. --job_name=shuffle-tfrecords \. --runner=DataflowRunner \. --staging_location=""${OUTPUT_BUCKET}/staging"" \. --temp_location=""${OUTPUT_BUCKET}/tempdir"" \. --save_main_session \. --region us-east1. ```. This worked for me. It took `15m32.746s`. I didn't get the numpy issue. So I decided to run `pip3 freeze` to get my current versions so you can compare:. ```bash. $ pip3 freeze. absl-py==2.1.0. apache-beam==2.50.0. astunparse==1.6.3. cachetools==5.3.3. certifi==2024.2.2. charset-normalizer==3.3.2. cloudpickle==2.2.1. crcmod==1.7. Deprecated==1.2.14. dill==0.3.1.1. dnspython==2.6.1. docopt==0.6.2. fastavro==1.9.4. fasteners==0.19. flatbuffers==24.3.7. gast==0.4.0. google-api-core==2.17.1. google-apitools==0.5.31. google-auth==2.28.2. google-auth-httplib2==0.1.1. google-auth-oauthlib==1.0.0. google-cloud-aiplatform==1.44.0. google-cloud-bigquery==3.19.0. google-cloud-bigquery-storage==2.24.0. google-cloud-bigtable==2.23.0. google-cloud-core==2.4.1. google-cloud-datastore==2.19.0. google-cloud-dlp==3.16.0. google-cloud-language==2.13.3. google-cloud-pubsub==2.20.2. google-cloud-pubsublite==1.9.0. google-cloud-recommendations-ai==0.10.10. google-cloud-resource-manager==1.12.3. google-cloud-spanner==3.44.0. google-cloud-storage==2.16.0. google-cloud-videointelligence==2.13.3. google-cloud-vision==3.7.2. google-crc32c==1.5.0. google-pasta==0.2.0. google-resumable-media==2.7.0. googleapis-common-protos==1.63.0. grpc-google-iam-v1==0.13.0. grpc-interceptor==0.15.4. grpcio==1.62.1. grpcio-status==1.62.1. h5py==3.10.0. hdfs==2.7.3. httplib2==0.22.0. idna==3.6. importlib_metadata==7.0.2. keras==2.13.1. libclang==18.1.1. Markdown==3.6. MarkupSafe==2.1.5. numpy==1.24.3. oauth2client==4.1.3. o",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:819,modifiability,VERSION,VERSION,819,"# Follow https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-training-case-study.md. I got a machine:. ```bash. host=""${USER}-deepvariant-vm"". zone=""us-west1-b"". gcloud compute instances create ${host} \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --maintenance-policy ""TERMINATE"" \. --accelerator=type=nvidia-tesla-p100,count=1 \. --image-family ""ubuntu-2004-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""n1-standard-16"" \. --boot-disk-size ""300"" \. --zone ""${zone}"" \. --min-cpu-platform ""Intel Skylake"". ```. I sshed into my machine:. ```bash. gcloud compute ssh pichuan-deepvariant-vm --zone us-west1-b. ```. I ran this with my own `YOUR_PROJECT` and `OUTPUT_GCS_BUCKET` setting. Then the following is basically just copy/paste from the doc:. ```. BUCKET=""gs://deepvariant"". VERSION=""1.6.1"". DOCKER_IMAGE=""google/deepvariant:${VERSION}"". MODEL_BUCKET=""${BUCKET}/models/DeepVariant/${VERSION}/DeepVariant-inception_v3-${VERSION}+data-wgs_standard"". GCS_PRETRAINED_WGS_MODEL=""${MODEL_BUCKET}/model.ckpt"". OUTPUT_BUCKET=""${OUTPUT_GCS_BUCKET}/customized_training"". TRAINING_DIR=""${OUTPUT_BUCKET}/training_dir"". BASE=""${HOME}/training-case-study"". DATA_BUCKET=gs://deepvariant/training-case-study/BGISEQ-HG001. INPUT_DIR=""${BASE}/input"". BIN_DIR=""${INPUT_DIR}/bin"". DATA_DIR=""${INPUT_DIR}/data"". OUTPUT_DIR=""${BASE}/output"". LOG_DIR=""${OUTPUT_DIR}/logs"". SHUFFLE_SCRIPT_DIR=""${HOME}/deepvariant/tools"". REF=""${DATA_DIR}/ucsc_hg19.fa"". BAM_CHR1=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr1.bam"". BAM_CHR20=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr20.bam"". BAM_CHR21=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr21.bam"". TRUTH_VCF=""${DATA_DIR}/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_PGandRTGphasetransfer_chrs_FIXED.vcf.gz"". TRUTH_BED=""${DATA_DIR}/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_nosomaticdel_chr.bed"". N_SHARDS=16. ```. ```bash. mkdir -p ""${OUTPUT_DIR}"". mkd",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:871,modifiability,VERSION,VERSION,871,"# Follow https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-training-case-study.md. I got a machine:. ```bash. host=""${USER}-deepvariant-vm"". zone=""us-west1-b"". gcloud compute instances create ${host} \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --maintenance-policy ""TERMINATE"" \. --accelerator=type=nvidia-tesla-p100,count=1 \. --image-family ""ubuntu-2004-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""n1-standard-16"" \. --boot-disk-size ""300"" \. --zone ""${zone}"" \. --min-cpu-platform ""Intel Skylake"". ```. I sshed into my machine:. ```bash. gcloud compute ssh pichuan-deepvariant-vm --zone us-west1-b. ```. I ran this with my own `YOUR_PROJECT` and `OUTPUT_GCS_BUCKET` setting. Then the following is basically just copy/paste from the doc:. ```. BUCKET=""gs://deepvariant"". VERSION=""1.6.1"". DOCKER_IMAGE=""google/deepvariant:${VERSION}"". MODEL_BUCKET=""${BUCKET}/models/DeepVariant/${VERSION}/DeepVariant-inception_v3-${VERSION}+data-wgs_standard"". GCS_PRETRAINED_WGS_MODEL=""${MODEL_BUCKET}/model.ckpt"". OUTPUT_BUCKET=""${OUTPUT_GCS_BUCKET}/customized_training"". TRAINING_DIR=""${OUTPUT_BUCKET}/training_dir"". BASE=""${HOME}/training-case-study"". DATA_BUCKET=gs://deepvariant/training-case-study/BGISEQ-HG001. INPUT_DIR=""${BASE}/input"". BIN_DIR=""${INPUT_DIR}/bin"". DATA_DIR=""${INPUT_DIR}/data"". OUTPUT_DIR=""${BASE}/output"". LOG_DIR=""${OUTPUT_DIR}/logs"". SHUFFLE_SCRIPT_DIR=""${HOME}/deepvariant/tools"". REF=""${DATA_DIR}/ucsc_hg19.fa"". BAM_CHR1=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr1.bam"". BAM_CHR20=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr20.bam"". BAM_CHR21=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr21.bam"". TRUTH_VCF=""${DATA_DIR}/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_PGandRTGphasetransfer_chrs_FIXED.vcf.gz"". TRUTH_BED=""${DATA_DIR}/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_nosomaticdel_chr.bed"". N_SHARDS=16. ```. ```bash. mkdir -p ""${OUTPUT_DIR}"". mkd",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:927,modifiability,VERSION,VERSION,927,"# Follow https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-training-case-study.md. I got a machine:. ```bash. host=""${USER}-deepvariant-vm"". zone=""us-west1-b"". gcloud compute instances create ${host} \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --maintenance-policy ""TERMINATE"" \. --accelerator=type=nvidia-tesla-p100,count=1 \. --image-family ""ubuntu-2004-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""n1-standard-16"" \. --boot-disk-size ""300"" \. --zone ""${zone}"" \. --min-cpu-platform ""Intel Skylake"". ```. I sshed into my machine:. ```bash. gcloud compute ssh pichuan-deepvariant-vm --zone us-west1-b. ```. I ran this with my own `YOUR_PROJECT` and `OUTPUT_GCS_BUCKET` setting. Then the following is basically just copy/paste from the doc:. ```. BUCKET=""gs://deepvariant"". VERSION=""1.6.1"". DOCKER_IMAGE=""google/deepvariant:${VERSION}"". MODEL_BUCKET=""${BUCKET}/models/DeepVariant/${VERSION}/DeepVariant-inception_v3-${VERSION}+data-wgs_standard"". GCS_PRETRAINED_WGS_MODEL=""${MODEL_BUCKET}/model.ckpt"". OUTPUT_BUCKET=""${OUTPUT_GCS_BUCKET}/customized_training"". TRAINING_DIR=""${OUTPUT_BUCKET}/training_dir"". BASE=""${HOME}/training-case-study"". DATA_BUCKET=gs://deepvariant/training-case-study/BGISEQ-HG001. INPUT_DIR=""${BASE}/input"". BIN_DIR=""${INPUT_DIR}/bin"". DATA_DIR=""${INPUT_DIR}/data"". OUTPUT_DIR=""${BASE}/output"". LOG_DIR=""${OUTPUT_DIR}/logs"". SHUFFLE_SCRIPT_DIR=""${HOME}/deepvariant/tools"". REF=""${DATA_DIR}/ucsc_hg19.fa"". BAM_CHR1=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr1.bam"". BAM_CHR20=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr20.bam"". BAM_CHR21=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr21.bam"". TRUTH_VCF=""${DATA_DIR}/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_PGandRTGphasetransfer_chrs_FIXED.vcf.gz"". TRUTH_BED=""${DATA_DIR}/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_nosomaticdel_chr.bed"". N_SHARDS=16. ```. ```bash. mkdir -p ""${OUTPUT_DIR}"". mkd",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:963,modifiability,VERSION,VERSION,963,"# Follow https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-training-case-study.md. I got a machine:. ```bash. host=""${USER}-deepvariant-vm"". zone=""us-west1-b"". gcloud compute instances create ${host} \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --maintenance-policy ""TERMINATE"" \. --accelerator=type=nvidia-tesla-p100,count=1 \. --image-family ""ubuntu-2004-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""n1-standard-16"" \. --boot-disk-size ""300"" \. --zone ""${zone}"" \. --min-cpu-platform ""Intel Skylake"". ```. I sshed into my machine:. ```bash. gcloud compute ssh pichuan-deepvariant-vm --zone us-west1-b. ```. I ran this with my own `YOUR_PROJECT` and `OUTPUT_GCS_BUCKET` setting. Then the following is basically just copy/paste from the doc:. ```. BUCKET=""gs://deepvariant"". VERSION=""1.6.1"". DOCKER_IMAGE=""google/deepvariant:${VERSION}"". MODEL_BUCKET=""${BUCKET}/models/DeepVariant/${VERSION}/DeepVariant-inception_v3-${VERSION}+data-wgs_standard"". GCS_PRETRAINED_WGS_MODEL=""${MODEL_BUCKET}/model.ckpt"". OUTPUT_BUCKET=""${OUTPUT_GCS_BUCKET}/customized_training"". TRAINING_DIR=""${OUTPUT_BUCKET}/training_dir"". BASE=""${HOME}/training-case-study"". DATA_BUCKET=gs://deepvariant/training-case-study/BGISEQ-HG001. INPUT_DIR=""${BASE}/input"". BIN_DIR=""${INPUT_DIR}/bin"". DATA_DIR=""${INPUT_DIR}/data"". OUTPUT_DIR=""${BASE}/output"". LOG_DIR=""${OUTPUT_DIR}/logs"". SHUFFLE_SCRIPT_DIR=""${HOME}/deepvariant/tools"". REF=""${DATA_DIR}/ucsc_hg19.fa"". BAM_CHR1=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr1.bam"". BAM_CHR20=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr20.bam"". BAM_CHR21=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr21.bam"". TRUTH_VCF=""${DATA_DIR}/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_PGandRTGphasetransfer_chrs_FIXED.vcf.gz"". TRUTH_BED=""${DATA_DIR}/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_nosomaticdel_chr.bed"". N_SHARDS=16. ```. ```bash. mkdir -p ""${OUTPUT_DIR}"". mkd",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:3333,modifiability,version,version,3333,"conf_*"" ""${DATA_DIR}"". ```. ```bash. sudo apt -y update. sudo apt -y install parallel. curl -O https://raw.githubusercontent.com/google/deepvariant/r1.6.1/scripts/install_nvidia_docker.sh. bash -x install_nvidia_docker.sh. ```. ```bash. sudo docker pull ${DOCKER_IMAGE} # Standard CPU Docker Image. sudo docker pull ${DOCKER_IMAGE}-gpu # GPU-enabled Docker image. ```. ```bash. ( time seq 0 $((N_SHARDS-1)) | \. parallel --halt 2 --line-buffer \. sudo docker run \. -v ${HOME}:${HOME} \. ${DOCKER_IMAGE} \. make_examples \. --mode training \. --ref ""${REF}"" \. --reads ""${BAM_CHR1}"" \. --examples ""${OUTPUT_DIR}/training_set.with_label.tfrecord@${N_SHARDS}.gz"" \. --truth_variants ""${TRUTH_VCF}"" \. --confident_regions ""${TRUTH_BED}"" \. --task {} \. --regions ""'chr1'"" \. --channels ""insert_size"" \. ) 2>&1 | tee ""${LOG_DIR}/training_set.with_label.make_examples.log"". ```. This took `20m0.146s`. ```. $ cat ""${OUTPUT_DIR}/training_set.with_label.tfrecord-00000-of-00016.gz.example_info.json"". {""version"": ""1.6.0"", ""shape"": [100, 221, 7], ""channels"": [1, 2, 3, 4, 5, 6, 19]}. ```. ```bash. gsutil -m cp ${OUTPUT_DIR}/training_set.with_label.tfrecord-?????-of-00016.gz* \. ${OUTPUT_BUCKET}. ```. ```bash. ( time seq 0 $((N_SHARDS-1)) | \. parallel --halt 2 --line-buffer \. sudo docker run \. -v /home/${USER}:/home/${USER} \. ${DOCKER_IMAGE} \. make_examples \. --mode training \. --ref ""${REF}"" \. --reads ""${BAM_CHR21}"" \. --examples ""${OUTPUT_DIR}/validation_set.with_label.tfrecord@${N_SHARDS}.gz"" \. --truth_variants ""${TRUTH_VCF}"" \. --confident_regions ""${TRUTH_BED}"" \. --task {} \. --regions ""'chr21'"" \. --channels ""insert_size"" \. ) 2>&1 | tee ""${LOG_DIR}/validation_set.with_label.make_examples.log"". ```. This took `5m25.905s`. ```bash. gsutil -m cp ${OUTPUT_DIR}/validation_set.with_label.tfrecord-?????-of-00016.gz* \. ${OUTPUT_BUCKET}. ```. # This parts starts shuffling... ```bash. sudo apt install -y python3.8-venv. # Create a virtualenv. python3 -m venv beam. # Activate the virtu",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:4644,modifiability,upgrad,upgrade,4644,"ome/${USER} \. ${DOCKER_IMAGE} \. make_examples \. --mode training \. --ref ""${REF}"" \. --reads ""${BAM_CHR21}"" \. --examples ""${OUTPUT_DIR}/validation_set.with_label.tfrecord@${N_SHARDS}.gz"" \. --truth_variants ""${TRUTH_VCF}"" \. --confident_regions ""${TRUTH_BED}"" \. --task {} \. --regions ""'chr21'"" \. --channels ""insert_size"" \. ) 2>&1 | tee ""${LOG_DIR}/validation_set.with_label.make_examples.log"". ```. This took `5m25.905s`. ```bash. gsutil -m cp ${OUTPUT_DIR}/validation_set.with_label.tfrecord-?????-of-00016.gz* \. ${OUTPUT_BUCKET}. ```. # This parts starts shuffling... ```bash. sudo apt install -y python3.8-venv. # Create a virtualenv. python3 -m venv beam. # Activate the virtualenv. . beam/bin/activate. ```. ```bash. mkdir -p ${SHUFFLE_SCRIPT_DIR}. wget https://raw.githubusercontent.com/google/deepvariant/r1.6.1/tools/shuffle_tfrecords_beam.py -O ${SHUFFLE_SCRIPT_DIR}/shuffle_tfrecords_beam.py. ```. ```bash. sudo apt -y update && sudo apt -y install python3-pip. pip3 install --upgrade pip. pip3 install setuptools --upgrade. pip3 install apache_beam[gcp]==2.50.0 # 2.51.0 didn't work in my run. pip3 install tensorflow # For parsing tf.Example in shuffle_tfrecords_beam.py. ```. ```bash. time python3 ${SHUFFLE_SCRIPT_DIR}/shuffle_tfrecords_beam.py \. --project=""${YOUR_PROJECT}"" \. --input_pattern_list=""${OUTPUT_BUCKET}""/training_set.with_label.tfrecord-?????-of-00016.gz \. --output_pattern_prefix=""${OUTPUT_BUCKET}/training_set.with_label.shuffled"" \. --output_dataset_name=""HG001"" \. --output_dataset_config_pbtxt=""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt"" \. --job_name=shuffle-tfrecords \. --runner=DataflowRunner \. --staging_location=""${OUTPUT_BUCKET}/staging"" \. --temp_location=""${OUTPUT_BUCKET}/tempdir"" \. --save_main_session \. --region us-east1. ```. This worked for me. It took `15m32.746s`. I didn't get the numpy issue. So I decided to run `pip3 freeze` to get my current versions so you can compare:. ```bash. $ pip3 freeze. absl-py==2.1.0. apache-beam",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:4683,modifiability,upgrad,upgrade,4683,"examples \. --mode training \. --ref ""${REF}"" \. --reads ""${BAM_CHR21}"" \. --examples ""${OUTPUT_DIR}/validation_set.with_label.tfrecord@${N_SHARDS}.gz"" \. --truth_variants ""${TRUTH_VCF}"" \. --confident_regions ""${TRUTH_BED}"" \. --task {} \. --regions ""'chr21'"" \. --channels ""insert_size"" \. ) 2>&1 | tee ""${LOG_DIR}/validation_set.with_label.make_examples.log"". ```. This took `5m25.905s`. ```bash. gsutil -m cp ${OUTPUT_DIR}/validation_set.with_label.tfrecord-?????-of-00016.gz* \. ${OUTPUT_BUCKET}. ```. # This parts starts shuffling... ```bash. sudo apt install -y python3.8-venv. # Create a virtualenv. python3 -m venv beam. # Activate the virtualenv. . beam/bin/activate. ```. ```bash. mkdir -p ${SHUFFLE_SCRIPT_DIR}. wget https://raw.githubusercontent.com/google/deepvariant/r1.6.1/tools/shuffle_tfrecords_beam.py -O ${SHUFFLE_SCRIPT_DIR}/shuffle_tfrecords_beam.py. ```. ```bash. sudo apt -y update && sudo apt -y install python3-pip. pip3 install --upgrade pip. pip3 install setuptools --upgrade. pip3 install apache_beam[gcp]==2.50.0 # 2.51.0 didn't work in my run. pip3 install tensorflow # For parsing tf.Example in shuffle_tfrecords_beam.py. ```. ```bash. time python3 ${SHUFFLE_SCRIPT_DIR}/shuffle_tfrecords_beam.py \. --project=""${YOUR_PROJECT}"" \. --input_pattern_list=""${OUTPUT_BUCKET}""/training_set.with_label.tfrecord-?????-of-00016.gz \. --output_pattern_prefix=""${OUTPUT_BUCKET}/training_set.with_label.shuffled"" \. --output_dataset_name=""HG001"" \. --output_dataset_config_pbtxt=""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt"" \. --job_name=shuffle-tfrecords \. --runner=DataflowRunner \. --staging_location=""${OUTPUT_BUCKET}/staging"" \. --temp_location=""${OUTPUT_BUCKET}/tempdir"" \. --save_main_session \. --region us-east1. ```. This worked for me. It took `15m32.746s`. I didn't get the numpy issue. So I decided to run `pip3 freeze` to get my current versions so you can compare:. ```bash. $ pip3 freeze. absl-py==2.1.0. apache-beam==2.50.0. astunparse==1.6.3. cachetools",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:5567,modifiability,version,versions,5567,"h. sudo apt -y update && sudo apt -y install python3-pip. pip3 install --upgrade pip. pip3 install setuptools --upgrade. pip3 install apache_beam[gcp]==2.50.0 # 2.51.0 didn't work in my run. pip3 install tensorflow # For parsing tf.Example in shuffle_tfrecords_beam.py. ```. ```bash. time python3 ${SHUFFLE_SCRIPT_DIR}/shuffle_tfrecords_beam.py \. --project=""${YOUR_PROJECT}"" \. --input_pattern_list=""${OUTPUT_BUCKET}""/training_set.with_label.tfrecord-?????-of-00016.gz \. --output_pattern_prefix=""${OUTPUT_BUCKET}/training_set.with_label.shuffled"" \. --output_dataset_name=""HG001"" \. --output_dataset_config_pbtxt=""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt"" \. --job_name=shuffle-tfrecords \. --runner=DataflowRunner \. --staging_location=""${OUTPUT_BUCKET}/staging"" \. --temp_location=""${OUTPUT_BUCKET}/tempdir"" \. --save_main_session \. --region us-east1. ```. This worked for me. It took `15m32.746s`. I didn't get the numpy issue. So I decided to run `pip3 freeze` to get my current versions so you can compare:. ```bash. $ pip3 freeze. absl-py==2.1.0. apache-beam==2.50.0. astunparse==1.6.3. cachetools==5.3.3. certifi==2024.2.2. charset-normalizer==3.3.2. cloudpickle==2.2.1. crcmod==1.7. Deprecated==1.2.14. dill==0.3.1.1. dnspython==2.6.1. docopt==0.6.2. fastavro==1.9.4. fasteners==0.19. flatbuffers==24.3.7. gast==0.4.0. google-api-core==2.17.1. google-apitools==0.5.31. google-auth==2.28.2. google-auth-httplib2==0.1.1. google-auth-oauthlib==1.0.0. google-cloud-aiplatform==1.44.0. google-cloud-bigquery==3.19.0. google-cloud-bigquery-storage==2.24.0. google-cloud-bigtable==2.23.0. google-cloud-core==2.4.1. google-cloud-datastore==2.19.0. google-cloud-dlp==3.16.0. google-cloud-language==2.13.3. google-cloud-pubsub==2.20.2. google-cloud-pubsublite==1.9.0. google-cloud-recommendations-ai==0.10.10. google-cloud-resource-manager==1.12.3. google-cloud-spanner==3.44.0. google-cloud-storage==2.16.0. google-cloud-videointelligence==2.13.3. google-cloud-vision==3.7.2. google-crc32",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:7032,modifiability,pac,packaging,7032,"rmalizer==3.3.2. cloudpickle==2.2.1. crcmod==1.7. Deprecated==1.2.14. dill==0.3.1.1. dnspython==2.6.1. docopt==0.6.2. fastavro==1.9.4. fasteners==0.19. flatbuffers==24.3.7. gast==0.4.0. google-api-core==2.17.1. google-apitools==0.5.31. google-auth==2.28.2. google-auth-httplib2==0.1.1. google-auth-oauthlib==1.0.0. google-cloud-aiplatform==1.44.0. google-cloud-bigquery==3.19.0. google-cloud-bigquery-storage==2.24.0. google-cloud-bigtable==2.23.0. google-cloud-core==2.4.1. google-cloud-datastore==2.19.0. google-cloud-dlp==3.16.0. google-cloud-language==2.13.3. google-cloud-pubsub==2.20.2. google-cloud-pubsublite==1.9.0. google-cloud-recommendations-ai==0.10.10. google-cloud-resource-manager==1.12.3. google-cloud-spanner==3.44.0. google-cloud-storage==2.16.0. google-cloud-videointelligence==2.13.3. google-cloud-vision==3.7.2. google-crc32c==1.5.0. google-pasta==0.2.0. google-resumable-media==2.7.0. googleapis-common-protos==1.63.0. grpc-google-iam-v1==0.13.0. grpc-interceptor==0.15.4. grpcio==1.62.1. grpcio-status==1.62.1. h5py==3.10.0. hdfs==2.7.3. httplib2==0.22.0. idna==3.6. importlib_metadata==7.0.2. keras==2.13.1. libclang==18.1.1. Markdown==3.6. MarkupSafe==2.1.5. numpy==1.24.3. oauth2client==4.1.3. oauthlib==3.2.2. objsize==0.6.1. opt-einsum==3.3.0. orjson==3.9.15. overrides==7.7.0. packaging==24.0. pkg_resources==0.0.0. proto-plus==1.23.0. protobuf==4.23.4. pyarrow==11.0.0. pyasn1==0.5.1. pyasn1-modules==0.3.0. pydot==1.4.2. pymongo==4.6.2. pyparsing==3.1.2. python-dateutil==2.9.0.post0. pytz==2024.1. regex==2023.12.25. requests==2.31.0. requests-oauthlib==1.4.0. rsa==4.9. shapely==2.0.3. six==1.16.0. sqlparse==0.4.4. tensorboard==2.13.0. tensorboard-data-server==0.7.2. tensorflow==2.13.1. tensorflow-estimator==2.13.0. tensorflow-io-gcs-filesystem==0.34.0. termcolor==2.4.0. typing_extensions==4.5.0. urllib3==2.2.1. Werkzeug==3.0.1. wrapt==1.16.0. zipp==3.18.1. zstandard==0.22.0. ```. ---. @helizabeth1103 , can you see which version might be different from yours?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:7148,modifiability,modul,modules,7148,"rmalizer==3.3.2. cloudpickle==2.2.1. crcmod==1.7. Deprecated==1.2.14. dill==0.3.1.1. dnspython==2.6.1. docopt==0.6.2. fastavro==1.9.4. fasteners==0.19. flatbuffers==24.3.7. gast==0.4.0. google-api-core==2.17.1. google-apitools==0.5.31. google-auth==2.28.2. google-auth-httplib2==0.1.1. google-auth-oauthlib==1.0.0. google-cloud-aiplatform==1.44.0. google-cloud-bigquery==3.19.0. google-cloud-bigquery-storage==2.24.0. google-cloud-bigtable==2.23.0. google-cloud-core==2.4.1. google-cloud-datastore==2.19.0. google-cloud-dlp==3.16.0. google-cloud-language==2.13.3. google-cloud-pubsub==2.20.2. google-cloud-pubsublite==1.9.0. google-cloud-recommendations-ai==0.10.10. google-cloud-resource-manager==1.12.3. google-cloud-spanner==3.44.0. google-cloud-storage==2.16.0. google-cloud-videointelligence==2.13.3. google-cloud-vision==3.7.2. google-crc32c==1.5.0. google-pasta==0.2.0. google-resumable-media==2.7.0. googleapis-common-protos==1.63.0. grpc-google-iam-v1==0.13.0. grpc-interceptor==0.15.4. grpcio==1.62.1. grpcio-status==1.62.1. h5py==3.10.0. hdfs==2.7.3. httplib2==0.22.0. idna==3.6. importlib_metadata==7.0.2. keras==2.13.1. libclang==18.1.1. Markdown==3.6. MarkupSafe==2.1.5. numpy==1.24.3. oauth2client==4.1.3. oauthlib==3.2.2. objsize==0.6.1. opt-einsum==3.3.0. orjson==3.9.15. overrides==7.7.0. packaging==24.0. pkg_resources==0.0.0. proto-plus==1.23.0. protobuf==4.23.4. pyarrow==11.0.0. pyasn1==0.5.1. pyasn1-modules==0.3.0. pydot==1.4.2. pymongo==4.6.2. pyparsing==3.1.2. python-dateutil==2.9.0.post0. pytz==2024.1. regex==2023.12.25. requests==2.31.0. requests-oauthlib==1.4.0. rsa==4.9. shapely==2.0.3. six==1.16.0. sqlparse==0.4.4. tensorboard==2.13.0. tensorboard-data-server==0.7.2. tensorflow==2.13.1. tensorflow-estimator==2.13.0. tensorflow-io-gcs-filesystem==0.34.0. termcolor==2.4.0. typing_extensions==4.5.0. urllib3==2.2.1. Werkzeug==3.0.1. wrapt==1.16.0. zipp==3.18.1. zstandard==0.22.0. ```. ---. @helizabeth1103 , can you see which version might be different from yours?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:7687,modifiability,version,version,7687,"rmalizer==3.3.2. cloudpickle==2.2.1. crcmod==1.7. Deprecated==1.2.14. dill==0.3.1.1. dnspython==2.6.1. docopt==0.6.2. fastavro==1.9.4. fasteners==0.19. flatbuffers==24.3.7. gast==0.4.0. google-api-core==2.17.1. google-apitools==0.5.31. google-auth==2.28.2. google-auth-httplib2==0.1.1. google-auth-oauthlib==1.0.0. google-cloud-aiplatform==1.44.0. google-cloud-bigquery==3.19.0. google-cloud-bigquery-storage==2.24.0. google-cloud-bigtable==2.23.0. google-cloud-core==2.4.1. google-cloud-datastore==2.19.0. google-cloud-dlp==3.16.0. google-cloud-language==2.13.3. google-cloud-pubsub==2.20.2. google-cloud-pubsublite==1.9.0. google-cloud-recommendations-ai==0.10.10. google-cloud-resource-manager==1.12.3. google-cloud-spanner==3.44.0. google-cloud-storage==2.16.0. google-cloud-videointelligence==2.13.3. google-cloud-vision==3.7.2. google-crc32c==1.5.0. google-pasta==0.2.0. google-resumable-media==2.7.0. googleapis-common-protos==1.63.0. grpc-google-iam-v1==0.13.0. grpc-interceptor==0.15.4. grpcio==1.62.1. grpcio-status==1.62.1. h5py==3.10.0. hdfs==2.7.3. httplib2==0.22.0. idna==3.6. importlib_metadata==7.0.2. keras==2.13.1. libclang==18.1.1. Markdown==3.6. MarkupSafe==2.1.5. numpy==1.24.3. oauth2client==4.1.3. oauthlib==3.2.2. objsize==0.6.1. opt-einsum==3.3.0. orjson==3.9.15. overrides==7.7.0. packaging==24.0. pkg_resources==0.0.0. proto-plus==1.23.0. protobuf==4.23.4. pyarrow==11.0.0. pyasn1==0.5.1. pyasn1-modules==0.3.0. pydot==1.4.2. pymongo==4.6.2. pyparsing==3.1.2. python-dateutil==2.9.0.post0. pytz==2024.1. regex==2023.12.25. requests==2.31.0. requests-oauthlib==1.4.0. rsa==4.9. shapely==2.0.3. six==1.16.0. sqlparse==0.4.4. tensorboard==2.13.0. tensorboard-data-server==0.7.2. tensorflow==2.13.1. tensorflow-estimator==2.13.0. tensorflow-io-gcs-filesystem==0.34.0. termcolor==2.4.0. typing_extensions==4.5.0. urllib3==2.2.1. Werkzeug==3.0.1. wrapt==1.16.0. zipp==3.18.1. zstandard==0.22.0. ```. ---. @helizabeth1103 , can you see which version might be different from yours?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:472,performance,disk,disk-size,472,"# Follow https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-training-case-study.md. I got a machine:. ```bash. host=""${USER}-deepvariant-vm"". zone=""us-west1-b"". gcloud compute instances create ${host} \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --maintenance-policy ""TERMINATE"" \. --accelerator=type=nvidia-tesla-p100,count=1 \. --image-family ""ubuntu-2004-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""n1-standard-16"" \. --boot-disk-size ""300"" \. --zone ""${zone}"" \. --min-cpu-platform ""Intel Skylake"". ```. I sshed into my machine:. ```bash. gcloud compute ssh pichuan-deepvariant-vm --zone us-west1-b. ```. I ran this with my own `YOUR_PROJECT` and `OUTPUT_GCS_BUCKET` setting. Then the following is basically just copy/paste from the doc:. ```. BUCKET=""gs://deepvariant"". VERSION=""1.6.1"". DOCKER_IMAGE=""google/deepvariant:${VERSION}"". MODEL_BUCKET=""${BUCKET}/models/DeepVariant/${VERSION}/DeepVariant-inception_v3-${VERSION}+data-wgs_standard"". GCS_PRETRAINED_WGS_MODEL=""${MODEL_BUCKET}/model.ckpt"". OUTPUT_BUCKET=""${OUTPUT_GCS_BUCKET}/customized_training"". TRAINING_DIR=""${OUTPUT_BUCKET}/training_dir"". BASE=""${HOME}/training-case-study"". DATA_BUCKET=gs://deepvariant/training-case-study/BGISEQ-HG001. INPUT_DIR=""${BASE}/input"". BIN_DIR=""${INPUT_DIR}/bin"". DATA_DIR=""${INPUT_DIR}/data"". OUTPUT_DIR=""${BASE}/output"". LOG_DIR=""${OUTPUT_DIR}/logs"". SHUFFLE_SCRIPT_DIR=""${HOME}/deepvariant/tools"". REF=""${DATA_DIR}/ucsc_hg19.fa"". BAM_CHR1=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr1.bam"". BAM_CHR20=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr20.bam"". BAM_CHR21=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr21.bam"". TRUTH_VCF=""${DATA_DIR}/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_PGandRTGphasetransfer_chrs_FIXED.vcf.gz"". TRUTH_BED=""${DATA_DIR}/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_nosomaticdel_chr.bed"". N_SHARDS=16. ```. ```bash. mkdir -p ""${OUTPUT_DIR}"". mkd",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:517,performance,cpu,cpu-platform,517,"# Follow https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-training-case-study.md. I got a machine:. ```bash. host=""${USER}-deepvariant-vm"". zone=""us-west1-b"". gcloud compute instances create ${host} \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --maintenance-policy ""TERMINATE"" \. --accelerator=type=nvidia-tesla-p100,count=1 \. --image-family ""ubuntu-2004-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""n1-standard-16"" \. --boot-disk-size ""300"" \. --zone ""${zone}"" \. --min-cpu-platform ""Intel Skylake"". ```. I sshed into my machine:. ```bash. gcloud compute ssh pichuan-deepvariant-vm --zone us-west1-b. ```. I ran this with my own `YOUR_PROJECT` and `OUTPUT_GCS_BUCKET` setting. Then the following is basically just copy/paste from the doc:. ```. BUCKET=""gs://deepvariant"". VERSION=""1.6.1"". DOCKER_IMAGE=""google/deepvariant:${VERSION}"". MODEL_BUCKET=""${BUCKET}/models/DeepVariant/${VERSION}/DeepVariant-inception_v3-${VERSION}+data-wgs_standard"". GCS_PRETRAINED_WGS_MODEL=""${MODEL_BUCKET}/model.ckpt"". OUTPUT_BUCKET=""${OUTPUT_GCS_BUCKET}/customized_training"". TRAINING_DIR=""${OUTPUT_BUCKET}/training_dir"". BASE=""${HOME}/training-case-study"". DATA_BUCKET=gs://deepvariant/training-case-study/BGISEQ-HG001. INPUT_DIR=""${BASE}/input"". BIN_DIR=""${INPUT_DIR}/bin"". DATA_DIR=""${INPUT_DIR}/data"". OUTPUT_DIR=""${BASE}/output"". LOG_DIR=""${OUTPUT_DIR}/logs"". SHUFFLE_SCRIPT_DIR=""${HOME}/deepvariant/tools"". REF=""${DATA_DIR}/ucsc_hg19.fa"". BAM_CHR1=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr1.bam"". BAM_CHR20=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr20.bam"". BAM_CHR21=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr21.bam"". TRUTH_VCF=""${DATA_DIR}/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_PGandRTGphasetransfer_chrs_FIXED.vcf.gz"". TRUTH_BED=""${DATA_DIR}/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_nosomaticdel_chr.bed"". N_SHARDS=16. ```. ```bash. mkdir -p ""${OUTPUT_DIR}"". mkd",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:2414,performance,parallel,parallel,2414,"ME}/deepvariant/tools"". REF=""${DATA_DIR}/ucsc_hg19.fa"". BAM_CHR1=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr1.bam"". BAM_CHR20=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr20.bam"". BAM_CHR21=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr21.bam"". TRUTH_VCF=""${DATA_DIR}/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_PGandRTGphasetransfer_chrs_FIXED.vcf.gz"". TRUTH_BED=""${DATA_DIR}/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_nosomaticdel_chr.bed"". N_SHARDS=16. ```. ```bash. mkdir -p ""${OUTPUT_DIR}"". mkdir -p ""${BIN_DIR}"". mkdir -p ""${DATA_DIR}"". mkdir -p ""${LOG_DIR}"". ```. ```bash. gsutil -m cp ${DATA_BUCKET}/BGISEQ_PE100_NA12878.sorted.chr*.bam* ""${DATA_DIR}"". gsutil -m cp -r ""${DATA_BUCKET}/ucsc_hg19.fa*"" ""${DATA_DIR}"". gsutil -m cp -r ""${DATA_BUCKET}/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_*"" ""${DATA_DIR}"". ```. ```bash. sudo apt -y update. sudo apt -y install parallel. curl -O https://raw.githubusercontent.com/google/deepvariant/r1.6.1/scripts/install_nvidia_docker.sh. bash -x install_nvidia_docker.sh. ```. ```bash. sudo docker pull ${DOCKER_IMAGE} # Standard CPU Docker Image. sudo docker pull ${DOCKER_IMAGE}-gpu # GPU-enabled Docker image. ```. ```bash. ( time seq 0 $((N_SHARDS-1)) | \. parallel --halt 2 --line-buffer \. sudo docker run \. -v ${HOME}:${HOME} \. ${DOCKER_IMAGE} \. make_examples \. --mode training \. --ref ""${REF}"" \. --reads ""${BAM_CHR1}"" \. --examples ""${OUTPUT_DIR}/training_set.with_label.tfrecord@${N_SHARDS}.gz"" \. --truth_variants ""${TRUTH_VCF}"" \. --confident_regions ""${TRUTH_BED}"" \. --task {} \. --regions ""'chr1'"" \. --channels ""insert_size"" \. ) 2>&1 | tee ""${LOG_DIR}/training_set.with_label.make_examples.log"". ```. This took `20m0.146s`. ```. $ cat ""${OUTPUT_DIR}/training_set.with_label.tfrecord-00000-of-00016.gz.example_info.json"". {""version"": ""1.6.0"", ""shape"": [100, 221, 7], ""channels"": [1, 2, 3, 4, 5, 6, 19]}. ```. ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:2618,performance,CPU,CPU,2618,"/BGISEQ_PE100_NA12878.sorted.chr21.bam"". TRUTH_VCF=""${DATA_DIR}/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_PGandRTGphasetransfer_chrs_FIXED.vcf.gz"". TRUTH_BED=""${DATA_DIR}/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_nosomaticdel_chr.bed"". N_SHARDS=16. ```. ```bash. mkdir -p ""${OUTPUT_DIR}"". mkdir -p ""${BIN_DIR}"". mkdir -p ""${DATA_DIR}"". mkdir -p ""${LOG_DIR}"". ```. ```bash. gsutil -m cp ${DATA_BUCKET}/BGISEQ_PE100_NA12878.sorted.chr*.bam* ""${DATA_DIR}"". gsutil -m cp -r ""${DATA_BUCKET}/ucsc_hg19.fa*"" ""${DATA_DIR}"". gsutil -m cp -r ""${DATA_BUCKET}/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_*"" ""${DATA_DIR}"". ```. ```bash. sudo apt -y update. sudo apt -y install parallel. curl -O https://raw.githubusercontent.com/google/deepvariant/r1.6.1/scripts/install_nvidia_docker.sh. bash -x install_nvidia_docker.sh. ```. ```bash. sudo docker pull ${DOCKER_IMAGE} # Standard CPU Docker Image. sudo docker pull ${DOCKER_IMAGE}-gpu # GPU-enabled Docker image. ```. ```bash. ( time seq 0 $((N_SHARDS-1)) | \. parallel --halt 2 --line-buffer \. sudo docker run \. -v ${HOME}:${HOME} \. ${DOCKER_IMAGE} \. make_examples \. --mode training \. --ref ""${REF}"" \. --reads ""${BAM_CHR1}"" \. --examples ""${OUTPUT_DIR}/training_set.with_label.tfrecord@${N_SHARDS}.gz"" \. --truth_variants ""${TRUTH_VCF}"" \. --confident_regions ""${TRUTH_BED}"" \. --task {} \. --regions ""'chr1'"" \. --channels ""insert_size"" \. ) 2>&1 | tee ""${LOG_DIR}/training_set.with_label.make_examples.log"". ```. This took `20m0.146s`. ```. $ cat ""${OUTPUT_DIR}/training_set.with_label.tfrecord-00000-of-00016.gz.example_info.json"". {""version"": ""1.6.0"", ""shape"": [100, 221, 7], ""channels"": [1, 2, 3, 4, 5, 6, 19]}. ```. ```bash. gsutil -m cp ${OUTPUT_DIR}/training_set.with_label.tfrecord-?????-of-00016.gz* \. ${OUTPUT_BUCKET}. ```. ```bash. ( time seq 0 $((N_SHARDS-1)) | \. parallel --halt 2 --line-buffer \. sudo docke",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:2669,performance,gpu,gpu,2669,"""${DATA_DIR}/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_PGandRTGphasetransfer_chrs_FIXED.vcf.gz"". TRUTH_BED=""${DATA_DIR}/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_nosomaticdel_chr.bed"". N_SHARDS=16. ```. ```bash. mkdir -p ""${OUTPUT_DIR}"". mkdir -p ""${BIN_DIR}"". mkdir -p ""${DATA_DIR}"". mkdir -p ""${LOG_DIR}"". ```. ```bash. gsutil -m cp ${DATA_BUCKET}/BGISEQ_PE100_NA12878.sorted.chr*.bam* ""${DATA_DIR}"". gsutil -m cp -r ""${DATA_BUCKET}/ucsc_hg19.fa*"" ""${DATA_DIR}"". gsutil -m cp -r ""${DATA_BUCKET}/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_*"" ""${DATA_DIR}"". ```. ```bash. sudo apt -y update. sudo apt -y install parallel. curl -O https://raw.githubusercontent.com/google/deepvariant/r1.6.1/scripts/install_nvidia_docker.sh. bash -x install_nvidia_docker.sh. ```. ```bash. sudo docker pull ${DOCKER_IMAGE} # Standard CPU Docker Image. sudo docker pull ${DOCKER_IMAGE}-gpu # GPU-enabled Docker image. ```. ```bash. ( time seq 0 $((N_SHARDS-1)) | \. parallel --halt 2 --line-buffer \. sudo docker run \. -v ${HOME}:${HOME} \. ${DOCKER_IMAGE} \. make_examples \. --mode training \. --ref ""${REF}"" \. --reads ""${BAM_CHR1}"" \. --examples ""${OUTPUT_DIR}/training_set.with_label.tfrecord@${N_SHARDS}.gz"" \. --truth_variants ""${TRUTH_VCF}"" \. --confident_regions ""${TRUTH_BED}"" \. --task {} \. --regions ""'chr1'"" \. --channels ""insert_size"" \. ) 2>&1 | tee ""${LOG_DIR}/training_set.with_label.make_examples.log"". ```. This took `20m0.146s`. ```. $ cat ""${OUTPUT_DIR}/training_set.with_label.tfrecord-00000-of-00016.gz.example_info.json"". {""version"": ""1.6.0"", ""shape"": [100, 221, 7], ""channels"": [1, 2, 3, 4, 5, 6, 19]}. ```. ```bash. gsutil -m cp ${OUTPUT_DIR}/training_set.with_label.tfrecord-?????-of-00016.gz* \. ${OUTPUT_BUCKET}. ```. ```bash. ( time seq 0 $((N_SHARDS-1)) | \. parallel --halt 2 --line-buffer \. sudo docker run \. -v /home/${USER}:/home/${USER} \. ${DOCKER",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:2675,performance,GPU,GPU-enabled,2675,"R}/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_PGandRTGphasetransfer_chrs_FIXED.vcf.gz"". TRUTH_BED=""${DATA_DIR}/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_nosomaticdel_chr.bed"". N_SHARDS=16. ```. ```bash. mkdir -p ""${OUTPUT_DIR}"". mkdir -p ""${BIN_DIR}"". mkdir -p ""${DATA_DIR}"". mkdir -p ""${LOG_DIR}"". ```. ```bash. gsutil -m cp ${DATA_BUCKET}/BGISEQ_PE100_NA12878.sorted.chr*.bam* ""${DATA_DIR}"". gsutil -m cp -r ""${DATA_BUCKET}/ucsc_hg19.fa*"" ""${DATA_DIR}"". gsutil -m cp -r ""${DATA_BUCKET}/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_*"" ""${DATA_DIR}"". ```. ```bash. sudo apt -y update. sudo apt -y install parallel. curl -O https://raw.githubusercontent.com/google/deepvariant/r1.6.1/scripts/install_nvidia_docker.sh. bash -x install_nvidia_docker.sh. ```. ```bash. sudo docker pull ${DOCKER_IMAGE} # Standard CPU Docker Image. sudo docker pull ${DOCKER_IMAGE}-gpu # GPU-enabled Docker image. ```. ```bash. ( time seq 0 $((N_SHARDS-1)) | \. parallel --halt 2 --line-buffer \. sudo docker run \. -v ${HOME}:${HOME} \. ${DOCKER_IMAGE} \. make_examples \. --mode training \. --ref ""${REF}"" \. --reads ""${BAM_CHR1}"" \. --examples ""${OUTPUT_DIR}/training_set.with_label.tfrecord@${N_SHARDS}.gz"" \. --truth_variants ""${TRUTH_VCF}"" \. --confident_regions ""${TRUTH_BED}"" \. --task {} \. --regions ""'chr1'"" \. --channels ""insert_size"" \. ) 2>&1 | tee ""${LOG_DIR}/training_set.with_label.make_examples.log"". ```. This took `20m0.146s`. ```. $ cat ""${OUTPUT_DIR}/training_set.with_label.tfrecord-00000-of-00016.gz.example_info.json"". {""version"": ""1.6.0"", ""shape"": [100, 221, 7], ""channels"": [1, 2, 3, 4, 5, 6, 19]}. ```. ```bash. gsutil -m cp ${OUTPUT_DIR}/training_set.with_label.tfrecord-?????-of-00016.gz* \. ${OUTPUT_BUCKET}. ```. ```bash. ( time seq 0 $((N_SHARDS-1)) | \. parallel --halt 2 --line-buffer \. sudo docker run \. -v /home/${USER}:/home/${USER} \. ${DOCKER_IMAGE} \.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:2717,performance,time,time,2717,"-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_PGandRTGphasetransfer_chrs_FIXED.vcf.gz"". TRUTH_BED=""${DATA_DIR}/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_nosomaticdel_chr.bed"". N_SHARDS=16. ```. ```bash. mkdir -p ""${OUTPUT_DIR}"". mkdir -p ""${BIN_DIR}"". mkdir -p ""${DATA_DIR}"". mkdir -p ""${LOG_DIR}"". ```. ```bash. gsutil -m cp ${DATA_BUCKET}/BGISEQ_PE100_NA12878.sorted.chr*.bam* ""${DATA_DIR}"". gsutil -m cp -r ""${DATA_BUCKET}/ucsc_hg19.fa*"" ""${DATA_DIR}"". gsutil -m cp -r ""${DATA_BUCKET}/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_*"" ""${DATA_DIR}"". ```. ```bash. sudo apt -y update. sudo apt -y install parallel. curl -O https://raw.githubusercontent.com/google/deepvariant/r1.6.1/scripts/install_nvidia_docker.sh. bash -x install_nvidia_docker.sh. ```. ```bash. sudo docker pull ${DOCKER_IMAGE} # Standard CPU Docker Image. sudo docker pull ${DOCKER_IMAGE}-gpu # GPU-enabled Docker image. ```. ```bash. ( time seq 0 $((N_SHARDS-1)) | \. parallel --halt 2 --line-buffer \. sudo docker run \. -v ${HOME}:${HOME} \. ${DOCKER_IMAGE} \. make_examples \. --mode training \. --ref ""${REF}"" \. --reads ""${BAM_CHR1}"" \. --examples ""${OUTPUT_DIR}/training_set.with_label.tfrecord@${N_SHARDS}.gz"" \. --truth_variants ""${TRUTH_VCF}"" \. --confident_regions ""${TRUTH_BED}"" \. --task {} \. --regions ""'chr1'"" \. --channels ""insert_size"" \. ) 2>&1 | tee ""${LOG_DIR}/training_set.with_label.make_examples.log"". ```. This took `20m0.146s`. ```. $ cat ""${OUTPUT_DIR}/training_set.with_label.tfrecord-00000-of-00016.gz.example_info.json"". {""version"": ""1.6.0"", ""shape"": [100, 221, 7], ""channels"": [1, 2, 3, 4, 5, 6, 19]}. ```. ```bash. gsutil -m cp ${OUTPUT_DIR}/training_set.with_label.tfrecord-?????-of-00016.gz* \. ${OUTPUT_BUCKET}. ```. ```bash. ( time seq 0 $((N_SHARDS-1)) | \. parallel --halt 2 --line-buffer \. sudo docker run \. -v /home/${USER}:/home/${USER} \. ${DOCKER_IMAGE} \. make_examples \. --mode training \. -",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:2749,performance,parallel,parallel,2749,"v.3.3.2_highconf_PGandRTGphasetransfer_chrs_FIXED.vcf.gz"". TRUTH_BED=""${DATA_DIR}/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_nosomaticdel_chr.bed"". N_SHARDS=16. ```. ```bash. mkdir -p ""${OUTPUT_DIR}"". mkdir -p ""${BIN_DIR}"". mkdir -p ""${DATA_DIR}"". mkdir -p ""${LOG_DIR}"". ```. ```bash. gsutil -m cp ${DATA_BUCKET}/BGISEQ_PE100_NA12878.sorted.chr*.bam* ""${DATA_DIR}"". gsutil -m cp -r ""${DATA_BUCKET}/ucsc_hg19.fa*"" ""${DATA_DIR}"". gsutil -m cp -r ""${DATA_BUCKET}/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_*"" ""${DATA_DIR}"". ```. ```bash. sudo apt -y update. sudo apt -y install parallel. curl -O https://raw.githubusercontent.com/google/deepvariant/r1.6.1/scripts/install_nvidia_docker.sh. bash -x install_nvidia_docker.sh. ```. ```bash. sudo docker pull ${DOCKER_IMAGE} # Standard CPU Docker Image. sudo docker pull ${DOCKER_IMAGE}-gpu # GPU-enabled Docker image. ```. ```bash. ( time seq 0 $((N_SHARDS-1)) | \. parallel --halt 2 --line-buffer \. sudo docker run \. -v ${HOME}:${HOME} \. ${DOCKER_IMAGE} \. make_examples \. --mode training \. --ref ""${REF}"" \. --reads ""${BAM_CHR1}"" \. --examples ""${OUTPUT_DIR}/training_set.with_label.tfrecord@${N_SHARDS}.gz"" \. --truth_variants ""${TRUTH_VCF}"" \. --confident_regions ""${TRUTH_BED}"" \. --task {} \. --regions ""'chr1'"" \. --channels ""insert_size"" \. ) 2>&1 | tee ""${LOG_DIR}/training_set.with_label.make_examples.log"". ```. This took `20m0.146s`. ```. $ cat ""${OUTPUT_DIR}/training_set.with_label.tfrecord-00000-of-00016.gz.example_info.json"". {""version"": ""1.6.0"", ""shape"": [100, 221, 7], ""channels"": [1, 2, 3, 4, 5, 6, 19]}. ```. ```bash. gsutil -m cp ${OUTPUT_DIR}/training_set.with_label.tfrecord-?????-of-00016.gz* \. ${OUTPUT_BUCKET}. ```. ```bash. ( time seq 0 $((N_SHARDS-1)) | \. parallel --halt 2 --line-buffer \. sudo docker run \. -v /home/${USER}:/home/${USER} \. ${DOCKER_IMAGE} \. make_examples \. --mode training \. --ref ""${REF}"" \. --reads ""${BAM_CH",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:3543,performance,time,time,3543,"dia_docker.sh. ```. ```bash. sudo docker pull ${DOCKER_IMAGE} # Standard CPU Docker Image. sudo docker pull ${DOCKER_IMAGE}-gpu # GPU-enabled Docker image. ```. ```bash. ( time seq 0 $((N_SHARDS-1)) | \. parallel --halt 2 --line-buffer \. sudo docker run \. -v ${HOME}:${HOME} \. ${DOCKER_IMAGE} \. make_examples \. --mode training \. --ref ""${REF}"" \. --reads ""${BAM_CHR1}"" \. --examples ""${OUTPUT_DIR}/training_set.with_label.tfrecord@${N_SHARDS}.gz"" \. --truth_variants ""${TRUTH_VCF}"" \. --confident_regions ""${TRUTH_BED}"" \. --task {} \. --regions ""'chr1'"" \. --channels ""insert_size"" \. ) 2>&1 | tee ""${LOG_DIR}/training_set.with_label.make_examples.log"". ```. This took `20m0.146s`. ```. $ cat ""${OUTPUT_DIR}/training_set.with_label.tfrecord-00000-of-00016.gz.example_info.json"". {""version"": ""1.6.0"", ""shape"": [100, 221, 7], ""channels"": [1, 2, 3, 4, 5, 6, 19]}. ```. ```bash. gsutil -m cp ${OUTPUT_DIR}/training_set.with_label.tfrecord-?????-of-00016.gz* \. ${OUTPUT_BUCKET}. ```. ```bash. ( time seq 0 $((N_SHARDS-1)) | \. parallel --halt 2 --line-buffer \. sudo docker run \. -v /home/${USER}:/home/${USER} \. ${DOCKER_IMAGE} \. make_examples \. --mode training \. --ref ""${REF}"" \. --reads ""${BAM_CHR21}"" \. --examples ""${OUTPUT_DIR}/validation_set.with_label.tfrecord@${N_SHARDS}.gz"" \. --truth_variants ""${TRUTH_VCF}"" \. --confident_regions ""${TRUTH_BED}"" \. --task {} \. --regions ""'chr21'"" \. --channels ""insert_size"" \. ) 2>&1 | tee ""${LOG_DIR}/validation_set.with_label.make_examples.log"". ```. This took `5m25.905s`. ```bash. gsutil -m cp ${OUTPUT_DIR}/validation_set.with_label.tfrecord-?????-of-00016.gz* \. ${OUTPUT_BUCKET}. ```. # This parts starts shuffling... ```bash. sudo apt install -y python3.8-venv. # Create a virtualenv. python3 -m venv beam. # Activate the virtualenv. . beam/bin/activate. ```. ```bash. mkdir -p ${SHUFFLE_SCRIPT_DIR}. wget https://raw.githubusercontent.com/google/deepvariant/r1.6.1/tools/shuffle_tfrecords_beam.py -O ${SHUFFLE_SCRIPT_DIR}/shuffle_tfre",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:3575,performance,parallel,parallel,3575,"docker pull ${DOCKER_IMAGE} # Standard CPU Docker Image. sudo docker pull ${DOCKER_IMAGE}-gpu # GPU-enabled Docker image. ```. ```bash. ( time seq 0 $((N_SHARDS-1)) | \. parallel --halt 2 --line-buffer \. sudo docker run \. -v ${HOME}:${HOME} \. ${DOCKER_IMAGE} \. make_examples \. --mode training \. --ref ""${REF}"" \. --reads ""${BAM_CHR1}"" \. --examples ""${OUTPUT_DIR}/training_set.with_label.tfrecord@${N_SHARDS}.gz"" \. --truth_variants ""${TRUTH_VCF}"" \. --confident_regions ""${TRUTH_BED}"" \. --task {} \. --regions ""'chr1'"" \. --channels ""insert_size"" \. ) 2>&1 | tee ""${LOG_DIR}/training_set.with_label.make_examples.log"". ```. This took `20m0.146s`. ```. $ cat ""${OUTPUT_DIR}/training_set.with_label.tfrecord-00000-of-00016.gz.example_info.json"". {""version"": ""1.6.0"", ""shape"": [100, 221, 7], ""channels"": [1, 2, 3, 4, 5, 6, 19]}. ```. ```bash. gsutil -m cp ${OUTPUT_DIR}/training_set.with_label.tfrecord-?????-of-00016.gz* \. ${OUTPUT_BUCKET}. ```. ```bash. ( time seq 0 $((N_SHARDS-1)) | \. parallel --halt 2 --line-buffer \. sudo docker run \. -v /home/${USER}:/home/${USER} \. ${DOCKER_IMAGE} \. make_examples \. --mode training \. --ref ""${REF}"" \. --reads ""${BAM_CHR21}"" \. --examples ""${OUTPUT_DIR}/validation_set.with_label.tfrecord@${N_SHARDS}.gz"" \. --truth_variants ""${TRUTH_VCF}"" \. --confident_regions ""${TRUTH_BED}"" \. --task {} \. --regions ""'chr21'"" \. --channels ""insert_size"" \. ) 2>&1 | tee ""${LOG_DIR}/validation_set.with_label.make_examples.log"". ```. This took `5m25.905s`. ```bash. gsutil -m cp ${OUTPUT_DIR}/validation_set.with_label.tfrecord-?????-of-00016.gz* \. ${OUTPUT_BUCKET}. ```. # This parts starts shuffling... ```bash. sudo apt install -y python3.8-venv. # Create a virtualenv. python3 -m venv beam. # Activate the virtualenv. . beam/bin/activate. ```. ```bash. mkdir -p ${SHUFFLE_SCRIPT_DIR}. wget https://raw.githubusercontent.com/google/deepvariant/r1.6.1/tools/shuffle_tfrecords_beam.py -O ${SHUFFLE_SCRIPT_DIR}/shuffle_tfrecords_beam.py. ```. ```bash. sudo ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:4855,performance,time,time,4855,"s ""${TRUTH_VCF}"" \. --confident_regions ""${TRUTH_BED}"" \. --task {} \. --regions ""'chr21'"" \. --channels ""insert_size"" \. ) 2>&1 | tee ""${LOG_DIR}/validation_set.with_label.make_examples.log"". ```. This took `5m25.905s`. ```bash. gsutil -m cp ${OUTPUT_DIR}/validation_set.with_label.tfrecord-?????-of-00016.gz* \. ${OUTPUT_BUCKET}. ```. # This parts starts shuffling... ```bash. sudo apt install -y python3.8-venv. # Create a virtualenv. python3 -m venv beam. # Activate the virtualenv. . beam/bin/activate. ```. ```bash. mkdir -p ${SHUFFLE_SCRIPT_DIR}. wget https://raw.githubusercontent.com/google/deepvariant/r1.6.1/tools/shuffle_tfrecords_beam.py -O ${SHUFFLE_SCRIPT_DIR}/shuffle_tfrecords_beam.py. ```. ```bash. sudo apt -y update && sudo apt -y install python3-pip. pip3 install --upgrade pip. pip3 install setuptools --upgrade. pip3 install apache_beam[gcp]==2.50.0 # 2.51.0 didn't work in my run. pip3 install tensorflow # For parsing tf.Example in shuffle_tfrecords_beam.py. ```. ```bash. time python3 ${SHUFFLE_SCRIPT_DIR}/shuffle_tfrecords_beam.py \. --project=""${YOUR_PROJECT}"" \. --input_pattern_list=""${OUTPUT_BUCKET}""/training_set.with_label.tfrecord-?????-of-00016.gz \. --output_pattern_prefix=""${OUTPUT_BUCKET}/training_set.with_label.shuffled"" \. --output_dataset_name=""HG001"" \. --output_dataset_config_pbtxt=""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt"" \. --job_name=shuffle-tfrecords \. --runner=DataflowRunner \. --staging_location=""${OUTPUT_BUCKET}/staging"" \. --temp_location=""${OUTPUT_BUCKET}/tempdir"" \. --save_main_session \. --region us-east1. ```. This worked for me. It took `15m32.746s`. I didn't get the numpy issue. So I decided to run `pip3 freeze` to get my current versions so you can compare:. ```bash. $ pip3 freeze. absl-py==2.1.0. apache-beam==2.50.0. astunparse==1.6.3. cachetools==5.3.3. certifi==2024.2.2. charset-normalizer==3.3.2. cloudpickle==2.2.1. crcmod==1.7. Deprecated==1.2.14. dill==0.3.1.1. dnspython==2.6.1. docopt==0.6.2. fastavro==1.9.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:5677,performance,cach,cachetools,5677,"-upgrade. pip3 install apache_beam[gcp]==2.50.0 # 2.51.0 didn't work in my run. pip3 install tensorflow # For parsing tf.Example in shuffle_tfrecords_beam.py. ```. ```bash. time python3 ${SHUFFLE_SCRIPT_DIR}/shuffle_tfrecords_beam.py \. --project=""${YOUR_PROJECT}"" \. --input_pattern_list=""${OUTPUT_BUCKET}""/training_set.with_label.tfrecord-?????-of-00016.gz \. --output_pattern_prefix=""${OUTPUT_BUCKET}/training_set.with_label.shuffled"" \. --output_dataset_name=""HG001"" \. --output_dataset_config_pbtxt=""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt"" \. --job_name=shuffle-tfrecords \. --runner=DataflowRunner \. --staging_location=""${OUTPUT_BUCKET}/staging"" \. --temp_location=""${OUTPUT_BUCKET}/tempdir"" \. --save_main_session \. --region us-east1. ```. This worked for me. It took `15m32.746s`. I didn't get the numpy issue. So I decided to run `pip3 freeze` to get my current versions so you can compare:. ```bash. $ pip3 freeze. absl-py==2.1.0. apache-beam==2.50.0. astunparse==1.6.3. cachetools==5.3.3. certifi==2024.2.2. charset-normalizer==3.3.2. cloudpickle==2.2.1. crcmod==1.7. Deprecated==1.2.14. dill==0.3.1.1. dnspython==2.6.1. docopt==0.6.2. fastavro==1.9.4. fasteners==0.19. flatbuffers==24.3.7. gast==0.4.0. google-api-core==2.17.1. google-apitools==0.5.31. google-auth==2.28.2. google-auth-httplib2==0.1.1. google-auth-oauthlib==1.0.0. google-cloud-aiplatform==1.44.0. google-cloud-bigquery==3.19.0. google-cloud-bigquery-storage==2.24.0. google-cloud-bigtable==2.23.0. google-cloud-core==2.4.1. google-cloud-datastore==2.19.0. google-cloud-dlp==3.16.0. google-cloud-language==2.13.3. google-cloud-pubsub==2.20.2. google-cloud-pubsublite==1.9.0. google-cloud-recommendations-ai==0.10.10. google-cloud-resource-manager==1.12.3. google-cloud-spanner==3.44.0. google-cloud-storage==2.16.0. google-cloud-videointelligence==2.13.3. google-cloud-vision==3.7.2. google-crc32c==1.5.0. google-pasta==0.2.0. google-resumable-media==2.7.0. googleapis-common-protos==1.63.0. grpc-google-iam",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:6405,performance,resourc,resource-manager,6405,on \. --region us-east1. ```. This worked for me. It took `15m32.746s`. I didn't get the numpy issue. So I decided to run `pip3 freeze` to get my current versions so you can compare:. ```bash. $ pip3 freeze. absl-py==2.1.0. apache-beam==2.50.0. astunparse==1.6.3. cachetools==5.3.3. certifi==2024.2.2. charset-normalizer==3.3.2. cloudpickle==2.2.1. crcmod==1.7. Deprecated==1.2.14. dill==0.3.1.1. dnspython==2.6.1. docopt==0.6.2. fastavro==1.9.4. fasteners==0.19. flatbuffers==24.3.7. gast==0.4.0. google-api-core==2.17.1. google-apitools==0.5.31. google-auth==2.28.2. google-auth-httplib2==0.1.1. google-auth-oauthlib==1.0.0. google-cloud-aiplatform==1.44.0. google-cloud-bigquery==3.19.0. google-cloud-bigquery-storage==2.24.0. google-cloud-bigtable==2.23.0. google-cloud-core==2.4.1. google-cloud-datastore==2.19.0. google-cloud-dlp==3.16.0. google-cloud-language==2.13.3. google-cloud-pubsub==2.20.2. google-cloud-pubsublite==1.9.0. google-cloud-recommendations-ai==0.10.10. google-cloud-resource-manager==1.12.3. google-cloud-spanner==3.44.0. google-cloud-storage==2.16.0. google-cloud-videointelligence==2.13.3. google-cloud-vision==3.7.2. google-crc32c==1.5.0. google-pasta==0.2.0. google-resumable-media==2.7.0. googleapis-common-protos==1.63.0. grpc-google-iam-v1==0.13.0. grpc-interceptor==0.15.4. grpcio==1.62.1. grpcio-status==1.62.1. h5py==3.10.0. hdfs==2.7.3. httplib2==0.22.0. idna==3.6. importlib_metadata==7.0.2. keras==2.13.1. libclang==18.1.1. Markdown==3.6. MarkupSafe==2.1.5. numpy==1.24.3. oauth2client==4.1.3. oauthlib==3.2.2. objsize==0.6.1. opt-einsum==3.3.0. orjson==3.9.15. overrides==7.7.0. packaging==24.0. pkg_resources==0.0.0. proto-plus==1.23.0. protobuf==4.23.4. pyarrow==11.0.0. pyasn1==0.5.1. pyasn1-modules==0.3.0. pydot==1.4.2. pymongo==4.6.2. pyparsing==3.1.2. python-dateutil==2.9.0.post0. pytz==2024.1. regex==2023.12.25. requests==2.31.0. requests-oauthlib==1.4.0. rsa==4.9. shapely==2.0.3. six==1.16.0. sqlparse==0.4.4. tensorboard==2.13.0. tensorboard-data-,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:275,reliability,mainten,maintenance-policy,275,"# Follow https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-training-case-study.md. I got a machine:. ```bash. host=""${USER}-deepvariant-vm"". zone=""us-west1-b"". gcloud compute instances create ${host} \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --maintenance-policy ""TERMINATE"" \. --accelerator=type=nvidia-tesla-p100,count=1 \. --image-family ""ubuntu-2004-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""n1-standard-16"" \. --boot-disk-size ""300"" \. --zone ""${zone}"" \. --min-cpu-platform ""Intel Skylake"". ```. I sshed into my machine:. ```bash. gcloud compute ssh pichuan-deepvariant-vm --zone us-west1-b. ```. I ran this with my own `YOUR_PROJECT` and `OUTPUT_GCS_BUCKET` setting. Then the following is basically just copy/paste from the doc:. ```. BUCKET=""gs://deepvariant"". VERSION=""1.6.1"". DOCKER_IMAGE=""google/deepvariant:${VERSION}"". MODEL_BUCKET=""${BUCKET}/models/DeepVariant/${VERSION}/DeepVariant-inception_v3-${VERSION}+data-wgs_standard"". GCS_PRETRAINED_WGS_MODEL=""${MODEL_BUCKET}/model.ckpt"". OUTPUT_BUCKET=""${OUTPUT_GCS_BUCKET}/customized_training"". TRAINING_DIR=""${OUTPUT_BUCKET}/training_dir"". BASE=""${HOME}/training-case-study"". DATA_BUCKET=gs://deepvariant/training-case-study/BGISEQ-HG001. INPUT_DIR=""${BASE}/input"". BIN_DIR=""${INPUT_DIR}/bin"". DATA_DIR=""${INPUT_DIR}/data"". OUTPUT_DIR=""${BASE}/output"". LOG_DIR=""${OUTPUT_DIR}/logs"". SHUFFLE_SCRIPT_DIR=""${HOME}/deepvariant/tools"". REF=""${DATA_DIR}/ucsc_hg19.fa"". BAM_CHR1=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr1.bam"". BAM_CHR20=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr20.bam"". BAM_CHR21=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr21.bam"". TRUTH_VCF=""${DATA_DIR}/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_PGandRTGphasetransfer_chrs_FIXED.vcf.gz"". TRUTH_BED=""${DATA_DIR}/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_nosomaticdel_chr.bed"". N_SHARDS=16. ```. ```bash. mkdir -p ""${OUTPUT_DIR}"". mkd",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:1269,safety,input,input,1269," --maintenance-policy ""TERMINATE"" \. --accelerator=type=nvidia-tesla-p100,count=1 \. --image-family ""ubuntu-2004-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""n1-standard-16"" \. --boot-disk-size ""300"" \. --zone ""${zone}"" \. --min-cpu-platform ""Intel Skylake"". ```. I sshed into my machine:. ```bash. gcloud compute ssh pichuan-deepvariant-vm --zone us-west1-b. ```. I ran this with my own `YOUR_PROJECT` and `OUTPUT_GCS_BUCKET` setting. Then the following is basically just copy/paste from the doc:. ```. BUCKET=""gs://deepvariant"". VERSION=""1.6.1"". DOCKER_IMAGE=""google/deepvariant:${VERSION}"". MODEL_BUCKET=""${BUCKET}/models/DeepVariant/${VERSION}/DeepVariant-inception_v3-${VERSION}+data-wgs_standard"". GCS_PRETRAINED_WGS_MODEL=""${MODEL_BUCKET}/model.ckpt"". OUTPUT_BUCKET=""${OUTPUT_GCS_BUCKET}/customized_training"". TRAINING_DIR=""${OUTPUT_BUCKET}/training_dir"". BASE=""${HOME}/training-case-study"". DATA_BUCKET=gs://deepvariant/training-case-study/BGISEQ-HG001. INPUT_DIR=""${BASE}/input"". BIN_DIR=""${INPUT_DIR}/bin"". DATA_DIR=""${INPUT_DIR}/data"". OUTPUT_DIR=""${BASE}/output"". LOG_DIR=""${OUTPUT_DIR}/logs"". SHUFFLE_SCRIPT_DIR=""${HOME}/deepvariant/tools"". REF=""${DATA_DIR}/ucsc_hg19.fa"". BAM_CHR1=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr1.bam"". BAM_CHR20=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr20.bam"". BAM_CHR21=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr21.bam"". TRUTH_VCF=""${DATA_DIR}/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_PGandRTGphasetransfer_chrs_FIXED.vcf.gz"". TRUTH_BED=""${DATA_DIR}/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_nosomaticdel_chr.bed"". N_SHARDS=16. ```. ```bash. mkdir -p ""${OUTPUT_DIR}"". mkdir -p ""${BIN_DIR}"". mkdir -p ""${DATA_DIR}"". mkdir -p ""${LOG_DIR}"". ```. ```bash. gsutil -m cp ${DATA_BUCKET}/BGISEQ_PE100_NA12878.sorted.chr*.bam* ""${DATA_DIR}"". gsutil -m cp -r ""${DATA_BUCKET}/ucsc_hg19.fa*"" ""${DATA_DIR}"". gsutil -m cp -r ""${DATA_BUCKET}/HG001_GRCh37_GIA",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:1387,safety,log,logs,1387," \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""n1-standard-16"" \. --boot-disk-size ""300"" \. --zone ""${zone}"" \. --min-cpu-platform ""Intel Skylake"". ```. I sshed into my machine:. ```bash. gcloud compute ssh pichuan-deepvariant-vm --zone us-west1-b. ```. I ran this with my own `YOUR_PROJECT` and `OUTPUT_GCS_BUCKET` setting. Then the following is basically just copy/paste from the doc:. ```. BUCKET=""gs://deepvariant"". VERSION=""1.6.1"". DOCKER_IMAGE=""google/deepvariant:${VERSION}"". MODEL_BUCKET=""${BUCKET}/models/DeepVariant/${VERSION}/DeepVariant-inception_v3-${VERSION}+data-wgs_standard"". GCS_PRETRAINED_WGS_MODEL=""${MODEL_BUCKET}/model.ckpt"". OUTPUT_BUCKET=""${OUTPUT_GCS_BUCKET}/customized_training"". TRAINING_DIR=""${OUTPUT_BUCKET}/training_dir"". BASE=""${HOME}/training-case-study"". DATA_BUCKET=gs://deepvariant/training-case-study/BGISEQ-HG001. INPUT_DIR=""${BASE}/input"". BIN_DIR=""${INPUT_DIR}/bin"". DATA_DIR=""${INPUT_DIR}/data"". OUTPUT_DIR=""${BASE}/output"". LOG_DIR=""${OUTPUT_DIR}/logs"". SHUFFLE_SCRIPT_DIR=""${HOME}/deepvariant/tools"". REF=""${DATA_DIR}/ucsc_hg19.fa"". BAM_CHR1=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr1.bam"". BAM_CHR20=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr20.bam"". BAM_CHR21=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr21.bam"". TRUTH_VCF=""${DATA_DIR}/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_PGandRTGphasetransfer_chrs_FIXED.vcf.gz"". TRUTH_BED=""${DATA_DIR}/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_nosomaticdel_chr.bed"". N_SHARDS=16. ```. ```bash. mkdir -p ""${OUTPUT_DIR}"". mkdir -p ""${BIN_DIR}"". mkdir -p ""${DATA_DIR}"". mkdir -p ""${LOG_DIR}"". ```. ```bash. gsutil -m cp ${DATA_BUCKET}/BGISEQ_PE100_NA12878.sorted.chr*.bam* ""${DATA_DIR}"". gsutil -m cp -r ""${DATA_BUCKET}/ucsc_hg19.fa*"" ""${DATA_DIR}"". gsutil -m cp -r ""${DATA_BUCKET}/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_*"" ""${DATA_DIR}"". ```. ```bash. sudo apt -y upd",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:2386,safety,updat,update,2386,"gs"". SHUFFLE_SCRIPT_DIR=""${HOME}/deepvariant/tools"". REF=""${DATA_DIR}/ucsc_hg19.fa"". BAM_CHR1=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr1.bam"". BAM_CHR20=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr20.bam"". BAM_CHR21=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr21.bam"". TRUTH_VCF=""${DATA_DIR}/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_PGandRTGphasetransfer_chrs_FIXED.vcf.gz"". TRUTH_BED=""${DATA_DIR}/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_nosomaticdel_chr.bed"". N_SHARDS=16. ```. ```bash. mkdir -p ""${OUTPUT_DIR}"". mkdir -p ""${BIN_DIR}"". mkdir -p ""${DATA_DIR}"". mkdir -p ""${LOG_DIR}"". ```. ```bash. gsutil -m cp ${DATA_BUCKET}/BGISEQ_PE100_NA12878.sorted.chr*.bam* ""${DATA_DIR}"". gsutil -m cp -r ""${DATA_BUCKET}/ucsc_hg19.fa*"" ""${DATA_DIR}"". gsutil -m cp -r ""${DATA_BUCKET}/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_*"" ""${DATA_DIR}"". ```. ```bash. sudo apt -y update. sudo apt -y install parallel. curl -O https://raw.githubusercontent.com/google/deepvariant/r1.6.1/scripts/install_nvidia_docker.sh. bash -x install_nvidia_docker.sh. ```. ```bash. sudo docker pull ${DOCKER_IMAGE} # Standard CPU Docker Image. sudo docker pull ${DOCKER_IMAGE}-gpu # GPU-enabled Docker image. ```. ```bash. ( time seq 0 $((N_SHARDS-1)) | \. parallel --halt 2 --line-buffer \. sudo docker run \. -v ${HOME}:${HOME} \. ${DOCKER_IMAGE} \. make_examples \. --mode training \. --ref ""${REF}"" \. --reads ""${BAM_CHR1}"" \. --examples ""${OUTPUT_DIR}/training_set.with_label.tfrecord@${N_SHARDS}.gz"" \. --truth_variants ""${TRUTH_VCF}"" \. --confident_regions ""${TRUTH_BED}"" \. --task {} \. --regions ""'chr1'"" \. --channels ""insert_size"" \. ) 2>&1 | tee ""${LOG_DIR}/training_set.with_label.make_examples.log"". ```. This took `20m0.146s`. ```. $ cat ""${OUTPUT_DIR}/training_set.with_label.tfrecord-00000-of-00016.gz.example_info.json"". {""version"": ""1.6.0"", ""shape"": [100, 221, 7], ""channels"": [",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:3200,safety,log,log,3200,"9.fa*"" ""${DATA_DIR}"". gsutil -m cp -r ""${DATA_BUCKET}/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_*"" ""${DATA_DIR}"". ```. ```bash. sudo apt -y update. sudo apt -y install parallel. curl -O https://raw.githubusercontent.com/google/deepvariant/r1.6.1/scripts/install_nvidia_docker.sh. bash -x install_nvidia_docker.sh. ```. ```bash. sudo docker pull ${DOCKER_IMAGE} # Standard CPU Docker Image. sudo docker pull ${DOCKER_IMAGE}-gpu # GPU-enabled Docker image. ```. ```bash. ( time seq 0 $((N_SHARDS-1)) | \. parallel --halt 2 --line-buffer \. sudo docker run \. -v ${HOME}:${HOME} \. ${DOCKER_IMAGE} \. make_examples \. --mode training \. --ref ""${REF}"" \. --reads ""${BAM_CHR1}"" \. --examples ""${OUTPUT_DIR}/training_set.with_label.tfrecord@${N_SHARDS}.gz"" \. --truth_variants ""${TRUTH_VCF}"" \. --confident_regions ""${TRUTH_BED}"" \. --task {} \. --regions ""'chr1'"" \. --channels ""insert_size"" \. ) 2>&1 | tee ""${LOG_DIR}/training_set.with_label.make_examples.log"". ```. This took `20m0.146s`. ```. $ cat ""${OUTPUT_DIR}/training_set.with_label.tfrecord-00000-of-00016.gz.example_info.json"". {""version"": ""1.6.0"", ""shape"": [100, 221, 7], ""channels"": [1, 2, 3, 4, 5, 6, 19]}. ```. ```bash. gsutil -m cp ${OUTPUT_DIR}/training_set.with_label.tfrecord-?????-of-00016.gz* \. ${OUTPUT_BUCKET}. ```. ```bash. ( time seq 0 $((N_SHARDS-1)) | \. parallel --halt 2 --line-buffer \. sudo docker run \. -v /home/${USER}:/home/${USER} \. ${DOCKER_IMAGE} \. make_examples \. --mode training \. --ref ""${REF}"" \. --reads ""${BAM_CHR21}"" \. --examples ""${OUTPUT_DIR}/validation_set.with_label.tfrecord@${N_SHARDS}.gz"" \. --truth_variants ""${TRUTH_VCF}"" \. --confident_regions ""${TRUTH_BED}"" \. --task {} \. --regions ""'chr21'"" \. --channels ""insert_size"" \. ) 2>&1 | tee ""${LOG_DIR}/validation_set.with_label.make_examples.log"". ```. This took `5m25.905s`. ```bash. gsutil -m cp ${OUTPUT_DIR}/validation_set.with_label.tfrecord-?????-of-00016.gz* \. ${OUTPUT_BUCKET}. ```. # This p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:4044,safety,log,log,4044,"t_regions ""${TRUTH_BED}"" \. --task {} \. --regions ""'chr1'"" \. --channels ""insert_size"" \. ) 2>&1 | tee ""${LOG_DIR}/training_set.with_label.make_examples.log"". ```. This took `20m0.146s`. ```. $ cat ""${OUTPUT_DIR}/training_set.with_label.tfrecord-00000-of-00016.gz.example_info.json"". {""version"": ""1.6.0"", ""shape"": [100, 221, 7], ""channels"": [1, 2, 3, 4, 5, 6, 19]}. ```. ```bash. gsutil -m cp ${OUTPUT_DIR}/training_set.with_label.tfrecord-?????-of-00016.gz* \. ${OUTPUT_BUCKET}. ```. ```bash. ( time seq 0 $((N_SHARDS-1)) | \. parallel --halt 2 --line-buffer \. sudo docker run \. -v /home/${USER}:/home/${USER} \. ${DOCKER_IMAGE} \. make_examples \. --mode training \. --ref ""${REF}"" \. --reads ""${BAM_CHR21}"" \. --examples ""${OUTPUT_DIR}/validation_set.with_label.tfrecord@${N_SHARDS}.gz"" \. --truth_variants ""${TRUTH_VCF}"" \. --confident_regions ""${TRUTH_BED}"" \. --task {} \. --regions ""'chr21'"" \. --channels ""insert_size"" \. ) 2>&1 | tee ""${LOG_DIR}/validation_set.with_label.make_examples.log"". ```. This took `5m25.905s`. ```bash. gsutil -m cp ${OUTPUT_DIR}/validation_set.with_label.tfrecord-?????-of-00016.gz* \. ${OUTPUT_BUCKET}. ```. # This parts starts shuffling... ```bash. sudo apt install -y python3.8-venv. # Create a virtualenv. python3 -m venv beam. # Activate the virtualenv. . beam/bin/activate. ```. ```bash. mkdir -p ${SHUFFLE_SCRIPT_DIR}. wget https://raw.githubusercontent.com/google/deepvariant/r1.6.1/tools/shuffle_tfrecords_beam.py -O ${SHUFFLE_SCRIPT_DIR}/shuffle_tfrecords_beam.py. ```. ```bash. sudo apt -y update && sudo apt -y install python3-pip. pip3 install --upgrade pip. pip3 install setuptools --upgrade. pip3 install apache_beam[gcp]==2.50.0 # 2.51.0 didn't work in my run. pip3 install tensorflow # For parsing tf.Example in shuffle_tfrecords_beam.py. ```. ```bash. time python3 ${SHUFFLE_SCRIPT_DIR}/shuffle_tfrecords_beam.py \. --project=""${YOUR_PROJECT}"" \. --input_pattern_list=""${OUTPUT_BUCKET}""/training_set.with_label.tfrecord-?????-of-00016.gz \. --",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:4586,safety,updat,update,4586,"t 2 --line-buffer \. sudo docker run \. -v /home/${USER}:/home/${USER} \. ${DOCKER_IMAGE} \. make_examples \. --mode training \. --ref ""${REF}"" \. --reads ""${BAM_CHR21}"" \. --examples ""${OUTPUT_DIR}/validation_set.with_label.tfrecord@${N_SHARDS}.gz"" \. --truth_variants ""${TRUTH_VCF}"" \. --confident_regions ""${TRUTH_BED}"" \. --task {} \. --regions ""'chr21'"" \. --channels ""insert_size"" \. ) 2>&1 | tee ""${LOG_DIR}/validation_set.with_label.make_examples.log"". ```. This took `5m25.905s`. ```bash. gsutil -m cp ${OUTPUT_DIR}/validation_set.with_label.tfrecord-?????-of-00016.gz* \. ${OUTPUT_BUCKET}. ```. # This parts starts shuffling... ```bash. sudo apt install -y python3.8-venv. # Create a virtualenv. python3 -m venv beam. # Activate the virtualenv. . beam/bin/activate. ```. ```bash. mkdir -p ${SHUFFLE_SCRIPT_DIR}. wget https://raw.githubusercontent.com/google/deepvariant/r1.6.1/tools/shuffle_tfrecords_beam.py -O ${SHUFFLE_SCRIPT_DIR}/shuffle_tfrecords_beam.py. ```. ```bash. sudo apt -y update && sudo apt -y install python3-pip. pip3 install --upgrade pip. pip3 install setuptools --upgrade. pip3 install apache_beam[gcp]==2.50.0 # 2.51.0 didn't work in my run. pip3 install tensorflow # For parsing tf.Example in shuffle_tfrecords_beam.py. ```. ```bash. time python3 ${SHUFFLE_SCRIPT_DIR}/shuffle_tfrecords_beam.py \. --project=""${YOUR_PROJECT}"" \. --input_pattern_list=""${OUTPUT_BUCKET}""/training_set.with_label.tfrecord-?????-of-00016.gz \. --output_pattern_prefix=""${OUTPUT_BUCKET}/training_set.with_label.shuffled"" \. --output_dataset_name=""HG001"" \. --output_dataset_config_pbtxt=""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt"" \. --job_name=shuffle-tfrecords \. --runner=DataflowRunner \. --staging_location=""${OUTPUT_BUCKET}/staging"" \. --temp_location=""${OUTPUT_BUCKET}/tempdir"" \. --save_main_session \. --region us-east1. ```. This worked for me. It took `15m32.746s`. I didn't get the numpy issue. So I decided to run `pip3 freeze` to get my current versions so you can co",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:6405,safety,resourc,resource-manager,6405,on \. --region us-east1. ```. This worked for me. It took `15m32.746s`. I didn't get the numpy issue. So I decided to run `pip3 freeze` to get my current versions so you can compare:. ```bash. $ pip3 freeze. absl-py==2.1.0. apache-beam==2.50.0. astunparse==1.6.3. cachetools==5.3.3. certifi==2024.2.2. charset-normalizer==3.3.2. cloudpickle==2.2.1. crcmod==1.7. Deprecated==1.2.14. dill==0.3.1.1. dnspython==2.6.1. docopt==0.6.2. fastavro==1.9.4. fasteners==0.19. flatbuffers==24.3.7. gast==0.4.0. google-api-core==2.17.1. google-apitools==0.5.31. google-auth==2.28.2. google-auth-httplib2==0.1.1. google-auth-oauthlib==1.0.0. google-cloud-aiplatform==1.44.0. google-cloud-bigquery==3.19.0. google-cloud-bigquery-storage==2.24.0. google-cloud-bigtable==2.23.0. google-cloud-core==2.4.1. google-cloud-datastore==2.19.0. google-cloud-dlp==3.16.0. google-cloud-language==2.13.3. google-cloud-pubsub==2.20.2. google-cloud-pubsublite==1.9.0. google-cloud-recommendations-ai==0.10.10. google-cloud-resource-manager==1.12.3. google-cloud-spanner==3.44.0. google-cloud-storage==2.16.0. google-cloud-videointelligence==2.13.3. google-cloud-vision==3.7.2. google-crc32c==1.5.0. google-pasta==0.2.0. google-resumable-media==2.7.0. googleapis-common-protos==1.63.0. grpc-google-iam-v1==0.13.0. grpc-interceptor==0.15.4. grpcio==1.62.1. grpcio-status==1.62.1. h5py==3.10.0. hdfs==2.7.3. httplib2==0.22.0. idna==3.6. importlib_metadata==7.0.2. keras==2.13.1. libclang==18.1.1. Markdown==3.6. MarkupSafe==2.1.5. numpy==1.24.3. oauth2client==4.1.3. oauthlib==3.2.2. objsize==0.6.1. opt-einsum==3.3.0. orjson==3.9.15. overrides==7.7.0. packaging==24.0. pkg_resources==0.0.0. proto-plus==1.23.0. protobuf==4.23.4. pyarrow==11.0.0. pyasn1==0.5.1. pyasn1-modules==0.3.0. pydot==1.4.2. pymongo==4.6.2. pyparsing==3.1.2. python-dateutil==2.9.0.post0. pytz==2024.1. regex==2023.12.25. requests==2.31.0. requests-oauthlib==1.4.0. rsa==4.9. shapely==2.0.3. six==1.16.0. sqlparse==0.4.4. tensorboard==2.13.0. tensorboard-data-,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:7148,safety,modul,modules,7148,"rmalizer==3.3.2. cloudpickle==2.2.1. crcmod==1.7. Deprecated==1.2.14. dill==0.3.1.1. dnspython==2.6.1. docopt==0.6.2. fastavro==1.9.4. fasteners==0.19. flatbuffers==24.3.7. gast==0.4.0. google-api-core==2.17.1. google-apitools==0.5.31. google-auth==2.28.2. google-auth-httplib2==0.1.1. google-auth-oauthlib==1.0.0. google-cloud-aiplatform==1.44.0. google-cloud-bigquery==3.19.0. google-cloud-bigquery-storage==2.24.0. google-cloud-bigtable==2.23.0. google-cloud-core==2.4.1. google-cloud-datastore==2.19.0. google-cloud-dlp==3.16.0. google-cloud-language==2.13.3. google-cloud-pubsub==2.20.2. google-cloud-pubsublite==1.9.0. google-cloud-recommendations-ai==0.10.10. google-cloud-resource-manager==1.12.3. google-cloud-spanner==3.44.0. google-cloud-storage==2.16.0. google-cloud-videointelligence==2.13.3. google-cloud-vision==3.7.2. google-crc32c==1.5.0. google-pasta==0.2.0. google-resumable-media==2.7.0. googleapis-common-protos==1.63.0. grpc-google-iam-v1==0.13.0. grpc-interceptor==0.15.4. grpcio==1.62.1. grpcio-status==1.62.1. h5py==3.10.0. hdfs==2.7.3. httplib2==0.22.0. idna==3.6. importlib_metadata==7.0.2. keras==2.13.1. libclang==18.1.1. Markdown==3.6. MarkupSafe==2.1.5. numpy==1.24.3. oauth2client==4.1.3. oauthlib==3.2.2. objsize==0.6.1. opt-einsum==3.3.0. orjson==3.9.15. overrides==7.7.0. packaging==24.0. pkg_resources==0.0.0. proto-plus==1.23.0. protobuf==4.23.4. pyarrow==11.0.0. pyasn1==0.5.1. pyasn1-modules==0.3.0. pydot==1.4.2. pymongo==4.6.2. pyparsing==3.1.2. python-dateutil==2.9.0.post0. pytz==2024.1. regex==2023.12.25. requests==2.31.0. requests-oauthlib==1.4.0. rsa==4.9. shapely==2.0.3. six==1.16.0. sqlparse==0.4.4. tensorboard==2.13.0. tensorboard-data-server==0.7.2. tensorflow==2.13.1. tensorflow-estimator==2.13.0. tensorflow-io-gcs-filesystem==0.34.0. termcolor==2.4.0. typing_extensions==4.5.0. urllib3==2.2.1. Werkzeug==3.0.1. wrapt==1.16.0. zipp==3.18.1. zstandard==0.22.0. ```. ---. @helizabeth1103 , can you see which version might be different from yours?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:287,security,polic,policy,287,"# Follow https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-training-case-study.md. I got a machine:. ```bash. host=""${USER}-deepvariant-vm"". zone=""us-west1-b"". gcloud compute instances create ${host} \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --maintenance-policy ""TERMINATE"" \. --accelerator=type=nvidia-tesla-p100,count=1 \. --image-family ""ubuntu-2004-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""n1-standard-16"" \. --boot-disk-size ""300"" \. --zone ""${zone}"" \. --min-cpu-platform ""Intel Skylake"". ```. I sshed into my machine:. ```bash. gcloud compute ssh pichuan-deepvariant-vm --zone us-west1-b. ```. I ran this with my own `YOUR_PROJECT` and `OUTPUT_GCS_BUCKET` setting. Then the following is basically just copy/paste from the doc:. ```. BUCKET=""gs://deepvariant"". VERSION=""1.6.1"". DOCKER_IMAGE=""google/deepvariant:${VERSION}"". MODEL_BUCKET=""${BUCKET}/models/DeepVariant/${VERSION}/DeepVariant-inception_v3-${VERSION}+data-wgs_standard"". GCS_PRETRAINED_WGS_MODEL=""${MODEL_BUCKET}/model.ckpt"". OUTPUT_BUCKET=""${OUTPUT_GCS_BUCKET}/customized_training"". TRAINING_DIR=""${OUTPUT_BUCKET}/training_dir"". BASE=""${HOME}/training-case-study"". DATA_BUCKET=gs://deepvariant/training-case-study/BGISEQ-HG001. INPUT_DIR=""${BASE}/input"". BIN_DIR=""${INPUT_DIR}/bin"". DATA_DIR=""${INPUT_DIR}/data"". OUTPUT_DIR=""${BASE}/output"". LOG_DIR=""${OUTPUT_DIR}/logs"". SHUFFLE_SCRIPT_DIR=""${HOME}/deepvariant/tools"". REF=""${DATA_DIR}/ucsc_hg19.fa"". BAM_CHR1=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr1.bam"". BAM_CHR20=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr20.bam"". BAM_CHR21=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr21.bam"". TRUTH_VCF=""${DATA_DIR}/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_PGandRTGphasetransfer_chrs_FIXED.vcf.gz"". TRUTH_BED=""${DATA_DIR}/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_nosomaticdel_chr.bed"". N_SHARDS=16. ```. ```bash. mkdir -p ""${OUTPUT_DIR}"". mkd",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:554,security,ssh,sshed,554,"# Follow https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-training-case-study.md. I got a machine:. ```bash. host=""${USER}-deepvariant-vm"". zone=""us-west1-b"". gcloud compute instances create ${host} \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --maintenance-policy ""TERMINATE"" \. --accelerator=type=nvidia-tesla-p100,count=1 \. --image-family ""ubuntu-2004-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""n1-standard-16"" \. --boot-disk-size ""300"" \. --zone ""${zone}"" \. --min-cpu-platform ""Intel Skylake"". ```. I sshed into my machine:. ```bash. gcloud compute ssh pichuan-deepvariant-vm --zone us-west1-b. ```. I ran this with my own `YOUR_PROJECT` and `OUTPUT_GCS_BUCKET` setting. Then the following is basically just copy/paste from the doc:. ```. BUCKET=""gs://deepvariant"". VERSION=""1.6.1"". DOCKER_IMAGE=""google/deepvariant:${VERSION}"". MODEL_BUCKET=""${BUCKET}/models/DeepVariant/${VERSION}/DeepVariant-inception_v3-${VERSION}+data-wgs_standard"". GCS_PRETRAINED_WGS_MODEL=""${MODEL_BUCKET}/model.ckpt"". OUTPUT_BUCKET=""${OUTPUT_GCS_BUCKET}/customized_training"". TRAINING_DIR=""${OUTPUT_BUCKET}/training_dir"". BASE=""${HOME}/training-case-study"". DATA_BUCKET=gs://deepvariant/training-case-study/BGISEQ-HG001. INPUT_DIR=""${BASE}/input"". BIN_DIR=""${INPUT_DIR}/bin"". DATA_DIR=""${INPUT_DIR}/data"". OUTPUT_DIR=""${BASE}/output"". LOG_DIR=""${OUTPUT_DIR}/logs"". SHUFFLE_SCRIPT_DIR=""${HOME}/deepvariant/tools"". REF=""${DATA_DIR}/ucsc_hg19.fa"". BAM_CHR1=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr1.bam"". BAM_CHR20=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr20.bam"". BAM_CHR21=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr21.bam"". TRUTH_VCF=""${DATA_DIR}/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_PGandRTGphasetransfer_chrs_FIXED.vcf.gz"". TRUTH_BED=""${DATA_DIR}/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_nosomaticdel_chr.bed"". N_SHARDS=16. ```. ```bash. mkdir -p ""${OUTPUT_DIR}"". mkd",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:602,security,ssh,ssh,602,"# Follow https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-training-case-study.md. I got a machine:. ```bash. host=""${USER}-deepvariant-vm"". zone=""us-west1-b"". gcloud compute instances create ${host} \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --maintenance-policy ""TERMINATE"" \. --accelerator=type=nvidia-tesla-p100,count=1 \. --image-family ""ubuntu-2004-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""n1-standard-16"" \. --boot-disk-size ""300"" \. --zone ""${zone}"" \. --min-cpu-platform ""Intel Skylake"". ```. I sshed into my machine:. ```bash. gcloud compute ssh pichuan-deepvariant-vm --zone us-west1-b. ```. I ran this with my own `YOUR_PROJECT` and `OUTPUT_GCS_BUCKET` setting. Then the following is basically just copy/paste from the doc:. ```. BUCKET=""gs://deepvariant"". VERSION=""1.6.1"". DOCKER_IMAGE=""google/deepvariant:${VERSION}"". MODEL_BUCKET=""${BUCKET}/models/DeepVariant/${VERSION}/DeepVariant-inception_v3-${VERSION}+data-wgs_standard"". GCS_PRETRAINED_WGS_MODEL=""${MODEL_BUCKET}/model.ckpt"". OUTPUT_BUCKET=""${OUTPUT_GCS_BUCKET}/customized_training"". TRAINING_DIR=""${OUTPUT_BUCKET}/training_dir"". BASE=""${HOME}/training-case-study"". DATA_BUCKET=gs://deepvariant/training-case-study/BGISEQ-HG001. INPUT_DIR=""${BASE}/input"". BIN_DIR=""${INPUT_DIR}/bin"". DATA_DIR=""${INPUT_DIR}/data"". OUTPUT_DIR=""${BASE}/output"". LOG_DIR=""${OUTPUT_DIR}/logs"". SHUFFLE_SCRIPT_DIR=""${HOME}/deepvariant/tools"". REF=""${DATA_DIR}/ucsc_hg19.fa"". BAM_CHR1=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr1.bam"". BAM_CHR20=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr20.bam"". BAM_CHR21=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr21.bam"". TRUTH_VCF=""${DATA_DIR}/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_PGandRTGphasetransfer_chrs_FIXED.vcf.gz"". TRUTH_BED=""${DATA_DIR}/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_nosomaticdel_chr.bed"". N_SHARDS=16. ```. ```bash. mkdir -p ""${OUTPUT_DIR}"". mkd",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:906,security,model,models,906,"# Follow https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-training-case-study.md. I got a machine:. ```bash. host=""${USER}-deepvariant-vm"". zone=""us-west1-b"". gcloud compute instances create ${host} \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --maintenance-policy ""TERMINATE"" \. --accelerator=type=nvidia-tesla-p100,count=1 \. --image-family ""ubuntu-2004-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""n1-standard-16"" \. --boot-disk-size ""300"" \. --zone ""${zone}"" \. --min-cpu-platform ""Intel Skylake"". ```. I sshed into my machine:. ```bash. gcloud compute ssh pichuan-deepvariant-vm --zone us-west1-b. ```. I ran this with my own `YOUR_PROJECT` and `OUTPUT_GCS_BUCKET` setting. Then the following is basically just copy/paste from the doc:. ```. BUCKET=""gs://deepvariant"". VERSION=""1.6.1"". DOCKER_IMAGE=""google/deepvariant:${VERSION}"". MODEL_BUCKET=""${BUCKET}/models/DeepVariant/${VERSION}/DeepVariant-inception_v3-${VERSION}+data-wgs_standard"". GCS_PRETRAINED_WGS_MODEL=""${MODEL_BUCKET}/model.ckpt"". OUTPUT_BUCKET=""${OUTPUT_GCS_BUCKET}/customized_training"". TRAINING_DIR=""${OUTPUT_BUCKET}/training_dir"". BASE=""${HOME}/training-case-study"". DATA_BUCKET=gs://deepvariant/training-case-study/BGISEQ-HG001. INPUT_DIR=""${BASE}/input"". BIN_DIR=""${INPUT_DIR}/bin"". DATA_DIR=""${INPUT_DIR}/data"". OUTPUT_DIR=""${BASE}/output"". LOG_DIR=""${OUTPUT_DIR}/logs"". SHUFFLE_SCRIPT_DIR=""${HOME}/deepvariant/tools"". REF=""${DATA_DIR}/ucsc_hg19.fa"". BAM_CHR1=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr1.bam"". BAM_CHR20=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr20.bam"". BAM_CHR21=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr21.bam"". TRUTH_VCF=""${DATA_DIR}/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_PGandRTGphasetransfer_chrs_FIXED.vcf.gz"". TRUTH_BED=""${DATA_DIR}/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_nosomaticdel_chr.bed"". N_SHARDS=16. ```. ```bash. mkdir -p ""${OUTPUT_DIR}"". mkd",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:1034,security,model,model,1034,"epvariant/blob/r1.6.1/docs/deepvariant-training-case-study.md. I got a machine:. ```bash. host=""${USER}-deepvariant-vm"". zone=""us-west1-b"". gcloud compute instances create ${host} \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --maintenance-policy ""TERMINATE"" \. --accelerator=type=nvidia-tesla-p100,count=1 \. --image-family ""ubuntu-2004-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""n1-standard-16"" \. --boot-disk-size ""300"" \. --zone ""${zone}"" \. --min-cpu-platform ""Intel Skylake"". ```. I sshed into my machine:. ```bash. gcloud compute ssh pichuan-deepvariant-vm --zone us-west1-b. ```. I ran this with my own `YOUR_PROJECT` and `OUTPUT_GCS_BUCKET` setting. Then the following is basically just copy/paste from the doc:. ```. BUCKET=""gs://deepvariant"". VERSION=""1.6.1"". DOCKER_IMAGE=""google/deepvariant:${VERSION}"". MODEL_BUCKET=""${BUCKET}/models/DeepVariant/${VERSION}/DeepVariant-inception_v3-${VERSION}+data-wgs_standard"". GCS_PRETRAINED_WGS_MODEL=""${MODEL_BUCKET}/model.ckpt"". OUTPUT_BUCKET=""${OUTPUT_GCS_BUCKET}/customized_training"". TRAINING_DIR=""${OUTPUT_BUCKET}/training_dir"". BASE=""${HOME}/training-case-study"". DATA_BUCKET=gs://deepvariant/training-case-study/BGISEQ-HG001. INPUT_DIR=""${BASE}/input"". BIN_DIR=""${INPUT_DIR}/bin"". DATA_DIR=""${INPUT_DIR}/data"". OUTPUT_DIR=""${BASE}/output"". LOG_DIR=""${OUTPUT_DIR}/logs"". SHUFFLE_SCRIPT_DIR=""${HOME}/deepvariant/tools"". REF=""${DATA_DIR}/ucsc_hg19.fa"". BAM_CHR1=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr1.bam"". BAM_CHR20=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr20.bam"". BAM_CHR21=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr21.bam"". TRUTH_VCF=""${DATA_DIR}/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_PGandRTGphasetransfer_chrs_FIXED.vcf.gz"". TRUTH_BED=""${DATA_DIR}/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_nosomaticdel_chr.bed"". N_SHARDS=16. ```. ```bash. mkdir -p ""${OUTPUT_DIR}"". mkdir -p ""${BIN_DIR}"". mkdir -p ""${DATA_",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:1387,security,log,logs,1387," \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""n1-standard-16"" \. --boot-disk-size ""300"" \. --zone ""${zone}"" \. --min-cpu-platform ""Intel Skylake"". ```. I sshed into my machine:. ```bash. gcloud compute ssh pichuan-deepvariant-vm --zone us-west1-b. ```. I ran this with my own `YOUR_PROJECT` and `OUTPUT_GCS_BUCKET` setting. Then the following is basically just copy/paste from the doc:. ```. BUCKET=""gs://deepvariant"". VERSION=""1.6.1"". DOCKER_IMAGE=""google/deepvariant:${VERSION}"". MODEL_BUCKET=""${BUCKET}/models/DeepVariant/${VERSION}/DeepVariant-inception_v3-${VERSION}+data-wgs_standard"". GCS_PRETRAINED_WGS_MODEL=""${MODEL_BUCKET}/model.ckpt"". OUTPUT_BUCKET=""${OUTPUT_GCS_BUCKET}/customized_training"". TRAINING_DIR=""${OUTPUT_BUCKET}/training_dir"". BASE=""${HOME}/training-case-study"". DATA_BUCKET=gs://deepvariant/training-case-study/BGISEQ-HG001. INPUT_DIR=""${BASE}/input"". BIN_DIR=""${INPUT_DIR}/bin"". DATA_DIR=""${INPUT_DIR}/data"". OUTPUT_DIR=""${BASE}/output"". LOG_DIR=""${OUTPUT_DIR}/logs"". SHUFFLE_SCRIPT_DIR=""${HOME}/deepvariant/tools"". REF=""${DATA_DIR}/ucsc_hg19.fa"". BAM_CHR1=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr1.bam"". BAM_CHR20=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr20.bam"". BAM_CHR21=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr21.bam"". TRUTH_VCF=""${DATA_DIR}/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_PGandRTGphasetransfer_chrs_FIXED.vcf.gz"". TRUTH_BED=""${DATA_DIR}/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_nosomaticdel_chr.bed"". N_SHARDS=16. ```. ```bash. mkdir -p ""${OUTPUT_DIR}"". mkdir -p ""${BIN_DIR}"". mkdir -p ""${DATA_DIR}"". mkdir -p ""${LOG_DIR}"". ```. ```bash. gsutil -m cp ${DATA_BUCKET}/BGISEQ_PE100_NA12878.sorted.chr*.bam* ""${DATA_DIR}"". gsutil -m cp -r ""${DATA_BUCKET}/ucsc_hg19.fa*"" ""${DATA_DIR}"". gsutil -m cp -r ""${DATA_BUCKET}/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_*"" ""${DATA_DIR}"". ```. ```bash. sudo apt -y upd",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:2379,security,apt,apt,2379,"_DIR}/logs"". SHUFFLE_SCRIPT_DIR=""${HOME}/deepvariant/tools"". REF=""${DATA_DIR}/ucsc_hg19.fa"". BAM_CHR1=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr1.bam"". BAM_CHR20=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr20.bam"". BAM_CHR21=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr21.bam"". TRUTH_VCF=""${DATA_DIR}/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_PGandRTGphasetransfer_chrs_FIXED.vcf.gz"". TRUTH_BED=""${DATA_DIR}/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_nosomaticdel_chr.bed"". N_SHARDS=16. ```. ```bash. mkdir -p ""${OUTPUT_DIR}"". mkdir -p ""${BIN_DIR}"". mkdir -p ""${DATA_DIR}"". mkdir -p ""${LOG_DIR}"". ```. ```bash. gsutil -m cp ${DATA_BUCKET}/BGISEQ_PE100_NA12878.sorted.chr*.bam* ""${DATA_DIR}"". gsutil -m cp -r ""${DATA_BUCKET}/ucsc_hg19.fa*"" ""${DATA_DIR}"". gsutil -m cp -r ""${DATA_BUCKET}/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_*"" ""${DATA_DIR}"". ```. ```bash. sudo apt -y update. sudo apt -y install parallel. curl -O https://raw.githubusercontent.com/google/deepvariant/r1.6.1/scripts/install_nvidia_docker.sh. bash -x install_nvidia_docker.sh. ```. ```bash. sudo docker pull ${DOCKER_IMAGE} # Standard CPU Docker Image. sudo docker pull ${DOCKER_IMAGE}-gpu # GPU-enabled Docker image. ```. ```bash. ( time seq 0 $((N_SHARDS-1)) | \. parallel --halt 2 --line-buffer \. sudo docker run \. -v ${HOME}:${HOME} \. ${DOCKER_IMAGE} \. make_examples \. --mode training \. --ref ""${REF}"" \. --reads ""${BAM_CHR1}"" \. --examples ""${OUTPUT_DIR}/training_set.with_label.tfrecord@${N_SHARDS}.gz"" \. --truth_variants ""${TRUTH_VCF}"" \. --confident_regions ""${TRUTH_BED}"" \. --task {} \. --regions ""'chr1'"" \. --channels ""insert_size"" \. ) 2>&1 | tee ""${LOG_DIR}/training_set.with_label.make_examples.log"". ```. This took `20m0.146s`. ```. $ cat ""${OUTPUT_DIR}/training_set.with_label.tfrecord-00000-of-00016.gz.example_info.json"". {""version"": ""1.6.0"", ""shape"": [100, 221, 7], ""chan",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:2386,security,updat,update,2386,"gs"". SHUFFLE_SCRIPT_DIR=""${HOME}/deepvariant/tools"". REF=""${DATA_DIR}/ucsc_hg19.fa"". BAM_CHR1=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr1.bam"". BAM_CHR20=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr20.bam"". BAM_CHR21=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr21.bam"". TRUTH_VCF=""${DATA_DIR}/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_PGandRTGphasetransfer_chrs_FIXED.vcf.gz"". TRUTH_BED=""${DATA_DIR}/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_nosomaticdel_chr.bed"". N_SHARDS=16. ```. ```bash. mkdir -p ""${OUTPUT_DIR}"". mkdir -p ""${BIN_DIR}"". mkdir -p ""${DATA_DIR}"". mkdir -p ""${LOG_DIR}"". ```. ```bash. gsutil -m cp ${DATA_BUCKET}/BGISEQ_PE100_NA12878.sorted.chr*.bam* ""${DATA_DIR}"". gsutil -m cp -r ""${DATA_BUCKET}/ucsc_hg19.fa*"" ""${DATA_DIR}"". gsutil -m cp -r ""${DATA_BUCKET}/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_*"" ""${DATA_DIR}"". ```. ```bash. sudo apt -y update. sudo apt -y install parallel. curl -O https://raw.githubusercontent.com/google/deepvariant/r1.6.1/scripts/install_nvidia_docker.sh. bash -x install_nvidia_docker.sh. ```. ```bash. sudo docker pull ${DOCKER_IMAGE} # Standard CPU Docker Image. sudo docker pull ${DOCKER_IMAGE}-gpu # GPU-enabled Docker image. ```. ```bash. ( time seq 0 $((N_SHARDS-1)) | \. parallel --halt 2 --line-buffer \. sudo docker run \. -v ${HOME}:${HOME} \. ${DOCKER_IMAGE} \. make_examples \. --mode training \. --ref ""${REF}"" \. --reads ""${BAM_CHR1}"" \. --examples ""${OUTPUT_DIR}/training_set.with_label.tfrecord@${N_SHARDS}.gz"" \. --truth_variants ""${TRUTH_VCF}"" \. --confident_regions ""${TRUTH_BED}"" \. --task {} \. --regions ""'chr1'"" \. --channels ""insert_size"" \. ) 2>&1 | tee ""${LOG_DIR}/training_set.with_label.make_examples.log"". ```. This took `20m0.146s`. ```. $ cat ""${OUTPUT_DIR}/training_set.with_label.tfrecord-00000-of-00016.gz.example_info.json"". {""version"": ""1.6.0"", ""shape"": [100, 221, 7], ""channels"": [",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:2399,security,apt,apt,2399,"_SCRIPT_DIR=""${HOME}/deepvariant/tools"". REF=""${DATA_DIR}/ucsc_hg19.fa"". BAM_CHR1=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr1.bam"". BAM_CHR20=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr20.bam"". BAM_CHR21=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr21.bam"". TRUTH_VCF=""${DATA_DIR}/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_PGandRTGphasetransfer_chrs_FIXED.vcf.gz"". TRUTH_BED=""${DATA_DIR}/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_nosomaticdel_chr.bed"". N_SHARDS=16. ```. ```bash. mkdir -p ""${OUTPUT_DIR}"". mkdir -p ""${BIN_DIR}"". mkdir -p ""${DATA_DIR}"". mkdir -p ""${LOG_DIR}"". ```. ```bash. gsutil -m cp ${DATA_BUCKET}/BGISEQ_PE100_NA12878.sorted.chr*.bam* ""${DATA_DIR}"". gsutil -m cp -r ""${DATA_BUCKET}/ucsc_hg19.fa*"" ""${DATA_DIR}"". gsutil -m cp -r ""${DATA_BUCKET}/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_*"" ""${DATA_DIR}"". ```. ```bash. sudo apt -y update. sudo apt -y install parallel. curl -O https://raw.githubusercontent.com/google/deepvariant/r1.6.1/scripts/install_nvidia_docker.sh. bash -x install_nvidia_docker.sh. ```. ```bash. sudo docker pull ${DOCKER_IMAGE} # Standard CPU Docker Image. sudo docker pull ${DOCKER_IMAGE}-gpu # GPU-enabled Docker image. ```. ```bash. ( time seq 0 $((N_SHARDS-1)) | \. parallel --halt 2 --line-buffer \. sudo docker run \. -v ${HOME}:${HOME} \. ${DOCKER_IMAGE} \. make_examples \. --mode training \. --ref ""${REF}"" \. --reads ""${BAM_CHR1}"" \. --examples ""${OUTPUT_DIR}/training_set.with_label.tfrecord@${N_SHARDS}.gz"" \. --truth_variants ""${TRUTH_VCF}"" \. --confident_regions ""${TRUTH_BED}"" \. --task {} \. --regions ""'chr1'"" \. --channels ""insert_size"" \. ) 2>&1 | tee ""${LOG_DIR}/training_set.with_label.make_examples.log"". ```. This took `20m0.146s`. ```. $ cat ""${OUTPUT_DIR}/training_set.with_label.tfrecord-00000-of-00016.gz.example_info.json"". {""version"": ""1.6.0"", ""shape"": [100, 221, 7], ""channels"": [1, 2, 3, 4, ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:3200,security,log,log,3200,"9.fa*"" ""${DATA_DIR}"". gsutil -m cp -r ""${DATA_BUCKET}/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_*"" ""${DATA_DIR}"". ```. ```bash. sudo apt -y update. sudo apt -y install parallel. curl -O https://raw.githubusercontent.com/google/deepvariant/r1.6.1/scripts/install_nvidia_docker.sh. bash -x install_nvidia_docker.sh. ```. ```bash. sudo docker pull ${DOCKER_IMAGE} # Standard CPU Docker Image. sudo docker pull ${DOCKER_IMAGE}-gpu # GPU-enabled Docker image. ```. ```bash. ( time seq 0 $((N_SHARDS-1)) | \. parallel --halt 2 --line-buffer \. sudo docker run \. -v ${HOME}:${HOME} \. ${DOCKER_IMAGE} \. make_examples \. --mode training \. --ref ""${REF}"" \. --reads ""${BAM_CHR1}"" \. --examples ""${OUTPUT_DIR}/training_set.with_label.tfrecord@${N_SHARDS}.gz"" \. --truth_variants ""${TRUTH_VCF}"" \. --confident_regions ""${TRUTH_BED}"" \. --task {} \. --regions ""'chr1'"" \. --channels ""insert_size"" \. ) 2>&1 | tee ""${LOG_DIR}/training_set.with_label.make_examples.log"". ```. This took `20m0.146s`. ```. $ cat ""${OUTPUT_DIR}/training_set.with_label.tfrecord-00000-of-00016.gz.example_info.json"". {""version"": ""1.6.0"", ""shape"": [100, 221, 7], ""channels"": [1, 2, 3, 4, 5, 6, 19]}. ```. ```bash. gsutil -m cp ${OUTPUT_DIR}/training_set.with_label.tfrecord-?????-of-00016.gz* \. ${OUTPUT_BUCKET}. ```. ```bash. ( time seq 0 $((N_SHARDS-1)) | \. parallel --halt 2 --line-buffer \. sudo docker run \. -v /home/${USER}:/home/${USER} \. ${DOCKER_IMAGE} \. make_examples \. --mode training \. --ref ""${REF}"" \. --reads ""${BAM_CHR21}"" \. --examples ""${OUTPUT_DIR}/validation_set.with_label.tfrecord@${N_SHARDS}.gz"" \. --truth_variants ""${TRUTH_VCF}"" \. --confident_regions ""${TRUTH_BED}"" \. --task {} \. --regions ""'chr21'"" \. --channels ""insert_size"" \. ) 2>&1 | tee ""${LOG_DIR}/validation_set.with_label.make_examples.log"". ```. This took `5m25.905s`. ```bash. gsutil -m cp ${OUTPUT_DIR}/validation_set.with_label.tfrecord-?????-of-00016.gz* \. ${OUTPUT_BUCKET}. ```. # This p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:4044,security,log,log,4044,"t_regions ""${TRUTH_BED}"" \. --task {} \. --regions ""'chr1'"" \. --channels ""insert_size"" \. ) 2>&1 | tee ""${LOG_DIR}/training_set.with_label.make_examples.log"". ```. This took `20m0.146s`. ```. $ cat ""${OUTPUT_DIR}/training_set.with_label.tfrecord-00000-of-00016.gz.example_info.json"". {""version"": ""1.6.0"", ""shape"": [100, 221, 7], ""channels"": [1, 2, 3, 4, 5, 6, 19]}. ```. ```bash. gsutil -m cp ${OUTPUT_DIR}/training_set.with_label.tfrecord-?????-of-00016.gz* \. ${OUTPUT_BUCKET}. ```. ```bash. ( time seq 0 $((N_SHARDS-1)) | \. parallel --halt 2 --line-buffer \. sudo docker run \. -v /home/${USER}:/home/${USER} \. ${DOCKER_IMAGE} \. make_examples \. --mode training \. --ref ""${REF}"" \. --reads ""${BAM_CHR21}"" \. --examples ""${OUTPUT_DIR}/validation_set.with_label.tfrecord@${N_SHARDS}.gz"" \. --truth_variants ""${TRUTH_VCF}"" \. --confident_regions ""${TRUTH_BED}"" \. --task {} \. --regions ""'chr21'"" \. --channels ""insert_size"" \. ) 2>&1 | tee ""${LOG_DIR}/validation_set.with_label.make_examples.log"". ```. This took `5m25.905s`. ```bash. gsutil -m cp ${OUTPUT_DIR}/validation_set.with_label.tfrecord-?????-of-00016.gz* \. ${OUTPUT_BUCKET}. ```. # This parts starts shuffling... ```bash. sudo apt install -y python3.8-venv. # Create a virtualenv. python3 -m venv beam. # Activate the virtualenv. . beam/bin/activate. ```. ```bash. mkdir -p ${SHUFFLE_SCRIPT_DIR}. wget https://raw.githubusercontent.com/google/deepvariant/r1.6.1/tools/shuffle_tfrecords_beam.py -O ${SHUFFLE_SCRIPT_DIR}/shuffle_tfrecords_beam.py. ```. ```bash. sudo apt -y update && sudo apt -y install python3-pip. pip3 install --upgrade pip. pip3 install setuptools --upgrade. pip3 install apache_beam[gcp]==2.50.0 # 2.51.0 didn't work in my run. pip3 install tensorflow # For parsing tf.Example in shuffle_tfrecords_beam.py. ```. ```bash. time python3 ${SHUFFLE_SCRIPT_DIR}/shuffle_tfrecords_beam.py \. --project=""${YOUR_PROJECT}"" \. --input_pattern_list=""${OUTPUT_BUCKET}""/training_set.with_label.tfrecord-?????-of-00016.gz \. --",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:4241,security,apt,apt,4241,"t ""${OUTPUT_DIR}/training_set.with_label.tfrecord-00000-of-00016.gz.example_info.json"". {""version"": ""1.6.0"", ""shape"": [100, 221, 7], ""channels"": [1, 2, 3, 4, 5, 6, 19]}. ```. ```bash. gsutil -m cp ${OUTPUT_DIR}/training_set.with_label.tfrecord-?????-of-00016.gz* \. ${OUTPUT_BUCKET}. ```. ```bash. ( time seq 0 $((N_SHARDS-1)) | \. parallel --halt 2 --line-buffer \. sudo docker run \. -v /home/${USER}:/home/${USER} \. ${DOCKER_IMAGE} \. make_examples \. --mode training \. --ref ""${REF}"" \. --reads ""${BAM_CHR21}"" \. --examples ""${OUTPUT_DIR}/validation_set.with_label.tfrecord@${N_SHARDS}.gz"" \. --truth_variants ""${TRUTH_VCF}"" \. --confident_regions ""${TRUTH_BED}"" \. --task {} \. --regions ""'chr21'"" \. --channels ""insert_size"" \. ) 2>&1 | tee ""${LOG_DIR}/validation_set.with_label.make_examples.log"". ```. This took `5m25.905s`. ```bash. gsutil -m cp ${OUTPUT_DIR}/validation_set.with_label.tfrecord-?????-of-00016.gz* \. ${OUTPUT_BUCKET}. ```. # This parts starts shuffling... ```bash. sudo apt install -y python3.8-venv. # Create a virtualenv. python3 -m venv beam. # Activate the virtualenv. . beam/bin/activate. ```. ```bash. mkdir -p ${SHUFFLE_SCRIPT_DIR}. wget https://raw.githubusercontent.com/google/deepvariant/r1.6.1/tools/shuffle_tfrecords_beam.py -O ${SHUFFLE_SCRIPT_DIR}/shuffle_tfrecords_beam.py. ```. ```bash. sudo apt -y update && sudo apt -y install python3-pip. pip3 install --upgrade pip. pip3 install setuptools --upgrade. pip3 install apache_beam[gcp]==2.50.0 # 2.51.0 didn't work in my run. pip3 install tensorflow # For parsing tf.Example in shuffle_tfrecords_beam.py. ```. ```bash. time python3 ${SHUFFLE_SCRIPT_DIR}/shuffle_tfrecords_beam.py \. --project=""${YOUR_PROJECT}"" \. --input_pattern_list=""${OUTPUT_BUCKET}""/training_set.with_label.tfrecord-?????-of-00016.gz \. --output_pattern_prefix=""${OUTPUT_BUCKET}/training_set.with_label.shuffled"" \. --output_dataset_name=""HG001"" \. --output_dataset_config_pbtxt=""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt"" \. -",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:4579,security,apt,apt,4579,"el --halt 2 --line-buffer \. sudo docker run \. -v /home/${USER}:/home/${USER} \. ${DOCKER_IMAGE} \. make_examples \. --mode training \. --ref ""${REF}"" \. --reads ""${BAM_CHR21}"" \. --examples ""${OUTPUT_DIR}/validation_set.with_label.tfrecord@${N_SHARDS}.gz"" \. --truth_variants ""${TRUTH_VCF}"" \. --confident_regions ""${TRUTH_BED}"" \. --task {} \. --regions ""'chr21'"" \. --channels ""insert_size"" \. ) 2>&1 | tee ""${LOG_DIR}/validation_set.with_label.make_examples.log"". ```. This took `5m25.905s`. ```bash. gsutil -m cp ${OUTPUT_DIR}/validation_set.with_label.tfrecord-?????-of-00016.gz* \. ${OUTPUT_BUCKET}. ```. # This parts starts shuffling... ```bash. sudo apt install -y python3.8-venv. # Create a virtualenv. python3 -m venv beam. # Activate the virtualenv. . beam/bin/activate. ```. ```bash. mkdir -p ${SHUFFLE_SCRIPT_DIR}. wget https://raw.githubusercontent.com/google/deepvariant/r1.6.1/tools/shuffle_tfrecords_beam.py -O ${SHUFFLE_SCRIPT_DIR}/shuffle_tfrecords_beam.py. ```. ```bash. sudo apt -y update && sudo apt -y install python3-pip. pip3 install --upgrade pip. pip3 install setuptools --upgrade. pip3 install apache_beam[gcp]==2.50.0 # 2.51.0 didn't work in my run. pip3 install tensorflow # For parsing tf.Example in shuffle_tfrecords_beam.py. ```. ```bash. time python3 ${SHUFFLE_SCRIPT_DIR}/shuffle_tfrecords_beam.py \. --project=""${YOUR_PROJECT}"" \. --input_pattern_list=""${OUTPUT_BUCKET}""/training_set.with_label.tfrecord-?????-of-00016.gz \. --output_pattern_prefix=""${OUTPUT_BUCKET}/training_set.with_label.shuffled"" \. --output_dataset_name=""HG001"" \. --output_dataset_config_pbtxt=""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt"" \. --job_name=shuffle-tfrecords \. --runner=DataflowRunner \. --staging_location=""${OUTPUT_BUCKET}/staging"" \. --temp_location=""${OUTPUT_BUCKET}/tempdir"" \. --save_main_session \. --region us-east1. ```. This worked for me. It took `15m32.746s`. I didn't get the numpy issue. So I decided to run `pip3 freeze` to get my current versions so yo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:4586,security,updat,update,4586,"t 2 --line-buffer \. sudo docker run \. -v /home/${USER}:/home/${USER} \. ${DOCKER_IMAGE} \. make_examples \. --mode training \. --ref ""${REF}"" \. --reads ""${BAM_CHR21}"" \. --examples ""${OUTPUT_DIR}/validation_set.with_label.tfrecord@${N_SHARDS}.gz"" \. --truth_variants ""${TRUTH_VCF}"" \. --confident_regions ""${TRUTH_BED}"" \. --task {} \. --regions ""'chr21'"" \. --channels ""insert_size"" \. ) 2>&1 | tee ""${LOG_DIR}/validation_set.with_label.make_examples.log"". ```. This took `5m25.905s`. ```bash. gsutil -m cp ${OUTPUT_DIR}/validation_set.with_label.tfrecord-?????-of-00016.gz* \. ${OUTPUT_BUCKET}. ```. # This parts starts shuffling... ```bash. sudo apt install -y python3.8-venv. # Create a virtualenv. python3 -m venv beam. # Activate the virtualenv. . beam/bin/activate. ```. ```bash. mkdir -p ${SHUFFLE_SCRIPT_DIR}. wget https://raw.githubusercontent.com/google/deepvariant/r1.6.1/tools/shuffle_tfrecords_beam.py -O ${SHUFFLE_SCRIPT_DIR}/shuffle_tfrecords_beam.py. ```. ```bash. sudo apt -y update && sudo apt -y install python3-pip. pip3 install --upgrade pip. pip3 install setuptools --upgrade. pip3 install apache_beam[gcp]==2.50.0 # 2.51.0 didn't work in my run. pip3 install tensorflow # For parsing tf.Example in shuffle_tfrecords_beam.py. ```. ```bash. time python3 ${SHUFFLE_SCRIPT_DIR}/shuffle_tfrecords_beam.py \. --project=""${YOUR_PROJECT}"" \. --input_pattern_list=""${OUTPUT_BUCKET}""/training_set.with_label.tfrecord-?????-of-00016.gz \. --output_pattern_prefix=""${OUTPUT_BUCKET}/training_set.with_label.shuffled"" \. --output_dataset_name=""HG001"" \. --output_dataset_config_pbtxt=""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt"" \. --job_name=shuffle-tfrecords \. --runner=DataflowRunner \. --staging_location=""${OUTPUT_BUCKET}/staging"" \. --temp_location=""${OUTPUT_BUCKET}/tempdir"" \. --save_main_session \. --region us-east1. ```. This worked for me. It took `15m32.746s`. I didn't get the numpy issue. So I decided to run `pip3 freeze` to get my current versions so you can co",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:4601,security,apt,apt,4601,"fer \. sudo docker run \. -v /home/${USER}:/home/${USER} \. ${DOCKER_IMAGE} \. make_examples \. --mode training \. --ref ""${REF}"" \. --reads ""${BAM_CHR21}"" \. --examples ""${OUTPUT_DIR}/validation_set.with_label.tfrecord@${N_SHARDS}.gz"" \. --truth_variants ""${TRUTH_VCF}"" \. --confident_regions ""${TRUTH_BED}"" \. --task {} \. --regions ""'chr21'"" \. --channels ""insert_size"" \. ) 2>&1 | tee ""${LOG_DIR}/validation_set.with_label.make_examples.log"". ```. This took `5m25.905s`. ```bash. gsutil -m cp ${OUTPUT_DIR}/validation_set.with_label.tfrecord-?????-of-00016.gz* \. ${OUTPUT_BUCKET}. ```. # This parts starts shuffling... ```bash. sudo apt install -y python3.8-venv. # Create a virtualenv. python3 -m venv beam. # Activate the virtualenv. . beam/bin/activate. ```. ```bash. mkdir -p ${SHUFFLE_SCRIPT_DIR}. wget https://raw.githubusercontent.com/google/deepvariant/r1.6.1/tools/shuffle_tfrecords_beam.py -O ${SHUFFLE_SCRIPT_DIR}/shuffle_tfrecords_beam.py. ```. ```bash. sudo apt -y update && sudo apt -y install python3-pip. pip3 install --upgrade pip. pip3 install setuptools --upgrade. pip3 install apache_beam[gcp]==2.50.0 # 2.51.0 didn't work in my run. pip3 install tensorflow # For parsing tf.Example in shuffle_tfrecords_beam.py. ```. ```bash. time python3 ${SHUFFLE_SCRIPT_DIR}/shuffle_tfrecords_beam.py \. --project=""${YOUR_PROJECT}"" \. --input_pattern_list=""${OUTPUT_BUCKET}""/training_set.with_label.tfrecord-?????-of-00016.gz \. --output_pattern_prefix=""${OUTPUT_BUCKET}/training_set.with_label.shuffled"" \. --output_dataset_name=""HG001"" \. --output_dataset_config_pbtxt=""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt"" \. --job_name=shuffle-tfrecords \. --runner=DataflowRunner \. --staging_location=""${OUTPUT_BUCKET}/staging"" \. --temp_location=""${OUTPUT_BUCKET}/tempdir"" \. --save_main_session \. --region us-east1. ```. This worked for me. It took `15m32.746s`. I didn't get the numpy issue. So I decided to run `pip3 freeze` to get my current versions so you can compare:. ```bas",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:5696,security,certif,certifi,5696,"tall apache_beam[gcp]==2.50.0 # 2.51.0 didn't work in my run. pip3 install tensorflow # For parsing tf.Example in shuffle_tfrecords_beam.py. ```. ```bash. time python3 ${SHUFFLE_SCRIPT_DIR}/shuffle_tfrecords_beam.py \. --project=""${YOUR_PROJECT}"" \. --input_pattern_list=""${OUTPUT_BUCKET}""/training_set.with_label.tfrecord-?????-of-00016.gz \. --output_pattern_prefix=""${OUTPUT_BUCKET}/training_set.with_label.shuffled"" \. --output_dataset_name=""HG001"" \. --output_dataset_config_pbtxt=""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt"" \. --job_name=shuffle-tfrecords \. --runner=DataflowRunner \. --staging_location=""${OUTPUT_BUCKET}/staging"" \. --temp_location=""${OUTPUT_BUCKET}/tempdir"" \. --save_main_session \. --region us-east1. ```. This worked for me. It took `15m32.746s`. I didn't get the numpy issue. So I decided to run `pip3 freeze` to get my current versions so you can compare:. ```bash. $ pip3 freeze. absl-py==2.1.0. apache-beam==2.50.0. astunparse==1.6.3. cachetools==5.3.3. certifi==2024.2.2. charset-normalizer==3.3.2. cloudpickle==2.2.1. crcmod==1.7. Deprecated==1.2.14. dill==0.3.1.1. dnspython==2.6.1. docopt==0.6.2. fastavro==1.9.4. fasteners==0.19. flatbuffers==24.3.7. gast==0.4.0. google-api-core==2.17.1. google-apitools==0.5.31. google-auth==2.28.2. google-auth-httplib2==0.1.1. google-auth-oauthlib==1.0.0. google-cloud-aiplatform==1.44.0. google-cloud-bigquery==3.19.0. google-cloud-bigquery-storage==2.24.0. google-cloud-bigtable==2.23.0. google-cloud-core==2.4.1. google-cloud-datastore==2.19.0. google-cloud-dlp==3.16.0. google-cloud-language==2.13.3. google-cloud-pubsub==2.20.2. google-cloud-pubsublite==1.9.0. google-cloud-recommendations-ai==0.10.10. google-cloud-resource-manager==1.12.3. google-cloud-spanner==3.44.0. google-cloud-storage==2.16.0. google-cloud-videointelligence==2.13.3. google-cloud-vision==3.7.2. google-crc32c==1.5.0. google-pasta==0.2.0. google-resumable-media==2.7.0. googleapis-common-protos==1.63.0. grpc-google-iam-v1==0.13.0. grpc-",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:5810,security,dns,dnspython,5810,"huffle_tfrecords_beam.py. ```. ```bash. time python3 ${SHUFFLE_SCRIPT_DIR}/shuffle_tfrecords_beam.py \. --project=""${YOUR_PROJECT}"" \. --input_pattern_list=""${OUTPUT_BUCKET}""/training_set.with_label.tfrecord-?????-of-00016.gz \. --output_pattern_prefix=""${OUTPUT_BUCKET}/training_set.with_label.shuffled"" \. --output_dataset_name=""HG001"" \. --output_dataset_config_pbtxt=""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt"" \. --job_name=shuffle-tfrecords \. --runner=DataflowRunner \. --staging_location=""${OUTPUT_BUCKET}/staging"" \. --temp_location=""${OUTPUT_BUCKET}/tempdir"" \. --save_main_session \. --region us-east1. ```. This worked for me. It took `15m32.746s`. I didn't get the numpy issue. So I decided to run `pip3 freeze` to get my current versions so you can compare:. ```bash. $ pip3 freeze. absl-py==2.1.0. apache-beam==2.50.0. astunparse==1.6.3. cachetools==5.3.3. certifi==2024.2.2. charset-normalizer==3.3.2. cloudpickle==2.2.1. crcmod==1.7. Deprecated==1.2.14. dill==0.3.1.1. dnspython==2.6.1. docopt==0.6.2. fastavro==1.9.4. fasteners==0.19. flatbuffers==24.3.7. gast==0.4.0. google-api-core==2.17.1. google-apitools==0.5.31. google-auth==2.28.2. google-auth-httplib2==0.1.1. google-auth-oauthlib==1.0.0. google-cloud-aiplatform==1.44.0. google-cloud-bigquery==3.19.0. google-cloud-bigquery-storage==2.24.0. google-cloud-bigtable==2.23.0. google-cloud-core==2.4.1. google-cloud-datastore==2.19.0. google-cloud-dlp==3.16.0. google-cloud-language==2.13.3. google-cloud-pubsub==2.20.2. google-cloud-pubsublite==1.9.0. google-cloud-recommendations-ai==0.10.10. google-cloud-resource-manager==1.12.3. google-cloud-spanner==3.44.0. google-cloud-storage==2.16.0. google-cloud-videointelligence==2.13.3. google-cloud-vision==3.7.2. google-crc32c==1.5.0. google-pasta==0.2.0. google-resumable-media==2.7.0. googleapis-common-protos==1.63.0. grpc-google-iam-v1==0.13.0. grpc-interceptor==0.15.4. grpcio==1.62.1. grpcio-status==1.62.1. h5py==3.10.0. hdfs==2.7.3. httplib2==0.22.0. idna==3.6.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:5968,security,auth,auth,5968,"=""${OUTPUT_BUCKET}""/training_set.with_label.tfrecord-?????-of-00016.gz \. --output_pattern_prefix=""${OUTPUT_BUCKET}/training_set.with_label.shuffled"" \. --output_dataset_name=""HG001"" \. --output_dataset_config_pbtxt=""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt"" \. --job_name=shuffle-tfrecords \. --runner=DataflowRunner \. --staging_location=""${OUTPUT_BUCKET}/staging"" \. --temp_location=""${OUTPUT_BUCKET}/tempdir"" \. --save_main_session \. --region us-east1. ```. This worked for me. It took `15m32.746s`. I didn't get the numpy issue. So I decided to run `pip3 freeze` to get my current versions so you can compare:. ```bash. $ pip3 freeze. absl-py==2.1.0. apache-beam==2.50.0. astunparse==1.6.3. cachetools==5.3.3. certifi==2024.2.2. charset-normalizer==3.3.2. cloudpickle==2.2.1. crcmod==1.7. Deprecated==1.2.14. dill==0.3.1.1. dnspython==2.6.1. docopt==0.6.2. fastavro==1.9.4. fasteners==0.19. flatbuffers==24.3.7. gast==0.4.0. google-api-core==2.17.1. google-apitools==0.5.31. google-auth==2.28.2. google-auth-httplib2==0.1.1. google-auth-oauthlib==1.0.0. google-cloud-aiplatform==1.44.0. google-cloud-bigquery==3.19.0. google-cloud-bigquery-storage==2.24.0. google-cloud-bigtable==2.23.0. google-cloud-core==2.4.1. google-cloud-datastore==2.19.0. google-cloud-dlp==3.16.0. google-cloud-language==2.13.3. google-cloud-pubsub==2.20.2. google-cloud-pubsublite==1.9.0. google-cloud-recommendations-ai==0.10.10. google-cloud-resource-manager==1.12.3. google-cloud-spanner==3.44.0. google-cloud-storage==2.16.0. google-cloud-videointelligence==2.13.3. google-cloud-vision==3.7.2. google-crc32c==1.5.0. google-pasta==0.2.0. google-resumable-media==2.7.0. googleapis-common-protos==1.63.0. grpc-google-iam-v1==0.13.0. grpc-interceptor==0.15.4. grpcio==1.62.1. grpcio-status==1.62.1. h5py==3.10.0. hdfs==2.7.3. httplib2==0.22.0. idna==3.6. importlib_metadata==7.0.2. keras==2.13.1. libclang==18.1.1. Markdown==3.6. MarkupSafe==2.1.5. numpy==1.24.3. oauth2client==4.1.3. oauthlib==3.2.2. objsize",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:5989,security,auth,auth-,5989,"aining_set.with_label.tfrecord-?????-of-00016.gz \. --output_pattern_prefix=""${OUTPUT_BUCKET}/training_set.with_label.shuffled"" \. --output_dataset_name=""HG001"" \. --output_dataset_config_pbtxt=""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt"" \. --job_name=shuffle-tfrecords \. --runner=DataflowRunner \. --staging_location=""${OUTPUT_BUCKET}/staging"" \. --temp_location=""${OUTPUT_BUCKET}/tempdir"" \. --save_main_session \. --region us-east1. ```. This worked for me. It took `15m32.746s`. I didn't get the numpy issue. So I decided to run `pip3 freeze` to get my current versions so you can compare:. ```bash. $ pip3 freeze. absl-py==2.1.0. apache-beam==2.50.0. astunparse==1.6.3. cachetools==5.3.3. certifi==2024.2.2. charset-normalizer==3.3.2. cloudpickle==2.2.1. crcmod==1.7. Deprecated==1.2.14. dill==0.3.1.1. dnspython==2.6.1. docopt==0.6.2. fastavro==1.9.4. fasteners==0.19. flatbuffers==24.3.7. gast==0.4.0. google-api-core==2.17.1. google-apitools==0.5.31. google-auth==2.28.2. google-auth-httplib2==0.1.1. google-auth-oauthlib==1.0.0. google-cloud-aiplatform==1.44.0. google-cloud-bigquery==3.19.0. google-cloud-bigquery-storage==2.24.0. google-cloud-bigtable==2.23.0. google-cloud-core==2.4.1. google-cloud-datastore==2.19.0. google-cloud-dlp==3.16.0. google-cloud-language==2.13.3. google-cloud-pubsub==2.20.2. google-cloud-pubsublite==1.9.0. google-cloud-recommendations-ai==0.10.10. google-cloud-resource-manager==1.12.3. google-cloud-spanner==3.44.0. google-cloud-storage==2.16.0. google-cloud-videointelligence==2.13.3. google-cloud-vision==3.7.2. google-crc32c==1.5.0. google-pasta==0.2.0. google-resumable-media==2.7.0. googleapis-common-protos==1.63.0. grpc-google-iam-v1==0.13.0. grpc-interceptor==0.15.4. grpcio==1.62.1. grpcio-status==1.62.1. h5py==3.10.0. hdfs==2.7.3. httplib2==0.22.0. idna==3.6. importlib_metadata==7.0.2. keras==2.13.1. libclang==18.1.1. Markdown==3.6. MarkupSafe==2.1.5. numpy==1.24.3. oauth2client==4.1.3. oauthlib==3.2.2. objsize==0.6.1. opt-einsum==3",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:6018,security,auth,auth-oauthlib,6018,"???-of-00016.gz \. --output_pattern_prefix=""${OUTPUT_BUCKET}/training_set.with_label.shuffled"" \. --output_dataset_name=""HG001"" \. --output_dataset_config_pbtxt=""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt"" \. --job_name=shuffle-tfrecords \. --runner=DataflowRunner \. --staging_location=""${OUTPUT_BUCKET}/staging"" \. --temp_location=""${OUTPUT_BUCKET}/tempdir"" \. --save_main_session \. --region us-east1. ```. This worked for me. It took `15m32.746s`. I didn't get the numpy issue. So I decided to run `pip3 freeze` to get my current versions so you can compare:. ```bash. $ pip3 freeze. absl-py==2.1.0. apache-beam==2.50.0. astunparse==1.6.3. cachetools==5.3.3. certifi==2024.2.2. charset-normalizer==3.3.2. cloudpickle==2.2.1. crcmod==1.7. Deprecated==1.2.14. dill==0.3.1.1. dnspython==2.6.1. docopt==0.6.2. fastavro==1.9.4. fasteners==0.19. flatbuffers==24.3.7. gast==0.4.0. google-api-core==2.17.1. google-apitools==0.5.31. google-auth==2.28.2. google-auth-httplib2==0.1.1. google-auth-oauthlib==1.0.0. google-cloud-aiplatform==1.44.0. google-cloud-bigquery==3.19.0. google-cloud-bigquery-storage==2.24.0. google-cloud-bigtable==2.23.0. google-cloud-core==2.4.1. google-cloud-datastore==2.19.0. google-cloud-dlp==3.16.0. google-cloud-language==2.13.3. google-cloud-pubsub==2.20.2. google-cloud-pubsublite==1.9.0. google-cloud-recommendations-ai==0.10.10. google-cloud-resource-manager==1.12.3. google-cloud-spanner==3.44.0. google-cloud-storage==2.16.0. google-cloud-videointelligence==2.13.3. google-cloud-vision==3.7.2. google-crc32c==1.5.0. google-pasta==0.2.0. google-resumable-media==2.7.0. googleapis-common-protos==1.63.0. grpc-google-iam-v1==0.13.0. grpc-interceptor==0.15.4. grpcio==1.62.1. grpcio-status==1.62.1. h5py==3.10.0. hdfs==2.7.3. httplib2==0.22.0. idna==3.6. importlib_metadata==7.0.2. keras==2.13.1. libclang==18.1.1. Markdown==3.6. MarkupSafe==2.1.5. numpy==1.24.3. oauth2client==4.1.3. oauthlib==3.2.2. objsize==0.6.1. opt-einsum==3.3.0. orjson==3.9.15. overrides==",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:6700,security,intercept,interceptor,6700,"024.2.2. charset-normalizer==3.3.2. cloudpickle==2.2.1. crcmod==1.7. Deprecated==1.2.14. dill==0.3.1.1. dnspython==2.6.1. docopt==0.6.2. fastavro==1.9.4. fasteners==0.19. flatbuffers==24.3.7. gast==0.4.0. google-api-core==2.17.1. google-apitools==0.5.31. google-auth==2.28.2. google-auth-httplib2==0.1.1. google-auth-oauthlib==1.0.0. google-cloud-aiplatform==1.44.0. google-cloud-bigquery==3.19.0. google-cloud-bigquery-storage==2.24.0. google-cloud-bigtable==2.23.0. google-cloud-core==2.4.1. google-cloud-datastore==2.19.0. google-cloud-dlp==3.16.0. google-cloud-language==2.13.3. google-cloud-pubsub==2.20.2. google-cloud-pubsublite==1.9.0. google-cloud-recommendations-ai==0.10.10. google-cloud-resource-manager==1.12.3. google-cloud-spanner==3.44.0. google-cloud-storage==2.16.0. google-cloud-videointelligence==2.13.3. google-cloud-vision==3.7.2. google-crc32c==1.5.0. google-pasta==0.2.0. google-resumable-media==2.7.0. googleapis-common-protos==1.63.0. grpc-google-iam-v1==0.13.0. grpc-interceptor==0.15.4. grpcio==1.62.1. grpcio-status==1.62.1. h5py==3.10.0. hdfs==2.7.3. httplib2==0.22.0. idna==3.6. importlib_metadata==7.0.2. keras==2.13.1. libclang==18.1.1. Markdown==3.6. MarkupSafe==2.1.5. numpy==1.24.3. oauth2client==4.1.3. oauthlib==3.2.2. objsize==0.6.1. opt-einsum==3.3.0. orjson==3.9.15. overrides==7.7.0. packaging==24.0. pkg_resources==0.0.0. proto-plus==1.23.0. protobuf==4.23.4. pyarrow==11.0.0. pyasn1==0.5.1. pyasn1-modules==0.3.0. pydot==1.4.2. pymongo==4.6.2. pyparsing==3.1.2. python-dateutil==2.9.0.post0. pytz==2024.1. regex==2023.12.25. requests==2.31.0. requests-oauthlib==1.4.0. rsa==4.9. shapely==2.0.3. six==1.16.0. sqlparse==0.4.4. tensorboard==2.13.0. tensorboard-data-server==0.7.2. tensorflow==2.13.1. tensorflow-estimator==2.13.0. tensorflow-io-gcs-filesystem==0.34.0. termcolor==2.4.0. typing_extensions==4.5.0. urllib3==2.2.1. Werkzeug==3.0.1. wrapt==1.16.0. zipp==3.18.1. zstandard==0.22.0. ```. ---. @helizabeth1103 , can you see which version might be di",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:7319,security,rsa,rsa,7319,"rmalizer==3.3.2. cloudpickle==2.2.1. crcmod==1.7. Deprecated==1.2.14. dill==0.3.1.1. dnspython==2.6.1. docopt==0.6.2. fastavro==1.9.4. fasteners==0.19. flatbuffers==24.3.7. gast==0.4.0. google-api-core==2.17.1. google-apitools==0.5.31. google-auth==2.28.2. google-auth-httplib2==0.1.1. google-auth-oauthlib==1.0.0. google-cloud-aiplatform==1.44.0. google-cloud-bigquery==3.19.0. google-cloud-bigquery-storage==2.24.0. google-cloud-bigtable==2.23.0. google-cloud-core==2.4.1. google-cloud-datastore==2.19.0. google-cloud-dlp==3.16.0. google-cloud-language==2.13.3. google-cloud-pubsub==2.20.2. google-cloud-pubsublite==1.9.0. google-cloud-recommendations-ai==0.10.10. google-cloud-resource-manager==1.12.3. google-cloud-spanner==3.44.0. google-cloud-storage==2.16.0. google-cloud-videointelligence==2.13.3. google-cloud-vision==3.7.2. google-crc32c==1.5.0. google-pasta==0.2.0. google-resumable-media==2.7.0. googleapis-common-protos==1.63.0. grpc-google-iam-v1==0.13.0. grpc-interceptor==0.15.4. grpcio==1.62.1. grpcio-status==1.62.1. h5py==3.10.0. hdfs==2.7.3. httplib2==0.22.0. idna==3.6. importlib_metadata==7.0.2. keras==2.13.1. libclang==18.1.1. Markdown==3.6. MarkupSafe==2.1.5. numpy==1.24.3. oauth2client==4.1.3. oauthlib==3.2.2. objsize==0.6.1. opt-einsum==3.3.0. orjson==3.9.15. overrides==7.7.0. packaging==24.0. pkg_resources==0.0.0. proto-plus==1.23.0. protobuf==4.23.4. pyarrow==11.0.0. pyasn1==0.5.1. pyasn1-modules==0.3.0. pydot==1.4.2. pymongo==4.6.2. pyparsing==3.1.2. python-dateutil==2.9.0.post0. pytz==2024.1. regex==2023.12.25. requests==2.31.0. requests-oauthlib==1.4.0. rsa==4.9. shapely==2.0.3. six==1.16.0. sqlparse==0.4.4. tensorboard==2.13.0. tensorboard-data-server==0.7.2. tensorflow==2.13.1. tensorflow-estimator==2.13.0. tensorflow-io-gcs-filesystem==0.34.0. termcolor==2.4.0. typing_extensions==4.5.0. urllib3==2.2.1. Werkzeug==3.0.1. wrapt==1.16.0. zipp==3.18.1. zstandard==0.22.0. ```. ---. @helizabeth1103 , can you see which version might be different from yours?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:1387,testability,log,logs,1387," \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""n1-standard-16"" \. --boot-disk-size ""300"" \. --zone ""${zone}"" \. --min-cpu-platform ""Intel Skylake"". ```. I sshed into my machine:. ```bash. gcloud compute ssh pichuan-deepvariant-vm --zone us-west1-b. ```. I ran this with my own `YOUR_PROJECT` and `OUTPUT_GCS_BUCKET` setting. Then the following is basically just copy/paste from the doc:. ```. BUCKET=""gs://deepvariant"". VERSION=""1.6.1"". DOCKER_IMAGE=""google/deepvariant:${VERSION}"". MODEL_BUCKET=""${BUCKET}/models/DeepVariant/${VERSION}/DeepVariant-inception_v3-${VERSION}+data-wgs_standard"". GCS_PRETRAINED_WGS_MODEL=""${MODEL_BUCKET}/model.ckpt"". OUTPUT_BUCKET=""${OUTPUT_GCS_BUCKET}/customized_training"". TRAINING_DIR=""${OUTPUT_BUCKET}/training_dir"". BASE=""${HOME}/training-case-study"". DATA_BUCKET=gs://deepvariant/training-case-study/BGISEQ-HG001. INPUT_DIR=""${BASE}/input"". BIN_DIR=""${INPUT_DIR}/bin"". DATA_DIR=""${INPUT_DIR}/data"". OUTPUT_DIR=""${BASE}/output"". LOG_DIR=""${OUTPUT_DIR}/logs"". SHUFFLE_SCRIPT_DIR=""${HOME}/deepvariant/tools"". REF=""${DATA_DIR}/ucsc_hg19.fa"". BAM_CHR1=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr1.bam"". BAM_CHR20=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr20.bam"". BAM_CHR21=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr21.bam"". TRUTH_VCF=""${DATA_DIR}/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_PGandRTGphasetransfer_chrs_FIXED.vcf.gz"". TRUTH_BED=""${DATA_DIR}/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_nosomaticdel_chr.bed"". N_SHARDS=16. ```. ```bash. mkdir -p ""${OUTPUT_DIR}"". mkdir -p ""${BIN_DIR}"". mkdir -p ""${DATA_DIR}"". mkdir -p ""${LOG_DIR}"". ```. ```bash. gsutil -m cp ${DATA_BUCKET}/BGISEQ_PE100_NA12878.sorted.chr*.bam* ""${DATA_DIR}"". gsutil -m cp -r ""${DATA_BUCKET}/ucsc_hg19.fa*"" ""${DATA_DIR}"". gsutil -m cp -r ""${DATA_BUCKET}/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_*"" ""${DATA_DIR}"". ```. ```bash. sudo apt -y upd",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:3200,testability,log,log,3200,"9.fa*"" ""${DATA_DIR}"". gsutil -m cp -r ""${DATA_BUCKET}/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_*"" ""${DATA_DIR}"". ```. ```bash. sudo apt -y update. sudo apt -y install parallel. curl -O https://raw.githubusercontent.com/google/deepvariant/r1.6.1/scripts/install_nvidia_docker.sh. bash -x install_nvidia_docker.sh. ```. ```bash. sudo docker pull ${DOCKER_IMAGE} # Standard CPU Docker Image. sudo docker pull ${DOCKER_IMAGE}-gpu # GPU-enabled Docker image. ```. ```bash. ( time seq 0 $((N_SHARDS-1)) | \. parallel --halt 2 --line-buffer \. sudo docker run \. -v ${HOME}:${HOME} \. ${DOCKER_IMAGE} \. make_examples \. --mode training \. --ref ""${REF}"" \. --reads ""${BAM_CHR1}"" \. --examples ""${OUTPUT_DIR}/training_set.with_label.tfrecord@${N_SHARDS}.gz"" \. --truth_variants ""${TRUTH_VCF}"" \. --confident_regions ""${TRUTH_BED}"" \. --task {} \. --regions ""'chr1'"" \. --channels ""insert_size"" \. ) 2>&1 | tee ""${LOG_DIR}/training_set.with_label.make_examples.log"". ```. This took `20m0.146s`. ```. $ cat ""${OUTPUT_DIR}/training_set.with_label.tfrecord-00000-of-00016.gz.example_info.json"". {""version"": ""1.6.0"", ""shape"": [100, 221, 7], ""channels"": [1, 2, 3, 4, 5, 6, 19]}. ```. ```bash. gsutil -m cp ${OUTPUT_DIR}/training_set.with_label.tfrecord-?????-of-00016.gz* \. ${OUTPUT_BUCKET}. ```. ```bash. ( time seq 0 $((N_SHARDS-1)) | \. parallel --halt 2 --line-buffer \. sudo docker run \. -v /home/${USER}:/home/${USER} \. ${DOCKER_IMAGE} \. make_examples \. --mode training \. --ref ""${REF}"" \. --reads ""${BAM_CHR21}"" \. --examples ""${OUTPUT_DIR}/validation_set.with_label.tfrecord@${N_SHARDS}.gz"" \. --truth_variants ""${TRUTH_VCF}"" \. --confident_regions ""${TRUTH_BED}"" \. --task {} \. --regions ""'chr21'"" \. --channels ""insert_size"" \. ) 2>&1 | tee ""${LOG_DIR}/validation_set.with_label.make_examples.log"". ```. This took `5m25.905s`. ```bash. gsutil -m cp ${OUTPUT_DIR}/validation_set.with_label.tfrecord-?????-of-00016.gz* \. ${OUTPUT_BUCKET}. ```. # This p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:4044,testability,log,log,4044,"t_regions ""${TRUTH_BED}"" \. --task {} \. --regions ""'chr1'"" \. --channels ""insert_size"" \. ) 2>&1 | tee ""${LOG_DIR}/training_set.with_label.make_examples.log"". ```. This took `20m0.146s`. ```. $ cat ""${OUTPUT_DIR}/training_set.with_label.tfrecord-00000-of-00016.gz.example_info.json"". {""version"": ""1.6.0"", ""shape"": [100, 221, 7], ""channels"": [1, 2, 3, 4, 5, 6, 19]}. ```. ```bash. gsutil -m cp ${OUTPUT_DIR}/training_set.with_label.tfrecord-?????-of-00016.gz* \. ${OUTPUT_BUCKET}. ```. ```bash. ( time seq 0 $((N_SHARDS-1)) | \. parallel --halt 2 --line-buffer \. sudo docker run \. -v /home/${USER}:/home/${USER} \. ${DOCKER_IMAGE} \. make_examples \. --mode training \. --ref ""${REF}"" \. --reads ""${BAM_CHR21}"" \. --examples ""${OUTPUT_DIR}/validation_set.with_label.tfrecord@${N_SHARDS}.gz"" \. --truth_variants ""${TRUTH_VCF}"" \. --confident_regions ""${TRUTH_BED}"" \. --task {} \. --regions ""'chr21'"" \. --channels ""insert_size"" \. ) 2>&1 | tee ""${LOG_DIR}/validation_set.with_label.make_examples.log"". ```. This took `5m25.905s`. ```bash. gsutil -m cp ${OUTPUT_DIR}/validation_set.with_label.tfrecord-?????-of-00016.gz* \. ${OUTPUT_BUCKET}. ```. # This parts starts shuffling... ```bash. sudo apt install -y python3.8-venv. # Create a virtualenv. python3 -m venv beam. # Activate the virtualenv. . beam/bin/activate. ```. ```bash. mkdir -p ${SHUFFLE_SCRIPT_DIR}. wget https://raw.githubusercontent.com/google/deepvariant/r1.6.1/tools/shuffle_tfrecords_beam.py -O ${SHUFFLE_SCRIPT_DIR}/shuffle_tfrecords_beam.py. ```. ```bash. sudo apt -y update && sudo apt -y install python3-pip. pip3 install --upgrade pip. pip3 install setuptools --upgrade. pip3 install apache_beam[gcp]==2.50.0 # 2.51.0 didn't work in my run. pip3 install tensorflow # For parsing tf.Example in shuffle_tfrecords_beam.py. ```. ```bash. time python3 ${SHUFFLE_SCRIPT_DIR}/shuffle_tfrecords_beam.py \. --project=""${YOUR_PROJECT}"" \. --input_pattern_list=""${OUTPUT_BUCKET}""/training_set.with_label.tfrecord-?????-of-00016.gz \. --",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:6405,testability,resourc,resource-manager,6405,on \. --region us-east1. ```. This worked for me. It took `15m32.746s`. I didn't get the numpy issue. So I decided to run `pip3 freeze` to get my current versions so you can compare:. ```bash. $ pip3 freeze. absl-py==2.1.0. apache-beam==2.50.0. astunparse==1.6.3. cachetools==5.3.3. certifi==2024.2.2. charset-normalizer==3.3.2. cloudpickle==2.2.1. crcmod==1.7. Deprecated==1.2.14. dill==0.3.1.1. dnspython==2.6.1. docopt==0.6.2. fastavro==1.9.4. fasteners==0.19. flatbuffers==24.3.7. gast==0.4.0. google-api-core==2.17.1. google-apitools==0.5.31. google-auth==2.28.2. google-auth-httplib2==0.1.1. google-auth-oauthlib==1.0.0. google-cloud-aiplatform==1.44.0. google-cloud-bigquery==3.19.0. google-cloud-bigquery-storage==2.24.0. google-cloud-bigtable==2.23.0. google-cloud-core==2.4.1. google-cloud-datastore==2.19.0. google-cloud-dlp==3.16.0. google-cloud-language==2.13.3. google-cloud-pubsub==2.20.2. google-cloud-pubsublite==1.9.0. google-cloud-recommendations-ai==0.10.10. google-cloud-resource-manager==1.12.3. google-cloud-spanner==3.44.0. google-cloud-storage==2.16.0. google-cloud-videointelligence==2.13.3. google-cloud-vision==3.7.2. google-crc32c==1.5.0. google-pasta==0.2.0. google-resumable-media==2.7.0. googleapis-common-protos==1.63.0. grpc-google-iam-v1==0.13.0. grpc-interceptor==0.15.4. grpcio==1.62.1. grpcio-status==1.62.1. h5py==3.10.0. hdfs==2.7.3. httplib2==0.22.0. idna==3.6. importlib_metadata==7.0.2. keras==2.13.1. libclang==18.1.1. Markdown==3.6. MarkupSafe==2.1.5. numpy==1.24.3. oauth2client==4.1.3. oauthlib==3.2.2. objsize==0.6.1. opt-einsum==3.3.0. orjson==3.9.15. overrides==7.7.0. packaging==24.0. pkg_resources==0.0.0. proto-plus==1.23.0. protobuf==4.23.4. pyarrow==11.0.0. pyasn1==0.5.1. pyasn1-modules==0.3.0. pydot==1.4.2. pymongo==4.6.2. pyparsing==3.1.2. python-dateutil==2.9.0.post0. pytz==2024.1. regex==2023.12.25. requests==2.31.0. requests-oauthlib==1.4.0. rsa==4.9. shapely==2.0.3. six==1.16.0. sqlparse==0.4.4. tensorboard==2.13.0. tensorboard-data-,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:135,usability,USER,USER,135,"# Follow https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-training-case-study.md. I got a machine:. ```bash. host=""${USER}-deepvariant-vm"". zone=""us-west1-b"". gcloud compute instances create ${host} \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --maintenance-policy ""TERMINATE"" \. --accelerator=type=nvidia-tesla-p100,count=1 \. --image-family ""ubuntu-2004-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""n1-standard-16"" \. --boot-disk-size ""300"" \. --zone ""${zone}"" \. --min-cpu-platform ""Intel Skylake"". ```. I sshed into my machine:. ```bash. gcloud compute ssh pichuan-deepvariant-vm --zone us-west1-b. ```. I ran this with my own `YOUR_PROJECT` and `OUTPUT_GCS_BUCKET` setting. Then the following is basically just copy/paste from the doc:. ```. BUCKET=""gs://deepvariant"". VERSION=""1.6.1"". DOCKER_IMAGE=""google/deepvariant:${VERSION}"". MODEL_BUCKET=""${BUCKET}/models/DeepVariant/${VERSION}/DeepVariant-inception_v3-${VERSION}+data-wgs_standard"". GCS_PRETRAINED_WGS_MODEL=""${MODEL_BUCKET}/model.ckpt"". OUTPUT_BUCKET=""${OUTPUT_GCS_BUCKET}/customized_training"". TRAINING_DIR=""${OUTPUT_BUCKET}/training_dir"". BASE=""${HOME}/training-case-study"". DATA_BUCKET=gs://deepvariant/training-case-study/BGISEQ-HG001. INPUT_DIR=""${BASE}/input"". BIN_DIR=""${INPUT_DIR}/bin"". DATA_DIR=""${INPUT_DIR}/data"". OUTPUT_DIR=""${BASE}/output"". LOG_DIR=""${OUTPUT_DIR}/logs"". SHUFFLE_SCRIPT_DIR=""${HOME}/deepvariant/tools"". REF=""${DATA_DIR}/ucsc_hg19.fa"". BAM_CHR1=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr1.bam"". BAM_CHR20=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr20.bam"". BAM_CHR21=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr21.bam"". TRUTH_VCF=""${DATA_DIR}/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_PGandRTGphasetransfer_chrs_FIXED.vcf.gz"". TRUTH_BED=""${DATA_DIR}/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_nosomaticdel_chr.bed"". N_SHARDS=16. ```. ```bash. mkdir -p ""${OUTPUT_DIR}"". mkd",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:1269,usability,input,input,1269," --maintenance-policy ""TERMINATE"" \. --accelerator=type=nvidia-tesla-p100,count=1 \. --image-family ""ubuntu-2004-lts"" \. --image-project ""ubuntu-os-cloud"" \. --machine-type ""n1-standard-16"" \. --boot-disk-size ""300"" \. --zone ""${zone}"" \. --min-cpu-platform ""Intel Skylake"". ```. I sshed into my machine:. ```bash. gcloud compute ssh pichuan-deepvariant-vm --zone us-west1-b. ```. I ran this with my own `YOUR_PROJECT` and `OUTPUT_GCS_BUCKET` setting. Then the following is basically just copy/paste from the doc:. ```. BUCKET=""gs://deepvariant"". VERSION=""1.6.1"". DOCKER_IMAGE=""google/deepvariant:${VERSION}"". MODEL_BUCKET=""${BUCKET}/models/DeepVariant/${VERSION}/DeepVariant-inception_v3-${VERSION}+data-wgs_standard"". GCS_PRETRAINED_WGS_MODEL=""${MODEL_BUCKET}/model.ckpt"". OUTPUT_BUCKET=""${OUTPUT_GCS_BUCKET}/customized_training"". TRAINING_DIR=""${OUTPUT_BUCKET}/training_dir"". BASE=""${HOME}/training-case-study"". DATA_BUCKET=gs://deepvariant/training-case-study/BGISEQ-HG001. INPUT_DIR=""${BASE}/input"". BIN_DIR=""${INPUT_DIR}/bin"". DATA_DIR=""${INPUT_DIR}/data"". OUTPUT_DIR=""${BASE}/output"". LOG_DIR=""${OUTPUT_DIR}/logs"". SHUFFLE_SCRIPT_DIR=""${HOME}/deepvariant/tools"". REF=""${DATA_DIR}/ucsc_hg19.fa"". BAM_CHR1=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr1.bam"". BAM_CHR20=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr20.bam"". BAM_CHR21=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr21.bam"". TRUTH_VCF=""${DATA_DIR}/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_PGandRTGphasetransfer_chrs_FIXED.vcf.gz"". TRUTH_BED=""${DATA_DIR}/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_nosomaticdel_chr.bed"". N_SHARDS=16. ```. ```bash. mkdir -p ""${OUTPUT_DIR}"". mkdir -p ""${BIN_DIR}"". mkdir -p ""${DATA_DIR}"". mkdir -p ""${LOG_DIR}"". ```. ```bash. gsutil -m cp ${DATA_BUCKET}/BGISEQ_PE100_NA12878.sorted.chr*.bam* ""${DATA_DIR}"". gsutil -m cp -r ""${DATA_BUCKET}/ucsc_hg19.fa*"" ""${DATA_DIR}"". gsutil -m cp -r ""${DATA_BUCKET}/HG001_GRCh37_GIA",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:1434,usability,tool,tools,1434,"ne-type ""n1-standard-16"" \. --boot-disk-size ""300"" \. --zone ""${zone}"" \. --min-cpu-platform ""Intel Skylake"". ```. I sshed into my machine:. ```bash. gcloud compute ssh pichuan-deepvariant-vm --zone us-west1-b. ```. I ran this with my own `YOUR_PROJECT` and `OUTPUT_GCS_BUCKET` setting. Then the following is basically just copy/paste from the doc:. ```. BUCKET=""gs://deepvariant"". VERSION=""1.6.1"". DOCKER_IMAGE=""google/deepvariant:${VERSION}"". MODEL_BUCKET=""${BUCKET}/models/DeepVariant/${VERSION}/DeepVariant-inception_v3-${VERSION}+data-wgs_standard"". GCS_PRETRAINED_WGS_MODEL=""${MODEL_BUCKET}/model.ckpt"". OUTPUT_BUCKET=""${OUTPUT_GCS_BUCKET}/customized_training"". TRAINING_DIR=""${OUTPUT_BUCKET}/training_dir"". BASE=""${HOME}/training-case-study"". DATA_BUCKET=gs://deepvariant/training-case-study/BGISEQ-HG001. INPUT_DIR=""${BASE}/input"". BIN_DIR=""${INPUT_DIR}/bin"". DATA_DIR=""${INPUT_DIR}/data"". OUTPUT_DIR=""${BASE}/output"". LOG_DIR=""${OUTPUT_DIR}/logs"". SHUFFLE_SCRIPT_DIR=""${HOME}/deepvariant/tools"". REF=""${DATA_DIR}/ucsc_hg19.fa"". BAM_CHR1=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr1.bam"". BAM_CHR20=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr20.bam"". BAM_CHR21=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr21.bam"". TRUTH_VCF=""${DATA_DIR}/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_PGandRTGphasetransfer_chrs_FIXED.vcf.gz"". TRUTH_BED=""${DATA_DIR}/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_nosomaticdel_chr.bed"". N_SHARDS=16. ```. ```bash. mkdir -p ""${OUTPUT_DIR}"". mkdir -p ""${BIN_DIR}"". mkdir -p ""${DATA_DIR}"". mkdir -p ""${LOG_DIR}"". ```. ```bash. gsutil -m cp ${DATA_BUCKET}/BGISEQ_PE100_NA12878.sorted.chr*.bam* ""${DATA_DIR}"". gsutil -m cp -r ""${DATA_BUCKET}/ucsc_hg19.fa*"" ""${DATA_DIR}"". gsutil -m cp -r ""${DATA_BUCKET}/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_*"" ""${DATA_DIR}"". ```. ```bash. sudo apt -y update. sudo apt -y install parallel. curl -O https",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:3640,usability,USER,USER,3640,"ocker pull ${DOCKER_IMAGE}-gpu # GPU-enabled Docker image. ```. ```bash. ( time seq 0 $((N_SHARDS-1)) | \. parallel --halt 2 --line-buffer \. sudo docker run \. -v ${HOME}:${HOME} \. ${DOCKER_IMAGE} \. make_examples \. --mode training \. --ref ""${REF}"" \. --reads ""${BAM_CHR1}"" \. --examples ""${OUTPUT_DIR}/training_set.with_label.tfrecord@${N_SHARDS}.gz"" \. --truth_variants ""${TRUTH_VCF}"" \. --confident_regions ""${TRUTH_BED}"" \. --task {} \. --regions ""'chr1'"" \. --channels ""insert_size"" \. ) 2>&1 | tee ""${LOG_DIR}/training_set.with_label.make_examples.log"". ```. This took `20m0.146s`. ```. $ cat ""${OUTPUT_DIR}/training_set.with_label.tfrecord-00000-of-00016.gz.example_info.json"". {""version"": ""1.6.0"", ""shape"": [100, 221, 7], ""channels"": [1, 2, 3, 4, 5, 6, 19]}. ```. ```bash. gsutil -m cp ${OUTPUT_DIR}/training_set.with_label.tfrecord-?????-of-00016.gz* \. ${OUTPUT_BUCKET}. ```. ```bash. ( time seq 0 $((N_SHARDS-1)) | \. parallel --halt 2 --line-buffer \. sudo docker run \. -v /home/${USER}:/home/${USER} \. ${DOCKER_IMAGE} \. make_examples \. --mode training \. --ref ""${REF}"" \. --reads ""${BAM_CHR21}"" \. --examples ""${OUTPUT_DIR}/validation_set.with_label.tfrecord@${N_SHARDS}.gz"" \. --truth_variants ""${TRUTH_VCF}"" \. --confident_regions ""${TRUTH_BED}"" \. --task {} \. --regions ""'chr21'"" \. --channels ""insert_size"" \. ) 2>&1 | tee ""${LOG_DIR}/validation_set.with_label.make_examples.log"". ```. This took `5m25.905s`. ```bash. gsutil -m cp ${OUTPUT_DIR}/validation_set.with_label.tfrecord-?????-of-00016.gz* \. ${OUTPUT_BUCKET}. ```. # This parts starts shuffling... ```bash. sudo apt install -y python3.8-venv. # Create a virtualenv. python3 -m venv beam. # Activate the virtualenv. . beam/bin/activate. ```. ```bash. mkdir -p ${SHUFFLE_SCRIPT_DIR}. wget https://raw.githubusercontent.com/google/deepvariant/r1.6.1/tools/shuffle_tfrecords_beam.py -O ${SHUFFLE_SCRIPT_DIR}/shuffle_tfrecords_beam.py. ```. ```bash. sudo apt -y update && sudo apt -y install python3-pip. pip3 install ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:3654,usability,USER,USER,3654,"OCKER_IMAGE}-gpu # GPU-enabled Docker image. ```. ```bash. ( time seq 0 $((N_SHARDS-1)) | \. parallel --halt 2 --line-buffer \. sudo docker run \. -v ${HOME}:${HOME} \. ${DOCKER_IMAGE} \. make_examples \. --mode training \. --ref ""${REF}"" \. --reads ""${BAM_CHR1}"" \. --examples ""${OUTPUT_DIR}/training_set.with_label.tfrecord@${N_SHARDS}.gz"" \. --truth_variants ""${TRUTH_VCF}"" \. --confident_regions ""${TRUTH_BED}"" \. --task {} \. --regions ""'chr1'"" \. --channels ""insert_size"" \. ) 2>&1 | tee ""${LOG_DIR}/training_set.with_label.make_examples.log"". ```. This took `20m0.146s`. ```. $ cat ""${OUTPUT_DIR}/training_set.with_label.tfrecord-00000-of-00016.gz.example_info.json"". {""version"": ""1.6.0"", ""shape"": [100, 221, 7], ""channels"": [1, 2, 3, 4, 5, 6, 19]}. ```. ```bash. gsutil -m cp ${OUTPUT_DIR}/training_set.with_label.tfrecord-?????-of-00016.gz* \. ${OUTPUT_BUCKET}. ```. ```bash. ( time seq 0 $((N_SHARDS-1)) | \. parallel --halt 2 --line-buffer \. sudo docker run \. -v /home/${USER}:/home/${USER} \. ${DOCKER_IMAGE} \. make_examples \. --mode training \. --ref ""${REF}"" \. --reads ""${BAM_CHR21}"" \. --examples ""${OUTPUT_DIR}/validation_set.with_label.tfrecord@${N_SHARDS}.gz"" \. --truth_variants ""${TRUTH_VCF}"" \. --confident_regions ""${TRUTH_BED}"" \. --task {} \. --regions ""'chr21'"" \. --channels ""insert_size"" \. ) 2>&1 | tee ""${LOG_DIR}/validation_set.with_label.make_examples.log"". ```. This took `5m25.905s`. ```bash. gsutil -m cp ${OUTPUT_DIR}/validation_set.with_label.tfrecord-?????-of-00016.gz* \. ${OUTPUT_BUCKET}. ```. # This parts starts shuffling... ```bash. sudo apt install -y python3.8-venv. # Create a virtualenv. python3 -m venv beam. # Activate the virtualenv. . beam/bin/activate. ```. ```bash. mkdir -p ${SHUFFLE_SCRIPT_DIR}. wget https://raw.githubusercontent.com/google/deepvariant/r1.6.1/tools/shuffle_tfrecords_beam.py -O ${SHUFFLE_SCRIPT_DIR}/shuffle_tfrecords_beam.py. ```. ```bash. sudo apt -y update && sudo apt -y install python3-pip. pip3 install --upgrade pip.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:4476,usability,tool,tools,4476,"frecord-?????-of-00016.gz* \. ${OUTPUT_BUCKET}. ```. ```bash. ( time seq 0 $((N_SHARDS-1)) | \. parallel --halt 2 --line-buffer \. sudo docker run \. -v /home/${USER}:/home/${USER} \. ${DOCKER_IMAGE} \. make_examples \. --mode training \. --ref ""${REF}"" \. --reads ""${BAM_CHR21}"" \. --examples ""${OUTPUT_DIR}/validation_set.with_label.tfrecord@${N_SHARDS}.gz"" \. --truth_variants ""${TRUTH_VCF}"" \. --confident_regions ""${TRUTH_BED}"" \. --task {} \. --regions ""'chr21'"" \. --channels ""insert_size"" \. ) 2>&1 | tee ""${LOG_DIR}/validation_set.with_label.make_examples.log"". ```. This took `5m25.905s`. ```bash. gsutil -m cp ${OUTPUT_DIR}/validation_set.with_label.tfrecord-?????-of-00016.gz* \. ${OUTPUT_BUCKET}. ```. # This parts starts shuffling... ```bash. sudo apt install -y python3.8-venv. # Create a virtualenv. python3 -m venv beam. # Activate the virtualenv. . beam/bin/activate. ```. ```bash. mkdir -p ${SHUFFLE_SCRIPT_DIR}. wget https://raw.githubusercontent.com/google/deepvariant/r1.6.1/tools/shuffle_tfrecords_beam.py -O ${SHUFFLE_SCRIPT_DIR}/shuffle_tfrecords_beam.py. ```. ```bash. sudo apt -y update && sudo apt -y install python3-pip. pip3 install --upgrade pip. pip3 install setuptools --upgrade. pip3 install apache_beam[gcp]==2.50.0 # 2.51.0 didn't work in my run. pip3 install tensorflow # For parsing tf.Example in shuffle_tfrecords_beam.py. ```. ```bash. time python3 ${SHUFFLE_SCRIPT_DIR}/shuffle_tfrecords_beam.py \. --project=""${YOUR_PROJECT}"" \. --input_pattern_list=""${OUTPUT_BUCKET}""/training_set.with_label.tfrecord-?????-of-00016.gz \. --output_pattern_prefix=""${OUTPUT_BUCKET}/training_set.with_label.shuffled"" \. --output_dataset_name=""HG001"" \. --output_dataset_config_pbtxt=""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt"" \. --job_name=shuffle-tfrecords \. --runner=DataflowRunner \. --staging_location=""${OUTPUT_BUCKET}/staging"" \. --temp_location=""${OUTPUT_BUCKET}/tempdir"" \. --save_main_session \. --region us-east1. ```. This worked for me. It took `15m32.7",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:6609,usability,resum,resumable-media,6609,ze. absl-py==2.1.0. apache-beam==2.50.0. astunparse==1.6.3. cachetools==5.3.3. certifi==2024.2.2. charset-normalizer==3.3.2. cloudpickle==2.2.1. crcmod==1.7. Deprecated==1.2.14. dill==0.3.1.1. dnspython==2.6.1. docopt==0.6.2. fastavro==1.9.4. fasteners==0.19. flatbuffers==24.3.7. gast==0.4.0. google-api-core==2.17.1. google-apitools==0.5.31. google-auth==2.28.2. google-auth-httplib2==0.1.1. google-auth-oauthlib==1.0.0. google-cloud-aiplatform==1.44.0. google-cloud-bigquery==3.19.0. google-cloud-bigquery-storage==2.24.0. google-cloud-bigtable==2.23.0. google-cloud-core==2.4.1. google-cloud-datastore==2.19.0. google-cloud-dlp==3.16.0. google-cloud-language==2.13.3. google-cloud-pubsub==2.20.2. google-cloud-pubsublite==1.9.0. google-cloud-recommendations-ai==0.10.10. google-cloud-resource-manager==1.12.3. google-cloud-spanner==3.44.0. google-cloud-storage==2.16.0. google-cloud-videointelligence==2.13.3. google-cloud-vision==3.7.2. google-crc32c==1.5.0. google-pasta==0.2.0. google-resumable-media==2.7.0. googleapis-common-protos==1.63.0. grpc-google-iam-v1==0.13.0. grpc-interceptor==0.15.4. grpcio==1.62.1. grpcio-status==1.62.1. h5py==3.10.0. hdfs==2.7.3. httplib2==0.22.0. idna==3.6. importlib_metadata==7.0.2. keras==2.13.1. libclang==18.1.1. Markdown==3.6. MarkupSafe==2.1.5. numpy==1.24.3. oauth2client==4.1.3. oauthlib==3.2.2. objsize==0.6.1. opt-einsum==3.3.0. orjson==3.9.15. overrides==7.7.0. packaging==24.0. pkg_resources==0.0.0. proto-plus==1.23.0. protobuf==4.23.4. pyarrow==11.0.0. pyasn1==0.5.1. pyasn1-modules==0.3.0. pydot==1.4.2. pymongo==4.6.2. pyparsing==3.1.2. python-dateutil==2.9.0.post0. pytz==2024.1. regex==2023.12.25. requests==2.31.0. requests-oauthlib==1.4.0. rsa==4.9. shapely==2.0.3. six==1.16.0. sqlparse==0.4.4. tensorboard==2.13.0. tensorboard-data-server==0.7.2. tensorflow==2.13.1. tensorflow-estimator==2.13.0. tensorflow-io-gcs-filesystem==0.34.0. termcolor==2.4.0. typing_extensions==4.5.0. urllib3==2.2.1. Werkzeug==3.0.1. wrapt==1.16.0. zipp==3.1,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:6744,usability,statu,status,6744,"rmalizer==3.3.2. cloudpickle==2.2.1. crcmod==1.7. Deprecated==1.2.14. dill==0.3.1.1. dnspython==2.6.1. docopt==0.6.2. fastavro==1.9.4. fasteners==0.19. flatbuffers==24.3.7. gast==0.4.0. google-api-core==2.17.1. google-apitools==0.5.31. google-auth==2.28.2. google-auth-httplib2==0.1.1. google-auth-oauthlib==1.0.0. google-cloud-aiplatform==1.44.0. google-cloud-bigquery==3.19.0. google-cloud-bigquery-storage==2.24.0. google-cloud-bigtable==2.23.0. google-cloud-core==2.4.1. google-cloud-datastore==2.19.0. google-cloud-dlp==3.16.0. google-cloud-language==2.13.3. google-cloud-pubsub==2.20.2. google-cloud-pubsublite==1.9.0. google-cloud-recommendations-ai==0.10.10. google-cloud-resource-manager==1.12.3. google-cloud-spanner==3.44.0. google-cloud-storage==2.16.0. google-cloud-videointelligence==2.13.3. google-cloud-vision==3.7.2. google-crc32c==1.5.0. google-pasta==0.2.0. google-resumable-media==2.7.0. googleapis-common-protos==1.63.0. grpc-google-iam-v1==0.13.0. grpc-interceptor==0.15.4. grpcio==1.62.1. grpcio-status==1.62.1. h5py==3.10.0. hdfs==2.7.3. httplib2==0.22.0. idna==3.6. importlib_metadata==7.0.2. keras==2.13.1. libclang==18.1.1. Markdown==3.6. MarkupSafe==2.1.5. numpy==1.24.3. oauth2client==4.1.3. oauthlib==3.2.2. objsize==0.6.1. opt-einsum==3.3.0. orjson==3.9.15. overrides==7.7.0. packaging==24.0. pkg_resources==0.0.0. proto-plus==1.23.0. protobuf==4.23.4. pyarrow==11.0.0. pyasn1==0.5.1. pyasn1-modules==0.3.0. pydot==1.4.2. pymongo==4.6.2. pyparsing==3.1.2. python-dateutil==2.9.0.post0. pytz==2024.1. regex==2023.12.25. requests==2.31.0. requests-oauthlib==1.4.0. rsa==4.9. shapely==2.0.3. six==1.16.0. sqlparse==0.4.4. tensorboard==2.13.0. tensorboard-data-server==0.7.2. tensorflow==2.13.1. tensorflow-estimator==2.13.0. tensorflow-io-gcs-filesystem==0.34.0. termcolor==2.4.0. typing_extensions==4.5.0. urllib3==2.2.1. Werkzeug==3.0.1. wrapt==1.16.0. zipp==3.18.1. zstandard==0.22.0. ```. ---. @helizabeth1103 , can you see which version might be different from yours?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:2,deployability,updat,updated,2,"I updated my versions of apache-beam and numpy to match yours and it seems like that worked. I was able to run the shuffle script on both my training file and the validation file. Thank you! To clarify your other response above, if I'm using one chromosome .bam each for my training and validation sets, then I should run make_examples once for each of those and shuffle on each of those single output files? . Thanks so much for your help, I really appreciate it!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:13,deployability,version,versions,13,"I updated my versions of apache-beam and numpy to match yours and it seems like that worked. I was able to run the shuffle script on both my training file and the validation file. Thank you! To clarify your other response above, if I'm using one chromosome .bam each for my training and validation sets, then I should run make_examples once for each of those and shuffle on each of those single output files? . Thanks so much for your help, I really appreciate it!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:13,integrability,version,versions,13,"I updated my versions of apache-beam and numpy to match yours and it seems like that worked. I was able to run the shuffle script on both my training file and the validation file. Thank you! To clarify your other response above, if I'm using one chromosome .bam each for my training and validation sets, then I should run make_examples once for each of those and shuffle on each of those single output files? . Thanks so much for your help, I really appreciate it!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:13,modifiability,version,versions,13,"I updated my versions of apache-beam and numpy to match yours and it seems like that worked. I was able to run the shuffle script on both my training file and the validation file. Thank you! To clarify your other response above, if I'm using one chromosome .bam each for my training and validation sets, then I should run make_examples once for each of those and shuffle on each of those single output files? . Thanks so much for your help, I really appreciate it!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:2,safety,updat,updated,2,"I updated my versions of apache-beam and numpy to match yours and it seems like that worked. I was able to run the shuffle script on both my training file and the validation file. Thank you! To clarify your other response above, if I'm using one chromosome .bam each for my training and validation sets, then I should run make_examples once for each of those and shuffle on each of those single output files? . Thanks so much for your help, I really appreciate it!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:163,safety,valid,validation,163,"I updated my versions of apache-beam and numpy to match yours and it seems like that worked. I was able to run the shuffle script on both my training file and the validation file. Thank you! To clarify your other response above, if I'm using one chromosome .bam each for my training and validation sets, then I should run make_examples once for each of those and shuffle on each of those single output files? . Thanks so much for your help, I really appreciate it!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:287,safety,valid,validation,287,"I updated my versions of apache-beam and numpy to match yours and it seems like that worked. I was able to run the shuffle script on both my training file and the validation file. Thank you! To clarify your other response above, if I'm using one chromosome .bam each for my training and validation sets, then I should run make_examples once for each of those and shuffle on each of those single output files? . Thanks so much for your help, I really appreciate it!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:2,security,updat,updated,2,"I updated my versions of apache-beam and numpy to match yours and it seems like that worked. I was able to run the shuffle script on both my training file and the validation file. Thank you! To clarify your other response above, if I'm using one chromosome .bam each for my training and validation sets, then I should run make_examples once for each of those and shuffle on each of those single output files? . Thanks so much for your help, I really appreciate it!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:163,security,validat,validation,163,"I updated my versions of apache-beam and numpy to match yours and it seems like that worked. I was able to run the shuffle script on both my training file and the validation file. Thank you! To clarify your other response above, if I'm using one chromosome .bam each for my training and validation sets, then I should run make_examples once for each of those and shuffle on each of those single output files? . Thanks so much for your help, I really appreciate it!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:287,security,validat,validation,287,"I updated my versions of apache-beam and numpy to match yours and it seems like that worked. I was able to run the shuffle script on both my training file and the validation file. Thank you! To clarify your other response above, if I'm using one chromosome .bam each for my training and validation sets, then I should run make_examples once for each of those and shuffle on each of those single output files? . Thanks so much for your help, I really appreciate it!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:435,usability,help,help,435,"I updated my versions of apache-beam and numpy to match yours and it seems like that worked. I was able to run the shuffle script on both my training file and the validation file. Thank you! To clarify your other response above, if I'm using one chromosome .bam each for my training and validation sets, then I should run make_examples once for each of those and shuffle on each of those single output files? . Thanks so much for your help, I really appreciate it!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:247,availability,consist,consistent,247,"@helizabeth1103 For training / validation sets -- the main point here is to keep them separate. . And then, for all data that goes into training set, they will need to be shuffle into one set of shards. So that you can get the num_examples, and a consistent path. For example, in our documentation you see something like:. ```. name: ""HG001"". tfrecord_path: ""OUTPUT_GCS_BUCKET/training_set.with_label.shuffled-?????-of-?????.tfrecord.gz"". num_examples: 342758. ```. For training, you need one `tfrecord_path` that refer to all the files (output of shulffling), and a num_examples. For validation, you need a separate file with similar format. Hope that's clear! I'll close this issue now that you're able to run shuffling!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:635,interoperability,format,format,635,"@helizabeth1103 For training / validation sets -- the main point here is to keep them separate. . And then, for all data that goes into training set, they will need to be shuffle into one set of shards. So that you can get the num_examples, and a consistent path. For example, in our documentation you see something like:. ```. name: ""HG001"". tfrecord_path: ""OUTPUT_GCS_BUCKET/training_set.with_label.shuffled-?????-of-?????.tfrecord.gz"". num_examples: 342758. ```. For training, you need one `tfrecord_path` that refer to all the files (output of shulffling), and a num_examples. For validation, you need a separate file with similar format. Hope that's clear! I'll close this issue now that you're able to run shuffling!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:31,safety,valid,validation,31,"@helizabeth1103 For training / validation sets -- the main point here is to keep them separate. . And then, for all data that goes into training set, they will need to be shuffle into one set of shards. So that you can get the num_examples, and a consistent path. For example, in our documentation you see something like:. ```. name: ""HG001"". tfrecord_path: ""OUTPUT_GCS_BUCKET/training_set.with_label.shuffled-?????-of-?????.tfrecord.gz"". num_examples: 342758. ```. For training, you need one `tfrecord_path` that refer to all the files (output of shulffling), and a num_examples. For validation, you need a separate file with similar format. Hope that's clear! I'll close this issue now that you're able to run shuffling!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:585,safety,valid,validation,585,"@helizabeth1103 For training / validation sets -- the main point here is to keep them separate. . And then, for all data that goes into training set, they will need to be shuffle into one set of shards. So that you can get the num_examples, and a consistent path. For example, in our documentation you see something like:. ```. name: ""HG001"". tfrecord_path: ""OUTPUT_GCS_BUCKET/training_set.with_label.shuffled-?????-of-?????.tfrecord.gz"". num_examples: 342758. ```. For training, you need one `tfrecord_path` that refer to all the files (output of shulffling), and a num_examples. For validation, you need a separate file with similar format. Hope that's clear! I'll close this issue now that you're able to run shuffling!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:31,security,validat,validation,31,"@helizabeth1103 For training / validation sets -- the main point here is to keep them separate. . And then, for all data that goes into training set, they will need to be shuffle into one set of shards. So that you can get the num_examples, and a consistent path. For example, in our documentation you see something like:. ```. name: ""HG001"". tfrecord_path: ""OUTPUT_GCS_BUCKET/training_set.with_label.shuffled-?????-of-?????.tfrecord.gz"". num_examples: 342758. ```. For training, you need one `tfrecord_path` that refer to all the files (output of shulffling), and a num_examples. For validation, you need a separate file with similar format. Hope that's clear! I'll close this issue now that you're able to run shuffling!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:585,security,validat,validation,585,"@helizabeth1103 For training / validation sets -- the main point here is to keep them separate. . And then, for all data that goes into training set, they will need to be shuffle into one set of shards. So that you can get the num_examples, and a consistent path. For example, in our documentation you see something like:. ```. name: ""HG001"". tfrecord_path: ""OUTPUT_GCS_BUCKET/training_set.with_label.shuffled-?????-of-?????.tfrecord.gz"". num_examples: 342758. ```. For training, you need one `tfrecord_path` that refer to all the files (output of shulffling), and a num_examples. For validation, you need a separate file with similar format. Hope that's clear! I'll close this issue now that you're able to run shuffling!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:247,usability,consist,consistent,247,"@helizabeth1103 For training / validation sets -- the main point here is to keep them separate. . And then, for all data that goes into training set, they will need to be shuffle into one set of shards. So that you can get the num_examples, and a consistent path. For example, in our documentation you see something like:. ```. name: ""HG001"". tfrecord_path: ""OUTPUT_GCS_BUCKET/training_set.with_label.shuffled-?????-of-?????.tfrecord.gz"". num_examples: 342758. ```. For training, you need one `tfrecord_path` that refer to all the files (output of shulffling), and a num_examples. For validation, you need a separate file with similar format. Hope that's clear! I'll close this issue now that you're able to run shuffling!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:284,usability,document,documentation,284,"@helizabeth1103 For training / validation sets -- the main point here is to keep them separate. . And then, for all data that goes into training set, they will need to be shuffle into one set of shards. So that you can get the num_examples, and a consistent path. For example, in our documentation you see something like:. ```. name: ""HG001"". tfrecord_path: ""OUTPUT_GCS_BUCKET/training_set.with_label.shuffled-?????-of-?????.tfrecord.gz"". num_examples: 342758. ```. For training, you need one `tfrecord_path` that refer to all the files (output of shulffling), and a num_examples. For validation, you need a separate file with similar format. Hope that's clear! I'll close this issue now that you're able to run shuffling!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:655,usability,clear,clear,655,"@helizabeth1103 For training / validation sets -- the main point here is to keep them separate. . And then, for all data that goes into training set, they will need to be shuffle into one set of shards. So that you can get the num_examples, and a consistent path. For example, in our documentation you see something like:. ```. name: ""HG001"". tfrecord_path: ""OUTPUT_GCS_BUCKET/training_set.with_label.shuffled-?????-of-?????.tfrecord.gz"". num_examples: 342758. ```. For training, you need one `tfrecord_path` that refer to all the files (output of shulffling), and a num_examples. For validation, you need a separate file with similar format. Hope that's clear! I'll close this issue now that you're able to run shuffling!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/793:667,usability,close,close,667,"@helizabeth1103 For training / validation sets -- the main point here is to keep them separate. . And then, for all data that goes into training set, they will need to be shuffle into one set of shards. So that you can get the num_examples, and a consistent path. For example, in our documentation you see something like:. ```. name: ""HG001"". tfrecord_path: ""OUTPUT_GCS_BUCKET/training_set.with_label.shuffled-?????-of-?????.tfrecord.gz"". num_examples: 342758. ```. For training, you need one `tfrecord_path` that refer to all the files (output of shulffling), and a num_examples. For validation, you need a separate file with similar format. Hope that's clear! I'll close this issue now that you're able to run shuffling!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/793
https://github.com/google/deepvariant/issues/794:135,interoperability,share,share,135,"Hi,. Thanks for brining up this issue. It'll be a bit tricky to debug this without having access to the files. Would it be possible to share the input files so we can try to reproduce this? Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/794
https://github.com/google/deepvariant/issues/794:145,safety,input,input,145,"Hi,. Thanks for brining up this issue. It'll be a bit tricky to debug this without having access to the files. Would it be possible to share the input files so we can try to reproduce this? Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/794
https://github.com/google/deepvariant/issues/794:90,security,access,access,90,"Hi,. Thanks for brining up this issue. It'll be a bit tricky to debug this without having access to the files. Would it be possible to share the input files so we can try to reproduce this? Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/794
https://github.com/google/deepvariant/issues/794:145,usability,input,input,145,"Hi,. Thanks for brining up this issue. It'll be a bit tricky to debug this without having access to the files. Would it be possible to share the input files so we can try to reproduce this? Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/794
https://github.com/google/deepvariant/issues/794:86,availability,fault,faults,86,"Sure thing! You can send me the files at lucasbrambrink@google.com. Additionally, Seg faults can sometimes happen from OOMs (running out of memory). Do you have the memory specs of the instance you are running this on? Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/794
https://github.com/google/deepvariant/issues/794:86,energy efficiency,fault,faults,86,"Sure thing! You can send me the files at lucasbrambrink@google.com. Additionally, Seg faults can sometimes happen from OOMs (running out of memory). Do you have the memory specs of the instance you are running this on? Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/794
https://github.com/google/deepvariant/issues/794:86,performance,fault,faults,86,"Sure thing! You can send me the files at lucasbrambrink@google.com. Additionally, Seg faults can sometimes happen from OOMs (running out of memory). Do you have the memory specs of the instance you are running this on? Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/794
https://github.com/google/deepvariant/issues/794:140,performance,memor,memory,140,"Sure thing! You can send me the files at lucasbrambrink@google.com. Additionally, Seg faults can sometimes happen from OOMs (running out of memory). Do you have the memory specs of the instance you are running this on? Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/794
https://github.com/google/deepvariant/issues/794:165,performance,memor,memory,165,"Sure thing! You can send me the files at lucasbrambrink@google.com. Additionally, Seg faults can sometimes happen from OOMs (running out of memory). Do you have the memory specs of the instance you are running this on? Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/794
https://github.com/google/deepvariant/issues/794:86,reliability,fault,faults,86,"Sure thing! You can send me the files at lucasbrambrink@google.com. Additionally, Seg faults can sometimes happen from OOMs (running out of memory). Do you have the memory specs of the instance you are running this on? Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/794
https://github.com/google/deepvariant/issues/794:86,safety,fault,faults,86,"Sure thing! You can send me the files at lucasbrambrink@google.com. Additionally, Seg faults can sometimes happen from OOMs (running out of memory). Do you have the memory specs of the instance you are running this on? Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/794
https://github.com/google/deepvariant/issues/794:140,usability,memor,memory,140,"Sure thing! You can send me the files at lucasbrambrink@google.com. Additionally, Seg faults can sometimes happen from OOMs (running out of memory). Do you have the memory specs of the instance you are running this on? Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/794
https://github.com/google/deepvariant/issues/794:165,usability,memor,memory,165,"Sure thing! You can send me the files at lucasbrambrink@google.com. Additionally, Seg faults can sometimes happen from OOMs (running out of memory). Do you have the memory specs of the instance you are running this on? Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/794
https://github.com/google/deepvariant/issues/794:356,availability,error,error,356,"It was run with 256G RAM node and all other samples finish in the same RAM nodes. I will send you data later. ________________________________. From: Lucas Brambrink ***@***.***>. Sent: Tuesday, March 26, 2024 6:46:34 PM. To: google/deepvariant ***@***.***>. Cc: Zhigui Bao ***@***.***>; Author ***@***.***>. Subject: Re: [google/deepvariant] Fatal Python error: Segmentation fault (Issue #794). Sure thing! You can send me the files at ***@***.******@***.***>. Additionally, Seg faults can sometimes happen from OOMs (running out of memory). Do you have the memory specs of the instance you are running this on? Thanks! —. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/794#issuecomment-2021094547>, or unsubscribe<https://github.com/notifications/unsubscribe-auth/AE5Y3VRRFLRCYTPDYDZDFY3Y2GX7VAVCNFSM6AAAAABFG7OINSVHI2DSMVQWIX3LMV43OSLTON2WKQ3PNVWWK3TUHMZDAMRRGA4TINJUG4>. You are receiving this because you authored the thread.Message ID: ***@***.***>.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/794
https://github.com/google/deepvariant/issues/794:376,availability,fault,fault,376,"It was run with 256G RAM node and all other samples finish in the same RAM nodes. I will send you data later. ________________________________. From: Lucas Brambrink ***@***.***>. Sent: Tuesday, March 26, 2024 6:46:34 PM. To: google/deepvariant ***@***.***>. Cc: Zhigui Bao ***@***.***>; Author ***@***.***>. Subject: Re: [google/deepvariant] Fatal Python error: Segmentation fault (Issue #794). Sure thing! You can send me the files at ***@***.******@***.***>. Additionally, Seg faults can sometimes happen from OOMs (running out of memory). Do you have the memory specs of the instance you are running this on? Thanks! —. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/794#issuecomment-2021094547>, or unsubscribe<https://github.com/notifications/unsubscribe-auth/AE5Y3VRRFLRCYTPDYDZDFY3Y2GX7VAVCNFSM6AAAAABFG7OINSVHI2DSMVQWIX3LMV43OSLTON2WKQ3PNVWWK3TUHMZDAMRRGA4TINJUG4>. You are receiving this because you authored the thread.Message ID: ***@***.***>.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/794
https://github.com/google/deepvariant/issues/794:480,availability,fault,faults,480,"It was run with 256G RAM node and all other samples finish in the same RAM nodes. I will send you data later. ________________________________. From: Lucas Brambrink ***@***.***>. Sent: Tuesday, March 26, 2024 6:46:34 PM. To: google/deepvariant ***@***.***>. Cc: Zhigui Bao ***@***.***>; Author ***@***.***>. Subject: Re: [google/deepvariant] Fatal Python error: Segmentation fault (Issue #794). Sure thing! You can send me the files at ***@***.******@***.***>. Additionally, Seg faults can sometimes happen from OOMs (running out of memory). Do you have the memory specs of the instance you are running this on? Thanks! —. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/794#issuecomment-2021094547>, or unsubscribe<https://github.com/notifications/unsubscribe-auth/AE5Y3VRRFLRCYTPDYDZDFY3Y2GX7VAVCNFSM6AAAAABFG7OINSVHI2DSMVQWIX3LMV43OSLTON2WKQ3PNVWWK3TUHMZDAMRRGA4TINJUG4>. You are receiving this because you authored the thread.Message ID: ***@***.***>.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/794
https://github.com/google/deepvariant/issues/794:376,energy efficiency,fault,fault,376,"It was run with 256G RAM node and all other samples finish in the same RAM nodes. I will send you data later. ________________________________. From: Lucas Brambrink ***@***.***>. Sent: Tuesday, March 26, 2024 6:46:34 PM. To: google/deepvariant ***@***.***>. Cc: Zhigui Bao ***@***.***>; Author ***@***.***>. Subject: Re: [google/deepvariant] Fatal Python error: Segmentation fault (Issue #794). Sure thing! You can send me the files at ***@***.******@***.***>. Additionally, Seg faults can sometimes happen from OOMs (running out of memory). Do you have the memory specs of the instance you are running this on? Thanks! —. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/794#issuecomment-2021094547>, or unsubscribe<https://github.com/notifications/unsubscribe-auth/AE5Y3VRRFLRCYTPDYDZDFY3Y2GX7VAVCNFSM6AAAAABFG7OINSVHI2DSMVQWIX3LMV43OSLTON2WKQ3PNVWWK3TUHMZDAMRRGA4TINJUG4>. You are receiving this because you authored the thread.Message ID: ***@***.***>.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/794
https://github.com/google/deepvariant/issues/794:480,energy efficiency,fault,faults,480,"It was run with 256G RAM node and all other samples finish in the same RAM nodes. I will send you data later. ________________________________. From: Lucas Brambrink ***@***.***>. Sent: Tuesday, March 26, 2024 6:46:34 PM. To: google/deepvariant ***@***.***>. Cc: Zhigui Bao ***@***.***>; Author ***@***.***>. Subject: Re: [google/deepvariant] Fatal Python error: Segmentation fault (Issue #794). Sure thing! You can send me the files at ***@***.******@***.***>. Additionally, Seg faults can sometimes happen from OOMs (running out of memory). Do you have the memory specs of the instance you are running this on? Thanks! —. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/794#issuecomment-2021094547>, or unsubscribe<https://github.com/notifications/unsubscribe-auth/AE5Y3VRRFLRCYTPDYDZDFY3Y2GX7VAVCNFSM6AAAAABFG7OINSVHI2DSMVQWIX3LMV43OSLTON2WKQ3PNVWWK3TUHMZDAMRRGA4TINJUG4>. You are receiving this because you authored the thread.Message ID: ***@***.***>.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/794
https://github.com/google/deepvariant/issues/794:309,integrability,Sub,Subject,309,"It was run with 256G RAM node and all other samples finish in the same RAM nodes. I will send you data later. ________________________________. From: Lucas Brambrink ***@***.***>. Sent: Tuesday, March 26, 2024 6:46:34 PM. To: google/deepvariant ***@***.***>. Cc: Zhigui Bao ***@***.***>; Author ***@***.***>. Subject: Re: [google/deepvariant] Fatal Python error: Segmentation fault (Issue #794). Sure thing! You can send me the files at ***@***.******@***.***>. Additionally, Seg faults can sometimes happen from OOMs (running out of memory). Do you have the memory specs of the instance you are running this on? Thanks! —. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/794#issuecomment-2021094547>, or unsubscribe<https://github.com/notifications/unsubscribe-auth/AE5Y3VRRFLRCYTPDYDZDFY3Y2GX7VAVCNFSM6AAAAABFG7OINSVHI2DSMVQWIX3LMV43OSLTON2WKQ3PNVWWK3TUHMZDAMRRGA4TINJUG4>. You are receiving this because you authored the thread.Message ID: ***@***.***>.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/794
https://github.com/google/deepvariant/issues/794:976,integrability,Messag,Message,976,"It was run with 256G RAM node and all other samples finish in the same RAM nodes. I will send you data later. ________________________________. From: Lucas Brambrink ***@***.***>. Sent: Tuesday, March 26, 2024 6:46:34 PM. To: google/deepvariant ***@***.***>. Cc: Zhigui Bao ***@***.***>; Author ***@***.***>. Subject: Re: [google/deepvariant] Fatal Python error: Segmentation fault (Issue #794). Sure thing! You can send me the files at ***@***.******@***.***>. Additionally, Seg faults can sometimes happen from OOMs (running out of memory). Do you have the memory specs of the instance you are running this on? Thanks! —. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/794#issuecomment-2021094547>, or unsubscribe<https://github.com/notifications/unsubscribe-auth/AE5Y3VRRFLRCYTPDYDZDFY3Y2GX7VAVCNFSM6AAAAABFG7OINSVHI2DSMVQWIX3LMV43OSLTON2WKQ3PNVWWK3TUHMZDAMRRGA4TINJUG4>. You are receiving this because you authored the thread.Message ID: ***@***.***>.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/794
https://github.com/google/deepvariant/issues/794:976,interoperability,Messag,Message,976,"It was run with 256G RAM node and all other samples finish in the same RAM nodes. I will send you data later. ________________________________. From: Lucas Brambrink ***@***.***>. Sent: Tuesday, March 26, 2024 6:46:34 PM. To: google/deepvariant ***@***.***>. Cc: Zhigui Bao ***@***.***>; Author ***@***.***>. Subject: Re: [google/deepvariant] Fatal Python error: Segmentation fault (Issue #794). Sure thing! You can send me the files at ***@***.******@***.***>. Additionally, Seg faults can sometimes happen from OOMs (running out of memory). Do you have the memory specs of the instance you are running this on? Thanks! —. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/794#issuecomment-2021094547>, or unsubscribe<https://github.com/notifications/unsubscribe-auth/AE5Y3VRRFLRCYTPDYDZDFY3Y2GX7VAVCNFSM6AAAAABFG7OINSVHI2DSMVQWIX3LMV43OSLTON2WKQ3PNVWWK3TUHMZDAMRRGA4TINJUG4>. You are receiving this because you authored the thread.Message ID: ***@***.***>.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/794
https://github.com/google/deepvariant/issues/794:356,performance,error,error,356,"It was run with 256G RAM node and all other samples finish in the same RAM nodes. I will send you data later. ________________________________. From: Lucas Brambrink ***@***.***>. Sent: Tuesday, March 26, 2024 6:46:34 PM. To: google/deepvariant ***@***.***>. Cc: Zhigui Bao ***@***.***>; Author ***@***.***>. Subject: Re: [google/deepvariant] Fatal Python error: Segmentation fault (Issue #794). Sure thing! You can send me the files at ***@***.******@***.***>. Additionally, Seg faults can sometimes happen from OOMs (running out of memory). Do you have the memory specs of the instance you are running this on? Thanks! —. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/794#issuecomment-2021094547>, or unsubscribe<https://github.com/notifications/unsubscribe-auth/AE5Y3VRRFLRCYTPDYDZDFY3Y2GX7VAVCNFSM6AAAAABFG7OINSVHI2DSMVQWIX3LMV43OSLTON2WKQ3PNVWWK3TUHMZDAMRRGA4TINJUG4>. You are receiving this because you authored the thread.Message ID: ***@***.***>.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/794
https://github.com/google/deepvariant/issues/794:376,performance,fault,fault,376,"It was run with 256G RAM node and all other samples finish in the same RAM nodes. I will send you data later. ________________________________. From: Lucas Brambrink ***@***.***>. Sent: Tuesday, March 26, 2024 6:46:34 PM. To: google/deepvariant ***@***.***>. Cc: Zhigui Bao ***@***.***>; Author ***@***.***>. Subject: Re: [google/deepvariant] Fatal Python error: Segmentation fault (Issue #794). Sure thing! You can send me the files at ***@***.******@***.***>. Additionally, Seg faults can sometimes happen from OOMs (running out of memory). Do you have the memory specs of the instance you are running this on? Thanks! —. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/794#issuecomment-2021094547>, or unsubscribe<https://github.com/notifications/unsubscribe-auth/AE5Y3VRRFLRCYTPDYDZDFY3Y2GX7VAVCNFSM6AAAAABFG7OINSVHI2DSMVQWIX3LMV43OSLTON2WKQ3PNVWWK3TUHMZDAMRRGA4TINJUG4>. You are receiving this because you authored the thread.Message ID: ***@***.***>.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/794
https://github.com/google/deepvariant/issues/794:480,performance,fault,faults,480,"It was run with 256G RAM node and all other samples finish in the same RAM nodes. I will send you data later. ________________________________. From: Lucas Brambrink ***@***.***>. Sent: Tuesday, March 26, 2024 6:46:34 PM. To: google/deepvariant ***@***.***>. Cc: Zhigui Bao ***@***.***>; Author ***@***.***>. Subject: Re: [google/deepvariant] Fatal Python error: Segmentation fault (Issue #794). Sure thing! You can send me the files at ***@***.******@***.***>. Additionally, Seg faults can sometimes happen from OOMs (running out of memory). Do you have the memory specs of the instance you are running this on? Thanks! —. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/794#issuecomment-2021094547>, or unsubscribe<https://github.com/notifications/unsubscribe-auth/AE5Y3VRRFLRCYTPDYDZDFY3Y2GX7VAVCNFSM6AAAAABFG7OINSVHI2DSMVQWIX3LMV43OSLTON2WKQ3PNVWWK3TUHMZDAMRRGA4TINJUG4>. You are receiving this because you authored the thread.Message ID: ***@***.***>.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/794
https://github.com/google/deepvariant/issues/794:534,performance,memor,memory,534,"It was run with 256G RAM node and all other samples finish in the same RAM nodes. I will send you data later. ________________________________. From: Lucas Brambrink ***@***.***>. Sent: Tuesday, March 26, 2024 6:46:34 PM. To: google/deepvariant ***@***.***>. Cc: Zhigui Bao ***@***.***>; Author ***@***.***>. Subject: Re: [google/deepvariant] Fatal Python error: Segmentation fault (Issue #794). Sure thing! You can send me the files at ***@***.******@***.***>. Additionally, Seg faults can sometimes happen from OOMs (running out of memory). Do you have the memory specs of the instance you are running this on? Thanks! —. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/794#issuecomment-2021094547>, or unsubscribe<https://github.com/notifications/unsubscribe-auth/AE5Y3VRRFLRCYTPDYDZDFY3Y2GX7VAVCNFSM6AAAAABFG7OINSVHI2DSMVQWIX3LMV43OSLTON2WKQ3PNVWWK3TUHMZDAMRRGA4TINJUG4>. You are receiving this because you authored the thread.Message ID: ***@***.***>.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/794
https://github.com/google/deepvariant/issues/794:559,performance,memor,memory,559,"It was run with 256G RAM node and all other samples finish in the same RAM nodes. I will send you data later. ________________________________. From: Lucas Brambrink ***@***.***>. Sent: Tuesday, March 26, 2024 6:46:34 PM. To: google/deepvariant ***@***.***>. Cc: Zhigui Bao ***@***.***>; Author ***@***.***>. Subject: Re: [google/deepvariant] Fatal Python error: Segmentation fault (Issue #794). Sure thing! You can send me the files at ***@***.******@***.***>. Additionally, Seg faults can sometimes happen from OOMs (running out of memory). Do you have the memory specs of the instance you are running this on? Thanks! —. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/794#issuecomment-2021094547>, or unsubscribe<https://github.com/notifications/unsubscribe-auth/AE5Y3VRRFLRCYTPDYDZDFY3Y2GX7VAVCNFSM6AAAAABFG7OINSVHI2DSMVQWIX3LMV43OSLTON2WKQ3PNVWWK3TUHMZDAMRRGA4TINJUG4>. You are receiving this because you authored the thread.Message ID: ***@***.***>.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/794
https://github.com/google/deepvariant/issues/794:376,reliability,fault,fault,376,"It was run with 256G RAM node and all other samples finish in the same RAM nodes. I will send you data later. ________________________________. From: Lucas Brambrink ***@***.***>. Sent: Tuesday, March 26, 2024 6:46:34 PM. To: google/deepvariant ***@***.***>. Cc: Zhigui Bao ***@***.***>; Author ***@***.***>. Subject: Re: [google/deepvariant] Fatal Python error: Segmentation fault (Issue #794). Sure thing! You can send me the files at ***@***.******@***.***>. Additionally, Seg faults can sometimes happen from OOMs (running out of memory). Do you have the memory specs of the instance you are running this on? Thanks! —. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/794#issuecomment-2021094547>, or unsubscribe<https://github.com/notifications/unsubscribe-auth/AE5Y3VRRFLRCYTPDYDZDFY3Y2GX7VAVCNFSM6AAAAABFG7OINSVHI2DSMVQWIX3LMV43OSLTON2WKQ3PNVWWK3TUHMZDAMRRGA4TINJUG4>. You are receiving this because you authored the thread.Message ID: ***@***.***>.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/794
https://github.com/google/deepvariant/issues/794:480,reliability,fault,faults,480,"It was run with 256G RAM node and all other samples finish in the same RAM nodes. I will send you data later. ________________________________. From: Lucas Brambrink ***@***.***>. Sent: Tuesday, March 26, 2024 6:46:34 PM. To: google/deepvariant ***@***.***>. Cc: Zhigui Bao ***@***.***>; Author ***@***.***>. Subject: Re: [google/deepvariant] Fatal Python error: Segmentation fault (Issue #794). Sure thing! You can send me the files at ***@***.******@***.***>. Additionally, Seg faults can sometimes happen from OOMs (running out of memory). Do you have the memory specs of the instance you are running this on? Thanks! —. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/794#issuecomment-2021094547>, or unsubscribe<https://github.com/notifications/unsubscribe-auth/AE5Y3VRRFLRCYTPDYDZDFY3Y2GX7VAVCNFSM6AAAAABFG7OINSVHI2DSMVQWIX3LMV43OSLTON2WKQ3PNVWWK3TUHMZDAMRRGA4TINJUG4>. You are receiving this because you authored the thread.Message ID: ***@***.***>.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/794
https://github.com/google/deepvariant/issues/794:356,safety,error,error,356,"It was run with 256G RAM node and all other samples finish in the same RAM nodes. I will send you data later. ________________________________. From: Lucas Brambrink ***@***.***>. Sent: Tuesday, March 26, 2024 6:46:34 PM. To: google/deepvariant ***@***.***>. Cc: Zhigui Bao ***@***.***>; Author ***@***.***>. Subject: Re: [google/deepvariant] Fatal Python error: Segmentation fault (Issue #794). Sure thing! You can send me the files at ***@***.******@***.***>. Additionally, Seg faults can sometimes happen from OOMs (running out of memory). Do you have the memory specs of the instance you are running this on? Thanks! —. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/794#issuecomment-2021094547>, or unsubscribe<https://github.com/notifications/unsubscribe-auth/AE5Y3VRRFLRCYTPDYDZDFY3Y2GX7VAVCNFSM6AAAAABFG7OINSVHI2DSMVQWIX3LMV43OSLTON2WKQ3PNVWWK3TUHMZDAMRRGA4TINJUG4>. You are receiving this because you authored the thread.Message ID: ***@***.***>.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/794
https://github.com/google/deepvariant/issues/794:376,safety,fault,fault,376,"It was run with 256G RAM node and all other samples finish in the same RAM nodes. I will send you data later. ________________________________. From: Lucas Brambrink ***@***.***>. Sent: Tuesday, March 26, 2024 6:46:34 PM. To: google/deepvariant ***@***.***>. Cc: Zhigui Bao ***@***.***>; Author ***@***.***>. Subject: Re: [google/deepvariant] Fatal Python error: Segmentation fault (Issue #794). Sure thing! You can send me the files at ***@***.******@***.***>. Additionally, Seg faults can sometimes happen from OOMs (running out of memory). Do you have the memory specs of the instance you are running this on? Thanks! —. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/794#issuecomment-2021094547>, or unsubscribe<https://github.com/notifications/unsubscribe-auth/AE5Y3VRRFLRCYTPDYDZDFY3Y2GX7VAVCNFSM6AAAAABFG7OINSVHI2DSMVQWIX3LMV43OSLTON2WKQ3PNVWWK3TUHMZDAMRRGA4TINJUG4>. You are receiving this because you authored the thread.Message ID: ***@***.***>.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/794
https://github.com/google/deepvariant/issues/794:480,safety,fault,faults,480,"It was run with 256G RAM node and all other samples finish in the same RAM nodes. I will send you data later. ________________________________. From: Lucas Brambrink ***@***.***>. Sent: Tuesday, March 26, 2024 6:46:34 PM. To: google/deepvariant ***@***.***>. Cc: Zhigui Bao ***@***.***>; Author ***@***.***>. Subject: Re: [google/deepvariant] Fatal Python error: Segmentation fault (Issue #794). Sure thing! You can send me the files at ***@***.******@***.***>. Additionally, Seg faults can sometimes happen from OOMs (running out of memory). Do you have the memory specs of the instance you are running this on? Thanks! —. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/794#issuecomment-2021094547>, or unsubscribe<https://github.com/notifications/unsubscribe-auth/AE5Y3VRRFLRCYTPDYDZDFY3Y2GX7VAVCNFSM6AAAAABFG7OINSVHI2DSMVQWIX3LMV43OSLTON2WKQ3PNVWWK3TUHMZDAMRRGA4TINJUG4>. You are receiving this because you authored the thread.Message ID: ***@***.***>.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/794
https://github.com/google/deepvariant/issues/794:288,security,Auth,Author,288,"It was run with 256G RAM node and all other samples finish in the same RAM nodes. I will send you data later. ________________________________. From: Lucas Brambrink ***@***.***>. Sent: Tuesday, March 26, 2024 6:46:34 PM. To: google/deepvariant ***@***.***>. Cc: Zhigui Bao ***@***.***>; Author ***@***.***>. Subject: Re: [google/deepvariant] Fatal Python error: Segmentation fault (Issue #794). Sure thing! You can send me the files at ***@***.******@***.***>. Additionally, Seg faults can sometimes happen from OOMs (running out of memory). Do you have the memory specs of the instance you are running this on? Thanks! —. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/794#issuecomment-2021094547>, or unsubscribe<https://github.com/notifications/unsubscribe-auth/AE5Y3VRRFLRCYTPDYDZDFY3Y2GX7VAVCNFSM6AAAAABFG7OINSVHI2DSMVQWIX3LMV43OSLTON2WKQ3PNVWWK3TUHMZDAMRRGA4TINJUG4>. You are receiving this because you authored the thread.Message ID: ***@***.***>.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/794
https://github.com/google/deepvariant/issues/794:807,security,auth,auth,807,"It was run with 256G RAM node and all other samples finish in the same RAM nodes. I will send you data later. ________________________________. From: Lucas Brambrink ***@***.***>. Sent: Tuesday, March 26, 2024 6:46:34 PM. To: google/deepvariant ***@***.***>. Cc: Zhigui Bao ***@***.***>; Author ***@***.***>. Subject: Re: [google/deepvariant] Fatal Python error: Segmentation fault (Issue #794). Sure thing! You can send me the files at ***@***.******@***.***>. Additionally, Seg faults can sometimes happen from OOMs (running out of memory). Do you have the memory specs of the instance you are running this on? Thanks! —. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/794#issuecomment-2021094547>, or unsubscribe<https://github.com/notifications/unsubscribe-auth/AE5Y3VRRFLRCYTPDYDZDFY3Y2GX7VAVCNFSM6AAAAABFG7OINSVHI2DSMVQWIX3LMV43OSLTON2WKQ3PNVWWK3TUHMZDAMRRGA4TINJUG4>. You are receiving this because you authored the thread.Message ID: ***@***.***>.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/794
https://github.com/google/deepvariant/issues/794:956,security,auth,authored,956,"It was run with 256G RAM node and all other samples finish in the same RAM nodes. I will send you data later. ________________________________. From: Lucas Brambrink ***@***.***>. Sent: Tuesday, March 26, 2024 6:46:34 PM. To: google/deepvariant ***@***.***>. Cc: Zhigui Bao ***@***.***>; Author ***@***.***>. Subject: Re: [google/deepvariant] Fatal Python error: Segmentation fault (Issue #794). Sure thing! You can send me the files at ***@***.******@***.***>. Additionally, Seg faults can sometimes happen from OOMs (running out of memory). Do you have the memory specs of the instance you are running this on? Thanks! —. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/794#issuecomment-2021094547>, or unsubscribe<https://github.com/notifications/unsubscribe-auth/AE5Y3VRRFLRCYTPDYDZDFY3Y2GX7VAVCNFSM6AAAAABFG7OINSVHI2DSMVQWIX3LMV43OSLTON2WKQ3PNVWWK3TUHMZDAMRRGA4TINJUG4>. You are receiving this because you authored the thread.Message ID: ***@***.***>.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/794
https://github.com/google/deepvariant/issues/794:356,usability,error,error,356,"It was run with 256G RAM node and all other samples finish in the same RAM nodes. I will send you data later. ________________________________. From: Lucas Brambrink ***@***.***>. Sent: Tuesday, March 26, 2024 6:46:34 PM. To: google/deepvariant ***@***.***>. Cc: Zhigui Bao ***@***.***>; Author ***@***.***>. Subject: Re: [google/deepvariant] Fatal Python error: Segmentation fault (Issue #794). Sure thing! You can send me the files at ***@***.******@***.***>. Additionally, Seg faults can sometimes happen from OOMs (running out of memory). Do you have the memory specs of the instance you are running this on? Thanks! —. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/794#issuecomment-2021094547>, or unsubscribe<https://github.com/notifications/unsubscribe-auth/AE5Y3VRRFLRCYTPDYDZDFY3Y2GX7VAVCNFSM6AAAAABFG7OINSVHI2DSMVQWIX3LMV43OSLTON2WKQ3PNVWWK3TUHMZDAMRRGA4TINJUG4>. You are receiving this because you authored the thread.Message ID: ***@***.***>.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/794
https://github.com/google/deepvariant/issues/794:534,usability,memor,memory,534,"It was run with 256G RAM node and all other samples finish in the same RAM nodes. I will send you data later. ________________________________. From: Lucas Brambrink ***@***.***>. Sent: Tuesday, March 26, 2024 6:46:34 PM. To: google/deepvariant ***@***.***>. Cc: Zhigui Bao ***@***.***>; Author ***@***.***>. Subject: Re: [google/deepvariant] Fatal Python error: Segmentation fault (Issue #794). Sure thing! You can send me the files at ***@***.******@***.***>. Additionally, Seg faults can sometimes happen from OOMs (running out of memory). Do you have the memory specs of the instance you are running this on? Thanks! —. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/794#issuecomment-2021094547>, or unsubscribe<https://github.com/notifications/unsubscribe-auth/AE5Y3VRRFLRCYTPDYDZDFY3Y2GX7VAVCNFSM6AAAAABFG7OINSVHI2DSMVQWIX3LMV43OSLTON2WKQ3PNVWWK3TUHMZDAMRRGA4TINJUG4>. You are receiving this because you authored the thread.Message ID: ***@***.***>.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/794
https://github.com/google/deepvariant/issues/794:559,usability,memor,memory,559,"It was run with 256G RAM node and all other samples finish in the same RAM nodes. I will send you data later. ________________________________. From: Lucas Brambrink ***@***.***>. Sent: Tuesday, March 26, 2024 6:46:34 PM. To: google/deepvariant ***@***.***>. Cc: Zhigui Bao ***@***.***>; Author ***@***.***>. Subject: Re: [google/deepvariant] Fatal Python error: Segmentation fault (Issue #794). Sure thing! You can send me the files at ***@***.******@***.***>. Additionally, Seg faults can sometimes happen from OOMs (running out of memory). Do you have the memory specs of the instance you are running this on? Thanks! —. Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/794#issuecomment-2021094547>, or unsubscribe<https://github.com/notifications/unsubscribe-auth/AE5Y3VRRFLRCYTPDYDZDFY3Y2GX7VAVCNFSM6AAAAABFG7OINSVHI2DSMVQWIX3LMV43OSLTON2WKQ3PNVWWK3TUHMZDAMRRGA4TINJUG4>. You are receiving this because you authored the thread.Message ID: ***@***.***>.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/794
https://github.com/google/deepvariant/issues/794:243,availability,echo,echo,243,"@baozg and @yangxin-9 , . Additionally to sending the bam files, can you please also see if the files are not truncated? You can run the following command to check if the files are OK:. ```bash. samtools quickcheck -v *.bam > bad_bams.fofn && echo 'all ok' || echo 'some files failed check, see bad_bams.fofn'. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/794
https://github.com/google/deepvariant/issues/794:260,availability,echo,echo,260,"@baozg and @yangxin-9 , . Additionally to sending the bam files, can you please also see if the files are not truncated? You can run the following command to check if the files are OK:. ```bash. samtools quickcheck -v *.bam > bad_bams.fofn && echo 'all ok' || echo 'some files failed check, see bad_bams.fofn'. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/794
https://github.com/google/deepvariant/issues/794:277,deployability,fail,failed,277,"@baozg and @yangxin-9 , . Additionally to sending the bam files, can you please also see if the files are not truncated? You can run the following command to check if the files are OK:. ```bash. samtools quickcheck -v *.bam > bad_bams.fofn && echo 'all ok' || echo 'some files failed check, see bad_bams.fofn'. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/794
https://github.com/google/deepvariant/issues/794:277,reliability,fail,failed,277,"@baozg and @yangxin-9 , . Additionally to sending the bam files, can you please also see if the files are not truncated? You can run the following command to check if the files are OK:. ```bash. samtools quickcheck -v *.bam > bad_bams.fofn && echo 'all ok' || echo 'some files failed check, see bad_bams.fofn'. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/794
https://github.com/google/deepvariant/issues/794:147,usability,command,command,147,"@baozg and @yangxin-9 , . Additionally to sending the bam files, can you please also see if the files are not truncated? You can run the following command to check if the files are OK:. ```bash. samtools quickcheck -v *.bam > bad_bams.fofn && echo 'all ok' || echo 'some files failed check, see bad_bams.fofn'. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/794
https://github.com/google/deepvariant/issues/794:93,availability,error,error,93,I have checked my bam file according to the command you gave and it shows that 'all ok'. The error may not be caused by the bam file.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/794
https://github.com/google/deepvariant/issues/794:93,performance,error,error,93,I have checked my bam file according to the command you gave and it shows that 'all ok'. The error may not be caused by the bam file.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/794
https://github.com/google/deepvariant/issues/794:93,safety,error,error,93,I have checked my bam file according to the command you gave and it shows that 'all ok'. The error may not be caused by the bam file.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/794
https://github.com/google/deepvariant/issues/794:44,usability,command,command,44,I have checked my bam file according to the command you gave and it shows that 'all ok'. The error may not be caused by the bam file.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/794
https://github.com/google/deepvariant/issues/794:93,usability,error,error,93,I have checked my bam file according to the command you gave and it shows that 'all ok'. The error may not be caused by the bam file.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/794
https://github.com/google/deepvariant/issues/794:89,availability,error,error,89,"@baozg. After carefully bisecting your BAM file, it looks like the region that throws an error is chr12:7721068-7735636. Looking at the pileup, there are 5 large (~11k) deletions in that region of 3 different lengths:. ![image](https://github.com/google/deepvariant/assets/8753889/18e84dd4-27df-4059-aced-f6f9573e1f9a). One is length `11,843`, two are `11,844` and two are `11,845`. It looks like the trouble comes from attempting to represent and realign those INDEL candidates with 2 reads each. DeepVariant can't actually call deletions that long. If you set the [vsc_min_count_indel](https://github.com/google/deepvariant/blob/r1.6.1/deepvariant/make_examples_options.py#L292-L299) to 3, the problem goes away. So adding `--make_examples_extra_args=vsc_min_count_indels=3` should fix the issue. If desired, you can run DeepVariant on just that region with `--regions=chr12:7721068-7735636`. We will work on fixing this on our end as well in our next release. @yangxin-9 To avoid mixing issues may or may not be related, please create a new issue that shows the command you ran and the output. Also, if possible, please send us the input files used so we can try to reproduce the issue ourselves.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/794
https://github.com/google/deepvariant/issues/794:954,deployability,releas,release,954,"@baozg. After carefully bisecting your BAM file, it looks like the region that throws an error is chr12:7721068-7735636. Looking at the pileup, there are 5 large (~11k) deletions in that region of 3 different lengths:. ![image](https://github.com/google/deepvariant/assets/8753889/18e84dd4-27df-4059-aced-f6f9573e1f9a). One is length `11,843`, two are `11,844` and two are `11,845`. It looks like the trouble comes from attempting to represent and realign those INDEL candidates with 2 reads each. DeepVariant can't actually call deletions that long. If you set the [vsc_min_count_indel](https://github.com/google/deepvariant/blob/r1.6.1/deepvariant/make_examples_options.py#L292-L299) to 3, the problem goes away. So adding `--make_examples_extra_args=vsc_min_count_indels=3` should fix the issue. If desired, you can run DeepVariant on just that region with `--regions=chr12:7721068-7735636`. We will work on fixing this on our end as well in our next release. @yangxin-9 To avoid mixing issues may or may not be related, please create a new issue that shows the command you ran and the output. Also, if possible, please send us the input files used so we can try to reproduce the issue ourselves.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/794
https://github.com/google/deepvariant/issues/794:89,performance,error,error,89,"@baozg. After carefully bisecting your BAM file, it looks like the region that throws an error is chr12:7721068-7735636. Looking at the pileup, there are 5 large (~11k) deletions in that region of 3 different lengths:. ![image](https://github.com/google/deepvariant/assets/8753889/18e84dd4-27df-4059-aced-f6f9573e1f9a). One is length `11,843`, two are `11,844` and two are `11,845`. It looks like the trouble comes from attempting to represent and realign those INDEL candidates with 2 reads each. DeepVariant can't actually call deletions that long. If you set the [vsc_min_count_indel](https://github.com/google/deepvariant/blob/r1.6.1/deepvariant/make_examples_options.py#L292-L299) to 3, the problem goes away. So adding `--make_examples_extra_args=vsc_min_count_indels=3` should fix the issue. If desired, you can run DeepVariant on just that region with `--regions=chr12:7721068-7735636`. We will work on fixing this on our end as well in our next release. @yangxin-9 To avoid mixing issues may or may not be related, please create a new issue that shows the command you ran and the output. Also, if possible, please send us the input files used so we can try to reproduce the issue ourselves.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/794
https://github.com/google/deepvariant/issues/794:89,safety,error,error,89,"@baozg. After carefully bisecting your BAM file, it looks like the region that throws an error is chr12:7721068-7735636. Looking at the pileup, there are 5 large (~11k) deletions in that region of 3 different lengths:. ![image](https://github.com/google/deepvariant/assets/8753889/18e84dd4-27df-4059-aced-f6f9573e1f9a). One is length `11,843`, two are `11,844` and two are `11,845`. It looks like the trouble comes from attempting to represent and realign those INDEL candidates with 2 reads each. DeepVariant can't actually call deletions that long. If you set the [vsc_min_count_indel](https://github.com/google/deepvariant/blob/r1.6.1/deepvariant/make_examples_options.py#L292-L299) to 3, the problem goes away. So adding `--make_examples_extra_args=vsc_min_count_indels=3` should fix the issue. If desired, you can run DeepVariant on just that region with `--regions=chr12:7721068-7735636`. We will work on fixing this on our end as well in our next release. @yangxin-9 To avoid mixing issues may or may not be related, please create a new issue that shows the command you ran and the output. Also, if possible, please send us the input files used so we can try to reproduce the issue ourselves.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/794
https://github.com/google/deepvariant/issues/794:977,safety,avoid,avoid,977,"@baozg. After carefully bisecting your BAM file, it looks like the region that throws an error is chr12:7721068-7735636. Looking at the pileup, there are 5 large (~11k) deletions in that region of 3 different lengths:. ![image](https://github.com/google/deepvariant/assets/8753889/18e84dd4-27df-4059-aced-f6f9573e1f9a). One is length `11,843`, two are `11,844` and two are `11,845`. It looks like the trouble comes from attempting to represent and realign those INDEL candidates with 2 reads each. DeepVariant can't actually call deletions that long. If you set the [vsc_min_count_indel](https://github.com/google/deepvariant/blob/r1.6.1/deepvariant/make_examples_options.py#L292-L299) to 3, the problem goes away. So adding `--make_examples_extra_args=vsc_min_count_indels=3` should fix the issue. If desired, you can run DeepVariant on just that region with `--regions=chr12:7721068-7735636`. We will work on fixing this on our end as well in our next release. @yangxin-9 To avoid mixing issues may or may not be related, please create a new issue that shows the command you ran and the output. Also, if possible, please send us the input files used so we can try to reproduce the issue ourselves.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/794
https://github.com/google/deepvariant/issues/794:1135,safety,input,input,1135,"@baozg. After carefully bisecting your BAM file, it looks like the region that throws an error is chr12:7721068-7735636. Looking at the pileup, there are 5 large (~11k) deletions in that region of 3 different lengths:. ![image](https://github.com/google/deepvariant/assets/8753889/18e84dd4-27df-4059-aced-f6f9573e1f9a). One is length `11,843`, two are `11,844` and two are `11,845`. It looks like the trouble comes from attempting to represent and realign those INDEL candidates with 2 reads each. DeepVariant can't actually call deletions that long. If you set the [vsc_min_count_indel](https://github.com/google/deepvariant/blob/r1.6.1/deepvariant/make_examples_options.py#L292-L299) to 3, the problem goes away. So adding `--make_examples_extra_args=vsc_min_count_indels=3` should fix the issue. If desired, you can run DeepVariant on just that region with `--regions=chr12:7721068-7735636`. We will work on fixing this on our end as well in our next release. @yangxin-9 To avoid mixing issues may or may not be related, please create a new issue that shows the command you ran and the output. Also, if possible, please send us the input files used so we can try to reproduce the issue ourselves.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/794
https://github.com/google/deepvariant/issues/794:89,usability,error,error,89,"@baozg. After carefully bisecting your BAM file, it looks like the region that throws an error is chr12:7721068-7735636. Looking at the pileup, there are 5 large (~11k) deletions in that region of 3 different lengths:. ![image](https://github.com/google/deepvariant/assets/8753889/18e84dd4-27df-4059-aced-f6f9573e1f9a). One is length `11,843`, two are `11,844` and two are `11,845`. It looks like the trouble comes from attempting to represent and realign those INDEL candidates with 2 reads each. DeepVariant can't actually call deletions that long. If you set the [vsc_min_count_indel](https://github.com/google/deepvariant/blob/r1.6.1/deepvariant/make_examples_options.py#L292-L299) to 3, the problem goes away. So adding `--make_examples_extra_args=vsc_min_count_indels=3` should fix the issue. If desired, you can run DeepVariant on just that region with `--regions=chr12:7721068-7735636`. We will work on fixing this on our end as well in our next release. @yangxin-9 To avoid mixing issues may or may not be related, please create a new issue that shows the command you ran and the output. Also, if possible, please send us the input files used so we can try to reproduce the issue ourselves.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/794
https://github.com/google/deepvariant/issues/794:1065,usability,command,command,1065,"@baozg. After carefully bisecting your BAM file, it looks like the region that throws an error is chr12:7721068-7735636. Looking at the pileup, there are 5 large (~11k) deletions in that region of 3 different lengths:. ![image](https://github.com/google/deepvariant/assets/8753889/18e84dd4-27df-4059-aced-f6f9573e1f9a). One is length `11,843`, two are `11,844` and two are `11,845`. It looks like the trouble comes from attempting to represent and realign those INDEL candidates with 2 reads each. DeepVariant can't actually call deletions that long. If you set the [vsc_min_count_indel](https://github.com/google/deepvariant/blob/r1.6.1/deepvariant/make_examples_options.py#L292-L299) to 3, the problem goes away. So adding `--make_examples_extra_args=vsc_min_count_indels=3` should fix the issue. If desired, you can run DeepVariant on just that region with `--regions=chr12:7721068-7735636`. We will work on fixing this on our end as well in our next release. @yangxin-9 To avoid mixing issues may or may not be related, please create a new issue that shows the command you ran and the output. Also, if possible, please send us the input files used so we can try to reproduce the issue ourselves.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/794
https://github.com/google/deepvariant/issues/794:1135,usability,input,input,1135,"@baozg. After carefully bisecting your BAM file, it looks like the region that throws an error is chr12:7721068-7735636. Looking at the pileup, there are 5 large (~11k) deletions in that region of 3 different lengths:. ![image](https://github.com/google/deepvariant/assets/8753889/18e84dd4-27df-4059-aced-f6f9573e1f9a). One is length `11,843`, two are `11,844` and two are `11,845`. It looks like the trouble comes from attempting to represent and realign those INDEL candidates with 2 reads each. DeepVariant can't actually call deletions that long. If you set the [vsc_min_count_indel](https://github.com/google/deepvariant/blob/r1.6.1/deepvariant/make_examples_options.py#L292-L299) to 3, the problem goes away. So adding `--make_examples_extra_args=vsc_min_count_indels=3` should fix the issue. If desired, you can run DeepVariant on just that region with `--regions=chr12:7721068-7735636`. We will work on fixing this on our end as well in our next release. @yangxin-9 To avoid mixing issues may or may not be related, please create a new issue that shows the command you ran and the output. Also, if possible, please send us the input files used so we can try to reproduce the issue ourselves.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/794
https://github.com/google/deepvariant/issues/794:212,deployability,fail,fails,212,Thanks for your careful examination. It's quite common to see this divergent region in outcrossing plants. It mixed with mapping noise and true variants. Is it possible to report this region / reads when realign fails? Or do I need pre exclude this region before DeepVariant calling?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/794
https://github.com/google/deepvariant/issues/794:212,reliability,fail,fails,212,Thanks for your careful examination. It's quite common to see this divergent region in outcrossing plants. It mixed with mapping noise and true variants. Is it possible to report this region / reads when realign fails? Or do I need pre exclude this region before DeepVariant calling?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/794
https://github.com/google/deepvariant/issues/794:99,testability,plan,plants,99,Thanks for your careful examination. It's quite common to see this divergent region in outcrossing plants. It mixed with mapping noise and true variants. Is it possible to report this region / reads when realign fails? Or do I need pre exclude this region before DeepVariant calling?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/794
https://github.com/google/deepvariant/issues/794:22,reliability,doe,does,22,"Right now DeepVariant does not have the ability to report such a region by itself and skip it. You will need to exclude the problematic regions before running DeepVariant, or use `vsc_min_count_indels` to avoid candidate generation in these cases.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/794
https://github.com/google/deepvariant/issues/794:205,safety,avoid,avoid,205,"Right now DeepVariant does not have the ability to report such a region by itself and skip it. You will need to exclude the problematic regions before running DeepVariant, or use `vsc_min_count_indels` to avoid candidate generation in these cases.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/794
https://github.com/google/deepvariant/issues/795:57,usability,document,documentation,57,"Thanks for providing a solution! Adding something to the documentation seems like a good idea, I'll keep you posted if and when we make changes.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/795
https://github.com/google/deepvariant/issues/795:27,deployability,updat,updated,27,The documentation has been updated and will be included in our next release. Thank you very much for the suggestion!,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/795
https://github.com/google/deepvariant/issues/795:68,deployability,releas,release,68,The documentation has been updated and will be included in our next release. Thank you very much for the suggestion!,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/795
https://github.com/google/deepvariant/issues/795:27,safety,updat,updated,27,The documentation has been updated and will be included in our next release. Thank you very much for the suggestion!,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/795
https://github.com/google/deepvariant/issues/795:27,security,updat,updated,27,The documentation has been updated and will be included in our next release. Thank you very much for the suggestion!,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/795
https://github.com/google/deepvariant/issues/795:4,usability,document,documentation,4,The documentation has been updated and will be included in our next release. Thank you very much for the suggestion!,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/795
https://github.com/google/deepvariant/issues/796:18,modifiability,Pac,PacBio,18,Hi @rabbitdsy For PacBio I recommend the DeepVariant present. I hope this answers your question.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/796
https://github.com/google/deepvariant/issues/797:304,availability,checkpoint,checkpoints,304,"The issue stems from a mismatch between the set of channels the model was trained on and the channels in the examples generated during `run_deepvariant`. The important bit in the logs you posted is:. ```. From /90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/training_dir_test2/checkpoints/ckpt-58/example_info.json: Shape of input examples: [100, 221, 6], Channels of input examples: [1, 2, 3, 4, 5, 6]. I0327 22:12:15.248034 139725850806080 dv_utils.py:365] From /local/scratch/haley.arnold/14698718/tmpg5h0cte0/make_examples.tfrecord-00000-of-00001.gz.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 19]. W0327 22:12:15.248203 139725850806080 call_variants.py:541] Input shape [100, 221, 7] and model shape [100, 221, 6] does not match. W0327 22:12:15.248327 139725850806080 call_variants.py:549] Input channels [1, 2, 3, 4, 5, 6, 19] and model channels [1, 2, 3, 4, 5, 6] do not match. ```. Your customized model was trained on `[1, 2, 3, 4, 5, 6]` (the `BASE_CHANNELS`) but the examples in `make_examples.tfrecord-00000-of-00001.gz` have an extra channel, 19 (`insert_size`), which gets [added to the WGS model preset](https://github.com/google/deepvariant/blob/r1.6.1/scripts/run_deepvariant.py#L369). . You can either:. a) include `--channels ""insert_size""` when [generating the training data](https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-training-case-study.md#training-set). b) don't set `--model_type WGS` when you call `run_deepvariant` (which you may not need to do regardless if you provide a `customized_model`). . The choice comes down to if you want to include the channel or not. Experiments have shown it provides a slight accuracy boost for WGS, but its not strictly necessary.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:1652,availability,down,down,1652,"The issue stems from a mismatch between the set of channels the model was trained on and the channels in the examples generated during `run_deepvariant`. The important bit in the logs you posted is:. ```. From /90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/training_dir_test2/checkpoints/ckpt-58/example_info.json: Shape of input examples: [100, 221, 6], Channels of input examples: [1, 2, 3, 4, 5, 6]. I0327 22:12:15.248034 139725850806080 dv_utils.py:365] From /local/scratch/haley.arnold/14698718/tmpg5h0cte0/make_examples.tfrecord-00000-of-00001.gz.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 19]. W0327 22:12:15.248203 139725850806080 call_variants.py:541] Input shape [100, 221, 7] and model shape [100, 221, 6] does not match. W0327 22:12:15.248327 139725850806080 call_variants.py:549] Input channels [1, 2, 3, 4, 5, 6, 19] and model channels [1, 2, 3, 4, 5, 6] do not match. ```. Your customized model was trained on `[1, 2, 3, 4, 5, 6]` (the `BASE_CHANNELS`) but the examples in `make_examples.tfrecord-00000-of-00001.gz` have an extra channel, 19 (`insert_size`), which gets [added to the WGS model preset](https://github.com/google/deepvariant/blob/r1.6.1/scripts/run_deepvariant.py#L369). . You can either:. a) include `--channels ""insert_size""` when [generating the training data](https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-training-case-study.md#training-set). b) don't set `--model_type WGS` when you call `run_deepvariant` (which you may not need to do regardless if you provide a `customized_model`). . The choice comes down to if you want to include the channel or not. Experiments have shown it provides a slight accuracy boost for WGS, but its not strictly necessary.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:1740,availability,sli,slight,1740,"The issue stems from a mismatch between the set of channels the model was trained on and the channels in the examples generated during `run_deepvariant`. The important bit in the logs you posted is:. ```. From /90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/training_dir_test2/checkpoints/ckpt-58/example_info.json: Shape of input examples: [100, 221, 6], Channels of input examples: [1, 2, 3, 4, 5, 6]. I0327 22:12:15.248034 139725850806080 dv_utils.py:365] From /local/scratch/haley.arnold/14698718/tmpg5h0cte0/make_examples.tfrecord-00000-of-00001.gz.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 19]. W0327 22:12:15.248203 139725850806080 call_variants.py:541] Input shape [100, 221, 7] and model shape [100, 221, 6] does not match. W0327 22:12:15.248327 139725850806080 call_variants.py:549] Input channels [1, 2, 3, 4, 5, 6, 19] and model channels [1, 2, 3, 4, 5, 6] do not match. ```. Your customized model was trained on `[1, 2, 3, 4, 5, 6]` (the `BASE_CHANNELS`) but the examples in `make_examples.tfrecord-00000-of-00001.gz` have an extra channel, 19 (`insert_size`), which gets [added to the WGS model preset](https://github.com/google/deepvariant/blob/r1.6.1/scripts/run_deepvariant.py#L369). . You can either:. a) include `--channels ""insert_size""` when [generating the training data](https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-training-case-study.md#training-set). b) don't set `--model_type WGS` when you call `run_deepvariant` (which you may not need to do regardless if you provide a `customized_model`). . The choice comes down to if you want to include the channel or not. Experiments have shown it provides a slight accuracy boost for WGS, but its not strictly necessary.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:179,deployability,log,logs,179,"The issue stems from a mismatch between the set of channels the model was trained on and the channels in the examples generated during `run_deepvariant`. The important bit in the logs you posted is:. ```. From /90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/training_dir_test2/checkpoints/ckpt-58/example_info.json: Shape of input examples: [100, 221, 6], Channels of input examples: [1, 2, 3, 4, 5, 6]. I0327 22:12:15.248034 139725850806080 dv_utils.py:365] From /local/scratch/haley.arnold/14698718/tmpg5h0cte0/make_examples.tfrecord-00000-of-00001.gz.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 19]. W0327 22:12:15.248203 139725850806080 call_variants.py:541] Input shape [100, 221, 7] and model shape [100, 221, 6] does not match. W0327 22:12:15.248327 139725850806080 call_variants.py:549] Input channels [1, 2, 3, 4, 5, 6, 19] and model channels [1, 2, 3, 4, 5, 6] do not match. ```. Your customized model was trained on `[1, 2, 3, 4, 5, 6]` (the `BASE_CHANNELS`) but the examples in `make_examples.tfrecord-00000-of-00001.gz` have an extra channel, 19 (`insert_size`), which gets [added to the WGS model preset](https://github.com/google/deepvariant/blob/r1.6.1/scripts/run_deepvariant.py#L369). . You can either:. a) include `--channels ""insert_size""` when [generating the training data](https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-training-case-study.md#training-set). b) don't set `--model_type WGS` when you call `run_deepvariant` (which you may not need to do regardless if you provide a `customized_model`). . The choice comes down to if you want to include the channel or not. Experiments have shown it provides a slight accuracy boost for WGS, but its not strictly necessary.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:64,energy efficiency,model,model,64,"The issue stems from a mismatch between the set of channels the model was trained on and the channels in the examples generated during `run_deepvariant`. The important bit in the logs you posted is:. ```. From /90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/training_dir_test2/checkpoints/ckpt-58/example_info.json: Shape of input examples: [100, 221, 6], Channels of input examples: [1, 2, 3, 4, 5, 6]. I0327 22:12:15.248034 139725850806080 dv_utils.py:365] From /local/scratch/haley.arnold/14698718/tmpg5h0cte0/make_examples.tfrecord-00000-of-00001.gz.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 19]. W0327 22:12:15.248203 139725850806080 call_variants.py:541] Input shape [100, 221, 7] and model shape [100, 221, 6] does not match. W0327 22:12:15.248327 139725850806080 call_variants.py:549] Input channels [1, 2, 3, 4, 5, 6, 19] and model channels [1, 2, 3, 4, 5, 6] do not match. ```. Your customized model was trained on `[1, 2, 3, 4, 5, 6]` (the `BASE_CHANNELS`) but the examples in `make_examples.tfrecord-00000-of-00001.gz` have an extra channel, 19 (`insert_size`), which gets [added to the WGS model preset](https://github.com/google/deepvariant/blob/r1.6.1/scripts/run_deepvariant.py#L369). . You can either:. a) include `--channels ""insert_size""` when [generating the training data](https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-training-case-study.md#training-set). b) don't set `--model_type WGS` when you call `run_deepvariant` (which you may not need to do regardless if you provide a `customized_model`). . The choice comes down to if you want to include the channel or not. Experiments have shown it provides a slight accuracy boost for WGS, but its not strictly necessary.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:782,energy efficiency,model,model,782,"The issue stems from a mismatch between the set of channels the model was trained on and the channels in the examples generated during `run_deepvariant`. The important bit in the logs you posted is:. ```. From /90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/training_dir_test2/checkpoints/ckpt-58/example_info.json: Shape of input examples: [100, 221, 6], Channels of input examples: [1, 2, 3, 4, 5, 6]. I0327 22:12:15.248034 139725850806080 dv_utils.py:365] From /local/scratch/haley.arnold/14698718/tmpg5h0cte0/make_examples.tfrecord-00000-of-00001.gz.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 19]. W0327 22:12:15.248203 139725850806080 call_variants.py:541] Input shape [100, 221, 7] and model shape [100, 221, 6] does not match. W0327 22:12:15.248327 139725850806080 call_variants.py:549] Input channels [1, 2, 3, 4, 5, 6, 19] and model channels [1, 2, 3, 4, 5, 6] do not match. ```. Your customized model was trained on `[1, 2, 3, 4, 5, 6]` (the `BASE_CHANNELS`) but the examples in `make_examples.tfrecord-00000-of-00001.gz` have an extra channel, 19 (`insert_size`), which gets [added to the WGS model preset](https://github.com/google/deepvariant/blob/r1.6.1/scripts/run_deepvariant.py#L369). . You can either:. a) include `--channels ""insert_size""` when [generating the training data](https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-training-case-study.md#training-set). b) don't set `--model_type WGS` when you call `run_deepvariant` (which you may not need to do regardless if you provide a `customized_model`). . The choice comes down to if you want to include the channel or not. Experiments have shown it provides a slight accuracy boost for WGS, but its not strictly necessary.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:926,energy efficiency,model,model,926,"The issue stems from a mismatch between the set of channels the model was trained on and the channels in the examples generated during `run_deepvariant`. The important bit in the logs you posted is:. ```. From /90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/training_dir_test2/checkpoints/ckpt-58/example_info.json: Shape of input examples: [100, 221, 6], Channels of input examples: [1, 2, 3, 4, 5, 6]. I0327 22:12:15.248034 139725850806080 dv_utils.py:365] From /local/scratch/haley.arnold/14698718/tmpg5h0cte0/make_examples.tfrecord-00000-of-00001.gz.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 19]. W0327 22:12:15.248203 139725850806080 call_variants.py:541] Input shape [100, 221, 7] and model shape [100, 221, 6] does not match. W0327 22:12:15.248327 139725850806080 call_variants.py:549] Input channels [1, 2, 3, 4, 5, 6, 19] and model channels [1, 2, 3, 4, 5, 6] do not match. ```. Your customized model was trained on `[1, 2, 3, 4, 5, 6]` (the `BASE_CHANNELS`) but the examples in `make_examples.tfrecord-00000-of-00001.gz` have an extra channel, 19 (`insert_size`), which gets [added to the WGS model preset](https://github.com/google/deepvariant/blob/r1.6.1/scripts/run_deepvariant.py#L369). . You can either:. a) include `--channels ""insert_size""` when [generating the training data](https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-training-case-study.md#training-set). b) don't set `--model_type WGS` when you call `run_deepvariant` (which you may not need to do regardless if you provide a `customized_model`). . The choice comes down to if you want to include the channel or not. Experiments have shown it provides a slight accuracy boost for WGS, but its not strictly necessary.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:995,energy efficiency,model,model,995,"The issue stems from a mismatch between the set of channels the model was trained on and the channels in the examples generated during `run_deepvariant`. The important bit in the logs you posted is:. ```. From /90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/training_dir_test2/checkpoints/ckpt-58/example_info.json: Shape of input examples: [100, 221, 6], Channels of input examples: [1, 2, 3, 4, 5, 6]. I0327 22:12:15.248034 139725850806080 dv_utils.py:365] From /local/scratch/haley.arnold/14698718/tmpg5h0cte0/make_examples.tfrecord-00000-of-00001.gz.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 19]. W0327 22:12:15.248203 139725850806080 call_variants.py:541] Input shape [100, 221, 7] and model shape [100, 221, 6] does not match. W0327 22:12:15.248327 139725850806080 call_variants.py:549] Input channels [1, 2, 3, 4, 5, 6, 19] and model channels [1, 2, 3, 4, 5, 6] do not match. ```. Your customized model was trained on `[1, 2, 3, 4, 5, 6]` (the `BASE_CHANNELS`) but the examples in `make_examples.tfrecord-00000-of-00001.gz` have an extra channel, 19 (`insert_size`), which gets [added to the WGS model preset](https://github.com/google/deepvariant/blob/r1.6.1/scripts/run_deepvariant.py#L369). . You can either:. a) include `--channels ""insert_size""` when [generating the training data](https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-training-case-study.md#training-set). b) don't set `--model_type WGS` when you call `run_deepvariant` (which you may not need to do regardless if you provide a `customized_model`). . The choice comes down to if you want to include the channel or not. Experiments have shown it provides a slight accuracy boost for WGS, but its not strictly necessary.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:1194,energy efficiency,model,model,1194,"The issue stems from a mismatch between the set of channels the model was trained on and the channels in the examples generated during `run_deepvariant`. The important bit in the logs you posted is:. ```. From /90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/training_dir_test2/checkpoints/ckpt-58/example_info.json: Shape of input examples: [100, 221, 6], Channels of input examples: [1, 2, 3, 4, 5, 6]. I0327 22:12:15.248034 139725850806080 dv_utils.py:365] From /local/scratch/haley.arnold/14698718/tmpg5h0cte0/make_examples.tfrecord-00000-of-00001.gz.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 19]. W0327 22:12:15.248203 139725850806080 call_variants.py:541] Input shape [100, 221, 7] and model shape [100, 221, 6] does not match. W0327 22:12:15.248327 139725850806080 call_variants.py:549] Input channels [1, 2, 3, 4, 5, 6, 19] and model channels [1, 2, 3, 4, 5, 6] do not match. ```. Your customized model was trained on `[1, 2, 3, 4, 5, 6]` (the `BASE_CHANNELS`) but the examples in `make_examples.tfrecord-00000-of-00001.gz` have an extra channel, 19 (`insert_size`), which gets [added to the WGS model preset](https://github.com/google/deepvariant/blob/r1.6.1/scripts/run_deepvariant.py#L369). . You can either:. a) include `--channels ""insert_size""` when [generating the training data](https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-training-case-study.md#training-set). b) don't set `--model_type WGS` when you call `run_deepvariant` (which you may not need to do regardless if you provide a `customized_model`). . The choice comes down to if you want to include the channel or not. Experiments have shown it provides a slight accuracy boost for WGS, but its not strictly necessary.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:23,interoperability,mismatch,mismatch,23,"The issue stems from a mismatch between the set of channels the model was trained on and the channels in the examples generated during `run_deepvariant`. The important bit in the logs you posted is:. ```. From /90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/training_dir_test2/checkpoints/ckpt-58/example_info.json: Shape of input examples: [100, 221, 6], Channels of input examples: [1, 2, 3, 4, 5, 6]. I0327 22:12:15.248034 139725850806080 dv_utils.py:365] From /local/scratch/haley.arnold/14698718/tmpg5h0cte0/make_examples.tfrecord-00000-of-00001.gz.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 19]. W0327 22:12:15.248203 139725850806080 call_variants.py:541] Input shape [100, 221, 7] and model shape [100, 221, 6] does not match. W0327 22:12:15.248327 139725850806080 call_variants.py:549] Input channels [1, 2, 3, 4, 5, 6, 19] and model channels [1, 2, 3, 4, 5, 6] do not match. ```. Your customized model was trained on `[1, 2, 3, 4, 5, 6]` (the `BASE_CHANNELS`) but the examples in `make_examples.tfrecord-00000-of-00001.gz` have an extra channel, 19 (`insert_size`), which gets [added to the WGS model preset](https://github.com/google/deepvariant/blob/r1.6.1/scripts/run_deepvariant.py#L369). . You can either:. a) include `--channels ""insert_size""` when [generating the training data](https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-training-case-study.md#training-set). b) don't set `--model_type WGS` when you call `run_deepvariant` (which you may not need to do regardless if you provide a `customized_model`). . The choice comes down to if you want to include the channel or not. Experiments have shown it provides a slight accuracy boost for WGS, but its not strictly necessary.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:304,reliability,checkpoint,checkpoints,304,"The issue stems from a mismatch between the set of channels the model was trained on and the channels in the examples generated during `run_deepvariant`. The important bit in the logs you posted is:. ```. From /90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/training_dir_test2/checkpoints/ckpt-58/example_info.json: Shape of input examples: [100, 221, 6], Channels of input examples: [1, 2, 3, 4, 5, 6]. I0327 22:12:15.248034 139725850806080 dv_utils.py:365] From /local/scratch/haley.arnold/14698718/tmpg5h0cte0/make_examples.tfrecord-00000-of-00001.gz.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 19]. W0327 22:12:15.248203 139725850806080 call_variants.py:541] Input shape [100, 221, 7] and model shape [100, 221, 6] does not match. W0327 22:12:15.248327 139725850806080 call_variants.py:549] Input channels [1, 2, 3, 4, 5, 6, 19] and model channels [1, 2, 3, 4, 5, 6] do not match. ```. Your customized model was trained on `[1, 2, 3, 4, 5, 6]` (the `BASE_CHANNELS`) but the examples in `make_examples.tfrecord-00000-of-00001.gz` have an extra channel, 19 (`insert_size`), which gets [added to the WGS model preset](https://github.com/google/deepvariant/blob/r1.6.1/scripts/run_deepvariant.py#L369). . You can either:. a) include `--channels ""insert_size""` when [generating the training data](https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-training-case-study.md#training-set). b) don't set `--model_type WGS` when you call `run_deepvariant` (which you may not need to do regardless if you provide a `customized_model`). . The choice comes down to if you want to include the channel or not. Experiments have shown it provides a slight accuracy boost for WGS, but its not strictly necessary.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:808,reliability,doe,does,808,"The issue stems from a mismatch between the set of channels the model was trained on and the channels in the examples generated during `run_deepvariant`. The important bit in the logs you posted is:. ```. From /90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/training_dir_test2/checkpoints/ckpt-58/example_info.json: Shape of input examples: [100, 221, 6], Channels of input examples: [1, 2, 3, 4, 5, 6]. I0327 22:12:15.248034 139725850806080 dv_utils.py:365] From /local/scratch/haley.arnold/14698718/tmpg5h0cte0/make_examples.tfrecord-00000-of-00001.gz.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 19]. W0327 22:12:15.248203 139725850806080 call_variants.py:541] Input shape [100, 221, 7] and model shape [100, 221, 6] does not match. W0327 22:12:15.248327 139725850806080 call_variants.py:549] Input channels [1, 2, 3, 4, 5, 6, 19] and model channels [1, 2, 3, 4, 5, 6] do not match. ```. Your customized model was trained on `[1, 2, 3, 4, 5, 6]` (the `BASE_CHANNELS`) but the examples in `make_examples.tfrecord-00000-of-00001.gz` have an extra channel, 19 (`insert_size`), which gets [added to the WGS model preset](https://github.com/google/deepvariant/blob/r1.6.1/scripts/run_deepvariant.py#L369). . You can either:. a) include `--channels ""insert_size""` when [generating the training data](https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-training-case-study.md#training-set). b) don't set `--model_type WGS` when you call `run_deepvariant` (which you may not need to do regardless if you provide a `customized_model`). . The choice comes down to if you want to include the channel or not. Experiments have shown it provides a slight accuracy boost for WGS, but its not strictly necessary.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:1740,reliability,sli,slight,1740,"The issue stems from a mismatch between the set of channels the model was trained on and the channels in the examples generated during `run_deepvariant`. The important bit in the logs you posted is:. ```. From /90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/training_dir_test2/checkpoints/ckpt-58/example_info.json: Shape of input examples: [100, 221, 6], Channels of input examples: [1, 2, 3, 4, 5, 6]. I0327 22:12:15.248034 139725850806080 dv_utils.py:365] From /local/scratch/haley.arnold/14698718/tmpg5h0cte0/make_examples.tfrecord-00000-of-00001.gz.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 19]. W0327 22:12:15.248203 139725850806080 call_variants.py:541] Input shape [100, 221, 7] and model shape [100, 221, 6] does not match. W0327 22:12:15.248327 139725850806080 call_variants.py:549] Input channels [1, 2, 3, 4, 5, 6, 19] and model channels [1, 2, 3, 4, 5, 6] do not match. ```. Your customized model was trained on `[1, 2, 3, 4, 5, 6]` (the `BASE_CHANNELS`) but the examples in `make_examples.tfrecord-00000-of-00001.gz` have an extra channel, 19 (`insert_size`), which gets [added to the WGS model preset](https://github.com/google/deepvariant/blob/r1.6.1/scripts/run_deepvariant.py#L369). . You can either:. a) include `--channels ""insert_size""` when [generating the training data](https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-training-case-study.md#training-set). b) don't set `--model_type WGS` when you call `run_deepvariant` (which you may not need to do regardless if you provide a `customized_model`). . The choice comes down to if you want to include the channel or not. Experiments have shown it provides a slight accuracy boost for WGS, but its not strictly necessary.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:179,safety,log,logs,179,"The issue stems from a mismatch between the set of channels the model was trained on and the channels in the examples generated during `run_deepvariant`. The important bit in the logs you posted is:. ```. From /90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/training_dir_test2/checkpoints/ckpt-58/example_info.json: Shape of input examples: [100, 221, 6], Channels of input examples: [1, 2, 3, 4, 5, 6]. I0327 22:12:15.248034 139725850806080 dv_utils.py:365] From /local/scratch/haley.arnold/14698718/tmpg5h0cte0/make_examples.tfrecord-00000-of-00001.gz.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 19]. W0327 22:12:15.248203 139725850806080 call_variants.py:541] Input shape [100, 221, 7] and model shape [100, 221, 6] does not match. W0327 22:12:15.248327 139725850806080 call_variants.py:549] Input channels [1, 2, 3, 4, 5, 6, 19] and model channels [1, 2, 3, 4, 5, 6] do not match. ```. Your customized model was trained on `[1, 2, 3, 4, 5, 6]` (the `BASE_CHANNELS`) but the examples in `make_examples.tfrecord-00000-of-00001.gz` have an extra channel, 19 (`insert_size`), which gets [added to the WGS model preset](https://github.com/google/deepvariant/blob/r1.6.1/scripts/run_deepvariant.py#L369). . You can either:. a) include `--channels ""insert_size""` when [generating the training data](https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-training-case-study.md#training-set). b) don't set `--model_type WGS` when you call `run_deepvariant` (which you may not need to do regardless if you provide a `customized_model`). . The choice comes down to if you want to include the channel or not. Experiments have shown it provides a slight accuracy boost for WGS, but its not strictly necessary.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:352,safety,input,input,352,"The issue stems from a mismatch between the set of channels the model was trained on and the channels in the examples generated during `run_deepvariant`. The important bit in the logs you posted is:. ```. From /90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/training_dir_test2/checkpoints/ckpt-58/example_info.json: Shape of input examples: [100, 221, 6], Channels of input examples: [1, 2, 3, 4, 5, 6]. I0327 22:12:15.248034 139725850806080 dv_utils.py:365] From /local/scratch/haley.arnold/14698718/tmpg5h0cte0/make_examples.tfrecord-00000-of-00001.gz.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 19]. W0327 22:12:15.248203 139725850806080 call_variants.py:541] Input shape [100, 221, 7] and model shape [100, 221, 6] does not match. W0327 22:12:15.248327 139725850806080 call_variants.py:549] Input channels [1, 2, 3, 4, 5, 6, 19] and model channels [1, 2, 3, 4, 5, 6] do not match. ```. Your customized model was trained on `[1, 2, 3, 4, 5, 6]` (the `BASE_CHANNELS`) but the examples in `make_examples.tfrecord-00000-of-00001.gz` have an extra channel, 19 (`insert_size`), which gets [added to the WGS model preset](https://github.com/google/deepvariant/blob/r1.6.1/scripts/run_deepvariant.py#L369). . You can either:. a) include `--channels ""insert_size""` when [generating the training data](https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-training-case-study.md#training-set). b) don't set `--model_type WGS` when you call `run_deepvariant` (which you may not need to do regardless if you provide a `customized_model`). . The choice comes down to if you want to include the channel or not. Experiments have shown it provides a slight accuracy boost for WGS, but its not strictly necessary.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:395,safety,input,input,395,"The issue stems from a mismatch between the set of channels the model was trained on and the channels in the examples generated during `run_deepvariant`. The important bit in the logs you posted is:. ```. From /90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/training_dir_test2/checkpoints/ckpt-58/example_info.json: Shape of input examples: [100, 221, 6], Channels of input examples: [1, 2, 3, 4, 5, 6]. I0327 22:12:15.248034 139725850806080 dv_utils.py:365] From /local/scratch/haley.arnold/14698718/tmpg5h0cte0/make_examples.tfrecord-00000-of-00001.gz.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 19]. W0327 22:12:15.248203 139725850806080 call_variants.py:541] Input shape [100, 221, 7] and model shape [100, 221, 6] does not match. W0327 22:12:15.248327 139725850806080 call_variants.py:549] Input channels [1, 2, 3, 4, 5, 6, 19] and model channels [1, 2, 3, 4, 5, 6] do not match. ```. Your customized model was trained on `[1, 2, 3, 4, 5, 6]` (the `BASE_CHANNELS`) but the examples in `make_examples.tfrecord-00000-of-00001.gz` have an extra channel, 19 (`insert_size`), which gets [added to the WGS model preset](https://github.com/google/deepvariant/blob/r1.6.1/scripts/run_deepvariant.py#L369). . You can either:. a) include `--channels ""insert_size""` when [generating the training data](https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-training-case-study.md#training-set). b) don't set `--model_type WGS` when you call `run_deepvariant` (which you may not need to do regardless if you provide a `customized_model`). . The choice comes down to if you want to include the channel or not. Experiments have shown it provides a slight accuracy boost for WGS, but its not strictly necessary.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:609,safety,input,input,609,"The issue stems from a mismatch between the set of channels the model was trained on and the channels in the examples generated during `run_deepvariant`. The important bit in the logs you posted is:. ```. From /90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/training_dir_test2/checkpoints/ckpt-58/example_info.json: Shape of input examples: [100, 221, 6], Channels of input examples: [1, 2, 3, 4, 5, 6]. I0327 22:12:15.248034 139725850806080 dv_utils.py:365] From /local/scratch/haley.arnold/14698718/tmpg5h0cte0/make_examples.tfrecord-00000-of-00001.gz.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 19]. W0327 22:12:15.248203 139725850806080 call_variants.py:541] Input shape [100, 221, 7] and model shape [100, 221, 6] does not match. W0327 22:12:15.248327 139725850806080 call_variants.py:549] Input channels [1, 2, 3, 4, 5, 6, 19] and model channels [1, 2, 3, 4, 5, 6] do not match. ```. Your customized model was trained on `[1, 2, 3, 4, 5, 6]` (the `BASE_CHANNELS`) but the examples in `make_examples.tfrecord-00000-of-00001.gz` have an extra channel, 19 (`insert_size`), which gets [added to the WGS model preset](https://github.com/google/deepvariant/blob/r1.6.1/scripts/run_deepvariant.py#L369). . You can either:. a) include `--channels ""insert_size""` when [generating the training data](https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-training-case-study.md#training-set). b) don't set `--model_type WGS` when you call `run_deepvariant` (which you may not need to do regardless if you provide a `customized_model`). . The choice comes down to if you want to include the channel or not. Experiments have shown it provides a slight accuracy boost for WGS, but its not strictly necessary.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:652,safety,input,input,652,"The issue stems from a mismatch between the set of channels the model was trained on and the channels in the examples generated during `run_deepvariant`. The important bit in the logs you posted is:. ```. From /90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/training_dir_test2/checkpoints/ckpt-58/example_info.json: Shape of input examples: [100, 221, 6], Channels of input examples: [1, 2, 3, 4, 5, 6]. I0327 22:12:15.248034 139725850806080 dv_utils.py:365] From /local/scratch/haley.arnold/14698718/tmpg5h0cte0/make_examples.tfrecord-00000-of-00001.gz.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 19]. W0327 22:12:15.248203 139725850806080 call_variants.py:541] Input shape [100, 221, 7] and model shape [100, 221, 6] does not match. W0327 22:12:15.248327 139725850806080 call_variants.py:549] Input channels [1, 2, 3, 4, 5, 6, 19] and model channels [1, 2, 3, 4, 5, 6] do not match. ```. Your customized model was trained on `[1, 2, 3, 4, 5, 6]` (the `BASE_CHANNELS`) but the examples in `make_examples.tfrecord-00000-of-00001.gz` have an extra channel, 19 (`insert_size`), which gets [added to the WGS model preset](https://github.com/google/deepvariant/blob/r1.6.1/scripts/run_deepvariant.py#L369). . You can either:. a) include `--channels ""insert_size""` when [generating the training data](https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-training-case-study.md#training-set). b) don't set `--model_type WGS` when you call `run_deepvariant` (which you may not need to do regardless if you provide a `customized_model`). . The choice comes down to if you want to include the channel or not. Experiments have shown it provides a slight accuracy boost for WGS, but its not strictly necessary.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:752,safety,Input,Input,752,"The issue stems from a mismatch between the set of channels the model was trained on and the channels in the examples generated during `run_deepvariant`. The important bit in the logs you posted is:. ```. From /90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/training_dir_test2/checkpoints/ckpt-58/example_info.json: Shape of input examples: [100, 221, 6], Channels of input examples: [1, 2, 3, 4, 5, 6]. I0327 22:12:15.248034 139725850806080 dv_utils.py:365] From /local/scratch/haley.arnold/14698718/tmpg5h0cte0/make_examples.tfrecord-00000-of-00001.gz.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 19]. W0327 22:12:15.248203 139725850806080 call_variants.py:541] Input shape [100, 221, 7] and model shape [100, 221, 6] does not match. W0327 22:12:15.248327 139725850806080 call_variants.py:549] Input channels [1, 2, 3, 4, 5, 6, 19] and model channels [1, 2, 3, 4, 5, 6] do not match. ```. Your customized model was trained on `[1, 2, 3, 4, 5, 6]` (the `BASE_CHANNELS`) but the examples in `make_examples.tfrecord-00000-of-00001.gz` have an extra channel, 19 (`insert_size`), which gets [added to the WGS model preset](https://github.com/google/deepvariant/blob/r1.6.1/scripts/run_deepvariant.py#L369). . You can either:. a) include `--channels ""insert_size""` when [generating the training data](https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-training-case-study.md#training-set). b) don't set `--model_type WGS` when you call `run_deepvariant` (which you may not need to do regardless if you provide a `customized_model`). . The choice comes down to if you want to include the channel or not. Experiments have shown it provides a slight accuracy boost for WGS, but its not strictly necessary.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:884,safety,Input,Input,884,"The issue stems from a mismatch between the set of channels the model was trained on and the channels in the examples generated during `run_deepvariant`. The important bit in the logs you posted is:. ```. From /90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/training_dir_test2/checkpoints/ckpt-58/example_info.json: Shape of input examples: [100, 221, 6], Channels of input examples: [1, 2, 3, 4, 5, 6]. I0327 22:12:15.248034 139725850806080 dv_utils.py:365] From /local/scratch/haley.arnold/14698718/tmpg5h0cte0/make_examples.tfrecord-00000-of-00001.gz.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 19]. W0327 22:12:15.248203 139725850806080 call_variants.py:541] Input shape [100, 221, 7] and model shape [100, 221, 6] does not match. W0327 22:12:15.248327 139725850806080 call_variants.py:549] Input channels [1, 2, 3, 4, 5, 6, 19] and model channels [1, 2, 3, 4, 5, 6] do not match. ```. Your customized model was trained on `[1, 2, 3, 4, 5, 6]` (the `BASE_CHANNELS`) but the examples in `make_examples.tfrecord-00000-of-00001.gz` have an extra channel, 19 (`insert_size`), which gets [added to the WGS model preset](https://github.com/google/deepvariant/blob/r1.6.1/scripts/run_deepvariant.py#L369). . You can either:. a) include `--channels ""insert_size""` when [generating the training data](https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-training-case-study.md#training-set). b) don't set `--model_type WGS` when you call `run_deepvariant` (which you may not need to do regardless if you provide a `customized_model`). . The choice comes down to if you want to include the channel or not. Experiments have shown it provides a slight accuracy boost for WGS, but its not strictly necessary.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:64,security,model,model,64,"The issue stems from a mismatch between the set of channels the model was trained on and the channels in the examples generated during `run_deepvariant`. The important bit in the logs you posted is:. ```. From /90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/training_dir_test2/checkpoints/ckpt-58/example_info.json: Shape of input examples: [100, 221, 6], Channels of input examples: [1, 2, 3, 4, 5, 6]. I0327 22:12:15.248034 139725850806080 dv_utils.py:365] From /local/scratch/haley.arnold/14698718/tmpg5h0cte0/make_examples.tfrecord-00000-of-00001.gz.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 19]. W0327 22:12:15.248203 139725850806080 call_variants.py:541] Input shape [100, 221, 7] and model shape [100, 221, 6] does not match. W0327 22:12:15.248327 139725850806080 call_variants.py:549] Input channels [1, 2, 3, 4, 5, 6, 19] and model channels [1, 2, 3, 4, 5, 6] do not match. ```. Your customized model was trained on `[1, 2, 3, 4, 5, 6]` (the `BASE_CHANNELS`) but the examples in `make_examples.tfrecord-00000-of-00001.gz` have an extra channel, 19 (`insert_size`), which gets [added to the WGS model preset](https://github.com/google/deepvariant/blob/r1.6.1/scripts/run_deepvariant.py#L369). . You can either:. a) include `--channels ""insert_size""` when [generating the training data](https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-training-case-study.md#training-set). b) don't set `--model_type WGS` when you call `run_deepvariant` (which you may not need to do regardless if you provide a `customized_model`). . The choice comes down to if you want to include the channel or not. Experiments have shown it provides a slight accuracy boost for WGS, but its not strictly necessary.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:179,security,log,logs,179,"The issue stems from a mismatch between the set of channels the model was trained on and the channels in the examples generated during `run_deepvariant`. The important bit in the logs you posted is:. ```. From /90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/training_dir_test2/checkpoints/ckpt-58/example_info.json: Shape of input examples: [100, 221, 6], Channels of input examples: [1, 2, 3, 4, 5, 6]. I0327 22:12:15.248034 139725850806080 dv_utils.py:365] From /local/scratch/haley.arnold/14698718/tmpg5h0cte0/make_examples.tfrecord-00000-of-00001.gz.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 19]. W0327 22:12:15.248203 139725850806080 call_variants.py:541] Input shape [100, 221, 7] and model shape [100, 221, 6] does not match. W0327 22:12:15.248327 139725850806080 call_variants.py:549] Input channels [1, 2, 3, 4, 5, 6, 19] and model channels [1, 2, 3, 4, 5, 6] do not match. ```. Your customized model was trained on `[1, 2, 3, 4, 5, 6]` (the `BASE_CHANNELS`) but the examples in `make_examples.tfrecord-00000-of-00001.gz` have an extra channel, 19 (`insert_size`), which gets [added to the WGS model preset](https://github.com/google/deepvariant/blob/r1.6.1/scripts/run_deepvariant.py#L369). . You can either:. a) include `--channels ""insert_size""` when [generating the training data](https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-training-case-study.md#training-set). b) don't set `--model_type WGS` when you call `run_deepvariant` (which you may not need to do regardless if you provide a `customized_model`). . The choice comes down to if you want to include the channel or not. Experiments have shown it provides a slight accuracy boost for WGS, but its not strictly necessary.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:782,security,model,model,782,"The issue stems from a mismatch between the set of channels the model was trained on and the channels in the examples generated during `run_deepvariant`. The important bit in the logs you posted is:. ```. From /90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/training_dir_test2/checkpoints/ckpt-58/example_info.json: Shape of input examples: [100, 221, 6], Channels of input examples: [1, 2, 3, 4, 5, 6]. I0327 22:12:15.248034 139725850806080 dv_utils.py:365] From /local/scratch/haley.arnold/14698718/tmpg5h0cte0/make_examples.tfrecord-00000-of-00001.gz.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 19]. W0327 22:12:15.248203 139725850806080 call_variants.py:541] Input shape [100, 221, 7] and model shape [100, 221, 6] does not match. W0327 22:12:15.248327 139725850806080 call_variants.py:549] Input channels [1, 2, 3, 4, 5, 6, 19] and model channels [1, 2, 3, 4, 5, 6] do not match. ```. Your customized model was trained on `[1, 2, 3, 4, 5, 6]` (the `BASE_CHANNELS`) but the examples in `make_examples.tfrecord-00000-of-00001.gz` have an extra channel, 19 (`insert_size`), which gets [added to the WGS model preset](https://github.com/google/deepvariant/blob/r1.6.1/scripts/run_deepvariant.py#L369). . You can either:. a) include `--channels ""insert_size""` when [generating the training data](https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-training-case-study.md#training-set). b) don't set `--model_type WGS` when you call `run_deepvariant` (which you may not need to do regardless if you provide a `customized_model`). . The choice comes down to if you want to include the channel or not. Experiments have shown it provides a slight accuracy boost for WGS, but its not strictly necessary.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:926,security,model,model,926,"The issue stems from a mismatch between the set of channels the model was trained on and the channels in the examples generated during `run_deepvariant`. The important bit in the logs you posted is:. ```. From /90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/training_dir_test2/checkpoints/ckpt-58/example_info.json: Shape of input examples: [100, 221, 6], Channels of input examples: [1, 2, 3, 4, 5, 6]. I0327 22:12:15.248034 139725850806080 dv_utils.py:365] From /local/scratch/haley.arnold/14698718/tmpg5h0cte0/make_examples.tfrecord-00000-of-00001.gz.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 19]. W0327 22:12:15.248203 139725850806080 call_variants.py:541] Input shape [100, 221, 7] and model shape [100, 221, 6] does not match. W0327 22:12:15.248327 139725850806080 call_variants.py:549] Input channels [1, 2, 3, 4, 5, 6, 19] and model channels [1, 2, 3, 4, 5, 6] do not match. ```. Your customized model was trained on `[1, 2, 3, 4, 5, 6]` (the `BASE_CHANNELS`) but the examples in `make_examples.tfrecord-00000-of-00001.gz` have an extra channel, 19 (`insert_size`), which gets [added to the WGS model preset](https://github.com/google/deepvariant/blob/r1.6.1/scripts/run_deepvariant.py#L369). . You can either:. a) include `--channels ""insert_size""` when [generating the training data](https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-training-case-study.md#training-set). b) don't set `--model_type WGS` when you call `run_deepvariant` (which you may not need to do regardless if you provide a `customized_model`). . The choice comes down to if you want to include the channel or not. Experiments have shown it provides a slight accuracy boost for WGS, but its not strictly necessary.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:995,security,model,model,995,"The issue stems from a mismatch between the set of channels the model was trained on and the channels in the examples generated during `run_deepvariant`. The important bit in the logs you posted is:. ```. From /90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/training_dir_test2/checkpoints/ckpt-58/example_info.json: Shape of input examples: [100, 221, 6], Channels of input examples: [1, 2, 3, 4, 5, 6]. I0327 22:12:15.248034 139725850806080 dv_utils.py:365] From /local/scratch/haley.arnold/14698718/tmpg5h0cte0/make_examples.tfrecord-00000-of-00001.gz.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 19]. W0327 22:12:15.248203 139725850806080 call_variants.py:541] Input shape [100, 221, 7] and model shape [100, 221, 6] does not match. W0327 22:12:15.248327 139725850806080 call_variants.py:549] Input channels [1, 2, 3, 4, 5, 6, 19] and model channels [1, 2, 3, 4, 5, 6] do not match. ```. Your customized model was trained on `[1, 2, 3, 4, 5, 6]` (the `BASE_CHANNELS`) but the examples in `make_examples.tfrecord-00000-of-00001.gz` have an extra channel, 19 (`insert_size`), which gets [added to the WGS model preset](https://github.com/google/deepvariant/blob/r1.6.1/scripts/run_deepvariant.py#L369). . You can either:. a) include `--channels ""insert_size""` when [generating the training data](https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-training-case-study.md#training-set). b) don't set `--model_type WGS` when you call `run_deepvariant` (which you may not need to do regardless if you provide a `customized_model`). . The choice comes down to if you want to include the channel or not. Experiments have shown it provides a slight accuracy boost for WGS, but its not strictly necessary.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:1194,security,model,model,1194,"The issue stems from a mismatch between the set of channels the model was trained on and the channels in the examples generated during `run_deepvariant`. The important bit in the logs you posted is:. ```. From /90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/training_dir_test2/checkpoints/ckpt-58/example_info.json: Shape of input examples: [100, 221, 6], Channels of input examples: [1, 2, 3, 4, 5, 6]. I0327 22:12:15.248034 139725850806080 dv_utils.py:365] From /local/scratch/haley.arnold/14698718/tmpg5h0cte0/make_examples.tfrecord-00000-of-00001.gz.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 19]. W0327 22:12:15.248203 139725850806080 call_variants.py:541] Input shape [100, 221, 7] and model shape [100, 221, 6] does not match. W0327 22:12:15.248327 139725850806080 call_variants.py:549] Input channels [1, 2, 3, 4, 5, 6, 19] and model channels [1, 2, 3, 4, 5, 6] do not match. ```. Your customized model was trained on `[1, 2, 3, 4, 5, 6]` (the `BASE_CHANNELS`) but the examples in `make_examples.tfrecord-00000-of-00001.gz` have an extra channel, 19 (`insert_size`), which gets [added to the WGS model preset](https://github.com/google/deepvariant/blob/r1.6.1/scripts/run_deepvariant.py#L369). . You can either:. a) include `--channels ""insert_size""` when [generating the training data](https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-training-case-study.md#training-set). b) don't set `--model_type WGS` when you call `run_deepvariant` (which you may not need to do regardless if you provide a `customized_model`). . The choice comes down to if you want to include the channel or not. Experiments have shown it provides a slight accuracy boost for WGS, but its not strictly necessary.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:179,testability,log,logs,179,"The issue stems from a mismatch between the set of channels the model was trained on and the channels in the examples generated during `run_deepvariant`. The important bit in the logs you posted is:. ```. From /90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/training_dir_test2/checkpoints/ckpt-58/example_info.json: Shape of input examples: [100, 221, 6], Channels of input examples: [1, 2, 3, 4, 5, 6]. I0327 22:12:15.248034 139725850806080 dv_utils.py:365] From /local/scratch/haley.arnold/14698718/tmpg5h0cte0/make_examples.tfrecord-00000-of-00001.gz.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 19]. W0327 22:12:15.248203 139725850806080 call_variants.py:541] Input shape [100, 221, 7] and model shape [100, 221, 6] does not match. W0327 22:12:15.248327 139725850806080 call_variants.py:549] Input channels [1, 2, 3, 4, 5, 6, 19] and model channels [1, 2, 3, 4, 5, 6] do not match. ```. Your customized model was trained on `[1, 2, 3, 4, 5, 6]` (the `BASE_CHANNELS`) but the examples in `make_examples.tfrecord-00000-of-00001.gz` have an extra channel, 19 (`insert_size`), which gets [added to the WGS model preset](https://github.com/google/deepvariant/blob/r1.6.1/scripts/run_deepvariant.py#L369). . You can either:. a) include `--channels ""insert_size""` when [generating the training data](https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-training-case-study.md#training-set). b) don't set `--model_type WGS` when you call `run_deepvariant` (which you may not need to do regardless if you provide a `customized_model`). . The choice comes down to if you want to include the channel or not. Experiments have shown it provides a slight accuracy boost for WGS, but its not strictly necessary.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:352,usability,input,input,352,"The issue stems from a mismatch between the set of channels the model was trained on and the channels in the examples generated during `run_deepvariant`. The important bit in the logs you posted is:. ```. From /90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/training_dir_test2/checkpoints/ckpt-58/example_info.json: Shape of input examples: [100, 221, 6], Channels of input examples: [1, 2, 3, 4, 5, 6]. I0327 22:12:15.248034 139725850806080 dv_utils.py:365] From /local/scratch/haley.arnold/14698718/tmpg5h0cte0/make_examples.tfrecord-00000-of-00001.gz.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 19]. W0327 22:12:15.248203 139725850806080 call_variants.py:541] Input shape [100, 221, 7] and model shape [100, 221, 6] does not match. W0327 22:12:15.248327 139725850806080 call_variants.py:549] Input channels [1, 2, 3, 4, 5, 6, 19] and model channels [1, 2, 3, 4, 5, 6] do not match. ```. Your customized model was trained on `[1, 2, 3, 4, 5, 6]` (the `BASE_CHANNELS`) but the examples in `make_examples.tfrecord-00000-of-00001.gz` have an extra channel, 19 (`insert_size`), which gets [added to the WGS model preset](https://github.com/google/deepvariant/blob/r1.6.1/scripts/run_deepvariant.py#L369). . You can either:. a) include `--channels ""insert_size""` when [generating the training data](https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-training-case-study.md#training-set). b) don't set `--model_type WGS` when you call `run_deepvariant` (which you may not need to do regardless if you provide a `customized_model`). . The choice comes down to if you want to include the channel or not. Experiments have shown it provides a slight accuracy boost for WGS, but its not strictly necessary.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:395,usability,input,input,395,"The issue stems from a mismatch between the set of channels the model was trained on and the channels in the examples generated during `run_deepvariant`. The important bit in the logs you posted is:. ```. From /90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/training_dir_test2/checkpoints/ckpt-58/example_info.json: Shape of input examples: [100, 221, 6], Channels of input examples: [1, 2, 3, 4, 5, 6]. I0327 22:12:15.248034 139725850806080 dv_utils.py:365] From /local/scratch/haley.arnold/14698718/tmpg5h0cte0/make_examples.tfrecord-00000-of-00001.gz.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 19]. W0327 22:12:15.248203 139725850806080 call_variants.py:541] Input shape [100, 221, 7] and model shape [100, 221, 6] does not match. W0327 22:12:15.248327 139725850806080 call_variants.py:549] Input channels [1, 2, 3, 4, 5, 6, 19] and model channels [1, 2, 3, 4, 5, 6] do not match. ```. Your customized model was trained on `[1, 2, 3, 4, 5, 6]` (the `BASE_CHANNELS`) but the examples in `make_examples.tfrecord-00000-of-00001.gz` have an extra channel, 19 (`insert_size`), which gets [added to the WGS model preset](https://github.com/google/deepvariant/blob/r1.6.1/scripts/run_deepvariant.py#L369). . You can either:. a) include `--channels ""insert_size""` when [generating the training data](https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-training-case-study.md#training-set). b) don't set `--model_type WGS` when you call `run_deepvariant` (which you may not need to do regardless if you provide a `customized_model`). . The choice comes down to if you want to include the channel or not. Experiments have shown it provides a slight accuracy boost for WGS, but its not strictly necessary.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:609,usability,input,input,609,"The issue stems from a mismatch between the set of channels the model was trained on and the channels in the examples generated during `run_deepvariant`. The important bit in the logs you posted is:. ```. From /90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/training_dir_test2/checkpoints/ckpt-58/example_info.json: Shape of input examples: [100, 221, 6], Channels of input examples: [1, 2, 3, 4, 5, 6]. I0327 22:12:15.248034 139725850806080 dv_utils.py:365] From /local/scratch/haley.arnold/14698718/tmpg5h0cte0/make_examples.tfrecord-00000-of-00001.gz.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 19]. W0327 22:12:15.248203 139725850806080 call_variants.py:541] Input shape [100, 221, 7] and model shape [100, 221, 6] does not match. W0327 22:12:15.248327 139725850806080 call_variants.py:549] Input channels [1, 2, 3, 4, 5, 6, 19] and model channels [1, 2, 3, 4, 5, 6] do not match. ```. Your customized model was trained on `[1, 2, 3, 4, 5, 6]` (the `BASE_CHANNELS`) but the examples in `make_examples.tfrecord-00000-of-00001.gz` have an extra channel, 19 (`insert_size`), which gets [added to the WGS model preset](https://github.com/google/deepvariant/blob/r1.6.1/scripts/run_deepvariant.py#L369). . You can either:. a) include `--channels ""insert_size""` when [generating the training data](https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-training-case-study.md#training-set). b) don't set `--model_type WGS` when you call `run_deepvariant` (which you may not need to do regardless if you provide a `customized_model`). . The choice comes down to if you want to include the channel or not. Experiments have shown it provides a slight accuracy boost for WGS, but its not strictly necessary.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:652,usability,input,input,652,"The issue stems from a mismatch between the set of channels the model was trained on and the channels in the examples generated during `run_deepvariant`. The important bit in the logs you posted is:. ```. From /90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/training_dir_test2/checkpoints/ckpt-58/example_info.json: Shape of input examples: [100, 221, 6], Channels of input examples: [1, 2, 3, 4, 5, 6]. I0327 22:12:15.248034 139725850806080 dv_utils.py:365] From /local/scratch/haley.arnold/14698718/tmpg5h0cte0/make_examples.tfrecord-00000-of-00001.gz.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 19]. W0327 22:12:15.248203 139725850806080 call_variants.py:541] Input shape [100, 221, 7] and model shape [100, 221, 6] does not match. W0327 22:12:15.248327 139725850806080 call_variants.py:549] Input channels [1, 2, 3, 4, 5, 6, 19] and model channels [1, 2, 3, 4, 5, 6] do not match. ```. Your customized model was trained on `[1, 2, 3, 4, 5, 6]` (the `BASE_CHANNELS`) but the examples in `make_examples.tfrecord-00000-of-00001.gz` have an extra channel, 19 (`insert_size`), which gets [added to the WGS model preset](https://github.com/google/deepvariant/blob/r1.6.1/scripts/run_deepvariant.py#L369). . You can either:. a) include `--channels ""insert_size""` when [generating the training data](https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-training-case-study.md#training-set). b) don't set `--model_type WGS` when you call `run_deepvariant` (which you may not need to do regardless if you provide a `customized_model`). . The choice comes down to if you want to include the channel or not. Experiments have shown it provides a slight accuracy boost for WGS, but its not strictly necessary.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:752,usability,Input,Input,752,"The issue stems from a mismatch between the set of channels the model was trained on and the channels in the examples generated during `run_deepvariant`. The important bit in the logs you posted is:. ```. From /90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/training_dir_test2/checkpoints/ckpt-58/example_info.json: Shape of input examples: [100, 221, 6], Channels of input examples: [1, 2, 3, 4, 5, 6]. I0327 22:12:15.248034 139725850806080 dv_utils.py:365] From /local/scratch/haley.arnold/14698718/tmpg5h0cte0/make_examples.tfrecord-00000-of-00001.gz.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 19]. W0327 22:12:15.248203 139725850806080 call_variants.py:541] Input shape [100, 221, 7] and model shape [100, 221, 6] does not match. W0327 22:12:15.248327 139725850806080 call_variants.py:549] Input channels [1, 2, 3, 4, 5, 6, 19] and model channels [1, 2, 3, 4, 5, 6] do not match. ```. Your customized model was trained on `[1, 2, 3, 4, 5, 6]` (the `BASE_CHANNELS`) but the examples in `make_examples.tfrecord-00000-of-00001.gz` have an extra channel, 19 (`insert_size`), which gets [added to the WGS model preset](https://github.com/google/deepvariant/blob/r1.6.1/scripts/run_deepvariant.py#L369). . You can either:. a) include `--channels ""insert_size""` when [generating the training data](https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-training-case-study.md#training-set). b) don't set `--model_type WGS` when you call `run_deepvariant` (which you may not need to do regardless if you provide a `customized_model`). . The choice comes down to if you want to include the channel or not. Experiments have shown it provides a slight accuracy boost for WGS, but its not strictly necessary.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:884,usability,Input,Input,884,"The issue stems from a mismatch between the set of channels the model was trained on and the channels in the examples generated during `run_deepvariant`. The important bit in the logs you posted is:. ```. From /90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/training_dir_test2/checkpoints/ckpt-58/example_info.json: Shape of input examples: [100, 221, 6], Channels of input examples: [1, 2, 3, 4, 5, 6]. I0327 22:12:15.248034 139725850806080 dv_utils.py:365] From /local/scratch/haley.arnold/14698718/tmpg5h0cte0/make_examples.tfrecord-00000-of-00001.gz.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 19]. W0327 22:12:15.248203 139725850806080 call_variants.py:541] Input shape [100, 221, 7] and model shape [100, 221, 6] does not match. W0327 22:12:15.248327 139725850806080 call_variants.py:549] Input channels [1, 2, 3, 4, 5, 6, 19] and model channels [1, 2, 3, 4, 5, 6] do not match. ```. Your customized model was trained on `[1, 2, 3, 4, 5, 6]` (the `BASE_CHANNELS`) but the examples in `make_examples.tfrecord-00000-of-00001.gz` have an extra channel, 19 (`insert_size`), which gets [added to the WGS model preset](https://github.com/google/deepvariant/blob/r1.6.1/scripts/run_deepvariant.py#L369). . You can either:. a) include `--channels ""insert_size""` when [generating the training data](https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-training-case-study.md#training-set). b) don't set `--model_type WGS` when you call `run_deepvariant` (which you may not need to do regardless if you provide a `customized_model`). . The choice comes down to if you want to include the channel or not. Experiments have shown it provides a slight accuracy boost for WGS, but its not strictly necessary.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:984,usability,custom,customized,984,"The issue stems from a mismatch between the set of channels the model was trained on and the channels in the examples generated during `run_deepvariant`. The important bit in the logs you posted is:. ```. From /90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/training_dir_test2/checkpoints/ckpt-58/example_info.json: Shape of input examples: [100, 221, 6], Channels of input examples: [1, 2, 3, 4, 5, 6]. I0327 22:12:15.248034 139725850806080 dv_utils.py:365] From /local/scratch/haley.arnold/14698718/tmpg5h0cte0/make_examples.tfrecord-00000-of-00001.gz.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 19]. W0327 22:12:15.248203 139725850806080 call_variants.py:541] Input shape [100, 221, 7] and model shape [100, 221, 6] does not match. W0327 22:12:15.248327 139725850806080 call_variants.py:549] Input channels [1, 2, 3, 4, 5, 6, 19] and model channels [1, 2, 3, 4, 5, 6] do not match. ```. Your customized model was trained on `[1, 2, 3, 4, 5, 6]` (the `BASE_CHANNELS`) but the examples in `make_examples.tfrecord-00000-of-00001.gz` have an extra channel, 19 (`insert_size`), which gets [added to the WGS model preset](https://github.com/google/deepvariant/blob/r1.6.1/scripts/run_deepvariant.py#L369). . You can either:. a) include `--channels ""insert_size""` when [generating the training data](https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-training-case-study.md#training-set). b) don't set `--model_type WGS` when you call `run_deepvariant` (which you may not need to do regardless if you provide a `customized_model`). . The choice comes down to if you want to include the channel or not. Experiments have shown it provides a slight accuracy boost for WGS, but its not strictly necessary.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:77,availability,error,error,77,"Hello, thank you for your reply! . Not setting `--model_type WGS ` led to an error. To clarify, when you say generating the training data, you're referring to including `--channels ""insert_size""` in the make_examples steps for the training and validation sets, correct? Or do you mean the step where the custom model is trained? . Thank you! . Haley",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:311,energy efficiency,model,model,311,"Hello, thank you for your reply! . Not setting `--model_type WGS ` led to an error. To clarify, when you say generating the training data, you're referring to including `--channels ""insert_size""` in the make_examples steps for the training and validation sets, correct? Or do you mean the step where the custom model is trained? . Thank you! . Haley",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:77,performance,error,error,77,"Hello, thank you for your reply! . Not setting `--model_type WGS ` led to an error. To clarify, when you say generating the training data, you're referring to including `--channels ""insert_size""` in the make_examples steps for the training and validation sets, correct? Or do you mean the step where the custom model is trained? . Thank you! . Haley",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:77,safety,error,error,77,"Hello, thank you for your reply! . Not setting `--model_type WGS ` led to an error. To clarify, when you say generating the training data, you're referring to including `--channels ""insert_size""` in the make_examples steps for the training and validation sets, correct? Or do you mean the step where the custom model is trained? . Thank you! . Haley",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:244,safety,valid,validation,244,"Hello, thank you for your reply! . Not setting `--model_type WGS ` led to an error. To clarify, when you say generating the training data, you're referring to including `--channels ""insert_size""` in the make_examples steps for the training and validation sets, correct? Or do you mean the step where the custom model is trained? . Thank you! . Haley",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:244,security,validat,validation,244,"Hello, thank you for your reply! . Not setting `--model_type WGS ` led to an error. To clarify, when you say generating the training data, you're referring to including `--channels ""insert_size""` in the make_examples steps for the training and validation sets, correct? Or do you mean the step where the custom model is trained? . Thank you! . Haley",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:311,security,model,model,311,"Hello, thank you for your reply! . Not setting `--model_type WGS ` led to an error. To clarify, when you say generating the training data, you're referring to including `--channels ""insert_size""` in the make_examples steps for the training and validation sets, correct? Or do you mean the step where the custom model is trained? . Thank you! . Haley",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:77,usability,error,error,77,"Hello, thank you for your reply! . Not setting `--model_type WGS ` led to an error. To clarify, when you say generating the training data, you're referring to including `--channels ""insert_size""` in the make_examples steps for the training and validation sets, correct? Or do you mean the step where the custom model is trained? . Thank you! . Haley",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:304,usability,custom,custom,304,"Hello, thank you for your reply! . Not setting `--model_type WGS ` led to an error. To clarify, when you say generating the training data, you're referring to including `--channels ""insert_size""` in the make_examples steps for the training and validation sets, correct? Or do you mean the step where the custom model is trained? . Thank you! . Haley",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:106,energy efficiency,model,model,106,"That's right, `channels` are set during `make_examples` when generating training and validation sets. The model will then use those during training.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:85,safety,valid,validation,85,"That's right, `channels` are set during `make_examples` when generating training and validation sets. The model will then use those during training.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:85,security,validat,validation,85,"That's right, `channels` are set during `make_examples` when generating training and validation sets. The model will then use those during training.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:106,security,model,model,106,"That's right, `channels` are set during `make_examples` when generating training and validation sets. The model will then use those during training.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:245,availability,checkpoint,checkpoints,245,"Thank you! I re-ran the training and validation sets with that flag, and re-shuffled them. Now, however, when I go to train the model (using the same parameters as the example case study--I just want to test out the process) I'm not getting any checkpoints in the output training directory, just the event log and the json file. What does this mean? Is the training step failing, or do I simply need to adjust my parameters? Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:306,deployability,log,log,306,"Thank you! I re-ran the training and validation sets with that flag, and re-shuffled them. Now, however, when I go to train the model (using the same parameters as the example case study--I just want to test out the process) I'm not getting any checkpoints in the output training directory, just the event log and the json file. What does this mean? Is the training step failing, or do I simply need to adjust my parameters? Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:371,deployability,fail,failing,371,"Thank you! I re-ran the training and validation sets with that flag, and re-shuffled them. Now, however, when I go to train the model (using the same parameters as the example case study--I just want to test out the process) I'm not getting any checkpoints in the output training directory, just the event log and the json file. What does this mean? Is the training step failing, or do I simply need to adjust my parameters? Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:128,energy efficiency,model,model,128,"Thank you! I re-ran the training and validation sets with that flag, and re-shuffled them. Now, however, when I go to train the model (using the same parameters as the example case study--I just want to test out the process) I'm not getting any checkpoints in the output training directory, just the event log and the json file. What does this mean? Is the training step failing, or do I simply need to adjust my parameters? Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:300,integrability,event,event,300,"Thank you! I re-ran the training and validation sets with that flag, and re-shuffled them. Now, however, when I go to train the model (using the same parameters as the example case study--I just want to test out the process) I'm not getting any checkpoints in the output training directory, just the event log and the json file. What does this mean? Is the training step failing, or do I simply need to adjust my parameters? Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:150,modifiability,paramet,parameters,150,"Thank you! I re-ran the training and validation sets with that flag, and re-shuffled them. Now, however, when I go to train the model (using the same parameters as the example case study--I just want to test out the process) I'm not getting any checkpoints in the output training directory, just the event log and the json file. What does this mean? Is the training step failing, or do I simply need to adjust my parameters? Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:413,modifiability,paramet,parameters,413,"Thank you! I re-ran the training and validation sets with that flag, and re-shuffled them. Now, however, when I go to train the model (using the same parameters as the example case study--I just want to test out the process) I'm not getting any checkpoints in the output training directory, just the event log and the json file. What does this mean? Is the training step failing, or do I simply need to adjust my parameters? Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:245,reliability,checkpoint,checkpoints,245,"Thank you! I re-ran the training and validation sets with that flag, and re-shuffled them. Now, however, when I go to train the model (using the same parameters as the example case study--I just want to test out the process) I'm not getting any checkpoints in the output training directory, just the event log and the json file. What does this mean? Is the training step failing, or do I simply need to adjust my parameters? Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:334,reliability,doe,does,334,"Thank you! I re-ran the training and validation sets with that flag, and re-shuffled them. Now, however, when I go to train the model (using the same parameters as the example case study--I just want to test out the process) I'm not getting any checkpoints in the output training directory, just the event log and the json file. What does this mean? Is the training step failing, or do I simply need to adjust my parameters? Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:371,reliability,fail,failing,371,"Thank you! I re-ran the training and validation sets with that flag, and re-shuffled them. Now, however, when I go to train the model (using the same parameters as the example case study--I just want to test out the process) I'm not getting any checkpoints in the output training directory, just the event log and the json file. What does this mean? Is the training step failing, or do I simply need to adjust my parameters? Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:37,safety,valid,validation,37,"Thank you! I re-ran the training and validation sets with that flag, and re-shuffled them. Now, however, when I go to train the model (using the same parameters as the example case study--I just want to test out the process) I'm not getting any checkpoints in the output training directory, just the event log and the json file. What does this mean? Is the training step failing, or do I simply need to adjust my parameters? Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:203,safety,test,test,203,"Thank you! I re-ran the training and validation sets with that flag, and re-shuffled them. Now, however, when I go to train the model (using the same parameters as the example case study--I just want to test out the process) I'm not getting any checkpoints in the output training directory, just the event log and the json file. What does this mean? Is the training step failing, or do I simply need to adjust my parameters? Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:306,safety,log,log,306,"Thank you! I re-ran the training and validation sets with that flag, and re-shuffled them. Now, however, when I go to train the model (using the same parameters as the example case study--I just want to test out the process) I'm not getting any checkpoints in the output training directory, just the event log and the json file. What does this mean? Is the training step failing, or do I simply need to adjust my parameters? Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:37,security,validat,validation,37,"Thank you! I re-ran the training and validation sets with that flag, and re-shuffled them. Now, however, when I go to train the model (using the same parameters as the example case study--I just want to test out the process) I'm not getting any checkpoints in the output training directory, just the event log and the json file. What does this mean? Is the training step failing, or do I simply need to adjust my parameters? Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:128,security,model,model,128,"Thank you! I re-ran the training and validation sets with that flag, and re-shuffled them. Now, however, when I go to train the model (using the same parameters as the example case study--I just want to test out the process) I'm not getting any checkpoints in the output training directory, just the event log and the json file. What does this mean? Is the training step failing, or do I simply need to adjust my parameters? Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:306,security,log,log,306,"Thank you! I re-ran the training and validation sets with that flag, and re-shuffled them. Now, however, when I go to train the model (using the same parameters as the example case study--I just want to test out the process) I'm not getting any checkpoints in the output training directory, just the event log and the json file. What does this mean? Is the training step failing, or do I simply need to adjust my parameters? Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:203,testability,test,test,203,"Thank you! I re-ran the training and validation sets with that flag, and re-shuffled them. Now, however, when I go to train the model (using the same parameters as the example case study--I just want to test out the process) I'm not getting any checkpoints in the output training directory, just the event log and the json file. What does this mean? Is the training step failing, or do I simply need to adjust my parameters? Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:306,testability,log,log,306,"Thank you! I re-ran the training and validation sets with that flag, and re-shuffled them. Now, however, when I go to train the model (using the same parameters as the example case study--I just want to test out the process) I'm not getting any checkpoints in the output training directory, just the event log and the json file. What does this mean? Is the training step failing, or do I simply need to adjust my parameters? Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:388,testability,simpl,simply,388,"Thank you! I re-ran the training and validation sets with that flag, and re-shuffled them. Now, however, when I go to train the model (using the same parameters as the example case study--I just want to test out the process) I'm not getting any checkpoints in the output training directory, just the event log and the json file. What does this mean? Is the training step failing, or do I simply need to adjust my parameters? Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:388,usability,simpl,simply,388,"Thank you! I re-ran the training and validation sets with that flag, and re-shuffled them. Now, however, when I go to train the model (using the same parameters as the example case study--I just want to test out the process) I'm not getting any checkpoints in the output training directory, just the event log and the json file. What does this mean? Is the training step failing, or do I simply need to adjust my parameters? Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:272,deployability,log,log,272,"1) Can you confirm that you have generated training/validation data? e.g, run . ```. gsutil cat ""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt"". ```. and . ```. cat ""${OUTPUT_DIR}/validation_set.dataset_config.pbtxt"". ```. 2) What do you see in the `${LOG_DIR}/train.log` file?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:52,safety,valid,validation,52,"1) Can you confirm that you have generated training/validation data? e.g, run . ```. gsutil cat ""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt"". ```. and . ```. cat ""${OUTPUT_DIR}/validation_set.dataset_config.pbtxt"". ```. 2) What do you see in the `${LOG_DIR}/train.log` file?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:272,safety,log,log,272,"1) Can you confirm that you have generated training/validation data? e.g, run . ```. gsutil cat ""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt"". ```. and . ```. cat ""${OUTPUT_DIR}/validation_set.dataset_config.pbtxt"". ```. 2) What do you see in the `${LOG_DIR}/train.log` file?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:52,security,validat,validation,52,"1) Can you confirm that you have generated training/validation data? e.g, run . ```. gsutil cat ""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt"". ```. and . ```. cat ""${OUTPUT_DIR}/validation_set.dataset_config.pbtxt"". ```. 2) What do you see in the `${LOG_DIR}/train.log` file?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:272,security,log,log,272,"1) Can you confirm that you have generated training/validation data? e.g, run . ```. gsutil cat ""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt"". ```. and . ```. cat ""${OUTPUT_DIR}/validation_set.dataset_config.pbtxt"". ```. 2) What do you see in the `${LOG_DIR}/train.log` file?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:272,testability,log,log,272,"1) Can you confirm that you have generated training/validation data? e.g, run . ```. gsutil cat ""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt"". ```. and . ```. cat ""${OUTPUT_DIR}/validation_set.dataset_config.pbtxt"". ```. 2) What do you see in the `${LOG_DIR}/train.log` file?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:11,usability,confirm,confirm,11,"1) Can you confirm that you have generated training/validation data? e.g, run . ```. gsutil cat ""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt"". ```. and . ```. cat ""${OUTPUT_DIR}/validation_set.dataset_config.pbtxt"". ```. 2) What do you see in the `${LOG_DIR}/train.log` file?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:62,deployability,log,log,62,"Yes, I definitely got each pbtxt file. Attached below are the log files from the model train step. When I ran this step before (when I had not used the --channels flag, and could not test the model), the .err file for the model training step looked as though it reached a stopping point, whereas in this run it looks like it simply stopped and did not reach that same point. It's definitely not a timeout issue, but I'm not sure what's causing it. . The pbtxt file for the validation set (training set looks similar) looks like this:. ```. # Generated by shuffle_tfrecords_beam.py. #. # --input_pattern_list=/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/validation_set.with_label_channlesize.tfrecord.gz. # --output_pattern_prefix=/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/validation_set.with_label_channelsize.shuffled. #. name: ""Chromosome3"". tfrecord_path: ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/validation_set.with_label_channelsize.shuffled-?????-of-?????.tfrecord.gz"". num_examples: 35759. # class1: 27257. # class0: 1777. # class2: 6725. ```. And here are the log files from the attempted model training: . [deepvariant_modeltrain-14705863-Atlas-0031.err.txt](https://github.com/google/deepvariant/files/14828238/deepvariant_modeltrain-14705863-Atlas-0031.err.txt). [deepvariant_modeltrain-14705863-Atlas-0031.out.txt](https://github.com/google/deepvariant/files/14828239/deepvariant_modeltrain-14705863-Atlas-0031.out.txt). Thank you for your help! Best, . Haley",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:1166,deployability,log,log,1166,"Yes, I definitely got each pbtxt file. Attached below are the log files from the model train step. When I ran this step before (when I had not used the --channels flag, and could not test the model), the .err file for the model training step looked as though it reached a stopping point, whereas in this run it looks like it simply stopped and did not reach that same point. It's definitely not a timeout issue, but I'm not sure what's causing it. . The pbtxt file for the validation set (training set looks similar) looks like this:. ```. # Generated by shuffle_tfrecords_beam.py. #. # --input_pattern_list=/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/validation_set.with_label_channlesize.tfrecord.gz. # --output_pattern_prefix=/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/validation_set.with_label_channelsize.shuffled. #. name: ""Chromosome3"". tfrecord_path: ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/validation_set.with_label_channelsize.shuffled-?????-of-?????.tfrecord.gz"". num_examples: 35759. # class1: 27257. # class0: 1777. # class2: 6725. ```. And here are the log files from the attempted model training: . [deepvariant_modeltrain-14705863-Atlas-0031.err.txt](https://github.com/google/deepvariant/files/14828238/deepvariant_modeltrain-14705863-Atlas-0031.err.txt). [deepvariant_modeltrain-14705863-Atlas-0031.out.txt](https://github.com/google/deepvariant/files/14828239/deepvariant_modeltrain-14705863-Atlas-0031.out.txt). Thank you for your help! Best, . Haley",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:81,energy efficiency,model,model,81,"Yes, I definitely got each pbtxt file. Attached below are the log files from the model train step. When I ran this step before (when I had not used the --channels flag, and could not test the model), the .err file for the model training step looked as though it reached a stopping point, whereas in this run it looks like it simply stopped and did not reach that same point. It's definitely not a timeout issue, but I'm not sure what's causing it. . The pbtxt file for the validation set (training set looks similar) looks like this:. ```. # Generated by shuffle_tfrecords_beam.py. #. # --input_pattern_list=/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/validation_set.with_label_channlesize.tfrecord.gz. # --output_pattern_prefix=/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/validation_set.with_label_channelsize.shuffled. #. name: ""Chromosome3"". tfrecord_path: ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/validation_set.with_label_channelsize.shuffled-?????-of-?????.tfrecord.gz"". num_examples: 35759. # class1: 27257. # class0: 1777. # class2: 6725. ```. And here are the log files from the attempted model training: . [deepvariant_modeltrain-14705863-Atlas-0031.err.txt](https://github.com/google/deepvariant/files/14828238/deepvariant_modeltrain-14705863-Atlas-0031.err.txt). [deepvariant_modeltrain-14705863-Atlas-0031.out.txt](https://github.com/google/deepvariant/files/14828239/deepvariant_modeltrain-14705863-Atlas-0031.out.txt). Thank you for your help! Best, . Haley",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:192,energy efficiency,model,model,192,"Yes, I definitely got each pbtxt file. Attached below are the log files from the model train step. When I ran this step before (when I had not used the --channels flag, and could not test the model), the .err file for the model training step looked as though it reached a stopping point, whereas in this run it looks like it simply stopped and did not reach that same point. It's definitely not a timeout issue, but I'm not sure what's causing it. . The pbtxt file for the validation set (training set looks similar) looks like this:. ```. # Generated by shuffle_tfrecords_beam.py. #. # --input_pattern_list=/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/validation_set.with_label_channlesize.tfrecord.gz. # --output_pattern_prefix=/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/validation_set.with_label_channelsize.shuffled. #. name: ""Chromosome3"". tfrecord_path: ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/validation_set.with_label_channelsize.shuffled-?????-of-?????.tfrecord.gz"". num_examples: 35759. # class1: 27257. # class0: 1777. # class2: 6725. ```. And here are the log files from the attempted model training: . [deepvariant_modeltrain-14705863-Atlas-0031.err.txt](https://github.com/google/deepvariant/files/14828238/deepvariant_modeltrain-14705863-Atlas-0031.err.txt). [deepvariant_modeltrain-14705863-Atlas-0031.out.txt](https://github.com/google/deepvariant/files/14828239/deepvariant_modeltrain-14705863-Atlas-0031.out.txt). Thank you for your help! Best, . Haley",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:222,energy efficiency,model,model,222,"Yes, I definitely got each pbtxt file. Attached below are the log files from the model train step. When I ran this step before (when I had not used the --channels flag, and could not test the model), the .err file for the model training step looked as though it reached a stopping point, whereas in this run it looks like it simply stopped and did not reach that same point. It's definitely not a timeout issue, but I'm not sure what's causing it. . The pbtxt file for the validation set (training set looks similar) looks like this:. ```. # Generated by shuffle_tfrecords_beam.py. #. # --input_pattern_list=/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/validation_set.with_label_channlesize.tfrecord.gz. # --output_pattern_prefix=/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/validation_set.with_label_channelsize.shuffled. #. name: ""Chromosome3"". tfrecord_path: ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/validation_set.with_label_channelsize.shuffled-?????-of-?????.tfrecord.gz"". num_examples: 35759. # class1: 27257. # class0: 1777. # class2: 6725. ```. And here are the log files from the attempted model training: . [deepvariant_modeltrain-14705863-Atlas-0031.err.txt](https://github.com/google/deepvariant/files/14828238/deepvariant_modeltrain-14705863-Atlas-0031.err.txt). [deepvariant_modeltrain-14705863-Atlas-0031.out.txt](https://github.com/google/deepvariant/files/14828239/deepvariant_modeltrain-14705863-Atlas-0031.out.txt). Thank you for your help! Best, . Haley",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:1195,energy efficiency,model,model,1195,"Yes, I definitely got each pbtxt file. Attached below are the log files from the model train step. When I ran this step before (when I had not used the --channels flag, and could not test the model), the .err file for the model training step looked as though it reached a stopping point, whereas in this run it looks like it simply stopped and did not reach that same point. It's definitely not a timeout issue, but I'm not sure what's causing it. . The pbtxt file for the validation set (training set looks similar) looks like this:. ```. # Generated by shuffle_tfrecords_beam.py. #. # --input_pattern_list=/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/validation_set.with_label_channlesize.tfrecord.gz. # --output_pattern_prefix=/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/validation_set.with_label_channelsize.shuffled. #. name: ""Chromosome3"". tfrecord_path: ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/validation_set.with_label_channelsize.shuffled-?????-of-?????.tfrecord.gz"". num_examples: 35759. # class1: 27257. # class0: 1777. # class2: 6725. ```. And here are the log files from the attempted model training: . [deepvariant_modeltrain-14705863-Atlas-0031.err.txt](https://github.com/google/deepvariant/files/14828238/deepvariant_modeltrain-14705863-Atlas-0031.err.txt). [deepvariant_modeltrain-14705863-Atlas-0031.out.txt](https://github.com/google/deepvariant/files/14828239/deepvariant_modeltrain-14705863-Atlas-0031.out.txt). Thank you for your help! Best, . Haley",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:397,performance,time,timeout,397,"Yes, I definitely got each pbtxt file. Attached below are the log files from the model train step. When I ran this step before (when I had not used the --channels flag, and could not test the model), the .err file for the model training step looked as though it reached a stopping point, whereas in this run it looks like it simply stopped and did not reach that same point. It's definitely not a timeout issue, but I'm not sure what's causing it. . The pbtxt file for the validation set (training set looks similar) looks like this:. ```. # Generated by shuffle_tfrecords_beam.py. #. # --input_pattern_list=/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/validation_set.with_label_channlesize.tfrecord.gz. # --output_pattern_prefix=/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/validation_set.with_label_channelsize.shuffled. #. name: ""Chromosome3"". tfrecord_path: ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/validation_set.with_label_channelsize.shuffled-?????-of-?????.tfrecord.gz"". num_examples: 35759. # class1: 27257. # class0: 1777. # class2: 6725. ```. And here are the log files from the attempted model training: . [deepvariant_modeltrain-14705863-Atlas-0031.err.txt](https://github.com/google/deepvariant/files/14828238/deepvariant_modeltrain-14705863-Atlas-0031.err.txt). [deepvariant_modeltrain-14705863-Atlas-0031.out.txt](https://github.com/google/deepvariant/files/14828239/deepvariant_modeltrain-14705863-Atlas-0031.out.txt). Thank you for your help! Best, . Haley",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:62,safety,log,log,62,"Yes, I definitely got each pbtxt file. Attached below are the log files from the model train step. When I ran this step before (when I had not used the --channels flag, and could not test the model), the .err file for the model training step looked as though it reached a stopping point, whereas in this run it looks like it simply stopped and did not reach that same point. It's definitely not a timeout issue, but I'm not sure what's causing it. . The pbtxt file for the validation set (training set looks similar) looks like this:. ```. # Generated by shuffle_tfrecords_beam.py. #. # --input_pattern_list=/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/validation_set.with_label_channlesize.tfrecord.gz. # --output_pattern_prefix=/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/validation_set.with_label_channelsize.shuffled. #. name: ""Chromosome3"". tfrecord_path: ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/validation_set.with_label_channelsize.shuffled-?????-of-?????.tfrecord.gz"". num_examples: 35759. # class1: 27257. # class0: 1777. # class2: 6725. ```. And here are the log files from the attempted model training: . [deepvariant_modeltrain-14705863-Atlas-0031.err.txt](https://github.com/google/deepvariant/files/14828238/deepvariant_modeltrain-14705863-Atlas-0031.err.txt). [deepvariant_modeltrain-14705863-Atlas-0031.out.txt](https://github.com/google/deepvariant/files/14828239/deepvariant_modeltrain-14705863-Atlas-0031.out.txt). Thank you for your help! Best, . Haley",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:183,safety,test,test,183,"Yes, I definitely got each pbtxt file. Attached below are the log files from the model train step. When I ran this step before (when I had not used the --channels flag, and could not test the model), the .err file for the model training step looked as though it reached a stopping point, whereas in this run it looks like it simply stopped and did not reach that same point. It's definitely not a timeout issue, but I'm not sure what's causing it. . The pbtxt file for the validation set (training set looks similar) looks like this:. ```. # Generated by shuffle_tfrecords_beam.py. #. # --input_pattern_list=/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/validation_set.with_label_channlesize.tfrecord.gz. # --output_pattern_prefix=/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/validation_set.with_label_channelsize.shuffled. #. name: ""Chromosome3"". tfrecord_path: ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/validation_set.with_label_channelsize.shuffled-?????-of-?????.tfrecord.gz"". num_examples: 35759. # class1: 27257. # class0: 1777. # class2: 6725. ```. And here are the log files from the attempted model training: . [deepvariant_modeltrain-14705863-Atlas-0031.err.txt](https://github.com/google/deepvariant/files/14828238/deepvariant_modeltrain-14705863-Atlas-0031.err.txt). [deepvariant_modeltrain-14705863-Atlas-0031.out.txt](https://github.com/google/deepvariant/files/14828239/deepvariant_modeltrain-14705863-Atlas-0031.out.txt). Thank you for your help! Best, . Haley",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:397,safety,timeout,timeout,397,"Yes, I definitely got each pbtxt file. Attached below are the log files from the model train step. When I ran this step before (when I had not used the --channels flag, and could not test the model), the .err file for the model training step looked as though it reached a stopping point, whereas in this run it looks like it simply stopped and did not reach that same point. It's definitely not a timeout issue, but I'm not sure what's causing it. . The pbtxt file for the validation set (training set looks similar) looks like this:. ```. # Generated by shuffle_tfrecords_beam.py. #. # --input_pattern_list=/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/validation_set.with_label_channlesize.tfrecord.gz. # --output_pattern_prefix=/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/validation_set.with_label_channelsize.shuffled. #. name: ""Chromosome3"". tfrecord_path: ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/validation_set.with_label_channelsize.shuffled-?????-of-?????.tfrecord.gz"". num_examples: 35759. # class1: 27257. # class0: 1777. # class2: 6725. ```. And here are the log files from the attempted model training: . [deepvariant_modeltrain-14705863-Atlas-0031.err.txt](https://github.com/google/deepvariant/files/14828238/deepvariant_modeltrain-14705863-Atlas-0031.err.txt). [deepvariant_modeltrain-14705863-Atlas-0031.out.txt](https://github.com/google/deepvariant/files/14828239/deepvariant_modeltrain-14705863-Atlas-0031.out.txt). Thank you for your help! Best, . Haley",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:473,safety,valid,validation,473,"Yes, I definitely got each pbtxt file. Attached below are the log files from the model train step. When I ran this step before (when I had not used the --channels flag, and could not test the model), the .err file for the model training step looked as though it reached a stopping point, whereas in this run it looks like it simply stopped and did not reach that same point. It's definitely not a timeout issue, but I'm not sure what's causing it. . The pbtxt file for the validation set (training set looks similar) looks like this:. ```. # Generated by shuffle_tfrecords_beam.py. #. # --input_pattern_list=/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/validation_set.with_label_channlesize.tfrecord.gz. # --output_pattern_prefix=/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/validation_set.with_label_channelsize.shuffled. #. name: ""Chromosome3"". tfrecord_path: ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/validation_set.with_label_channelsize.shuffled-?????-of-?????.tfrecord.gz"". num_examples: 35759. # class1: 27257. # class0: 1777. # class2: 6725. ```. And here are the log files from the attempted model training: . [deepvariant_modeltrain-14705863-Atlas-0031.err.txt](https://github.com/google/deepvariant/files/14828238/deepvariant_modeltrain-14705863-Atlas-0031.err.txt). [deepvariant_modeltrain-14705863-Atlas-0031.out.txt](https://github.com/google/deepvariant/files/14828239/deepvariant_modeltrain-14705863-Atlas-0031.out.txt). Thank you for your help! Best, . Haley",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:1166,safety,log,log,1166,"Yes, I definitely got each pbtxt file. Attached below are the log files from the model train step. When I ran this step before (when I had not used the --channels flag, and could not test the model), the .err file for the model training step looked as though it reached a stopping point, whereas in this run it looks like it simply stopped and did not reach that same point. It's definitely not a timeout issue, but I'm not sure what's causing it. . The pbtxt file for the validation set (training set looks similar) looks like this:. ```. # Generated by shuffle_tfrecords_beam.py. #. # --input_pattern_list=/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/validation_set.with_label_channlesize.tfrecord.gz. # --output_pattern_prefix=/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/validation_set.with_label_channelsize.shuffled. #. name: ""Chromosome3"". tfrecord_path: ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/validation_set.with_label_channelsize.shuffled-?????-of-?????.tfrecord.gz"". num_examples: 35759. # class1: 27257. # class0: 1777. # class2: 6725. ```. And here are the log files from the attempted model training: . [deepvariant_modeltrain-14705863-Atlas-0031.err.txt](https://github.com/google/deepvariant/files/14828238/deepvariant_modeltrain-14705863-Atlas-0031.err.txt). [deepvariant_modeltrain-14705863-Atlas-0031.out.txt](https://github.com/google/deepvariant/files/14828239/deepvariant_modeltrain-14705863-Atlas-0031.out.txt). Thank you for your help! Best, . Haley",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:62,security,log,log,62,"Yes, I definitely got each pbtxt file. Attached below are the log files from the model train step. When I ran this step before (when I had not used the --channels flag, and could not test the model), the .err file for the model training step looked as though it reached a stopping point, whereas in this run it looks like it simply stopped and did not reach that same point. It's definitely not a timeout issue, but I'm not sure what's causing it. . The pbtxt file for the validation set (training set looks similar) looks like this:. ```. # Generated by shuffle_tfrecords_beam.py. #. # --input_pattern_list=/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/validation_set.with_label_channlesize.tfrecord.gz. # --output_pattern_prefix=/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/validation_set.with_label_channelsize.shuffled. #. name: ""Chromosome3"". tfrecord_path: ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/validation_set.with_label_channelsize.shuffled-?????-of-?????.tfrecord.gz"". num_examples: 35759. # class1: 27257. # class0: 1777. # class2: 6725. ```. And here are the log files from the attempted model training: . [deepvariant_modeltrain-14705863-Atlas-0031.err.txt](https://github.com/google/deepvariant/files/14828238/deepvariant_modeltrain-14705863-Atlas-0031.err.txt). [deepvariant_modeltrain-14705863-Atlas-0031.out.txt](https://github.com/google/deepvariant/files/14828239/deepvariant_modeltrain-14705863-Atlas-0031.out.txt). Thank you for your help! Best, . Haley",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:81,security,model,model,81,"Yes, I definitely got each pbtxt file. Attached below are the log files from the model train step. When I ran this step before (when I had not used the --channels flag, and could not test the model), the .err file for the model training step looked as though it reached a stopping point, whereas in this run it looks like it simply stopped and did not reach that same point. It's definitely not a timeout issue, but I'm not sure what's causing it. . The pbtxt file for the validation set (training set looks similar) looks like this:. ```. # Generated by shuffle_tfrecords_beam.py. #. # --input_pattern_list=/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/validation_set.with_label_channlesize.tfrecord.gz. # --output_pattern_prefix=/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/validation_set.with_label_channelsize.shuffled. #. name: ""Chromosome3"". tfrecord_path: ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/validation_set.with_label_channelsize.shuffled-?????-of-?????.tfrecord.gz"". num_examples: 35759. # class1: 27257. # class0: 1777. # class2: 6725. ```. And here are the log files from the attempted model training: . [deepvariant_modeltrain-14705863-Atlas-0031.err.txt](https://github.com/google/deepvariant/files/14828238/deepvariant_modeltrain-14705863-Atlas-0031.err.txt). [deepvariant_modeltrain-14705863-Atlas-0031.out.txt](https://github.com/google/deepvariant/files/14828239/deepvariant_modeltrain-14705863-Atlas-0031.out.txt). Thank you for your help! Best, . Haley",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:192,security,model,model,192,"Yes, I definitely got each pbtxt file. Attached below are the log files from the model train step. When I ran this step before (when I had not used the --channels flag, and could not test the model), the .err file for the model training step looked as though it reached a stopping point, whereas in this run it looks like it simply stopped and did not reach that same point. It's definitely not a timeout issue, but I'm not sure what's causing it. . The pbtxt file for the validation set (training set looks similar) looks like this:. ```. # Generated by shuffle_tfrecords_beam.py. #. # --input_pattern_list=/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/validation_set.with_label_channlesize.tfrecord.gz. # --output_pattern_prefix=/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/validation_set.with_label_channelsize.shuffled. #. name: ""Chromosome3"". tfrecord_path: ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/validation_set.with_label_channelsize.shuffled-?????-of-?????.tfrecord.gz"". num_examples: 35759. # class1: 27257. # class0: 1777. # class2: 6725. ```. And here are the log files from the attempted model training: . [deepvariant_modeltrain-14705863-Atlas-0031.err.txt](https://github.com/google/deepvariant/files/14828238/deepvariant_modeltrain-14705863-Atlas-0031.err.txt). [deepvariant_modeltrain-14705863-Atlas-0031.out.txt](https://github.com/google/deepvariant/files/14828239/deepvariant_modeltrain-14705863-Atlas-0031.out.txt). Thank you for your help! Best, . Haley",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:222,security,model,model,222,"Yes, I definitely got each pbtxt file. Attached below are the log files from the model train step. When I ran this step before (when I had not used the --channels flag, and could not test the model), the .err file for the model training step looked as though it reached a stopping point, whereas in this run it looks like it simply stopped and did not reach that same point. It's definitely not a timeout issue, but I'm not sure what's causing it. . The pbtxt file for the validation set (training set looks similar) looks like this:. ```. # Generated by shuffle_tfrecords_beam.py. #. # --input_pattern_list=/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/validation_set.with_label_channlesize.tfrecord.gz. # --output_pattern_prefix=/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/validation_set.with_label_channelsize.shuffled. #. name: ""Chromosome3"". tfrecord_path: ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/validation_set.with_label_channelsize.shuffled-?????-of-?????.tfrecord.gz"". num_examples: 35759. # class1: 27257. # class0: 1777. # class2: 6725. ```. And here are the log files from the attempted model training: . [deepvariant_modeltrain-14705863-Atlas-0031.err.txt](https://github.com/google/deepvariant/files/14828238/deepvariant_modeltrain-14705863-Atlas-0031.err.txt). [deepvariant_modeltrain-14705863-Atlas-0031.out.txt](https://github.com/google/deepvariant/files/14828239/deepvariant_modeltrain-14705863-Atlas-0031.out.txt). Thank you for your help! Best, . Haley",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:473,security,validat,validation,473,"Yes, I definitely got each pbtxt file. Attached below are the log files from the model train step. When I ran this step before (when I had not used the --channels flag, and could not test the model), the .err file for the model training step looked as though it reached a stopping point, whereas in this run it looks like it simply stopped and did not reach that same point. It's definitely not a timeout issue, but I'm not sure what's causing it. . The pbtxt file for the validation set (training set looks similar) looks like this:. ```. # Generated by shuffle_tfrecords_beam.py. #. # --input_pattern_list=/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/validation_set.with_label_channlesize.tfrecord.gz. # --output_pattern_prefix=/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/validation_set.with_label_channelsize.shuffled. #. name: ""Chromosome3"". tfrecord_path: ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/validation_set.with_label_channelsize.shuffled-?????-of-?????.tfrecord.gz"". num_examples: 35759. # class1: 27257. # class0: 1777. # class2: 6725. ```. And here are the log files from the attempted model training: . [deepvariant_modeltrain-14705863-Atlas-0031.err.txt](https://github.com/google/deepvariant/files/14828238/deepvariant_modeltrain-14705863-Atlas-0031.err.txt). [deepvariant_modeltrain-14705863-Atlas-0031.out.txt](https://github.com/google/deepvariant/files/14828239/deepvariant_modeltrain-14705863-Atlas-0031.out.txt). Thank you for your help! Best, . Haley",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:1166,security,log,log,1166,"Yes, I definitely got each pbtxt file. Attached below are the log files from the model train step. When I ran this step before (when I had not used the --channels flag, and could not test the model), the .err file for the model training step looked as though it reached a stopping point, whereas in this run it looks like it simply stopped and did not reach that same point. It's definitely not a timeout issue, but I'm not sure what's causing it. . The pbtxt file for the validation set (training set looks similar) looks like this:. ```. # Generated by shuffle_tfrecords_beam.py. #. # --input_pattern_list=/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/validation_set.with_label_channlesize.tfrecord.gz. # --output_pattern_prefix=/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/validation_set.with_label_channelsize.shuffled. #. name: ""Chromosome3"". tfrecord_path: ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/validation_set.with_label_channelsize.shuffled-?????-of-?????.tfrecord.gz"". num_examples: 35759. # class1: 27257. # class0: 1777. # class2: 6725. ```. And here are the log files from the attempted model training: . [deepvariant_modeltrain-14705863-Atlas-0031.err.txt](https://github.com/google/deepvariant/files/14828238/deepvariant_modeltrain-14705863-Atlas-0031.err.txt). [deepvariant_modeltrain-14705863-Atlas-0031.out.txt](https://github.com/google/deepvariant/files/14828239/deepvariant_modeltrain-14705863-Atlas-0031.out.txt). Thank you for your help! Best, . Haley",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:1195,security,model,model,1195,"Yes, I definitely got each pbtxt file. Attached below are the log files from the model train step. When I ran this step before (when I had not used the --channels flag, and could not test the model), the .err file for the model training step looked as though it reached a stopping point, whereas in this run it looks like it simply stopped and did not reach that same point. It's definitely not a timeout issue, but I'm not sure what's causing it. . The pbtxt file for the validation set (training set looks similar) looks like this:. ```. # Generated by shuffle_tfrecords_beam.py. #. # --input_pattern_list=/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/validation_set.with_label_channlesize.tfrecord.gz. # --output_pattern_prefix=/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/validation_set.with_label_channelsize.shuffled. #. name: ""Chromosome3"". tfrecord_path: ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/validation_set.with_label_channelsize.shuffled-?????-of-?????.tfrecord.gz"". num_examples: 35759. # class1: 27257. # class0: 1777. # class2: 6725. ```. And here are the log files from the attempted model training: . [deepvariant_modeltrain-14705863-Atlas-0031.err.txt](https://github.com/google/deepvariant/files/14828238/deepvariant_modeltrain-14705863-Atlas-0031.err.txt). [deepvariant_modeltrain-14705863-Atlas-0031.out.txt](https://github.com/google/deepvariant/files/14828239/deepvariant_modeltrain-14705863-Atlas-0031.out.txt). Thank you for your help! Best, . Haley",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:62,testability,log,log,62,"Yes, I definitely got each pbtxt file. Attached below are the log files from the model train step. When I ran this step before (when I had not used the --channels flag, and could not test the model), the .err file for the model training step looked as though it reached a stopping point, whereas in this run it looks like it simply stopped and did not reach that same point. It's definitely not a timeout issue, but I'm not sure what's causing it. . The pbtxt file for the validation set (training set looks similar) looks like this:. ```. # Generated by shuffle_tfrecords_beam.py. #. # --input_pattern_list=/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/validation_set.with_label_channlesize.tfrecord.gz. # --output_pattern_prefix=/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/validation_set.with_label_channelsize.shuffled. #. name: ""Chromosome3"". tfrecord_path: ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/validation_set.with_label_channelsize.shuffled-?????-of-?????.tfrecord.gz"". num_examples: 35759. # class1: 27257. # class0: 1777. # class2: 6725. ```. And here are the log files from the attempted model training: . [deepvariant_modeltrain-14705863-Atlas-0031.err.txt](https://github.com/google/deepvariant/files/14828238/deepvariant_modeltrain-14705863-Atlas-0031.err.txt). [deepvariant_modeltrain-14705863-Atlas-0031.out.txt](https://github.com/google/deepvariant/files/14828239/deepvariant_modeltrain-14705863-Atlas-0031.out.txt). Thank you for your help! Best, . Haley",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:183,testability,test,test,183,"Yes, I definitely got each pbtxt file. Attached below are the log files from the model train step. When I ran this step before (when I had not used the --channels flag, and could not test the model), the .err file for the model training step looked as though it reached a stopping point, whereas in this run it looks like it simply stopped and did not reach that same point. It's definitely not a timeout issue, but I'm not sure what's causing it. . The pbtxt file for the validation set (training set looks similar) looks like this:. ```. # Generated by shuffle_tfrecords_beam.py. #. # --input_pattern_list=/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/validation_set.with_label_channlesize.tfrecord.gz. # --output_pattern_prefix=/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/validation_set.with_label_channelsize.shuffled. #. name: ""Chromosome3"". tfrecord_path: ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/validation_set.with_label_channelsize.shuffled-?????-of-?????.tfrecord.gz"". num_examples: 35759. # class1: 27257. # class0: 1777. # class2: 6725. ```. And here are the log files from the attempted model training: . [deepvariant_modeltrain-14705863-Atlas-0031.err.txt](https://github.com/google/deepvariant/files/14828238/deepvariant_modeltrain-14705863-Atlas-0031.err.txt). [deepvariant_modeltrain-14705863-Atlas-0031.out.txt](https://github.com/google/deepvariant/files/14828239/deepvariant_modeltrain-14705863-Atlas-0031.out.txt). Thank you for your help! Best, . Haley",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:325,testability,simpl,simply,325,"Yes, I definitely got each pbtxt file. Attached below are the log files from the model train step. When I ran this step before (when I had not used the --channels flag, and could not test the model), the .err file for the model training step looked as though it reached a stopping point, whereas in this run it looks like it simply stopped and did not reach that same point. It's definitely not a timeout issue, but I'm not sure what's causing it. . The pbtxt file for the validation set (training set looks similar) looks like this:. ```. # Generated by shuffle_tfrecords_beam.py. #. # --input_pattern_list=/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/validation_set.with_label_channlesize.tfrecord.gz. # --output_pattern_prefix=/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/validation_set.with_label_channelsize.shuffled. #. name: ""Chromosome3"". tfrecord_path: ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/validation_set.with_label_channelsize.shuffled-?????-of-?????.tfrecord.gz"". num_examples: 35759. # class1: 27257. # class0: 1777. # class2: 6725. ```. And here are the log files from the attempted model training: . [deepvariant_modeltrain-14705863-Atlas-0031.err.txt](https://github.com/google/deepvariant/files/14828238/deepvariant_modeltrain-14705863-Atlas-0031.err.txt). [deepvariant_modeltrain-14705863-Atlas-0031.out.txt](https://github.com/google/deepvariant/files/14828239/deepvariant_modeltrain-14705863-Atlas-0031.out.txt). Thank you for your help! Best, . Haley",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:1166,testability,log,log,1166,"Yes, I definitely got each pbtxt file. Attached below are the log files from the model train step. When I ran this step before (when I had not used the --channels flag, and could not test the model), the .err file for the model training step looked as though it reached a stopping point, whereas in this run it looks like it simply stopped and did not reach that same point. It's definitely not a timeout issue, but I'm not sure what's causing it. . The pbtxt file for the validation set (training set looks similar) looks like this:. ```. # Generated by shuffle_tfrecords_beam.py. #. # --input_pattern_list=/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/validation_set.with_label_channlesize.tfrecord.gz. # --output_pattern_prefix=/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/validation_set.with_label_channelsize.shuffled. #. name: ""Chromosome3"". tfrecord_path: ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/validation_set.with_label_channelsize.shuffled-?????-of-?????.tfrecord.gz"". num_examples: 35759. # class1: 27257. # class0: 1777. # class2: 6725. ```. And here are the log files from the attempted model training: . [deepvariant_modeltrain-14705863-Atlas-0031.err.txt](https://github.com/google/deepvariant/files/14828238/deepvariant_modeltrain-14705863-Atlas-0031.err.txt). [deepvariant_modeltrain-14705863-Atlas-0031.out.txt](https://github.com/google/deepvariant/files/14828239/deepvariant_modeltrain-14705863-Atlas-0031.out.txt). Thank you for your help! Best, . Haley",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:272,usability,stop,stopping,272,"Yes, I definitely got each pbtxt file. Attached below are the log files from the model train step. When I ran this step before (when I had not used the --channels flag, and could not test the model), the .err file for the model training step looked as though it reached a stopping point, whereas in this run it looks like it simply stopped and did not reach that same point. It's definitely not a timeout issue, but I'm not sure what's causing it. . The pbtxt file for the validation set (training set looks similar) looks like this:. ```. # Generated by shuffle_tfrecords_beam.py. #. # --input_pattern_list=/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/validation_set.with_label_channlesize.tfrecord.gz. # --output_pattern_prefix=/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/validation_set.with_label_channelsize.shuffled. #. name: ""Chromosome3"". tfrecord_path: ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/validation_set.with_label_channelsize.shuffled-?????-of-?????.tfrecord.gz"". num_examples: 35759. # class1: 27257. # class0: 1777. # class2: 6725. ```. And here are the log files from the attempted model training: . [deepvariant_modeltrain-14705863-Atlas-0031.err.txt](https://github.com/google/deepvariant/files/14828238/deepvariant_modeltrain-14705863-Atlas-0031.err.txt). [deepvariant_modeltrain-14705863-Atlas-0031.out.txt](https://github.com/google/deepvariant/files/14828239/deepvariant_modeltrain-14705863-Atlas-0031.out.txt). Thank you for your help! Best, . Haley",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:325,usability,simpl,simply,325,"Yes, I definitely got each pbtxt file. Attached below are the log files from the model train step. When I ran this step before (when I had not used the --channels flag, and could not test the model), the .err file for the model training step looked as though it reached a stopping point, whereas in this run it looks like it simply stopped and did not reach that same point. It's definitely not a timeout issue, but I'm not sure what's causing it. . The pbtxt file for the validation set (training set looks similar) looks like this:. ```. # Generated by shuffle_tfrecords_beam.py. #. # --input_pattern_list=/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/validation_set.with_label_channlesize.tfrecord.gz. # --output_pattern_prefix=/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/validation_set.with_label_channelsize.shuffled. #. name: ""Chromosome3"". tfrecord_path: ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/validation_set.with_label_channelsize.shuffled-?????-of-?????.tfrecord.gz"". num_examples: 35759. # class1: 27257. # class0: 1777. # class2: 6725. ```. And here are the log files from the attempted model training: . [deepvariant_modeltrain-14705863-Atlas-0031.err.txt](https://github.com/google/deepvariant/files/14828238/deepvariant_modeltrain-14705863-Atlas-0031.err.txt). [deepvariant_modeltrain-14705863-Atlas-0031.out.txt](https://github.com/google/deepvariant/files/14828239/deepvariant_modeltrain-14705863-Atlas-0031.out.txt). Thank you for your help! Best, . Haley",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:332,usability,stop,stopped,332,"Yes, I definitely got each pbtxt file. Attached below are the log files from the model train step. When I ran this step before (when I had not used the --channels flag, and could not test the model), the .err file for the model training step looked as though it reached a stopping point, whereas in this run it looks like it simply stopped and did not reach that same point. It's definitely not a timeout issue, but I'm not sure what's causing it. . The pbtxt file for the validation set (training set looks similar) looks like this:. ```. # Generated by shuffle_tfrecords_beam.py. #. # --input_pattern_list=/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/validation_set.with_label_channlesize.tfrecord.gz. # --output_pattern_prefix=/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/validation_set.with_label_channelsize.shuffled. #. name: ""Chromosome3"". tfrecord_path: ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/validation_set.with_label_channelsize.shuffled-?????-of-?????.tfrecord.gz"". num_examples: 35759. # class1: 27257. # class0: 1777. # class2: 6725. ```. And here are the log files from the attempted model training: . [deepvariant_modeltrain-14705863-Atlas-0031.err.txt](https://github.com/google/deepvariant/files/14828238/deepvariant_modeltrain-14705863-Atlas-0031.err.txt). [deepvariant_modeltrain-14705863-Atlas-0031.out.txt](https://github.com/google/deepvariant/files/14828239/deepvariant_modeltrain-14705863-Atlas-0031.out.txt). Thank you for your help! Best, . Haley",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:1550,usability,help,help,1550,"Yes, I definitely got each pbtxt file. Attached below are the log files from the model train step. When I ran this step before (when I had not used the --channels flag, and could not test the model), the .err file for the model training step looked as though it reached a stopping point, whereas in this run it looks like it simply stopped and did not reach that same point. It's definitely not a timeout issue, but I'm not sure what's causing it. . The pbtxt file for the validation set (training set looks similar) looks like this:. ```. # Generated by shuffle_tfrecords_beam.py. #. # --input_pattern_list=/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/validation_set.with_label_channlesize.tfrecord.gz. # --output_pattern_prefix=/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/validation_set.with_label_channelsize.shuffled. #. name: ""Chromosome3"". tfrecord_path: ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/validation_set.with_label_channelsize.shuffled-?????-of-?????.tfrecord.gz"". num_examples: 35759. # class1: 27257. # class0: 1777. # class2: 6725. ```. And here are the log files from the attempted model training: . [deepvariant_modeltrain-14705863-Atlas-0031.err.txt](https://github.com/google/deepvariant/files/14828238/deepvariant_modeltrain-14705863-Atlas-0031.err.txt). [deepvariant_modeltrain-14705863-Atlas-0031.out.txt](https://github.com/google/deepvariant/files/14828239/deepvariant_modeltrain-14705863-Atlas-0031.out.txt). Thank you for your help! Best, . Haley",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:71,availability,checkpoint,checkpoint,71,"Not sure what changed, but I ran the same code again and it produced a checkpoint output file this time! . Is the model_eval step no longer necessary? I see it in the 1.5 version of the case study documentation but not in the 1.6 version. . Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:171,deployability,version,version,171,"Not sure what changed, but I ran the same code again and it produced a checkpoint output file this time! . Is the model_eval step no longer necessary? I see it in the 1.5 version of the case study documentation but not in the 1.6 version. . Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:230,deployability,version,version,230,"Not sure what changed, but I ran the same code again and it produced a checkpoint output file this time! . Is the model_eval step no longer necessary? I see it in the 1.5 version of the case study documentation but not in the 1.6 version. . Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:171,integrability,version,version,171,"Not sure what changed, but I ran the same code again and it produced a checkpoint output file this time! . Is the model_eval step no longer necessary? I see it in the 1.5 version of the case study documentation but not in the 1.6 version. . Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:230,integrability,version,version,230,"Not sure what changed, but I ran the same code again and it produced a checkpoint output file this time! . Is the model_eval step no longer necessary? I see it in the 1.5 version of the case study documentation but not in the 1.6 version. . Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:171,modifiability,version,version,171,"Not sure what changed, but I ran the same code again and it produced a checkpoint output file this time! . Is the model_eval step no longer necessary? I see it in the 1.5 version of the case study documentation but not in the 1.6 version. . Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:230,modifiability,version,version,230,"Not sure what changed, but I ran the same code again and it produced a checkpoint output file this time! . Is the model_eval step no longer necessary? I see it in the 1.5 version of the case study documentation but not in the 1.6 version. . Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:99,performance,time,time,99,"Not sure what changed, but I ran the same code again and it produced a checkpoint output file this time! . Is the model_eval step no longer necessary? I see it in the 1.5 version of the case study documentation but not in the 1.6 version. . Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:71,reliability,checkpoint,checkpoint,71,"Not sure what changed, but I ran the same code again and it produced a checkpoint output file this time! . Is the model_eval step no longer necessary? I see it in the 1.5 version of the case study documentation but not in the 1.6 version. . Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:197,usability,document,documentation,197,"Not sure what changed, but I ran the same code again and it produced a checkpoint output file this time! . Is the model_eval step no longer necessary? I see it in the 1.5 version of the case study documentation but not in the 1.6 version. . Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:46,availability,checkpoint,checkpoints,46,"Great! The logs you posted confirmed that the checkpoints were not being written, but it's not clear _why_ that was the case. I will close this issue for now, but please don't hesitate to reopen if you encounter it again! To your second question, that's correct! In 1.6, we migrated our training and inference platform from Slim to Keras, and as part of this effort we combined `model_train` and `model_eval` with a single executable `train` to make training easier.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:324,availability,Sli,Slim,324,"Great! The logs you posted confirmed that the checkpoints were not being written, but it's not clear _why_ that was the case. I will close this issue for now, but please don't hesitate to reopen if you encounter it again! To your second question, that's correct! In 1.6, we migrated our training and inference platform from Slim to Keras, and as part of this effort we combined `model_train` and `model_eval` with a single executable `train` to make training easier.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:11,deployability,log,logs,11,"Great! The logs you posted confirmed that the checkpoints were not being written, but it's not clear _why_ that was the case. I will close this issue for now, but please don't hesitate to reopen if you encounter it again! To your second question, that's correct! In 1.6, we migrated our training and inference platform from Slim to Keras, and as part of this effort we combined `model_train` and `model_eval` with a single executable `train` to make training easier.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:310,interoperability,platform,platform,310,"Great! The logs you posted confirmed that the checkpoints were not being written, but it's not clear _why_ that was the case. I will close this issue for now, but please don't hesitate to reopen if you encounter it again! To your second question, that's correct! In 1.6, we migrated our training and inference platform from Slim to Keras, and as part of this effort we combined `model_train` and `model_eval` with a single executable `train` to make training easier.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:46,reliability,checkpoint,checkpoints,46,"Great! The logs you posted confirmed that the checkpoints were not being written, but it's not clear _why_ that was the case. I will close this issue for now, but please don't hesitate to reopen if you encounter it again! To your second question, that's correct! In 1.6, we migrated our training and inference platform from Slim to Keras, and as part of this effort we combined `model_train` and `model_eval` with a single executable `train` to make training easier.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:324,reliability,Sli,Slim,324,"Great! The logs you posted confirmed that the checkpoints were not being written, but it's not clear _why_ that was the case. I will close this issue for now, but please don't hesitate to reopen if you encounter it again! To your second question, that's correct! In 1.6, we migrated our training and inference platform from Slim to Keras, and as part of this effort we combined `model_train` and `model_eval` with a single executable `train` to make training easier.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:11,safety,log,logs,11,"Great! The logs you posted confirmed that the checkpoints were not being written, but it's not clear _why_ that was the case. I will close this issue for now, but please don't hesitate to reopen if you encounter it again! To your second question, that's correct! In 1.6, we migrated our training and inference platform from Slim to Keras, and as part of this effort we combined `model_train` and `model_eval` with a single executable `train` to make training easier.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:11,security,log,logs,11,"Great! The logs you posted confirmed that the checkpoints were not being written, but it's not clear _why_ that was the case. I will close this issue for now, but please don't hesitate to reopen if you encounter it again! To your second question, that's correct! In 1.6, we migrated our training and inference platform from Slim to Keras, and as part of this effort we combined `model_train` and `model_eval` with a single executable `train` to make training easier.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:11,testability,log,logs,11,"Great! The logs you posted confirmed that the checkpoints were not being written, but it's not clear _why_ that was the case. I will close this issue for now, but please don't hesitate to reopen if you encounter it again! To your second question, that's correct! In 1.6, we migrated our training and inference platform from Slim to Keras, and as part of this effort we combined `model_train` and `model_eval` with a single executable `train` to make training easier.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:27,usability,confirm,confirmed,27,"Great! The logs you posted confirmed that the checkpoints were not being written, but it's not clear _why_ that was the case. I will close this issue for now, but please don't hesitate to reopen if you encounter it again! To your second question, that's correct! In 1.6, we migrated our training and inference platform from Slim to Keras, and as part of this effort we combined `model_train` and `model_eval` with a single executable `train` to make training easier.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:95,usability,clear,clear,95,"Great! The logs you posted confirmed that the checkpoints were not being written, but it's not clear _why_ that was the case. I will close this issue for now, but please don't hesitate to reopen if you encounter it again! To your second question, that's correct! In 1.6, we migrated our training and inference platform from Slim to Keras, and as part of this effort we combined `model_train` and `model_eval` with a single executable `train` to make training easier.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:133,usability,close,close,133,"Great! The logs you posted confirmed that the checkpoints were not being written, but it's not clear _why_ that was the case. I will close this issue for now, but please don't hesitate to reopen if you encounter it again! To your second question, that's correct! In 1.6, we migrated our training and inference platform from Slim to Keras, and as part of this effort we combined `model_train` and `model_eval` with a single executable `train` to make training easier.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:124,availability,checkpoint,checkpoints,124,"@helizabeth1103 @lucasbrambrink I'll clear up some confusion real quick here - the updated training script will only output checkpoints if tune performance outperforms existing performance. If you look closely in the log file you can see this line:. ```. I0401 03:09:48.932735 140045983049536 train.py:471] Skipping checkpoint with tune/f1_weighted=0.83932966 < previous best tune/f1_weighted=0.8400078. ```. Which states that checkpointing is being skipped because the performance was worse. So in general, if you aren't seeing checkpoints you likely need to adjust parameters or train for longer.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:316,availability,checkpoint,checkpoint,316,"@helizabeth1103 @lucasbrambrink I'll clear up some confusion real quick here - the updated training script will only output checkpoints if tune performance outperforms existing performance. If you look closely in the log file you can see this line:. ```. I0401 03:09:48.932735 140045983049536 train.py:471] Skipping checkpoint with tune/f1_weighted=0.83932966 < previous best tune/f1_weighted=0.8400078. ```. Which states that checkpointing is being skipped because the performance was worse. So in general, if you aren't seeing checkpoints you likely need to adjust parameters or train for longer.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:415,availability,state,states,415,"@helizabeth1103 @lucasbrambrink I'll clear up some confusion real quick here - the updated training script will only output checkpoints if tune performance outperforms existing performance. If you look closely in the log file you can see this line:. ```. I0401 03:09:48.932735 140045983049536 train.py:471] Skipping checkpoint with tune/f1_weighted=0.83932966 < previous best tune/f1_weighted=0.8400078. ```. Which states that checkpointing is being skipped because the performance was worse. So in general, if you aren't seeing checkpoints you likely need to adjust parameters or train for longer.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:427,availability,checkpoint,checkpointing,427,"@helizabeth1103 @lucasbrambrink I'll clear up some confusion real quick here - the updated training script will only output checkpoints if tune performance outperforms existing performance. If you look closely in the log file you can see this line:. ```. I0401 03:09:48.932735 140045983049536 train.py:471] Skipping checkpoint with tune/f1_weighted=0.83932966 < previous best tune/f1_weighted=0.8400078. ```. Which states that checkpointing is being skipped because the performance was worse. So in general, if you aren't seeing checkpoints you likely need to adjust parameters or train for longer.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:529,availability,checkpoint,checkpoints,529,"@helizabeth1103 @lucasbrambrink I'll clear up some confusion real quick here - the updated training script will only output checkpoints if tune performance outperforms existing performance. If you look closely in the log file you can see this line:. ```. I0401 03:09:48.932735 140045983049536 train.py:471] Skipping checkpoint with tune/f1_weighted=0.83932966 < previous best tune/f1_weighted=0.8400078. ```. Which states that checkpointing is being skipped because the performance was worse. So in general, if you aren't seeing checkpoints you likely need to adjust parameters or train for longer.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:83,deployability,updat,updated,83,"@helizabeth1103 @lucasbrambrink I'll clear up some confusion real quick here - the updated training script will only output checkpoints if tune performance outperforms existing performance. If you look closely in the log file you can see this line:. ```. I0401 03:09:48.932735 140045983049536 train.py:471] Skipping checkpoint with tune/f1_weighted=0.83932966 < previous best tune/f1_weighted=0.8400078. ```. Which states that checkpointing is being skipped because the performance was worse. So in general, if you aren't seeing checkpoints you likely need to adjust parameters or train for longer.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:217,deployability,log,log,217,"@helizabeth1103 @lucasbrambrink I'll clear up some confusion real quick here - the updated training script will only output checkpoints if tune performance outperforms existing performance. If you look closely in the log file you can see this line:. ```. I0401 03:09:48.932735 140045983049536 train.py:471] Skipping checkpoint with tune/f1_weighted=0.83932966 < previous best tune/f1_weighted=0.8400078. ```. Which states that checkpointing is being skipped because the performance was worse. So in general, if you aren't seeing checkpoints you likely need to adjust parameters or train for longer.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:415,integrability,state,states,415,"@helizabeth1103 @lucasbrambrink I'll clear up some confusion real quick here - the updated training script will only output checkpoints if tune performance outperforms existing performance. If you look closely in the log file you can see this line:. ```. I0401 03:09:48.932735 140045983049536 train.py:471] Skipping checkpoint with tune/f1_weighted=0.83932966 < previous best tune/f1_weighted=0.8400078. ```. Which states that checkpointing is being skipped because the performance was worse. So in general, if you aren't seeing checkpoints you likely need to adjust parameters or train for longer.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:567,modifiability,paramet,parameters,567,"@helizabeth1103 @lucasbrambrink I'll clear up some confusion real quick here - the updated training script will only output checkpoints if tune performance outperforms existing performance. If you look closely in the log file you can see this line:. ```. I0401 03:09:48.932735 140045983049536 train.py:471] Skipping checkpoint with tune/f1_weighted=0.83932966 < previous best tune/f1_weighted=0.8400078. ```. Which states that checkpointing is being skipped because the performance was worse. So in general, if you aren't seeing checkpoints you likely need to adjust parameters or train for longer.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:139,performance,tune,tune,139,"@helizabeth1103 @lucasbrambrink I'll clear up some confusion real quick here - the updated training script will only output checkpoints if tune performance outperforms existing performance. If you look closely in the log file you can see this line:. ```. I0401 03:09:48.932735 140045983049536 train.py:471] Skipping checkpoint with tune/f1_weighted=0.83932966 < previous best tune/f1_weighted=0.8400078. ```. Which states that checkpointing is being skipped because the performance was worse. So in general, if you aren't seeing checkpoints you likely need to adjust parameters or train for longer.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:144,performance,perform,performance,144,"@helizabeth1103 @lucasbrambrink I'll clear up some confusion real quick here - the updated training script will only output checkpoints if tune performance outperforms existing performance. If you look closely in the log file you can see this line:. ```. I0401 03:09:48.932735 140045983049536 train.py:471] Skipping checkpoint with tune/f1_weighted=0.83932966 < previous best tune/f1_weighted=0.8400078. ```. Which states that checkpointing is being skipped because the performance was worse. So in general, if you aren't seeing checkpoints you likely need to adjust parameters or train for longer.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:177,performance,perform,performance,177,"@helizabeth1103 @lucasbrambrink I'll clear up some confusion real quick here - the updated training script will only output checkpoints if tune performance outperforms existing performance. If you look closely in the log file you can see this line:. ```. I0401 03:09:48.932735 140045983049536 train.py:471] Skipping checkpoint with tune/f1_weighted=0.83932966 < previous best tune/f1_weighted=0.8400078. ```. Which states that checkpointing is being skipped because the performance was worse. So in general, if you aren't seeing checkpoints you likely need to adjust parameters or train for longer.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:332,performance,tune,tune,332,"@helizabeth1103 @lucasbrambrink I'll clear up some confusion real quick here - the updated training script will only output checkpoints if tune performance outperforms existing performance. If you look closely in the log file you can see this line:. ```. I0401 03:09:48.932735 140045983049536 train.py:471] Skipping checkpoint with tune/f1_weighted=0.83932966 < previous best tune/f1_weighted=0.8400078. ```. Which states that checkpointing is being skipped because the performance was worse. So in general, if you aren't seeing checkpoints you likely need to adjust parameters or train for longer.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:376,performance,tune,tune,376,"@helizabeth1103 @lucasbrambrink I'll clear up some confusion real quick here - the updated training script will only output checkpoints if tune performance outperforms existing performance. If you look closely in the log file you can see this line:. ```. I0401 03:09:48.932735 140045983049536 train.py:471] Skipping checkpoint with tune/f1_weighted=0.83932966 < previous best tune/f1_weighted=0.8400078. ```. Which states that checkpointing is being skipped because the performance was worse. So in general, if you aren't seeing checkpoints you likely need to adjust parameters or train for longer.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:470,performance,perform,performance,470,"@helizabeth1103 @lucasbrambrink I'll clear up some confusion real quick here - the updated training script will only output checkpoints if tune performance outperforms existing performance. If you look closely in the log file you can see this line:. ```. I0401 03:09:48.932735 140045983049536 train.py:471] Skipping checkpoint with tune/f1_weighted=0.83932966 < previous best tune/f1_weighted=0.8400078. ```. Which states that checkpointing is being skipped because the performance was worse. So in general, if you aren't seeing checkpoints you likely need to adjust parameters or train for longer.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:124,reliability,checkpoint,checkpoints,124,"@helizabeth1103 @lucasbrambrink I'll clear up some confusion real quick here - the updated training script will only output checkpoints if tune performance outperforms existing performance. If you look closely in the log file you can see this line:. ```. I0401 03:09:48.932735 140045983049536 train.py:471] Skipping checkpoint with tune/f1_weighted=0.83932966 < previous best tune/f1_weighted=0.8400078. ```. Which states that checkpointing is being skipped because the performance was worse. So in general, if you aren't seeing checkpoints you likely need to adjust parameters or train for longer.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:316,reliability,checkpoint,checkpoint,316,"@helizabeth1103 @lucasbrambrink I'll clear up some confusion real quick here - the updated training script will only output checkpoints if tune performance outperforms existing performance. If you look closely in the log file you can see this line:. ```. I0401 03:09:48.932735 140045983049536 train.py:471] Skipping checkpoint with tune/f1_weighted=0.83932966 < previous best tune/f1_weighted=0.8400078. ```. Which states that checkpointing is being skipped because the performance was worse. So in general, if you aren't seeing checkpoints you likely need to adjust parameters or train for longer.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:427,reliability,checkpoint,checkpointing,427,"@helizabeth1103 @lucasbrambrink I'll clear up some confusion real quick here - the updated training script will only output checkpoints if tune performance outperforms existing performance. If you look closely in the log file you can see this line:. ```. I0401 03:09:48.932735 140045983049536 train.py:471] Skipping checkpoint with tune/f1_weighted=0.83932966 < previous best tune/f1_weighted=0.8400078. ```. Which states that checkpointing is being skipped because the performance was worse. So in general, if you aren't seeing checkpoints you likely need to adjust parameters or train for longer.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:529,reliability,checkpoint,checkpoints,529,"@helizabeth1103 @lucasbrambrink I'll clear up some confusion real quick here - the updated training script will only output checkpoints if tune performance outperforms existing performance. If you look closely in the log file you can see this line:. ```. I0401 03:09:48.932735 140045983049536 train.py:471] Skipping checkpoint with tune/f1_weighted=0.83932966 < previous best tune/f1_weighted=0.8400078. ```. Which states that checkpointing is being skipped because the performance was worse. So in general, if you aren't seeing checkpoints you likely need to adjust parameters or train for longer.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:83,safety,updat,updated,83,"@helizabeth1103 @lucasbrambrink I'll clear up some confusion real quick here - the updated training script will only output checkpoints if tune performance outperforms existing performance. If you look closely in the log file you can see this line:. ```. I0401 03:09:48.932735 140045983049536 train.py:471] Skipping checkpoint with tune/f1_weighted=0.83932966 < previous best tune/f1_weighted=0.8400078. ```. Which states that checkpointing is being skipped because the performance was worse. So in general, if you aren't seeing checkpoints you likely need to adjust parameters or train for longer.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:217,safety,log,log,217,"@helizabeth1103 @lucasbrambrink I'll clear up some confusion real quick here - the updated training script will only output checkpoints if tune performance outperforms existing performance. If you look closely in the log file you can see this line:. ```. I0401 03:09:48.932735 140045983049536 train.py:471] Skipping checkpoint with tune/f1_weighted=0.83932966 < previous best tune/f1_weighted=0.8400078. ```. Which states that checkpointing is being skipped because the performance was worse. So in general, if you aren't seeing checkpoints you likely need to adjust parameters or train for longer.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:83,security,updat,updated,83,"@helizabeth1103 @lucasbrambrink I'll clear up some confusion real quick here - the updated training script will only output checkpoints if tune performance outperforms existing performance. If you look closely in the log file you can see this line:. ```. I0401 03:09:48.932735 140045983049536 train.py:471] Skipping checkpoint with tune/f1_weighted=0.83932966 < previous best tune/f1_weighted=0.8400078. ```. Which states that checkpointing is being skipped because the performance was worse. So in general, if you aren't seeing checkpoints you likely need to adjust parameters or train for longer.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:217,security,log,log,217,"@helizabeth1103 @lucasbrambrink I'll clear up some confusion real quick here - the updated training script will only output checkpoints if tune performance outperforms existing performance. If you look closely in the log file you can see this line:. ```. I0401 03:09:48.932735 140045983049536 train.py:471] Skipping checkpoint with tune/f1_weighted=0.83932966 < previous best tune/f1_weighted=0.8400078. ```. Which states that checkpointing is being skipped because the performance was worse. So in general, if you aren't seeing checkpoints you likely need to adjust parameters or train for longer.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:217,testability,log,log,217,"@helizabeth1103 @lucasbrambrink I'll clear up some confusion real quick here - the updated training script will only output checkpoints if tune performance outperforms existing performance. If you look closely in the log file you can see this line:. ```. I0401 03:09:48.932735 140045983049536 train.py:471] Skipping checkpoint with tune/f1_weighted=0.83932966 < previous best tune/f1_weighted=0.8400078. ```. Which states that checkpointing is being skipped because the performance was worse. So in general, if you aren't seeing checkpoints you likely need to adjust parameters or train for longer.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:37,usability,clear,clear,37,"@helizabeth1103 @lucasbrambrink I'll clear up some confusion real quick here - the updated training script will only output checkpoints if tune performance outperforms existing performance. If you look closely in the log file you can see this line:. ```. I0401 03:09:48.932735 140045983049536 train.py:471] Skipping checkpoint with tune/f1_weighted=0.83932966 < previous best tune/f1_weighted=0.8400078. ```. Which states that checkpointing is being skipped because the performance was worse. So in general, if you aren't seeing checkpoints you likely need to adjust parameters or train for longer.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:144,usability,perform,performance,144,"@helizabeth1103 @lucasbrambrink I'll clear up some confusion real quick here - the updated training script will only output checkpoints if tune performance outperforms existing performance. If you look closely in the log file you can see this line:. ```. I0401 03:09:48.932735 140045983049536 train.py:471] Skipping checkpoint with tune/f1_weighted=0.83932966 < previous best tune/f1_weighted=0.8400078. ```. Which states that checkpointing is being skipped because the performance was worse. So in general, if you aren't seeing checkpoints you likely need to adjust parameters or train for longer.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:177,usability,perform,performance,177,"@helizabeth1103 @lucasbrambrink I'll clear up some confusion real quick here - the updated training script will only output checkpoints if tune performance outperforms existing performance. If you look closely in the log file you can see this line:. ```. I0401 03:09:48.932735 140045983049536 train.py:471] Skipping checkpoint with tune/f1_weighted=0.83932966 < previous best tune/f1_weighted=0.8400078. ```. Which states that checkpointing is being skipped because the performance was worse. So in general, if you aren't seeing checkpoints you likely need to adjust parameters or train for longer.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:202,usability,close,closely,202,"@helizabeth1103 @lucasbrambrink I'll clear up some confusion real quick here - the updated training script will only output checkpoints if tune performance outperforms existing performance. If you look closely in the log file you can see this line:. ```. I0401 03:09:48.932735 140045983049536 train.py:471] Skipping checkpoint with tune/f1_weighted=0.83932966 < previous best tune/f1_weighted=0.8400078. ```. Which states that checkpointing is being skipped because the performance was worse. So in general, if you aren't seeing checkpoints you likely need to adjust parameters or train for longer.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:470,usability,perform,performance,470,"@helizabeth1103 @lucasbrambrink I'll clear up some confusion real quick here - the updated training script will only output checkpoints if tune performance outperforms existing performance. If you look closely in the log file you can see this line:. ```. I0401 03:09:48.932735 140045983049536 train.py:471] Skipping checkpoint with tune/f1_weighted=0.83932966 < previous best tune/f1_weighted=0.8400078. ```. Which states that checkpointing is being skipped because the performance was worse. So in general, if you aren't seeing checkpoints you likely need to adjust parameters or train for longer.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:145,availability,sli,slightly,145,"Thanks for clearing that up! I appreciate it. I did use hap.py to compare the customized model to the WGS model and it appears to have performed slightly worse, so I'll keep this in mind for future tests.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:89,energy efficiency,model,model,89,"Thanks for clearing that up! I appreciate it. I did use hap.py to compare the customized model to the WGS model and it appears to have performed slightly worse, so I'll keep this in mind for future tests.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:106,energy efficiency,model,model,106,"Thanks for clearing that up! I appreciate it. I did use hap.py to compare the customized model to the WGS model and it appears to have performed slightly worse, so I'll keep this in mind for future tests.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:135,performance,perform,performed,135,"Thanks for clearing that up! I appreciate it. I did use hap.py to compare the customized model to the WGS model and it appears to have performed slightly worse, so I'll keep this in mind for future tests.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:145,reliability,sli,slightly,145,"Thanks for clearing that up! I appreciate it. I did use hap.py to compare the customized model to the WGS model and it appears to have performed slightly worse, so I'll keep this in mind for future tests.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:198,safety,test,tests,198,"Thanks for clearing that up! I appreciate it. I did use hap.py to compare the customized model to the WGS model and it appears to have performed slightly worse, so I'll keep this in mind for future tests.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:89,security,model,model,89,"Thanks for clearing that up! I appreciate it. I did use hap.py to compare the customized model to the WGS model and it appears to have performed slightly worse, so I'll keep this in mind for future tests.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:106,security,model,model,106,"Thanks for clearing that up! I appreciate it. I did use hap.py to compare the customized model to the WGS model and it appears to have performed slightly worse, so I'll keep this in mind for future tests.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:198,testability,test,tests,198,"Thanks for clearing that up! I appreciate it. I did use hap.py to compare the customized model to the WGS model and it appears to have performed slightly worse, so I'll keep this in mind for future tests.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:11,usability,clear,clearing,11,"Thanks for clearing that up! I appreciate it. I did use hap.py to compare the customized model to the WGS model and it appears to have performed slightly worse, so I'll keep this in mind for future tests.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:78,usability,custom,customized,78,"Thanks for clearing that up! I appreciate it. I did use hap.py to compare the customized model to the WGS model and it appears to have performed slightly worse, so I'll keep this in mind for future tests.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/797:135,usability,perform,performed,135,"Thanks for clearing that up! I appreciate it. I did use hap.py to compare the customized model to the WGS model and it appears to have performed slightly worse, so I'll keep this in mind for future tests.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/797
https://github.com/google/deepvariant/issues/798:189,availability,avail,available-operating-modes,189,"It looks like you are using PacBio data, but not setting the PB `--mode` flag to `pacbio` ([docs](https://docs.nvidia.com/clara/parabricks/4.2.1/documentation/tooldocs/man_deepvariant.html#available-operating-modes)). The default for that flag is `shortread` which will not work with the long reads in PacBio data.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/798
https://github.com/google/deepvariant/issues/798:28,modifiability,Pac,PacBio,28,"It looks like you are using PacBio data, but not setting the PB `--mode` flag to `pacbio` ([docs](https://docs.nvidia.com/clara/parabricks/4.2.1/documentation/tooldocs/man_deepvariant.html#available-operating-modes)). The default for that flag is `shortread` which will not work with the long reads in PacBio data.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/798
https://github.com/google/deepvariant/issues/798:82,modifiability,pac,pacbio,82,"It looks like you are using PacBio data, but not setting the PB `--mode` flag to `pacbio` ([docs](https://docs.nvidia.com/clara/parabricks/4.2.1/documentation/tooldocs/man_deepvariant.html#available-operating-modes)). The default for that flag is `shortread` which will not work with the long reads in PacBio data.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/798
https://github.com/google/deepvariant/issues/798:302,modifiability,Pac,PacBio,302,"It looks like you are using PacBio data, but not setting the PB `--mode` flag to `pacbio` ([docs](https://docs.nvidia.com/clara/parabricks/4.2.1/documentation/tooldocs/man_deepvariant.html#available-operating-modes)). The default for that flag is `shortread` which will not work with the long reads in PacBio data.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/798
https://github.com/google/deepvariant/issues/798:189,reliability,availab,available-operating-modes,189,"It looks like you are using PacBio data, but not setting the PB `--mode` flag to `pacbio` ([docs](https://docs.nvidia.com/clara/parabricks/4.2.1/documentation/tooldocs/man_deepvariant.html#available-operating-modes)). The default for that flag is `shortread` which will not work with the long reads in PacBio data.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/798
https://github.com/google/deepvariant/issues/798:189,safety,avail,available-operating-modes,189,"It looks like you are using PacBio data, but not setting the PB `--mode` flag to `pacbio` ([docs](https://docs.nvidia.com/clara/parabricks/4.2.1/documentation/tooldocs/man_deepvariant.html#available-operating-modes)). The default for that flag is `shortread` which will not work with the long reads in PacBio data.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/798
https://github.com/google/deepvariant/issues/798:189,security,availab,available-operating-modes,189,"It looks like you are using PacBio data, but not setting the PB `--mode` flag to `pacbio` ([docs](https://docs.nvidia.com/clara/parabricks/4.2.1/documentation/tooldocs/man_deepvariant.html#available-operating-modes)). The default for that flag is `shortread` which will not work with the long reads in PacBio data.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/798
https://github.com/google/deepvariant/issues/798:145,usability,document,documentation,145,"It looks like you are using PacBio data, but not setting the PB `--mode` flag to `pacbio` ([docs](https://docs.nvidia.com/clara/parabricks/4.2.1/documentation/tooldocs/man_deepvariant.html#available-operating-modes)). The default for that flag is `shortread` which will not work with the long reads in PacBio data.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/798
https://github.com/google/deepvariant/issues/798:159,usability,tool,tooldocs,159,"It looks like you are using PacBio data, but not setting the PB `--mode` flag to `pacbio` ([docs](https://docs.nvidia.com/clara/parabricks/4.2.1/documentation/tooldocs/man_deepvariant.html#available-operating-modes)). The default for that flag is `shortread` which will not work with the long reads in PacBio data.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/798
https://github.com/google/deepvariant/issues/799:34,energy efficiency,gpu,gpus,34,"Oh nevermind, I think I realize --gpus is a flag for docker, not for deeptrio.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/799
https://github.com/google/deepvariant/issues/799:34,performance,gpu,gpus,34,"Oh nevermind, I think I realize --gpus is a flag for docker, not for deeptrio.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/799
https://github.com/google/deepvariant/issues/800:396,integrability,filter,filters,396,"Hi, thanks for the question. 1. RefCalls are candidates that were determined to match the reference and are therefore not variants. They are still included in the VCF file to show they were considered, since it can be just as important to know with high confidence that a particular variant does _not_ exist as it is to know that it does exist. 2. The `postprocess_variants` step applies quality filters, which can be set via flags. - [--qual_filter](https://github.com/google/deepvariant/blob/r1.6.1/deepvariant/postprocess_variants.py#L98-L102) will remove any variant with `QUAL < qual_filter`. - [--cnn_homref_call_min_gq](https://github.com/google/deepvariant/blob/r1.6.1/deepvariant/postprocess_variants.py#L103-L110) will set `RefCalls` to `./.` instead of `0/0` with `GQ < cnn_homref_call_min_gq`. - [--multi_allelic_qual_filter](https://github.com/google/deepvariant/blob/r1.6.1/deepvariant/postprocess_variants.py#L111-L115) will remove multi-allelic variants with `QUAL < multi_allelic_qual_filter`. The default value for `--qual_filter` is 1.0. If this is too low, you can set this to a higher value using the [--postprocess_variants_extra_arg](https://github.com/google/deepvariant/blob/r1.6.1/scripts/run_deepvariant.py#L221-L229) flag in `run_deepvariant`, e.g. `--postprocess_variants_extra_args=""qual_filter=5.0""`. I hope that helps!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/800
https://github.com/google/deepvariant/issues/800:291,reliability,doe,does,291,"Hi, thanks for the question. 1. RefCalls are candidates that were determined to match the reference and are therefore not variants. They are still included in the VCF file to show they were considered, since it can be just as important to know with high confidence that a particular variant does _not_ exist as it is to know that it does exist. 2. The `postprocess_variants` step applies quality filters, which can be set via flags. - [--qual_filter](https://github.com/google/deepvariant/blob/r1.6.1/deepvariant/postprocess_variants.py#L98-L102) will remove any variant with `QUAL < qual_filter`. - [--cnn_homref_call_min_gq](https://github.com/google/deepvariant/blob/r1.6.1/deepvariant/postprocess_variants.py#L103-L110) will set `RefCalls` to `./.` instead of `0/0` with `GQ < cnn_homref_call_min_gq`. - [--multi_allelic_qual_filter](https://github.com/google/deepvariant/blob/r1.6.1/deepvariant/postprocess_variants.py#L111-L115) will remove multi-allelic variants with `QUAL < multi_allelic_qual_filter`. The default value for `--qual_filter` is 1.0. If this is too low, you can set this to a higher value using the [--postprocess_variants_extra_arg](https://github.com/google/deepvariant/blob/r1.6.1/scripts/run_deepvariant.py#L221-L229) flag in `run_deepvariant`, e.g. `--postprocess_variants_extra_args=""qual_filter=5.0""`. I hope that helps!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/800
https://github.com/google/deepvariant/issues/800:333,reliability,doe,does,333,"Hi, thanks for the question. 1. RefCalls are candidates that were determined to match the reference and are therefore not variants. They are still included in the VCF file to show they were considered, since it can be just as important to know with high confidence that a particular variant does _not_ exist as it is to know that it does exist. 2. The `postprocess_variants` step applies quality filters, which can be set via flags. - [--qual_filter](https://github.com/google/deepvariant/blob/r1.6.1/deepvariant/postprocess_variants.py#L98-L102) will remove any variant with `QUAL < qual_filter`. - [--cnn_homref_call_min_gq](https://github.com/google/deepvariant/blob/r1.6.1/deepvariant/postprocess_variants.py#L103-L110) will set `RefCalls` to `./.` instead of `0/0` with `GQ < cnn_homref_call_min_gq`. - [--multi_allelic_qual_filter](https://github.com/google/deepvariant/blob/r1.6.1/deepvariant/postprocess_variants.py#L111-L115) will remove multi-allelic variants with `QUAL < multi_allelic_qual_filter`. The default value for `--qual_filter` is 1.0. If this is too low, you can set this to a higher value using the [--postprocess_variants_extra_arg](https://github.com/google/deepvariant/blob/r1.6.1/scripts/run_deepvariant.py#L221-L229) flag in `run_deepvariant`, e.g. `--postprocess_variants_extra_args=""qual_filter=5.0""`. I hope that helps!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/800
https://github.com/google/deepvariant/issues/800:1344,usability,help,helps,1344,"Hi, thanks for the question. 1. RefCalls are candidates that were determined to match the reference and are therefore not variants. They are still included in the VCF file to show they were considered, since it can be just as important to know with high confidence that a particular variant does _not_ exist as it is to know that it does exist. 2. The `postprocess_variants` step applies quality filters, which can be set via flags. - [--qual_filter](https://github.com/google/deepvariant/blob/r1.6.1/deepvariant/postprocess_variants.py#L98-L102) will remove any variant with `QUAL < qual_filter`. - [--cnn_homref_call_min_gq](https://github.com/google/deepvariant/blob/r1.6.1/deepvariant/postprocess_variants.py#L103-L110) will set `RefCalls` to `./.` instead of `0/0` with `GQ < cnn_homref_call_min_gq`. - [--multi_allelic_qual_filter](https://github.com/google/deepvariant/blob/r1.6.1/deepvariant/postprocess_variants.py#L111-L115) will remove multi-allelic variants with `QUAL < multi_allelic_qual_filter`. The default value for `--qual_filter` is 1.0. If this is too low, you can set this to a higher value using the [--postprocess_variants_extra_arg](https://github.com/google/deepvariant/blob/r1.6.1/scripts/run_deepvariant.py#L221-L229) flag in `run_deepvariant`, e.g. `--postprocess_variants_extra_args=""qual_filter=5.0""`. I hope that helps!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/800
https://github.com/google/deepvariant/issues/801:466,deployability,api,api,466,"Thank you for the question! . Take a look at how we instantiate InceptionV3 [keras_modeling.py](https://github.com/google/deepvariant/blob/r1.6.1/deepvariant/keras_modeling.py#L275-L364). The [input_shape](https://github.com/google/deepvariant/blob/r1.6.1/deepvariant/train_inceptionv3.py#L278-L280) is inferred from the examples. InceptionV3 can handle any number of channels provided you are not using the `imagenet` classifier. . [documentation](https://keras.io/api/applications/inceptionv3/) mentions 3 channels if you are using the `imagenet` preset, and load the weights pre-trained on ImageNet. This is not the case if you set `weights=None`.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/801
https://github.com/google/deepvariant/issues/801:561,energy efficiency,load,load,561,"Thank you for the question! . Take a look at how we instantiate InceptionV3 [keras_modeling.py](https://github.com/google/deepvariant/blob/r1.6.1/deepvariant/keras_modeling.py#L275-L364). The [input_shape](https://github.com/google/deepvariant/blob/r1.6.1/deepvariant/train_inceptionv3.py#L278-L280) is inferred from the examples. InceptionV3 can handle any number of channels provided you are not using the `imagenet` classifier. . [documentation](https://keras.io/api/applications/inceptionv3/) mentions 3 channels if you are using the `imagenet` preset, and load the weights pre-trained on ImageNet. This is not the case if you set `weights=None`.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/801
https://github.com/google/deepvariant/issues/801:466,integrability,api,api,466,"Thank you for the question! . Take a look at how we instantiate InceptionV3 [keras_modeling.py](https://github.com/google/deepvariant/blob/r1.6.1/deepvariant/keras_modeling.py#L275-L364). The [input_shape](https://github.com/google/deepvariant/blob/r1.6.1/deepvariant/train_inceptionv3.py#L278-L280) is inferred from the examples. InceptionV3 can handle any number of channels provided you are not using the `imagenet` classifier. . [documentation](https://keras.io/api/applications/inceptionv3/) mentions 3 channels if you are using the `imagenet` preset, and load the weights pre-trained on ImageNet. This is not the case if you set `weights=None`.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/801
https://github.com/google/deepvariant/issues/801:466,interoperability,api,api,466,"Thank you for the question! . Take a look at how we instantiate InceptionV3 [keras_modeling.py](https://github.com/google/deepvariant/blob/r1.6.1/deepvariant/keras_modeling.py#L275-L364). The [input_shape](https://github.com/google/deepvariant/blob/r1.6.1/deepvariant/train_inceptionv3.py#L278-L280) is inferred from the examples. InceptionV3 can handle any number of channels provided you are not using the `imagenet` classifier. . [documentation](https://keras.io/api/applications/inceptionv3/) mentions 3 channels if you are using the `imagenet` preset, and load the weights pre-trained on ImageNet. This is not the case if you set `weights=None`.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/801
https://github.com/google/deepvariant/issues/801:561,performance,load,load,561,"Thank you for the question! . Take a look at how we instantiate InceptionV3 [keras_modeling.py](https://github.com/google/deepvariant/blob/r1.6.1/deepvariant/keras_modeling.py#L275-L364). The [input_shape](https://github.com/google/deepvariant/blob/r1.6.1/deepvariant/train_inceptionv3.py#L278-L280) is inferred from the examples. InceptionV3 can handle any number of channels provided you are not using the `imagenet` classifier. . [documentation](https://keras.io/api/applications/inceptionv3/) mentions 3 channels if you are using the `imagenet` preset, and load the weights pre-trained on ImageNet. This is not the case if you set `weights=None`.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/801
https://github.com/google/deepvariant/issues/801:434,usability,document,documentation,434,"Thank you for the question! . Take a look at how we instantiate InceptionV3 [keras_modeling.py](https://github.com/google/deepvariant/blob/r1.6.1/deepvariant/keras_modeling.py#L275-L364). The [input_shape](https://github.com/google/deepvariant/blob/r1.6.1/deepvariant/train_inceptionv3.py#L278-L280) is inferred from the examples. InceptionV3 can handle any number of channels provided you are not using the `imagenet` classifier. . [documentation](https://keras.io/api/applications/inceptionv3/) mentions 3 channels if you are using the `imagenet` preset, and load the weights pre-trained on ImageNet. This is not the case if you set `weights=None`.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/801
https://github.com/google/deepvariant/issues/801:126,energy efficiency,model,model,126,"@lucasbrambrink Thank you for your reply! Now If I want to use the pretrained Inceptionv3 of deepvariant, where can i get the model?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/801
https://github.com/google/deepvariant/issues/801:126,security,model,model,126,"@lucasbrambrink Thank you for your reply! Now If I want to use the pretrained Inceptionv3 of deepvariant, where can i get the model?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/801
https://github.com/google/deepvariant/issues/801:23,availability,checkpoint,checkpoints,23,"Sure! You can find the checkpoints for each sequencing technology at `gs://deepvariant/models/DeepVariant/1.6.1/checkpoints/`. For example, the model for Illumina data can be found at `gs://deepvariant/models/DeepVariant/1.6.1/checkpoints/wgs/deepvariant.wgs.ckpt`. These models are mounted in our [Dockerfile](https://github.com/google/deepvariant/blob/r1.6.1/Dockerfile#L156-L200). Take a look at our [custom training case study](https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-training-case-study.md#advanced-case-study-train-a-customized-snp-and-small-indel-variant-caller-for-bgiseq-500-data) if you want to learn more.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/801
https://github.com/google/deepvariant/issues/801:112,availability,checkpoint,checkpoints,112,"Sure! You can find the checkpoints for each sequencing technology at `gs://deepvariant/models/DeepVariant/1.6.1/checkpoints/`. For example, the model for Illumina data can be found at `gs://deepvariant/models/DeepVariant/1.6.1/checkpoints/wgs/deepvariant.wgs.ckpt`. These models are mounted in our [Dockerfile](https://github.com/google/deepvariant/blob/r1.6.1/Dockerfile#L156-L200). Take a look at our [custom training case study](https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-training-case-study.md#advanced-case-study-train-a-customized-snp-and-small-indel-variant-caller-for-bgiseq-500-data) if you want to learn more.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/801
https://github.com/google/deepvariant/issues/801:227,availability,checkpoint,checkpoints,227,"Sure! You can find the checkpoints for each sequencing technology at `gs://deepvariant/models/DeepVariant/1.6.1/checkpoints/`. For example, the model for Illumina data can be found at `gs://deepvariant/models/DeepVariant/1.6.1/checkpoints/wgs/deepvariant.wgs.ckpt`. These models are mounted in our [Dockerfile](https://github.com/google/deepvariant/blob/r1.6.1/Dockerfile#L156-L200). Take a look at our [custom training case study](https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-training-case-study.md#advanced-case-study-train-a-customized-snp-and-small-indel-variant-caller-for-bgiseq-500-data) if you want to learn more.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/801
https://github.com/google/deepvariant/issues/801:87,energy efficiency,model,models,87,"Sure! You can find the checkpoints for each sequencing technology at `gs://deepvariant/models/DeepVariant/1.6.1/checkpoints/`. For example, the model for Illumina data can be found at `gs://deepvariant/models/DeepVariant/1.6.1/checkpoints/wgs/deepvariant.wgs.ckpt`. These models are mounted in our [Dockerfile](https://github.com/google/deepvariant/blob/r1.6.1/Dockerfile#L156-L200). Take a look at our [custom training case study](https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-training-case-study.md#advanced-case-study-train-a-customized-snp-and-small-indel-variant-caller-for-bgiseq-500-data) if you want to learn more.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/801
https://github.com/google/deepvariant/issues/801:144,energy efficiency,model,model,144,"Sure! You can find the checkpoints for each sequencing technology at `gs://deepvariant/models/DeepVariant/1.6.1/checkpoints/`. For example, the model for Illumina data can be found at `gs://deepvariant/models/DeepVariant/1.6.1/checkpoints/wgs/deepvariant.wgs.ckpt`. These models are mounted in our [Dockerfile](https://github.com/google/deepvariant/blob/r1.6.1/Dockerfile#L156-L200). Take a look at our [custom training case study](https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-training-case-study.md#advanced-case-study-train-a-customized-snp-and-small-indel-variant-caller-for-bgiseq-500-data) if you want to learn more.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/801
https://github.com/google/deepvariant/issues/801:202,energy efficiency,model,models,202,"Sure! You can find the checkpoints for each sequencing technology at `gs://deepvariant/models/DeepVariant/1.6.1/checkpoints/`. For example, the model for Illumina data can be found at `gs://deepvariant/models/DeepVariant/1.6.1/checkpoints/wgs/deepvariant.wgs.ckpt`. These models are mounted in our [Dockerfile](https://github.com/google/deepvariant/blob/r1.6.1/Dockerfile#L156-L200). Take a look at our [custom training case study](https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-training-case-study.md#advanced-case-study-train-a-customized-snp-and-small-indel-variant-caller-for-bgiseq-500-data) if you want to learn more.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/801
https://github.com/google/deepvariant/issues/801:272,energy efficiency,model,models,272,"Sure! You can find the checkpoints for each sequencing technology at `gs://deepvariant/models/DeepVariant/1.6.1/checkpoints/`. For example, the model for Illumina data can be found at `gs://deepvariant/models/DeepVariant/1.6.1/checkpoints/wgs/deepvariant.wgs.ckpt`. These models are mounted in our [Dockerfile](https://github.com/google/deepvariant/blob/r1.6.1/Dockerfile#L156-L200). Take a look at our [custom training case study](https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-training-case-study.md#advanced-case-study-train-a-customized-snp-and-small-indel-variant-caller-for-bgiseq-500-data) if you want to learn more.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/801
https://github.com/google/deepvariant/issues/801:23,reliability,checkpoint,checkpoints,23,"Sure! You can find the checkpoints for each sequencing technology at `gs://deepvariant/models/DeepVariant/1.6.1/checkpoints/`. For example, the model for Illumina data can be found at `gs://deepvariant/models/DeepVariant/1.6.1/checkpoints/wgs/deepvariant.wgs.ckpt`. These models are mounted in our [Dockerfile](https://github.com/google/deepvariant/blob/r1.6.1/Dockerfile#L156-L200). Take a look at our [custom training case study](https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-training-case-study.md#advanced-case-study-train-a-customized-snp-and-small-indel-variant-caller-for-bgiseq-500-data) if you want to learn more.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/801
https://github.com/google/deepvariant/issues/801:112,reliability,checkpoint,checkpoints,112,"Sure! You can find the checkpoints for each sequencing technology at `gs://deepvariant/models/DeepVariant/1.6.1/checkpoints/`. For example, the model for Illumina data can be found at `gs://deepvariant/models/DeepVariant/1.6.1/checkpoints/wgs/deepvariant.wgs.ckpt`. These models are mounted in our [Dockerfile](https://github.com/google/deepvariant/blob/r1.6.1/Dockerfile#L156-L200). Take a look at our [custom training case study](https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-training-case-study.md#advanced-case-study-train-a-customized-snp-and-small-indel-variant-caller-for-bgiseq-500-data) if you want to learn more.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/801
https://github.com/google/deepvariant/issues/801:227,reliability,checkpoint,checkpoints,227,"Sure! You can find the checkpoints for each sequencing technology at `gs://deepvariant/models/DeepVariant/1.6.1/checkpoints/`. For example, the model for Illumina data can be found at `gs://deepvariant/models/DeepVariant/1.6.1/checkpoints/wgs/deepvariant.wgs.ckpt`. These models are mounted in our [Dockerfile](https://github.com/google/deepvariant/blob/r1.6.1/Dockerfile#L156-L200). Take a look at our [custom training case study](https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-training-case-study.md#advanced-case-study-train-a-customized-snp-and-small-indel-variant-caller-for-bgiseq-500-data) if you want to learn more.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/801
https://github.com/google/deepvariant/issues/801:87,security,model,models,87,"Sure! You can find the checkpoints for each sequencing technology at `gs://deepvariant/models/DeepVariant/1.6.1/checkpoints/`. For example, the model for Illumina data can be found at `gs://deepvariant/models/DeepVariant/1.6.1/checkpoints/wgs/deepvariant.wgs.ckpt`. These models are mounted in our [Dockerfile](https://github.com/google/deepvariant/blob/r1.6.1/Dockerfile#L156-L200). Take a look at our [custom training case study](https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-training-case-study.md#advanced-case-study-train-a-customized-snp-and-small-indel-variant-caller-for-bgiseq-500-data) if you want to learn more.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/801
https://github.com/google/deepvariant/issues/801:144,security,model,model,144,"Sure! You can find the checkpoints for each sequencing technology at `gs://deepvariant/models/DeepVariant/1.6.1/checkpoints/`. For example, the model for Illumina data can be found at `gs://deepvariant/models/DeepVariant/1.6.1/checkpoints/wgs/deepvariant.wgs.ckpt`. These models are mounted in our [Dockerfile](https://github.com/google/deepvariant/blob/r1.6.1/Dockerfile#L156-L200). Take a look at our [custom training case study](https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-training-case-study.md#advanced-case-study-train-a-customized-snp-and-small-indel-variant-caller-for-bgiseq-500-data) if you want to learn more.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/801
https://github.com/google/deepvariant/issues/801:202,security,model,models,202,"Sure! You can find the checkpoints for each sequencing technology at `gs://deepvariant/models/DeepVariant/1.6.1/checkpoints/`. For example, the model for Illumina data can be found at `gs://deepvariant/models/DeepVariant/1.6.1/checkpoints/wgs/deepvariant.wgs.ckpt`. These models are mounted in our [Dockerfile](https://github.com/google/deepvariant/blob/r1.6.1/Dockerfile#L156-L200). Take a look at our [custom training case study](https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-training-case-study.md#advanced-case-study-train-a-customized-snp-and-small-indel-variant-caller-for-bgiseq-500-data) if you want to learn more.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/801
https://github.com/google/deepvariant/issues/801:272,security,model,models,272,"Sure! You can find the checkpoints for each sequencing technology at `gs://deepvariant/models/DeepVariant/1.6.1/checkpoints/`. For example, the model for Illumina data can be found at `gs://deepvariant/models/DeepVariant/1.6.1/checkpoints/wgs/deepvariant.wgs.ckpt`. These models are mounted in our [Dockerfile](https://github.com/google/deepvariant/blob/r1.6.1/Dockerfile#L156-L200). Take a look at our [custom training case study](https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-training-case-study.md#advanced-case-study-train-a-customized-snp-and-small-indel-variant-caller-for-bgiseq-500-data) if you want to learn more.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/801
https://github.com/google/deepvariant/issues/801:404,usability,custom,custom,404,"Sure! You can find the checkpoints for each sequencing technology at `gs://deepvariant/models/DeepVariant/1.6.1/checkpoints/`. For example, the model for Illumina data can be found at `gs://deepvariant/models/DeepVariant/1.6.1/checkpoints/wgs/deepvariant.wgs.ckpt`. These models are mounted in our [Dockerfile](https://github.com/google/deepvariant/blob/r1.6.1/Dockerfile#L156-L200). Take a look at our [custom training case study](https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-training-case-study.md#advanced-case-study-train-a-customized-snp-and-small-indel-variant-caller-for-bgiseq-500-data) if you want to learn more.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/801
https://github.com/google/deepvariant/issues/801:550,usability,custom,customized-snp-and-small-indel-variant-caller-for-bgiseq-,550,"Sure! You can find the checkpoints for each sequencing technology at `gs://deepvariant/models/DeepVariant/1.6.1/checkpoints/`. For example, the model for Illumina data can be found at `gs://deepvariant/models/DeepVariant/1.6.1/checkpoints/wgs/deepvariant.wgs.ckpt`. These models are mounted in our [Dockerfile](https://github.com/google/deepvariant/blob/r1.6.1/Dockerfile#L156-L200). Take a look at our [custom training case study](https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-training-case-study.md#advanced-case-study-train-a-customized-snp-and-small-indel-variant-caller-for-bgiseq-500-data) if you want to learn more.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/801
https://github.com/google/deepvariant/issues/801:632,usability,learn,learn,632,"Sure! You can find the checkpoints for each sequencing technology at `gs://deepvariant/models/DeepVariant/1.6.1/checkpoints/`. For example, the model for Illumina data can be found at `gs://deepvariant/models/DeepVariant/1.6.1/checkpoints/wgs/deepvariant.wgs.ckpt`. These models are mounted in our [Dockerfile](https://github.com/google/deepvariant/blob/r1.6.1/Dockerfile#L156-L200). Take a look at our [custom training case study](https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-training-case-study.md#advanced-case-study-train-a-customized-snp-and-small-indel-variant-caller-for-bgiseq-500-data) if you want to learn more.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/801
https://github.com/google/deepvariant/issues/802:25,deployability,log,logs,25,"I'm seeing an OOM in the logs:. ```. OP_REQUIRES failed at conv_ops.cc:698 : RESOURCE_EXHAUSTED: OOM when allocating tensor with shape[16384,32,37,110]. ```. It also shows your training params:. ```. Training Examples: 8264746. Batch Size: 16384. Epochs: 1. Steps per epoch: 504. Steps per tune: 1500000. Num train steps: 504. ```. It seems that the `--config.batch_size=512` is not being picked up. It could be related to setting `num_epochs=0`, try changing that to the original 10. If that doesn't work, you could edit the batch_size in `dv_config.py` directly. . Let me know if that helps!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/802
https://github.com/google/deepvariant/issues/802:49,deployability,fail,failed,49,"I'm seeing an OOM in the logs:. ```. OP_REQUIRES failed at conv_ops.cc:698 : RESOURCE_EXHAUSTED: OOM when allocating tensor with shape[16384,32,37,110]. ```. It also shows your training params:. ```. Training Examples: 8264746. Batch Size: 16384. Epochs: 1. Steps per epoch: 504. Steps per tune: 1500000. Num train steps: 504. ```. It seems that the `--config.batch_size=512` is not being picked up. It could be related to setting `num_epochs=0`, try changing that to the original 10. If that doesn't work, you could edit the batch_size in `dv_config.py` directly. . Let me know if that helps!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/802
https://github.com/google/deepvariant/issues/802:106,energy efficiency,alloc,allocating,106,"I'm seeing an OOM in the logs:. ```. OP_REQUIRES failed at conv_ops.cc:698 : RESOURCE_EXHAUSTED: OOM when allocating tensor with shape[16384,32,37,110]. ```. It also shows your training params:. ```. Training Examples: 8264746. Batch Size: 16384. Epochs: 1. Steps per epoch: 504. Steps per tune: 1500000. Num train steps: 504. ```. It seems that the `--config.batch_size=512` is not being picked up. It could be related to setting `num_epochs=0`, try changing that to the original 10. If that doesn't work, you could edit the batch_size in `dv_config.py` directly. . Let me know if that helps!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/802
https://github.com/google/deepvariant/issues/802:228,integrability,Batch,Batch,228,"I'm seeing an OOM in the logs:. ```. OP_REQUIRES failed at conv_ops.cc:698 : RESOURCE_EXHAUSTED: OOM when allocating tensor with shape[16384,32,37,110]. ```. It also shows your training params:. ```. Training Examples: 8264746. Batch Size: 16384. Epochs: 1. Steps per epoch: 504. Steps per tune: 1500000. Num train steps: 504. ```. It seems that the `--config.batch_size=512` is not being picked up. It could be related to setting `num_epochs=0`, try changing that to the original 10. If that doesn't work, you could edit the batch_size in `dv_config.py` directly. . Let me know if that helps!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/802
https://github.com/google/deepvariant/issues/802:228,performance,Batch,Batch,228,"I'm seeing an OOM in the logs:. ```. OP_REQUIRES failed at conv_ops.cc:698 : RESOURCE_EXHAUSTED: OOM when allocating tensor with shape[16384,32,37,110]. ```. It also shows your training params:. ```. Training Examples: 8264746. Batch Size: 16384. Epochs: 1. Steps per epoch: 504. Steps per tune: 1500000. Num train steps: 504. ```. It seems that the `--config.batch_size=512` is not being picked up. It could be related to setting `num_epochs=0`, try changing that to the original 10. If that doesn't work, you could edit the batch_size in `dv_config.py` directly. . Let me know if that helps!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/802
https://github.com/google/deepvariant/issues/802:290,performance,tune,tune,290,"I'm seeing an OOM in the logs:. ```. OP_REQUIRES failed at conv_ops.cc:698 : RESOURCE_EXHAUSTED: OOM when allocating tensor with shape[16384,32,37,110]. ```. It also shows your training params:. ```. Training Examples: 8264746. Batch Size: 16384. Epochs: 1. Steps per epoch: 504. Steps per tune: 1500000. Num train steps: 504. ```. It seems that the `--config.batch_size=512` is not being picked up. It could be related to setting `num_epochs=0`, try changing that to the original 10. If that doesn't work, you could edit the batch_size in `dv_config.py` directly. . Let me know if that helps!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/802
https://github.com/google/deepvariant/issues/802:49,reliability,fail,failed,49,"I'm seeing an OOM in the logs:. ```. OP_REQUIRES failed at conv_ops.cc:698 : RESOURCE_EXHAUSTED: OOM when allocating tensor with shape[16384,32,37,110]. ```. It also shows your training params:. ```. Training Examples: 8264746. Batch Size: 16384. Epochs: 1. Steps per epoch: 504. Steps per tune: 1500000. Num train steps: 504. ```. It seems that the `--config.batch_size=512` is not being picked up. It could be related to setting `num_epochs=0`, try changing that to the original 10. If that doesn't work, you could edit the batch_size in `dv_config.py` directly. . Let me know if that helps!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/802
https://github.com/google/deepvariant/issues/802:493,reliability,doe,doesn,493,"I'm seeing an OOM in the logs:. ```. OP_REQUIRES failed at conv_ops.cc:698 : RESOURCE_EXHAUSTED: OOM when allocating tensor with shape[16384,32,37,110]. ```. It also shows your training params:. ```. Training Examples: 8264746. Batch Size: 16384. Epochs: 1. Steps per epoch: 504. Steps per tune: 1500000. Num train steps: 504. ```. It seems that the `--config.batch_size=512` is not being picked up. It could be related to setting `num_epochs=0`, try changing that to the original 10. If that doesn't work, you could edit the batch_size in `dv_config.py` directly. . Let me know if that helps!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/802
https://github.com/google/deepvariant/issues/802:25,safety,log,logs,25,"I'm seeing an OOM in the logs:. ```. OP_REQUIRES failed at conv_ops.cc:698 : RESOURCE_EXHAUSTED: OOM when allocating tensor with shape[16384,32,37,110]. ```. It also shows your training params:. ```. Training Examples: 8264746. Batch Size: 16384. Epochs: 1. Steps per epoch: 504. Steps per tune: 1500000. Num train steps: 504. ```. It seems that the `--config.batch_size=512` is not being picked up. It could be related to setting `num_epochs=0`, try changing that to the original 10. If that doesn't work, you could edit the batch_size in `dv_config.py` directly. . Let me know if that helps!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/802
https://github.com/google/deepvariant/issues/802:25,security,log,logs,25,"I'm seeing an OOM in the logs:. ```. OP_REQUIRES failed at conv_ops.cc:698 : RESOURCE_EXHAUSTED: OOM when allocating tensor with shape[16384,32,37,110]. ```. It also shows your training params:. ```. Training Examples: 8264746. Batch Size: 16384. Epochs: 1. Steps per epoch: 504. Steps per tune: 1500000. Num train steps: 504. ```. It seems that the `--config.batch_size=512` is not being picked up. It could be related to setting `num_epochs=0`, try changing that to the original 10. If that doesn't work, you could edit the batch_size in `dv_config.py` directly. . Let me know if that helps!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/802
https://github.com/google/deepvariant/issues/802:25,testability,log,logs,25,"I'm seeing an OOM in the logs:. ```. OP_REQUIRES failed at conv_ops.cc:698 : RESOURCE_EXHAUSTED: OOM when allocating tensor with shape[16384,32,37,110]. ```. It also shows your training params:. ```. Training Examples: 8264746. Batch Size: 16384. Epochs: 1. Steps per epoch: 504. Steps per tune: 1500000. Num train steps: 504. ```. It seems that the `--config.batch_size=512` is not being picked up. It could be related to setting `num_epochs=0`, try changing that to the original 10. If that doesn't work, you could edit the batch_size in `dv_config.py` directly. . Let me know if that helps!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/802
https://github.com/google/deepvariant/issues/802:587,usability,help,helps,587,"I'm seeing an OOM in the logs:. ```. OP_REQUIRES failed at conv_ops.cc:698 : RESOURCE_EXHAUSTED: OOM when allocating tensor with shape[16384,32,37,110]. ```. It also shows your training params:. ```. Training Examples: 8264746. Batch Size: 16384. Epochs: 1. Steps per epoch: 504. Steps per tune: 1500000. Num train steps: 504. ```. It seems that the `--config.batch_size=512` is not being picked up. It could be related to setting `num_epochs=0`, try changing that to the original 10. If that doesn't work, you could edit the batch_size in `dv_config.py` directly. . Let me know if that helps!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/802
https://github.com/google/deepvariant/issues/802:194,availability,error,error,194,"Hi @lucasbrambrink,. I actually tried with different batch_size (32 and 512) but the batch_size takes longer so I switched to 512. I also tried with epoch=10 but still have encountered the same error. I just updated my error log file with the error ```No checkpoint found.```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/802
https://github.com/google/deepvariant/issues/802:219,availability,error,error,219,"Hi @lucasbrambrink,. I actually tried with different batch_size (32 and 512) but the batch_size takes longer so I switched to 512. I also tried with epoch=10 but still have encountered the same error. I just updated my error log file with the error ```No checkpoint found.```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/802
https://github.com/google/deepvariant/issues/802:243,availability,error,error,243,"Hi @lucasbrambrink,. I actually tried with different batch_size (32 and 512) but the batch_size takes longer so I switched to 512. I also tried with epoch=10 but still have encountered the same error. I just updated my error log file with the error ```No checkpoint found.```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/802
https://github.com/google/deepvariant/issues/802:255,availability,checkpoint,checkpoint,255,"Hi @lucasbrambrink,. I actually tried with different batch_size (32 and 512) but the batch_size takes longer so I switched to 512. I also tried with epoch=10 but still have encountered the same error. I just updated my error log file with the error ```No checkpoint found.```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/802
https://github.com/google/deepvariant/issues/802:208,deployability,updat,updated,208,"Hi @lucasbrambrink,. I actually tried with different batch_size (32 and 512) but the batch_size takes longer so I switched to 512. I also tried with epoch=10 but still have encountered the same error. I just updated my error log file with the error ```No checkpoint found.```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/802
https://github.com/google/deepvariant/issues/802:225,deployability,log,log,225,"Hi @lucasbrambrink,. I actually tried with different batch_size (32 and 512) but the batch_size takes longer so I switched to 512. I also tried with epoch=10 but still have encountered the same error. I just updated my error log file with the error ```No checkpoint found.```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/802
https://github.com/google/deepvariant/issues/802:194,performance,error,error,194,"Hi @lucasbrambrink,. I actually tried with different batch_size (32 and 512) but the batch_size takes longer so I switched to 512. I also tried with epoch=10 but still have encountered the same error. I just updated my error log file with the error ```No checkpoint found.```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/802
https://github.com/google/deepvariant/issues/802:219,performance,error,error,219,"Hi @lucasbrambrink,. I actually tried with different batch_size (32 and 512) but the batch_size takes longer so I switched to 512. I also tried with epoch=10 but still have encountered the same error. I just updated my error log file with the error ```No checkpoint found.```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/802
https://github.com/google/deepvariant/issues/802:243,performance,error,error,243,"Hi @lucasbrambrink,. I actually tried with different batch_size (32 and 512) but the batch_size takes longer so I switched to 512. I also tried with epoch=10 but still have encountered the same error. I just updated my error log file with the error ```No checkpoint found.```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/802
https://github.com/google/deepvariant/issues/802:255,reliability,checkpoint,checkpoint,255,"Hi @lucasbrambrink,. I actually tried with different batch_size (32 and 512) but the batch_size takes longer so I switched to 512. I also tried with epoch=10 but still have encountered the same error. I just updated my error log file with the error ```No checkpoint found.```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/802
https://github.com/google/deepvariant/issues/802:194,safety,error,error,194,"Hi @lucasbrambrink,. I actually tried with different batch_size (32 and 512) but the batch_size takes longer so I switched to 512. I also tried with epoch=10 but still have encountered the same error. I just updated my error log file with the error ```No checkpoint found.```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/802
https://github.com/google/deepvariant/issues/802:208,safety,updat,updated,208,"Hi @lucasbrambrink,. I actually tried with different batch_size (32 and 512) but the batch_size takes longer so I switched to 512. I also tried with epoch=10 but still have encountered the same error. I just updated my error log file with the error ```No checkpoint found.```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/802
https://github.com/google/deepvariant/issues/802:219,safety,error,error,219,"Hi @lucasbrambrink,. I actually tried with different batch_size (32 and 512) but the batch_size takes longer so I switched to 512. I also tried with epoch=10 but still have encountered the same error. I just updated my error log file with the error ```No checkpoint found.```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/802
https://github.com/google/deepvariant/issues/802:225,safety,log,log,225,"Hi @lucasbrambrink,. I actually tried with different batch_size (32 and 512) but the batch_size takes longer so I switched to 512. I also tried with epoch=10 but still have encountered the same error. I just updated my error log file with the error ```No checkpoint found.```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/802
https://github.com/google/deepvariant/issues/802:243,safety,error,error,243,"Hi @lucasbrambrink,. I actually tried with different batch_size (32 and 512) but the batch_size takes longer so I switched to 512. I also tried with epoch=10 but still have encountered the same error. I just updated my error log file with the error ```No checkpoint found.```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/802
https://github.com/google/deepvariant/issues/802:208,security,updat,updated,208,"Hi @lucasbrambrink,. I actually tried with different batch_size (32 and 512) but the batch_size takes longer so I switched to 512. I also tried with epoch=10 but still have encountered the same error. I just updated my error log file with the error ```No checkpoint found.```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/802
https://github.com/google/deepvariant/issues/802:225,security,log,log,225,"Hi @lucasbrambrink,. I actually tried with different batch_size (32 and 512) but the batch_size takes longer so I switched to 512. I also tried with epoch=10 but still have encountered the same error. I just updated my error log file with the error ```No checkpoint found.```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/802
https://github.com/google/deepvariant/issues/802:225,testability,log,log,225,"Hi @lucasbrambrink,. I actually tried with different batch_size (32 and 512) but the batch_size takes longer so I switched to 512. I also tried with epoch=10 but still have encountered the same error. I just updated my error log file with the error ```No checkpoint found.```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/802
https://github.com/google/deepvariant/issues/802:194,usability,error,error,194,"Hi @lucasbrambrink,. I actually tried with different batch_size (32 and 512) but the batch_size takes longer so I switched to 512. I also tried with epoch=10 but still have encountered the same error. I just updated my error log file with the error ```No checkpoint found.```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/802
https://github.com/google/deepvariant/issues/802:219,usability,error,error,219,"Hi @lucasbrambrink,. I actually tried with different batch_size (32 and 512) but the batch_size takes longer so I switched to 512. I also tried with epoch=10 but still have encountered the same error. I just updated my error log file with the error ```No checkpoint found.```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/802
https://github.com/google/deepvariant/issues/802:243,usability,error,error,243,"Hi @lucasbrambrink,. I actually tried with different batch_size (32 and 512) but the batch_size takes longer so I switched to 512. I also tried with epoch=10 but still have encountered the same error. I just updated my error log file with the error ```No checkpoint found.```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/802
https://github.com/google/deepvariant/issues/802:184,availability,checkpoint,checkpoint,184,"@sophienguyen01 can you try to run this again without using `--debug=true`? This runs tensorflow in eager mode which will be very inefficient. The other issue is that you don't have a checkpoint file because you didn't train long enough - and no checkpoint outperformed the existing performance on your tune dataset. Try re-running with `--debug=false` and `--config.num_epochs=10` and see where that gets you. If you get an OOM error with batch_size=512, reduce it and try again. If training produces a better model, it will be output in the `experiment_dir`.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/802
https://github.com/google/deepvariant/issues/802:246,availability,checkpoint,checkpoint,246,"@sophienguyen01 can you try to run this again without using `--debug=true`? This runs tensorflow in eager mode which will be very inefficient. The other issue is that you don't have a checkpoint file because you didn't train long enough - and no checkpoint outperformed the existing performance on your tune dataset. Try re-running with `--debug=false` and `--config.num_epochs=10` and see where that gets you. If you get an OOM error with batch_size=512, reduce it and try again. If training produces a better model, it will be output in the `experiment_dir`.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/802
https://github.com/google/deepvariant/issues/802:429,availability,error,error,429,"@sophienguyen01 can you try to run this again without using `--debug=true`? This runs tensorflow in eager mode which will be very inefficient. The other issue is that you don't have a checkpoint file because you didn't train long enough - and no checkpoint outperformed the existing performance on your tune dataset. Try re-running with `--debug=false` and `--config.num_epochs=10` and see where that gets you. If you get an OOM error with batch_size=512, reduce it and try again. If training produces a better model, it will be output in the `experiment_dir`.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/802
https://github.com/google/deepvariant/issues/802:456,energy efficiency,reduc,reduce,456,"@sophienguyen01 can you try to run this again without using `--debug=true`? This runs tensorflow in eager mode which will be very inefficient. The other issue is that you don't have a checkpoint file because you didn't train long enough - and no checkpoint outperformed the existing performance on your tune dataset. Try re-running with `--debug=false` and `--config.num_epochs=10` and see where that gets you. If you get an OOM error with batch_size=512, reduce it and try again. If training produces a better model, it will be output in the `experiment_dir`.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/802
https://github.com/google/deepvariant/issues/802:511,energy efficiency,model,model,511,"@sophienguyen01 can you try to run this again without using `--debug=true`? This runs tensorflow in eager mode which will be very inefficient. The other issue is that you don't have a checkpoint file because you didn't train long enough - and no checkpoint outperformed the existing performance on your tune dataset. Try re-running with `--debug=false` and `--config.num_epochs=10` and see where that gets you. If you get an OOM error with batch_size=512, reduce it and try again. If training produces a better model, it will be output in the `experiment_dir`.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/802
https://github.com/google/deepvariant/issues/802:283,performance,perform,performance,283,"@sophienguyen01 can you try to run this again without using `--debug=true`? This runs tensorflow in eager mode which will be very inefficient. The other issue is that you don't have a checkpoint file because you didn't train long enough - and no checkpoint outperformed the existing performance on your tune dataset. Try re-running with `--debug=false` and `--config.num_epochs=10` and see where that gets you. If you get an OOM error with batch_size=512, reduce it and try again. If training produces a better model, it will be output in the `experiment_dir`.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/802
https://github.com/google/deepvariant/issues/802:303,performance,tune,tune,303,"@sophienguyen01 can you try to run this again without using `--debug=true`? This runs tensorflow in eager mode which will be very inefficient. The other issue is that you don't have a checkpoint file because you didn't train long enough - and no checkpoint outperformed the existing performance on your tune dataset. Try re-running with `--debug=false` and `--config.num_epochs=10` and see where that gets you. If you get an OOM error with batch_size=512, reduce it and try again. If training produces a better model, it will be output in the `experiment_dir`.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/802
https://github.com/google/deepvariant/issues/802:429,performance,error,error,429,"@sophienguyen01 can you try to run this again without using `--debug=true`? This runs tensorflow in eager mode which will be very inefficient. The other issue is that you don't have a checkpoint file because you didn't train long enough - and no checkpoint outperformed the existing performance on your tune dataset. Try re-running with `--debug=false` and `--config.num_epochs=10` and see where that gets you. If you get an OOM error with batch_size=512, reduce it and try again. If training produces a better model, it will be output in the `experiment_dir`.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/802
https://github.com/google/deepvariant/issues/802:184,reliability,checkpoint,checkpoint,184,"@sophienguyen01 can you try to run this again without using `--debug=true`? This runs tensorflow in eager mode which will be very inefficient. The other issue is that you don't have a checkpoint file because you didn't train long enough - and no checkpoint outperformed the existing performance on your tune dataset. Try re-running with `--debug=false` and `--config.num_epochs=10` and see where that gets you. If you get an OOM error with batch_size=512, reduce it and try again. If training produces a better model, it will be output in the `experiment_dir`.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/802
https://github.com/google/deepvariant/issues/802:246,reliability,checkpoint,checkpoint,246,"@sophienguyen01 can you try to run this again without using `--debug=true`? This runs tensorflow in eager mode which will be very inefficient. The other issue is that you don't have a checkpoint file because you didn't train long enough - and no checkpoint outperformed the existing performance on your tune dataset. Try re-running with `--debug=false` and `--config.num_epochs=10` and see where that gets you. If you get an OOM error with batch_size=512, reduce it and try again. If training produces a better model, it will be output in the `experiment_dir`.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/802
https://github.com/google/deepvariant/issues/802:429,safety,error,error,429,"@sophienguyen01 can you try to run this again without using `--debug=true`? This runs tensorflow in eager mode which will be very inefficient. The other issue is that you don't have a checkpoint file because you didn't train long enough - and no checkpoint outperformed the existing performance on your tune dataset. Try re-running with `--debug=false` and `--config.num_epochs=10` and see where that gets you. If you get an OOM error with batch_size=512, reduce it and try again. If training produces a better model, it will be output in the `experiment_dir`.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/802
https://github.com/google/deepvariant/issues/802:511,security,model,model,511,"@sophienguyen01 can you try to run this again without using `--debug=true`? This runs tensorflow in eager mode which will be very inefficient. The other issue is that you don't have a checkpoint file because you didn't train long enough - and no checkpoint outperformed the existing performance on your tune dataset. Try re-running with `--debug=false` and `--config.num_epochs=10` and see where that gets you. If you get an OOM error with batch_size=512, reduce it and try again. If training produces a better model, it will be output in the `experiment_dir`.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/802
https://github.com/google/deepvariant/issues/802:283,usability,perform,performance,283,"@sophienguyen01 can you try to run this again without using `--debug=true`? This runs tensorflow in eager mode which will be very inefficient. The other issue is that you don't have a checkpoint file because you didn't train long enough - and no checkpoint outperformed the existing performance on your tune dataset. Try re-running with `--debug=false` and `--config.num_epochs=10` and see where that gets you. If you get an OOM error with batch_size=512, reduce it and try again. If training produces a better model, it will be output in the `experiment_dir`.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/802
https://github.com/google/deepvariant/issues/802:429,usability,error,error,429,"@sophienguyen01 can you try to run this again without using `--debug=true`? This runs tensorflow in eager mode which will be very inefficient. The other issue is that you don't have a checkpoint file because you didn't train long enough - and no checkpoint outperformed the existing performance on your tune dataset. Try re-running with `--debug=false` and `--config.num_epochs=10` and see where that gets you. If you get an OOM error with batch_size=512, reduce it and try again. If training produces a better model, it will be output in the `experiment_dir`.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/802
https://github.com/google/deepvariant/issues/802:117,availability,error,error,117,"Hi @danielecook ,. I tried without ```--debug=false``` and set ```--config.num_epochs=10``` but I still get the same error that ```--config.num_epochs=10```. I attached my log file here . [train_040324.log](https://github.com/google/deepvariant/files/14873081/train_040324.log). THis is the command I used:. ```. BIN_VERSION=""1.6.1"". DOCKER_IMAGE=""google/deepvariant:${BIN_VERSION}"". time sudo docker run --gpus 1 \. -v /home/${USER}:/home/${USER} \. -w /home/${USER} \. ${DOCKER_IMAGE}-gpu \. train \. --config=s3-mount/deepvariant_training/script/dv_config.py:base \. --config.train_dataset_pbtxt=""${SHUFFLE_DIR}/training_set.dataset_config.pbtxt"" \. --config.tune_dataset_pbtxt=""${SHUFFLE_DIR}/validation_set.dataset_config.pbtxt"" \. --config.init_checkpoint=""${GCS_PRETRAINED_WGS_MODEL}"" \. --config.num_epochs=10 \. --config.learning_rate=0.02 \. --config.num_validation_examples=0 \. --experiment_dir=""model_train"" \. --strategy=mirrored \. --config.batch_size=512. ```. Did I miss anything?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/802
https://github.com/google/deepvariant/issues/802:172,deployability,log,log,172,"Hi @danielecook ,. I tried without ```--debug=false``` and set ```--config.num_epochs=10``` but I still get the same error that ```--config.num_epochs=10```. I attached my log file here . [train_040324.log](https://github.com/google/deepvariant/files/14873081/train_040324.log). THis is the command I used:. ```. BIN_VERSION=""1.6.1"". DOCKER_IMAGE=""google/deepvariant:${BIN_VERSION}"". time sudo docker run --gpus 1 \. -v /home/${USER}:/home/${USER} \. -w /home/${USER} \. ${DOCKER_IMAGE}-gpu \. train \. --config=s3-mount/deepvariant_training/script/dv_config.py:base \. --config.train_dataset_pbtxt=""${SHUFFLE_DIR}/training_set.dataset_config.pbtxt"" \. --config.tune_dataset_pbtxt=""${SHUFFLE_DIR}/validation_set.dataset_config.pbtxt"" \. --config.init_checkpoint=""${GCS_PRETRAINED_WGS_MODEL}"" \. --config.num_epochs=10 \. --config.learning_rate=0.02 \. --config.num_validation_examples=0 \. --experiment_dir=""model_train"" \. --strategy=mirrored \. --config.batch_size=512. ```. Did I miss anything?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/802
https://github.com/google/deepvariant/issues/802:202,deployability,log,log,202,"Hi @danielecook ,. I tried without ```--debug=false``` and set ```--config.num_epochs=10``` but I still get the same error that ```--config.num_epochs=10```. I attached my log file here . [train_040324.log](https://github.com/google/deepvariant/files/14873081/train_040324.log). THis is the command I used:. ```. BIN_VERSION=""1.6.1"". DOCKER_IMAGE=""google/deepvariant:${BIN_VERSION}"". time sudo docker run --gpus 1 \. -v /home/${USER}:/home/${USER} \. -w /home/${USER} \. ${DOCKER_IMAGE}-gpu \. train \. --config=s3-mount/deepvariant_training/script/dv_config.py:base \. --config.train_dataset_pbtxt=""${SHUFFLE_DIR}/training_set.dataset_config.pbtxt"" \. --config.tune_dataset_pbtxt=""${SHUFFLE_DIR}/validation_set.dataset_config.pbtxt"" \. --config.init_checkpoint=""${GCS_PRETRAINED_WGS_MODEL}"" \. --config.num_epochs=10 \. --config.learning_rate=0.02 \. --config.num_validation_examples=0 \. --experiment_dir=""model_train"" \. --strategy=mirrored \. --config.batch_size=512. ```. Did I miss anything?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/802
https://github.com/google/deepvariant/issues/802:273,deployability,log,log,273,"Hi @danielecook ,. I tried without ```--debug=false``` and set ```--config.num_epochs=10``` but I still get the same error that ```--config.num_epochs=10```. I attached my log file here . [train_040324.log](https://github.com/google/deepvariant/files/14873081/train_040324.log). THis is the command I used:. ```. BIN_VERSION=""1.6.1"". DOCKER_IMAGE=""google/deepvariant:${BIN_VERSION}"". time sudo docker run --gpus 1 \. -v /home/${USER}:/home/${USER} \. -w /home/${USER} \. ${DOCKER_IMAGE}-gpu \. train \. --config=s3-mount/deepvariant_training/script/dv_config.py:base \. --config.train_dataset_pbtxt=""${SHUFFLE_DIR}/training_set.dataset_config.pbtxt"" \. --config.tune_dataset_pbtxt=""${SHUFFLE_DIR}/validation_set.dataset_config.pbtxt"" \. --config.init_checkpoint=""${GCS_PRETRAINED_WGS_MODEL}"" \. --config.num_epochs=10 \. --config.learning_rate=0.02 \. --config.num_validation_examples=0 \. --experiment_dir=""model_train"" \. --strategy=mirrored \. --config.batch_size=512. ```. Did I miss anything?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/802
https://github.com/google/deepvariant/issues/802:407,energy efficiency,gpu,gpus,407,"Hi @danielecook ,. I tried without ```--debug=false``` and set ```--config.num_epochs=10``` but I still get the same error that ```--config.num_epochs=10```. I attached my log file here . [train_040324.log](https://github.com/google/deepvariant/files/14873081/train_040324.log). THis is the command I used:. ```. BIN_VERSION=""1.6.1"". DOCKER_IMAGE=""google/deepvariant:${BIN_VERSION}"". time sudo docker run --gpus 1 \. -v /home/${USER}:/home/${USER} \. -w /home/${USER} \. ${DOCKER_IMAGE}-gpu \. train \. --config=s3-mount/deepvariant_training/script/dv_config.py:base \. --config.train_dataset_pbtxt=""${SHUFFLE_DIR}/training_set.dataset_config.pbtxt"" \. --config.tune_dataset_pbtxt=""${SHUFFLE_DIR}/validation_set.dataset_config.pbtxt"" \. --config.init_checkpoint=""${GCS_PRETRAINED_WGS_MODEL}"" \. --config.num_epochs=10 \. --config.learning_rate=0.02 \. --config.num_validation_examples=0 \. --experiment_dir=""model_train"" \. --strategy=mirrored \. --config.batch_size=512. ```. Did I miss anything?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/802
https://github.com/google/deepvariant/issues/802:487,energy efficiency,gpu,gpu,487,"Hi @danielecook ,. I tried without ```--debug=false``` and set ```--config.num_epochs=10``` but I still get the same error that ```--config.num_epochs=10```. I attached my log file here . [train_040324.log](https://github.com/google/deepvariant/files/14873081/train_040324.log). THis is the command I used:. ```. BIN_VERSION=""1.6.1"". DOCKER_IMAGE=""google/deepvariant:${BIN_VERSION}"". time sudo docker run --gpus 1 \. -v /home/${USER}:/home/${USER} \. -w /home/${USER} \. ${DOCKER_IMAGE}-gpu \. train \. --config=s3-mount/deepvariant_training/script/dv_config.py:base \. --config.train_dataset_pbtxt=""${SHUFFLE_DIR}/training_set.dataset_config.pbtxt"" \. --config.tune_dataset_pbtxt=""${SHUFFLE_DIR}/validation_set.dataset_config.pbtxt"" \. --config.init_checkpoint=""${GCS_PRETRAINED_WGS_MODEL}"" \. --config.num_epochs=10 \. --config.learning_rate=0.02 \. --config.num_validation_examples=0 \. --experiment_dir=""model_train"" \. --strategy=mirrored \. --config.batch_size=512. ```. Did I miss anything?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/802
https://github.com/google/deepvariant/issues/802:117,performance,error,error,117,"Hi @danielecook ,. I tried without ```--debug=false``` and set ```--config.num_epochs=10``` but I still get the same error that ```--config.num_epochs=10```. I attached my log file here . [train_040324.log](https://github.com/google/deepvariant/files/14873081/train_040324.log). THis is the command I used:. ```. BIN_VERSION=""1.6.1"". DOCKER_IMAGE=""google/deepvariant:${BIN_VERSION}"". time sudo docker run --gpus 1 \. -v /home/${USER}:/home/${USER} \. -w /home/${USER} \. ${DOCKER_IMAGE}-gpu \. train \. --config=s3-mount/deepvariant_training/script/dv_config.py:base \. --config.train_dataset_pbtxt=""${SHUFFLE_DIR}/training_set.dataset_config.pbtxt"" \. --config.tune_dataset_pbtxt=""${SHUFFLE_DIR}/validation_set.dataset_config.pbtxt"" \. --config.init_checkpoint=""${GCS_PRETRAINED_WGS_MODEL}"" \. --config.num_epochs=10 \. --config.learning_rate=0.02 \. --config.num_validation_examples=0 \. --experiment_dir=""model_train"" \. --strategy=mirrored \. --config.batch_size=512. ```. Did I miss anything?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/802
https://github.com/google/deepvariant/issues/802:384,performance,time,time,384,"Hi @danielecook ,. I tried without ```--debug=false``` and set ```--config.num_epochs=10``` but I still get the same error that ```--config.num_epochs=10```. I attached my log file here . [train_040324.log](https://github.com/google/deepvariant/files/14873081/train_040324.log). THis is the command I used:. ```. BIN_VERSION=""1.6.1"". DOCKER_IMAGE=""google/deepvariant:${BIN_VERSION}"". time sudo docker run --gpus 1 \. -v /home/${USER}:/home/${USER} \. -w /home/${USER} \. ${DOCKER_IMAGE}-gpu \. train \. --config=s3-mount/deepvariant_training/script/dv_config.py:base \. --config.train_dataset_pbtxt=""${SHUFFLE_DIR}/training_set.dataset_config.pbtxt"" \. --config.tune_dataset_pbtxt=""${SHUFFLE_DIR}/validation_set.dataset_config.pbtxt"" \. --config.init_checkpoint=""${GCS_PRETRAINED_WGS_MODEL}"" \. --config.num_epochs=10 \. --config.learning_rate=0.02 \. --config.num_validation_examples=0 \. --experiment_dir=""model_train"" \. --strategy=mirrored \. --config.batch_size=512. ```. Did I miss anything?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/802
https://github.com/google/deepvariant/issues/802:407,performance,gpu,gpus,407,"Hi @danielecook ,. I tried without ```--debug=false``` and set ```--config.num_epochs=10``` but I still get the same error that ```--config.num_epochs=10```. I attached my log file here . [train_040324.log](https://github.com/google/deepvariant/files/14873081/train_040324.log). THis is the command I used:. ```. BIN_VERSION=""1.6.1"". DOCKER_IMAGE=""google/deepvariant:${BIN_VERSION}"". time sudo docker run --gpus 1 \. -v /home/${USER}:/home/${USER} \. -w /home/${USER} \. ${DOCKER_IMAGE}-gpu \. train \. --config=s3-mount/deepvariant_training/script/dv_config.py:base \. --config.train_dataset_pbtxt=""${SHUFFLE_DIR}/training_set.dataset_config.pbtxt"" \. --config.tune_dataset_pbtxt=""${SHUFFLE_DIR}/validation_set.dataset_config.pbtxt"" \. --config.init_checkpoint=""${GCS_PRETRAINED_WGS_MODEL}"" \. --config.num_epochs=10 \. --config.learning_rate=0.02 \. --config.num_validation_examples=0 \. --experiment_dir=""model_train"" \. --strategy=mirrored \. --config.batch_size=512. ```. Did I miss anything?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/802
https://github.com/google/deepvariant/issues/802:487,performance,gpu,gpu,487,"Hi @danielecook ,. I tried without ```--debug=false``` and set ```--config.num_epochs=10``` but I still get the same error that ```--config.num_epochs=10```. I attached my log file here . [train_040324.log](https://github.com/google/deepvariant/files/14873081/train_040324.log). THis is the command I used:. ```. BIN_VERSION=""1.6.1"". DOCKER_IMAGE=""google/deepvariant:${BIN_VERSION}"". time sudo docker run --gpus 1 \. -v /home/${USER}:/home/${USER} \. -w /home/${USER} \. ${DOCKER_IMAGE}-gpu \. train \. --config=s3-mount/deepvariant_training/script/dv_config.py:base \. --config.train_dataset_pbtxt=""${SHUFFLE_DIR}/training_set.dataset_config.pbtxt"" \. --config.tune_dataset_pbtxt=""${SHUFFLE_DIR}/validation_set.dataset_config.pbtxt"" \. --config.init_checkpoint=""${GCS_PRETRAINED_WGS_MODEL}"" \. --config.num_epochs=10 \. --config.learning_rate=0.02 \. --config.num_validation_examples=0 \. --experiment_dir=""model_train"" \. --strategy=mirrored \. --config.batch_size=512. ```. Did I miss anything?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/802
https://github.com/google/deepvariant/issues/802:117,safety,error,error,117,"Hi @danielecook ,. I tried without ```--debug=false``` and set ```--config.num_epochs=10``` but I still get the same error that ```--config.num_epochs=10```. I attached my log file here . [train_040324.log](https://github.com/google/deepvariant/files/14873081/train_040324.log). THis is the command I used:. ```. BIN_VERSION=""1.6.1"". DOCKER_IMAGE=""google/deepvariant:${BIN_VERSION}"". time sudo docker run --gpus 1 \. -v /home/${USER}:/home/${USER} \. -w /home/${USER} \. ${DOCKER_IMAGE}-gpu \. train \. --config=s3-mount/deepvariant_training/script/dv_config.py:base \. --config.train_dataset_pbtxt=""${SHUFFLE_DIR}/training_set.dataset_config.pbtxt"" \. --config.tune_dataset_pbtxt=""${SHUFFLE_DIR}/validation_set.dataset_config.pbtxt"" \. --config.init_checkpoint=""${GCS_PRETRAINED_WGS_MODEL}"" \. --config.num_epochs=10 \. --config.learning_rate=0.02 \. --config.num_validation_examples=0 \. --experiment_dir=""model_train"" \. --strategy=mirrored \. --config.batch_size=512. ```. Did I miss anything?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/802
https://github.com/google/deepvariant/issues/802:172,safety,log,log,172,"Hi @danielecook ,. I tried without ```--debug=false``` and set ```--config.num_epochs=10``` but I still get the same error that ```--config.num_epochs=10```. I attached my log file here . [train_040324.log](https://github.com/google/deepvariant/files/14873081/train_040324.log). THis is the command I used:. ```. BIN_VERSION=""1.6.1"". DOCKER_IMAGE=""google/deepvariant:${BIN_VERSION}"". time sudo docker run --gpus 1 \. -v /home/${USER}:/home/${USER} \. -w /home/${USER} \. ${DOCKER_IMAGE}-gpu \. train \. --config=s3-mount/deepvariant_training/script/dv_config.py:base \. --config.train_dataset_pbtxt=""${SHUFFLE_DIR}/training_set.dataset_config.pbtxt"" \. --config.tune_dataset_pbtxt=""${SHUFFLE_DIR}/validation_set.dataset_config.pbtxt"" \. --config.init_checkpoint=""${GCS_PRETRAINED_WGS_MODEL}"" \. --config.num_epochs=10 \. --config.learning_rate=0.02 \. --config.num_validation_examples=0 \. --experiment_dir=""model_train"" \. --strategy=mirrored \. --config.batch_size=512. ```. Did I miss anything?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/802
https://github.com/google/deepvariant/issues/802:202,safety,log,log,202,"Hi @danielecook ,. I tried without ```--debug=false``` and set ```--config.num_epochs=10``` but I still get the same error that ```--config.num_epochs=10```. I attached my log file here . [train_040324.log](https://github.com/google/deepvariant/files/14873081/train_040324.log). THis is the command I used:. ```. BIN_VERSION=""1.6.1"". DOCKER_IMAGE=""google/deepvariant:${BIN_VERSION}"". time sudo docker run --gpus 1 \. -v /home/${USER}:/home/${USER} \. -w /home/${USER} \. ${DOCKER_IMAGE}-gpu \. train \. --config=s3-mount/deepvariant_training/script/dv_config.py:base \. --config.train_dataset_pbtxt=""${SHUFFLE_DIR}/training_set.dataset_config.pbtxt"" \. --config.tune_dataset_pbtxt=""${SHUFFLE_DIR}/validation_set.dataset_config.pbtxt"" \. --config.init_checkpoint=""${GCS_PRETRAINED_WGS_MODEL}"" \. --config.num_epochs=10 \. --config.learning_rate=0.02 \. --config.num_validation_examples=0 \. --experiment_dir=""model_train"" \. --strategy=mirrored \. --config.batch_size=512. ```. Did I miss anything?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/802
https://github.com/google/deepvariant/issues/802:273,safety,log,log,273,"Hi @danielecook ,. I tried without ```--debug=false``` and set ```--config.num_epochs=10``` but I still get the same error that ```--config.num_epochs=10```. I attached my log file here . [train_040324.log](https://github.com/google/deepvariant/files/14873081/train_040324.log). THis is the command I used:. ```. BIN_VERSION=""1.6.1"". DOCKER_IMAGE=""google/deepvariant:${BIN_VERSION}"". time sudo docker run --gpus 1 \. -v /home/${USER}:/home/${USER} \. -w /home/${USER} \. ${DOCKER_IMAGE}-gpu \. train \. --config=s3-mount/deepvariant_training/script/dv_config.py:base \. --config.train_dataset_pbtxt=""${SHUFFLE_DIR}/training_set.dataset_config.pbtxt"" \. --config.tune_dataset_pbtxt=""${SHUFFLE_DIR}/validation_set.dataset_config.pbtxt"" \. --config.init_checkpoint=""${GCS_PRETRAINED_WGS_MODEL}"" \. --config.num_epochs=10 \. --config.learning_rate=0.02 \. --config.num_validation_examples=0 \. --experiment_dir=""model_train"" \. --strategy=mirrored \. --config.batch_size=512. ```. Did I miss anything?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/802
https://github.com/google/deepvariant/issues/802:172,security,log,log,172,"Hi @danielecook ,. I tried without ```--debug=false``` and set ```--config.num_epochs=10``` but I still get the same error that ```--config.num_epochs=10```. I attached my log file here . [train_040324.log](https://github.com/google/deepvariant/files/14873081/train_040324.log). THis is the command I used:. ```. BIN_VERSION=""1.6.1"". DOCKER_IMAGE=""google/deepvariant:${BIN_VERSION}"". time sudo docker run --gpus 1 \. -v /home/${USER}:/home/${USER} \. -w /home/${USER} \. ${DOCKER_IMAGE}-gpu \. train \. --config=s3-mount/deepvariant_training/script/dv_config.py:base \. --config.train_dataset_pbtxt=""${SHUFFLE_DIR}/training_set.dataset_config.pbtxt"" \. --config.tune_dataset_pbtxt=""${SHUFFLE_DIR}/validation_set.dataset_config.pbtxt"" \. --config.init_checkpoint=""${GCS_PRETRAINED_WGS_MODEL}"" \. --config.num_epochs=10 \. --config.learning_rate=0.02 \. --config.num_validation_examples=0 \. --experiment_dir=""model_train"" \. --strategy=mirrored \. --config.batch_size=512. ```. Did I miss anything?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/802
https://github.com/google/deepvariant/issues/802:202,security,log,log,202,"Hi @danielecook ,. I tried without ```--debug=false``` and set ```--config.num_epochs=10``` but I still get the same error that ```--config.num_epochs=10```. I attached my log file here . [train_040324.log](https://github.com/google/deepvariant/files/14873081/train_040324.log). THis is the command I used:. ```. BIN_VERSION=""1.6.1"". DOCKER_IMAGE=""google/deepvariant:${BIN_VERSION}"". time sudo docker run --gpus 1 \. -v /home/${USER}:/home/${USER} \. -w /home/${USER} \. ${DOCKER_IMAGE}-gpu \. train \. --config=s3-mount/deepvariant_training/script/dv_config.py:base \. --config.train_dataset_pbtxt=""${SHUFFLE_DIR}/training_set.dataset_config.pbtxt"" \. --config.tune_dataset_pbtxt=""${SHUFFLE_DIR}/validation_set.dataset_config.pbtxt"" \. --config.init_checkpoint=""${GCS_PRETRAINED_WGS_MODEL}"" \. --config.num_epochs=10 \. --config.learning_rate=0.02 \. --config.num_validation_examples=0 \. --experiment_dir=""model_train"" \. --strategy=mirrored \. --config.batch_size=512. ```. Did I miss anything?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/802
https://github.com/google/deepvariant/issues/802:273,security,log,log,273,"Hi @danielecook ,. I tried without ```--debug=false``` and set ```--config.num_epochs=10``` but I still get the same error that ```--config.num_epochs=10```. I attached my log file here . [train_040324.log](https://github.com/google/deepvariant/files/14873081/train_040324.log). THis is the command I used:. ```. BIN_VERSION=""1.6.1"". DOCKER_IMAGE=""google/deepvariant:${BIN_VERSION}"". time sudo docker run --gpus 1 \. -v /home/${USER}:/home/${USER} \. -w /home/${USER} \. ${DOCKER_IMAGE}-gpu \. train \. --config=s3-mount/deepvariant_training/script/dv_config.py:base \. --config.train_dataset_pbtxt=""${SHUFFLE_DIR}/training_set.dataset_config.pbtxt"" \. --config.tune_dataset_pbtxt=""${SHUFFLE_DIR}/validation_set.dataset_config.pbtxt"" \. --config.init_checkpoint=""${GCS_PRETRAINED_WGS_MODEL}"" \. --config.num_epochs=10 \. --config.learning_rate=0.02 \. --config.num_validation_examples=0 \. --experiment_dir=""model_train"" \. --strategy=mirrored \. --config.batch_size=512. ```. Did I miss anything?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/802
https://github.com/google/deepvariant/issues/802:172,testability,log,log,172,"Hi @danielecook ,. I tried without ```--debug=false``` and set ```--config.num_epochs=10``` but I still get the same error that ```--config.num_epochs=10```. I attached my log file here . [train_040324.log](https://github.com/google/deepvariant/files/14873081/train_040324.log). THis is the command I used:. ```. BIN_VERSION=""1.6.1"". DOCKER_IMAGE=""google/deepvariant:${BIN_VERSION}"". time sudo docker run --gpus 1 \. -v /home/${USER}:/home/${USER} \. -w /home/${USER} \. ${DOCKER_IMAGE}-gpu \. train \. --config=s3-mount/deepvariant_training/script/dv_config.py:base \. --config.train_dataset_pbtxt=""${SHUFFLE_DIR}/training_set.dataset_config.pbtxt"" \. --config.tune_dataset_pbtxt=""${SHUFFLE_DIR}/validation_set.dataset_config.pbtxt"" \. --config.init_checkpoint=""${GCS_PRETRAINED_WGS_MODEL}"" \. --config.num_epochs=10 \. --config.learning_rate=0.02 \. --config.num_validation_examples=0 \. --experiment_dir=""model_train"" \. --strategy=mirrored \. --config.batch_size=512. ```. Did I miss anything?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/802
https://github.com/google/deepvariant/issues/802:202,testability,log,log,202,"Hi @danielecook ,. I tried without ```--debug=false``` and set ```--config.num_epochs=10``` but I still get the same error that ```--config.num_epochs=10```. I attached my log file here . [train_040324.log](https://github.com/google/deepvariant/files/14873081/train_040324.log). THis is the command I used:. ```. BIN_VERSION=""1.6.1"". DOCKER_IMAGE=""google/deepvariant:${BIN_VERSION}"". time sudo docker run --gpus 1 \. -v /home/${USER}:/home/${USER} \. -w /home/${USER} \. ${DOCKER_IMAGE}-gpu \. train \. --config=s3-mount/deepvariant_training/script/dv_config.py:base \. --config.train_dataset_pbtxt=""${SHUFFLE_DIR}/training_set.dataset_config.pbtxt"" \. --config.tune_dataset_pbtxt=""${SHUFFLE_DIR}/validation_set.dataset_config.pbtxt"" \. --config.init_checkpoint=""${GCS_PRETRAINED_WGS_MODEL}"" \. --config.num_epochs=10 \. --config.learning_rate=0.02 \. --config.num_validation_examples=0 \. --experiment_dir=""model_train"" \. --strategy=mirrored \. --config.batch_size=512. ```. Did I miss anything?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/802
https://github.com/google/deepvariant/issues/802:273,testability,log,log,273,"Hi @danielecook ,. I tried without ```--debug=false``` and set ```--config.num_epochs=10``` but I still get the same error that ```--config.num_epochs=10```. I attached my log file here . [train_040324.log](https://github.com/google/deepvariant/files/14873081/train_040324.log). THis is the command I used:. ```. BIN_VERSION=""1.6.1"". DOCKER_IMAGE=""google/deepvariant:${BIN_VERSION}"". time sudo docker run --gpus 1 \. -v /home/${USER}:/home/${USER} \. -w /home/${USER} \. ${DOCKER_IMAGE}-gpu \. train \. --config=s3-mount/deepvariant_training/script/dv_config.py:base \. --config.train_dataset_pbtxt=""${SHUFFLE_DIR}/training_set.dataset_config.pbtxt"" \. --config.tune_dataset_pbtxt=""${SHUFFLE_DIR}/validation_set.dataset_config.pbtxt"" \. --config.init_checkpoint=""${GCS_PRETRAINED_WGS_MODEL}"" \. --config.num_epochs=10 \. --config.learning_rate=0.02 \. --config.num_validation_examples=0 \. --experiment_dir=""model_train"" \. --strategy=mirrored \. --config.batch_size=512. ```. Did I miss anything?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/802
https://github.com/google/deepvariant/issues/802:117,usability,error,error,117,"Hi @danielecook ,. I tried without ```--debug=false``` and set ```--config.num_epochs=10``` but I still get the same error that ```--config.num_epochs=10```. I attached my log file here . [train_040324.log](https://github.com/google/deepvariant/files/14873081/train_040324.log). THis is the command I used:. ```. BIN_VERSION=""1.6.1"". DOCKER_IMAGE=""google/deepvariant:${BIN_VERSION}"". time sudo docker run --gpus 1 \. -v /home/${USER}:/home/${USER} \. -w /home/${USER} \. ${DOCKER_IMAGE}-gpu \. train \. --config=s3-mount/deepvariant_training/script/dv_config.py:base \. --config.train_dataset_pbtxt=""${SHUFFLE_DIR}/training_set.dataset_config.pbtxt"" \. --config.tune_dataset_pbtxt=""${SHUFFLE_DIR}/validation_set.dataset_config.pbtxt"" \. --config.init_checkpoint=""${GCS_PRETRAINED_WGS_MODEL}"" \. --config.num_epochs=10 \. --config.learning_rate=0.02 \. --config.num_validation_examples=0 \. --experiment_dir=""model_train"" \. --strategy=mirrored \. --config.batch_size=512. ```. Did I miss anything?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/802
https://github.com/google/deepvariant/issues/802:291,usability,command,command,291,"Hi @danielecook ,. I tried without ```--debug=false``` and set ```--config.num_epochs=10``` but I still get the same error that ```--config.num_epochs=10```. I attached my log file here . [train_040324.log](https://github.com/google/deepvariant/files/14873081/train_040324.log). THis is the command I used:. ```. BIN_VERSION=""1.6.1"". DOCKER_IMAGE=""google/deepvariant:${BIN_VERSION}"". time sudo docker run --gpus 1 \. -v /home/${USER}:/home/${USER} \. -w /home/${USER} \. ${DOCKER_IMAGE}-gpu \. train \. --config=s3-mount/deepvariant_training/script/dv_config.py:base \. --config.train_dataset_pbtxt=""${SHUFFLE_DIR}/training_set.dataset_config.pbtxt"" \. --config.tune_dataset_pbtxt=""${SHUFFLE_DIR}/validation_set.dataset_config.pbtxt"" \. --config.init_checkpoint=""${GCS_PRETRAINED_WGS_MODEL}"" \. --config.num_epochs=10 \. --config.learning_rate=0.02 \. --config.num_validation_examples=0 \. --experiment_dir=""model_train"" \. --strategy=mirrored \. --config.batch_size=512. ```. Did I miss anything?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/802
https://github.com/google/deepvariant/issues/802:428,usability,USER,USER,428,"Hi @danielecook ,. I tried without ```--debug=false``` and set ```--config.num_epochs=10``` but I still get the same error that ```--config.num_epochs=10```. I attached my log file here . [train_040324.log](https://github.com/google/deepvariant/files/14873081/train_040324.log). THis is the command I used:. ```. BIN_VERSION=""1.6.1"". DOCKER_IMAGE=""google/deepvariant:${BIN_VERSION}"". time sudo docker run --gpus 1 \. -v /home/${USER}:/home/${USER} \. -w /home/${USER} \. ${DOCKER_IMAGE}-gpu \. train \. --config=s3-mount/deepvariant_training/script/dv_config.py:base \. --config.train_dataset_pbtxt=""${SHUFFLE_DIR}/training_set.dataset_config.pbtxt"" \. --config.tune_dataset_pbtxt=""${SHUFFLE_DIR}/validation_set.dataset_config.pbtxt"" \. --config.init_checkpoint=""${GCS_PRETRAINED_WGS_MODEL}"" \. --config.num_epochs=10 \. --config.learning_rate=0.02 \. --config.num_validation_examples=0 \. --experiment_dir=""model_train"" \. --strategy=mirrored \. --config.batch_size=512. ```. Did I miss anything?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/802
https://github.com/google/deepvariant/issues/802:442,usability,USER,USER,442,"Hi @danielecook ,. I tried without ```--debug=false``` and set ```--config.num_epochs=10``` but I still get the same error that ```--config.num_epochs=10```. I attached my log file here . [train_040324.log](https://github.com/google/deepvariant/files/14873081/train_040324.log). THis is the command I used:. ```. BIN_VERSION=""1.6.1"". DOCKER_IMAGE=""google/deepvariant:${BIN_VERSION}"". time sudo docker run --gpus 1 \. -v /home/${USER}:/home/${USER} \. -w /home/${USER} \. ${DOCKER_IMAGE}-gpu \. train \. --config=s3-mount/deepvariant_training/script/dv_config.py:base \. --config.train_dataset_pbtxt=""${SHUFFLE_DIR}/training_set.dataset_config.pbtxt"" \. --config.tune_dataset_pbtxt=""${SHUFFLE_DIR}/validation_set.dataset_config.pbtxt"" \. --config.init_checkpoint=""${GCS_PRETRAINED_WGS_MODEL}"" \. --config.num_epochs=10 \. --config.learning_rate=0.02 \. --config.num_validation_examples=0 \. --experiment_dir=""model_train"" \. --strategy=mirrored \. --config.batch_size=512. ```. Did I miss anything?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/802
https://github.com/google/deepvariant/issues/802:462,usability,USER,USER,462,"Hi @danielecook ,. I tried without ```--debug=false``` and set ```--config.num_epochs=10``` but I still get the same error that ```--config.num_epochs=10```. I attached my log file here . [train_040324.log](https://github.com/google/deepvariant/files/14873081/train_040324.log). THis is the command I used:. ```. BIN_VERSION=""1.6.1"". DOCKER_IMAGE=""google/deepvariant:${BIN_VERSION}"". time sudo docker run --gpus 1 \. -v /home/${USER}:/home/${USER} \. -w /home/${USER} \. ${DOCKER_IMAGE}-gpu \. train \. --config=s3-mount/deepvariant_training/script/dv_config.py:base \. --config.train_dataset_pbtxt=""${SHUFFLE_DIR}/training_set.dataset_config.pbtxt"" \. --config.tune_dataset_pbtxt=""${SHUFFLE_DIR}/validation_set.dataset_config.pbtxt"" \. --config.init_checkpoint=""${GCS_PRETRAINED_WGS_MODEL}"" \. --config.num_epochs=10 \. --config.learning_rate=0.02 \. --config.num_validation_examples=0 \. --experiment_dir=""model_train"" \. --strategy=mirrored \. --config.batch_size=512. ```. Did I miss anything?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/802
https://github.com/google/deepvariant/issues/802:850,availability,checkpoint,checkpoints,850,"@sophienguyen01 - from the log file it looks like everything worked. Here are all the tune/categorical accuracies from your training data. ```. tune/categorical_accuracy=0.9944317936897278. tune/categorical_accuracy=0.9909400343894958. tune/categorical_accuracy=0.9915463924407959. tune/categorical_accuracy=0.9925118088722229. tune/categorical_accuracy=0.9921825528144836. tune/categorical_accuracy=0.9924613237380981. tune/categorical_accuracy=0.9926846623420715. tune/categorical_accuracy=0.9929667711257935. tune/categorical_accuracy=0.9925829172134399. tune/categorical_accuracy=0.9926416277885437. tune/categorical_accuracy=0.9923893213272095. tune/categorical_accuracy=0.9925225377082825. ```. The first number represents accuracy direct from the pretrained model. Since none of the subsequent tuning evaluations outperformed the original, no checkpoints were created. One thing you could try: reduce the learning rate, and see if that helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/802
https://github.com/google/deepvariant/issues/802:27,deployability,log,log,27,"@sophienguyen01 - from the log file it looks like everything worked. Here are all the tune/categorical accuracies from your training data. ```. tune/categorical_accuracy=0.9944317936897278. tune/categorical_accuracy=0.9909400343894958. tune/categorical_accuracy=0.9915463924407959. tune/categorical_accuracy=0.9925118088722229. tune/categorical_accuracy=0.9921825528144836. tune/categorical_accuracy=0.9924613237380981. tune/categorical_accuracy=0.9926846623420715. tune/categorical_accuracy=0.9929667711257935. tune/categorical_accuracy=0.9925829172134399. tune/categorical_accuracy=0.9926416277885437. tune/categorical_accuracy=0.9923893213272095. tune/categorical_accuracy=0.9925225377082825. ```. The first number represents accuracy direct from the pretrained model. Since none of the subsequent tuning evaluations outperformed the original, no checkpoints were created. One thing you could try: reduce the learning rate, and see if that helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/802
https://github.com/google/deepvariant/issues/802:765,energy efficiency,model,model,765,"@sophienguyen01 - from the log file it looks like everything worked. Here are all the tune/categorical accuracies from your training data. ```. tune/categorical_accuracy=0.9944317936897278. tune/categorical_accuracy=0.9909400343894958. tune/categorical_accuracy=0.9915463924407959. tune/categorical_accuracy=0.9925118088722229. tune/categorical_accuracy=0.9921825528144836. tune/categorical_accuracy=0.9924613237380981. tune/categorical_accuracy=0.9926846623420715. tune/categorical_accuracy=0.9929667711257935. tune/categorical_accuracy=0.9925829172134399. tune/categorical_accuracy=0.9926416277885437. tune/categorical_accuracy=0.9923893213272095. tune/categorical_accuracy=0.9925225377082825. ```. The first number represents accuracy direct from the pretrained model. Since none of the subsequent tuning evaluations outperformed the original, no checkpoints were created. One thing you could try: reduce the learning rate, and see if that helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/802
https://github.com/google/deepvariant/issues/802:901,energy efficiency,reduc,reduce,901,"@sophienguyen01 - from the log file it looks like everything worked. Here are all the tune/categorical accuracies from your training data. ```. tune/categorical_accuracy=0.9944317936897278. tune/categorical_accuracy=0.9909400343894958. tune/categorical_accuracy=0.9915463924407959. tune/categorical_accuracy=0.9925118088722229. tune/categorical_accuracy=0.9921825528144836. tune/categorical_accuracy=0.9924613237380981. tune/categorical_accuracy=0.9926846623420715. tune/categorical_accuracy=0.9929667711257935. tune/categorical_accuracy=0.9925829172134399. tune/categorical_accuracy=0.9926416277885437. tune/categorical_accuracy=0.9923893213272095. tune/categorical_accuracy=0.9925225377082825. ```. The first number represents accuracy direct from the pretrained model. Since none of the subsequent tuning evaluations outperformed the original, no checkpoints were created. One thing you could try: reduce the learning rate, and see if that helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/802
https://github.com/google/deepvariant/issues/802:790,integrability,sub,subsequent,790,"@sophienguyen01 - from the log file it looks like everything worked. Here are all the tune/categorical accuracies from your training data. ```. tune/categorical_accuracy=0.9944317936897278. tune/categorical_accuracy=0.9909400343894958. tune/categorical_accuracy=0.9915463924407959. tune/categorical_accuracy=0.9925118088722229. tune/categorical_accuracy=0.9921825528144836. tune/categorical_accuracy=0.9924613237380981. tune/categorical_accuracy=0.9926846623420715. tune/categorical_accuracy=0.9929667711257935. tune/categorical_accuracy=0.9925829172134399. tune/categorical_accuracy=0.9926416277885437. tune/categorical_accuracy=0.9923893213272095. tune/categorical_accuracy=0.9925225377082825. ```. The first number represents accuracy direct from the pretrained model. Since none of the subsequent tuning evaluations outperformed the original, no checkpoints were created. One thing you could try: reduce the learning rate, and see if that helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/802
https://github.com/google/deepvariant/issues/802:86,performance,tune,tune,86,"@sophienguyen01 - from the log file it looks like everything worked. Here are all the tune/categorical accuracies from your training data. ```. tune/categorical_accuracy=0.9944317936897278. tune/categorical_accuracy=0.9909400343894958. tune/categorical_accuracy=0.9915463924407959. tune/categorical_accuracy=0.9925118088722229. tune/categorical_accuracy=0.9921825528144836. tune/categorical_accuracy=0.9924613237380981. tune/categorical_accuracy=0.9926846623420715. tune/categorical_accuracy=0.9929667711257935. tune/categorical_accuracy=0.9925829172134399. tune/categorical_accuracy=0.9926416277885437. tune/categorical_accuracy=0.9923893213272095. tune/categorical_accuracy=0.9925225377082825. ```. The first number represents accuracy direct from the pretrained model. Since none of the subsequent tuning evaluations outperformed the original, no checkpoints were created. One thing you could try: reduce the learning rate, and see if that helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/802
https://github.com/google/deepvariant/issues/802:144,performance,tune,tune,144,"@sophienguyen01 - from the log file it looks like everything worked. Here are all the tune/categorical accuracies from your training data. ```. tune/categorical_accuracy=0.9944317936897278. tune/categorical_accuracy=0.9909400343894958. tune/categorical_accuracy=0.9915463924407959. tune/categorical_accuracy=0.9925118088722229. tune/categorical_accuracy=0.9921825528144836. tune/categorical_accuracy=0.9924613237380981. tune/categorical_accuracy=0.9926846623420715. tune/categorical_accuracy=0.9929667711257935. tune/categorical_accuracy=0.9925829172134399. tune/categorical_accuracy=0.9926416277885437. tune/categorical_accuracy=0.9923893213272095. tune/categorical_accuracy=0.9925225377082825. ```. The first number represents accuracy direct from the pretrained model. Since none of the subsequent tuning evaluations outperformed the original, no checkpoints were created. One thing you could try: reduce the learning rate, and see if that helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/802
https://github.com/google/deepvariant/issues/802:190,performance,tune,tune,190,"@sophienguyen01 - from the log file it looks like everything worked. Here are all the tune/categorical accuracies from your training data. ```. tune/categorical_accuracy=0.9944317936897278. tune/categorical_accuracy=0.9909400343894958. tune/categorical_accuracy=0.9915463924407959. tune/categorical_accuracy=0.9925118088722229. tune/categorical_accuracy=0.9921825528144836. tune/categorical_accuracy=0.9924613237380981. tune/categorical_accuracy=0.9926846623420715. tune/categorical_accuracy=0.9929667711257935. tune/categorical_accuracy=0.9925829172134399. tune/categorical_accuracy=0.9926416277885437. tune/categorical_accuracy=0.9923893213272095. tune/categorical_accuracy=0.9925225377082825. ```. The first number represents accuracy direct from the pretrained model. Since none of the subsequent tuning evaluations outperformed the original, no checkpoints were created. One thing you could try: reduce the learning rate, and see if that helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/802
https://github.com/google/deepvariant/issues/802:236,performance,tune,tune,236,"@sophienguyen01 - from the log file it looks like everything worked. Here are all the tune/categorical accuracies from your training data. ```. tune/categorical_accuracy=0.9944317936897278. tune/categorical_accuracy=0.9909400343894958. tune/categorical_accuracy=0.9915463924407959. tune/categorical_accuracy=0.9925118088722229. tune/categorical_accuracy=0.9921825528144836. tune/categorical_accuracy=0.9924613237380981. tune/categorical_accuracy=0.9926846623420715. tune/categorical_accuracy=0.9929667711257935. tune/categorical_accuracy=0.9925829172134399. tune/categorical_accuracy=0.9926416277885437. tune/categorical_accuracy=0.9923893213272095. tune/categorical_accuracy=0.9925225377082825. ```. The first number represents accuracy direct from the pretrained model. Since none of the subsequent tuning evaluations outperformed the original, no checkpoints were created. One thing you could try: reduce the learning rate, and see if that helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/802
https://github.com/google/deepvariant/issues/802:282,performance,tune,tune,282,"@sophienguyen01 - from the log file it looks like everything worked. Here are all the tune/categorical accuracies from your training data. ```. tune/categorical_accuracy=0.9944317936897278. tune/categorical_accuracy=0.9909400343894958. tune/categorical_accuracy=0.9915463924407959. tune/categorical_accuracy=0.9925118088722229. tune/categorical_accuracy=0.9921825528144836. tune/categorical_accuracy=0.9924613237380981. tune/categorical_accuracy=0.9926846623420715. tune/categorical_accuracy=0.9929667711257935. tune/categorical_accuracy=0.9925829172134399. tune/categorical_accuracy=0.9926416277885437. tune/categorical_accuracy=0.9923893213272095. tune/categorical_accuracy=0.9925225377082825. ```. The first number represents accuracy direct from the pretrained model. Since none of the subsequent tuning evaluations outperformed the original, no checkpoints were created. One thing you could try: reduce the learning rate, and see if that helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/802
https://github.com/google/deepvariant/issues/802:328,performance,tune,tune,328,"@sophienguyen01 - from the log file it looks like everything worked. Here are all the tune/categorical accuracies from your training data. ```. tune/categorical_accuracy=0.9944317936897278. tune/categorical_accuracy=0.9909400343894958. tune/categorical_accuracy=0.9915463924407959. tune/categorical_accuracy=0.9925118088722229. tune/categorical_accuracy=0.9921825528144836. tune/categorical_accuracy=0.9924613237380981. tune/categorical_accuracy=0.9926846623420715. tune/categorical_accuracy=0.9929667711257935. tune/categorical_accuracy=0.9925829172134399. tune/categorical_accuracy=0.9926416277885437. tune/categorical_accuracy=0.9923893213272095. tune/categorical_accuracy=0.9925225377082825. ```. The first number represents accuracy direct from the pretrained model. Since none of the subsequent tuning evaluations outperformed the original, no checkpoints were created. One thing you could try: reduce the learning rate, and see if that helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/802
https://github.com/google/deepvariant/issues/802:374,performance,tune,tune,374,"@sophienguyen01 - from the log file it looks like everything worked. Here are all the tune/categorical accuracies from your training data. ```. tune/categorical_accuracy=0.9944317936897278. tune/categorical_accuracy=0.9909400343894958. tune/categorical_accuracy=0.9915463924407959. tune/categorical_accuracy=0.9925118088722229. tune/categorical_accuracy=0.9921825528144836. tune/categorical_accuracy=0.9924613237380981. tune/categorical_accuracy=0.9926846623420715. tune/categorical_accuracy=0.9929667711257935. tune/categorical_accuracy=0.9925829172134399. tune/categorical_accuracy=0.9926416277885437. tune/categorical_accuracy=0.9923893213272095. tune/categorical_accuracy=0.9925225377082825. ```. The first number represents accuracy direct from the pretrained model. Since none of the subsequent tuning evaluations outperformed the original, no checkpoints were created. One thing you could try: reduce the learning rate, and see if that helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/802
https://github.com/google/deepvariant/issues/802:420,performance,tune,tune,420,"@sophienguyen01 - from the log file it looks like everything worked. Here are all the tune/categorical accuracies from your training data. ```. tune/categorical_accuracy=0.9944317936897278. tune/categorical_accuracy=0.9909400343894958. tune/categorical_accuracy=0.9915463924407959. tune/categorical_accuracy=0.9925118088722229. tune/categorical_accuracy=0.9921825528144836. tune/categorical_accuracy=0.9924613237380981. tune/categorical_accuracy=0.9926846623420715. tune/categorical_accuracy=0.9929667711257935. tune/categorical_accuracy=0.9925829172134399. tune/categorical_accuracy=0.9926416277885437. tune/categorical_accuracy=0.9923893213272095. tune/categorical_accuracy=0.9925225377082825. ```. The first number represents accuracy direct from the pretrained model. Since none of the subsequent tuning evaluations outperformed the original, no checkpoints were created. One thing you could try: reduce the learning rate, and see if that helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/802
https://github.com/google/deepvariant/issues/802:466,performance,tune,tune,466,"@sophienguyen01 - from the log file it looks like everything worked. Here are all the tune/categorical accuracies from your training data. ```. tune/categorical_accuracy=0.9944317936897278. tune/categorical_accuracy=0.9909400343894958. tune/categorical_accuracy=0.9915463924407959. tune/categorical_accuracy=0.9925118088722229. tune/categorical_accuracy=0.9921825528144836. tune/categorical_accuracy=0.9924613237380981. tune/categorical_accuracy=0.9926846623420715. tune/categorical_accuracy=0.9929667711257935. tune/categorical_accuracy=0.9925829172134399. tune/categorical_accuracy=0.9926416277885437. tune/categorical_accuracy=0.9923893213272095. tune/categorical_accuracy=0.9925225377082825. ```. The first number represents accuracy direct from the pretrained model. Since none of the subsequent tuning evaluations outperformed the original, no checkpoints were created. One thing you could try: reduce the learning rate, and see if that helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/802
https://github.com/google/deepvariant/issues/802:512,performance,tune,tune,512,"@sophienguyen01 - from the log file it looks like everything worked. Here are all the tune/categorical accuracies from your training data. ```. tune/categorical_accuracy=0.9944317936897278. tune/categorical_accuracy=0.9909400343894958. tune/categorical_accuracy=0.9915463924407959. tune/categorical_accuracy=0.9925118088722229. tune/categorical_accuracy=0.9921825528144836. tune/categorical_accuracy=0.9924613237380981. tune/categorical_accuracy=0.9926846623420715. tune/categorical_accuracy=0.9929667711257935. tune/categorical_accuracy=0.9925829172134399. tune/categorical_accuracy=0.9926416277885437. tune/categorical_accuracy=0.9923893213272095. tune/categorical_accuracy=0.9925225377082825. ```. The first number represents accuracy direct from the pretrained model. Since none of the subsequent tuning evaluations outperformed the original, no checkpoints were created. One thing you could try: reduce the learning rate, and see if that helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/802
https://github.com/google/deepvariant/issues/802:558,performance,tune,tune,558,"@sophienguyen01 - from the log file it looks like everything worked. Here are all the tune/categorical accuracies from your training data. ```. tune/categorical_accuracy=0.9944317936897278. tune/categorical_accuracy=0.9909400343894958. tune/categorical_accuracy=0.9915463924407959. tune/categorical_accuracy=0.9925118088722229. tune/categorical_accuracy=0.9921825528144836. tune/categorical_accuracy=0.9924613237380981. tune/categorical_accuracy=0.9926846623420715. tune/categorical_accuracy=0.9929667711257935. tune/categorical_accuracy=0.9925829172134399. tune/categorical_accuracy=0.9926416277885437. tune/categorical_accuracy=0.9923893213272095. tune/categorical_accuracy=0.9925225377082825. ```. The first number represents accuracy direct from the pretrained model. Since none of the subsequent tuning evaluations outperformed the original, no checkpoints were created. One thing you could try: reduce the learning rate, and see if that helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/802
https://github.com/google/deepvariant/issues/802:604,performance,tune,tune,604,"@sophienguyen01 - from the log file it looks like everything worked. Here are all the tune/categorical accuracies from your training data. ```. tune/categorical_accuracy=0.9944317936897278. tune/categorical_accuracy=0.9909400343894958. tune/categorical_accuracy=0.9915463924407959. tune/categorical_accuracy=0.9925118088722229. tune/categorical_accuracy=0.9921825528144836. tune/categorical_accuracy=0.9924613237380981. tune/categorical_accuracy=0.9926846623420715. tune/categorical_accuracy=0.9929667711257935. tune/categorical_accuracy=0.9925829172134399. tune/categorical_accuracy=0.9926416277885437. tune/categorical_accuracy=0.9923893213272095. tune/categorical_accuracy=0.9925225377082825. ```. The first number represents accuracy direct from the pretrained model. Since none of the subsequent tuning evaluations outperformed the original, no checkpoints were created. One thing you could try: reduce the learning rate, and see if that helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/802
https://github.com/google/deepvariant/issues/802:650,performance,tune,tune,650,"@sophienguyen01 - from the log file it looks like everything worked. Here are all the tune/categorical accuracies from your training data. ```. tune/categorical_accuracy=0.9944317936897278. tune/categorical_accuracy=0.9909400343894958. tune/categorical_accuracy=0.9915463924407959. tune/categorical_accuracy=0.9925118088722229. tune/categorical_accuracy=0.9921825528144836. tune/categorical_accuracy=0.9924613237380981. tune/categorical_accuracy=0.9926846623420715. tune/categorical_accuracy=0.9929667711257935. tune/categorical_accuracy=0.9925829172134399. tune/categorical_accuracy=0.9926416277885437. tune/categorical_accuracy=0.9923893213272095. tune/categorical_accuracy=0.9925225377082825. ```. The first number represents accuracy direct from the pretrained model. Since none of the subsequent tuning evaluations outperformed the original, no checkpoints were created. One thing you could try: reduce the learning rate, and see if that helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/802
https://github.com/google/deepvariant/issues/802:850,reliability,checkpoint,checkpoints,850,"@sophienguyen01 - from the log file it looks like everything worked. Here are all the tune/categorical accuracies from your training data. ```. tune/categorical_accuracy=0.9944317936897278. tune/categorical_accuracy=0.9909400343894958. tune/categorical_accuracy=0.9915463924407959. tune/categorical_accuracy=0.9925118088722229. tune/categorical_accuracy=0.9921825528144836. tune/categorical_accuracy=0.9924613237380981. tune/categorical_accuracy=0.9926846623420715. tune/categorical_accuracy=0.9929667711257935. tune/categorical_accuracy=0.9925829172134399. tune/categorical_accuracy=0.9926416277885437. tune/categorical_accuracy=0.9923893213272095. tune/categorical_accuracy=0.9925225377082825. ```. The first number represents accuracy direct from the pretrained model. Since none of the subsequent tuning evaluations outperformed the original, no checkpoints were created. One thing you could try: reduce the learning rate, and see if that helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/802
https://github.com/google/deepvariant/issues/802:27,safety,log,log,27,"@sophienguyen01 - from the log file it looks like everything worked. Here are all the tune/categorical accuracies from your training data. ```. tune/categorical_accuracy=0.9944317936897278. tune/categorical_accuracy=0.9909400343894958. tune/categorical_accuracy=0.9915463924407959. tune/categorical_accuracy=0.9925118088722229. tune/categorical_accuracy=0.9921825528144836. tune/categorical_accuracy=0.9924613237380981. tune/categorical_accuracy=0.9926846623420715. tune/categorical_accuracy=0.9929667711257935. tune/categorical_accuracy=0.9925829172134399. tune/categorical_accuracy=0.9926416277885437. tune/categorical_accuracy=0.9923893213272095. tune/categorical_accuracy=0.9925225377082825. ```. The first number represents accuracy direct from the pretrained model. Since none of the subsequent tuning evaluations outperformed the original, no checkpoints were created. One thing you could try: reduce the learning rate, and see if that helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/802
https://github.com/google/deepvariant/issues/802:27,security,log,log,27,"@sophienguyen01 - from the log file it looks like everything worked. Here are all the tune/categorical accuracies from your training data. ```. tune/categorical_accuracy=0.9944317936897278. tune/categorical_accuracy=0.9909400343894958. tune/categorical_accuracy=0.9915463924407959. tune/categorical_accuracy=0.9925118088722229. tune/categorical_accuracy=0.9921825528144836. tune/categorical_accuracy=0.9924613237380981. tune/categorical_accuracy=0.9926846623420715. tune/categorical_accuracy=0.9929667711257935. tune/categorical_accuracy=0.9925829172134399. tune/categorical_accuracy=0.9926416277885437. tune/categorical_accuracy=0.9923893213272095. tune/categorical_accuracy=0.9925225377082825. ```. The first number represents accuracy direct from the pretrained model. Since none of the subsequent tuning evaluations outperformed the original, no checkpoints were created. One thing you could try: reduce the learning rate, and see if that helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/802
https://github.com/google/deepvariant/issues/802:765,security,model,model,765,"@sophienguyen01 - from the log file it looks like everything worked. Here are all the tune/categorical accuracies from your training data. ```. tune/categorical_accuracy=0.9944317936897278. tune/categorical_accuracy=0.9909400343894958. tune/categorical_accuracy=0.9915463924407959. tune/categorical_accuracy=0.9925118088722229. tune/categorical_accuracy=0.9921825528144836. tune/categorical_accuracy=0.9924613237380981. tune/categorical_accuracy=0.9926846623420715. tune/categorical_accuracy=0.9929667711257935. tune/categorical_accuracy=0.9925829172134399. tune/categorical_accuracy=0.9926416277885437. tune/categorical_accuracy=0.9923893213272095. tune/categorical_accuracy=0.9925225377082825. ```. The first number represents accuracy direct from the pretrained model. Since none of the subsequent tuning evaluations outperformed the original, no checkpoints were created. One thing you could try: reduce the learning rate, and see if that helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/802
https://github.com/google/deepvariant/issues/802:27,testability,log,log,27,"@sophienguyen01 - from the log file it looks like everything worked. Here are all the tune/categorical accuracies from your training data. ```. tune/categorical_accuracy=0.9944317936897278. tune/categorical_accuracy=0.9909400343894958. tune/categorical_accuracy=0.9915463924407959. tune/categorical_accuracy=0.9925118088722229. tune/categorical_accuracy=0.9921825528144836. tune/categorical_accuracy=0.9924613237380981. tune/categorical_accuracy=0.9926846623420715. tune/categorical_accuracy=0.9929667711257935. tune/categorical_accuracy=0.9925829172134399. tune/categorical_accuracy=0.9926416277885437. tune/categorical_accuracy=0.9923893213272095. tune/categorical_accuracy=0.9925225377082825. ```. The first number represents accuracy direct from the pretrained model. Since none of the subsequent tuning evaluations outperformed the original, no checkpoints were created. One thing you could try: reduce the learning rate, and see if that helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/802
https://github.com/google/deepvariant/issues/802:912,usability,learn,learning,912,"@sophienguyen01 - from the log file it looks like everything worked. Here are all the tune/categorical accuracies from your training data. ```. tune/categorical_accuracy=0.9944317936897278. tune/categorical_accuracy=0.9909400343894958. tune/categorical_accuracy=0.9915463924407959. tune/categorical_accuracy=0.9925118088722229. tune/categorical_accuracy=0.9921825528144836. tune/categorical_accuracy=0.9924613237380981. tune/categorical_accuracy=0.9926846623420715. tune/categorical_accuracy=0.9929667711257935. tune/categorical_accuracy=0.9925829172134399. tune/categorical_accuracy=0.9926416277885437. tune/categorical_accuracy=0.9923893213272095. tune/categorical_accuracy=0.9925225377082825. ```. The first number represents accuracy direct from the pretrained model. Since none of the subsequent tuning evaluations outperformed the original, no checkpoints were created. One thing you could try: reduce the learning rate, and see if that helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/802
https://github.com/google/deepvariant/issues/802:943,usability,help,helps,943,"@sophienguyen01 - from the log file it looks like everything worked. Here are all the tune/categorical accuracies from your training data. ```. tune/categorical_accuracy=0.9944317936897278. tune/categorical_accuracy=0.9909400343894958. tune/categorical_accuracy=0.9915463924407959. tune/categorical_accuracy=0.9925118088722229. tune/categorical_accuracy=0.9921825528144836. tune/categorical_accuracy=0.9924613237380981. tune/categorical_accuracy=0.9926846623420715. tune/categorical_accuracy=0.9929667711257935. tune/categorical_accuracy=0.9925829172134399. tune/categorical_accuracy=0.9926416277885437. tune/categorical_accuracy=0.9923893213272095. tune/categorical_accuracy=0.9925225377082825. ```. The first number represents accuracy direct from the pretrained model. Since none of the subsequent tuning evaluations outperformed the original, no checkpoints were created. One thing you could try: reduce the learning rate, and see if that helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/802
https://github.com/google/deepvariant/issues/802:78,deployability,updat,updates,78,"Hi @sophienguyen01 , let me know if you have a chance to try and provide some updates here. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/802
https://github.com/google/deepvariant/issues/802:78,safety,updat,updates,78,"Hi @sophienguyen01 , let me know if you have a chance to try and provide some updates here. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/802
https://github.com/google/deepvariant/issues/802:78,security,updat,updates,78,"Hi @sophienguyen01 , let me know if you have a chance to try and provide some updates here. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/802
https://github.com/google/deepvariant/issues/802:169,energy efficiency,model,model,169,"Hi Pichuan,. You can close this issue now. I will try with different samples. I tried to lower the learning rate but it still does not exceed the performance of default model. . I will have to train on different samples. Thanks",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/802
https://github.com/google/deepvariant/issues/802:146,performance,perform,performance,146,"Hi Pichuan,. You can close this issue now. I will try with different samples. I tried to lower the learning rate but it still does not exceed the performance of default model. . I will have to train on different samples. Thanks",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/802
https://github.com/google/deepvariant/issues/802:126,reliability,doe,does,126,"Hi Pichuan,. You can close this issue now. I will try with different samples. I tried to lower the learning rate but it still does not exceed the performance of default model. . I will have to train on different samples. Thanks",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/802
https://github.com/google/deepvariant/issues/802:169,security,model,model,169,"Hi Pichuan,. You can close this issue now. I will try with different samples. I tried to lower the learning rate but it still does not exceed the performance of default model. . I will have to train on different samples. Thanks",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/802
https://github.com/google/deepvariant/issues/802:21,usability,close,close,21,"Hi Pichuan,. You can close this issue now. I will try with different samples. I tried to lower the learning rate but it still does not exceed the performance of default model. . I will have to train on different samples. Thanks",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/802
https://github.com/google/deepvariant/issues/802:99,usability,learn,learning,99,"Hi Pichuan,. You can close this issue now. I will try with different samples. I tried to lower the learning rate but it still does not exceed the performance of default model. . I will have to train on different samples. Thanks",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/802
https://github.com/google/deepvariant/issues/802:146,usability,perform,performance,146,"Hi Pichuan,. You can close this issue now. I will try with different samples. I tried to lower the learning rate but it still does not exceed the performance of default model. . I will have to train on different samples. Thanks",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/802
https://github.com/google/deepvariant/issues/802:107,availability,checkpoint,checkpoint,107,"HI @pichuan, . I trained on a new dataset and run into similar issue. This time there are files created in checkpoint but I still get the same error. Only the first epoch has low tune/categorical_accuracy and the next remaining epoch the accuracy higher than 0.9. I attached the log file here . [train_041924.log](https://github.com/google/deepvariant/files/15082130/train_041924.log). Here is the parameter I used to train: . ```-config.num_epochs=10 \. --config.learning_rate=0.0001 \. --config.num_validation_examples=0 \. --experiment_dir=""model_train"" \. --strategy=mirrored \. --config.batch_size=32 \. ```. Would you take a look and let me know what's going wrong? Thank you",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/802
https://github.com/google/deepvariant/issues/802:143,availability,error,error,143,"HI @pichuan, . I trained on a new dataset and run into similar issue. This time there are files created in checkpoint but I still get the same error. Only the first epoch has low tune/categorical_accuracy and the next remaining epoch the accuracy higher than 0.9. I attached the log file here . [train_041924.log](https://github.com/google/deepvariant/files/15082130/train_041924.log). Here is the parameter I used to train: . ```-config.num_epochs=10 \. --config.learning_rate=0.0001 \. --config.num_validation_examples=0 \. --experiment_dir=""model_train"" \. --strategy=mirrored \. --config.batch_size=32 \. ```. Would you take a look and let me know what's going wrong? Thank you",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/802
https://github.com/google/deepvariant/issues/802:279,deployability,log,log,279,"HI @pichuan, . I trained on a new dataset and run into similar issue. This time there are files created in checkpoint but I still get the same error. Only the first epoch has low tune/categorical_accuracy and the next remaining epoch the accuracy higher than 0.9. I attached the log file here . [train_041924.log](https://github.com/google/deepvariant/files/15082130/train_041924.log). Here is the parameter I used to train: . ```-config.num_epochs=10 \. --config.learning_rate=0.0001 \. --config.num_validation_examples=0 \. --experiment_dir=""model_train"" \. --strategy=mirrored \. --config.batch_size=32 \. ```. Would you take a look and let me know what's going wrong? Thank you",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/802
https://github.com/google/deepvariant/issues/802:309,deployability,log,log,309,"HI @pichuan, . I trained on a new dataset and run into similar issue. This time there are files created in checkpoint but I still get the same error. Only the first epoch has low tune/categorical_accuracy and the next remaining epoch the accuracy higher than 0.9. I attached the log file here . [train_041924.log](https://github.com/google/deepvariant/files/15082130/train_041924.log). Here is the parameter I used to train: . ```-config.num_epochs=10 \. --config.learning_rate=0.0001 \. --config.num_validation_examples=0 \. --experiment_dir=""model_train"" \. --strategy=mirrored \. --config.batch_size=32 \. ```. Would you take a look and let me know what's going wrong? Thank you",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/802
https://github.com/google/deepvariant/issues/802:380,deployability,log,log,380,"HI @pichuan, . I trained on a new dataset and run into similar issue. This time there are files created in checkpoint but I still get the same error. Only the first epoch has low tune/categorical_accuracy and the next remaining epoch the accuracy higher than 0.9. I attached the log file here . [train_041924.log](https://github.com/google/deepvariant/files/15082130/train_041924.log). Here is the parameter I used to train: . ```-config.num_epochs=10 \. --config.learning_rate=0.0001 \. --config.num_validation_examples=0 \. --experiment_dir=""model_train"" \. --strategy=mirrored \. --config.batch_size=32 \. ```. Would you take a look and let me know what's going wrong? Thank you",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/802
https://github.com/google/deepvariant/issues/802:398,modifiability,paramet,parameter,398,"HI @pichuan, . I trained on a new dataset and run into similar issue. This time there are files created in checkpoint but I still get the same error. Only the first epoch has low tune/categorical_accuracy and the next remaining epoch the accuracy higher than 0.9. I attached the log file here . [train_041924.log](https://github.com/google/deepvariant/files/15082130/train_041924.log). Here is the parameter I used to train: . ```-config.num_epochs=10 \. --config.learning_rate=0.0001 \. --config.num_validation_examples=0 \. --experiment_dir=""model_train"" \. --strategy=mirrored \. --config.batch_size=32 \. ```. Would you take a look and let me know what's going wrong? Thank you",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/802
https://github.com/google/deepvariant/issues/802:75,performance,time,time,75,"HI @pichuan, . I trained on a new dataset and run into similar issue. This time there are files created in checkpoint but I still get the same error. Only the first epoch has low tune/categorical_accuracy and the next remaining epoch the accuracy higher than 0.9. I attached the log file here . [train_041924.log](https://github.com/google/deepvariant/files/15082130/train_041924.log). Here is the parameter I used to train: . ```-config.num_epochs=10 \. --config.learning_rate=0.0001 \. --config.num_validation_examples=0 \. --experiment_dir=""model_train"" \. --strategy=mirrored \. --config.batch_size=32 \. ```. Would you take a look and let me know what's going wrong? Thank you",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/802
https://github.com/google/deepvariant/issues/802:143,performance,error,error,143,"HI @pichuan, . I trained on a new dataset and run into similar issue. This time there are files created in checkpoint but I still get the same error. Only the first epoch has low tune/categorical_accuracy and the next remaining epoch the accuracy higher than 0.9. I attached the log file here . [train_041924.log](https://github.com/google/deepvariant/files/15082130/train_041924.log). Here is the parameter I used to train: . ```-config.num_epochs=10 \. --config.learning_rate=0.0001 \. --config.num_validation_examples=0 \. --experiment_dir=""model_train"" \. --strategy=mirrored \. --config.batch_size=32 \. ```. Would you take a look and let me know what's going wrong? Thank you",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/802
https://github.com/google/deepvariant/issues/802:179,performance,tune,tune,179,"HI @pichuan, . I trained on a new dataset and run into similar issue. This time there are files created in checkpoint but I still get the same error. Only the first epoch has low tune/categorical_accuracy and the next remaining epoch the accuracy higher than 0.9. I attached the log file here . [train_041924.log](https://github.com/google/deepvariant/files/15082130/train_041924.log). Here is the parameter I used to train: . ```-config.num_epochs=10 \. --config.learning_rate=0.0001 \. --config.num_validation_examples=0 \. --experiment_dir=""model_train"" \. --strategy=mirrored \. --config.batch_size=32 \. ```. Would you take a look and let me know what's going wrong? Thank you",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/802
https://github.com/google/deepvariant/issues/802:107,reliability,checkpoint,checkpoint,107,"HI @pichuan, . I trained on a new dataset and run into similar issue. This time there are files created in checkpoint but I still get the same error. Only the first epoch has low tune/categorical_accuracy and the next remaining epoch the accuracy higher than 0.9. I attached the log file here . [train_041924.log](https://github.com/google/deepvariant/files/15082130/train_041924.log). Here is the parameter I used to train: . ```-config.num_epochs=10 \. --config.learning_rate=0.0001 \. --config.num_validation_examples=0 \. --experiment_dir=""model_train"" \. --strategy=mirrored \. --config.batch_size=32 \. ```. Would you take a look and let me know what's going wrong? Thank you",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/802
https://github.com/google/deepvariant/issues/802:143,safety,error,error,143,"HI @pichuan, . I trained on a new dataset and run into similar issue. This time there are files created in checkpoint but I still get the same error. Only the first epoch has low tune/categorical_accuracy and the next remaining epoch the accuracy higher than 0.9. I attached the log file here . [train_041924.log](https://github.com/google/deepvariant/files/15082130/train_041924.log). Here is the parameter I used to train: . ```-config.num_epochs=10 \. --config.learning_rate=0.0001 \. --config.num_validation_examples=0 \. --experiment_dir=""model_train"" \. --strategy=mirrored \. --config.batch_size=32 \. ```. Would you take a look and let me know what's going wrong? Thank you",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/802
https://github.com/google/deepvariant/issues/802:279,safety,log,log,279,"HI @pichuan, . I trained on a new dataset and run into similar issue. This time there are files created in checkpoint but I still get the same error. Only the first epoch has low tune/categorical_accuracy and the next remaining epoch the accuracy higher than 0.9. I attached the log file here . [train_041924.log](https://github.com/google/deepvariant/files/15082130/train_041924.log). Here is the parameter I used to train: . ```-config.num_epochs=10 \. --config.learning_rate=0.0001 \. --config.num_validation_examples=0 \. --experiment_dir=""model_train"" \. --strategy=mirrored \. --config.batch_size=32 \. ```. Would you take a look and let me know what's going wrong? Thank you",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/802
https://github.com/google/deepvariant/issues/802:309,safety,log,log,309,"HI @pichuan, . I trained on a new dataset and run into similar issue. This time there are files created in checkpoint but I still get the same error. Only the first epoch has low tune/categorical_accuracy and the next remaining epoch the accuracy higher than 0.9. I attached the log file here . [train_041924.log](https://github.com/google/deepvariant/files/15082130/train_041924.log). Here is the parameter I used to train: . ```-config.num_epochs=10 \. --config.learning_rate=0.0001 \. --config.num_validation_examples=0 \. --experiment_dir=""model_train"" \. --strategy=mirrored \. --config.batch_size=32 \. ```. Would you take a look and let me know what's going wrong? Thank you",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/802
https://github.com/google/deepvariant/issues/802:380,safety,log,log,380,"HI @pichuan, . I trained on a new dataset and run into similar issue. This time there are files created in checkpoint but I still get the same error. Only the first epoch has low tune/categorical_accuracy and the next remaining epoch the accuracy higher than 0.9. I attached the log file here . [train_041924.log](https://github.com/google/deepvariant/files/15082130/train_041924.log). Here is the parameter I used to train: . ```-config.num_epochs=10 \. --config.learning_rate=0.0001 \. --config.num_validation_examples=0 \. --experiment_dir=""model_train"" \. --strategy=mirrored \. --config.batch_size=32 \. ```. Would you take a look and let me know what's going wrong? Thank you",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/802
https://github.com/google/deepvariant/issues/802:279,security,log,log,279,"HI @pichuan, . I trained on a new dataset and run into similar issue. This time there are files created in checkpoint but I still get the same error. Only the first epoch has low tune/categorical_accuracy and the next remaining epoch the accuracy higher than 0.9. I attached the log file here . [train_041924.log](https://github.com/google/deepvariant/files/15082130/train_041924.log). Here is the parameter I used to train: . ```-config.num_epochs=10 \. --config.learning_rate=0.0001 \. --config.num_validation_examples=0 \. --experiment_dir=""model_train"" \. --strategy=mirrored \. --config.batch_size=32 \. ```. Would you take a look and let me know what's going wrong? Thank you",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/802
https://github.com/google/deepvariant/issues/802:309,security,log,log,309,"HI @pichuan, . I trained on a new dataset and run into similar issue. This time there are files created in checkpoint but I still get the same error. Only the first epoch has low tune/categorical_accuracy and the next remaining epoch the accuracy higher than 0.9. I attached the log file here . [train_041924.log](https://github.com/google/deepvariant/files/15082130/train_041924.log). Here is the parameter I used to train: . ```-config.num_epochs=10 \. --config.learning_rate=0.0001 \. --config.num_validation_examples=0 \. --experiment_dir=""model_train"" \. --strategy=mirrored \. --config.batch_size=32 \. ```. Would you take a look and let me know what's going wrong? Thank you",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/802
https://github.com/google/deepvariant/issues/802:380,security,log,log,380,"HI @pichuan, . I trained on a new dataset and run into similar issue. This time there are files created in checkpoint but I still get the same error. Only the first epoch has low tune/categorical_accuracy and the next remaining epoch the accuracy higher than 0.9. I attached the log file here . [train_041924.log](https://github.com/google/deepvariant/files/15082130/train_041924.log). Here is the parameter I used to train: . ```-config.num_epochs=10 \. --config.learning_rate=0.0001 \. --config.num_validation_examples=0 \. --experiment_dir=""model_train"" \. --strategy=mirrored \. --config.batch_size=32 \. ```. Would you take a look and let me know what's going wrong? Thank you",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/802
https://github.com/google/deepvariant/issues/802:279,testability,log,log,279,"HI @pichuan, . I trained on a new dataset and run into similar issue. This time there are files created in checkpoint but I still get the same error. Only the first epoch has low tune/categorical_accuracy and the next remaining epoch the accuracy higher than 0.9. I attached the log file here . [train_041924.log](https://github.com/google/deepvariant/files/15082130/train_041924.log). Here is the parameter I used to train: . ```-config.num_epochs=10 \. --config.learning_rate=0.0001 \. --config.num_validation_examples=0 \. --experiment_dir=""model_train"" \. --strategy=mirrored \. --config.batch_size=32 \. ```. Would you take a look and let me know what's going wrong? Thank you",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/802
https://github.com/google/deepvariant/issues/802:309,testability,log,log,309,"HI @pichuan, . I trained on a new dataset and run into similar issue. This time there are files created in checkpoint but I still get the same error. Only the first epoch has low tune/categorical_accuracy and the next remaining epoch the accuracy higher than 0.9. I attached the log file here . [train_041924.log](https://github.com/google/deepvariant/files/15082130/train_041924.log). Here is the parameter I used to train: . ```-config.num_epochs=10 \. --config.learning_rate=0.0001 \. --config.num_validation_examples=0 \. --experiment_dir=""model_train"" \. --strategy=mirrored \. --config.batch_size=32 \. ```. Would you take a look and let me know what's going wrong? Thank you",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/802
https://github.com/google/deepvariant/issues/802:380,testability,log,log,380,"HI @pichuan, . I trained on a new dataset and run into similar issue. This time there are files created in checkpoint but I still get the same error. Only the first epoch has low tune/categorical_accuracy and the next remaining epoch the accuracy higher than 0.9. I attached the log file here . [train_041924.log](https://github.com/google/deepvariant/files/15082130/train_041924.log). Here is the parameter I used to train: . ```-config.num_epochs=10 \. --config.learning_rate=0.0001 \. --config.num_validation_examples=0 \. --experiment_dir=""model_train"" \. --strategy=mirrored \. --config.batch_size=32 \. ```. Would you take a look and let me know what's going wrong? Thank you",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/802
https://github.com/google/deepvariant/issues/802:143,usability,error,error,143,"HI @pichuan, . I trained on a new dataset and run into similar issue. This time there are files created in checkpoint but I still get the same error. Only the first epoch has low tune/categorical_accuracy and the next remaining epoch the accuracy higher than 0.9. I attached the log file here . [train_041924.log](https://github.com/google/deepvariant/files/15082130/train_041924.log). Here is the parameter I used to train: . ```-config.num_epochs=10 \. --config.learning_rate=0.0001 \. --config.num_validation_examples=0 \. --experiment_dir=""model_train"" \. --strategy=mirrored \. --config.batch_size=32 \. ```. Would you take a look and let me know what's going wrong? Thank you",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/802
https://github.com/google/deepvariant/issues/802:224,availability,checkpoint,checkpoint,224,"Hi @sophienguyen01 , . Is there a reason why you're setting `--config.num_validation_examples=0`? You'll need to have a reasonable amount of num_validation_examples for the model to be able to evaluate and pick a reasonable checkpoint.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/802
https://github.com/google/deepvariant/issues/802:173,energy efficiency,model,model,173,"Hi @sophienguyen01 , . Is there a reason why you're setting `--config.num_validation_examples=0`? You'll need to have a reasonable amount of num_validation_examples for the model to be able to evaluate and pick a reasonable checkpoint.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/802
https://github.com/google/deepvariant/issues/802:224,reliability,checkpoint,checkpoint,224,"Hi @sophienguyen01 , . Is there a reason why you're setting `--config.num_validation_examples=0`? You'll need to have a reasonable amount of num_validation_examples for the model to be able to evaluate and pick a reasonable checkpoint.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/802
https://github.com/google/deepvariant/issues/802:173,security,model,model,173,"Hi @sophienguyen01 , . Is there a reason why you're setting `--config.num_validation_examples=0`? You'll need to have a reasonable amount of num_validation_examples for the model to be able to evaluate and pick a reasonable checkpoint.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/802
https://github.com/google/deepvariant/issues/802:64,safety,valid,validation,64,"According to file dv_config.py : . ```. # If set to 0, use full validation dataset. config.num_validation_examples = 0. ```. Also, the[ training tutorial](https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-training-case-study.md) also use `--config.num_validation_examples=0 `",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/802
https://github.com/google/deepvariant/issues/802:64,security,validat,validation,64,"According to file dv_config.py : . ```. # If set to 0, use full validation dataset. config.num_validation_examples = 0. ```. Also, the[ training tutorial](https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-training-case-study.md) also use `--config.num_validation_examples=0 `",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/802
https://github.com/google/deepvariant/issues/802:68,availability,error,error,68,"Hi @sophienguyen01 , can you specifically point out the line of the error? All the lines in the logs are API warnings, you can safely ignore those.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/802
https://github.com/google/deepvariant/issues/802:96,deployability,log,logs,96,"Hi @sophienguyen01 , can you specifically point out the line of the error? All the lines in the logs are API warnings, you can safely ignore those.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/802
https://github.com/google/deepvariant/issues/802:105,deployability,API,API,105,"Hi @sophienguyen01 , can you specifically point out the line of the error? All the lines in the logs are API warnings, you can safely ignore those.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/802
https://github.com/google/deepvariant/issues/802:105,integrability,API,API,105,"Hi @sophienguyen01 , can you specifically point out the line of the error? All the lines in the logs are API warnings, you can safely ignore those.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/802
https://github.com/google/deepvariant/issues/802:29,interoperability,specif,specifically,29,"Hi @sophienguyen01 , can you specifically point out the line of the error? All the lines in the logs are API warnings, you can safely ignore those.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/802
https://github.com/google/deepvariant/issues/802:105,interoperability,API,API,105,"Hi @sophienguyen01 , can you specifically point out the line of the error? All the lines in the logs are API warnings, you can safely ignore those.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/802
https://github.com/google/deepvariant/issues/802:68,performance,error,error,68,"Hi @sophienguyen01 , can you specifically point out the line of the error? All the lines in the logs are API warnings, you can safely ignore those.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/802
https://github.com/google/deepvariant/issues/802:68,safety,error,error,68,"Hi @sophienguyen01 , can you specifically point out the line of the error? All the lines in the logs are API warnings, you can safely ignore those.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/802
https://github.com/google/deepvariant/issues/802:96,safety,log,logs,96,"Hi @sophienguyen01 , can you specifically point out the line of the error? All the lines in the logs are API warnings, you can safely ignore those.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/802
https://github.com/google/deepvariant/issues/802:127,safety,safe,safely,127,"Hi @sophienguyen01 , can you specifically point out the line of the error? All the lines in the logs are API warnings, you can safely ignore those.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/802
https://github.com/google/deepvariant/issues/802:96,security,log,logs,96,"Hi @sophienguyen01 , can you specifically point out the line of the error? All the lines in the logs are API warnings, you can safely ignore those.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/802
https://github.com/google/deepvariant/issues/802:96,testability,log,logs,96,"Hi @sophienguyen01 , can you specifically point out the line of the error? All the lines in the logs are API warnings, you can safely ignore those.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/802
https://github.com/google/deepvariant/issues/802:68,usability,error,error,68,"Hi @sophienguyen01 , can you specifically point out the line of the error? All the lines in the logs are API warnings, you can safely ignore those.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/802
https://github.com/google/deepvariant/issues/802:34,availability,checkpoint,checkpoints,34,"@sophienguyen01 the logs indicate checkpoints are output:. ```. I0423 18:41:59.026870 139913113728832 train.py:456] Saved checkpoint tune/f1_weighted=0.9114237 step=3352 epoch=1 path=model_train/checkpoints/ckpt-3352. I0423 18:44:53.215049 139913113728832 train.py:456] Saved checkpoint tune/f1_weighted=0.91949123 step=6704 epoch=2 path=model_train/checkpoints/ckpt-6704. I0423 18:47:47.292658 139913113728832 train.py:456] Saved checkpoint tune/f1_weighted=0.92320794 step=10056 epoch=3 path=model_train/checkpoints/ckpt-10056. ```. But as @kishwarshafin suggests, the warnings at the end are normal and can be ignored.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/802
https://github.com/google/deepvariant/issues/802:122,availability,checkpoint,checkpoint,122,"@sophienguyen01 the logs indicate checkpoints are output:. ```. I0423 18:41:59.026870 139913113728832 train.py:456] Saved checkpoint tune/f1_weighted=0.9114237 step=3352 epoch=1 path=model_train/checkpoints/ckpt-3352. I0423 18:44:53.215049 139913113728832 train.py:456] Saved checkpoint tune/f1_weighted=0.91949123 step=6704 epoch=2 path=model_train/checkpoints/ckpt-6704. I0423 18:47:47.292658 139913113728832 train.py:456] Saved checkpoint tune/f1_weighted=0.92320794 step=10056 epoch=3 path=model_train/checkpoints/ckpt-10056. ```. But as @kishwarshafin suggests, the warnings at the end are normal and can be ignored.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/802
https://github.com/google/deepvariant/issues/802:195,availability,checkpoint,checkpoints,195,"@sophienguyen01 the logs indicate checkpoints are output:. ```. I0423 18:41:59.026870 139913113728832 train.py:456] Saved checkpoint tune/f1_weighted=0.9114237 step=3352 epoch=1 path=model_train/checkpoints/ckpt-3352. I0423 18:44:53.215049 139913113728832 train.py:456] Saved checkpoint tune/f1_weighted=0.91949123 step=6704 epoch=2 path=model_train/checkpoints/ckpt-6704. I0423 18:47:47.292658 139913113728832 train.py:456] Saved checkpoint tune/f1_weighted=0.92320794 step=10056 epoch=3 path=model_train/checkpoints/ckpt-10056. ```. But as @kishwarshafin suggests, the warnings at the end are normal and can be ignored.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/802
https://github.com/google/deepvariant/issues/802:276,availability,checkpoint,checkpoint,276,"@sophienguyen01 the logs indicate checkpoints are output:. ```. I0423 18:41:59.026870 139913113728832 train.py:456] Saved checkpoint tune/f1_weighted=0.9114237 step=3352 epoch=1 path=model_train/checkpoints/ckpt-3352. I0423 18:44:53.215049 139913113728832 train.py:456] Saved checkpoint tune/f1_weighted=0.91949123 step=6704 epoch=2 path=model_train/checkpoints/ckpt-6704. I0423 18:47:47.292658 139913113728832 train.py:456] Saved checkpoint tune/f1_weighted=0.92320794 step=10056 epoch=3 path=model_train/checkpoints/ckpt-10056. ```. But as @kishwarshafin suggests, the warnings at the end are normal and can be ignored.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/802
https://github.com/google/deepvariant/issues/802:350,availability,checkpoint,checkpoints,350,"@sophienguyen01 the logs indicate checkpoints are output:. ```. I0423 18:41:59.026870 139913113728832 train.py:456] Saved checkpoint tune/f1_weighted=0.9114237 step=3352 epoch=1 path=model_train/checkpoints/ckpt-3352. I0423 18:44:53.215049 139913113728832 train.py:456] Saved checkpoint tune/f1_weighted=0.91949123 step=6704 epoch=2 path=model_train/checkpoints/ckpt-6704. I0423 18:47:47.292658 139913113728832 train.py:456] Saved checkpoint tune/f1_weighted=0.92320794 step=10056 epoch=3 path=model_train/checkpoints/ckpt-10056. ```. But as @kishwarshafin suggests, the warnings at the end are normal and can be ignored.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/802
https://github.com/google/deepvariant/issues/802:431,availability,checkpoint,checkpoint,431,"@sophienguyen01 the logs indicate checkpoints are output:. ```. I0423 18:41:59.026870 139913113728832 train.py:456] Saved checkpoint tune/f1_weighted=0.9114237 step=3352 epoch=1 path=model_train/checkpoints/ckpt-3352. I0423 18:44:53.215049 139913113728832 train.py:456] Saved checkpoint tune/f1_weighted=0.91949123 step=6704 epoch=2 path=model_train/checkpoints/ckpt-6704. I0423 18:47:47.292658 139913113728832 train.py:456] Saved checkpoint tune/f1_weighted=0.92320794 step=10056 epoch=3 path=model_train/checkpoints/ckpt-10056. ```. But as @kishwarshafin suggests, the warnings at the end are normal and can be ignored.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/802
https://github.com/google/deepvariant/issues/802:506,availability,checkpoint,checkpoints,506,"@sophienguyen01 the logs indicate checkpoints are output:. ```. I0423 18:41:59.026870 139913113728832 train.py:456] Saved checkpoint tune/f1_weighted=0.9114237 step=3352 epoch=1 path=model_train/checkpoints/ckpt-3352. I0423 18:44:53.215049 139913113728832 train.py:456] Saved checkpoint tune/f1_weighted=0.91949123 step=6704 epoch=2 path=model_train/checkpoints/ckpt-6704. I0423 18:47:47.292658 139913113728832 train.py:456] Saved checkpoint tune/f1_weighted=0.92320794 step=10056 epoch=3 path=model_train/checkpoints/ckpt-10056. ```. But as @kishwarshafin suggests, the warnings at the end are normal and can be ignored.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/802
https://github.com/google/deepvariant/issues/802:20,deployability,log,logs,20,"@sophienguyen01 the logs indicate checkpoints are output:. ```. I0423 18:41:59.026870 139913113728832 train.py:456] Saved checkpoint tune/f1_weighted=0.9114237 step=3352 epoch=1 path=model_train/checkpoints/ckpt-3352. I0423 18:44:53.215049 139913113728832 train.py:456] Saved checkpoint tune/f1_weighted=0.91949123 step=6704 epoch=2 path=model_train/checkpoints/ckpt-6704. I0423 18:47:47.292658 139913113728832 train.py:456] Saved checkpoint tune/f1_weighted=0.92320794 step=10056 epoch=3 path=model_train/checkpoints/ckpt-10056. ```. But as @kishwarshafin suggests, the warnings at the end are normal and can be ignored.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/802
https://github.com/google/deepvariant/issues/802:133,performance,tune,tune,133,"@sophienguyen01 the logs indicate checkpoints are output:. ```. I0423 18:41:59.026870 139913113728832 train.py:456] Saved checkpoint tune/f1_weighted=0.9114237 step=3352 epoch=1 path=model_train/checkpoints/ckpt-3352. I0423 18:44:53.215049 139913113728832 train.py:456] Saved checkpoint tune/f1_weighted=0.91949123 step=6704 epoch=2 path=model_train/checkpoints/ckpt-6704. I0423 18:47:47.292658 139913113728832 train.py:456] Saved checkpoint tune/f1_weighted=0.92320794 step=10056 epoch=3 path=model_train/checkpoints/ckpt-10056. ```. But as @kishwarshafin suggests, the warnings at the end are normal and can be ignored.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/802
https://github.com/google/deepvariant/issues/802:287,performance,tune,tune,287,"@sophienguyen01 the logs indicate checkpoints are output:. ```. I0423 18:41:59.026870 139913113728832 train.py:456] Saved checkpoint tune/f1_weighted=0.9114237 step=3352 epoch=1 path=model_train/checkpoints/ckpt-3352. I0423 18:44:53.215049 139913113728832 train.py:456] Saved checkpoint tune/f1_weighted=0.91949123 step=6704 epoch=2 path=model_train/checkpoints/ckpt-6704. I0423 18:47:47.292658 139913113728832 train.py:456] Saved checkpoint tune/f1_weighted=0.92320794 step=10056 epoch=3 path=model_train/checkpoints/ckpt-10056. ```. But as @kishwarshafin suggests, the warnings at the end are normal and can be ignored.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/802
https://github.com/google/deepvariant/issues/802:442,performance,tune,tune,442,"@sophienguyen01 the logs indicate checkpoints are output:. ```. I0423 18:41:59.026870 139913113728832 train.py:456] Saved checkpoint tune/f1_weighted=0.9114237 step=3352 epoch=1 path=model_train/checkpoints/ckpt-3352. I0423 18:44:53.215049 139913113728832 train.py:456] Saved checkpoint tune/f1_weighted=0.91949123 step=6704 epoch=2 path=model_train/checkpoints/ckpt-6704. I0423 18:47:47.292658 139913113728832 train.py:456] Saved checkpoint tune/f1_weighted=0.92320794 step=10056 epoch=3 path=model_train/checkpoints/ckpt-10056. ```. But as @kishwarshafin suggests, the warnings at the end are normal and can be ignored.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/802
https://github.com/google/deepvariant/issues/802:34,reliability,checkpoint,checkpoints,34,"@sophienguyen01 the logs indicate checkpoints are output:. ```. I0423 18:41:59.026870 139913113728832 train.py:456] Saved checkpoint tune/f1_weighted=0.9114237 step=3352 epoch=1 path=model_train/checkpoints/ckpt-3352. I0423 18:44:53.215049 139913113728832 train.py:456] Saved checkpoint tune/f1_weighted=0.91949123 step=6704 epoch=2 path=model_train/checkpoints/ckpt-6704. I0423 18:47:47.292658 139913113728832 train.py:456] Saved checkpoint tune/f1_weighted=0.92320794 step=10056 epoch=3 path=model_train/checkpoints/ckpt-10056. ```. But as @kishwarshafin suggests, the warnings at the end are normal and can be ignored.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/802
https://github.com/google/deepvariant/issues/802:122,reliability,checkpoint,checkpoint,122,"@sophienguyen01 the logs indicate checkpoints are output:. ```. I0423 18:41:59.026870 139913113728832 train.py:456] Saved checkpoint tune/f1_weighted=0.9114237 step=3352 epoch=1 path=model_train/checkpoints/ckpt-3352. I0423 18:44:53.215049 139913113728832 train.py:456] Saved checkpoint tune/f1_weighted=0.91949123 step=6704 epoch=2 path=model_train/checkpoints/ckpt-6704. I0423 18:47:47.292658 139913113728832 train.py:456] Saved checkpoint tune/f1_weighted=0.92320794 step=10056 epoch=3 path=model_train/checkpoints/ckpt-10056. ```. But as @kishwarshafin suggests, the warnings at the end are normal and can be ignored.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/802
https://github.com/google/deepvariant/issues/802:195,reliability,checkpoint,checkpoints,195,"@sophienguyen01 the logs indicate checkpoints are output:. ```. I0423 18:41:59.026870 139913113728832 train.py:456] Saved checkpoint tune/f1_weighted=0.9114237 step=3352 epoch=1 path=model_train/checkpoints/ckpt-3352. I0423 18:44:53.215049 139913113728832 train.py:456] Saved checkpoint tune/f1_weighted=0.91949123 step=6704 epoch=2 path=model_train/checkpoints/ckpt-6704. I0423 18:47:47.292658 139913113728832 train.py:456] Saved checkpoint tune/f1_weighted=0.92320794 step=10056 epoch=3 path=model_train/checkpoints/ckpt-10056. ```. But as @kishwarshafin suggests, the warnings at the end are normal and can be ignored.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/802
https://github.com/google/deepvariant/issues/802:276,reliability,checkpoint,checkpoint,276,"@sophienguyen01 the logs indicate checkpoints are output:. ```. I0423 18:41:59.026870 139913113728832 train.py:456] Saved checkpoint tune/f1_weighted=0.9114237 step=3352 epoch=1 path=model_train/checkpoints/ckpt-3352. I0423 18:44:53.215049 139913113728832 train.py:456] Saved checkpoint tune/f1_weighted=0.91949123 step=6704 epoch=2 path=model_train/checkpoints/ckpt-6704. I0423 18:47:47.292658 139913113728832 train.py:456] Saved checkpoint tune/f1_weighted=0.92320794 step=10056 epoch=3 path=model_train/checkpoints/ckpt-10056. ```. But as @kishwarshafin suggests, the warnings at the end are normal and can be ignored.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/802
https://github.com/google/deepvariant/issues/802:350,reliability,checkpoint,checkpoints,350,"@sophienguyen01 the logs indicate checkpoints are output:. ```. I0423 18:41:59.026870 139913113728832 train.py:456] Saved checkpoint tune/f1_weighted=0.9114237 step=3352 epoch=1 path=model_train/checkpoints/ckpt-3352. I0423 18:44:53.215049 139913113728832 train.py:456] Saved checkpoint tune/f1_weighted=0.91949123 step=6704 epoch=2 path=model_train/checkpoints/ckpt-6704. I0423 18:47:47.292658 139913113728832 train.py:456] Saved checkpoint tune/f1_weighted=0.92320794 step=10056 epoch=3 path=model_train/checkpoints/ckpt-10056. ```. But as @kishwarshafin suggests, the warnings at the end are normal and can be ignored.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/802
https://github.com/google/deepvariant/issues/802:431,reliability,checkpoint,checkpoint,431,"@sophienguyen01 the logs indicate checkpoints are output:. ```. I0423 18:41:59.026870 139913113728832 train.py:456] Saved checkpoint tune/f1_weighted=0.9114237 step=3352 epoch=1 path=model_train/checkpoints/ckpt-3352. I0423 18:44:53.215049 139913113728832 train.py:456] Saved checkpoint tune/f1_weighted=0.91949123 step=6704 epoch=2 path=model_train/checkpoints/ckpt-6704. I0423 18:47:47.292658 139913113728832 train.py:456] Saved checkpoint tune/f1_weighted=0.92320794 step=10056 epoch=3 path=model_train/checkpoints/ckpt-10056. ```. But as @kishwarshafin suggests, the warnings at the end are normal and can be ignored.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/802
https://github.com/google/deepvariant/issues/802:506,reliability,checkpoint,checkpoints,506,"@sophienguyen01 the logs indicate checkpoints are output:. ```. I0423 18:41:59.026870 139913113728832 train.py:456] Saved checkpoint tune/f1_weighted=0.9114237 step=3352 epoch=1 path=model_train/checkpoints/ckpt-3352. I0423 18:44:53.215049 139913113728832 train.py:456] Saved checkpoint tune/f1_weighted=0.91949123 step=6704 epoch=2 path=model_train/checkpoints/ckpt-6704. I0423 18:47:47.292658 139913113728832 train.py:456] Saved checkpoint tune/f1_weighted=0.92320794 step=10056 epoch=3 path=model_train/checkpoints/ckpt-10056. ```. But as @kishwarshafin suggests, the warnings at the end are normal and can be ignored.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/802
https://github.com/google/deepvariant/issues/802:20,safety,log,logs,20,"@sophienguyen01 the logs indicate checkpoints are output:. ```. I0423 18:41:59.026870 139913113728832 train.py:456] Saved checkpoint tune/f1_weighted=0.9114237 step=3352 epoch=1 path=model_train/checkpoints/ckpt-3352. I0423 18:44:53.215049 139913113728832 train.py:456] Saved checkpoint tune/f1_weighted=0.91949123 step=6704 epoch=2 path=model_train/checkpoints/ckpt-6704. I0423 18:47:47.292658 139913113728832 train.py:456] Saved checkpoint tune/f1_weighted=0.92320794 step=10056 epoch=3 path=model_train/checkpoints/ckpt-10056. ```. But as @kishwarshafin suggests, the warnings at the end are normal and can be ignored.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/802
https://github.com/google/deepvariant/issues/802:20,security,log,logs,20,"@sophienguyen01 the logs indicate checkpoints are output:. ```. I0423 18:41:59.026870 139913113728832 train.py:456] Saved checkpoint tune/f1_weighted=0.9114237 step=3352 epoch=1 path=model_train/checkpoints/ckpt-3352. I0423 18:44:53.215049 139913113728832 train.py:456] Saved checkpoint tune/f1_weighted=0.91949123 step=6704 epoch=2 path=model_train/checkpoints/ckpt-6704. I0423 18:47:47.292658 139913113728832 train.py:456] Saved checkpoint tune/f1_weighted=0.92320794 step=10056 epoch=3 path=model_train/checkpoints/ckpt-10056. ```. But as @kishwarshafin suggests, the warnings at the end are normal and can be ignored.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/802
https://github.com/google/deepvariant/issues/802:20,testability,log,logs,20,"@sophienguyen01 the logs indicate checkpoints are output:. ```. I0423 18:41:59.026870 139913113728832 train.py:456] Saved checkpoint tune/f1_weighted=0.9114237 step=3352 epoch=1 path=model_train/checkpoints/ckpt-3352. I0423 18:44:53.215049 139913113728832 train.py:456] Saved checkpoint tune/f1_weighted=0.91949123 step=6704 epoch=2 path=model_train/checkpoints/ckpt-6704. I0423 18:47:47.292658 139913113728832 train.py:456] Saved checkpoint tune/f1_weighted=0.92320794 step=10056 epoch=3 path=model_train/checkpoints/ckpt-10056. ```. But as @kishwarshafin suggests, the warnings at the end are normal and can be ignored.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/802
https://github.com/google/deepvariant/issues/802:25,usability,indicat,indicate,25,"@sophienguyen01 the logs indicate checkpoints are output:. ```. I0423 18:41:59.026870 139913113728832 train.py:456] Saved checkpoint tune/f1_weighted=0.9114237 step=3352 epoch=1 path=model_train/checkpoints/ckpt-3352. I0423 18:44:53.215049 139913113728832 train.py:456] Saved checkpoint tune/f1_weighted=0.91949123 step=6704 epoch=2 path=model_train/checkpoints/ckpt-6704. I0423 18:47:47.292658 139913113728832 train.py:456] Saved checkpoint tune/f1_weighted=0.92320794 step=10056 epoch=3 path=model_train/checkpoints/ckpt-10056. ```. But as @kishwarshafin suggests, the warnings at the end are normal and can be ignored.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/802
https://github.com/google/deepvariant/issues/802:48,availability,checkpoint,checkpoints,48,"Thank you for your input, I am able to find the checkpoints with this training.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/802
https://github.com/google/deepvariant/issues/802:48,reliability,checkpoint,checkpoints,48,"Thank you for your input, I am able to find the checkpoints with this training.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/802
https://github.com/google/deepvariant/issues/802:19,safety,input,input,19,"Thank you for your input, I am able to find the checkpoints with this training.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/802
https://github.com/google/deepvariant/issues/802:19,usability,input,input,19,"Thank you for your input, I am able to find the checkpoints with this training.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/802
https://github.com/google/deepvariant/issues/803:1018,deployability,contain,containing,1018,"The `postprocess_variants` may OOM because it loads all variant calls into memory in order to sort them. We are actively working on addressing this. That said, you can re-start each of the 3 steps individually by calling them directly. Instead of running `run_deeptrio`, e.g. ```. singularity run. google/deepvariant:deeptrio-""${BIN_VERSION}"" \. /opt/deepvariant/bin/deeptrio/run_deeptrio. ```. you can run `postprocess_variants`. ```. singularity run. google/deepvariant:deeptrio-""${BIN_VERSION}"" \. /opt/deepvariant/bin/postprocess_variants. ```. Please note that the [flags](https://github.com/google/deepvariant/blob/r1.6.1/scripts/run_deeptrio.py#L633-L668) will differ. At minimum you will need to set `--ref`, `--infile` and `--outfile`. The `--infile` is the output from `call_variants`, which has the following [pattern](https://github.com/google/deepvariant/blob/r1.6.1/scripts/run_deeptrio.py#L790-L794), e.g. `call_variants_output_parent1.tfrecord.gz`. You should be able to find them in the tmp directory containing all the make_examples files. Hopefully that helps!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/803
https://github.com/google/deepvariant/issues/803:46,energy efficiency,load,loads,46,"The `postprocess_variants` may OOM because it loads all variant calls into memory in order to sort them. We are actively working on addressing this. That said, you can re-start each of the 3 steps individually by calling them directly. Instead of running `run_deeptrio`, e.g. ```. singularity run. google/deepvariant:deeptrio-""${BIN_VERSION}"" \. /opt/deepvariant/bin/deeptrio/run_deeptrio. ```. you can run `postprocess_variants`. ```. singularity run. google/deepvariant:deeptrio-""${BIN_VERSION}"" \. /opt/deepvariant/bin/postprocess_variants. ```. Please note that the [flags](https://github.com/google/deepvariant/blob/r1.6.1/scripts/run_deeptrio.py#L633-L668) will differ. At minimum you will need to set `--ref`, `--infile` and `--outfile`. The `--infile` is the output from `call_variants`, which has the following [pattern](https://github.com/google/deepvariant/blob/r1.6.1/scripts/run_deeptrio.py#L790-L794), e.g. `call_variants_output_parent1.tfrecord.gz`. You should be able to find them in the tmp directory containing all the make_examples files. Hopefully that helps!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/803
https://github.com/google/deepvariant/issues/803:46,performance,load,loads,46,"The `postprocess_variants` may OOM because it loads all variant calls into memory in order to sort them. We are actively working on addressing this. That said, you can re-start each of the 3 steps individually by calling them directly. Instead of running `run_deeptrio`, e.g. ```. singularity run. google/deepvariant:deeptrio-""${BIN_VERSION}"" \. /opt/deepvariant/bin/deeptrio/run_deeptrio. ```. you can run `postprocess_variants`. ```. singularity run. google/deepvariant:deeptrio-""${BIN_VERSION}"" \. /opt/deepvariant/bin/postprocess_variants. ```. Please note that the [flags](https://github.com/google/deepvariant/blob/r1.6.1/scripts/run_deeptrio.py#L633-L668) will differ. At minimum you will need to set `--ref`, `--infile` and `--outfile`. The `--infile` is the output from `call_variants`, which has the following [pattern](https://github.com/google/deepvariant/blob/r1.6.1/scripts/run_deeptrio.py#L790-L794), e.g. `call_variants_output_parent1.tfrecord.gz`. You should be able to find them in the tmp directory containing all the make_examples files. Hopefully that helps!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/803
https://github.com/google/deepvariant/issues/803:75,performance,memor,memory,75,"The `postprocess_variants` may OOM because it loads all variant calls into memory in order to sort them. We are actively working on addressing this. That said, you can re-start each of the 3 steps individually by calling them directly. Instead of running `run_deeptrio`, e.g. ```. singularity run. google/deepvariant:deeptrio-""${BIN_VERSION}"" \. /opt/deepvariant/bin/deeptrio/run_deeptrio. ```. you can run `postprocess_variants`. ```. singularity run. google/deepvariant:deeptrio-""${BIN_VERSION}"" \. /opt/deepvariant/bin/postprocess_variants. ```. Please note that the [flags](https://github.com/google/deepvariant/blob/r1.6.1/scripts/run_deeptrio.py#L633-L668) will differ. At minimum you will need to set `--ref`, `--infile` and `--outfile`. The `--infile` is the output from `call_variants`, which has the following [pattern](https://github.com/google/deepvariant/blob/r1.6.1/scripts/run_deeptrio.py#L790-L794), e.g. `call_variants_output_parent1.tfrecord.gz`. You should be able to find them in the tmp directory containing all the make_examples files. Hopefully that helps!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/803
https://github.com/google/deepvariant/issues/803:75,usability,memor,memory,75,"The `postprocess_variants` may OOM because it loads all variant calls into memory in order to sort them. We are actively working on addressing this. That said, you can re-start each of the 3 steps individually by calling them directly. Instead of running `run_deeptrio`, e.g. ```. singularity run. google/deepvariant:deeptrio-""${BIN_VERSION}"" \. /opt/deepvariant/bin/deeptrio/run_deeptrio. ```. you can run `postprocess_variants`. ```. singularity run. google/deepvariant:deeptrio-""${BIN_VERSION}"" \. /opt/deepvariant/bin/postprocess_variants. ```. Please note that the [flags](https://github.com/google/deepvariant/blob/r1.6.1/scripts/run_deeptrio.py#L633-L668) will differ. At minimum you will need to set `--ref`, `--infile` and `--outfile`. The `--infile` is the output from `call_variants`, which has the following [pattern](https://github.com/google/deepvariant/blob/r1.6.1/scripts/run_deeptrio.py#L790-L794), e.g. `call_variants_output_parent1.tfrecord.gz`. You should be able to find them in the tmp directory containing all the make_examples files. Hopefully that helps!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/803
https://github.com/google/deepvariant/issues/803:679,usability,minim,minimum,679,"The `postprocess_variants` may OOM because it loads all variant calls into memory in order to sort them. We are actively working on addressing this. That said, you can re-start each of the 3 steps individually by calling them directly. Instead of running `run_deeptrio`, e.g. ```. singularity run. google/deepvariant:deeptrio-""${BIN_VERSION}"" \. /opt/deepvariant/bin/deeptrio/run_deeptrio. ```. you can run `postprocess_variants`. ```. singularity run. google/deepvariant:deeptrio-""${BIN_VERSION}"" \. /opt/deepvariant/bin/postprocess_variants. ```. Please note that the [flags](https://github.com/google/deepvariant/blob/r1.6.1/scripts/run_deeptrio.py#L633-L668) will differ. At minimum you will need to set `--ref`, `--infile` and `--outfile`. The `--infile` is the output from `call_variants`, which has the following [pattern](https://github.com/google/deepvariant/blob/r1.6.1/scripts/run_deeptrio.py#L790-L794), e.g. `call_variants_output_parent1.tfrecord.gz`. You should be able to find them in the tmp directory containing all the make_examples files. Hopefully that helps!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/803
https://github.com/google/deepvariant/issues/803:1073,usability,help,helps,1073,"The `postprocess_variants` may OOM because it loads all variant calls into memory in order to sort them. We are actively working on addressing this. That said, you can re-start each of the 3 steps individually by calling them directly. Instead of running `run_deeptrio`, e.g. ```. singularity run. google/deepvariant:deeptrio-""${BIN_VERSION}"" \. /opt/deepvariant/bin/deeptrio/run_deeptrio. ```. you can run `postprocess_variants`. ```. singularity run. google/deepvariant:deeptrio-""${BIN_VERSION}"" \. /opt/deepvariant/bin/postprocess_variants. ```. Please note that the [flags](https://github.com/google/deepvariant/blob/r1.6.1/scripts/run_deeptrio.py#L633-L668) will differ. At minimum you will need to set `--ref`, `--infile` and `--outfile`. The `--infile` is the output from `call_variants`, which has the following [pattern](https://github.com/google/deepvariant/blob/r1.6.1/scripts/run_deeptrio.py#L790-L794), e.g. `call_variants_output_parent1.tfrecord.gz`. You should be able to find them in the tmp directory containing all the make_examples files. Hopefully that helps!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/803
https://github.com/google/deepvariant/issues/803:92,deployability,pipelin,pipeline,92,"Also, after that step, is there anything else I need to run or is that the last step of the pipeline?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/803
https://github.com/google/deepvariant/issues/803:92,integrability,pipelin,pipeline,92,"Also, after that step, is there anything else I need to run or is that the last step of the pipeline?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/803
https://github.com/google/deepvariant/issues/804:24,availability,down,down,24,"I solve this by shuting down multiprocessing (`--cpus ""0""`) using the following command. ```bash. postprocess_variants --cpus ""0"" --ref ""/input/GCA_000001405.15_GRCh38_no_alt_analysis_set.fasta"" --infile ""/tmp/call_variants_output.tfrecord.gz"" --outfile ""/input/{OUTVCF.gz}"" . ```. But I still not sure what cuase Broken pipe lipe. I also tried less core like `--cpu ""12""` still get `BrokenPipeError: [Errno 32] Broken pipe`.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/804
https://github.com/google/deepvariant/issues/804:49,energy efficiency,cpu,cpus,49,"I solve this by shuting down multiprocessing (`--cpus ""0""`) using the following command. ```bash. postprocess_variants --cpus ""0"" --ref ""/input/GCA_000001405.15_GRCh38_no_alt_analysis_set.fasta"" --infile ""/tmp/call_variants_output.tfrecord.gz"" --outfile ""/input/{OUTVCF.gz}"" . ```. But I still not sure what cuase Broken pipe lipe. I also tried less core like `--cpu ""12""` still get `BrokenPipeError: [Errno 32] Broken pipe`.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/804
https://github.com/google/deepvariant/issues/804:121,energy efficiency,cpu,cpus,121,"I solve this by shuting down multiprocessing (`--cpus ""0""`) using the following command. ```bash. postprocess_variants --cpus ""0"" --ref ""/input/GCA_000001405.15_GRCh38_no_alt_analysis_set.fasta"" --infile ""/tmp/call_variants_output.tfrecord.gz"" --outfile ""/input/{OUTVCF.gz}"" . ```. But I still not sure what cuase Broken pipe lipe. I also tried less core like `--cpu ""12""` still get `BrokenPipeError: [Errno 32] Broken pipe`.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/804
https://github.com/google/deepvariant/issues/804:350,energy efficiency,core,core,350,"I solve this by shuting down multiprocessing (`--cpus ""0""`) using the following command. ```bash. postprocess_variants --cpus ""0"" --ref ""/input/GCA_000001405.15_GRCh38_no_alt_analysis_set.fasta"" --infile ""/tmp/call_variants_output.tfrecord.gz"" --outfile ""/input/{OUTVCF.gz}"" . ```. But I still not sure what cuase Broken pipe lipe. I also tried less core like `--cpu ""12""` still get `BrokenPipeError: [Errno 32] Broken pipe`.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/804
https://github.com/google/deepvariant/issues/804:363,energy efficiency,cpu,cpu,363,"I solve this by shuting down multiprocessing (`--cpus ""0""`) using the following command. ```bash. postprocess_variants --cpus ""0"" --ref ""/input/GCA_000001405.15_GRCh38_no_alt_analysis_set.fasta"" --infile ""/tmp/call_variants_output.tfrecord.gz"" --outfile ""/input/{OUTVCF.gz}"" . ```. But I still not sure what cuase Broken pipe lipe. I also tried less core like `--cpu ""12""` still get `BrokenPipeError: [Errno 32] Broken pipe`.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/804
https://github.com/google/deepvariant/issues/804:49,performance,cpu,cpus,49,"I solve this by shuting down multiprocessing (`--cpus ""0""`) using the following command. ```bash. postprocess_variants --cpus ""0"" --ref ""/input/GCA_000001405.15_GRCh38_no_alt_analysis_set.fasta"" --infile ""/tmp/call_variants_output.tfrecord.gz"" --outfile ""/input/{OUTVCF.gz}"" . ```. But I still not sure what cuase Broken pipe lipe. I also tried less core like `--cpu ""12""` still get `BrokenPipeError: [Errno 32] Broken pipe`.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/804
https://github.com/google/deepvariant/issues/804:121,performance,cpu,cpus,121,"I solve this by shuting down multiprocessing (`--cpus ""0""`) using the following command. ```bash. postprocess_variants --cpus ""0"" --ref ""/input/GCA_000001405.15_GRCh38_no_alt_analysis_set.fasta"" --infile ""/tmp/call_variants_output.tfrecord.gz"" --outfile ""/input/{OUTVCF.gz}"" . ```. But I still not sure what cuase Broken pipe lipe. I also tried less core like `--cpu ""12""` still get `BrokenPipeError: [Errno 32] Broken pipe`.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/804
https://github.com/google/deepvariant/issues/804:363,performance,cpu,cpu,363,"I solve this by shuting down multiprocessing (`--cpus ""0""`) using the following command. ```bash. postprocess_variants --cpus ""0"" --ref ""/input/GCA_000001405.15_GRCh38_no_alt_analysis_set.fasta"" --infile ""/tmp/call_variants_output.tfrecord.gz"" --outfile ""/input/{OUTVCF.gz}"" . ```. But I still not sure what cuase Broken pipe lipe. I also tried less core like `--cpu ""12""` still get `BrokenPipeError: [Errno 32] Broken pipe`.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/804
https://github.com/google/deepvariant/issues/804:138,safety,input,input,138,"I solve this by shuting down multiprocessing (`--cpus ""0""`) using the following command. ```bash. postprocess_variants --cpus ""0"" --ref ""/input/GCA_000001405.15_GRCh38_no_alt_analysis_set.fasta"" --infile ""/tmp/call_variants_output.tfrecord.gz"" --outfile ""/input/{OUTVCF.gz}"" . ```. But I still not sure what cuase Broken pipe lipe. I also tried less core like `--cpu ""12""` still get `BrokenPipeError: [Errno 32] Broken pipe`.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/804
https://github.com/google/deepvariant/issues/804:256,safety,input,input,256,"I solve this by shuting down multiprocessing (`--cpus ""0""`) using the following command. ```bash. postprocess_variants --cpus ""0"" --ref ""/input/GCA_000001405.15_GRCh38_no_alt_analysis_set.fasta"" --infile ""/tmp/call_variants_output.tfrecord.gz"" --outfile ""/input/{OUTVCF.gz}"" . ```. But I still not sure what cuase Broken pipe lipe. I also tried less core like `--cpu ""12""` still get `BrokenPipeError: [Errno 32] Broken pipe`.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/804
https://github.com/google/deepvariant/issues/804:80,usability,command,command,80,"I solve this by shuting down multiprocessing (`--cpus ""0""`) using the following command. ```bash. postprocess_variants --cpus ""0"" --ref ""/input/GCA_000001405.15_GRCh38_no_alt_analysis_set.fasta"" --infile ""/tmp/call_variants_output.tfrecord.gz"" --outfile ""/input/{OUTVCF.gz}"" . ```. But I still not sure what cuase Broken pipe lipe. I also tried less core like `--cpu ""12""` still get `BrokenPipeError: [Errno 32] Broken pipe`.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/804
https://github.com/google/deepvariant/issues/804:138,usability,input,input,138,"I solve this by shuting down multiprocessing (`--cpus ""0""`) using the following command. ```bash. postprocess_variants --cpus ""0"" --ref ""/input/GCA_000001405.15_GRCh38_no_alt_analysis_set.fasta"" --infile ""/tmp/call_variants_output.tfrecord.gz"" --outfile ""/input/{OUTVCF.gz}"" . ```. But I still not sure what cuase Broken pipe lipe. I also tried less core like `--cpu ""12""` still get `BrokenPipeError: [Errno 32] Broken pipe`.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/804
https://github.com/google/deepvariant/issues/804:256,usability,input,input,256,"I solve this by shuting down multiprocessing (`--cpus ""0""`) using the following command. ```bash. postprocess_variants --cpus ""0"" --ref ""/input/GCA_000001405.15_GRCh38_no_alt_analysis_set.fasta"" --infile ""/tmp/call_variants_output.tfrecord.gz"" --outfile ""/input/{OUTVCF.gz}"" . ```. But I still not sure what cuase Broken pipe lipe. I also tried less core like `--cpu ""12""` still get `BrokenPipeError: [Errno 32] Broken pipe`.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/804
https://github.com/google/deepvariant/issues/804:26,interoperability,share,share,26,Is it possible for you to share `/tmp/call_variants_output.tfrecord.gz` (you can send it to lucasbrambrink@google.com)? I would be very interested in trying to reproduce the issue myself. I have a theory as to what may be the root cause.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/804
https://github.com/google/deepvariant/issues/804:940,testability,coverag,coverage,940,"@lucasbrambrink Thank you for you response, I do kept those files but there are 16 of them, listed as below. ```. call_variants_output-00000-of-00016.tfrecord.gz. call_variants_output-00001-of-00016.tfrecord.gz. call_variants_output-00002-of-00016.tfrecord.gz. call_variants_output-00003-of-00016.tfrecord.gz. call_variants_output-00004-of-00016.tfrecord.gz. call_variants_output-00005-of-00016.tfrecord.gz. call_variants_output-00006-of-00016.tfrecord.gz. call_variants_output-00007-of-00016.tfrecord.gz. call_variants_output-00008-of-00016.tfrecord.gz. call_variants_output-00009-of-00016.tfrecord.gz. call_variants_output-00010-of-00016.tfrecord.gz. call_variants_output-00011-of-00016.tfrecord.gz. call_variants_output-00012-of-00016.tfrecord.gz. call_variants_output-00013-of-00016.tfrecord.gz. call_variants_output-00014-of-00016.tfrecord.gz. call_variants_output-00015-of-00016.tfrecord.gz. ```. each is about 200 MB due to the high coverage of the sequencing, do you want all of them?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/804
https://github.com/google/deepvariant/issues/804:1,deployability,Log,LogCrab,1,@LogCrab Thank you for providing the files. We've tried to reproduce this in various ways without success. Could you provide the specs of the machine you ran this on? Thank you for your patience!,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/804
https://github.com/google/deepvariant/issues/804:1,safety,Log,LogCrab,1,@LogCrab Thank you for providing the files. We've tried to reproduce this in various ways without success. Could you provide the specs of the machine you ran this on? Thank you for your patience!,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/804
https://github.com/google/deepvariant/issues/804:1,security,Log,LogCrab,1,@LogCrab Thank you for providing the files. We've tried to reproduce this in various ways without success. Could you provide the specs of the machine you ran this on? Thank you for your patience!,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/804
https://github.com/google/deepvariant/issues/804:1,testability,Log,LogCrab,1,@LogCrab Thank you for providing the files. We've tried to reproduce this in various ways without success. Could you provide the specs of the machine you ran this on? Thank you for your patience!,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/804
https://github.com/google/deepvariant/issues/804:88,deployability,version,version,88,"@lucasbrambrink Sorry the the dealy. My machine is running Ubuntu 22.04. The `cat /proc/version` output is. `Linux version 6.2.0-35-generic (buildd@bos03-amd64-016) (x86_64-linux-gnu-gcc-11 (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0, GNU ld (GNU Binutils for Ubuntu) 2.38) #35~22.04.1-Ubuntu SMP PREEMPT_DYNAMIC Fri Oct 6 10:23:26 UTC 2`. In terms of hardware are dual Intel Platium 8352Y, dual RTX 3090 and 512 G of RAM and full HDD array. Hope this can help.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/804
https://github.com/google/deepvariant/issues/804:115,deployability,version,version,115,"@lucasbrambrink Sorry the the dealy. My machine is running Ubuntu 22.04. The `cat /proc/version` output is. `Linux version 6.2.0-35-generic (buildd@bos03-amd64-016) (x86_64-linux-gnu-gcc-11 (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0, GNU ld (GNU Binutils for Ubuntu) 2.38) #35~22.04.1-Ubuntu SMP PREEMPT_DYNAMIC Fri Oct 6 10:23:26 UTC 2`. In terms of hardware are dual Intel Platium 8352Y, dual RTX 3090 and 512 G of RAM and full HDD array. Hope this can help.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/804
https://github.com/google/deepvariant/issues/804:141,deployability,build,buildd,141,"@lucasbrambrink Sorry the the dealy. My machine is running Ubuntu 22.04. The `cat /proc/version` output is. `Linux version 6.2.0-35-generic (buildd@bos03-amd64-016) (x86_64-linux-gnu-gcc-11 (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0, GNU ld (GNU Binutils for Ubuntu) 2.38) #35~22.04.1-Ubuntu SMP PREEMPT_DYNAMIC Fri Oct 6 10:23:26 UTC 2`. In terms of hardware are dual Intel Platium 8352Y, dual RTX 3090 and 512 G of RAM and full HDD array. Hope this can help.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/804
https://github.com/google/deepvariant/issues/804:88,integrability,version,version,88,"@lucasbrambrink Sorry the the dealy. My machine is running Ubuntu 22.04. The `cat /proc/version` output is. `Linux version 6.2.0-35-generic (buildd@bos03-amd64-016) (x86_64-linux-gnu-gcc-11 (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0, GNU ld (GNU Binutils for Ubuntu) 2.38) #35~22.04.1-Ubuntu SMP PREEMPT_DYNAMIC Fri Oct 6 10:23:26 UTC 2`. In terms of hardware are dual Intel Platium 8352Y, dual RTX 3090 and 512 G of RAM and full HDD array. Hope this can help.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/804
https://github.com/google/deepvariant/issues/804:115,integrability,version,version,115,"@lucasbrambrink Sorry the the dealy. My machine is running Ubuntu 22.04. The `cat /proc/version` output is. `Linux version 6.2.0-35-generic (buildd@bos03-amd64-016) (x86_64-linux-gnu-gcc-11 (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0, GNU ld (GNU Binutils for Ubuntu) 2.38) #35~22.04.1-Ubuntu SMP PREEMPT_DYNAMIC Fri Oct 6 10:23:26 UTC 2`. In terms of hardware are dual Intel Platium 8352Y, dual RTX 3090 and 512 G of RAM and full HDD array. Hope this can help.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/804
https://github.com/google/deepvariant/issues/804:88,modifiability,version,version,88,"@lucasbrambrink Sorry the the dealy. My machine is running Ubuntu 22.04. The `cat /proc/version` output is. `Linux version 6.2.0-35-generic (buildd@bos03-amd64-016) (x86_64-linux-gnu-gcc-11 (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0, GNU ld (GNU Binutils for Ubuntu) 2.38) #35~22.04.1-Ubuntu SMP PREEMPT_DYNAMIC Fri Oct 6 10:23:26 UTC 2`. In terms of hardware are dual Intel Platium 8352Y, dual RTX 3090 and 512 G of RAM and full HDD array. Hope this can help.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/804
https://github.com/google/deepvariant/issues/804:115,modifiability,version,version,115,"@lucasbrambrink Sorry the the dealy. My machine is running Ubuntu 22.04. The `cat /proc/version` output is. `Linux version 6.2.0-35-generic (buildd@bos03-amd64-016) (x86_64-linux-gnu-gcc-11 (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0, GNU ld (GNU Binutils for Ubuntu) 2.38) #35~22.04.1-Ubuntu SMP PREEMPT_DYNAMIC Fri Oct 6 10:23:26 UTC 2`. In terms of hardware are dual Intel Platium 8352Y, dual RTX 3090 and 512 G of RAM and full HDD array. Hope this can help.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/804
https://github.com/google/deepvariant/issues/804:450,usability,help,help,450,"@lucasbrambrink Sorry the the dealy. My machine is running Ubuntu 22.04. The `cat /proc/version` output is. `Linux version 6.2.0-35-generic (buildd@bos03-amd64-016) (x86_64-linux-gnu-gcc-11 (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0, GNU ld (GNU Binutils for Ubuntu) 2.38) #35~22.04.1-Ubuntu SMP PREEMPT_DYNAMIC Fri Oct 6 10:23:26 UTC 2`. In terms of hardware are dual Intel Platium 8352Y, dual RTX 3090 and 512 G of RAM and full HDD array. Hope this can help.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/804
https://github.com/google/deepvariant/issues/804:45,availability,error,error,45,"So we were unable to reproduce this specific error. Regardless, we are overhauling how multiprocessing is used in `postprocess_variants` with our next release, which will very likely avoid this type of error. I am closing this issue for now. If someone is experiencing this issue and would like an experimental docker container to run, please comment on this issue and we will provide one!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/804
https://github.com/google/deepvariant/issues/804:202,availability,error,error,202,"So we were unable to reproduce this specific error. Regardless, we are overhauling how multiprocessing is used in `postprocess_variants` with our next release, which will very likely avoid this type of error. I am closing this issue for now. If someone is experiencing this issue and would like an experimental docker container to run, please comment on this issue and we will provide one!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/804
https://github.com/google/deepvariant/issues/804:151,deployability,releas,release,151,"So we were unable to reproduce this specific error. Regardless, we are overhauling how multiprocessing is used in `postprocess_variants` with our next release, which will very likely avoid this type of error. I am closing this issue for now. If someone is experiencing this issue and would like an experimental docker container to run, please comment on this issue and we will provide one!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/804
https://github.com/google/deepvariant/issues/804:318,deployability,contain,container,318,"So we were unable to reproduce this specific error. Regardless, we are overhauling how multiprocessing is used in `postprocess_variants` with our next release, which will very likely avoid this type of error. I am closing this issue for now. If someone is experiencing this issue and would like an experimental docker container to run, please comment on this issue and we will provide one!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/804
https://github.com/google/deepvariant/issues/804:36,interoperability,specif,specific,36,"So we were unable to reproduce this specific error. Regardless, we are overhauling how multiprocessing is used in `postprocess_variants` with our next release, which will very likely avoid this type of error. I am closing this issue for now. If someone is experiencing this issue and would like an experimental docker container to run, please comment on this issue and we will provide one!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/804
https://github.com/google/deepvariant/issues/804:45,performance,error,error,45,"So we were unable to reproduce this specific error. Regardless, we are overhauling how multiprocessing is used in `postprocess_variants` with our next release, which will very likely avoid this type of error. I am closing this issue for now. If someone is experiencing this issue and would like an experimental docker container to run, please comment on this issue and we will provide one!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/804
https://github.com/google/deepvariant/issues/804:202,performance,error,error,202,"So we were unable to reproduce this specific error. Regardless, we are overhauling how multiprocessing is used in `postprocess_variants` with our next release, which will very likely avoid this type of error. I am closing this issue for now. If someone is experiencing this issue and would like an experimental docker container to run, please comment on this issue and we will provide one!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/804
https://github.com/google/deepvariant/issues/804:45,safety,error,error,45,"So we were unable to reproduce this specific error. Regardless, we are overhauling how multiprocessing is used in `postprocess_variants` with our next release, which will very likely avoid this type of error. I am closing this issue for now. If someone is experiencing this issue and would like an experimental docker container to run, please comment on this issue and we will provide one!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/804
https://github.com/google/deepvariant/issues/804:183,safety,avoid,avoid,183,"So we were unable to reproduce this specific error. Regardless, we are overhauling how multiprocessing is used in `postprocess_variants` with our next release, which will very likely avoid this type of error. I am closing this issue for now. If someone is experiencing this issue and would like an experimental docker container to run, please comment on this issue and we will provide one!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/804
https://github.com/google/deepvariant/issues/804:202,safety,error,error,202,"So we were unable to reproduce this specific error. Regardless, we are overhauling how multiprocessing is used in `postprocess_variants` with our next release, which will very likely avoid this type of error. I am closing this issue for now. If someone is experiencing this issue and would like an experimental docker container to run, please comment on this issue and we will provide one!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/804
https://github.com/google/deepvariant/issues/804:45,usability,error,error,45,"So we were unable to reproduce this specific error. Regardless, we are overhauling how multiprocessing is used in `postprocess_variants` with our next release, which will very likely avoid this type of error. I am closing this issue for now. If someone is experiencing this issue and would like an experimental docker container to run, please comment on this issue and we will provide one!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/804
https://github.com/google/deepvariant/issues/804:202,usability,error,error,202,"So we were unable to reproduce this specific error. Regardless, we are overhauling how multiprocessing is used in `postprocess_variants` with our next release, which will very likely avoid this type of error. I am closing this issue for now. If someone is experiencing this issue and would like an experimental docker container to run, please comment on this issue and we will provide one!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/804
https://github.com/google/deepvariant/issues/804:256,usability,experien,experiencing,256,"So we were unable to reproduce this specific error. Regardless, we are overhauling how multiprocessing is used in `postprocess_variants` with our next release, which will very likely avoid this type of error. I am closing this issue for now. If someone is experiencing this issue and would like an experimental docker container to run, please comment on this issue and we will provide one!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/804
https://github.com/google/deepvariant/issues/804:175,availability,avail,available,175,"Hi, I am experiencing similar issue - VM, 32 threads, 64GB RAM. Could you provide the experimental container? . Btw, thank you for outstanding work while making this software available for us. This tool has great value is reliable and important for us. Regards, . Tomasz Stokowy, Leader Scientific Computing, University of Bergen, Norway. Running via docker 1.6.1, earlier steps work smoothly. cat /proc/version. Linux version 6.1.0-22-amd64 (debian-kernel@lists.debian.org) (gcc-12 (Debian 12.2.0-14) 12.2.0, GNU ld (GNU Binutils for Debian) 2.40) #1 SMP PREEMPT_DYNAMIC Debian 6.1.94-1 (2024-06-21). Error log:. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""/Reference/core_ref_GRCh38_hla_decoy_ebv/genome.fa"" --infile ""/Output/call_variants_output.tfrecord.gz"" --outfile ""/Output/CoriellIndex.vcf"" --cpus ""32"" --gvcf_outfile ""/Output/CoriellIndex.gvcf"" --nonvariant_site_tfrecord_path ""/Output/gvcf.tfrecord@32.gz"". I0823 15:16:56.752997 139658307389248 postprocess_variants.py:1211] Using sample name from call_variants output. Sample name: CoriellIndex. 2024-08-23 15:16:56.766309: I deepvariant/postprocess_variants.cc:94] Read from: /Output/call_variants_output-00000-of-00001.tfrecord.gz. 2024-08-23 15:18:01.806248: I deepvariant/postprocess_variants.cc:109] Total #entries in single_site_calls = 10880665. I0823 15:20:45.074391 139658307389248 postprocess_variants.py:1313] CVO sorting took 3.805263650417328 minutes. I0823 15:20:45.077561 139658307389248 postprocess_variants.py:1316] Transforming call_variants_output to variants. I0823 15:20:45.077694 139658307389248 postprocess_variants.py:1318] Using 32 CPUs for parallelization of variant transformation. I0823 15:20:51.014987 139658307389248 postprocess_variants.py:1211] Using sample name from call_variants output. Sample name: CoriellIndex. real	8m32.455s. user	7m11.835s. sys	1m25.577s. Process ForkPoolWorker-2:. Traceback (most recent call last):. File ""/usr/lib/python3.8/multiprocess",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/804
https://github.com/google/deepvariant/issues/804:222,availability,reliab,reliable,222,"Hi, I am experiencing similar issue - VM, 32 threads, 64GB RAM. Could you provide the experimental container? . Btw, thank you for outstanding work while making this software available for us. This tool has great value is reliable and important for us. Regards, . Tomasz Stokowy, Leader Scientific Computing, University of Bergen, Norway. Running via docker 1.6.1, earlier steps work smoothly. cat /proc/version. Linux version 6.1.0-22-amd64 (debian-kernel@lists.debian.org) (gcc-12 (Debian 12.2.0-14) 12.2.0, GNU ld (GNU Binutils for Debian) 2.40) #1 SMP PREEMPT_DYNAMIC Debian 6.1.94-1 (2024-06-21). Error log:. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""/Reference/core_ref_GRCh38_hla_decoy_ebv/genome.fa"" --infile ""/Output/call_variants_output.tfrecord.gz"" --outfile ""/Output/CoriellIndex.vcf"" --cpus ""32"" --gvcf_outfile ""/Output/CoriellIndex.gvcf"" --nonvariant_site_tfrecord_path ""/Output/gvcf.tfrecord@32.gz"". I0823 15:16:56.752997 139658307389248 postprocess_variants.py:1211] Using sample name from call_variants output. Sample name: CoriellIndex. 2024-08-23 15:16:56.766309: I deepvariant/postprocess_variants.cc:94] Read from: /Output/call_variants_output-00000-of-00001.tfrecord.gz. 2024-08-23 15:18:01.806248: I deepvariant/postprocess_variants.cc:109] Total #entries in single_site_calls = 10880665. I0823 15:20:45.074391 139658307389248 postprocess_variants.py:1313] CVO sorting took 3.805263650417328 minutes. I0823 15:20:45.077561 139658307389248 postprocess_variants.py:1316] Transforming call_variants_output to variants. I0823 15:20:45.077694 139658307389248 postprocess_variants.py:1318] Using 32 CPUs for parallelization of variant transformation. I0823 15:20:51.014987 139658307389248 postprocess_variants.py:1211] Using sample name from call_variants output. Sample name: CoriellIndex. real	8m32.455s. user	7m11.835s. sys	1m25.577s. Process ForkPoolWorker-2:. Traceback (most recent call last):. File ""/usr/lib/python3.8/multiprocess",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/804
https://github.com/google/deepvariant/issues/804:602,availability,Error,Error,602,"Hi, I am experiencing similar issue - VM, 32 threads, 64GB RAM. Could you provide the experimental container? . Btw, thank you for outstanding work while making this software available for us. This tool has great value is reliable and important for us. Regards, . Tomasz Stokowy, Leader Scientific Computing, University of Bergen, Norway. Running via docker 1.6.1, earlier steps work smoothly. cat /proc/version. Linux version 6.1.0-22-amd64 (debian-kernel@lists.debian.org) (gcc-12 (Debian 12.2.0-14) 12.2.0, GNU ld (GNU Binutils for Debian) 2.40) #1 SMP PREEMPT_DYNAMIC Debian 6.1.94-1 (2024-06-21). Error log:. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""/Reference/core_ref_GRCh38_hla_decoy_ebv/genome.fa"" --infile ""/Output/call_variants_output.tfrecord.gz"" --outfile ""/Output/CoriellIndex.vcf"" --cpus ""32"" --gvcf_outfile ""/Output/CoriellIndex.gvcf"" --nonvariant_site_tfrecord_path ""/Output/gvcf.tfrecord@32.gz"". I0823 15:16:56.752997 139658307389248 postprocess_variants.py:1211] Using sample name from call_variants output. Sample name: CoriellIndex. 2024-08-23 15:16:56.766309: I deepvariant/postprocess_variants.cc:94] Read from: /Output/call_variants_output-00000-of-00001.tfrecord.gz. 2024-08-23 15:18:01.806248: I deepvariant/postprocess_variants.cc:109] Total #entries in single_site_calls = 10880665. I0823 15:20:45.074391 139658307389248 postprocess_variants.py:1313] CVO sorting took 3.805263650417328 minutes. I0823 15:20:45.077561 139658307389248 postprocess_variants.py:1316] Transforming call_variants_output to variants. I0823 15:20:45.077694 139658307389248 postprocess_variants.py:1318] Using 32 CPUs for parallelization of variant transformation. I0823 15:20:51.014987 139658307389248 postprocess_variants.py:1211] Using sample name from call_variants output. Sample name: CoriellIndex. real	8m32.455s. user	7m11.835s. sys	1m25.577s. Process ForkPoolWorker-2:. Traceback (most recent call last):. File ""/usr/lib/python3.8/multiprocess",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/804
https://github.com/google/deepvariant/issues/804:99,deployability,contain,container,99,"Hi, I am experiencing similar issue - VM, 32 threads, 64GB RAM. Could you provide the experimental container? . Btw, thank you for outstanding work while making this software available for us. This tool has great value is reliable and important for us. Regards, . Tomasz Stokowy, Leader Scientific Computing, University of Bergen, Norway. Running via docker 1.6.1, earlier steps work smoothly. cat /proc/version. Linux version 6.1.0-22-amd64 (debian-kernel@lists.debian.org) (gcc-12 (Debian 12.2.0-14) 12.2.0, GNU ld (GNU Binutils for Debian) 2.40) #1 SMP PREEMPT_DYNAMIC Debian 6.1.94-1 (2024-06-21). Error log:. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""/Reference/core_ref_GRCh38_hla_decoy_ebv/genome.fa"" --infile ""/Output/call_variants_output.tfrecord.gz"" --outfile ""/Output/CoriellIndex.vcf"" --cpus ""32"" --gvcf_outfile ""/Output/CoriellIndex.gvcf"" --nonvariant_site_tfrecord_path ""/Output/gvcf.tfrecord@32.gz"". I0823 15:16:56.752997 139658307389248 postprocess_variants.py:1211] Using sample name from call_variants output. Sample name: CoriellIndex. 2024-08-23 15:16:56.766309: I deepvariant/postprocess_variants.cc:94] Read from: /Output/call_variants_output-00000-of-00001.tfrecord.gz. 2024-08-23 15:18:01.806248: I deepvariant/postprocess_variants.cc:109] Total #entries in single_site_calls = 10880665. I0823 15:20:45.074391 139658307389248 postprocess_variants.py:1313] CVO sorting took 3.805263650417328 minutes. I0823 15:20:45.077561 139658307389248 postprocess_variants.py:1316] Transforming call_variants_output to variants. I0823 15:20:45.077694 139658307389248 postprocess_variants.py:1318] Using 32 CPUs for parallelization of variant transformation. I0823 15:20:51.014987 139658307389248 postprocess_variants.py:1211] Using sample name from call_variants output. Sample name: CoriellIndex. real	8m32.455s. user	7m11.835s. sys	1m25.577s. Process ForkPoolWorker-2:. Traceback (most recent call last):. File ""/usr/lib/python3.8/multiprocess",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/804
https://github.com/google/deepvariant/issues/804:404,deployability,version,version,404,"Hi, I am experiencing similar issue - VM, 32 threads, 64GB RAM. Could you provide the experimental container? . Btw, thank you for outstanding work while making this software available for us. This tool has great value is reliable and important for us. Regards, . Tomasz Stokowy, Leader Scientific Computing, University of Bergen, Norway. Running via docker 1.6.1, earlier steps work smoothly. cat /proc/version. Linux version 6.1.0-22-amd64 (debian-kernel@lists.debian.org) (gcc-12 (Debian 12.2.0-14) 12.2.0, GNU ld (GNU Binutils for Debian) 2.40) #1 SMP PREEMPT_DYNAMIC Debian 6.1.94-1 (2024-06-21). Error log:. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""/Reference/core_ref_GRCh38_hla_decoy_ebv/genome.fa"" --infile ""/Output/call_variants_output.tfrecord.gz"" --outfile ""/Output/CoriellIndex.vcf"" --cpus ""32"" --gvcf_outfile ""/Output/CoriellIndex.gvcf"" --nonvariant_site_tfrecord_path ""/Output/gvcf.tfrecord@32.gz"". I0823 15:16:56.752997 139658307389248 postprocess_variants.py:1211] Using sample name from call_variants output. Sample name: CoriellIndex. 2024-08-23 15:16:56.766309: I deepvariant/postprocess_variants.cc:94] Read from: /Output/call_variants_output-00000-of-00001.tfrecord.gz. 2024-08-23 15:18:01.806248: I deepvariant/postprocess_variants.cc:109] Total #entries in single_site_calls = 10880665. I0823 15:20:45.074391 139658307389248 postprocess_variants.py:1313] CVO sorting took 3.805263650417328 minutes. I0823 15:20:45.077561 139658307389248 postprocess_variants.py:1316] Transforming call_variants_output to variants. I0823 15:20:45.077694 139658307389248 postprocess_variants.py:1318] Using 32 CPUs for parallelization of variant transformation. I0823 15:20:51.014987 139658307389248 postprocess_variants.py:1211] Using sample name from call_variants output. Sample name: CoriellIndex. real	8m32.455s. user	7m11.835s. sys	1m25.577s. Process ForkPoolWorker-2:. Traceback (most recent call last):. File ""/usr/lib/python3.8/multiprocess",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/804
https://github.com/google/deepvariant/issues/804:419,deployability,version,version,419,"Hi, I am experiencing similar issue - VM, 32 threads, 64GB RAM. Could you provide the experimental container? . Btw, thank you for outstanding work while making this software available for us. This tool has great value is reliable and important for us. Regards, . Tomasz Stokowy, Leader Scientific Computing, University of Bergen, Norway. Running via docker 1.6.1, earlier steps work smoothly. cat /proc/version. Linux version 6.1.0-22-amd64 (debian-kernel@lists.debian.org) (gcc-12 (Debian 12.2.0-14) 12.2.0, GNU ld (GNU Binutils for Debian) 2.40) #1 SMP PREEMPT_DYNAMIC Debian 6.1.94-1 (2024-06-21). Error log:. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""/Reference/core_ref_GRCh38_hla_decoy_ebv/genome.fa"" --infile ""/Output/call_variants_output.tfrecord.gz"" --outfile ""/Output/CoriellIndex.vcf"" --cpus ""32"" --gvcf_outfile ""/Output/CoriellIndex.gvcf"" --nonvariant_site_tfrecord_path ""/Output/gvcf.tfrecord@32.gz"". I0823 15:16:56.752997 139658307389248 postprocess_variants.py:1211] Using sample name from call_variants output. Sample name: CoriellIndex. 2024-08-23 15:16:56.766309: I deepvariant/postprocess_variants.cc:94] Read from: /Output/call_variants_output-00000-of-00001.tfrecord.gz. 2024-08-23 15:18:01.806248: I deepvariant/postprocess_variants.cc:109] Total #entries in single_site_calls = 10880665. I0823 15:20:45.074391 139658307389248 postprocess_variants.py:1313] CVO sorting took 3.805263650417328 minutes. I0823 15:20:45.077561 139658307389248 postprocess_variants.py:1316] Transforming call_variants_output to variants. I0823 15:20:45.077694 139658307389248 postprocess_variants.py:1318] Using 32 CPUs for parallelization of variant transformation. I0823 15:20:51.014987 139658307389248 postprocess_variants.py:1211] Using sample name from call_variants output. Sample name: CoriellIndex. real	8m32.455s. user	7m11.835s. sys	1m25.577s. Process ForkPoolWorker-2:. Traceback (most recent call last):. File ""/usr/lib/python3.8/multiprocess",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/804
https://github.com/google/deepvariant/issues/804:608,deployability,log,log,608,"Hi, I am experiencing similar issue - VM, 32 threads, 64GB RAM. Could you provide the experimental container? . Btw, thank you for outstanding work while making this software available for us. This tool has great value is reliable and important for us. Regards, . Tomasz Stokowy, Leader Scientific Computing, University of Bergen, Norway. Running via docker 1.6.1, earlier steps work smoothly. cat /proc/version. Linux version 6.1.0-22-amd64 (debian-kernel@lists.debian.org) (gcc-12 (Debian 12.2.0-14) 12.2.0, GNU ld (GNU Binutils for Debian) 2.40) #1 SMP PREEMPT_DYNAMIC Debian 6.1.94-1 (2024-06-21). Error log:. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""/Reference/core_ref_GRCh38_hla_decoy_ebv/genome.fa"" --infile ""/Output/call_variants_output.tfrecord.gz"" --outfile ""/Output/CoriellIndex.vcf"" --cpus ""32"" --gvcf_outfile ""/Output/CoriellIndex.gvcf"" --nonvariant_site_tfrecord_path ""/Output/gvcf.tfrecord@32.gz"". I0823 15:16:56.752997 139658307389248 postprocess_variants.py:1211] Using sample name from call_variants output. Sample name: CoriellIndex. 2024-08-23 15:16:56.766309: I deepvariant/postprocess_variants.cc:94] Read from: /Output/call_variants_output-00000-of-00001.tfrecord.gz. 2024-08-23 15:18:01.806248: I deepvariant/postprocess_variants.cc:109] Total #entries in single_site_calls = 10880665. I0823 15:20:45.074391 139658307389248 postprocess_variants.py:1313] CVO sorting took 3.805263650417328 minutes. I0823 15:20:45.077561 139658307389248 postprocess_variants.py:1316] Transforming call_variants_output to variants. I0823 15:20:45.077694 139658307389248 postprocess_variants.py:1318] Using 32 CPUs for parallelization of variant transformation. I0823 15:20:51.014987 139658307389248 postprocess_variants.py:1211] Using sample name from call_variants output. Sample name: CoriellIndex. real	8m32.455s. user	7m11.835s. sys	1m25.577s. Process ForkPoolWorker-2:. Traceback (most recent call last):. File ""/usr/lib/python3.8/multiprocess",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/804
https://github.com/google/deepvariant/issues/804:844,energy efficiency,cpu,cpus,844,"Hi, I am experiencing similar issue - VM, 32 threads, 64GB RAM. Could you provide the experimental container? . Btw, thank you for outstanding work while making this software available for us. This tool has great value is reliable and important for us. Regards, . Tomasz Stokowy, Leader Scientific Computing, University of Bergen, Norway. Running via docker 1.6.1, earlier steps work smoothly. cat /proc/version. Linux version 6.1.0-22-amd64 (debian-kernel@lists.debian.org) (gcc-12 (Debian 12.2.0-14) 12.2.0, GNU ld (GNU Binutils for Debian) 2.40) #1 SMP PREEMPT_DYNAMIC Debian 6.1.94-1 (2024-06-21). Error log:. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""/Reference/core_ref_GRCh38_hla_decoy_ebv/genome.fa"" --infile ""/Output/call_variants_output.tfrecord.gz"" --outfile ""/Output/CoriellIndex.vcf"" --cpus ""32"" --gvcf_outfile ""/Output/CoriellIndex.gvcf"" --nonvariant_site_tfrecord_path ""/Output/gvcf.tfrecord@32.gz"". I0823 15:16:56.752997 139658307389248 postprocess_variants.py:1211] Using sample name from call_variants output. Sample name: CoriellIndex. 2024-08-23 15:16:56.766309: I deepvariant/postprocess_variants.cc:94] Read from: /Output/call_variants_output-00000-of-00001.tfrecord.gz. 2024-08-23 15:18:01.806248: I deepvariant/postprocess_variants.cc:109] Total #entries in single_site_calls = 10880665. I0823 15:20:45.074391 139658307389248 postprocess_variants.py:1313] CVO sorting took 3.805263650417328 minutes. I0823 15:20:45.077561 139658307389248 postprocess_variants.py:1316] Transforming call_variants_output to variants. I0823 15:20:45.077694 139658307389248 postprocess_variants.py:1318] Using 32 CPUs for parallelization of variant transformation. I0823 15:20:51.014987 139658307389248 postprocess_variants.py:1211] Using sample name from call_variants output. Sample name: CoriellIndex. real	8m32.455s. user	7m11.835s. sys	1m25.577s. Process ForkPoolWorker-2:. Traceback (most recent call last):. File ""/usr/lib/python3.8/multiprocess",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/804
https://github.com/google/deepvariant/issues/804:1661,energy efficiency,CPU,CPUs,1661,"riant/bin/postprocess_variants --ref ""/Reference/core_ref_GRCh38_hla_decoy_ebv/genome.fa"" --infile ""/Output/call_variants_output.tfrecord.gz"" --outfile ""/Output/CoriellIndex.vcf"" --cpus ""32"" --gvcf_outfile ""/Output/CoriellIndex.gvcf"" --nonvariant_site_tfrecord_path ""/Output/gvcf.tfrecord@32.gz"". I0823 15:16:56.752997 139658307389248 postprocess_variants.py:1211] Using sample name from call_variants output. Sample name: CoriellIndex. 2024-08-23 15:16:56.766309: I deepvariant/postprocess_variants.cc:94] Read from: /Output/call_variants_output-00000-of-00001.tfrecord.gz. 2024-08-23 15:18:01.806248: I deepvariant/postprocess_variants.cc:109] Total #entries in single_site_calls = 10880665. I0823 15:20:45.074391 139658307389248 postprocess_variants.py:1313] CVO sorting took 3.805263650417328 minutes. I0823 15:20:45.077561 139658307389248 postprocess_variants.py:1316] Transforming call_variants_output to variants. I0823 15:20:45.077694 139658307389248 postprocess_variants.py:1318] Using 32 CPUs for parallelization of variant transformation. I0823 15:20:51.014987 139658307389248 postprocess_variants.py:1211] Using sample name from call_variants output. Sample name: CoriellIndex. real	8m32.455s. user	7m11.835s. sys	1m25.577s. Process ForkPoolWorker-2:. Traceback (most recent call last):. File ""/usr/lib/python3.8/multiprocessing/pool.py"", line 131, in worker. put((job, i, result)). File ""/usr/lib/python3.8/multiprocessing/queues.py"", line 368, in put. self._writer.send_bytes(obj). File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 200, in send_bytes. self._send_bytes(m[offset:offset + size]). File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 404, in _send_bytes. self._send(header). File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 368, in _send. n = write(self._handle, buf). BrokenPipeError: [Errno 32] Broken pipe. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/usr/lib/python3.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/804
https://github.com/google/deepvariant/issues/804:404,integrability,version,version,404,"Hi, I am experiencing similar issue - VM, 32 threads, 64GB RAM. Could you provide the experimental container? . Btw, thank you for outstanding work while making this software available for us. This tool has great value is reliable and important for us. Regards, . Tomasz Stokowy, Leader Scientific Computing, University of Bergen, Norway. Running via docker 1.6.1, earlier steps work smoothly. cat /proc/version. Linux version 6.1.0-22-amd64 (debian-kernel@lists.debian.org) (gcc-12 (Debian 12.2.0-14) 12.2.0, GNU ld (GNU Binutils for Debian) 2.40) #1 SMP PREEMPT_DYNAMIC Debian 6.1.94-1 (2024-06-21). Error log:. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""/Reference/core_ref_GRCh38_hla_decoy_ebv/genome.fa"" --infile ""/Output/call_variants_output.tfrecord.gz"" --outfile ""/Output/CoriellIndex.vcf"" --cpus ""32"" --gvcf_outfile ""/Output/CoriellIndex.gvcf"" --nonvariant_site_tfrecord_path ""/Output/gvcf.tfrecord@32.gz"". I0823 15:16:56.752997 139658307389248 postprocess_variants.py:1211] Using sample name from call_variants output. Sample name: CoriellIndex. 2024-08-23 15:16:56.766309: I deepvariant/postprocess_variants.cc:94] Read from: /Output/call_variants_output-00000-of-00001.tfrecord.gz. 2024-08-23 15:18:01.806248: I deepvariant/postprocess_variants.cc:109] Total #entries in single_site_calls = 10880665. I0823 15:20:45.074391 139658307389248 postprocess_variants.py:1313] CVO sorting took 3.805263650417328 minutes. I0823 15:20:45.077561 139658307389248 postprocess_variants.py:1316] Transforming call_variants_output to variants. I0823 15:20:45.077694 139658307389248 postprocess_variants.py:1318] Using 32 CPUs for parallelization of variant transformation. I0823 15:20:51.014987 139658307389248 postprocess_variants.py:1211] Using sample name from call_variants output. Sample name: CoriellIndex. real	8m32.455s. user	7m11.835s. sys	1m25.577s. Process ForkPoolWorker-2:. Traceback (most recent call last):. File ""/usr/lib/python3.8/multiprocess",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/804
https://github.com/google/deepvariant/issues/804:419,integrability,version,version,419,"Hi, I am experiencing similar issue - VM, 32 threads, 64GB RAM. Could you provide the experimental container? . Btw, thank you for outstanding work while making this software available for us. This tool has great value is reliable and important for us. Regards, . Tomasz Stokowy, Leader Scientific Computing, University of Bergen, Norway. Running via docker 1.6.1, earlier steps work smoothly. cat /proc/version. Linux version 6.1.0-22-amd64 (debian-kernel@lists.debian.org) (gcc-12 (Debian 12.2.0-14) 12.2.0, GNU ld (GNU Binutils for Debian) 2.40) #1 SMP PREEMPT_DYNAMIC Debian 6.1.94-1 (2024-06-21). Error log:. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""/Reference/core_ref_GRCh38_hla_decoy_ebv/genome.fa"" --infile ""/Output/call_variants_output.tfrecord.gz"" --outfile ""/Output/CoriellIndex.vcf"" --cpus ""32"" --gvcf_outfile ""/Output/CoriellIndex.gvcf"" --nonvariant_site_tfrecord_path ""/Output/gvcf.tfrecord@32.gz"". I0823 15:16:56.752997 139658307389248 postprocess_variants.py:1211] Using sample name from call_variants output. Sample name: CoriellIndex. 2024-08-23 15:16:56.766309: I deepvariant/postprocess_variants.cc:94] Read from: /Output/call_variants_output-00000-of-00001.tfrecord.gz. 2024-08-23 15:18:01.806248: I deepvariant/postprocess_variants.cc:109] Total #entries in single_site_calls = 10880665. I0823 15:20:45.074391 139658307389248 postprocess_variants.py:1313] CVO sorting took 3.805263650417328 minutes. I0823 15:20:45.077561 139658307389248 postprocess_variants.py:1316] Transforming call_variants_output to variants. I0823 15:20:45.077694 139658307389248 postprocess_variants.py:1318] Using 32 CPUs for parallelization of variant transformation. I0823 15:20:51.014987 139658307389248 postprocess_variants.py:1211] Using sample name from call_variants output. Sample name: CoriellIndex. real	8m32.455s. user	7m11.835s. sys	1m25.577s. Process ForkPoolWorker-2:. Traceback (most recent call last):. File ""/usr/lib/python3.8/multiprocess",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/804
https://github.com/google/deepvariant/issues/804:1537,integrability,Transform,Transforming,1537,"2.40) #1 SMP PREEMPT_DYNAMIC Debian 6.1.94-1 (2024-06-21). Error log:. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""/Reference/core_ref_GRCh38_hla_decoy_ebv/genome.fa"" --infile ""/Output/call_variants_output.tfrecord.gz"" --outfile ""/Output/CoriellIndex.vcf"" --cpus ""32"" --gvcf_outfile ""/Output/CoriellIndex.gvcf"" --nonvariant_site_tfrecord_path ""/Output/gvcf.tfrecord@32.gz"". I0823 15:16:56.752997 139658307389248 postprocess_variants.py:1211] Using sample name from call_variants output. Sample name: CoriellIndex. 2024-08-23 15:16:56.766309: I deepvariant/postprocess_variants.cc:94] Read from: /Output/call_variants_output-00000-of-00001.tfrecord.gz. 2024-08-23 15:18:01.806248: I deepvariant/postprocess_variants.cc:109] Total #entries in single_site_calls = 10880665. I0823 15:20:45.074391 139658307389248 postprocess_variants.py:1313] CVO sorting took 3.805263650417328 minutes. I0823 15:20:45.077561 139658307389248 postprocess_variants.py:1316] Transforming call_variants_output to variants. I0823 15:20:45.077694 139658307389248 postprocess_variants.py:1318] Using 32 CPUs for parallelization of variant transformation. I0823 15:20:51.014987 139658307389248 postprocess_variants.py:1211] Using sample name from call_variants output. Sample name: CoriellIndex. real	8m32.455s. user	7m11.835s. sys	1m25.577s. Process ForkPoolWorker-2:. Traceback (most recent call last):. File ""/usr/lib/python3.8/multiprocessing/pool.py"", line 131, in worker. put((job, i, result)). File ""/usr/lib/python3.8/multiprocessing/queues.py"", line 368, in put. self._writer.send_bytes(obj). File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 200, in send_bytes. self._send_bytes(m[offset:offset + size]). File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 404, in _send_bytes. self._send(header). File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 368, in _send. n = write(self._handle, buf). BrokenPipeError: [Errno 32] Broken pipe. During h",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/804
https://github.com/google/deepvariant/issues/804:1697,integrability,transform,transformation,1697,"ference/core_ref_GRCh38_hla_decoy_ebv/genome.fa"" --infile ""/Output/call_variants_output.tfrecord.gz"" --outfile ""/Output/CoriellIndex.vcf"" --cpus ""32"" --gvcf_outfile ""/Output/CoriellIndex.gvcf"" --nonvariant_site_tfrecord_path ""/Output/gvcf.tfrecord@32.gz"". I0823 15:16:56.752997 139658307389248 postprocess_variants.py:1211] Using sample name from call_variants output. Sample name: CoriellIndex. 2024-08-23 15:16:56.766309: I deepvariant/postprocess_variants.cc:94] Read from: /Output/call_variants_output-00000-of-00001.tfrecord.gz. 2024-08-23 15:18:01.806248: I deepvariant/postprocess_variants.cc:109] Total #entries in single_site_calls = 10880665. I0823 15:20:45.074391 139658307389248 postprocess_variants.py:1313] CVO sorting took 3.805263650417328 minutes. I0823 15:20:45.077561 139658307389248 postprocess_variants.py:1316] Transforming call_variants_output to variants. I0823 15:20:45.077694 139658307389248 postprocess_variants.py:1318] Using 32 CPUs for parallelization of variant transformation. I0823 15:20:51.014987 139658307389248 postprocess_variants.py:1211] Using sample name from call_variants output. Sample name: CoriellIndex. real	8m32.455s. user	7m11.835s. sys	1m25.577s. Process ForkPoolWorker-2:. Traceback (most recent call last):. File ""/usr/lib/python3.8/multiprocessing/pool.py"", line 131, in worker. put((job, i, result)). File ""/usr/lib/python3.8/multiprocessing/queues.py"", line 368, in put. self._writer.send_bytes(obj). File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 200, in send_bytes. self._send_bytes(m[offset:offset + size]). File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 404, in _send_bytes. self._send(header). File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 368, in _send. n = write(self._handle, buf). BrokenPipeError: [Errno 32] Broken pipe. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/usr/lib/python3.8/multiprocessing/process.py"", line 315, ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/804
https://github.com/google/deepvariant/issues/804:2099,integrability,queue,queues,2099,"24-08-23 15:16:56.766309: I deepvariant/postprocess_variants.cc:94] Read from: /Output/call_variants_output-00000-of-00001.tfrecord.gz. 2024-08-23 15:18:01.806248: I deepvariant/postprocess_variants.cc:109] Total #entries in single_site_calls = 10880665. I0823 15:20:45.074391 139658307389248 postprocess_variants.py:1313] CVO sorting took 3.805263650417328 minutes. I0823 15:20:45.077561 139658307389248 postprocess_variants.py:1316] Transforming call_variants_output to variants. I0823 15:20:45.077694 139658307389248 postprocess_variants.py:1318] Using 32 CPUs for parallelization of variant transformation. I0823 15:20:51.014987 139658307389248 postprocess_variants.py:1211] Using sample name from call_variants output. Sample name: CoriellIndex. real	8m32.455s. user	7m11.835s. sys	1m25.577s. Process ForkPoolWorker-2:. Traceback (most recent call last):. File ""/usr/lib/python3.8/multiprocessing/pool.py"", line 131, in worker. put((job, i, result)). File ""/usr/lib/python3.8/multiprocessing/queues.py"", line 368, in put. self._writer.send_bytes(obj). File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 200, in send_bytes. self._send_bytes(m[offset:offset + size]). File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 404, in _send_bytes. self._send(header). File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 368, in _send. n = write(self._handle, buf). BrokenPipeError: [Errno 32] Broken pipe. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/usr/lib/python3.8/multiprocessing/process.py"", line 315, in _bootstrap. self.run(). File ""/usr/lib/python3.8/multiprocessing/process.py"", line 108, in run. self._target(*self._args, **self._kwargs). File ""/usr/lib/python3.8/multiprocessing/pool.py"", line 136, in worker. put((job, i, (False, wrapped))). File ""/usr/lib/python3.8/multiprocessing/queues.py"", line 368, in put. self._writer.send_bytes(obj). File ""/usr/lib/python3.8/multiprocessing/connectio",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/804
https://github.com/google/deepvariant/issues/804:2939,integrability,wrap,wrapped,2939,"recent call last):. File ""/usr/lib/python3.8/multiprocessing/pool.py"", line 131, in worker. put((job, i, result)). File ""/usr/lib/python3.8/multiprocessing/queues.py"", line 368, in put. self._writer.send_bytes(obj). File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 200, in send_bytes. self._send_bytes(m[offset:offset + size]). File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 404, in _send_bytes. self._send(header). File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 368, in _send. n = write(self._handle, buf). BrokenPipeError: [Errno 32] Broken pipe. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/usr/lib/python3.8/multiprocessing/process.py"", line 315, in _bootstrap. self.run(). File ""/usr/lib/python3.8/multiprocessing/process.py"", line 108, in run. self._target(*self._args, **self._kwargs). File ""/usr/lib/python3.8/multiprocessing/pool.py"", line 136, in worker. put((job, i, (False, wrapped))). File ""/usr/lib/python3.8/multiprocessing/queues.py"", line 368, in put. self._writer.send_bytes(obj). File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 200, in send_bytes. self._send_bytes(m[offset:offset + size]). File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 404, in _send_bytes. self._send(header). File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 368, in _send. n = write(self._handle, buf). BrokenPipeError: [Errno 32] Broken pipe. Process ForkPoolWorker-28:. Traceback (most recent call last):. File ""/usr/lib/python3.8/multiprocessing/pool.py"", line 131, in worker. put((job, i, result)). File ""/usr/lib/python3.8/multiprocessing/queues.py"", line 368, in put. self._writer.send_bytes(obj). File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 200, in send_bytes. self._send_bytes(m[offset:offset + size]). File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 404, in _send_bytes. self._send(header). File ""/usr/lib/python3.8/mu",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/804
https://github.com/google/deepvariant/issues/804:2992,integrability,queue,queues,2992,"ocessing/pool.py"", line 131, in worker. put((job, i, result)). File ""/usr/lib/python3.8/multiprocessing/queues.py"", line 368, in put. self._writer.send_bytes(obj). File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 200, in send_bytes. self._send_bytes(m[offset:offset + size]). File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 404, in _send_bytes. self._send(header). File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 368, in _send. n = write(self._handle, buf). BrokenPipeError: [Errno 32] Broken pipe. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/usr/lib/python3.8/multiprocessing/process.py"", line 315, in _bootstrap. self.run(). File ""/usr/lib/python3.8/multiprocessing/process.py"", line 108, in run. self._target(*self._args, **self._kwargs). File ""/usr/lib/python3.8/multiprocessing/pool.py"", line 136, in worker. put((job, i, (False, wrapped))). File ""/usr/lib/python3.8/multiprocessing/queues.py"", line 368, in put. self._writer.send_bytes(obj). File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 200, in send_bytes. self._send_bytes(m[offset:offset + size]). File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 404, in _send_bytes. self._send(header). File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 368, in _send. n = write(self._handle, buf). BrokenPipeError: [Errno 32] Broken pipe. Process ForkPoolWorker-28:. Traceback (most recent call last):. File ""/usr/lib/python3.8/multiprocessing/pool.py"", line 131, in worker. put((job, i, result)). File ""/usr/lib/python3.8/multiprocessing/queues.py"", line 368, in put. self._writer.send_bytes(obj). File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 200, in send_bytes. self._send_bytes(m[offset:offset + size]). File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 404, in _send_bytes. self._send(header). File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 368, in _send. n ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/804
https://github.com/google/deepvariant/issues/804:3628,integrability,queue,queues,3628," in put. self._writer.send_bytes(obj). File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 200, in send_bytes. self._send_bytes(m[offset:offset + size]). File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 404, in _send_bytes. self._send(header). File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 368, in _send. n = write(self._handle, buf). BrokenPipeError: [Errno 32] Broken pipe. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/usr/lib/python3.8/multiprocessing/process.py"", line 315, in _bootstrap. self.run(). File ""/usr/lib/python3.8/multiprocessing/process.py"", line 108, in run. self._target(*self._args, **self._kwargs). File ""/usr/lib/python3.8/multiprocessing/pool.py"", line 136, in worker. put((job, i, (False, wrapped))). File ""/usr/lib/python3.8/multiprocessing/queues.py"", line 368, in put. self._writer.send_bytes(obj). File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 200, in send_bytes. self._send_bytes(m[offset:offset + size]). File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 404, in _send_bytes. self._send(header). File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 368, in _send. n = write(self._handle, buf). BrokenPipeError: [Errno 32] Broken pipe. Process ForkPoolWorker-28:. Traceback (most recent call last):. File ""/usr/lib/python3.8/multiprocessing/pool.py"", line 131, in worker. put((job, i, result)). File ""/usr/lib/python3.8/multiprocessing/queues.py"", line 368, in put. self._writer.send_bytes(obj). File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 200, in send_bytes. self._send_bytes(m[offset:offset + size]). File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 404, in _send_bytes. self._send(header). File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 368, in _send. n = write(self._handle, buf). BrokenPipeError: [Errno 32] Broken pipe. (similar records from other workers repeating here ...).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/804
https://github.com/google/deepvariant/issues/804:1537,interoperability,Transform,Transforming,1537,"2.40) #1 SMP PREEMPT_DYNAMIC Debian 6.1.94-1 (2024-06-21). Error log:. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""/Reference/core_ref_GRCh38_hla_decoy_ebv/genome.fa"" --infile ""/Output/call_variants_output.tfrecord.gz"" --outfile ""/Output/CoriellIndex.vcf"" --cpus ""32"" --gvcf_outfile ""/Output/CoriellIndex.gvcf"" --nonvariant_site_tfrecord_path ""/Output/gvcf.tfrecord@32.gz"". I0823 15:16:56.752997 139658307389248 postprocess_variants.py:1211] Using sample name from call_variants output. Sample name: CoriellIndex. 2024-08-23 15:16:56.766309: I deepvariant/postprocess_variants.cc:94] Read from: /Output/call_variants_output-00000-of-00001.tfrecord.gz. 2024-08-23 15:18:01.806248: I deepvariant/postprocess_variants.cc:109] Total #entries in single_site_calls = 10880665. I0823 15:20:45.074391 139658307389248 postprocess_variants.py:1313] CVO sorting took 3.805263650417328 minutes. I0823 15:20:45.077561 139658307389248 postprocess_variants.py:1316] Transforming call_variants_output to variants. I0823 15:20:45.077694 139658307389248 postprocess_variants.py:1318] Using 32 CPUs for parallelization of variant transformation. I0823 15:20:51.014987 139658307389248 postprocess_variants.py:1211] Using sample name from call_variants output. Sample name: CoriellIndex. real	8m32.455s. user	7m11.835s. sys	1m25.577s. Process ForkPoolWorker-2:. Traceback (most recent call last):. File ""/usr/lib/python3.8/multiprocessing/pool.py"", line 131, in worker. put((job, i, result)). File ""/usr/lib/python3.8/multiprocessing/queues.py"", line 368, in put. self._writer.send_bytes(obj). File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 200, in send_bytes. self._send_bytes(m[offset:offset + size]). File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 404, in _send_bytes. self._send(header). File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 368, in _send. n = write(self._handle, buf). BrokenPipeError: [Errno 32] Broken pipe. During h",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/804
https://github.com/google/deepvariant/issues/804:1697,interoperability,transform,transformation,1697,"ference/core_ref_GRCh38_hla_decoy_ebv/genome.fa"" --infile ""/Output/call_variants_output.tfrecord.gz"" --outfile ""/Output/CoriellIndex.vcf"" --cpus ""32"" --gvcf_outfile ""/Output/CoriellIndex.gvcf"" --nonvariant_site_tfrecord_path ""/Output/gvcf.tfrecord@32.gz"". I0823 15:16:56.752997 139658307389248 postprocess_variants.py:1211] Using sample name from call_variants output. Sample name: CoriellIndex. 2024-08-23 15:16:56.766309: I deepvariant/postprocess_variants.cc:94] Read from: /Output/call_variants_output-00000-of-00001.tfrecord.gz. 2024-08-23 15:18:01.806248: I deepvariant/postprocess_variants.cc:109] Total #entries in single_site_calls = 10880665. I0823 15:20:45.074391 139658307389248 postprocess_variants.py:1313] CVO sorting took 3.805263650417328 minutes. I0823 15:20:45.077561 139658307389248 postprocess_variants.py:1316] Transforming call_variants_output to variants. I0823 15:20:45.077694 139658307389248 postprocess_variants.py:1318] Using 32 CPUs for parallelization of variant transformation. I0823 15:20:51.014987 139658307389248 postprocess_variants.py:1211] Using sample name from call_variants output. Sample name: CoriellIndex. real	8m32.455s. user	7m11.835s. sys	1m25.577s. Process ForkPoolWorker-2:. Traceback (most recent call last):. File ""/usr/lib/python3.8/multiprocessing/pool.py"", line 131, in worker. put((job, i, result)). File ""/usr/lib/python3.8/multiprocessing/queues.py"", line 368, in put. self._writer.send_bytes(obj). File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 200, in send_bytes. self._send_bytes(m[offset:offset + size]). File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 404, in _send_bytes. self._send(header). File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 368, in _send. n = write(self._handle, buf). BrokenPipeError: [Errno 32] Broken pipe. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/usr/lib/python3.8/multiprocessing/process.py"", line 315, ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/804
https://github.com/google/deepvariant/issues/804:404,modifiability,version,version,404,"Hi, I am experiencing similar issue - VM, 32 threads, 64GB RAM. Could you provide the experimental container? . Btw, thank you for outstanding work while making this software available for us. This tool has great value is reliable and important for us. Regards, . Tomasz Stokowy, Leader Scientific Computing, University of Bergen, Norway. Running via docker 1.6.1, earlier steps work smoothly. cat /proc/version. Linux version 6.1.0-22-amd64 (debian-kernel@lists.debian.org) (gcc-12 (Debian 12.2.0-14) 12.2.0, GNU ld (GNU Binutils for Debian) 2.40) #1 SMP PREEMPT_DYNAMIC Debian 6.1.94-1 (2024-06-21). Error log:. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""/Reference/core_ref_GRCh38_hla_decoy_ebv/genome.fa"" --infile ""/Output/call_variants_output.tfrecord.gz"" --outfile ""/Output/CoriellIndex.vcf"" --cpus ""32"" --gvcf_outfile ""/Output/CoriellIndex.gvcf"" --nonvariant_site_tfrecord_path ""/Output/gvcf.tfrecord@32.gz"". I0823 15:16:56.752997 139658307389248 postprocess_variants.py:1211] Using sample name from call_variants output. Sample name: CoriellIndex. 2024-08-23 15:16:56.766309: I deepvariant/postprocess_variants.cc:94] Read from: /Output/call_variants_output-00000-of-00001.tfrecord.gz. 2024-08-23 15:18:01.806248: I deepvariant/postprocess_variants.cc:109] Total #entries in single_site_calls = 10880665. I0823 15:20:45.074391 139658307389248 postprocess_variants.py:1313] CVO sorting took 3.805263650417328 minutes. I0823 15:20:45.077561 139658307389248 postprocess_variants.py:1316] Transforming call_variants_output to variants. I0823 15:20:45.077694 139658307389248 postprocess_variants.py:1318] Using 32 CPUs for parallelization of variant transformation. I0823 15:20:51.014987 139658307389248 postprocess_variants.py:1211] Using sample name from call_variants output. Sample name: CoriellIndex. real	8m32.455s. user	7m11.835s. sys	1m25.577s. Process ForkPoolWorker-2:. Traceback (most recent call last):. File ""/usr/lib/python3.8/multiprocess",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/804
https://github.com/google/deepvariant/issues/804:419,modifiability,version,version,419,"Hi, I am experiencing similar issue - VM, 32 threads, 64GB RAM. Could you provide the experimental container? . Btw, thank you for outstanding work while making this software available for us. This tool has great value is reliable and important for us. Regards, . Tomasz Stokowy, Leader Scientific Computing, University of Bergen, Norway. Running via docker 1.6.1, earlier steps work smoothly. cat /proc/version. Linux version 6.1.0-22-amd64 (debian-kernel@lists.debian.org) (gcc-12 (Debian 12.2.0-14) 12.2.0, GNU ld (GNU Binutils for Debian) 2.40) #1 SMP PREEMPT_DYNAMIC Debian 6.1.94-1 (2024-06-21). Error log:. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""/Reference/core_ref_GRCh38_hla_decoy_ebv/genome.fa"" --infile ""/Output/call_variants_output.tfrecord.gz"" --outfile ""/Output/CoriellIndex.vcf"" --cpus ""32"" --gvcf_outfile ""/Output/CoriellIndex.gvcf"" --nonvariant_site_tfrecord_path ""/Output/gvcf.tfrecord@32.gz"". I0823 15:16:56.752997 139658307389248 postprocess_variants.py:1211] Using sample name from call_variants output. Sample name: CoriellIndex. 2024-08-23 15:16:56.766309: I deepvariant/postprocess_variants.cc:94] Read from: /Output/call_variants_output-00000-of-00001.tfrecord.gz. 2024-08-23 15:18:01.806248: I deepvariant/postprocess_variants.cc:109] Total #entries in single_site_calls = 10880665. I0823 15:20:45.074391 139658307389248 postprocess_variants.py:1313] CVO sorting took 3.805263650417328 minutes. I0823 15:20:45.077561 139658307389248 postprocess_variants.py:1316] Transforming call_variants_output to variants. I0823 15:20:45.077694 139658307389248 postprocess_variants.py:1318] Using 32 CPUs for parallelization of variant transformation. I0823 15:20:51.014987 139658307389248 postprocess_variants.py:1211] Using sample name from call_variants output. Sample name: CoriellIndex. real	8m32.455s. user	7m11.835s. sys	1m25.577s. Process ForkPoolWorker-2:. Traceback (most recent call last):. File ""/usr/lib/python3.8/multiprocess",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/804
https://github.com/google/deepvariant/issues/804:602,performance,Error,Error,602,"Hi, I am experiencing similar issue - VM, 32 threads, 64GB RAM. Could you provide the experimental container? . Btw, thank you for outstanding work while making this software available for us. This tool has great value is reliable and important for us. Regards, . Tomasz Stokowy, Leader Scientific Computing, University of Bergen, Norway. Running via docker 1.6.1, earlier steps work smoothly. cat /proc/version. Linux version 6.1.0-22-amd64 (debian-kernel@lists.debian.org) (gcc-12 (Debian 12.2.0-14) 12.2.0, GNU ld (GNU Binutils for Debian) 2.40) #1 SMP PREEMPT_DYNAMIC Debian 6.1.94-1 (2024-06-21). Error log:. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""/Reference/core_ref_GRCh38_hla_decoy_ebv/genome.fa"" --infile ""/Output/call_variants_output.tfrecord.gz"" --outfile ""/Output/CoriellIndex.vcf"" --cpus ""32"" --gvcf_outfile ""/Output/CoriellIndex.gvcf"" --nonvariant_site_tfrecord_path ""/Output/gvcf.tfrecord@32.gz"". I0823 15:16:56.752997 139658307389248 postprocess_variants.py:1211] Using sample name from call_variants output. Sample name: CoriellIndex. 2024-08-23 15:16:56.766309: I deepvariant/postprocess_variants.cc:94] Read from: /Output/call_variants_output-00000-of-00001.tfrecord.gz. 2024-08-23 15:18:01.806248: I deepvariant/postprocess_variants.cc:109] Total #entries in single_site_calls = 10880665. I0823 15:20:45.074391 139658307389248 postprocess_variants.py:1313] CVO sorting took 3.805263650417328 minutes. I0823 15:20:45.077561 139658307389248 postprocess_variants.py:1316] Transforming call_variants_output to variants. I0823 15:20:45.077694 139658307389248 postprocess_variants.py:1318] Using 32 CPUs for parallelization of variant transformation. I0823 15:20:51.014987 139658307389248 postprocess_variants.py:1211] Using sample name from call_variants output. Sample name: CoriellIndex. real	8m32.455s. user	7m11.835s. sys	1m25.577s. Process ForkPoolWorker-2:. Traceback (most recent call last):. File ""/usr/lib/python3.8/multiprocess",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/804
https://github.com/google/deepvariant/issues/804:647,performance,time,time,647,"Hi, I am experiencing similar issue - VM, 32 threads, 64GB RAM. Could you provide the experimental container? . Btw, thank you for outstanding work while making this software available for us. This tool has great value is reliable and important for us. Regards, . Tomasz Stokowy, Leader Scientific Computing, University of Bergen, Norway. Running via docker 1.6.1, earlier steps work smoothly. cat /proc/version. Linux version 6.1.0-22-amd64 (debian-kernel@lists.debian.org) (gcc-12 (Debian 12.2.0-14) 12.2.0, GNU ld (GNU Binutils for Debian) 2.40) #1 SMP PREEMPT_DYNAMIC Debian 6.1.94-1 (2024-06-21). Error log:. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""/Reference/core_ref_GRCh38_hla_decoy_ebv/genome.fa"" --infile ""/Output/call_variants_output.tfrecord.gz"" --outfile ""/Output/CoriellIndex.vcf"" --cpus ""32"" --gvcf_outfile ""/Output/CoriellIndex.gvcf"" --nonvariant_site_tfrecord_path ""/Output/gvcf.tfrecord@32.gz"". I0823 15:16:56.752997 139658307389248 postprocess_variants.py:1211] Using sample name from call_variants output. Sample name: CoriellIndex. 2024-08-23 15:16:56.766309: I deepvariant/postprocess_variants.cc:94] Read from: /Output/call_variants_output-00000-of-00001.tfrecord.gz. 2024-08-23 15:18:01.806248: I deepvariant/postprocess_variants.cc:109] Total #entries in single_site_calls = 10880665. I0823 15:20:45.074391 139658307389248 postprocess_variants.py:1313] CVO sorting took 3.805263650417328 minutes. I0823 15:20:45.077561 139658307389248 postprocess_variants.py:1316] Transforming call_variants_output to variants. I0823 15:20:45.077694 139658307389248 postprocess_variants.py:1318] Using 32 CPUs for parallelization of variant transformation. I0823 15:20:51.014987 139658307389248 postprocess_variants.py:1211] Using sample name from call_variants output. Sample name: CoriellIndex. real	8m32.455s. user	7m11.835s. sys	1m25.577s. Process ForkPoolWorker-2:. Traceback (most recent call last):. File ""/usr/lib/python3.8/multiprocess",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/804
https://github.com/google/deepvariant/issues/804:844,performance,cpu,cpus,844,"Hi, I am experiencing similar issue - VM, 32 threads, 64GB RAM. Could you provide the experimental container? . Btw, thank you for outstanding work while making this software available for us. This tool has great value is reliable and important for us. Regards, . Tomasz Stokowy, Leader Scientific Computing, University of Bergen, Norway. Running via docker 1.6.1, earlier steps work smoothly. cat /proc/version. Linux version 6.1.0-22-amd64 (debian-kernel@lists.debian.org) (gcc-12 (Debian 12.2.0-14) 12.2.0, GNU ld (GNU Binutils for Debian) 2.40) #1 SMP PREEMPT_DYNAMIC Debian 6.1.94-1 (2024-06-21). Error log:. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""/Reference/core_ref_GRCh38_hla_decoy_ebv/genome.fa"" --infile ""/Output/call_variants_output.tfrecord.gz"" --outfile ""/Output/CoriellIndex.vcf"" --cpus ""32"" --gvcf_outfile ""/Output/CoriellIndex.gvcf"" --nonvariant_site_tfrecord_path ""/Output/gvcf.tfrecord@32.gz"". I0823 15:16:56.752997 139658307389248 postprocess_variants.py:1211] Using sample name from call_variants output. Sample name: CoriellIndex. 2024-08-23 15:16:56.766309: I deepvariant/postprocess_variants.cc:94] Read from: /Output/call_variants_output-00000-of-00001.tfrecord.gz. 2024-08-23 15:18:01.806248: I deepvariant/postprocess_variants.cc:109] Total #entries in single_site_calls = 10880665. I0823 15:20:45.074391 139658307389248 postprocess_variants.py:1313] CVO sorting took 3.805263650417328 minutes. I0823 15:20:45.077561 139658307389248 postprocess_variants.py:1316] Transforming call_variants_output to variants. I0823 15:20:45.077694 139658307389248 postprocess_variants.py:1318] Using 32 CPUs for parallelization of variant transformation. I0823 15:20:51.014987 139658307389248 postprocess_variants.py:1211] Using sample name from call_variants output. Sample name: CoriellIndex. real	8m32.455s. user	7m11.835s. sys	1m25.577s. Process ForkPoolWorker-2:. Traceback (most recent call last):. File ""/usr/lib/python3.8/multiprocess",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/804
https://github.com/google/deepvariant/issues/804:1661,performance,CPU,CPUs,1661,"riant/bin/postprocess_variants --ref ""/Reference/core_ref_GRCh38_hla_decoy_ebv/genome.fa"" --infile ""/Output/call_variants_output.tfrecord.gz"" --outfile ""/Output/CoriellIndex.vcf"" --cpus ""32"" --gvcf_outfile ""/Output/CoriellIndex.gvcf"" --nonvariant_site_tfrecord_path ""/Output/gvcf.tfrecord@32.gz"". I0823 15:16:56.752997 139658307389248 postprocess_variants.py:1211] Using sample name from call_variants output. Sample name: CoriellIndex. 2024-08-23 15:16:56.766309: I deepvariant/postprocess_variants.cc:94] Read from: /Output/call_variants_output-00000-of-00001.tfrecord.gz. 2024-08-23 15:18:01.806248: I deepvariant/postprocess_variants.cc:109] Total #entries in single_site_calls = 10880665. I0823 15:20:45.074391 139658307389248 postprocess_variants.py:1313] CVO sorting took 3.805263650417328 minutes. I0823 15:20:45.077561 139658307389248 postprocess_variants.py:1316] Transforming call_variants_output to variants. I0823 15:20:45.077694 139658307389248 postprocess_variants.py:1318] Using 32 CPUs for parallelization of variant transformation. I0823 15:20:51.014987 139658307389248 postprocess_variants.py:1211] Using sample name from call_variants output. Sample name: CoriellIndex. real	8m32.455s. user	7m11.835s. sys	1m25.577s. Process ForkPoolWorker-2:. Traceback (most recent call last):. File ""/usr/lib/python3.8/multiprocessing/pool.py"", line 131, in worker. put((job, i, result)). File ""/usr/lib/python3.8/multiprocessing/queues.py"", line 368, in put. self._writer.send_bytes(obj). File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 200, in send_bytes. self._send_bytes(m[offset:offset + size]). File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 404, in _send_bytes. self._send(header). File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 368, in _send. n = write(self._handle, buf). BrokenPipeError: [Errno 32] Broken pipe. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/usr/lib/python3.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/804
https://github.com/google/deepvariant/issues/804:1670,performance,parallel,parallelization,1670,"rocess_variants --ref ""/Reference/core_ref_GRCh38_hla_decoy_ebv/genome.fa"" --infile ""/Output/call_variants_output.tfrecord.gz"" --outfile ""/Output/CoriellIndex.vcf"" --cpus ""32"" --gvcf_outfile ""/Output/CoriellIndex.gvcf"" --nonvariant_site_tfrecord_path ""/Output/gvcf.tfrecord@32.gz"". I0823 15:16:56.752997 139658307389248 postprocess_variants.py:1211] Using sample name from call_variants output. Sample name: CoriellIndex. 2024-08-23 15:16:56.766309: I deepvariant/postprocess_variants.cc:94] Read from: /Output/call_variants_output-00000-of-00001.tfrecord.gz. 2024-08-23 15:18:01.806248: I deepvariant/postprocess_variants.cc:109] Total #entries in single_site_calls = 10880665. I0823 15:20:45.074391 139658307389248 postprocess_variants.py:1313] CVO sorting took 3.805263650417328 minutes. I0823 15:20:45.077561 139658307389248 postprocess_variants.py:1316] Transforming call_variants_output to variants. I0823 15:20:45.077694 139658307389248 postprocess_variants.py:1318] Using 32 CPUs for parallelization of variant transformation. I0823 15:20:51.014987 139658307389248 postprocess_variants.py:1211] Using sample name from call_variants output. Sample name: CoriellIndex. real	8m32.455s. user	7m11.835s. sys	1m25.577s. Process ForkPoolWorker-2:. Traceback (most recent call last):. File ""/usr/lib/python3.8/multiprocessing/pool.py"", line 131, in worker. put((job, i, result)). File ""/usr/lib/python3.8/multiprocessing/queues.py"", line 368, in put. self._writer.send_bytes(obj). File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 200, in send_bytes. self._send_bytes(m[offset:offset + size]). File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 404, in _send_bytes. self._send(header). File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 368, in _send. n = write(self._handle, buf). BrokenPipeError: [Errno 32] Broken pipe. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/usr/lib/python3.8/multiprocessi",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/804
https://github.com/google/deepvariant/issues/804:2099,performance,queue,queues,2099,"24-08-23 15:16:56.766309: I deepvariant/postprocess_variants.cc:94] Read from: /Output/call_variants_output-00000-of-00001.tfrecord.gz. 2024-08-23 15:18:01.806248: I deepvariant/postprocess_variants.cc:109] Total #entries in single_site_calls = 10880665. I0823 15:20:45.074391 139658307389248 postprocess_variants.py:1313] CVO sorting took 3.805263650417328 minutes. I0823 15:20:45.077561 139658307389248 postprocess_variants.py:1316] Transforming call_variants_output to variants. I0823 15:20:45.077694 139658307389248 postprocess_variants.py:1318] Using 32 CPUs for parallelization of variant transformation. I0823 15:20:51.014987 139658307389248 postprocess_variants.py:1211] Using sample name from call_variants output. Sample name: CoriellIndex. real	8m32.455s. user	7m11.835s. sys	1m25.577s. Process ForkPoolWorker-2:. Traceback (most recent call last):. File ""/usr/lib/python3.8/multiprocessing/pool.py"", line 131, in worker. put((job, i, result)). File ""/usr/lib/python3.8/multiprocessing/queues.py"", line 368, in put. self._writer.send_bytes(obj). File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 200, in send_bytes. self._send_bytes(m[offset:offset + size]). File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 404, in _send_bytes. self._send(header). File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 368, in _send. n = write(self._handle, buf). BrokenPipeError: [Errno 32] Broken pipe. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/usr/lib/python3.8/multiprocessing/process.py"", line 315, in _bootstrap. self.run(). File ""/usr/lib/python3.8/multiprocessing/process.py"", line 108, in run. self._target(*self._args, **self._kwargs). File ""/usr/lib/python3.8/multiprocessing/pool.py"", line 136, in worker. put((job, i, (False, wrapped))). File ""/usr/lib/python3.8/multiprocessing/queues.py"", line 368, in put. self._writer.send_bytes(obj). File ""/usr/lib/python3.8/multiprocessing/connectio",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/804
https://github.com/google/deepvariant/issues/804:2992,performance,queue,queues,2992,"ocessing/pool.py"", line 131, in worker. put((job, i, result)). File ""/usr/lib/python3.8/multiprocessing/queues.py"", line 368, in put. self._writer.send_bytes(obj). File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 200, in send_bytes. self._send_bytes(m[offset:offset + size]). File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 404, in _send_bytes. self._send(header). File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 368, in _send. n = write(self._handle, buf). BrokenPipeError: [Errno 32] Broken pipe. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/usr/lib/python3.8/multiprocessing/process.py"", line 315, in _bootstrap. self.run(). File ""/usr/lib/python3.8/multiprocessing/process.py"", line 108, in run. self._target(*self._args, **self._kwargs). File ""/usr/lib/python3.8/multiprocessing/pool.py"", line 136, in worker. put((job, i, (False, wrapped))). File ""/usr/lib/python3.8/multiprocessing/queues.py"", line 368, in put. self._writer.send_bytes(obj). File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 200, in send_bytes. self._send_bytes(m[offset:offset + size]). File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 404, in _send_bytes. self._send(header). File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 368, in _send. n = write(self._handle, buf). BrokenPipeError: [Errno 32] Broken pipe. Process ForkPoolWorker-28:. Traceback (most recent call last):. File ""/usr/lib/python3.8/multiprocessing/pool.py"", line 131, in worker. put((job, i, result)). File ""/usr/lib/python3.8/multiprocessing/queues.py"", line 368, in put. self._writer.send_bytes(obj). File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 200, in send_bytes. self._send_bytes(m[offset:offset + size]). File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 404, in _send_bytes. self._send(header). File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 368, in _send. n ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/804
https://github.com/google/deepvariant/issues/804:3628,performance,queue,queues,3628," in put. self._writer.send_bytes(obj). File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 200, in send_bytes. self._send_bytes(m[offset:offset + size]). File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 404, in _send_bytes. self._send(header). File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 368, in _send. n = write(self._handle, buf). BrokenPipeError: [Errno 32] Broken pipe. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/usr/lib/python3.8/multiprocessing/process.py"", line 315, in _bootstrap. self.run(). File ""/usr/lib/python3.8/multiprocessing/process.py"", line 108, in run. self._target(*self._args, **self._kwargs). File ""/usr/lib/python3.8/multiprocessing/pool.py"", line 136, in worker. put((job, i, (False, wrapped))). File ""/usr/lib/python3.8/multiprocessing/queues.py"", line 368, in put. self._writer.send_bytes(obj). File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 200, in send_bytes. self._send_bytes(m[offset:offset + size]). File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 404, in _send_bytes. self._send(header). File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 368, in _send. n = write(self._handle, buf). BrokenPipeError: [Errno 32] Broken pipe. Process ForkPoolWorker-28:. Traceback (most recent call last):. File ""/usr/lib/python3.8/multiprocessing/pool.py"", line 131, in worker. put((job, i, result)). File ""/usr/lib/python3.8/multiprocessing/queues.py"", line 368, in put. self._writer.send_bytes(obj). File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 200, in send_bytes. self._send_bytes(m[offset:offset + size]). File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 404, in _send_bytes. self._send(header). File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 368, in _send. n = write(self._handle, buf). BrokenPipeError: [Errno 32] Broken pipe. (similar records from other workers repeating here ...).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/804
https://github.com/google/deepvariant/issues/804:175,reliability,availab,available,175,"Hi, I am experiencing similar issue - VM, 32 threads, 64GB RAM. Could you provide the experimental container? . Btw, thank you for outstanding work while making this software available for us. This tool has great value is reliable and important for us. Regards, . Tomasz Stokowy, Leader Scientific Computing, University of Bergen, Norway. Running via docker 1.6.1, earlier steps work smoothly. cat /proc/version. Linux version 6.1.0-22-amd64 (debian-kernel@lists.debian.org) (gcc-12 (Debian 12.2.0-14) 12.2.0, GNU ld (GNU Binutils for Debian) 2.40) #1 SMP PREEMPT_DYNAMIC Debian 6.1.94-1 (2024-06-21). Error log:. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""/Reference/core_ref_GRCh38_hla_decoy_ebv/genome.fa"" --infile ""/Output/call_variants_output.tfrecord.gz"" --outfile ""/Output/CoriellIndex.vcf"" --cpus ""32"" --gvcf_outfile ""/Output/CoriellIndex.gvcf"" --nonvariant_site_tfrecord_path ""/Output/gvcf.tfrecord@32.gz"". I0823 15:16:56.752997 139658307389248 postprocess_variants.py:1211] Using sample name from call_variants output. Sample name: CoriellIndex. 2024-08-23 15:16:56.766309: I deepvariant/postprocess_variants.cc:94] Read from: /Output/call_variants_output-00000-of-00001.tfrecord.gz. 2024-08-23 15:18:01.806248: I deepvariant/postprocess_variants.cc:109] Total #entries in single_site_calls = 10880665. I0823 15:20:45.074391 139658307389248 postprocess_variants.py:1313] CVO sorting took 3.805263650417328 minutes. I0823 15:20:45.077561 139658307389248 postprocess_variants.py:1316] Transforming call_variants_output to variants. I0823 15:20:45.077694 139658307389248 postprocess_variants.py:1318] Using 32 CPUs for parallelization of variant transformation. I0823 15:20:51.014987 139658307389248 postprocess_variants.py:1211] Using sample name from call_variants output. Sample name: CoriellIndex. real	8m32.455s. user	7m11.835s. sys	1m25.577s. Process ForkPoolWorker-2:. Traceback (most recent call last):. File ""/usr/lib/python3.8/multiprocess",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/804
https://github.com/google/deepvariant/issues/804:222,reliability,reliab,reliable,222,"Hi, I am experiencing similar issue - VM, 32 threads, 64GB RAM. Could you provide the experimental container? . Btw, thank you for outstanding work while making this software available for us. This tool has great value is reliable and important for us. Regards, . Tomasz Stokowy, Leader Scientific Computing, University of Bergen, Norway. Running via docker 1.6.1, earlier steps work smoothly. cat /proc/version. Linux version 6.1.0-22-amd64 (debian-kernel@lists.debian.org) (gcc-12 (Debian 12.2.0-14) 12.2.0, GNU ld (GNU Binutils for Debian) 2.40) #1 SMP PREEMPT_DYNAMIC Debian 6.1.94-1 (2024-06-21). Error log:. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""/Reference/core_ref_GRCh38_hla_decoy_ebv/genome.fa"" --infile ""/Output/call_variants_output.tfrecord.gz"" --outfile ""/Output/CoriellIndex.vcf"" --cpus ""32"" --gvcf_outfile ""/Output/CoriellIndex.gvcf"" --nonvariant_site_tfrecord_path ""/Output/gvcf.tfrecord@32.gz"". I0823 15:16:56.752997 139658307389248 postprocess_variants.py:1211] Using sample name from call_variants output. Sample name: CoriellIndex. 2024-08-23 15:16:56.766309: I deepvariant/postprocess_variants.cc:94] Read from: /Output/call_variants_output-00000-of-00001.tfrecord.gz. 2024-08-23 15:18:01.806248: I deepvariant/postprocess_variants.cc:109] Total #entries in single_site_calls = 10880665. I0823 15:20:45.074391 139658307389248 postprocess_variants.py:1313] CVO sorting took 3.805263650417328 minutes. I0823 15:20:45.077561 139658307389248 postprocess_variants.py:1316] Transforming call_variants_output to variants. I0823 15:20:45.077694 139658307389248 postprocess_variants.py:1318] Using 32 CPUs for parallelization of variant transformation. I0823 15:20:51.014987 139658307389248 postprocess_variants.py:1211] Using sample name from call_variants output. Sample name: CoriellIndex. real	8m32.455s. user	7m11.835s. sys	1m25.577s. Process ForkPoolWorker-2:. Traceback (most recent call last):. File ""/usr/lib/python3.8/multiprocess",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/804
https://github.com/google/deepvariant/issues/804:175,safety,avail,available,175,"Hi, I am experiencing similar issue - VM, 32 threads, 64GB RAM. Could you provide the experimental container? . Btw, thank you for outstanding work while making this software available for us. This tool has great value is reliable and important for us. Regards, . Tomasz Stokowy, Leader Scientific Computing, University of Bergen, Norway. Running via docker 1.6.1, earlier steps work smoothly. cat /proc/version. Linux version 6.1.0-22-amd64 (debian-kernel@lists.debian.org) (gcc-12 (Debian 12.2.0-14) 12.2.0, GNU ld (GNU Binutils for Debian) 2.40) #1 SMP PREEMPT_DYNAMIC Debian 6.1.94-1 (2024-06-21). Error log:. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""/Reference/core_ref_GRCh38_hla_decoy_ebv/genome.fa"" --infile ""/Output/call_variants_output.tfrecord.gz"" --outfile ""/Output/CoriellIndex.vcf"" --cpus ""32"" --gvcf_outfile ""/Output/CoriellIndex.gvcf"" --nonvariant_site_tfrecord_path ""/Output/gvcf.tfrecord@32.gz"". I0823 15:16:56.752997 139658307389248 postprocess_variants.py:1211] Using sample name from call_variants output. Sample name: CoriellIndex. 2024-08-23 15:16:56.766309: I deepvariant/postprocess_variants.cc:94] Read from: /Output/call_variants_output-00000-of-00001.tfrecord.gz. 2024-08-23 15:18:01.806248: I deepvariant/postprocess_variants.cc:109] Total #entries in single_site_calls = 10880665. I0823 15:20:45.074391 139658307389248 postprocess_variants.py:1313] CVO sorting took 3.805263650417328 minutes. I0823 15:20:45.077561 139658307389248 postprocess_variants.py:1316] Transforming call_variants_output to variants. I0823 15:20:45.077694 139658307389248 postprocess_variants.py:1318] Using 32 CPUs for parallelization of variant transformation. I0823 15:20:51.014987 139658307389248 postprocess_variants.py:1211] Using sample name from call_variants output. Sample name: CoriellIndex. real	8m32.455s. user	7m11.835s. sys	1m25.577s. Process ForkPoolWorker-2:. Traceback (most recent call last):. File ""/usr/lib/python3.8/multiprocess",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/804
https://github.com/google/deepvariant/issues/804:602,safety,Error,Error,602,"Hi, I am experiencing similar issue - VM, 32 threads, 64GB RAM. Could you provide the experimental container? . Btw, thank you for outstanding work while making this software available for us. This tool has great value is reliable and important for us. Regards, . Tomasz Stokowy, Leader Scientific Computing, University of Bergen, Norway. Running via docker 1.6.1, earlier steps work smoothly. cat /proc/version. Linux version 6.1.0-22-amd64 (debian-kernel@lists.debian.org) (gcc-12 (Debian 12.2.0-14) 12.2.0, GNU ld (GNU Binutils for Debian) 2.40) #1 SMP PREEMPT_DYNAMIC Debian 6.1.94-1 (2024-06-21). Error log:. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""/Reference/core_ref_GRCh38_hla_decoy_ebv/genome.fa"" --infile ""/Output/call_variants_output.tfrecord.gz"" --outfile ""/Output/CoriellIndex.vcf"" --cpus ""32"" --gvcf_outfile ""/Output/CoriellIndex.gvcf"" --nonvariant_site_tfrecord_path ""/Output/gvcf.tfrecord@32.gz"". I0823 15:16:56.752997 139658307389248 postprocess_variants.py:1211] Using sample name from call_variants output. Sample name: CoriellIndex. 2024-08-23 15:16:56.766309: I deepvariant/postprocess_variants.cc:94] Read from: /Output/call_variants_output-00000-of-00001.tfrecord.gz. 2024-08-23 15:18:01.806248: I deepvariant/postprocess_variants.cc:109] Total #entries in single_site_calls = 10880665. I0823 15:20:45.074391 139658307389248 postprocess_variants.py:1313] CVO sorting took 3.805263650417328 minutes. I0823 15:20:45.077561 139658307389248 postprocess_variants.py:1316] Transforming call_variants_output to variants. I0823 15:20:45.077694 139658307389248 postprocess_variants.py:1318] Using 32 CPUs for parallelization of variant transformation. I0823 15:20:51.014987 139658307389248 postprocess_variants.py:1211] Using sample name from call_variants output. Sample name: CoriellIndex. real	8m32.455s. user	7m11.835s. sys	1m25.577s. Process ForkPoolWorker-2:. Traceback (most recent call last):. File ""/usr/lib/python3.8/multiprocess",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/804
https://github.com/google/deepvariant/issues/804:608,safety,log,log,608,"Hi, I am experiencing similar issue - VM, 32 threads, 64GB RAM. Could you provide the experimental container? . Btw, thank you for outstanding work while making this software available for us. This tool has great value is reliable and important for us. Regards, . Tomasz Stokowy, Leader Scientific Computing, University of Bergen, Norway. Running via docker 1.6.1, earlier steps work smoothly. cat /proc/version. Linux version 6.1.0-22-amd64 (debian-kernel@lists.debian.org) (gcc-12 (Debian 12.2.0-14) 12.2.0, GNU ld (GNU Binutils for Debian) 2.40) #1 SMP PREEMPT_DYNAMIC Debian 6.1.94-1 (2024-06-21). Error log:. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""/Reference/core_ref_GRCh38_hla_decoy_ebv/genome.fa"" --infile ""/Output/call_variants_output.tfrecord.gz"" --outfile ""/Output/CoriellIndex.vcf"" --cpus ""32"" --gvcf_outfile ""/Output/CoriellIndex.gvcf"" --nonvariant_site_tfrecord_path ""/Output/gvcf.tfrecord@32.gz"". I0823 15:16:56.752997 139658307389248 postprocess_variants.py:1211] Using sample name from call_variants output. Sample name: CoriellIndex. 2024-08-23 15:16:56.766309: I deepvariant/postprocess_variants.cc:94] Read from: /Output/call_variants_output-00000-of-00001.tfrecord.gz. 2024-08-23 15:18:01.806248: I deepvariant/postprocess_variants.cc:109] Total #entries in single_site_calls = 10880665. I0823 15:20:45.074391 139658307389248 postprocess_variants.py:1313] CVO sorting took 3.805263650417328 minutes. I0823 15:20:45.077561 139658307389248 postprocess_variants.py:1316] Transforming call_variants_output to variants. I0823 15:20:45.077694 139658307389248 postprocess_variants.py:1318] Using 32 CPUs for parallelization of variant transformation. I0823 15:20:51.014987 139658307389248 postprocess_variants.py:1211] Using sample name from call_variants output. Sample name: CoriellIndex. real	8m32.455s. user	7m11.835s. sys	1m25.577s. Process ForkPoolWorker-2:. Traceback (most recent call last):. File ""/usr/lib/python3.8/multiprocess",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/804
https://github.com/google/deepvariant/issues/804:2564,safety,except,exception,2564,"t to variants. I0823 15:20:45.077694 139658307389248 postprocess_variants.py:1318] Using 32 CPUs for parallelization of variant transformation. I0823 15:20:51.014987 139658307389248 postprocess_variants.py:1211] Using sample name from call_variants output. Sample name: CoriellIndex. real	8m32.455s. user	7m11.835s. sys	1m25.577s. Process ForkPoolWorker-2:. Traceback (most recent call last):. File ""/usr/lib/python3.8/multiprocessing/pool.py"", line 131, in worker. put((job, i, result)). File ""/usr/lib/python3.8/multiprocessing/queues.py"", line 368, in put. self._writer.send_bytes(obj). File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 200, in send_bytes. self._send_bytes(m[offset:offset + size]). File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 404, in _send_bytes. self._send(header). File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 368, in _send. n = write(self._handle, buf). BrokenPipeError: [Errno 32] Broken pipe. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/usr/lib/python3.8/multiprocessing/process.py"", line 315, in _bootstrap. self.run(). File ""/usr/lib/python3.8/multiprocessing/process.py"", line 108, in run. self._target(*self._args, **self._kwargs). File ""/usr/lib/python3.8/multiprocessing/pool.py"", line 136, in worker. put((job, i, (False, wrapped))). File ""/usr/lib/python3.8/multiprocessing/queues.py"", line 368, in put. self._writer.send_bytes(obj). File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 200, in send_bytes. self._send_bytes(m[offset:offset + size]). File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 404, in _send_bytes. self._send(header). File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 368, in _send. n = write(self._handle, buf). BrokenPipeError: [Errno 32] Broken pipe. Process ForkPoolWorker-28:. Traceback (most recent call last):. File ""/usr/lib/python3.8/multiprocessing/pool.py"", line 131, in worker. put((",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/804
https://github.com/google/deepvariant/issues/804:2583,safety,except,exception,2583,"3 15:20:45.077694 139658307389248 postprocess_variants.py:1318] Using 32 CPUs for parallelization of variant transformation. I0823 15:20:51.014987 139658307389248 postprocess_variants.py:1211] Using sample name from call_variants output. Sample name: CoriellIndex. real	8m32.455s. user	7m11.835s. sys	1m25.577s. Process ForkPoolWorker-2:. Traceback (most recent call last):. File ""/usr/lib/python3.8/multiprocessing/pool.py"", line 131, in worker. put((job, i, result)). File ""/usr/lib/python3.8/multiprocessing/queues.py"", line 368, in put. self._writer.send_bytes(obj). File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 200, in send_bytes. self._send_bytes(m[offset:offset + size]). File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 404, in _send_bytes. self._send(header). File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 368, in _send. n = write(self._handle, buf). BrokenPipeError: [Errno 32] Broken pipe. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/usr/lib/python3.8/multiprocessing/process.py"", line 315, in _bootstrap. self.run(). File ""/usr/lib/python3.8/multiprocessing/process.py"", line 108, in run. self._target(*self._args, **self._kwargs). File ""/usr/lib/python3.8/multiprocessing/pool.py"", line 136, in worker. put((job, i, (False, wrapped))). File ""/usr/lib/python3.8/multiprocessing/queues.py"", line 368, in put. self._writer.send_bytes(obj). File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 200, in send_bytes. self._send_bytes(m[offset:offset + size]). File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 404, in _send_bytes. self._send(header). File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 368, in _send. n = write(self._handle, buf). BrokenPipeError: [Errno 32] Broken pipe. Process ForkPoolWorker-28:. Traceback (most recent call last):. File ""/usr/lib/python3.8/multiprocessing/pool.py"", line 131, in worker. put((job, i, result)). F",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/804
https://github.com/google/deepvariant/issues/804:175,security,availab,available,175,"Hi, I am experiencing similar issue - VM, 32 threads, 64GB RAM. Could you provide the experimental container? . Btw, thank you for outstanding work while making this software available for us. This tool has great value is reliable and important for us. Regards, . Tomasz Stokowy, Leader Scientific Computing, University of Bergen, Norway. Running via docker 1.6.1, earlier steps work smoothly. cat /proc/version. Linux version 6.1.0-22-amd64 (debian-kernel@lists.debian.org) (gcc-12 (Debian 12.2.0-14) 12.2.0, GNU ld (GNU Binutils for Debian) 2.40) #1 SMP PREEMPT_DYNAMIC Debian 6.1.94-1 (2024-06-21). Error log:. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""/Reference/core_ref_GRCh38_hla_decoy_ebv/genome.fa"" --infile ""/Output/call_variants_output.tfrecord.gz"" --outfile ""/Output/CoriellIndex.vcf"" --cpus ""32"" --gvcf_outfile ""/Output/CoriellIndex.gvcf"" --nonvariant_site_tfrecord_path ""/Output/gvcf.tfrecord@32.gz"". I0823 15:16:56.752997 139658307389248 postprocess_variants.py:1211] Using sample name from call_variants output. Sample name: CoriellIndex. 2024-08-23 15:16:56.766309: I deepvariant/postprocess_variants.cc:94] Read from: /Output/call_variants_output-00000-of-00001.tfrecord.gz. 2024-08-23 15:18:01.806248: I deepvariant/postprocess_variants.cc:109] Total #entries in single_site_calls = 10880665. I0823 15:20:45.074391 139658307389248 postprocess_variants.py:1313] CVO sorting took 3.805263650417328 minutes. I0823 15:20:45.077561 139658307389248 postprocess_variants.py:1316] Transforming call_variants_output to variants. I0823 15:20:45.077694 139658307389248 postprocess_variants.py:1318] Using 32 CPUs for parallelization of variant transformation. I0823 15:20:51.014987 139658307389248 postprocess_variants.py:1211] Using sample name from call_variants output. Sample name: CoriellIndex. real	8m32.455s. user	7m11.835s. sys	1m25.577s. Process ForkPoolWorker-2:. Traceback (most recent call last):. File ""/usr/lib/python3.8/multiprocess",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/804
https://github.com/google/deepvariant/issues/804:608,security,log,log,608,"Hi, I am experiencing similar issue - VM, 32 threads, 64GB RAM. Could you provide the experimental container? . Btw, thank you for outstanding work while making this software available for us. This tool has great value is reliable and important for us. Regards, . Tomasz Stokowy, Leader Scientific Computing, University of Bergen, Norway. Running via docker 1.6.1, earlier steps work smoothly. cat /proc/version. Linux version 6.1.0-22-amd64 (debian-kernel@lists.debian.org) (gcc-12 (Debian 12.2.0-14) 12.2.0, GNU ld (GNU Binutils for Debian) 2.40) #1 SMP PREEMPT_DYNAMIC Debian 6.1.94-1 (2024-06-21). Error log:. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""/Reference/core_ref_GRCh38_hla_decoy_ebv/genome.fa"" --infile ""/Output/call_variants_output.tfrecord.gz"" --outfile ""/Output/CoriellIndex.vcf"" --cpus ""32"" --gvcf_outfile ""/Output/CoriellIndex.gvcf"" --nonvariant_site_tfrecord_path ""/Output/gvcf.tfrecord@32.gz"". I0823 15:16:56.752997 139658307389248 postprocess_variants.py:1211] Using sample name from call_variants output. Sample name: CoriellIndex. 2024-08-23 15:16:56.766309: I deepvariant/postprocess_variants.cc:94] Read from: /Output/call_variants_output-00000-of-00001.tfrecord.gz. 2024-08-23 15:18:01.806248: I deepvariant/postprocess_variants.cc:109] Total #entries in single_site_calls = 10880665. I0823 15:20:45.074391 139658307389248 postprocess_variants.py:1313] CVO sorting took 3.805263650417328 minutes. I0823 15:20:45.077561 139658307389248 postprocess_variants.py:1316] Transforming call_variants_output to variants. I0823 15:20:45.077694 139658307389248 postprocess_variants.py:1318] Using 32 CPUs for parallelization of variant transformation. I0823 15:20:51.014987 139658307389248 postprocess_variants.py:1211] Using sample name from call_variants output. Sample name: CoriellIndex. real	8m32.455s. user	7m11.835s. sys	1m25.577s. Process ForkPoolWorker-2:. Traceback (most recent call last):. File ""/usr/lib/python3.8/multiprocess",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/804
https://github.com/google/deepvariant/issues/804:608,testability,log,log,608,"Hi, I am experiencing similar issue - VM, 32 threads, 64GB RAM. Could you provide the experimental container? . Btw, thank you for outstanding work while making this software available for us. This tool has great value is reliable and important for us. Regards, . Tomasz Stokowy, Leader Scientific Computing, University of Bergen, Norway. Running via docker 1.6.1, earlier steps work smoothly. cat /proc/version. Linux version 6.1.0-22-amd64 (debian-kernel@lists.debian.org) (gcc-12 (Debian 12.2.0-14) 12.2.0, GNU ld (GNU Binutils for Debian) 2.40) #1 SMP PREEMPT_DYNAMIC Debian 6.1.94-1 (2024-06-21). Error log:. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""/Reference/core_ref_GRCh38_hla_decoy_ebv/genome.fa"" --infile ""/Output/call_variants_output.tfrecord.gz"" --outfile ""/Output/CoriellIndex.vcf"" --cpus ""32"" --gvcf_outfile ""/Output/CoriellIndex.gvcf"" --nonvariant_site_tfrecord_path ""/Output/gvcf.tfrecord@32.gz"". I0823 15:16:56.752997 139658307389248 postprocess_variants.py:1211] Using sample name from call_variants output. Sample name: CoriellIndex. 2024-08-23 15:16:56.766309: I deepvariant/postprocess_variants.cc:94] Read from: /Output/call_variants_output-00000-of-00001.tfrecord.gz. 2024-08-23 15:18:01.806248: I deepvariant/postprocess_variants.cc:109] Total #entries in single_site_calls = 10880665. I0823 15:20:45.074391 139658307389248 postprocess_variants.py:1313] CVO sorting took 3.805263650417328 minutes. I0823 15:20:45.077561 139658307389248 postprocess_variants.py:1316] Transforming call_variants_output to variants. I0823 15:20:45.077694 139658307389248 postprocess_variants.py:1318] Using 32 CPUs for parallelization of variant transformation. I0823 15:20:51.014987 139658307389248 postprocess_variants.py:1211] Using sample name from call_variants output. Sample name: CoriellIndex. real	8m32.455s. user	7m11.835s. sys	1m25.577s. Process ForkPoolWorker-2:. Traceback (most recent call last):. File ""/usr/lib/python3.8/multiprocess",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/804
https://github.com/google/deepvariant/issues/804:1927,testability,Trace,Traceback,1927,"utput/gvcf.tfrecord@32.gz"". I0823 15:16:56.752997 139658307389248 postprocess_variants.py:1211] Using sample name from call_variants output. Sample name: CoriellIndex. 2024-08-23 15:16:56.766309: I deepvariant/postprocess_variants.cc:94] Read from: /Output/call_variants_output-00000-of-00001.tfrecord.gz. 2024-08-23 15:18:01.806248: I deepvariant/postprocess_variants.cc:109] Total #entries in single_site_calls = 10880665. I0823 15:20:45.074391 139658307389248 postprocess_variants.py:1313] CVO sorting took 3.805263650417328 minutes. I0823 15:20:45.077561 139658307389248 postprocess_variants.py:1316] Transforming call_variants_output to variants. I0823 15:20:45.077694 139658307389248 postprocess_variants.py:1318] Using 32 CPUs for parallelization of variant transformation. I0823 15:20:51.014987 139658307389248 postprocess_variants.py:1211] Using sample name from call_variants output. Sample name: CoriellIndex. real	8m32.455s. user	7m11.835s. sys	1m25.577s. Process ForkPoolWorker-2:. Traceback (most recent call last):. File ""/usr/lib/python3.8/multiprocessing/pool.py"", line 131, in worker. put((job, i, result)). File ""/usr/lib/python3.8/multiprocessing/queues.py"", line 368, in put. self._writer.send_bytes(obj). File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 200, in send_bytes. self._send_bytes(m[offset:offset + size]). File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 404, in _send_bytes. self._send(header). File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 368, in _send. n = write(self._handle, buf). BrokenPipeError: [Errno 32] Broken pipe. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/usr/lib/python3.8/multiprocessing/process.py"", line 315, in _bootstrap. self.run(). File ""/usr/lib/python3.8/multiprocessing/process.py"", line 108, in run. self._target(*self._args, **self._kwargs). File ""/usr/lib/python3.8/multiprocessing/pool.py"", line 136, in worker. put((job, i, (",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/804
https://github.com/google/deepvariant/issues/804:2604,testability,Trace,Traceback,2604,"658307389248 postprocess_variants.py:1318] Using 32 CPUs for parallelization of variant transformation. I0823 15:20:51.014987 139658307389248 postprocess_variants.py:1211] Using sample name from call_variants output. Sample name: CoriellIndex. real	8m32.455s. user	7m11.835s. sys	1m25.577s. Process ForkPoolWorker-2:. Traceback (most recent call last):. File ""/usr/lib/python3.8/multiprocessing/pool.py"", line 131, in worker. put((job, i, result)). File ""/usr/lib/python3.8/multiprocessing/queues.py"", line 368, in put. self._writer.send_bytes(obj). File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 200, in send_bytes. self._send_bytes(m[offset:offset + size]). File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 404, in _send_bytes. self._send(header). File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 368, in _send. n = write(self._handle, buf). BrokenPipeError: [Errno 32] Broken pipe. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/usr/lib/python3.8/multiprocessing/process.py"", line 315, in _bootstrap. self.run(). File ""/usr/lib/python3.8/multiprocessing/process.py"", line 108, in run. self._target(*self._args, **self._kwargs). File ""/usr/lib/python3.8/multiprocessing/pool.py"", line 136, in worker. put((job, i, (False, wrapped))). File ""/usr/lib/python3.8/multiprocessing/queues.py"", line 368, in put. self._writer.send_bytes(obj). File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 200, in send_bytes. self._send_bytes(m[offset:offset + size]). File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 404, in _send_bytes. self._send(header). File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 368, in _send. n = write(self._handle, buf). BrokenPipeError: [Errno 32] Broken pipe. Process ForkPoolWorker-28:. Traceback (most recent call last):. File ""/usr/lib/python3.8/multiprocessing/pool.py"", line 131, in worker. put((job, i, result)). File ""/usr/lib/python3",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/804
https://github.com/google/deepvariant/issues/804:3456,testability,Trace,Traceback,3456," in put. self._writer.send_bytes(obj). File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 200, in send_bytes. self._send_bytes(m[offset:offset + size]). File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 404, in _send_bytes. self._send(header). File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 368, in _send. n = write(self._handle, buf). BrokenPipeError: [Errno 32] Broken pipe. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/usr/lib/python3.8/multiprocessing/process.py"", line 315, in _bootstrap. self.run(). File ""/usr/lib/python3.8/multiprocessing/process.py"", line 108, in run. self._target(*self._args, **self._kwargs). File ""/usr/lib/python3.8/multiprocessing/pool.py"", line 136, in worker. put((job, i, (False, wrapped))). File ""/usr/lib/python3.8/multiprocessing/queues.py"", line 368, in put. self._writer.send_bytes(obj). File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 200, in send_bytes. self._send_bytes(m[offset:offset + size]). File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 404, in _send_bytes. self._send(header). File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 368, in _send. n = write(self._handle, buf). BrokenPipeError: [Errno 32] Broken pipe. Process ForkPoolWorker-28:. Traceback (most recent call last):. File ""/usr/lib/python3.8/multiprocessing/pool.py"", line 131, in worker. put((job, i, result)). File ""/usr/lib/python3.8/multiprocessing/queues.py"", line 368, in put. self._writer.send_bytes(obj). File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 200, in send_bytes. self._send_bytes(m[offset:offset + size]). File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 404, in _send_bytes. self._send(header). File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 368, in _send. n = write(self._handle, buf). BrokenPipeError: [Errno 32] Broken pipe. (similar records from other workers repeating here ...).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/804
https://github.com/google/deepvariant/issues/804:9,usability,experien,experiencing,9,"Hi, I am experiencing similar issue - VM, 32 threads, 64GB RAM. Could you provide the experimental container? . Btw, thank you for outstanding work while making this software available for us. This tool has great value is reliable and important for us. Regards, . Tomasz Stokowy, Leader Scientific Computing, University of Bergen, Norway. Running via docker 1.6.1, earlier steps work smoothly. cat /proc/version. Linux version 6.1.0-22-amd64 (debian-kernel@lists.debian.org) (gcc-12 (Debian 12.2.0-14) 12.2.0, GNU ld (GNU Binutils for Debian) 2.40) #1 SMP PREEMPT_DYNAMIC Debian 6.1.94-1 (2024-06-21). Error log:. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""/Reference/core_ref_GRCh38_hla_decoy_ebv/genome.fa"" --infile ""/Output/call_variants_output.tfrecord.gz"" --outfile ""/Output/CoriellIndex.vcf"" --cpus ""32"" --gvcf_outfile ""/Output/CoriellIndex.gvcf"" --nonvariant_site_tfrecord_path ""/Output/gvcf.tfrecord@32.gz"". I0823 15:16:56.752997 139658307389248 postprocess_variants.py:1211] Using sample name from call_variants output. Sample name: CoriellIndex. 2024-08-23 15:16:56.766309: I deepvariant/postprocess_variants.cc:94] Read from: /Output/call_variants_output-00000-of-00001.tfrecord.gz. 2024-08-23 15:18:01.806248: I deepvariant/postprocess_variants.cc:109] Total #entries in single_site_calls = 10880665. I0823 15:20:45.074391 139658307389248 postprocess_variants.py:1313] CVO sorting took 3.805263650417328 minutes. I0823 15:20:45.077561 139658307389248 postprocess_variants.py:1316] Transforming call_variants_output to variants. I0823 15:20:45.077694 139658307389248 postprocess_variants.py:1318] Using 32 CPUs for parallelization of variant transformation. I0823 15:20:51.014987 139658307389248 postprocess_variants.py:1211] Using sample name from call_variants output. Sample name: CoriellIndex. real	8m32.455s. user	7m11.835s. sys	1m25.577s. Process ForkPoolWorker-2:. Traceback (most recent call last):. File ""/usr/lib/python3.8/multiprocess",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/804
https://github.com/google/deepvariant/issues/804:198,usability,tool,tool,198,"Hi, I am experiencing similar issue - VM, 32 threads, 64GB RAM. Could you provide the experimental container? . Btw, thank you for outstanding work while making this software available for us. This tool has great value is reliable and important for us. Regards, . Tomasz Stokowy, Leader Scientific Computing, University of Bergen, Norway. Running via docker 1.6.1, earlier steps work smoothly. cat /proc/version. Linux version 6.1.0-22-amd64 (debian-kernel@lists.debian.org) (gcc-12 (Debian 12.2.0-14) 12.2.0, GNU ld (GNU Binutils for Debian) 2.40) #1 SMP PREEMPT_DYNAMIC Debian 6.1.94-1 (2024-06-21). Error log:. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""/Reference/core_ref_GRCh38_hla_decoy_ebv/genome.fa"" --infile ""/Output/call_variants_output.tfrecord.gz"" --outfile ""/Output/CoriellIndex.vcf"" --cpus ""32"" --gvcf_outfile ""/Output/CoriellIndex.gvcf"" --nonvariant_site_tfrecord_path ""/Output/gvcf.tfrecord@32.gz"". I0823 15:16:56.752997 139658307389248 postprocess_variants.py:1211] Using sample name from call_variants output. Sample name: CoriellIndex. 2024-08-23 15:16:56.766309: I deepvariant/postprocess_variants.cc:94] Read from: /Output/call_variants_output-00000-of-00001.tfrecord.gz. 2024-08-23 15:18:01.806248: I deepvariant/postprocess_variants.cc:109] Total #entries in single_site_calls = 10880665. I0823 15:20:45.074391 139658307389248 postprocess_variants.py:1313] CVO sorting took 3.805263650417328 minutes. I0823 15:20:45.077561 139658307389248 postprocess_variants.py:1316] Transforming call_variants_output to variants. I0823 15:20:45.077694 139658307389248 postprocess_variants.py:1318] Using 32 CPUs for parallelization of variant transformation. I0823 15:20:51.014987 139658307389248 postprocess_variants.py:1211] Using sample name from call_variants output. Sample name: CoriellIndex. real	8m32.455s. user	7m11.835s. sys	1m25.577s. Process ForkPoolWorker-2:. Traceback (most recent call last):. File ""/usr/lib/python3.8/multiprocess",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/804
https://github.com/google/deepvariant/issues/804:602,usability,Error,Error,602,"Hi, I am experiencing similar issue - VM, 32 threads, 64GB RAM. Could you provide the experimental container? . Btw, thank you for outstanding work while making this software available for us. This tool has great value is reliable and important for us. Regards, . Tomasz Stokowy, Leader Scientific Computing, University of Bergen, Norway. Running via docker 1.6.1, earlier steps work smoothly. cat /proc/version. Linux version 6.1.0-22-amd64 (debian-kernel@lists.debian.org) (gcc-12 (Debian 12.2.0-14) 12.2.0, GNU ld (GNU Binutils for Debian) 2.40) #1 SMP PREEMPT_DYNAMIC Debian 6.1.94-1 (2024-06-21). Error log:. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""/Reference/core_ref_GRCh38_hla_decoy_ebv/genome.fa"" --infile ""/Output/call_variants_output.tfrecord.gz"" --outfile ""/Output/CoriellIndex.vcf"" --cpus ""32"" --gvcf_outfile ""/Output/CoriellIndex.gvcf"" --nonvariant_site_tfrecord_path ""/Output/gvcf.tfrecord@32.gz"". I0823 15:16:56.752997 139658307389248 postprocess_variants.py:1211] Using sample name from call_variants output. Sample name: CoriellIndex. 2024-08-23 15:16:56.766309: I deepvariant/postprocess_variants.cc:94] Read from: /Output/call_variants_output-00000-of-00001.tfrecord.gz. 2024-08-23 15:18:01.806248: I deepvariant/postprocess_variants.cc:109] Total #entries in single_site_calls = 10880665. I0823 15:20:45.074391 139658307389248 postprocess_variants.py:1313] CVO sorting took 3.805263650417328 minutes. I0823 15:20:45.077561 139658307389248 postprocess_variants.py:1316] Transforming call_variants_output to variants. I0823 15:20:45.077694 139658307389248 postprocess_variants.py:1318] Using 32 CPUs for parallelization of variant transformation. I0823 15:20:51.014987 139658307389248 postprocess_variants.py:1211] Using sample name from call_variants output. Sample name: CoriellIndex. real	8m32.455s. user	7m11.835s. sys	1m25.577s. Process ForkPoolWorker-2:. Traceback (most recent call last):. File ""/usr/lib/python3.8/multiprocess",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/804
https://github.com/google/deepvariant/issues/804:632,usability,command,command,632,"Hi, I am experiencing similar issue - VM, 32 threads, 64GB RAM. Could you provide the experimental container? . Btw, thank you for outstanding work while making this software available for us. This tool has great value is reliable and important for us. Regards, . Tomasz Stokowy, Leader Scientific Computing, University of Bergen, Norway. Running via docker 1.6.1, earlier steps work smoothly. cat /proc/version. Linux version 6.1.0-22-amd64 (debian-kernel@lists.debian.org) (gcc-12 (Debian 12.2.0-14) 12.2.0, GNU ld (GNU Binutils for Debian) 2.40) #1 SMP PREEMPT_DYNAMIC Debian 6.1.94-1 (2024-06-21). Error log:. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""/Reference/core_ref_GRCh38_hla_decoy_ebv/genome.fa"" --infile ""/Output/call_variants_output.tfrecord.gz"" --outfile ""/Output/CoriellIndex.vcf"" --cpus ""32"" --gvcf_outfile ""/Output/CoriellIndex.gvcf"" --nonvariant_site_tfrecord_path ""/Output/gvcf.tfrecord@32.gz"". I0823 15:16:56.752997 139658307389248 postprocess_variants.py:1211] Using sample name from call_variants output. Sample name: CoriellIndex. 2024-08-23 15:16:56.766309: I deepvariant/postprocess_variants.cc:94] Read from: /Output/call_variants_output-00000-of-00001.tfrecord.gz. 2024-08-23 15:18:01.806248: I deepvariant/postprocess_variants.cc:109] Total #entries in single_site_calls = 10880665. I0823 15:20:45.074391 139658307389248 postprocess_variants.py:1313] CVO sorting took 3.805263650417328 minutes. I0823 15:20:45.077561 139658307389248 postprocess_variants.py:1316] Transforming call_variants_output to variants. I0823 15:20:45.077694 139658307389248 postprocess_variants.py:1318] Using 32 CPUs for parallelization of variant transformation. I0823 15:20:51.014987 139658307389248 postprocess_variants.py:1211] Using sample name from call_variants output. Sample name: CoriellIndex. real	8m32.455s. user	7m11.835s. sys	1m25.577s. Process ForkPoolWorker-2:. Traceback (most recent call last):. File ""/usr/lib/python3.8/multiprocess",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/804
https://github.com/google/deepvariant/issues/804:1869,usability,user,user,1869,"Output/CoriellIndex.gvcf"" --nonvariant_site_tfrecord_path ""/Output/gvcf.tfrecord@32.gz"". I0823 15:16:56.752997 139658307389248 postprocess_variants.py:1211] Using sample name from call_variants output. Sample name: CoriellIndex. 2024-08-23 15:16:56.766309: I deepvariant/postprocess_variants.cc:94] Read from: /Output/call_variants_output-00000-of-00001.tfrecord.gz. 2024-08-23 15:18:01.806248: I deepvariant/postprocess_variants.cc:109] Total #entries in single_site_calls = 10880665. I0823 15:20:45.074391 139658307389248 postprocess_variants.py:1313] CVO sorting took 3.805263650417328 minutes. I0823 15:20:45.077561 139658307389248 postprocess_variants.py:1316] Transforming call_variants_output to variants. I0823 15:20:45.077694 139658307389248 postprocess_variants.py:1318] Using 32 CPUs for parallelization of variant transformation. I0823 15:20:51.014987 139658307389248 postprocess_variants.py:1211] Using sample name from call_variants output. Sample name: CoriellIndex. real	8m32.455s. user	7m11.835s. sys	1m25.577s. Process ForkPoolWorker-2:. Traceback (most recent call last):. File ""/usr/lib/python3.8/multiprocessing/pool.py"", line 131, in worker. put((job, i, result)). File ""/usr/lib/python3.8/multiprocessing/queues.py"", line 368, in put. self._writer.send_bytes(obj). File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 200, in send_bytes. self._send_bytes(m[offset:offset + size]). File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 404, in _send_bytes. self._send(header). File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 368, in _send. n = write(self._handle, buf). BrokenPipeError: [Errno 32] Broken pipe. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/usr/lib/python3.8/multiprocessing/process.py"", line 315, in _bootstrap. self.run(). File ""/usr/lib/python3.8/multiprocessing/process.py"", line 108, in run. self._target(*self._args, **self._kwargs). File ""/usr/lib/python3.8/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/804
https://github.com/google/deepvariant/issues/806:238,deployability,version,version,238,"Hi @Zer0day-0 ,. thanks for the report. DeepVariant conda actually isn't maintained by our team. I can take a look later this week. . But before that , I have a question - is it possible for you to use other solutions, such as our Docker version or Singularity? If you can describe your situation and why you might only be able to use conda, that will also be useful context for us. . Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/806
https://github.com/google/deepvariant/issues/806:238,integrability,version,version,238,"Hi @Zer0day-0 ,. thanks for the report. DeepVariant conda actually isn't maintained by our team. I can take a look later this week. . But before that , I have a question - is it possible for you to use other solutions, such as our Docker version or Singularity? If you can describe your situation and why you might only be able to use conda, that will also be useful context for us. . Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/806
https://github.com/google/deepvariant/issues/806:73,modifiability,maintain,maintained,73,"Hi @Zer0day-0 ,. thanks for the report. DeepVariant conda actually isn't maintained by our team. I can take a look later this week. . But before that , I have a question - is it possible for you to use other solutions, such as our Docker version or Singularity? If you can describe your situation and why you might only be able to use conda, that will also be useful context for us. . Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/806
https://github.com/google/deepvariant/issues/806:238,modifiability,version,version,238,"Hi @Zer0day-0 ,. thanks for the report. DeepVariant conda actually isn't maintained by our team. I can take a look later this week. . But before that , I have a question - is it possible for you to use other solutions, such as our Docker version or Singularity? If you can describe your situation and why you might only be able to use conda, that will also be useful context for us. . Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/806
https://github.com/google/deepvariant/issues/806:73,safety,maintain,maintained,73,"Hi @Zer0day-0 ,. thanks for the report. DeepVariant conda actually isn't maintained by our team. I can take a look later this week. . But before that , I have a question - is it possible for you to use other solutions, such as our Docker version or Singularity? If you can describe your situation and why you might only be able to use conda, that will also be useful context for us. . Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/806
https://github.com/google/deepvariant/issues/806:91,security,team,team,91,"Hi @Zer0day-0 ,. thanks for the report. DeepVariant conda actually isn't maintained by our team. I can take a look later this week. . But before that , I have a question - is it possible for you to use other solutions, such as our Docker version or Singularity? If you can describe your situation and why you might only be able to use conda, that will also be useful context for us. . Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/806
https://github.com/google/deepvariant/issues/806:367,testability,context,context,367,"Hi @Zer0day-0 ,. thanks for the report. DeepVariant conda actually isn't maintained by our team. I can take a look later this week. . But before that , I have a question - is it possible for you to use other solutions, such as our Docker version or Singularity? If you can describe your situation and why you might only be able to use conda, that will also be useful context for us. . Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/806
https://github.com/google/deepvariant/issues/806:67,availability,cluster,cluster,67,"Hello @pichuan, . Thanks for response, actually I am running a SGE cluster in order to run my analysis, and the sysadmins keeps use of docker restricted(root access issue). Hence I am tryng to use conda.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/806
https://github.com/google/deepvariant/issues/806:67,deployability,cluster,cluster,67,"Hello @pichuan, . Thanks for response, actually I am running a SGE cluster in order to run my analysis, and the sysadmins keeps use of docker restricted(root access issue). Hence I am tryng to use conda.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/806
https://github.com/google/deepvariant/issues/806:158,security,access,access,158,"Hello @pichuan, . Thanks for response, actually I am running a SGE cluster in order to run my analysis, and the sysadmins keeps use of docker restricted(root access issue). Hence I am tryng to use conda.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/806
https://github.com/google/deepvariant/issues/806:1373,availability,error,error,1373,"R}-centos7"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""centos-7"" \. --image-project ""centos-cloud"" \. --machine-type ""e2-standard-16"" \. --zone ""us-west1-b"". gcloud compute ssh pichuan-centos7 --zone ""us-west1-b"". ```. Version:. ```. $ uname -a. Linux pichuan-centos7 3.10.0-1160.114.2.el7.x86_64 #1 SMP Wed Mar 20 15:54:52 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux. ```. Install conda:. ```bash. curl https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh > Miniconda3-latest-Linux-x86_64.sh. bash Miniconda3-latest-Linux-x86_64.sh -b -u -p $HOME/miniconda. eval ""$(${HOME}/miniconda/bin/conda shell.bash hook)"". ```. To repeat what I did in, I tried: https://github.com/google/deepvariant/issues/736#issuecomment-1829204521. ```bash. conda config --add channels defaults && \. conda config --add channels bioconda && \. conda config --add channels conda-forge. conda create -y -n dv-env deepvariant. conda activate dv-env. ```. It completed without any error messages. I see:. ```. (dv-env) [pichuan@pichuan-centos7 ~]$ ls /home/pichuan/miniconda/envs/dv-env/share/. bash-completion deepvariant-1.5.0-0 doc et examples google-cloud-sdk-359.0.0-0 icu info keyutils licenses locale man tabset terminfo zsh. (dv-env) [pichuan@pichuan-centos7 ~]$ ls /home/pichuan/miniconda/envs/dv-env/share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0. call_variants_keras.zip freeze_graph.zip make_examples.zip multisample_make_examples.zip runtime_by_region_vis.zip vcf_stats_report.zip. call_variants.zip licenses.zip model_eval.zip postprocess_variants.zip settings.sh. deeptrio make_examples_somatic.zip model_train.zip run-prereq.sh show_examples.zip. ```. As mentioned in my previous investigation, I don't actually know how to use Conda though. But it seems like at least with the setting above, it can install. If you can try that and see if it works for you. And do what you'd usually do to run and see if it works, that will be great! Please le",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/806
https://github.com/google/deepvariant/issues/806:86,deployability,instal,install,86,"Thank you @Zer0day-0 . I went back and looked at my notes on the last time I tried to install with Conda: https://github.com/google/deepvariant/issues/736#issuecomment-1829204521. I noticed that you're using CentOS. I think I might be testing with CentOS at the time. So let me try that. # Get a CentOS machine to test. I used:. ```bash. gcloud compute instances create ""${USER}-centos7"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""centos-7"" \. --image-project ""centos-cloud"" \. --machine-type ""e2-standard-16"" \. --zone ""us-west1-b"". gcloud compute ssh pichuan-centos7 --zone ""us-west1-b"". ```. Version:. ```. $ uname -a. Linux pichuan-centos7 3.10.0-1160.114.2.el7.x86_64 #1 SMP Wed Mar 20 15:54:52 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux. ```. Install conda:. ```bash. curl https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh > Miniconda3-latest-Linux-x86_64.sh. bash Miniconda3-latest-Linux-x86_64.sh -b -u -p $HOME/miniconda. eval ""$(${HOME}/miniconda/bin/conda shell.bash hook)"". ```. To repeat what I did in, I tried: https://github.com/google/deepvariant/issues/736#issuecomment-1829204521. ```bash. conda config --add channels defaults && \. conda config --add channels bioconda && \. conda config --add channels conda-forge. conda create -y -n dv-env deepvariant. conda activate dv-env. ```. It completed without any error messages. I see:. ```. (dv-env) [pichuan@pichuan-centos7 ~]$ ls /home/pichuan/miniconda/envs/dv-env/share/. bash-completion deepvariant-1.5.0-0 doc et examples google-cloud-sdk-359.0.0-0 icu info keyutils licenses locale man tabset terminfo zsh. (dv-env) [pichuan@pichuan-centos7 ~]$ ls /home/pichuan/miniconda/envs/dv-env/share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0. call_variants_keras.zip freeze_graph.zip make_examples.zip multisample_make_examples.zip runtime_by_region_vis.zip vcf_stats_report.zip. call_variants.zip licenses.zip model_eval.zip postprocess_variants.zip settings.sh. deept",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/806
https://github.com/google/deepvariant/issues/806:624,deployability,Version,Version,624,"Thank you @Zer0day-0 . I went back and looked at my notes on the last time I tried to install with Conda: https://github.com/google/deepvariant/issues/736#issuecomment-1829204521. I noticed that you're using CentOS. I think I might be testing with CentOS at the time. So let me try that. # Get a CentOS machine to test. I used:. ```bash. gcloud compute instances create ""${USER}-centos7"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""centos-7"" \. --image-project ""centos-cloud"" \. --machine-type ""e2-standard-16"" \. --zone ""us-west1-b"". gcloud compute ssh pichuan-centos7 --zone ""us-west1-b"". ```. Version:. ```. $ uname -a. Linux pichuan-centos7 3.10.0-1160.114.2.el7.x86_64 #1 SMP Wed Mar 20 15:54:52 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux. ```. Install conda:. ```bash. curl https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh > Miniconda3-latest-Linux-x86_64.sh. bash Miniconda3-latest-Linux-x86_64.sh -b -u -p $HOME/miniconda. eval ""$(${HOME}/miniconda/bin/conda shell.bash hook)"". ```. To repeat what I did in, I tried: https://github.com/google/deepvariant/issues/736#issuecomment-1829204521. ```bash. conda config --add channels defaults && \. conda config --add channels bioconda && \. conda config --add channels conda-forge. conda create -y -n dv-env deepvariant. conda activate dv-env. ```. It completed without any error messages. I see:. ```. (dv-env) [pichuan@pichuan-centos7 ~]$ ls /home/pichuan/miniconda/envs/dv-env/share/. bash-completion deepvariant-1.5.0-0 doc et examples google-cloud-sdk-359.0.0-0 icu info keyutils licenses locale man tabset terminfo zsh. (dv-env) [pichuan@pichuan-centos7 ~]$ ls /home/pichuan/miniconda/envs/dv-env/share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0. call_variants_keras.zip freeze_graph.zip make_examples.zip multisample_make_examples.zip runtime_by_region_vis.zip vcf_stats_report.zip. call_variants.zip licenses.zip model_eval.zip postprocess_variants.zip settings.sh. deept",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/806
https://github.com/google/deepvariant/issues/806:775,deployability,Instal,Install,775,"Thank you @Zer0day-0 . I went back and looked at my notes on the last time I tried to install with Conda: https://github.com/google/deepvariant/issues/736#issuecomment-1829204521. I noticed that you're using CentOS. I think I might be testing with CentOS at the time. So let me try that. # Get a CentOS machine to test. I used:. ```bash. gcloud compute instances create ""${USER}-centos7"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""centos-7"" \. --image-project ""centos-cloud"" \. --machine-type ""e2-standard-16"" \. --zone ""us-west1-b"". gcloud compute ssh pichuan-centos7 --zone ""us-west1-b"". ```. Version:. ```. $ uname -a. Linux pichuan-centos7 3.10.0-1160.114.2.el7.x86_64 #1 SMP Wed Mar 20 15:54:52 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux. ```. Install conda:. ```bash. curl https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh > Miniconda3-latest-Linux-x86_64.sh. bash Miniconda3-latest-Linux-x86_64.sh -b -u -p $HOME/miniconda. eval ""$(${HOME}/miniconda/bin/conda shell.bash hook)"". ```. To repeat what I did in, I tried: https://github.com/google/deepvariant/issues/736#issuecomment-1829204521. ```bash. conda config --add channels defaults && \. conda config --add channels bioconda && \. conda config --add channels conda-forge. conda create -y -n dv-env deepvariant. conda activate dv-env. ```. It completed without any error messages. I see:. ```. (dv-env) [pichuan@pichuan-centos7 ~]$ ls /home/pichuan/miniconda/envs/dv-env/share/. bash-completion deepvariant-1.5.0-0 doc et examples google-cloud-sdk-359.0.0-0 icu info keyutils licenses locale man tabset terminfo zsh. (dv-env) [pichuan@pichuan-centos7 ~]$ ls /home/pichuan/miniconda/envs/dv-env/share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0. call_variants_keras.zip freeze_graph.zip make_examples.zip multisample_make_examples.zip runtime_by_region_vis.zip vcf_stats_report.zip. call_variants.zip licenses.zip model_eval.zip postprocess_variants.zip settings.sh. deept",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/806
https://github.com/google/deepvariant/issues/806:2232,deployability,instal,install,2232,"val ""$(${HOME}/miniconda/bin/conda shell.bash hook)"". ```. To repeat what I did in, I tried: https://github.com/google/deepvariant/issues/736#issuecomment-1829204521. ```bash. conda config --add channels defaults && \. conda config --add channels bioconda && \. conda config --add channels conda-forge. conda create -y -n dv-env deepvariant. conda activate dv-env. ```. It completed without any error messages. I see:. ```. (dv-env) [pichuan@pichuan-centos7 ~]$ ls /home/pichuan/miniconda/envs/dv-env/share/. bash-completion deepvariant-1.5.0-0 doc et examples google-cloud-sdk-359.0.0-0 icu info keyutils licenses locale man tabset terminfo zsh. (dv-env) [pichuan@pichuan-centos7 ~]$ ls /home/pichuan/miniconda/envs/dv-env/share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0. call_variants_keras.zip freeze_graph.zip make_examples.zip multisample_make_examples.zip runtime_by_region_vis.zip vcf_stats_report.zip. call_variants.zip licenses.zip model_eval.zip postprocess_variants.zip settings.sh. deeptrio make_examples_somatic.zip model_train.zip run-prereq.sh show_examples.zip. ```. As mentioned in my previous investigation, I don't actually know how to use Conda though. But it seems like at least with the setting above, it can install. If you can try that and see if it works for you. And do what you'd usually do to run and see if it works, that will be great! Please let me know. By the way, . I noticed you were using `bioconda/label/cf201901::deepvariant`. I actually don't know what that does. (Can you explain where that came from and why you needed that?). But to understand be behavior, I also tried something like:. `conda create -y -n dv-env bioconda/label/cf201901::deepvariant`. If I did that, then it still seems to be work, but it somehow had an older version `deepvariant-0.7.2-1` in /home/pichuan/miniconda/envs/dv-env/share/, which I don't understand why. I'd recommend that you run using my first command, so you can have a more recent version to try out!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/806
https://github.com/google/deepvariant/issues/806:2771,deployability,version,version,2771,"val ""$(${HOME}/miniconda/bin/conda shell.bash hook)"". ```. To repeat what I did in, I tried: https://github.com/google/deepvariant/issues/736#issuecomment-1829204521. ```bash. conda config --add channels defaults && \. conda config --add channels bioconda && \. conda config --add channels conda-forge. conda create -y -n dv-env deepvariant. conda activate dv-env. ```. It completed without any error messages. I see:. ```. (dv-env) [pichuan@pichuan-centos7 ~]$ ls /home/pichuan/miniconda/envs/dv-env/share/. bash-completion deepvariant-1.5.0-0 doc et examples google-cloud-sdk-359.0.0-0 icu info keyutils licenses locale man tabset terminfo zsh. (dv-env) [pichuan@pichuan-centos7 ~]$ ls /home/pichuan/miniconda/envs/dv-env/share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0. call_variants_keras.zip freeze_graph.zip make_examples.zip multisample_make_examples.zip runtime_by_region_vis.zip vcf_stats_report.zip. call_variants.zip licenses.zip model_eval.zip postprocess_variants.zip settings.sh. deeptrio make_examples_somatic.zip model_train.zip run-prereq.sh show_examples.zip. ```. As mentioned in my previous investigation, I don't actually know how to use Conda though. But it seems like at least with the setting above, it can install. If you can try that and see if it works for you. And do what you'd usually do to run and see if it works, that will be great! Please let me know. By the way, . I noticed you were using `bioconda/label/cf201901::deepvariant`. I actually don't know what that does. (Can you explain where that came from and why you needed that?). But to understand be behavior, I also tried something like:. `conda create -y -n dv-env bioconda/label/cf201901::deepvariant`. If I did that, then it still seems to be work, but it somehow had an older version `deepvariant-0.7.2-1` in /home/pichuan/miniconda/envs/dv-env/share/, which I don't understand why. I'd recommend that you run using my first command, so you can have a more recent version to try out!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/806
https://github.com/google/deepvariant/issues/806:2959,deployability,version,version,2959,"val ""$(${HOME}/miniconda/bin/conda shell.bash hook)"". ```. To repeat what I did in, I tried: https://github.com/google/deepvariant/issues/736#issuecomment-1829204521. ```bash. conda config --add channels defaults && \. conda config --add channels bioconda && \. conda config --add channels conda-forge. conda create -y -n dv-env deepvariant. conda activate dv-env. ```. It completed without any error messages. I see:. ```. (dv-env) [pichuan@pichuan-centos7 ~]$ ls /home/pichuan/miniconda/envs/dv-env/share/. bash-completion deepvariant-1.5.0-0 doc et examples google-cloud-sdk-359.0.0-0 icu info keyutils licenses locale man tabset terminfo zsh. (dv-env) [pichuan@pichuan-centos7 ~]$ ls /home/pichuan/miniconda/envs/dv-env/share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0. call_variants_keras.zip freeze_graph.zip make_examples.zip multisample_make_examples.zip runtime_by_region_vis.zip vcf_stats_report.zip. call_variants.zip licenses.zip model_eval.zip postprocess_variants.zip settings.sh. deeptrio make_examples_somatic.zip model_train.zip run-prereq.sh show_examples.zip. ```. As mentioned in my previous investigation, I don't actually know how to use Conda though. But it seems like at least with the setting above, it can install. If you can try that and see if it works for you. And do what you'd usually do to run and see if it works, that will be great! Please let me know. By the way, . I noticed you were using `bioconda/label/cf201901::deepvariant`. I actually don't know what that does. (Can you explain where that came from and why you needed that?). But to understand be behavior, I also tried something like:. `conda create -y -n dv-env bioconda/label/cf201901::deepvariant`. If I did that, then it still seems to be work, but it somehow had an older version `deepvariant-0.7.2-1` in /home/pichuan/miniconda/envs/dv-env/share/, which I don't understand why. I'd recommend that you run using my first command, so you can have a more recent version to try out!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/806
https://github.com/google/deepvariant/issues/806:425,energy efficiency,cloud,cloud-platform,425,"Thank you @Zer0day-0 . I went back and looked at my notes on the last time I tried to install with Conda: https://github.com/google/deepvariant/issues/736#issuecomment-1829204521. I noticed that you're using CentOS. I think I might be testing with CentOS at the time. So let me try that. # Get a CentOS machine to test. I used:. ```bash. gcloud compute instances create ""${USER}-centos7"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""centos-7"" \. --image-project ""centos-cloud"" \. --machine-type ""e2-standard-16"" \. --zone ""us-west1-b"". gcloud compute ssh pichuan-centos7 --zone ""us-west1-b"". ```. Version:. ```. $ uname -a. Linux pichuan-centos7 3.10.0-1160.114.2.el7.x86_64 #1 SMP Wed Mar 20 15:54:52 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux. ```. Install conda:. ```bash. curl https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh > Miniconda3-latest-Linux-x86_64.sh. bash Miniconda3-latest-Linux-x86_64.sh -b -u -p $HOME/miniconda. eval ""$(${HOME}/miniconda/bin/conda shell.bash hook)"". ```. To repeat what I did in, I tried: https://github.com/google/deepvariant/issues/736#issuecomment-1829204521. ```bash. conda config --add channels defaults && \. conda config --add channels bioconda && \. conda config --add channels conda-forge. conda create -y -n dv-env deepvariant. conda activate dv-env. ```. It completed without any error messages. I see:. ```. (dv-env) [pichuan@pichuan-centos7 ~]$ ls /home/pichuan/miniconda/envs/dv-env/share/. bash-completion deepvariant-1.5.0-0 doc et examples google-cloud-sdk-359.0.0-0 icu info keyutils licenses locale man tabset terminfo zsh. (dv-env) [pichuan@pichuan-centos7 ~]$ ls /home/pichuan/miniconda/envs/dv-env/share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0. call_variants_keras.zip freeze_graph.zip make_examples.zip multisample_make_examples.zip runtime_by_region_vis.zip vcf_stats_report.zip. call_variants.zip licenses.zip model_eval.zip postprocess_variants.zip settings.sh. deept",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/806
https://github.com/google/deepvariant/issues/806:497,energy efficiency,cloud,cloud,497,"Thank you @Zer0day-0 . I went back and looked at my notes on the last time I tried to install with Conda: https://github.com/google/deepvariant/issues/736#issuecomment-1829204521. I noticed that you're using CentOS. I think I might be testing with CentOS at the time. So let me try that. # Get a CentOS machine to test. I used:. ```bash. gcloud compute instances create ""${USER}-centos7"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""centos-7"" \. --image-project ""centos-cloud"" \. --machine-type ""e2-standard-16"" \. --zone ""us-west1-b"". gcloud compute ssh pichuan-centos7 --zone ""us-west1-b"". ```. Version:. ```. $ uname -a. Linux pichuan-centos7 3.10.0-1160.114.2.el7.x86_64 #1 SMP Wed Mar 20 15:54:52 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux. ```. Install conda:. ```bash. curl https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh > Miniconda3-latest-Linux-x86_64.sh. bash Miniconda3-latest-Linux-x86_64.sh -b -u -p $HOME/miniconda. eval ""$(${HOME}/miniconda/bin/conda shell.bash hook)"". ```. To repeat what I did in, I tried: https://github.com/google/deepvariant/issues/736#issuecomment-1829204521. ```bash. conda config --add channels defaults && \. conda config --add channels bioconda && \. conda config --add channels conda-forge. conda create -y -n dv-env deepvariant. conda activate dv-env. ```. It completed without any error messages. I see:. ```. (dv-env) [pichuan@pichuan-centos7 ~]$ ls /home/pichuan/miniconda/envs/dv-env/share/. bash-completion deepvariant-1.5.0-0 doc et examples google-cloud-sdk-359.0.0-0 icu info keyutils licenses locale man tabset terminfo zsh. (dv-env) [pichuan@pichuan-centos7 ~]$ ls /home/pichuan/miniconda/envs/dv-env/share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0. call_variants_keras.zip freeze_graph.zip make_examples.zip multisample_make_examples.zip runtime_by_region_vis.zip vcf_stats_report.zip. call_variants.zip licenses.zip model_eval.zip postprocess_variants.zip settings.sh. deept",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/806
https://github.com/google/deepvariant/issues/806:1546,energy efficiency,cloud,cloud-sdk-,1546,"s-west1-b"". gcloud compute ssh pichuan-centos7 --zone ""us-west1-b"". ```. Version:. ```. $ uname -a. Linux pichuan-centos7 3.10.0-1160.114.2.el7.x86_64 #1 SMP Wed Mar 20 15:54:52 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux. ```. Install conda:. ```bash. curl https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh > Miniconda3-latest-Linux-x86_64.sh. bash Miniconda3-latest-Linux-x86_64.sh -b -u -p $HOME/miniconda. eval ""$(${HOME}/miniconda/bin/conda shell.bash hook)"". ```. To repeat what I did in, I tried: https://github.com/google/deepvariant/issues/736#issuecomment-1829204521. ```bash. conda config --add channels defaults && \. conda config --add channels bioconda && \. conda config --add channels conda-forge. conda create -y -n dv-env deepvariant. conda activate dv-env. ```. It completed without any error messages. I see:. ```. (dv-env) [pichuan@pichuan-centos7 ~]$ ls /home/pichuan/miniconda/envs/dv-env/share/. bash-completion deepvariant-1.5.0-0 doc et examples google-cloud-sdk-359.0.0-0 icu info keyutils licenses locale man tabset terminfo zsh. (dv-env) [pichuan@pichuan-centos7 ~]$ ls /home/pichuan/miniconda/envs/dv-env/share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0. call_variants_keras.zip freeze_graph.zip make_examples.zip multisample_make_examples.zip runtime_by_region_vis.zip vcf_stats_report.zip. call_variants.zip licenses.zip model_eval.zip postprocess_variants.zip settings.sh. deeptrio make_examples_somatic.zip model_train.zip run-prereq.sh show_examples.zip. ```. As mentioned in my previous investigation, I don't actually know how to use Conda though. But it seems like at least with the setting above, it can install. If you can try that and see if it works for you. And do what you'd usually do to run and see if it works, that will be great! Please let me know. By the way, . I noticed you were using `bioconda/label/cf201901::deepvariant`. I actually don't know what that does. (Can you explain where that came from and why y",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/806
https://github.com/google/deepvariant/issues/806:624,integrability,Version,Version,624,"Thank you @Zer0day-0 . I went back and looked at my notes on the last time I tried to install with Conda: https://github.com/google/deepvariant/issues/736#issuecomment-1829204521. I noticed that you're using CentOS. I think I might be testing with CentOS at the time. So let me try that. # Get a CentOS machine to test. I used:. ```bash. gcloud compute instances create ""${USER}-centos7"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""centos-7"" \. --image-project ""centos-cloud"" \. --machine-type ""e2-standard-16"" \. --zone ""us-west1-b"". gcloud compute ssh pichuan-centos7 --zone ""us-west1-b"". ```. Version:. ```. $ uname -a. Linux pichuan-centos7 3.10.0-1160.114.2.el7.x86_64 #1 SMP Wed Mar 20 15:54:52 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux. ```. Install conda:. ```bash. curl https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh > Miniconda3-latest-Linux-x86_64.sh. bash Miniconda3-latest-Linux-x86_64.sh -b -u -p $HOME/miniconda. eval ""$(${HOME}/miniconda/bin/conda shell.bash hook)"". ```. To repeat what I did in, I tried: https://github.com/google/deepvariant/issues/736#issuecomment-1829204521. ```bash. conda config --add channels defaults && \. conda config --add channels bioconda && \. conda config --add channels conda-forge. conda create -y -n dv-env deepvariant. conda activate dv-env. ```. It completed without any error messages. I see:. ```. (dv-env) [pichuan@pichuan-centos7 ~]$ ls /home/pichuan/miniconda/envs/dv-env/share/. bash-completion deepvariant-1.5.0-0 doc et examples google-cloud-sdk-359.0.0-0 icu info keyutils licenses locale man tabset terminfo zsh. (dv-env) [pichuan@pichuan-centos7 ~]$ ls /home/pichuan/miniconda/envs/dv-env/share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0. call_variants_keras.zip freeze_graph.zip make_examples.zip multisample_make_examples.zip runtime_by_region_vis.zip vcf_stats_report.zip. call_variants.zip licenses.zip model_eval.zip postprocess_variants.zip settings.sh. deept",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/806
https://github.com/google/deepvariant/issues/806:1379,integrability,messag,messages,1379,"os7"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""centos-7"" \. --image-project ""centos-cloud"" \. --machine-type ""e2-standard-16"" \. --zone ""us-west1-b"". gcloud compute ssh pichuan-centos7 --zone ""us-west1-b"". ```. Version:. ```. $ uname -a. Linux pichuan-centos7 3.10.0-1160.114.2.el7.x86_64 #1 SMP Wed Mar 20 15:54:52 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux. ```. Install conda:. ```bash. curl https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh > Miniconda3-latest-Linux-x86_64.sh. bash Miniconda3-latest-Linux-x86_64.sh -b -u -p $HOME/miniconda. eval ""$(${HOME}/miniconda/bin/conda shell.bash hook)"". ```. To repeat what I did in, I tried: https://github.com/google/deepvariant/issues/736#issuecomment-1829204521. ```bash. conda config --add channels defaults && \. conda config --add channels bioconda && \. conda config --add channels conda-forge. conda create -y -n dv-env deepvariant. conda activate dv-env. ```. It completed without any error messages. I see:. ```. (dv-env) [pichuan@pichuan-centos7 ~]$ ls /home/pichuan/miniconda/envs/dv-env/share/. bash-completion deepvariant-1.5.0-0 doc et examples google-cloud-sdk-359.0.0-0 icu info keyutils licenses locale man tabset terminfo zsh. (dv-env) [pichuan@pichuan-centos7 ~]$ ls /home/pichuan/miniconda/envs/dv-env/share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0. call_variants_keras.zip freeze_graph.zip make_examples.zip multisample_make_examples.zip runtime_by_region_vis.zip vcf_stats_report.zip. call_variants.zip licenses.zip model_eval.zip postprocess_variants.zip settings.sh. deeptrio make_examples_somatic.zip model_train.zip run-prereq.sh show_examples.zip. ```. As mentioned in my previous investigation, I don't actually know how to use Conda though. But it seems like at least with the setting above, it can install. If you can try that and see if it works for you. And do what you'd usually do to run and see if it works, that will be great! Please let me kn",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/806
https://github.com/google/deepvariant/issues/806:2771,integrability,version,version,2771,"val ""$(${HOME}/miniconda/bin/conda shell.bash hook)"". ```. To repeat what I did in, I tried: https://github.com/google/deepvariant/issues/736#issuecomment-1829204521. ```bash. conda config --add channels defaults && \. conda config --add channels bioconda && \. conda config --add channels conda-forge. conda create -y -n dv-env deepvariant. conda activate dv-env. ```. It completed without any error messages. I see:. ```. (dv-env) [pichuan@pichuan-centos7 ~]$ ls /home/pichuan/miniconda/envs/dv-env/share/. bash-completion deepvariant-1.5.0-0 doc et examples google-cloud-sdk-359.0.0-0 icu info keyutils licenses locale man tabset terminfo zsh. (dv-env) [pichuan@pichuan-centos7 ~]$ ls /home/pichuan/miniconda/envs/dv-env/share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0. call_variants_keras.zip freeze_graph.zip make_examples.zip multisample_make_examples.zip runtime_by_region_vis.zip vcf_stats_report.zip. call_variants.zip licenses.zip model_eval.zip postprocess_variants.zip settings.sh. deeptrio make_examples_somatic.zip model_train.zip run-prereq.sh show_examples.zip. ```. As mentioned in my previous investigation, I don't actually know how to use Conda though. But it seems like at least with the setting above, it can install. If you can try that and see if it works for you. And do what you'd usually do to run and see if it works, that will be great! Please let me know. By the way, . I noticed you were using `bioconda/label/cf201901::deepvariant`. I actually don't know what that does. (Can you explain where that came from and why you needed that?). But to understand be behavior, I also tried something like:. `conda create -y -n dv-env bioconda/label/cf201901::deepvariant`. If I did that, then it still seems to be work, but it somehow had an older version `deepvariant-0.7.2-1` in /home/pichuan/miniconda/envs/dv-env/share/, which I don't understand why. I'd recommend that you run using my first command, so you can have a more recent version to try out!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/806
https://github.com/google/deepvariant/issues/806:2959,integrability,version,version,2959,"val ""$(${HOME}/miniconda/bin/conda shell.bash hook)"". ```. To repeat what I did in, I tried: https://github.com/google/deepvariant/issues/736#issuecomment-1829204521. ```bash. conda config --add channels defaults && \. conda config --add channels bioconda && \. conda config --add channels conda-forge. conda create -y -n dv-env deepvariant. conda activate dv-env. ```. It completed without any error messages. I see:. ```. (dv-env) [pichuan@pichuan-centos7 ~]$ ls /home/pichuan/miniconda/envs/dv-env/share/. bash-completion deepvariant-1.5.0-0 doc et examples google-cloud-sdk-359.0.0-0 icu info keyutils licenses locale man tabset terminfo zsh. (dv-env) [pichuan@pichuan-centos7 ~]$ ls /home/pichuan/miniconda/envs/dv-env/share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0. call_variants_keras.zip freeze_graph.zip make_examples.zip multisample_make_examples.zip runtime_by_region_vis.zip vcf_stats_report.zip. call_variants.zip licenses.zip model_eval.zip postprocess_variants.zip settings.sh. deeptrio make_examples_somatic.zip model_train.zip run-prereq.sh show_examples.zip. ```. As mentioned in my previous investigation, I don't actually know how to use Conda though. But it seems like at least with the setting above, it can install. If you can try that and see if it works for you. And do what you'd usually do to run and see if it works, that will be great! Please let me know. By the way, . I noticed you were using `bioconda/label/cf201901::deepvariant`. I actually don't know what that does. (Can you explain where that came from and why you needed that?). But to understand be behavior, I also tried something like:. `conda create -y -n dv-env bioconda/label/cf201901::deepvariant`. If I did that, then it still seems to be work, but it somehow had an older version `deepvariant-0.7.2-1` in /home/pichuan/miniconda/envs/dv-env/share/, which I don't understand why. I'd recommend that you run using my first command, so you can have a more recent version to try out!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/806
https://github.com/google/deepvariant/issues/806:431,interoperability,platform,platform,431,"Thank you @Zer0day-0 . I went back and looked at my notes on the last time I tried to install with Conda: https://github.com/google/deepvariant/issues/736#issuecomment-1829204521. I noticed that you're using CentOS. I think I might be testing with CentOS at the time. So let me try that. # Get a CentOS machine to test. I used:. ```bash. gcloud compute instances create ""${USER}-centos7"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""centos-7"" \. --image-project ""centos-cloud"" \. --machine-type ""e2-standard-16"" \. --zone ""us-west1-b"". gcloud compute ssh pichuan-centos7 --zone ""us-west1-b"". ```. Version:. ```. $ uname -a. Linux pichuan-centos7 3.10.0-1160.114.2.el7.x86_64 #1 SMP Wed Mar 20 15:54:52 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux. ```. Install conda:. ```bash. curl https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh > Miniconda3-latest-Linux-x86_64.sh. bash Miniconda3-latest-Linux-x86_64.sh -b -u -p $HOME/miniconda. eval ""$(${HOME}/miniconda/bin/conda shell.bash hook)"". ```. To repeat what I did in, I tried: https://github.com/google/deepvariant/issues/736#issuecomment-1829204521. ```bash. conda config --add channels defaults && \. conda config --add channels bioconda && \. conda config --add channels conda-forge. conda create -y -n dv-env deepvariant. conda activate dv-env. ```. It completed without any error messages. I see:. ```. (dv-env) [pichuan@pichuan-centos7 ~]$ ls /home/pichuan/miniconda/envs/dv-env/share/. bash-completion deepvariant-1.5.0-0 doc et examples google-cloud-sdk-359.0.0-0 icu info keyutils licenses locale man tabset terminfo zsh. (dv-env) [pichuan@pichuan-centos7 ~]$ ls /home/pichuan/miniconda/envs/dv-env/share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0. call_variants_keras.zip freeze_graph.zip make_examples.zip multisample_make_examples.zip runtime_by_region_vis.zip vcf_stats_report.zip. call_variants.zip licenses.zip model_eval.zip postprocess_variants.zip settings.sh. deept",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/806
https://github.com/google/deepvariant/issues/806:526,interoperability,standard,standard-,526,"Thank you @Zer0day-0 . I went back and looked at my notes on the last time I tried to install with Conda: https://github.com/google/deepvariant/issues/736#issuecomment-1829204521. I noticed that you're using CentOS. I think I might be testing with CentOS at the time. So let me try that. # Get a CentOS machine to test. I used:. ```bash. gcloud compute instances create ""${USER}-centos7"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""centos-7"" \. --image-project ""centos-cloud"" \. --machine-type ""e2-standard-16"" \. --zone ""us-west1-b"". gcloud compute ssh pichuan-centos7 --zone ""us-west1-b"". ```. Version:. ```. $ uname -a. Linux pichuan-centos7 3.10.0-1160.114.2.el7.x86_64 #1 SMP Wed Mar 20 15:54:52 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux. ```. Install conda:. ```bash. curl https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh > Miniconda3-latest-Linux-x86_64.sh. bash Miniconda3-latest-Linux-x86_64.sh -b -u -p $HOME/miniconda. eval ""$(${HOME}/miniconda/bin/conda shell.bash hook)"". ```. To repeat what I did in, I tried: https://github.com/google/deepvariant/issues/736#issuecomment-1829204521. ```bash. conda config --add channels defaults && \. conda config --add channels bioconda && \. conda config --add channels conda-forge. conda create -y -n dv-env deepvariant. conda activate dv-env. ```. It completed without any error messages. I see:. ```. (dv-env) [pichuan@pichuan-centos7 ~]$ ls /home/pichuan/miniconda/envs/dv-env/share/. bash-completion deepvariant-1.5.0-0 doc et examples google-cloud-sdk-359.0.0-0 icu info keyutils licenses locale man tabset terminfo zsh. (dv-env) [pichuan@pichuan-centos7 ~]$ ls /home/pichuan/miniconda/envs/dv-env/share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0. call_variants_keras.zip freeze_graph.zip make_examples.zip multisample_make_examples.zip runtime_by_region_vis.zip vcf_stats_report.zip. call_variants.zip licenses.zip model_eval.zip postprocess_variants.zip settings.sh. deept",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/806
https://github.com/google/deepvariant/issues/806:1379,interoperability,messag,messages,1379,"os7"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""centos-7"" \. --image-project ""centos-cloud"" \. --machine-type ""e2-standard-16"" \. --zone ""us-west1-b"". gcloud compute ssh pichuan-centos7 --zone ""us-west1-b"". ```. Version:. ```. $ uname -a. Linux pichuan-centos7 3.10.0-1160.114.2.el7.x86_64 #1 SMP Wed Mar 20 15:54:52 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux. ```. Install conda:. ```bash. curl https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh > Miniconda3-latest-Linux-x86_64.sh. bash Miniconda3-latest-Linux-x86_64.sh -b -u -p $HOME/miniconda. eval ""$(${HOME}/miniconda/bin/conda shell.bash hook)"". ```. To repeat what I did in, I tried: https://github.com/google/deepvariant/issues/736#issuecomment-1829204521. ```bash. conda config --add channels defaults && \. conda config --add channels bioconda && \. conda config --add channels conda-forge. conda create -y -n dv-env deepvariant. conda activate dv-env. ```. It completed without any error messages. I see:. ```. (dv-env) [pichuan@pichuan-centos7 ~]$ ls /home/pichuan/miniconda/envs/dv-env/share/. bash-completion deepvariant-1.5.0-0 doc et examples google-cloud-sdk-359.0.0-0 icu info keyutils licenses locale man tabset terminfo zsh. (dv-env) [pichuan@pichuan-centos7 ~]$ ls /home/pichuan/miniconda/envs/dv-env/share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0. call_variants_keras.zip freeze_graph.zip make_examples.zip multisample_make_examples.zip runtime_by_region_vis.zip vcf_stats_report.zip. call_variants.zip licenses.zip model_eval.zip postprocess_variants.zip settings.sh. deeptrio make_examples_somatic.zip model_train.zip run-prereq.sh show_examples.zip. ```. As mentioned in my previous investigation, I don't actually know how to use Conda though. But it seems like at least with the setting above, it can install. If you can try that and see if it works for you. And do what you'd usually do to run and see if it works, that will be great! Please let me kn",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/806
https://github.com/google/deepvariant/issues/806:1479,interoperability,share,share,1479,"roject ""centos-cloud"" \. --machine-type ""e2-standard-16"" \. --zone ""us-west1-b"". gcloud compute ssh pichuan-centos7 --zone ""us-west1-b"". ```. Version:. ```. $ uname -a. Linux pichuan-centos7 3.10.0-1160.114.2.el7.x86_64 #1 SMP Wed Mar 20 15:54:52 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux. ```. Install conda:. ```bash. curl https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh > Miniconda3-latest-Linux-x86_64.sh. bash Miniconda3-latest-Linux-x86_64.sh -b -u -p $HOME/miniconda. eval ""$(${HOME}/miniconda/bin/conda shell.bash hook)"". ```. To repeat what I did in, I tried: https://github.com/google/deepvariant/issues/736#issuecomment-1829204521. ```bash. conda config --add channels defaults && \. conda config --add channels bioconda && \. conda config --add channels conda-forge. conda create -y -n dv-env deepvariant. conda activate dv-env. ```. It completed without any error messages. I see:. ```. (dv-env) [pichuan@pichuan-centos7 ~]$ ls /home/pichuan/miniconda/envs/dv-env/share/. bash-completion deepvariant-1.5.0-0 doc et examples google-cloud-sdk-359.0.0-0 icu info keyutils licenses locale man tabset terminfo zsh. (dv-env) [pichuan@pichuan-centos7 ~]$ ls /home/pichuan/miniconda/envs/dv-env/share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0. call_variants_keras.zip freeze_graph.zip make_examples.zip multisample_make_examples.zip runtime_by_region_vis.zip vcf_stats_report.zip. call_variants.zip licenses.zip model_eval.zip postprocess_variants.zip settings.sh. deeptrio make_examples_somatic.zip model_train.zip run-prereq.sh show_examples.zip. ```. As mentioned in my previous investigation, I don't actually know how to use Conda though. But it seems like at least with the setting above, it can install. If you can try that and see if it works for you. And do what you'd usually do to run and see if it works, that will be great! Please let me know. By the way, . I noticed you were using `bioconda/label/cf201901::deepvariant`. I actually don't",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/806
https://github.com/google/deepvariant/issues/806:1702,interoperability,share,share,1702,"SMP Wed Mar 20 15:54:52 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux. ```. Install conda:. ```bash. curl https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh > Miniconda3-latest-Linux-x86_64.sh. bash Miniconda3-latest-Linux-x86_64.sh -b -u -p $HOME/miniconda. eval ""$(${HOME}/miniconda/bin/conda shell.bash hook)"". ```. To repeat what I did in, I tried: https://github.com/google/deepvariant/issues/736#issuecomment-1829204521. ```bash. conda config --add channels defaults && \. conda config --add channels bioconda && \. conda config --add channels conda-forge. conda create -y -n dv-env deepvariant. conda activate dv-env. ```. It completed without any error messages. I see:. ```. (dv-env) [pichuan@pichuan-centos7 ~]$ ls /home/pichuan/miniconda/envs/dv-env/share/. bash-completion deepvariant-1.5.0-0 doc et examples google-cloud-sdk-359.0.0-0 icu info keyutils licenses locale man tabset terminfo zsh. (dv-env) [pichuan@pichuan-centos7 ~]$ ls /home/pichuan/miniconda/envs/dv-env/share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0. call_variants_keras.zip freeze_graph.zip make_examples.zip multisample_make_examples.zip runtime_by_region_vis.zip vcf_stats_report.zip. call_variants.zip licenses.zip model_eval.zip postprocess_variants.zip settings.sh. deeptrio make_examples_somatic.zip model_train.zip run-prereq.sh show_examples.zip. ```. As mentioned in my previous investigation, I don't actually know how to use Conda though. But it seems like at least with the setting above, it can install. If you can try that and see if it works for you. And do what you'd usually do to run and see if it works, that will be great! Please let me know. By the way, . I noticed you were using `bioconda/label/cf201901::deepvariant`. I actually don't know what that does. (Can you explain where that came from and why you needed that?). But to understand be behavior, I also tried something like:. `conda create -y -n dv-env bioconda/label/cf201901::deepvariant`. If I did ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/806
https://github.com/google/deepvariant/issues/806:2840,interoperability,share,share,2840,"val ""$(${HOME}/miniconda/bin/conda shell.bash hook)"". ```. To repeat what I did in, I tried: https://github.com/google/deepvariant/issues/736#issuecomment-1829204521. ```bash. conda config --add channels defaults && \. conda config --add channels bioconda && \. conda config --add channels conda-forge. conda create -y -n dv-env deepvariant. conda activate dv-env. ```. It completed without any error messages. I see:. ```. (dv-env) [pichuan@pichuan-centos7 ~]$ ls /home/pichuan/miniconda/envs/dv-env/share/. bash-completion deepvariant-1.5.0-0 doc et examples google-cloud-sdk-359.0.0-0 icu info keyutils licenses locale man tabset terminfo zsh. (dv-env) [pichuan@pichuan-centos7 ~]$ ls /home/pichuan/miniconda/envs/dv-env/share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0. call_variants_keras.zip freeze_graph.zip make_examples.zip multisample_make_examples.zip runtime_by_region_vis.zip vcf_stats_report.zip. call_variants.zip licenses.zip model_eval.zip postprocess_variants.zip settings.sh. deeptrio make_examples_somatic.zip model_train.zip run-prereq.sh show_examples.zip. ```. As mentioned in my previous investigation, I don't actually know how to use Conda though. But it seems like at least with the setting above, it can install. If you can try that and see if it works for you. And do what you'd usually do to run and see if it works, that will be great! Please let me know. By the way, . I noticed you were using `bioconda/label/cf201901::deepvariant`. I actually don't know what that does. (Can you explain where that came from and why you needed that?). But to understand be behavior, I also tried something like:. `conda create -y -n dv-env bioconda/label/cf201901::deepvariant`. If I did that, then it still seems to be work, but it somehow had an older version `deepvariant-0.7.2-1` in /home/pichuan/miniconda/envs/dv-env/share/, which I don't understand why. I'd recommend that you run using my first command, so you can have a more recent version to try out!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/806
https://github.com/google/deepvariant/issues/806:624,modifiability,Version,Version,624,"Thank you @Zer0day-0 . I went back and looked at my notes on the last time I tried to install with Conda: https://github.com/google/deepvariant/issues/736#issuecomment-1829204521. I noticed that you're using CentOS. I think I might be testing with CentOS at the time. So let me try that. # Get a CentOS machine to test. I used:. ```bash. gcloud compute instances create ""${USER}-centos7"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""centos-7"" \. --image-project ""centos-cloud"" \. --machine-type ""e2-standard-16"" \. --zone ""us-west1-b"". gcloud compute ssh pichuan-centos7 --zone ""us-west1-b"". ```. Version:. ```. $ uname -a. Linux pichuan-centos7 3.10.0-1160.114.2.el7.x86_64 #1 SMP Wed Mar 20 15:54:52 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux. ```. Install conda:. ```bash. curl https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh > Miniconda3-latest-Linux-x86_64.sh. bash Miniconda3-latest-Linux-x86_64.sh -b -u -p $HOME/miniconda. eval ""$(${HOME}/miniconda/bin/conda shell.bash hook)"". ```. To repeat what I did in, I tried: https://github.com/google/deepvariant/issues/736#issuecomment-1829204521. ```bash. conda config --add channels defaults && \. conda config --add channels bioconda && \. conda config --add channels conda-forge. conda create -y -n dv-env deepvariant. conda activate dv-env. ```. It completed without any error messages. I see:. ```. (dv-env) [pichuan@pichuan-centos7 ~]$ ls /home/pichuan/miniconda/envs/dv-env/share/. bash-completion deepvariant-1.5.0-0 doc et examples google-cloud-sdk-359.0.0-0 icu info keyutils licenses locale man tabset terminfo zsh. (dv-env) [pichuan@pichuan-centos7 ~]$ ls /home/pichuan/miniconda/envs/dv-env/share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0. call_variants_keras.zip freeze_graph.zip make_examples.zip multisample_make_examples.zip runtime_by_region_vis.zip vcf_stats_report.zip. call_variants.zip licenses.zip model_eval.zip postprocess_variants.zip settings.sh. deept",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/806
https://github.com/google/deepvariant/issues/806:2771,modifiability,version,version,2771,"val ""$(${HOME}/miniconda/bin/conda shell.bash hook)"". ```. To repeat what I did in, I tried: https://github.com/google/deepvariant/issues/736#issuecomment-1829204521. ```bash. conda config --add channels defaults && \. conda config --add channels bioconda && \. conda config --add channels conda-forge. conda create -y -n dv-env deepvariant. conda activate dv-env. ```. It completed without any error messages. I see:. ```. (dv-env) [pichuan@pichuan-centos7 ~]$ ls /home/pichuan/miniconda/envs/dv-env/share/. bash-completion deepvariant-1.5.0-0 doc et examples google-cloud-sdk-359.0.0-0 icu info keyutils licenses locale man tabset terminfo zsh. (dv-env) [pichuan@pichuan-centos7 ~]$ ls /home/pichuan/miniconda/envs/dv-env/share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0. call_variants_keras.zip freeze_graph.zip make_examples.zip multisample_make_examples.zip runtime_by_region_vis.zip vcf_stats_report.zip. call_variants.zip licenses.zip model_eval.zip postprocess_variants.zip settings.sh. deeptrio make_examples_somatic.zip model_train.zip run-prereq.sh show_examples.zip. ```. As mentioned in my previous investigation, I don't actually know how to use Conda though. But it seems like at least with the setting above, it can install. If you can try that and see if it works for you. And do what you'd usually do to run and see if it works, that will be great! Please let me know. By the way, . I noticed you were using `bioconda/label/cf201901::deepvariant`. I actually don't know what that does. (Can you explain where that came from and why you needed that?). But to understand be behavior, I also tried something like:. `conda create -y -n dv-env bioconda/label/cf201901::deepvariant`. If I did that, then it still seems to be work, but it somehow had an older version `deepvariant-0.7.2-1` in /home/pichuan/miniconda/envs/dv-env/share/, which I don't understand why. I'd recommend that you run using my first command, so you can have a more recent version to try out!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/806
https://github.com/google/deepvariant/issues/806:2959,modifiability,version,version,2959,"val ""$(${HOME}/miniconda/bin/conda shell.bash hook)"". ```. To repeat what I did in, I tried: https://github.com/google/deepvariant/issues/736#issuecomment-1829204521. ```bash. conda config --add channels defaults && \. conda config --add channels bioconda && \. conda config --add channels conda-forge. conda create -y -n dv-env deepvariant. conda activate dv-env. ```. It completed without any error messages. I see:. ```. (dv-env) [pichuan@pichuan-centos7 ~]$ ls /home/pichuan/miniconda/envs/dv-env/share/. bash-completion deepvariant-1.5.0-0 doc et examples google-cloud-sdk-359.0.0-0 icu info keyutils licenses locale man tabset terminfo zsh. (dv-env) [pichuan@pichuan-centos7 ~]$ ls /home/pichuan/miniconda/envs/dv-env/share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0. call_variants_keras.zip freeze_graph.zip make_examples.zip multisample_make_examples.zip runtime_by_region_vis.zip vcf_stats_report.zip. call_variants.zip licenses.zip model_eval.zip postprocess_variants.zip settings.sh. deeptrio make_examples_somatic.zip model_train.zip run-prereq.sh show_examples.zip. ```. As mentioned in my previous investigation, I don't actually know how to use Conda though. But it seems like at least with the setting above, it can install. If you can try that and see if it works for you. And do what you'd usually do to run and see if it works, that will be great! Please let me know. By the way, . I noticed you were using `bioconda/label/cf201901::deepvariant`. I actually don't know what that does. (Can you explain where that came from and why you needed that?). But to understand be behavior, I also tried something like:. `conda create -y -n dv-env bioconda/label/cf201901::deepvariant`. If I did that, then it still seems to be work, but it somehow had an older version `deepvariant-0.7.2-1` in /home/pichuan/miniconda/envs/dv-env/share/, which I don't understand why. I'd recommend that you run using my first command, so you can have a more recent version to try out!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/806
https://github.com/google/deepvariant/issues/806:70,performance,time,time,70,"Thank you @Zer0day-0 . I went back and looked at my notes on the last time I tried to install with Conda: https://github.com/google/deepvariant/issues/736#issuecomment-1829204521. I noticed that you're using CentOS. I think I might be testing with CentOS at the time. So let me try that. # Get a CentOS machine to test. I used:. ```bash. gcloud compute instances create ""${USER}-centos7"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""centos-7"" \. --image-project ""centos-cloud"" \. --machine-type ""e2-standard-16"" \. --zone ""us-west1-b"". gcloud compute ssh pichuan-centos7 --zone ""us-west1-b"". ```. Version:. ```. $ uname -a. Linux pichuan-centos7 3.10.0-1160.114.2.el7.x86_64 #1 SMP Wed Mar 20 15:54:52 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux. ```. Install conda:. ```bash. curl https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh > Miniconda3-latest-Linux-x86_64.sh. bash Miniconda3-latest-Linux-x86_64.sh -b -u -p $HOME/miniconda. eval ""$(${HOME}/miniconda/bin/conda shell.bash hook)"". ```. To repeat what I did in, I tried: https://github.com/google/deepvariant/issues/736#issuecomment-1829204521. ```bash. conda config --add channels defaults && \. conda config --add channels bioconda && \. conda config --add channels conda-forge. conda create -y -n dv-env deepvariant. conda activate dv-env. ```. It completed without any error messages. I see:. ```. (dv-env) [pichuan@pichuan-centos7 ~]$ ls /home/pichuan/miniconda/envs/dv-env/share/. bash-completion deepvariant-1.5.0-0 doc et examples google-cloud-sdk-359.0.0-0 icu info keyutils licenses locale man tabset terminfo zsh. (dv-env) [pichuan@pichuan-centos7 ~]$ ls /home/pichuan/miniconda/envs/dv-env/share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0. call_variants_keras.zip freeze_graph.zip make_examples.zip multisample_make_examples.zip runtime_by_region_vis.zip vcf_stats_report.zip. call_variants.zip licenses.zip model_eval.zip postprocess_variants.zip settings.sh. deept",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/806
https://github.com/google/deepvariant/issues/806:262,performance,time,time,262,"Thank you @Zer0day-0 . I went back and looked at my notes on the last time I tried to install with Conda: https://github.com/google/deepvariant/issues/736#issuecomment-1829204521. I noticed that you're using CentOS. I think I might be testing with CentOS at the time. So let me try that. # Get a CentOS machine to test. I used:. ```bash. gcloud compute instances create ""${USER}-centos7"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""centos-7"" \. --image-project ""centos-cloud"" \. --machine-type ""e2-standard-16"" \. --zone ""us-west1-b"". gcloud compute ssh pichuan-centos7 --zone ""us-west1-b"". ```. Version:. ```. $ uname -a. Linux pichuan-centos7 3.10.0-1160.114.2.el7.x86_64 #1 SMP Wed Mar 20 15:54:52 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux. ```. Install conda:. ```bash. curl https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh > Miniconda3-latest-Linux-x86_64.sh. bash Miniconda3-latest-Linux-x86_64.sh -b -u -p $HOME/miniconda. eval ""$(${HOME}/miniconda/bin/conda shell.bash hook)"". ```. To repeat what I did in, I tried: https://github.com/google/deepvariant/issues/736#issuecomment-1829204521. ```bash. conda config --add channels defaults && \. conda config --add channels bioconda && \. conda config --add channels conda-forge. conda create -y -n dv-env deepvariant. conda activate dv-env. ```. It completed without any error messages. I see:. ```. (dv-env) [pichuan@pichuan-centos7 ~]$ ls /home/pichuan/miniconda/envs/dv-env/share/. bash-completion deepvariant-1.5.0-0 doc et examples google-cloud-sdk-359.0.0-0 icu info keyutils licenses locale man tabset terminfo zsh. (dv-env) [pichuan@pichuan-centos7 ~]$ ls /home/pichuan/miniconda/envs/dv-env/share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0. call_variants_keras.zip freeze_graph.zip make_examples.zip multisample_make_examples.zip runtime_by_region_vis.zip vcf_stats_report.zip. call_variants.zip licenses.zip model_eval.zip postprocess_variants.zip settings.sh. deept",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/806
https://github.com/google/deepvariant/issues/806:1373,performance,error,error,1373,"R}-centos7"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""centos-7"" \. --image-project ""centos-cloud"" \. --machine-type ""e2-standard-16"" \. --zone ""us-west1-b"". gcloud compute ssh pichuan-centos7 --zone ""us-west1-b"". ```. Version:. ```. $ uname -a. Linux pichuan-centos7 3.10.0-1160.114.2.el7.x86_64 #1 SMP Wed Mar 20 15:54:52 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux. ```. Install conda:. ```bash. curl https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh > Miniconda3-latest-Linux-x86_64.sh. bash Miniconda3-latest-Linux-x86_64.sh -b -u -p $HOME/miniconda. eval ""$(${HOME}/miniconda/bin/conda shell.bash hook)"". ```. To repeat what I did in, I tried: https://github.com/google/deepvariant/issues/736#issuecomment-1829204521. ```bash. conda config --add channels defaults && \. conda config --add channels bioconda && \. conda config --add channels conda-forge. conda create -y -n dv-env deepvariant. conda activate dv-env. ```. It completed without any error messages. I see:. ```. (dv-env) [pichuan@pichuan-centos7 ~]$ ls /home/pichuan/miniconda/envs/dv-env/share/. bash-completion deepvariant-1.5.0-0 doc et examples google-cloud-sdk-359.0.0-0 icu info keyutils licenses locale man tabset terminfo zsh. (dv-env) [pichuan@pichuan-centos7 ~]$ ls /home/pichuan/miniconda/envs/dv-env/share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0. call_variants_keras.zip freeze_graph.zip make_examples.zip multisample_make_examples.zip runtime_by_region_vis.zip vcf_stats_report.zip. call_variants.zip licenses.zip model_eval.zip postprocess_variants.zip settings.sh. deeptrio make_examples_somatic.zip model_train.zip run-prereq.sh show_examples.zip. ```. As mentioned in my previous investigation, I don't actually know how to use Conda though. But it seems like at least with the setting above, it can install. If you can try that and see if it works for you. And do what you'd usually do to run and see if it works, that will be great! Please le",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/806
https://github.com/google/deepvariant/issues/806:2498,reliability,doe,does,2498,"val ""$(${HOME}/miniconda/bin/conda shell.bash hook)"". ```. To repeat what I did in, I tried: https://github.com/google/deepvariant/issues/736#issuecomment-1829204521. ```bash. conda config --add channels defaults && \. conda config --add channels bioconda && \. conda config --add channels conda-forge. conda create -y -n dv-env deepvariant. conda activate dv-env. ```. It completed without any error messages. I see:. ```. (dv-env) [pichuan@pichuan-centos7 ~]$ ls /home/pichuan/miniconda/envs/dv-env/share/. bash-completion deepvariant-1.5.0-0 doc et examples google-cloud-sdk-359.0.0-0 icu info keyutils licenses locale man tabset terminfo zsh. (dv-env) [pichuan@pichuan-centos7 ~]$ ls /home/pichuan/miniconda/envs/dv-env/share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0. call_variants_keras.zip freeze_graph.zip make_examples.zip multisample_make_examples.zip runtime_by_region_vis.zip vcf_stats_report.zip. call_variants.zip licenses.zip model_eval.zip postprocess_variants.zip settings.sh. deeptrio make_examples_somatic.zip model_train.zip run-prereq.sh show_examples.zip. ```. As mentioned in my previous investigation, I don't actually know how to use Conda though. But it seems like at least with the setting above, it can install. If you can try that and see if it works for you. And do what you'd usually do to run and see if it works, that will be great! Please let me know. By the way, . I noticed you were using `bioconda/label/cf201901::deepvariant`. I actually don't know what that does. (Can you explain where that came from and why you needed that?). But to understand be behavior, I also tried something like:. `conda create -y -n dv-env bioconda/label/cf201901::deepvariant`. If I did that, then it still seems to be work, but it somehow had an older version `deepvariant-0.7.2-1` in /home/pichuan/miniconda/envs/dv-env/share/, which I don't understand why. I'd recommend that you run using my first command, so you can have a more recent version to try out!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/806
https://github.com/google/deepvariant/issues/806:235,safety,test,testing,235,"Thank you @Zer0day-0 . I went back and looked at my notes on the last time I tried to install with Conda: https://github.com/google/deepvariant/issues/736#issuecomment-1829204521. I noticed that you're using CentOS. I think I might be testing with CentOS at the time. So let me try that. # Get a CentOS machine to test. I used:. ```bash. gcloud compute instances create ""${USER}-centos7"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""centos-7"" \. --image-project ""centos-cloud"" \. --machine-type ""e2-standard-16"" \. --zone ""us-west1-b"". gcloud compute ssh pichuan-centos7 --zone ""us-west1-b"". ```. Version:. ```. $ uname -a. Linux pichuan-centos7 3.10.0-1160.114.2.el7.x86_64 #1 SMP Wed Mar 20 15:54:52 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux. ```. Install conda:. ```bash. curl https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh > Miniconda3-latest-Linux-x86_64.sh. bash Miniconda3-latest-Linux-x86_64.sh -b -u -p $HOME/miniconda. eval ""$(${HOME}/miniconda/bin/conda shell.bash hook)"". ```. To repeat what I did in, I tried: https://github.com/google/deepvariant/issues/736#issuecomment-1829204521. ```bash. conda config --add channels defaults && \. conda config --add channels bioconda && \. conda config --add channels conda-forge. conda create -y -n dv-env deepvariant. conda activate dv-env. ```. It completed without any error messages. I see:. ```. (dv-env) [pichuan@pichuan-centos7 ~]$ ls /home/pichuan/miniconda/envs/dv-env/share/. bash-completion deepvariant-1.5.0-0 doc et examples google-cloud-sdk-359.0.0-0 icu info keyutils licenses locale man tabset terminfo zsh. (dv-env) [pichuan@pichuan-centos7 ~]$ ls /home/pichuan/miniconda/envs/dv-env/share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0. call_variants_keras.zip freeze_graph.zip make_examples.zip multisample_make_examples.zip runtime_by_region_vis.zip vcf_stats_report.zip. call_variants.zip licenses.zip model_eval.zip postprocess_variants.zip settings.sh. deept",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/806
https://github.com/google/deepvariant/issues/806:314,safety,test,test,314,"Thank you @Zer0day-0 . I went back and looked at my notes on the last time I tried to install with Conda: https://github.com/google/deepvariant/issues/736#issuecomment-1829204521. I noticed that you're using CentOS. I think I might be testing with CentOS at the time. So let me try that. # Get a CentOS machine to test. I used:. ```bash. gcloud compute instances create ""${USER}-centos7"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""centos-7"" \. --image-project ""centos-cloud"" \. --machine-type ""e2-standard-16"" \. --zone ""us-west1-b"". gcloud compute ssh pichuan-centos7 --zone ""us-west1-b"". ```. Version:. ```. $ uname -a. Linux pichuan-centos7 3.10.0-1160.114.2.el7.x86_64 #1 SMP Wed Mar 20 15:54:52 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux. ```. Install conda:. ```bash. curl https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh > Miniconda3-latest-Linux-x86_64.sh. bash Miniconda3-latest-Linux-x86_64.sh -b -u -p $HOME/miniconda. eval ""$(${HOME}/miniconda/bin/conda shell.bash hook)"". ```. To repeat what I did in, I tried: https://github.com/google/deepvariant/issues/736#issuecomment-1829204521. ```bash. conda config --add channels defaults && \. conda config --add channels bioconda && \. conda config --add channels conda-forge. conda create -y -n dv-env deepvariant. conda activate dv-env. ```. It completed without any error messages. I see:. ```. (dv-env) [pichuan@pichuan-centos7 ~]$ ls /home/pichuan/miniconda/envs/dv-env/share/. bash-completion deepvariant-1.5.0-0 doc et examples google-cloud-sdk-359.0.0-0 icu info keyutils licenses locale man tabset terminfo zsh. (dv-env) [pichuan@pichuan-centos7 ~]$ ls /home/pichuan/miniconda/envs/dv-env/share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0. call_variants_keras.zip freeze_graph.zip make_examples.zip multisample_make_examples.zip runtime_by_region_vis.zip vcf_stats_report.zip. call_variants.zip licenses.zip model_eval.zip postprocess_variants.zip settings.sh. deept",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/806
https://github.com/google/deepvariant/issues/806:1351,safety,compl,completed,1351,"tances create ""${USER}-centos7"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""centos-7"" \. --image-project ""centos-cloud"" \. --machine-type ""e2-standard-16"" \. --zone ""us-west1-b"". gcloud compute ssh pichuan-centos7 --zone ""us-west1-b"". ```. Version:. ```. $ uname -a. Linux pichuan-centos7 3.10.0-1160.114.2.el7.x86_64 #1 SMP Wed Mar 20 15:54:52 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux. ```. Install conda:. ```bash. curl https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh > Miniconda3-latest-Linux-x86_64.sh. bash Miniconda3-latest-Linux-x86_64.sh -b -u -p $HOME/miniconda. eval ""$(${HOME}/miniconda/bin/conda shell.bash hook)"". ```. To repeat what I did in, I tried: https://github.com/google/deepvariant/issues/736#issuecomment-1829204521. ```bash. conda config --add channels defaults && \. conda config --add channels bioconda && \. conda config --add channels conda-forge. conda create -y -n dv-env deepvariant. conda activate dv-env. ```. It completed without any error messages. I see:. ```. (dv-env) [pichuan@pichuan-centos7 ~]$ ls /home/pichuan/miniconda/envs/dv-env/share/. bash-completion deepvariant-1.5.0-0 doc et examples google-cloud-sdk-359.0.0-0 icu info keyutils licenses locale man tabset terminfo zsh. (dv-env) [pichuan@pichuan-centos7 ~]$ ls /home/pichuan/miniconda/envs/dv-env/share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0. call_variants_keras.zip freeze_graph.zip make_examples.zip multisample_make_examples.zip runtime_by_region_vis.zip vcf_stats_report.zip. call_variants.zip licenses.zip model_eval.zip postprocess_variants.zip settings.sh. deeptrio make_examples_somatic.zip model_train.zip run-prereq.sh show_examples.zip. ```. As mentioned in my previous investigation, I don't actually know how to use Conda though. But it seems like at least with the setting above, it can install. If you can try that and see if it works for you. And do what you'd usually do to run and see if it works, that will",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/806
https://github.com/google/deepvariant/issues/806:1373,safety,error,error,1373,"R}-centos7"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""centos-7"" \. --image-project ""centos-cloud"" \. --machine-type ""e2-standard-16"" \. --zone ""us-west1-b"". gcloud compute ssh pichuan-centos7 --zone ""us-west1-b"". ```. Version:. ```. $ uname -a. Linux pichuan-centos7 3.10.0-1160.114.2.el7.x86_64 #1 SMP Wed Mar 20 15:54:52 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux. ```. Install conda:. ```bash. curl https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh > Miniconda3-latest-Linux-x86_64.sh. bash Miniconda3-latest-Linux-x86_64.sh -b -u -p $HOME/miniconda. eval ""$(${HOME}/miniconda/bin/conda shell.bash hook)"". ```. To repeat what I did in, I tried: https://github.com/google/deepvariant/issues/736#issuecomment-1829204521. ```bash. conda config --add channels defaults && \. conda config --add channels bioconda && \. conda config --add channels conda-forge. conda create -y -n dv-env deepvariant. conda activate dv-env. ```. It completed without any error messages. I see:. ```. (dv-env) [pichuan@pichuan-centos7 ~]$ ls /home/pichuan/miniconda/envs/dv-env/share/. bash-completion deepvariant-1.5.0-0 doc et examples google-cloud-sdk-359.0.0-0 icu info keyutils licenses locale man tabset terminfo zsh. (dv-env) [pichuan@pichuan-centos7 ~]$ ls /home/pichuan/miniconda/envs/dv-env/share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0. call_variants_keras.zip freeze_graph.zip make_examples.zip multisample_make_examples.zip runtime_by_region_vis.zip vcf_stats_report.zip. call_variants.zip licenses.zip model_eval.zip postprocess_variants.zip settings.sh. deeptrio make_examples_somatic.zip model_train.zip run-prereq.sh show_examples.zip. ```. As mentioned in my previous investigation, I don't actually know how to use Conda though. But it seems like at least with the setting above, it can install. If you can try that and see if it works for you. And do what you'd usually do to run and see if it works, that will be great! Please le",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/806
https://github.com/google/deepvariant/issues/806:1492,safety,compl,completion,1492,"cloud"" \. --machine-type ""e2-standard-16"" \. --zone ""us-west1-b"". gcloud compute ssh pichuan-centos7 --zone ""us-west1-b"". ```. Version:. ```. $ uname -a. Linux pichuan-centos7 3.10.0-1160.114.2.el7.x86_64 #1 SMP Wed Mar 20 15:54:52 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux. ```. Install conda:. ```bash. curl https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh > Miniconda3-latest-Linux-x86_64.sh. bash Miniconda3-latest-Linux-x86_64.sh -b -u -p $HOME/miniconda. eval ""$(${HOME}/miniconda/bin/conda shell.bash hook)"". ```. To repeat what I did in, I tried: https://github.com/google/deepvariant/issues/736#issuecomment-1829204521. ```bash. conda config --add channels defaults && \. conda config --add channels bioconda && \. conda config --add channels conda-forge. conda create -y -n dv-env deepvariant. conda activate dv-env. ```. It completed without any error messages. I see:. ```. (dv-env) [pichuan@pichuan-centos7 ~]$ ls /home/pichuan/miniconda/envs/dv-env/share/. bash-completion deepvariant-1.5.0-0 doc et examples google-cloud-sdk-359.0.0-0 icu info keyutils licenses locale man tabset terminfo zsh. (dv-env) [pichuan@pichuan-centos7 ~]$ ls /home/pichuan/miniconda/envs/dv-env/share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0. call_variants_keras.zip freeze_graph.zip make_examples.zip multisample_make_examples.zip runtime_by_region_vis.zip vcf_stats_report.zip. call_variants.zip licenses.zip model_eval.zip postprocess_variants.zip settings.sh. deeptrio make_examples_somatic.zip model_train.zip run-prereq.sh show_examples.zip. ```. As mentioned in my previous investigation, I don't actually know how to use Conda though. But it seems like at least with the setting above, it can install. If you can try that and see if it works for you. And do what you'd usually do to run and see if it works, that will be great! Please let me know. By the way, . I noticed you were using `bioconda/label/cf201901::deepvariant`. I actually don't know what that",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/806
https://github.com/google/deepvariant/issues/806:578,security,ssh,ssh,578,"Thank you @Zer0day-0 . I went back and looked at my notes on the last time I tried to install with Conda: https://github.com/google/deepvariant/issues/736#issuecomment-1829204521. I noticed that you're using CentOS. I think I might be testing with CentOS at the time. So let me try that. # Get a CentOS machine to test. I used:. ```bash. gcloud compute instances create ""${USER}-centos7"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""centos-7"" \. --image-project ""centos-cloud"" \. --machine-type ""e2-standard-16"" \. --zone ""us-west1-b"". gcloud compute ssh pichuan-centos7 --zone ""us-west1-b"". ```. Version:. ```. $ uname -a. Linux pichuan-centos7 3.10.0-1160.114.2.el7.x86_64 #1 SMP Wed Mar 20 15:54:52 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux. ```. Install conda:. ```bash. curl https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh > Miniconda3-latest-Linux-x86_64.sh. bash Miniconda3-latest-Linux-x86_64.sh -b -u -p $HOME/miniconda. eval ""$(${HOME}/miniconda/bin/conda shell.bash hook)"". ```. To repeat what I did in, I tried: https://github.com/google/deepvariant/issues/736#issuecomment-1829204521. ```bash. conda config --add channels defaults && \. conda config --add channels bioconda && \. conda config --add channels conda-forge. conda create -y -n dv-env deepvariant. conda activate dv-env. ```. It completed without any error messages. I see:. ```. (dv-env) [pichuan@pichuan-centos7 ~]$ ls /home/pichuan/miniconda/envs/dv-env/share/. bash-completion deepvariant-1.5.0-0 doc et examples google-cloud-sdk-359.0.0-0 icu info keyutils licenses locale man tabset terminfo zsh. (dv-env) [pichuan@pichuan-centos7 ~]$ ls /home/pichuan/miniconda/envs/dv-env/share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0. call_variants_keras.zip freeze_graph.zip make_examples.zip multisample_make_examples.zip runtime_by_region_vis.zip vcf_stats_report.zip. call_variants.zip licenses.zip model_eval.zip postprocess_variants.zip settings.sh. deept",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/806
https://github.com/google/deepvariant/issues/806:1351,security,compl,completed,1351,"tances create ""${USER}-centos7"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""centos-7"" \. --image-project ""centos-cloud"" \. --machine-type ""e2-standard-16"" \. --zone ""us-west1-b"". gcloud compute ssh pichuan-centos7 --zone ""us-west1-b"". ```. Version:. ```. $ uname -a. Linux pichuan-centos7 3.10.0-1160.114.2.el7.x86_64 #1 SMP Wed Mar 20 15:54:52 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux. ```. Install conda:. ```bash. curl https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh > Miniconda3-latest-Linux-x86_64.sh. bash Miniconda3-latest-Linux-x86_64.sh -b -u -p $HOME/miniconda. eval ""$(${HOME}/miniconda/bin/conda shell.bash hook)"". ```. To repeat what I did in, I tried: https://github.com/google/deepvariant/issues/736#issuecomment-1829204521. ```bash. conda config --add channels defaults && \. conda config --add channels bioconda && \. conda config --add channels conda-forge. conda create -y -n dv-env deepvariant. conda activate dv-env. ```. It completed without any error messages. I see:. ```. (dv-env) [pichuan@pichuan-centos7 ~]$ ls /home/pichuan/miniconda/envs/dv-env/share/. bash-completion deepvariant-1.5.0-0 doc et examples google-cloud-sdk-359.0.0-0 icu info keyutils licenses locale man tabset terminfo zsh. (dv-env) [pichuan@pichuan-centos7 ~]$ ls /home/pichuan/miniconda/envs/dv-env/share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0. call_variants_keras.zip freeze_graph.zip make_examples.zip multisample_make_examples.zip runtime_by_region_vis.zip vcf_stats_report.zip. call_variants.zip licenses.zip model_eval.zip postprocess_variants.zip settings.sh. deeptrio make_examples_somatic.zip model_train.zip run-prereq.sh show_examples.zip. ```. As mentioned in my previous investigation, I don't actually know how to use Conda though. But it seems like at least with the setting above, it can install. If you can try that and see if it works for you. And do what you'd usually do to run and see if it works, that will",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/806
https://github.com/google/deepvariant/issues/806:1492,security,compl,completion,1492,"cloud"" \. --machine-type ""e2-standard-16"" \. --zone ""us-west1-b"". gcloud compute ssh pichuan-centos7 --zone ""us-west1-b"". ```. Version:. ```. $ uname -a. Linux pichuan-centos7 3.10.0-1160.114.2.el7.x86_64 #1 SMP Wed Mar 20 15:54:52 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux. ```. Install conda:. ```bash. curl https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh > Miniconda3-latest-Linux-x86_64.sh. bash Miniconda3-latest-Linux-x86_64.sh -b -u -p $HOME/miniconda. eval ""$(${HOME}/miniconda/bin/conda shell.bash hook)"". ```. To repeat what I did in, I tried: https://github.com/google/deepvariant/issues/736#issuecomment-1829204521. ```bash. conda config --add channels defaults && \. conda config --add channels bioconda && \. conda config --add channels conda-forge. conda create -y -n dv-env deepvariant. conda activate dv-env. ```. It completed without any error messages. I see:. ```. (dv-env) [pichuan@pichuan-centos7 ~]$ ls /home/pichuan/miniconda/envs/dv-env/share/. bash-completion deepvariant-1.5.0-0 doc et examples google-cloud-sdk-359.0.0-0 icu info keyutils licenses locale man tabset terminfo zsh. (dv-env) [pichuan@pichuan-centos7 ~]$ ls /home/pichuan/miniconda/envs/dv-env/share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0. call_variants_keras.zip freeze_graph.zip make_examples.zip multisample_make_examples.zip runtime_by_region_vis.zip vcf_stats_report.zip. call_variants.zip licenses.zip model_eval.zip postprocess_variants.zip settings.sh. deeptrio make_examples_somatic.zip model_train.zip run-prereq.sh show_examples.zip. ```. As mentioned in my previous investigation, I don't actually know how to use Conda though. But it seems like at least with the setting above, it can install. If you can try that and see if it works for you. And do what you'd usually do to run and see if it works, that will be great! Please let me know. By the way, . I noticed you were using `bioconda/label/cf201901::deepvariant`. I actually don't know what that",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/806
https://github.com/google/deepvariant/issues/806:235,testability,test,testing,235,"Thank you @Zer0day-0 . I went back and looked at my notes on the last time I tried to install with Conda: https://github.com/google/deepvariant/issues/736#issuecomment-1829204521. I noticed that you're using CentOS. I think I might be testing with CentOS at the time. So let me try that. # Get a CentOS machine to test. I used:. ```bash. gcloud compute instances create ""${USER}-centos7"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""centos-7"" \. --image-project ""centos-cloud"" \. --machine-type ""e2-standard-16"" \. --zone ""us-west1-b"". gcloud compute ssh pichuan-centos7 --zone ""us-west1-b"". ```. Version:. ```. $ uname -a. Linux pichuan-centos7 3.10.0-1160.114.2.el7.x86_64 #1 SMP Wed Mar 20 15:54:52 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux. ```. Install conda:. ```bash. curl https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh > Miniconda3-latest-Linux-x86_64.sh. bash Miniconda3-latest-Linux-x86_64.sh -b -u -p $HOME/miniconda. eval ""$(${HOME}/miniconda/bin/conda shell.bash hook)"". ```. To repeat what I did in, I tried: https://github.com/google/deepvariant/issues/736#issuecomment-1829204521. ```bash. conda config --add channels defaults && \. conda config --add channels bioconda && \. conda config --add channels conda-forge. conda create -y -n dv-env deepvariant. conda activate dv-env. ```. It completed without any error messages. I see:. ```. (dv-env) [pichuan@pichuan-centos7 ~]$ ls /home/pichuan/miniconda/envs/dv-env/share/. bash-completion deepvariant-1.5.0-0 doc et examples google-cloud-sdk-359.0.0-0 icu info keyutils licenses locale man tabset terminfo zsh. (dv-env) [pichuan@pichuan-centos7 ~]$ ls /home/pichuan/miniconda/envs/dv-env/share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0. call_variants_keras.zip freeze_graph.zip make_examples.zip multisample_make_examples.zip runtime_by_region_vis.zip vcf_stats_report.zip. call_variants.zip licenses.zip model_eval.zip postprocess_variants.zip settings.sh. deept",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/806
https://github.com/google/deepvariant/issues/806:314,testability,test,test,314,"Thank you @Zer0day-0 . I went back and looked at my notes on the last time I tried to install with Conda: https://github.com/google/deepvariant/issues/736#issuecomment-1829204521. I noticed that you're using CentOS. I think I might be testing with CentOS at the time. So let me try that. # Get a CentOS machine to test. I used:. ```bash. gcloud compute instances create ""${USER}-centos7"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""centos-7"" \. --image-project ""centos-cloud"" \. --machine-type ""e2-standard-16"" \. --zone ""us-west1-b"". gcloud compute ssh pichuan-centos7 --zone ""us-west1-b"". ```. Version:. ```. $ uname -a. Linux pichuan-centos7 3.10.0-1160.114.2.el7.x86_64 #1 SMP Wed Mar 20 15:54:52 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux. ```. Install conda:. ```bash. curl https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh > Miniconda3-latest-Linux-x86_64.sh. bash Miniconda3-latest-Linux-x86_64.sh -b -u -p $HOME/miniconda. eval ""$(${HOME}/miniconda/bin/conda shell.bash hook)"". ```. To repeat what I did in, I tried: https://github.com/google/deepvariant/issues/736#issuecomment-1829204521. ```bash. conda config --add channels defaults && \. conda config --add channels bioconda && \. conda config --add channels conda-forge. conda create -y -n dv-env deepvariant. conda activate dv-env. ```. It completed without any error messages. I see:. ```. (dv-env) [pichuan@pichuan-centos7 ~]$ ls /home/pichuan/miniconda/envs/dv-env/share/. bash-completion deepvariant-1.5.0-0 doc et examples google-cloud-sdk-359.0.0-0 icu info keyutils licenses locale man tabset terminfo zsh. (dv-env) [pichuan@pichuan-centos7 ~]$ ls /home/pichuan/miniconda/envs/dv-env/share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0. call_variants_keras.zip freeze_graph.zip make_examples.zip multisample_make_examples.zip runtime_by_region_vis.zip vcf_stats_report.zip. call_variants.zip licenses.zip model_eval.zip postprocess_variants.zip settings.sh. deept",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/806
https://github.com/google/deepvariant/issues/806:1024,testability,hook,hook,1024,"ent back and looked at my notes on the last time I tried to install with Conda: https://github.com/google/deepvariant/issues/736#issuecomment-1829204521. I noticed that you're using CentOS. I think I might be testing with CentOS at the time. So let me try that. # Get a CentOS machine to test. I used:. ```bash. gcloud compute instances create ""${USER}-centos7"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""centos-7"" \. --image-project ""centos-cloud"" \. --machine-type ""e2-standard-16"" \. --zone ""us-west1-b"". gcloud compute ssh pichuan-centos7 --zone ""us-west1-b"". ```. Version:. ```. $ uname -a. Linux pichuan-centos7 3.10.0-1160.114.2.el7.x86_64 #1 SMP Wed Mar 20 15:54:52 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux. ```. Install conda:. ```bash. curl https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh > Miniconda3-latest-Linux-x86_64.sh. bash Miniconda3-latest-Linux-x86_64.sh -b -u -p $HOME/miniconda. eval ""$(${HOME}/miniconda/bin/conda shell.bash hook)"". ```. To repeat what I did in, I tried: https://github.com/google/deepvariant/issues/736#issuecomment-1829204521. ```bash. conda config --add channels defaults && \. conda config --add channels bioconda && \. conda config --add channels conda-forge. conda create -y -n dv-env deepvariant. conda activate dv-env. ```. It completed without any error messages. I see:. ```. (dv-env) [pichuan@pichuan-centos7 ~]$ ls /home/pichuan/miniconda/envs/dv-env/share/. bash-completion deepvariant-1.5.0-0 doc et examples google-cloud-sdk-359.0.0-0 icu info keyutils licenses locale man tabset terminfo zsh. (dv-env) [pichuan@pichuan-centos7 ~]$ ls /home/pichuan/miniconda/envs/dv-env/share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0. call_variants_keras.zip freeze_graph.zip make_examples.zip multisample_make_examples.zip runtime_by_region_vis.zip vcf_stats_report.zip. call_variants.zip licenses.zip model_eval.zip postprocess_variants.zip settings.sh. deeptrio make_examples_somatic.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/806
https://github.com/google/deepvariant/issues/806:2576,testability,understand,understand,2576,"val ""$(${HOME}/miniconda/bin/conda shell.bash hook)"". ```. To repeat what I did in, I tried: https://github.com/google/deepvariant/issues/736#issuecomment-1829204521. ```bash. conda config --add channels defaults && \. conda config --add channels bioconda && \. conda config --add channels conda-forge. conda create -y -n dv-env deepvariant. conda activate dv-env. ```. It completed without any error messages. I see:. ```. (dv-env) [pichuan@pichuan-centos7 ~]$ ls /home/pichuan/miniconda/envs/dv-env/share/. bash-completion deepvariant-1.5.0-0 doc et examples google-cloud-sdk-359.0.0-0 icu info keyutils licenses locale man tabset terminfo zsh. (dv-env) [pichuan@pichuan-centos7 ~]$ ls /home/pichuan/miniconda/envs/dv-env/share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0. call_variants_keras.zip freeze_graph.zip make_examples.zip multisample_make_examples.zip runtime_by_region_vis.zip vcf_stats_report.zip. call_variants.zip licenses.zip model_eval.zip postprocess_variants.zip settings.sh. deeptrio make_examples_somatic.zip model_train.zip run-prereq.sh show_examples.zip. ```. As mentioned in my previous investigation, I don't actually know how to use Conda though. But it seems like at least with the setting above, it can install. If you can try that and see if it works for you. And do what you'd usually do to run and see if it works, that will be great! Please let me know. By the way, . I noticed you were using `bioconda/label/cf201901::deepvariant`. I actually don't know what that does. (Can you explain where that came from and why you needed that?). But to understand be behavior, I also tried something like:. `conda create -y -n dv-env bioconda/label/cf201901::deepvariant`. If I did that, then it still seems to be work, but it somehow had an older version `deepvariant-0.7.2-1` in /home/pichuan/miniconda/envs/dv-env/share/, which I don't understand why. I'd recommend that you run using my first command, so you can have a more recent version to try out!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/806
https://github.com/google/deepvariant/issues/806:2862,testability,understand,understand,2862,"val ""$(${HOME}/miniconda/bin/conda shell.bash hook)"". ```. To repeat what I did in, I tried: https://github.com/google/deepvariant/issues/736#issuecomment-1829204521. ```bash. conda config --add channels defaults && \. conda config --add channels bioconda && \. conda config --add channels conda-forge. conda create -y -n dv-env deepvariant. conda activate dv-env. ```. It completed without any error messages. I see:. ```. (dv-env) [pichuan@pichuan-centos7 ~]$ ls /home/pichuan/miniconda/envs/dv-env/share/. bash-completion deepvariant-1.5.0-0 doc et examples google-cloud-sdk-359.0.0-0 icu info keyutils licenses locale man tabset terminfo zsh. (dv-env) [pichuan@pichuan-centos7 ~]$ ls /home/pichuan/miniconda/envs/dv-env/share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0. call_variants_keras.zip freeze_graph.zip make_examples.zip multisample_make_examples.zip runtime_by_region_vis.zip vcf_stats_report.zip. call_variants.zip licenses.zip model_eval.zip postprocess_variants.zip settings.sh. deeptrio make_examples_somatic.zip model_train.zip run-prereq.sh show_examples.zip. ```. As mentioned in my previous investigation, I don't actually know how to use Conda though. But it seems like at least with the setting above, it can install. If you can try that and see if it works for you. And do what you'd usually do to run and see if it works, that will be great! Please let me know. By the way, . I noticed you were using `bioconda/label/cf201901::deepvariant`. I actually don't know what that does. (Can you explain where that came from and why you needed that?). But to understand be behavior, I also tried something like:. `conda create -y -n dv-env bioconda/label/cf201901::deepvariant`. If I did that, then it still seems to be work, but it somehow had an older version `deepvariant-0.7.2-1` in /home/pichuan/miniconda/envs/dv-env/share/, which I don't understand why. I'd recommend that you run using my first command, so you can have a more recent version to try out!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/806
https://github.com/google/deepvariant/issues/806:373,usability,USER,USER,373,"Thank you @Zer0day-0 . I went back and looked at my notes on the last time I tried to install with Conda: https://github.com/google/deepvariant/issues/736#issuecomment-1829204521. I noticed that you're using CentOS. I think I might be testing with CentOS at the time. So let me try that. # Get a CentOS machine to test. I used:. ```bash. gcloud compute instances create ""${USER}-centos7"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""centos-7"" \. --image-project ""centos-cloud"" \. --machine-type ""e2-standard-16"" \. --zone ""us-west1-b"". gcloud compute ssh pichuan-centos7 --zone ""us-west1-b"". ```. Version:. ```. $ uname -a. Linux pichuan-centos7 3.10.0-1160.114.2.el7.x86_64 #1 SMP Wed Mar 20 15:54:52 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux. ```. Install conda:. ```bash. curl https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh > Miniconda3-latest-Linux-x86_64.sh. bash Miniconda3-latest-Linux-x86_64.sh -b -u -p $HOME/miniconda. eval ""$(${HOME}/miniconda/bin/conda shell.bash hook)"". ```. To repeat what I did in, I tried: https://github.com/google/deepvariant/issues/736#issuecomment-1829204521. ```bash. conda config --add channels defaults && \. conda config --add channels bioconda && \. conda config --add channels conda-forge. conda create -y -n dv-env deepvariant. conda activate dv-env. ```. It completed without any error messages. I see:. ```. (dv-env) [pichuan@pichuan-centos7 ~]$ ls /home/pichuan/miniconda/envs/dv-env/share/. bash-completion deepvariant-1.5.0-0 doc et examples google-cloud-sdk-359.0.0-0 icu info keyutils licenses locale man tabset terminfo zsh. (dv-env) [pichuan@pichuan-centos7 ~]$ ls /home/pichuan/miniconda/envs/dv-env/share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0. call_variants_keras.zip freeze_graph.zip make_examples.zip multisample_make_examples.zip runtime_by_region_vis.zip vcf_stats_report.zip. call_variants.zip licenses.zip model_eval.zip postprocess_variants.zip settings.sh. deept",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/806
https://github.com/google/deepvariant/issues/806:1373,usability,error,error,1373,"R}-centos7"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""centos-7"" \. --image-project ""centos-cloud"" \. --machine-type ""e2-standard-16"" \. --zone ""us-west1-b"". gcloud compute ssh pichuan-centos7 --zone ""us-west1-b"". ```. Version:. ```. $ uname -a. Linux pichuan-centos7 3.10.0-1160.114.2.el7.x86_64 #1 SMP Wed Mar 20 15:54:52 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux. ```. Install conda:. ```bash. curl https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh > Miniconda3-latest-Linux-x86_64.sh. bash Miniconda3-latest-Linux-x86_64.sh -b -u -p $HOME/miniconda. eval ""$(${HOME}/miniconda/bin/conda shell.bash hook)"". ```. To repeat what I did in, I tried: https://github.com/google/deepvariant/issues/736#issuecomment-1829204521. ```bash. conda config --add channels defaults && \. conda config --add channels bioconda && \. conda config --add channels conda-forge. conda create -y -n dv-env deepvariant. conda activate dv-env. ```. It completed without any error messages. I see:. ```. (dv-env) [pichuan@pichuan-centos7 ~]$ ls /home/pichuan/miniconda/envs/dv-env/share/. bash-completion deepvariant-1.5.0-0 doc et examples google-cloud-sdk-359.0.0-0 icu info keyutils licenses locale man tabset terminfo zsh. (dv-env) [pichuan@pichuan-centos7 ~]$ ls /home/pichuan/miniconda/envs/dv-env/share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0. call_variants_keras.zip freeze_graph.zip make_examples.zip multisample_make_examples.zip runtime_by_region_vis.zip vcf_stats_report.zip. call_variants.zip licenses.zip model_eval.zip postprocess_variants.zip settings.sh. deeptrio make_examples_somatic.zip model_train.zip run-prereq.sh show_examples.zip. ```. As mentioned in my previous investigation, I don't actually know how to use Conda though. But it seems like at least with the setting above, it can install. If you can try that and see if it works for you. And do what you'd usually do to run and see if it works, that will be great! Please le",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/806
https://github.com/google/deepvariant/issues/806:2590,usability,behavi,behavior,2590,"val ""$(${HOME}/miniconda/bin/conda shell.bash hook)"". ```. To repeat what I did in, I tried: https://github.com/google/deepvariant/issues/736#issuecomment-1829204521. ```bash. conda config --add channels defaults && \. conda config --add channels bioconda && \. conda config --add channels conda-forge. conda create -y -n dv-env deepvariant. conda activate dv-env. ```. It completed without any error messages. I see:. ```. (dv-env) [pichuan@pichuan-centos7 ~]$ ls /home/pichuan/miniconda/envs/dv-env/share/. bash-completion deepvariant-1.5.0-0 doc et examples google-cloud-sdk-359.0.0-0 icu info keyutils licenses locale man tabset terminfo zsh. (dv-env) [pichuan@pichuan-centos7 ~]$ ls /home/pichuan/miniconda/envs/dv-env/share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0. call_variants_keras.zip freeze_graph.zip make_examples.zip multisample_make_examples.zip runtime_by_region_vis.zip vcf_stats_report.zip. call_variants.zip licenses.zip model_eval.zip postprocess_variants.zip settings.sh. deeptrio make_examples_somatic.zip model_train.zip run-prereq.sh show_examples.zip. ```. As mentioned in my previous investigation, I don't actually know how to use Conda though. But it seems like at least with the setting above, it can install. If you can try that and see if it works for you. And do what you'd usually do to run and see if it works, that will be great! Please let me know. By the way, . I noticed you were using `bioconda/label/cf201901::deepvariant`. I actually don't know what that does. (Can you explain where that came from and why you needed that?). But to understand be behavior, I also tried something like:. `conda create -y -n dv-env bioconda/label/cf201901::deepvariant`. If I did that, then it still seems to be work, but it somehow had an older version `deepvariant-0.7.2-1` in /home/pichuan/miniconda/envs/dv-env/share/, which I don't understand why. I'd recommend that you run using my first command, so you can have a more recent version to try out!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/806
https://github.com/google/deepvariant/issues/806:2920,usability,command,command,2920,"val ""$(${HOME}/miniconda/bin/conda shell.bash hook)"". ```. To repeat what I did in, I tried: https://github.com/google/deepvariant/issues/736#issuecomment-1829204521. ```bash. conda config --add channels defaults && \. conda config --add channels bioconda && \. conda config --add channels conda-forge. conda create -y -n dv-env deepvariant. conda activate dv-env. ```. It completed without any error messages. I see:. ```. (dv-env) [pichuan@pichuan-centos7 ~]$ ls /home/pichuan/miniconda/envs/dv-env/share/. bash-completion deepvariant-1.5.0-0 doc et examples google-cloud-sdk-359.0.0-0 icu info keyutils licenses locale man tabset terminfo zsh. (dv-env) [pichuan@pichuan-centos7 ~]$ ls /home/pichuan/miniconda/envs/dv-env/share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0. call_variants_keras.zip freeze_graph.zip make_examples.zip multisample_make_examples.zip runtime_by_region_vis.zip vcf_stats_report.zip. call_variants.zip licenses.zip model_eval.zip postprocess_variants.zip settings.sh. deeptrio make_examples_somatic.zip model_train.zip run-prereq.sh show_examples.zip. ```. As mentioned in my previous investigation, I don't actually know how to use Conda though. But it seems like at least with the setting above, it can install. If you can try that and see if it works for you. And do what you'd usually do to run and see if it works, that will be great! Please let me know. By the way, . I noticed you were using `bioconda/label/cf201901::deepvariant`. I actually don't know what that does. (Can you explain where that came from and why you needed that?). But to understand be behavior, I also tried something like:. `conda create -y -n dv-env bioconda/label/cf201901::deepvariant`. If I did that, then it still seems to be work, but it somehow had an older version `deepvariant-0.7.2-1` in /home/pichuan/miniconda/envs/dv-env/share/, which I don't understand why. I'd recommend that you run using my first command, so you can have a more recent version to try out!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/806
https://github.com/google/deepvariant/issues/806:296,availability,error,errors,296,"thank you for your kind support @pichuan . It seems I was running a really old and obsolete version of anaconda(4.2.1) in my SGE. It took some gymnastics to install it for a single user and run across the nodes but I updated my conda to current version. after that installation completed with no errors. . the ""bioconda/label/cf201901::deepvariant"" was provided in bioconda page, which is supposed to be another installation solution. I think this issue can be closed. For anyone in future running in a problem similar to me, I'll make a repo with the installation code in my profile!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/806
https://github.com/google/deepvariant/issues/806:92,deployability,version,version,92,"thank you for your kind support @pichuan . It seems I was running a really old and obsolete version of anaconda(4.2.1) in my SGE. It took some gymnastics to install it for a single user and run across the nodes but I updated my conda to current version. after that installation completed with no errors. . the ""bioconda/label/cf201901::deepvariant"" was provided in bioconda page, which is supposed to be another installation solution. I think this issue can be closed. For anyone in future running in a problem similar to me, I'll make a repo with the installation code in my profile!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/806
https://github.com/google/deepvariant/issues/806:157,deployability,instal,install,157,"thank you for your kind support @pichuan . It seems I was running a really old and obsolete version of anaconda(4.2.1) in my SGE. It took some gymnastics to install it for a single user and run across the nodes but I updated my conda to current version. after that installation completed with no errors. . the ""bioconda/label/cf201901::deepvariant"" was provided in bioconda page, which is supposed to be another installation solution. I think this issue can be closed. For anyone in future running in a problem similar to me, I'll make a repo with the installation code in my profile!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/806
https://github.com/google/deepvariant/issues/806:217,deployability,updat,updated,217,"thank you for your kind support @pichuan . It seems I was running a really old and obsolete version of anaconda(4.2.1) in my SGE. It took some gymnastics to install it for a single user and run across the nodes but I updated my conda to current version. after that installation completed with no errors. . the ""bioconda/label/cf201901::deepvariant"" was provided in bioconda page, which is supposed to be another installation solution. I think this issue can be closed. For anyone in future running in a problem similar to me, I'll make a repo with the installation code in my profile!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/806
https://github.com/google/deepvariant/issues/806:245,deployability,version,version,245,"thank you for your kind support @pichuan . It seems I was running a really old and obsolete version of anaconda(4.2.1) in my SGE. It took some gymnastics to install it for a single user and run across the nodes but I updated my conda to current version. after that installation completed with no errors. . the ""bioconda/label/cf201901::deepvariant"" was provided in bioconda page, which is supposed to be another installation solution. I think this issue can be closed. For anyone in future running in a problem similar to me, I'll make a repo with the installation code in my profile!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/806
https://github.com/google/deepvariant/issues/806:265,deployability,instal,installation,265,"thank you for your kind support @pichuan . It seems I was running a really old and obsolete version of anaconda(4.2.1) in my SGE. It took some gymnastics to install it for a single user and run across the nodes but I updated my conda to current version. after that installation completed with no errors. . the ""bioconda/label/cf201901::deepvariant"" was provided in bioconda page, which is supposed to be another installation solution. I think this issue can be closed. For anyone in future running in a problem similar to me, I'll make a repo with the installation code in my profile!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/806
https://github.com/google/deepvariant/issues/806:412,deployability,instal,installation,412,"thank you for your kind support @pichuan . It seems I was running a really old and obsolete version of anaconda(4.2.1) in my SGE. It took some gymnastics to install it for a single user and run across the nodes but I updated my conda to current version. after that installation completed with no errors. . the ""bioconda/label/cf201901::deepvariant"" was provided in bioconda page, which is supposed to be another installation solution. I think this issue can be closed. For anyone in future running in a problem similar to me, I'll make a repo with the installation code in my profile!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/806
https://github.com/google/deepvariant/issues/806:552,deployability,instal,installation,552,"thank you for your kind support @pichuan . It seems I was running a really old and obsolete version of anaconda(4.2.1) in my SGE. It took some gymnastics to install it for a single user and run across the nodes but I updated my conda to current version. after that installation completed with no errors. . the ""bioconda/label/cf201901::deepvariant"" was provided in bioconda page, which is supposed to be another installation solution. I think this issue can be closed. For anyone in future running in a problem similar to me, I'll make a repo with the installation code in my profile!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/806
https://github.com/google/deepvariant/issues/806:237,energy efficiency,current,current,237,"thank you for your kind support @pichuan . It seems I was running a really old and obsolete version of anaconda(4.2.1) in my SGE. It took some gymnastics to install it for a single user and run across the nodes but I updated my conda to current version. after that installation completed with no errors. . the ""bioconda/label/cf201901::deepvariant"" was provided in bioconda page, which is supposed to be another installation solution. I think this issue can be closed. For anyone in future running in a problem similar to me, I'll make a repo with the installation code in my profile!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/806
https://github.com/google/deepvariant/issues/806:576,energy efficiency,profil,profile,576,"thank you for your kind support @pichuan . It seems I was running a really old and obsolete version of anaconda(4.2.1) in my SGE. It took some gymnastics to install it for a single user and run across the nodes but I updated my conda to current version. after that installation completed with no errors. . the ""bioconda/label/cf201901::deepvariant"" was provided in bioconda page, which is supposed to be another installation solution. I think this issue can be closed. For anyone in future running in a problem similar to me, I'll make a repo with the installation code in my profile!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/806
https://github.com/google/deepvariant/issues/806:92,integrability,version,version,92,"thank you for your kind support @pichuan . It seems I was running a really old and obsolete version of anaconda(4.2.1) in my SGE. It took some gymnastics to install it for a single user and run across the nodes but I updated my conda to current version. after that installation completed with no errors. . the ""bioconda/label/cf201901::deepvariant"" was provided in bioconda page, which is supposed to be another installation solution. I think this issue can be closed. For anyone in future running in a problem similar to me, I'll make a repo with the installation code in my profile!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/806
https://github.com/google/deepvariant/issues/806:245,integrability,version,version,245,"thank you for your kind support @pichuan . It seems I was running a really old and obsolete version of anaconda(4.2.1) in my SGE. It took some gymnastics to install it for a single user and run across the nodes but I updated my conda to current version. after that installation completed with no errors. . the ""bioconda/label/cf201901::deepvariant"" was provided in bioconda page, which is supposed to be another installation solution. I think this issue can be closed. For anyone in future running in a problem similar to me, I'll make a repo with the installation code in my profile!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/806
https://github.com/google/deepvariant/issues/806:92,modifiability,version,version,92,"thank you for your kind support @pichuan . It seems I was running a really old and obsolete version of anaconda(4.2.1) in my SGE. It took some gymnastics to install it for a single user and run across the nodes but I updated my conda to current version. after that installation completed with no errors. . the ""bioconda/label/cf201901::deepvariant"" was provided in bioconda page, which is supposed to be another installation solution. I think this issue can be closed. For anyone in future running in a problem similar to me, I'll make a repo with the installation code in my profile!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/806
https://github.com/google/deepvariant/issues/806:245,modifiability,version,version,245,"thank you for your kind support @pichuan . It seems I was running a really old and obsolete version of anaconda(4.2.1) in my SGE. It took some gymnastics to install it for a single user and run across the nodes but I updated my conda to current version. after that installation completed with no errors. . the ""bioconda/label/cf201901::deepvariant"" was provided in bioconda page, which is supposed to be another installation solution. I think this issue can be closed. For anyone in future running in a problem similar to me, I'll make a repo with the installation code in my profile!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/806
https://github.com/google/deepvariant/issues/806:296,performance,error,errors,296,"thank you for your kind support @pichuan . It seems I was running a really old and obsolete version of anaconda(4.2.1) in my SGE. It took some gymnastics to install it for a single user and run across the nodes but I updated my conda to current version. after that installation completed with no errors. . the ""bioconda/label/cf201901::deepvariant"" was provided in bioconda page, which is supposed to be another installation solution. I think this issue can be closed. For anyone in future running in a problem similar to me, I'll make a repo with the installation code in my profile!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/806
https://github.com/google/deepvariant/issues/806:576,performance,profil,profile,576,"thank you for your kind support @pichuan . It seems I was running a really old and obsolete version of anaconda(4.2.1) in my SGE. It took some gymnastics to install it for a single user and run across the nodes but I updated my conda to current version. after that installation completed with no errors. . the ""bioconda/label/cf201901::deepvariant"" was provided in bioconda page, which is supposed to be another installation solution. I think this issue can be closed. For anyone in future running in a problem similar to me, I'll make a repo with the installation code in my profile!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/806
https://github.com/google/deepvariant/issues/806:217,safety,updat,updated,217,"thank you for your kind support @pichuan . It seems I was running a really old and obsolete version of anaconda(4.2.1) in my SGE. It took some gymnastics to install it for a single user and run across the nodes but I updated my conda to current version. after that installation completed with no errors. . the ""bioconda/label/cf201901::deepvariant"" was provided in bioconda page, which is supposed to be another installation solution. I think this issue can be closed. For anyone in future running in a problem similar to me, I'll make a repo with the installation code in my profile!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/806
https://github.com/google/deepvariant/issues/806:278,safety,compl,completed,278,"thank you for your kind support @pichuan . It seems I was running a really old and obsolete version of anaconda(4.2.1) in my SGE. It took some gymnastics to install it for a single user and run across the nodes but I updated my conda to current version. after that installation completed with no errors. . the ""bioconda/label/cf201901::deepvariant"" was provided in bioconda page, which is supposed to be another installation solution. I think this issue can be closed. For anyone in future running in a problem similar to me, I'll make a repo with the installation code in my profile!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/806
https://github.com/google/deepvariant/issues/806:296,safety,error,errors,296,"thank you for your kind support @pichuan . It seems I was running a really old and obsolete version of anaconda(4.2.1) in my SGE. It took some gymnastics to install it for a single user and run across the nodes but I updated my conda to current version. after that installation completed with no errors. . the ""bioconda/label/cf201901::deepvariant"" was provided in bioconda page, which is supposed to be another installation solution. I think this issue can be closed. For anyone in future running in a problem similar to me, I'll make a repo with the installation code in my profile!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/806
https://github.com/google/deepvariant/issues/806:217,security,updat,updated,217,"thank you for your kind support @pichuan . It seems I was running a really old and obsolete version of anaconda(4.2.1) in my SGE. It took some gymnastics to install it for a single user and run across the nodes but I updated my conda to current version. after that installation completed with no errors. . the ""bioconda/label/cf201901::deepvariant"" was provided in bioconda page, which is supposed to be another installation solution. I think this issue can be closed. For anyone in future running in a problem similar to me, I'll make a repo with the installation code in my profile!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/806
https://github.com/google/deepvariant/issues/806:278,security,compl,completed,278,"thank you for your kind support @pichuan . It seems I was running a really old and obsolete version of anaconda(4.2.1) in my SGE. It took some gymnastics to install it for a single user and run across the nodes but I updated my conda to current version. after that installation completed with no errors. . the ""bioconda/label/cf201901::deepvariant"" was provided in bioconda page, which is supposed to be another installation solution. I think this issue can be closed. For anyone in future running in a problem similar to me, I'll make a repo with the installation code in my profile!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/806
https://github.com/google/deepvariant/issues/806:24,usability,support,support,24,"thank you for your kind support @pichuan . It seems I was running a really old and obsolete version of anaconda(4.2.1) in my SGE. It took some gymnastics to install it for a single user and run across the nodes but I updated my conda to current version. after that installation completed with no errors. . the ""bioconda/label/cf201901::deepvariant"" was provided in bioconda page, which is supposed to be another installation solution. I think this issue can be closed. For anyone in future running in a problem similar to me, I'll make a repo with the installation code in my profile!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/806
https://github.com/google/deepvariant/issues/806:181,usability,user,user,181,"thank you for your kind support @pichuan . It seems I was running a really old and obsolete version of anaconda(4.2.1) in my SGE. It took some gymnastics to install it for a single user and run across the nodes but I updated my conda to current version. after that installation completed with no errors. . the ""bioconda/label/cf201901::deepvariant"" was provided in bioconda page, which is supposed to be another installation solution. I think this issue can be closed. For anyone in future running in a problem similar to me, I'll make a repo with the installation code in my profile!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/806
https://github.com/google/deepvariant/issues/806:296,usability,error,errors,296,"thank you for your kind support @pichuan . It seems I was running a really old and obsolete version of anaconda(4.2.1) in my SGE. It took some gymnastics to install it for a single user and run across the nodes but I updated my conda to current version. after that installation completed with no errors. . the ""bioconda/label/cf201901::deepvariant"" was provided in bioconda page, which is supposed to be another installation solution. I think this issue can be closed. For anyone in future running in a problem similar to me, I'll make a repo with the installation code in my profile!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/806
https://github.com/google/deepvariant/issues/806:461,usability,close,closed,461,"thank you for your kind support @pichuan . It seems I was running a really old and obsolete version of anaconda(4.2.1) in my SGE. It took some gymnastics to install it for a single user and run across the nodes but I updated my conda to current version. after that installation completed with no errors. . the ""bioconda/label/cf201901::deepvariant"" was provided in bioconda page, which is supposed to be another installation solution. I think this issue can be closed. For anyone in future running in a problem similar to me, I'll make a repo with the installation code in my profile!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/806
https://github.com/google/deepvariant/issues/807:141,availability,error,error,141,"> **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6.1/docs/FAQ.md**:. > . > **Describe the issue:** Am getting the error as ""Fatal Python error: Segmentation fault"". > . > **Setup**. > . > * Operating system: Ubuntu 22.04.2 LTS. > * DeepVariant version: 1.6.1. > * Installation method (Docker, built from source, etc.): Docker. > * Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) Its a Pabcio CLR data. Read Input is provided in Fastq format and reference in FASTA format. > . > **Steps to reproduce:**. > . > * Command: sudo docker run . > -v ""${INPUT_DIR}"":""/input"" . > -v ""${OUTPUT_DIR}"":""/output"" . > google/deepvariant:""${BIN_VERSION}"" . > /opt/deepvariant/bin/run_deepvariant . > --model_type=PACBIO . > --ref=/input/RILWLs1.fasta . > --reads=/input/Out.fastq . > --output_vcf=/output/output.vcf.gz . > --output_gvcf=/output/output.g.vcf.gz . > --intermediate_results_dir /output/intermediate_results_dir . > --num_shards=15. > * Error trace: (if applicable). > . > **Does the quick start test work on your system?** Please test with https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md. Yes. Test data works fine. ![Screenshot from 2024-04-17 12-24-22](https://private-user-images.githubusercontent.com/68117296/323111309-41ac66ff-ff52-493f-b18f-f017921caa86.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTMzMzcyOTIsIm5iZiI6MTcxMzMzNjk5MiwicGF0aCI6Ii82ODExNzI5Ni8zMjMxMTEzMDktNDFhYzY2ZmYtZmY1Mi00OTNmLWIxOGYtZjAxNzkyMWNhYTg2LnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDA0MTclMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQwNDE3VDA2NTYzMlomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTg3ZDQ3ZTBmNDFjYWQ4YWQyNmM4MDdmYTJiYjVjNzlhYmI1MDA2NzQxOGY3MjA1ZjU1ODY3ZDUzOTcyMTkyNzQmWC1BbXotU2lnbmVkSGVhZGVycz1ob",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/807
https://github.com/google/deepvariant/issues/807:164,availability,error,error,164,"> **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6.1/docs/FAQ.md**:. > . > **Describe the issue:** Am getting the error as ""Fatal Python error: Segmentation fault"". > . > **Setup**. > . > * Operating system: Ubuntu 22.04.2 LTS. > * DeepVariant version: 1.6.1. > * Installation method (Docker, built from source, etc.): Docker. > * Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) Its a Pabcio CLR data. Read Input is provided in Fastq format and reference in FASTA format. > . > **Steps to reproduce:**. > . > * Command: sudo docker run . > -v ""${INPUT_DIR}"":""/input"" . > -v ""${OUTPUT_DIR}"":""/output"" . > google/deepvariant:""${BIN_VERSION}"" . > /opt/deepvariant/bin/run_deepvariant . > --model_type=PACBIO . > --ref=/input/RILWLs1.fasta . > --reads=/input/Out.fastq . > --output_vcf=/output/output.vcf.gz . > --output_gvcf=/output/output.g.vcf.gz . > --intermediate_results_dir /output/intermediate_results_dir . > --num_shards=15. > * Error trace: (if applicable). > . > **Does the quick start test work on your system?** Please test with https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md. Yes. Test data works fine. ![Screenshot from 2024-04-17 12-24-22](https://private-user-images.githubusercontent.com/68117296/323111309-41ac66ff-ff52-493f-b18f-f017921caa86.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTMzMzcyOTIsIm5iZiI6MTcxMzMzNjk5MiwicGF0aCI6Ii82ODExNzI5Ni8zMjMxMTEzMDktNDFhYzY2ZmYtZmY1Mi00OTNmLWIxOGYtZjAxNzkyMWNhYTg2LnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDA0MTclMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQwNDE3VDA2NTYzMlomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTg3ZDQ3ZTBmNDFjYWQ4YWQyNmM4MDdmYTJiYjVjNzlhYmI1MDA2NzQxOGY3MjA1ZjU1ODY3ZDUzOTcyMTkyNzQmWC1BbXotU2lnbmVkSGVhZGVycz1ob",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/807
https://github.com/google/deepvariant/issues/807:184,availability,fault,fault,184,"> **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6.1/docs/FAQ.md**:. > . > **Describe the issue:** Am getting the error as ""Fatal Python error: Segmentation fault"". > . > **Setup**. > . > * Operating system: Ubuntu 22.04.2 LTS. > * DeepVariant version: 1.6.1. > * Installation method (Docker, built from source, etc.): Docker. > * Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) Its a Pabcio CLR data. Read Input is provided in Fastq format and reference in FASTA format. > . > **Steps to reproduce:**. > . > * Command: sudo docker run . > -v ""${INPUT_DIR}"":""/input"" . > -v ""${OUTPUT_DIR}"":""/output"" . > google/deepvariant:""${BIN_VERSION}"" . > /opt/deepvariant/bin/run_deepvariant . > --model_type=PACBIO . > --ref=/input/RILWLs1.fasta . > --reads=/input/Out.fastq . > --output_vcf=/output/output.vcf.gz . > --output_gvcf=/output/output.g.vcf.gz . > --intermediate_results_dir /output/intermediate_results_dir . > --num_shards=15. > * Error trace: (if applicable). > . > **Does the quick start test work on your system?** Please test with https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md. Yes. Test data works fine. ![Screenshot from 2024-04-17 12-24-22](https://private-user-images.githubusercontent.com/68117296/323111309-41ac66ff-ff52-493f-b18f-f017921caa86.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTMzMzcyOTIsIm5iZiI6MTcxMzMzNjk5MiwicGF0aCI6Ii82ODExNzI5Ni8zMjMxMTEzMDktNDFhYzY2ZmYtZmY1Mi00OTNmLWIxOGYtZjAxNzkyMWNhYTg2LnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDA0MTclMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQwNDE3VDA2NTYzMlomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTg3ZDQ3ZTBmNDFjYWQ4YWQyNmM4MDdmYTJiYjVjNzlhYmI1MDA2NzQxOGY3MjA1ZjU1ODY3ZDUzOTcyMTkyNzQmWC1BbXotU2lnbmVkSGVhZGVycz1ob",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/807
https://github.com/google/deepvariant/issues/807:217,availability,Operat,Operating,217,"> **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6.1/docs/FAQ.md**:. > . > **Describe the issue:** Am getting the error as ""Fatal Python error: Segmentation fault"". > . > **Setup**. > . > * Operating system: Ubuntu 22.04.2 LTS. > * DeepVariant version: 1.6.1. > * Installation method (Docker, built from source, etc.): Docker. > * Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) Its a Pabcio CLR data. Read Input is provided in Fastq format and reference in FASTA format. > . > **Steps to reproduce:**. > . > * Command: sudo docker run . > -v ""${INPUT_DIR}"":""/input"" . > -v ""${OUTPUT_DIR}"":""/output"" . > google/deepvariant:""${BIN_VERSION}"" . > /opt/deepvariant/bin/run_deepvariant . > --model_type=PACBIO . > --ref=/input/RILWLs1.fasta . > --reads=/input/Out.fastq . > --output_vcf=/output/output.vcf.gz . > --output_gvcf=/output/output.g.vcf.gz . > --intermediate_results_dir /output/intermediate_results_dir . > --num_shards=15. > * Error trace: (if applicable). > . > **Does the quick start test work on your system?** Please test with https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md. Yes. Test data works fine. ![Screenshot from 2024-04-17 12-24-22](https://private-user-images.githubusercontent.com/68117296/323111309-41ac66ff-ff52-493f-b18f-f017921caa86.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTMzMzcyOTIsIm5iZiI6MTcxMzMzNjk5MiwicGF0aCI6Ii82ODExNzI5Ni8zMjMxMTEzMDktNDFhYzY2ZmYtZmY1Mi00OTNmLWIxOGYtZjAxNzkyMWNhYTg2LnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDA0MTclMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQwNDE3VDA2NTYzMlomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTg3ZDQ3ZTBmNDFjYWQ4YWQyNmM4MDdmYTJiYjVjNzlhYmI1MDA2NzQxOGY3MjA1ZjU1ODY3ZDUzOTcyMTkyNzQmWC1BbXotU2lnbmVkSGVhZGVycz1ob",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/807
https://github.com/google/deepvariant/issues/807:1021,availability,Error,Error,1021," FAQ? https://github.com/google/deepvariant/blob/r1.6.1/docs/FAQ.md**:. > . > **Describe the issue:** Am getting the error as ""Fatal Python error: Segmentation fault"". > . > **Setup**. > . > * Operating system: Ubuntu 22.04.2 LTS. > * DeepVariant version: 1.6.1. > * Installation method (Docker, built from source, etc.): Docker. > * Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) Its a Pabcio CLR data. Read Input is provided in Fastq format and reference in FASTA format. > . > **Steps to reproduce:**. > . > * Command: sudo docker run . > -v ""${INPUT_DIR}"":""/input"" . > -v ""${OUTPUT_DIR}"":""/output"" . > google/deepvariant:""${BIN_VERSION}"" . > /opt/deepvariant/bin/run_deepvariant . > --model_type=PACBIO . > --ref=/input/RILWLs1.fasta . > --reads=/input/Out.fastq . > --output_vcf=/output/output.vcf.gz . > --output_gvcf=/output/output.g.vcf.gz . > --intermediate_results_dir /output/intermediate_results_dir . > --num_shards=15. > * Error trace: (if applicable). > . > **Does the quick start test work on your system?** Please test with https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md. Yes. Test data works fine. ![Screenshot from 2024-04-17 12-24-22](https://private-user-images.githubusercontent.com/68117296/323111309-41ac66ff-ff52-493f-b18f-f017921caa86.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTMzMzcyOTIsIm5iZiI6MTcxMzMzNjk5MiwicGF0aCI6Ii82ODExNzI5Ni8zMjMxMTEzMDktNDFhYzY2ZmYtZmY1Mi00OTNmLWIxOGYtZjAxNzkyMWNhYTg2LnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDA0MTclMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQwNDE3VDA2NTYzMlomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTg3ZDQ3ZTBmNDFjYWQ4YWQyNmM4MDdmYTJiYjVjNzlhYmI1MDA2NzQxOGY3MjA1ZjU1ODY3ZDUzOTcyMTkyNzQmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/807
https://github.com/google/deepvariant/issues/807:271,deployability,version,version,271,"> **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6.1/docs/FAQ.md**:. > . > **Describe the issue:** Am getting the error as ""Fatal Python error: Segmentation fault"". > . > **Setup**. > . > * Operating system: Ubuntu 22.04.2 LTS. > * DeepVariant version: 1.6.1. > * Installation method (Docker, built from source, etc.): Docker. > * Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) Its a Pabcio CLR data. Read Input is provided in Fastq format and reference in FASTA format. > . > **Steps to reproduce:**. > . > * Command: sudo docker run . > -v ""${INPUT_DIR}"":""/input"" . > -v ""${OUTPUT_DIR}"":""/output"" . > google/deepvariant:""${BIN_VERSION}"" . > /opt/deepvariant/bin/run_deepvariant . > --model_type=PACBIO . > --ref=/input/RILWLs1.fasta . > --reads=/input/Out.fastq . > --output_vcf=/output/output.vcf.gz . > --output_gvcf=/output/output.g.vcf.gz . > --intermediate_results_dir /output/intermediate_results_dir . > --num_shards=15. > * Error trace: (if applicable). > . > **Does the quick start test work on your system?** Please test with https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md. Yes. Test data works fine. ![Screenshot from 2024-04-17 12-24-22](https://private-user-images.githubusercontent.com/68117296/323111309-41ac66ff-ff52-493f-b18f-f017921caa86.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTMzMzcyOTIsIm5iZiI6MTcxMzMzNjk5MiwicGF0aCI6Ii82ODExNzI5Ni8zMjMxMTEzMDktNDFhYzY2ZmYtZmY1Mi00OTNmLWIxOGYtZjAxNzkyMWNhYTg2LnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDA0MTclMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQwNDE3VDA2NTYzMlomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTg3ZDQ3ZTBmNDFjYWQ4YWQyNmM4MDdmYTJiYjVjNzlhYmI1MDA2NzQxOGY3MjA1ZjU1ODY3ZDUzOTcyMTkyNzQmWC1BbXotU2lnbmVkSGVhZGVycz1ob",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/807
https://github.com/google/deepvariant/issues/807:291,deployability,Instal,Installation,291,"> **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6.1/docs/FAQ.md**:. > . > **Describe the issue:** Am getting the error as ""Fatal Python error: Segmentation fault"". > . > **Setup**. > . > * Operating system: Ubuntu 22.04.2 LTS. > * DeepVariant version: 1.6.1. > * Installation method (Docker, built from source, etc.): Docker. > * Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) Its a Pabcio CLR data. Read Input is provided in Fastq format and reference in FASTA format. > . > **Steps to reproduce:**. > . > * Command: sudo docker run . > -v ""${INPUT_DIR}"":""/input"" . > -v ""${OUTPUT_DIR}"":""/output"" . > google/deepvariant:""${BIN_VERSION}"" . > /opt/deepvariant/bin/run_deepvariant . > --model_type=PACBIO . > --ref=/input/RILWLs1.fasta . > --reads=/input/Out.fastq . > --output_vcf=/output/output.vcf.gz . > --output_gvcf=/output/output.g.vcf.gz . > --intermediate_results_dir /output/intermediate_results_dir . > --num_shards=15. > * Error trace: (if applicable). > . > **Does the quick start test work on your system?** Please test with https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md. Yes. Test data works fine. ![Screenshot from 2024-04-17 12-24-22](https://private-user-images.githubusercontent.com/68117296/323111309-41ac66ff-ff52-493f-b18f-f017921caa86.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTMzMzcyOTIsIm5iZiI6MTcxMzMzNjk5MiwicGF0aCI6Ii82ODExNzI5Ni8zMjMxMTEzMDktNDFhYzY2ZmYtZmY1Mi00OTNmLWIxOGYtZjAxNzkyMWNhYTg2LnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDA0MTclMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQwNDE3VDA2NTYzMlomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTg3ZDQ3ZTBmNDFjYWQ4YWQyNmM4MDdmYTJiYjVjNzlhYmI1MDA2NzQxOGY3MjA1ZjU1ODY3ZDUzOTcyMTkyNzQmWC1BbXotU2lnbmVkSGVhZGVycz1ob",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/807
https://github.com/google/deepvariant/issues/807:184,energy efficiency,fault,fault,184,"> **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6.1/docs/FAQ.md**:. > . > **Describe the issue:** Am getting the error as ""Fatal Python error: Segmentation fault"". > . > **Setup**. > . > * Operating system: Ubuntu 22.04.2 LTS. > * DeepVariant version: 1.6.1. > * Installation method (Docker, built from source, etc.): Docker. > * Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) Its a Pabcio CLR data. Read Input is provided in Fastq format and reference in FASTA format. > . > **Steps to reproduce:**. > . > * Command: sudo docker run . > -v ""${INPUT_DIR}"":""/input"" . > -v ""${OUTPUT_DIR}"":""/output"" . > google/deepvariant:""${BIN_VERSION}"" . > /opt/deepvariant/bin/run_deepvariant . > --model_type=PACBIO . > --ref=/input/RILWLs1.fasta . > --reads=/input/Out.fastq . > --output_vcf=/output/output.vcf.gz . > --output_gvcf=/output/output.g.vcf.gz . > --intermediate_results_dir /output/intermediate_results_dir . > --num_shards=15. > * Error trace: (if applicable). > . > **Does the quick start test work on your system?** Please test with https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md. Yes. Test data works fine. ![Screenshot from 2024-04-17 12-24-22](https://private-user-images.githubusercontent.com/68117296/323111309-41ac66ff-ff52-493f-b18f-f017921caa86.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTMzMzcyOTIsIm5iZiI6MTcxMzMzNjk5MiwicGF0aCI6Ii82ODExNzI5Ni8zMjMxMTEzMDktNDFhYzY2ZmYtZmY1Mi00OTNmLWIxOGYtZjAxNzkyMWNhYTg2LnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDA0MTclMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQwNDE3VDA2NTYzMlomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTg3ZDQ3ZTBmNDFjYWQ4YWQyNmM4MDdmYTJiYjVjNzlhYmI1MDA2NzQxOGY3MjA1ZjU1ODY3ZDUzOTcyMTkyNzQmWC1BbXotU2lnbmVkSGVhZGVycz1ob",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/807
https://github.com/google/deepvariant/issues/807:271,integrability,version,version,271,"> **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6.1/docs/FAQ.md**:. > . > **Describe the issue:** Am getting the error as ""Fatal Python error: Segmentation fault"". > . > **Setup**. > . > * Operating system: Ubuntu 22.04.2 LTS. > * DeepVariant version: 1.6.1. > * Installation method (Docker, built from source, etc.): Docker. > * Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) Its a Pabcio CLR data. Read Input is provided in Fastq format and reference in FASTA format. > . > **Steps to reproduce:**. > . > * Command: sudo docker run . > -v ""${INPUT_DIR}"":""/input"" . > -v ""${OUTPUT_DIR}"":""/output"" . > google/deepvariant:""${BIN_VERSION}"" . > /opt/deepvariant/bin/run_deepvariant . > --model_type=PACBIO . > --ref=/input/RILWLs1.fasta . > --reads=/input/Out.fastq . > --output_vcf=/output/output.vcf.gz . > --output_gvcf=/output/output.g.vcf.gz . > --intermediate_results_dir /output/intermediate_results_dir . > --num_shards=15. > * Error trace: (if applicable). > . > **Does the quick start test work on your system?** Please test with https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md. Yes. Test data works fine. ![Screenshot from 2024-04-17 12-24-22](https://private-user-images.githubusercontent.com/68117296/323111309-41ac66ff-ff52-493f-b18f-f017921caa86.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTMzMzcyOTIsIm5iZiI6MTcxMzMzNjk5MiwicGF0aCI6Ii82ODExNzI5Ni8zMjMxMTEzMDktNDFhYzY2ZmYtZmY1Mi00OTNmLWIxOGYtZjAxNzkyMWNhYTg2LnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDA0MTclMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQwNDE3VDA2NTYzMlomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTg3ZDQ3ZTBmNDFjYWQ4YWQyNmM4MDdmYTJiYjVjNzlhYmI1MDA2NzQxOGY3MjA1ZjU1ODY3ZDUzOTcyMTkyNzQmWC1BbXotU2lnbmVkSGVhZGVycz1ob",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/807
https://github.com/google/deepvariant/issues/807:520,interoperability,format,format,520,"> **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6.1/docs/FAQ.md**:. > . > **Describe the issue:** Am getting the error as ""Fatal Python error: Segmentation fault"". > . > **Setup**. > . > * Operating system: Ubuntu 22.04.2 LTS. > * DeepVariant version: 1.6.1. > * Installation method (Docker, built from source, etc.): Docker. > * Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) Its a Pabcio CLR data. Read Input is provided in Fastq format and reference in FASTA format. > . > **Steps to reproduce:**. > . > * Command: sudo docker run . > -v ""${INPUT_DIR}"":""/input"" . > -v ""${OUTPUT_DIR}"":""/output"" . > google/deepvariant:""${BIN_VERSION}"" . > /opt/deepvariant/bin/run_deepvariant . > --model_type=PACBIO . > --ref=/input/RILWLs1.fasta . > --reads=/input/Out.fastq . > --output_vcf=/output/output.vcf.gz . > --output_gvcf=/output/output.g.vcf.gz . > --intermediate_results_dir /output/intermediate_results_dir . > --num_shards=15. > * Error trace: (if applicable). > . > **Does the quick start test work on your system?** Please test with https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md. Yes. Test data works fine. ![Screenshot from 2024-04-17 12-24-22](https://private-user-images.githubusercontent.com/68117296/323111309-41ac66ff-ff52-493f-b18f-f017921caa86.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTMzMzcyOTIsIm5iZiI6MTcxMzMzNjk5MiwicGF0aCI6Ii82ODExNzI5Ni8zMjMxMTEzMDktNDFhYzY2ZmYtZmY1Mi00OTNmLWIxOGYtZjAxNzkyMWNhYTg2LnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDA0MTclMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQwNDE3VDA2NTYzMlomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTg3ZDQ3ZTBmNDFjYWQ4YWQyNmM4MDdmYTJiYjVjNzlhYmI1MDA2NzQxOGY3MjA1ZjU1ODY3ZDUzOTcyMTkyNzQmWC1BbXotU2lnbmVkSGVhZGVycz1ob",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/807
https://github.com/google/deepvariant/issues/807:550,interoperability,format,format,550,"> **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6.1/docs/FAQ.md**:. > . > **Describe the issue:** Am getting the error as ""Fatal Python error: Segmentation fault"". > . > **Setup**. > . > * Operating system: Ubuntu 22.04.2 LTS. > * DeepVariant version: 1.6.1. > * Installation method (Docker, built from source, etc.): Docker. > * Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) Its a Pabcio CLR data. Read Input is provided in Fastq format and reference in FASTA format. > . > **Steps to reproduce:**. > . > * Command: sudo docker run . > -v ""${INPUT_DIR}"":""/input"" . > -v ""${OUTPUT_DIR}"":""/output"" . > google/deepvariant:""${BIN_VERSION}"" . > /opt/deepvariant/bin/run_deepvariant . > --model_type=PACBIO . > --ref=/input/RILWLs1.fasta . > --reads=/input/Out.fastq . > --output_vcf=/output/output.vcf.gz . > --output_gvcf=/output/output.g.vcf.gz . > --intermediate_results_dir /output/intermediate_results_dir . > --num_shards=15. > * Error trace: (if applicable). > . > **Does the quick start test work on your system?** Please test with https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md. Yes. Test data works fine. ![Screenshot from 2024-04-17 12-24-22](https://private-user-images.githubusercontent.com/68117296/323111309-41ac66ff-ff52-493f-b18f-f017921caa86.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTMzMzcyOTIsIm5iZiI6MTcxMzMzNjk5MiwicGF0aCI6Ii82ODExNzI5Ni8zMjMxMTEzMDktNDFhYzY2ZmYtZmY1Mi00OTNmLWIxOGYtZjAxNzkyMWNhYTg2LnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDA0MTclMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQwNDE3VDA2NTYzMlomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTg3ZDQ3ZTBmNDFjYWQ4YWQyNmM4MDdmYTJiYjVjNzlhYmI1MDA2NzQxOGY3MjA1ZjU1ODY3ZDUzOTcyMTkyNzQmWC1BbXotU2lnbmVkSGVhZGVycz1ob",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/807
https://github.com/google/deepvariant/issues/807:271,modifiability,version,version,271,"> **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6.1/docs/FAQ.md**:. > . > **Describe the issue:** Am getting the error as ""Fatal Python error: Segmentation fault"". > . > **Setup**. > . > * Operating system: Ubuntu 22.04.2 LTS. > * DeepVariant version: 1.6.1. > * Installation method (Docker, built from source, etc.): Docker. > * Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) Its a Pabcio CLR data. Read Input is provided in Fastq format and reference in FASTA format. > . > **Steps to reproduce:**. > . > * Command: sudo docker run . > -v ""${INPUT_DIR}"":""/input"" . > -v ""${OUTPUT_DIR}"":""/output"" . > google/deepvariant:""${BIN_VERSION}"" . > /opt/deepvariant/bin/run_deepvariant . > --model_type=PACBIO . > --ref=/input/RILWLs1.fasta . > --reads=/input/Out.fastq . > --output_vcf=/output/output.vcf.gz . > --output_gvcf=/output/output.g.vcf.gz . > --intermediate_results_dir /output/intermediate_results_dir . > --num_shards=15. > * Error trace: (if applicable). > . > **Does the quick start test work on your system?** Please test with https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md. Yes. Test data works fine. ![Screenshot from 2024-04-17 12-24-22](https://private-user-images.githubusercontent.com/68117296/323111309-41ac66ff-ff52-493f-b18f-f017921caa86.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTMzMzcyOTIsIm5iZiI6MTcxMzMzNjk5MiwicGF0aCI6Ii82ODExNzI5Ni8zMjMxMTEzMDktNDFhYzY2ZmYtZmY1Mi00OTNmLWIxOGYtZjAxNzkyMWNhYTg2LnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDA0MTclMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQwNDE3VDA2NTYzMlomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTg3ZDQ3ZTBmNDFjYWQ4YWQyNmM4MDdmYTJiYjVjNzlhYmI1MDA2NzQxOGY3MjA1ZjU1ODY3ZDUzOTcyMTkyNzQmWC1BbXotU2lnbmVkSGVhZGVycz1ob",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/807
https://github.com/google/deepvariant/issues/807:784,modifiability,PAC,PACBIO,784,"> **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6.1/docs/FAQ.md**:. > . > **Describe the issue:** Am getting the error as ""Fatal Python error: Segmentation fault"". > . > **Setup**. > . > * Operating system: Ubuntu 22.04.2 LTS. > * DeepVariant version: 1.6.1. > * Installation method (Docker, built from source, etc.): Docker. > * Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) Its a Pabcio CLR data. Read Input is provided in Fastq format and reference in FASTA format. > . > **Steps to reproduce:**. > . > * Command: sudo docker run . > -v ""${INPUT_DIR}"":""/input"" . > -v ""${OUTPUT_DIR}"":""/output"" . > google/deepvariant:""${BIN_VERSION}"" . > /opt/deepvariant/bin/run_deepvariant . > --model_type=PACBIO . > --ref=/input/RILWLs1.fasta . > --reads=/input/Out.fastq . > --output_vcf=/output/output.vcf.gz . > --output_gvcf=/output/output.g.vcf.gz . > --intermediate_results_dir /output/intermediate_results_dir . > --num_shards=15. > * Error trace: (if applicable). > . > **Does the quick start test work on your system?** Please test with https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md. Yes. Test data works fine. ![Screenshot from 2024-04-17 12-24-22](https://private-user-images.githubusercontent.com/68117296/323111309-41ac66ff-ff52-493f-b18f-f017921caa86.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTMzMzcyOTIsIm5iZiI6MTcxMzMzNjk5MiwicGF0aCI6Ii82ODExNzI5Ni8zMjMxMTEzMDktNDFhYzY2ZmYtZmY1Mi00OTNmLWIxOGYtZjAxNzkyMWNhYTg2LnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDA0MTclMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQwNDE3VDA2NTYzMlomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTg3ZDQ3ZTBmNDFjYWQ4YWQyNmM4MDdmYTJiYjVjNzlhYmI1MDA2NzQxOGY3MjA1ZjU1ODY3ZDUzOTcyMTkyNzQmWC1BbXotU2lnbmVkSGVhZGVycz1ob",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/807
https://github.com/google/deepvariant/issues/807:141,performance,error,error,141,"> **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6.1/docs/FAQ.md**:. > . > **Describe the issue:** Am getting the error as ""Fatal Python error: Segmentation fault"". > . > **Setup**. > . > * Operating system: Ubuntu 22.04.2 LTS. > * DeepVariant version: 1.6.1. > * Installation method (Docker, built from source, etc.): Docker. > * Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) Its a Pabcio CLR data. Read Input is provided in Fastq format and reference in FASTA format. > . > **Steps to reproduce:**. > . > * Command: sudo docker run . > -v ""${INPUT_DIR}"":""/input"" . > -v ""${OUTPUT_DIR}"":""/output"" . > google/deepvariant:""${BIN_VERSION}"" . > /opt/deepvariant/bin/run_deepvariant . > --model_type=PACBIO . > --ref=/input/RILWLs1.fasta . > --reads=/input/Out.fastq . > --output_vcf=/output/output.vcf.gz . > --output_gvcf=/output/output.g.vcf.gz . > --intermediate_results_dir /output/intermediate_results_dir . > --num_shards=15. > * Error trace: (if applicable). > . > **Does the quick start test work on your system?** Please test with https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md. Yes. Test data works fine. ![Screenshot from 2024-04-17 12-24-22](https://private-user-images.githubusercontent.com/68117296/323111309-41ac66ff-ff52-493f-b18f-f017921caa86.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTMzMzcyOTIsIm5iZiI6MTcxMzMzNjk5MiwicGF0aCI6Ii82ODExNzI5Ni8zMjMxMTEzMDktNDFhYzY2ZmYtZmY1Mi00OTNmLWIxOGYtZjAxNzkyMWNhYTg2LnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDA0MTclMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQwNDE3VDA2NTYzMlomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTg3ZDQ3ZTBmNDFjYWQ4YWQyNmM4MDdmYTJiYjVjNzlhYmI1MDA2NzQxOGY3MjA1ZjU1ODY3ZDUzOTcyMTkyNzQmWC1BbXotU2lnbmVkSGVhZGVycz1ob",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/807
https://github.com/google/deepvariant/issues/807:164,performance,error,error,164,"> **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6.1/docs/FAQ.md**:. > . > **Describe the issue:** Am getting the error as ""Fatal Python error: Segmentation fault"". > . > **Setup**. > . > * Operating system: Ubuntu 22.04.2 LTS. > * DeepVariant version: 1.6.1. > * Installation method (Docker, built from source, etc.): Docker. > * Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) Its a Pabcio CLR data. Read Input is provided in Fastq format and reference in FASTA format. > . > **Steps to reproduce:**. > . > * Command: sudo docker run . > -v ""${INPUT_DIR}"":""/input"" . > -v ""${OUTPUT_DIR}"":""/output"" . > google/deepvariant:""${BIN_VERSION}"" . > /opt/deepvariant/bin/run_deepvariant . > --model_type=PACBIO . > --ref=/input/RILWLs1.fasta . > --reads=/input/Out.fastq . > --output_vcf=/output/output.vcf.gz . > --output_gvcf=/output/output.g.vcf.gz . > --intermediate_results_dir /output/intermediate_results_dir . > --num_shards=15. > * Error trace: (if applicable). > . > **Does the quick start test work on your system?** Please test with https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md. Yes. Test data works fine. ![Screenshot from 2024-04-17 12-24-22](https://private-user-images.githubusercontent.com/68117296/323111309-41ac66ff-ff52-493f-b18f-f017921caa86.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTMzMzcyOTIsIm5iZiI6MTcxMzMzNjk5MiwicGF0aCI6Ii82ODExNzI5Ni8zMjMxMTEzMDktNDFhYzY2ZmYtZmY1Mi00OTNmLWIxOGYtZjAxNzkyMWNhYTg2LnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDA0MTclMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQwNDE3VDA2NTYzMlomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTg3ZDQ3ZTBmNDFjYWQ4YWQyNmM4MDdmYTJiYjVjNzlhYmI1MDA2NzQxOGY3MjA1ZjU1ODY3ZDUzOTcyMTkyNzQmWC1BbXotU2lnbmVkSGVhZGVycz1ob",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/807
https://github.com/google/deepvariant/issues/807:184,performance,fault,fault,184,"> **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6.1/docs/FAQ.md**:. > . > **Describe the issue:** Am getting the error as ""Fatal Python error: Segmentation fault"". > . > **Setup**. > . > * Operating system: Ubuntu 22.04.2 LTS. > * DeepVariant version: 1.6.1. > * Installation method (Docker, built from source, etc.): Docker. > * Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) Its a Pabcio CLR data. Read Input is provided in Fastq format and reference in FASTA format. > . > **Steps to reproduce:**. > . > * Command: sudo docker run . > -v ""${INPUT_DIR}"":""/input"" . > -v ""${OUTPUT_DIR}"":""/output"" . > google/deepvariant:""${BIN_VERSION}"" . > /opt/deepvariant/bin/run_deepvariant . > --model_type=PACBIO . > --ref=/input/RILWLs1.fasta . > --reads=/input/Out.fastq . > --output_vcf=/output/output.vcf.gz . > --output_gvcf=/output/output.g.vcf.gz . > --intermediate_results_dir /output/intermediate_results_dir . > --num_shards=15. > * Error trace: (if applicable). > . > **Does the quick start test work on your system?** Please test with https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md. Yes. Test data works fine. ![Screenshot from 2024-04-17 12-24-22](https://private-user-images.githubusercontent.com/68117296/323111309-41ac66ff-ff52-493f-b18f-f017921caa86.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTMzMzcyOTIsIm5iZiI6MTcxMzMzNjk5MiwicGF0aCI6Ii82ODExNzI5Ni8zMjMxMTEzMDktNDFhYzY2ZmYtZmY1Mi00OTNmLWIxOGYtZjAxNzkyMWNhYTg2LnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDA0MTclMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQwNDE3VDA2NTYzMlomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTg3ZDQ3ZTBmNDFjYWQ4YWQyNmM4MDdmYTJiYjVjNzlhYmI1MDA2NzQxOGY3MjA1ZjU1ODY3ZDUzOTcyMTkyNzQmWC1BbXotU2lnbmVkSGVhZGVycz1ob",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/807
https://github.com/google/deepvariant/issues/807:1021,performance,Error,Error,1021," FAQ? https://github.com/google/deepvariant/blob/r1.6.1/docs/FAQ.md**:. > . > **Describe the issue:** Am getting the error as ""Fatal Python error: Segmentation fault"". > . > **Setup**. > . > * Operating system: Ubuntu 22.04.2 LTS. > * DeepVariant version: 1.6.1. > * Installation method (Docker, built from source, etc.): Docker. > * Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) Its a Pabcio CLR data. Read Input is provided in Fastq format and reference in FASTA format. > . > **Steps to reproduce:**. > . > * Command: sudo docker run . > -v ""${INPUT_DIR}"":""/input"" . > -v ""${OUTPUT_DIR}"":""/output"" . > google/deepvariant:""${BIN_VERSION}"" . > /opt/deepvariant/bin/run_deepvariant . > --model_type=PACBIO . > --ref=/input/RILWLs1.fasta . > --reads=/input/Out.fastq . > --output_vcf=/output/output.vcf.gz . > --output_gvcf=/output/output.g.vcf.gz . > --intermediate_results_dir /output/intermediate_results_dir . > --num_shards=15. > * Error trace: (if applicable). > . > **Does the quick start test work on your system?** Please test with https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md. Yes. Test data works fine. ![Screenshot from 2024-04-17 12-24-22](https://private-user-images.githubusercontent.com/68117296/323111309-41ac66ff-ff52-493f-b18f-f017921caa86.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTMzMzcyOTIsIm5iZiI6MTcxMzMzNjk5MiwicGF0aCI6Ii82ODExNzI5Ni8zMjMxMTEzMDktNDFhYzY2ZmYtZmY1Mi00OTNmLWIxOGYtZjAxNzkyMWNhYTg2LnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDA0MTclMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQwNDE3VDA2NTYzMlomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTg3ZDQ3ZTBmNDFjYWQ4YWQyNmM4MDdmYTJiYjVjNzlhYmI1MDA2NzQxOGY3MjA1ZjU1ODY3ZDUzOTcyMTkyNzQmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/807
https://github.com/google/deepvariant/issues/807:184,reliability,fault,fault,184,"> **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6.1/docs/FAQ.md**:. > . > **Describe the issue:** Am getting the error as ""Fatal Python error: Segmentation fault"". > . > **Setup**. > . > * Operating system: Ubuntu 22.04.2 LTS. > * DeepVariant version: 1.6.1. > * Installation method (Docker, built from source, etc.): Docker. > * Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) Its a Pabcio CLR data. Read Input is provided in Fastq format and reference in FASTA format. > . > **Steps to reproduce:**. > . > * Command: sudo docker run . > -v ""${INPUT_DIR}"":""/input"" . > -v ""${OUTPUT_DIR}"":""/output"" . > google/deepvariant:""${BIN_VERSION}"" . > /opt/deepvariant/bin/run_deepvariant . > --model_type=PACBIO . > --ref=/input/RILWLs1.fasta . > --reads=/input/Out.fastq . > --output_vcf=/output/output.vcf.gz . > --output_gvcf=/output/output.g.vcf.gz . > --intermediate_results_dir /output/intermediate_results_dir . > --num_shards=15. > * Error trace: (if applicable). > . > **Does the quick start test work on your system?** Please test with https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md. Yes. Test data works fine. ![Screenshot from 2024-04-17 12-24-22](https://private-user-images.githubusercontent.com/68117296/323111309-41ac66ff-ff52-493f-b18f-f017921caa86.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTMzMzcyOTIsIm5iZiI6MTcxMzMzNjk5MiwicGF0aCI6Ii82ODExNzI5Ni8zMjMxMTEzMDktNDFhYzY2ZmYtZmY1Mi00OTNmLWIxOGYtZjAxNzkyMWNhYTg2LnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDA0MTclMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQwNDE3VDA2NTYzMlomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTg3ZDQ3ZTBmNDFjYWQ4YWQyNmM4MDdmYTJiYjVjNzlhYmI1MDA2NzQxOGY3MjA1ZjU1ODY3ZDUzOTcyMTkyNzQmWC1BbXotU2lnbmVkSGVhZGVycz1ob",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/807
https://github.com/google/deepvariant/issues/807:1059,reliability,Doe,Does,1059,"ariant/blob/r1.6.1/docs/FAQ.md**:. > . > **Describe the issue:** Am getting the error as ""Fatal Python error: Segmentation fault"". > . > **Setup**. > . > * Operating system: Ubuntu 22.04.2 LTS. > * DeepVariant version: 1.6.1. > * Installation method (Docker, built from source, etc.): Docker. > * Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) Its a Pabcio CLR data. Read Input is provided in Fastq format and reference in FASTA format. > . > **Steps to reproduce:**. > . > * Command: sudo docker run . > -v ""${INPUT_DIR}"":""/input"" . > -v ""${OUTPUT_DIR}"":""/output"" . > google/deepvariant:""${BIN_VERSION}"" . > /opt/deepvariant/bin/run_deepvariant . > --model_type=PACBIO . > --ref=/input/RILWLs1.fasta . > --reads=/input/Out.fastq . > --output_vcf=/output/output.vcf.gz . > --output_gvcf=/output/output.g.vcf.gz . > --intermediate_results_dir /output/intermediate_results_dir . > --num_shards=15. > * Error trace: (if applicable). > . > **Does the quick start test work on your system?** Please test with https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md. Yes. Test data works fine. ![Screenshot from 2024-04-17 12-24-22](https://private-user-images.githubusercontent.com/68117296/323111309-41ac66ff-ff52-493f-b18f-f017921caa86.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTMzMzcyOTIsIm5iZiI6MTcxMzMzNjk5MiwicGF0aCI6Ii82ODExNzI5Ni8zMjMxMTEzMDktNDFhYzY2ZmYtZmY1Mi00OTNmLWIxOGYtZjAxNzkyMWNhYTg2LnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDA0MTclMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQwNDE3VDA2NTYzMlomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTg3ZDQ3ZTBmNDFjYWQ4YWQyNmM4MDdmYTJiYjVjNzlhYmI1MDA2NzQxOGY3MjA1ZjU1ODY3ZDUzOTcyMTkyNzQmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.gtqyKpmVpHdy0Y",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/807
https://github.com/google/deepvariant/issues/807:141,safety,error,error,141,"> **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6.1/docs/FAQ.md**:. > . > **Describe the issue:** Am getting the error as ""Fatal Python error: Segmentation fault"". > . > **Setup**. > . > * Operating system: Ubuntu 22.04.2 LTS. > * DeepVariant version: 1.6.1. > * Installation method (Docker, built from source, etc.): Docker. > * Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) Its a Pabcio CLR data. Read Input is provided in Fastq format and reference in FASTA format. > . > **Steps to reproduce:**. > . > * Command: sudo docker run . > -v ""${INPUT_DIR}"":""/input"" . > -v ""${OUTPUT_DIR}"":""/output"" . > google/deepvariant:""${BIN_VERSION}"" . > /opt/deepvariant/bin/run_deepvariant . > --model_type=PACBIO . > --ref=/input/RILWLs1.fasta . > --reads=/input/Out.fastq . > --output_vcf=/output/output.vcf.gz . > --output_gvcf=/output/output.g.vcf.gz . > --intermediate_results_dir /output/intermediate_results_dir . > --num_shards=15. > * Error trace: (if applicable). > . > **Does the quick start test work on your system?** Please test with https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md. Yes. Test data works fine. ![Screenshot from 2024-04-17 12-24-22](https://private-user-images.githubusercontent.com/68117296/323111309-41ac66ff-ff52-493f-b18f-f017921caa86.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTMzMzcyOTIsIm5iZiI6MTcxMzMzNjk5MiwicGF0aCI6Ii82ODExNzI5Ni8zMjMxMTEzMDktNDFhYzY2ZmYtZmY1Mi00OTNmLWIxOGYtZjAxNzkyMWNhYTg2LnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDA0MTclMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQwNDE3VDA2NTYzMlomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTg3ZDQ3ZTBmNDFjYWQ4YWQyNmM4MDdmYTJiYjVjNzlhYmI1MDA2NzQxOGY3MjA1ZjU1ODY3ZDUzOTcyMTkyNzQmWC1BbXotU2lnbmVkSGVhZGVycz1ob",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/807
https://github.com/google/deepvariant/issues/807:164,safety,error,error,164,"> **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6.1/docs/FAQ.md**:. > . > **Describe the issue:** Am getting the error as ""Fatal Python error: Segmentation fault"". > . > **Setup**. > . > * Operating system: Ubuntu 22.04.2 LTS. > * DeepVariant version: 1.6.1. > * Installation method (Docker, built from source, etc.): Docker. > * Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) Its a Pabcio CLR data. Read Input is provided in Fastq format and reference in FASTA format. > . > **Steps to reproduce:**. > . > * Command: sudo docker run . > -v ""${INPUT_DIR}"":""/input"" . > -v ""${OUTPUT_DIR}"":""/output"" . > google/deepvariant:""${BIN_VERSION}"" . > /opt/deepvariant/bin/run_deepvariant . > --model_type=PACBIO . > --ref=/input/RILWLs1.fasta . > --reads=/input/Out.fastq . > --output_vcf=/output/output.vcf.gz . > --output_gvcf=/output/output.g.vcf.gz . > --intermediate_results_dir /output/intermediate_results_dir . > --num_shards=15. > * Error trace: (if applicable). > . > **Does the quick start test work on your system?** Please test with https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md. Yes. Test data works fine. ![Screenshot from 2024-04-17 12-24-22](https://private-user-images.githubusercontent.com/68117296/323111309-41ac66ff-ff52-493f-b18f-f017921caa86.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTMzMzcyOTIsIm5iZiI6MTcxMzMzNjk5MiwicGF0aCI6Ii82ODExNzI5Ni8zMjMxMTEzMDktNDFhYzY2ZmYtZmY1Mi00OTNmLWIxOGYtZjAxNzkyMWNhYTg2LnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDA0MTclMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQwNDE3VDA2NTYzMlomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTg3ZDQ3ZTBmNDFjYWQ4YWQyNmM4MDdmYTJiYjVjNzlhYmI1MDA2NzQxOGY3MjA1ZjU1ODY3ZDUzOTcyMTkyNzQmWC1BbXotU2lnbmVkSGVhZGVycz1ob",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/807
https://github.com/google/deepvariant/issues/807:184,safety,fault,fault,184,"> **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6.1/docs/FAQ.md**:. > . > **Describe the issue:** Am getting the error as ""Fatal Python error: Segmentation fault"". > . > **Setup**. > . > * Operating system: Ubuntu 22.04.2 LTS. > * DeepVariant version: 1.6.1. > * Installation method (Docker, built from source, etc.): Docker. > * Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) Its a Pabcio CLR data. Read Input is provided in Fastq format and reference in FASTA format. > . > **Steps to reproduce:**. > . > * Command: sudo docker run . > -v ""${INPUT_DIR}"":""/input"" . > -v ""${OUTPUT_DIR}"":""/output"" . > google/deepvariant:""${BIN_VERSION}"" . > /opt/deepvariant/bin/run_deepvariant . > --model_type=PACBIO . > --ref=/input/RILWLs1.fasta . > --reads=/input/Out.fastq . > --output_vcf=/output/output.vcf.gz . > --output_gvcf=/output/output.g.vcf.gz . > --intermediate_results_dir /output/intermediate_results_dir . > --num_shards=15. > * Error trace: (if applicable). > . > **Does the quick start test work on your system?** Please test with https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md. Yes. Test data works fine. ![Screenshot from 2024-04-17 12-24-22](https://private-user-images.githubusercontent.com/68117296/323111309-41ac66ff-ff52-493f-b18f-f017921caa86.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTMzMzcyOTIsIm5iZiI6MTcxMzMzNjk5MiwicGF0aCI6Ii82ODExNzI5Ni8zMjMxMTEzMDktNDFhYzY2ZmYtZmY1Mi00OTNmLWIxOGYtZjAxNzkyMWNhYTg2LnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDA0MTclMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQwNDE3VDA2NTYzMlomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTg3ZDQ3ZTBmNDFjYWQ4YWQyNmM4MDdmYTJiYjVjNzlhYmI1MDA2NzQxOGY3MjA1ZjU1ODY3ZDUzOTcyMTkyNzQmWC1BbXotU2lnbmVkSGVhZGVycz1ob",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/807
https://github.com/google/deepvariant/issues/807:493,safety,Input,Input,493,"> **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6.1/docs/FAQ.md**:. > . > **Describe the issue:** Am getting the error as ""Fatal Python error: Segmentation fault"". > . > **Setup**. > . > * Operating system: Ubuntu 22.04.2 LTS. > * DeepVariant version: 1.6.1. > * Installation method (Docker, built from source, etc.): Docker. > * Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) Its a Pabcio CLR data. Read Input is provided in Fastq format and reference in FASTA format. > . > **Steps to reproduce:**. > . > * Command: sudo docker run . > -v ""${INPUT_DIR}"":""/input"" . > -v ""${OUTPUT_DIR}"":""/output"" . > google/deepvariant:""${BIN_VERSION}"" . > /opt/deepvariant/bin/run_deepvariant . > --model_type=PACBIO . > --ref=/input/RILWLs1.fasta . > --reads=/input/Out.fastq . > --output_vcf=/output/output.vcf.gz . > --output_gvcf=/output/output.g.vcf.gz . > --intermediate_results_dir /output/intermediate_results_dir . > --num_shards=15. > * Error trace: (if applicable). > . > **Does the quick start test work on your system?** Please test with https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md. Yes. Test data works fine. ![Screenshot from 2024-04-17 12-24-22](https://private-user-images.githubusercontent.com/68117296/323111309-41ac66ff-ff52-493f-b18f-f017921caa86.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTMzMzcyOTIsIm5iZiI6MTcxMzMzNjk5MiwicGF0aCI6Ii82ODExNzI5Ni8zMjMxMTEzMDktNDFhYzY2ZmYtZmY1Mi00OTNmLWIxOGYtZjAxNzkyMWNhYTg2LnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDA0MTclMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQwNDE3VDA2NTYzMlomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTg3ZDQ3ZTBmNDFjYWQ4YWQyNmM4MDdmYTJiYjVjNzlhYmI1MDA2NzQxOGY3MjA1ZjU1ODY3ZDUzOTcyMTkyNzQmWC1BbXotU2lnbmVkSGVhZGVycz1ob",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/807
https://github.com/google/deepvariant/issues/807:646,safety,input,input,646,"> **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6.1/docs/FAQ.md**:. > . > **Describe the issue:** Am getting the error as ""Fatal Python error: Segmentation fault"". > . > **Setup**. > . > * Operating system: Ubuntu 22.04.2 LTS. > * DeepVariant version: 1.6.1. > * Installation method (Docker, built from source, etc.): Docker. > * Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) Its a Pabcio CLR data. Read Input is provided in Fastq format and reference in FASTA format. > . > **Steps to reproduce:**. > . > * Command: sudo docker run . > -v ""${INPUT_DIR}"":""/input"" . > -v ""${OUTPUT_DIR}"":""/output"" . > google/deepvariant:""${BIN_VERSION}"" . > /opt/deepvariant/bin/run_deepvariant . > --model_type=PACBIO . > --ref=/input/RILWLs1.fasta . > --reads=/input/Out.fastq . > --output_vcf=/output/output.vcf.gz . > --output_gvcf=/output/output.g.vcf.gz . > --intermediate_results_dir /output/intermediate_results_dir . > --num_shards=15. > * Error trace: (if applicable). > . > **Does the quick start test work on your system?** Please test with https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md. Yes. Test data works fine. ![Screenshot from 2024-04-17 12-24-22](https://private-user-images.githubusercontent.com/68117296/323111309-41ac66ff-ff52-493f-b18f-f017921caa86.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTMzMzcyOTIsIm5iZiI6MTcxMzMzNjk5MiwicGF0aCI6Ii82ODExNzI5Ni8zMjMxMTEzMDktNDFhYzY2ZmYtZmY1Mi00OTNmLWIxOGYtZjAxNzkyMWNhYTg2LnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDA0MTclMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQwNDE3VDA2NTYzMlomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTg3ZDQ3ZTBmNDFjYWQ4YWQyNmM4MDdmYTJiYjVjNzlhYmI1MDA2NzQxOGY3MjA1ZjU1ODY3ZDUzOTcyMTkyNzQmWC1BbXotU2lnbmVkSGVhZGVycz1ob",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/807
https://github.com/google/deepvariant/issues/807:802,safety,input,input,802,"> **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6.1/docs/FAQ.md**:. > . > **Describe the issue:** Am getting the error as ""Fatal Python error: Segmentation fault"". > . > **Setup**. > . > * Operating system: Ubuntu 22.04.2 LTS. > * DeepVariant version: 1.6.1. > * Installation method (Docker, built from source, etc.): Docker. > * Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) Its a Pabcio CLR data. Read Input is provided in Fastq format and reference in FASTA format. > . > **Steps to reproduce:**. > . > * Command: sudo docker run . > -v ""${INPUT_DIR}"":""/input"" . > -v ""${OUTPUT_DIR}"":""/output"" . > google/deepvariant:""${BIN_VERSION}"" . > /opt/deepvariant/bin/run_deepvariant . > --model_type=PACBIO . > --ref=/input/RILWLs1.fasta . > --reads=/input/Out.fastq . > --output_vcf=/output/output.vcf.gz . > --output_gvcf=/output/output.g.vcf.gz . > --intermediate_results_dir /output/intermediate_results_dir . > --num_shards=15. > * Error trace: (if applicable). > . > **Does the quick start test work on your system?** Please test with https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md. Yes. Test data works fine. ![Screenshot from 2024-04-17 12-24-22](https://private-user-images.githubusercontent.com/68117296/323111309-41ac66ff-ff52-493f-b18f-f017921caa86.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTMzMzcyOTIsIm5iZiI6MTcxMzMzNjk5MiwicGF0aCI6Ii82ODExNzI5Ni8zMjMxMTEzMDktNDFhYzY2ZmYtZmY1Mi00OTNmLWIxOGYtZjAxNzkyMWNhYTg2LnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDA0MTclMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQwNDE3VDA2NTYzMlomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTg3ZDQ3ZTBmNDFjYWQ4YWQyNmM4MDdmYTJiYjVjNzlhYmI1MDA2NzQxOGY3MjA1ZjU1ODY3ZDUzOTcyMTkyNzQmWC1BbXotU2lnbmVkSGVhZGVycz1ob",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/807
https://github.com/google/deepvariant/issues/807:835,safety,input,input,835,"> **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6.1/docs/FAQ.md**:. > . > **Describe the issue:** Am getting the error as ""Fatal Python error: Segmentation fault"". > . > **Setup**. > . > * Operating system: Ubuntu 22.04.2 LTS. > * DeepVariant version: 1.6.1. > * Installation method (Docker, built from source, etc.): Docker. > * Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) Its a Pabcio CLR data. Read Input is provided in Fastq format and reference in FASTA format. > . > **Steps to reproduce:**. > . > * Command: sudo docker run . > -v ""${INPUT_DIR}"":""/input"" . > -v ""${OUTPUT_DIR}"":""/output"" . > google/deepvariant:""${BIN_VERSION}"" . > /opt/deepvariant/bin/run_deepvariant . > --model_type=PACBIO . > --ref=/input/RILWLs1.fasta . > --reads=/input/Out.fastq . > --output_vcf=/output/output.vcf.gz . > --output_gvcf=/output/output.g.vcf.gz . > --intermediate_results_dir /output/intermediate_results_dir . > --num_shards=15. > * Error trace: (if applicable). > . > **Does the quick start test work on your system?** Please test with https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md. Yes. Test data works fine. ![Screenshot from 2024-04-17 12-24-22](https://private-user-images.githubusercontent.com/68117296/323111309-41ac66ff-ff52-493f-b18f-f017921caa86.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTMzMzcyOTIsIm5iZiI6MTcxMzMzNjk5MiwicGF0aCI6Ii82ODExNzI5Ni8zMjMxMTEzMDktNDFhYzY2ZmYtZmY1Mi00OTNmLWIxOGYtZjAxNzkyMWNhYTg2LnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDA0MTclMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQwNDE3VDA2NTYzMlomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTg3ZDQ3ZTBmNDFjYWQ4YWQyNmM4MDdmYTJiYjVjNzlhYmI1MDA2NzQxOGY3MjA1ZjU1ODY3ZDUzOTcyMTkyNzQmWC1BbXotU2lnbmVkSGVhZGVycz1ob",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/807
https://github.com/google/deepvariant/issues/807:1021,safety,Error,Error,1021," FAQ? https://github.com/google/deepvariant/blob/r1.6.1/docs/FAQ.md**:. > . > **Describe the issue:** Am getting the error as ""Fatal Python error: Segmentation fault"". > . > **Setup**. > . > * Operating system: Ubuntu 22.04.2 LTS. > * DeepVariant version: 1.6.1. > * Installation method (Docker, built from source, etc.): Docker. > * Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) Its a Pabcio CLR data. Read Input is provided in Fastq format and reference in FASTA format. > . > **Steps to reproduce:**. > . > * Command: sudo docker run . > -v ""${INPUT_DIR}"":""/input"" . > -v ""${OUTPUT_DIR}"":""/output"" . > google/deepvariant:""${BIN_VERSION}"" . > /opt/deepvariant/bin/run_deepvariant . > --model_type=PACBIO . > --ref=/input/RILWLs1.fasta . > --reads=/input/Out.fastq . > --output_vcf=/output/output.vcf.gz . > --output_gvcf=/output/output.g.vcf.gz . > --intermediate_results_dir /output/intermediate_results_dir . > --num_shards=15. > * Error trace: (if applicable). > . > **Does the quick start test work on your system?** Please test with https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md. Yes. Test data works fine. ![Screenshot from 2024-04-17 12-24-22](https://private-user-images.githubusercontent.com/68117296/323111309-41ac66ff-ff52-493f-b18f-f017921caa86.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTMzMzcyOTIsIm5iZiI6MTcxMzMzNjk5MiwicGF0aCI6Ii82ODExNzI5Ni8zMjMxMTEzMDktNDFhYzY2ZmYtZmY1Mi00OTNmLWIxOGYtZjAxNzkyMWNhYTg2LnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDA0MTclMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQwNDE3VDA2NTYzMlomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTg3ZDQ3ZTBmNDFjYWQ4YWQyNmM4MDdmYTJiYjVjNzlhYmI1MDA2NzQxOGY3MjA1ZjU1ODY3ZDUzOTcyMTkyNzQmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/807
https://github.com/google/deepvariant/issues/807:1080,safety,test,test,1080,"cs/FAQ.md**:. > . > **Describe the issue:** Am getting the error as ""Fatal Python error: Segmentation fault"". > . > **Setup**. > . > * Operating system: Ubuntu 22.04.2 LTS. > * DeepVariant version: 1.6.1. > * Installation method (Docker, built from source, etc.): Docker. > * Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) Its a Pabcio CLR data. Read Input is provided in Fastq format and reference in FASTA format. > . > **Steps to reproduce:**. > . > * Command: sudo docker run . > -v ""${INPUT_DIR}"":""/input"" . > -v ""${OUTPUT_DIR}"":""/output"" . > google/deepvariant:""${BIN_VERSION}"" . > /opt/deepvariant/bin/run_deepvariant . > --model_type=PACBIO . > --ref=/input/RILWLs1.fasta . > --reads=/input/Out.fastq . > --output_vcf=/output/output.vcf.gz . > --output_gvcf=/output/output.g.vcf.gz . > --intermediate_results_dir /output/intermediate_results_dir . > --num_shards=15. > * Error trace: (if applicable). > . > **Does the quick start test work on your system?** Please test with https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md. Yes. Test data works fine. ![Screenshot from 2024-04-17 12-24-22](https://private-user-images.githubusercontent.com/68117296/323111309-41ac66ff-ff52-493f-b18f-f017921caa86.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTMzMzcyOTIsIm5iZiI6MTcxMzMzNjk5MiwicGF0aCI6Ii82ODExNzI5Ni8zMjMxMTEzMDktNDFhYzY2ZmYtZmY1Mi00OTNmLWIxOGYtZjAxNzkyMWNhYTg2LnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDA0MTclMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQwNDE3VDA2NTYzMlomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTg3ZDQ3ZTBmNDFjYWQ4YWQyNmM4MDdmYTJiYjVjNzlhYmI1MDA2NzQxOGY3MjA1ZjU1ODY3ZDUzOTcyMTkyNzQmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.gtqyKpmVpHdy0Yw9XgACJqtqoRcB3SuNknz",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/807
https://github.com/google/deepvariant/issues/807:1115,safety,test,test,1115,"issue:** Am getting the error as ""Fatal Python error: Segmentation fault"". > . > **Setup**. > . > * Operating system: Ubuntu 22.04.2 LTS. > * DeepVariant version: 1.6.1. > * Installation method (Docker, built from source, etc.): Docker. > * Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) Its a Pabcio CLR data. Read Input is provided in Fastq format and reference in FASTA format. > . > **Steps to reproduce:**. > . > * Command: sudo docker run . > -v ""${INPUT_DIR}"":""/input"" . > -v ""${OUTPUT_DIR}"":""/output"" . > google/deepvariant:""${BIN_VERSION}"" . > /opt/deepvariant/bin/run_deepvariant . > --model_type=PACBIO . > --ref=/input/RILWLs1.fasta . > --reads=/input/Out.fastq . > --output_vcf=/output/output.vcf.gz . > --output_gvcf=/output/output.g.vcf.gz . > --intermediate_results_dir /output/intermediate_results_dir . > --num_shards=15. > * Error trace: (if applicable). > . > **Does the quick start test work on your system?** Please test with https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md. Yes. Test data works fine. ![Screenshot from 2024-04-17 12-24-22](https://private-user-images.githubusercontent.com/68117296/323111309-41ac66ff-ff52-493f-b18f-f017921caa86.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTMzMzcyOTIsIm5iZiI6MTcxMzMzNjk5MiwicGF0aCI6Ii82ODExNzI5Ni8zMjMxMTEzMDktNDFhYzY2ZmYtZmY1Mi00OTNmLWIxOGYtZjAxNzkyMWNhYTg2LnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDA0MTclMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQwNDE3VDA2NTYzMlomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTg3ZDQ3ZTBmNDFjYWQ4YWQyNmM4MDdmYTJiYjVjNzlhYmI1MDA2NzQxOGY3MjA1ZjU1ODY3ZDUzOTcyMTkyNzQmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.gtqyKpmVpHdy0Yw9XgACJqtqoRcB3SuNknzCYOE8y-g). > . > Is there any way t",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/807
https://github.com/google/deepvariant/issues/807:1211,safety,Test,Test,1211,"> * Operating system: Ubuntu 22.04.2 LTS. > * DeepVariant version: 1.6.1. > * Installation method (Docker, built from source, etc.): Docker. > * Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) Its a Pabcio CLR data. Read Input is provided in Fastq format and reference in FASTA format. > . > **Steps to reproduce:**. > . > * Command: sudo docker run . > -v ""${INPUT_DIR}"":""/input"" . > -v ""${OUTPUT_DIR}"":""/output"" . > google/deepvariant:""${BIN_VERSION}"" . > /opt/deepvariant/bin/run_deepvariant . > --model_type=PACBIO . > --ref=/input/RILWLs1.fasta . > --reads=/input/Out.fastq . > --output_vcf=/output/output.vcf.gz . > --output_gvcf=/output/output.g.vcf.gz . > --intermediate_results_dir /output/intermediate_results_dir . > --num_shards=15. > * Error trace: (if applicable). > . > **Does the quick start test work on your system?** Please test with https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md. Yes. Test data works fine. ![Screenshot from 2024-04-17 12-24-22](https://private-user-images.githubusercontent.com/68117296/323111309-41ac66ff-ff52-493f-b18f-f017921caa86.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTMzMzcyOTIsIm5iZiI6MTcxMzMzNjk5MiwicGF0aCI6Ii82ODExNzI5Ni8zMjMxMTEzMDktNDFhYzY2ZmYtZmY1Mi00OTNmLWIxOGYtZjAxNzkyMWNhYTg2LnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDA0MTclMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQwNDE3VDA2NTYzMlomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTg3ZDQ3ZTBmNDFjYWQ4YWQyNmM4MDdmYTJiYjVjNzlhYmI1MDA2NzQxOGY3MjA1ZjU1ODY3ZDUzOTcyMTkyNzQmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.gtqyKpmVpHdy0Yw9XgACJqtqoRcB3SuNknzCYOE8y-g). > . > Is there any way to reproduce the issue by using the quick start? > . > **Any additional context:**. Its a 256GB R",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/807
https://github.com/google/deepvariant/issues/807:1382,security,jwt,jwt,1382,"ing system: Ubuntu 22.04.2 LTS. > * DeepVariant version: 1.6.1. > * Installation method (Docker, built from source, etc.): Docker. > * Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) Its a Pabcio CLR data. Read Input is provided in Fastq format and reference in FASTA format. > . > **Steps to reproduce:**. > . > * Command: sudo docker run . > -v ""${INPUT_DIR}"":""/input"" . > -v ""${OUTPUT_DIR}"":""/output"" . > google/deepvariant:""${BIN_VERSION}"" . > /opt/deepvariant/bin/run_deepvariant . > --model_type=PACBIO . > --ref=/input/RILWLs1.fasta . > --reads=/input/Out.fastq . > --output_vcf=/output/output.vcf.gz . > --output_gvcf=/output/output.g.vcf.gz . > --intermediate_results_dir /output/intermediate_results_dir . > --num_shards=15. > * Error trace: (if applicable). > . > **Does the quick start test work on your system?** Please test with https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md. Yes. Test data works fine. ![Screenshot from 2024-04-17 12-24-22](https://private-user-images.githubusercontent.com/68117296/323111309-41ac66ff-ff52-493f-b18f-f017921caa86.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTMzMzcyOTIsIm5iZiI6MTcxMzMzNjk5MiwicGF0aCI6Ii82ODExNzI5Ni8zMjMxMTEzMDktNDFhYzY2ZmYtZmY1Mi00OTNmLWIxOGYtZjAxNzkyMWNhYTg2LnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDA0MTclMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQwNDE3VDA2NTYzMlomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTg3ZDQ3ZTBmNDFjYWQ4YWQyNmM4MDdmYTJiYjVjNzlhYmI1MDA2NzQxOGY3MjA1ZjU1ODY3ZDUzOTcyMTkyNzQmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.gtqyKpmVpHdy0Yw9XgACJqtqoRcB3SuNknzCYOE8y-g). > . > Is there any way to reproduce the issue by using the quick start? > . > **Any additional context:**. Its a 256GB RAM system.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/807
https://github.com/google/deepvariant/issues/807:384,testability,instrument,instrument,384,"> **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6.1/docs/FAQ.md**:. > . > **Describe the issue:** Am getting the error as ""Fatal Python error: Segmentation fault"". > . > **Setup**. > . > * Operating system: Ubuntu 22.04.2 LTS. > * DeepVariant version: 1.6.1. > * Installation method (Docker, built from source, etc.): Docker. > * Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) Its a Pabcio CLR data. Read Input is provided in Fastq format and reference in FASTA format. > . > **Steps to reproduce:**. > . > * Command: sudo docker run . > -v ""${INPUT_DIR}"":""/input"" . > -v ""${OUTPUT_DIR}"":""/output"" . > google/deepvariant:""${BIN_VERSION}"" . > /opt/deepvariant/bin/run_deepvariant . > --model_type=PACBIO . > --ref=/input/RILWLs1.fasta . > --reads=/input/Out.fastq . > --output_vcf=/output/output.vcf.gz . > --output_gvcf=/output/output.g.vcf.gz . > --intermediate_results_dir /output/intermediate_results_dir . > --num_shards=15. > * Error trace: (if applicable). > . > **Does the quick start test work on your system?** Please test with https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md. Yes. Test data works fine. ![Screenshot from 2024-04-17 12-24-22](https://private-user-images.githubusercontent.com/68117296/323111309-41ac66ff-ff52-493f-b18f-f017921caa86.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTMzMzcyOTIsIm5iZiI6MTcxMzMzNjk5MiwicGF0aCI6Ii82ODExNzI5Ni8zMjMxMTEzMDktNDFhYzY2ZmYtZmY1Mi00OTNmLWIxOGYtZjAxNzkyMWNhYTg2LnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDA0MTclMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQwNDE3VDA2NTYzMlomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTg3ZDQ3ZTBmNDFjYWQ4YWQyNmM4MDdmYTJiYjVjNzlhYmI1MDA2NzQxOGY3MjA1ZjU1ODY3ZDUzOTcyMTkyNzQmWC1BbXotU2lnbmVkSGVhZGVycz1ob",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/807
https://github.com/google/deepvariant/issues/807:1027,testability,trace,trace,1027,"https://github.com/google/deepvariant/blob/r1.6.1/docs/FAQ.md**:. > . > **Describe the issue:** Am getting the error as ""Fatal Python error: Segmentation fault"". > . > **Setup**. > . > * Operating system: Ubuntu 22.04.2 LTS. > * DeepVariant version: 1.6.1. > * Installation method (Docker, built from source, etc.): Docker. > * Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) Its a Pabcio CLR data. Read Input is provided in Fastq format and reference in FASTA format. > . > **Steps to reproduce:**. > . > * Command: sudo docker run . > -v ""${INPUT_DIR}"":""/input"" . > -v ""${OUTPUT_DIR}"":""/output"" . > google/deepvariant:""${BIN_VERSION}"" . > /opt/deepvariant/bin/run_deepvariant . > --model_type=PACBIO . > --ref=/input/RILWLs1.fasta . > --reads=/input/Out.fastq . > --output_vcf=/output/output.vcf.gz . > --output_gvcf=/output/output.g.vcf.gz . > --intermediate_results_dir /output/intermediate_results_dir . > --num_shards=15. > * Error trace: (if applicable). > . > **Does the quick start test work on your system?** Please test with https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md. Yes. Test data works fine. ![Screenshot from 2024-04-17 12-24-22](https://private-user-images.githubusercontent.com/68117296/323111309-41ac66ff-ff52-493f-b18f-f017921caa86.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTMzMzcyOTIsIm5iZiI6MTcxMzMzNjk5MiwicGF0aCI6Ii82ODExNzI5Ni8zMjMxMTEzMDktNDFhYzY2ZmYtZmY1Mi00OTNmLWIxOGYtZjAxNzkyMWNhYTg2LnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDA0MTclMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQwNDE3VDA2NTYzMlomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTg3ZDQ3ZTBmNDFjYWQ4YWQyNmM4MDdmYTJiYjVjNzlhYmI1MDA2NzQxOGY3MjA1ZjU1ODY3ZDUzOTcyMTkyNzQmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTA",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/807
https://github.com/google/deepvariant/issues/807:1080,testability,test,test,1080,"cs/FAQ.md**:. > . > **Describe the issue:** Am getting the error as ""Fatal Python error: Segmentation fault"". > . > **Setup**. > . > * Operating system: Ubuntu 22.04.2 LTS. > * DeepVariant version: 1.6.1. > * Installation method (Docker, built from source, etc.): Docker. > * Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) Its a Pabcio CLR data. Read Input is provided in Fastq format and reference in FASTA format. > . > **Steps to reproduce:**. > . > * Command: sudo docker run . > -v ""${INPUT_DIR}"":""/input"" . > -v ""${OUTPUT_DIR}"":""/output"" . > google/deepvariant:""${BIN_VERSION}"" . > /opt/deepvariant/bin/run_deepvariant . > --model_type=PACBIO . > --ref=/input/RILWLs1.fasta . > --reads=/input/Out.fastq . > --output_vcf=/output/output.vcf.gz . > --output_gvcf=/output/output.g.vcf.gz . > --intermediate_results_dir /output/intermediate_results_dir . > --num_shards=15. > * Error trace: (if applicable). > . > **Does the quick start test work on your system?** Please test with https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md. Yes. Test data works fine. ![Screenshot from 2024-04-17 12-24-22](https://private-user-images.githubusercontent.com/68117296/323111309-41ac66ff-ff52-493f-b18f-f017921caa86.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTMzMzcyOTIsIm5iZiI6MTcxMzMzNjk5MiwicGF0aCI6Ii82ODExNzI5Ni8zMjMxMTEzMDktNDFhYzY2ZmYtZmY1Mi00OTNmLWIxOGYtZjAxNzkyMWNhYTg2LnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDA0MTclMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQwNDE3VDA2NTYzMlomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTg3ZDQ3ZTBmNDFjYWQ4YWQyNmM4MDdmYTJiYjVjNzlhYmI1MDA2NzQxOGY3MjA1ZjU1ODY3ZDUzOTcyMTkyNzQmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.gtqyKpmVpHdy0Yw9XgACJqtqoRcB3SuNknz",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/807
https://github.com/google/deepvariant/issues/807:1115,testability,test,test,1115,"issue:** Am getting the error as ""Fatal Python error: Segmentation fault"". > . > **Setup**. > . > * Operating system: Ubuntu 22.04.2 LTS. > * DeepVariant version: 1.6.1. > * Installation method (Docker, built from source, etc.): Docker. > * Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) Its a Pabcio CLR data. Read Input is provided in Fastq format and reference in FASTA format. > . > **Steps to reproduce:**. > . > * Command: sudo docker run . > -v ""${INPUT_DIR}"":""/input"" . > -v ""${OUTPUT_DIR}"":""/output"" . > google/deepvariant:""${BIN_VERSION}"" . > /opt/deepvariant/bin/run_deepvariant . > --model_type=PACBIO . > --ref=/input/RILWLs1.fasta . > --reads=/input/Out.fastq . > --output_vcf=/output/output.vcf.gz . > --output_gvcf=/output/output.g.vcf.gz . > --intermediate_results_dir /output/intermediate_results_dir . > --num_shards=15. > * Error trace: (if applicable). > . > **Does the quick start test work on your system?** Please test with https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md. Yes. Test data works fine. ![Screenshot from 2024-04-17 12-24-22](https://private-user-images.githubusercontent.com/68117296/323111309-41ac66ff-ff52-493f-b18f-f017921caa86.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTMzMzcyOTIsIm5iZiI6MTcxMzMzNjk5MiwicGF0aCI6Ii82ODExNzI5Ni8zMjMxMTEzMDktNDFhYzY2ZmYtZmY1Mi00OTNmLWIxOGYtZjAxNzkyMWNhYTg2LnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDA0MTclMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQwNDE3VDA2NTYzMlomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTg3ZDQ3ZTBmNDFjYWQ4YWQyNmM4MDdmYTJiYjVjNzlhYmI1MDA2NzQxOGY3MjA1ZjU1ODY3ZDUzOTcyMTkyNzQmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.gtqyKpmVpHdy0Yw9XgACJqtqoRcB3SuNknzCYOE8y-g). > . > Is there any way t",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/807
https://github.com/google/deepvariant/issues/807:1211,testability,Test,Test,1211,"> * Operating system: Ubuntu 22.04.2 LTS. > * DeepVariant version: 1.6.1. > * Installation method (Docker, built from source, etc.): Docker. > * Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) Its a Pabcio CLR data. Read Input is provided in Fastq format and reference in FASTA format. > . > **Steps to reproduce:**. > . > * Command: sudo docker run . > -v ""${INPUT_DIR}"":""/input"" . > -v ""${OUTPUT_DIR}"":""/output"" . > google/deepvariant:""${BIN_VERSION}"" . > /opt/deepvariant/bin/run_deepvariant . > --model_type=PACBIO . > --ref=/input/RILWLs1.fasta . > --reads=/input/Out.fastq . > --output_vcf=/output/output.vcf.gz . > --output_gvcf=/output/output.g.vcf.gz . > --intermediate_results_dir /output/intermediate_results_dir . > --num_shards=15. > * Error trace: (if applicable). > . > **Does the quick start test work on your system?** Please test with https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md. Yes. Test data works fine. ![Screenshot from 2024-04-17 12-24-22](https://private-user-images.githubusercontent.com/68117296/323111309-41ac66ff-ff52-493f-b18f-f017921caa86.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTMzMzcyOTIsIm5iZiI6MTcxMzMzNjk5MiwicGF0aCI6Ii82ODExNzI5Ni8zMjMxMTEzMDktNDFhYzY2ZmYtZmY1Mi00OTNmLWIxOGYtZjAxNzkyMWNhYTg2LnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDA0MTclMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQwNDE3VDA2NTYzMlomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTg3ZDQ3ZTBmNDFjYWQ4YWQyNmM4MDdmYTJiYjVjNzlhYmI1MDA2NzQxOGY3MjA1ZjU1ODY3ZDUzOTcyMTkyNzQmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.gtqyKpmVpHdy0Yw9XgACJqtqoRcB3SuNknzCYOE8y-g). > . > Is there any way to reproduce the issue by using the quick start? > . > **Any additional context:**. Its a 256GB R",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/807
https://github.com/google/deepvariant/issues/807:2188,testability,context,context,2188,"ing system: Ubuntu 22.04.2 LTS. > * DeepVariant version: 1.6.1. > * Installation method (Docker, built from source, etc.): Docker. > * Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) Its a Pabcio CLR data. Read Input is provided in Fastq format and reference in FASTA format. > . > **Steps to reproduce:**. > . > * Command: sudo docker run . > -v ""${INPUT_DIR}"":""/input"" . > -v ""${OUTPUT_DIR}"":""/output"" . > google/deepvariant:""${BIN_VERSION}"" . > /opt/deepvariant/bin/run_deepvariant . > --model_type=PACBIO . > --ref=/input/RILWLs1.fasta . > --reads=/input/Out.fastq . > --output_vcf=/output/output.vcf.gz . > --output_gvcf=/output/output.g.vcf.gz . > --intermediate_results_dir /output/intermediate_results_dir . > --num_shards=15. > * Error trace: (if applicable). > . > **Does the quick start test work on your system?** Please test with https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md. Yes. Test data works fine. ![Screenshot from 2024-04-17 12-24-22](https://private-user-images.githubusercontent.com/68117296/323111309-41ac66ff-ff52-493f-b18f-f017921caa86.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTMzMzcyOTIsIm5iZiI6MTcxMzMzNjk5MiwicGF0aCI6Ii82ODExNzI5Ni8zMjMxMTEzMDktNDFhYzY2ZmYtZmY1Mi00OTNmLWIxOGYtZjAxNzkyMWNhYTg2LnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDA0MTclMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQwNDE3VDA2NTYzMlomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTg3ZDQ3ZTBmNDFjYWQ4YWQyNmM4MDdmYTJiYjVjNzlhYmI1MDA2NzQxOGY3MjA1ZjU1ODY3ZDUzOTcyMTkyNzQmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.gtqyKpmVpHdy0Yw9XgACJqtqoRcB3SuNknzCYOE8y-g). > . > Is there any way to reproduce the issue by using the quick start? > . > **Any additional context:**. Its a 256GB RAM system.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/807
https://github.com/google/deepvariant/issues/807:141,usability,error,error,141,"> **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6.1/docs/FAQ.md**:. > . > **Describe the issue:** Am getting the error as ""Fatal Python error: Segmentation fault"". > . > **Setup**. > . > * Operating system: Ubuntu 22.04.2 LTS. > * DeepVariant version: 1.6.1. > * Installation method (Docker, built from source, etc.): Docker. > * Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) Its a Pabcio CLR data. Read Input is provided in Fastq format and reference in FASTA format. > . > **Steps to reproduce:**. > . > * Command: sudo docker run . > -v ""${INPUT_DIR}"":""/input"" . > -v ""${OUTPUT_DIR}"":""/output"" . > google/deepvariant:""${BIN_VERSION}"" . > /opt/deepvariant/bin/run_deepvariant . > --model_type=PACBIO . > --ref=/input/RILWLs1.fasta . > --reads=/input/Out.fastq . > --output_vcf=/output/output.vcf.gz . > --output_gvcf=/output/output.g.vcf.gz . > --intermediate_results_dir /output/intermediate_results_dir . > --num_shards=15. > * Error trace: (if applicable). > . > **Does the quick start test work on your system?** Please test with https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md. Yes. Test data works fine. ![Screenshot from 2024-04-17 12-24-22](https://private-user-images.githubusercontent.com/68117296/323111309-41ac66ff-ff52-493f-b18f-f017921caa86.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTMzMzcyOTIsIm5iZiI6MTcxMzMzNjk5MiwicGF0aCI6Ii82ODExNzI5Ni8zMjMxMTEzMDktNDFhYzY2ZmYtZmY1Mi00OTNmLWIxOGYtZjAxNzkyMWNhYTg2LnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDA0MTclMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQwNDE3VDA2NTYzMlomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTg3ZDQ3ZTBmNDFjYWQ4YWQyNmM4MDdmYTJiYjVjNzlhYmI1MDA2NzQxOGY3MjA1ZjU1ODY3ZDUzOTcyMTkyNzQmWC1BbXotU2lnbmVkSGVhZGVycz1ob",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/807
https://github.com/google/deepvariant/issues/807:164,usability,error,error,164,"> **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6.1/docs/FAQ.md**:. > . > **Describe the issue:** Am getting the error as ""Fatal Python error: Segmentation fault"". > . > **Setup**. > . > * Operating system: Ubuntu 22.04.2 LTS. > * DeepVariant version: 1.6.1. > * Installation method (Docker, built from source, etc.): Docker. > * Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) Its a Pabcio CLR data. Read Input is provided in Fastq format and reference in FASTA format. > . > **Steps to reproduce:**. > . > * Command: sudo docker run . > -v ""${INPUT_DIR}"":""/input"" . > -v ""${OUTPUT_DIR}"":""/output"" . > google/deepvariant:""${BIN_VERSION}"" . > /opt/deepvariant/bin/run_deepvariant . > --model_type=PACBIO . > --ref=/input/RILWLs1.fasta . > --reads=/input/Out.fastq . > --output_vcf=/output/output.vcf.gz . > --output_gvcf=/output/output.g.vcf.gz . > --intermediate_results_dir /output/intermediate_results_dir . > --num_shards=15. > * Error trace: (if applicable). > . > **Does the quick start test work on your system?** Please test with https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md. Yes. Test data works fine. ![Screenshot from 2024-04-17 12-24-22](https://private-user-images.githubusercontent.com/68117296/323111309-41ac66ff-ff52-493f-b18f-f017921caa86.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTMzMzcyOTIsIm5iZiI6MTcxMzMzNjk5MiwicGF0aCI6Ii82ODExNzI5Ni8zMjMxMTEzMDktNDFhYzY2ZmYtZmY1Mi00OTNmLWIxOGYtZjAxNzkyMWNhYTg2LnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDA0MTclMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQwNDE3VDA2NTYzMlomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTg3ZDQ3ZTBmNDFjYWQ4YWQyNmM4MDdmYTJiYjVjNzlhYmI1MDA2NzQxOGY3MjA1ZjU1ODY3ZDUzOTcyMTkyNzQmWC1BbXotU2lnbmVkSGVhZGVycz1ob",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/807
https://github.com/google/deepvariant/issues/807:493,usability,Input,Input,493,"> **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6.1/docs/FAQ.md**:. > . > **Describe the issue:** Am getting the error as ""Fatal Python error: Segmentation fault"". > . > **Setup**. > . > * Operating system: Ubuntu 22.04.2 LTS. > * DeepVariant version: 1.6.1. > * Installation method (Docker, built from source, etc.): Docker. > * Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) Its a Pabcio CLR data. Read Input is provided in Fastq format and reference in FASTA format. > . > **Steps to reproduce:**. > . > * Command: sudo docker run . > -v ""${INPUT_DIR}"":""/input"" . > -v ""${OUTPUT_DIR}"":""/output"" . > google/deepvariant:""${BIN_VERSION}"" . > /opt/deepvariant/bin/run_deepvariant . > --model_type=PACBIO . > --ref=/input/RILWLs1.fasta . > --reads=/input/Out.fastq . > --output_vcf=/output/output.vcf.gz . > --output_gvcf=/output/output.g.vcf.gz . > --intermediate_results_dir /output/intermediate_results_dir . > --num_shards=15. > * Error trace: (if applicable). > . > **Does the quick start test work on your system?** Please test with https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md. Yes. Test data works fine. ![Screenshot from 2024-04-17 12-24-22](https://private-user-images.githubusercontent.com/68117296/323111309-41ac66ff-ff52-493f-b18f-f017921caa86.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTMzMzcyOTIsIm5iZiI6MTcxMzMzNjk5MiwicGF0aCI6Ii82ODExNzI5Ni8zMjMxMTEzMDktNDFhYzY2ZmYtZmY1Mi00OTNmLWIxOGYtZjAxNzkyMWNhYTg2LnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDA0MTclMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQwNDE3VDA2NTYzMlomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTg3ZDQ3ZTBmNDFjYWQ4YWQyNmM4MDdmYTJiYjVjNzlhYmI1MDA2NzQxOGY3MjA1ZjU1ODY3ZDUzOTcyMTkyNzQmWC1BbXotU2lnbmVkSGVhZGVycz1ob",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/807
https://github.com/google/deepvariant/issues/807:597,usability,Command,Command,597,"> **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6.1/docs/FAQ.md**:. > . > **Describe the issue:** Am getting the error as ""Fatal Python error: Segmentation fault"". > . > **Setup**. > . > * Operating system: Ubuntu 22.04.2 LTS. > * DeepVariant version: 1.6.1. > * Installation method (Docker, built from source, etc.): Docker. > * Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) Its a Pabcio CLR data. Read Input is provided in Fastq format and reference in FASTA format. > . > **Steps to reproduce:**. > . > * Command: sudo docker run . > -v ""${INPUT_DIR}"":""/input"" . > -v ""${OUTPUT_DIR}"":""/output"" . > google/deepvariant:""${BIN_VERSION}"" . > /opt/deepvariant/bin/run_deepvariant . > --model_type=PACBIO . > --ref=/input/RILWLs1.fasta . > --reads=/input/Out.fastq . > --output_vcf=/output/output.vcf.gz . > --output_gvcf=/output/output.g.vcf.gz . > --intermediate_results_dir /output/intermediate_results_dir . > --num_shards=15. > * Error trace: (if applicable). > . > **Does the quick start test work on your system?** Please test with https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md. Yes. Test data works fine. ![Screenshot from 2024-04-17 12-24-22](https://private-user-images.githubusercontent.com/68117296/323111309-41ac66ff-ff52-493f-b18f-f017921caa86.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTMzMzcyOTIsIm5iZiI6MTcxMzMzNjk5MiwicGF0aCI6Ii82ODExNzI5Ni8zMjMxMTEzMDktNDFhYzY2ZmYtZmY1Mi00OTNmLWIxOGYtZjAxNzkyMWNhYTg2LnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDA0MTclMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQwNDE3VDA2NTYzMlomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTg3ZDQ3ZTBmNDFjYWQ4YWQyNmM4MDdmYTJiYjVjNzlhYmI1MDA2NzQxOGY3MjA1ZjU1ODY3ZDUzOTcyMTkyNzQmWC1BbXotU2lnbmVkSGVhZGVycz1ob",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/807
https://github.com/google/deepvariant/issues/807:646,usability,input,input,646,"> **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6.1/docs/FAQ.md**:. > . > **Describe the issue:** Am getting the error as ""Fatal Python error: Segmentation fault"". > . > **Setup**. > . > * Operating system: Ubuntu 22.04.2 LTS. > * DeepVariant version: 1.6.1. > * Installation method (Docker, built from source, etc.): Docker. > * Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) Its a Pabcio CLR data. Read Input is provided in Fastq format and reference in FASTA format. > . > **Steps to reproduce:**. > . > * Command: sudo docker run . > -v ""${INPUT_DIR}"":""/input"" . > -v ""${OUTPUT_DIR}"":""/output"" . > google/deepvariant:""${BIN_VERSION}"" . > /opt/deepvariant/bin/run_deepvariant . > --model_type=PACBIO . > --ref=/input/RILWLs1.fasta . > --reads=/input/Out.fastq . > --output_vcf=/output/output.vcf.gz . > --output_gvcf=/output/output.g.vcf.gz . > --intermediate_results_dir /output/intermediate_results_dir . > --num_shards=15. > * Error trace: (if applicable). > . > **Does the quick start test work on your system?** Please test with https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md. Yes. Test data works fine. ![Screenshot from 2024-04-17 12-24-22](https://private-user-images.githubusercontent.com/68117296/323111309-41ac66ff-ff52-493f-b18f-f017921caa86.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTMzMzcyOTIsIm5iZiI6MTcxMzMzNjk5MiwicGF0aCI6Ii82ODExNzI5Ni8zMjMxMTEzMDktNDFhYzY2ZmYtZmY1Mi00OTNmLWIxOGYtZjAxNzkyMWNhYTg2LnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDA0MTclMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQwNDE3VDA2NTYzMlomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTg3ZDQ3ZTBmNDFjYWQ4YWQyNmM4MDdmYTJiYjVjNzlhYmI1MDA2NzQxOGY3MjA1ZjU1ODY3ZDUzOTcyMTkyNzQmWC1BbXotU2lnbmVkSGVhZGVycz1ob",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/807
https://github.com/google/deepvariant/issues/807:802,usability,input,input,802,"> **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6.1/docs/FAQ.md**:. > . > **Describe the issue:** Am getting the error as ""Fatal Python error: Segmentation fault"". > . > **Setup**. > . > * Operating system: Ubuntu 22.04.2 LTS. > * DeepVariant version: 1.6.1. > * Installation method (Docker, built from source, etc.): Docker. > * Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) Its a Pabcio CLR data. Read Input is provided in Fastq format and reference in FASTA format. > . > **Steps to reproduce:**. > . > * Command: sudo docker run . > -v ""${INPUT_DIR}"":""/input"" . > -v ""${OUTPUT_DIR}"":""/output"" . > google/deepvariant:""${BIN_VERSION}"" . > /opt/deepvariant/bin/run_deepvariant . > --model_type=PACBIO . > --ref=/input/RILWLs1.fasta . > --reads=/input/Out.fastq . > --output_vcf=/output/output.vcf.gz . > --output_gvcf=/output/output.g.vcf.gz . > --intermediate_results_dir /output/intermediate_results_dir . > --num_shards=15. > * Error trace: (if applicable). > . > **Does the quick start test work on your system?** Please test with https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md. Yes. Test data works fine. ![Screenshot from 2024-04-17 12-24-22](https://private-user-images.githubusercontent.com/68117296/323111309-41ac66ff-ff52-493f-b18f-f017921caa86.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTMzMzcyOTIsIm5iZiI6MTcxMzMzNjk5MiwicGF0aCI6Ii82ODExNzI5Ni8zMjMxMTEzMDktNDFhYzY2ZmYtZmY1Mi00OTNmLWIxOGYtZjAxNzkyMWNhYTg2LnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDA0MTclMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQwNDE3VDA2NTYzMlomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTg3ZDQ3ZTBmNDFjYWQ4YWQyNmM4MDdmYTJiYjVjNzlhYmI1MDA2NzQxOGY3MjA1ZjU1ODY3ZDUzOTcyMTkyNzQmWC1BbXotU2lnbmVkSGVhZGVycz1ob",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/807
https://github.com/google/deepvariant/issues/807:835,usability,input,input,835,"> **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6.1/docs/FAQ.md**:. > . > **Describe the issue:** Am getting the error as ""Fatal Python error: Segmentation fault"". > . > **Setup**. > . > * Operating system: Ubuntu 22.04.2 LTS. > * DeepVariant version: 1.6.1. > * Installation method (Docker, built from source, etc.): Docker. > * Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) Its a Pabcio CLR data. Read Input is provided in Fastq format and reference in FASTA format. > . > **Steps to reproduce:**. > . > * Command: sudo docker run . > -v ""${INPUT_DIR}"":""/input"" . > -v ""${OUTPUT_DIR}"":""/output"" . > google/deepvariant:""${BIN_VERSION}"" . > /opt/deepvariant/bin/run_deepvariant . > --model_type=PACBIO . > --ref=/input/RILWLs1.fasta . > --reads=/input/Out.fastq . > --output_vcf=/output/output.vcf.gz . > --output_gvcf=/output/output.g.vcf.gz . > --intermediate_results_dir /output/intermediate_results_dir . > --num_shards=15. > * Error trace: (if applicable). > . > **Does the quick start test work on your system?** Please test with https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md. Yes. Test data works fine. ![Screenshot from 2024-04-17 12-24-22](https://private-user-images.githubusercontent.com/68117296/323111309-41ac66ff-ff52-493f-b18f-f017921caa86.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTMzMzcyOTIsIm5iZiI6MTcxMzMzNjk5MiwicGF0aCI6Ii82ODExNzI5Ni8zMjMxMTEzMDktNDFhYzY2ZmYtZmY1Mi00OTNmLWIxOGYtZjAxNzkyMWNhYTg2LnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDA0MTclMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQwNDE3VDA2NTYzMlomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTg3ZDQ3ZTBmNDFjYWQ4YWQyNmM4MDdmYTJiYjVjNzlhYmI1MDA2NzQxOGY3MjA1ZjU1ODY3ZDUzOTcyMTkyNzQmWC1BbXotU2lnbmVkSGVhZGVycz1ob",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/807
https://github.com/google/deepvariant/issues/807:1021,usability,Error,Error,1021," FAQ? https://github.com/google/deepvariant/blob/r1.6.1/docs/FAQ.md**:. > . > **Describe the issue:** Am getting the error as ""Fatal Python error: Segmentation fault"". > . > **Setup**. > . > * Operating system: Ubuntu 22.04.2 LTS. > * DeepVariant version: 1.6.1. > * Installation method (Docker, built from source, etc.): Docker. > * Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) Its a Pabcio CLR data. Read Input is provided in Fastq format and reference in FASTA format. > . > **Steps to reproduce:**. > . > * Command: sudo docker run . > -v ""${INPUT_DIR}"":""/input"" . > -v ""${OUTPUT_DIR}"":""/output"" . > google/deepvariant:""${BIN_VERSION}"" . > /opt/deepvariant/bin/run_deepvariant . > --model_type=PACBIO . > --ref=/input/RILWLs1.fasta . > --reads=/input/Out.fastq . > --output_vcf=/output/output.vcf.gz . > --output_gvcf=/output/output.g.vcf.gz . > --intermediate_results_dir /output/intermediate_results_dir . > --num_shards=15. > * Error trace: (if applicable). > . > **Does the quick start test work on your system?** Please test with https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md. Yes. Test data works fine. ![Screenshot from 2024-04-17 12-24-22](https://private-user-images.githubusercontent.com/68117296/323111309-41ac66ff-ff52-493f-b18f-f017921caa86.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTMzMzcyOTIsIm5iZiI6MTcxMzMzNjk5MiwicGF0aCI6Ii82ODExNzI5Ni8zMjMxMTEzMDktNDFhYzY2ZmYtZmY1Mi00OTNmLWIxOGYtZjAxNzkyMWNhYTg2LnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDA0MTclMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQwNDE3VDA2NTYzMlomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTg3ZDQ3ZTBmNDFjYWQ4YWQyNmM4MDdmYTJiYjVjNzlhYmI1MDA2NzQxOGY3MjA1ZjU1ODY3ZDUzOTcyMTkyNzQmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/807
https://github.com/google/deepvariant/issues/807:1288,usability,user,user-images,1288,"ing system: Ubuntu 22.04.2 LTS. > * DeepVariant version: 1.6.1. > * Installation method (Docker, built from source, etc.): Docker. > * Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) Its a Pabcio CLR data. Read Input is provided in Fastq format and reference in FASTA format. > . > **Steps to reproduce:**. > . > * Command: sudo docker run . > -v ""${INPUT_DIR}"":""/input"" . > -v ""${OUTPUT_DIR}"":""/output"" . > google/deepvariant:""${BIN_VERSION}"" . > /opt/deepvariant/bin/run_deepvariant . > --model_type=PACBIO . > --ref=/input/RILWLs1.fasta . > --reads=/input/Out.fastq . > --output_vcf=/output/output.vcf.gz . > --output_gvcf=/output/output.g.vcf.gz . > --intermediate_results_dir /output/intermediate_results_dir . > --num_shards=15. > * Error trace: (if applicable). > . > **Does the quick start test work on your system?** Please test with https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md. Yes. Test data works fine. ![Screenshot from 2024-04-17 12-24-22](https://private-user-images.githubusercontent.com/68117296/323111309-41ac66ff-ff52-493f-b18f-f017921caa86.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTMzMzcyOTIsIm5iZiI6MTcxMzMzNjk5MiwicGF0aCI6Ii82ODExNzI5Ni8zMjMxMTEzMDktNDFhYzY2ZmYtZmY1Mi00OTNmLWIxOGYtZjAxNzkyMWNhYTg2LnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDA0MTclMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQwNDE3VDA2NTYzMlomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTg3ZDQ3ZTBmNDFjYWQ4YWQyNmM4MDdmYTJiYjVjNzlhYmI1MDA2NzQxOGY3MjA1ZjU1ODY3ZDUzOTcyMTkyNzQmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.gtqyKpmVpHdy0Yw9XgACJqtqoRcB3SuNknzCYOE8y-g). > . > Is there any way to reproduce the issue by using the quick start? > . > **Any additional context:**. Its a 256GB RAM system.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/807
https://github.com/google/deepvariant/issues/807:119,interoperability,share,share,119,"Hi @navishkumarb ,. It seems like you've marked this issue as closed. If there's any findings that would be helpful to share with the forum here, please do. If you have further questions, please feel free to reach out again.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/807
https://github.com/google/deepvariant/issues/807:62,usability,close,closed,62,"Hi @navishkumarb ,. It seems like you've marked this issue as closed. If there's any findings that would be helpful to share with the forum here, please do. If you have further questions, please feel free to reach out again.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/807
https://github.com/google/deepvariant/issues/807:108,usability,help,helpful,108,"Hi @navishkumarb ,. It seems like you've marked this issue as closed. If there's any findings that would be helpful to share with the forum here, please do. If you have further questions, please feel free to reach out again.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/807
https://github.com/google/deepvariant/issues/807:123,availability,error,error,123,"Hi . Hi @pichuan . Input I have provided was a raw FASTQ read files instead of aligned BAM file. . So this was causing the error and it worked fine post that. . Thanks. > Hi @navishkumarb , It seems like you've marked this issue as closed. If there's any findings that would be helpful to share with the forum here, please do. > . > If you have further questions, please feel free to reach out again.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/807
https://github.com/google/deepvariant/issues/807:289,interoperability,share,share,289,"Hi . Hi @pichuan . Input I have provided was a raw FASTQ read files instead of aligned BAM file. . So this was causing the error and it worked fine post that. . Thanks. > Hi @navishkumarb , It seems like you've marked this issue as closed. If there's any findings that would be helpful to share with the forum here, please do. > . > If you have further questions, please feel free to reach out again.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/807
https://github.com/google/deepvariant/issues/807:123,performance,error,error,123,"Hi . Hi @pichuan . Input I have provided was a raw FASTQ read files instead of aligned BAM file. . So this was causing the error and it worked fine post that. . Thanks. > Hi @navishkumarb , It seems like you've marked this issue as closed. If there's any findings that would be helpful to share with the forum here, please do. > . > If you have further questions, please feel free to reach out again.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/807
https://github.com/google/deepvariant/issues/807:19,safety,Input,Input,19,"Hi . Hi @pichuan . Input I have provided was a raw FASTQ read files instead of aligned BAM file. . So this was causing the error and it worked fine post that. . Thanks. > Hi @navishkumarb , It seems like you've marked this issue as closed. If there's any findings that would be helpful to share with the forum here, please do. > . > If you have further questions, please feel free to reach out again.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/807
https://github.com/google/deepvariant/issues/807:123,safety,error,error,123,"Hi . Hi @pichuan . Input I have provided was a raw FASTQ read files instead of aligned BAM file. . So this was causing the error and it worked fine post that. . Thanks. > Hi @navishkumarb , It seems like you've marked this issue as closed. If there's any findings that would be helpful to share with the forum here, please do. > . > If you have further questions, please feel free to reach out again.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/807
https://github.com/google/deepvariant/issues/807:19,usability,Input,Input,19,"Hi . Hi @pichuan . Input I have provided was a raw FASTQ read files instead of aligned BAM file. . So this was causing the error and it worked fine post that. . Thanks. > Hi @navishkumarb , It seems like you've marked this issue as closed. If there's any findings that would be helpful to share with the forum here, please do. > . > If you have further questions, please feel free to reach out again.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/807
https://github.com/google/deepvariant/issues/807:123,usability,error,error,123,"Hi . Hi @pichuan . Input I have provided was a raw FASTQ read files instead of aligned BAM file. . So this was causing the error and it worked fine post that. . Thanks. > Hi @navishkumarb , It seems like you've marked this issue as closed. If there's any findings that would be helpful to share with the forum here, please do. > . > If you have further questions, please feel free to reach out again.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/807
https://github.com/google/deepvariant/issues/807:232,usability,close,closed,232,"Hi . Hi @pichuan . Input I have provided was a raw FASTQ read files instead of aligned BAM file. . So this was causing the error and it worked fine post that. . Thanks. > Hi @navishkumarb , It seems like you've marked this issue as closed. If there's any findings that would be helpful to share with the forum here, please do. > . > If you have further questions, please feel free to reach out again.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/807
https://github.com/google/deepvariant/issues/807:278,usability,help,helpful,278,"Hi . Hi @pichuan . Input I have provided was a raw FASTQ read files instead of aligned BAM file. . So this was causing the error and it worked fine post that. . Thanks. > Hi @navishkumarb , It seems like you've marked this issue as closed. If there's any findings that would be helpful to share with the forum here, please do. > . > If you have further questions, please feel free to reach out again.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/807
https://github.com/google/deepvariant/issues/808:556,energy efficiency,adapt,adapt,556,"Hi @sophienguyen01 . You can try https://github.com/google/deepvariant/blob/r1.6.1/deepvariant/labeler/labeled_examples_to_vcf.py. This is more experimental and not officially documented yet. But you can find in our Docker:. https://github.com/google/deepvariant/blob/r1.6.1/Dockerfile#L152. For now, please read the code and use the flags there. As mentioned, this is experimental and we have not officially supported it yet, so I can't say it'll work for the specific use case that you want. But you can at least look at the code and see whether you can adapt it. . Another more typical approach is to run through the rest of the steps (call_variants, postprocess_variants) and get the list of variants that way.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/808
https://github.com/google/deepvariant/issues/808:556,integrability,adapt,adapt,556,"Hi @sophienguyen01 . You can try https://github.com/google/deepvariant/blob/r1.6.1/deepvariant/labeler/labeled_examples_to_vcf.py. This is more experimental and not officially documented yet. But you can find in our Docker:. https://github.com/google/deepvariant/blob/r1.6.1/Dockerfile#L152. For now, please read the code and use the flags there. As mentioned, this is experimental and we have not officially supported it yet, so I can't say it'll work for the specific use case that you want. But you can at least look at the code and see whether you can adapt it. . Another more typical approach is to run through the rest of the steps (call_variants, postprocess_variants) and get the list of variants that way.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/808
https://github.com/google/deepvariant/issues/808:461,interoperability,specif,specific,461,"Hi @sophienguyen01 . You can try https://github.com/google/deepvariant/blob/r1.6.1/deepvariant/labeler/labeled_examples_to_vcf.py. This is more experimental and not officially documented yet. But you can find in our Docker:. https://github.com/google/deepvariant/blob/r1.6.1/Dockerfile#L152. For now, please read the code and use the flags there. As mentioned, this is experimental and we have not officially supported it yet, so I can't say it'll work for the specific use case that you want. But you can at least look at the code and see whether you can adapt it. . Another more typical approach is to run through the rest of the steps (call_variants, postprocess_variants) and get the list of variants that way.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/808
https://github.com/google/deepvariant/issues/808:556,interoperability,adapt,adapt,556,"Hi @sophienguyen01 . You can try https://github.com/google/deepvariant/blob/r1.6.1/deepvariant/labeler/labeled_examples_to_vcf.py. This is more experimental and not officially documented yet. But you can find in our Docker:. https://github.com/google/deepvariant/blob/r1.6.1/Dockerfile#L152. For now, please read the code and use the flags there. As mentioned, this is experimental and we have not officially supported it yet, so I can't say it'll work for the specific use case that you want. But you can at least look at the code and see whether you can adapt it. . Another more typical approach is to run through the rest of the steps (call_variants, postprocess_variants) and get the list of variants that way.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/808
https://github.com/google/deepvariant/issues/808:556,modifiability,adapt,adapt,556,"Hi @sophienguyen01 . You can try https://github.com/google/deepvariant/blob/r1.6.1/deepvariant/labeler/labeled_examples_to_vcf.py. This is more experimental and not officially documented yet. But you can find in our Docker:. https://github.com/google/deepvariant/blob/r1.6.1/Dockerfile#L152. For now, please read the code and use the flags there. As mentioned, this is experimental and we have not officially supported it yet, so I can't say it'll work for the specific use case that you want. But you can at least look at the code and see whether you can adapt it. . Another more typical approach is to run through the rest of the steps (call_variants, postprocess_variants) and get the list of variants that way.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/808
https://github.com/google/deepvariant/issues/808:176,usability,document,documented,176,"Hi @sophienguyen01 . You can try https://github.com/google/deepvariant/blob/r1.6.1/deepvariant/labeler/labeled_examples_to_vcf.py. This is more experimental and not officially documented yet. But you can find in our Docker:. https://github.com/google/deepvariant/blob/r1.6.1/Dockerfile#L152. For now, please read the code and use the flags there. As mentioned, this is experimental and we have not officially supported it yet, so I can't say it'll work for the specific use case that you want. But you can at least look at the code and see whether you can adapt it. . Another more typical approach is to run through the rest of the steps (call_variants, postprocess_variants) and get the list of variants that way.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/808
https://github.com/google/deepvariant/issues/808:409,usability,support,supported,409,"Hi @sophienguyen01 . You can try https://github.com/google/deepvariant/blob/r1.6.1/deepvariant/labeler/labeled_examples_to_vcf.py. This is more experimental and not officially documented yet. But you can find in our Docker:. https://github.com/google/deepvariant/blob/r1.6.1/Dockerfile#L152. For now, please read the code and use the flags there. As mentioned, this is experimental and we have not officially supported it yet, so I can't say it'll work for the specific use case that you want. But you can at least look at the code and see whether you can adapt it. . Another more typical approach is to run through the rest of the steps (call_variants, postprocess_variants) and get the list of variants that way.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/808
https://github.com/google/deepvariant/issues/809:283,availability,error,error,283,"Hi @Wenfei-Xian,. A max MAPQ score of 42 will likely have some effect, but I expect not an enormous one. I suspect that MAPQ at the lower end of the ranges would be more important, since if well-calibrated a difference between PHRED=42 and PHRED=60 is a very low additional absolute error probability. I have some bowtie mapped reads handy for a GIAB sample. I think I can conduct a quick experiment to see if that intuition is right.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/809
https://github.com/google/deepvariant/issues/809:283,performance,error,error,283,"Hi @Wenfei-Xian,. A max MAPQ score of 42 will likely have some effect, but I expect not an enormous one. I suspect that MAPQ at the lower end of the ranges would be more important, since if well-calibrated a difference between PHRED=42 and PHRED=60 is a very low additional absolute error probability. I have some bowtie mapped reads handy for a GIAB sample. I think I can conduct a quick experiment to see if that intuition is right.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/809
https://github.com/google/deepvariant/issues/809:283,safety,error,error,283,"Hi @Wenfei-Xian,. A max MAPQ score of 42 will likely have some effect, but I expect not an enormous one. I suspect that MAPQ at the lower end of the ranges would be more important, since if well-calibrated a difference between PHRED=42 and PHRED=60 is a very low additional absolute error probability. I have some bowtie mapped reads handy for a GIAB sample. I think I can conduct a quick experiment to see if that intuition is right.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/809
https://github.com/google/deepvariant/issues/809:283,usability,error,error,283,"Hi @Wenfei-Xian,. A max MAPQ score of 42 will likely have some effect, but I expect not an enormous one. I suspect that MAPQ at the lower end of the ranges would be more important, since if well-calibrated a difference between PHRED=42 and PHRED=60 is a very low additional absolute error probability. I have some bowtie mapped reads handy for a GIAB sample. I think I can conduct a quick experiment to see if that intuition is right.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/809
https://github.com/google/deepvariant/issues/809:415,usability,intuit,intuition,415,"Hi @Wenfei-Xian,. A max MAPQ score of 42 will likely have some effect, but I expect not an enormous one. I suspect that MAPQ at the lower end of the ranges would be more important, since if well-calibrated a difference between PHRED=42 and PHRED=60 is a very low additional absolute error probability. I have some bowtie mapped reads handy for a GIAB sample. I think I can conduct a quick experiment to see if that intuition is right.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/809
https://github.com/google/deepvariant/issues/809:233,deployability,observ,observed,233,"Hi @Wenfei-Xian . I finished the experiments. There is certainly a noticeable effect from MAPQ limits, more than I expected. For my experiment, I rewrote the BAM file, setting the MAPQ to 60 for any read with MAPQ of 36 or higher (I observed 44 as the highest MAPQ value an more variability to MAPQ values than seen with BWA. |Experiment|SNP Recall|SNP Precision|SNP F1|INDEL Recall|Indel Precision|Indel F1|. |------------|----------|--------------|--------|------------|---------------|--------|. |Default BAM|0.9673|0.9967|0.9817|0.9717|0.9956|0.9835|. |MAPQ 36+ -> 60|0.9758|0.9964|0.9859|0.9829|0.9960|0.9894|. This implies you will get better performance with DeepVariant if you set those higher MAPQ values to 60. Note that in general, DeepVariant hasn't been trained with Bowtie2 data and you'd likely get better performance overall by a re-training for it.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/809
https://github.com/google/deepvariant/issues/809:279,modifiability,variab,variability,279,"Hi @Wenfei-Xian . I finished the experiments. There is certainly a noticeable effect from MAPQ limits, more than I expected. For my experiment, I rewrote the BAM file, setting the MAPQ to 60 for any read with MAPQ of 36 or higher (I observed 44 as the highest MAPQ value an more variability to MAPQ values than seen with BWA. |Experiment|SNP Recall|SNP Precision|SNP F1|INDEL Recall|Indel Precision|Indel F1|. |------------|----------|--------------|--------|------------|---------------|--------|. |Default BAM|0.9673|0.9967|0.9817|0.9717|0.9956|0.9835|. |MAPQ 36+ -> 60|0.9758|0.9964|0.9859|0.9829|0.9960|0.9894|. This implies you will get better performance with DeepVariant if you set those higher MAPQ values to 60. Note that in general, DeepVariant hasn't been trained with Bowtie2 data and you'd likely get better performance overall by a re-training for it.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/809
https://github.com/google/deepvariant/issues/809:649,performance,perform,performance,649,"Hi @Wenfei-Xian . I finished the experiments. There is certainly a noticeable effect from MAPQ limits, more than I expected. For my experiment, I rewrote the BAM file, setting the MAPQ to 60 for any read with MAPQ of 36 or higher (I observed 44 as the highest MAPQ value an more variability to MAPQ values than seen with BWA. |Experiment|SNP Recall|SNP Precision|SNP F1|INDEL Recall|Indel Precision|Indel F1|. |------------|----------|--------------|--------|------------|---------------|--------|. |Default BAM|0.9673|0.9967|0.9817|0.9717|0.9956|0.9835|. |MAPQ 36+ -> 60|0.9758|0.9964|0.9859|0.9829|0.9960|0.9894|. This implies you will get better performance with DeepVariant if you set those higher MAPQ values to 60. Note that in general, DeepVariant hasn't been trained with Bowtie2 data and you'd likely get better performance overall by a re-training for it.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/809
https://github.com/google/deepvariant/issues/809:821,performance,perform,performance,821,"Hi @Wenfei-Xian . I finished the experiments. There is certainly a noticeable effect from MAPQ limits, more than I expected. For my experiment, I rewrote the BAM file, setting the MAPQ to 60 for any read with MAPQ of 36 or higher (I observed 44 as the highest MAPQ value an more variability to MAPQ values than seen with BWA. |Experiment|SNP Recall|SNP Precision|SNP F1|INDEL Recall|Indel Precision|Indel F1|. |------------|----------|--------------|--------|------------|---------------|--------|. |Default BAM|0.9673|0.9967|0.9817|0.9717|0.9956|0.9835|. |MAPQ 36+ -> 60|0.9758|0.9964|0.9859|0.9829|0.9960|0.9894|. This implies you will get better performance with DeepVariant if you set those higher MAPQ values to 60. Note that in general, DeepVariant hasn't been trained with Bowtie2 data and you'd likely get better performance overall by a re-training for it.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/809
https://github.com/google/deepvariant/issues/809:233,testability,observ,observed,233,"Hi @Wenfei-Xian . I finished the experiments. There is certainly a noticeable effect from MAPQ limits, more than I expected. For my experiment, I rewrote the BAM file, setting the MAPQ to 60 for any read with MAPQ of 36 or higher (I observed 44 as the highest MAPQ value an more variability to MAPQ values than seen with BWA. |Experiment|SNP Recall|SNP Precision|SNP F1|INDEL Recall|Indel Precision|Indel F1|. |------------|----------|--------------|--------|------------|---------------|--------|. |Default BAM|0.9673|0.9967|0.9817|0.9717|0.9956|0.9835|. |MAPQ 36+ -> 60|0.9758|0.9964|0.9859|0.9829|0.9960|0.9894|. This implies you will get better performance with DeepVariant if you set those higher MAPQ values to 60. Note that in general, DeepVariant hasn't been trained with Bowtie2 data and you'd likely get better performance overall by a re-training for it.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/809
https://github.com/google/deepvariant/issues/809:649,usability,perform,performance,649,"Hi @Wenfei-Xian . I finished the experiments. There is certainly a noticeable effect from MAPQ limits, more than I expected. For my experiment, I rewrote the BAM file, setting the MAPQ to 60 for any read with MAPQ of 36 or higher (I observed 44 as the highest MAPQ value an more variability to MAPQ values than seen with BWA. |Experiment|SNP Recall|SNP Precision|SNP F1|INDEL Recall|Indel Precision|Indel F1|. |------------|----------|--------------|--------|------------|---------------|--------|. |Default BAM|0.9673|0.9967|0.9817|0.9717|0.9956|0.9835|. |MAPQ 36+ -> 60|0.9758|0.9964|0.9859|0.9829|0.9960|0.9894|. This implies you will get better performance with DeepVariant if you set those higher MAPQ values to 60. Note that in general, DeepVariant hasn't been trained with Bowtie2 data and you'd likely get better performance overall by a re-training for it.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/809
https://github.com/google/deepvariant/issues/809:821,usability,perform,performance,821,"Hi @Wenfei-Xian . I finished the experiments. There is certainly a noticeable effect from MAPQ limits, more than I expected. For my experiment, I rewrote the BAM file, setting the MAPQ to 60 for any read with MAPQ of 36 or higher (I observed 44 as the highest MAPQ value an more variability to MAPQ values than seen with BWA. |Experiment|SNP Recall|SNP Precision|SNP F1|INDEL Recall|Indel Precision|Indel F1|. |------------|----------|--------------|--------|------------|---------------|--------|. |Default BAM|0.9673|0.9967|0.9817|0.9717|0.9956|0.9835|. |MAPQ 36+ -> 60|0.9758|0.9964|0.9859|0.9829|0.9960|0.9894|. This implies you will get better performance with DeepVariant if you set those higher MAPQ values to 60. Note that in general, DeepVariant hasn't been trained with Bowtie2 data and you'd likely get better performance overall by a re-training for it.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/809
https://github.com/google/deepvariant/issues/810:132,deployability,log,logs,132,"@githubtefo , you are providing the raw read inputs to DeepVariant? I requires reads aligned to the reference. Also, please see the logs after you run DeepVariant in the output directory.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/810
https://github.com/google/deepvariant/issues/810:45,safety,input,inputs,45,"@githubtefo , you are providing the raw read inputs to DeepVariant? I requires reads aligned to the reference. Also, please see the logs after you run DeepVariant in the output directory.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/810
https://github.com/google/deepvariant/issues/810:132,safety,log,logs,132,"@githubtefo , you are providing the raw read inputs to DeepVariant? I requires reads aligned to the reference. Also, please see the logs after you run DeepVariant in the output directory.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/810
https://github.com/google/deepvariant/issues/810:132,security,log,logs,132,"@githubtefo , you are providing the raw read inputs to DeepVariant? I requires reads aligned to the reference. Also, please see the logs after you run DeepVariant in the output directory.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/810
https://github.com/google/deepvariant/issues/810:132,testability,log,logs,132,"@githubtefo , you are providing the raw read inputs to DeepVariant? I requires reads aligned to the reference. Also, please see the logs after you run DeepVariant in the output directory.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/810
https://github.com/google/deepvariant/issues/810:45,usability,input,inputs,45,"@githubtefo , you are providing the raw read inputs to DeepVariant? I requires reads aligned to the reference. Also, please see the logs after you run DeepVariant in the output directory.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/810
https://github.com/google/deepvariant/issues/810:139,deployability,log,logs,139,"Hi @kishwarshafin, thanks for your reply. I am providing sorted and aligned inputs to DeepVariant, generated by pbmm2. I am not seeing any logs in the output dir, it is completely empty :(",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/810
https://github.com/google/deepvariant/issues/810:76,safety,input,inputs,76,"Hi @kishwarshafin, thanks for your reply. I am providing sorted and aligned inputs to DeepVariant, generated by pbmm2. I am not seeing any logs in the output dir, it is completely empty :(",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/810
https://github.com/google/deepvariant/issues/810:139,safety,log,logs,139,"Hi @kishwarshafin, thanks for your reply. I am providing sorted and aligned inputs to DeepVariant, generated by pbmm2. I am not seeing any logs in the output dir, it is completely empty :(",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/810
https://github.com/google/deepvariant/issues/810:169,safety,compl,completely,169,"Hi @kishwarshafin, thanks for your reply. I am providing sorted and aligned inputs to DeepVariant, generated by pbmm2. I am not seeing any logs in the output dir, it is completely empty :(",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/810
https://github.com/google/deepvariant/issues/810:139,security,log,logs,139,"Hi @kishwarshafin, thanks for your reply. I am providing sorted and aligned inputs to DeepVariant, generated by pbmm2. I am not seeing any logs in the output dir, it is completely empty :(",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/810
https://github.com/google/deepvariant/issues/810:169,security,compl,completely,169,"Hi @kishwarshafin, thanks for your reply. I am providing sorted and aligned inputs to DeepVariant, generated by pbmm2. I am not seeing any logs in the output dir, it is completely empty :(",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/810
https://github.com/google/deepvariant/issues/810:139,testability,log,logs,139,"Hi @kishwarshafin, thanks for your reply. I am providing sorted and aligned inputs to DeepVariant, generated by pbmm2. I am not seeing any logs in the output dir, it is completely empty :(",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/810
https://github.com/google/deepvariant/issues/810:76,usability,input,inputs,76,"Hi @kishwarshafin, thanks for your reply. I am providing sorted and aligned inputs to DeepVariant, generated by pbmm2. I am not seeing any logs in the output dir, it is completely empty :(",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/810
https://github.com/google/deepvariant/issues/810:60,deployability,log,logs,60,"Hi @githubtefo ,. When you run the command, you should have logs in the terminal. Can you provide those logs? And, can you try something simple like https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-pacbio-model-case-study.md first? That should certainly have logs when you run it. If not, there's something else wrong in your environment that we need to understand first.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/810
https://github.com/google/deepvariant/issues/810:104,deployability,log,logs,104,"Hi @githubtefo ,. When you run the command, you should have logs in the terminal. Can you provide those logs? And, can you try something simple like https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-pacbio-model-case-study.md first? That should certainly have logs when you run it. If not, there's something else wrong in your environment that we need to understand first.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/810
https://github.com/google/deepvariant/issues/810:277,deployability,log,logs,277,"Hi @githubtefo ,. When you run the command, you should have logs in the terminal. Can you provide those logs? And, can you try something simple like https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-pacbio-model-case-study.md first? That should certainly have logs when you run it. If not, there's something else wrong in your environment that we need to understand first.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/810
https://github.com/google/deepvariant/issues/810:223,energy efficiency,model,model-case-study,223,"Hi @githubtefo ,. When you run the command, you should have logs in the terminal. Can you provide those logs? And, can you try something simple like https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-pacbio-model-case-study.md first? That should certainly have logs when you run it. If not, there's something else wrong in your environment that we need to understand first.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/810
https://github.com/google/deepvariant/issues/810:216,modifiability,pac,pacbio-model-case-study,216,"Hi @githubtefo ,. When you run the command, you should have logs in the terminal. Can you provide those logs? And, can you try something simple like https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-pacbio-model-case-study.md first? That should certainly have logs when you run it. If not, there's something else wrong in your environment that we need to understand first.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/810
https://github.com/google/deepvariant/issues/810:60,safety,log,logs,60,"Hi @githubtefo ,. When you run the command, you should have logs in the terminal. Can you provide those logs? And, can you try something simple like https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-pacbio-model-case-study.md first? That should certainly have logs when you run it. If not, there's something else wrong in your environment that we need to understand first.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/810
https://github.com/google/deepvariant/issues/810:104,safety,log,logs,104,"Hi @githubtefo ,. When you run the command, you should have logs in the terminal. Can you provide those logs? And, can you try something simple like https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-pacbio-model-case-study.md first? That should certainly have logs when you run it. If not, there's something else wrong in your environment that we need to understand first.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/810
https://github.com/google/deepvariant/issues/810:277,safety,log,logs,277,"Hi @githubtefo ,. When you run the command, you should have logs in the terminal. Can you provide those logs? And, can you try something simple like https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-pacbio-model-case-study.md first? That should certainly have logs when you run it. If not, there's something else wrong in your environment that we need to understand first.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/810
https://github.com/google/deepvariant/issues/810:60,security,log,logs,60,"Hi @githubtefo ,. When you run the command, you should have logs in the terminal. Can you provide those logs? And, can you try something simple like https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-pacbio-model-case-study.md first? That should certainly have logs when you run it. If not, there's something else wrong in your environment that we need to understand first.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/810
https://github.com/google/deepvariant/issues/810:104,security,log,logs,104,"Hi @githubtefo ,. When you run the command, you should have logs in the terminal. Can you provide those logs? And, can you try something simple like https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-pacbio-model-case-study.md first? That should certainly have logs when you run it. If not, there's something else wrong in your environment that we need to understand first.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/810
https://github.com/google/deepvariant/issues/810:223,security,model,model-case-study,223,"Hi @githubtefo ,. When you run the command, you should have logs in the terminal. Can you provide those logs? And, can you try something simple like https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-pacbio-model-case-study.md first? That should certainly have logs when you run it. If not, there's something else wrong in your environment that we need to understand first.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/810
https://github.com/google/deepvariant/issues/810:277,security,log,logs,277,"Hi @githubtefo ,. When you run the command, you should have logs in the terminal. Can you provide those logs? And, can you try something simple like https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-pacbio-model-case-study.md first? That should certainly have logs when you run it. If not, there's something else wrong in your environment that we need to understand first.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/810
https://github.com/google/deepvariant/issues/810:60,testability,log,logs,60,"Hi @githubtefo ,. When you run the command, you should have logs in the terminal. Can you provide those logs? And, can you try something simple like https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-pacbio-model-case-study.md first? That should certainly have logs when you run it. If not, there's something else wrong in your environment that we need to understand first.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/810
https://github.com/google/deepvariant/issues/810:104,testability,log,logs,104,"Hi @githubtefo ,. When you run the command, you should have logs in the terminal. Can you provide those logs? And, can you try something simple like https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-pacbio-model-case-study.md first? That should certainly have logs when you run it. If not, there's something else wrong in your environment that we need to understand first.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/810
https://github.com/google/deepvariant/issues/810:137,testability,simpl,simple,137,"Hi @githubtefo ,. When you run the command, you should have logs in the terminal. Can you provide those logs? And, can you try something simple like https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-pacbio-model-case-study.md first? That should certainly have logs when you run it. If not, there's something else wrong in your environment that we need to understand first.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/810
https://github.com/google/deepvariant/issues/810:277,testability,log,logs,277,"Hi @githubtefo ,. When you run the command, you should have logs in the terminal. Can you provide those logs? And, can you try something simple like https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-pacbio-model-case-study.md first? That should certainly have logs when you run it. If not, there's something else wrong in your environment that we need to understand first.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/810
https://github.com/google/deepvariant/issues/810:372,testability,understand,understand,372,"Hi @githubtefo ,. When you run the command, you should have logs in the terminal. Can you provide those logs? And, can you try something simple like https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-pacbio-model-case-study.md first? That should certainly have logs when you run it. If not, there's something else wrong in your environment that we need to understand first.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/810
https://github.com/google/deepvariant/issues/810:35,usability,command,command,35,"Hi @githubtefo ,. When you run the command, you should have logs in the terminal. Can you provide those logs? And, can you try something simple like https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-pacbio-model-case-study.md first? That should certainly have logs when you run it. If not, there's something else wrong in your environment that we need to understand first.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/810
https://github.com/google/deepvariant/issues/810:137,usability,simpl,simple,137,"Hi @githubtefo ,. When you run the command, you should have logs in the terminal. Can you provide those logs? And, can you try something simple like https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-pacbio-model-case-study.md first? That should certainly have logs when you run it. If not, there's something else wrong in your environment that we need to understand first.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/810
https://github.com/google/deepvariant/issues/810:31,deployability,log,logs,31,"Sure! please find attached the logs in the terminal. In the meantime, I will run the simple case study. Thank you! [output.log](https://github.com/google/deepvariant/files/15052710/output.log).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/810
https://github.com/google/deepvariant/issues/810:123,deployability,log,log,123,"Sure! please find attached the logs in the terminal. In the meantime, I will run the simple case study. Thank you! [output.log](https://github.com/google/deepvariant/files/15052710/output.log).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/810
https://github.com/google/deepvariant/issues/810:188,deployability,log,log,188,"Sure! please find attached the logs in the terminal. In the meantime, I will run the simple case study. Thank you! [output.log](https://github.com/google/deepvariant/files/15052710/output.log).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/810
https://github.com/google/deepvariant/issues/810:31,safety,log,logs,31,"Sure! please find attached the logs in the terminal. In the meantime, I will run the simple case study. Thank you! [output.log](https://github.com/google/deepvariant/files/15052710/output.log).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/810
https://github.com/google/deepvariant/issues/810:123,safety,log,log,123,"Sure! please find attached the logs in the terminal. In the meantime, I will run the simple case study. Thank you! [output.log](https://github.com/google/deepvariant/files/15052710/output.log).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/810
https://github.com/google/deepvariant/issues/810:188,safety,log,log,188,"Sure! please find attached the logs in the terminal. In the meantime, I will run the simple case study. Thank you! [output.log](https://github.com/google/deepvariant/files/15052710/output.log).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/810
https://github.com/google/deepvariant/issues/810:31,security,log,logs,31,"Sure! please find attached the logs in the terminal. In the meantime, I will run the simple case study. Thank you! [output.log](https://github.com/google/deepvariant/files/15052710/output.log).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/810
https://github.com/google/deepvariant/issues/810:123,security,log,log,123,"Sure! please find attached the logs in the terminal. In the meantime, I will run the simple case study. Thank you! [output.log](https://github.com/google/deepvariant/files/15052710/output.log).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/810
https://github.com/google/deepvariant/issues/810:188,security,log,log,188,"Sure! please find attached the logs in the terminal. In the meantime, I will run the simple case study. Thank you! [output.log](https://github.com/google/deepvariant/files/15052710/output.log).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/810
https://github.com/google/deepvariant/issues/810:31,testability,log,logs,31,"Sure! please find attached the logs in the terminal. In the meantime, I will run the simple case study. Thank you! [output.log](https://github.com/google/deepvariant/files/15052710/output.log).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/810
https://github.com/google/deepvariant/issues/810:85,testability,simpl,simple,85,"Sure! please find attached the logs in the terminal. In the meantime, I will run the simple case study. Thank you! [output.log](https://github.com/google/deepvariant/files/15052710/output.log).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/810
https://github.com/google/deepvariant/issues/810:123,testability,log,log,123,"Sure! please find attached the logs in the terminal. In the meantime, I will run the simple case study. Thank you! [output.log](https://github.com/google/deepvariant/files/15052710/output.log).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/810
https://github.com/google/deepvariant/issues/810:188,testability,log,log,188,"Sure! please find attached the logs in the terminal. In the meantime, I will run the simple case study. Thank you! [output.log](https://github.com/google/deepvariant/files/15052710/output.log).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/810
https://github.com/google/deepvariant/issues/810:85,usability,simpl,simple,85,"Sure! please find attached the logs in the terminal. In the meantime, I will run the simple case study. Thank you! [output.log](https://github.com/google/deepvariant/files/15052710/output.log).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/810
https://github.com/google/deepvariant/issues/810:1964,availability,robust,robust,1964,"Thank you @githubtefo for the log. The last step certainly looks strange to me:. ```. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""/reference/Homo_sapiens.GRCh37.dna.primary_assembly.fa"" --infile ""/tmp/tmpba2iuryg/call_variants_output.tfrecord.gz"" --outfile ""/output/output.vcf.gz"" --cpus ""16"" --gvcf_outfile ""/output/output.g.vcf.gz"" --nonvariant_site_tfrecord_path ""/tmp/tmpba2iuryg/gvcf.tfrecord@16.gz"". I0418 11:23:10.756783 140607260936000 postprocess_variants.py:1211] Using sample name from call_variants output. Sample name: 15337_Control. 2024-04-18 11:23:10.761841: I deepvariant/postprocess_variants.cc:94] Read from: /tmp/tmpba2iuryg/call_variants_output-00000-of-00001.tfrecord.gz. 2024-04-18 11:23:39.285004: I deepvariant/postprocess_variants.cc:109] Total #entries in single_site_calls = 8753635. I0418 11:24:21.877126 140607260936000 postprocess_variants.py:1313] CVO sorting took 1.1852648933728536 minutes. I0418 11:24:21.877437 140607260936000 postprocess_variants.py:1316] Transforming call_variants_output to variants. I0418 11:24:21.877472 140607260936000 postprocess_variants.py:1318] Using 16 CPUs for parallelization of variant transformation. I0418 11:24:35.259627 140607260936000 postprocess_variants.py:1211] Using sample name from call_variants output. Sample name: 15337_Control. real	2m10.376s. user	1m41.800s. sys	0m24.640s. ```. I would expect `postprocess_variants` to be doing more. I believe @lucasbrambrink is looking into another issue in `postprocess_variants` related to multiprocessing (https://github.com/google/deepvariant/issues/804), but I'm not sure if this relevant. For now, can you try disable multiprocessing in the `postprocess_variants` step by setting `--cpus 0`. If you're using the run_deepvariant one-step script, you can add:. ```bash. --postprocess_variants_extra_args=""cpus=0"". ```. and see if that works for you. Please let us know. We'll try to make this more robust in the future!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/810
https://github.com/google/deepvariant/issues/810:30,deployability,log,log,30,"Thank you @githubtefo for the log. The last step certainly looks strange to me:. ```. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""/reference/Homo_sapiens.GRCh37.dna.primary_assembly.fa"" --infile ""/tmp/tmpba2iuryg/call_variants_output.tfrecord.gz"" --outfile ""/output/output.vcf.gz"" --cpus ""16"" --gvcf_outfile ""/output/output.g.vcf.gz"" --nonvariant_site_tfrecord_path ""/tmp/tmpba2iuryg/gvcf.tfrecord@16.gz"". I0418 11:23:10.756783 140607260936000 postprocess_variants.py:1211] Using sample name from call_variants output. Sample name: 15337_Control. 2024-04-18 11:23:10.761841: I deepvariant/postprocess_variants.cc:94] Read from: /tmp/tmpba2iuryg/call_variants_output-00000-of-00001.tfrecord.gz. 2024-04-18 11:23:39.285004: I deepvariant/postprocess_variants.cc:109] Total #entries in single_site_calls = 8753635. I0418 11:24:21.877126 140607260936000 postprocess_variants.py:1313] CVO sorting took 1.1852648933728536 minutes. I0418 11:24:21.877437 140607260936000 postprocess_variants.py:1316] Transforming call_variants_output to variants. I0418 11:24:21.877472 140607260936000 postprocess_variants.py:1318] Using 16 CPUs for parallelization of variant transformation. I0418 11:24:35.259627 140607260936000 postprocess_variants.py:1211] Using sample name from call_variants output. Sample name: 15337_Control. real	2m10.376s. user	1m41.800s. sys	0m24.640s. ```. I would expect `postprocess_variants` to be doing more. I believe @lucasbrambrink is looking into another issue in `postprocess_variants` related to multiprocessing (https://github.com/google/deepvariant/issues/804), but I'm not sure if this relevant. For now, can you try disable multiprocessing in the `postprocess_variants` step by setting `--cpus 0`. If you're using the run_deepvariant one-step script, you can add:. ```bash. --postprocess_variants_extra_args=""cpus=0"". ```. and see if that works for you. Please let us know. We'll try to make this more robust in the future!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/810
https://github.com/google/deepvariant/issues/810:326,energy efficiency,cpu,cpus,326,"Thank you @githubtefo for the log. The last step certainly looks strange to me:. ```. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""/reference/Homo_sapiens.GRCh37.dna.primary_assembly.fa"" --infile ""/tmp/tmpba2iuryg/call_variants_output.tfrecord.gz"" --outfile ""/output/output.vcf.gz"" --cpus ""16"" --gvcf_outfile ""/output/output.g.vcf.gz"" --nonvariant_site_tfrecord_path ""/tmp/tmpba2iuryg/gvcf.tfrecord@16.gz"". I0418 11:23:10.756783 140607260936000 postprocess_variants.py:1211] Using sample name from call_variants output. Sample name: 15337_Control. 2024-04-18 11:23:10.761841: I deepvariant/postprocess_variants.cc:94] Read from: /tmp/tmpba2iuryg/call_variants_output-00000-of-00001.tfrecord.gz. 2024-04-18 11:23:39.285004: I deepvariant/postprocess_variants.cc:109] Total #entries in single_site_calls = 8753635. I0418 11:24:21.877126 140607260936000 postprocess_variants.py:1313] CVO sorting took 1.1852648933728536 minutes. I0418 11:24:21.877437 140607260936000 postprocess_variants.py:1316] Transforming call_variants_output to variants. I0418 11:24:21.877472 140607260936000 postprocess_variants.py:1318] Using 16 CPUs for parallelization of variant transformation. I0418 11:24:35.259627 140607260936000 postprocess_variants.py:1211] Using sample name from call_variants output. Sample name: 15337_Control. real	2m10.376s. user	1m41.800s. sys	0m24.640s. ```. I would expect `postprocess_variants` to be doing more. I believe @lucasbrambrink is looking into another issue in `postprocess_variants` related to multiprocessing (https://github.com/google/deepvariant/issues/804), but I'm not sure if this relevant. For now, can you try disable multiprocessing in the `postprocess_variants` step by setting `--cpus 0`. If you're using the run_deepvariant one-step script, you can add:. ```bash. --postprocess_variants_extra_args=""cpus=0"". ```. and see if that works for you. Please let us know. We'll try to make this more robust in the future!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/810
https://github.com/google/deepvariant/issues/810:1160,energy efficiency,CPU,CPUs,1160,"Thank you @githubtefo for the log. The last step certainly looks strange to me:. ```. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""/reference/Homo_sapiens.GRCh37.dna.primary_assembly.fa"" --infile ""/tmp/tmpba2iuryg/call_variants_output.tfrecord.gz"" --outfile ""/output/output.vcf.gz"" --cpus ""16"" --gvcf_outfile ""/output/output.g.vcf.gz"" --nonvariant_site_tfrecord_path ""/tmp/tmpba2iuryg/gvcf.tfrecord@16.gz"". I0418 11:23:10.756783 140607260936000 postprocess_variants.py:1211] Using sample name from call_variants output. Sample name: 15337_Control. 2024-04-18 11:23:10.761841: I deepvariant/postprocess_variants.cc:94] Read from: /tmp/tmpba2iuryg/call_variants_output-00000-of-00001.tfrecord.gz. 2024-04-18 11:23:39.285004: I deepvariant/postprocess_variants.cc:109] Total #entries in single_site_calls = 8753635. I0418 11:24:21.877126 140607260936000 postprocess_variants.py:1313] CVO sorting took 1.1852648933728536 minutes. I0418 11:24:21.877437 140607260936000 postprocess_variants.py:1316] Transforming call_variants_output to variants. I0418 11:24:21.877472 140607260936000 postprocess_variants.py:1318] Using 16 CPUs for parallelization of variant transformation. I0418 11:24:35.259627 140607260936000 postprocess_variants.py:1211] Using sample name from call_variants output. Sample name: 15337_Control. real	2m10.376s. user	1m41.800s. sys	0m24.640s. ```. I would expect `postprocess_variants` to be doing more. I believe @lucasbrambrink is looking into another issue in `postprocess_variants` related to multiprocessing (https://github.com/google/deepvariant/issues/804), but I'm not sure if this relevant. For now, can you try disable multiprocessing in the `postprocess_variants` step by setting `--cpus 0`. If you're using the run_deepvariant one-step script, you can add:. ```bash. --postprocess_variants_extra_args=""cpus=0"". ```. and see if that works for you. Please let us know. We'll try to make this more robust in the future!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/810
https://github.com/google/deepvariant/issues/810:1751,energy efficiency,cpu,cpus,1751,"Thank you @githubtefo for the log. The last step certainly looks strange to me:. ```. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""/reference/Homo_sapiens.GRCh37.dna.primary_assembly.fa"" --infile ""/tmp/tmpba2iuryg/call_variants_output.tfrecord.gz"" --outfile ""/output/output.vcf.gz"" --cpus ""16"" --gvcf_outfile ""/output/output.g.vcf.gz"" --nonvariant_site_tfrecord_path ""/tmp/tmpba2iuryg/gvcf.tfrecord@16.gz"". I0418 11:23:10.756783 140607260936000 postprocess_variants.py:1211] Using sample name from call_variants output. Sample name: 15337_Control. 2024-04-18 11:23:10.761841: I deepvariant/postprocess_variants.cc:94] Read from: /tmp/tmpba2iuryg/call_variants_output-00000-of-00001.tfrecord.gz. 2024-04-18 11:23:39.285004: I deepvariant/postprocess_variants.cc:109] Total #entries in single_site_calls = 8753635. I0418 11:24:21.877126 140607260936000 postprocess_variants.py:1313] CVO sorting took 1.1852648933728536 minutes. I0418 11:24:21.877437 140607260936000 postprocess_variants.py:1316] Transforming call_variants_output to variants. I0418 11:24:21.877472 140607260936000 postprocess_variants.py:1318] Using 16 CPUs for parallelization of variant transformation. I0418 11:24:35.259627 140607260936000 postprocess_variants.py:1211] Using sample name from call_variants output. Sample name: 15337_Control. real	2m10.376s. user	1m41.800s. sys	0m24.640s. ```. I would expect `postprocess_variants` to be doing more. I believe @lucasbrambrink is looking into another issue in `postprocess_variants` related to multiprocessing (https://github.com/google/deepvariant/issues/804), but I'm not sure if this relevant. For now, can you try disable multiprocessing in the `postprocess_variants` step by setting `--cpus 0`. If you're using the run_deepvariant one-step script, you can add:. ```bash. --postprocess_variants_extra_args=""cpus=0"". ```. and see if that works for you. Please let us know. We'll try to make this more robust in the future!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/810
https://github.com/google/deepvariant/issues/810:1871,energy efficiency,cpu,cpus,1871,"Thank you @githubtefo for the log. The last step certainly looks strange to me:. ```. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""/reference/Homo_sapiens.GRCh37.dna.primary_assembly.fa"" --infile ""/tmp/tmpba2iuryg/call_variants_output.tfrecord.gz"" --outfile ""/output/output.vcf.gz"" --cpus ""16"" --gvcf_outfile ""/output/output.g.vcf.gz"" --nonvariant_site_tfrecord_path ""/tmp/tmpba2iuryg/gvcf.tfrecord@16.gz"". I0418 11:23:10.756783 140607260936000 postprocess_variants.py:1211] Using sample name from call_variants output. Sample name: 15337_Control. 2024-04-18 11:23:10.761841: I deepvariant/postprocess_variants.cc:94] Read from: /tmp/tmpba2iuryg/call_variants_output-00000-of-00001.tfrecord.gz. 2024-04-18 11:23:39.285004: I deepvariant/postprocess_variants.cc:109] Total #entries in single_site_calls = 8753635. I0418 11:24:21.877126 140607260936000 postprocess_variants.py:1313] CVO sorting took 1.1852648933728536 minutes. I0418 11:24:21.877437 140607260936000 postprocess_variants.py:1316] Transforming call_variants_output to variants. I0418 11:24:21.877472 140607260936000 postprocess_variants.py:1318] Using 16 CPUs for parallelization of variant transformation. I0418 11:24:35.259627 140607260936000 postprocess_variants.py:1211] Using sample name from call_variants output. Sample name: 15337_Control. real	2m10.376s. user	1m41.800s. sys	0m24.640s. ```. I would expect `postprocess_variants` to be doing more. I believe @lucasbrambrink is looking into another issue in `postprocess_variants` related to multiprocessing (https://github.com/google/deepvariant/issues/804), but I'm not sure if this relevant. For now, can you try disable multiprocessing in the `postprocess_variants` step by setting `--cpus 0`. If you're using the run_deepvariant one-step script, you can add:. ```bash. --postprocess_variants_extra_args=""cpus=0"". ```. and see if that works for you. Please let us know. We'll try to make this more robust in the future!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/810
https://github.com/google/deepvariant/issues/810:1036,integrability,Transform,Transforming,1036,"Thank you @githubtefo for the log. The last step certainly looks strange to me:. ```. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""/reference/Homo_sapiens.GRCh37.dna.primary_assembly.fa"" --infile ""/tmp/tmpba2iuryg/call_variants_output.tfrecord.gz"" --outfile ""/output/output.vcf.gz"" --cpus ""16"" --gvcf_outfile ""/output/output.g.vcf.gz"" --nonvariant_site_tfrecord_path ""/tmp/tmpba2iuryg/gvcf.tfrecord@16.gz"". I0418 11:23:10.756783 140607260936000 postprocess_variants.py:1211] Using sample name from call_variants output. Sample name: 15337_Control. 2024-04-18 11:23:10.761841: I deepvariant/postprocess_variants.cc:94] Read from: /tmp/tmpba2iuryg/call_variants_output-00000-of-00001.tfrecord.gz. 2024-04-18 11:23:39.285004: I deepvariant/postprocess_variants.cc:109] Total #entries in single_site_calls = 8753635. I0418 11:24:21.877126 140607260936000 postprocess_variants.py:1313] CVO sorting took 1.1852648933728536 minutes. I0418 11:24:21.877437 140607260936000 postprocess_variants.py:1316] Transforming call_variants_output to variants. I0418 11:24:21.877472 140607260936000 postprocess_variants.py:1318] Using 16 CPUs for parallelization of variant transformation. I0418 11:24:35.259627 140607260936000 postprocess_variants.py:1211] Using sample name from call_variants output. Sample name: 15337_Control. real	2m10.376s. user	1m41.800s. sys	0m24.640s. ```. I would expect `postprocess_variants` to be doing more. I believe @lucasbrambrink is looking into another issue in `postprocess_variants` related to multiprocessing (https://github.com/google/deepvariant/issues/804), but I'm not sure if this relevant. For now, can you try disable multiprocessing in the `postprocess_variants` step by setting `--cpus 0`. If you're using the run_deepvariant one-step script, you can add:. ```bash. --postprocess_variants_extra_args=""cpus=0"". ```. and see if that works for you. Please let us know. We'll try to make this more robust in the future!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/810
https://github.com/google/deepvariant/issues/810:1196,integrability,transform,transformation,1196,"Thank you @githubtefo for the log. The last step certainly looks strange to me:. ```. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""/reference/Homo_sapiens.GRCh37.dna.primary_assembly.fa"" --infile ""/tmp/tmpba2iuryg/call_variants_output.tfrecord.gz"" --outfile ""/output/output.vcf.gz"" --cpus ""16"" --gvcf_outfile ""/output/output.g.vcf.gz"" --nonvariant_site_tfrecord_path ""/tmp/tmpba2iuryg/gvcf.tfrecord@16.gz"". I0418 11:23:10.756783 140607260936000 postprocess_variants.py:1211] Using sample name from call_variants output. Sample name: 15337_Control. 2024-04-18 11:23:10.761841: I deepvariant/postprocess_variants.cc:94] Read from: /tmp/tmpba2iuryg/call_variants_output-00000-of-00001.tfrecord.gz. 2024-04-18 11:23:39.285004: I deepvariant/postprocess_variants.cc:109] Total #entries in single_site_calls = 8753635. I0418 11:24:21.877126 140607260936000 postprocess_variants.py:1313] CVO sorting took 1.1852648933728536 minutes. I0418 11:24:21.877437 140607260936000 postprocess_variants.py:1316] Transforming call_variants_output to variants. I0418 11:24:21.877472 140607260936000 postprocess_variants.py:1318] Using 16 CPUs for parallelization of variant transformation. I0418 11:24:35.259627 140607260936000 postprocess_variants.py:1211] Using sample name from call_variants output. Sample name: 15337_Control. real	2m10.376s. user	1m41.800s. sys	0m24.640s. ```. I would expect `postprocess_variants` to be doing more. I believe @lucasbrambrink is looking into another issue in `postprocess_variants` related to multiprocessing (https://github.com/google/deepvariant/issues/804), but I'm not sure if this relevant. For now, can you try disable multiprocessing in the `postprocess_variants` step by setting `--cpus 0`. If you're using the run_deepvariant one-step script, you can add:. ```bash. --postprocess_variants_extra_args=""cpus=0"". ```. and see if that works for you. Please let us know. We'll try to make this more robust in the future!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/810
https://github.com/google/deepvariant/issues/810:1036,interoperability,Transform,Transforming,1036,"Thank you @githubtefo for the log. The last step certainly looks strange to me:. ```. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""/reference/Homo_sapiens.GRCh37.dna.primary_assembly.fa"" --infile ""/tmp/tmpba2iuryg/call_variants_output.tfrecord.gz"" --outfile ""/output/output.vcf.gz"" --cpus ""16"" --gvcf_outfile ""/output/output.g.vcf.gz"" --nonvariant_site_tfrecord_path ""/tmp/tmpba2iuryg/gvcf.tfrecord@16.gz"". I0418 11:23:10.756783 140607260936000 postprocess_variants.py:1211] Using sample name from call_variants output. Sample name: 15337_Control. 2024-04-18 11:23:10.761841: I deepvariant/postprocess_variants.cc:94] Read from: /tmp/tmpba2iuryg/call_variants_output-00000-of-00001.tfrecord.gz. 2024-04-18 11:23:39.285004: I deepvariant/postprocess_variants.cc:109] Total #entries in single_site_calls = 8753635. I0418 11:24:21.877126 140607260936000 postprocess_variants.py:1313] CVO sorting took 1.1852648933728536 minutes. I0418 11:24:21.877437 140607260936000 postprocess_variants.py:1316] Transforming call_variants_output to variants. I0418 11:24:21.877472 140607260936000 postprocess_variants.py:1318] Using 16 CPUs for parallelization of variant transformation. I0418 11:24:35.259627 140607260936000 postprocess_variants.py:1211] Using sample name from call_variants output. Sample name: 15337_Control. real	2m10.376s. user	1m41.800s. sys	0m24.640s. ```. I would expect `postprocess_variants` to be doing more. I believe @lucasbrambrink is looking into another issue in `postprocess_variants` related to multiprocessing (https://github.com/google/deepvariant/issues/804), but I'm not sure if this relevant. For now, can you try disable multiprocessing in the `postprocess_variants` step by setting `--cpus 0`. If you're using the run_deepvariant one-step script, you can add:. ```bash. --postprocess_variants_extra_args=""cpus=0"". ```. and see if that works for you. Please let us know. We'll try to make this more robust in the future!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/810
https://github.com/google/deepvariant/issues/810:1196,interoperability,transform,transformation,1196,"Thank you @githubtefo for the log. The last step certainly looks strange to me:. ```. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""/reference/Homo_sapiens.GRCh37.dna.primary_assembly.fa"" --infile ""/tmp/tmpba2iuryg/call_variants_output.tfrecord.gz"" --outfile ""/output/output.vcf.gz"" --cpus ""16"" --gvcf_outfile ""/output/output.g.vcf.gz"" --nonvariant_site_tfrecord_path ""/tmp/tmpba2iuryg/gvcf.tfrecord@16.gz"". I0418 11:23:10.756783 140607260936000 postprocess_variants.py:1211] Using sample name from call_variants output. Sample name: 15337_Control. 2024-04-18 11:23:10.761841: I deepvariant/postprocess_variants.cc:94] Read from: /tmp/tmpba2iuryg/call_variants_output-00000-of-00001.tfrecord.gz. 2024-04-18 11:23:39.285004: I deepvariant/postprocess_variants.cc:109] Total #entries in single_site_calls = 8753635. I0418 11:24:21.877126 140607260936000 postprocess_variants.py:1313] CVO sorting took 1.1852648933728536 minutes. I0418 11:24:21.877437 140607260936000 postprocess_variants.py:1316] Transforming call_variants_output to variants. I0418 11:24:21.877472 140607260936000 postprocess_variants.py:1318] Using 16 CPUs for parallelization of variant transformation. I0418 11:24:35.259627 140607260936000 postprocess_variants.py:1211] Using sample name from call_variants output. Sample name: 15337_Control. real	2m10.376s. user	1m41.800s. sys	0m24.640s. ```. I would expect `postprocess_variants` to be doing more. I believe @lucasbrambrink is looking into another issue in `postprocess_variants` related to multiprocessing (https://github.com/google/deepvariant/issues/804), but I'm not sure if this relevant. For now, can you try disable multiprocessing in the `postprocess_variants` step by setting `--cpus 0`. If you're using the run_deepvariant one-step script, you can add:. ```bash. --postprocess_variants_extra_args=""cpus=0"". ```. and see if that works for you. Please let us know. We'll try to make this more robust in the future!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/810
https://github.com/google/deepvariant/issues/810:119,performance,time,time,119,"Thank you @githubtefo for the log. The last step certainly looks strange to me:. ```. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""/reference/Homo_sapiens.GRCh37.dna.primary_assembly.fa"" --infile ""/tmp/tmpba2iuryg/call_variants_output.tfrecord.gz"" --outfile ""/output/output.vcf.gz"" --cpus ""16"" --gvcf_outfile ""/output/output.g.vcf.gz"" --nonvariant_site_tfrecord_path ""/tmp/tmpba2iuryg/gvcf.tfrecord@16.gz"". I0418 11:23:10.756783 140607260936000 postprocess_variants.py:1211] Using sample name from call_variants output. Sample name: 15337_Control. 2024-04-18 11:23:10.761841: I deepvariant/postprocess_variants.cc:94] Read from: /tmp/tmpba2iuryg/call_variants_output-00000-of-00001.tfrecord.gz. 2024-04-18 11:23:39.285004: I deepvariant/postprocess_variants.cc:109] Total #entries in single_site_calls = 8753635. I0418 11:24:21.877126 140607260936000 postprocess_variants.py:1313] CVO sorting took 1.1852648933728536 minutes. I0418 11:24:21.877437 140607260936000 postprocess_variants.py:1316] Transforming call_variants_output to variants. I0418 11:24:21.877472 140607260936000 postprocess_variants.py:1318] Using 16 CPUs for parallelization of variant transformation. I0418 11:24:35.259627 140607260936000 postprocess_variants.py:1211] Using sample name from call_variants output. Sample name: 15337_Control. real	2m10.376s. user	1m41.800s. sys	0m24.640s. ```. I would expect `postprocess_variants` to be doing more. I believe @lucasbrambrink is looking into another issue in `postprocess_variants` related to multiprocessing (https://github.com/google/deepvariant/issues/804), but I'm not sure if this relevant. For now, can you try disable multiprocessing in the `postprocess_variants` step by setting `--cpus 0`. If you're using the run_deepvariant one-step script, you can add:. ```bash. --postprocess_variants_extra_args=""cpus=0"". ```. and see if that works for you. Please let us know. We'll try to make this more robust in the future!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/810
https://github.com/google/deepvariant/issues/810:326,performance,cpu,cpus,326,"Thank you @githubtefo for the log. The last step certainly looks strange to me:. ```. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""/reference/Homo_sapiens.GRCh37.dna.primary_assembly.fa"" --infile ""/tmp/tmpba2iuryg/call_variants_output.tfrecord.gz"" --outfile ""/output/output.vcf.gz"" --cpus ""16"" --gvcf_outfile ""/output/output.g.vcf.gz"" --nonvariant_site_tfrecord_path ""/tmp/tmpba2iuryg/gvcf.tfrecord@16.gz"". I0418 11:23:10.756783 140607260936000 postprocess_variants.py:1211] Using sample name from call_variants output. Sample name: 15337_Control. 2024-04-18 11:23:10.761841: I deepvariant/postprocess_variants.cc:94] Read from: /tmp/tmpba2iuryg/call_variants_output-00000-of-00001.tfrecord.gz. 2024-04-18 11:23:39.285004: I deepvariant/postprocess_variants.cc:109] Total #entries in single_site_calls = 8753635. I0418 11:24:21.877126 140607260936000 postprocess_variants.py:1313] CVO sorting took 1.1852648933728536 minutes. I0418 11:24:21.877437 140607260936000 postprocess_variants.py:1316] Transforming call_variants_output to variants. I0418 11:24:21.877472 140607260936000 postprocess_variants.py:1318] Using 16 CPUs for parallelization of variant transformation. I0418 11:24:35.259627 140607260936000 postprocess_variants.py:1211] Using sample name from call_variants output. Sample name: 15337_Control. real	2m10.376s. user	1m41.800s. sys	0m24.640s. ```. I would expect `postprocess_variants` to be doing more. I believe @lucasbrambrink is looking into another issue in `postprocess_variants` related to multiprocessing (https://github.com/google/deepvariant/issues/804), but I'm not sure if this relevant. For now, can you try disable multiprocessing in the `postprocess_variants` step by setting `--cpus 0`. If you're using the run_deepvariant one-step script, you can add:. ```bash. --postprocess_variants_extra_args=""cpus=0"". ```. and see if that works for you. Please let us know. We'll try to make this more robust in the future!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/810
https://github.com/google/deepvariant/issues/810:1160,performance,CPU,CPUs,1160,"Thank you @githubtefo for the log. The last step certainly looks strange to me:. ```. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""/reference/Homo_sapiens.GRCh37.dna.primary_assembly.fa"" --infile ""/tmp/tmpba2iuryg/call_variants_output.tfrecord.gz"" --outfile ""/output/output.vcf.gz"" --cpus ""16"" --gvcf_outfile ""/output/output.g.vcf.gz"" --nonvariant_site_tfrecord_path ""/tmp/tmpba2iuryg/gvcf.tfrecord@16.gz"". I0418 11:23:10.756783 140607260936000 postprocess_variants.py:1211] Using sample name from call_variants output. Sample name: 15337_Control. 2024-04-18 11:23:10.761841: I deepvariant/postprocess_variants.cc:94] Read from: /tmp/tmpba2iuryg/call_variants_output-00000-of-00001.tfrecord.gz. 2024-04-18 11:23:39.285004: I deepvariant/postprocess_variants.cc:109] Total #entries in single_site_calls = 8753635. I0418 11:24:21.877126 140607260936000 postprocess_variants.py:1313] CVO sorting took 1.1852648933728536 minutes. I0418 11:24:21.877437 140607260936000 postprocess_variants.py:1316] Transforming call_variants_output to variants. I0418 11:24:21.877472 140607260936000 postprocess_variants.py:1318] Using 16 CPUs for parallelization of variant transformation. I0418 11:24:35.259627 140607260936000 postprocess_variants.py:1211] Using sample name from call_variants output. Sample name: 15337_Control. real	2m10.376s. user	1m41.800s. sys	0m24.640s. ```. I would expect `postprocess_variants` to be doing more. I believe @lucasbrambrink is looking into another issue in `postprocess_variants` related to multiprocessing (https://github.com/google/deepvariant/issues/804), but I'm not sure if this relevant. For now, can you try disable multiprocessing in the `postprocess_variants` step by setting `--cpus 0`. If you're using the run_deepvariant one-step script, you can add:. ```bash. --postprocess_variants_extra_args=""cpus=0"". ```. and see if that works for you. Please let us know. We'll try to make this more robust in the future!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/810
https://github.com/google/deepvariant/issues/810:1169,performance,parallel,parallelization,1169,"Thank you @githubtefo for the log. The last step certainly looks strange to me:. ```. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""/reference/Homo_sapiens.GRCh37.dna.primary_assembly.fa"" --infile ""/tmp/tmpba2iuryg/call_variants_output.tfrecord.gz"" --outfile ""/output/output.vcf.gz"" --cpus ""16"" --gvcf_outfile ""/output/output.g.vcf.gz"" --nonvariant_site_tfrecord_path ""/tmp/tmpba2iuryg/gvcf.tfrecord@16.gz"". I0418 11:23:10.756783 140607260936000 postprocess_variants.py:1211] Using sample name from call_variants output. Sample name: 15337_Control. 2024-04-18 11:23:10.761841: I deepvariant/postprocess_variants.cc:94] Read from: /tmp/tmpba2iuryg/call_variants_output-00000-of-00001.tfrecord.gz. 2024-04-18 11:23:39.285004: I deepvariant/postprocess_variants.cc:109] Total #entries in single_site_calls = 8753635. I0418 11:24:21.877126 140607260936000 postprocess_variants.py:1313] CVO sorting took 1.1852648933728536 minutes. I0418 11:24:21.877437 140607260936000 postprocess_variants.py:1316] Transforming call_variants_output to variants. I0418 11:24:21.877472 140607260936000 postprocess_variants.py:1318] Using 16 CPUs for parallelization of variant transformation. I0418 11:24:35.259627 140607260936000 postprocess_variants.py:1211] Using sample name from call_variants output. Sample name: 15337_Control. real	2m10.376s. user	1m41.800s. sys	0m24.640s. ```. I would expect `postprocess_variants` to be doing more. I believe @lucasbrambrink is looking into another issue in `postprocess_variants` related to multiprocessing (https://github.com/google/deepvariant/issues/804), but I'm not sure if this relevant. For now, can you try disable multiprocessing in the `postprocess_variants` step by setting `--cpus 0`. If you're using the run_deepvariant one-step script, you can add:. ```bash. --postprocess_variants_extra_args=""cpus=0"". ```. and see if that works for you. Please let us know. We'll try to make this more robust in the future!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/810
https://github.com/google/deepvariant/issues/810:1751,performance,cpu,cpus,1751,"Thank you @githubtefo for the log. The last step certainly looks strange to me:. ```. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""/reference/Homo_sapiens.GRCh37.dna.primary_assembly.fa"" --infile ""/tmp/tmpba2iuryg/call_variants_output.tfrecord.gz"" --outfile ""/output/output.vcf.gz"" --cpus ""16"" --gvcf_outfile ""/output/output.g.vcf.gz"" --nonvariant_site_tfrecord_path ""/tmp/tmpba2iuryg/gvcf.tfrecord@16.gz"". I0418 11:23:10.756783 140607260936000 postprocess_variants.py:1211] Using sample name from call_variants output. Sample name: 15337_Control. 2024-04-18 11:23:10.761841: I deepvariant/postprocess_variants.cc:94] Read from: /tmp/tmpba2iuryg/call_variants_output-00000-of-00001.tfrecord.gz. 2024-04-18 11:23:39.285004: I deepvariant/postprocess_variants.cc:109] Total #entries in single_site_calls = 8753635. I0418 11:24:21.877126 140607260936000 postprocess_variants.py:1313] CVO sorting took 1.1852648933728536 minutes. I0418 11:24:21.877437 140607260936000 postprocess_variants.py:1316] Transforming call_variants_output to variants. I0418 11:24:21.877472 140607260936000 postprocess_variants.py:1318] Using 16 CPUs for parallelization of variant transformation. I0418 11:24:35.259627 140607260936000 postprocess_variants.py:1211] Using sample name from call_variants output. Sample name: 15337_Control. real	2m10.376s. user	1m41.800s. sys	0m24.640s. ```. I would expect `postprocess_variants` to be doing more. I believe @lucasbrambrink is looking into another issue in `postprocess_variants` related to multiprocessing (https://github.com/google/deepvariant/issues/804), but I'm not sure if this relevant. For now, can you try disable multiprocessing in the `postprocess_variants` step by setting `--cpus 0`. If you're using the run_deepvariant one-step script, you can add:. ```bash. --postprocess_variants_extra_args=""cpus=0"". ```. and see if that works for you. Please let us know. We'll try to make this more robust in the future!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/810
https://github.com/google/deepvariant/issues/810:1871,performance,cpu,cpus,1871,"Thank you @githubtefo for the log. The last step certainly looks strange to me:. ```. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""/reference/Homo_sapiens.GRCh37.dna.primary_assembly.fa"" --infile ""/tmp/tmpba2iuryg/call_variants_output.tfrecord.gz"" --outfile ""/output/output.vcf.gz"" --cpus ""16"" --gvcf_outfile ""/output/output.g.vcf.gz"" --nonvariant_site_tfrecord_path ""/tmp/tmpba2iuryg/gvcf.tfrecord@16.gz"". I0418 11:23:10.756783 140607260936000 postprocess_variants.py:1211] Using sample name from call_variants output. Sample name: 15337_Control. 2024-04-18 11:23:10.761841: I deepvariant/postprocess_variants.cc:94] Read from: /tmp/tmpba2iuryg/call_variants_output-00000-of-00001.tfrecord.gz. 2024-04-18 11:23:39.285004: I deepvariant/postprocess_variants.cc:109] Total #entries in single_site_calls = 8753635. I0418 11:24:21.877126 140607260936000 postprocess_variants.py:1313] CVO sorting took 1.1852648933728536 minutes. I0418 11:24:21.877437 140607260936000 postprocess_variants.py:1316] Transforming call_variants_output to variants. I0418 11:24:21.877472 140607260936000 postprocess_variants.py:1318] Using 16 CPUs for parallelization of variant transformation. I0418 11:24:35.259627 140607260936000 postprocess_variants.py:1211] Using sample name from call_variants output. Sample name: 15337_Control. real	2m10.376s. user	1m41.800s. sys	0m24.640s. ```. I would expect `postprocess_variants` to be doing more. I believe @lucasbrambrink is looking into another issue in `postprocess_variants` related to multiprocessing (https://github.com/google/deepvariant/issues/804), but I'm not sure if this relevant. For now, can you try disable multiprocessing in the `postprocess_variants` step by setting `--cpus 0`. If you're using the run_deepvariant one-step script, you can add:. ```bash. --postprocess_variants_extra_args=""cpus=0"". ```. and see if that works for you. Please let us know. We'll try to make this more robust in the future!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/810
https://github.com/google/deepvariant/issues/810:1964,reliability,robust,robust,1964,"Thank you @githubtefo for the log. The last step certainly looks strange to me:. ```. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""/reference/Homo_sapiens.GRCh37.dna.primary_assembly.fa"" --infile ""/tmp/tmpba2iuryg/call_variants_output.tfrecord.gz"" --outfile ""/output/output.vcf.gz"" --cpus ""16"" --gvcf_outfile ""/output/output.g.vcf.gz"" --nonvariant_site_tfrecord_path ""/tmp/tmpba2iuryg/gvcf.tfrecord@16.gz"". I0418 11:23:10.756783 140607260936000 postprocess_variants.py:1211] Using sample name from call_variants output. Sample name: 15337_Control. 2024-04-18 11:23:10.761841: I deepvariant/postprocess_variants.cc:94] Read from: /tmp/tmpba2iuryg/call_variants_output-00000-of-00001.tfrecord.gz. 2024-04-18 11:23:39.285004: I deepvariant/postprocess_variants.cc:109] Total #entries in single_site_calls = 8753635. I0418 11:24:21.877126 140607260936000 postprocess_variants.py:1313] CVO sorting took 1.1852648933728536 minutes. I0418 11:24:21.877437 140607260936000 postprocess_variants.py:1316] Transforming call_variants_output to variants. I0418 11:24:21.877472 140607260936000 postprocess_variants.py:1318] Using 16 CPUs for parallelization of variant transformation. I0418 11:24:35.259627 140607260936000 postprocess_variants.py:1211] Using sample name from call_variants output. Sample name: 15337_Control. real	2m10.376s. user	1m41.800s. sys	0m24.640s. ```. I would expect `postprocess_variants` to be doing more. I believe @lucasbrambrink is looking into another issue in `postprocess_variants` related to multiprocessing (https://github.com/google/deepvariant/issues/804), but I'm not sure if this relevant. For now, can you try disable multiprocessing in the `postprocess_variants` step by setting `--cpus 0`. If you're using the run_deepvariant one-step script, you can add:. ```bash. --postprocess_variants_extra_args=""cpus=0"". ```. and see if that works for you. Please let us know. We'll try to make this more robust in the future!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/810
https://github.com/google/deepvariant/issues/810:30,safety,log,log,30,"Thank you @githubtefo for the log. The last step certainly looks strange to me:. ```. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""/reference/Homo_sapiens.GRCh37.dna.primary_assembly.fa"" --infile ""/tmp/tmpba2iuryg/call_variants_output.tfrecord.gz"" --outfile ""/output/output.vcf.gz"" --cpus ""16"" --gvcf_outfile ""/output/output.g.vcf.gz"" --nonvariant_site_tfrecord_path ""/tmp/tmpba2iuryg/gvcf.tfrecord@16.gz"". I0418 11:23:10.756783 140607260936000 postprocess_variants.py:1211] Using sample name from call_variants output. Sample name: 15337_Control. 2024-04-18 11:23:10.761841: I deepvariant/postprocess_variants.cc:94] Read from: /tmp/tmpba2iuryg/call_variants_output-00000-of-00001.tfrecord.gz. 2024-04-18 11:23:39.285004: I deepvariant/postprocess_variants.cc:109] Total #entries in single_site_calls = 8753635. I0418 11:24:21.877126 140607260936000 postprocess_variants.py:1313] CVO sorting took 1.1852648933728536 minutes. I0418 11:24:21.877437 140607260936000 postprocess_variants.py:1316] Transforming call_variants_output to variants. I0418 11:24:21.877472 140607260936000 postprocess_variants.py:1318] Using 16 CPUs for parallelization of variant transformation. I0418 11:24:35.259627 140607260936000 postprocess_variants.py:1211] Using sample name from call_variants output. Sample name: 15337_Control. real	2m10.376s. user	1m41.800s. sys	0m24.640s. ```. I would expect `postprocess_variants` to be doing more. I believe @lucasbrambrink is looking into another issue in `postprocess_variants` related to multiprocessing (https://github.com/google/deepvariant/issues/804), but I'm not sure if this relevant. For now, can you try disable multiprocessing in the `postprocess_variants` step by setting `--cpus 0`. If you're using the run_deepvariant one-step script, you can add:. ```bash. --postprocess_variants_extra_args=""cpus=0"". ```. and see if that works for you. Please let us know. We'll try to make this more robust in the future!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/810
https://github.com/google/deepvariant/issues/810:1964,safety,robust,robust,1964,"Thank you @githubtefo for the log. The last step certainly looks strange to me:. ```. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""/reference/Homo_sapiens.GRCh37.dna.primary_assembly.fa"" --infile ""/tmp/tmpba2iuryg/call_variants_output.tfrecord.gz"" --outfile ""/output/output.vcf.gz"" --cpus ""16"" --gvcf_outfile ""/output/output.g.vcf.gz"" --nonvariant_site_tfrecord_path ""/tmp/tmpba2iuryg/gvcf.tfrecord@16.gz"". I0418 11:23:10.756783 140607260936000 postprocess_variants.py:1211] Using sample name from call_variants output. Sample name: 15337_Control. 2024-04-18 11:23:10.761841: I deepvariant/postprocess_variants.cc:94] Read from: /tmp/tmpba2iuryg/call_variants_output-00000-of-00001.tfrecord.gz. 2024-04-18 11:23:39.285004: I deepvariant/postprocess_variants.cc:109] Total #entries in single_site_calls = 8753635. I0418 11:24:21.877126 140607260936000 postprocess_variants.py:1313] CVO sorting took 1.1852648933728536 minutes. I0418 11:24:21.877437 140607260936000 postprocess_variants.py:1316] Transforming call_variants_output to variants. I0418 11:24:21.877472 140607260936000 postprocess_variants.py:1318] Using 16 CPUs for parallelization of variant transformation. I0418 11:24:35.259627 140607260936000 postprocess_variants.py:1211] Using sample name from call_variants output. Sample name: 15337_Control. real	2m10.376s. user	1m41.800s. sys	0m24.640s. ```. I would expect `postprocess_variants` to be doing more. I believe @lucasbrambrink is looking into another issue in `postprocess_variants` related to multiprocessing (https://github.com/google/deepvariant/issues/804), but I'm not sure if this relevant. For now, can you try disable multiprocessing in the `postprocess_variants` step by setting `--cpus 0`. If you're using the run_deepvariant one-step script, you can add:. ```bash. --postprocess_variants_extra_args=""cpus=0"". ```. and see if that works for you. Please let us know. We'll try to make this more robust in the future!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/810
https://github.com/google/deepvariant/issues/810:30,security,log,log,30,"Thank you @githubtefo for the log. The last step certainly looks strange to me:. ```. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""/reference/Homo_sapiens.GRCh37.dna.primary_assembly.fa"" --infile ""/tmp/tmpba2iuryg/call_variants_output.tfrecord.gz"" --outfile ""/output/output.vcf.gz"" --cpus ""16"" --gvcf_outfile ""/output/output.g.vcf.gz"" --nonvariant_site_tfrecord_path ""/tmp/tmpba2iuryg/gvcf.tfrecord@16.gz"". I0418 11:23:10.756783 140607260936000 postprocess_variants.py:1211] Using sample name from call_variants output. Sample name: 15337_Control. 2024-04-18 11:23:10.761841: I deepvariant/postprocess_variants.cc:94] Read from: /tmp/tmpba2iuryg/call_variants_output-00000-of-00001.tfrecord.gz. 2024-04-18 11:23:39.285004: I deepvariant/postprocess_variants.cc:109] Total #entries in single_site_calls = 8753635. I0418 11:24:21.877126 140607260936000 postprocess_variants.py:1313] CVO sorting took 1.1852648933728536 minutes. I0418 11:24:21.877437 140607260936000 postprocess_variants.py:1316] Transforming call_variants_output to variants. I0418 11:24:21.877472 140607260936000 postprocess_variants.py:1318] Using 16 CPUs for parallelization of variant transformation. I0418 11:24:35.259627 140607260936000 postprocess_variants.py:1211] Using sample name from call_variants output. Sample name: 15337_Control. real	2m10.376s. user	1m41.800s. sys	0m24.640s. ```. I would expect `postprocess_variants` to be doing more. I believe @lucasbrambrink is looking into another issue in `postprocess_variants` related to multiprocessing (https://github.com/google/deepvariant/issues/804), but I'm not sure if this relevant. For now, can you try disable multiprocessing in the `postprocess_variants` step by setting `--cpus 0`. If you're using the run_deepvariant one-step script, you can add:. ```bash. --postprocess_variants_extra_args=""cpus=0"". ```. and see if that works for you. Please let us know. We'll try to make this more robust in the future!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/810
https://github.com/google/deepvariant/issues/810:30,testability,log,log,30,"Thank you @githubtefo for the log. The last step certainly looks strange to me:. ```. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""/reference/Homo_sapiens.GRCh37.dna.primary_assembly.fa"" --infile ""/tmp/tmpba2iuryg/call_variants_output.tfrecord.gz"" --outfile ""/output/output.vcf.gz"" --cpus ""16"" --gvcf_outfile ""/output/output.g.vcf.gz"" --nonvariant_site_tfrecord_path ""/tmp/tmpba2iuryg/gvcf.tfrecord@16.gz"". I0418 11:23:10.756783 140607260936000 postprocess_variants.py:1211] Using sample name from call_variants output. Sample name: 15337_Control. 2024-04-18 11:23:10.761841: I deepvariant/postprocess_variants.cc:94] Read from: /tmp/tmpba2iuryg/call_variants_output-00000-of-00001.tfrecord.gz. 2024-04-18 11:23:39.285004: I deepvariant/postprocess_variants.cc:109] Total #entries in single_site_calls = 8753635. I0418 11:24:21.877126 140607260936000 postprocess_variants.py:1313] CVO sorting took 1.1852648933728536 minutes. I0418 11:24:21.877437 140607260936000 postprocess_variants.py:1316] Transforming call_variants_output to variants. I0418 11:24:21.877472 140607260936000 postprocess_variants.py:1318] Using 16 CPUs for parallelization of variant transformation. I0418 11:24:35.259627 140607260936000 postprocess_variants.py:1211] Using sample name from call_variants output. Sample name: 15337_Control. real	2m10.376s. user	1m41.800s. sys	0m24.640s. ```. I would expect `postprocess_variants` to be doing more. I believe @lucasbrambrink is looking into another issue in `postprocess_variants` related to multiprocessing (https://github.com/google/deepvariant/issues/804), but I'm not sure if this relevant. For now, can you try disable multiprocessing in the `postprocess_variants` step by setting `--cpus 0`. If you're using the run_deepvariant one-step script, you can add:. ```bash. --postprocess_variants_extra_args=""cpus=0"". ```. and see if that works for you. Please let us know. We'll try to make this more robust in the future!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/810
https://github.com/google/deepvariant/issues/810:104,usability,command,command,104,"Thank you @githubtefo for the log. The last step certainly looks strange to me:. ```. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""/reference/Homo_sapiens.GRCh37.dna.primary_assembly.fa"" --infile ""/tmp/tmpba2iuryg/call_variants_output.tfrecord.gz"" --outfile ""/output/output.vcf.gz"" --cpus ""16"" --gvcf_outfile ""/output/output.g.vcf.gz"" --nonvariant_site_tfrecord_path ""/tmp/tmpba2iuryg/gvcf.tfrecord@16.gz"". I0418 11:23:10.756783 140607260936000 postprocess_variants.py:1211] Using sample name from call_variants output. Sample name: 15337_Control. 2024-04-18 11:23:10.761841: I deepvariant/postprocess_variants.cc:94] Read from: /tmp/tmpba2iuryg/call_variants_output-00000-of-00001.tfrecord.gz. 2024-04-18 11:23:39.285004: I deepvariant/postprocess_variants.cc:109] Total #entries in single_site_calls = 8753635. I0418 11:24:21.877126 140607260936000 postprocess_variants.py:1313] CVO sorting took 1.1852648933728536 minutes. I0418 11:24:21.877437 140607260936000 postprocess_variants.py:1316] Transforming call_variants_output to variants. I0418 11:24:21.877472 140607260936000 postprocess_variants.py:1318] Using 16 CPUs for parallelization of variant transformation. I0418 11:24:35.259627 140607260936000 postprocess_variants.py:1211] Using sample name from call_variants output. Sample name: 15337_Control. real	2m10.376s. user	1m41.800s. sys	0m24.640s. ```. I would expect `postprocess_variants` to be doing more. I believe @lucasbrambrink is looking into another issue in `postprocess_variants` related to multiprocessing (https://github.com/google/deepvariant/issues/804), but I'm not sure if this relevant. For now, can you try disable multiprocessing in the `postprocess_variants` step by setting `--cpus 0`. If you're using the run_deepvariant one-step script, you can add:. ```bash. --postprocess_variants_extra_args=""cpus=0"". ```. and see if that works for you. Please let us know. We'll try to make this more robust in the future!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/810
https://github.com/google/deepvariant/issues/810:1369,usability,user,user,1369,"Thank you @githubtefo for the log. The last step certainly looks strange to me:. ```. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""/reference/Homo_sapiens.GRCh37.dna.primary_assembly.fa"" --infile ""/tmp/tmpba2iuryg/call_variants_output.tfrecord.gz"" --outfile ""/output/output.vcf.gz"" --cpus ""16"" --gvcf_outfile ""/output/output.g.vcf.gz"" --nonvariant_site_tfrecord_path ""/tmp/tmpba2iuryg/gvcf.tfrecord@16.gz"". I0418 11:23:10.756783 140607260936000 postprocess_variants.py:1211] Using sample name from call_variants output. Sample name: 15337_Control. 2024-04-18 11:23:10.761841: I deepvariant/postprocess_variants.cc:94] Read from: /tmp/tmpba2iuryg/call_variants_output-00000-of-00001.tfrecord.gz. 2024-04-18 11:23:39.285004: I deepvariant/postprocess_variants.cc:109] Total #entries in single_site_calls = 8753635. I0418 11:24:21.877126 140607260936000 postprocess_variants.py:1313] CVO sorting took 1.1852648933728536 minutes. I0418 11:24:21.877437 140607260936000 postprocess_variants.py:1316] Transforming call_variants_output to variants. I0418 11:24:21.877472 140607260936000 postprocess_variants.py:1318] Using 16 CPUs for parallelization of variant transformation. I0418 11:24:35.259627 140607260936000 postprocess_variants.py:1211] Using sample name from call_variants output. Sample name: 15337_Control. real	2m10.376s. user	1m41.800s. sys	0m24.640s. ```. I would expect `postprocess_variants` to be doing more. I believe @lucasbrambrink is looking into another issue in `postprocess_variants` related to multiprocessing (https://github.com/google/deepvariant/issues/804), but I'm not sure if this relevant. For now, can you try disable multiprocessing in the `postprocess_variants` step by setting `--cpus 0`. If you're using the run_deepvariant one-step script, you can add:. ```bash. --postprocess_variants_extra_args=""cpus=0"". ```. and see if that works for you. Please let us know. We'll try to make this more robust in the future!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/810
https://github.com/google/deepvariant/issues/810:654,deployability,releas,releases,654,"@githubtefo It might be worth trying a smaller chromosome (like chr20, or try Quick Start) to make sure that things work end-to-end on your machine first. If you use `--postprocess_variants_extra_args=""cpus=0""` , it's only the last step (`postprocess_variants`) that will be without multiprocessing. That step takes < 1hr for our PacBio BAM - you can see in https://github.com/google/deepvariant/blob/r1.5/docs/metrics.md#runtime-2 (this is before multiprocessing was used in postprocess_variants. @githubtefo We understand that speeding up DeepVariant is very important. We're actively making improvements! Thank you for reporting the issue. Our future releases will be better because of your feedback!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/810
https://github.com/google/deepvariant/issues/810:202,energy efficiency,cpu,cpus,202,"@githubtefo It might be worth trying a smaller chromosome (like chr20, or try Quick Start) to make sure that things work end-to-end on your machine first. If you use `--postprocess_variants_extra_args=""cpus=0""` , it's only the last step (`postprocess_variants`) that will be without multiprocessing. That step takes < 1hr for our PacBio BAM - you can see in https://github.com/google/deepvariant/blob/r1.5/docs/metrics.md#runtime-2 (this is before multiprocessing was used in postprocess_variants. @githubtefo We understand that speeding up DeepVariant is very important. We're actively making improvements! Thank you for reporting the issue. Our future releases will be better because of your feedback!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/810
https://github.com/google/deepvariant/issues/810:330,modifiability,Pac,PacBio,330,"@githubtefo It might be worth trying a smaller chromosome (like chr20, or try Quick Start) to make sure that things work end-to-end on your machine first. If you use `--postprocess_variants_extra_args=""cpus=0""` , it's only the last step (`postprocess_variants`) that will be without multiprocessing. That step takes < 1hr for our PacBio BAM - you can see in https://github.com/google/deepvariant/blob/r1.5/docs/metrics.md#runtime-2 (this is before multiprocessing was used in postprocess_variants. @githubtefo We understand that speeding up DeepVariant is very important. We're actively making improvements! Thank you for reporting the issue. Our future releases will be better because of your feedback!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/810
https://github.com/google/deepvariant/issues/810:202,performance,cpu,cpus,202,"@githubtefo It might be worth trying a smaller chromosome (like chr20, or try Quick Start) to make sure that things work end-to-end on your machine first. If you use `--postprocess_variants_extra_args=""cpus=0""` , it's only the last step (`postprocess_variants`) that will be without multiprocessing. That step takes < 1hr for our PacBio BAM - you can see in https://github.com/google/deepvariant/blob/r1.5/docs/metrics.md#runtime-2 (this is before multiprocessing was used in postprocess_variants. @githubtefo We understand that speeding up DeepVariant is very important. We're actively making improvements! Thank you for reporting the issue. Our future releases will be better because of your feedback!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/810
https://github.com/google/deepvariant/issues/810:513,testability,understand,understand,513,"@githubtefo It might be worth trying a smaller chromosome (like chr20, or try Quick Start) to make sure that things work end-to-end on your machine first. If you use `--postprocess_variants_extra_args=""cpus=0""` , it's only the last step (`postprocess_variants`) that will be without multiprocessing. That step takes < 1hr for our PacBio BAM - you can see in https://github.com/google/deepvariant/blob/r1.5/docs/metrics.md#runtime-2 (this is before multiprocessing was used in postprocess_variants. @githubtefo We understand that speeding up DeepVariant is very important. We're actively making improvements! Thank you for reporting the issue. Our future releases will be better because of your feedback!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/810
https://github.com/google/deepvariant/issues/810:694,usability,feedback,feedback,694,"@githubtefo It might be worth trying a smaller chromosome (like chr20, or try Quick Start) to make sure that things work end-to-end on your machine first. If you use `--postprocess_variants_extra_args=""cpus=0""` , it's only the last step (`postprocess_variants`) that will be without multiprocessing. That step takes < 1hr for our PacBio BAM - you can see in https://github.com/google/deepvariant/blob/r1.5/docs/metrics.md#runtime-2 (this is before multiprocessing was used in postprocess_variants. @githubtefo We understand that speeding up DeepVariant is very important. We're actively making improvements! Thank you for reporting the issue. Our future releases will be better because of your feedback!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/810
https://github.com/google/deepvariant/issues/810:141,deployability,fail,failed,141,"Thanks, @pichuan. Always happy to contribute to the open source community. I ran twice the command with the new argument and in bot cases it failed :( the external hard drive where I have allocated the bam file got ejected and a. [output2.log](https://github.com/google/deepvariant/files/15167067/output2.log). lso I noticed that the syslog and kern.log became insanely big (~200GB), leaving me with no extra disk space. Any ideas what might be going on?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/810
https://github.com/google/deepvariant/issues/810:239,deployability,log,log,239,"Thanks, @pichuan. Always happy to contribute to the open source community. I ran twice the command with the new argument and in bot cases it failed :( the external hard drive where I have allocated the bam file got ejected and a. [output2.log](https://github.com/google/deepvariant/files/15167067/output2.log). lso I noticed that the syslog and kern.log became insanely big (~200GB), leaving me with no extra disk space. Any ideas what might be going on?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/810
https://github.com/google/deepvariant/issues/810:305,deployability,log,log,305,"Thanks, @pichuan. Always happy to contribute to the open source community. I ran twice the command with the new argument and in bot cases it failed :( the external hard drive where I have allocated the bam file got ejected and a. [output2.log](https://github.com/google/deepvariant/files/15167067/output2.log). lso I noticed that the syslog and kern.log became insanely big (~200GB), leaving me with no extra disk space. Any ideas what might be going on?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/810
https://github.com/google/deepvariant/issues/810:350,deployability,log,log,350,"Thanks, @pichuan. Always happy to contribute to the open source community. I ran twice the command with the new argument and in bot cases it failed :( the external hard drive where I have allocated the bam file got ejected and a. [output2.log](https://github.com/google/deepvariant/files/15167067/output2.log). lso I noticed that the syslog and kern.log became insanely big (~200GB), leaving me with no extra disk space. Any ideas what might be going on?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/810
https://github.com/google/deepvariant/issues/810:188,energy efficiency,alloc,allocated,188,"Thanks, @pichuan. Always happy to contribute to the open source community. I ran twice the command with the new argument and in bot cases it failed :( the external hard drive where I have allocated the bam file got ejected and a. [output2.log](https://github.com/google/deepvariant/files/15167067/output2.log). lso I noticed that the syslog and kern.log became insanely big (~200GB), leaving me with no extra disk space. Any ideas what might be going on?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/810
https://github.com/google/deepvariant/issues/810:409,performance,disk,disk,409,"Thanks, @pichuan. Always happy to contribute to the open source community. I ran twice the command with the new argument and in bot cases it failed :( the external hard drive where I have allocated the bam file got ejected and a. [output2.log](https://github.com/google/deepvariant/files/15167067/output2.log). lso I noticed that the syslog and kern.log became insanely big (~200GB), leaving me with no extra disk space. Any ideas what might be going on?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/810
https://github.com/google/deepvariant/issues/810:141,reliability,fail,failed,141,"Thanks, @pichuan. Always happy to contribute to the open source community. I ran twice the command with the new argument and in bot cases it failed :( the external hard drive where I have allocated the bam file got ejected and a. [output2.log](https://github.com/google/deepvariant/files/15167067/output2.log). lso I noticed that the syslog and kern.log became insanely big (~200GB), leaving me with no extra disk space. Any ideas what might be going on?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/810
https://github.com/google/deepvariant/issues/810:239,safety,log,log,239,"Thanks, @pichuan. Always happy to contribute to the open source community. I ran twice the command with the new argument and in bot cases it failed :( the external hard drive where I have allocated the bam file got ejected and a. [output2.log](https://github.com/google/deepvariant/files/15167067/output2.log). lso I noticed that the syslog and kern.log became insanely big (~200GB), leaving me with no extra disk space. Any ideas what might be going on?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/810
https://github.com/google/deepvariant/issues/810:305,safety,log,log,305,"Thanks, @pichuan. Always happy to contribute to the open source community. I ran twice the command with the new argument and in bot cases it failed :( the external hard drive where I have allocated the bam file got ejected and a. [output2.log](https://github.com/google/deepvariant/files/15167067/output2.log). lso I noticed that the syslog and kern.log became insanely big (~200GB), leaving me with no extra disk space. Any ideas what might be going on?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/810
https://github.com/google/deepvariant/issues/810:350,safety,log,log,350,"Thanks, @pichuan. Always happy to contribute to the open source community. I ran twice the command with the new argument and in bot cases it failed :( the external hard drive where I have allocated the bam file got ejected and a. [output2.log](https://github.com/google/deepvariant/files/15167067/output2.log). lso I noticed that the syslog and kern.log became insanely big (~200GB), leaving me with no extra disk space. Any ideas what might be going on?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/810
https://github.com/google/deepvariant/issues/810:239,security,log,log,239,"Thanks, @pichuan. Always happy to contribute to the open source community. I ran twice the command with the new argument and in bot cases it failed :( the external hard drive where I have allocated the bam file got ejected and a. [output2.log](https://github.com/google/deepvariant/files/15167067/output2.log). lso I noticed that the syslog and kern.log became insanely big (~200GB), leaving me with no extra disk space. Any ideas what might be going on?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/810
https://github.com/google/deepvariant/issues/810:305,security,log,log,305,"Thanks, @pichuan. Always happy to contribute to the open source community. I ran twice the command with the new argument and in bot cases it failed :( the external hard drive where I have allocated the bam file got ejected and a. [output2.log](https://github.com/google/deepvariant/files/15167067/output2.log). lso I noticed that the syslog and kern.log became insanely big (~200GB), leaving me with no extra disk space. Any ideas what might be going on?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/810
https://github.com/google/deepvariant/issues/810:350,security,log,log,350,"Thanks, @pichuan. Always happy to contribute to the open source community. I ran twice the command with the new argument and in bot cases it failed :( the external hard drive where I have allocated the bam file got ejected and a. [output2.log](https://github.com/google/deepvariant/files/15167067/output2.log). lso I noticed that the syslog and kern.log became insanely big (~200GB), leaving me with no extra disk space. Any ideas what might be going on?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/810
https://github.com/google/deepvariant/issues/810:239,testability,log,log,239,"Thanks, @pichuan. Always happy to contribute to the open source community. I ran twice the command with the new argument and in bot cases it failed :( the external hard drive where I have allocated the bam file got ejected and a. [output2.log](https://github.com/google/deepvariant/files/15167067/output2.log). lso I noticed that the syslog and kern.log became insanely big (~200GB), leaving me with no extra disk space. Any ideas what might be going on?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/810
https://github.com/google/deepvariant/issues/810:305,testability,log,log,305,"Thanks, @pichuan. Always happy to contribute to the open source community. I ran twice the command with the new argument and in bot cases it failed :( the external hard drive where I have allocated the bam file got ejected and a. [output2.log](https://github.com/google/deepvariant/files/15167067/output2.log). lso I noticed that the syslog and kern.log became insanely big (~200GB), leaving me with no extra disk space. Any ideas what might be going on?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/810
https://github.com/google/deepvariant/issues/810:350,testability,log,log,350,"Thanks, @pichuan. Always happy to contribute to the open source community. I ran twice the command with the new argument and in bot cases it failed :( the external hard drive where I have allocated the bam file got ejected and a. [output2.log](https://github.com/google/deepvariant/files/15167067/output2.log). lso I noticed that the syslog and kern.log became insanely big (~200GB), leaving me with no extra disk space. Any ideas what might be going on?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/810
https://github.com/google/deepvariant/issues/810:91,usability,command,command,91,"Thanks, @pichuan. Always happy to contribute to the open source community. I ran twice the command with the new argument and in bot cases it failed :( the external hard drive where I have allocated the bam file got ejected and a. [output2.log](https://github.com/google/deepvariant/files/15167067/output2.log). lso I noticed that the syslog and kern.log became insanely big (~200GB), leaving me with no extra disk space. Any ideas what might be going on?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/810
https://github.com/google/deepvariant/issues/810:31,deployability,resourc,resource,31,"Hi @githubtefo , if the system resource is limited, I suggested that it's easier to split by chromosome. For example, start with `--regions chr1` in your first run, then chr2, etc etc. This way, in each run you'll get a VCF for each chromosome, and at the end you can still have all the calls for the whole genome. This is a bit manual, but if you do that, you'll also be able to parallelize if you have multiple machines to run on. You can also start with a smaller chromosome, for example you can start with `--regions chr22` and work your way back. That's probably even better because you can make sure the smaller chromosomes work before you try the larger ones. Let me know if this works!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/810
https://github.com/google/deepvariant/issues/810:31,energy efficiency,resourc,resource,31,"Hi @githubtefo , if the system resource is limited, I suggested that it's easier to split by chromosome. For example, start with `--regions chr1` in your first run, then chr2, etc etc. This way, in each run you'll get a VCF for each chromosome, and at the end you can still have all the calls for the whole genome. This is a bit manual, but if you do that, you'll also be able to parallelize if you have multiple machines to run on. You can also start with a smaller chromosome, for example you can start with `--regions chr22` and work your way back. That's probably even better because you can make sure the smaller chromosomes work before you try the larger ones. Let me know if this works!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/810
https://github.com/google/deepvariant/issues/810:31,performance,resourc,resource,31,"Hi @githubtefo , if the system resource is limited, I suggested that it's easier to split by chromosome. For example, start with `--regions chr1` in your first run, then chr2, etc etc. This way, in each run you'll get a VCF for each chromosome, and at the end you can still have all the calls for the whole genome. This is a bit manual, but if you do that, you'll also be able to parallelize if you have multiple machines to run on. You can also start with a smaller chromosome, for example you can start with `--regions chr22` and work your way back. That's probably even better because you can make sure the smaller chromosomes work before you try the larger ones. Let me know if this works!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/810
https://github.com/google/deepvariant/issues/810:380,performance,parallel,parallelize,380,"Hi @githubtefo , if the system resource is limited, I suggested that it's easier to split by chromosome. For example, start with `--regions chr1` in your first run, then chr2, etc etc. This way, in each run you'll get a VCF for each chromosome, and at the end you can still have all the calls for the whole genome. This is a bit manual, but if you do that, you'll also be able to parallelize if you have multiple machines to run on. You can also start with a smaller chromosome, for example you can start with `--regions chr22` and work your way back. That's probably even better because you can make sure the smaller chromosomes work before you try the larger ones. Let me know if this works!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/810
https://github.com/google/deepvariant/issues/810:31,safety,resourc,resource,31,"Hi @githubtefo , if the system resource is limited, I suggested that it's easier to split by chromosome. For example, start with `--regions chr1` in your first run, then chr2, etc etc. This way, in each run you'll get a VCF for each chromosome, and at the end you can still have all the calls for the whole genome. This is a bit manual, but if you do that, you'll also be able to parallelize if you have multiple machines to run on. You can also start with a smaller chromosome, for example you can start with `--regions chr22` and work your way back. That's probably even better because you can make sure the smaller chromosomes work before you try the larger ones. Let me know if this works!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/810
https://github.com/google/deepvariant/issues/810:31,testability,resourc,resource,31,"Hi @githubtefo , if the system resource is limited, I suggested that it's easier to split by chromosome. For example, start with `--regions chr1` in your first run, then chr2, etc etc. This way, in each run you'll get a VCF for each chromosome, and at the end you can still have all the calls for the whole genome. This is a bit manual, but if you do that, you'll also be able to parallelize if you have multiple machines to run on. You can also start with a smaller chromosome, for example you can start with `--regions chr22` and work your way back. That's probably even better because you can make sure the smaller chromosomes work before you try the larger ones. Let me know if this works!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/810
https://github.com/google/deepvariant/issues/811:25,deployability,updat,update,25,"Hi @ASLeonard ,. A quick update: We think this is because we need to adjust our PL in our gVCF as well. I'm working on a code change, but it might take a bit longer than I thought. I just want to let you know that this is still in my queue!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/811
https://github.com/google/deepvariant/issues/811:234,integrability,queue,queue,234,"Hi @ASLeonard ,. A quick update: We think this is because we need to adjust our PL in our gVCF as well. I'm working on a code change, but it might take a bit longer than I thought. I just want to let you know that this is still in my queue!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/811
https://github.com/google/deepvariant/issues/811:234,performance,queue,queue,234,"Hi @ASLeonard ,. A quick update: We think this is because we need to adjust our PL in our gVCF as well. I'm working on a code change, but it might take a bit longer than I thought. I just want to let you know that this is still in my queue!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/811
https://github.com/google/deepvariant/issues/811:25,safety,updat,update,25,"Hi @ASLeonard ,. A quick update: We think this is because we need to adjust our PL in our gVCF as well. I'm working on a code change, but it might take a bit longer than I thought. I just want to let you know that this is still in my queue!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/811
https://github.com/google/deepvariant/issues/811:25,security,updat,update,25,"Hi @ASLeonard ,. A quick update: We think this is because we need to adjust our PL in our gVCF as well. I'm working on a code change, but it might take a bit longer than I thought. I just want to let you know that this is still in my queue!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/811
https://github.com/google/deepvariant/issues/811:0,deployability,Updat,Update,0,"Update: Internally our team has been making other improvements to postprocess_variants, so I've not been actively looking into this issue. I'll plan to resume in the next few weeks.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/811
https://github.com/google/deepvariant/issues/811:0,safety,Updat,Update,0,"Update: Internally our team has been making other improvements to postprocess_variants, so I've not been actively looking into this issue. I'll plan to resume in the next few weeks.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/811
https://github.com/google/deepvariant/issues/811:0,security,Updat,Update,0,"Update: Internally our team has been making other improvements to postprocess_variants, so I've not been actively looking into this issue. I'll plan to resume in the next few weeks.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/811
https://github.com/google/deepvariant/issues/811:23,security,team,team,23,"Update: Internally our team has been making other improvements to postprocess_variants, so I've not been actively looking into this issue. I'll plan to resume in the next few weeks.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/811
https://github.com/google/deepvariant/issues/811:144,testability,plan,plan,144,"Update: Internally our team has been making other improvements to postprocess_variants, so I've not been actively looking into this issue. I'll plan to resume in the next few weeks.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/811
https://github.com/google/deepvariant/issues/811:152,usability,resum,resume,152,"Update: Internally our team has been making other improvements to postprocess_variants, so I've not been actively looking into this issue. I'll plan to resume in the next few weeks.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/811
https://github.com/google/deepvariant/issues/811:0,deployability,Updat,Update,0,"Update: I've done some investigation, and I think we need to change the logic in make_examples, specifically somewhere around here:. https://github.com/google/deepvariant/blob/r1.6.1/deepvariant/variant_caller.py#L207-L230. I've done a prototype but the behavior isn't quite what I expected yet. I'll continue to work on this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/811
https://github.com/google/deepvariant/issues/811:72,deployability,log,logic,72,"Update: I've done some investigation, and I think we need to change the logic in make_examples, specifically somewhere around here:. https://github.com/google/deepvariant/blob/r1.6.1/deepvariant/variant_caller.py#L207-L230. I've done a prototype but the behavior isn't quite what I expected yet. I'll continue to work on this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/811
https://github.com/google/deepvariant/issues/811:301,deployability,continu,continue,301,"Update: I've done some investigation, and I think we need to change the logic in make_examples, specifically somewhere around here:. https://github.com/google/deepvariant/blob/r1.6.1/deepvariant/variant_caller.py#L207-L230. I've done a prototype but the behavior isn't quite what I expected yet. I'll continue to work on this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/811
https://github.com/google/deepvariant/issues/811:96,interoperability,specif,specifically,96,"Update: I've done some investigation, and I think we need to change the logic in make_examples, specifically somewhere around here:. https://github.com/google/deepvariant/blob/r1.6.1/deepvariant/variant_caller.py#L207-L230. I've done a prototype but the behavior isn't quite what I expected yet. I'll continue to work on this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/811
https://github.com/google/deepvariant/issues/811:0,safety,Updat,Update,0,"Update: I've done some investigation, and I think we need to change the logic in make_examples, specifically somewhere around here:. https://github.com/google/deepvariant/blob/r1.6.1/deepvariant/variant_caller.py#L207-L230. I've done a prototype but the behavior isn't quite what I expected yet. I'll continue to work on this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/811
https://github.com/google/deepvariant/issues/811:72,safety,log,logic,72,"Update: I've done some investigation, and I think we need to change the logic in make_examples, specifically somewhere around here:. https://github.com/google/deepvariant/blob/r1.6.1/deepvariant/variant_caller.py#L207-L230. I've done a prototype but the behavior isn't quite what I expected yet. I'll continue to work on this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/811
https://github.com/google/deepvariant/issues/811:0,security,Updat,Update,0,"Update: I've done some investigation, and I think we need to change the logic in make_examples, specifically somewhere around here:. https://github.com/google/deepvariant/blob/r1.6.1/deepvariant/variant_caller.py#L207-L230. I've done a prototype but the behavior isn't quite what I expected yet. I'll continue to work on this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/811
https://github.com/google/deepvariant/issues/811:72,security,log,logic,72,"Update: I've done some investigation, and I think we need to change the logic in make_examples, specifically somewhere around here:. https://github.com/google/deepvariant/blob/r1.6.1/deepvariant/variant_caller.py#L207-L230. I've done a prototype but the behavior isn't quite what I expected yet. I'll continue to work on this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/811
https://github.com/google/deepvariant/issues/811:72,testability,log,logic,72,"Update: I've done some investigation, and I think we need to change the logic in make_examples, specifically somewhere around here:. https://github.com/google/deepvariant/blob/r1.6.1/deepvariant/variant_caller.py#L207-L230. I've done a prototype but the behavior isn't quite what I expected yet. I'll continue to work on this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/811
https://github.com/google/deepvariant/issues/811:236,usability,prototyp,prototype,236,"Update: I've done some investigation, and I think we need to change the logic in make_examples, specifically somewhere around here:. https://github.com/google/deepvariant/blob/r1.6.1/deepvariant/variant_caller.py#L207-L230. I've done a prototype but the behavior isn't quite what I expected yet. I'll continue to work on this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/811
https://github.com/google/deepvariant/issues/811:254,usability,behavi,behavior,254,"Update: I've done some investigation, and I think we need to change the logic in make_examples, specifically somewhere around here:. https://github.com/google/deepvariant/blob/r1.6.1/deepvariant/variant_caller.py#L207-L230. I've done a prototype but the behavior isn't quite what I expected yet. I'll continue to work on this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/811
https://github.com/google/deepvariant/issues/812:36,deployability,instal,installing,36,"@Carl-labhub, in this case, you are installing `nucleus` as user but DeepVariant is installed differently? Can you provide the full command for DeepVariant?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:84,deployability,instal,installed,84,"@Carl-labhub, in this case, you are installing `nucleus` as user but DeepVariant is installed differently? Can you provide the full command for DeepVariant?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:60,usability,user,user,60,"@Carl-labhub, in this case, you are installing `nucleus` as user but DeepVariant is installed differently? Can you provide the full command for DeepVariant?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:132,usability,command,command,132,"@Carl-labhub, in this case, you are installing `nucleus` as user but DeepVariant is installed differently? Can you provide the full command for DeepVariant?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:449,availability,error,error,449,"The command for installing DeepVariant? . `singularity build DeepVariant_1.6.1.sif docker://google/deepvariant:1.6.1`. Or the full command that is written to stdout when DeepVariant runs? For this latter case, after installing nucleus the full command for deepvariant is not written to output. The last line of output is `KeyError: 'SerializedDType'`. I dont have the output saved from the test data run to retrieve the full command output with the error prior to installing nucleus as user, and will re-run that and update this comment with it in a few hours. I did, however, get the same error attempting to run deepvariant with my own data (prior to installing nucleus as user), and the output and command from that are below:. ```. for bam in $READS; do. 	echo ""running deepvariant on $bam"". 	run_deepvariant --model_type=PACBIO --ref=$REF --reads=$bam --output_vcf=$OUTDIR/$bam.vcf.gz --output_gvcf=$OUTDIR/$bam.g.vcf.gz --logging_dir=$LOGDIR --num_shards=$CORES. 	echo ""finished with $bam"". done. #output in block comment below. # running deepvariant on /work/cjm124/SWFst/VarCalling/reads/bc2001_aligned_sorted.bam. # 2024-04-23 11:42:51.281492: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA. # To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. # I0423 11:42:57.943745 140073410221888 run_deepvariant.py:519] Re-using the directory for intermediate results in /tmp/tmpkmab_2kw. # ***** Intermediate results will be written to /tmp/tmpkmab_2kw in docker. ****. # ***** Running the command:*****. # time seq 0 11 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/work/cjm124/SWFst/lvar3ref/Lvar_scaffolds.fasta"" --reads ""/work/cjm124/SWFst/VarCalling/reads/bc2001_aligned_sorted.bam"" --examples ""/tmp/tmpkmab_2kw/make_example",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:590,availability,error,error,590,"The command for installing DeepVariant? . `singularity build DeepVariant_1.6.1.sif docker://google/deepvariant:1.6.1`. Or the full command that is written to stdout when DeepVariant runs? For this latter case, after installing nucleus the full command for deepvariant is not written to output. The last line of output is `KeyError: 'SerializedDType'`. I dont have the output saved from the test data run to retrieve the full command output with the error prior to installing nucleus as user, and will re-run that and update this comment with it in a few hours. I did, however, get the same error attempting to run deepvariant with my own data (prior to installing nucleus as user), and the output and command from that are below:. ```. for bam in $READS; do. 	echo ""running deepvariant on $bam"". 	run_deepvariant --model_type=PACBIO --ref=$REF --reads=$bam --output_vcf=$OUTDIR/$bam.vcf.gz --output_gvcf=$OUTDIR/$bam.g.vcf.gz --logging_dir=$LOGDIR --num_shards=$CORES. 	echo ""finished with $bam"". done. #output in block comment below. # running deepvariant on /work/cjm124/SWFst/VarCalling/reads/bc2001_aligned_sorted.bam. # 2024-04-23 11:42:51.281492: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA. # To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. # I0423 11:42:57.943745 140073410221888 run_deepvariant.py:519] Re-using the directory for intermediate results in /tmp/tmpkmab_2kw. # ***** Intermediate results will be written to /tmp/tmpkmab_2kw in docker. ****. # ***** Running the command:*****. # time seq 0 11 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/work/cjm124/SWFst/lvar3ref/Lvar_scaffolds.fasta"" --reads ""/work/cjm124/SWFst/VarCalling/reads/bc2001_aligned_sorted.bam"" --examples ""/tmp/tmpkmab_2kw/make_example",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:760,availability,echo,echo,760,"The command for installing DeepVariant? . `singularity build DeepVariant_1.6.1.sif docker://google/deepvariant:1.6.1`. Or the full command that is written to stdout when DeepVariant runs? For this latter case, after installing nucleus the full command for deepvariant is not written to output. The last line of output is `KeyError: 'SerializedDType'`. I dont have the output saved from the test data run to retrieve the full command output with the error prior to installing nucleus as user, and will re-run that and update this comment with it in a few hours. I did, however, get the same error attempting to run deepvariant with my own data (prior to installing nucleus as user), and the output and command from that are below:. ```. for bam in $READS; do. 	echo ""running deepvariant on $bam"". 	run_deepvariant --model_type=PACBIO --ref=$REF --reads=$bam --output_vcf=$OUTDIR/$bam.vcf.gz --output_gvcf=$OUTDIR/$bam.g.vcf.gz --logging_dir=$LOGDIR --num_shards=$CORES. 	echo ""finished with $bam"". done. #output in block comment below. # running deepvariant on /work/cjm124/SWFst/VarCalling/reads/bc2001_aligned_sorted.bam. # 2024-04-23 11:42:51.281492: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA. # To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. # I0423 11:42:57.943745 140073410221888 run_deepvariant.py:519] Re-using the directory for intermediate results in /tmp/tmpkmab_2kw. # ***** Intermediate results will be written to /tmp/tmpkmab_2kw in docker. ****. # ***** Running the command:*****. # time seq 0 11 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/work/cjm124/SWFst/lvar3ref/Lvar_scaffolds.fasta"" --reads ""/work/cjm124/SWFst/VarCalling/reads/bc2001_aligned_sorted.bam"" --examples ""/tmp/tmpkmab_2kw/make_example",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:970,availability,echo,echo,970,"The command for installing DeepVariant? . `singularity build DeepVariant_1.6.1.sif docker://google/deepvariant:1.6.1`. Or the full command that is written to stdout when DeepVariant runs? For this latter case, after installing nucleus the full command for deepvariant is not written to output. The last line of output is `KeyError: 'SerializedDType'`. I dont have the output saved from the test data run to retrieve the full command output with the error prior to installing nucleus as user, and will re-run that and update this comment with it in a few hours. I did, however, get the same error attempting to run deepvariant with my own data (prior to installing nucleus as user), and the output and command from that are below:. ```. for bam in $READS; do. 	echo ""running deepvariant on $bam"". 	run_deepvariant --model_type=PACBIO --ref=$REF --reads=$bam --output_vcf=$OUTDIR/$bam.vcf.gz --output_gvcf=$OUTDIR/$bam.g.vcf.gz --logging_dir=$LOGDIR --num_shards=$CORES. 	echo ""finished with $bam"". done. #output in block comment below. # running deepvariant on /work/cjm124/SWFst/VarCalling/reads/bc2001_aligned_sorted.bam. # 2024-04-23 11:42:51.281492: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA. # To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. # I0423 11:42:57.943745 140073410221888 run_deepvariant.py:519] Re-using the directory for intermediate results in /tmp/tmpkmab_2kw. # ***** Intermediate results will be written to /tmp/tmpkmab_2kw in docker. ****. # ***** Running the command:*****. # time seq 0 11 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/work/cjm124/SWFst/lvar3ref/Lvar_scaffolds.fasta"" --reads ""/work/cjm124/SWFst/VarCalling/reads/bc2001_aligned_sorted.bam"" --examples ""/tmp/tmpkmab_2kw/make_example",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:1353,availability,operat,operations,1353," have the output saved from the test data run to retrieve the full command output with the error prior to installing nucleus as user, and will re-run that and update this comment with it in a few hours. I did, however, get the same error attempting to run deepvariant with my own data (prior to installing nucleus as user), and the output and command from that are below:. ```. for bam in $READS; do. 	echo ""running deepvariant on $bam"". 	run_deepvariant --model_type=PACBIO --ref=$REF --reads=$bam --output_vcf=$OUTDIR/$bam.vcf.gz --output_gvcf=$OUTDIR/$bam.g.vcf.gz --logging_dir=$LOGDIR --num_shards=$CORES. 	echo ""finished with $bam"". done. #output in block comment below. # running deepvariant on /work/cjm124/SWFst/VarCalling/reads/bc2001_aligned_sorted.bam. # 2024-04-23 11:42:51.281492: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA. # To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. # I0423 11:42:57.943745 140073410221888 run_deepvariant.py:519] Re-using the directory for intermediate results in /tmp/tmpkmab_2kw. # ***** Intermediate results will be written to /tmp/tmpkmab_2kw in docker. ****. # ***** Running the command:*****. # time seq 0 11 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/work/cjm124/SWFst/lvar3ref/Lvar_scaffolds.fasta"" --reads ""/work/cjm124/SWFst/VarCalling/reads/bc2001_aligned_sorted.bam"" --examples ""/tmp/tmpkmab_2kw/make_examples.tfrecord@12.gz"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --gvcf ""/tmp/tmpkmab_2kw/gvcf.tfrecord@12.gz"" --max_reads_per_partition ""600"" --min_mapping_quality ""1"" --parse_sam_aux_fields --partition_size ""25000"" --phase_reads --pileup_image_width ""199"" --norealign_reads --sort_by_haplotypes --track_ref_reads --vsc_min_fraction_indels ""0.12"" --t",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:1409,availability,operat,operations,1409,"e the full command output with the error prior to installing nucleus as user, and will re-run that and update this comment with it in a few hours. I did, however, get the same error attempting to run deepvariant with my own data (prior to installing nucleus as user), and the output and command from that are below:. ```. for bam in $READS; do. 	echo ""running deepvariant on $bam"". 	run_deepvariant --model_type=PACBIO --ref=$REF --reads=$bam --output_vcf=$OUTDIR/$bam.vcf.gz --output_gvcf=$OUTDIR/$bam.g.vcf.gz --logging_dir=$LOGDIR --num_shards=$CORES. 	echo ""finished with $bam"". done. #output in block comment below. # running deepvariant on /work/cjm124/SWFst/VarCalling/reads/bc2001_aligned_sorted.bam. # 2024-04-23 11:42:51.281492: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA. # To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. # I0423 11:42:57.943745 140073410221888 run_deepvariant.py:519] Re-using the directory for intermediate results in /tmp/tmpkmab_2kw. # ***** Intermediate results will be written to /tmp/tmpkmab_2kw in docker. ****. # ***** Running the command:*****. # time seq 0 11 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/work/cjm124/SWFst/lvar3ref/Lvar_scaffolds.fasta"" --reads ""/work/cjm124/SWFst/VarCalling/reads/bc2001_aligned_sorted.bam"" --examples ""/tmp/tmpkmab_2kw/make_examples.tfrecord@12.gz"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --gvcf ""/tmp/tmpkmab_2kw/gvcf.tfrecord@12.gz"" --max_reads_per_partition ""600"" --min_mapping_quality ""1"" --parse_sam_aux_fields --partition_size ""25000"" --phase_reads --pileup_image_width ""199"" --norealign_reads --sort_by_haplotypes --track_ref_reads --vsc_min_fraction_indels ""0.12"" --task {}. # perl: warning: Setting locale failed. # perl: ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:16,deployability,instal,installing,16,"The command for installing DeepVariant? . `singularity build DeepVariant_1.6.1.sif docker://google/deepvariant:1.6.1`. Or the full command that is written to stdout when DeepVariant runs? For this latter case, after installing nucleus the full command for deepvariant is not written to output. The last line of output is `KeyError: 'SerializedDType'`. I dont have the output saved from the test data run to retrieve the full command output with the error prior to installing nucleus as user, and will re-run that and update this comment with it in a few hours. I did, however, get the same error attempting to run deepvariant with my own data (prior to installing nucleus as user), and the output and command from that are below:. ```. for bam in $READS; do. 	echo ""running deepvariant on $bam"". 	run_deepvariant --model_type=PACBIO --ref=$REF --reads=$bam --output_vcf=$OUTDIR/$bam.vcf.gz --output_gvcf=$OUTDIR/$bam.g.vcf.gz --logging_dir=$LOGDIR --num_shards=$CORES. 	echo ""finished with $bam"". done. #output in block comment below. # running deepvariant on /work/cjm124/SWFst/VarCalling/reads/bc2001_aligned_sorted.bam. # 2024-04-23 11:42:51.281492: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA. # To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. # I0423 11:42:57.943745 140073410221888 run_deepvariant.py:519] Re-using the directory for intermediate results in /tmp/tmpkmab_2kw. # ***** Intermediate results will be written to /tmp/tmpkmab_2kw in docker. ****. # ***** Running the command:*****. # time seq 0 11 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/work/cjm124/SWFst/lvar3ref/Lvar_scaffolds.fasta"" --reads ""/work/cjm124/SWFst/VarCalling/reads/bc2001_aligned_sorted.bam"" --examples ""/tmp/tmpkmab_2kw/make_example",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:55,deployability,build,build,55,"The command for installing DeepVariant? . `singularity build DeepVariant_1.6.1.sif docker://google/deepvariant:1.6.1`. Or the full command that is written to stdout when DeepVariant runs? For this latter case, after installing nucleus the full command for deepvariant is not written to output. The last line of output is `KeyError: 'SerializedDType'`. I dont have the output saved from the test data run to retrieve the full command output with the error prior to installing nucleus as user, and will re-run that and update this comment with it in a few hours. I did, however, get the same error attempting to run deepvariant with my own data (prior to installing nucleus as user), and the output and command from that are below:. ```. for bam in $READS; do. 	echo ""running deepvariant on $bam"". 	run_deepvariant --model_type=PACBIO --ref=$REF --reads=$bam --output_vcf=$OUTDIR/$bam.vcf.gz --output_gvcf=$OUTDIR/$bam.g.vcf.gz --logging_dir=$LOGDIR --num_shards=$CORES. 	echo ""finished with $bam"". done. #output in block comment below. # running deepvariant on /work/cjm124/SWFst/VarCalling/reads/bc2001_aligned_sorted.bam. # 2024-04-23 11:42:51.281492: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA. # To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. # I0423 11:42:57.943745 140073410221888 run_deepvariant.py:519] Re-using the directory for intermediate results in /tmp/tmpkmab_2kw. # ***** Intermediate results will be written to /tmp/tmpkmab_2kw in docker. ****. # ***** Running the command:*****. # time seq 0 11 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/work/cjm124/SWFst/lvar3ref/Lvar_scaffolds.fasta"" --reads ""/work/cjm124/SWFst/VarCalling/reads/bc2001_aligned_sorted.bam"" --examples ""/tmp/tmpkmab_2kw/make_example",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:216,deployability,instal,installing,216,"The command for installing DeepVariant? . `singularity build DeepVariant_1.6.1.sif docker://google/deepvariant:1.6.1`. Or the full command that is written to stdout when DeepVariant runs? For this latter case, after installing nucleus the full command for deepvariant is not written to output. The last line of output is `KeyError: 'SerializedDType'`. I dont have the output saved from the test data run to retrieve the full command output with the error prior to installing nucleus as user, and will re-run that and update this comment with it in a few hours. I did, however, get the same error attempting to run deepvariant with my own data (prior to installing nucleus as user), and the output and command from that are below:. ```. for bam in $READS; do. 	echo ""running deepvariant on $bam"". 	run_deepvariant --model_type=PACBIO --ref=$REF --reads=$bam --output_vcf=$OUTDIR/$bam.vcf.gz --output_gvcf=$OUTDIR/$bam.g.vcf.gz --logging_dir=$LOGDIR --num_shards=$CORES. 	echo ""finished with $bam"". done. #output in block comment below. # running deepvariant on /work/cjm124/SWFst/VarCalling/reads/bc2001_aligned_sorted.bam. # 2024-04-23 11:42:51.281492: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA. # To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. # I0423 11:42:57.943745 140073410221888 run_deepvariant.py:519] Re-using the directory for intermediate results in /tmp/tmpkmab_2kw. # ***** Intermediate results will be written to /tmp/tmpkmab_2kw in docker. ****. # ***** Running the command:*****. # time seq 0 11 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/work/cjm124/SWFst/lvar3ref/Lvar_scaffolds.fasta"" --reads ""/work/cjm124/SWFst/VarCalling/reads/bc2001_aligned_sorted.bam"" --examples ""/tmp/tmpkmab_2kw/make_example",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:464,deployability,instal,installing,464,"The command for installing DeepVariant? . `singularity build DeepVariant_1.6.1.sif docker://google/deepvariant:1.6.1`. Or the full command that is written to stdout when DeepVariant runs? For this latter case, after installing nucleus the full command for deepvariant is not written to output. The last line of output is `KeyError: 'SerializedDType'`. I dont have the output saved from the test data run to retrieve the full command output with the error prior to installing nucleus as user, and will re-run that and update this comment with it in a few hours. I did, however, get the same error attempting to run deepvariant with my own data (prior to installing nucleus as user), and the output and command from that are below:. ```. for bam in $READS; do. 	echo ""running deepvariant on $bam"". 	run_deepvariant --model_type=PACBIO --ref=$REF --reads=$bam --output_vcf=$OUTDIR/$bam.vcf.gz --output_gvcf=$OUTDIR/$bam.g.vcf.gz --logging_dir=$LOGDIR --num_shards=$CORES. 	echo ""finished with $bam"". done. #output in block comment below. # running deepvariant on /work/cjm124/SWFst/VarCalling/reads/bc2001_aligned_sorted.bam. # 2024-04-23 11:42:51.281492: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA. # To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. # I0423 11:42:57.943745 140073410221888 run_deepvariant.py:519] Re-using the directory for intermediate results in /tmp/tmpkmab_2kw. # ***** Intermediate results will be written to /tmp/tmpkmab_2kw in docker. ****. # ***** Running the command:*****. # time seq 0 11 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/work/cjm124/SWFst/lvar3ref/Lvar_scaffolds.fasta"" --reads ""/work/cjm124/SWFst/VarCalling/reads/bc2001_aligned_sorted.bam"" --examples ""/tmp/tmpkmab_2kw/make_example",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:517,deployability,updat,update,517,"The command for installing DeepVariant? . `singularity build DeepVariant_1.6.1.sif docker://google/deepvariant:1.6.1`. Or the full command that is written to stdout when DeepVariant runs? For this latter case, after installing nucleus the full command for deepvariant is not written to output. The last line of output is `KeyError: 'SerializedDType'`. I dont have the output saved from the test data run to retrieve the full command output with the error prior to installing nucleus as user, and will re-run that and update this comment with it in a few hours. I did, however, get the same error attempting to run deepvariant with my own data (prior to installing nucleus as user), and the output and command from that are below:. ```. for bam in $READS; do. 	echo ""running deepvariant on $bam"". 	run_deepvariant --model_type=PACBIO --ref=$REF --reads=$bam --output_vcf=$OUTDIR/$bam.vcf.gz --output_gvcf=$OUTDIR/$bam.g.vcf.gz --logging_dir=$LOGDIR --num_shards=$CORES. 	echo ""finished with $bam"". done. #output in block comment below. # running deepvariant on /work/cjm124/SWFst/VarCalling/reads/bc2001_aligned_sorted.bam. # 2024-04-23 11:42:51.281492: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA. # To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. # I0423 11:42:57.943745 140073410221888 run_deepvariant.py:519] Re-using the directory for intermediate results in /tmp/tmpkmab_2kw. # ***** Intermediate results will be written to /tmp/tmpkmab_2kw in docker. ****. # ***** Running the command:*****. # time seq 0 11 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/work/cjm124/SWFst/lvar3ref/Lvar_scaffolds.fasta"" --reads ""/work/cjm124/SWFst/VarCalling/reads/bc2001_aligned_sorted.bam"" --examples ""/tmp/tmpkmab_2kw/make_example",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:653,deployability,instal,installing,653,"The command for installing DeepVariant? . `singularity build DeepVariant_1.6.1.sif docker://google/deepvariant:1.6.1`. Or the full command that is written to stdout when DeepVariant runs? For this latter case, after installing nucleus the full command for deepvariant is not written to output. The last line of output is `KeyError: 'SerializedDType'`. I dont have the output saved from the test data run to retrieve the full command output with the error prior to installing nucleus as user, and will re-run that and update this comment with it in a few hours. I did, however, get the same error attempting to run deepvariant with my own data (prior to installing nucleus as user), and the output and command from that are below:. ```. for bam in $READS; do. 	echo ""running deepvariant on $bam"". 	run_deepvariant --model_type=PACBIO --ref=$REF --reads=$bam --output_vcf=$OUTDIR/$bam.vcf.gz --output_gvcf=$OUTDIR/$bam.g.vcf.gz --logging_dir=$LOGDIR --num_shards=$CORES. 	echo ""finished with $bam"". done. #output in block comment below. # running deepvariant on /work/cjm124/SWFst/VarCalling/reads/bc2001_aligned_sorted.bam. # 2024-04-23 11:42:51.281492: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA. # To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. # I0423 11:42:57.943745 140073410221888 run_deepvariant.py:519] Re-using the directory for intermediate results in /tmp/tmpkmab_2kw. # ***** Intermediate results will be written to /tmp/tmpkmab_2kw in docker. ****. # ***** Running the command:*****. # time seq 0 11 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/work/cjm124/SWFst/lvar3ref/Lvar_scaffolds.fasta"" --reads ""/work/cjm124/SWFst/VarCalling/reads/bc2001_aligned_sorted.bam"" --examples ""/tmp/tmpkmab_2kw/make_example",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:941,deployability,LOG,LOGDIR,941,"The command for installing DeepVariant? . `singularity build DeepVariant_1.6.1.sif docker://google/deepvariant:1.6.1`. Or the full command that is written to stdout when DeepVariant runs? For this latter case, after installing nucleus the full command for deepvariant is not written to output. The last line of output is `KeyError: 'SerializedDType'`. I dont have the output saved from the test data run to retrieve the full command output with the error prior to installing nucleus as user, and will re-run that and update this comment with it in a few hours. I did, however, get the same error attempting to run deepvariant with my own data (prior to installing nucleus as user), and the output and command from that are below:. ```. for bam in $READS; do. 	echo ""running deepvariant on $bam"". 	run_deepvariant --model_type=PACBIO --ref=$REF --reads=$bam --output_vcf=$OUTDIR/$bam.vcf.gz --output_gvcf=$OUTDIR/$bam.g.vcf.gz --logging_dir=$LOGDIR --num_shards=$CORES. 	echo ""finished with $bam"". done. #output in block comment below. # running deepvariant on /work/cjm124/SWFst/VarCalling/reads/bc2001_aligned_sorted.bam. # 2024-04-23 11:42:51.281492: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA. # To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. # I0423 11:42:57.943745 140073410221888 run_deepvariant.py:519] Re-using the directory for intermediate results in /tmp/tmpkmab_2kw. # ***** Intermediate results will be written to /tmp/tmpkmab_2kw in docker. ****. # ***** Running the command:*****. # time seq 0 11 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/work/cjm124/SWFst/lvar3ref/Lvar_scaffolds.fasta"" --reads ""/work/cjm124/SWFst/VarCalling/reads/bc2001_aligned_sorted.bam"" --examples ""/tmp/tmpkmab_2kw/make_example",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:2398,deployability,fail,failed,2398,"n other operations, rebuild TensorFlow with the appropriate compiler flags. # I0423 11:42:57.943745 140073410221888 run_deepvariant.py:519] Re-using the directory for intermediate results in /tmp/tmpkmab_2kw. # ***** Intermediate results will be written to /tmp/tmpkmab_2kw in docker. ****. # ***** Running the command:*****. # time seq 0 11 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/work/cjm124/SWFst/lvar3ref/Lvar_scaffolds.fasta"" --reads ""/work/cjm124/SWFst/VarCalling/reads/bc2001_aligned_sorted.bam"" --examples ""/tmp/tmpkmab_2kw/make_examples.tfrecord@12.gz"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --gvcf ""/tmp/tmpkmab_2kw/gvcf.tfrecord@12.gz"" --max_reads_per_partition ""600"" --min_mapping_quality ""1"" --parse_sam_aux_fields --partition_size ""25000"" --phase_reads --pileup_image_width ""199"" --norealign_reads --sort_by_haplotypes --track_ref_reads --vsc_min_fraction_indels ""0.12"" --task {}. # perl: warning: Setting locale failed. # perl: warning: Please check that your locale settings:. # 	LANGUAGE = (unset),. # 	LC_ALL = (unset),. # 	LC_CTYPE = ""C.UTF-8"",. # 	LANG = ""en_US.UTF-8"". # are supported and installed on your system. # perl: warning: Falling back to the standard locale (""C""). # perl: warning: Setting locale failed. # perl: warning: Please check that your locale settings:. # 	LANGUAGE = (unset),. # 	LC_ALL = (unset),. # 	LC_CTYPE = ""C.UTF-8"",. # 	LANG = ""en_US.UTF-8"". # are supported and installed on your system. # perl: warning: Falling back to the standard locale (""C""). # I0423 11:43:12.358298 140211385890624 genomics_reader.py:222] Reading /work/cjm124/SWFst/VarCalling/reads/bc2001_aligned_sorted.bam with NativeSamReader. # W0423 11:43:12.358482 140211385890624 make_examples_core.py:344] No non-empty sample name found in the input reads. DeepVariant will use default as the sample name. You can also provide a sample name with the --sample_name argument. # I0423 11:43:12.365553 140211385890624 ma",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:2581,deployability,instal,installed,2581,"ts in /tmp/tmpkmab_2kw. # ***** Intermediate results will be written to /tmp/tmpkmab_2kw in docker. ****. # ***** Running the command:*****. # time seq 0 11 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/work/cjm124/SWFst/lvar3ref/Lvar_scaffolds.fasta"" --reads ""/work/cjm124/SWFst/VarCalling/reads/bc2001_aligned_sorted.bam"" --examples ""/tmp/tmpkmab_2kw/make_examples.tfrecord@12.gz"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --gvcf ""/tmp/tmpkmab_2kw/gvcf.tfrecord@12.gz"" --max_reads_per_partition ""600"" --min_mapping_quality ""1"" --parse_sam_aux_fields --partition_size ""25000"" --phase_reads --pileup_image_width ""199"" --norealign_reads --sort_by_haplotypes --track_ref_reads --vsc_min_fraction_indels ""0.12"" --task {}. # perl: warning: Setting locale failed. # perl: warning: Please check that your locale settings:. # 	LANGUAGE = (unset),. # 	LC_ALL = (unset),. # 	LC_CTYPE = ""C.UTF-8"",. # 	LANG = ""en_US.UTF-8"". # are supported and installed on your system. # perl: warning: Falling back to the standard locale (""C""). # perl: warning: Setting locale failed. # perl: warning: Please check that your locale settings:. # 	LANGUAGE = (unset),. # 	LC_ALL = (unset),. # 	LC_CTYPE = ""C.UTF-8"",. # 	LANG = ""en_US.UTF-8"". # are supported and installed on your system. # perl: warning: Falling back to the standard locale (""C""). # I0423 11:43:12.358298 140211385890624 genomics_reader.py:222] Reading /work/cjm124/SWFst/VarCalling/reads/bc2001_aligned_sorted.bam with NativeSamReader. # W0423 11:43:12.358482 140211385890624 make_examples_core.py:344] No non-empty sample name found in the input reads. DeepVariant will use default as the sample name. You can also provide a sample name with the --sample_name argument. # I0423 11:43:12.365553 140211385890624 make_examples_core.py:301] Task 0/12: Preparing inputs. # I0423 11:43:12.377128 140211385890624 genomics_reader.py:222] Reading /work/cjm124/SWFst/VarCalling/reads/bc2001_aligned_sorted.b",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:2699,deployability,fail,failed,2699,"nning the command:*****. # time seq 0 11 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/work/cjm124/SWFst/lvar3ref/Lvar_scaffolds.fasta"" --reads ""/work/cjm124/SWFst/VarCalling/reads/bc2001_aligned_sorted.bam"" --examples ""/tmp/tmpkmab_2kw/make_examples.tfrecord@12.gz"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --gvcf ""/tmp/tmpkmab_2kw/gvcf.tfrecord@12.gz"" --max_reads_per_partition ""600"" --min_mapping_quality ""1"" --parse_sam_aux_fields --partition_size ""25000"" --phase_reads --pileup_image_width ""199"" --norealign_reads --sort_by_haplotypes --track_ref_reads --vsc_min_fraction_indels ""0.12"" --task {}. # perl: warning: Setting locale failed. # perl: warning: Please check that your locale settings:. # 	LANGUAGE = (unset),. # 	LC_ALL = (unset),. # 	LC_CTYPE = ""C.UTF-8"",. # 	LANG = ""en_US.UTF-8"". # are supported and installed on your system. # perl: warning: Falling back to the standard locale (""C""). # perl: warning: Setting locale failed. # perl: warning: Please check that your locale settings:. # 	LANGUAGE = (unset),. # 	LC_ALL = (unset),. # 	LC_CTYPE = ""C.UTF-8"",. # 	LANG = ""en_US.UTF-8"". # are supported and installed on your system. # perl: warning: Falling back to the standard locale (""C""). # I0423 11:43:12.358298 140211385890624 genomics_reader.py:222] Reading /work/cjm124/SWFst/VarCalling/reads/bc2001_aligned_sorted.bam with NativeSamReader. # W0423 11:43:12.358482 140211385890624 make_examples_core.py:344] No non-empty sample name found in the input reads. DeepVariant will use default as the sample name. You can also provide a sample name with the --sample_name argument. # I0423 11:43:12.365553 140211385890624 make_examples_core.py:301] Task 0/12: Preparing inputs. # I0423 11:43:12.377128 140211385890624 genomics_reader.py:222] Reading /work/cjm124/SWFst/VarCalling/reads/bc2001_aligned_sorted.bam with NativeSamReader. # I0423 11:43:12.405545 140211385890624 make_examples_core.py:301] Task 0/12: Common contig",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:2882,deployability,instal,installed,2882,"--reads ""/work/cjm124/SWFst/VarCalling/reads/bc2001_aligned_sorted.bam"" --examples ""/tmp/tmpkmab_2kw/make_examples.tfrecord@12.gz"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --gvcf ""/tmp/tmpkmab_2kw/gvcf.tfrecord@12.gz"" --max_reads_per_partition ""600"" --min_mapping_quality ""1"" --parse_sam_aux_fields --partition_size ""25000"" --phase_reads --pileup_image_width ""199"" --norealign_reads --sort_by_haplotypes --track_ref_reads --vsc_min_fraction_indels ""0.12"" --task {}. # perl: warning: Setting locale failed. # perl: warning: Please check that your locale settings:. # 	LANGUAGE = (unset),. # 	LC_ALL = (unset),. # 	LC_CTYPE = ""C.UTF-8"",. # 	LANG = ""en_US.UTF-8"". # are supported and installed on your system. # perl: warning: Falling back to the standard locale (""C""). # perl: warning: Setting locale failed. # perl: warning: Please check that your locale settings:. # 	LANGUAGE = (unset),. # 	LC_ALL = (unset),. # 	LC_CTYPE = ""C.UTF-8"",. # 	LANG = ""en_US.UTF-8"". # are supported and installed on your system. # perl: warning: Falling back to the standard locale (""C""). # I0423 11:43:12.358298 140211385890624 genomics_reader.py:222] Reading /work/cjm124/SWFst/VarCalling/reads/bc2001_aligned_sorted.bam with NativeSamReader. # W0423 11:43:12.358482 140211385890624 make_examples_core.py:344] No non-empty sample name found in the input reads. DeepVariant will use default as the sample name. You can also provide a sample name with the --sample_name argument. # I0423 11:43:12.365553 140211385890624 make_examples_core.py:301] Task 0/12: Preparing inputs. # I0423 11:43:12.377128 140211385890624 genomics_reader.py:222] Reading /work/cjm124/SWFst/VarCalling/reads/bc2001_aligned_sorted.bam with NativeSamReader. # I0423 11:43:12.405545 140211385890624 make_examples_core.py:301] Task 0/12: Common contigs are ['chr1', 'chr2', 'chr3', 'chr4', 'chr5', 'chr6', 'chr7', 'chr8', 'chr9', 'chr10', 'chr11', 'chr12', 'chr13', 'chr14', 'chr15', 'chr16', 'chr17', 'chr18', 'chr19', 'unplaced_scaffo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:6903,deployability,Fail,Failed,6903,"12: Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. # I0423 11:43:12.538744 140211385890624 genomics_reader.py:222] Reading /work/cjm124/SWFst/VarCalling/reads/bc2001_aligned_sorted.bam with NativeSamReader. # I0423 11:43:12.636761 140211385890624 genomics_reader.py:222] Reading /work/cjm124/SWFst/VarCalling/reads/bc2001_aligned_sorted.bam with NativeSamReader. # I0423 11:43:12.637369 140211385890624 make_examples_core.py:301] Task 0/12: Writing gvcf records to /tmp/tmpkmab_2kw/gvcf.tfrecord-00000-of-00012.gz. # I0423 11:43:12.637865 140211385890624 make_examples_core.py:301] Task 0/12: Writing examples to /tmp/tmpkmab_2kw/make_examples.tfrecord-00000-of-00012.gz. # I0423 11:43:12.637962 140211385890624 make_examples_core.py:301] Task 0/12: Overhead for preparing inputs: 0 seconds. # 2024-04-23 11:43:12.645232: W ./third_party/nucleus/util/proto_clif_converter.h:75] Failed to cast type N6google8protobuf14DynamicMessageE. # Traceback (most recent call last):. # File ""/tmp/Bazel.runfiles_qy0tffir/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 234, in <module>. # app.run(main). # File ""/tmp/Bazel.runfiles_qy0tffir/runfiles/absl_py/absl/app.py"", line 312, in run. # _run_main(main, args). # File ""/tmp/Bazel.runfiles_qy0tffir/runfiles/absl_py/absl/app.py"", line 258, in _run_main. # sys.exit(main(argv)). # File ""/tmp/Bazel.runfiles_qy0tffir/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 224, in main. # make_examples_core.make_examples_runner(options). # File ""/tmp/Bazel.runfiles_qy0tffir/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2838, in make_examples_runner. # region_processor.process(region, region_n). # File ""/tmp/Bazel.runfiles_qy0tffir/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1695, in process. # sample_reads = self.region_reads_norealign(. # File ""/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:7111,deployability,modul,module,7111,"4 genomics_reader.py:222] Reading /work/cjm124/SWFst/VarCalling/reads/bc2001_aligned_sorted.bam with NativeSamReader. # I0423 11:43:12.636761 140211385890624 genomics_reader.py:222] Reading /work/cjm124/SWFst/VarCalling/reads/bc2001_aligned_sorted.bam with NativeSamReader. # I0423 11:43:12.637369 140211385890624 make_examples_core.py:301] Task 0/12: Writing gvcf records to /tmp/tmpkmab_2kw/gvcf.tfrecord-00000-of-00012.gz. # I0423 11:43:12.637865 140211385890624 make_examples_core.py:301] Task 0/12: Writing examples to /tmp/tmpkmab_2kw/make_examples.tfrecord-00000-of-00012.gz. # I0423 11:43:12.637962 140211385890624 make_examples_core.py:301] Task 0/12: Overhead for preparing inputs: 0 seconds. # 2024-04-23 11:43:12.645232: W ./third_party/nucleus/util/proto_clif_converter.h:75] Failed to cast type N6google8protobuf14DynamicMessageE. # Traceback (most recent call last):. # File ""/tmp/Bazel.runfiles_qy0tffir/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 234, in <module>. # app.run(main). # File ""/tmp/Bazel.runfiles_qy0tffir/runfiles/absl_py/absl/app.py"", line 312, in run. # _run_main(main, args). # File ""/tmp/Bazel.runfiles_qy0tffir/runfiles/absl_py/absl/app.py"", line 258, in _run_main. # sys.exit(main(argv)). # File ""/tmp/Bazel.runfiles_qy0tffir/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 224, in main. # make_examples_core.make_examples_runner(options). # File ""/tmp/Bazel.runfiles_qy0tffir/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2838, in make_examples_runner. # region_processor.process(region, region_n). # File ""/tmp/Bazel.runfiles_qy0tffir/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1695, in process. # sample_reads = self.region_reads_norealign(. # File ""/tmp/Bazel.runfiles_qy0tffir/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1817, in region_reads_norealign. # reads = reservoir_sample_reads(. # File ""/tmp/Bazel.runfiles_qy0tffir/ru",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:8883,deployability,fail,failed,8883,"examples_core.py"", line 2838, in make_examples_runner. # region_processor.process(region, region_n). # File ""/tmp/Bazel.runfiles_qy0tffir/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1695, in process. # sample_reads = self.region_reads_norealign(. # File ""/tmp/Bazel.runfiles_qy0tffir/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1817, in region_reads_norealign. # reads = reservoir_sample_reads(. # File ""/tmp/Bazel.runfiles_qy0tffir/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 976, in reservoir_sample_reads. # return utils.reservoir_sample(iterable_of_reads, k, random). # File ""/tmp/Bazel.runfiles_qy0tffir/runfiles/com_google_deepvariant/third_party/nucleus/util/utils.py"", line 117, in reservoir_sample. # for i, item in enumerate(iterable):. # File ""/tmp/Bazel.runfiles_qy0tffir/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 95, in __next__. # record, not_done = self._raw_next(). # File ""/tmp/Bazel.runfiles_qy0tffir/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 154, in _raw_next. # not_done = self._cc_iterable.PythonNext(record). # RuntimeError: PythonNext() argument read is not valid: Dynamic cast failed. # parallel: This job failed:. # /opt/deepvariant/bin/make_examples --mode calling --ref /work/cjm124/SWFst/lvar3ref/Lvar_scaffolds.fasta --reads /work/cjm124/SWFst/VarCalling/reads/bc2001_aligned_sorted.bam --examples /tmp/tmpkmab_2kw/make_examples.tfrecord@12.gz --add_hp_channel --alt_aligned_pileup diff_channels --gvcf /tmp/tmpkmab_2kw/gvcf.tfrecord@12.gz --max_reads_per_partition 600 --min_mapping_quality 1 --parse_sam_aux_fields --partition_size 25000 --phase_reads --pileup_image_width 199 --norealign_reads --sort_by_haplotypes --track_ref_reads --vsc_min_fraction_indels 0.12 --task 0. # real	0m15.200s. # user	0m3.162s. # sys	0m1.161s. # finished with /work/cjm124/SWFst/VarCalling/reads/bc2001_aligned_sorted.bam. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:8912,deployability,fail,failed,8912,"examples_core.py"", line 2838, in make_examples_runner. # region_processor.process(region, region_n). # File ""/tmp/Bazel.runfiles_qy0tffir/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1695, in process. # sample_reads = self.region_reads_norealign(. # File ""/tmp/Bazel.runfiles_qy0tffir/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1817, in region_reads_norealign. # reads = reservoir_sample_reads(. # File ""/tmp/Bazel.runfiles_qy0tffir/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 976, in reservoir_sample_reads. # return utils.reservoir_sample(iterable_of_reads, k, random). # File ""/tmp/Bazel.runfiles_qy0tffir/runfiles/com_google_deepvariant/third_party/nucleus/util/utils.py"", line 117, in reservoir_sample. # for i, item in enumerate(iterable):. # File ""/tmp/Bazel.runfiles_qy0tffir/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 95, in __next__. # record, not_done = self._raw_next(). # File ""/tmp/Bazel.runfiles_qy0tffir/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 154, in _raw_next. # not_done = self._cc_iterable.PythonNext(record). # RuntimeError: PythonNext() argument read is not valid: Dynamic cast failed. # parallel: This job failed:. # /opt/deepvariant/bin/make_examples --mode calling --ref /work/cjm124/SWFst/lvar3ref/Lvar_scaffolds.fasta --reads /work/cjm124/SWFst/VarCalling/reads/bc2001_aligned_sorted.bam --examples /tmp/tmpkmab_2kw/make_examples.tfrecord@12.gz --add_hp_channel --alt_aligned_pileup diff_channels --gvcf /tmp/tmpkmab_2kw/gvcf.tfrecord@12.gz --max_reads_per_partition 600 --min_mapping_quality 1 --parse_sam_aux_fields --partition_size 25000 --phase_reads --pileup_image_width 199 --norealign_reads --sort_by_haplotypes --track_ref_reads --vsc_min_fraction_indels 0.12 --task 0. # real	0m15.200s. # user	0m3.162s. # sys	0m1.161s. # finished with /work/cjm124/SWFst/VarCalling/reads/bc2001_aligned_sorted.bam. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:962,energy efficiency,CORE,CORES,962,"The command for installing DeepVariant? . `singularity build DeepVariant_1.6.1.sif docker://google/deepvariant:1.6.1`. Or the full command that is written to stdout when DeepVariant runs? For this latter case, after installing nucleus the full command for deepvariant is not written to output. The last line of output is `KeyError: 'SerializedDType'`. I dont have the output saved from the test data run to retrieve the full command output with the error prior to installing nucleus as user, and will re-run that and update this comment with it in a few hours. I did, however, get the same error attempting to run deepvariant with my own data (prior to installing nucleus as user), and the output and command from that are below:. ```. for bam in $READS; do. 	echo ""running deepvariant on $bam"". 	run_deepvariant --model_type=PACBIO --ref=$REF --reads=$bam --output_vcf=$OUTDIR/$bam.vcf.gz --output_gvcf=$OUTDIR/$bam.g.vcf.gz --logging_dir=$LOGDIR --num_shards=$CORES. 	echo ""finished with $bam"". done. #output in block comment below. # running deepvariant on /work/cjm124/SWFst/VarCalling/reads/bc2001_aligned_sorted.bam. # 2024-04-23 11:42:51.281492: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA. # To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. # I0423 11:42:57.943745 140073410221888 run_deepvariant.py:519] Re-using the directory for intermediate results in /tmp/tmpkmab_2kw. # ***** Intermediate results will be written to /tmp/tmpkmab_2kw in docker. ****. # ***** Running the command:*****. # time seq 0 11 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/work/cjm124/SWFst/lvar3ref/Lvar_scaffolds.fasta"" --reads ""/work/cjm124/SWFst/VarCalling/reads/bc2001_aligned_sorted.bam"" --examples ""/tmp/tmpkmab_2kw/make_example",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:1166,energy efficiency,core,core,1166,"n DeepVariant runs? For this latter case, after installing nucleus the full command for deepvariant is not written to output. The last line of output is `KeyError: 'SerializedDType'`. I dont have the output saved from the test data run to retrieve the full command output with the error prior to installing nucleus as user, and will re-run that and update this comment with it in a few hours. I did, however, get the same error attempting to run deepvariant with my own data (prior to installing nucleus as user), and the output and command from that are below:. ```. for bam in $READS; do. 	echo ""running deepvariant on $bam"". 	run_deepvariant --model_type=PACBIO --ref=$REF --reads=$bam --output_vcf=$OUTDIR/$bam.vcf.gz --output_gvcf=$OUTDIR/$bam.g.vcf.gz --logging_dir=$LOGDIR --num_shards=$CORES. 	echo ""finished with $bam"". done. #output in block comment below. # running deepvariant on /work/cjm124/SWFst/VarCalling/reads/bc2001_aligned_sorted.bam. # 2024-04-23 11:42:51.281492: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA. # To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. # I0423 11:42:57.943745 140073410221888 run_deepvariant.py:519] Re-using the directory for intermediate results in /tmp/tmpkmab_2kw. # ***** Intermediate results will be written to /tmp/tmpkmab_2kw in docker. ****. # ***** Running the command:*****. # time seq 0 11 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/work/cjm124/SWFst/lvar3ref/Lvar_scaffolds.fasta"" --reads ""/work/cjm124/SWFst/VarCalling/reads/bc2001_aligned_sorted.bam"" --examples ""/tmp/tmpkmab_2kw/make_examples.tfrecord@12.gz"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --gvcf ""/tmp/tmpkmab_2kw/gvcf.tfrecord@12.gz"" --max_reads_per_partition ""600"" --min_mapping_qual",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:1232,energy efficiency,optim,optimized,1232,"e full command for deepvariant is not written to output. The last line of output is `KeyError: 'SerializedDType'`. I dont have the output saved from the test data run to retrieve the full command output with the error prior to installing nucleus as user, and will re-run that and update this comment with it in a few hours. I did, however, get the same error attempting to run deepvariant with my own data (prior to installing nucleus as user), and the output and command from that are below:. ```. for bam in $READS; do. 	echo ""running deepvariant on $bam"". 	run_deepvariant --model_type=PACBIO --ref=$REF --reads=$bam --output_vcf=$OUTDIR/$bam.vcf.gz --output_gvcf=$OUTDIR/$bam.g.vcf.gz --logging_dir=$LOGDIR --num_shards=$CORES. 	echo ""finished with $bam"". done. #output in block comment below. # running deepvariant on /work/cjm124/SWFst/VarCalling/reads/bc2001_aligned_sorted.bam. # 2024-04-23 11:42:51.281492: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA. # To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. # I0423 11:42:57.943745 140073410221888 run_deepvariant.py:519] Re-using the directory for intermediate results in /tmp/tmpkmab_2kw. # ***** Intermediate results will be written to /tmp/tmpkmab_2kw in docker. ****. # ***** Running the command:*****. # time seq 0 11 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/work/cjm124/SWFst/lvar3ref/Lvar_scaffolds.fasta"" --reads ""/work/cjm124/SWFst/VarCalling/reads/bc2001_aligned_sorted.bam"" --examples ""/tmp/tmpkmab_2kw/make_examples.tfrecord@12.gz"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --gvcf ""/tmp/tmpkmab_2kw/gvcf.tfrecord@12.gz"" --max_reads_per_partition ""600"" --min_mapping_quality ""1"" --parse_sam_aux_fields --partition_size ""25000"" --phase_reads",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:1312,energy efficiency,CPU,CPU,1312,"put is `KeyError: 'SerializedDType'`. I dont have the output saved from the test data run to retrieve the full command output with the error prior to installing nucleus as user, and will re-run that and update this comment with it in a few hours. I did, however, get the same error attempting to run deepvariant with my own data (prior to installing nucleus as user), and the output and command from that are below:. ```. for bam in $READS; do. 	echo ""running deepvariant on $bam"". 	run_deepvariant --model_type=PACBIO --ref=$REF --reads=$bam --output_vcf=$OUTDIR/$bam.vcf.gz --output_gvcf=$OUTDIR/$bam.g.vcf.gz --logging_dir=$LOGDIR --num_shards=$CORES. 	echo ""finished with $bam"". done. #output in block comment below. # running deepvariant on /work/cjm124/SWFst/VarCalling/reads/bc2001_aligned_sorted.bam. # 2024-04-23 11:42:51.281492: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA. # To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. # I0423 11:42:57.943745 140073410221888 run_deepvariant.py:519] Re-using the directory for intermediate results in /tmp/tmpkmab_2kw. # ***** Intermediate results will be written to /tmp/tmpkmab_2kw in docker. ****. # ***** Running the command:*****. # time seq 0 11 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/work/cjm124/SWFst/lvar3ref/Lvar_scaffolds.fasta"" --reads ""/work/cjm124/SWFst/VarCalling/reads/bc2001_aligned_sorted.bam"" --examples ""/tmp/tmpkmab_2kw/make_examples.tfrecord@12.gz"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --gvcf ""/tmp/tmpkmab_2kw/gvcf.tfrecord@12.gz"" --max_reads_per_partition ""600"" --min_mapping_quality ""1"" --parse_sam_aux_fields --partition_size ""25000"" --phase_reads --pileup_image_width ""199"" --norealign_reads --sort_by_haplotypes --track_re",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:1773,integrability,buffer,buffer,1773,"epvariant on $bam"". 	run_deepvariant --model_type=PACBIO --ref=$REF --reads=$bam --output_vcf=$OUTDIR/$bam.vcf.gz --output_gvcf=$OUTDIR/$bam.g.vcf.gz --logging_dir=$LOGDIR --num_shards=$CORES. 	echo ""finished with $bam"". done. #output in block comment below. # running deepvariant on /work/cjm124/SWFst/VarCalling/reads/bc2001_aligned_sorted.bam. # 2024-04-23 11:42:51.281492: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA. # To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. # I0423 11:42:57.943745 140073410221888 run_deepvariant.py:519] Re-using the directory for intermediate results in /tmp/tmpkmab_2kw. # ***** Intermediate results will be written to /tmp/tmpkmab_2kw in docker. ****. # ***** Running the command:*****. # time seq 0 11 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/work/cjm124/SWFst/lvar3ref/Lvar_scaffolds.fasta"" --reads ""/work/cjm124/SWFst/VarCalling/reads/bc2001_aligned_sorted.bam"" --examples ""/tmp/tmpkmab_2kw/make_examples.tfrecord@12.gz"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --gvcf ""/tmp/tmpkmab_2kw/gvcf.tfrecord@12.gz"" --max_reads_per_partition ""600"" --min_mapping_quality ""1"" --parse_sam_aux_fields --partition_size ""25000"" --phase_reads --pileup_image_width ""199"" --norealign_reads --sort_by_haplotypes --track_ref_reads --vsc_min_fraction_indels ""0.12"" --task {}. # perl: warning: Setting locale failed. # perl: warning: Please check that your locale settings:. # 	LANGUAGE = (unset),. # 	LC_ALL = (unset),. # 	LC_CTYPE = ""C.UTF-8"",. # 	LANG = ""en_US.UTF-8"". # are supported and installed on your system. # perl: warning: Falling back to the standard locale (""C""). # perl: warning: Setting locale failed. # perl: warning: Please check that your locale settings:. # 	LANGUAGE",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:1171,interoperability,platform,platform,1171,"ariant runs? For this latter case, after installing nucleus the full command for deepvariant is not written to output. The last line of output is `KeyError: 'SerializedDType'`. I dont have the output saved from the test data run to retrieve the full command output with the error prior to installing nucleus as user, and will re-run that and update this comment with it in a few hours. I did, however, get the same error attempting to run deepvariant with my own data (prior to installing nucleus as user), and the output and command from that are below:. ```. for bam in $READS; do. 	echo ""running deepvariant on $bam"". 	run_deepvariant --model_type=PACBIO --ref=$REF --reads=$bam --output_vcf=$OUTDIR/$bam.vcf.gz --output_gvcf=$OUTDIR/$bam.g.vcf.gz --logging_dir=$LOGDIR --num_shards=$CORES. 	echo ""finished with $bam"". done. #output in block comment below. # running deepvariant on /work/cjm124/SWFst/VarCalling/reads/bc2001_aligned_sorted.bam. # 2024-04-23 11:42:51.281492: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA. # To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. # I0423 11:42:57.943745 140073410221888 run_deepvariant.py:519] Re-using the directory for intermediate results in /tmp/tmpkmab_2kw. # ***** Intermediate results will be written to /tmp/tmpkmab_2kw in docker. ****. # ***** Running the command:*****. # time seq 0 11 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/work/cjm124/SWFst/lvar3ref/Lvar_scaffolds.fasta"" --reads ""/work/cjm124/SWFst/VarCalling/reads/bc2001_aligned_sorted.bam"" --examples ""/tmp/tmpkmab_2kw/make_examples.tfrecord@12.gz"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --gvcf ""/tmp/tmpkmab_2kw/gvcf.tfrecord@12.gz"" --max_reads_per_partition ""600"" --min_mapping_quality ""1""",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:2644,interoperability,standard,standard,2644,"ritten to /tmp/tmpkmab_2kw in docker. ****. # ***** Running the command:*****. # time seq 0 11 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/work/cjm124/SWFst/lvar3ref/Lvar_scaffolds.fasta"" --reads ""/work/cjm124/SWFst/VarCalling/reads/bc2001_aligned_sorted.bam"" --examples ""/tmp/tmpkmab_2kw/make_examples.tfrecord@12.gz"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --gvcf ""/tmp/tmpkmab_2kw/gvcf.tfrecord@12.gz"" --max_reads_per_partition ""600"" --min_mapping_quality ""1"" --parse_sam_aux_fields --partition_size ""25000"" --phase_reads --pileup_image_width ""199"" --norealign_reads --sort_by_haplotypes --track_ref_reads --vsc_min_fraction_indels ""0.12"" --task {}. # perl: warning: Setting locale failed. # perl: warning: Please check that your locale settings:. # 	LANGUAGE = (unset),. # 	LC_ALL = (unset),. # 	LC_CTYPE = ""C.UTF-8"",. # 	LANG = ""en_US.UTF-8"". # are supported and installed on your system. # perl: warning: Falling back to the standard locale (""C""). # perl: warning: Setting locale failed. # perl: warning: Please check that your locale settings:. # 	LANGUAGE = (unset),. # 	LC_ALL = (unset),. # 	LC_CTYPE = ""C.UTF-8"",. # 	LANG = ""en_US.UTF-8"". # are supported and installed on your system. # perl: warning: Falling back to the standard locale (""C""). # I0423 11:43:12.358298 140211385890624 genomics_reader.py:222] Reading /work/cjm124/SWFst/VarCalling/reads/bc2001_aligned_sorted.bam with NativeSamReader. # W0423 11:43:12.358482 140211385890624 make_examples_core.py:344] No non-empty sample name found in the input reads. DeepVariant will use default as the sample name. You can also provide a sample name with the --sample_name argument. # I0423 11:43:12.365553 140211385890624 make_examples_core.py:301] Task 0/12: Preparing inputs. # I0423 11:43:12.377128 140211385890624 genomics_reader.py:222] Reading /work/cjm124/SWFst/VarCalling/reads/bc2001_aligned_sorted.bam with NativeSamReader. # I0423 11:43:12.405545 1402113858906",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:2945,interoperability,standard,standard,2945,"rted.bam"" --examples ""/tmp/tmpkmab_2kw/make_examples.tfrecord@12.gz"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --gvcf ""/tmp/tmpkmab_2kw/gvcf.tfrecord@12.gz"" --max_reads_per_partition ""600"" --min_mapping_quality ""1"" --parse_sam_aux_fields --partition_size ""25000"" --phase_reads --pileup_image_width ""199"" --norealign_reads --sort_by_haplotypes --track_ref_reads --vsc_min_fraction_indels ""0.12"" --task {}. # perl: warning: Setting locale failed. # perl: warning: Please check that your locale settings:. # 	LANGUAGE = (unset),. # 	LC_ALL = (unset),. # 	LC_CTYPE = ""C.UTF-8"",. # 	LANG = ""en_US.UTF-8"". # are supported and installed on your system. # perl: warning: Falling back to the standard locale (""C""). # perl: warning: Setting locale failed. # perl: warning: Please check that your locale settings:. # 	LANGUAGE = (unset),. # 	LC_ALL = (unset),. # 	LC_CTYPE = ""C.UTF-8"",. # 	LANG = ""en_US.UTF-8"". # are supported and installed on your system. # perl: warning: Falling back to the standard locale (""C""). # I0423 11:43:12.358298 140211385890624 genomics_reader.py:222] Reading /work/cjm124/SWFst/VarCalling/reads/bc2001_aligned_sorted.bam with NativeSamReader. # W0423 11:43:12.358482 140211385890624 make_examples_core.py:344] No non-empty sample name found in the input reads. DeepVariant will use default as the sample name. You can also provide a sample name with the --sample_name argument. # I0423 11:43:12.365553 140211385890624 make_examples_core.py:301] Task 0/12: Preparing inputs. # I0423 11:43:12.377128 140211385890624 genomics_reader.py:222] Reading /work/cjm124/SWFst/VarCalling/reads/bc2001_aligned_sorted.bam with NativeSamReader. # I0423 11:43:12.405545 140211385890624 make_examples_core.py:301] Task 0/12: Common contigs are ['chr1', 'chr2', 'chr3', 'chr4', 'chr5', 'chr6', 'chr7', 'chr8', 'chr9', 'chr10', 'chr11', 'chr12', 'chr13', 'chr14', 'chr15', 'chr16', 'chr17', 'chr18', 'chr19', 'unplaced_scaffold20', 'unplaced_scaffold21', 'unplaced_scaffold22', 'unplaced",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:826,modifiability,PAC,PACBIO,826,"The command for installing DeepVariant? . `singularity build DeepVariant_1.6.1.sif docker://google/deepvariant:1.6.1`. Or the full command that is written to stdout when DeepVariant runs? For this latter case, after installing nucleus the full command for deepvariant is not written to output. The last line of output is `KeyError: 'SerializedDType'`. I dont have the output saved from the test data run to retrieve the full command output with the error prior to installing nucleus as user, and will re-run that and update this comment with it in a few hours. I did, however, get the same error attempting to run deepvariant with my own data (prior to installing nucleus as user), and the output and command from that are below:. ```. for bam in $READS; do. 	echo ""running deepvariant on $bam"". 	run_deepvariant --model_type=PACBIO --ref=$REF --reads=$bam --output_vcf=$OUTDIR/$bam.vcf.gz --output_gvcf=$OUTDIR/$bam.g.vcf.gz --logging_dir=$LOGDIR --num_shards=$CORES. 	echo ""finished with $bam"". done. #output in block comment below. # running deepvariant on /work/cjm124/SWFst/VarCalling/reads/bc2001_aligned_sorted.bam. # 2024-04-23 11:42:51.281492: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA. # To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. # I0423 11:42:57.943745 140073410221888 run_deepvariant.py:519] Re-using the directory for intermediate results in /tmp/tmpkmab_2kw. # ***** Intermediate results will be written to /tmp/tmpkmab_2kw in docker. ****. # ***** Running the command:*****. # time seq 0 11 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/work/cjm124/SWFst/lvar3ref/Lvar_scaffolds.fasta"" --reads ""/work/cjm124/SWFst/VarCalling/reads/bc2001_aligned_sorted.bam"" --examples ""/tmp/tmpkmab_2kw/make_example",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:1568,modifiability,interm,intermediate,1568,"r, get the same error attempting to run deepvariant with my own data (prior to installing nucleus as user), and the output and command from that are below:. ```. for bam in $READS; do. 	echo ""running deepvariant on $bam"". 	run_deepvariant --model_type=PACBIO --ref=$REF --reads=$bam --output_vcf=$OUTDIR/$bam.vcf.gz --output_gvcf=$OUTDIR/$bam.g.vcf.gz --logging_dir=$LOGDIR --num_shards=$CORES. 	echo ""finished with $bam"". done. #output in block comment below. # running deepvariant on /work/cjm124/SWFst/VarCalling/reads/bc2001_aligned_sorted.bam. # 2024-04-23 11:42:51.281492: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA. # To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. # I0423 11:42:57.943745 140073410221888 run_deepvariant.py:519] Re-using the directory for intermediate results in /tmp/tmpkmab_2kw. # ***** Intermediate results will be written to /tmp/tmpkmab_2kw in docker. ****. # ***** Running the command:*****. # time seq 0 11 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/work/cjm124/SWFst/lvar3ref/Lvar_scaffolds.fasta"" --reads ""/work/cjm124/SWFst/VarCalling/reads/bc2001_aligned_sorted.bam"" --examples ""/tmp/tmpkmab_2kw/make_examples.tfrecord@12.gz"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --gvcf ""/tmp/tmpkmab_2kw/gvcf.tfrecord@12.gz"" --max_reads_per_partition ""600"" --min_mapping_quality ""1"" --parse_sam_aux_fields --partition_size ""25000"" --phase_reads --pileup_image_width ""199"" --norealign_reads --sort_by_haplotypes --track_ref_reads --vsc_min_fraction_indels ""0.12"" --task {}. # perl: warning: Setting locale failed. # perl: warning: Please check that your locale settings:. # 	LANGUAGE = (unset),. # 	LC_ALL = (unset),. # 	LC_CTYPE = ""C.UTF-8"",. # 	LANG = ""en_US.UTF-8"". # are support",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:1618,modifiability,Interm,Intermediate,1618,"t with my own data (prior to installing nucleus as user), and the output and command from that are below:. ```. for bam in $READS; do. 	echo ""running deepvariant on $bam"". 	run_deepvariant --model_type=PACBIO --ref=$REF --reads=$bam --output_vcf=$OUTDIR/$bam.vcf.gz --output_gvcf=$OUTDIR/$bam.g.vcf.gz --logging_dir=$LOGDIR --num_shards=$CORES. 	echo ""finished with $bam"". done. #output in block comment below. # running deepvariant on /work/cjm124/SWFst/VarCalling/reads/bc2001_aligned_sorted.bam. # 2024-04-23 11:42:51.281492: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA. # To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. # I0423 11:42:57.943745 140073410221888 run_deepvariant.py:519] Re-using the directory for intermediate results in /tmp/tmpkmab_2kw. # ***** Intermediate results will be written to /tmp/tmpkmab_2kw in docker. ****. # ***** Running the command:*****. # time seq 0 11 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/work/cjm124/SWFst/lvar3ref/Lvar_scaffolds.fasta"" --reads ""/work/cjm124/SWFst/VarCalling/reads/bc2001_aligned_sorted.bam"" --examples ""/tmp/tmpkmab_2kw/make_examples.tfrecord@12.gz"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --gvcf ""/tmp/tmpkmab_2kw/gvcf.tfrecord@12.gz"" --max_reads_per_partition ""600"" --min_mapping_quality ""1"" --parse_sam_aux_fields --partition_size ""25000"" --phase_reads --pileup_image_width ""199"" --norealign_reads --sort_by_haplotypes --track_ref_reads --vsc_min_fraction_indels ""0.12"" --task {}. # perl: warning: Setting locale failed. # perl: warning: Please check that your locale settings:. # 	LANGUAGE = (unset),. # 	LC_ALL = (unset),. # 	LC_CTYPE = ""C.UTF-8"",. # 	LANG = ""en_US.UTF-8"". # are supported and installed on your system. # perl: warning: ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:6018,modifiability,deco,decode,6018,"'unplaced_scaffold70', 'unplaced_scaffold71', 'unplaced_scaffold72', 'unplaced_scaffold73', 'unplaced_scaffold74', 'unplaced_scaffold75', 'unplaced_scaffold76', 'unplaced_scaffold77', 'unplaced_scaffold78', 'unplaced_scaffold79', 'unplaced_scaffold80', 'unplaced_scaffold81', 'unplaced_scaffold82', 'unplaced_scaffold83', 'unplaced_scaffold84', 'unplaced_scaffold85', 'unplaced_scaffold86', 'unplaced_scaffold87', 'unplaced_scaffold88', 'unplaced_scaffold89', 'unplaced_scaffold90', 'unplaced_scaffold91', 'unplaced_scaffold92', 'unplaced_scaffold93', 'unplaced_scaffold94', 'unplaced_scaffold95', 'unplaced_scaffold96', 'unplaced_scaffold97', 'unplaced_scaffold98', 'unplaced_scaffold99', 'unplaced_scaffold100', 'unplaced_scaffold101', 'unplaced_scaffold102', 'unplaced_scaffold103', 'unplaced_scaffold104']. # I0423 11:43:12.466705 140211385890624 make_examples_core.py:301] Task 0/12: Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. # I0423 11:43:12.538744 140211385890624 genomics_reader.py:222] Reading /work/cjm124/SWFst/VarCalling/reads/bc2001_aligned_sorted.bam with NativeSamReader. # I0423 11:43:12.636761 140211385890624 genomics_reader.py:222] Reading /work/cjm124/SWFst/VarCalling/reads/bc2001_aligned_sorted.bam with NativeSamReader. # I0423 11:43:12.637369 140211385890624 make_examples_core.py:301] Task 0/12: Writing gvcf records to /tmp/tmpkmab_2kw/gvcf.tfrecord-00000-of-00012.gz. # I0423 11:43:12.637865 140211385890624 make_examples_core.py:301] Task 0/12: Writing examples to /tmp/tmpkmab_2kw/make_examples.tfrecord-00000-of-00012.gz. # I0423 11:43:12.637962 140211385890624 make_examples_core.py:301] Task 0/12: Overhead for preparing inputs: 0 seconds. # 2024-04-23 11:43:12.645232: W ./third_party/nucleus/util/proto_clif_converter.h:75] Failed to cast type N6google8protobuf14DynamicMessageE. # Traceback (most recent call last):. # File ""/tmp/Bazel.runfi",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:7111,modifiability,modul,module,7111,"4 genomics_reader.py:222] Reading /work/cjm124/SWFst/VarCalling/reads/bc2001_aligned_sorted.bam with NativeSamReader. # I0423 11:43:12.636761 140211385890624 genomics_reader.py:222] Reading /work/cjm124/SWFst/VarCalling/reads/bc2001_aligned_sorted.bam with NativeSamReader. # I0423 11:43:12.637369 140211385890624 make_examples_core.py:301] Task 0/12: Writing gvcf records to /tmp/tmpkmab_2kw/gvcf.tfrecord-00000-of-00012.gz. # I0423 11:43:12.637865 140211385890624 make_examples_core.py:301] Task 0/12: Writing examples to /tmp/tmpkmab_2kw/make_examples.tfrecord-00000-of-00012.gz. # I0423 11:43:12.637962 140211385890624 make_examples_core.py:301] Task 0/12: Overhead for preparing inputs: 0 seconds. # 2024-04-23 11:43:12.645232: W ./third_party/nucleus/util/proto_clif_converter.h:75] Failed to cast type N6google8protobuf14DynamicMessageE. # Traceback (most recent call last):. # File ""/tmp/Bazel.runfiles_qy0tffir/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 234, in <module>. # app.run(main). # File ""/tmp/Bazel.runfiles_qy0tffir/runfiles/absl_py/absl/app.py"", line 312, in run. # _run_main(main, args). # File ""/tmp/Bazel.runfiles_qy0tffir/runfiles/absl_py/absl/app.py"", line 258, in _run_main. # sys.exit(main(argv)). # File ""/tmp/Bazel.runfiles_qy0tffir/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 224, in main. # make_examples_core.make_examples_runner(options). # File ""/tmp/Bazel.runfiles_qy0tffir/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2838, in make_examples_runner. # region_processor.process(region, region_n). # File ""/tmp/Bazel.runfiles_qy0tffir/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1695, in process. # sample_reads = self.region_reads_norealign(. # File ""/tmp/Bazel.runfiles_qy0tffir/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1817, in region_reads_norealign. # reads = reservoir_sample_reads(. # File ""/tmp/Bazel.runfiles_qy0tffir/ru",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:449,performance,error,error,449,"The command for installing DeepVariant? . `singularity build DeepVariant_1.6.1.sif docker://google/deepvariant:1.6.1`. Or the full command that is written to stdout when DeepVariant runs? For this latter case, after installing nucleus the full command for deepvariant is not written to output. The last line of output is `KeyError: 'SerializedDType'`. I dont have the output saved from the test data run to retrieve the full command output with the error prior to installing nucleus as user, and will re-run that and update this comment with it in a few hours. I did, however, get the same error attempting to run deepvariant with my own data (prior to installing nucleus as user), and the output and command from that are below:. ```. for bam in $READS; do. 	echo ""running deepvariant on $bam"". 	run_deepvariant --model_type=PACBIO --ref=$REF --reads=$bam --output_vcf=$OUTDIR/$bam.vcf.gz --output_gvcf=$OUTDIR/$bam.g.vcf.gz --logging_dir=$LOGDIR --num_shards=$CORES. 	echo ""finished with $bam"". done. #output in block comment below. # running deepvariant on /work/cjm124/SWFst/VarCalling/reads/bc2001_aligned_sorted.bam. # 2024-04-23 11:42:51.281492: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA. # To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. # I0423 11:42:57.943745 140073410221888 run_deepvariant.py:519] Re-using the directory for intermediate results in /tmp/tmpkmab_2kw. # ***** Intermediate results will be written to /tmp/tmpkmab_2kw in docker. ****. # ***** Running the command:*****. # time seq 0 11 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/work/cjm124/SWFst/lvar3ref/Lvar_scaffolds.fasta"" --reads ""/work/cjm124/SWFst/VarCalling/reads/bc2001_aligned_sorted.bam"" --examples ""/tmp/tmpkmab_2kw/make_example",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:590,performance,error,error,590,"The command for installing DeepVariant? . `singularity build DeepVariant_1.6.1.sif docker://google/deepvariant:1.6.1`. Or the full command that is written to stdout when DeepVariant runs? For this latter case, after installing nucleus the full command for deepvariant is not written to output. The last line of output is `KeyError: 'SerializedDType'`. I dont have the output saved from the test data run to retrieve the full command output with the error prior to installing nucleus as user, and will re-run that and update this comment with it in a few hours. I did, however, get the same error attempting to run deepvariant with my own data (prior to installing nucleus as user), and the output and command from that are below:. ```. for bam in $READS; do. 	echo ""running deepvariant on $bam"". 	run_deepvariant --model_type=PACBIO --ref=$REF --reads=$bam --output_vcf=$OUTDIR/$bam.vcf.gz --output_gvcf=$OUTDIR/$bam.g.vcf.gz --logging_dir=$LOGDIR --num_shards=$CORES. 	echo ""finished with $bam"". done. #output in block comment below. # running deepvariant on /work/cjm124/SWFst/VarCalling/reads/bc2001_aligned_sorted.bam. # 2024-04-23 11:42:51.281492: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA. # To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. # I0423 11:42:57.943745 140073410221888 run_deepvariant.py:519] Re-using the directory for intermediate results in /tmp/tmpkmab_2kw. # ***** Intermediate results will be written to /tmp/tmpkmab_2kw in docker. ****. # ***** Running the command:*****. # time seq 0 11 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/work/cjm124/SWFst/lvar3ref/Lvar_scaffolds.fasta"" --reads ""/work/cjm124/SWFst/VarCalling/reads/bc2001_aligned_sorted.bam"" --examples ""/tmp/tmpkmab_2kw/make_example",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:1232,performance,optimiz,optimized,1232,"e full command for deepvariant is not written to output. The last line of output is `KeyError: 'SerializedDType'`. I dont have the output saved from the test data run to retrieve the full command output with the error prior to installing nucleus as user, and will re-run that and update this comment with it in a few hours. I did, however, get the same error attempting to run deepvariant with my own data (prior to installing nucleus as user), and the output and command from that are below:. ```. for bam in $READS; do. 	echo ""running deepvariant on $bam"". 	run_deepvariant --model_type=PACBIO --ref=$REF --reads=$bam --output_vcf=$OUTDIR/$bam.vcf.gz --output_gvcf=$OUTDIR/$bam.g.vcf.gz --logging_dir=$LOGDIR --num_shards=$CORES. 	echo ""finished with $bam"". done. #output in block comment below. # running deepvariant on /work/cjm124/SWFst/VarCalling/reads/bc2001_aligned_sorted.bam. # 2024-04-23 11:42:51.281492: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA. # To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. # I0423 11:42:57.943745 140073410221888 run_deepvariant.py:519] Re-using the directory for intermediate results in /tmp/tmpkmab_2kw. # ***** Intermediate results will be written to /tmp/tmpkmab_2kw in docker. ****. # ***** Running the command:*****. # time seq 0 11 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/work/cjm124/SWFst/lvar3ref/Lvar_scaffolds.fasta"" --reads ""/work/cjm124/SWFst/VarCalling/reads/bc2001_aligned_sorted.bam"" --examples ""/tmp/tmpkmab_2kw/make_examples.tfrecord@12.gz"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --gvcf ""/tmp/tmpkmab_2kw/gvcf.tfrecord@12.gz"" --max_reads_per_partition ""600"" --min_mapping_quality ""1"" --parse_sam_aux_fields --partition_size ""25000"" --phase_reads",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:1266,performance,Network,Network,1266," not written to output. The last line of output is `KeyError: 'SerializedDType'`. I dont have the output saved from the test data run to retrieve the full command output with the error prior to installing nucleus as user, and will re-run that and update this comment with it in a few hours. I did, however, get the same error attempting to run deepvariant with my own data (prior to installing nucleus as user), and the output and command from that are below:. ```. for bam in $READS; do. 	echo ""running deepvariant on $bam"". 	run_deepvariant --model_type=PACBIO --ref=$REF --reads=$bam --output_vcf=$OUTDIR/$bam.vcf.gz --output_gvcf=$OUTDIR/$bam.g.vcf.gz --logging_dir=$LOGDIR --num_shards=$CORES. 	echo ""finished with $bam"". done. #output in block comment below. # running deepvariant on /work/cjm124/SWFst/VarCalling/reads/bc2001_aligned_sorted.bam. # 2024-04-23 11:42:51.281492: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA. # To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. # I0423 11:42:57.943745 140073410221888 run_deepvariant.py:519] Re-using the directory for intermediate results in /tmp/tmpkmab_2kw. # ***** Intermediate results will be written to /tmp/tmpkmab_2kw in docker. ****. # ***** Running the command:*****. # time seq 0 11 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/work/cjm124/SWFst/lvar3ref/Lvar_scaffolds.fasta"" --reads ""/work/cjm124/SWFst/VarCalling/reads/bc2001_aligned_sorted.bam"" --examples ""/tmp/tmpkmab_2kw/make_examples.tfrecord@12.gz"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --gvcf ""/tmp/tmpkmab_2kw/gvcf.tfrecord@12.gz"" --max_reads_per_partition ""600"" --min_mapping_quality ""1"" --parse_sam_aux_fields --partition_size ""25000"" --phase_reads --pileup_image_width ""199"" --nor",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:1312,performance,CPU,CPU,1312,"put is `KeyError: 'SerializedDType'`. I dont have the output saved from the test data run to retrieve the full command output with the error prior to installing nucleus as user, and will re-run that and update this comment with it in a few hours. I did, however, get the same error attempting to run deepvariant with my own data (prior to installing nucleus as user), and the output and command from that are below:. ```. for bam in $READS; do. 	echo ""running deepvariant on $bam"". 	run_deepvariant --model_type=PACBIO --ref=$REF --reads=$bam --output_vcf=$OUTDIR/$bam.vcf.gz --output_gvcf=$OUTDIR/$bam.g.vcf.gz --logging_dir=$LOGDIR --num_shards=$CORES. 	echo ""finished with $bam"". done. #output in block comment below. # running deepvariant on /work/cjm124/SWFst/VarCalling/reads/bc2001_aligned_sorted.bam. # 2024-04-23 11:42:51.281492: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA. # To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. # I0423 11:42:57.943745 140073410221888 run_deepvariant.py:519] Re-using the directory for intermediate results in /tmp/tmpkmab_2kw. # ***** Intermediate results will be written to /tmp/tmpkmab_2kw in docker. ****. # ***** Running the command:*****. # time seq 0 11 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/work/cjm124/SWFst/lvar3ref/Lvar_scaffolds.fasta"" --reads ""/work/cjm124/SWFst/VarCalling/reads/bc2001_aligned_sorted.bam"" --examples ""/tmp/tmpkmab_2kw/make_examples.tfrecord@12.gz"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --gvcf ""/tmp/tmpkmab_2kw/gvcf.tfrecord@12.gz"" --max_reads_per_partition ""600"" --min_mapping_quality ""1"" --parse_sam_aux_fields --partition_size ""25000"" --phase_reads --pileup_image_width ""199"" --norealign_reads --sort_by_haplotypes --track_re",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:1332,performance,perform,performance-critical,1332,"dDType'`. I dont have the output saved from the test data run to retrieve the full command output with the error prior to installing nucleus as user, and will re-run that and update this comment with it in a few hours. I did, however, get the same error attempting to run deepvariant with my own data (prior to installing nucleus as user), and the output and command from that are below:. ```. for bam in $READS; do. 	echo ""running deepvariant on $bam"". 	run_deepvariant --model_type=PACBIO --ref=$REF --reads=$bam --output_vcf=$OUTDIR/$bam.vcf.gz --output_gvcf=$OUTDIR/$bam.g.vcf.gz --logging_dir=$LOGDIR --num_shards=$CORES. 	echo ""finished with $bam"". done. #output in block comment below. # running deepvariant on /work/cjm124/SWFst/VarCalling/reads/bc2001_aligned_sorted.bam. # 2024-04-23 11:42:51.281492: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA. # To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. # I0423 11:42:57.943745 140073410221888 run_deepvariant.py:519] Re-using the directory for intermediate results in /tmp/tmpkmab_2kw. # ***** Intermediate results will be written to /tmp/tmpkmab_2kw in docker. ****. # ***** Running the command:*****. # time seq 0 11 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/work/cjm124/SWFst/lvar3ref/Lvar_scaffolds.fasta"" --reads ""/work/cjm124/SWFst/VarCalling/reads/bc2001_aligned_sorted.bam"" --examples ""/tmp/tmpkmab_2kw/make_examples.tfrecord@12.gz"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --gvcf ""/tmp/tmpkmab_2kw/gvcf.tfrecord@12.gz"" --max_reads_per_partition ""600"" --min_mapping_quality ""1"" --parse_sam_aux_fields --partition_size ""25000"" --phase_reads --pileup_image_width ""199"" --norealign_reads --sort_by_haplotypes --track_ref_reads --vsc_min_fraction_i",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:1729,performance,time,time,1729,"```. for bam in $READS; do. 	echo ""running deepvariant on $bam"". 	run_deepvariant --model_type=PACBIO --ref=$REF --reads=$bam --output_vcf=$OUTDIR/$bam.vcf.gz --output_gvcf=$OUTDIR/$bam.g.vcf.gz --logging_dir=$LOGDIR --num_shards=$CORES. 	echo ""finished with $bam"". done. #output in block comment below. # running deepvariant on /work/cjm124/SWFst/VarCalling/reads/bc2001_aligned_sorted.bam. # 2024-04-23 11:42:51.281492: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA. # To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. # I0423 11:42:57.943745 140073410221888 run_deepvariant.py:519] Re-using the directory for intermediate results in /tmp/tmpkmab_2kw. # ***** Intermediate results will be written to /tmp/tmpkmab_2kw in docker. ****. # ***** Running the command:*****. # time seq 0 11 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/work/cjm124/SWFst/lvar3ref/Lvar_scaffolds.fasta"" --reads ""/work/cjm124/SWFst/VarCalling/reads/bc2001_aligned_sorted.bam"" --examples ""/tmp/tmpkmab_2kw/make_examples.tfrecord@12.gz"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --gvcf ""/tmp/tmpkmab_2kw/gvcf.tfrecord@12.gz"" --max_reads_per_partition ""600"" --min_mapping_quality ""1"" --parse_sam_aux_fields --partition_size ""25000"" --phase_reads --pileup_image_width ""199"" --norealign_reads --sort_by_haplotypes --track_ref_reads --vsc_min_fraction_indels ""0.12"" --task {}. # perl: warning: Setting locale failed. # perl: warning: Please check that your locale settings:. # 	LANGUAGE = (unset),. # 	LC_ALL = (unset),. # 	LC_CTYPE = ""C.UTF-8"",. # 	LANG = ""en_US.UTF-8"". # are supported and installed on your system. # perl: warning: Falling back to the standard locale (""C""). # perl: warning: Setting locale failed. # perl: warning: Please ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:1745,performance,parallel,parallel,1745,"EADS; do. 	echo ""running deepvariant on $bam"". 	run_deepvariant --model_type=PACBIO --ref=$REF --reads=$bam --output_vcf=$OUTDIR/$bam.vcf.gz --output_gvcf=$OUTDIR/$bam.g.vcf.gz --logging_dir=$LOGDIR --num_shards=$CORES. 	echo ""finished with $bam"". done. #output in block comment below. # running deepvariant on /work/cjm124/SWFst/VarCalling/reads/bc2001_aligned_sorted.bam. # 2024-04-23 11:42:51.281492: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA. # To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. # I0423 11:42:57.943745 140073410221888 run_deepvariant.py:519] Re-using the directory for intermediate results in /tmp/tmpkmab_2kw. # ***** Intermediate results will be written to /tmp/tmpkmab_2kw in docker. ****. # ***** Running the command:*****. # time seq 0 11 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/work/cjm124/SWFst/lvar3ref/Lvar_scaffolds.fasta"" --reads ""/work/cjm124/SWFst/VarCalling/reads/bc2001_aligned_sorted.bam"" --examples ""/tmp/tmpkmab_2kw/make_examples.tfrecord@12.gz"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --gvcf ""/tmp/tmpkmab_2kw/gvcf.tfrecord@12.gz"" --max_reads_per_partition ""600"" --min_mapping_quality ""1"" --parse_sam_aux_fields --partition_size ""25000"" --phase_reads --pileup_image_width ""199"" --norealign_reads --sort_by_haplotypes --track_ref_reads --vsc_min_fraction_indels ""0.12"" --task {}. # perl: warning: Setting locale failed. # perl: warning: Please check that your locale settings:. # 	LANGUAGE = (unset),. # 	LC_ALL = (unset),. # 	LC_CTYPE = ""C.UTF-8"",. # 	LANG = ""en_US.UTF-8"". # are supported and installed on your system. # perl: warning: Falling back to the standard locale (""C""). # perl: warning: Setting locale failed. # perl: warning: Please check that your lo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:6775,performance,Overhead,Overhead,6775,"2', 'unplaced_scaffold103', 'unplaced_scaffold104']. # I0423 11:43:12.466705 140211385890624 make_examples_core.py:301] Task 0/12: Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. # I0423 11:43:12.538744 140211385890624 genomics_reader.py:222] Reading /work/cjm124/SWFst/VarCalling/reads/bc2001_aligned_sorted.bam with NativeSamReader. # I0423 11:43:12.636761 140211385890624 genomics_reader.py:222] Reading /work/cjm124/SWFst/VarCalling/reads/bc2001_aligned_sorted.bam with NativeSamReader. # I0423 11:43:12.637369 140211385890624 make_examples_core.py:301] Task 0/12: Writing gvcf records to /tmp/tmpkmab_2kw/gvcf.tfrecord-00000-of-00012.gz. # I0423 11:43:12.637865 140211385890624 make_examples_core.py:301] Task 0/12: Writing examples to /tmp/tmpkmab_2kw/make_examples.tfrecord-00000-of-00012.gz. # I0423 11:43:12.637962 140211385890624 make_examples_core.py:301] Task 0/12: Overhead for preparing inputs: 0 seconds. # 2024-04-23 11:43:12.645232: W ./third_party/nucleus/util/proto_clif_converter.h:75] Failed to cast type N6google8protobuf14DynamicMessageE. # Traceback (most recent call last):. # File ""/tmp/Bazel.runfiles_qy0tffir/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 234, in <module>. # app.run(main). # File ""/tmp/Bazel.runfiles_qy0tffir/runfiles/absl_py/absl/app.py"", line 312, in run. # _run_main(main, args). # File ""/tmp/Bazel.runfiles_qy0tffir/runfiles/absl_py/absl/app.py"", line 258, in _run_main. # sys.exit(main(argv)). # File ""/tmp/Bazel.runfiles_qy0tffir/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 224, in main. # make_examples_core.make_examples_runner(options). # File ""/tmp/Bazel.runfiles_qy0tffir/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2838, in make_examples_runner. # region_processor.process(region, region_n). # File ""/tmp/Bazel.runfiles_qy0tffir/runfiles/com_google_",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:8893,performance,parallel,parallel,8893,"examples_core.py"", line 2838, in make_examples_runner. # region_processor.process(region, region_n). # File ""/tmp/Bazel.runfiles_qy0tffir/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1695, in process. # sample_reads = self.region_reads_norealign(. # File ""/tmp/Bazel.runfiles_qy0tffir/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1817, in region_reads_norealign. # reads = reservoir_sample_reads(. # File ""/tmp/Bazel.runfiles_qy0tffir/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 976, in reservoir_sample_reads. # return utils.reservoir_sample(iterable_of_reads, k, random). # File ""/tmp/Bazel.runfiles_qy0tffir/runfiles/com_google_deepvariant/third_party/nucleus/util/utils.py"", line 117, in reservoir_sample. # for i, item in enumerate(iterable):. # File ""/tmp/Bazel.runfiles_qy0tffir/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 95, in __next__. # record, not_done = self._raw_next(). # File ""/tmp/Bazel.runfiles_qy0tffir/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 154, in _raw_next. # not_done = self._cc_iterable.PythonNext(record). # RuntimeError: PythonNext() argument read is not valid: Dynamic cast failed. # parallel: This job failed:. # /opt/deepvariant/bin/make_examples --mode calling --ref /work/cjm124/SWFst/lvar3ref/Lvar_scaffolds.fasta --reads /work/cjm124/SWFst/VarCalling/reads/bc2001_aligned_sorted.bam --examples /tmp/tmpkmab_2kw/make_examples.tfrecord@12.gz --add_hp_channel --alt_aligned_pileup diff_channels --gvcf /tmp/tmpkmab_2kw/gvcf.tfrecord@12.gz --max_reads_per_partition 600 --min_mapping_quality 1 --parse_sam_aux_fields --partition_size 25000 --phase_reads --pileup_image_width 199 --norealign_reads --sort_by_haplotypes --track_ref_reads --vsc_min_fraction_indels 0.12 --task 0. # real	0m15.200s. # user	0m3.162s. # sys	0m1.161s. # finished with /work/cjm124/SWFst/VarCalling/reads/bc2001_aligned_sorted.bam. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:2398,reliability,fail,failed,2398,"n other operations, rebuild TensorFlow with the appropriate compiler flags. # I0423 11:42:57.943745 140073410221888 run_deepvariant.py:519] Re-using the directory for intermediate results in /tmp/tmpkmab_2kw. # ***** Intermediate results will be written to /tmp/tmpkmab_2kw in docker. ****. # ***** Running the command:*****. # time seq 0 11 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/work/cjm124/SWFst/lvar3ref/Lvar_scaffolds.fasta"" --reads ""/work/cjm124/SWFst/VarCalling/reads/bc2001_aligned_sorted.bam"" --examples ""/tmp/tmpkmab_2kw/make_examples.tfrecord@12.gz"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --gvcf ""/tmp/tmpkmab_2kw/gvcf.tfrecord@12.gz"" --max_reads_per_partition ""600"" --min_mapping_quality ""1"" --parse_sam_aux_fields --partition_size ""25000"" --phase_reads --pileup_image_width ""199"" --norealign_reads --sort_by_haplotypes --track_ref_reads --vsc_min_fraction_indels ""0.12"" --task {}. # perl: warning: Setting locale failed. # perl: warning: Please check that your locale settings:. # 	LANGUAGE = (unset),. # 	LC_ALL = (unset),. # 	LC_CTYPE = ""C.UTF-8"",. # 	LANG = ""en_US.UTF-8"". # are supported and installed on your system. # perl: warning: Falling back to the standard locale (""C""). # perl: warning: Setting locale failed. # perl: warning: Please check that your locale settings:. # 	LANGUAGE = (unset),. # 	LC_ALL = (unset),. # 	LC_CTYPE = ""C.UTF-8"",. # 	LANG = ""en_US.UTF-8"". # are supported and installed on your system. # perl: warning: Falling back to the standard locale (""C""). # I0423 11:43:12.358298 140211385890624 genomics_reader.py:222] Reading /work/cjm124/SWFst/VarCalling/reads/bc2001_aligned_sorted.bam with NativeSamReader. # W0423 11:43:12.358482 140211385890624 make_examples_core.py:344] No non-empty sample name found in the input reads. DeepVariant will use default as the sample name. You can also provide a sample name with the --sample_name argument. # I0423 11:43:12.365553 140211385890624 ma",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:2699,reliability,fail,failed,2699,"nning the command:*****. # time seq 0 11 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/work/cjm124/SWFst/lvar3ref/Lvar_scaffolds.fasta"" --reads ""/work/cjm124/SWFst/VarCalling/reads/bc2001_aligned_sorted.bam"" --examples ""/tmp/tmpkmab_2kw/make_examples.tfrecord@12.gz"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --gvcf ""/tmp/tmpkmab_2kw/gvcf.tfrecord@12.gz"" --max_reads_per_partition ""600"" --min_mapping_quality ""1"" --parse_sam_aux_fields --partition_size ""25000"" --phase_reads --pileup_image_width ""199"" --norealign_reads --sort_by_haplotypes --track_ref_reads --vsc_min_fraction_indels ""0.12"" --task {}. # perl: warning: Setting locale failed. # perl: warning: Please check that your locale settings:. # 	LANGUAGE = (unset),. # 	LC_ALL = (unset),. # 	LC_CTYPE = ""C.UTF-8"",. # 	LANG = ""en_US.UTF-8"". # are supported and installed on your system. # perl: warning: Falling back to the standard locale (""C""). # perl: warning: Setting locale failed. # perl: warning: Please check that your locale settings:. # 	LANGUAGE = (unset),. # 	LC_ALL = (unset),. # 	LC_CTYPE = ""C.UTF-8"",. # 	LANG = ""en_US.UTF-8"". # are supported and installed on your system. # perl: warning: Falling back to the standard locale (""C""). # I0423 11:43:12.358298 140211385890624 genomics_reader.py:222] Reading /work/cjm124/SWFst/VarCalling/reads/bc2001_aligned_sorted.bam with NativeSamReader. # W0423 11:43:12.358482 140211385890624 make_examples_core.py:344] No non-empty sample name found in the input reads. DeepVariant will use default as the sample name. You can also provide a sample name with the --sample_name argument. # I0423 11:43:12.365553 140211385890624 make_examples_core.py:301] Task 0/12: Preparing inputs. # I0423 11:43:12.377128 140211385890624 genomics_reader.py:222] Reading /work/cjm124/SWFst/VarCalling/reads/bc2001_aligned_sorted.bam with NativeSamReader. # I0423 11:43:12.405545 140211385890624 make_examples_core.py:301] Task 0/12: Common contig",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:6903,reliability,Fail,Failed,6903,"12: Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. # I0423 11:43:12.538744 140211385890624 genomics_reader.py:222] Reading /work/cjm124/SWFst/VarCalling/reads/bc2001_aligned_sorted.bam with NativeSamReader. # I0423 11:43:12.636761 140211385890624 genomics_reader.py:222] Reading /work/cjm124/SWFst/VarCalling/reads/bc2001_aligned_sorted.bam with NativeSamReader. # I0423 11:43:12.637369 140211385890624 make_examples_core.py:301] Task 0/12: Writing gvcf records to /tmp/tmpkmab_2kw/gvcf.tfrecord-00000-of-00012.gz. # I0423 11:43:12.637865 140211385890624 make_examples_core.py:301] Task 0/12: Writing examples to /tmp/tmpkmab_2kw/make_examples.tfrecord-00000-of-00012.gz. # I0423 11:43:12.637962 140211385890624 make_examples_core.py:301] Task 0/12: Overhead for preparing inputs: 0 seconds. # 2024-04-23 11:43:12.645232: W ./third_party/nucleus/util/proto_clif_converter.h:75] Failed to cast type N6google8protobuf14DynamicMessageE. # Traceback (most recent call last):. # File ""/tmp/Bazel.runfiles_qy0tffir/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 234, in <module>. # app.run(main). # File ""/tmp/Bazel.runfiles_qy0tffir/runfiles/absl_py/absl/app.py"", line 312, in run. # _run_main(main, args). # File ""/tmp/Bazel.runfiles_qy0tffir/runfiles/absl_py/absl/app.py"", line 258, in _run_main. # sys.exit(main(argv)). # File ""/tmp/Bazel.runfiles_qy0tffir/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 224, in main. # make_examples_core.make_examples_runner(options). # File ""/tmp/Bazel.runfiles_qy0tffir/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2838, in make_examples_runner. # region_processor.process(region, region_n). # File ""/tmp/Bazel.runfiles_qy0tffir/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1695, in process. # sample_reads = self.region_reads_norealign(. # File ""/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:8883,reliability,fail,failed,8883,"examples_core.py"", line 2838, in make_examples_runner. # region_processor.process(region, region_n). # File ""/tmp/Bazel.runfiles_qy0tffir/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1695, in process. # sample_reads = self.region_reads_norealign(. # File ""/tmp/Bazel.runfiles_qy0tffir/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1817, in region_reads_norealign. # reads = reservoir_sample_reads(. # File ""/tmp/Bazel.runfiles_qy0tffir/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 976, in reservoir_sample_reads. # return utils.reservoir_sample(iterable_of_reads, k, random). # File ""/tmp/Bazel.runfiles_qy0tffir/runfiles/com_google_deepvariant/third_party/nucleus/util/utils.py"", line 117, in reservoir_sample. # for i, item in enumerate(iterable):. # File ""/tmp/Bazel.runfiles_qy0tffir/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 95, in __next__. # record, not_done = self._raw_next(). # File ""/tmp/Bazel.runfiles_qy0tffir/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 154, in _raw_next. # not_done = self._cc_iterable.PythonNext(record). # RuntimeError: PythonNext() argument read is not valid: Dynamic cast failed. # parallel: This job failed:. # /opt/deepvariant/bin/make_examples --mode calling --ref /work/cjm124/SWFst/lvar3ref/Lvar_scaffolds.fasta --reads /work/cjm124/SWFst/VarCalling/reads/bc2001_aligned_sorted.bam --examples /tmp/tmpkmab_2kw/make_examples.tfrecord@12.gz --add_hp_channel --alt_aligned_pileup diff_channels --gvcf /tmp/tmpkmab_2kw/gvcf.tfrecord@12.gz --max_reads_per_partition 600 --min_mapping_quality 1 --parse_sam_aux_fields --partition_size 25000 --phase_reads --pileup_image_width 199 --norealign_reads --sort_by_haplotypes --track_ref_reads --vsc_min_fraction_indels 0.12 --task 0. # real	0m15.200s. # user	0m3.162s. # sys	0m1.161s. # finished with /work/cjm124/SWFst/VarCalling/reads/bc2001_aligned_sorted.bam. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:8912,reliability,fail,failed,8912,"examples_core.py"", line 2838, in make_examples_runner. # region_processor.process(region, region_n). # File ""/tmp/Bazel.runfiles_qy0tffir/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1695, in process. # sample_reads = self.region_reads_norealign(. # File ""/tmp/Bazel.runfiles_qy0tffir/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1817, in region_reads_norealign. # reads = reservoir_sample_reads(. # File ""/tmp/Bazel.runfiles_qy0tffir/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 976, in reservoir_sample_reads. # return utils.reservoir_sample(iterable_of_reads, k, random). # File ""/tmp/Bazel.runfiles_qy0tffir/runfiles/com_google_deepvariant/third_party/nucleus/util/utils.py"", line 117, in reservoir_sample. # for i, item in enumerate(iterable):. # File ""/tmp/Bazel.runfiles_qy0tffir/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 95, in __next__. # record, not_done = self._raw_next(). # File ""/tmp/Bazel.runfiles_qy0tffir/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 154, in _raw_next. # not_done = self._cc_iterable.PythonNext(record). # RuntimeError: PythonNext() argument read is not valid: Dynamic cast failed. # parallel: This job failed:. # /opt/deepvariant/bin/make_examples --mode calling --ref /work/cjm124/SWFst/lvar3ref/Lvar_scaffolds.fasta --reads /work/cjm124/SWFst/VarCalling/reads/bc2001_aligned_sorted.bam --examples /tmp/tmpkmab_2kw/make_examples.tfrecord@12.gz --add_hp_channel --alt_aligned_pileup diff_channels --gvcf /tmp/tmpkmab_2kw/gvcf.tfrecord@12.gz --max_reads_per_partition 600 --min_mapping_quality 1 --parse_sam_aux_fields --partition_size 25000 --phase_reads --pileup_image_width 199 --norealign_reads --sort_by_haplotypes --track_ref_reads --vsc_min_fraction_indels 0.12 --task 0. # real	0m15.200s. # user	0m3.162s. # sys	0m1.161s. # finished with /work/cjm124/SWFst/VarCalling/reads/bc2001_aligned_sorted.bam. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:390,safety,test,test,390,"The command for installing DeepVariant? . `singularity build DeepVariant_1.6.1.sif docker://google/deepvariant:1.6.1`. Or the full command that is written to stdout when DeepVariant runs? For this latter case, after installing nucleus the full command for deepvariant is not written to output. The last line of output is `KeyError: 'SerializedDType'`. I dont have the output saved from the test data run to retrieve the full command output with the error prior to installing nucleus as user, and will re-run that and update this comment with it in a few hours. I did, however, get the same error attempting to run deepvariant with my own data (prior to installing nucleus as user), and the output and command from that are below:. ```. for bam in $READS; do. 	echo ""running deepvariant on $bam"". 	run_deepvariant --model_type=PACBIO --ref=$REF --reads=$bam --output_vcf=$OUTDIR/$bam.vcf.gz --output_gvcf=$OUTDIR/$bam.g.vcf.gz --logging_dir=$LOGDIR --num_shards=$CORES. 	echo ""finished with $bam"". done. #output in block comment below. # running deepvariant on /work/cjm124/SWFst/VarCalling/reads/bc2001_aligned_sorted.bam. # 2024-04-23 11:42:51.281492: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA. # To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. # I0423 11:42:57.943745 140073410221888 run_deepvariant.py:519] Re-using the directory for intermediate results in /tmp/tmpkmab_2kw. # ***** Intermediate results will be written to /tmp/tmpkmab_2kw in docker. ****. # ***** Running the command:*****. # time seq 0 11 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/work/cjm124/SWFst/lvar3ref/Lvar_scaffolds.fasta"" --reads ""/work/cjm124/SWFst/VarCalling/reads/bc2001_aligned_sorted.bam"" --examples ""/tmp/tmpkmab_2kw/make_example",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:449,safety,error,error,449,"The command for installing DeepVariant? . `singularity build DeepVariant_1.6.1.sif docker://google/deepvariant:1.6.1`. Or the full command that is written to stdout when DeepVariant runs? For this latter case, after installing nucleus the full command for deepvariant is not written to output. The last line of output is `KeyError: 'SerializedDType'`. I dont have the output saved from the test data run to retrieve the full command output with the error prior to installing nucleus as user, and will re-run that and update this comment with it in a few hours. I did, however, get the same error attempting to run deepvariant with my own data (prior to installing nucleus as user), and the output and command from that are below:. ```. for bam in $READS; do. 	echo ""running deepvariant on $bam"". 	run_deepvariant --model_type=PACBIO --ref=$REF --reads=$bam --output_vcf=$OUTDIR/$bam.vcf.gz --output_gvcf=$OUTDIR/$bam.g.vcf.gz --logging_dir=$LOGDIR --num_shards=$CORES. 	echo ""finished with $bam"". done. #output in block comment below. # running deepvariant on /work/cjm124/SWFst/VarCalling/reads/bc2001_aligned_sorted.bam. # 2024-04-23 11:42:51.281492: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA. # To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. # I0423 11:42:57.943745 140073410221888 run_deepvariant.py:519] Re-using the directory for intermediate results in /tmp/tmpkmab_2kw. # ***** Intermediate results will be written to /tmp/tmpkmab_2kw in docker. ****. # ***** Running the command:*****. # time seq 0 11 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/work/cjm124/SWFst/lvar3ref/Lvar_scaffolds.fasta"" --reads ""/work/cjm124/SWFst/VarCalling/reads/bc2001_aligned_sorted.bam"" --examples ""/tmp/tmpkmab_2kw/make_example",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:517,safety,updat,update,517,"The command for installing DeepVariant? . `singularity build DeepVariant_1.6.1.sif docker://google/deepvariant:1.6.1`. Or the full command that is written to stdout when DeepVariant runs? For this latter case, after installing nucleus the full command for deepvariant is not written to output. The last line of output is `KeyError: 'SerializedDType'`. I dont have the output saved from the test data run to retrieve the full command output with the error prior to installing nucleus as user, and will re-run that and update this comment with it in a few hours. I did, however, get the same error attempting to run deepvariant with my own data (prior to installing nucleus as user), and the output and command from that are below:. ```. for bam in $READS; do. 	echo ""running deepvariant on $bam"". 	run_deepvariant --model_type=PACBIO --ref=$REF --reads=$bam --output_vcf=$OUTDIR/$bam.vcf.gz --output_gvcf=$OUTDIR/$bam.g.vcf.gz --logging_dir=$LOGDIR --num_shards=$CORES. 	echo ""finished with $bam"". done. #output in block comment below. # running deepvariant on /work/cjm124/SWFst/VarCalling/reads/bc2001_aligned_sorted.bam. # 2024-04-23 11:42:51.281492: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA. # To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. # I0423 11:42:57.943745 140073410221888 run_deepvariant.py:519] Re-using the directory for intermediate results in /tmp/tmpkmab_2kw. # ***** Intermediate results will be written to /tmp/tmpkmab_2kw in docker. ****. # ***** Running the command:*****. # time seq 0 11 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/work/cjm124/SWFst/lvar3ref/Lvar_scaffolds.fasta"" --reads ""/work/cjm124/SWFst/VarCalling/reads/bc2001_aligned_sorted.bam"" --examples ""/tmp/tmpkmab_2kw/make_example",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:590,safety,error,error,590,"The command for installing DeepVariant? . `singularity build DeepVariant_1.6.1.sif docker://google/deepvariant:1.6.1`. Or the full command that is written to stdout when DeepVariant runs? For this latter case, after installing nucleus the full command for deepvariant is not written to output. The last line of output is `KeyError: 'SerializedDType'`. I dont have the output saved from the test data run to retrieve the full command output with the error prior to installing nucleus as user, and will re-run that and update this comment with it in a few hours. I did, however, get the same error attempting to run deepvariant with my own data (prior to installing nucleus as user), and the output and command from that are below:. ```. for bam in $READS; do. 	echo ""running deepvariant on $bam"". 	run_deepvariant --model_type=PACBIO --ref=$REF --reads=$bam --output_vcf=$OUTDIR/$bam.vcf.gz --output_gvcf=$OUTDIR/$bam.g.vcf.gz --logging_dir=$LOGDIR --num_shards=$CORES. 	echo ""finished with $bam"". done. #output in block comment below. # running deepvariant on /work/cjm124/SWFst/VarCalling/reads/bc2001_aligned_sorted.bam. # 2024-04-23 11:42:51.281492: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA. # To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. # I0423 11:42:57.943745 140073410221888 run_deepvariant.py:519] Re-using the directory for intermediate results in /tmp/tmpkmab_2kw. # ***** Intermediate results will be written to /tmp/tmpkmab_2kw in docker. ****. # ***** Running the command:*****. # time seq 0 11 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/work/cjm124/SWFst/lvar3ref/Lvar_scaffolds.fasta"" --reads ""/work/cjm124/SWFst/VarCalling/reads/bc2001_aligned_sorted.bam"" --examples ""/tmp/tmpkmab_2kw/make_example",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:941,safety,LOG,LOGDIR,941,"The command for installing DeepVariant? . `singularity build DeepVariant_1.6.1.sif docker://google/deepvariant:1.6.1`. Or the full command that is written to stdout when DeepVariant runs? For this latter case, after installing nucleus the full command for deepvariant is not written to output. The last line of output is `KeyError: 'SerializedDType'`. I dont have the output saved from the test data run to retrieve the full command output with the error prior to installing nucleus as user, and will re-run that and update this comment with it in a few hours. I did, however, get the same error attempting to run deepvariant with my own data (prior to installing nucleus as user), and the output and command from that are below:. ```. for bam in $READS; do. 	echo ""running deepvariant on $bam"". 	run_deepvariant --model_type=PACBIO --ref=$REF --reads=$bam --output_vcf=$OUTDIR/$bam.vcf.gz --output_gvcf=$OUTDIR/$bam.g.vcf.gz --logging_dir=$LOGDIR --num_shards=$CORES. 	echo ""finished with $bam"". done. #output in block comment below. # running deepvariant on /work/cjm124/SWFst/VarCalling/reads/bc2001_aligned_sorted.bam. # 2024-04-23 11:42:51.281492: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA. # To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. # I0423 11:42:57.943745 140073410221888 run_deepvariant.py:519] Re-using the directory for intermediate results in /tmp/tmpkmab_2kw. # ***** Intermediate results will be written to /tmp/tmpkmab_2kw in docker. ****. # ***** Running the command:*****. # time seq 0 11 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/work/cjm124/SWFst/lvar3ref/Lvar_scaffolds.fasta"" --reads ""/work/cjm124/SWFst/VarCalling/reads/bc2001_aligned_sorted.bam"" --examples ""/tmp/tmpkmab_2kw/make_example",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:3229,safety,input,input,3229,"reads --pileup_image_width ""199"" --norealign_reads --sort_by_haplotypes --track_ref_reads --vsc_min_fraction_indels ""0.12"" --task {}. # perl: warning: Setting locale failed. # perl: warning: Please check that your locale settings:. # 	LANGUAGE = (unset),. # 	LC_ALL = (unset),. # 	LC_CTYPE = ""C.UTF-8"",. # 	LANG = ""en_US.UTF-8"". # are supported and installed on your system. # perl: warning: Falling back to the standard locale (""C""). # perl: warning: Setting locale failed. # perl: warning: Please check that your locale settings:. # 	LANGUAGE = (unset),. # 	LC_ALL = (unset),. # 	LC_CTYPE = ""C.UTF-8"",. # 	LANG = ""en_US.UTF-8"". # are supported and installed on your system. # perl: warning: Falling back to the standard locale (""C""). # I0423 11:43:12.358298 140211385890624 genomics_reader.py:222] Reading /work/cjm124/SWFst/VarCalling/reads/bc2001_aligned_sorted.bam with NativeSamReader. # W0423 11:43:12.358482 140211385890624 make_examples_core.py:344] No non-empty sample name found in the input reads. DeepVariant will use default as the sample name. You can also provide a sample name with the --sample_name argument. # I0423 11:43:12.365553 140211385890624 make_examples_core.py:301] Task 0/12: Preparing inputs. # I0423 11:43:12.377128 140211385890624 genomics_reader.py:222] Reading /work/cjm124/SWFst/VarCalling/reads/bc2001_aligned_sorted.bam with NativeSamReader. # I0423 11:43:12.405545 140211385890624 make_examples_core.py:301] Task 0/12: Common contigs are ['chr1', 'chr2', 'chr3', 'chr4', 'chr5', 'chr6', 'chr7', 'chr8', 'chr9', 'chr10', 'chr11', 'chr12', 'chr13', 'chr14', 'chr15', 'chr16', 'chr17', 'chr18', 'chr19', 'unplaced_scaffold20', 'unplaced_scaffold21', 'unplaced_scaffold22', 'unplaced_scaffold23', 'unplaced_scaffold24', 'unplaced_scaffold25', 'unplaced_scaffold26', 'unplaced_scaffold27', 'unplaced_scaffold28', 'unplaced_scaffold29', 'unplaced_scaffold30', 'unplaced_scaffold31', 'unplaced_scaffold32', 'unplaced_scaffold33', 'unplaced_scaffold34', 'unplaced_scaffo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:3447,safety,input,inputs,3447,"le settings:. # 	LANGUAGE = (unset),. # 	LC_ALL = (unset),. # 	LC_CTYPE = ""C.UTF-8"",. # 	LANG = ""en_US.UTF-8"". # are supported and installed on your system. # perl: warning: Falling back to the standard locale (""C""). # perl: warning: Setting locale failed. # perl: warning: Please check that your locale settings:. # 	LANGUAGE = (unset),. # 	LC_ALL = (unset),. # 	LC_CTYPE = ""C.UTF-8"",. # 	LANG = ""en_US.UTF-8"". # are supported and installed on your system. # perl: warning: Falling back to the standard locale (""C""). # I0423 11:43:12.358298 140211385890624 genomics_reader.py:222] Reading /work/cjm124/SWFst/VarCalling/reads/bc2001_aligned_sorted.bam with NativeSamReader. # W0423 11:43:12.358482 140211385890624 make_examples_core.py:344] No non-empty sample name found in the input reads. DeepVariant will use default as the sample name. You can also provide a sample name with the --sample_name argument. # I0423 11:43:12.365553 140211385890624 make_examples_core.py:301] Task 0/12: Preparing inputs. # I0423 11:43:12.377128 140211385890624 genomics_reader.py:222] Reading /work/cjm124/SWFst/VarCalling/reads/bc2001_aligned_sorted.bam with NativeSamReader. # I0423 11:43:12.405545 140211385890624 make_examples_core.py:301] Task 0/12: Common contigs are ['chr1', 'chr2', 'chr3', 'chr4', 'chr5', 'chr6', 'chr7', 'chr8', 'chr9', 'chr10', 'chr11', 'chr12', 'chr13', 'chr14', 'chr15', 'chr16', 'chr17', 'chr18', 'chr19', 'unplaced_scaffold20', 'unplaced_scaffold21', 'unplaced_scaffold22', 'unplaced_scaffold23', 'unplaced_scaffold24', 'unplaced_scaffold25', 'unplaced_scaffold26', 'unplaced_scaffold27', 'unplaced_scaffold28', 'unplaced_scaffold29', 'unplaced_scaffold30', 'unplaced_scaffold31', 'unplaced_scaffold32', 'unplaced_scaffold33', 'unplaced_scaffold34', 'unplaced_scaffold35', 'unplaced_scaffold36', 'unplaced_scaffold37', 'unplaced_scaffold38', 'unplaced_scaffold39', 'unplaced_scaffold40', 'unplaced_scaffold41', 'unplaced_scaffold42', 'unplaced_scaffold43', 'unplaced_scaffold44', 'unp",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:5993,safety,input,input,5993,", 'unplaced_scaffold69', 'unplaced_scaffold70', 'unplaced_scaffold71', 'unplaced_scaffold72', 'unplaced_scaffold73', 'unplaced_scaffold74', 'unplaced_scaffold75', 'unplaced_scaffold76', 'unplaced_scaffold77', 'unplaced_scaffold78', 'unplaced_scaffold79', 'unplaced_scaffold80', 'unplaced_scaffold81', 'unplaced_scaffold82', 'unplaced_scaffold83', 'unplaced_scaffold84', 'unplaced_scaffold85', 'unplaced_scaffold86', 'unplaced_scaffold87', 'unplaced_scaffold88', 'unplaced_scaffold89', 'unplaced_scaffold90', 'unplaced_scaffold91', 'unplaced_scaffold92', 'unplaced_scaffold93', 'unplaced_scaffold94', 'unplaced_scaffold95', 'unplaced_scaffold96', 'unplaced_scaffold97', 'unplaced_scaffold98', 'unplaced_scaffold99', 'unplaced_scaffold100', 'unplaced_scaffold101', 'unplaced_scaffold102', 'unplaced_scaffold103', 'unplaced_scaffold104']. # I0423 11:43:12.466705 140211385890624 make_examples_core.py:301] Task 0/12: Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. # I0423 11:43:12.538744 140211385890624 genomics_reader.py:222] Reading /work/cjm124/SWFst/VarCalling/reads/bc2001_aligned_sorted.bam with NativeSamReader. # I0423 11:43:12.636761 140211385890624 genomics_reader.py:222] Reading /work/cjm124/SWFst/VarCalling/reads/bc2001_aligned_sorted.bam with NativeSamReader. # I0423 11:43:12.637369 140211385890624 make_examples_core.py:301] Task 0/12: Writing gvcf records to /tmp/tmpkmab_2kw/gvcf.tfrecord-00000-of-00012.gz. # I0423 11:43:12.637865 140211385890624 make_examples_core.py:301] Task 0/12: Writing examples to /tmp/tmpkmab_2kw/make_examples.tfrecord-00000-of-00012.gz. # I0423 11:43:12.637962 140211385890624 make_examples_core.py:301] Task 0/12: Overhead for preparing inputs: 0 seconds. # 2024-04-23 11:43:12.645232: W ./third_party/nucleus/util/proto_clif_converter.h:75] Failed to cast type N6google8protobuf14DynamicMessageE. # Traceback (most recent call last):.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:6798,safety,input,inputs,6798,"103', 'unplaced_scaffold104']. # I0423 11:43:12.466705 140211385890624 make_examples_core.py:301] Task 0/12: Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. # I0423 11:43:12.538744 140211385890624 genomics_reader.py:222] Reading /work/cjm124/SWFst/VarCalling/reads/bc2001_aligned_sorted.bam with NativeSamReader. # I0423 11:43:12.636761 140211385890624 genomics_reader.py:222] Reading /work/cjm124/SWFst/VarCalling/reads/bc2001_aligned_sorted.bam with NativeSamReader. # I0423 11:43:12.637369 140211385890624 make_examples_core.py:301] Task 0/12: Writing gvcf records to /tmp/tmpkmab_2kw/gvcf.tfrecord-00000-of-00012.gz. # I0423 11:43:12.637865 140211385890624 make_examples_core.py:301] Task 0/12: Writing examples to /tmp/tmpkmab_2kw/make_examples.tfrecord-00000-of-00012.gz. # I0423 11:43:12.637962 140211385890624 make_examples_core.py:301] Task 0/12: Overhead for preparing inputs: 0 seconds. # 2024-04-23 11:43:12.645232: W ./third_party/nucleus/util/proto_clif_converter.h:75] Failed to cast type N6google8protobuf14DynamicMessageE. # Traceback (most recent call last):. # File ""/tmp/Bazel.runfiles_qy0tffir/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 234, in <module>. # app.run(main). # File ""/tmp/Bazel.runfiles_qy0tffir/runfiles/absl_py/absl/app.py"", line 312, in run. # _run_main(main, args). # File ""/tmp/Bazel.runfiles_qy0tffir/runfiles/absl_py/absl/app.py"", line 258, in _run_main. # sys.exit(main(argv)). # File ""/tmp/Bazel.runfiles_qy0tffir/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 224, in main. # make_examples_core.make_examples_runner(options). # File ""/tmp/Bazel.runfiles_qy0tffir/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2838, in make_examples_runner. # region_processor.process(region, region_n). # File ""/tmp/Bazel.runfiles_qy0tffir/runfiles/com_google_deepvariant/deepvarian",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:7111,safety,modul,module,7111,"4 genomics_reader.py:222] Reading /work/cjm124/SWFst/VarCalling/reads/bc2001_aligned_sorted.bam with NativeSamReader. # I0423 11:43:12.636761 140211385890624 genomics_reader.py:222] Reading /work/cjm124/SWFst/VarCalling/reads/bc2001_aligned_sorted.bam with NativeSamReader. # I0423 11:43:12.637369 140211385890624 make_examples_core.py:301] Task 0/12: Writing gvcf records to /tmp/tmpkmab_2kw/gvcf.tfrecord-00000-of-00012.gz. # I0423 11:43:12.637865 140211385890624 make_examples_core.py:301] Task 0/12: Writing examples to /tmp/tmpkmab_2kw/make_examples.tfrecord-00000-of-00012.gz. # I0423 11:43:12.637962 140211385890624 make_examples_core.py:301] Task 0/12: Overhead for preparing inputs: 0 seconds. # 2024-04-23 11:43:12.645232: W ./third_party/nucleus/util/proto_clif_converter.h:75] Failed to cast type N6google8protobuf14DynamicMessageE. # Traceback (most recent call last):. # File ""/tmp/Bazel.runfiles_qy0tffir/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 234, in <module>. # app.run(main). # File ""/tmp/Bazel.runfiles_qy0tffir/runfiles/absl_py/absl/app.py"", line 312, in run. # _run_main(main, args). # File ""/tmp/Bazel.runfiles_qy0tffir/runfiles/absl_py/absl/app.py"", line 258, in _run_main. # sys.exit(main(argv)). # File ""/tmp/Bazel.runfiles_qy0tffir/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 224, in main. # make_examples_core.make_examples_runner(options). # File ""/tmp/Bazel.runfiles_qy0tffir/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2838, in make_examples_runner. # region_processor.process(region, region_n). # File ""/tmp/Bazel.runfiles_qy0tffir/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1695, in process. # sample_reads = self.region_reads_norealign(. # File ""/tmp/Bazel.runfiles_qy0tffir/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1817, in region_reads_norealign. # reads = reservoir_sample_reads(. # File ""/tmp/Bazel.runfiles_qy0tffir/ru",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:8863,safety,valid,valid,8863,"examples_core.py"", line 2838, in make_examples_runner. # region_processor.process(region, region_n). # File ""/tmp/Bazel.runfiles_qy0tffir/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1695, in process. # sample_reads = self.region_reads_norealign(. # File ""/tmp/Bazel.runfiles_qy0tffir/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1817, in region_reads_norealign. # reads = reservoir_sample_reads(. # File ""/tmp/Bazel.runfiles_qy0tffir/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 976, in reservoir_sample_reads. # return utils.reservoir_sample(iterable_of_reads, k, random). # File ""/tmp/Bazel.runfiles_qy0tffir/runfiles/com_google_deepvariant/third_party/nucleus/util/utils.py"", line 117, in reservoir_sample. # for i, item in enumerate(iterable):. # File ""/tmp/Bazel.runfiles_qy0tffir/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 95, in __next__. # record, not_done = self._raw_next(). # File ""/tmp/Bazel.runfiles_qy0tffir/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 154, in _raw_next. # not_done = self._cc_iterable.PythonNext(record). # RuntimeError: PythonNext() argument read is not valid: Dynamic cast failed. # parallel: This job failed:. # /opt/deepvariant/bin/make_examples --mode calling --ref /work/cjm124/SWFst/lvar3ref/Lvar_scaffolds.fasta --reads /work/cjm124/SWFst/VarCalling/reads/bc2001_aligned_sorted.bam --examples /tmp/tmpkmab_2kw/make_examples.tfrecord@12.gz --add_hp_channel --alt_aligned_pileup diff_channels --gvcf /tmp/tmpkmab_2kw/gvcf.tfrecord@12.gz --max_reads_per_partition 600 --min_mapping_quality 1 --parse_sam_aux_fields --partition_size 25000 --phase_reads --pileup_image_width 199 --norealign_reads --sort_by_haplotypes --track_ref_reads --vsc_min_fraction_indels 0.12 --task 0. # real	0m15.200s. # user	0m3.162s. # sys	0m1.161s. # finished with /work/cjm124/SWFst/VarCalling/reads/bc2001_aligned_sorted.bam. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:517,security,updat,update,517,"The command for installing DeepVariant? . `singularity build DeepVariant_1.6.1.sif docker://google/deepvariant:1.6.1`. Or the full command that is written to stdout when DeepVariant runs? For this latter case, after installing nucleus the full command for deepvariant is not written to output. The last line of output is `KeyError: 'SerializedDType'`. I dont have the output saved from the test data run to retrieve the full command output with the error prior to installing nucleus as user, and will re-run that and update this comment with it in a few hours. I did, however, get the same error attempting to run deepvariant with my own data (prior to installing nucleus as user), and the output and command from that are below:. ```. for bam in $READS; do. 	echo ""running deepvariant on $bam"". 	run_deepvariant --model_type=PACBIO --ref=$REF --reads=$bam --output_vcf=$OUTDIR/$bam.vcf.gz --output_gvcf=$OUTDIR/$bam.g.vcf.gz --logging_dir=$LOGDIR --num_shards=$CORES. 	echo ""finished with $bam"". done. #output in block comment below. # running deepvariant on /work/cjm124/SWFst/VarCalling/reads/bc2001_aligned_sorted.bam. # 2024-04-23 11:42:51.281492: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA. # To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. # I0423 11:42:57.943745 140073410221888 run_deepvariant.py:519] Re-using the directory for intermediate results in /tmp/tmpkmab_2kw. # ***** Intermediate results will be written to /tmp/tmpkmab_2kw in docker. ****. # ***** Running the command:*****. # time seq 0 11 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/work/cjm124/SWFst/lvar3ref/Lvar_scaffolds.fasta"" --reads ""/work/cjm124/SWFst/VarCalling/reads/bc2001_aligned_sorted.bam"" --examples ""/tmp/tmpkmab_2kw/make_example",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:941,security,LOG,LOGDIR,941,"The command for installing DeepVariant? . `singularity build DeepVariant_1.6.1.sif docker://google/deepvariant:1.6.1`. Or the full command that is written to stdout when DeepVariant runs? For this latter case, after installing nucleus the full command for deepvariant is not written to output. The last line of output is `KeyError: 'SerializedDType'`. I dont have the output saved from the test data run to retrieve the full command output with the error prior to installing nucleus as user, and will re-run that and update this comment with it in a few hours. I did, however, get the same error attempting to run deepvariant with my own data (prior to installing nucleus as user), and the output and command from that are below:. ```. for bam in $READS; do. 	echo ""running deepvariant on $bam"". 	run_deepvariant --model_type=PACBIO --ref=$REF --reads=$bam --output_vcf=$OUTDIR/$bam.vcf.gz --output_gvcf=$OUTDIR/$bam.g.vcf.gz --logging_dir=$LOGDIR --num_shards=$CORES. 	echo ""finished with $bam"". done. #output in block comment below. # running deepvariant on /work/cjm124/SWFst/VarCalling/reads/bc2001_aligned_sorted.bam. # 2024-04-23 11:42:51.281492: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA. # To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. # I0423 11:42:57.943745 140073410221888 run_deepvariant.py:519] Re-using the directory for intermediate results in /tmp/tmpkmab_2kw. # ***** Intermediate results will be written to /tmp/tmpkmab_2kw in docker. ****. # ***** Running the command:*****. # time seq 0 11 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/work/cjm124/SWFst/lvar3ref/Lvar_scaffolds.fasta"" --reads ""/work/cjm124/SWFst/VarCalling/reads/bc2001_aligned_sorted.bam"" --examples ""/tmp/tmpkmab_2kw/make_example",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:1266,security,Network,Network,1266," not written to output. The last line of output is `KeyError: 'SerializedDType'`. I dont have the output saved from the test data run to retrieve the full command output with the error prior to installing nucleus as user, and will re-run that and update this comment with it in a few hours. I did, however, get the same error attempting to run deepvariant with my own data (prior to installing nucleus as user), and the output and command from that are below:. ```. for bam in $READS; do. 	echo ""running deepvariant on $bam"". 	run_deepvariant --model_type=PACBIO --ref=$REF --reads=$bam --output_vcf=$OUTDIR/$bam.vcf.gz --output_gvcf=$OUTDIR/$bam.g.vcf.gz --logging_dir=$LOGDIR --num_shards=$CORES. 	echo ""finished with $bam"". done. #output in block comment below. # running deepvariant on /work/cjm124/SWFst/VarCalling/reads/bc2001_aligned_sorted.bam. # 2024-04-23 11:42:51.281492: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA. # To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. # I0423 11:42:57.943745 140073410221888 run_deepvariant.py:519] Re-using the directory for intermediate results in /tmp/tmpkmab_2kw. # ***** Intermediate results will be written to /tmp/tmpkmab_2kw in docker. ****. # ***** Running the command:*****. # time seq 0 11 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/work/cjm124/SWFst/lvar3ref/Lvar_scaffolds.fasta"" --reads ""/work/cjm124/SWFst/VarCalling/reads/bc2001_aligned_sorted.bam"" --examples ""/tmp/tmpkmab_2kw/make_examples.tfrecord@12.gz"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --gvcf ""/tmp/tmpkmab_2kw/gvcf.tfrecord@12.gz"" --max_reads_per_partition ""600"" --min_mapping_quality ""1"" --parse_sam_aux_fields --partition_size ""25000"" --phase_reads --pileup_image_width ""199"" --nor",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:390,testability,test,test,390,"The command for installing DeepVariant? . `singularity build DeepVariant_1.6.1.sif docker://google/deepvariant:1.6.1`. Or the full command that is written to stdout when DeepVariant runs? For this latter case, after installing nucleus the full command for deepvariant is not written to output. The last line of output is `KeyError: 'SerializedDType'`. I dont have the output saved from the test data run to retrieve the full command output with the error prior to installing nucleus as user, and will re-run that and update this comment with it in a few hours. I did, however, get the same error attempting to run deepvariant with my own data (prior to installing nucleus as user), and the output and command from that are below:. ```. for bam in $READS; do. 	echo ""running deepvariant on $bam"". 	run_deepvariant --model_type=PACBIO --ref=$REF --reads=$bam --output_vcf=$OUTDIR/$bam.vcf.gz --output_gvcf=$OUTDIR/$bam.g.vcf.gz --logging_dir=$LOGDIR --num_shards=$CORES. 	echo ""finished with $bam"". done. #output in block comment below. # running deepvariant on /work/cjm124/SWFst/VarCalling/reads/bc2001_aligned_sorted.bam. # 2024-04-23 11:42:51.281492: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA. # To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. # I0423 11:42:57.943745 140073410221888 run_deepvariant.py:519] Re-using the directory for intermediate results in /tmp/tmpkmab_2kw. # ***** Intermediate results will be written to /tmp/tmpkmab_2kw in docker. ****. # ***** Running the command:*****. # time seq 0 11 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/work/cjm124/SWFst/lvar3ref/Lvar_scaffolds.fasta"" --reads ""/work/cjm124/SWFst/VarCalling/reads/bc2001_aligned_sorted.bam"" --examples ""/tmp/tmpkmab_2kw/make_example",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:941,testability,LOG,LOGDIR,941,"The command for installing DeepVariant? . `singularity build DeepVariant_1.6.1.sif docker://google/deepvariant:1.6.1`. Or the full command that is written to stdout when DeepVariant runs? For this latter case, after installing nucleus the full command for deepvariant is not written to output. The last line of output is `KeyError: 'SerializedDType'`. I dont have the output saved from the test data run to retrieve the full command output with the error prior to installing nucleus as user, and will re-run that and update this comment with it in a few hours. I did, however, get the same error attempting to run deepvariant with my own data (prior to installing nucleus as user), and the output and command from that are below:. ```. for bam in $READS; do. 	echo ""running deepvariant on $bam"". 	run_deepvariant --model_type=PACBIO --ref=$REF --reads=$bam --output_vcf=$OUTDIR/$bam.vcf.gz --output_gvcf=$OUTDIR/$bam.g.vcf.gz --logging_dir=$LOGDIR --num_shards=$CORES. 	echo ""finished with $bam"". done. #output in block comment below. # running deepvariant on /work/cjm124/SWFst/VarCalling/reads/bc2001_aligned_sorted.bam. # 2024-04-23 11:42:51.281492: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA. # To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. # I0423 11:42:57.943745 140073410221888 run_deepvariant.py:519] Re-using the directory for intermediate results in /tmp/tmpkmab_2kw. # ***** Intermediate results will be written to /tmp/tmpkmab_2kw in docker. ****. # ***** Running the command:*****. # time seq 0 11 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/work/cjm124/SWFst/lvar3ref/Lvar_scaffolds.fasta"" --reads ""/work/cjm124/SWFst/VarCalling/reads/bc2001_aligned_sorted.bam"" --examples ""/tmp/tmpkmab_2kw/make_example",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:6961,testability,Trace,Traceback,6961,"rue. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. # I0423 11:43:12.538744 140211385890624 genomics_reader.py:222] Reading /work/cjm124/SWFst/VarCalling/reads/bc2001_aligned_sorted.bam with NativeSamReader. # I0423 11:43:12.636761 140211385890624 genomics_reader.py:222] Reading /work/cjm124/SWFst/VarCalling/reads/bc2001_aligned_sorted.bam with NativeSamReader. # I0423 11:43:12.637369 140211385890624 make_examples_core.py:301] Task 0/12: Writing gvcf records to /tmp/tmpkmab_2kw/gvcf.tfrecord-00000-of-00012.gz. # I0423 11:43:12.637865 140211385890624 make_examples_core.py:301] Task 0/12: Writing examples to /tmp/tmpkmab_2kw/make_examples.tfrecord-00000-of-00012.gz. # I0423 11:43:12.637962 140211385890624 make_examples_core.py:301] Task 0/12: Overhead for preparing inputs: 0 seconds. # 2024-04-23 11:43:12.645232: W ./third_party/nucleus/util/proto_clif_converter.h:75] Failed to cast type N6google8protobuf14DynamicMessageE. # Traceback (most recent call last):. # File ""/tmp/Bazel.runfiles_qy0tffir/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 234, in <module>. # app.run(main). # File ""/tmp/Bazel.runfiles_qy0tffir/runfiles/absl_py/absl/app.py"", line 312, in run. # _run_main(main, args). # File ""/tmp/Bazel.runfiles_qy0tffir/runfiles/absl_py/absl/app.py"", line 258, in _run_main. # sys.exit(main(argv)). # File ""/tmp/Bazel.runfiles_qy0tffir/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 224, in main. # make_examples_core.make_examples_runner(options). # File ""/tmp/Bazel.runfiles_qy0tffir/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2838, in make_examples_runner. # region_processor.process(region, region_n). # File ""/tmp/Bazel.runfiles_qy0tffir/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1695, in process. # sample_reads = self.region_reads_norealign(. # File ""/tmp/Bazel.runfiles_qy0tffir/runfiles/com_google_deepvariant/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:4,usability,command,command,4,"The command for installing DeepVariant? . `singularity build DeepVariant_1.6.1.sif docker://google/deepvariant:1.6.1`. Or the full command that is written to stdout when DeepVariant runs? For this latter case, after installing nucleus the full command for deepvariant is not written to output. The last line of output is `KeyError: 'SerializedDType'`. I dont have the output saved from the test data run to retrieve the full command output with the error prior to installing nucleus as user, and will re-run that and update this comment with it in a few hours. I did, however, get the same error attempting to run deepvariant with my own data (prior to installing nucleus as user), and the output and command from that are below:. ```. for bam in $READS; do. 	echo ""running deepvariant on $bam"". 	run_deepvariant --model_type=PACBIO --ref=$REF --reads=$bam --output_vcf=$OUTDIR/$bam.vcf.gz --output_gvcf=$OUTDIR/$bam.g.vcf.gz --logging_dir=$LOGDIR --num_shards=$CORES. 	echo ""finished with $bam"". done. #output in block comment below. # running deepvariant on /work/cjm124/SWFst/VarCalling/reads/bc2001_aligned_sorted.bam. # 2024-04-23 11:42:51.281492: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA. # To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. # I0423 11:42:57.943745 140073410221888 run_deepvariant.py:519] Re-using the directory for intermediate results in /tmp/tmpkmab_2kw. # ***** Intermediate results will be written to /tmp/tmpkmab_2kw in docker. ****. # ***** Running the command:*****. # time seq 0 11 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/work/cjm124/SWFst/lvar3ref/Lvar_scaffolds.fasta"" --reads ""/work/cjm124/SWFst/VarCalling/reads/bc2001_aligned_sorted.bam"" --examples ""/tmp/tmpkmab_2kw/make_example",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:131,usability,command,command,131,"The command for installing DeepVariant? . `singularity build DeepVariant_1.6.1.sif docker://google/deepvariant:1.6.1`. Or the full command that is written to stdout when DeepVariant runs? For this latter case, after installing nucleus the full command for deepvariant is not written to output. The last line of output is `KeyError: 'SerializedDType'`. I dont have the output saved from the test data run to retrieve the full command output with the error prior to installing nucleus as user, and will re-run that and update this comment with it in a few hours. I did, however, get the same error attempting to run deepvariant with my own data (prior to installing nucleus as user), and the output and command from that are below:. ```. for bam in $READS; do. 	echo ""running deepvariant on $bam"". 	run_deepvariant --model_type=PACBIO --ref=$REF --reads=$bam --output_vcf=$OUTDIR/$bam.vcf.gz --output_gvcf=$OUTDIR/$bam.g.vcf.gz --logging_dir=$LOGDIR --num_shards=$CORES. 	echo ""finished with $bam"". done. #output in block comment below. # running deepvariant on /work/cjm124/SWFst/VarCalling/reads/bc2001_aligned_sorted.bam. # 2024-04-23 11:42:51.281492: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA. # To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. # I0423 11:42:57.943745 140073410221888 run_deepvariant.py:519] Re-using the directory for intermediate results in /tmp/tmpkmab_2kw. # ***** Intermediate results will be written to /tmp/tmpkmab_2kw in docker. ****. # ***** Running the command:*****. # time seq 0 11 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/work/cjm124/SWFst/lvar3ref/Lvar_scaffolds.fasta"" --reads ""/work/cjm124/SWFst/VarCalling/reads/bc2001_aligned_sorted.bam"" --examples ""/tmp/tmpkmab_2kw/make_example",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:244,usability,command,command,244,"The command for installing DeepVariant? . `singularity build DeepVariant_1.6.1.sif docker://google/deepvariant:1.6.1`. Or the full command that is written to stdout when DeepVariant runs? For this latter case, after installing nucleus the full command for deepvariant is not written to output. The last line of output is `KeyError: 'SerializedDType'`. I dont have the output saved from the test data run to retrieve the full command output with the error prior to installing nucleus as user, and will re-run that and update this comment with it in a few hours. I did, however, get the same error attempting to run deepvariant with my own data (prior to installing nucleus as user), and the output and command from that are below:. ```. for bam in $READS; do. 	echo ""running deepvariant on $bam"". 	run_deepvariant --model_type=PACBIO --ref=$REF --reads=$bam --output_vcf=$OUTDIR/$bam.vcf.gz --output_gvcf=$OUTDIR/$bam.g.vcf.gz --logging_dir=$LOGDIR --num_shards=$CORES. 	echo ""finished with $bam"". done. #output in block comment below. # running deepvariant on /work/cjm124/SWFst/VarCalling/reads/bc2001_aligned_sorted.bam. # 2024-04-23 11:42:51.281492: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA. # To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. # I0423 11:42:57.943745 140073410221888 run_deepvariant.py:519] Re-using the directory for intermediate results in /tmp/tmpkmab_2kw. # ***** Intermediate results will be written to /tmp/tmpkmab_2kw in docker. ****. # ***** Running the command:*****. # time seq 0 11 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/work/cjm124/SWFst/lvar3ref/Lvar_scaffolds.fasta"" --reads ""/work/cjm124/SWFst/VarCalling/reads/bc2001_aligned_sorted.bam"" --examples ""/tmp/tmpkmab_2kw/make_example",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:425,usability,command,command,425,"The command for installing DeepVariant? . `singularity build DeepVariant_1.6.1.sif docker://google/deepvariant:1.6.1`. Or the full command that is written to stdout when DeepVariant runs? For this latter case, after installing nucleus the full command for deepvariant is not written to output. The last line of output is `KeyError: 'SerializedDType'`. I dont have the output saved from the test data run to retrieve the full command output with the error prior to installing nucleus as user, and will re-run that and update this comment with it in a few hours. I did, however, get the same error attempting to run deepvariant with my own data (prior to installing nucleus as user), and the output and command from that are below:. ```. for bam in $READS; do. 	echo ""running deepvariant on $bam"". 	run_deepvariant --model_type=PACBIO --ref=$REF --reads=$bam --output_vcf=$OUTDIR/$bam.vcf.gz --output_gvcf=$OUTDIR/$bam.g.vcf.gz --logging_dir=$LOGDIR --num_shards=$CORES. 	echo ""finished with $bam"". done. #output in block comment below. # running deepvariant on /work/cjm124/SWFst/VarCalling/reads/bc2001_aligned_sorted.bam. # 2024-04-23 11:42:51.281492: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA. # To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. # I0423 11:42:57.943745 140073410221888 run_deepvariant.py:519] Re-using the directory for intermediate results in /tmp/tmpkmab_2kw. # ***** Intermediate results will be written to /tmp/tmpkmab_2kw in docker. ****. # ***** Running the command:*****. # time seq 0 11 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/work/cjm124/SWFst/lvar3ref/Lvar_scaffolds.fasta"" --reads ""/work/cjm124/SWFst/VarCalling/reads/bc2001_aligned_sorted.bam"" --examples ""/tmp/tmpkmab_2kw/make_example",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:449,usability,error,error,449,"The command for installing DeepVariant? . `singularity build DeepVariant_1.6.1.sif docker://google/deepvariant:1.6.1`. Or the full command that is written to stdout when DeepVariant runs? For this latter case, after installing nucleus the full command for deepvariant is not written to output. The last line of output is `KeyError: 'SerializedDType'`. I dont have the output saved from the test data run to retrieve the full command output with the error prior to installing nucleus as user, and will re-run that and update this comment with it in a few hours. I did, however, get the same error attempting to run deepvariant with my own data (prior to installing nucleus as user), and the output and command from that are below:. ```. for bam in $READS; do. 	echo ""running deepvariant on $bam"". 	run_deepvariant --model_type=PACBIO --ref=$REF --reads=$bam --output_vcf=$OUTDIR/$bam.vcf.gz --output_gvcf=$OUTDIR/$bam.g.vcf.gz --logging_dir=$LOGDIR --num_shards=$CORES. 	echo ""finished with $bam"". done. #output in block comment below. # running deepvariant on /work/cjm124/SWFst/VarCalling/reads/bc2001_aligned_sorted.bam. # 2024-04-23 11:42:51.281492: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA. # To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. # I0423 11:42:57.943745 140073410221888 run_deepvariant.py:519] Re-using the directory for intermediate results in /tmp/tmpkmab_2kw. # ***** Intermediate results will be written to /tmp/tmpkmab_2kw in docker. ****. # ***** Running the command:*****. # time seq 0 11 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/work/cjm124/SWFst/lvar3ref/Lvar_scaffolds.fasta"" --reads ""/work/cjm124/SWFst/VarCalling/reads/bc2001_aligned_sorted.bam"" --examples ""/tmp/tmpkmab_2kw/make_example",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:486,usability,user,user,486,"The command for installing DeepVariant? . `singularity build DeepVariant_1.6.1.sif docker://google/deepvariant:1.6.1`. Or the full command that is written to stdout when DeepVariant runs? For this latter case, after installing nucleus the full command for deepvariant is not written to output. The last line of output is `KeyError: 'SerializedDType'`. I dont have the output saved from the test data run to retrieve the full command output with the error prior to installing nucleus as user, and will re-run that and update this comment with it in a few hours. I did, however, get the same error attempting to run deepvariant with my own data (prior to installing nucleus as user), and the output and command from that are below:. ```. for bam in $READS; do. 	echo ""running deepvariant on $bam"". 	run_deepvariant --model_type=PACBIO --ref=$REF --reads=$bam --output_vcf=$OUTDIR/$bam.vcf.gz --output_gvcf=$OUTDIR/$bam.g.vcf.gz --logging_dir=$LOGDIR --num_shards=$CORES. 	echo ""finished with $bam"". done. #output in block comment below. # running deepvariant on /work/cjm124/SWFst/VarCalling/reads/bc2001_aligned_sorted.bam. # 2024-04-23 11:42:51.281492: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA. # To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. # I0423 11:42:57.943745 140073410221888 run_deepvariant.py:519] Re-using the directory for intermediate results in /tmp/tmpkmab_2kw. # ***** Intermediate results will be written to /tmp/tmpkmab_2kw in docker. ****. # ***** Running the command:*****. # time seq 0 11 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/work/cjm124/SWFst/lvar3ref/Lvar_scaffolds.fasta"" --reads ""/work/cjm124/SWFst/VarCalling/reads/bc2001_aligned_sorted.bam"" --examples ""/tmp/tmpkmab_2kw/make_example",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:590,usability,error,error,590,"The command for installing DeepVariant? . `singularity build DeepVariant_1.6.1.sif docker://google/deepvariant:1.6.1`. Or the full command that is written to stdout when DeepVariant runs? For this latter case, after installing nucleus the full command for deepvariant is not written to output. The last line of output is `KeyError: 'SerializedDType'`. I dont have the output saved from the test data run to retrieve the full command output with the error prior to installing nucleus as user, and will re-run that and update this comment with it in a few hours. I did, however, get the same error attempting to run deepvariant with my own data (prior to installing nucleus as user), and the output and command from that are below:. ```. for bam in $READS; do. 	echo ""running deepvariant on $bam"". 	run_deepvariant --model_type=PACBIO --ref=$REF --reads=$bam --output_vcf=$OUTDIR/$bam.vcf.gz --output_gvcf=$OUTDIR/$bam.g.vcf.gz --logging_dir=$LOGDIR --num_shards=$CORES. 	echo ""finished with $bam"". done. #output in block comment below. # running deepvariant on /work/cjm124/SWFst/VarCalling/reads/bc2001_aligned_sorted.bam. # 2024-04-23 11:42:51.281492: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA. # To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. # I0423 11:42:57.943745 140073410221888 run_deepvariant.py:519] Re-using the directory for intermediate results in /tmp/tmpkmab_2kw. # ***** Intermediate results will be written to /tmp/tmpkmab_2kw in docker. ****. # ***** Running the command:*****. # time seq 0 11 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/work/cjm124/SWFst/lvar3ref/Lvar_scaffolds.fasta"" --reads ""/work/cjm124/SWFst/VarCalling/reads/bc2001_aligned_sorted.bam"" --examples ""/tmp/tmpkmab_2kw/make_example",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:675,usability,user,user,675,"The command for installing DeepVariant? . `singularity build DeepVariant_1.6.1.sif docker://google/deepvariant:1.6.1`. Or the full command that is written to stdout when DeepVariant runs? For this latter case, after installing nucleus the full command for deepvariant is not written to output. The last line of output is `KeyError: 'SerializedDType'`. I dont have the output saved from the test data run to retrieve the full command output with the error prior to installing nucleus as user, and will re-run that and update this comment with it in a few hours. I did, however, get the same error attempting to run deepvariant with my own data (prior to installing nucleus as user), and the output and command from that are below:. ```. for bam in $READS; do. 	echo ""running deepvariant on $bam"". 	run_deepvariant --model_type=PACBIO --ref=$REF --reads=$bam --output_vcf=$OUTDIR/$bam.vcf.gz --output_gvcf=$OUTDIR/$bam.g.vcf.gz --logging_dir=$LOGDIR --num_shards=$CORES. 	echo ""finished with $bam"". done. #output in block comment below. # running deepvariant on /work/cjm124/SWFst/VarCalling/reads/bc2001_aligned_sorted.bam. # 2024-04-23 11:42:51.281492: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA. # To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. # I0423 11:42:57.943745 140073410221888 run_deepvariant.py:519] Re-using the directory for intermediate results in /tmp/tmpkmab_2kw. # ***** Intermediate results will be written to /tmp/tmpkmab_2kw in docker. ****. # ***** Running the command:*****. # time seq 0 11 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/work/cjm124/SWFst/lvar3ref/Lvar_scaffolds.fasta"" --reads ""/work/cjm124/SWFst/VarCalling/reads/bc2001_aligned_sorted.bam"" --examples ""/tmp/tmpkmab_2kw/make_example",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:701,usability,command,command,701,"The command for installing DeepVariant? . `singularity build DeepVariant_1.6.1.sif docker://google/deepvariant:1.6.1`. Or the full command that is written to stdout when DeepVariant runs? For this latter case, after installing nucleus the full command for deepvariant is not written to output. The last line of output is `KeyError: 'SerializedDType'`. I dont have the output saved from the test data run to retrieve the full command output with the error prior to installing nucleus as user, and will re-run that and update this comment with it in a few hours. I did, however, get the same error attempting to run deepvariant with my own data (prior to installing nucleus as user), and the output and command from that are below:. ```. for bam in $READS; do. 	echo ""running deepvariant on $bam"". 	run_deepvariant --model_type=PACBIO --ref=$REF --reads=$bam --output_vcf=$OUTDIR/$bam.vcf.gz --output_gvcf=$OUTDIR/$bam.g.vcf.gz --logging_dir=$LOGDIR --num_shards=$CORES. 	echo ""finished with $bam"". done. #output in block comment below. # running deepvariant on /work/cjm124/SWFst/VarCalling/reads/bc2001_aligned_sorted.bam. # 2024-04-23 11:42:51.281492: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA. # To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. # I0423 11:42:57.943745 140073410221888 run_deepvariant.py:519] Re-using the directory for intermediate results in /tmp/tmpkmab_2kw. # ***** Intermediate results will be written to /tmp/tmpkmab_2kw in docker. ****. # ***** Running the command:*****. # time seq 0 11 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/work/cjm124/SWFst/lvar3ref/Lvar_scaffolds.fasta"" --reads ""/work/cjm124/SWFst/VarCalling/reads/bc2001_aligned_sorted.bam"" --examples ""/tmp/tmpkmab_2kw/make_example",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:1332,usability,perform,performance-critical,1332,"dDType'`. I dont have the output saved from the test data run to retrieve the full command output with the error prior to installing nucleus as user, and will re-run that and update this comment with it in a few hours. I did, however, get the same error attempting to run deepvariant with my own data (prior to installing nucleus as user), and the output and command from that are below:. ```. for bam in $READS; do. 	echo ""running deepvariant on $bam"". 	run_deepvariant --model_type=PACBIO --ref=$REF --reads=$bam --output_vcf=$OUTDIR/$bam.vcf.gz --output_gvcf=$OUTDIR/$bam.g.vcf.gz --logging_dir=$LOGDIR --num_shards=$CORES. 	echo ""finished with $bam"". done. #output in block comment below. # running deepvariant on /work/cjm124/SWFst/VarCalling/reads/bc2001_aligned_sorted.bam. # 2024-04-23 11:42:51.281492: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA. # To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. # I0423 11:42:57.943745 140073410221888 run_deepvariant.py:519] Re-using the directory for intermediate results in /tmp/tmpkmab_2kw. # ***** Intermediate results will be written to /tmp/tmpkmab_2kw in docker. ****. # ***** Running the command:*****. # time seq 0 11 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/work/cjm124/SWFst/lvar3ref/Lvar_scaffolds.fasta"" --reads ""/work/cjm124/SWFst/VarCalling/reads/bc2001_aligned_sorted.bam"" --examples ""/tmp/tmpkmab_2kw/make_examples.tfrecord@12.gz"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --gvcf ""/tmp/tmpkmab_2kw/gvcf.tfrecord@12.gz"" --max_reads_per_partition ""600"" --min_mapping_quality ""1"" --parse_sam_aux_fields --partition_size ""25000"" --phase_reads --pileup_image_width ""199"" --norealign_reads --sort_by_haplotypes --track_ref_reads --vsc_min_fraction_i",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:1712,usability,command,command,1712,"at are below:. ```. for bam in $READS; do. 	echo ""running deepvariant on $bam"". 	run_deepvariant --model_type=PACBIO --ref=$REF --reads=$bam --output_vcf=$OUTDIR/$bam.vcf.gz --output_gvcf=$OUTDIR/$bam.g.vcf.gz --logging_dir=$LOGDIR --num_shards=$CORES. 	echo ""finished with $bam"". done. #output in block comment below. # running deepvariant on /work/cjm124/SWFst/VarCalling/reads/bc2001_aligned_sorted.bam. # 2024-04-23 11:42:51.281492: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA. # To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. # I0423 11:42:57.943745 140073410221888 run_deepvariant.py:519] Re-using the directory for intermediate results in /tmp/tmpkmab_2kw. # ***** Intermediate results will be written to /tmp/tmpkmab_2kw in docker. ****. # ***** Running the command:*****. # time seq 0 11 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/work/cjm124/SWFst/lvar3ref/Lvar_scaffolds.fasta"" --reads ""/work/cjm124/SWFst/VarCalling/reads/bc2001_aligned_sorted.bam"" --examples ""/tmp/tmpkmab_2kw/make_examples.tfrecord@12.gz"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --gvcf ""/tmp/tmpkmab_2kw/gvcf.tfrecord@12.gz"" --max_reads_per_partition ""600"" --min_mapping_quality ""1"" --parse_sam_aux_fields --partition_size ""25000"" --phase_reads --pileup_image_width ""199"" --norealign_reads --sort_by_haplotypes --track_ref_reads --vsc_min_fraction_indels ""0.12"" --task {}. # perl: warning: Setting locale failed. # perl: warning: Please check that your locale settings:. # 	LANGUAGE = (unset),. # 	LC_ALL = (unset),. # 	LC_CTYPE = ""C.UTF-8"",. # 	LANG = ""en_US.UTF-8"". # are supported and installed on your system. # perl: warning: Falling back to the standard locale (""C""). # perl: warning: Setting locale failed. # perl: w",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:2567,usability,support,supported,2567,"rmediate results in /tmp/tmpkmab_2kw. # ***** Intermediate results will be written to /tmp/tmpkmab_2kw in docker. ****. # ***** Running the command:*****. # time seq 0 11 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/work/cjm124/SWFst/lvar3ref/Lvar_scaffolds.fasta"" --reads ""/work/cjm124/SWFst/VarCalling/reads/bc2001_aligned_sorted.bam"" --examples ""/tmp/tmpkmab_2kw/make_examples.tfrecord@12.gz"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --gvcf ""/tmp/tmpkmab_2kw/gvcf.tfrecord@12.gz"" --max_reads_per_partition ""600"" --min_mapping_quality ""1"" --parse_sam_aux_fields --partition_size ""25000"" --phase_reads --pileup_image_width ""199"" --norealign_reads --sort_by_haplotypes --track_ref_reads --vsc_min_fraction_indels ""0.12"" --task {}. # perl: warning: Setting locale failed. # perl: warning: Please check that your locale settings:. # 	LANGUAGE = (unset),. # 	LC_ALL = (unset),. # 	LC_CTYPE = ""C.UTF-8"",. # 	LANG = ""en_US.UTF-8"". # are supported and installed on your system. # perl: warning: Falling back to the standard locale (""C""). # perl: warning: Setting locale failed. # perl: warning: Please check that your locale settings:. # 	LANGUAGE = (unset),. # 	LC_ALL = (unset),. # 	LC_CTYPE = ""C.UTF-8"",. # 	LANG = ""en_US.UTF-8"". # are supported and installed on your system. # perl: warning: Falling back to the standard locale (""C""). # I0423 11:43:12.358298 140211385890624 genomics_reader.py:222] Reading /work/cjm124/SWFst/VarCalling/reads/bc2001_aligned_sorted.bam with NativeSamReader. # W0423 11:43:12.358482 140211385890624 make_examples_core.py:344] No non-empty sample name found in the input reads. DeepVariant will use default as the sample name. You can also provide a sample name with the --sample_name argument. # I0423 11:43:12.365553 140211385890624 make_examples_core.py:301] Task 0/12: Preparing inputs. # I0423 11:43:12.377128 140211385890624 genomics_reader.py:222] Reading /work/cjm124/SWFst/VarCalling/reads/bc2001_al",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:2868,usability,support,supported,2868,"ffolds.fasta"" --reads ""/work/cjm124/SWFst/VarCalling/reads/bc2001_aligned_sorted.bam"" --examples ""/tmp/tmpkmab_2kw/make_examples.tfrecord@12.gz"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --gvcf ""/tmp/tmpkmab_2kw/gvcf.tfrecord@12.gz"" --max_reads_per_partition ""600"" --min_mapping_quality ""1"" --parse_sam_aux_fields --partition_size ""25000"" --phase_reads --pileup_image_width ""199"" --norealign_reads --sort_by_haplotypes --track_ref_reads --vsc_min_fraction_indels ""0.12"" --task {}. # perl: warning: Setting locale failed. # perl: warning: Please check that your locale settings:. # 	LANGUAGE = (unset),. # 	LC_ALL = (unset),. # 	LC_CTYPE = ""C.UTF-8"",. # 	LANG = ""en_US.UTF-8"". # are supported and installed on your system. # perl: warning: Falling back to the standard locale (""C""). # perl: warning: Setting locale failed. # perl: warning: Please check that your locale settings:. # 	LANGUAGE = (unset),. # 	LC_ALL = (unset),. # 	LC_CTYPE = ""C.UTF-8"",. # 	LANG = ""en_US.UTF-8"". # are supported and installed on your system. # perl: warning: Falling back to the standard locale (""C""). # I0423 11:43:12.358298 140211385890624 genomics_reader.py:222] Reading /work/cjm124/SWFst/VarCalling/reads/bc2001_aligned_sorted.bam with NativeSamReader. # W0423 11:43:12.358482 140211385890624 make_examples_core.py:344] No non-empty sample name found in the input reads. DeepVariant will use default as the sample name. You can also provide a sample name with the --sample_name argument. # I0423 11:43:12.365553 140211385890624 make_examples_core.py:301] Task 0/12: Preparing inputs. # I0423 11:43:12.377128 140211385890624 genomics_reader.py:222] Reading /work/cjm124/SWFst/VarCalling/reads/bc2001_aligned_sorted.bam with NativeSamReader. # I0423 11:43:12.405545 140211385890624 make_examples_core.py:301] Task 0/12: Common contigs are ['chr1', 'chr2', 'chr3', 'chr4', 'chr5', 'chr6', 'chr7', 'chr8', 'chr9', 'chr10', 'chr11', 'chr12', 'chr13', 'chr14', 'chr15', 'chr16', 'chr17', 'chr18', 'chr19', 'u",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:3229,usability,input,input,3229,"reads --pileup_image_width ""199"" --norealign_reads --sort_by_haplotypes --track_ref_reads --vsc_min_fraction_indels ""0.12"" --task {}. # perl: warning: Setting locale failed. # perl: warning: Please check that your locale settings:. # 	LANGUAGE = (unset),. # 	LC_ALL = (unset),. # 	LC_CTYPE = ""C.UTF-8"",. # 	LANG = ""en_US.UTF-8"". # are supported and installed on your system. # perl: warning: Falling back to the standard locale (""C""). # perl: warning: Setting locale failed. # perl: warning: Please check that your locale settings:. # 	LANGUAGE = (unset),. # 	LC_ALL = (unset),. # 	LC_CTYPE = ""C.UTF-8"",. # 	LANG = ""en_US.UTF-8"". # are supported and installed on your system. # perl: warning: Falling back to the standard locale (""C""). # I0423 11:43:12.358298 140211385890624 genomics_reader.py:222] Reading /work/cjm124/SWFst/VarCalling/reads/bc2001_aligned_sorted.bam with NativeSamReader. # W0423 11:43:12.358482 140211385890624 make_examples_core.py:344] No non-empty sample name found in the input reads. DeepVariant will use default as the sample name. You can also provide a sample name with the --sample_name argument. # I0423 11:43:12.365553 140211385890624 make_examples_core.py:301] Task 0/12: Preparing inputs. # I0423 11:43:12.377128 140211385890624 genomics_reader.py:222] Reading /work/cjm124/SWFst/VarCalling/reads/bc2001_aligned_sorted.bam with NativeSamReader. # I0423 11:43:12.405545 140211385890624 make_examples_core.py:301] Task 0/12: Common contigs are ['chr1', 'chr2', 'chr3', 'chr4', 'chr5', 'chr6', 'chr7', 'chr8', 'chr9', 'chr10', 'chr11', 'chr12', 'chr13', 'chr14', 'chr15', 'chr16', 'chr17', 'chr18', 'chr19', 'unplaced_scaffold20', 'unplaced_scaffold21', 'unplaced_scaffold22', 'unplaced_scaffold23', 'unplaced_scaffold24', 'unplaced_scaffold25', 'unplaced_scaffold26', 'unplaced_scaffold27', 'unplaced_scaffold28', 'unplaced_scaffold29', 'unplaced_scaffold30', 'unplaced_scaffold31', 'unplaced_scaffold32', 'unplaced_scaffold33', 'unplaced_scaffold34', 'unplaced_scaffo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:3447,usability,input,inputs,3447,"le settings:. # 	LANGUAGE = (unset),. # 	LC_ALL = (unset),. # 	LC_CTYPE = ""C.UTF-8"",. # 	LANG = ""en_US.UTF-8"". # are supported and installed on your system. # perl: warning: Falling back to the standard locale (""C""). # perl: warning: Setting locale failed. # perl: warning: Please check that your locale settings:. # 	LANGUAGE = (unset),. # 	LC_ALL = (unset),. # 	LC_CTYPE = ""C.UTF-8"",. # 	LANG = ""en_US.UTF-8"". # are supported and installed on your system. # perl: warning: Falling back to the standard locale (""C""). # I0423 11:43:12.358298 140211385890624 genomics_reader.py:222] Reading /work/cjm124/SWFst/VarCalling/reads/bc2001_aligned_sorted.bam with NativeSamReader. # W0423 11:43:12.358482 140211385890624 make_examples_core.py:344] No non-empty sample name found in the input reads. DeepVariant will use default as the sample name. You can also provide a sample name with the --sample_name argument. # I0423 11:43:12.365553 140211385890624 make_examples_core.py:301] Task 0/12: Preparing inputs. # I0423 11:43:12.377128 140211385890624 genomics_reader.py:222] Reading /work/cjm124/SWFst/VarCalling/reads/bc2001_aligned_sorted.bam with NativeSamReader. # I0423 11:43:12.405545 140211385890624 make_examples_core.py:301] Task 0/12: Common contigs are ['chr1', 'chr2', 'chr3', 'chr4', 'chr5', 'chr6', 'chr7', 'chr8', 'chr9', 'chr10', 'chr11', 'chr12', 'chr13', 'chr14', 'chr15', 'chr16', 'chr17', 'chr18', 'chr19', 'unplaced_scaffold20', 'unplaced_scaffold21', 'unplaced_scaffold22', 'unplaced_scaffold23', 'unplaced_scaffold24', 'unplaced_scaffold25', 'unplaced_scaffold26', 'unplaced_scaffold27', 'unplaced_scaffold28', 'unplaced_scaffold29', 'unplaced_scaffold30', 'unplaced_scaffold31', 'unplaced_scaffold32', 'unplaced_scaffold33', 'unplaced_scaffold34', 'unplaced_scaffold35', 'unplaced_scaffold36', 'unplaced_scaffold37', 'unplaced_scaffold38', 'unplaced_scaffold39', 'unplaced_scaffold40', 'unplaced_scaffold41', 'unplaced_scaffold42', 'unplaced_scaffold43', 'unplaced_scaffold44', 'unp",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:5993,usability,input,input,5993,", 'unplaced_scaffold69', 'unplaced_scaffold70', 'unplaced_scaffold71', 'unplaced_scaffold72', 'unplaced_scaffold73', 'unplaced_scaffold74', 'unplaced_scaffold75', 'unplaced_scaffold76', 'unplaced_scaffold77', 'unplaced_scaffold78', 'unplaced_scaffold79', 'unplaced_scaffold80', 'unplaced_scaffold81', 'unplaced_scaffold82', 'unplaced_scaffold83', 'unplaced_scaffold84', 'unplaced_scaffold85', 'unplaced_scaffold86', 'unplaced_scaffold87', 'unplaced_scaffold88', 'unplaced_scaffold89', 'unplaced_scaffold90', 'unplaced_scaffold91', 'unplaced_scaffold92', 'unplaced_scaffold93', 'unplaced_scaffold94', 'unplaced_scaffold95', 'unplaced_scaffold96', 'unplaced_scaffold97', 'unplaced_scaffold98', 'unplaced_scaffold99', 'unplaced_scaffold100', 'unplaced_scaffold101', 'unplaced_scaffold102', 'unplaced_scaffold103', 'unplaced_scaffold104']. # I0423 11:43:12.466705 140211385890624 make_examples_core.py:301] Task 0/12: Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. # I0423 11:43:12.538744 140211385890624 genomics_reader.py:222] Reading /work/cjm124/SWFst/VarCalling/reads/bc2001_aligned_sorted.bam with NativeSamReader. # I0423 11:43:12.636761 140211385890624 genomics_reader.py:222] Reading /work/cjm124/SWFst/VarCalling/reads/bc2001_aligned_sorted.bam with NativeSamReader. # I0423 11:43:12.637369 140211385890624 make_examples_core.py:301] Task 0/12: Writing gvcf records to /tmp/tmpkmab_2kw/gvcf.tfrecord-00000-of-00012.gz. # I0423 11:43:12.637865 140211385890624 make_examples_core.py:301] Task 0/12: Writing examples to /tmp/tmpkmab_2kw/make_examples.tfrecord-00000-of-00012.gz. # I0423 11:43:12.637962 140211385890624 make_examples_core.py:301] Task 0/12: Overhead for preparing inputs: 0 seconds. # 2024-04-23 11:43:12.645232: W ./third_party/nucleus/util/proto_clif_converter.h:75] Failed to cast type N6google8protobuf14DynamicMessageE. # Traceback (most recent call last):.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:6798,usability,input,inputs,6798,"103', 'unplaced_scaffold104']. # I0423 11:43:12.466705 140211385890624 make_examples_core.py:301] Task 0/12: Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. # I0423 11:43:12.538744 140211385890624 genomics_reader.py:222] Reading /work/cjm124/SWFst/VarCalling/reads/bc2001_aligned_sorted.bam with NativeSamReader. # I0423 11:43:12.636761 140211385890624 genomics_reader.py:222] Reading /work/cjm124/SWFst/VarCalling/reads/bc2001_aligned_sorted.bam with NativeSamReader. # I0423 11:43:12.637369 140211385890624 make_examples_core.py:301] Task 0/12: Writing gvcf records to /tmp/tmpkmab_2kw/gvcf.tfrecord-00000-of-00012.gz. # I0423 11:43:12.637865 140211385890624 make_examples_core.py:301] Task 0/12: Writing examples to /tmp/tmpkmab_2kw/make_examples.tfrecord-00000-of-00012.gz. # I0423 11:43:12.637962 140211385890624 make_examples_core.py:301] Task 0/12: Overhead for preparing inputs: 0 seconds. # 2024-04-23 11:43:12.645232: W ./third_party/nucleus/util/proto_clif_converter.h:75] Failed to cast type N6google8protobuf14DynamicMessageE. # Traceback (most recent call last):. # File ""/tmp/Bazel.runfiles_qy0tffir/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 234, in <module>. # app.run(main). # File ""/tmp/Bazel.runfiles_qy0tffir/runfiles/absl_py/absl/app.py"", line 312, in run. # _run_main(main, args). # File ""/tmp/Bazel.runfiles_qy0tffir/runfiles/absl_py/absl/app.py"", line 258, in _run_main. # sys.exit(main(argv)). # File ""/tmp/Bazel.runfiles_qy0tffir/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 224, in main. # make_examples_core.make_examples_runner(options). # File ""/tmp/Bazel.runfiles_qy0tffir/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2838, in make_examples_runner. # region_processor.process(region, region_n). # File ""/tmp/Bazel.runfiles_qy0tffir/runfiles/com_google_deepvariant/deepvarian",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:9508,usability,user,user,9508,"examples_core.py"", line 2838, in make_examples_runner. # region_processor.process(region, region_n). # File ""/tmp/Bazel.runfiles_qy0tffir/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1695, in process. # sample_reads = self.region_reads_norealign(. # File ""/tmp/Bazel.runfiles_qy0tffir/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1817, in region_reads_norealign. # reads = reservoir_sample_reads(. # File ""/tmp/Bazel.runfiles_qy0tffir/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 976, in reservoir_sample_reads. # return utils.reservoir_sample(iterable_of_reads, k, random). # File ""/tmp/Bazel.runfiles_qy0tffir/runfiles/com_google_deepvariant/third_party/nucleus/util/utils.py"", line 117, in reservoir_sample. # for i, item in enumerate(iterable):. # File ""/tmp/Bazel.runfiles_qy0tffir/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 95, in __next__. # record, not_done = self._raw_next(). # File ""/tmp/Bazel.runfiles_qy0tffir/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 154, in _raw_next. # not_done = self._cc_iterable.PythonNext(record). # RuntimeError: PythonNext() argument read is not valid: Dynamic cast failed. # parallel: This job failed:. # /opt/deepvariant/bin/make_examples --mode calling --ref /work/cjm124/SWFst/lvar3ref/Lvar_scaffolds.fasta --reads /work/cjm124/SWFst/VarCalling/reads/bc2001_aligned_sorted.bam --examples /tmp/tmpkmab_2kw/make_examples.tfrecord@12.gz --add_hp_channel --alt_aligned_pileup diff_channels --gvcf /tmp/tmpkmab_2kw/gvcf.tfrecord@12.gz --max_reads_per_partition 600 --min_mapping_quality 1 --parse_sam_aux_fields --partition_size 25000 --phase_reads --pileup_image_width 199 --norealign_reads --sort_by_haplotypes --track_ref_reads --vsc_min_fraction_indels 0.12 --task 0. # real	0m15.200s. # user	0m3.162s. # sys	0m1.161s. # finished with /work/cjm124/SWFst/VarCalling/reads/bc2001_aligned_sorted.bam. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:194,availability,error,error,194,"Hi @Carl-labhub , one thing to confirm:. In the original post, you said:. Installation method (Docker, built from source, etc.): Docker. But from the information you provided, it seems like the error you encountered was when you ran with Singularity. Can you confirm: Do you see the error both when you use Docker and Singularity, or just Singularity? I'll plan to try to reproduce on my side. But clarifying that will be helpful. Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:283,availability,error,error,283,"Hi @Carl-labhub , one thing to confirm:. In the original post, you said:. Installation method (Docker, built from source, etc.): Docker. But from the information you provided, it seems like the error you encountered was when you ran with Singularity. Can you confirm: Do you see the error both when you use Docker and Singularity, or just Singularity? I'll plan to try to reproduce on my side. But clarifying that will be helpful. Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:74,deployability,Instal,Installation,74,"Hi @Carl-labhub , one thing to confirm:. In the original post, you said:. Installation method (Docker, built from source, etc.): Docker. But from the information you provided, it seems like the error you encountered was when you ran with Singularity. Can you confirm: Do you see the error both when you use Docker and Singularity, or just Singularity? I'll plan to try to reproduce on my side. But clarifying that will be helpful. Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:194,performance,error,error,194,"Hi @Carl-labhub , one thing to confirm:. In the original post, you said:. Installation method (Docker, built from source, etc.): Docker. But from the information you provided, it seems like the error you encountered was when you ran with Singularity. Can you confirm: Do you see the error both when you use Docker and Singularity, or just Singularity? I'll plan to try to reproduce on my side. But clarifying that will be helpful. Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:283,performance,error,error,283,"Hi @Carl-labhub , one thing to confirm:. In the original post, you said:. Installation method (Docker, built from source, etc.): Docker. But from the information you provided, it seems like the error you encountered was when you ran with Singularity. Can you confirm: Do you see the error both when you use Docker and Singularity, or just Singularity? I'll plan to try to reproduce on my side. But clarifying that will be helpful. Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:194,safety,error,error,194,"Hi @Carl-labhub , one thing to confirm:. In the original post, you said:. Installation method (Docker, built from source, etc.): Docker. But from the information you provided, it seems like the error you encountered was when you ran with Singularity. Can you confirm: Do you see the error both when you use Docker and Singularity, or just Singularity? I'll plan to try to reproduce on my side. But clarifying that will be helpful. Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:283,safety,error,error,283,"Hi @Carl-labhub , one thing to confirm:. In the original post, you said:. Installation method (Docker, built from source, etc.): Docker. But from the information you provided, it seems like the error you encountered was when you ran with Singularity. Can you confirm: Do you see the error both when you use Docker and Singularity, or just Singularity? I'll plan to try to reproduce on my side. But clarifying that will be helpful. Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:357,testability,plan,plan,357,"Hi @Carl-labhub , one thing to confirm:. In the original post, you said:. Installation method (Docker, built from source, etc.): Docker. But from the information you provided, it seems like the error you encountered was when you ran with Singularity. Can you confirm: Do you see the error both when you use Docker and Singularity, or just Singularity? I'll plan to try to reproduce on my side. But clarifying that will be helpful. Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:31,usability,confirm,confirm,31,"Hi @Carl-labhub , one thing to confirm:. In the original post, you said:. Installation method (Docker, built from source, etc.): Docker. But from the information you provided, it seems like the error you encountered was when you ran with Singularity. Can you confirm: Do you see the error both when you use Docker and Singularity, or just Singularity? I'll plan to try to reproduce on my side. But clarifying that will be helpful. Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:194,usability,error,error,194,"Hi @Carl-labhub , one thing to confirm:. In the original post, you said:. Installation method (Docker, built from source, etc.): Docker. But from the information you provided, it seems like the error you encountered was when you ran with Singularity. Can you confirm: Do you see the error both when you use Docker and Singularity, or just Singularity? I'll plan to try to reproduce on my side. But clarifying that will be helpful. Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:259,usability,confirm,confirm,259,"Hi @Carl-labhub , one thing to confirm:. In the original post, you said:. Installation method (Docker, built from source, etc.): Docker. But from the information you provided, it seems like the error you encountered was when you ran with Singularity. Can you confirm: Do you see the error both when you use Docker and Singularity, or just Singularity? I'll plan to try to reproduce on my side. But clarifying that will be helpful. Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:283,usability,error,error,283,"Hi @Carl-labhub , one thing to confirm:. In the original post, you said:. Installation method (Docker, built from source, etc.): Docker. But from the information you provided, it seems like the error you encountered was when you ran with Singularity. Can you confirm: Do you see the error both when you use Docker and Singularity, or just Singularity? I'll plan to try to reproduce on my side. But clarifying that will be helpful. Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:422,usability,help,helpful,422,"Hi @Carl-labhub , one thing to confirm:. In the original post, you said:. Installation method (Docker, built from source, etc.): Docker. But from the information you provided, it seems like the error you encountered was when you ran with Singularity. Can you confirm: Do you see the error both when you use Docker and Singularity, or just Singularity? I'll plan to try to reproduce on my side. But clarifying that will be helpful. Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:136,security,session,session,136,"Sorry, It’s singularity. I built using singularity, and am using singularity to run it. When I run it, I’m doing so from an interactive session with `singularity exec`",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
https://github.com/google/deepvariant/issues/812:124,usability,interact,interactive,124,"Sorry, It’s singularity. I built using singularity, and am using singularity to run it. When I run it, I’m doing so from an interactive session with `singularity exec`",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/812
