id,quality_attribute,keyword,matched_word,match_idx,sentence,source,author,repo,version,wiki,url
https://github.com/scverse/scanpy/issues/1556:2184,usability,consist,consistent,2184,"com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. ### Minimal code sample (that we can copy&paste without having any data). I am getting this error and although i downgraded the scipy to 1.4.1 as per one recommendation here but still didn't solve it! ```python. import numpy as np. import pandas as pd. from sklearn.datasets import load_iris. from sklearn.pipeline import make_pipeline. from sklearn.preprocessing import StandardScaler, Normalizer. from scipy.cluster.hierarchy import linkage, dendrogram. df_euro = pd.read_csv('https://assets.datacamp.com/production/repositories/655/datasets/2a1f3ab7bcc76eef1b8e1eb29afbd54c4ebf86f2/eurovision-2016.csv'). samples = df.iloc[:, 2:7].values[:42]. country_names = df.iloc[:, 1].values[:42]. mergings = linkage(samples, method='single'). # Plot the dendrogram. plt.figure(figsize=(15, 5)). dendrogram(mergings,. labels=country_names,. leaf_rotation=90, . leaf_font_size=6);. ```. ```pytb. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-185-e360a70857f0> in <module>. 9 # Plot the dendrogram. 10 plt.figure(figsize=(15, 5)). ---> 11 dendrogram(mergings, . 12 labels=companies,. 13 leaf_rotation=90,. C:\ProgramData\Anaconda3\lib\site-packages\scipy\cluster\hierarchy.py in dendrogram(Z, p, truncate_mode, color_threshold, get_leaves, orientation, labels, count_sort, distance_sort, show_leaf_counts, no_plot, no_labels, leaf_font_size, leaf_rotation, leaf_label_func, show_contracted, link_color_func, ax, above_threshold_color). 3275 ""'bottom', or 'right'""). 3276 . -> 3277 if labels and Z.shape[0] + 1 != len(labels):. 3278 raise ValueError(""Dimensions of Z and labels must be consistent.""). 3279 . ValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all(). ```. #### Versions. Python: 3.8.6. Numpy: 1.18.5. scipy: 1.4.1. pandas : 1.0.5.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1556
https://github.com/scverse/scanpy/pull/1557:22,safety,test,testing,22,Replace sklearn.utils.testing with numpy.testing; sklearn.utils.testing is deprecated in scikit-learn 0.24: https://github.com/scikit-learn/scikit-learn/pull/17133,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1557
https://github.com/scverse/scanpy/pull/1557:41,safety,test,testing,41,Replace sklearn.utils.testing with numpy.testing; sklearn.utils.testing is deprecated in scikit-learn 0.24: https://github.com/scikit-learn/scikit-learn/pull/17133,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1557
https://github.com/scverse/scanpy/pull/1557:64,safety,test,testing,64,Replace sklearn.utils.testing with numpy.testing; sklearn.utils.testing is deprecated in scikit-learn 0.24: https://github.com/scikit-learn/scikit-learn/pull/17133,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1557
https://github.com/scverse/scanpy/pull/1557:22,testability,test,testing,22,Replace sklearn.utils.testing with numpy.testing; sklearn.utils.testing is deprecated in scikit-learn 0.24: https://github.com/scikit-learn/scikit-learn/pull/17133,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1557
https://github.com/scverse/scanpy/pull/1557:41,testability,test,testing,41,Replace sklearn.utils.testing with numpy.testing; sklearn.utils.testing is deprecated in scikit-learn 0.24: https://github.com/scikit-learn/scikit-learn/pull/17133,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1557
https://github.com/scverse/scanpy/pull/1557:64,testability,test,testing,64,Replace sklearn.utils.testing with numpy.testing; sklearn.utils.testing is deprecated in scikit-learn 0.24: https://github.com/scikit-learn/scikit-learn/pull/17133,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1557
https://github.com/scverse/scanpy/pull/1557:96,usability,learn,learn,96,Replace sklearn.utils.testing with numpy.testing; sklearn.utils.testing is deprecated in scikit-learn 0.24: https://github.com/scikit-learn/scikit-learn/pull/17133,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1557
https://github.com/scverse/scanpy/pull/1557:134,usability,learn,learn,134,Replace sklearn.utils.testing with numpy.testing; sklearn.utils.testing is deprecated in scikit-learn 0.24: https://github.com/scikit-learn/scikit-learn/pull/17133,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1557
https://github.com/scverse/scanpy/pull/1557:147,usability,learn,learn,147,Replace sklearn.utils.testing with numpy.testing; sklearn.utils.testing is deprecated in scikit-learn 0.24: https://github.com/scikit-learn/scikit-learn/pull/17133,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1557
https://github.com/scverse/scanpy/pull/1558:15,performance,cach,cached,15,"Do not use the cached rp_forest; As @falexwolf noticed, caching `rp_forest` from previous neighbors calculations can cause problems.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1558
https://github.com/scverse/scanpy/pull/1558:56,performance,cach,caching,56,"Do not use the cached rp_forest; As @falexwolf noticed, caching `rp_forest` from previous neighbors calculations can cause problems.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1558
https://github.com/scverse/scanpy/issues/1559:63,deployability,pipelin,pipeline,63,"Hello All, I am trying to learn and create single cell RNA seq pipeline for my project. When I was doing quality control, I met this problem. Can anyone help me? Thank you a lot.; - [ ] I have checked that this issue has not already been reported. - [ ] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. ### Minimal code sample (that we can copy&paste without having any data). ```python. mito_genes = adata.var_names.str.startswith('mt-'). # for each cell compute fraction of counts in mito genes vs. all genes. # the `.A1` is only necessary as X is sparse (to transform to a dense array after summing). adata.obs['percent_mito'] = np.sum(. adata[:, mito_genes].X, axis=1).A1 / np.sum(adata.X, axis=1).A1. # add the total counts per cell as observations-annotation to adata. adata.obs['n_counts'] = adata.X.sum(axis=1).A1. print(sum(mito_genes)). ```. ```pytb. AttributeError Traceback (most recent call last). <ipython-input-18-7f1a7fee3beb> in <module>. 2 # for each cell compute fraction of counts in mito genes vs. all genes. 3 # the `.A1` is only necessary as X is sparse (to transform to a dense array after summing). ----> 4 adata.obs['percent_mito'] = np.sum(. 5 adata[:, mito_genes].X, axis=1).A1 / np.sum(adata.X, axis=1).A1. 6 # add the total counts per cell as observations-annotation to adata. ```. #### Versions. <details>. AttributeError: 'ArrayView' object has no attribute 'A1'. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1559
https://github.com/scverse/scanpy/issues/1559:301,deployability,version,version,301,"Hello All, I am trying to learn and create single cell RNA seq pipeline for my project. When I was doing quality control, I met this problem. Can anyone help me? Thank you a lot.; - [ ] I have checked that this issue has not already been reported. - [ ] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. ### Minimal code sample (that we can copy&paste without having any data). ```python. mito_genes = adata.var_names.str.startswith('mt-'). # for each cell compute fraction of counts in mito genes vs. all genes. # the `.A1` is only necessary as X is sparse (to transform to a dense array after summing). adata.obs['percent_mito'] = np.sum(. adata[:, mito_genes].X, axis=1).A1 / np.sum(adata.X, axis=1).A1. # add the total counts per cell as observations-annotation to adata. adata.obs['n_counts'] = adata.X.sum(axis=1).A1. print(sum(mito_genes)). ```. ```pytb. AttributeError Traceback (most recent call last). <ipython-input-18-7f1a7fee3beb> in <module>. 2 # for each cell compute fraction of counts in mito genes vs. all genes. 3 # the `.A1` is only necessary as X is sparse (to transform to a dense array after summing). ----> 4 adata.obs['percent_mito'] = np.sum(. 5 adata[:, mito_genes].X, axis=1).A1 / np.sum(adata.X, axis=1).A1. 6 # add the total counts per cell as observations-annotation to adata. ```. #### Versions. <details>. AttributeError: 'ArrayView' object has no attribute 'A1'. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1559
https://github.com/scverse/scanpy/issues/1559:1030,deployability,observ,observations-annotation,1030,"Hello All, I am trying to learn and create single cell RNA seq pipeline for my project. When I was doing quality control, I met this problem. Can anyone help me? Thank you a lot.; - [ ] I have checked that this issue has not already been reported. - [ ] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. ### Minimal code sample (that we can copy&paste without having any data). ```python. mito_genes = adata.var_names.str.startswith('mt-'). # for each cell compute fraction of counts in mito genes vs. all genes. # the `.A1` is only necessary as X is sparse (to transform to a dense array after summing). adata.obs['percent_mito'] = np.sum(. adata[:, mito_genes].X, axis=1).A1 / np.sum(adata.X, axis=1).A1. # add the total counts per cell as observations-annotation to adata. adata.obs['n_counts'] = adata.X.sum(axis=1).A1. print(sum(mito_genes)). ```. ```pytb. AttributeError Traceback (most recent call last). <ipython-input-18-7f1a7fee3beb> in <module>. 2 # for each cell compute fraction of counts in mito genes vs. all genes. 3 # the `.A1` is only necessary as X is sparse (to transform to a dense array after summing). ----> 4 adata.obs['percent_mito'] = np.sum(. 5 adata[:, mito_genes].X, axis=1).A1 / np.sum(adata.X, axis=1).A1. 6 # add the total counts per cell as observations-annotation to adata. ```. #### Versions. <details>. AttributeError: 'ArrayView' object has no attribute 'A1'. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1559
https://github.com/scverse/scanpy/issues/1559:1236,deployability,modul,module,1236,"Hello All, I am trying to learn and create single cell RNA seq pipeline for my project. When I was doing quality control, I met this problem. Can anyone help me? Thank you a lot.; - [ ] I have checked that this issue has not already been reported. - [ ] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. ### Minimal code sample (that we can copy&paste without having any data). ```python. mito_genes = adata.var_names.str.startswith('mt-'). # for each cell compute fraction of counts in mito genes vs. all genes. # the `.A1` is only necessary as X is sparse (to transform to a dense array after summing). adata.obs['percent_mito'] = np.sum(. adata[:, mito_genes].X, axis=1).A1 / np.sum(adata.X, axis=1).A1. # add the total counts per cell as observations-annotation to adata. adata.obs['n_counts'] = adata.X.sum(axis=1).A1. print(sum(mito_genes)). ```. ```pytb. AttributeError Traceback (most recent call last). <ipython-input-18-7f1a7fee3beb> in <module>. 2 # for each cell compute fraction of counts in mito genes vs. all genes. 3 # the `.A1` is only necessary as X is sparse (to transform to a dense array after summing). ----> 4 adata.obs['percent_mito'] = np.sum(. 5 adata[:, mito_genes].X, axis=1).A1 / np.sum(adata.X, axis=1).A1. 6 # add the total counts per cell as observations-annotation to adata. ```. #### Versions. <details>. AttributeError: 'ArrayView' object has no attribute 'A1'. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1559
https://github.com/scverse/scanpy/issues/1559:1562,deployability,observ,observations-annotation,1562,"Hello All, I am trying to learn and create single cell RNA seq pipeline for my project. When I was doing quality control, I met this problem. Can anyone help me? Thank you a lot.; - [ ] I have checked that this issue has not already been reported. - [ ] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. ### Minimal code sample (that we can copy&paste without having any data). ```python. mito_genes = adata.var_names.str.startswith('mt-'). # for each cell compute fraction of counts in mito genes vs. all genes. # the `.A1` is only necessary as X is sparse (to transform to a dense array after summing). adata.obs['percent_mito'] = np.sum(. adata[:, mito_genes].X, axis=1).A1 / np.sum(adata.X, axis=1).A1. # add the total counts per cell as observations-annotation to adata. adata.obs['n_counts'] = adata.X.sum(axis=1).A1. print(sum(mito_genes)). ```. ```pytb. AttributeError Traceback (most recent call last). <ipython-input-18-7f1a7fee3beb> in <module>. 2 # for each cell compute fraction of counts in mito genes vs. all genes. 3 # the `.A1` is only necessary as X is sparse (to transform to a dense array after summing). ----> 4 adata.obs['percent_mito'] = np.sum(. 5 adata[:, mito_genes].X, axis=1).A1 / np.sum(adata.X, axis=1).A1. 6 # add the total counts per cell as observations-annotation to adata. ```. #### Versions. <details>. AttributeError: 'ArrayView' object has no attribute 'A1'. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1559
https://github.com/scverse/scanpy/issues/1559:1606,deployability,Version,Versions,1606,"Hello All, I am trying to learn and create single cell RNA seq pipeline for my project. When I was doing quality control, I met this problem. Can anyone help me? Thank you a lot.; - [ ] I have checked that this issue has not already been reported. - [ ] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. ### Minimal code sample (that we can copy&paste without having any data). ```python. mito_genes = adata.var_names.str.startswith('mt-'). # for each cell compute fraction of counts in mito genes vs. all genes. # the `.A1` is only necessary as X is sparse (to transform to a dense array after summing). adata.obs['percent_mito'] = np.sum(. adata[:, mito_genes].X, axis=1).A1 / np.sum(adata.X, axis=1).A1. # add the total counts per cell as observations-annotation to adata. adata.obs['n_counts'] = adata.X.sum(axis=1).A1. print(sum(mito_genes)). ```. ```pytb. AttributeError Traceback (most recent call last). <ipython-input-18-7f1a7fee3beb> in <module>. 2 # for each cell compute fraction of counts in mito genes vs. all genes. 3 # the `.A1` is only necessary as X is sparse (to transform to a dense array after summing). ----> 4 adata.obs['percent_mito'] = np.sum(. 5 adata[:, mito_genes].X, axis=1).A1 / np.sum(adata.X, axis=1).A1. 6 # add the total counts per cell as observations-annotation to adata. ```. #### Versions. <details>. AttributeError: 'ArrayView' object has no attribute 'A1'. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1559
https://github.com/scverse/scanpy/issues/1559:63,integrability,pipelin,pipeline,63,"Hello All, I am trying to learn and create single cell RNA seq pipeline for my project. When I was doing quality control, I met this problem. Can anyone help me? Thank you a lot.; - [ ] I have checked that this issue has not already been reported. - [ ] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. ### Minimal code sample (that we can copy&paste without having any data). ```python. mito_genes = adata.var_names.str.startswith('mt-'). # for each cell compute fraction of counts in mito genes vs. all genes. # the `.A1` is only necessary as X is sparse (to transform to a dense array after summing). adata.obs['percent_mito'] = np.sum(. adata[:, mito_genes].X, axis=1).A1 / np.sum(adata.X, axis=1).A1. # add the total counts per cell as observations-annotation to adata. adata.obs['n_counts'] = adata.X.sum(axis=1).A1. print(sum(mito_genes)). ```. ```pytb. AttributeError Traceback (most recent call last). <ipython-input-18-7f1a7fee3beb> in <module>. 2 # for each cell compute fraction of counts in mito genes vs. all genes. 3 # the `.A1` is only necessary as X is sparse (to transform to a dense array after summing). ----> 4 adata.obs['percent_mito'] = np.sum(. 5 adata[:, mito_genes].X, axis=1).A1 / np.sum(adata.X, axis=1).A1. 6 # add the total counts per cell as observations-annotation to adata. ```. #### Versions. <details>. AttributeError: 'ArrayView' object has no attribute 'A1'. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1559
https://github.com/scverse/scanpy/issues/1559:301,integrability,version,version,301,"Hello All, I am trying to learn and create single cell RNA seq pipeline for my project. When I was doing quality control, I met this problem. Can anyone help me? Thank you a lot.; - [ ] I have checked that this issue has not already been reported. - [ ] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. ### Minimal code sample (that we can copy&paste without having any data). ```python. mito_genes = adata.var_names.str.startswith('mt-'). # for each cell compute fraction of counts in mito genes vs. all genes. # the `.A1` is only necessary as X is sparse (to transform to a dense array after summing). adata.obs['percent_mito'] = np.sum(. adata[:, mito_genes].X, axis=1).A1 / np.sum(adata.X, axis=1).A1. # add the total counts per cell as observations-annotation to adata. adata.obs['n_counts'] = adata.X.sum(axis=1).A1. print(sum(mito_genes)). ```. ```pytb. AttributeError Traceback (most recent call last). <ipython-input-18-7f1a7fee3beb> in <module>. 2 # for each cell compute fraction of counts in mito genes vs. all genes. 3 # the `.A1` is only necessary as X is sparse (to transform to a dense array after summing). ----> 4 adata.obs['percent_mito'] = np.sum(. 5 adata[:, mito_genes].X, axis=1).A1 / np.sum(adata.X, axis=1).A1. 6 # add the total counts per cell as observations-annotation to adata. ```. #### Versions. <details>. AttributeError: 'ArrayView' object has no attribute 'A1'. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1559
https://github.com/scverse/scanpy/issues/1559:850,integrability,transform,transform,850,"Hello All, I am trying to learn and create single cell RNA seq pipeline for my project. When I was doing quality control, I met this problem. Can anyone help me? Thank you a lot.; - [ ] I have checked that this issue has not already been reported. - [ ] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. ### Minimal code sample (that we can copy&paste without having any data). ```python. mito_genes = adata.var_names.str.startswith('mt-'). # for each cell compute fraction of counts in mito genes vs. all genes. # the `.A1` is only necessary as X is sparse (to transform to a dense array after summing). adata.obs['percent_mito'] = np.sum(. adata[:, mito_genes].X, axis=1).A1 / np.sum(adata.X, axis=1).A1. # add the total counts per cell as observations-annotation to adata. adata.obs['n_counts'] = adata.X.sum(axis=1).A1. print(sum(mito_genes)). ```. ```pytb. AttributeError Traceback (most recent call last). <ipython-input-18-7f1a7fee3beb> in <module>. 2 # for each cell compute fraction of counts in mito genes vs. all genes. 3 # the `.A1` is only necessary as X is sparse (to transform to a dense array after summing). ----> 4 adata.obs['percent_mito'] = np.sum(. 5 adata[:, mito_genes].X, axis=1).A1 / np.sum(adata.X, axis=1).A1. 6 # add the total counts per cell as observations-annotation to adata. ```. #### Versions. <details>. AttributeError: 'ArrayView' object has no attribute 'A1'. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1559
https://github.com/scverse/scanpy/issues/1559:1370,integrability,transform,transform,1370,"Hello All, I am trying to learn and create single cell RNA seq pipeline for my project. When I was doing quality control, I met this problem. Can anyone help me? Thank you a lot.; - [ ] I have checked that this issue has not already been reported. - [ ] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. ### Minimal code sample (that we can copy&paste without having any data). ```python. mito_genes = adata.var_names.str.startswith('mt-'). # for each cell compute fraction of counts in mito genes vs. all genes. # the `.A1` is only necessary as X is sparse (to transform to a dense array after summing). adata.obs['percent_mito'] = np.sum(. adata[:, mito_genes].X, axis=1).A1 / np.sum(adata.X, axis=1).A1. # add the total counts per cell as observations-annotation to adata. adata.obs['n_counts'] = adata.X.sum(axis=1).A1. print(sum(mito_genes)). ```. ```pytb. AttributeError Traceback (most recent call last). <ipython-input-18-7f1a7fee3beb> in <module>. 2 # for each cell compute fraction of counts in mito genes vs. all genes. 3 # the `.A1` is only necessary as X is sparse (to transform to a dense array after summing). ----> 4 adata.obs['percent_mito'] = np.sum(. 5 adata[:, mito_genes].X, axis=1).A1 / np.sum(adata.X, axis=1).A1. 6 # add the total counts per cell as observations-annotation to adata. ```. #### Versions. <details>. AttributeError: 'ArrayView' object has no attribute 'A1'. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1559
https://github.com/scverse/scanpy/issues/1559:1606,integrability,Version,Versions,1606,"Hello All, I am trying to learn and create single cell RNA seq pipeline for my project. When I was doing quality control, I met this problem. Can anyone help me? Thank you a lot.; - [ ] I have checked that this issue has not already been reported. - [ ] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. ### Minimal code sample (that we can copy&paste without having any data). ```python. mito_genes = adata.var_names.str.startswith('mt-'). # for each cell compute fraction of counts in mito genes vs. all genes. # the `.A1` is only necessary as X is sparse (to transform to a dense array after summing). adata.obs['percent_mito'] = np.sum(. adata[:, mito_genes].X, axis=1).A1 / np.sum(adata.X, axis=1).A1. # add the total counts per cell as observations-annotation to adata. adata.obs['n_counts'] = adata.X.sum(axis=1).A1. print(sum(mito_genes)). ```. ```pytb. AttributeError Traceback (most recent call last). <ipython-input-18-7f1a7fee3beb> in <module>. 2 # for each cell compute fraction of counts in mito genes vs. all genes. 3 # the `.A1` is only necessary as X is sparse (to transform to a dense array after summing). ----> 4 adata.obs['percent_mito'] = np.sum(. 5 adata[:, mito_genes].X, axis=1).A1 / np.sum(adata.X, axis=1).A1. 6 # add the total counts per cell as observations-annotation to adata. ```. #### Versions. <details>. AttributeError: 'ArrayView' object has no attribute 'A1'. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1559
https://github.com/scverse/scanpy/issues/1559:850,interoperability,transform,transform,850,"Hello All, I am trying to learn and create single cell RNA seq pipeline for my project. When I was doing quality control, I met this problem. Can anyone help me? Thank you a lot.; - [ ] I have checked that this issue has not already been reported. - [ ] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. ### Minimal code sample (that we can copy&paste without having any data). ```python. mito_genes = adata.var_names.str.startswith('mt-'). # for each cell compute fraction of counts in mito genes vs. all genes. # the `.A1` is only necessary as X is sparse (to transform to a dense array after summing). adata.obs['percent_mito'] = np.sum(. adata[:, mito_genes].X, axis=1).A1 / np.sum(adata.X, axis=1).A1. # add the total counts per cell as observations-annotation to adata. adata.obs['n_counts'] = adata.X.sum(axis=1).A1. print(sum(mito_genes)). ```. ```pytb. AttributeError Traceback (most recent call last). <ipython-input-18-7f1a7fee3beb> in <module>. 2 # for each cell compute fraction of counts in mito genes vs. all genes. 3 # the `.A1` is only necessary as X is sparse (to transform to a dense array after summing). ----> 4 adata.obs['percent_mito'] = np.sum(. 5 adata[:, mito_genes].X, axis=1).A1 / np.sum(adata.X, axis=1).A1. 6 # add the total counts per cell as observations-annotation to adata. ```. #### Versions. <details>. AttributeError: 'ArrayView' object has no attribute 'A1'. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1559
https://github.com/scverse/scanpy/issues/1559:1370,interoperability,transform,transform,1370,"Hello All, I am trying to learn and create single cell RNA seq pipeline for my project. When I was doing quality control, I met this problem. Can anyone help me? Thank you a lot.; - [ ] I have checked that this issue has not already been reported. - [ ] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. ### Minimal code sample (that we can copy&paste without having any data). ```python. mito_genes = adata.var_names.str.startswith('mt-'). # for each cell compute fraction of counts in mito genes vs. all genes. # the `.A1` is only necessary as X is sparse (to transform to a dense array after summing). adata.obs['percent_mito'] = np.sum(. adata[:, mito_genes].X, axis=1).A1 / np.sum(adata.X, axis=1).A1. # add the total counts per cell as observations-annotation to adata. adata.obs['n_counts'] = adata.X.sum(axis=1).A1. print(sum(mito_genes)). ```. ```pytb. AttributeError Traceback (most recent call last). <ipython-input-18-7f1a7fee3beb> in <module>. 2 # for each cell compute fraction of counts in mito genes vs. all genes. 3 # the `.A1` is only necessary as X is sparse (to transform to a dense array after summing). ----> 4 adata.obs['percent_mito'] = np.sum(. 5 adata[:, mito_genes].X, axis=1).A1 / np.sum(adata.X, axis=1).A1. 6 # add the total counts per cell as observations-annotation to adata. ```. #### Versions. <details>. AttributeError: 'ArrayView' object has no attribute 'A1'. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1559
https://github.com/scverse/scanpy/issues/1559:301,modifiability,version,version,301,"Hello All, I am trying to learn and create single cell RNA seq pipeline for my project. When I was doing quality control, I met this problem. Can anyone help me? Thank you a lot.; - [ ] I have checked that this issue has not already been reported. - [ ] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. ### Minimal code sample (that we can copy&paste without having any data). ```python. mito_genes = adata.var_names.str.startswith('mt-'). # for each cell compute fraction of counts in mito genes vs. all genes. # the `.A1` is only necessary as X is sparse (to transform to a dense array after summing). adata.obs['percent_mito'] = np.sum(. adata[:, mito_genes].X, axis=1).A1 / np.sum(adata.X, axis=1).A1. # add the total counts per cell as observations-annotation to adata. adata.obs['n_counts'] = adata.X.sum(axis=1).A1. print(sum(mito_genes)). ```. ```pytb. AttributeError Traceback (most recent call last). <ipython-input-18-7f1a7fee3beb> in <module>. 2 # for each cell compute fraction of counts in mito genes vs. all genes. 3 # the `.A1` is only necessary as X is sparse (to transform to a dense array after summing). ----> 4 adata.obs['percent_mito'] = np.sum(. 5 adata[:, mito_genes].X, axis=1).A1 / np.sum(adata.X, axis=1).A1. 6 # add the total counts per cell as observations-annotation to adata. ```. #### Versions. <details>. AttributeError: 'ArrayView' object has no attribute 'A1'. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1559
https://github.com/scverse/scanpy/issues/1559:1236,modifiability,modul,module,1236,"Hello All, I am trying to learn and create single cell RNA seq pipeline for my project. When I was doing quality control, I met this problem. Can anyone help me? Thank you a lot.; - [ ] I have checked that this issue has not already been reported. - [ ] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. ### Minimal code sample (that we can copy&paste without having any data). ```python. mito_genes = adata.var_names.str.startswith('mt-'). # for each cell compute fraction of counts in mito genes vs. all genes. # the `.A1` is only necessary as X is sparse (to transform to a dense array after summing). adata.obs['percent_mito'] = np.sum(. adata[:, mito_genes].X, axis=1).A1 / np.sum(adata.X, axis=1).A1. # add the total counts per cell as observations-annotation to adata. adata.obs['n_counts'] = adata.X.sum(axis=1).A1. print(sum(mito_genes)). ```. ```pytb. AttributeError Traceback (most recent call last). <ipython-input-18-7f1a7fee3beb> in <module>. 2 # for each cell compute fraction of counts in mito genes vs. all genes. 3 # the `.A1` is only necessary as X is sparse (to transform to a dense array after summing). ----> 4 adata.obs['percent_mito'] = np.sum(. 5 adata[:, mito_genes].X, axis=1).A1 / np.sum(adata.X, axis=1).A1. 6 # add the total counts per cell as observations-annotation to adata. ```. #### Versions. <details>. AttributeError: 'ArrayView' object has no attribute 'A1'. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1559
https://github.com/scverse/scanpy/issues/1559:1606,modifiability,Version,Versions,1606,"Hello All, I am trying to learn and create single cell RNA seq pipeline for my project. When I was doing quality control, I met this problem. Can anyone help me? Thank you a lot.; - [ ] I have checked that this issue has not already been reported. - [ ] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. ### Minimal code sample (that we can copy&paste without having any data). ```python. mito_genes = adata.var_names.str.startswith('mt-'). # for each cell compute fraction of counts in mito genes vs. all genes. # the `.A1` is only necessary as X is sparse (to transform to a dense array after summing). adata.obs['percent_mito'] = np.sum(. adata[:, mito_genes].X, axis=1).A1 / np.sum(adata.X, axis=1).A1. # add the total counts per cell as observations-annotation to adata. adata.obs['n_counts'] = adata.X.sum(axis=1).A1. print(sum(mito_genes)). ```. ```pytb. AttributeError Traceback (most recent call last). <ipython-input-18-7f1a7fee3beb> in <module>. 2 # for each cell compute fraction of counts in mito genes vs. all genes. 3 # the `.A1` is only necessary as X is sparse (to transform to a dense array after summing). ----> 4 adata.obs['percent_mito'] = np.sum(. 5 adata[:, mito_genes].X, axis=1).A1 / np.sum(adata.X, axis=1).A1. 6 # add the total counts per cell as observations-annotation to adata. ```. #### Versions. <details>. AttributeError: 'ArrayView' object has no attribute 'A1'. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1559
https://github.com/scverse/scanpy/issues/1559:1209,safety,input,input-,1209,"Hello All, I am trying to learn and create single cell RNA seq pipeline for my project. When I was doing quality control, I met this problem. Can anyone help me? Thank you a lot.; - [ ] I have checked that this issue has not already been reported. - [ ] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. ### Minimal code sample (that we can copy&paste without having any data). ```python. mito_genes = adata.var_names.str.startswith('mt-'). # for each cell compute fraction of counts in mito genes vs. all genes. # the `.A1` is only necessary as X is sparse (to transform to a dense array after summing). adata.obs['percent_mito'] = np.sum(. adata[:, mito_genes].X, axis=1).A1 / np.sum(adata.X, axis=1).A1. # add the total counts per cell as observations-annotation to adata. adata.obs['n_counts'] = adata.X.sum(axis=1).A1. print(sum(mito_genes)). ```. ```pytb. AttributeError Traceback (most recent call last). <ipython-input-18-7f1a7fee3beb> in <module>. 2 # for each cell compute fraction of counts in mito genes vs. all genes. 3 # the `.A1` is only necessary as X is sparse (to transform to a dense array after summing). ----> 4 adata.obs['percent_mito'] = np.sum(. 5 adata[:, mito_genes].X, axis=1).A1 / np.sum(adata.X, axis=1).A1. 6 # add the total counts per cell as observations-annotation to adata. ```. #### Versions. <details>. AttributeError: 'ArrayView' object has no attribute 'A1'. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1559
https://github.com/scverse/scanpy/issues/1559:1236,safety,modul,module,1236,"Hello All, I am trying to learn and create single cell RNA seq pipeline for my project. When I was doing quality control, I met this problem. Can anyone help me? Thank you a lot.; - [ ] I have checked that this issue has not already been reported. - [ ] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. ### Minimal code sample (that we can copy&paste without having any data). ```python. mito_genes = adata.var_names.str.startswith('mt-'). # for each cell compute fraction of counts in mito genes vs. all genes. # the `.A1` is only necessary as X is sparse (to transform to a dense array after summing). adata.obs['percent_mito'] = np.sum(. adata[:, mito_genes].X, axis=1).A1 / np.sum(adata.X, axis=1).A1. # add the total counts per cell as observations-annotation to adata. adata.obs['n_counts'] = adata.X.sum(axis=1).A1. print(sum(mito_genes)). ```. ```pytb. AttributeError Traceback (most recent call last). <ipython-input-18-7f1a7fee3beb> in <module>. 2 # for each cell compute fraction of counts in mito genes vs. all genes. 3 # the `.A1` is only necessary as X is sparse (to transform to a dense array after summing). ----> 4 adata.obs['percent_mito'] = np.sum(. 5 adata[:, mito_genes].X, axis=1).A1 / np.sum(adata.X, axis=1).A1. 6 # add the total counts per cell as observations-annotation to adata. ```. #### Versions. <details>. AttributeError: 'ArrayView' object has no attribute 'A1'. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1559
https://github.com/scverse/scanpy/issues/1559:113,security,control,control,113,"Hello All, I am trying to learn and create single cell RNA seq pipeline for my project. When I was doing quality control, I met this problem. Can anyone help me? Thank you a lot.; - [ ] I have checked that this issue has not already been reported. - [ ] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. ### Minimal code sample (that we can copy&paste without having any data). ```python. mito_genes = adata.var_names.str.startswith('mt-'). # for each cell compute fraction of counts in mito genes vs. all genes. # the `.A1` is only necessary as X is sparse (to transform to a dense array after summing). adata.obs['percent_mito'] = np.sum(. adata[:, mito_genes].X, axis=1).A1 / np.sum(adata.X, axis=1).A1. # add the total counts per cell as observations-annotation to adata. adata.obs['n_counts'] = adata.X.sum(axis=1).A1. print(sum(mito_genes)). ```. ```pytb. AttributeError Traceback (most recent call last). <ipython-input-18-7f1a7fee3beb> in <module>. 2 # for each cell compute fraction of counts in mito genes vs. all genes. 3 # the `.A1` is only necessary as X is sparse (to transform to a dense array after summing). ----> 4 adata.obs['percent_mito'] = np.sum(. 5 adata[:, mito_genes].X, axis=1).A1 / np.sum(adata.X, axis=1).A1. 6 # add the total counts per cell as observations-annotation to adata. ```. #### Versions. <details>. AttributeError: 'ArrayView' object has no attribute 'A1'. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1559
https://github.com/scverse/scanpy/issues/1559:113,testability,control,control,113,"Hello All, I am trying to learn and create single cell RNA seq pipeline for my project. When I was doing quality control, I met this problem. Can anyone help me? Thank you a lot.; - [ ] I have checked that this issue has not already been reported. - [ ] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. ### Minimal code sample (that we can copy&paste without having any data). ```python. mito_genes = adata.var_names.str.startswith('mt-'). # for each cell compute fraction of counts in mito genes vs. all genes. # the `.A1` is only necessary as X is sparse (to transform to a dense array after summing). adata.obs['percent_mito'] = np.sum(. adata[:, mito_genes].X, axis=1).A1 / np.sum(adata.X, axis=1).A1. # add the total counts per cell as observations-annotation to adata. adata.obs['n_counts'] = adata.X.sum(axis=1).A1. print(sum(mito_genes)). ```. ```pytb. AttributeError Traceback (most recent call last). <ipython-input-18-7f1a7fee3beb> in <module>. 2 # for each cell compute fraction of counts in mito genes vs. all genes. 3 # the `.A1` is only necessary as X is sparse (to transform to a dense array after summing). ----> 4 adata.obs['percent_mito'] = np.sum(. 5 adata[:, mito_genes].X, axis=1).A1 / np.sum(adata.X, axis=1).A1. 6 # add the total counts per cell as observations-annotation to adata. ```. #### Versions. <details>. AttributeError: 'ArrayView' object has no attribute 'A1'. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1559
https://github.com/scverse/scanpy/issues/1559:1030,testability,observ,observations-annotation,1030,"Hello All, I am trying to learn and create single cell RNA seq pipeline for my project. When I was doing quality control, I met this problem. Can anyone help me? Thank you a lot.; - [ ] I have checked that this issue has not already been reported. - [ ] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. ### Minimal code sample (that we can copy&paste without having any data). ```python. mito_genes = adata.var_names.str.startswith('mt-'). # for each cell compute fraction of counts in mito genes vs. all genes. # the `.A1` is only necessary as X is sparse (to transform to a dense array after summing). adata.obs['percent_mito'] = np.sum(. adata[:, mito_genes].X, axis=1).A1 / np.sum(adata.X, axis=1).A1. # add the total counts per cell as observations-annotation to adata. adata.obs['n_counts'] = adata.X.sum(axis=1).A1. print(sum(mito_genes)). ```. ```pytb. AttributeError Traceback (most recent call last). <ipython-input-18-7f1a7fee3beb> in <module>. 2 # for each cell compute fraction of counts in mito genes vs. all genes. 3 # the `.A1` is only necessary as X is sparse (to transform to a dense array after summing). ----> 4 adata.obs['percent_mito'] = np.sum(. 5 adata[:, mito_genes].X, axis=1).A1 / np.sum(adata.X, axis=1).A1. 6 # add the total counts per cell as observations-annotation to adata. ```. #### Versions. <details>. AttributeError: 'ArrayView' object has no attribute 'A1'. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1559
https://github.com/scverse/scanpy/issues/1559:1165,testability,Trace,Traceback,1165,"Hello All, I am trying to learn and create single cell RNA seq pipeline for my project. When I was doing quality control, I met this problem. Can anyone help me? Thank you a lot.; - [ ] I have checked that this issue has not already been reported. - [ ] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. ### Minimal code sample (that we can copy&paste without having any data). ```python. mito_genes = adata.var_names.str.startswith('mt-'). # for each cell compute fraction of counts in mito genes vs. all genes. # the `.A1` is only necessary as X is sparse (to transform to a dense array after summing). adata.obs['percent_mito'] = np.sum(. adata[:, mito_genes].X, axis=1).A1 / np.sum(adata.X, axis=1).A1. # add the total counts per cell as observations-annotation to adata. adata.obs['n_counts'] = adata.X.sum(axis=1).A1. print(sum(mito_genes)). ```. ```pytb. AttributeError Traceback (most recent call last). <ipython-input-18-7f1a7fee3beb> in <module>. 2 # for each cell compute fraction of counts in mito genes vs. all genes. 3 # the `.A1` is only necessary as X is sparse (to transform to a dense array after summing). ----> 4 adata.obs['percent_mito'] = np.sum(. 5 adata[:, mito_genes].X, axis=1).A1 / np.sum(adata.X, axis=1).A1. 6 # add the total counts per cell as observations-annotation to adata. ```. #### Versions. <details>. AttributeError: 'ArrayView' object has no attribute 'A1'. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1559
https://github.com/scverse/scanpy/issues/1559:1562,testability,observ,observations-annotation,1562,"Hello All, I am trying to learn and create single cell RNA seq pipeline for my project. When I was doing quality control, I met this problem. Can anyone help me? Thank you a lot.; - [ ] I have checked that this issue has not already been reported. - [ ] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. ### Minimal code sample (that we can copy&paste without having any data). ```python. mito_genes = adata.var_names.str.startswith('mt-'). # for each cell compute fraction of counts in mito genes vs. all genes. # the `.A1` is only necessary as X is sparse (to transform to a dense array after summing). adata.obs['percent_mito'] = np.sum(. adata[:, mito_genes].X, axis=1).A1 / np.sum(adata.X, axis=1).A1. # add the total counts per cell as observations-annotation to adata. adata.obs['n_counts'] = adata.X.sum(axis=1).A1. print(sum(mito_genes)). ```. ```pytb. AttributeError Traceback (most recent call last). <ipython-input-18-7f1a7fee3beb> in <module>. 2 # for each cell compute fraction of counts in mito genes vs. all genes. 3 # the `.A1` is only necessary as X is sparse (to transform to a dense array after summing). ----> 4 adata.obs['percent_mito'] = np.sum(. 5 adata[:, mito_genes].X, axis=1).A1 / np.sum(adata.X, axis=1).A1. 6 # add the total counts per cell as observations-annotation to adata. ```. #### Versions. <details>. AttributeError: 'ArrayView' object has no attribute 'A1'. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1559
https://github.com/scverse/scanpy/issues/1559:26,usability,learn,learn,26,"Hello All, I am trying to learn and create single cell RNA seq pipeline for my project. When I was doing quality control, I met this problem. Can anyone help me? Thank you a lot.; - [ ] I have checked that this issue has not already been reported. - [ ] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. ### Minimal code sample (that we can copy&paste without having any data). ```python. mito_genes = adata.var_names.str.startswith('mt-'). # for each cell compute fraction of counts in mito genes vs. all genes. # the `.A1` is only necessary as X is sparse (to transform to a dense array after summing). adata.obs['percent_mito'] = np.sum(. adata[:, mito_genes].X, axis=1).A1 / np.sum(adata.X, axis=1).A1. # add the total counts per cell as observations-annotation to adata. adata.obs['n_counts'] = adata.X.sum(axis=1).A1. print(sum(mito_genes)). ```. ```pytb. AttributeError Traceback (most recent call last). <ipython-input-18-7f1a7fee3beb> in <module>. 2 # for each cell compute fraction of counts in mito genes vs. all genes. 3 # the `.A1` is only necessary as X is sparse (to transform to a dense array after summing). ----> 4 adata.obs['percent_mito'] = np.sum(. 5 adata[:, mito_genes].X, axis=1).A1 / np.sum(adata.X, axis=1).A1. 6 # add the total counts per cell as observations-annotation to adata. ```. #### Versions. <details>. AttributeError: 'ArrayView' object has no attribute 'A1'. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1559
https://github.com/scverse/scanpy/issues/1559:153,usability,help,help,153,"Hello All, I am trying to learn and create single cell RNA seq pipeline for my project. When I was doing quality control, I met this problem. Can anyone help me? Thank you a lot.; - [ ] I have checked that this issue has not already been reported. - [ ] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. ### Minimal code sample (that we can copy&paste without having any data). ```python. mito_genes = adata.var_names.str.startswith('mt-'). # for each cell compute fraction of counts in mito genes vs. all genes. # the `.A1` is only necessary as X is sparse (to transform to a dense array after summing). adata.obs['percent_mito'] = np.sum(. adata[:, mito_genes].X, axis=1).A1 / np.sum(adata.X, axis=1).A1. # add the total counts per cell as observations-annotation to adata. adata.obs['n_counts'] = adata.X.sum(axis=1).A1. print(sum(mito_genes)). ```. ```pytb. AttributeError Traceback (most recent call last). <ipython-input-18-7f1a7fee3beb> in <module>. 2 # for each cell compute fraction of counts in mito genes vs. all genes. 3 # the `.A1` is only necessary as X is sparse (to transform to a dense array after summing). ----> 4 adata.obs['percent_mito'] = np.sum(. 5 adata[:, mito_genes].X, axis=1).A1 / np.sum(adata.X, axis=1).A1. 6 # add the total counts per cell as observations-annotation to adata. ```. #### Versions. <details>. AttributeError: 'ArrayView' object has no attribute 'A1'. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1559
https://github.com/scverse/scanpy/issues/1559:261,usability,confirm,confirmed,261,"Hello All, I am trying to learn and create single cell RNA seq pipeline for my project. When I was doing quality control, I met this problem. Can anyone help me? Thank you a lot.; - [ ] I have checked that this issue has not already been reported. - [ ] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. ### Minimal code sample (that we can copy&paste without having any data). ```python. mito_genes = adata.var_names.str.startswith('mt-'). # for each cell compute fraction of counts in mito genes vs. all genes. # the `.A1` is only necessary as X is sparse (to transform to a dense array after summing). adata.obs['percent_mito'] = np.sum(. adata[:, mito_genes].X, axis=1).A1 / np.sum(adata.X, axis=1).A1. # add the total counts per cell as observations-annotation to adata. adata.obs['n_counts'] = adata.X.sum(axis=1).A1. print(sum(mito_genes)). ```. ```pytb. AttributeError Traceback (most recent call last). <ipython-input-18-7f1a7fee3beb> in <module>. 2 # for each cell compute fraction of counts in mito genes vs. all genes. 3 # the `.A1` is only necessary as X is sparse (to transform to a dense array after summing). ----> 4 adata.obs['percent_mito'] = np.sum(. 5 adata[:, mito_genes].X, axis=1).A1 / np.sum(adata.X, axis=1).A1. 6 # add the total counts per cell as observations-annotation to adata. ```. #### Versions. <details>. AttributeError: 'ArrayView' object has no attribute 'A1'. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1559
https://github.com/scverse/scanpy/issues/1559:344,usability,confirm,confirmed,344,"Hello All, I am trying to learn and create single cell RNA seq pipeline for my project. When I was doing quality control, I met this problem. Can anyone help me? Thank you a lot.; - [ ] I have checked that this issue has not already been reported. - [ ] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. ### Minimal code sample (that we can copy&paste without having any data). ```python. mito_genes = adata.var_names.str.startswith('mt-'). # for each cell compute fraction of counts in mito genes vs. all genes. # the `.A1` is only necessary as X is sparse (to transform to a dense array after summing). adata.obs['percent_mito'] = np.sum(. adata[:, mito_genes].X, axis=1).A1 / np.sum(adata.X, axis=1).A1. # add the total counts per cell as observations-annotation to adata. adata.obs['n_counts'] = adata.X.sum(axis=1).A1. print(sum(mito_genes)). ```. ```pytb. AttributeError Traceback (most recent call last). <ipython-input-18-7f1a7fee3beb> in <module>. 2 # for each cell compute fraction of counts in mito genes vs. all genes. 3 # the `.A1` is only necessary as X is sparse (to transform to a dense array after summing). ----> 4 adata.obs['percent_mito'] = np.sum(. 5 adata[:, mito_genes].X, axis=1).A1 / np.sum(adata.X, axis=1).A1. 6 # add the total counts per cell as observations-annotation to adata. ```. #### Versions. <details>. AttributeError: 'ArrayView' object has no attribute 'A1'. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1559
https://github.com/scverse/scanpy/issues/1559:435,usability,guid,guide,435,"Hello All, I am trying to learn and create single cell RNA seq pipeline for my project. When I was doing quality control, I met this problem. Can anyone help me? Thank you a lot.; - [ ] I have checked that this issue has not already been reported. - [ ] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. ### Minimal code sample (that we can copy&paste without having any data). ```python. mito_genes = adata.var_names.str.startswith('mt-'). # for each cell compute fraction of counts in mito genes vs. all genes. # the `.A1` is only necessary as X is sparse (to transform to a dense array after summing). adata.obs['percent_mito'] = np.sum(. adata[:, mito_genes].X, axis=1).A1 / np.sum(adata.X, axis=1).A1. # add the total counts per cell as observations-annotation to adata. adata.obs['n_counts'] = adata.X.sum(axis=1).A1. print(sum(mito_genes)). ```. ```pytb. AttributeError Traceback (most recent call last). <ipython-input-18-7f1a7fee3beb> in <module>. 2 # for each cell compute fraction of counts in mito genes vs. all genes. 3 # the `.A1` is only necessary as X is sparse (to transform to a dense array after summing). ----> 4 adata.obs['percent_mito'] = np.sum(. 5 adata[:, mito_genes].X, axis=1).A1 / np.sum(adata.X, axis=1).A1. 6 # add the total counts per cell as observations-annotation to adata. ```. #### Versions. <details>. AttributeError: 'ArrayView' object has no attribute 'A1'. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1559
https://github.com/scverse/scanpy/issues/1559:490,usability,minim,minimal-bug-reports,490,"Hello All, I am trying to learn and create single cell RNA seq pipeline for my project. When I was doing quality control, I met this problem. Can anyone help me? Thank you a lot.; - [ ] I have checked that this issue has not already been reported. - [ ] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. ### Minimal code sample (that we can copy&paste without having any data). ```python. mito_genes = adata.var_names.str.startswith('mt-'). # for each cell compute fraction of counts in mito genes vs. all genes. # the `.A1` is only necessary as X is sparse (to transform to a dense array after summing). adata.obs['percent_mito'] = np.sum(. adata[:, mito_genes].X, axis=1).A1 / np.sum(adata.X, axis=1).A1. # add the total counts per cell as observations-annotation to adata. adata.obs['n_counts'] = adata.X.sum(axis=1).A1. print(sum(mito_genes)). ```. ```pytb. AttributeError Traceback (most recent call last). <ipython-input-18-7f1a7fee3beb> in <module>. 2 # for each cell compute fraction of counts in mito genes vs. all genes. 3 # the `.A1` is only necessary as X is sparse (to transform to a dense array after summing). ----> 4 adata.obs['percent_mito'] = np.sum(. 5 adata[:, mito_genes].X, axis=1).A1 / np.sum(adata.X, axis=1).A1. 6 # add the total counts per cell as observations-annotation to adata. ```. #### Versions. <details>. AttributeError: 'ArrayView' object has no attribute 'A1'. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1559
https://github.com/scverse/scanpy/issues/1559:596,usability,Minim,Minimal,596,"Hello All, I am trying to learn and create single cell RNA seq pipeline for my project. When I was doing quality control, I met this problem. Can anyone help me? Thank you a lot.; - [ ] I have checked that this issue has not already been reported. - [ ] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. ### Minimal code sample (that we can copy&paste without having any data). ```python. mito_genes = adata.var_names.str.startswith('mt-'). # for each cell compute fraction of counts in mito genes vs. all genes. # the `.A1` is only necessary as X is sparse (to transform to a dense array after summing). adata.obs['percent_mito'] = np.sum(. adata[:, mito_genes].X, axis=1).A1 / np.sum(adata.X, axis=1).A1. # add the total counts per cell as observations-annotation to adata. adata.obs['n_counts'] = adata.X.sum(axis=1).A1. print(sum(mito_genes)). ```. ```pytb. AttributeError Traceback (most recent call last). <ipython-input-18-7f1a7fee3beb> in <module>. 2 # for each cell compute fraction of counts in mito genes vs. all genes. 3 # the `.A1` is only necessary as X is sparse (to transform to a dense array after summing). ----> 4 adata.obs['percent_mito'] = np.sum(. 5 adata[:, mito_genes].X, axis=1).A1 / np.sum(adata.X, axis=1).A1. 6 # add the total counts per cell as observations-annotation to adata. ```. #### Versions. <details>. AttributeError: 'ArrayView' object has no attribute 'A1'. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1559
https://github.com/scverse/scanpy/issues/1559:1209,usability,input,input-,1209,"Hello All, I am trying to learn and create single cell RNA seq pipeline for my project. When I was doing quality control, I met this problem. Can anyone help me? Thank you a lot.; - [ ] I have checked that this issue has not already been reported. - [ ] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. ### Minimal code sample (that we can copy&paste without having any data). ```python. mito_genes = adata.var_names.str.startswith('mt-'). # for each cell compute fraction of counts in mito genes vs. all genes. # the `.A1` is only necessary as X is sparse (to transform to a dense array after summing). adata.obs['percent_mito'] = np.sum(. adata[:, mito_genes].X, axis=1).A1 / np.sum(adata.X, axis=1).A1. # add the total counts per cell as observations-annotation to adata. adata.obs['n_counts'] = adata.X.sum(axis=1).A1. print(sum(mito_genes)). ```. ```pytb. AttributeError Traceback (most recent call last). <ipython-input-18-7f1a7fee3beb> in <module>. 2 # for each cell compute fraction of counts in mito genes vs. all genes. 3 # the `.A1` is only necessary as X is sparse (to transform to a dense array after summing). ----> 4 adata.obs['percent_mito'] = np.sum(. 5 adata[:, mito_genes].X, axis=1).A1 / np.sum(adata.X, axis=1).A1. 6 # add the total counts per cell as observations-annotation to adata. ```. #### Versions. <details>. AttributeError: 'ArrayView' object has no attribute 'A1'. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1559
https://github.com/scverse/scanpy/issues/1560:154,availability,error,error,154,"Hello, I am trying to calculate highly variable genes from my data sets using the above code from the scanpy script published on github. I am facing this error. could someone please help me with this? Thank you very much.; - [ ] I have checked that this issue has not already been reported. - [ ] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. ### Minimal code sample (that we can copy&paste without having any data). ```python. sc.pp.highly_variable_genes(adata, flavor='cell_ranger', n_top_genes=4000). print('\n','Number of highly variable genes: {:d}'.format(np.sum(adata.var['highly_variable']))). ```. ```pytb. ValueError Traceback (most recent call last). <ipython-input-46-616fc10e63ff> in <module>. ----> 1 sc.pp.highly_variable_genes(adata, flavor='cell_ranger', n_top_genes=4000). 2 print('\n','Number of highly variable genes: {:d}'.format(np.sum(adata.var['highly_variable']))). ~\anaconda3\lib\site-packages\scanpy\preprocessing\_highly_variable_genes.py in highly_variable_genes(adata, layer, n_top_genes, min_disp, max_disp, min_mean, max_mean, span, n_bins, flavor, subset, inplace, batch_key). 424 . 425 if batch_key is None:. --> 426 df = _highly_variable_genes_single_batch(. 427 adata,. 428 layer=layer,. ~\anaconda3\lib\site-packages\scanpy\preprocessing\_highly_variable_genes.py in _highly_variable_genes_single_batch(adata, layer, min_disp, max_disp, min_mean, max_mean, n_top_genes, n_bins, flavor). 242 from statsmodels import robust. 243 . --> 244 df['mean_bin'] = pd.cut(. 245 df['means'],. 246 np.r_[-np.inf, np.percentile(df['means'], np.arange(10, 105, 5)), np.inf],. ~\anaconda3\lib\site-packages\pandas\core\reshape\tile.py in cut(x, bins, right, labels, retbins, precision, ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1560
https://github.com/scverse/scanpy/issues/1560:1745,availability,robust,robust,1745,"adata, flavor='cell_ranger', n_top_genes=4000). print('\n','Number of highly variable genes: {:d}'.format(np.sum(adata.var['highly_variable']))). ```. ```pytb. ValueError Traceback (most recent call last). <ipython-input-46-616fc10e63ff> in <module>. ----> 1 sc.pp.highly_variable_genes(adata, flavor='cell_ranger', n_top_genes=4000). 2 print('\n','Number of highly variable genes: {:d}'.format(np.sum(adata.var['highly_variable']))). ~\anaconda3\lib\site-packages\scanpy\preprocessing\_highly_variable_genes.py in highly_variable_genes(adata, layer, n_top_genes, min_disp, max_disp, min_mean, max_mean, span, n_bins, flavor, subset, inplace, batch_key). 424 . 425 if batch_key is None:. --> 426 df = _highly_variable_genes_single_batch(. 427 adata,. 428 layer=layer,. ~\anaconda3\lib\site-packages\scanpy\preprocessing\_highly_variable_genes.py in _highly_variable_genes_single_batch(adata, layer, min_disp, max_disp, min_mean, max_mean, n_top_genes, n_bins, flavor). 242 from statsmodels import robust. 243 . --> 244 df['mean_bin'] = pd.cut(. 245 df['means'],. 246 np.r_[-np.inf, np.percentile(df['means'], np.arange(10, 105, 5)), np.inf],. ~\anaconda3\lib\site-packages\pandas\core\reshape\tile.py in cut(x, bins, right, labels, retbins, precision, include_lowest, duplicates, ordered). 273 raise ValueError(""bins must increase monotonically.""). 274 . --> 275 fac, bins = _bins_to_cuts(. 276 x,. 277 bins,. ~\anaconda3\lib\site-packages\pandas\core\reshape\tile.py in _bins_to_cuts(x, bins, right, labels, precision, include_lowest, dtype, duplicates, ordered). 399 if len(unique_bins) < len(bins) and len(bins) != 2:. 400 if duplicates == ""raise"":. --> 401 raise ValueError(. 402 f""Bin edges must be unique: {repr(bins)}.\n"". 403 f""You can drop duplicate edges by setting the 'duplicates' kwarg"". ValueError: Bin edges must be unique: array([ -inf, 1.00000000e-12, 1.00000000e-12, 1.00000000e-12,. 1.00000000e-12, 1.00000000e-12, 1.00000000e-12, 1.00000000e-12,. 1.00000000e-12, 1.11241045e-04, 2",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1560
https://github.com/scverse/scanpy/issues/1560:344,deployability,version,version,344,"Hello, I am trying to calculate highly variable genes from my data sets using the above code from the scanpy script published on github. I am facing this error. could someone please help me with this? Thank you very much.; - [ ] I have checked that this issue has not already been reported. - [ ] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. ### Minimal code sample (that we can copy&paste without having any data). ```python. sc.pp.highly_variable_genes(adata, flavor='cell_ranger', n_top_genes=4000). print('\n','Number of highly variable genes: {:d}'.format(np.sum(adata.var['highly_variable']))). ```. ```pytb. ValueError Traceback (most recent call last). <ipython-input-46-616fc10e63ff> in <module>. ----> 1 sc.pp.highly_variable_genes(adata, flavor='cell_ranger', n_top_genes=4000). 2 print('\n','Number of highly variable genes: {:d}'.format(np.sum(adata.var['highly_variable']))). ~\anaconda3\lib\site-packages\scanpy\preprocessing\_highly_variable_genes.py in highly_variable_genes(adata, layer, n_top_genes, min_disp, max_disp, min_mean, max_mean, span, n_bins, flavor, subset, inplace, batch_key). 424 . 425 if batch_key is None:. --> 426 df = _highly_variable_genes_single_batch(. 427 adata,. 428 layer=layer,. ~\anaconda3\lib\site-packages\scanpy\preprocessing\_highly_variable_genes.py in _highly_variable_genes_single_batch(adata, layer, min_disp, max_disp, min_mean, max_mean, n_top_genes, n_bins, flavor). 242 from statsmodels import robust. 243 . --> 244 df['mean_bin'] = pd.cut(. 245 df['means'],. 246 np.r_[-np.inf, np.percentile(df['means'], np.arange(10, 105, 5)), np.inf],. ~\anaconda3\lib\site-packages\pandas\core\reshape\tile.py in cut(x, bins, right, labels, retbins, precision, ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1560
https://github.com/scverse/scanpy/issues/1560:990,deployability,modul,module,990,"Hello, I am trying to calculate highly variable genes from my data sets using the above code from the scanpy script published on github. I am facing this error. could someone please help me with this? Thank you very much.; - [ ] I have checked that this issue has not already been reported. - [ ] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. ### Minimal code sample (that we can copy&paste without having any data). ```python. sc.pp.highly_variable_genes(adata, flavor='cell_ranger', n_top_genes=4000). print('\n','Number of highly variable genes: {:d}'.format(np.sum(adata.var['highly_variable']))). ```. ```pytb. ValueError Traceback (most recent call last). <ipython-input-46-616fc10e63ff> in <module>. ----> 1 sc.pp.highly_variable_genes(adata, flavor='cell_ranger', n_top_genes=4000). 2 print('\n','Number of highly variable genes: {:d}'.format(np.sum(adata.var['highly_variable']))). ~\anaconda3\lib\site-packages\scanpy\preprocessing\_highly_variable_genes.py in highly_variable_genes(adata, layer, n_top_genes, min_disp, max_disp, min_mean, max_mean, span, n_bins, flavor, subset, inplace, batch_key). 424 . 425 if batch_key is None:. --> 426 df = _highly_variable_genes_single_batch(. 427 adata,. 428 layer=layer,. ~\anaconda3\lib\site-packages\scanpy\preprocessing\_highly_variable_genes.py in _highly_variable_genes_single_batch(adata, layer, min_disp, max_disp, min_mean, max_mean, n_top_genes, n_bins, flavor). 242 from statsmodels import robust. 243 . --> 244 df['mean_bin'] = pd.cut(. 245 df['means'],. 246 np.r_[-np.inf, np.percentile(df['means'], np.arange(10, 105, 5)), np.inf],. ~\anaconda3\lib\site-packages\pandas\core\reshape\tile.py in cut(x, bins, right, labels, retbins, precision, ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1560
https://github.com/scverse/scanpy/issues/1560:2991,deployability,Version,Versions,2991,"e genes: {:d}'.format(np.sum(adata.var['highly_variable']))). ~\anaconda3\lib\site-packages\scanpy\preprocessing\_highly_variable_genes.py in highly_variable_genes(adata, layer, n_top_genes, min_disp, max_disp, min_mean, max_mean, span, n_bins, flavor, subset, inplace, batch_key). 424 . 425 if batch_key is None:. --> 426 df = _highly_variable_genes_single_batch(. 427 adata,. 428 layer=layer,. ~\anaconda3\lib\site-packages\scanpy\preprocessing\_highly_variable_genes.py in _highly_variable_genes_single_batch(adata, layer, min_disp, max_disp, min_mean, max_mean, n_top_genes, n_bins, flavor). 242 from statsmodels import robust. 243 . --> 244 df['mean_bin'] = pd.cut(. 245 df['means'],. 246 np.r_[-np.inf, np.percentile(df['means'], np.arange(10, 105, 5)), np.inf],. ~\anaconda3\lib\site-packages\pandas\core\reshape\tile.py in cut(x, bins, right, labels, retbins, precision, include_lowest, duplicates, ordered). 273 raise ValueError(""bins must increase monotonically.""). 274 . --> 275 fac, bins = _bins_to_cuts(. 276 x,. 277 bins,. ~\anaconda3\lib\site-packages\pandas\core\reshape\tile.py in _bins_to_cuts(x, bins, right, labels, precision, include_lowest, dtype, duplicates, ordered). 399 if len(unique_bins) < len(bins) and len(bins) != 2:. 400 if duplicates == ""raise"":. --> 401 raise ValueError(. 402 f""Bin edges must be unique: {repr(bins)}.\n"". 403 f""You can drop duplicate edges by setting the 'duplicates' kwarg"". ValueError: Bin edges must be unique: array([ -inf, 1.00000000e-12, 1.00000000e-12, 1.00000000e-12,. 1.00000000e-12, 1.00000000e-12, 1.00000000e-12, 1.00000000e-12,. 1.00000000e-12, 1.11241045e-04, 2.44672419e-04, 5.69339462e-04,. 1.35840576e-03, 3.49938189e-03, 1.02259867e-02, 2.97723795e-02,. 7.22420720e-02, 1.46845240e-01, 2.97005969e-01, 3.42389128e+00,. inf]). You can drop duplicate edges by setting the 'duplicates' kwarg. ```. #### Versions. <details>. [Paste the output of scanpy.logging.print_versions() leaving a blank line after the details tag]. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1560
https://github.com/scverse/scanpy/issues/1560:3040,deployability,log,logging,3040,"e genes: {:d}'.format(np.sum(adata.var['highly_variable']))). ~\anaconda3\lib\site-packages\scanpy\preprocessing\_highly_variable_genes.py in highly_variable_genes(adata, layer, n_top_genes, min_disp, max_disp, min_mean, max_mean, span, n_bins, flavor, subset, inplace, batch_key). 424 . 425 if batch_key is None:. --> 426 df = _highly_variable_genes_single_batch(. 427 adata,. 428 layer=layer,. ~\anaconda3\lib\site-packages\scanpy\preprocessing\_highly_variable_genes.py in _highly_variable_genes_single_batch(adata, layer, min_disp, max_disp, min_mean, max_mean, n_top_genes, n_bins, flavor). 242 from statsmodels import robust. 243 . --> 244 df['mean_bin'] = pd.cut(. 245 df['means'],. 246 np.r_[-np.inf, np.percentile(df['means'], np.arange(10, 105, 5)), np.inf],. ~\anaconda3\lib\site-packages\pandas\core\reshape\tile.py in cut(x, bins, right, labels, retbins, precision, include_lowest, duplicates, ordered). 273 raise ValueError(""bins must increase monotonically.""). 274 . --> 275 fac, bins = _bins_to_cuts(. 276 x,. 277 bins,. ~\anaconda3\lib\site-packages\pandas\core\reshape\tile.py in _bins_to_cuts(x, bins, right, labels, precision, include_lowest, dtype, duplicates, ordered). 399 if len(unique_bins) < len(bins) and len(bins) != 2:. 400 if duplicates == ""raise"":. --> 401 raise ValueError(. 402 f""Bin edges must be unique: {repr(bins)}.\n"". 403 f""You can drop duplicate edges by setting the 'duplicates' kwarg"". ValueError: Bin edges must be unique: array([ -inf, 1.00000000e-12, 1.00000000e-12, 1.00000000e-12,. 1.00000000e-12, 1.00000000e-12, 1.00000000e-12, 1.00000000e-12,. 1.00000000e-12, 1.11241045e-04, 2.44672419e-04, 5.69339462e-04,. 1.35840576e-03, 3.49938189e-03, 1.02259867e-02, 2.97723795e-02,. 7.22420720e-02, 1.46845240e-01, 2.97005969e-01, 3.42389128e+00,. inf]). You can drop duplicate edges by setting the 'duplicates' kwarg. ```. #### Versions. <details>. [Paste the output of scanpy.logging.print_versions() leaving a blank line after the details tag]. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1560
https://github.com/scverse/scanpy/issues/1560:1928,energy efficiency,core,core,1928,"most recent call last). <ipython-input-46-616fc10e63ff> in <module>. ----> 1 sc.pp.highly_variable_genes(adata, flavor='cell_ranger', n_top_genes=4000). 2 print('\n','Number of highly variable genes: {:d}'.format(np.sum(adata.var['highly_variable']))). ~\anaconda3\lib\site-packages\scanpy\preprocessing\_highly_variable_genes.py in highly_variable_genes(adata, layer, n_top_genes, min_disp, max_disp, min_mean, max_mean, span, n_bins, flavor, subset, inplace, batch_key). 424 . 425 if batch_key is None:. --> 426 df = _highly_variable_genes_single_batch(. 427 adata,. 428 layer=layer,. ~\anaconda3\lib\site-packages\scanpy\preprocessing\_highly_variable_genes.py in _highly_variable_genes_single_batch(adata, layer, min_disp, max_disp, min_mean, max_mean, n_top_genes, n_bins, flavor). 242 from statsmodels import robust. 243 . --> 244 df['mean_bin'] = pd.cut(. 245 df['means'],. 246 np.r_[-np.inf, np.percentile(df['means'], np.arange(10, 105, 5)), np.inf],. ~\anaconda3\lib\site-packages\pandas\core\reshape\tile.py in cut(x, bins, right, labels, retbins, precision, include_lowest, duplicates, ordered). 273 raise ValueError(""bins must increase monotonically.""). 274 . --> 275 fac, bins = _bins_to_cuts(. 276 x,. 277 bins,. ~\anaconda3\lib\site-packages\pandas\core\reshape\tile.py in _bins_to_cuts(x, bins, right, labels, precision, include_lowest, dtype, duplicates, ordered). 399 if len(unique_bins) < len(bins) and len(bins) != 2:. 400 if duplicates == ""raise"":. --> 401 raise ValueError(. 402 f""Bin edges must be unique: {repr(bins)}.\n"". 403 f""You can drop duplicate edges by setting the 'duplicates' kwarg"". ValueError: Bin edges must be unique: array([ -inf, 1.00000000e-12, 1.00000000e-12, 1.00000000e-12,. 1.00000000e-12, 1.00000000e-12, 1.00000000e-12, 1.00000000e-12,. 1.00000000e-12, 1.11241045e-04, 2.44672419e-04, 5.69339462e-04,. 1.35840576e-03, 3.49938189e-03, 1.02259867e-02, 2.97723795e-02,. 7.22420720e-02, 1.46845240e-01, 2.97005969e-01, 3.42389128e+00,. inf]). You can drop ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1560
https://github.com/scverse/scanpy/issues/1560:2195,energy efficiency,core,core,2195,"e genes: {:d}'.format(np.sum(adata.var['highly_variable']))). ~\anaconda3\lib\site-packages\scanpy\preprocessing\_highly_variable_genes.py in highly_variable_genes(adata, layer, n_top_genes, min_disp, max_disp, min_mean, max_mean, span, n_bins, flavor, subset, inplace, batch_key). 424 . 425 if batch_key is None:. --> 426 df = _highly_variable_genes_single_batch(. 427 adata,. 428 layer=layer,. ~\anaconda3\lib\site-packages\scanpy\preprocessing\_highly_variable_genes.py in _highly_variable_genes_single_batch(adata, layer, min_disp, max_disp, min_mean, max_mean, n_top_genes, n_bins, flavor). 242 from statsmodels import robust. 243 . --> 244 df['mean_bin'] = pd.cut(. 245 df['means'],. 246 np.r_[-np.inf, np.percentile(df['means'], np.arange(10, 105, 5)), np.inf],. ~\anaconda3\lib\site-packages\pandas\core\reshape\tile.py in cut(x, bins, right, labels, retbins, precision, include_lowest, duplicates, ordered). 273 raise ValueError(""bins must increase monotonically.""). 274 . --> 275 fac, bins = _bins_to_cuts(. 276 x,. 277 bins,. ~\anaconda3\lib\site-packages\pandas\core\reshape\tile.py in _bins_to_cuts(x, bins, right, labels, precision, include_lowest, dtype, duplicates, ordered). 399 if len(unique_bins) < len(bins) and len(bins) != 2:. 400 if duplicates == ""raise"":. --> 401 raise ValueError(. 402 f""Bin edges must be unique: {repr(bins)}.\n"". 403 f""You can drop duplicate edges by setting the 'duplicates' kwarg"". ValueError: Bin edges must be unique: array([ -inf, 1.00000000e-12, 1.00000000e-12, 1.00000000e-12,. 1.00000000e-12, 1.00000000e-12, 1.00000000e-12, 1.00000000e-12,. 1.00000000e-12, 1.11241045e-04, 2.44672419e-04, 5.69339462e-04,. 1.35840576e-03, 3.49938189e-03, 1.02259867e-02, 2.97723795e-02,. 7.22420720e-02, 1.46845240e-01, 2.97005969e-01, 3.42389128e+00,. inf]). You can drop duplicate edges by setting the 'duplicates' kwarg. ```. #### Versions. <details>. [Paste the output of scanpy.logging.print_versions() leaving a blank line after the details tag]. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1560
https://github.com/scverse/scanpy/issues/1560:116,integrability,pub,published,116,"Hello, I am trying to calculate highly variable genes from my data sets using the above code from the scanpy script published on github. I am facing this error. could someone please help me with this? Thank you very much.; - [ ] I have checked that this issue has not already been reported. - [ ] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. ### Minimal code sample (that we can copy&paste without having any data). ```python. sc.pp.highly_variable_genes(adata, flavor='cell_ranger', n_top_genes=4000). print('\n','Number of highly variable genes: {:d}'.format(np.sum(adata.var['highly_variable']))). ```. ```pytb. ValueError Traceback (most recent call last). <ipython-input-46-616fc10e63ff> in <module>. ----> 1 sc.pp.highly_variable_genes(adata, flavor='cell_ranger', n_top_genes=4000). 2 print('\n','Number of highly variable genes: {:d}'.format(np.sum(adata.var['highly_variable']))). ~\anaconda3\lib\site-packages\scanpy\preprocessing\_highly_variable_genes.py in highly_variable_genes(adata, layer, n_top_genes, min_disp, max_disp, min_mean, max_mean, span, n_bins, flavor, subset, inplace, batch_key). 424 . 425 if batch_key is None:. --> 426 df = _highly_variable_genes_single_batch(. 427 adata,. 428 layer=layer,. ~\anaconda3\lib\site-packages\scanpy\preprocessing\_highly_variable_genes.py in _highly_variable_genes_single_batch(adata, layer, min_disp, max_disp, min_mean, max_mean, n_top_genes, n_bins, flavor). 242 from statsmodels import robust. 243 . --> 244 df['mean_bin'] = pd.cut(. 245 df['means'],. 246 np.r_[-np.inf, np.percentile(df['means'], np.arange(10, 105, 5)), np.inf],. ~\anaconda3\lib\site-packages\pandas\core\reshape\tile.py in cut(x, bins, right, labels, retbins, precision, ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1560
https://github.com/scverse/scanpy/issues/1560:344,integrability,version,version,344,"Hello, I am trying to calculate highly variable genes from my data sets using the above code from the scanpy script published on github. I am facing this error. could someone please help me with this? Thank you very much.; - [ ] I have checked that this issue has not already been reported. - [ ] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. ### Minimal code sample (that we can copy&paste without having any data). ```python. sc.pp.highly_variable_genes(adata, flavor='cell_ranger', n_top_genes=4000). print('\n','Number of highly variable genes: {:d}'.format(np.sum(adata.var['highly_variable']))). ```. ```pytb. ValueError Traceback (most recent call last). <ipython-input-46-616fc10e63ff> in <module>. ----> 1 sc.pp.highly_variable_genes(adata, flavor='cell_ranger', n_top_genes=4000). 2 print('\n','Number of highly variable genes: {:d}'.format(np.sum(adata.var['highly_variable']))). ~\anaconda3\lib\site-packages\scanpy\preprocessing\_highly_variable_genes.py in highly_variable_genes(adata, layer, n_top_genes, min_disp, max_disp, min_mean, max_mean, span, n_bins, flavor, subset, inplace, batch_key). 424 . 425 if batch_key is None:. --> 426 df = _highly_variable_genes_single_batch(. 427 adata,. 428 layer=layer,. ~\anaconda3\lib\site-packages\scanpy\preprocessing\_highly_variable_genes.py in _highly_variable_genes_single_batch(adata, layer, min_disp, max_disp, min_mean, max_mean, n_top_genes, n_bins, flavor). 242 from statsmodels import robust. 243 . --> 244 df['mean_bin'] = pd.cut(. 245 df['means'],. 246 np.r_[-np.inf, np.percentile(df['means'], np.arange(10, 105, 5)), np.inf],. ~\anaconda3\lib\site-packages\pandas\core\reshape\tile.py in cut(x, bins, right, labels, retbins, precision, ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1560
https://github.com/scverse/scanpy/issues/1560:1374,integrability,sub,subset,1374,"l) I have confirmed this bug exists on the master branch of scanpy. ---. **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. ### Minimal code sample (that we can copy&paste without having any data). ```python. sc.pp.highly_variable_genes(adata, flavor='cell_ranger', n_top_genes=4000). print('\n','Number of highly variable genes: {:d}'.format(np.sum(adata.var['highly_variable']))). ```. ```pytb. ValueError Traceback (most recent call last). <ipython-input-46-616fc10e63ff> in <module>. ----> 1 sc.pp.highly_variable_genes(adata, flavor='cell_ranger', n_top_genes=4000). 2 print('\n','Number of highly variable genes: {:d}'.format(np.sum(adata.var['highly_variable']))). ~\anaconda3\lib\site-packages\scanpy\preprocessing\_highly_variable_genes.py in highly_variable_genes(adata, layer, n_top_genes, min_disp, max_disp, min_mean, max_mean, span, n_bins, flavor, subset, inplace, batch_key). 424 . 425 if batch_key is None:. --> 426 df = _highly_variable_genes_single_batch(. 427 adata,. 428 layer=layer,. ~\anaconda3\lib\site-packages\scanpy\preprocessing\_highly_variable_genes.py in _highly_variable_genes_single_batch(adata, layer, min_disp, max_disp, min_mean, max_mean, n_top_genes, n_bins, flavor). 242 from statsmodels import robust. 243 . --> 244 df['mean_bin'] = pd.cut(. 245 df['means'],. 246 np.r_[-np.inf, np.percentile(df['means'], np.arange(10, 105, 5)), np.inf],. ~\anaconda3\lib\site-packages\pandas\core\reshape\tile.py in cut(x, bins, right, labels, retbins, precision, include_lowest, duplicates, ordered). 273 raise ValueError(""bins must increase monotonically.""). 274 . --> 275 fac, bins = _bins_to_cuts(. 276 x,. 277 bins,. ~\anaconda3\lib\site-packages\pandas\core\reshape\tile.py in _bins_to_cuts(x, bins, right, labels, precision, include_lowest, dtype, duplicates, ordered). 399 if len(unique_bins) < len(bins) and len(bins) != 2:. 400 if ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1560
https://github.com/scverse/scanpy/issues/1560:2991,integrability,Version,Versions,2991,"e genes: {:d}'.format(np.sum(adata.var['highly_variable']))). ~\anaconda3\lib\site-packages\scanpy\preprocessing\_highly_variable_genes.py in highly_variable_genes(adata, layer, n_top_genes, min_disp, max_disp, min_mean, max_mean, span, n_bins, flavor, subset, inplace, batch_key). 424 . 425 if batch_key is None:. --> 426 df = _highly_variable_genes_single_batch(. 427 adata,. 428 layer=layer,. ~\anaconda3\lib\site-packages\scanpy\preprocessing\_highly_variable_genes.py in _highly_variable_genes_single_batch(adata, layer, min_disp, max_disp, min_mean, max_mean, n_top_genes, n_bins, flavor). 242 from statsmodels import robust. 243 . --> 244 df['mean_bin'] = pd.cut(. 245 df['means'],. 246 np.r_[-np.inf, np.percentile(df['means'], np.arange(10, 105, 5)), np.inf],. ~\anaconda3\lib\site-packages\pandas\core\reshape\tile.py in cut(x, bins, right, labels, retbins, precision, include_lowest, duplicates, ordered). 273 raise ValueError(""bins must increase monotonically.""). 274 . --> 275 fac, bins = _bins_to_cuts(. 276 x,. 277 bins,. ~\anaconda3\lib\site-packages\pandas\core\reshape\tile.py in _bins_to_cuts(x, bins, right, labels, precision, include_lowest, dtype, duplicates, ordered). 399 if len(unique_bins) < len(bins) and len(bins) != 2:. 400 if duplicates == ""raise"":. --> 401 raise ValueError(. 402 f""Bin edges must be unique: {repr(bins)}.\n"". 403 f""You can drop duplicate edges by setting the 'duplicates' kwarg"". ValueError: Bin edges must be unique: array([ -inf, 1.00000000e-12, 1.00000000e-12, 1.00000000e-12,. 1.00000000e-12, 1.00000000e-12, 1.00000000e-12, 1.00000000e-12,. 1.00000000e-12, 1.11241045e-04, 2.44672419e-04, 5.69339462e-04,. 1.35840576e-03, 3.49938189e-03, 1.02259867e-02, 2.97723795e-02,. 7.22420720e-02, 1.46845240e-01, 2.97005969e-01, 3.42389128e+00,. inf]). You can drop duplicate edges by setting the 'duplicates' kwarg. ```. #### Versions. <details>. [Paste the output of scanpy.logging.print_versions() leaving a blank line after the details tag]. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1560
https://github.com/scverse/scanpy/issues/1560:847,interoperability,format,format,847,"Hello, I am trying to calculate highly variable genes from my data sets using the above code from the scanpy script published on github. I am facing this error. could someone please help me with this? Thank you very much.; - [ ] I have checked that this issue has not already been reported. - [ ] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. ### Minimal code sample (that we can copy&paste without having any data). ```python. sc.pp.highly_variable_genes(adata, flavor='cell_ranger', n_top_genes=4000). print('\n','Number of highly variable genes: {:d}'.format(np.sum(adata.var['highly_variable']))). ```. ```pytb. ValueError Traceback (most recent call last). <ipython-input-46-616fc10e63ff> in <module>. ----> 1 sc.pp.highly_variable_genes(adata, flavor='cell_ranger', n_top_genes=4000). 2 print('\n','Number of highly variable genes: {:d}'.format(np.sum(adata.var['highly_variable']))). ~\anaconda3\lib\site-packages\scanpy\preprocessing\_highly_variable_genes.py in highly_variable_genes(adata, layer, n_top_genes, min_disp, max_disp, min_mean, max_mean, span, n_bins, flavor, subset, inplace, batch_key). 424 . 425 if batch_key is None:. --> 426 df = _highly_variable_genes_single_batch(. 427 adata,. 428 layer=layer,. ~\anaconda3\lib\site-packages\scanpy\preprocessing\_highly_variable_genes.py in _highly_variable_genes_single_batch(adata, layer, min_disp, max_disp, min_mean, max_mean, n_top_genes, n_bins, flavor). 242 from statsmodels import robust. 243 . --> 244 df['mean_bin'] = pd.cut(. 245 df['means'],. 246 np.r_[-np.inf, np.percentile(df['means'], np.arange(10, 105, 5)), np.inf],. ~\anaconda3\lib\site-packages\pandas\core\reshape\tile.py in cut(x, bins, right, labels, retbins, precision, ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1560
https://github.com/scverse/scanpy/issues/1560:1136,interoperability,format,format,1136,"am facing this error. could someone please help me with this? Thank you very much.; - [ ] I have checked that this issue has not already been reported. - [ ] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. ### Minimal code sample (that we can copy&paste without having any data). ```python. sc.pp.highly_variable_genes(adata, flavor='cell_ranger', n_top_genes=4000). print('\n','Number of highly variable genes: {:d}'.format(np.sum(adata.var['highly_variable']))). ```. ```pytb. ValueError Traceback (most recent call last). <ipython-input-46-616fc10e63ff> in <module>. ----> 1 sc.pp.highly_variable_genes(adata, flavor='cell_ranger', n_top_genes=4000). 2 print('\n','Number of highly variable genes: {:d}'.format(np.sum(adata.var['highly_variable']))). ~\anaconda3\lib\site-packages\scanpy\preprocessing\_highly_variable_genes.py in highly_variable_genes(adata, layer, n_top_genes, min_disp, max_disp, min_mean, max_mean, span, n_bins, flavor, subset, inplace, batch_key). 424 . 425 if batch_key is None:. --> 426 df = _highly_variable_genes_single_batch(. 427 adata,. 428 layer=layer,. ~\anaconda3\lib\site-packages\scanpy\preprocessing\_highly_variable_genes.py in _highly_variable_genes_single_batch(adata, layer, min_disp, max_disp, min_mean, max_mean, n_top_genes, n_bins, flavor). 242 from statsmodels import robust. 243 . --> 244 df['mean_bin'] = pd.cut(. 245 df['means'],. 246 np.r_[-np.inf, np.percentile(df['means'], np.arange(10, 105, 5)), np.inf],. ~\anaconda3\lib\site-packages\pandas\core\reshape\tile.py in cut(x, bins, right, labels, retbins, precision, include_lowest, duplicates, ordered). 273 raise ValueError(""bins must increase monotonically.""). 274 . --> 275 fac, bins = _bins_to_cuts(. ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1560
https://github.com/scverse/scanpy/issues/1560:39,modifiability,variab,variable,39,"Hello, I am trying to calculate highly variable genes from my data sets using the above code from the scanpy script published on github. I am facing this error. could someone please help me with this? Thank you very much.; - [ ] I have checked that this issue has not already been reported. - [ ] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. ### Minimal code sample (that we can copy&paste without having any data). ```python. sc.pp.highly_variable_genes(adata, flavor='cell_ranger', n_top_genes=4000). print('\n','Number of highly variable genes: {:d}'.format(np.sum(adata.var['highly_variable']))). ```. ```pytb. ValueError Traceback (most recent call last). <ipython-input-46-616fc10e63ff> in <module>. ----> 1 sc.pp.highly_variable_genes(adata, flavor='cell_ranger', n_top_genes=4000). 2 print('\n','Number of highly variable genes: {:d}'.format(np.sum(adata.var['highly_variable']))). ~\anaconda3\lib\site-packages\scanpy\preprocessing\_highly_variable_genes.py in highly_variable_genes(adata, layer, n_top_genes, min_disp, max_disp, min_mean, max_mean, span, n_bins, flavor, subset, inplace, batch_key). 424 . 425 if batch_key is None:. --> 426 df = _highly_variable_genes_single_batch(. 427 adata,. 428 layer=layer,. ~\anaconda3\lib\site-packages\scanpy\preprocessing\_highly_variable_genes.py in _highly_variable_genes_single_batch(adata, layer, min_disp, max_disp, min_mean, max_mean, n_top_genes, n_bins, flavor). 242 from statsmodels import robust. 243 . --> 244 df['mean_bin'] = pd.cut(. 245 df['means'],. 246 np.r_[-np.inf, np.percentile(df['means'], np.arange(10, 105, 5)), np.inf],. ~\anaconda3\lib\site-packages\pandas\core\reshape\tile.py in cut(x, bins, right, labels, retbins, precision, ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1560
https://github.com/scverse/scanpy/issues/1560:344,modifiability,version,version,344,"Hello, I am trying to calculate highly variable genes from my data sets using the above code from the scanpy script published on github. I am facing this error. could someone please help me with this? Thank you very much.; - [ ] I have checked that this issue has not already been reported. - [ ] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. ### Minimal code sample (that we can copy&paste without having any data). ```python. sc.pp.highly_variable_genes(adata, flavor='cell_ranger', n_top_genes=4000). print('\n','Number of highly variable genes: {:d}'.format(np.sum(adata.var['highly_variable']))). ```. ```pytb. ValueError Traceback (most recent call last). <ipython-input-46-616fc10e63ff> in <module>. ----> 1 sc.pp.highly_variable_genes(adata, flavor='cell_ranger', n_top_genes=4000). 2 print('\n','Number of highly variable genes: {:d}'.format(np.sum(adata.var['highly_variable']))). ~\anaconda3\lib\site-packages\scanpy\preprocessing\_highly_variable_genes.py in highly_variable_genes(adata, layer, n_top_genes, min_disp, max_disp, min_mean, max_mean, span, n_bins, flavor, subset, inplace, batch_key). 424 . 425 if batch_key is None:. --> 426 df = _highly_variable_genes_single_batch(. 427 adata,. 428 layer=layer,. ~\anaconda3\lib\site-packages\scanpy\preprocessing\_highly_variable_genes.py in _highly_variable_genes_single_batch(adata, layer, min_disp, max_disp, min_mean, max_mean, n_top_genes, n_bins, flavor). 242 from statsmodels import robust. 243 . --> 244 df['mean_bin'] = pd.cut(. 245 df['means'],. 246 np.r_[-np.inf, np.percentile(df['means'], np.arange(10, 105, 5)), np.inf],. ~\anaconda3\lib\site-packages\pandas\core\reshape\tile.py in cut(x, bins, right, labels, retbins, precision, ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1560
https://github.com/scverse/scanpy/issues/1560:825,modifiability,variab,variable,825,"Hello, I am trying to calculate highly variable genes from my data sets using the above code from the scanpy script published on github. I am facing this error. could someone please help me with this? Thank you very much.; - [ ] I have checked that this issue has not already been reported. - [ ] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. ### Minimal code sample (that we can copy&paste without having any data). ```python. sc.pp.highly_variable_genes(adata, flavor='cell_ranger', n_top_genes=4000). print('\n','Number of highly variable genes: {:d}'.format(np.sum(adata.var['highly_variable']))). ```. ```pytb. ValueError Traceback (most recent call last). <ipython-input-46-616fc10e63ff> in <module>. ----> 1 sc.pp.highly_variable_genes(adata, flavor='cell_ranger', n_top_genes=4000). 2 print('\n','Number of highly variable genes: {:d}'.format(np.sum(adata.var['highly_variable']))). ~\anaconda3\lib\site-packages\scanpy\preprocessing\_highly_variable_genes.py in highly_variable_genes(adata, layer, n_top_genes, min_disp, max_disp, min_mean, max_mean, span, n_bins, flavor, subset, inplace, batch_key). 424 . 425 if batch_key is None:. --> 426 df = _highly_variable_genes_single_batch(. 427 adata,. 428 layer=layer,. ~\anaconda3\lib\site-packages\scanpy\preprocessing\_highly_variable_genes.py in _highly_variable_genes_single_batch(adata, layer, min_disp, max_disp, min_mean, max_mean, n_top_genes, n_bins, flavor). 242 from statsmodels import robust. 243 . --> 244 df['mean_bin'] = pd.cut(. 245 df['means'],. 246 np.r_[-np.inf, np.percentile(df['means'], np.arange(10, 105, 5)), np.inf],. ~\anaconda3\lib\site-packages\pandas\core\reshape\tile.py in cut(x, bins, right, labels, retbins, precision, ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1560
https://github.com/scverse/scanpy/issues/1560:990,modifiability,modul,module,990,"Hello, I am trying to calculate highly variable genes from my data sets using the above code from the scanpy script published on github. I am facing this error. could someone please help me with this? Thank you very much.; - [ ] I have checked that this issue has not already been reported. - [ ] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. ### Minimal code sample (that we can copy&paste without having any data). ```python. sc.pp.highly_variable_genes(adata, flavor='cell_ranger', n_top_genes=4000). print('\n','Number of highly variable genes: {:d}'.format(np.sum(adata.var['highly_variable']))). ```. ```pytb. ValueError Traceback (most recent call last). <ipython-input-46-616fc10e63ff> in <module>. ----> 1 sc.pp.highly_variable_genes(adata, flavor='cell_ranger', n_top_genes=4000). 2 print('\n','Number of highly variable genes: {:d}'.format(np.sum(adata.var['highly_variable']))). ~\anaconda3\lib\site-packages\scanpy\preprocessing\_highly_variable_genes.py in highly_variable_genes(adata, layer, n_top_genes, min_disp, max_disp, min_mean, max_mean, span, n_bins, flavor, subset, inplace, batch_key). 424 . 425 if batch_key is None:. --> 426 df = _highly_variable_genes_single_batch(. 427 adata,. 428 layer=layer,. ~\anaconda3\lib\site-packages\scanpy\preprocessing\_highly_variable_genes.py in _highly_variable_genes_single_batch(adata, layer, min_disp, max_disp, min_mean, max_mean, n_top_genes, n_bins, flavor). 242 from statsmodels import robust. 243 . --> 244 df['mean_bin'] = pd.cut(. 245 df['means'],. 246 np.r_[-np.inf, np.percentile(df['means'], np.arange(10, 105, 5)), np.inf],. ~\anaconda3\lib\site-packages\pandas\core\reshape\tile.py in cut(x, bins, right, labels, retbins, precision, ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1560
https://github.com/scverse/scanpy/issues/1560:1114,modifiability,variab,variable,1114,"blished on github. I am facing this error. could someone please help me with this? Thank you very much.; - [ ] I have checked that this issue has not already been reported. - [ ] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. ### Minimal code sample (that we can copy&paste without having any data). ```python. sc.pp.highly_variable_genes(adata, flavor='cell_ranger', n_top_genes=4000). print('\n','Number of highly variable genes: {:d}'.format(np.sum(adata.var['highly_variable']))). ```. ```pytb. ValueError Traceback (most recent call last). <ipython-input-46-616fc10e63ff> in <module>. ----> 1 sc.pp.highly_variable_genes(adata, flavor='cell_ranger', n_top_genes=4000). 2 print('\n','Number of highly variable genes: {:d}'.format(np.sum(adata.var['highly_variable']))). ~\anaconda3\lib\site-packages\scanpy\preprocessing\_highly_variable_genes.py in highly_variable_genes(adata, layer, n_top_genes, min_disp, max_disp, min_mean, max_mean, span, n_bins, flavor, subset, inplace, batch_key). 424 . 425 if batch_key is None:. --> 426 df = _highly_variable_genes_single_batch(. 427 adata,. 428 layer=layer,. ~\anaconda3\lib\site-packages\scanpy\preprocessing\_highly_variable_genes.py in _highly_variable_genes_single_batch(adata, layer, min_disp, max_disp, min_mean, max_mean, n_top_genes, n_bins, flavor). 242 from statsmodels import robust. 243 . --> 244 df['mean_bin'] = pd.cut(. 245 df['means'],. 246 np.r_[-np.inf, np.percentile(df['means'], np.arange(10, 105, 5)), np.inf],. ~\anaconda3\lib\site-packages\pandas\core\reshape\tile.py in cut(x, bins, right, labels, retbins, precision, include_lowest, duplicates, ordered). 273 raise ValueError(""bins must increase monotonically.""). 274 . --> 275 fac, bi",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1560
https://github.com/scverse/scanpy/issues/1560:1204,modifiability,pac,packages,1204,"ou very much.; - [ ] I have checked that this issue has not already been reported. - [ ] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. ### Minimal code sample (that we can copy&paste without having any data). ```python. sc.pp.highly_variable_genes(adata, flavor='cell_ranger', n_top_genes=4000). print('\n','Number of highly variable genes: {:d}'.format(np.sum(adata.var['highly_variable']))). ```. ```pytb. ValueError Traceback (most recent call last). <ipython-input-46-616fc10e63ff> in <module>. ----> 1 sc.pp.highly_variable_genes(adata, flavor='cell_ranger', n_top_genes=4000). 2 print('\n','Number of highly variable genes: {:d}'.format(np.sum(adata.var['highly_variable']))). ~\anaconda3\lib\site-packages\scanpy\preprocessing\_highly_variable_genes.py in highly_variable_genes(adata, layer, n_top_genes, min_disp, max_disp, min_mean, max_mean, span, n_bins, flavor, subset, inplace, batch_key). 424 . 425 if batch_key is None:. --> 426 df = _highly_variable_genes_single_batch(. 427 adata,. 428 layer=layer,. ~\anaconda3\lib\site-packages\scanpy\preprocessing\_highly_variable_genes.py in _highly_variable_genes_single_batch(adata, layer, min_disp, max_disp, min_mean, max_mean, n_top_genes, n_bins, flavor). 242 from statsmodels import robust. 243 . --> 244 df['mean_bin'] = pd.cut(. 245 df['means'],. 246 np.r_[-np.inf, np.percentile(df['means'], np.arange(10, 105, 5)), np.inf],. ~\anaconda3\lib\site-packages\pandas\core\reshape\tile.py in cut(x, bins, right, labels, retbins, precision, include_lowest, duplicates, ordered). 273 raise ValueError(""bins must increase monotonically.""). 274 . --> 275 fac, bins = _bins_to_cuts(. 276 x,. 277 bins,. ~\anaconda3\lib\site-packages\pandas\core\reshape\",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1560
https://github.com/scverse/scanpy/issues/1560:1292,modifiability,layer,layer,1292,"] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. ### Minimal code sample (that we can copy&paste without having any data). ```python. sc.pp.highly_variable_genes(adata, flavor='cell_ranger', n_top_genes=4000). print('\n','Number of highly variable genes: {:d}'.format(np.sum(adata.var['highly_variable']))). ```. ```pytb. ValueError Traceback (most recent call last). <ipython-input-46-616fc10e63ff> in <module>. ----> 1 sc.pp.highly_variable_genes(adata, flavor='cell_ranger', n_top_genes=4000). 2 print('\n','Number of highly variable genes: {:d}'.format(np.sum(adata.var['highly_variable']))). ~\anaconda3\lib\site-packages\scanpy\preprocessing\_highly_variable_genes.py in highly_variable_genes(adata, layer, n_top_genes, min_disp, max_disp, min_mean, max_mean, span, n_bins, flavor, subset, inplace, batch_key). 424 . 425 if batch_key is None:. --> 426 df = _highly_variable_genes_single_batch(. 427 adata,. 428 layer=layer,. ~\anaconda3\lib\site-packages\scanpy\preprocessing\_highly_variable_genes.py in _highly_variable_genes_single_batch(adata, layer, min_disp, max_disp, min_mean, max_mean, n_top_genes, n_bins, flavor). 242 from statsmodels import robust. 243 . --> 244 df['mean_bin'] = pd.cut(. 245 df['means'],. 246 np.r_[-np.inf, np.percentile(df['means'], np.arange(10, 105, 5)), np.inf],. ~\anaconda3\lib\site-packages\pandas\core\reshape\tile.py in cut(x, bins, right, labels, retbins, precision, include_lowest, duplicates, ordered). 273 raise ValueError(""bins must increase monotonically.""). 274 . --> 275 fac, bins = _bins_to_cuts(. 276 x,. 277 bins,. ~\anaconda3\lib\site-packages\pandas\core\reshape\tile.py in _bins_to_cuts(x, bins, right, labels, precision, include_lowest, dtype, dupl",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1560
https://github.com/scverse/scanpy/issues/1560:1503,modifiability,layer,layer,1503,"n.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. ### Minimal code sample (that we can copy&paste without having any data). ```python. sc.pp.highly_variable_genes(adata, flavor='cell_ranger', n_top_genes=4000). print('\n','Number of highly variable genes: {:d}'.format(np.sum(adata.var['highly_variable']))). ```. ```pytb. ValueError Traceback (most recent call last). <ipython-input-46-616fc10e63ff> in <module>. ----> 1 sc.pp.highly_variable_genes(adata, flavor='cell_ranger', n_top_genes=4000). 2 print('\n','Number of highly variable genes: {:d}'.format(np.sum(adata.var['highly_variable']))). ~\anaconda3\lib\site-packages\scanpy\preprocessing\_highly_variable_genes.py in highly_variable_genes(adata, layer, n_top_genes, min_disp, max_disp, min_mean, max_mean, span, n_bins, flavor, subset, inplace, batch_key). 424 . 425 if batch_key is None:. --> 426 df = _highly_variable_genes_single_batch(. 427 adata,. 428 layer=layer,. ~\anaconda3\lib\site-packages\scanpy\preprocessing\_highly_variable_genes.py in _highly_variable_genes_single_batch(adata, layer, min_disp, max_disp, min_mean, max_mean, n_top_genes, n_bins, flavor). 242 from statsmodels import robust. 243 . --> 244 df['mean_bin'] = pd.cut(. 245 df['means'],. 246 np.r_[-np.inf, np.percentile(df['means'], np.arange(10, 105, 5)), np.inf],. ~\anaconda3\lib\site-packages\pandas\core\reshape\tile.py in cut(x, bins, right, labels, retbins, precision, include_lowest, duplicates, ordered). 273 raise ValueError(""bins must increase monotonically.""). 274 . --> 275 fac, bins = _bins_to_cuts(. 276 x,. 277 bins,. ~\anaconda3\lib\site-packages\pandas\core\reshape\tile.py in _bins_to_cuts(x, bins, right, labels, precision, include_lowest, dtype, duplicates, ordered). 399 if len(unique_bins) < len(bins) and len(bins) != 2:. 400 if duplicates == ""raise"":. --> 401 raise ValueError(. 402 f""Bin edges must be unique: {repr(bins)}.\n"". 403 f""You can drop duplicate",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1560
https://github.com/scverse/scanpy/issues/1560:1509,modifiability,layer,layer,1509,"blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. ### Minimal code sample (that we can copy&paste without having any data). ```python. sc.pp.highly_variable_genes(adata, flavor='cell_ranger', n_top_genes=4000). print('\n','Number of highly variable genes: {:d}'.format(np.sum(adata.var['highly_variable']))). ```. ```pytb. ValueError Traceback (most recent call last). <ipython-input-46-616fc10e63ff> in <module>. ----> 1 sc.pp.highly_variable_genes(adata, flavor='cell_ranger', n_top_genes=4000). 2 print('\n','Number of highly variable genes: {:d}'.format(np.sum(adata.var['highly_variable']))). ~\anaconda3\lib\site-packages\scanpy\preprocessing\_highly_variable_genes.py in highly_variable_genes(adata, layer, n_top_genes, min_disp, max_disp, min_mean, max_mean, span, n_bins, flavor, subset, inplace, batch_key). 424 . 425 if batch_key is None:. --> 426 df = _highly_variable_genes_single_batch(. 427 adata,. 428 layer=layer,. ~\anaconda3\lib\site-packages\scanpy\preprocessing\_highly_variable_genes.py in _highly_variable_genes_single_batch(adata, layer, min_disp, max_disp, min_mean, max_mean, n_top_genes, n_bins, flavor). 242 from statsmodels import robust. 243 . --> 244 df['mean_bin'] = pd.cut(. 245 df['means'],. 246 np.r_[-np.inf, np.percentile(df['means'], np.arange(10, 105, 5)), np.inf],. ~\anaconda3\lib\site-packages\pandas\core\reshape\tile.py in cut(x, bins, right, labels, retbins, precision, include_lowest, duplicates, ordered). 273 raise ValueError(""bins must increase monotonically.""). 274 . --> 275 fac, bins = _bins_to_cuts(. 276 x,. 277 bins,. ~\anaconda3\lib\site-packages\pandas\core\reshape\tile.py in _bins_to_cuts(x, bins, right, labels, precision, include_lowest, dtype, duplicates, ordered). 399 if len(unique_bins) < len(bins) and len(bins) != 2:. 400 if duplicates == ""raise"":. --> 401 raise ValueError(. 402 f""Bin edges must be unique: {repr(bins)}.\n"". 403 f""You can drop duplicate edges",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1560
https://github.com/scverse/scanpy/issues/1560:1538,modifiability,pac,packages,1538,"ug-reports) detailing how to provide the necessary information for us to reproduce your bug. ### Minimal code sample (that we can copy&paste without having any data). ```python. sc.pp.highly_variable_genes(adata, flavor='cell_ranger', n_top_genes=4000). print('\n','Number of highly variable genes: {:d}'.format(np.sum(adata.var['highly_variable']))). ```. ```pytb. ValueError Traceback (most recent call last). <ipython-input-46-616fc10e63ff> in <module>. ----> 1 sc.pp.highly_variable_genes(adata, flavor='cell_ranger', n_top_genes=4000). 2 print('\n','Number of highly variable genes: {:d}'.format(np.sum(adata.var['highly_variable']))). ~\anaconda3\lib\site-packages\scanpy\preprocessing\_highly_variable_genes.py in highly_variable_genes(adata, layer, n_top_genes, min_disp, max_disp, min_mean, max_mean, span, n_bins, flavor, subset, inplace, batch_key). 424 . 425 if batch_key is None:. --> 426 df = _highly_variable_genes_single_batch(. 427 adata,. 428 layer=layer,. ~\anaconda3\lib\site-packages\scanpy\preprocessing\_highly_variable_genes.py in _highly_variable_genes_single_batch(adata, layer, min_disp, max_disp, min_mean, max_mean, n_top_genes, n_bins, flavor). 242 from statsmodels import robust. 243 . --> 244 df['mean_bin'] = pd.cut(. 245 df['means'],. 246 np.r_[-np.inf, np.percentile(df['means'], np.arange(10, 105, 5)), np.inf],. ~\anaconda3\lib\site-packages\pandas\core\reshape\tile.py in cut(x, bins, right, labels, retbins, precision, include_lowest, duplicates, ordered). 273 raise ValueError(""bins must increase monotonically.""). 274 . --> 275 fac, bins = _bins_to_cuts(. 276 x,. 277 bins,. ~\anaconda3\lib\site-packages\pandas\core\reshape\tile.py in _bins_to_cuts(x, bins, right, labels, precision, include_lowest, dtype, duplicates, ordered). 399 if len(unique_bins) < len(bins) and len(bins) != 2:. 400 if duplicates == ""raise"":. --> 401 raise ValueError(. 402 f""Bin edges must be unique: {repr(bins)}.\n"". 403 f""You can drop duplicate edges by setting the 'duplicates' k",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1560
https://github.com/scverse/scanpy/issues/1560:1640,modifiability,layer,layer,1640,"mal code sample (that we can copy&paste without having any data). ```python. sc.pp.highly_variable_genes(adata, flavor='cell_ranger', n_top_genes=4000). print('\n','Number of highly variable genes: {:d}'.format(np.sum(adata.var['highly_variable']))). ```. ```pytb. ValueError Traceback (most recent call last). <ipython-input-46-616fc10e63ff> in <module>. ----> 1 sc.pp.highly_variable_genes(adata, flavor='cell_ranger', n_top_genes=4000). 2 print('\n','Number of highly variable genes: {:d}'.format(np.sum(adata.var['highly_variable']))). ~\anaconda3\lib\site-packages\scanpy\preprocessing\_highly_variable_genes.py in highly_variable_genes(adata, layer, n_top_genes, min_disp, max_disp, min_mean, max_mean, span, n_bins, flavor, subset, inplace, batch_key). 424 . 425 if batch_key is None:. --> 426 df = _highly_variable_genes_single_batch(. 427 adata,. 428 layer=layer,. ~\anaconda3\lib\site-packages\scanpy\preprocessing\_highly_variable_genes.py in _highly_variable_genes_single_batch(adata, layer, min_disp, max_disp, min_mean, max_mean, n_top_genes, n_bins, flavor). 242 from statsmodels import robust. 243 . --> 244 df['mean_bin'] = pd.cut(. 245 df['means'],. 246 np.r_[-np.inf, np.percentile(df['means'], np.arange(10, 105, 5)), np.inf],. ~\anaconda3\lib\site-packages\pandas\core\reshape\tile.py in cut(x, bins, right, labels, retbins, precision, include_lowest, duplicates, ordered). 273 raise ValueError(""bins must increase monotonically.""). 274 . --> 275 fac, bins = _bins_to_cuts(. 276 x,. 277 bins,. ~\anaconda3\lib\site-packages\pandas\core\reshape\tile.py in _bins_to_cuts(x, bins, right, labels, precision, include_lowest, dtype, duplicates, ordered). 399 if len(unique_bins) < len(bins) and len(bins) != 2:. 400 if duplicates == ""raise"":. --> 401 raise ValueError(. 402 f""Bin edges must be unique: {repr(bins)}.\n"". 403 f""You can drop duplicate edges by setting the 'duplicates' kwarg"". ValueError: Bin edges must be unique: array([ -inf, 1.00000000e-12, 1.00000000e-12, 1.00000000",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1560
https://github.com/scverse/scanpy/issues/1560:1912,modifiability,pac,packages,1912,"or Traceback (most recent call last). <ipython-input-46-616fc10e63ff> in <module>. ----> 1 sc.pp.highly_variable_genes(adata, flavor='cell_ranger', n_top_genes=4000). 2 print('\n','Number of highly variable genes: {:d}'.format(np.sum(adata.var['highly_variable']))). ~\anaconda3\lib\site-packages\scanpy\preprocessing\_highly_variable_genes.py in highly_variable_genes(adata, layer, n_top_genes, min_disp, max_disp, min_mean, max_mean, span, n_bins, flavor, subset, inplace, batch_key). 424 . 425 if batch_key is None:. --> 426 df = _highly_variable_genes_single_batch(. 427 adata,. 428 layer=layer,. ~\anaconda3\lib\site-packages\scanpy\preprocessing\_highly_variable_genes.py in _highly_variable_genes_single_batch(adata, layer, min_disp, max_disp, min_mean, max_mean, n_top_genes, n_bins, flavor). 242 from statsmodels import robust. 243 . --> 244 df['mean_bin'] = pd.cut(. 245 df['means'],. 246 np.r_[-np.inf, np.percentile(df['means'], np.arange(10, 105, 5)), np.inf],. ~\anaconda3\lib\site-packages\pandas\core\reshape\tile.py in cut(x, bins, right, labels, retbins, precision, include_lowest, duplicates, ordered). 273 raise ValueError(""bins must increase monotonically.""). 274 . --> 275 fac, bins = _bins_to_cuts(. 276 x,. 277 bins,. ~\anaconda3\lib\site-packages\pandas\core\reshape\tile.py in _bins_to_cuts(x, bins, right, labels, precision, include_lowest, dtype, duplicates, ordered). 399 if len(unique_bins) < len(bins) and len(bins) != 2:. 400 if duplicates == ""raise"":. --> 401 raise ValueError(. 402 f""Bin edges must be unique: {repr(bins)}.\n"". 403 f""You can drop duplicate edges by setting the 'duplicates' kwarg"". ValueError: Bin edges must be unique: array([ -inf, 1.00000000e-12, 1.00000000e-12, 1.00000000e-12,. 1.00000000e-12, 1.00000000e-12, 1.00000000e-12, 1.00000000e-12,. 1.00000000e-12, 1.11241045e-04, 2.44672419e-04, 5.69339462e-04,. 1.35840576e-03, 3.49938189e-03, 1.02259867e-02, 2.97723795e-02,. 7.22420720e-02, 1.46845240e-01, 2.97005969e-01, 3.42389128e+00,. inf]).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1560
https://github.com/scverse/scanpy/issues/1560:2179,modifiability,pac,packages,2179,"e genes: {:d}'.format(np.sum(adata.var['highly_variable']))). ~\anaconda3\lib\site-packages\scanpy\preprocessing\_highly_variable_genes.py in highly_variable_genes(adata, layer, n_top_genes, min_disp, max_disp, min_mean, max_mean, span, n_bins, flavor, subset, inplace, batch_key). 424 . 425 if batch_key is None:. --> 426 df = _highly_variable_genes_single_batch(. 427 adata,. 428 layer=layer,. ~\anaconda3\lib\site-packages\scanpy\preprocessing\_highly_variable_genes.py in _highly_variable_genes_single_batch(adata, layer, min_disp, max_disp, min_mean, max_mean, n_top_genes, n_bins, flavor). 242 from statsmodels import robust. 243 . --> 244 df['mean_bin'] = pd.cut(. 245 df['means'],. 246 np.r_[-np.inf, np.percentile(df['means'], np.arange(10, 105, 5)), np.inf],. ~\anaconda3\lib\site-packages\pandas\core\reshape\tile.py in cut(x, bins, right, labels, retbins, precision, include_lowest, duplicates, ordered). 273 raise ValueError(""bins must increase monotonically.""). 274 . --> 275 fac, bins = _bins_to_cuts(. 276 x,. 277 bins,. ~\anaconda3\lib\site-packages\pandas\core\reshape\tile.py in _bins_to_cuts(x, bins, right, labels, precision, include_lowest, dtype, duplicates, ordered). 399 if len(unique_bins) < len(bins) and len(bins) != 2:. 400 if duplicates == ""raise"":. --> 401 raise ValueError(. 402 f""Bin edges must be unique: {repr(bins)}.\n"". 403 f""You can drop duplicate edges by setting the 'duplicates' kwarg"". ValueError: Bin edges must be unique: array([ -inf, 1.00000000e-12, 1.00000000e-12, 1.00000000e-12,. 1.00000000e-12, 1.00000000e-12, 1.00000000e-12, 1.00000000e-12,. 1.00000000e-12, 1.11241045e-04, 2.44672419e-04, 5.69339462e-04,. 1.35840576e-03, 3.49938189e-03, 1.02259867e-02, 2.97723795e-02,. 7.22420720e-02, 1.46845240e-01, 2.97005969e-01, 3.42389128e+00,. inf]). You can drop duplicate edges by setting the 'duplicates' kwarg. ```. #### Versions. <details>. [Paste the output of scanpy.logging.print_versions() leaving a blank line after the details tag]. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1560
https://github.com/scverse/scanpy/issues/1560:2991,modifiability,Version,Versions,2991,"e genes: {:d}'.format(np.sum(adata.var['highly_variable']))). ~\anaconda3\lib\site-packages\scanpy\preprocessing\_highly_variable_genes.py in highly_variable_genes(adata, layer, n_top_genes, min_disp, max_disp, min_mean, max_mean, span, n_bins, flavor, subset, inplace, batch_key). 424 . 425 if batch_key is None:. --> 426 df = _highly_variable_genes_single_batch(. 427 adata,. 428 layer=layer,. ~\anaconda3\lib\site-packages\scanpy\preprocessing\_highly_variable_genes.py in _highly_variable_genes_single_batch(adata, layer, min_disp, max_disp, min_mean, max_mean, n_top_genes, n_bins, flavor). 242 from statsmodels import robust. 243 . --> 244 df['mean_bin'] = pd.cut(. 245 df['means'],. 246 np.r_[-np.inf, np.percentile(df['means'], np.arange(10, 105, 5)), np.inf],. ~\anaconda3\lib\site-packages\pandas\core\reshape\tile.py in cut(x, bins, right, labels, retbins, precision, include_lowest, duplicates, ordered). 273 raise ValueError(""bins must increase monotonically.""). 274 . --> 275 fac, bins = _bins_to_cuts(. 276 x,. 277 bins,. ~\anaconda3\lib\site-packages\pandas\core\reshape\tile.py in _bins_to_cuts(x, bins, right, labels, precision, include_lowest, dtype, duplicates, ordered). 399 if len(unique_bins) < len(bins) and len(bins) != 2:. 400 if duplicates == ""raise"":. --> 401 raise ValueError(. 402 f""Bin edges must be unique: {repr(bins)}.\n"". 403 f""You can drop duplicate edges by setting the 'duplicates' kwarg"". ValueError: Bin edges must be unique: array([ -inf, 1.00000000e-12, 1.00000000e-12, 1.00000000e-12,. 1.00000000e-12, 1.00000000e-12, 1.00000000e-12, 1.00000000e-12,. 1.00000000e-12, 1.11241045e-04, 2.44672419e-04, 5.69339462e-04,. 1.35840576e-03, 3.49938189e-03, 1.02259867e-02, 2.97723795e-02,. 7.22420720e-02, 1.46845240e-01, 2.97005969e-01, 3.42389128e+00,. inf]). You can drop duplicate edges by setting the 'duplicates' kwarg. ```. #### Versions. <details>. [Paste the output of scanpy.logging.print_versions() leaving a blank line after the details tag]. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1560
https://github.com/scverse/scanpy/issues/1560:154,performance,error,error,154,"Hello, I am trying to calculate highly variable genes from my data sets using the above code from the scanpy script published on github. I am facing this error. could someone please help me with this? Thank you very much.; - [ ] I have checked that this issue has not already been reported. - [ ] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. ### Minimal code sample (that we can copy&paste without having any data). ```python. sc.pp.highly_variable_genes(adata, flavor='cell_ranger', n_top_genes=4000). print('\n','Number of highly variable genes: {:d}'.format(np.sum(adata.var['highly_variable']))). ```. ```pytb. ValueError Traceback (most recent call last). <ipython-input-46-616fc10e63ff> in <module>. ----> 1 sc.pp.highly_variable_genes(adata, flavor='cell_ranger', n_top_genes=4000). 2 print('\n','Number of highly variable genes: {:d}'.format(np.sum(adata.var['highly_variable']))). ~\anaconda3\lib\site-packages\scanpy\preprocessing\_highly_variable_genes.py in highly_variable_genes(adata, layer, n_top_genes, min_disp, max_disp, min_mean, max_mean, span, n_bins, flavor, subset, inplace, batch_key). 424 . 425 if batch_key is None:. --> 426 df = _highly_variable_genes_single_batch(. 427 adata,. 428 layer=layer,. ~\anaconda3\lib\site-packages\scanpy\preprocessing\_highly_variable_genes.py in _highly_variable_genes_single_batch(adata, layer, min_disp, max_disp, min_mean, max_mean, n_top_genes, n_bins, flavor). 242 from statsmodels import robust. 243 . --> 244 df['mean_bin'] = pd.cut(. 245 df['means'],. 246 np.r_[-np.inf, np.percentile(df['means'], np.arange(10, 105, 5)), np.inf],. ~\anaconda3\lib\site-packages\pandas\core\reshape\tile.py in cut(x, bins, right, labels, retbins, precision, ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1560
https://github.com/scverse/scanpy/issues/1560:1745,reliability,robust,robust,1745,"adata, flavor='cell_ranger', n_top_genes=4000). print('\n','Number of highly variable genes: {:d}'.format(np.sum(adata.var['highly_variable']))). ```. ```pytb. ValueError Traceback (most recent call last). <ipython-input-46-616fc10e63ff> in <module>. ----> 1 sc.pp.highly_variable_genes(adata, flavor='cell_ranger', n_top_genes=4000). 2 print('\n','Number of highly variable genes: {:d}'.format(np.sum(adata.var['highly_variable']))). ~\anaconda3\lib\site-packages\scanpy\preprocessing\_highly_variable_genes.py in highly_variable_genes(adata, layer, n_top_genes, min_disp, max_disp, min_mean, max_mean, span, n_bins, flavor, subset, inplace, batch_key). 424 . 425 if batch_key is None:. --> 426 df = _highly_variable_genes_single_batch(. 427 adata,. 428 layer=layer,. ~\anaconda3\lib\site-packages\scanpy\preprocessing\_highly_variable_genes.py in _highly_variable_genes_single_batch(adata, layer, min_disp, max_disp, min_mean, max_mean, n_top_genes, n_bins, flavor). 242 from statsmodels import robust. 243 . --> 244 df['mean_bin'] = pd.cut(. 245 df['means'],. 246 np.r_[-np.inf, np.percentile(df['means'], np.arange(10, 105, 5)), np.inf],. ~\anaconda3\lib\site-packages\pandas\core\reshape\tile.py in cut(x, bins, right, labels, retbins, precision, include_lowest, duplicates, ordered). 273 raise ValueError(""bins must increase monotonically.""). 274 . --> 275 fac, bins = _bins_to_cuts(. 276 x,. 277 bins,. ~\anaconda3\lib\site-packages\pandas\core\reshape\tile.py in _bins_to_cuts(x, bins, right, labels, precision, include_lowest, dtype, duplicates, ordered). 399 if len(unique_bins) < len(bins) and len(bins) != 2:. 400 if duplicates == ""raise"":. --> 401 raise ValueError(. 402 f""Bin edges must be unique: {repr(bins)}.\n"". 403 f""You can drop duplicate edges by setting the 'duplicates' kwarg"". ValueError: Bin edges must be unique: array([ -inf, 1.00000000e-12, 1.00000000e-12, 1.00000000e-12,. 1.00000000e-12, 1.00000000e-12, 1.00000000e-12, 1.00000000e-12,. 1.00000000e-12, 1.11241045e-04, 2",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1560
https://github.com/scverse/scanpy/issues/1560:154,safety,error,error,154,"Hello, I am trying to calculate highly variable genes from my data sets using the above code from the scanpy script published on github. I am facing this error. could someone please help me with this? Thank you very much.; - [ ] I have checked that this issue has not already been reported. - [ ] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. ### Minimal code sample (that we can copy&paste without having any data). ```python. sc.pp.highly_variable_genes(adata, flavor='cell_ranger', n_top_genes=4000). print('\n','Number of highly variable genes: {:d}'.format(np.sum(adata.var['highly_variable']))). ```. ```pytb. ValueError Traceback (most recent call last). <ipython-input-46-616fc10e63ff> in <module>. ----> 1 sc.pp.highly_variable_genes(adata, flavor='cell_ranger', n_top_genes=4000). 2 print('\n','Number of highly variable genes: {:d}'.format(np.sum(adata.var['highly_variable']))). ~\anaconda3\lib\site-packages\scanpy\preprocessing\_highly_variable_genes.py in highly_variable_genes(adata, layer, n_top_genes, min_disp, max_disp, min_mean, max_mean, span, n_bins, flavor, subset, inplace, batch_key). 424 . 425 if batch_key is None:. --> 426 df = _highly_variable_genes_single_batch(. 427 adata,. 428 layer=layer,. ~\anaconda3\lib\site-packages\scanpy\preprocessing\_highly_variable_genes.py in _highly_variable_genes_single_batch(adata, layer, min_disp, max_disp, min_mean, max_mean, n_top_genes, n_bins, flavor). 242 from statsmodels import robust. 243 . --> 244 df['mean_bin'] = pd.cut(. 245 df['means'],. 246 np.r_[-np.inf, np.percentile(df['means'], np.arange(10, 105, 5)), np.inf],. ~\anaconda3\lib\site-packages\pandas\core\reshape\tile.py in cut(x, bins, right, labels, retbins, precision, ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1560
https://github.com/scverse/scanpy/issues/1560:963,safety,input,input-,963,"Hello, I am trying to calculate highly variable genes from my data sets using the above code from the scanpy script published on github. I am facing this error. could someone please help me with this? Thank you very much.; - [ ] I have checked that this issue has not already been reported. - [ ] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. ### Minimal code sample (that we can copy&paste without having any data). ```python. sc.pp.highly_variable_genes(adata, flavor='cell_ranger', n_top_genes=4000). print('\n','Number of highly variable genes: {:d}'.format(np.sum(adata.var['highly_variable']))). ```. ```pytb. ValueError Traceback (most recent call last). <ipython-input-46-616fc10e63ff> in <module>. ----> 1 sc.pp.highly_variable_genes(adata, flavor='cell_ranger', n_top_genes=4000). 2 print('\n','Number of highly variable genes: {:d}'.format(np.sum(adata.var['highly_variable']))). ~\anaconda3\lib\site-packages\scanpy\preprocessing\_highly_variable_genes.py in highly_variable_genes(adata, layer, n_top_genes, min_disp, max_disp, min_mean, max_mean, span, n_bins, flavor, subset, inplace, batch_key). 424 . 425 if batch_key is None:. --> 426 df = _highly_variable_genes_single_batch(. 427 adata,. 428 layer=layer,. ~\anaconda3\lib\site-packages\scanpy\preprocessing\_highly_variable_genes.py in _highly_variable_genes_single_batch(adata, layer, min_disp, max_disp, min_mean, max_mean, n_top_genes, n_bins, flavor). 242 from statsmodels import robust. 243 . --> 244 df['mean_bin'] = pd.cut(. 245 df['means'],. 246 np.r_[-np.inf, np.percentile(df['means'], np.arange(10, 105, 5)), np.inf],. ~\anaconda3\lib\site-packages\pandas\core\reshape\tile.py in cut(x, bins, right, labels, retbins, precision, ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1560
https://github.com/scverse/scanpy/issues/1560:990,safety,modul,module,990,"Hello, I am trying to calculate highly variable genes from my data sets using the above code from the scanpy script published on github. I am facing this error. could someone please help me with this? Thank you very much.; - [ ] I have checked that this issue has not already been reported. - [ ] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. ### Minimal code sample (that we can copy&paste without having any data). ```python. sc.pp.highly_variable_genes(adata, flavor='cell_ranger', n_top_genes=4000). print('\n','Number of highly variable genes: {:d}'.format(np.sum(adata.var['highly_variable']))). ```. ```pytb. ValueError Traceback (most recent call last). <ipython-input-46-616fc10e63ff> in <module>. ----> 1 sc.pp.highly_variable_genes(adata, flavor='cell_ranger', n_top_genes=4000). 2 print('\n','Number of highly variable genes: {:d}'.format(np.sum(adata.var['highly_variable']))). ~\anaconda3\lib\site-packages\scanpy\preprocessing\_highly_variable_genes.py in highly_variable_genes(adata, layer, n_top_genes, min_disp, max_disp, min_mean, max_mean, span, n_bins, flavor, subset, inplace, batch_key). 424 . 425 if batch_key is None:. --> 426 df = _highly_variable_genes_single_batch(. 427 adata,. 428 layer=layer,. ~\anaconda3\lib\site-packages\scanpy\preprocessing\_highly_variable_genes.py in _highly_variable_genes_single_batch(adata, layer, min_disp, max_disp, min_mean, max_mean, n_top_genes, n_bins, flavor). 242 from statsmodels import robust. 243 . --> 244 df['mean_bin'] = pd.cut(. 245 df['means'],. 246 np.r_[-np.inf, np.percentile(df['means'], np.arange(10, 105, 5)), np.inf],. ~\anaconda3\lib\site-packages\pandas\core\reshape\tile.py in cut(x, bins, right, labels, retbins, precision, ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1560
https://github.com/scverse/scanpy/issues/1560:1745,safety,robust,robust,1745,"adata, flavor='cell_ranger', n_top_genes=4000). print('\n','Number of highly variable genes: {:d}'.format(np.sum(adata.var['highly_variable']))). ```. ```pytb. ValueError Traceback (most recent call last). <ipython-input-46-616fc10e63ff> in <module>. ----> 1 sc.pp.highly_variable_genes(adata, flavor='cell_ranger', n_top_genes=4000). 2 print('\n','Number of highly variable genes: {:d}'.format(np.sum(adata.var['highly_variable']))). ~\anaconda3\lib\site-packages\scanpy\preprocessing\_highly_variable_genes.py in highly_variable_genes(adata, layer, n_top_genes, min_disp, max_disp, min_mean, max_mean, span, n_bins, flavor, subset, inplace, batch_key). 424 . 425 if batch_key is None:. --> 426 df = _highly_variable_genes_single_batch(. 427 adata,. 428 layer=layer,. ~\anaconda3\lib\site-packages\scanpy\preprocessing\_highly_variable_genes.py in _highly_variable_genes_single_batch(adata, layer, min_disp, max_disp, min_mean, max_mean, n_top_genes, n_bins, flavor). 242 from statsmodels import robust. 243 . --> 244 df['mean_bin'] = pd.cut(. 245 df['means'],. 246 np.r_[-np.inf, np.percentile(df['means'], np.arange(10, 105, 5)), np.inf],. ~\anaconda3\lib\site-packages\pandas\core\reshape\tile.py in cut(x, bins, right, labels, retbins, precision, include_lowest, duplicates, ordered). 273 raise ValueError(""bins must increase monotonically.""). 274 . --> 275 fac, bins = _bins_to_cuts(. 276 x,. 277 bins,. ~\anaconda3\lib\site-packages\pandas\core\reshape\tile.py in _bins_to_cuts(x, bins, right, labels, precision, include_lowest, dtype, duplicates, ordered). 399 if len(unique_bins) < len(bins) and len(bins) != 2:. 400 if duplicates == ""raise"":. --> 401 raise ValueError(. 402 f""Bin edges must be unique: {repr(bins)}.\n"". 403 f""You can drop duplicate edges by setting the 'duplicates' kwarg"". ValueError: Bin edges must be unique: array([ -inf, 1.00000000e-12, 1.00000000e-12, 1.00000000e-12,. 1.00000000e-12, 1.00000000e-12, 1.00000000e-12, 1.00000000e-12,. 1.00000000e-12, 1.11241045e-04, 2",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1560
https://github.com/scverse/scanpy/issues/1560:3040,safety,log,logging,3040,"e genes: {:d}'.format(np.sum(adata.var['highly_variable']))). ~\anaconda3\lib\site-packages\scanpy\preprocessing\_highly_variable_genes.py in highly_variable_genes(adata, layer, n_top_genes, min_disp, max_disp, min_mean, max_mean, span, n_bins, flavor, subset, inplace, batch_key). 424 . 425 if batch_key is None:. --> 426 df = _highly_variable_genes_single_batch(. 427 adata,. 428 layer=layer,. ~\anaconda3\lib\site-packages\scanpy\preprocessing\_highly_variable_genes.py in _highly_variable_genes_single_batch(adata, layer, min_disp, max_disp, min_mean, max_mean, n_top_genes, n_bins, flavor). 242 from statsmodels import robust. 243 . --> 244 df['mean_bin'] = pd.cut(. 245 df['means'],. 246 np.r_[-np.inf, np.percentile(df['means'], np.arange(10, 105, 5)), np.inf],. ~\anaconda3\lib\site-packages\pandas\core\reshape\tile.py in cut(x, bins, right, labels, retbins, precision, include_lowest, duplicates, ordered). 273 raise ValueError(""bins must increase monotonically.""). 274 . --> 275 fac, bins = _bins_to_cuts(. 276 x,. 277 bins,. ~\anaconda3\lib\site-packages\pandas\core\reshape\tile.py in _bins_to_cuts(x, bins, right, labels, precision, include_lowest, dtype, duplicates, ordered). 399 if len(unique_bins) < len(bins) and len(bins) != 2:. 400 if duplicates == ""raise"":. --> 401 raise ValueError(. 402 f""Bin edges must be unique: {repr(bins)}.\n"". 403 f""You can drop duplicate edges by setting the 'duplicates' kwarg"". ValueError: Bin edges must be unique: array([ -inf, 1.00000000e-12, 1.00000000e-12, 1.00000000e-12,. 1.00000000e-12, 1.00000000e-12, 1.00000000e-12, 1.00000000e-12,. 1.00000000e-12, 1.11241045e-04, 2.44672419e-04, 5.69339462e-04,. 1.35840576e-03, 3.49938189e-03, 1.02259867e-02, 2.97723795e-02,. 7.22420720e-02, 1.46845240e-01, 2.97005969e-01, 3.42389128e+00,. inf]). You can drop duplicate edges by setting the 'duplicates' kwarg. ```. #### Versions. <details>. [Paste the output of scanpy.logging.print_versions() leaving a blank line after the details tag]. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1560
https://github.com/scverse/scanpy/issues/1560:3040,security,log,logging,3040,"e genes: {:d}'.format(np.sum(adata.var['highly_variable']))). ~\anaconda3\lib\site-packages\scanpy\preprocessing\_highly_variable_genes.py in highly_variable_genes(adata, layer, n_top_genes, min_disp, max_disp, min_mean, max_mean, span, n_bins, flavor, subset, inplace, batch_key). 424 . 425 if batch_key is None:. --> 426 df = _highly_variable_genes_single_batch(. 427 adata,. 428 layer=layer,. ~\anaconda3\lib\site-packages\scanpy\preprocessing\_highly_variable_genes.py in _highly_variable_genes_single_batch(adata, layer, min_disp, max_disp, min_mean, max_mean, n_top_genes, n_bins, flavor). 242 from statsmodels import robust. 243 . --> 244 df['mean_bin'] = pd.cut(. 245 df['means'],. 246 np.r_[-np.inf, np.percentile(df['means'], np.arange(10, 105, 5)), np.inf],. ~\anaconda3\lib\site-packages\pandas\core\reshape\tile.py in cut(x, bins, right, labels, retbins, precision, include_lowest, duplicates, ordered). 273 raise ValueError(""bins must increase monotonically.""). 274 . --> 275 fac, bins = _bins_to_cuts(. 276 x,. 277 bins,. ~\anaconda3\lib\site-packages\pandas\core\reshape\tile.py in _bins_to_cuts(x, bins, right, labels, precision, include_lowest, dtype, duplicates, ordered). 399 if len(unique_bins) < len(bins) and len(bins) != 2:. 400 if duplicates == ""raise"":. --> 401 raise ValueError(. 402 f""Bin edges must be unique: {repr(bins)}.\n"". 403 f""You can drop duplicate edges by setting the 'duplicates' kwarg"". ValueError: Bin edges must be unique: array([ -inf, 1.00000000e-12, 1.00000000e-12, 1.00000000e-12,. 1.00000000e-12, 1.00000000e-12, 1.00000000e-12, 1.00000000e-12,. 1.00000000e-12, 1.11241045e-04, 2.44672419e-04, 5.69339462e-04,. 1.35840576e-03, 3.49938189e-03, 1.02259867e-02, 2.97723795e-02,. 7.22420720e-02, 1.46845240e-01, 2.97005969e-01, 3.42389128e+00,. inf]). You can drop duplicate edges by setting the 'duplicates' kwarg. ```. #### Versions. <details>. [Paste the output of scanpy.logging.print_versions() leaving a blank line after the details tag]. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1560
https://github.com/scverse/scanpy/issues/1560:919,testability,Trace,Traceback,919,"Hello, I am trying to calculate highly variable genes from my data sets using the above code from the scanpy script published on github. I am facing this error. could someone please help me with this? Thank you very much.; - [ ] I have checked that this issue has not already been reported. - [ ] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. ### Minimal code sample (that we can copy&paste without having any data). ```python. sc.pp.highly_variable_genes(adata, flavor='cell_ranger', n_top_genes=4000). print('\n','Number of highly variable genes: {:d}'.format(np.sum(adata.var['highly_variable']))). ```. ```pytb. ValueError Traceback (most recent call last). <ipython-input-46-616fc10e63ff> in <module>. ----> 1 sc.pp.highly_variable_genes(adata, flavor='cell_ranger', n_top_genes=4000). 2 print('\n','Number of highly variable genes: {:d}'.format(np.sum(adata.var['highly_variable']))). ~\anaconda3\lib\site-packages\scanpy\preprocessing\_highly_variable_genes.py in highly_variable_genes(adata, layer, n_top_genes, min_disp, max_disp, min_mean, max_mean, span, n_bins, flavor, subset, inplace, batch_key). 424 . 425 if batch_key is None:. --> 426 df = _highly_variable_genes_single_batch(. 427 adata,. 428 layer=layer,. ~\anaconda3\lib\site-packages\scanpy\preprocessing\_highly_variable_genes.py in _highly_variable_genes_single_batch(adata, layer, min_disp, max_disp, min_mean, max_mean, n_top_genes, n_bins, flavor). 242 from statsmodels import robust. 243 . --> 244 df['mean_bin'] = pd.cut(. 245 df['means'],. 246 np.r_[-np.inf, np.percentile(df['means'], np.arange(10, 105, 5)), np.inf],. ~\anaconda3\lib\site-packages\pandas\core\reshape\tile.py in cut(x, bins, right, labels, retbins, precision, ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1560
https://github.com/scverse/scanpy/issues/1560:3040,testability,log,logging,3040,"e genes: {:d}'.format(np.sum(adata.var['highly_variable']))). ~\anaconda3\lib\site-packages\scanpy\preprocessing\_highly_variable_genes.py in highly_variable_genes(adata, layer, n_top_genes, min_disp, max_disp, min_mean, max_mean, span, n_bins, flavor, subset, inplace, batch_key). 424 . 425 if batch_key is None:. --> 426 df = _highly_variable_genes_single_batch(. 427 adata,. 428 layer=layer,. ~\anaconda3\lib\site-packages\scanpy\preprocessing\_highly_variable_genes.py in _highly_variable_genes_single_batch(adata, layer, min_disp, max_disp, min_mean, max_mean, n_top_genes, n_bins, flavor). 242 from statsmodels import robust. 243 . --> 244 df['mean_bin'] = pd.cut(. 245 df['means'],. 246 np.r_[-np.inf, np.percentile(df['means'], np.arange(10, 105, 5)), np.inf],. ~\anaconda3\lib\site-packages\pandas\core\reshape\tile.py in cut(x, bins, right, labels, retbins, precision, include_lowest, duplicates, ordered). 273 raise ValueError(""bins must increase monotonically.""). 274 . --> 275 fac, bins = _bins_to_cuts(. 276 x,. 277 bins,. ~\anaconda3\lib\site-packages\pandas\core\reshape\tile.py in _bins_to_cuts(x, bins, right, labels, precision, include_lowest, dtype, duplicates, ordered). 399 if len(unique_bins) < len(bins) and len(bins) != 2:. 400 if duplicates == ""raise"":. --> 401 raise ValueError(. 402 f""Bin edges must be unique: {repr(bins)}.\n"". 403 f""You can drop duplicate edges by setting the 'duplicates' kwarg"". ValueError: Bin edges must be unique: array([ -inf, 1.00000000e-12, 1.00000000e-12, 1.00000000e-12,. 1.00000000e-12, 1.00000000e-12, 1.00000000e-12, 1.00000000e-12,. 1.00000000e-12, 1.11241045e-04, 2.44672419e-04, 5.69339462e-04,. 1.35840576e-03, 3.49938189e-03, 1.02259867e-02, 2.97723795e-02,. 7.22420720e-02, 1.46845240e-01, 2.97005969e-01, 3.42389128e+00,. inf]). You can drop duplicate edges by setting the 'duplicates' kwarg. ```. #### Versions. <details>. [Paste the output of scanpy.logging.print_versions() leaving a blank line after the details tag]. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1560
https://github.com/scverse/scanpy/issues/1560:154,usability,error,error,154,"Hello, I am trying to calculate highly variable genes from my data sets using the above code from the scanpy script published on github. I am facing this error. could someone please help me with this? Thank you very much.; - [ ] I have checked that this issue has not already been reported. - [ ] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. ### Minimal code sample (that we can copy&paste without having any data). ```python. sc.pp.highly_variable_genes(adata, flavor='cell_ranger', n_top_genes=4000). print('\n','Number of highly variable genes: {:d}'.format(np.sum(adata.var['highly_variable']))). ```. ```pytb. ValueError Traceback (most recent call last). <ipython-input-46-616fc10e63ff> in <module>. ----> 1 sc.pp.highly_variable_genes(adata, flavor='cell_ranger', n_top_genes=4000). 2 print('\n','Number of highly variable genes: {:d}'.format(np.sum(adata.var['highly_variable']))). ~\anaconda3\lib\site-packages\scanpy\preprocessing\_highly_variable_genes.py in highly_variable_genes(adata, layer, n_top_genes, min_disp, max_disp, min_mean, max_mean, span, n_bins, flavor, subset, inplace, batch_key). 424 . 425 if batch_key is None:. --> 426 df = _highly_variable_genes_single_batch(. 427 adata,. 428 layer=layer,. ~\anaconda3\lib\site-packages\scanpy\preprocessing\_highly_variable_genes.py in _highly_variable_genes_single_batch(adata, layer, min_disp, max_disp, min_mean, max_mean, n_top_genes, n_bins, flavor). 242 from statsmodels import robust. 243 . --> 244 df['mean_bin'] = pd.cut(. 245 df['means'],. 246 np.r_[-np.inf, np.percentile(df['means'], np.arange(10, 105, 5)), np.inf],. ~\anaconda3\lib\site-packages\pandas\core\reshape\tile.py in cut(x, bins, right, labels, retbins, precision, ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1560
https://github.com/scverse/scanpy/issues/1560:182,usability,help,help,182,"Hello, I am trying to calculate highly variable genes from my data sets using the above code from the scanpy script published on github. I am facing this error. could someone please help me with this? Thank you very much.; - [ ] I have checked that this issue has not already been reported. - [ ] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. ### Minimal code sample (that we can copy&paste without having any data). ```python. sc.pp.highly_variable_genes(adata, flavor='cell_ranger', n_top_genes=4000). print('\n','Number of highly variable genes: {:d}'.format(np.sum(adata.var['highly_variable']))). ```. ```pytb. ValueError Traceback (most recent call last). <ipython-input-46-616fc10e63ff> in <module>. ----> 1 sc.pp.highly_variable_genes(adata, flavor='cell_ranger', n_top_genes=4000). 2 print('\n','Number of highly variable genes: {:d}'.format(np.sum(adata.var['highly_variable']))). ~\anaconda3\lib\site-packages\scanpy\preprocessing\_highly_variable_genes.py in highly_variable_genes(adata, layer, n_top_genes, min_disp, max_disp, min_mean, max_mean, span, n_bins, flavor, subset, inplace, batch_key). 424 . 425 if batch_key is None:. --> 426 df = _highly_variable_genes_single_batch(. 427 adata,. 428 layer=layer,. ~\anaconda3\lib\site-packages\scanpy\preprocessing\_highly_variable_genes.py in _highly_variable_genes_single_batch(adata, layer, min_disp, max_disp, min_mean, max_mean, n_top_genes, n_bins, flavor). 242 from statsmodels import robust. 243 . --> 244 df['mean_bin'] = pd.cut(. 245 df['means'],. 246 np.r_[-np.inf, np.percentile(df['means'], np.arange(10, 105, 5)), np.inf],. ~\anaconda3\lib\site-packages\pandas\core\reshape\tile.py in cut(x, bins, right, labels, retbins, precision, ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1560
https://github.com/scverse/scanpy/issues/1560:304,usability,confirm,confirmed,304,"Hello, I am trying to calculate highly variable genes from my data sets using the above code from the scanpy script published on github. I am facing this error. could someone please help me with this? Thank you very much.; - [ ] I have checked that this issue has not already been reported. - [ ] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. ### Minimal code sample (that we can copy&paste without having any data). ```python. sc.pp.highly_variable_genes(adata, flavor='cell_ranger', n_top_genes=4000). print('\n','Number of highly variable genes: {:d}'.format(np.sum(adata.var['highly_variable']))). ```. ```pytb. ValueError Traceback (most recent call last). <ipython-input-46-616fc10e63ff> in <module>. ----> 1 sc.pp.highly_variable_genes(adata, flavor='cell_ranger', n_top_genes=4000). 2 print('\n','Number of highly variable genes: {:d}'.format(np.sum(adata.var['highly_variable']))). ~\anaconda3\lib\site-packages\scanpy\preprocessing\_highly_variable_genes.py in highly_variable_genes(adata, layer, n_top_genes, min_disp, max_disp, min_mean, max_mean, span, n_bins, flavor, subset, inplace, batch_key). 424 . 425 if batch_key is None:. --> 426 df = _highly_variable_genes_single_batch(. 427 adata,. 428 layer=layer,. ~\anaconda3\lib\site-packages\scanpy\preprocessing\_highly_variable_genes.py in _highly_variable_genes_single_batch(adata, layer, min_disp, max_disp, min_mean, max_mean, n_top_genes, n_bins, flavor). 242 from statsmodels import robust. 243 . --> 244 df['mean_bin'] = pd.cut(. 245 df['means'],. 246 np.r_[-np.inf, np.percentile(df['means'], np.arange(10, 105, 5)), np.inf],. ~\anaconda3\lib\site-packages\pandas\core\reshape\tile.py in cut(x, bins, right, labels, retbins, precision, ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1560
https://github.com/scverse/scanpy/issues/1560:387,usability,confirm,confirmed,387,"Hello, I am trying to calculate highly variable genes from my data sets using the above code from the scanpy script published on github. I am facing this error. could someone please help me with this? Thank you very much.; - [ ] I have checked that this issue has not already been reported. - [ ] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. ### Minimal code sample (that we can copy&paste without having any data). ```python. sc.pp.highly_variable_genes(adata, flavor='cell_ranger', n_top_genes=4000). print('\n','Number of highly variable genes: {:d}'.format(np.sum(adata.var['highly_variable']))). ```. ```pytb. ValueError Traceback (most recent call last). <ipython-input-46-616fc10e63ff> in <module>. ----> 1 sc.pp.highly_variable_genes(adata, flavor='cell_ranger', n_top_genes=4000). 2 print('\n','Number of highly variable genes: {:d}'.format(np.sum(adata.var['highly_variable']))). ~\anaconda3\lib\site-packages\scanpy\preprocessing\_highly_variable_genes.py in highly_variable_genes(adata, layer, n_top_genes, min_disp, max_disp, min_mean, max_mean, span, n_bins, flavor, subset, inplace, batch_key). 424 . 425 if batch_key is None:. --> 426 df = _highly_variable_genes_single_batch(. 427 adata,. 428 layer=layer,. ~\anaconda3\lib\site-packages\scanpy\preprocessing\_highly_variable_genes.py in _highly_variable_genes_single_batch(adata, layer, min_disp, max_disp, min_mean, max_mean, n_top_genes, n_bins, flavor). 242 from statsmodels import robust. 243 . --> 244 df['mean_bin'] = pd.cut(. 245 df['means'],. 246 np.r_[-np.inf, np.percentile(df['means'], np.arange(10, 105, 5)), np.inf],. ~\anaconda3\lib\site-packages\pandas\core\reshape\tile.py in cut(x, bins, right, labels, retbins, precision, ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1560
https://github.com/scverse/scanpy/issues/1560:478,usability,guid,guide,478,"Hello, I am trying to calculate highly variable genes from my data sets using the above code from the scanpy script published on github. I am facing this error. could someone please help me with this? Thank you very much.; - [ ] I have checked that this issue has not already been reported. - [ ] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. ### Minimal code sample (that we can copy&paste without having any data). ```python. sc.pp.highly_variable_genes(adata, flavor='cell_ranger', n_top_genes=4000). print('\n','Number of highly variable genes: {:d}'.format(np.sum(adata.var['highly_variable']))). ```. ```pytb. ValueError Traceback (most recent call last). <ipython-input-46-616fc10e63ff> in <module>. ----> 1 sc.pp.highly_variable_genes(adata, flavor='cell_ranger', n_top_genes=4000). 2 print('\n','Number of highly variable genes: {:d}'.format(np.sum(adata.var['highly_variable']))). ~\anaconda3\lib\site-packages\scanpy\preprocessing\_highly_variable_genes.py in highly_variable_genes(adata, layer, n_top_genes, min_disp, max_disp, min_mean, max_mean, span, n_bins, flavor, subset, inplace, batch_key). 424 . 425 if batch_key is None:. --> 426 df = _highly_variable_genes_single_batch(. 427 adata,. 428 layer=layer,. ~\anaconda3\lib\site-packages\scanpy\preprocessing\_highly_variable_genes.py in _highly_variable_genes_single_batch(adata, layer, min_disp, max_disp, min_mean, max_mean, n_top_genes, n_bins, flavor). 242 from statsmodels import robust. 243 . --> 244 df['mean_bin'] = pd.cut(. 245 df['means'],. 246 np.r_[-np.inf, np.percentile(df['means'], np.arange(10, 105, 5)), np.inf],. ~\anaconda3\lib\site-packages\pandas\core\reshape\tile.py in cut(x, bins, right, labels, retbins, precision, ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1560
https://github.com/scverse/scanpy/issues/1560:533,usability,minim,minimal-bug-reports,533,"Hello, I am trying to calculate highly variable genes from my data sets using the above code from the scanpy script published on github. I am facing this error. could someone please help me with this? Thank you very much.; - [ ] I have checked that this issue has not already been reported. - [ ] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. ### Minimal code sample (that we can copy&paste without having any data). ```python. sc.pp.highly_variable_genes(adata, flavor='cell_ranger', n_top_genes=4000). print('\n','Number of highly variable genes: {:d}'.format(np.sum(adata.var['highly_variable']))). ```. ```pytb. ValueError Traceback (most recent call last). <ipython-input-46-616fc10e63ff> in <module>. ----> 1 sc.pp.highly_variable_genes(adata, flavor='cell_ranger', n_top_genes=4000). 2 print('\n','Number of highly variable genes: {:d}'.format(np.sum(adata.var['highly_variable']))). ~\anaconda3\lib\site-packages\scanpy\preprocessing\_highly_variable_genes.py in highly_variable_genes(adata, layer, n_top_genes, min_disp, max_disp, min_mean, max_mean, span, n_bins, flavor, subset, inplace, batch_key). 424 . 425 if batch_key is None:. --> 426 df = _highly_variable_genes_single_batch(. 427 adata,. 428 layer=layer,. ~\anaconda3\lib\site-packages\scanpy\preprocessing\_highly_variable_genes.py in _highly_variable_genes_single_batch(adata, layer, min_disp, max_disp, min_mean, max_mean, n_top_genes, n_bins, flavor). 242 from statsmodels import robust. 243 . --> 244 df['mean_bin'] = pd.cut(. 245 df['means'],. 246 np.r_[-np.inf, np.percentile(df['means'], np.arange(10, 105, 5)), np.inf],. ~\anaconda3\lib\site-packages\pandas\core\reshape\tile.py in cut(x, bins, right, labels, retbins, precision, ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1560
https://github.com/scverse/scanpy/issues/1560:639,usability,Minim,Minimal,639,"Hello, I am trying to calculate highly variable genes from my data sets using the above code from the scanpy script published on github. I am facing this error. could someone please help me with this? Thank you very much.; - [ ] I have checked that this issue has not already been reported. - [ ] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. ### Minimal code sample (that we can copy&paste without having any data). ```python. sc.pp.highly_variable_genes(adata, flavor='cell_ranger', n_top_genes=4000). print('\n','Number of highly variable genes: {:d}'.format(np.sum(adata.var['highly_variable']))). ```. ```pytb. ValueError Traceback (most recent call last). <ipython-input-46-616fc10e63ff> in <module>. ----> 1 sc.pp.highly_variable_genes(adata, flavor='cell_ranger', n_top_genes=4000). 2 print('\n','Number of highly variable genes: {:d}'.format(np.sum(adata.var['highly_variable']))). ~\anaconda3\lib\site-packages\scanpy\preprocessing\_highly_variable_genes.py in highly_variable_genes(adata, layer, n_top_genes, min_disp, max_disp, min_mean, max_mean, span, n_bins, flavor, subset, inplace, batch_key). 424 . 425 if batch_key is None:. --> 426 df = _highly_variable_genes_single_batch(. 427 adata,. 428 layer=layer,. ~\anaconda3\lib\site-packages\scanpy\preprocessing\_highly_variable_genes.py in _highly_variable_genes_single_batch(adata, layer, min_disp, max_disp, min_mean, max_mean, n_top_genes, n_bins, flavor). 242 from statsmodels import robust. 243 . --> 244 df['mean_bin'] = pd.cut(. 245 df['means'],. 246 np.r_[-np.inf, np.percentile(df['means'], np.arange(10, 105, 5)), np.inf],. ~\anaconda3\lib\site-packages\pandas\core\reshape\tile.py in cut(x, bins, right, labels, retbins, precision, ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1560
https://github.com/scverse/scanpy/issues/1560:963,usability,input,input-,963,"Hello, I am trying to calculate highly variable genes from my data sets using the above code from the scanpy script published on github. I am facing this error. could someone please help me with this? Thank you very much.; - [ ] I have checked that this issue has not already been reported. - [ ] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. ### Minimal code sample (that we can copy&paste without having any data). ```python. sc.pp.highly_variable_genes(adata, flavor='cell_ranger', n_top_genes=4000). print('\n','Number of highly variable genes: {:d}'.format(np.sum(adata.var['highly_variable']))). ```. ```pytb. ValueError Traceback (most recent call last). <ipython-input-46-616fc10e63ff> in <module>. ----> 1 sc.pp.highly_variable_genes(adata, flavor='cell_ranger', n_top_genes=4000). 2 print('\n','Number of highly variable genes: {:d}'.format(np.sum(adata.var['highly_variable']))). ~\anaconda3\lib\site-packages\scanpy\preprocessing\_highly_variable_genes.py in highly_variable_genes(adata, layer, n_top_genes, min_disp, max_disp, min_mean, max_mean, span, n_bins, flavor, subset, inplace, batch_key). 424 . 425 if batch_key is None:. --> 426 df = _highly_variable_genes_single_batch(. 427 adata,. 428 layer=layer,. ~\anaconda3\lib\site-packages\scanpy\preprocessing\_highly_variable_genes.py in _highly_variable_genes_single_batch(adata, layer, min_disp, max_disp, min_mean, max_mean, n_top_genes, n_bins, flavor). 242 from statsmodels import robust. 243 . --> 244 df['mean_bin'] = pd.cut(. 245 df['means'],. 246 np.r_[-np.inf, np.percentile(df['means'], np.arange(10, 105, 5)), np.inf],. ~\anaconda3\lib\site-packages\pandas\core\reshape\tile.py in cut(x, bins, right, labels, retbins, precision, ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1560
https://github.com/scverse/scanpy/pull/1561:864,deployability,API,API,864,"Switch t-SNE implementation to openTSNE; @dkobak @ivirshup @Koncopd This is a first stab #1233. Features. - [X] Construct t-SNE embeddings. - [ ] Recipes. - [ ] Ingest functionality. As discussed, this PR currently implements t-SNE with uniform affinity kernels, making it fit in nicely with the existing `sc.pp.neighbors` architecture. While this isn't technically t-SNE per se, it's visually almost impossible to tell them apart. It would also make sense to add a `tsne` option to `sc.pp.neighbors`, but from what I can tell, there's no direct way to change the existing code to do this. It looks like `sc.pp.neighbors` calls UMAP to calculate the nearest neighbors directly, calculating the UMAP weights. We'd probably have to do something similar to the `gauss` option and just overwrite the UMAP weights after the fact. Does this sound reasonable? I like the API of calling `sc.tl.tsne.recipe_X(adata)`. Adding the recipes would be simple here; we can just add a simple class wrapper around `_tsne` which which would `__call__` tsne, and a bunch of static methods to the wrapper for recipes. This is kind-of messy and probably not something you guys do anywhere else throughout the code base, so I'd appreciate your feedback on this. Do you like this, or should we do it in a different way? The `ingest` functionality should be fairly straightforward as well, just adding a `tsne` option to `embedding_method`. All of these things should be pretty easy to do, but I'd like to see that this is moving in a direction you like first.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1561
https://github.com/scverse/scanpy/pull/1561:205,energy efficiency,current,currently,205,"Switch t-SNE implementation to openTSNE; @dkobak @ivirshup @Koncopd This is a first stab #1233. Features. - [X] Construct t-SNE embeddings. - [ ] Recipes. - [ ] Ingest functionality. As discussed, this PR currently implements t-SNE with uniform affinity kernels, making it fit in nicely with the existing `sc.pp.neighbors` architecture. While this isn't technically t-SNE per se, it's visually almost impossible to tell them apart. It would also make sense to add a `tsne` option to `sc.pp.neighbors`, but from what I can tell, there's no direct way to change the existing code to do this. It looks like `sc.pp.neighbors` calls UMAP to calculate the nearest neighbors directly, calculating the UMAP weights. We'd probably have to do something similar to the `gauss` option and just overwrite the UMAP weights after the fact. Does this sound reasonable? I like the API of calling `sc.tl.tsne.recipe_X(adata)`. Adding the recipes would be simple here; we can just add a simple class wrapper around `_tsne` which which would `__call__` tsne, and a bunch of static methods to the wrapper for recipes. This is kind-of messy and probably not something you guys do anywhere else throughout the code base, so I'd appreciate your feedback on this. Do you like this, or should we do it in a different way? The `ingest` functionality should be fairly straightforward as well, just adding a `tsne` option to `embedding_method`. All of these things should be pretty easy to do, but I'd like to see that this is moving in a direction you like first.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1561
https://github.com/scverse/scanpy/pull/1561:864,integrability,API,API,864,"Switch t-SNE implementation to openTSNE; @dkobak @ivirshup @Koncopd This is a first stab #1233. Features. - [X] Construct t-SNE embeddings. - [ ] Recipes. - [ ] Ingest functionality. As discussed, this PR currently implements t-SNE with uniform affinity kernels, making it fit in nicely with the existing `sc.pp.neighbors` architecture. While this isn't technically t-SNE per se, it's visually almost impossible to tell them apart. It would also make sense to add a `tsne` option to `sc.pp.neighbors`, but from what I can tell, there's no direct way to change the existing code to do this. It looks like `sc.pp.neighbors` calls UMAP to calculate the nearest neighbors directly, calculating the UMAP weights. We'd probably have to do something similar to the `gauss` option and just overwrite the UMAP weights after the fact. Does this sound reasonable? I like the API of calling `sc.tl.tsne.recipe_X(adata)`. Adding the recipes would be simple here; we can just add a simple class wrapper around `_tsne` which which would `__call__` tsne, and a bunch of static methods to the wrapper for recipes. This is kind-of messy and probably not something you guys do anywhere else throughout the code base, so I'd appreciate your feedback on this. Do you like this, or should we do it in a different way? The `ingest` functionality should be fairly straightforward as well, just adding a `tsne` option to `embedding_method`. All of these things should be pretty easy to do, but I'd like to see that this is moving in a direction you like first.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1561
https://github.com/scverse/scanpy/pull/1561:981,integrability,wrap,wrapper,981,"Switch t-SNE implementation to openTSNE; @dkobak @ivirshup @Koncopd This is a first stab #1233. Features. - [X] Construct t-SNE embeddings. - [ ] Recipes. - [ ] Ingest functionality. As discussed, this PR currently implements t-SNE with uniform affinity kernels, making it fit in nicely with the existing `sc.pp.neighbors` architecture. While this isn't technically t-SNE per se, it's visually almost impossible to tell them apart. It would also make sense to add a `tsne` option to `sc.pp.neighbors`, but from what I can tell, there's no direct way to change the existing code to do this. It looks like `sc.pp.neighbors` calls UMAP to calculate the nearest neighbors directly, calculating the UMAP weights. We'd probably have to do something similar to the `gauss` option and just overwrite the UMAP weights after the fact. Does this sound reasonable? I like the API of calling `sc.tl.tsne.recipe_X(adata)`. Adding the recipes would be simple here; we can just add a simple class wrapper around `_tsne` which which would `__call__` tsne, and a bunch of static methods to the wrapper for recipes. This is kind-of messy and probably not something you guys do anywhere else throughout the code base, so I'd appreciate your feedback on this. Do you like this, or should we do it in a different way? The `ingest` functionality should be fairly straightforward as well, just adding a `tsne` option to `embedding_method`. All of these things should be pretty easy to do, but I'd like to see that this is moving in a direction you like first.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1561
https://github.com/scverse/scanpy/pull/1561:1076,integrability,wrap,wrapper,1076,"Switch t-SNE implementation to openTSNE; @dkobak @ivirshup @Koncopd This is a first stab #1233. Features. - [X] Construct t-SNE embeddings. - [ ] Recipes. - [ ] Ingest functionality. As discussed, this PR currently implements t-SNE with uniform affinity kernels, making it fit in nicely with the existing `sc.pp.neighbors` architecture. While this isn't technically t-SNE per se, it's visually almost impossible to tell them apart. It would also make sense to add a `tsne` option to `sc.pp.neighbors`, but from what I can tell, there's no direct way to change the existing code to do this. It looks like `sc.pp.neighbors` calls UMAP to calculate the nearest neighbors directly, calculating the UMAP weights. We'd probably have to do something similar to the `gauss` option and just overwrite the UMAP weights after the fact. Does this sound reasonable? I like the API of calling `sc.tl.tsne.recipe_X(adata)`. Adding the recipes would be simple here; we can just add a simple class wrapper around `_tsne` which which would `__call__` tsne, and a bunch of static methods to the wrapper for recipes. This is kind-of messy and probably not something you guys do anywhere else throughout the code base, so I'd appreciate your feedback on this. Do you like this, or should we do it in a different way? The `ingest` functionality should be fairly straightforward as well, just adding a `tsne` option to `embedding_method`. All of these things should be pretty easy to do, but I'd like to see that this is moving in a direction you like first.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1561
https://github.com/scverse/scanpy/pull/1561:323,interoperability,architectur,architecture,323,"Switch t-SNE implementation to openTSNE; @dkobak @ivirshup @Koncopd This is a first stab #1233. Features. - [X] Construct t-SNE embeddings. - [ ] Recipes. - [ ] Ingest functionality. As discussed, this PR currently implements t-SNE with uniform affinity kernels, making it fit in nicely with the existing `sc.pp.neighbors` architecture. While this isn't technically t-SNE per se, it's visually almost impossible to tell them apart. It would also make sense to add a `tsne` option to `sc.pp.neighbors`, but from what I can tell, there's no direct way to change the existing code to do this. It looks like `sc.pp.neighbors` calls UMAP to calculate the nearest neighbors directly, calculating the UMAP weights. We'd probably have to do something similar to the `gauss` option and just overwrite the UMAP weights after the fact. Does this sound reasonable? I like the API of calling `sc.tl.tsne.recipe_X(adata)`. Adding the recipes would be simple here; we can just add a simple class wrapper around `_tsne` which which would `__call__` tsne, and a bunch of static methods to the wrapper for recipes. This is kind-of messy and probably not something you guys do anywhere else throughout the code base, so I'd appreciate your feedback on this. Do you like this, or should we do it in a different way? The `ingest` functionality should be fairly straightforward as well, just adding a `tsne` option to `embedding_method`. All of these things should be pretty easy to do, but I'd like to see that this is moving in a direction you like first.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1561
https://github.com/scverse/scanpy/pull/1561:864,interoperability,API,API,864,"Switch t-SNE implementation to openTSNE; @dkobak @ivirshup @Koncopd This is a first stab #1233. Features. - [X] Construct t-SNE embeddings. - [ ] Recipes. - [ ] Ingest functionality. As discussed, this PR currently implements t-SNE with uniform affinity kernels, making it fit in nicely with the existing `sc.pp.neighbors` architecture. While this isn't technically t-SNE per se, it's visually almost impossible to tell them apart. It would also make sense to add a `tsne` option to `sc.pp.neighbors`, but from what I can tell, there's no direct way to change the existing code to do this. It looks like `sc.pp.neighbors` calls UMAP to calculate the nearest neighbors directly, calculating the UMAP weights. We'd probably have to do something similar to the `gauss` option and just overwrite the UMAP weights after the fact. Does this sound reasonable? I like the API of calling `sc.tl.tsne.recipe_X(adata)`. Adding the recipes would be simple here; we can just add a simple class wrapper around `_tsne` which which would `__call__` tsne, and a bunch of static methods to the wrapper for recipes. This is kind-of messy and probably not something you guys do anywhere else throughout the code base, so I'd appreciate your feedback on this. Do you like this, or should we do it in a different way? The `ingest` functionality should be fairly straightforward as well, just adding a `tsne` option to `embedding_method`. All of these things should be pretty easy to do, but I'd like to see that this is moving in a direction you like first.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1561
https://github.com/scverse/scanpy/pull/1561:981,interoperability,wrapper,wrapper,981,"Switch t-SNE implementation to openTSNE; @dkobak @ivirshup @Koncopd This is a first stab #1233. Features. - [X] Construct t-SNE embeddings. - [ ] Recipes. - [ ] Ingest functionality. As discussed, this PR currently implements t-SNE with uniform affinity kernels, making it fit in nicely with the existing `sc.pp.neighbors` architecture. While this isn't technically t-SNE per se, it's visually almost impossible to tell them apart. It would also make sense to add a `tsne` option to `sc.pp.neighbors`, but from what I can tell, there's no direct way to change the existing code to do this. It looks like `sc.pp.neighbors` calls UMAP to calculate the nearest neighbors directly, calculating the UMAP weights. We'd probably have to do something similar to the `gauss` option and just overwrite the UMAP weights after the fact. Does this sound reasonable? I like the API of calling `sc.tl.tsne.recipe_X(adata)`. Adding the recipes would be simple here; we can just add a simple class wrapper around `_tsne` which which would `__call__` tsne, and a bunch of static methods to the wrapper for recipes. This is kind-of messy and probably not something you guys do anywhere else throughout the code base, so I'd appreciate your feedback on this. Do you like this, or should we do it in a different way? The `ingest` functionality should be fairly straightforward as well, just adding a `tsne` option to `embedding_method`. All of these things should be pretty easy to do, but I'd like to see that this is moving in a direction you like first.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1561
https://github.com/scverse/scanpy/pull/1561:1076,interoperability,wrapper,wrapper,1076,"Switch t-SNE implementation to openTSNE; @dkobak @ivirshup @Koncopd This is a first stab #1233. Features. - [X] Construct t-SNE embeddings. - [ ] Recipes. - [ ] Ingest functionality. As discussed, this PR currently implements t-SNE with uniform affinity kernels, making it fit in nicely with the existing `sc.pp.neighbors` architecture. While this isn't technically t-SNE per se, it's visually almost impossible to tell them apart. It would also make sense to add a `tsne` option to `sc.pp.neighbors`, but from what I can tell, there's no direct way to change the existing code to do this. It looks like `sc.pp.neighbors` calls UMAP to calculate the nearest neighbors directly, calculating the UMAP weights. We'd probably have to do something similar to the `gauss` option and just overwrite the UMAP weights after the fact. Does this sound reasonable? I like the API of calling `sc.tl.tsne.recipe_X(adata)`. Adding the recipes would be simple here; we can just add a simple class wrapper around `_tsne` which which would `__call__` tsne, and a bunch of static methods to the wrapper for recipes. This is kind-of messy and probably not something you guys do anywhere else throughout the code base, so I'd appreciate your feedback on this. Do you like this, or should we do it in a different way? The `ingest` functionality should be fairly straightforward as well, just adding a `tsne` option to `embedding_method`. All of these things should be pretty easy to do, but I'd like to see that this is moving in a direction you like first.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1561
https://github.com/scverse/scanpy/pull/1561:825,reliability,Doe,Does,825,"Switch t-SNE implementation to openTSNE; @dkobak @ivirshup @Koncopd This is a first stab #1233. Features. - [X] Construct t-SNE embeddings. - [ ] Recipes. - [ ] Ingest functionality. As discussed, this PR currently implements t-SNE with uniform affinity kernels, making it fit in nicely with the existing `sc.pp.neighbors` architecture. While this isn't technically t-SNE per se, it's visually almost impossible to tell them apart. It would also make sense to add a `tsne` option to `sc.pp.neighbors`, but from what I can tell, there's no direct way to change the existing code to do this. It looks like `sc.pp.neighbors` calls UMAP to calculate the nearest neighbors directly, calculating the UMAP weights. We'd probably have to do something similar to the `gauss` option and just overwrite the UMAP weights after the fact. Does this sound reasonable? I like the API of calling `sc.tl.tsne.recipe_X(adata)`. Adding the recipes would be simple here; we can just add a simple class wrapper around `_tsne` which which would `__call__` tsne, and a bunch of static methods to the wrapper for recipes. This is kind-of messy and probably not something you guys do anywhere else throughout the code base, so I'd appreciate your feedback on this. Do you like this, or should we do it in a different way? The `ingest` functionality should be fairly straightforward as well, just adding a `tsne` option to `embedding_method`. All of these things should be pretty easy to do, but I'd like to see that this is moving in a direction you like first.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1561
https://github.com/scverse/scanpy/pull/1561:937,testability,simpl,simple,937,"Switch t-SNE implementation to openTSNE; @dkobak @ivirshup @Koncopd This is a first stab #1233. Features. - [X] Construct t-SNE embeddings. - [ ] Recipes. - [ ] Ingest functionality. As discussed, this PR currently implements t-SNE with uniform affinity kernels, making it fit in nicely with the existing `sc.pp.neighbors` architecture. While this isn't technically t-SNE per se, it's visually almost impossible to tell them apart. It would also make sense to add a `tsne` option to `sc.pp.neighbors`, but from what I can tell, there's no direct way to change the existing code to do this. It looks like `sc.pp.neighbors` calls UMAP to calculate the nearest neighbors directly, calculating the UMAP weights. We'd probably have to do something similar to the `gauss` option and just overwrite the UMAP weights after the fact. Does this sound reasonable? I like the API of calling `sc.tl.tsne.recipe_X(adata)`. Adding the recipes would be simple here; we can just add a simple class wrapper around `_tsne` which which would `__call__` tsne, and a bunch of static methods to the wrapper for recipes. This is kind-of messy and probably not something you guys do anywhere else throughout the code base, so I'd appreciate your feedback on this. Do you like this, or should we do it in a different way? The `ingest` functionality should be fairly straightforward as well, just adding a `tsne` option to `embedding_method`. All of these things should be pretty easy to do, but I'd like to see that this is moving in a direction you like first.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1561
https://github.com/scverse/scanpy/pull/1561:968,testability,simpl,simple,968,"Switch t-SNE implementation to openTSNE; @dkobak @ivirshup @Koncopd This is a first stab #1233. Features. - [X] Construct t-SNE embeddings. - [ ] Recipes. - [ ] Ingest functionality. As discussed, this PR currently implements t-SNE with uniform affinity kernels, making it fit in nicely with the existing `sc.pp.neighbors` architecture. While this isn't technically t-SNE per se, it's visually almost impossible to tell them apart. It would also make sense to add a `tsne` option to `sc.pp.neighbors`, but from what I can tell, there's no direct way to change the existing code to do this. It looks like `sc.pp.neighbors` calls UMAP to calculate the nearest neighbors directly, calculating the UMAP weights. We'd probably have to do something similar to the `gauss` option and just overwrite the UMAP weights after the fact. Does this sound reasonable? I like the API of calling `sc.tl.tsne.recipe_X(adata)`. Adding the recipes would be simple here; we can just add a simple class wrapper around `_tsne` which which would `__call__` tsne, and a bunch of static methods to the wrapper for recipes. This is kind-of messy and probably not something you guys do anywhere else throughout the code base, so I'd appreciate your feedback on this. Do you like this, or should we do it in a different way? The `ingest` functionality should be fairly straightforward as well, just adding a `tsne` option to `embedding_method`. All of these things should be pretty easy to do, but I'd like to see that this is moving in a direction you like first.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1561
https://github.com/scverse/scanpy/pull/1561:385,usability,visual,visually,385,"Switch t-SNE implementation to openTSNE; @dkobak @ivirshup @Koncopd This is a first stab #1233. Features. - [X] Construct t-SNE embeddings. - [ ] Recipes. - [ ] Ingest functionality. As discussed, this PR currently implements t-SNE with uniform affinity kernels, making it fit in nicely with the existing `sc.pp.neighbors` architecture. While this isn't technically t-SNE per se, it's visually almost impossible to tell them apart. It would also make sense to add a `tsne` option to `sc.pp.neighbors`, but from what I can tell, there's no direct way to change the existing code to do this. It looks like `sc.pp.neighbors` calls UMAP to calculate the nearest neighbors directly, calculating the UMAP weights. We'd probably have to do something similar to the `gauss` option and just overwrite the UMAP weights after the fact. Does this sound reasonable? I like the API of calling `sc.tl.tsne.recipe_X(adata)`. Adding the recipes would be simple here; we can just add a simple class wrapper around `_tsne` which which would `__call__` tsne, and a bunch of static methods to the wrapper for recipes. This is kind-of messy and probably not something you guys do anywhere else throughout the code base, so I'd appreciate your feedback on this. Do you like this, or should we do it in a different way? The `ingest` functionality should be fairly straightforward as well, just adding a `tsne` option to `embedding_method`. All of these things should be pretty easy to do, but I'd like to see that this is moving in a direction you like first.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1561
https://github.com/scverse/scanpy/pull/1561:937,usability,simpl,simple,937,"Switch t-SNE implementation to openTSNE; @dkobak @ivirshup @Koncopd This is a first stab #1233. Features. - [X] Construct t-SNE embeddings. - [ ] Recipes. - [ ] Ingest functionality. As discussed, this PR currently implements t-SNE with uniform affinity kernels, making it fit in nicely with the existing `sc.pp.neighbors` architecture. While this isn't technically t-SNE per se, it's visually almost impossible to tell them apart. It would also make sense to add a `tsne` option to `sc.pp.neighbors`, but from what I can tell, there's no direct way to change the existing code to do this. It looks like `sc.pp.neighbors` calls UMAP to calculate the nearest neighbors directly, calculating the UMAP weights. We'd probably have to do something similar to the `gauss` option and just overwrite the UMAP weights after the fact. Does this sound reasonable? I like the API of calling `sc.tl.tsne.recipe_X(adata)`. Adding the recipes would be simple here; we can just add a simple class wrapper around `_tsne` which which would `__call__` tsne, and a bunch of static methods to the wrapper for recipes. This is kind-of messy and probably not something you guys do anywhere else throughout the code base, so I'd appreciate your feedback on this. Do you like this, or should we do it in a different way? The `ingest` functionality should be fairly straightforward as well, just adding a `tsne` option to `embedding_method`. All of these things should be pretty easy to do, but I'd like to see that this is moving in a direction you like first.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1561
https://github.com/scverse/scanpy/pull/1561:968,usability,simpl,simple,968,"Switch t-SNE implementation to openTSNE; @dkobak @ivirshup @Koncopd This is a first stab #1233. Features. - [X] Construct t-SNE embeddings. - [ ] Recipes. - [ ] Ingest functionality. As discussed, this PR currently implements t-SNE with uniform affinity kernels, making it fit in nicely with the existing `sc.pp.neighbors` architecture. While this isn't technically t-SNE per se, it's visually almost impossible to tell them apart. It would also make sense to add a `tsne` option to `sc.pp.neighbors`, but from what I can tell, there's no direct way to change the existing code to do this. It looks like `sc.pp.neighbors` calls UMAP to calculate the nearest neighbors directly, calculating the UMAP weights. We'd probably have to do something similar to the `gauss` option and just overwrite the UMAP weights after the fact. Does this sound reasonable? I like the API of calling `sc.tl.tsne.recipe_X(adata)`. Adding the recipes would be simple here; we can just add a simple class wrapper around `_tsne` which which would `__call__` tsne, and a bunch of static methods to the wrapper for recipes. This is kind-of messy and probably not something you guys do anywhere else throughout the code base, so I'd appreciate your feedback on this. Do you like this, or should we do it in a different way? The `ingest` functionality should be fairly straightforward as well, just adding a `tsne` option to `embedding_method`. All of these things should be pretty easy to do, but I'd like to see that this is moving in a direction you like first.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1561
https://github.com/scverse/scanpy/pull/1561:1221,usability,feedback,feedback,1221,"Switch t-SNE implementation to openTSNE; @dkobak @ivirshup @Koncopd This is a first stab #1233. Features. - [X] Construct t-SNE embeddings. - [ ] Recipes. - [ ] Ingest functionality. As discussed, this PR currently implements t-SNE with uniform affinity kernels, making it fit in nicely with the existing `sc.pp.neighbors` architecture. While this isn't technically t-SNE per se, it's visually almost impossible to tell them apart. It would also make sense to add a `tsne` option to `sc.pp.neighbors`, but from what I can tell, there's no direct way to change the existing code to do this. It looks like `sc.pp.neighbors` calls UMAP to calculate the nearest neighbors directly, calculating the UMAP weights. We'd probably have to do something similar to the `gauss` option and just overwrite the UMAP weights after the fact. Does this sound reasonable? I like the API of calling `sc.tl.tsne.recipe_X(adata)`. Adding the recipes would be simple here; we can just add a simple class wrapper around `_tsne` which which would `__call__` tsne, and a bunch of static methods to the wrapper for recipes. This is kind-of messy and probably not something you guys do anywhere else throughout the code base, so I'd appreciate your feedback on this. Do you like this, or should we do it in a different way? The `ingest` functionality should be fairly straightforward as well, just adding a `tsne` option to `embedding_method`. All of these things should be pretty easy to do, but I'd like to see that this is moving in a direction you like first.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1561
https://github.com/scverse/scanpy/issues/1562:92,deployability,fail,fails,92,"matrixplot group order changes with pandas 1.2.0; @fidelram. The test `test_matrixplot_obj` fails if `pandas>1.2.0` as the groups order changes. These groups are meant to be sorted with `plot.add_totals(sort='descending')`. All groups have the same value here, so my assumption is this is fine. I'm going to change the test for now, but it'd be good to hear back from you on whether this is a bug or not. It would probably be good if we could ensure a stable sort was used so this won't change in future.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1562
https://github.com/scverse/scanpy/issues/1562:92,reliability,fail,fails,92,"matrixplot group order changes with pandas 1.2.0; @fidelram. The test `test_matrixplot_obj` fails if `pandas>1.2.0` as the groups order changes. These groups are meant to be sorted with `plot.add_totals(sort='descending')`. All groups have the same value here, so my assumption is this is fine. I'm going to change the test for now, but it'd be good to hear back from you on whether this is a bug or not. It would probably be good if we could ensure a stable sort was used so this won't change in future.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1562
https://github.com/scverse/scanpy/issues/1562:65,safety,test,test,65,"matrixplot group order changes with pandas 1.2.0; @fidelram. The test `test_matrixplot_obj` fails if `pandas>1.2.0` as the groups order changes. These groups are meant to be sorted with `plot.add_totals(sort='descending')`. All groups have the same value here, so my assumption is this is fine. I'm going to change the test for now, but it'd be good to hear back from you on whether this is a bug or not. It would probably be good if we could ensure a stable sort was used so this won't change in future.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1562
https://github.com/scverse/scanpy/issues/1562:319,safety,test,test,319,"matrixplot group order changes with pandas 1.2.0; @fidelram. The test `test_matrixplot_obj` fails if `pandas>1.2.0` as the groups order changes. These groups are meant to be sorted with `plot.add_totals(sort='descending')`. All groups have the same value here, so my assumption is this is fine. I'm going to change the test for now, but it'd be good to hear back from you on whether this is a bug or not. It would probably be good if we could ensure a stable sort was used so this won't change in future.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1562
https://github.com/scverse/scanpy/issues/1562:65,testability,test,test,65,"matrixplot group order changes with pandas 1.2.0; @fidelram. The test `test_matrixplot_obj` fails if `pandas>1.2.0` as the groups order changes. These groups are meant to be sorted with `plot.add_totals(sort='descending')`. All groups have the same value here, so my assumption is this is fine. I'm going to change the test for now, but it'd be good to hear back from you on whether this is a bug or not. It would probably be good if we could ensure a stable sort was used so this won't change in future.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1562
https://github.com/scverse/scanpy/issues/1562:319,testability,test,test,319,"matrixplot group order changes with pandas 1.2.0; @fidelram. The test `test_matrixplot_obj` fails if `pandas>1.2.0` as the groups order changes. These groups are meant to be sorted with `plot.add_totals(sort='descending')`. All groups have the same value here, so my assumption is this is fine. I'm going to change the test for now, but it'd be good to hear back from you on whether this is a bug or not. It would probably be good if we could ensure a stable sort was used so this won't change in future.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1562
https://github.com/scverse/scanpy/issues/1563:1021,availability,slo,slow,1021,"Pre-commit (dev workflow); I'd like to start using [pre-commit](https://pre-commit.com) with scanpy and anndata. Pre-commit is essentially a tool that manages scripts we'd like to run before each commit, e.g. linting and formatting, so it becomes essentially impossible to forget these. I think this can allow PRs to progress faster since it gives us a way to codify formatting requirements  so we don't have to remember them  and have these checks happen locally  so we don't have to wait on CI. Of course, having these checks run depends on developers installing pre-commit, so we can also run these checks on CI ([example ci script](https://github.com/pandas-dev/pandas/blob/master/.github/workflows/pre-commit.yml), [example run](https://github.com/pandas-dev/pandas/pull/38745/checks?check_run_id=1624558250)). There is a question of what things we'd like to add here. For sure: `black`. I think import checks (e.g. no unused imports) and `flake8` would be good too. We can also add custom checks for things like slow imports. I think this would be a good time to run `black` over the whole codebase so we don't have exempted files any more. My questions for the dev team:. * Does this sound good? * Do you have more ideas for checks/ tools? @michalk8, I saw you added this to [`squidpy`](https://github.com/theislab/squidpy/pull/203). How's the experience been there  that is, any major foot guns we should look out for? Also, are there any tools you're using (beyond the basic `black`, `isort`, `flake8`) you'd especially recommend?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1563
https://github.com/scverse/scanpy/issues/1563:151,deployability,manag,manages,151,"Pre-commit (dev workflow); I'd like to start using [pre-commit](https://pre-commit.com) with scanpy and anndata. Pre-commit is essentially a tool that manages scripts we'd like to run before each commit, e.g. linting and formatting, so it becomes essentially impossible to forget these. I think this can allow PRs to progress faster since it gives us a way to codify formatting requirements  so we don't have to remember them  and have these checks happen locally  so we don't have to wait on CI. Of course, having these checks run depends on developers installing pre-commit, so we can also run these checks on CI ([example ci script](https://github.com/pandas-dev/pandas/blob/master/.github/workflows/pre-commit.yml), [example run](https://github.com/pandas-dev/pandas/pull/38745/checks?check_run_id=1624558250)). There is a question of what things we'd like to add here. For sure: `black`. I think import checks (e.g. no unused imports) and `flake8` would be good too. We can also add custom checks for things like slow imports. I think this would be a good time to run `black` over the whole codebase so we don't have exempted files any more. My questions for the dev team:. * Does this sound good? * Do you have more ideas for checks/ tools? @michalk8, I saw you added this to [`squidpy`](https://github.com/theislab/squidpy/pull/203). How's the experience been there  that is, any major foot guns we should look out for? Also, are there any tools you're using (beyond the basic `black`, `isort`, `flake8`) you'd especially recommend?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1563
https://github.com/scverse/scanpy/issues/1563:535,deployability,depend,depends,535,"Pre-commit (dev workflow); I'd like to start using [pre-commit](https://pre-commit.com) with scanpy and anndata. Pre-commit is essentially a tool that manages scripts we'd like to run before each commit, e.g. linting and formatting, so it becomes essentially impossible to forget these. I think this can allow PRs to progress faster since it gives us a way to codify formatting requirements  so we don't have to remember them  and have these checks happen locally  so we don't have to wait on CI. Of course, having these checks run depends on developers installing pre-commit, so we can also run these checks on CI ([example ci script](https://github.com/pandas-dev/pandas/blob/master/.github/workflows/pre-commit.yml), [example run](https://github.com/pandas-dev/pandas/pull/38745/checks?check_run_id=1624558250)). There is a question of what things we'd like to add here. For sure: `black`. I think import checks (e.g. no unused imports) and `flake8` would be good too. We can also add custom checks for things like slow imports. I think this would be a good time to run `black` over the whole codebase so we don't have exempted files any more. My questions for the dev team:. * Does this sound good? * Do you have more ideas for checks/ tools? @michalk8, I saw you added this to [`squidpy`](https://github.com/theislab/squidpy/pull/203). How's the experience been there  that is, any major foot guns we should look out for? Also, are there any tools you're using (beyond the basic `black`, `isort`, `flake8`) you'd especially recommend?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1563
https://github.com/scverse/scanpy/issues/1563:557,deployability,instal,installing,557,"Pre-commit (dev workflow); I'd like to start using [pre-commit](https://pre-commit.com) with scanpy and anndata. Pre-commit is essentially a tool that manages scripts we'd like to run before each commit, e.g. linting and formatting, so it becomes essentially impossible to forget these. I think this can allow PRs to progress faster since it gives us a way to codify formatting requirements  so we don't have to remember them  and have these checks happen locally  so we don't have to wait on CI. Of course, having these checks run depends on developers installing pre-commit, so we can also run these checks on CI ([example ci script](https://github.com/pandas-dev/pandas/blob/master/.github/workflows/pre-commit.yml), [example run](https://github.com/pandas-dev/pandas/pull/38745/checks?check_run_id=1624558250)). There is a question of what things we'd like to add here. For sure: `black`. I think import checks (e.g. no unused imports) and `flake8` would be good too. We can also add custom checks for things like slow imports. I think this would be a good time to run `black` over the whole codebase so we don't have exempted files any more. My questions for the dev team:. * Does this sound good? * Do you have more ideas for checks/ tools? @michalk8, I saw you added this to [`squidpy`](https://github.com/theislab/squidpy/pull/203). How's the experience been there  that is, any major foot guns we should look out for? Also, are there any tools you're using (beyond the basic `black`, `isort`, `flake8`) you'd especially recommend?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1563
https://github.com/scverse/scanpy/issues/1563:151,energy efficiency,manag,manages,151,"Pre-commit (dev workflow); I'd like to start using [pre-commit](https://pre-commit.com) with scanpy and anndata. Pre-commit is essentially a tool that manages scripts we'd like to run before each commit, e.g. linting and formatting, so it becomes essentially impossible to forget these. I think this can allow PRs to progress faster since it gives us a way to codify formatting requirements  so we don't have to remember them  and have these checks happen locally  so we don't have to wait on CI. Of course, having these checks run depends on developers installing pre-commit, so we can also run these checks on CI ([example ci script](https://github.com/pandas-dev/pandas/blob/master/.github/workflows/pre-commit.yml), [example run](https://github.com/pandas-dev/pandas/pull/38745/checks?check_run_id=1624558250)). There is a question of what things we'd like to add here. For sure: `black`. I think import checks (e.g. no unused imports) and `flake8` would be good too. We can also add custom checks for things like slow imports. I think this would be a good time to run `black` over the whole codebase so we don't have exempted files any more. My questions for the dev team:. * Does this sound good? * Do you have more ideas for checks/ tools? @michalk8, I saw you added this to [`squidpy`](https://github.com/theislab/squidpy/pull/203). How's the experience been there  that is, any major foot guns we should look out for? Also, are there any tools you're using (beyond the basic `black`, `isort`, `flake8`) you'd especially recommend?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1563
https://github.com/scverse/scanpy/issues/1563:535,integrability,depend,depends,535,"Pre-commit (dev workflow); I'd like to start using [pre-commit](https://pre-commit.com) with scanpy and anndata. Pre-commit is essentially a tool that manages scripts we'd like to run before each commit, e.g. linting and formatting, so it becomes essentially impossible to forget these. I think this can allow PRs to progress faster since it gives us a way to codify formatting requirements  so we don't have to remember them  and have these checks happen locally  so we don't have to wait on CI. Of course, having these checks run depends on developers installing pre-commit, so we can also run these checks on CI ([example ci script](https://github.com/pandas-dev/pandas/blob/master/.github/workflows/pre-commit.yml), [example run](https://github.com/pandas-dev/pandas/pull/38745/checks?check_run_id=1624558250)). There is a question of what things we'd like to add here. For sure: `black`. I think import checks (e.g. no unused imports) and `flake8` would be good too. We can also add custom checks for things like slow imports. I think this would be a good time to run `black` over the whole codebase so we don't have exempted files any more. My questions for the dev team:. * Does this sound good? * Do you have more ideas for checks/ tools? @michalk8, I saw you added this to [`squidpy`](https://github.com/theislab/squidpy/pull/203). How's the experience been there  that is, any major foot guns we should look out for? Also, are there any tools you're using (beyond the basic `black`, `isort`, `flake8`) you'd especially recommend?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1563
https://github.com/scverse/scanpy/issues/1563:221,interoperability,format,formatting,221,"Pre-commit (dev workflow); I'd like to start using [pre-commit](https://pre-commit.com) with scanpy and anndata. Pre-commit is essentially a tool that manages scripts we'd like to run before each commit, e.g. linting and formatting, so it becomes essentially impossible to forget these. I think this can allow PRs to progress faster since it gives us a way to codify formatting requirements  so we don't have to remember them  and have these checks happen locally  so we don't have to wait on CI. Of course, having these checks run depends on developers installing pre-commit, so we can also run these checks on CI ([example ci script](https://github.com/pandas-dev/pandas/blob/master/.github/workflows/pre-commit.yml), [example run](https://github.com/pandas-dev/pandas/pull/38745/checks?check_run_id=1624558250)). There is a question of what things we'd like to add here. For sure: `black`. I think import checks (e.g. no unused imports) and `flake8` would be good too. We can also add custom checks for things like slow imports. I think this would be a good time to run `black` over the whole codebase so we don't have exempted files any more. My questions for the dev team:. * Does this sound good? * Do you have more ideas for checks/ tools? @michalk8, I saw you added this to [`squidpy`](https://github.com/theislab/squidpy/pull/203). How's the experience been there  that is, any major foot guns we should look out for? Also, are there any tools you're using (beyond the basic `black`, `isort`, `flake8`) you'd especially recommend?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1563
https://github.com/scverse/scanpy/issues/1563:367,interoperability,format,formatting,367,"Pre-commit (dev workflow); I'd like to start using [pre-commit](https://pre-commit.com) with scanpy and anndata. Pre-commit is essentially a tool that manages scripts we'd like to run before each commit, e.g. linting and formatting, so it becomes essentially impossible to forget these. I think this can allow PRs to progress faster since it gives us a way to codify formatting requirements  so we don't have to remember them  and have these checks happen locally  so we don't have to wait on CI. Of course, having these checks run depends on developers installing pre-commit, so we can also run these checks on CI ([example ci script](https://github.com/pandas-dev/pandas/blob/master/.github/workflows/pre-commit.yml), [example run](https://github.com/pandas-dev/pandas/pull/38745/checks?check_run_id=1624558250)). There is a question of what things we'd like to add here. For sure: `black`. I think import checks (e.g. no unused imports) and `flake8` would be good too. We can also add custom checks for things like slow imports. I think this would be a good time to run `black` over the whole codebase so we don't have exempted files any more. My questions for the dev team:. * Does this sound good? * Do you have more ideas for checks/ tools? @michalk8, I saw you added this to [`squidpy`](https://github.com/theislab/squidpy/pull/203). How's the experience been there  that is, any major foot guns we should look out for? Also, are there any tools you're using (beyond the basic `black`, `isort`, `flake8`) you'd especially recommend?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1563
https://github.com/scverse/scanpy/issues/1563:535,modifiability,depend,depends,535,"Pre-commit (dev workflow); I'd like to start using [pre-commit](https://pre-commit.com) with scanpy and anndata. Pre-commit is essentially a tool that manages scripts we'd like to run before each commit, e.g. linting and formatting, so it becomes essentially impossible to forget these. I think this can allow PRs to progress faster since it gives us a way to codify formatting requirements  so we don't have to remember them  and have these checks happen locally  so we don't have to wait on CI. Of course, having these checks run depends on developers installing pre-commit, so we can also run these checks on CI ([example ci script](https://github.com/pandas-dev/pandas/blob/master/.github/workflows/pre-commit.yml), [example run](https://github.com/pandas-dev/pandas/pull/38745/checks?check_run_id=1624558250)). There is a question of what things we'd like to add here. For sure: `black`. I think import checks (e.g. no unused imports) and `flake8` would be good too. We can also add custom checks for things like slow imports. I think this would be a good time to run `black` over the whole codebase so we don't have exempted files any more. My questions for the dev team:. * Does this sound good? * Do you have more ideas for checks/ tools? @michalk8, I saw you added this to [`squidpy`](https://github.com/theislab/squidpy/pull/203). How's the experience been there  that is, any major foot guns we should look out for? Also, are there any tools you're using (beyond the basic `black`, `isort`, `flake8`) you'd especially recommend?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1563
https://github.com/scverse/scanpy/issues/1563:1064,performance,time,time,1064,"Pre-commit (dev workflow); I'd like to start using [pre-commit](https://pre-commit.com) with scanpy and anndata. Pre-commit is essentially a tool that manages scripts we'd like to run before each commit, e.g. linting and formatting, so it becomes essentially impossible to forget these. I think this can allow PRs to progress faster since it gives us a way to codify formatting requirements  so we don't have to remember them  and have these checks happen locally  so we don't have to wait on CI. Of course, having these checks run depends on developers installing pre-commit, so we can also run these checks on CI ([example ci script](https://github.com/pandas-dev/pandas/blob/master/.github/workflows/pre-commit.yml), [example run](https://github.com/pandas-dev/pandas/pull/38745/checks?check_run_id=1624558250)). There is a question of what things we'd like to add here. For sure: `black`. I think import checks (e.g. no unused imports) and `flake8` would be good too. We can also add custom checks for things like slow imports. I think this would be a good time to run `black` over the whole codebase so we don't have exempted files any more. My questions for the dev team:. * Does this sound good? * Do you have more ideas for checks/ tools? @michalk8, I saw you added this to [`squidpy`](https://github.com/theislab/squidpy/pull/203). How's the experience been there  that is, any major foot guns we should look out for? Also, are there any tools you're using (beyond the basic `black`, `isort`, `flake8`) you'd especially recommend?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1563
https://github.com/scverse/scanpy/issues/1563:1021,reliability,slo,slow,1021,"Pre-commit (dev workflow); I'd like to start using [pre-commit](https://pre-commit.com) with scanpy and anndata. Pre-commit is essentially a tool that manages scripts we'd like to run before each commit, e.g. linting and formatting, so it becomes essentially impossible to forget these. I think this can allow PRs to progress faster since it gives us a way to codify formatting requirements  so we don't have to remember them  and have these checks happen locally  so we don't have to wait on CI. Of course, having these checks run depends on developers installing pre-commit, so we can also run these checks on CI ([example ci script](https://github.com/pandas-dev/pandas/blob/master/.github/workflows/pre-commit.yml), [example run](https://github.com/pandas-dev/pandas/pull/38745/checks?check_run_id=1624558250)). There is a question of what things we'd like to add here. For sure: `black`. I think import checks (e.g. no unused imports) and `flake8` would be good too. We can also add custom checks for things like slow imports. I think this would be a good time to run `black` over the whole codebase so we don't have exempted files any more. My questions for the dev team:. * Does this sound good? * Do you have more ideas for checks/ tools? @michalk8, I saw you added this to [`squidpy`](https://github.com/theislab/squidpy/pull/203). How's the experience been there  that is, any major foot guns we should look out for? Also, are there any tools you're using (beyond the basic `black`, `isort`, `flake8`) you'd especially recommend?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1563
https://github.com/scverse/scanpy/issues/1563:1184,reliability,Doe,Does,1184,"Pre-commit (dev workflow); I'd like to start using [pre-commit](https://pre-commit.com) with scanpy and anndata. Pre-commit is essentially a tool that manages scripts we'd like to run before each commit, e.g. linting and formatting, so it becomes essentially impossible to forget these. I think this can allow PRs to progress faster since it gives us a way to codify formatting requirements  so we don't have to remember them  and have these checks happen locally  so we don't have to wait on CI. Of course, having these checks run depends on developers installing pre-commit, so we can also run these checks on CI ([example ci script](https://github.com/pandas-dev/pandas/blob/master/.github/workflows/pre-commit.yml), [example run](https://github.com/pandas-dev/pandas/pull/38745/checks?check_run_id=1624558250)). There is a question of what things we'd like to add here. For sure: `black`. I think import checks (e.g. no unused imports) and `flake8` would be good too. We can also add custom checks for things like slow imports. I think this would be a good time to run `black` over the whole codebase so we don't have exempted files any more. My questions for the dev team:. * Does this sound good? * Do you have more ideas for checks/ tools? @michalk8, I saw you added this to [`squidpy`](https://github.com/theislab/squidpy/pull/203). How's the experience been there  that is, any major foot guns we should look out for? Also, are there any tools you're using (beyond the basic `black`, `isort`, `flake8`) you'd especially recommend?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1563
https://github.com/scverse/scanpy/issues/1563:151,safety,manag,manages,151,"Pre-commit (dev workflow); I'd like to start using [pre-commit](https://pre-commit.com) with scanpy and anndata. Pre-commit is essentially a tool that manages scripts we'd like to run before each commit, e.g. linting and formatting, so it becomes essentially impossible to forget these. I think this can allow PRs to progress faster since it gives us a way to codify formatting requirements  so we don't have to remember them  and have these checks happen locally  so we don't have to wait on CI. Of course, having these checks run depends on developers installing pre-commit, so we can also run these checks on CI ([example ci script](https://github.com/pandas-dev/pandas/blob/master/.github/workflows/pre-commit.yml), [example run](https://github.com/pandas-dev/pandas/pull/38745/checks?check_run_id=1624558250)). There is a question of what things we'd like to add here. For sure: `black`. I think import checks (e.g. no unused imports) and `flake8` would be good too. We can also add custom checks for things like slow imports. I think this would be a good time to run `black` over the whole codebase so we don't have exempted files any more. My questions for the dev team:. * Does this sound good? * Do you have more ideas for checks/ tools? @michalk8, I saw you added this to [`squidpy`](https://github.com/theislab/squidpy/pull/203). How's the experience been there  that is, any major foot guns we should look out for? Also, are there any tools you're using (beyond the basic `black`, `isort`, `flake8`) you'd especially recommend?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1563
https://github.com/scverse/scanpy/issues/1563:413,safety,reme,remember,413,"Pre-commit (dev workflow); I'd like to start using [pre-commit](https://pre-commit.com) with scanpy and anndata. Pre-commit is essentially a tool that manages scripts we'd like to run before each commit, e.g. linting and formatting, so it becomes essentially impossible to forget these. I think this can allow PRs to progress faster since it gives us a way to codify formatting requirements  so we don't have to remember them  and have these checks happen locally  so we don't have to wait on CI. Of course, having these checks run depends on developers installing pre-commit, so we can also run these checks on CI ([example ci script](https://github.com/pandas-dev/pandas/blob/master/.github/workflows/pre-commit.yml), [example run](https://github.com/pandas-dev/pandas/pull/38745/checks?check_run_id=1624558250)). There is a question of what things we'd like to add here. For sure: `black`. I think import checks (e.g. no unused imports) and `flake8` would be good too. We can also add custom checks for things like slow imports. I think this would be a good time to run `black` over the whole codebase so we don't have exempted files any more. My questions for the dev team:. * Does this sound good? * Do you have more ideas for checks/ tools? @michalk8, I saw you added this to [`squidpy`](https://github.com/theislab/squidpy/pull/203). How's the experience been there  that is, any major foot guns we should look out for? Also, are there any tools you're using (beyond the basic `black`, `isort`, `flake8`) you'd especially recommend?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1563
https://github.com/scverse/scanpy/issues/1563:535,safety,depend,depends,535,"Pre-commit (dev workflow); I'd like to start using [pre-commit](https://pre-commit.com) with scanpy and anndata. Pre-commit is essentially a tool that manages scripts we'd like to run before each commit, e.g. linting and formatting, so it becomes essentially impossible to forget these. I think this can allow PRs to progress faster since it gives us a way to codify formatting requirements  so we don't have to remember them  and have these checks happen locally  so we don't have to wait on CI. Of course, having these checks run depends on developers installing pre-commit, so we can also run these checks on CI ([example ci script](https://github.com/pandas-dev/pandas/blob/master/.github/workflows/pre-commit.yml), [example run](https://github.com/pandas-dev/pandas/pull/38745/checks?check_run_id=1624558250)). There is a question of what things we'd like to add here. For sure: `black`. I think import checks (e.g. no unused imports) and `flake8` would be good too. We can also add custom checks for things like slow imports. I think this would be a good time to run `black` over the whole codebase so we don't have exempted files any more. My questions for the dev team:. * Does this sound good? * Do you have more ideas for checks/ tools? @michalk8, I saw you added this to [`squidpy`](https://github.com/theislab/squidpy/pull/203). How's the experience been there  that is, any major foot guns we should look out for? Also, are there any tools you're using (beyond the basic `black`, `isort`, `flake8`) you'd especially recommend?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1563
https://github.com/scverse/scanpy/issues/1563:1175,security,team,team,1175,"Pre-commit (dev workflow); I'd like to start using [pre-commit](https://pre-commit.com) with scanpy and anndata. Pre-commit is essentially a tool that manages scripts we'd like to run before each commit, e.g. linting and formatting, so it becomes essentially impossible to forget these. I think this can allow PRs to progress faster since it gives us a way to codify formatting requirements  so we don't have to remember them  and have these checks happen locally  so we don't have to wait on CI. Of course, having these checks run depends on developers installing pre-commit, so we can also run these checks on CI ([example ci script](https://github.com/pandas-dev/pandas/blob/master/.github/workflows/pre-commit.yml), [example run](https://github.com/pandas-dev/pandas/pull/38745/checks?check_run_id=1624558250)). There is a question of what things we'd like to add here. For sure: `black`. I think import checks (e.g. no unused imports) and `flake8` would be good too. We can also add custom checks for things like slow imports. I think this would be a good time to run `black` over the whole codebase so we don't have exempted files any more. My questions for the dev team:. * Does this sound good? * Do you have more ideas for checks/ tools? @michalk8, I saw you added this to [`squidpy`](https://github.com/theislab/squidpy/pull/203). How's the experience been there  that is, any major foot guns we should look out for? Also, are there any tools you're using (beyond the basic `black`, `isort`, `flake8`) you'd especially recommend?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1563
https://github.com/scverse/scanpy/issues/1563:1498,security,iso,isort,1498,"Pre-commit (dev workflow); I'd like to start using [pre-commit](https://pre-commit.com) with scanpy and anndata. Pre-commit is essentially a tool that manages scripts we'd like to run before each commit, e.g. linting and formatting, so it becomes essentially impossible to forget these. I think this can allow PRs to progress faster since it gives us a way to codify formatting requirements  so we don't have to remember them  and have these checks happen locally  so we don't have to wait on CI. Of course, having these checks run depends on developers installing pre-commit, so we can also run these checks on CI ([example ci script](https://github.com/pandas-dev/pandas/blob/master/.github/workflows/pre-commit.yml), [example run](https://github.com/pandas-dev/pandas/pull/38745/checks?check_run_id=1624558250)). There is a question of what things we'd like to add here. For sure: `black`. I think import checks (e.g. no unused imports) and `flake8` would be good too. We can also add custom checks for things like slow imports. I think this would be a good time to run `black` over the whole codebase so we don't have exempted files any more. My questions for the dev team:. * Does this sound good? * Do you have more ideas for checks/ tools? @michalk8, I saw you added this to [`squidpy`](https://github.com/theislab/squidpy/pull/203). How's the experience been there  that is, any major foot guns we should look out for? Also, are there any tools you're using (beyond the basic `black`, `isort`, `flake8`) you'd especially recommend?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1563
https://github.com/scverse/scanpy/issues/1563:535,testability,depend,depends,535,"Pre-commit (dev workflow); I'd like to start using [pre-commit](https://pre-commit.com) with scanpy and anndata. Pre-commit is essentially a tool that manages scripts we'd like to run before each commit, e.g. linting and formatting, so it becomes essentially impossible to forget these. I think this can allow PRs to progress faster since it gives us a way to codify formatting requirements  so we don't have to remember them  and have these checks happen locally  so we don't have to wait on CI. Of course, having these checks run depends on developers installing pre-commit, so we can also run these checks on CI ([example ci script](https://github.com/pandas-dev/pandas/blob/master/.github/workflows/pre-commit.yml), [example run](https://github.com/pandas-dev/pandas/pull/38745/checks?check_run_id=1624558250)). There is a question of what things we'd like to add here. For sure: `black`. I think import checks (e.g. no unused imports) and `flake8` would be good too. We can also add custom checks for things like slow imports. I think this would be a good time to run `black` over the whole codebase so we don't have exempted files any more. My questions for the dev team:. * Does this sound good? * Do you have more ideas for checks/ tools? @michalk8, I saw you added this to [`squidpy`](https://github.com/theislab/squidpy/pull/203). How's the experience been there  that is, any major foot guns we should look out for? Also, are there any tools you're using (beyond the basic `black`, `isort`, `flake8`) you'd especially recommend?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1563
https://github.com/scverse/scanpy/issues/1563:16,usability,workflow,workflow,16,"Pre-commit (dev workflow); I'd like to start using [pre-commit](https://pre-commit.com) with scanpy and anndata. Pre-commit is essentially a tool that manages scripts we'd like to run before each commit, e.g. linting and formatting, so it becomes essentially impossible to forget these. I think this can allow PRs to progress faster since it gives us a way to codify formatting requirements  so we don't have to remember them  and have these checks happen locally  so we don't have to wait on CI. Of course, having these checks run depends on developers installing pre-commit, so we can also run these checks on CI ([example ci script](https://github.com/pandas-dev/pandas/blob/master/.github/workflows/pre-commit.yml), [example run](https://github.com/pandas-dev/pandas/pull/38745/checks?check_run_id=1624558250)). There is a question of what things we'd like to add here. For sure: `black`. I think import checks (e.g. no unused imports) and `flake8` would be good too. We can also add custom checks for things like slow imports. I think this would be a good time to run `black` over the whole codebase so we don't have exempted files any more. My questions for the dev team:. * Does this sound good? * Do you have more ideas for checks/ tools? @michalk8, I saw you added this to [`squidpy`](https://github.com/theislab/squidpy/pull/203). How's the experience been there  that is, any major foot guns we should look out for? Also, are there any tools you're using (beyond the basic `black`, `isort`, `flake8`) you'd especially recommend?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1563
https://github.com/scverse/scanpy/issues/1563:141,usability,tool,tool,141,"Pre-commit (dev workflow); I'd like to start using [pre-commit](https://pre-commit.com) with scanpy and anndata. Pre-commit is essentially a tool that manages scripts we'd like to run before each commit, e.g. linting and formatting, so it becomes essentially impossible to forget these. I think this can allow PRs to progress faster since it gives us a way to codify formatting requirements  so we don't have to remember them  and have these checks happen locally  so we don't have to wait on CI. Of course, having these checks run depends on developers installing pre-commit, so we can also run these checks on CI ([example ci script](https://github.com/pandas-dev/pandas/blob/master/.github/workflows/pre-commit.yml), [example run](https://github.com/pandas-dev/pandas/pull/38745/checks?check_run_id=1624558250)). There is a question of what things we'd like to add here. For sure: `black`. I think import checks (e.g. no unused imports) and `flake8` would be good too. We can also add custom checks for things like slow imports. I think this would be a good time to run `black` over the whole codebase so we don't have exempted files any more. My questions for the dev team:. * Does this sound good? * Do you have more ideas for checks/ tools? @michalk8, I saw you added this to [`squidpy`](https://github.com/theislab/squidpy/pull/203). How's the experience been there  that is, any major foot guns we should look out for? Also, are there any tools you're using (beyond the basic `black`, `isort`, `flake8`) you'd especially recommend?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1563
https://github.com/scverse/scanpy/issues/1563:317,usability,progress,progress,317,"Pre-commit (dev workflow); I'd like to start using [pre-commit](https://pre-commit.com) with scanpy and anndata. Pre-commit is essentially a tool that manages scripts we'd like to run before each commit, e.g. linting and formatting, so it becomes essentially impossible to forget these. I think this can allow PRs to progress faster since it gives us a way to codify formatting requirements  so we don't have to remember them  and have these checks happen locally  so we don't have to wait on CI. Of course, having these checks run depends on developers installing pre-commit, so we can also run these checks on CI ([example ci script](https://github.com/pandas-dev/pandas/blob/master/.github/workflows/pre-commit.yml), [example run](https://github.com/pandas-dev/pandas/pull/38745/checks?check_run_id=1624558250)). There is a question of what things we'd like to add here. For sure: `black`. I think import checks (e.g. no unused imports) and `flake8` would be good too. We can also add custom checks for things like slow imports. I think this would be a good time to run `black` over the whole codebase so we don't have exempted files any more. My questions for the dev team:. * Does this sound good? * Do you have more ideas for checks/ tools? @michalk8, I saw you added this to [`squidpy`](https://github.com/theislab/squidpy/pull/203). How's the experience been there  that is, any major foot guns we should look out for? Also, are there any tools you're using (beyond the basic `black`, `isort`, `flake8`) you'd especially recommend?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1563
https://github.com/scverse/scanpy/issues/1563:696,usability,workflow,workflows,696,"Pre-commit (dev workflow); I'd like to start using [pre-commit](https://pre-commit.com) with scanpy and anndata. Pre-commit is essentially a tool that manages scripts we'd like to run before each commit, e.g. linting and formatting, so it becomes essentially impossible to forget these. I think this can allow PRs to progress faster since it gives us a way to codify formatting requirements  so we don't have to remember them  and have these checks happen locally  so we don't have to wait on CI. Of course, having these checks run depends on developers installing pre-commit, so we can also run these checks on CI ([example ci script](https://github.com/pandas-dev/pandas/blob/master/.github/workflows/pre-commit.yml), [example run](https://github.com/pandas-dev/pandas/pull/38745/checks?check_run_id=1624558250)). There is a question of what things we'd like to add here. For sure: `black`. I think import checks (e.g. no unused imports) and `flake8` would be good too. We can also add custom checks for things like slow imports. I think this would be a good time to run `black` over the whole codebase so we don't have exempted files any more. My questions for the dev team:. * Does this sound good? * Do you have more ideas for checks/ tools? @michalk8, I saw you added this to [`squidpy`](https://github.com/theislab/squidpy/pull/203). How's the experience been there  that is, any major foot guns we should look out for? Also, are there any tools you're using (beyond the basic `black`, `isort`, `flake8`) you'd especially recommend?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1563
https://github.com/scverse/scanpy/issues/1563:991,usability,custom,custom,991,"Pre-commit (dev workflow); I'd like to start using [pre-commit](https://pre-commit.com) with scanpy and anndata. Pre-commit is essentially a tool that manages scripts we'd like to run before each commit, e.g. linting and formatting, so it becomes essentially impossible to forget these. I think this can allow PRs to progress faster since it gives us a way to codify formatting requirements  so we don't have to remember them  and have these checks happen locally  so we don't have to wait on CI. Of course, having these checks run depends on developers installing pre-commit, so we can also run these checks on CI ([example ci script](https://github.com/pandas-dev/pandas/blob/master/.github/workflows/pre-commit.yml), [example run](https://github.com/pandas-dev/pandas/pull/38745/checks?check_run_id=1624558250)). There is a question of what things we'd like to add here. For sure: `black`. I think import checks (e.g. no unused imports) and `flake8` would be good too. We can also add custom checks for things like slow imports. I think this would be a good time to run `black` over the whole codebase so we don't have exempted files any more. My questions for the dev team:. * Does this sound good? * Do you have more ideas for checks/ tools? @michalk8, I saw you added this to [`squidpy`](https://github.com/theislab/squidpy/pull/203). How's the experience been there  that is, any major foot guns we should look out for? Also, are there any tools you're using (beyond the basic `black`, `isort`, `flake8`) you'd especially recommend?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1563
https://github.com/scverse/scanpy/issues/1563:1243,usability,tool,tools,1243,"Pre-commit (dev workflow); I'd like to start using [pre-commit](https://pre-commit.com) with scanpy and anndata. Pre-commit is essentially a tool that manages scripts we'd like to run before each commit, e.g. linting and formatting, so it becomes essentially impossible to forget these. I think this can allow PRs to progress faster since it gives us a way to codify formatting requirements  so we don't have to remember them  and have these checks happen locally  so we don't have to wait on CI. Of course, having these checks run depends on developers installing pre-commit, so we can also run these checks on CI ([example ci script](https://github.com/pandas-dev/pandas/blob/master/.github/workflows/pre-commit.yml), [example run](https://github.com/pandas-dev/pandas/pull/38745/checks?check_run_id=1624558250)). There is a question of what things we'd like to add here. For sure: `black`. I think import checks (e.g. no unused imports) and `flake8` would be good too. We can also add custom checks for things like slow imports. I think this would be a good time to run `black` over the whole codebase so we don't have exempted files any more. My questions for the dev team:. * Does this sound good? * Do you have more ideas for checks/ tools? @michalk8, I saw you added this to [`squidpy`](https://github.com/theislab/squidpy/pull/203). How's the experience been there  that is, any major foot guns we should look out for? Also, are there any tools you're using (beyond the basic `black`, `isort`, `flake8`) you'd especially recommend?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1563
https://github.com/scverse/scanpy/issues/1563:1354,usability,experien,experience,1354,"Pre-commit (dev workflow); I'd like to start using [pre-commit](https://pre-commit.com) with scanpy and anndata. Pre-commit is essentially a tool that manages scripts we'd like to run before each commit, e.g. linting and formatting, so it becomes essentially impossible to forget these. I think this can allow PRs to progress faster since it gives us a way to codify formatting requirements  so we don't have to remember them  and have these checks happen locally  so we don't have to wait on CI. Of course, having these checks run depends on developers installing pre-commit, so we can also run these checks on CI ([example ci script](https://github.com/pandas-dev/pandas/blob/master/.github/workflows/pre-commit.yml), [example run](https://github.com/pandas-dev/pandas/pull/38745/checks?check_run_id=1624558250)). There is a question of what things we'd like to add here. For sure: `black`. I think import checks (e.g. no unused imports) and `flake8` would be good too. We can also add custom checks for things like slow imports. I think this would be a good time to run `black` over the whole codebase so we don't have exempted files any more. My questions for the dev team:. * Does this sound good? * Do you have more ideas for checks/ tools? @michalk8, I saw you added this to [`squidpy`](https://github.com/theislab/squidpy/pull/203). How's the experience been there  that is, any major foot guns we should look out for? Also, are there any tools you're using (beyond the basic `black`, `isort`, `flake8`) you'd especially recommend?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1563
https://github.com/scverse/scanpy/issues/1563:1451,usability,tool,tools,1451,"Pre-commit (dev workflow); I'd like to start using [pre-commit](https://pre-commit.com) with scanpy and anndata. Pre-commit is essentially a tool that manages scripts we'd like to run before each commit, e.g. linting and formatting, so it becomes essentially impossible to forget these. I think this can allow PRs to progress faster since it gives us a way to codify formatting requirements  so we don't have to remember them  and have these checks happen locally  so we don't have to wait on CI. Of course, having these checks run depends on developers installing pre-commit, so we can also run these checks on CI ([example ci script](https://github.com/pandas-dev/pandas/blob/master/.github/workflows/pre-commit.yml), [example run](https://github.com/pandas-dev/pandas/pull/38745/checks?check_run_id=1624558250)). There is a question of what things we'd like to add here. For sure: `black`. I think import checks (e.g. no unused imports) and `flake8` would be good too. We can also add custom checks for things like slow imports. I think this would be a good time to run `black` over the whole codebase so we don't have exempted files any more. My questions for the dev team:. * Does this sound good? * Do you have more ideas for checks/ tools? @michalk8, I saw you added this to [`squidpy`](https://github.com/theislab/squidpy/pull/203). How's the experience been there  that is, any major foot guns we should look out for? Also, are there any tools you're using (beyond the basic `black`, `isort`, `flake8`) you'd especially recommend?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1563
https://github.com/scverse/scanpy/pull/1564:359,deployability,pipelin,pipelines,359,"Better CI test results; Testing some azure stuff for better CI test results. Mainly:. * I don't like what `pytest-azurepipelines` does with warnings. * I'd like to have test coverage. * Azure's is a bit meh (no diffs), maybe we should use codecovs ([example usage](https://github.com/codecov/example-python/blob/74883884e480b523e0db9e92e97264908ecb9b8f/azure-pipelines.yml#L32-L34)). * I like having the test results be easy to read",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1564
https://github.com/scverse/scanpy/pull/1564:359,integrability,pipelin,pipelines,359,"Better CI test results; Testing some azure stuff for better CI test results. Mainly:. * I don't like what `pytest-azurepipelines` does with warnings. * I'd like to have test coverage. * Azure's is a bit meh (no diffs), maybe we should use codecovs ([example usage](https://github.com/codecov/example-python/blob/74883884e480b523e0db9e92e97264908ecb9b8f/azure-pipelines.yml#L32-L34)). * I like having the test results be easy to read",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1564
https://github.com/scverse/scanpy/pull/1564:130,reliability,doe,does,130,"Better CI test results; Testing some azure stuff for better CI test results. Mainly:. * I don't like what `pytest-azurepipelines` does with warnings. * I'd like to have test coverage. * Azure's is a bit meh (no diffs), maybe we should use codecovs ([example usage](https://github.com/codecov/example-python/blob/74883884e480b523e0db9e92e97264908ecb9b8f/azure-pipelines.yml#L32-L34)). * I like having the test results be easy to read",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1564
https://github.com/scverse/scanpy/pull/1564:10,safety,test,test,10,"Better CI test results; Testing some azure stuff for better CI test results. Mainly:. * I don't like what `pytest-azurepipelines` does with warnings. * I'd like to have test coverage. * Azure's is a bit meh (no diffs), maybe we should use codecovs ([example usage](https://github.com/codecov/example-python/blob/74883884e480b523e0db9e92e97264908ecb9b8f/azure-pipelines.yml#L32-L34)). * I like having the test results be easy to read",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1564
https://github.com/scverse/scanpy/pull/1564:24,safety,Test,Testing,24,"Better CI test results; Testing some azure stuff for better CI test results. Mainly:. * I don't like what `pytest-azurepipelines` does with warnings. * I'd like to have test coverage. * Azure's is a bit meh (no diffs), maybe we should use codecovs ([example usage](https://github.com/codecov/example-python/blob/74883884e480b523e0db9e92e97264908ecb9b8f/azure-pipelines.yml#L32-L34)). * I like having the test results be easy to read",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1564
https://github.com/scverse/scanpy/pull/1564:63,safety,test,test,63,"Better CI test results; Testing some azure stuff for better CI test results. Mainly:. * I don't like what `pytest-azurepipelines` does with warnings. * I'd like to have test coverage. * Azure's is a bit meh (no diffs), maybe we should use codecovs ([example usage](https://github.com/codecov/example-python/blob/74883884e480b523e0db9e92e97264908ecb9b8f/azure-pipelines.yml#L32-L34)). * I like having the test results be easy to read",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1564
https://github.com/scverse/scanpy/pull/1564:169,safety,test,test,169,"Better CI test results; Testing some azure stuff for better CI test results. Mainly:. * I don't like what `pytest-azurepipelines` does with warnings. * I'd like to have test coverage. * Azure's is a bit meh (no diffs), maybe we should use codecovs ([example usage](https://github.com/codecov/example-python/blob/74883884e480b523e0db9e92e97264908ecb9b8f/azure-pipelines.yml#L32-L34)). * I like having the test results be easy to read",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1564
https://github.com/scverse/scanpy/pull/1564:404,safety,test,test,404,"Better CI test results; Testing some azure stuff for better CI test results. Mainly:. * I don't like what `pytest-azurepipelines` does with warnings. * I'd like to have test coverage. * Azure's is a bit meh (no diffs), maybe we should use codecovs ([example usage](https://github.com/codecov/example-python/blob/74883884e480b523e0db9e92e97264908ecb9b8f/azure-pipelines.yml#L32-L34)). * I like having the test results be easy to read",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1564
https://github.com/scverse/scanpy/pull/1564:10,testability,test,test,10,"Better CI test results; Testing some azure stuff for better CI test results. Mainly:. * I don't like what `pytest-azurepipelines` does with warnings. * I'd like to have test coverage. * Azure's is a bit meh (no diffs), maybe we should use codecovs ([example usage](https://github.com/codecov/example-python/blob/74883884e480b523e0db9e92e97264908ecb9b8f/azure-pipelines.yml#L32-L34)). * I like having the test results be easy to read",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1564
https://github.com/scverse/scanpy/pull/1564:24,testability,Test,Testing,24,"Better CI test results; Testing some azure stuff for better CI test results. Mainly:. * I don't like what `pytest-azurepipelines` does with warnings. * I'd like to have test coverage. * Azure's is a bit meh (no diffs), maybe we should use codecovs ([example usage](https://github.com/codecov/example-python/blob/74883884e480b523e0db9e92e97264908ecb9b8f/azure-pipelines.yml#L32-L34)). * I like having the test results be easy to read",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1564
https://github.com/scverse/scanpy/pull/1564:63,testability,test,test,63,"Better CI test results; Testing some azure stuff for better CI test results. Mainly:. * I don't like what `pytest-azurepipelines` does with warnings. * I'd like to have test coverage. * Azure's is a bit meh (no diffs), maybe we should use codecovs ([example usage](https://github.com/codecov/example-python/blob/74883884e480b523e0db9e92e97264908ecb9b8f/azure-pipelines.yml#L32-L34)). * I like having the test results be easy to read",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1564
https://github.com/scverse/scanpy/pull/1564:169,testability,test,test,169,"Better CI test results; Testing some azure stuff for better CI test results. Mainly:. * I don't like what `pytest-azurepipelines` does with warnings. * I'd like to have test coverage. * Azure's is a bit meh (no diffs), maybe we should use codecovs ([example usage](https://github.com/codecov/example-python/blob/74883884e480b523e0db9e92e97264908ecb9b8f/azure-pipelines.yml#L32-L34)). * I like having the test results be easy to read",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1564
https://github.com/scverse/scanpy/pull/1564:174,testability,coverag,coverage,174,"Better CI test results; Testing some azure stuff for better CI test results. Mainly:. * I don't like what `pytest-azurepipelines` does with warnings. * I'd like to have test coverage. * Azure's is a bit meh (no diffs), maybe we should use codecovs ([example usage](https://github.com/codecov/example-python/blob/74883884e480b523e0db9e92e97264908ecb9b8f/azure-pipelines.yml#L32-L34)). * I like having the test results be easy to read",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1564
https://github.com/scverse/scanpy/pull/1564:404,testability,test,test,404,"Better CI test results; Testing some azure stuff for better CI test results. Mainly:. * I don't like what `pytest-azurepipelines` does with warnings. * I'd like to have test coverage. * Azure's is a bit meh (no diffs), maybe we should use codecovs ([example usage](https://github.com/codecov/example-python/blob/74883884e480b523e0db9e92e97264908ecb9b8f/azure-pipelines.yml#L32-L34)). * I like having the test results be easy to read",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1564
https://github.com/scverse/scanpy/issues/1565:72,availability,cluster,clusters,72,"split view; Hi,. Cellranger supports split view according to smaples or clusters, they have the same x-axis range and y-axis range, then we can see the genes expression on split view. Does scanpy support this function ? ![split-view](https://user-images.githubusercontent.com/29703450/103519656-dfbf5100-4eb0-11eb-92a2-473fac842017.png).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1565
https://github.com/scverse/scanpy/issues/1565:72,deployability,cluster,clusters,72,"split view; Hi,. Cellranger supports split view according to smaples or clusters, they have the same x-axis range and y-axis range, then we can see the genes expression on split view. Does scanpy support this function ? ![split-view](https://user-images.githubusercontent.com/29703450/103519656-dfbf5100-4eb0-11eb-92a2-473fac842017.png).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1565
https://github.com/scverse/scanpy/issues/1565:184,reliability,Doe,Does,184,"split view; Hi,. Cellranger supports split view according to smaples or clusters, they have the same x-axis range and y-axis range, then we can see the genes expression on split view. Does scanpy support this function ? ![split-view](https://user-images.githubusercontent.com/29703450/103519656-dfbf5100-4eb0-11eb-92a2-473fac842017.png).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1565
https://github.com/scverse/scanpy/issues/1565:28,usability,support,supports,28,"split view; Hi,. Cellranger supports split view according to smaples or clusters, they have the same x-axis range and y-axis range, then we can see the genes expression on split view. Does scanpy support this function ? ![split-view](https://user-images.githubusercontent.com/29703450/103519656-dfbf5100-4eb0-11eb-92a2-473fac842017.png).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1565
https://github.com/scverse/scanpy/issues/1565:196,usability,support,support,196,"split view; Hi,. Cellranger supports split view according to smaples or clusters, they have the same x-axis range and y-axis range, then we can see the genes expression on split view. Does scanpy support this function ? ![split-view](https://user-images.githubusercontent.com/29703450/103519656-dfbf5100-4eb0-11eb-92a2-473fac842017.png).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1565
https://github.com/scverse/scanpy/issues/1565:242,usability,user,user-images,242,"split view; Hi,. Cellranger supports split view according to smaples or clusters, they have the same x-axis range and y-axis range, then we can see the genes expression on split view. Does scanpy support this function ? ![split-view](https://user-images.githubusercontent.com/29703450/103519656-dfbf5100-4eb0-11eb-92a2-473fac842017.png).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1565
https://github.com/scverse/scanpy/issues/1566:74,availability,error,error,74,"hello, I am trying to use normalisation part of scanpy and encounter this error. I tried to install Louvain but it is not helping. could anyone please help me. Thank you ; - [ ] I have checked that this issue has not already been reported. - [ ] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. ### Minimal code sample (that we can copy&paste without having any data). ```python. #Perform a clustering for scran normalization in clusters. adata_pp = adata.copy(). sc.pp.normalize_per_cell(adata_pp, counts_per_cell_after=1e6). sc.pp.log1p(adata_pp). sc.pp.pca(adata_pp, n_comps=15). sc.pp.neighbors(adata_pp). sc.tl.louvain(adata_pp, key_added='groups', resolution=0.5). ```. ```pytb. ModuleNotFoundError Traceback (most recent call last). <ipython-input-11-785c54721f17> in <module>. 8 sc.pp.pca(adata_pp, n_comps=15). 9 sc.pp.neighbors(adata_pp). ---> 10 sc.tl.louvain(adata_pp, key_added='groups', resolution=0.5). ~\anaconda3\lib\site-packages\scanpy\tools\_louvain.py in louvain(adata, resolution, random_state, restrict_to, key_added, adjacency, flavor, directed, use_weights, partition_type, partition_kwargs, neighbors_key, obsp, copy). 135 weights = None. 136 if flavor == 'vtraag':. --> 137 import louvain. 138 if partition_type is None:. 139 partition_type = louvain.RBConfigurationVertexPartition. ModuleNotFoundError: No module named 'louvain'. ```. #### Versions. <details>. [Paste the output of scanpy.logging.print_versions() leaving a blank line after the details tag]. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1566
https://github.com/scverse/scanpy/issues/1566:680,availability,cluster,clustering,680,"hello, I am trying to use normalisation part of scanpy and encounter this error. I tried to install Louvain but it is not helping. could anyone please help me. Thank you ; - [ ] I have checked that this issue has not already been reported. - [ ] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. ### Minimal code sample (that we can copy&paste without having any data). ```python. #Perform a clustering for scran normalization in clusters. adata_pp = adata.copy(). sc.pp.normalize_per_cell(adata_pp, counts_per_cell_after=1e6). sc.pp.log1p(adata_pp). sc.pp.pca(adata_pp, n_comps=15). sc.pp.neighbors(adata_pp). sc.tl.louvain(adata_pp, key_added='groups', resolution=0.5). ```. ```pytb. ModuleNotFoundError Traceback (most recent call last). <ipython-input-11-785c54721f17> in <module>. 8 sc.pp.pca(adata_pp, n_comps=15). 9 sc.pp.neighbors(adata_pp). ---> 10 sc.tl.louvain(adata_pp, key_added='groups', resolution=0.5). ~\anaconda3\lib\site-packages\scanpy\tools\_louvain.py in louvain(adata, resolution, random_state, restrict_to, key_added, adjacency, flavor, directed, use_weights, partition_type, partition_kwargs, neighbors_key, obsp, copy). 135 weights = None. 136 if flavor == 'vtraag':. --> 137 import louvain. 138 if partition_type is None:. 139 partition_type = louvain.RBConfigurationVertexPartition. ModuleNotFoundError: No module named 'louvain'. ```. #### Versions. <details>. [Paste the output of scanpy.logging.print_versions() leaving a blank line after the details tag]. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1566
https://github.com/scverse/scanpy/issues/1566:718,availability,cluster,clusters,718,"hello, I am trying to use normalisation part of scanpy and encounter this error. I tried to install Louvain but it is not helping. could anyone please help me. Thank you ; - [ ] I have checked that this issue has not already been reported. - [ ] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. ### Minimal code sample (that we can copy&paste without having any data). ```python. #Perform a clustering for scran normalization in clusters. adata_pp = adata.copy(). sc.pp.normalize_per_cell(adata_pp, counts_per_cell_after=1e6). sc.pp.log1p(adata_pp). sc.pp.pca(adata_pp, n_comps=15). sc.pp.neighbors(adata_pp). sc.tl.louvain(adata_pp, key_added='groups', resolution=0.5). ```. ```pytb. ModuleNotFoundError Traceback (most recent call last). <ipython-input-11-785c54721f17> in <module>. 8 sc.pp.pca(adata_pp, n_comps=15). 9 sc.pp.neighbors(adata_pp). ---> 10 sc.tl.louvain(adata_pp, key_added='groups', resolution=0.5). ~\anaconda3\lib\site-packages\scanpy\tools\_louvain.py in louvain(adata, resolution, random_state, restrict_to, key_added, adjacency, flavor, directed, use_weights, partition_type, partition_kwargs, neighbors_key, obsp, copy). 135 weights = None. 136 if flavor == 'vtraag':. --> 137 import louvain. 138 if partition_type is None:. 139 partition_type = louvain.RBConfigurationVertexPartition. ModuleNotFoundError: No module named 'louvain'. ```. #### Versions. <details>. [Paste the output of scanpy.logging.print_versions() leaving a blank line after the details tag]. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1566
https://github.com/scverse/scanpy/issues/1566:92,deployability,instal,install,92,"hello, I am trying to use normalisation part of scanpy and encounter this error. I tried to install Louvain but it is not helping. could anyone please help me. Thank you ; - [ ] I have checked that this issue has not already been reported. - [ ] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. ### Minimal code sample (that we can copy&paste without having any data). ```python. #Perform a clustering for scran normalization in clusters. adata_pp = adata.copy(). sc.pp.normalize_per_cell(adata_pp, counts_per_cell_after=1e6). sc.pp.log1p(adata_pp). sc.pp.pca(adata_pp, n_comps=15). sc.pp.neighbors(adata_pp). sc.tl.louvain(adata_pp, key_added='groups', resolution=0.5). ```. ```pytb. ModuleNotFoundError Traceback (most recent call last). <ipython-input-11-785c54721f17> in <module>. 8 sc.pp.pca(adata_pp, n_comps=15). 9 sc.pp.neighbors(adata_pp). ---> 10 sc.tl.louvain(adata_pp, key_added='groups', resolution=0.5). ~\anaconda3\lib\site-packages\scanpy\tools\_louvain.py in louvain(adata, resolution, random_state, restrict_to, key_added, adjacency, flavor, directed, use_weights, partition_type, partition_kwargs, neighbors_key, obsp, copy). 135 weights = None. 136 if flavor == 'vtraag':. --> 137 import louvain. 138 if partition_type is None:. 139 partition_type = louvain.RBConfigurationVertexPartition. ModuleNotFoundError: No module named 'louvain'. ```. #### Versions. <details>. [Paste the output of scanpy.logging.print_versions() leaving a blank line after the details tag]. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1566
https://github.com/scverse/scanpy/issues/1566:293,deployability,version,version,293,"hello, I am trying to use normalisation part of scanpy and encounter this error. I tried to install Louvain but it is not helping. could anyone please help me. Thank you ; - [ ] I have checked that this issue has not already been reported. - [ ] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. ### Minimal code sample (that we can copy&paste without having any data). ```python. #Perform a clustering for scran normalization in clusters. adata_pp = adata.copy(). sc.pp.normalize_per_cell(adata_pp, counts_per_cell_after=1e6). sc.pp.log1p(adata_pp). sc.pp.pca(adata_pp, n_comps=15). sc.pp.neighbors(adata_pp). sc.tl.louvain(adata_pp, key_added='groups', resolution=0.5). ```. ```pytb. ModuleNotFoundError Traceback (most recent call last). <ipython-input-11-785c54721f17> in <module>. 8 sc.pp.pca(adata_pp, n_comps=15). 9 sc.pp.neighbors(adata_pp). ---> 10 sc.tl.louvain(adata_pp, key_added='groups', resolution=0.5). ~\anaconda3\lib\site-packages\scanpy\tools\_louvain.py in louvain(adata, resolution, random_state, restrict_to, key_added, adjacency, flavor, directed, use_weights, partition_type, partition_kwargs, neighbors_key, obsp, copy). 135 weights = None. 136 if flavor == 'vtraag':. --> 137 import louvain. 138 if partition_type is None:. 139 partition_type = louvain.RBConfigurationVertexPartition. ModuleNotFoundError: No module named 'louvain'. ```. #### Versions. <details>. [Paste the output of scanpy.logging.print_versions() leaving a blank line after the details tag]. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1566
https://github.com/scverse/scanpy/issues/1566:680,deployability,cluster,clustering,680,"hello, I am trying to use normalisation part of scanpy and encounter this error. I tried to install Louvain but it is not helping. could anyone please help me. Thank you ; - [ ] I have checked that this issue has not already been reported. - [ ] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. ### Minimal code sample (that we can copy&paste without having any data). ```python. #Perform a clustering for scran normalization in clusters. adata_pp = adata.copy(). sc.pp.normalize_per_cell(adata_pp, counts_per_cell_after=1e6). sc.pp.log1p(adata_pp). sc.pp.pca(adata_pp, n_comps=15). sc.pp.neighbors(adata_pp). sc.tl.louvain(adata_pp, key_added='groups', resolution=0.5). ```. ```pytb. ModuleNotFoundError Traceback (most recent call last). <ipython-input-11-785c54721f17> in <module>. 8 sc.pp.pca(adata_pp, n_comps=15). 9 sc.pp.neighbors(adata_pp). ---> 10 sc.tl.louvain(adata_pp, key_added='groups', resolution=0.5). ~\anaconda3\lib\site-packages\scanpy\tools\_louvain.py in louvain(adata, resolution, random_state, restrict_to, key_added, adjacency, flavor, directed, use_weights, partition_type, partition_kwargs, neighbors_key, obsp, copy). 135 weights = None. 136 if flavor == 'vtraag':. --> 137 import louvain. 138 if partition_type is None:. 139 partition_type = louvain.RBConfigurationVertexPartition. ModuleNotFoundError: No module named 'louvain'. ```. #### Versions. <details>. [Paste the output of scanpy.logging.print_versions() leaving a blank line after the details tag]. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1566
https://github.com/scverse/scanpy/issues/1566:718,deployability,cluster,clusters,718,"hello, I am trying to use normalisation part of scanpy and encounter this error. I tried to install Louvain but it is not helping. could anyone please help me. Thank you ; - [ ] I have checked that this issue has not already been reported. - [ ] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. ### Minimal code sample (that we can copy&paste without having any data). ```python. #Perform a clustering for scran normalization in clusters. adata_pp = adata.copy(). sc.pp.normalize_per_cell(adata_pp, counts_per_cell_after=1e6). sc.pp.log1p(adata_pp). sc.pp.pca(adata_pp, n_comps=15). sc.pp.neighbors(adata_pp). sc.tl.louvain(adata_pp, key_added='groups', resolution=0.5). ```. ```pytb. ModuleNotFoundError Traceback (most recent call last). <ipython-input-11-785c54721f17> in <module>. 8 sc.pp.pca(adata_pp, n_comps=15). 9 sc.pp.neighbors(adata_pp). ---> 10 sc.tl.louvain(adata_pp, key_added='groups', resolution=0.5). ~\anaconda3\lib\site-packages\scanpy\tools\_louvain.py in louvain(adata, resolution, random_state, restrict_to, key_added, adjacency, flavor, directed, use_weights, partition_type, partition_kwargs, neighbors_key, obsp, copy). 135 weights = None. 136 if flavor == 'vtraag':. --> 137 import louvain. 138 if partition_type is None:. 139 partition_type = louvain.RBConfigurationVertexPartition. ModuleNotFoundError: No module named 'louvain'. ```. #### Versions. <details>. [Paste the output of scanpy.logging.print_versions() leaving a blank line after the details tag]. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1566
https://github.com/scverse/scanpy/issues/1566:974,deployability,Modul,ModuleNotFoundError,974,"hello, I am trying to use normalisation part of scanpy and encounter this error. I tried to install Louvain but it is not helping. could anyone please help me. Thank you ; - [ ] I have checked that this issue has not already been reported. - [ ] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. ### Minimal code sample (that we can copy&paste without having any data). ```python. #Perform a clustering for scran normalization in clusters. adata_pp = adata.copy(). sc.pp.normalize_per_cell(adata_pp, counts_per_cell_after=1e6). sc.pp.log1p(adata_pp). sc.pp.pca(adata_pp, n_comps=15). sc.pp.neighbors(adata_pp). sc.tl.louvain(adata_pp, key_added='groups', resolution=0.5). ```. ```pytb. ModuleNotFoundError Traceback (most recent call last). <ipython-input-11-785c54721f17> in <module>. 8 sc.pp.pca(adata_pp, n_comps=15). 9 sc.pp.neighbors(adata_pp). ---> 10 sc.tl.louvain(adata_pp, key_added='groups', resolution=0.5). ~\anaconda3\lib\site-packages\scanpy\tools\_louvain.py in louvain(adata, resolution, random_state, restrict_to, key_added, adjacency, flavor, directed, use_weights, partition_type, partition_kwargs, neighbors_key, obsp, copy). 135 weights = None. 136 if flavor == 'vtraag':. --> 137 import louvain. 138 if partition_type is None:. 139 partition_type = louvain.RBConfigurationVertexPartition. ModuleNotFoundError: No module named 'louvain'. ```. #### Versions. <details>. [Paste the output of scanpy.logging.print_versions() leaving a blank line after the details tag]. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1566
https://github.com/scverse/scanpy/issues/1566:1065,deployability,modul,module,1065,"hello, I am trying to use normalisation part of scanpy and encounter this error. I tried to install Louvain but it is not helping. could anyone please help me. Thank you ; - [ ] I have checked that this issue has not already been reported. - [ ] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. ### Minimal code sample (that we can copy&paste without having any data). ```python. #Perform a clustering for scran normalization in clusters. adata_pp = adata.copy(). sc.pp.normalize_per_cell(adata_pp, counts_per_cell_after=1e6). sc.pp.log1p(adata_pp). sc.pp.pca(adata_pp, n_comps=15). sc.pp.neighbors(adata_pp). sc.tl.louvain(adata_pp, key_added='groups', resolution=0.5). ```. ```pytb. ModuleNotFoundError Traceback (most recent call last). <ipython-input-11-785c54721f17> in <module>. 8 sc.pp.pca(adata_pp, n_comps=15). 9 sc.pp.neighbors(adata_pp). ---> 10 sc.tl.louvain(adata_pp, key_added='groups', resolution=0.5). ~\anaconda3\lib\site-packages\scanpy\tools\_louvain.py in louvain(adata, resolution, random_state, restrict_to, key_added, adjacency, flavor, directed, use_weights, partition_type, partition_kwargs, neighbors_key, obsp, copy). 135 weights = None. 136 if flavor == 'vtraag':. --> 137 import louvain. 138 if partition_type is None:. 139 partition_type = louvain.RBConfigurationVertexPartition. ModuleNotFoundError: No module named 'louvain'. ```. #### Versions. <details>. [Paste the output of scanpy.logging.print_versions() leaving a blank line after the details tag]. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1566
https://github.com/scverse/scanpy/issues/1566:1599,deployability,Modul,ModuleNotFoundError,1599,"hello, I am trying to use normalisation part of scanpy and encounter this error. I tried to install Louvain but it is not helping. could anyone please help me. Thank you ; - [ ] I have checked that this issue has not already been reported. - [ ] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. ### Minimal code sample (that we can copy&paste without having any data). ```python. #Perform a clustering for scran normalization in clusters. adata_pp = adata.copy(). sc.pp.normalize_per_cell(adata_pp, counts_per_cell_after=1e6). sc.pp.log1p(adata_pp). sc.pp.pca(adata_pp, n_comps=15). sc.pp.neighbors(adata_pp). sc.tl.louvain(adata_pp, key_added='groups', resolution=0.5). ```. ```pytb. ModuleNotFoundError Traceback (most recent call last). <ipython-input-11-785c54721f17> in <module>. 8 sc.pp.pca(adata_pp, n_comps=15). 9 sc.pp.neighbors(adata_pp). ---> 10 sc.tl.louvain(adata_pp, key_added='groups', resolution=0.5). ~\anaconda3\lib\site-packages\scanpy\tools\_louvain.py in louvain(adata, resolution, random_state, restrict_to, key_added, adjacency, flavor, directed, use_weights, partition_type, partition_kwargs, neighbors_key, obsp, copy). 135 weights = None. 136 if flavor == 'vtraag':. --> 137 import louvain. 138 if partition_type is None:. 139 partition_type = louvain.RBConfigurationVertexPartition. ModuleNotFoundError: No module named 'louvain'. ```. #### Versions. <details>. [Paste the output of scanpy.logging.print_versions() leaving a blank line after the details tag]. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1566
https://github.com/scverse/scanpy/issues/1566:1623,deployability,modul,module,1623,"hello, I am trying to use normalisation part of scanpy and encounter this error. I tried to install Louvain but it is not helping. could anyone please help me. Thank you ; - [ ] I have checked that this issue has not already been reported. - [ ] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. ### Minimal code sample (that we can copy&paste without having any data). ```python. #Perform a clustering for scran normalization in clusters. adata_pp = adata.copy(). sc.pp.normalize_per_cell(adata_pp, counts_per_cell_after=1e6). sc.pp.log1p(adata_pp). sc.pp.pca(adata_pp, n_comps=15). sc.pp.neighbors(adata_pp). sc.tl.louvain(adata_pp, key_added='groups', resolution=0.5). ```. ```pytb. ModuleNotFoundError Traceback (most recent call last). <ipython-input-11-785c54721f17> in <module>. 8 sc.pp.pca(adata_pp, n_comps=15). 9 sc.pp.neighbors(adata_pp). ---> 10 sc.tl.louvain(adata_pp, key_added='groups', resolution=0.5). ~\anaconda3\lib\site-packages\scanpy\tools\_louvain.py in louvain(adata, resolution, random_state, restrict_to, key_added, adjacency, flavor, directed, use_weights, partition_type, partition_kwargs, neighbors_key, obsp, copy). 135 weights = None. 136 if flavor == 'vtraag':. --> 137 import louvain. 138 if partition_type is None:. 139 partition_type = louvain.RBConfigurationVertexPartition. ModuleNotFoundError: No module named 'louvain'. ```. #### Versions. <details>. [Paste the output of scanpy.logging.print_versions() leaving a blank line after the details tag]. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1566
https://github.com/scverse/scanpy/issues/1566:1657,deployability,Version,Versions,1657,"hello, I am trying to use normalisation part of scanpy and encounter this error. I tried to install Louvain but it is not helping. could anyone please help me. Thank you ; - [ ] I have checked that this issue has not already been reported. - [ ] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. ### Minimal code sample (that we can copy&paste without having any data). ```python. #Perform a clustering for scran normalization in clusters. adata_pp = adata.copy(). sc.pp.normalize_per_cell(adata_pp, counts_per_cell_after=1e6). sc.pp.log1p(adata_pp). sc.pp.pca(adata_pp, n_comps=15). sc.pp.neighbors(adata_pp). sc.tl.louvain(adata_pp, key_added='groups', resolution=0.5). ```. ```pytb. ModuleNotFoundError Traceback (most recent call last). <ipython-input-11-785c54721f17> in <module>. 8 sc.pp.pca(adata_pp, n_comps=15). 9 sc.pp.neighbors(adata_pp). ---> 10 sc.tl.louvain(adata_pp, key_added='groups', resolution=0.5). ~\anaconda3\lib\site-packages\scanpy\tools\_louvain.py in louvain(adata, resolution, random_state, restrict_to, key_added, adjacency, flavor, directed, use_weights, partition_type, partition_kwargs, neighbors_key, obsp, copy). 135 weights = None. 136 if flavor == 'vtraag':. --> 137 import louvain. 138 if partition_type is None:. 139 partition_type = louvain.RBConfigurationVertexPartition. ModuleNotFoundError: No module named 'louvain'. ```. #### Versions. <details>. [Paste the output of scanpy.logging.print_versions() leaving a blank line after the details tag]. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1566
https://github.com/scverse/scanpy/issues/1566:1706,deployability,log,logging,1706,"hello, I am trying to use normalisation part of scanpy and encounter this error. I tried to install Louvain but it is not helping. could anyone please help me. Thank you ; - [ ] I have checked that this issue has not already been reported. - [ ] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. ### Minimal code sample (that we can copy&paste without having any data). ```python. #Perform a clustering for scran normalization in clusters. adata_pp = adata.copy(). sc.pp.normalize_per_cell(adata_pp, counts_per_cell_after=1e6). sc.pp.log1p(adata_pp). sc.pp.pca(adata_pp, n_comps=15). sc.pp.neighbors(adata_pp). sc.tl.louvain(adata_pp, key_added='groups', resolution=0.5). ```. ```pytb. ModuleNotFoundError Traceback (most recent call last). <ipython-input-11-785c54721f17> in <module>. 8 sc.pp.pca(adata_pp, n_comps=15). 9 sc.pp.neighbors(adata_pp). ---> 10 sc.tl.louvain(adata_pp, key_added='groups', resolution=0.5). ~\anaconda3\lib\site-packages\scanpy\tools\_louvain.py in louvain(adata, resolution, random_state, restrict_to, key_added, adjacency, flavor, directed, use_weights, partition_type, partition_kwargs, neighbors_key, obsp, copy). 135 weights = None. 136 if flavor == 'vtraag':. --> 137 import louvain. 138 if partition_type is None:. 139 partition_type = louvain.RBConfigurationVertexPartition. ModuleNotFoundError: No module named 'louvain'. ```. #### Versions. <details>. [Paste the output of scanpy.logging.print_versions() leaving a blank line after the details tag]. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1566
https://github.com/scverse/scanpy/issues/1566:293,integrability,version,version,293,"hello, I am trying to use normalisation part of scanpy and encounter this error. I tried to install Louvain but it is not helping. could anyone please help me. Thank you ; - [ ] I have checked that this issue has not already been reported. - [ ] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. ### Minimal code sample (that we can copy&paste without having any data). ```python. #Perform a clustering for scran normalization in clusters. adata_pp = adata.copy(). sc.pp.normalize_per_cell(adata_pp, counts_per_cell_after=1e6). sc.pp.log1p(adata_pp). sc.pp.pca(adata_pp, n_comps=15). sc.pp.neighbors(adata_pp). sc.tl.louvain(adata_pp, key_added='groups', resolution=0.5). ```. ```pytb. ModuleNotFoundError Traceback (most recent call last). <ipython-input-11-785c54721f17> in <module>. 8 sc.pp.pca(adata_pp, n_comps=15). 9 sc.pp.neighbors(adata_pp). ---> 10 sc.tl.louvain(adata_pp, key_added='groups', resolution=0.5). ~\anaconda3\lib\site-packages\scanpy\tools\_louvain.py in louvain(adata, resolution, random_state, restrict_to, key_added, adjacency, flavor, directed, use_weights, partition_type, partition_kwargs, neighbors_key, obsp, copy). 135 weights = None. 136 if flavor == 'vtraag':. --> 137 import louvain. 138 if partition_type is None:. 139 partition_type = louvain.RBConfigurationVertexPartition. ModuleNotFoundError: No module named 'louvain'. ```. #### Versions. <details>. [Paste the output of scanpy.logging.print_versions() leaving a blank line after the details tag]. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1566
https://github.com/scverse/scanpy/issues/1566:1657,integrability,Version,Versions,1657,"hello, I am trying to use normalisation part of scanpy and encounter this error. I tried to install Louvain but it is not helping. could anyone please help me. Thank you ; - [ ] I have checked that this issue has not already been reported. - [ ] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. ### Minimal code sample (that we can copy&paste without having any data). ```python. #Perform a clustering for scran normalization in clusters. adata_pp = adata.copy(). sc.pp.normalize_per_cell(adata_pp, counts_per_cell_after=1e6). sc.pp.log1p(adata_pp). sc.pp.pca(adata_pp, n_comps=15). sc.pp.neighbors(adata_pp). sc.tl.louvain(adata_pp, key_added='groups', resolution=0.5). ```. ```pytb. ModuleNotFoundError Traceback (most recent call last). <ipython-input-11-785c54721f17> in <module>. 8 sc.pp.pca(adata_pp, n_comps=15). 9 sc.pp.neighbors(adata_pp). ---> 10 sc.tl.louvain(adata_pp, key_added='groups', resolution=0.5). ~\anaconda3\lib\site-packages\scanpy\tools\_louvain.py in louvain(adata, resolution, random_state, restrict_to, key_added, adjacency, flavor, directed, use_weights, partition_type, partition_kwargs, neighbors_key, obsp, copy). 135 weights = None. 136 if flavor == 'vtraag':. --> 137 import louvain. 138 if partition_type is None:. 139 partition_type = louvain.RBConfigurationVertexPartition. ModuleNotFoundError: No module named 'louvain'. ```. #### Versions. <details>. [Paste the output of scanpy.logging.print_versions() leaving a blank line after the details tag]. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1566
https://github.com/scverse/scanpy/issues/1566:293,modifiability,version,version,293,"hello, I am trying to use normalisation part of scanpy and encounter this error. I tried to install Louvain but it is not helping. could anyone please help me. Thank you ; - [ ] I have checked that this issue has not already been reported. - [ ] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. ### Minimal code sample (that we can copy&paste without having any data). ```python. #Perform a clustering for scran normalization in clusters. adata_pp = adata.copy(). sc.pp.normalize_per_cell(adata_pp, counts_per_cell_after=1e6). sc.pp.log1p(adata_pp). sc.pp.pca(adata_pp, n_comps=15). sc.pp.neighbors(adata_pp). sc.tl.louvain(adata_pp, key_added='groups', resolution=0.5). ```. ```pytb. ModuleNotFoundError Traceback (most recent call last). <ipython-input-11-785c54721f17> in <module>. 8 sc.pp.pca(adata_pp, n_comps=15). 9 sc.pp.neighbors(adata_pp). ---> 10 sc.tl.louvain(adata_pp, key_added='groups', resolution=0.5). ~\anaconda3\lib\site-packages\scanpy\tools\_louvain.py in louvain(adata, resolution, random_state, restrict_to, key_added, adjacency, flavor, directed, use_weights, partition_type, partition_kwargs, neighbors_key, obsp, copy). 135 weights = None. 136 if flavor == 'vtraag':. --> 137 import louvain. 138 if partition_type is None:. 139 partition_type = louvain.RBConfigurationVertexPartition. ModuleNotFoundError: No module named 'louvain'. ```. #### Versions. <details>. [Paste the output of scanpy.logging.print_versions() leaving a blank line after the details tag]. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1566
https://github.com/scverse/scanpy/issues/1566:974,modifiability,Modul,ModuleNotFoundError,974,"hello, I am trying to use normalisation part of scanpy and encounter this error. I tried to install Louvain but it is not helping. could anyone please help me. Thank you ; - [ ] I have checked that this issue has not already been reported. - [ ] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. ### Minimal code sample (that we can copy&paste without having any data). ```python. #Perform a clustering for scran normalization in clusters. adata_pp = adata.copy(). sc.pp.normalize_per_cell(adata_pp, counts_per_cell_after=1e6). sc.pp.log1p(adata_pp). sc.pp.pca(adata_pp, n_comps=15). sc.pp.neighbors(adata_pp). sc.tl.louvain(adata_pp, key_added='groups', resolution=0.5). ```. ```pytb. ModuleNotFoundError Traceback (most recent call last). <ipython-input-11-785c54721f17> in <module>. 8 sc.pp.pca(adata_pp, n_comps=15). 9 sc.pp.neighbors(adata_pp). ---> 10 sc.tl.louvain(adata_pp, key_added='groups', resolution=0.5). ~\anaconda3\lib\site-packages\scanpy\tools\_louvain.py in louvain(adata, resolution, random_state, restrict_to, key_added, adjacency, flavor, directed, use_weights, partition_type, partition_kwargs, neighbors_key, obsp, copy). 135 weights = None. 136 if flavor == 'vtraag':. --> 137 import louvain. 138 if partition_type is None:. 139 partition_type = louvain.RBConfigurationVertexPartition. ModuleNotFoundError: No module named 'louvain'. ```. #### Versions. <details>. [Paste the output of scanpy.logging.print_versions() leaving a blank line after the details tag]. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1566
https://github.com/scverse/scanpy/issues/1566:1065,modifiability,modul,module,1065,"hello, I am trying to use normalisation part of scanpy and encounter this error. I tried to install Louvain but it is not helping. could anyone please help me. Thank you ; - [ ] I have checked that this issue has not already been reported. - [ ] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. ### Minimal code sample (that we can copy&paste without having any data). ```python. #Perform a clustering for scran normalization in clusters. adata_pp = adata.copy(). sc.pp.normalize_per_cell(adata_pp, counts_per_cell_after=1e6). sc.pp.log1p(adata_pp). sc.pp.pca(adata_pp, n_comps=15). sc.pp.neighbors(adata_pp). sc.tl.louvain(adata_pp, key_added='groups', resolution=0.5). ```. ```pytb. ModuleNotFoundError Traceback (most recent call last). <ipython-input-11-785c54721f17> in <module>. 8 sc.pp.pca(adata_pp, n_comps=15). 9 sc.pp.neighbors(adata_pp). ---> 10 sc.tl.louvain(adata_pp, key_added='groups', resolution=0.5). ~\anaconda3\lib\site-packages\scanpy\tools\_louvain.py in louvain(adata, resolution, random_state, restrict_to, key_added, adjacency, flavor, directed, use_weights, partition_type, partition_kwargs, neighbors_key, obsp, copy). 135 weights = None. 136 if flavor == 'vtraag':. --> 137 import louvain. 138 if partition_type is None:. 139 partition_type = louvain.RBConfigurationVertexPartition. ModuleNotFoundError: No module named 'louvain'. ```. #### Versions. <details>. [Paste the output of scanpy.logging.print_versions() leaving a blank line after the details tag]. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1566
https://github.com/scverse/scanpy/issues/1566:1228,modifiability,pac,packages,1228,"hello, I am trying to use normalisation part of scanpy and encounter this error. I tried to install Louvain but it is not helping. could anyone please help me. Thank you ; - [ ] I have checked that this issue has not already been reported. - [ ] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. ### Minimal code sample (that we can copy&paste without having any data). ```python. #Perform a clustering for scran normalization in clusters. adata_pp = adata.copy(). sc.pp.normalize_per_cell(adata_pp, counts_per_cell_after=1e6). sc.pp.log1p(adata_pp). sc.pp.pca(adata_pp, n_comps=15). sc.pp.neighbors(adata_pp). sc.tl.louvain(adata_pp, key_added='groups', resolution=0.5). ```. ```pytb. ModuleNotFoundError Traceback (most recent call last). <ipython-input-11-785c54721f17> in <module>. 8 sc.pp.pca(adata_pp, n_comps=15). 9 sc.pp.neighbors(adata_pp). ---> 10 sc.tl.louvain(adata_pp, key_added='groups', resolution=0.5). ~\anaconda3\lib\site-packages\scanpy\tools\_louvain.py in louvain(adata, resolution, random_state, restrict_to, key_added, adjacency, flavor, directed, use_weights, partition_type, partition_kwargs, neighbors_key, obsp, copy). 135 weights = None. 136 if flavor == 'vtraag':. --> 137 import louvain. 138 if partition_type is None:. 139 partition_type = louvain.RBConfigurationVertexPartition. ModuleNotFoundError: No module named 'louvain'. ```. #### Versions. <details>. [Paste the output of scanpy.logging.print_versions() leaving a blank line after the details tag]. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1566
https://github.com/scverse/scanpy/issues/1566:1599,modifiability,Modul,ModuleNotFoundError,1599,"hello, I am trying to use normalisation part of scanpy and encounter this error. I tried to install Louvain but it is not helping. could anyone please help me. Thank you ; - [ ] I have checked that this issue has not already been reported. - [ ] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. ### Minimal code sample (that we can copy&paste without having any data). ```python. #Perform a clustering for scran normalization in clusters. adata_pp = adata.copy(). sc.pp.normalize_per_cell(adata_pp, counts_per_cell_after=1e6). sc.pp.log1p(adata_pp). sc.pp.pca(adata_pp, n_comps=15). sc.pp.neighbors(adata_pp). sc.tl.louvain(adata_pp, key_added='groups', resolution=0.5). ```. ```pytb. ModuleNotFoundError Traceback (most recent call last). <ipython-input-11-785c54721f17> in <module>. 8 sc.pp.pca(adata_pp, n_comps=15). 9 sc.pp.neighbors(adata_pp). ---> 10 sc.tl.louvain(adata_pp, key_added='groups', resolution=0.5). ~\anaconda3\lib\site-packages\scanpy\tools\_louvain.py in louvain(adata, resolution, random_state, restrict_to, key_added, adjacency, flavor, directed, use_weights, partition_type, partition_kwargs, neighbors_key, obsp, copy). 135 weights = None. 136 if flavor == 'vtraag':. --> 137 import louvain. 138 if partition_type is None:. 139 partition_type = louvain.RBConfigurationVertexPartition. ModuleNotFoundError: No module named 'louvain'. ```. #### Versions. <details>. [Paste the output of scanpy.logging.print_versions() leaving a blank line after the details tag]. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1566
https://github.com/scverse/scanpy/issues/1566:1623,modifiability,modul,module,1623,"hello, I am trying to use normalisation part of scanpy and encounter this error. I tried to install Louvain but it is not helping. could anyone please help me. Thank you ; - [ ] I have checked that this issue has not already been reported. - [ ] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. ### Minimal code sample (that we can copy&paste without having any data). ```python. #Perform a clustering for scran normalization in clusters. adata_pp = adata.copy(). sc.pp.normalize_per_cell(adata_pp, counts_per_cell_after=1e6). sc.pp.log1p(adata_pp). sc.pp.pca(adata_pp, n_comps=15). sc.pp.neighbors(adata_pp). sc.tl.louvain(adata_pp, key_added='groups', resolution=0.5). ```. ```pytb. ModuleNotFoundError Traceback (most recent call last). <ipython-input-11-785c54721f17> in <module>. 8 sc.pp.pca(adata_pp, n_comps=15). 9 sc.pp.neighbors(adata_pp). ---> 10 sc.tl.louvain(adata_pp, key_added='groups', resolution=0.5). ~\anaconda3\lib\site-packages\scanpy\tools\_louvain.py in louvain(adata, resolution, random_state, restrict_to, key_added, adjacency, flavor, directed, use_weights, partition_type, partition_kwargs, neighbors_key, obsp, copy). 135 weights = None. 136 if flavor == 'vtraag':. --> 137 import louvain. 138 if partition_type is None:. 139 partition_type = louvain.RBConfigurationVertexPartition. ModuleNotFoundError: No module named 'louvain'. ```. #### Versions. <details>. [Paste the output of scanpy.logging.print_versions() leaving a blank line after the details tag]. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1566
https://github.com/scverse/scanpy/issues/1566:1657,modifiability,Version,Versions,1657,"hello, I am trying to use normalisation part of scanpy and encounter this error. I tried to install Louvain but it is not helping. could anyone please help me. Thank you ; - [ ] I have checked that this issue has not already been reported. - [ ] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. ### Minimal code sample (that we can copy&paste without having any data). ```python. #Perform a clustering for scran normalization in clusters. adata_pp = adata.copy(). sc.pp.normalize_per_cell(adata_pp, counts_per_cell_after=1e6). sc.pp.log1p(adata_pp). sc.pp.pca(adata_pp, n_comps=15). sc.pp.neighbors(adata_pp). sc.tl.louvain(adata_pp, key_added='groups', resolution=0.5). ```. ```pytb. ModuleNotFoundError Traceback (most recent call last). <ipython-input-11-785c54721f17> in <module>. 8 sc.pp.pca(adata_pp, n_comps=15). 9 sc.pp.neighbors(adata_pp). ---> 10 sc.tl.louvain(adata_pp, key_added='groups', resolution=0.5). ~\anaconda3\lib\site-packages\scanpy\tools\_louvain.py in louvain(adata, resolution, random_state, restrict_to, key_added, adjacency, flavor, directed, use_weights, partition_type, partition_kwargs, neighbors_key, obsp, copy). 135 weights = None. 136 if flavor == 'vtraag':. --> 137 import louvain. 138 if partition_type is None:. 139 partition_type = louvain.RBConfigurationVertexPartition. ModuleNotFoundError: No module named 'louvain'. ```. #### Versions. <details>. [Paste the output of scanpy.logging.print_versions() leaving a blank line after the details tag]. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1566
https://github.com/scverse/scanpy/issues/1566:74,performance,error,error,74,"hello, I am trying to use normalisation part of scanpy and encounter this error. I tried to install Louvain but it is not helping. could anyone please help me. Thank you ; - [ ] I have checked that this issue has not already been reported. - [ ] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. ### Minimal code sample (that we can copy&paste without having any data). ```python. #Perform a clustering for scran normalization in clusters. adata_pp = adata.copy(). sc.pp.normalize_per_cell(adata_pp, counts_per_cell_after=1e6). sc.pp.log1p(adata_pp). sc.pp.pca(adata_pp, n_comps=15). sc.pp.neighbors(adata_pp). sc.tl.louvain(adata_pp, key_added='groups', resolution=0.5). ```. ```pytb. ModuleNotFoundError Traceback (most recent call last). <ipython-input-11-785c54721f17> in <module>. 8 sc.pp.pca(adata_pp, n_comps=15). 9 sc.pp.neighbors(adata_pp). ---> 10 sc.tl.louvain(adata_pp, key_added='groups', resolution=0.5). ~\anaconda3\lib\site-packages\scanpy\tools\_louvain.py in louvain(adata, resolution, random_state, restrict_to, key_added, adjacency, flavor, directed, use_weights, partition_type, partition_kwargs, neighbors_key, obsp, copy). 135 weights = None. 136 if flavor == 'vtraag':. --> 137 import louvain. 138 if partition_type is None:. 139 partition_type = louvain.RBConfigurationVertexPartition. ModuleNotFoundError: No module named 'louvain'. ```. #### Versions. <details>. [Paste the output of scanpy.logging.print_versions() leaving a blank line after the details tag]. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1566
https://github.com/scverse/scanpy/issues/1566:670,performance,Perform,Perform,670,"hello, I am trying to use normalisation part of scanpy and encounter this error. I tried to install Louvain but it is not helping. could anyone please help me. Thank you ; - [ ] I have checked that this issue has not already been reported. - [ ] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. ### Minimal code sample (that we can copy&paste without having any data). ```python. #Perform a clustering for scran normalization in clusters. adata_pp = adata.copy(). sc.pp.normalize_per_cell(adata_pp, counts_per_cell_after=1e6). sc.pp.log1p(adata_pp). sc.pp.pca(adata_pp, n_comps=15). sc.pp.neighbors(adata_pp). sc.tl.louvain(adata_pp, key_added='groups', resolution=0.5). ```. ```pytb. ModuleNotFoundError Traceback (most recent call last). <ipython-input-11-785c54721f17> in <module>. 8 sc.pp.pca(adata_pp, n_comps=15). 9 sc.pp.neighbors(adata_pp). ---> 10 sc.tl.louvain(adata_pp, key_added='groups', resolution=0.5). ~\anaconda3\lib\site-packages\scanpy\tools\_louvain.py in louvain(adata, resolution, random_state, restrict_to, key_added, adjacency, flavor, directed, use_weights, partition_type, partition_kwargs, neighbors_key, obsp, copy). 135 weights = None. 136 if flavor == 'vtraag':. --> 137 import louvain. 138 if partition_type is None:. 139 partition_type = louvain.RBConfigurationVertexPartition. ModuleNotFoundError: No module named 'louvain'. ```. #### Versions. <details>. [Paste the output of scanpy.logging.print_versions() leaving a blank line after the details tag]. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1566
https://github.com/scverse/scanpy/issues/1566:74,safety,error,error,74,"hello, I am trying to use normalisation part of scanpy and encounter this error. I tried to install Louvain but it is not helping. could anyone please help me. Thank you ; - [ ] I have checked that this issue has not already been reported. - [ ] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. ### Minimal code sample (that we can copy&paste without having any data). ```python. #Perform a clustering for scran normalization in clusters. adata_pp = adata.copy(). sc.pp.normalize_per_cell(adata_pp, counts_per_cell_after=1e6). sc.pp.log1p(adata_pp). sc.pp.pca(adata_pp, n_comps=15). sc.pp.neighbors(adata_pp). sc.tl.louvain(adata_pp, key_added='groups', resolution=0.5). ```. ```pytb. ModuleNotFoundError Traceback (most recent call last). <ipython-input-11-785c54721f17> in <module>. 8 sc.pp.pca(adata_pp, n_comps=15). 9 sc.pp.neighbors(adata_pp). ---> 10 sc.tl.louvain(adata_pp, key_added='groups', resolution=0.5). ~\anaconda3\lib\site-packages\scanpy\tools\_louvain.py in louvain(adata, resolution, random_state, restrict_to, key_added, adjacency, flavor, directed, use_weights, partition_type, partition_kwargs, neighbors_key, obsp, copy). 135 weights = None. 136 if flavor == 'vtraag':. --> 137 import louvain. 138 if partition_type is None:. 139 partition_type = louvain.RBConfigurationVertexPartition. ModuleNotFoundError: No module named 'louvain'. ```. #### Versions. <details>. [Paste the output of scanpy.logging.print_versions() leaving a blank line after the details tag]. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1566
https://github.com/scverse/scanpy/issues/1566:974,safety,Modul,ModuleNotFoundError,974,"hello, I am trying to use normalisation part of scanpy and encounter this error. I tried to install Louvain but it is not helping. could anyone please help me. Thank you ; - [ ] I have checked that this issue has not already been reported. - [ ] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. ### Minimal code sample (that we can copy&paste without having any data). ```python. #Perform a clustering for scran normalization in clusters. adata_pp = adata.copy(). sc.pp.normalize_per_cell(adata_pp, counts_per_cell_after=1e6). sc.pp.log1p(adata_pp). sc.pp.pca(adata_pp, n_comps=15). sc.pp.neighbors(adata_pp). sc.tl.louvain(adata_pp, key_added='groups', resolution=0.5). ```. ```pytb. ModuleNotFoundError Traceback (most recent call last). <ipython-input-11-785c54721f17> in <module>. 8 sc.pp.pca(adata_pp, n_comps=15). 9 sc.pp.neighbors(adata_pp). ---> 10 sc.tl.louvain(adata_pp, key_added='groups', resolution=0.5). ~\anaconda3\lib\site-packages\scanpy\tools\_louvain.py in louvain(adata, resolution, random_state, restrict_to, key_added, adjacency, flavor, directed, use_weights, partition_type, partition_kwargs, neighbors_key, obsp, copy). 135 weights = None. 136 if flavor == 'vtraag':. --> 137 import louvain. 138 if partition_type is None:. 139 partition_type = louvain.RBConfigurationVertexPartition. ModuleNotFoundError: No module named 'louvain'. ```. #### Versions. <details>. [Paste the output of scanpy.logging.print_versions() leaving a blank line after the details tag]. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1566
https://github.com/scverse/scanpy/issues/1566:1038,safety,input,input-,1038,"hello, I am trying to use normalisation part of scanpy and encounter this error. I tried to install Louvain but it is not helping. could anyone please help me. Thank you ; - [ ] I have checked that this issue has not already been reported. - [ ] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. ### Minimal code sample (that we can copy&paste without having any data). ```python. #Perform a clustering for scran normalization in clusters. adata_pp = adata.copy(). sc.pp.normalize_per_cell(adata_pp, counts_per_cell_after=1e6). sc.pp.log1p(adata_pp). sc.pp.pca(adata_pp, n_comps=15). sc.pp.neighbors(adata_pp). sc.tl.louvain(adata_pp, key_added='groups', resolution=0.5). ```. ```pytb. ModuleNotFoundError Traceback (most recent call last). <ipython-input-11-785c54721f17> in <module>. 8 sc.pp.pca(adata_pp, n_comps=15). 9 sc.pp.neighbors(adata_pp). ---> 10 sc.tl.louvain(adata_pp, key_added='groups', resolution=0.5). ~\anaconda3\lib\site-packages\scanpy\tools\_louvain.py in louvain(adata, resolution, random_state, restrict_to, key_added, adjacency, flavor, directed, use_weights, partition_type, partition_kwargs, neighbors_key, obsp, copy). 135 weights = None. 136 if flavor == 'vtraag':. --> 137 import louvain. 138 if partition_type is None:. 139 partition_type = louvain.RBConfigurationVertexPartition. ModuleNotFoundError: No module named 'louvain'. ```. #### Versions. <details>. [Paste the output of scanpy.logging.print_versions() leaving a blank line after the details tag]. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1566
https://github.com/scverse/scanpy/issues/1566:1065,safety,modul,module,1065,"hello, I am trying to use normalisation part of scanpy and encounter this error. I tried to install Louvain but it is not helping. could anyone please help me. Thank you ; - [ ] I have checked that this issue has not already been reported. - [ ] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. ### Minimal code sample (that we can copy&paste without having any data). ```python. #Perform a clustering for scran normalization in clusters. adata_pp = adata.copy(). sc.pp.normalize_per_cell(adata_pp, counts_per_cell_after=1e6). sc.pp.log1p(adata_pp). sc.pp.pca(adata_pp, n_comps=15). sc.pp.neighbors(adata_pp). sc.tl.louvain(adata_pp, key_added='groups', resolution=0.5). ```. ```pytb. ModuleNotFoundError Traceback (most recent call last). <ipython-input-11-785c54721f17> in <module>. 8 sc.pp.pca(adata_pp, n_comps=15). 9 sc.pp.neighbors(adata_pp). ---> 10 sc.tl.louvain(adata_pp, key_added='groups', resolution=0.5). ~\anaconda3\lib\site-packages\scanpy\tools\_louvain.py in louvain(adata, resolution, random_state, restrict_to, key_added, adjacency, flavor, directed, use_weights, partition_type, partition_kwargs, neighbors_key, obsp, copy). 135 weights = None. 136 if flavor == 'vtraag':. --> 137 import louvain. 138 if partition_type is None:. 139 partition_type = louvain.RBConfigurationVertexPartition. ModuleNotFoundError: No module named 'louvain'. ```. #### Versions. <details>. [Paste the output of scanpy.logging.print_versions() leaving a blank line after the details tag]. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1566
https://github.com/scverse/scanpy/issues/1566:1599,safety,Modul,ModuleNotFoundError,1599,"hello, I am trying to use normalisation part of scanpy and encounter this error. I tried to install Louvain but it is not helping. could anyone please help me. Thank you ; - [ ] I have checked that this issue has not already been reported. - [ ] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. ### Minimal code sample (that we can copy&paste without having any data). ```python. #Perform a clustering for scran normalization in clusters. adata_pp = adata.copy(). sc.pp.normalize_per_cell(adata_pp, counts_per_cell_after=1e6). sc.pp.log1p(adata_pp). sc.pp.pca(adata_pp, n_comps=15). sc.pp.neighbors(adata_pp). sc.tl.louvain(adata_pp, key_added='groups', resolution=0.5). ```. ```pytb. ModuleNotFoundError Traceback (most recent call last). <ipython-input-11-785c54721f17> in <module>. 8 sc.pp.pca(adata_pp, n_comps=15). 9 sc.pp.neighbors(adata_pp). ---> 10 sc.tl.louvain(adata_pp, key_added='groups', resolution=0.5). ~\anaconda3\lib\site-packages\scanpy\tools\_louvain.py in louvain(adata, resolution, random_state, restrict_to, key_added, adjacency, flavor, directed, use_weights, partition_type, partition_kwargs, neighbors_key, obsp, copy). 135 weights = None. 136 if flavor == 'vtraag':. --> 137 import louvain. 138 if partition_type is None:. 139 partition_type = louvain.RBConfigurationVertexPartition. ModuleNotFoundError: No module named 'louvain'. ```. #### Versions. <details>. [Paste the output of scanpy.logging.print_versions() leaving a blank line after the details tag]. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1566
https://github.com/scverse/scanpy/issues/1566:1623,safety,modul,module,1623,"hello, I am trying to use normalisation part of scanpy and encounter this error. I tried to install Louvain but it is not helping. could anyone please help me. Thank you ; - [ ] I have checked that this issue has not already been reported. - [ ] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. ### Minimal code sample (that we can copy&paste without having any data). ```python. #Perform a clustering for scran normalization in clusters. adata_pp = adata.copy(). sc.pp.normalize_per_cell(adata_pp, counts_per_cell_after=1e6). sc.pp.log1p(adata_pp). sc.pp.pca(adata_pp, n_comps=15). sc.pp.neighbors(adata_pp). sc.tl.louvain(adata_pp, key_added='groups', resolution=0.5). ```. ```pytb. ModuleNotFoundError Traceback (most recent call last). <ipython-input-11-785c54721f17> in <module>. 8 sc.pp.pca(adata_pp, n_comps=15). 9 sc.pp.neighbors(adata_pp). ---> 10 sc.tl.louvain(adata_pp, key_added='groups', resolution=0.5). ~\anaconda3\lib\site-packages\scanpy\tools\_louvain.py in louvain(adata, resolution, random_state, restrict_to, key_added, adjacency, flavor, directed, use_weights, partition_type, partition_kwargs, neighbors_key, obsp, copy). 135 weights = None. 136 if flavor == 'vtraag':. --> 137 import louvain. 138 if partition_type is None:. 139 partition_type = louvain.RBConfigurationVertexPartition. ModuleNotFoundError: No module named 'louvain'. ```. #### Versions. <details>. [Paste the output of scanpy.logging.print_versions() leaving a blank line after the details tag]. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1566
https://github.com/scverse/scanpy/issues/1566:1706,safety,log,logging,1706,"hello, I am trying to use normalisation part of scanpy and encounter this error. I tried to install Louvain but it is not helping. could anyone please help me. Thank you ; - [ ] I have checked that this issue has not already been reported. - [ ] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. ### Minimal code sample (that we can copy&paste without having any data). ```python. #Perform a clustering for scran normalization in clusters. adata_pp = adata.copy(). sc.pp.normalize_per_cell(adata_pp, counts_per_cell_after=1e6). sc.pp.log1p(adata_pp). sc.pp.pca(adata_pp, n_comps=15). sc.pp.neighbors(adata_pp). sc.tl.louvain(adata_pp, key_added='groups', resolution=0.5). ```. ```pytb. ModuleNotFoundError Traceback (most recent call last). <ipython-input-11-785c54721f17> in <module>. 8 sc.pp.pca(adata_pp, n_comps=15). 9 sc.pp.neighbors(adata_pp). ---> 10 sc.tl.louvain(adata_pp, key_added='groups', resolution=0.5). ~\anaconda3\lib\site-packages\scanpy\tools\_louvain.py in louvain(adata, resolution, random_state, restrict_to, key_added, adjacency, flavor, directed, use_weights, partition_type, partition_kwargs, neighbors_key, obsp, copy). 135 weights = None. 136 if flavor == 'vtraag':. --> 137 import louvain. 138 if partition_type is None:. 139 partition_type = louvain.RBConfigurationVertexPartition. ModuleNotFoundError: No module named 'louvain'. ```. #### Versions. <details>. [Paste the output of scanpy.logging.print_versions() leaving a blank line after the details tag]. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1566
https://github.com/scverse/scanpy/issues/1566:1706,security,log,logging,1706,"hello, I am trying to use normalisation part of scanpy and encounter this error. I tried to install Louvain but it is not helping. could anyone please help me. Thank you ; - [ ] I have checked that this issue has not already been reported. - [ ] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. ### Minimal code sample (that we can copy&paste without having any data). ```python. #Perform a clustering for scran normalization in clusters. adata_pp = adata.copy(). sc.pp.normalize_per_cell(adata_pp, counts_per_cell_after=1e6). sc.pp.log1p(adata_pp). sc.pp.pca(adata_pp, n_comps=15). sc.pp.neighbors(adata_pp). sc.tl.louvain(adata_pp, key_added='groups', resolution=0.5). ```. ```pytb. ModuleNotFoundError Traceback (most recent call last). <ipython-input-11-785c54721f17> in <module>. 8 sc.pp.pca(adata_pp, n_comps=15). 9 sc.pp.neighbors(adata_pp). ---> 10 sc.tl.louvain(adata_pp, key_added='groups', resolution=0.5). ~\anaconda3\lib\site-packages\scanpy\tools\_louvain.py in louvain(adata, resolution, random_state, restrict_to, key_added, adjacency, flavor, directed, use_weights, partition_type, partition_kwargs, neighbors_key, obsp, copy). 135 weights = None. 136 if flavor == 'vtraag':. --> 137 import louvain. 138 if partition_type is None:. 139 partition_type = louvain.RBConfigurationVertexPartition. ModuleNotFoundError: No module named 'louvain'. ```. #### Versions. <details>. [Paste the output of scanpy.logging.print_versions() leaving a blank line after the details tag]. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1566
https://github.com/scverse/scanpy/issues/1566:994,testability,Trace,Traceback,994,"hello, I am trying to use normalisation part of scanpy and encounter this error. I tried to install Louvain but it is not helping. could anyone please help me. Thank you ; - [ ] I have checked that this issue has not already been reported. - [ ] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. ### Minimal code sample (that we can copy&paste without having any data). ```python. #Perform a clustering for scran normalization in clusters. adata_pp = adata.copy(). sc.pp.normalize_per_cell(adata_pp, counts_per_cell_after=1e6). sc.pp.log1p(adata_pp). sc.pp.pca(adata_pp, n_comps=15). sc.pp.neighbors(adata_pp). sc.tl.louvain(adata_pp, key_added='groups', resolution=0.5). ```. ```pytb. ModuleNotFoundError Traceback (most recent call last). <ipython-input-11-785c54721f17> in <module>. 8 sc.pp.pca(adata_pp, n_comps=15). 9 sc.pp.neighbors(adata_pp). ---> 10 sc.tl.louvain(adata_pp, key_added='groups', resolution=0.5). ~\anaconda3\lib\site-packages\scanpy\tools\_louvain.py in louvain(adata, resolution, random_state, restrict_to, key_added, adjacency, flavor, directed, use_weights, partition_type, partition_kwargs, neighbors_key, obsp, copy). 135 weights = None. 136 if flavor == 'vtraag':. --> 137 import louvain. 138 if partition_type is None:. 139 partition_type = louvain.RBConfigurationVertexPartition. ModuleNotFoundError: No module named 'louvain'. ```. #### Versions. <details>. [Paste the output of scanpy.logging.print_versions() leaving a blank line after the details tag]. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1566
https://github.com/scverse/scanpy/issues/1566:1706,testability,log,logging,1706,"hello, I am trying to use normalisation part of scanpy and encounter this error. I tried to install Louvain but it is not helping. could anyone please help me. Thank you ; - [ ] I have checked that this issue has not already been reported. - [ ] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. ### Minimal code sample (that we can copy&paste without having any data). ```python. #Perform a clustering for scran normalization in clusters. adata_pp = adata.copy(). sc.pp.normalize_per_cell(adata_pp, counts_per_cell_after=1e6). sc.pp.log1p(adata_pp). sc.pp.pca(adata_pp, n_comps=15). sc.pp.neighbors(adata_pp). sc.tl.louvain(adata_pp, key_added='groups', resolution=0.5). ```. ```pytb. ModuleNotFoundError Traceback (most recent call last). <ipython-input-11-785c54721f17> in <module>. 8 sc.pp.pca(adata_pp, n_comps=15). 9 sc.pp.neighbors(adata_pp). ---> 10 sc.tl.louvain(adata_pp, key_added='groups', resolution=0.5). ~\anaconda3\lib\site-packages\scanpy\tools\_louvain.py in louvain(adata, resolution, random_state, restrict_to, key_added, adjacency, flavor, directed, use_weights, partition_type, partition_kwargs, neighbors_key, obsp, copy). 135 weights = None. 136 if flavor == 'vtraag':. --> 137 import louvain. 138 if partition_type is None:. 139 partition_type = louvain.RBConfigurationVertexPartition. ModuleNotFoundError: No module named 'louvain'. ```. #### Versions. <details>. [Paste the output of scanpy.logging.print_versions() leaving a blank line after the details tag]. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1566
https://github.com/scverse/scanpy/issues/1566:74,usability,error,error,74,"hello, I am trying to use normalisation part of scanpy and encounter this error. I tried to install Louvain but it is not helping. could anyone please help me. Thank you ; - [ ] I have checked that this issue has not already been reported. - [ ] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. ### Minimal code sample (that we can copy&paste without having any data). ```python. #Perform a clustering for scran normalization in clusters. adata_pp = adata.copy(). sc.pp.normalize_per_cell(adata_pp, counts_per_cell_after=1e6). sc.pp.log1p(adata_pp). sc.pp.pca(adata_pp, n_comps=15). sc.pp.neighbors(adata_pp). sc.tl.louvain(adata_pp, key_added='groups', resolution=0.5). ```. ```pytb. ModuleNotFoundError Traceback (most recent call last). <ipython-input-11-785c54721f17> in <module>. 8 sc.pp.pca(adata_pp, n_comps=15). 9 sc.pp.neighbors(adata_pp). ---> 10 sc.tl.louvain(adata_pp, key_added='groups', resolution=0.5). ~\anaconda3\lib\site-packages\scanpy\tools\_louvain.py in louvain(adata, resolution, random_state, restrict_to, key_added, adjacency, flavor, directed, use_weights, partition_type, partition_kwargs, neighbors_key, obsp, copy). 135 weights = None. 136 if flavor == 'vtraag':. --> 137 import louvain. 138 if partition_type is None:. 139 partition_type = louvain.RBConfigurationVertexPartition. ModuleNotFoundError: No module named 'louvain'. ```. #### Versions. <details>. [Paste the output of scanpy.logging.print_versions() leaving a blank line after the details tag]. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1566
https://github.com/scverse/scanpy/issues/1566:122,usability,help,helping,122,"hello, I am trying to use normalisation part of scanpy and encounter this error. I tried to install Louvain but it is not helping. could anyone please help me. Thank you ; - [ ] I have checked that this issue has not already been reported. - [ ] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. ### Minimal code sample (that we can copy&paste without having any data). ```python. #Perform a clustering for scran normalization in clusters. adata_pp = adata.copy(). sc.pp.normalize_per_cell(adata_pp, counts_per_cell_after=1e6). sc.pp.log1p(adata_pp). sc.pp.pca(adata_pp, n_comps=15). sc.pp.neighbors(adata_pp). sc.tl.louvain(adata_pp, key_added='groups', resolution=0.5). ```. ```pytb. ModuleNotFoundError Traceback (most recent call last). <ipython-input-11-785c54721f17> in <module>. 8 sc.pp.pca(adata_pp, n_comps=15). 9 sc.pp.neighbors(adata_pp). ---> 10 sc.tl.louvain(adata_pp, key_added='groups', resolution=0.5). ~\anaconda3\lib\site-packages\scanpy\tools\_louvain.py in louvain(adata, resolution, random_state, restrict_to, key_added, adjacency, flavor, directed, use_weights, partition_type, partition_kwargs, neighbors_key, obsp, copy). 135 weights = None. 136 if flavor == 'vtraag':. --> 137 import louvain. 138 if partition_type is None:. 139 partition_type = louvain.RBConfigurationVertexPartition. ModuleNotFoundError: No module named 'louvain'. ```. #### Versions. <details>. [Paste the output of scanpy.logging.print_versions() leaving a blank line after the details tag]. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1566
https://github.com/scverse/scanpy/issues/1566:151,usability,help,help,151,"hello, I am trying to use normalisation part of scanpy and encounter this error. I tried to install Louvain but it is not helping. could anyone please help me. Thank you ; - [ ] I have checked that this issue has not already been reported. - [ ] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. ### Minimal code sample (that we can copy&paste without having any data). ```python. #Perform a clustering for scran normalization in clusters. adata_pp = adata.copy(). sc.pp.normalize_per_cell(adata_pp, counts_per_cell_after=1e6). sc.pp.log1p(adata_pp). sc.pp.pca(adata_pp, n_comps=15). sc.pp.neighbors(adata_pp). sc.tl.louvain(adata_pp, key_added='groups', resolution=0.5). ```. ```pytb. ModuleNotFoundError Traceback (most recent call last). <ipython-input-11-785c54721f17> in <module>. 8 sc.pp.pca(adata_pp, n_comps=15). 9 sc.pp.neighbors(adata_pp). ---> 10 sc.tl.louvain(adata_pp, key_added='groups', resolution=0.5). ~\anaconda3\lib\site-packages\scanpy\tools\_louvain.py in louvain(adata, resolution, random_state, restrict_to, key_added, adjacency, flavor, directed, use_weights, partition_type, partition_kwargs, neighbors_key, obsp, copy). 135 weights = None. 136 if flavor == 'vtraag':. --> 137 import louvain. 138 if partition_type is None:. 139 partition_type = louvain.RBConfigurationVertexPartition. ModuleNotFoundError: No module named 'louvain'. ```. #### Versions. <details>. [Paste the output of scanpy.logging.print_versions() leaving a blank line after the details tag]. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1566
https://github.com/scverse/scanpy/issues/1566:253,usability,confirm,confirmed,253,"hello, I am trying to use normalisation part of scanpy and encounter this error. I tried to install Louvain but it is not helping. could anyone please help me. Thank you ; - [ ] I have checked that this issue has not already been reported. - [ ] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. ### Minimal code sample (that we can copy&paste without having any data). ```python. #Perform a clustering for scran normalization in clusters. adata_pp = adata.copy(). sc.pp.normalize_per_cell(adata_pp, counts_per_cell_after=1e6). sc.pp.log1p(adata_pp). sc.pp.pca(adata_pp, n_comps=15). sc.pp.neighbors(adata_pp). sc.tl.louvain(adata_pp, key_added='groups', resolution=0.5). ```. ```pytb. ModuleNotFoundError Traceback (most recent call last). <ipython-input-11-785c54721f17> in <module>. 8 sc.pp.pca(adata_pp, n_comps=15). 9 sc.pp.neighbors(adata_pp). ---> 10 sc.tl.louvain(adata_pp, key_added='groups', resolution=0.5). ~\anaconda3\lib\site-packages\scanpy\tools\_louvain.py in louvain(adata, resolution, random_state, restrict_to, key_added, adjacency, flavor, directed, use_weights, partition_type, partition_kwargs, neighbors_key, obsp, copy). 135 weights = None. 136 if flavor == 'vtraag':. --> 137 import louvain. 138 if partition_type is None:. 139 partition_type = louvain.RBConfigurationVertexPartition. ModuleNotFoundError: No module named 'louvain'. ```. #### Versions. <details>. [Paste the output of scanpy.logging.print_versions() leaving a blank line after the details tag]. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1566
https://github.com/scverse/scanpy/issues/1566:336,usability,confirm,confirmed,336,"hello, I am trying to use normalisation part of scanpy and encounter this error. I tried to install Louvain but it is not helping. could anyone please help me. Thank you ; - [ ] I have checked that this issue has not already been reported. - [ ] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. ### Minimal code sample (that we can copy&paste without having any data). ```python. #Perform a clustering for scran normalization in clusters. adata_pp = adata.copy(). sc.pp.normalize_per_cell(adata_pp, counts_per_cell_after=1e6). sc.pp.log1p(adata_pp). sc.pp.pca(adata_pp, n_comps=15). sc.pp.neighbors(adata_pp). sc.tl.louvain(adata_pp, key_added='groups', resolution=0.5). ```. ```pytb. ModuleNotFoundError Traceback (most recent call last). <ipython-input-11-785c54721f17> in <module>. 8 sc.pp.pca(adata_pp, n_comps=15). 9 sc.pp.neighbors(adata_pp). ---> 10 sc.tl.louvain(adata_pp, key_added='groups', resolution=0.5). ~\anaconda3\lib\site-packages\scanpy\tools\_louvain.py in louvain(adata, resolution, random_state, restrict_to, key_added, adjacency, flavor, directed, use_weights, partition_type, partition_kwargs, neighbors_key, obsp, copy). 135 weights = None. 136 if flavor == 'vtraag':. --> 137 import louvain. 138 if partition_type is None:. 139 partition_type = louvain.RBConfigurationVertexPartition. ModuleNotFoundError: No module named 'louvain'. ```. #### Versions. <details>. [Paste the output of scanpy.logging.print_versions() leaving a blank line after the details tag]. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1566
https://github.com/scverse/scanpy/issues/1566:427,usability,guid,guide,427,"hello, I am trying to use normalisation part of scanpy and encounter this error. I tried to install Louvain but it is not helping. could anyone please help me. Thank you ; - [ ] I have checked that this issue has not already been reported. - [ ] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. ### Minimal code sample (that we can copy&paste without having any data). ```python. #Perform a clustering for scran normalization in clusters. adata_pp = adata.copy(). sc.pp.normalize_per_cell(adata_pp, counts_per_cell_after=1e6). sc.pp.log1p(adata_pp). sc.pp.pca(adata_pp, n_comps=15). sc.pp.neighbors(adata_pp). sc.tl.louvain(adata_pp, key_added='groups', resolution=0.5). ```. ```pytb. ModuleNotFoundError Traceback (most recent call last). <ipython-input-11-785c54721f17> in <module>. 8 sc.pp.pca(adata_pp, n_comps=15). 9 sc.pp.neighbors(adata_pp). ---> 10 sc.tl.louvain(adata_pp, key_added='groups', resolution=0.5). ~\anaconda3\lib\site-packages\scanpy\tools\_louvain.py in louvain(adata, resolution, random_state, restrict_to, key_added, adjacency, flavor, directed, use_weights, partition_type, partition_kwargs, neighbors_key, obsp, copy). 135 weights = None. 136 if flavor == 'vtraag':. --> 137 import louvain. 138 if partition_type is None:. 139 partition_type = louvain.RBConfigurationVertexPartition. ModuleNotFoundError: No module named 'louvain'. ```. #### Versions. <details>. [Paste the output of scanpy.logging.print_versions() leaving a blank line after the details tag]. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1566
https://github.com/scverse/scanpy/issues/1566:482,usability,minim,minimal-bug-reports,482,"hello, I am trying to use normalisation part of scanpy and encounter this error. I tried to install Louvain but it is not helping. could anyone please help me. Thank you ; - [ ] I have checked that this issue has not already been reported. - [ ] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. ### Minimal code sample (that we can copy&paste without having any data). ```python. #Perform a clustering for scran normalization in clusters. adata_pp = adata.copy(). sc.pp.normalize_per_cell(adata_pp, counts_per_cell_after=1e6). sc.pp.log1p(adata_pp). sc.pp.pca(adata_pp, n_comps=15). sc.pp.neighbors(adata_pp). sc.tl.louvain(adata_pp, key_added='groups', resolution=0.5). ```. ```pytb. ModuleNotFoundError Traceback (most recent call last). <ipython-input-11-785c54721f17> in <module>. 8 sc.pp.pca(adata_pp, n_comps=15). 9 sc.pp.neighbors(adata_pp). ---> 10 sc.tl.louvain(adata_pp, key_added='groups', resolution=0.5). ~\anaconda3\lib\site-packages\scanpy\tools\_louvain.py in louvain(adata, resolution, random_state, restrict_to, key_added, adjacency, flavor, directed, use_weights, partition_type, partition_kwargs, neighbors_key, obsp, copy). 135 weights = None. 136 if flavor == 'vtraag':. --> 137 import louvain. 138 if partition_type is None:. 139 partition_type = louvain.RBConfigurationVertexPartition. ModuleNotFoundError: No module named 'louvain'. ```. #### Versions. <details>. [Paste the output of scanpy.logging.print_versions() leaving a blank line after the details tag]. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1566
https://github.com/scverse/scanpy/issues/1566:588,usability,Minim,Minimal,588,"hello, I am trying to use normalisation part of scanpy and encounter this error. I tried to install Louvain but it is not helping. could anyone please help me. Thank you ; - [ ] I have checked that this issue has not already been reported. - [ ] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. ### Minimal code sample (that we can copy&paste without having any data). ```python. #Perform a clustering for scran normalization in clusters. adata_pp = adata.copy(). sc.pp.normalize_per_cell(adata_pp, counts_per_cell_after=1e6). sc.pp.log1p(adata_pp). sc.pp.pca(adata_pp, n_comps=15). sc.pp.neighbors(adata_pp). sc.tl.louvain(adata_pp, key_added='groups', resolution=0.5). ```. ```pytb. ModuleNotFoundError Traceback (most recent call last). <ipython-input-11-785c54721f17> in <module>. 8 sc.pp.pca(adata_pp, n_comps=15). 9 sc.pp.neighbors(adata_pp). ---> 10 sc.tl.louvain(adata_pp, key_added='groups', resolution=0.5). ~\anaconda3\lib\site-packages\scanpy\tools\_louvain.py in louvain(adata, resolution, random_state, restrict_to, key_added, adjacency, flavor, directed, use_weights, partition_type, partition_kwargs, neighbors_key, obsp, copy). 135 weights = None. 136 if flavor == 'vtraag':. --> 137 import louvain. 138 if partition_type is None:. 139 partition_type = louvain.RBConfigurationVertexPartition. ModuleNotFoundError: No module named 'louvain'. ```. #### Versions. <details>. [Paste the output of scanpy.logging.print_versions() leaving a blank line after the details tag]. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1566
https://github.com/scverse/scanpy/issues/1566:670,usability,Perform,Perform,670,"hello, I am trying to use normalisation part of scanpy and encounter this error. I tried to install Louvain but it is not helping. could anyone please help me. Thank you ; - [ ] I have checked that this issue has not already been reported. - [ ] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. ### Minimal code sample (that we can copy&paste without having any data). ```python. #Perform a clustering for scran normalization in clusters. adata_pp = adata.copy(). sc.pp.normalize_per_cell(adata_pp, counts_per_cell_after=1e6). sc.pp.log1p(adata_pp). sc.pp.pca(adata_pp, n_comps=15). sc.pp.neighbors(adata_pp). sc.tl.louvain(adata_pp, key_added='groups', resolution=0.5). ```. ```pytb. ModuleNotFoundError Traceback (most recent call last). <ipython-input-11-785c54721f17> in <module>. 8 sc.pp.pca(adata_pp, n_comps=15). 9 sc.pp.neighbors(adata_pp). ---> 10 sc.tl.louvain(adata_pp, key_added='groups', resolution=0.5). ~\anaconda3\lib\site-packages\scanpy\tools\_louvain.py in louvain(adata, resolution, random_state, restrict_to, key_added, adjacency, flavor, directed, use_weights, partition_type, partition_kwargs, neighbors_key, obsp, copy). 135 weights = None. 136 if flavor == 'vtraag':. --> 137 import louvain. 138 if partition_type is None:. 139 partition_type = louvain.RBConfigurationVertexPartition. ModuleNotFoundError: No module named 'louvain'. ```. #### Versions. <details>. [Paste the output of scanpy.logging.print_versions() leaving a blank line after the details tag]. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1566
https://github.com/scverse/scanpy/issues/1566:1038,usability,input,input-,1038,"hello, I am trying to use normalisation part of scanpy and encounter this error. I tried to install Louvain but it is not helping. could anyone please help me. Thank you ; - [ ] I have checked that this issue has not already been reported. - [ ] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. ### Minimal code sample (that we can copy&paste without having any data). ```python. #Perform a clustering for scran normalization in clusters. adata_pp = adata.copy(). sc.pp.normalize_per_cell(adata_pp, counts_per_cell_after=1e6). sc.pp.log1p(adata_pp). sc.pp.pca(adata_pp, n_comps=15). sc.pp.neighbors(adata_pp). sc.tl.louvain(adata_pp, key_added='groups', resolution=0.5). ```. ```pytb. ModuleNotFoundError Traceback (most recent call last). <ipython-input-11-785c54721f17> in <module>. 8 sc.pp.pca(adata_pp, n_comps=15). 9 sc.pp.neighbors(adata_pp). ---> 10 sc.tl.louvain(adata_pp, key_added='groups', resolution=0.5). ~\anaconda3\lib\site-packages\scanpy\tools\_louvain.py in louvain(adata, resolution, random_state, restrict_to, key_added, adjacency, flavor, directed, use_weights, partition_type, partition_kwargs, neighbors_key, obsp, copy). 135 weights = None. 136 if flavor == 'vtraag':. --> 137 import louvain. 138 if partition_type is None:. 139 partition_type = louvain.RBConfigurationVertexPartition. ModuleNotFoundError: No module named 'louvain'. ```. #### Versions. <details>. [Paste the output of scanpy.logging.print_versions() leaving a blank line after the details tag]. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1566
https://github.com/scverse/scanpy/issues/1566:1244,usability,tool,tools,1244,"hello, I am trying to use normalisation part of scanpy and encounter this error. I tried to install Louvain but it is not helping. could anyone please help me. Thank you ; - [ ] I have checked that this issue has not already been reported. - [ ] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. ### Minimal code sample (that we can copy&paste without having any data). ```python. #Perform a clustering for scran normalization in clusters. adata_pp = adata.copy(). sc.pp.normalize_per_cell(adata_pp, counts_per_cell_after=1e6). sc.pp.log1p(adata_pp). sc.pp.pca(adata_pp, n_comps=15). sc.pp.neighbors(adata_pp). sc.tl.louvain(adata_pp, key_added='groups', resolution=0.5). ```. ```pytb. ModuleNotFoundError Traceback (most recent call last). <ipython-input-11-785c54721f17> in <module>. 8 sc.pp.pca(adata_pp, n_comps=15). 9 sc.pp.neighbors(adata_pp). ---> 10 sc.tl.louvain(adata_pp, key_added='groups', resolution=0.5). ~\anaconda3\lib\site-packages\scanpy\tools\_louvain.py in louvain(adata, resolution, random_state, restrict_to, key_added, adjacency, flavor, directed, use_weights, partition_type, partition_kwargs, neighbors_key, obsp, copy). 135 weights = None. 136 if flavor == 'vtraag':. --> 137 import louvain. 138 if partition_type is None:. 139 partition_type = louvain.RBConfigurationVertexPartition. ModuleNotFoundError: No module named 'louvain'. ```. #### Versions. <details>. [Paste the output of scanpy.logging.print_versions() leaving a blank line after the details tag]. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1566
https://github.com/scverse/scanpy/issues/1567:128,availability,error,error,128,"Kernel dies - sc.pp.neighbors; Hi there. Everytime I run the code _sc.pp.neighbors_ the kernel dies. Unfortunately, there is no error message or error code. It just dies while computing neighbors. Other scanpy codes like _sc.pp.filter_cells_ and _sc.pp.filter_genes_ work without a problem. I'm using:. - windows 10 64-bit 24 gb ram. - python 3.8.5 in jupyter notebook. - numpy 1.19.4. - scanpy 1.6.0. Is there someone who would be able to solve this issue? Thank you very much!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1567
https://github.com/scverse/scanpy/issues/1567:145,availability,error,error,145,"Kernel dies - sc.pp.neighbors; Hi there. Everytime I run the code _sc.pp.neighbors_ the kernel dies. Unfortunately, there is no error message or error code. It just dies while computing neighbors. Other scanpy codes like _sc.pp.filter_cells_ and _sc.pp.filter_genes_ work without a problem. I'm using:. - windows 10 64-bit 24 gb ram. - python 3.8.5 in jupyter notebook. - numpy 1.19.4. - scanpy 1.6.0. Is there someone who would be able to solve this issue? Thank you very much!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1567
https://github.com/scverse/scanpy/issues/1567:134,integrability,messag,message,134,"Kernel dies - sc.pp.neighbors; Hi there. Everytime I run the code _sc.pp.neighbors_ the kernel dies. Unfortunately, there is no error message or error code. It just dies while computing neighbors. Other scanpy codes like _sc.pp.filter_cells_ and _sc.pp.filter_genes_ work without a problem. I'm using:. - windows 10 64-bit 24 gb ram. - python 3.8.5 in jupyter notebook. - numpy 1.19.4. - scanpy 1.6.0. Is there someone who would be able to solve this issue? Thank you very much!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1567
https://github.com/scverse/scanpy/issues/1567:134,interoperability,messag,message,134,"Kernel dies - sc.pp.neighbors; Hi there. Everytime I run the code _sc.pp.neighbors_ the kernel dies. Unfortunately, there is no error message or error code. It just dies while computing neighbors. Other scanpy codes like _sc.pp.filter_cells_ and _sc.pp.filter_genes_ work without a problem. I'm using:. - windows 10 64-bit 24 gb ram. - python 3.8.5 in jupyter notebook. - numpy 1.19.4. - scanpy 1.6.0. Is there someone who would be able to solve this issue? Thank you very much!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1567
https://github.com/scverse/scanpy/issues/1567:128,performance,error,error,128,"Kernel dies - sc.pp.neighbors; Hi there. Everytime I run the code _sc.pp.neighbors_ the kernel dies. Unfortunately, there is no error message or error code. It just dies while computing neighbors. Other scanpy codes like _sc.pp.filter_cells_ and _sc.pp.filter_genes_ work without a problem. I'm using:. - windows 10 64-bit 24 gb ram. - python 3.8.5 in jupyter notebook. - numpy 1.19.4. - scanpy 1.6.0. Is there someone who would be able to solve this issue? Thank you very much!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1567
https://github.com/scverse/scanpy/issues/1567:145,performance,error,error,145,"Kernel dies - sc.pp.neighbors; Hi there. Everytime I run the code _sc.pp.neighbors_ the kernel dies. Unfortunately, there is no error message or error code. It just dies while computing neighbors. Other scanpy codes like _sc.pp.filter_cells_ and _sc.pp.filter_genes_ work without a problem. I'm using:. - windows 10 64-bit 24 gb ram. - python 3.8.5 in jupyter notebook. - numpy 1.19.4. - scanpy 1.6.0. Is there someone who would be able to solve this issue? Thank you very much!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1567
https://github.com/scverse/scanpy/issues/1567:128,safety,error,error,128,"Kernel dies - sc.pp.neighbors; Hi there. Everytime I run the code _sc.pp.neighbors_ the kernel dies. Unfortunately, there is no error message or error code. It just dies while computing neighbors. Other scanpy codes like _sc.pp.filter_cells_ and _sc.pp.filter_genes_ work without a problem. I'm using:. - windows 10 64-bit 24 gb ram. - python 3.8.5 in jupyter notebook. - numpy 1.19.4. - scanpy 1.6.0. Is there someone who would be able to solve this issue? Thank you very much!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1567
https://github.com/scverse/scanpy/issues/1567:145,safety,error,error,145,"Kernel dies - sc.pp.neighbors; Hi there. Everytime I run the code _sc.pp.neighbors_ the kernel dies. Unfortunately, there is no error message or error code. It just dies while computing neighbors. Other scanpy codes like _sc.pp.filter_cells_ and _sc.pp.filter_genes_ work without a problem. I'm using:. - windows 10 64-bit 24 gb ram. - python 3.8.5 in jupyter notebook. - numpy 1.19.4. - scanpy 1.6.0. Is there someone who would be able to solve this issue? Thank you very much!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1567
https://github.com/scverse/scanpy/issues/1567:128,usability,error,error,128,"Kernel dies - sc.pp.neighbors; Hi there. Everytime I run the code _sc.pp.neighbors_ the kernel dies. Unfortunately, there is no error message or error code. It just dies while computing neighbors. Other scanpy codes like _sc.pp.filter_cells_ and _sc.pp.filter_genes_ work without a problem. I'm using:. - windows 10 64-bit 24 gb ram. - python 3.8.5 in jupyter notebook. - numpy 1.19.4. - scanpy 1.6.0. Is there someone who would be able to solve this issue? Thank you very much!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1567
https://github.com/scverse/scanpy/issues/1567:145,usability,error,error,145,"Kernel dies - sc.pp.neighbors; Hi there. Everytime I run the code _sc.pp.neighbors_ the kernel dies. Unfortunately, there is no error message or error code. It just dies while computing neighbors. Other scanpy codes like _sc.pp.filter_cells_ and _sc.pp.filter_genes_ work without a problem. I'm using:. - windows 10 64-bit 24 gb ram. - python 3.8.5 in jupyter notebook. - numpy 1.19.4. - scanpy 1.6.0. Is there someone who would be able to solve this issue? Thank you very much!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1567
https://github.com/scverse/scanpy/issues/1568:660,energy efficiency,Current,Currently,660,"Embedding - set scatter colour when 'color' (by obs/var) is not used; <!-- What kind of feature would you like to request? -->. - [ x] Additional function parameters / changed functionality / changed defaults? - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`? - [ ] New plotting function: A kind of plot you would like to seein `sc.pl`? - [ ] External tools: Do you know an existing package that should go into `sc.external.*`? - [ ] Other? <!-- Please describe your wishes below: -->. Add parameter for setting embedding/UMAP colour when the ""color"" parameter is not used - e.g. set one colour for all points. Currently, the only option to set colour for all cells seems to be to add additional constant obs feature and modify uns to set the colour for the new feature. .",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1568
https://github.com/scverse/scanpy/issues/1568:155,modifiability,paramet,parameters,155,"Embedding - set scatter colour when 'color' (by obs/var) is not used; <!-- What kind of feature would you like to request? -->. - [ x] Additional function parameters / changed functionality / changed defaults? - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`? - [ ] New plotting function: A kind of plot you would like to seein `sc.pl`? - [ ] External tools: Do you know an existing package that should go into `sc.external.*`? - [ ] Other? <!-- Please describe your wishes below: -->. Add parameter for setting embedding/UMAP colour when the ""color"" parameter is not used - e.g. set one colour for all points. Currently, the only option to set colour for all cells seems to be to add additional constant obs feature and modify uns to set the colour for the new feature. .",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1568
https://github.com/scverse/scanpy/issues/1568:432,modifiability,pac,package,432,"Embedding - set scatter colour when 'color' (by obs/var) is not used; <!-- What kind of feature would you like to request? -->. - [ x] Additional function parameters / changed functionality / changed defaults? - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`? - [ ] New plotting function: A kind of plot you would like to seein `sc.pl`? - [ ] External tools: Do you know an existing package that should go into `sc.external.*`? - [ ] Other? <!-- Please describe your wishes below: -->. Add parameter for setting embedding/UMAP colour when the ""color"" parameter is not used - e.g. set one colour for all points. Currently, the only option to set colour for all cells seems to be to add additional constant obs feature and modify uns to set the colour for the new feature. .",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1568
https://github.com/scverse/scanpy/issues/1568:539,modifiability,paramet,parameter,539,"Embedding - set scatter colour when 'color' (by obs/var) is not used; <!-- What kind of feature would you like to request? -->. - [ x] Additional function parameters / changed functionality / changed defaults? - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`? - [ ] New plotting function: A kind of plot you would like to seein `sc.pl`? - [ ] External tools: Do you know an existing package that should go into `sc.external.*`? - [ ] Other? <!-- Please describe your wishes below: -->. Add parameter for setting embedding/UMAP colour when the ""color"" parameter is not used - e.g. set one colour for all points. Currently, the only option to set colour for all cells seems to be to add additional constant obs feature and modify uns to set the colour for the new feature. .",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1568
https://github.com/scverse/scanpy/issues/1568:600,modifiability,paramet,parameter,600,"Embedding - set scatter colour when 'color' (by obs/var) is not used; <!-- What kind of feature would you like to request? -->. - [ x] Additional function parameters / changed functionality / changed defaults? - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`? - [ ] New plotting function: A kind of plot you would like to seein `sc.pl`? - [ ] External tools: Do you know an existing package that should go into `sc.external.*`? - [ ] Other? <!-- Please describe your wishes below: -->. Add parameter for setting embedding/UMAP colour when the ""color"" parameter is not used - e.g. set one colour for all points. Currently, the only option to set colour for all cells seems to be to add additional constant obs feature and modify uns to set the colour for the new feature. .",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1568
https://github.com/scverse/scanpy/issues/1568:770,security,modif,modify,770,"Embedding - set scatter colour when 'color' (by obs/var) is not used; <!-- What kind of feature would you like to request? -->. - [ x] Additional function parameters / changed functionality / changed defaults? - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`? - [ ] New plotting function: A kind of plot you would like to seein `sc.pl`? - [ ] External tools: Do you know an existing package that should go into `sc.external.*`? - [ ] Other? <!-- Please describe your wishes below: -->. Add parameter for setting embedding/UMAP colour when the ""color"" parameter is not used - e.g. set one colour for all points. Currently, the only option to set colour for all cells seems to be to add additional constant obs feature and modify uns to set the colour for the new feature. .",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1568
https://github.com/scverse/scanpy/issues/1568:237,testability,simpl,simple,237,"Embedding - set scatter colour when 'color' (by obs/var) is not used; <!-- What kind of feature would you like to request? -->. - [ x] Additional function parameters / changed functionality / changed defaults? - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`? - [ ] New plotting function: A kind of plot you would like to seein `sc.pl`? - [ ] External tools: Do you know an existing package that should go into `sc.external.*`? - [ ] Other? <!-- Please describe your wishes below: -->. Add parameter for setting embedding/UMAP colour when the ""color"" parameter is not used - e.g. set one colour for all points. Currently, the only option to set colour for all cells seems to be to add additional constant obs feature and modify uns to set the colour for the new feature. .",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1568
https://github.com/scverse/scanpy/issues/1568:229,usability,tool,tool,229,"Embedding - set scatter colour when 'color' (by obs/var) is not used; <!-- What kind of feature would you like to request? -->. - [ x] Additional function parameters / changed functionality / changed defaults? - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`? - [ ] New plotting function: A kind of plot you would like to seein `sc.pl`? - [ ] External tools: Do you know an existing package that should go into `sc.external.*`? - [ ] Other? <!-- Please describe your wishes below: -->. Add parameter for setting embedding/UMAP colour when the ""color"" parameter is not used - e.g. set one colour for all points. Currently, the only option to set colour for all cells seems to be to add additional constant obs feature and modify uns to set the colour for the new feature. .",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1568
https://github.com/scverse/scanpy/issues/1568:237,usability,simpl,simple,237,"Embedding - set scatter colour when 'color' (by obs/var) is not used; <!-- What kind of feature would you like to request? -->. - [ x] Additional function parameters / changed functionality / changed defaults? - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`? - [ ] New plotting function: A kind of plot you would like to seein `sc.pl`? - [ ] External tools: Do you know an existing package that should go into `sc.external.*`? - [ ] Other? <!-- Please describe your wishes below: -->. Add parameter for setting embedding/UMAP colour when the ""color"" parameter is not used - e.g. set one colour for all points. Currently, the only option to set colour for all cells seems to be to add additional constant obs feature and modify uns to set the colour for the new feature. .",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1568
https://github.com/scverse/scanpy/issues/1568:253,usability,tool,tool,253,"Embedding - set scatter colour when 'color' (by obs/var) is not used; <!-- What kind of feature would you like to request? -->. - [ x] Additional function parameters / changed functionality / changed defaults? - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`? - [ ] New plotting function: A kind of plot you would like to seein `sc.pl`? - [ ] External tools: Do you know an existing package that should go into `sc.external.*`? - [ ] Other? <!-- Please describe your wishes below: -->. Add parameter for setting embedding/UMAP colour when the ""color"" parameter is not used - e.g. set one colour for all points. Currently, the only option to set colour for all cells seems to be to add additional constant obs feature and modify uns to set the colour for the new feature. .",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1568
https://github.com/scverse/scanpy/issues/1568:301,usability,tool,tools,301,"Embedding - set scatter colour when 'color' (by obs/var) is not used; <!-- What kind of feature would you like to request? -->. - [ x] Additional function parameters / changed functionality / changed defaults? - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`? - [ ] New plotting function: A kind of plot you would like to seein `sc.pl`? - [ ] External tools: Do you know an existing package that should go into `sc.external.*`? - [ ] Other? <!-- Please describe your wishes below: -->. Add parameter for setting embedding/UMAP colour when the ""color"" parameter is not used - e.g. set one colour for all points. Currently, the only option to set colour for all cells seems to be to add additional constant obs feature and modify uns to set the colour for the new feature. .",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1568
https://github.com/scverse/scanpy/issues/1568:401,usability,tool,tools,401,"Embedding - set scatter colour when 'color' (by obs/var) is not used; <!-- What kind of feature would you like to request? -->. - [ x] Additional function parameters / changed functionality / changed defaults? - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`? - [ ] New plotting function: A kind of plot you would like to seein `sc.pl`? - [ ] External tools: Do you know an existing package that should go into `sc.external.*`? - [ ] Other? <!-- Please describe your wishes below: -->. Add parameter for setting embedding/UMAP colour when the ""color"" parameter is not used - e.g. set one colour for all points. Currently, the only option to set colour for all cells seems to be to add additional constant obs feature and modify uns to set the colour for the new feature. .",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1568
https://github.com/scverse/scanpy/pull/1569:0,deployability,Updat,Update,0,Update release notes for 1.7.0; To find all PRs that have been merged since the last release: `repo:theislab/scanpy closed:>YYYY-MM-DD is:pr is:merged` where the date is the date of last release. Boy were there a lot. There should be some automation here.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1569
https://github.com/scverse/scanpy/pull/1569:7,deployability,releas,release,7,Update release notes for 1.7.0; To find all PRs that have been merged since the last release: `repo:theislab/scanpy closed:>YYYY-MM-DD is:pr is:merged` where the date is the date of last release. Boy were there a lot. There should be some automation here.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1569
https://github.com/scverse/scanpy/pull/1569:85,deployability,releas,release,85,Update release notes for 1.7.0; To find all PRs that have been merged since the last release: `repo:theislab/scanpy closed:>YYYY-MM-DD is:pr is:merged` where the date is the date of last release. Boy were there a lot. There should be some automation here.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1569
https://github.com/scverse/scanpy/pull/1569:187,deployability,releas,release,187,Update release notes for 1.7.0; To find all PRs that have been merged since the last release: `repo:theislab/scanpy closed:>YYYY-MM-DD is:pr is:merged` where the date is the date of last release. Boy were there a lot. There should be some automation here.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1569
https://github.com/scverse/scanpy/pull/1569:239,deployability,automat,automation,239,Update release notes for 1.7.0; To find all PRs that have been merged since the last release: `repo:theislab/scanpy closed:>YYYY-MM-DD is:pr is:merged` where the date is the date of last release. Boy were there a lot. There should be some automation here.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1569
https://github.com/scverse/scanpy/pull/1569:0,safety,Updat,Update,0,Update release notes for 1.7.0; To find all PRs that have been merged since the last release: `repo:theislab/scanpy closed:>YYYY-MM-DD is:pr is:merged` where the date is the date of last release. Boy were there a lot. There should be some automation here.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1569
https://github.com/scverse/scanpy/pull/1569:0,security,Updat,Update,0,Update release notes for 1.7.0; To find all PRs that have been merged since the last release: `repo:theislab/scanpy closed:>YYYY-MM-DD is:pr is:merged` where the date is the date of last release. Boy were there a lot. There should be some automation here.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1569
https://github.com/scverse/scanpy/pull/1569:239,testability,automat,automation,239,Update release notes for 1.7.0; To find all PRs that have been merged since the last release: `repo:theislab/scanpy closed:>YYYY-MM-DD is:pr is:merged` where the date is the date of last release. Boy were there a lot. There should be some automation here.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1569
https://github.com/scverse/scanpy/pull/1569:116,usability,close,closed,116,Update release notes for 1.7.0; To find all PRs that have been merged since the last release: `repo:theislab/scanpy closed:>YYYY-MM-DD is:pr is:merged` where the date is the date of last release. Boy were there a lot. There should be some automation here.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1569
https://github.com/scverse/scanpy/issues/1570:82,deployability,releas,releases,82,Backport guide; We'd like to be able to back port So that we can do actual bugfix releases. We can use some tooling (https://meeseeksbox.github.io) to make this easier. I need to document this process so that we can get back ports going as soon as we make the next release (turns out it get's harder to back port when the PR was merged a few months ago).,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1570
https://github.com/scverse/scanpy/issues/1570:265,deployability,releas,release,265,Backport guide; We'd like to be able to back port So that we can do actual bugfix releases. We can use some tooling (https://meeseeksbox.github.io) to make this easier. I need to document this process so that we can get back ports going as soon as we make the next release (turns out it get's harder to back port when the PR was merged a few months ago).,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1570
https://github.com/scverse/scanpy/issues/1570:9,usability,guid,guide,9,Backport guide; We'd like to be able to back port So that we can do actual bugfix releases. We can use some tooling (https://meeseeksbox.github.io) to make this easier. I need to document this process so that we can get back ports going as soon as we make the next release (turns out it get's harder to back port when the PR was merged a few months ago).,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1570
https://github.com/scverse/scanpy/issues/1570:108,usability,tool,tooling,108,Backport guide; We'd like to be able to back port So that we can do actual bugfix releases. We can use some tooling (https://meeseeksbox.github.io) to make this easier. I need to document this process so that we can get back ports going as soon as we make the next release (turns out it get's harder to back port when the PR was merged a few months ago).,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1570
https://github.com/scverse/scanpy/issues/1570:179,usability,document,document,179,Backport guide; We'd like to be able to back port So that we can do actual bugfix releases. We can use some tooling (https://meeseeksbox.github.io) to make this easier. I need to document this process so that we can get back ports going as soon as we make the next release (turns out it get's harder to back port when the PR was merged a few months ago).,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1570
https://github.com/scverse/scanpy/pull/1571:0,deployability,Updat,Update,0,"Update the News section; @ivirshup, as discussed!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1571
https://github.com/scverse/scanpy/pull/1571:0,safety,Updat,Update,0,"Update the News section; @ivirshup, as discussed!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1571
https://github.com/scverse/scanpy/pull/1571:0,security,Updat,Update,0,"Update the News section; @ivirshup, as discussed!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1571
https://github.com/scverse/scanpy/issues/1572:66,availability,error,errors,66,Cannot write my anndata object to file. Already checked the other errors;,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1572
https://github.com/scverse/scanpy/issues/1572:66,performance,error,errors,66,Cannot write my anndata object to file. Already checked the other errors;,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1572
https://github.com/scverse/scanpy/issues/1572:66,safety,error,errors,66,Cannot write my anndata object to file. Already checked the other errors;,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1572
https://github.com/scverse/scanpy/issues/1572:66,usability,error,errors,66,Cannot write my anndata object to file. Already checked the other errors;,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1572
https://github.com/scverse/scanpy/issues/1573:0,availability,Cluster,Cluster,0,"Cluster content plot for integrated data; <!-- What kind of feature would you like to request? -->. - [ ] Additional function parameters / changed functionality / changed defaults? - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`? - [x] New plotting function: A kind of plot you would like to seein `sc.pl`? - [ ] External tools: Do you know an existing package that should go into `sc.external.*`? - [ ] Other? <!-- Please describe your wishes below: -->. I am integrating multiple datasets. In the integrated object I wanted to see for each cluster the percentage of each dataset it consisted of (also see [this question](https://scanpy.discourse.group/t/cluster-statistics/134) in the scanpy discourse group). I wrote code for it on my own, but am not sure which part of `sc.pl` you want it to go to, so here is the code for your consideration:. ```python. import matplotlib.pyplot as plt. import scanpy as sc. import numpy as np. # given integrated object adata, clustered via the leiden algorithm and. # with the batch ID in the 'batch' slot, and a collection of batch_names:. # count the number of occurrences of each batch ID for each cluster ID. count_series = adata.obs.groupby(['leiden', 'batch']).size(). new_df = count_series.to_frame(name = 'size').reset_index(). # convert from multi index to pivot. constitution = new_df.pivot(index='leiden', columns='batch')['size']. # convert to %batch (but could be modified to show different things instead. perc_clust = np.array((constitution.T / np.sum(constitution.T, axis=0))). # keep track of the batch, cluster IDs so we can use them for plotting. clusters = adata.obs.leiden.cat.categories. batches = adata.obs.batch.cat.categories. # actual plotting; basically stacked barplots. # replace styling with scanpy defaults probably? fig, ax = plt.subplots(). ax.grid(False). ax.bar(clusters, perc_clust[0], 0.6, yerr=0, label=platy_names[0]). bottom = np.zeros(clusters.shape). for i, b in enume",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1573
https://github.com/scverse/scanpy/issues/1573:592,availability,cluster,cluster,592,"Cluster content plot for integrated data; <!-- What kind of feature would you like to request? -->. - [ ] Additional function parameters / changed functionality / changed defaults? - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`? - [x] New plotting function: A kind of plot you would like to seein `sc.pl`? - [ ] External tools: Do you know an existing package that should go into `sc.external.*`? - [ ] Other? <!-- Please describe your wishes below: -->. I am integrating multiple datasets. In the integrated object I wanted to see for each cluster the percentage of each dataset it consisted of (also see [this question](https://scanpy.discourse.group/t/cluster-statistics/134) in the scanpy discourse group). I wrote code for it on my own, but am not sure which part of `sc.pl` you want it to go to, so here is the code for your consideration:. ```python. import matplotlib.pyplot as plt. import scanpy as sc. import numpy as np. # given integrated object adata, clustered via the leiden algorithm and. # with the batch ID in the 'batch' slot, and a collection of batch_names:. # count the number of occurrences of each batch ID for each cluster ID. count_series = adata.obs.groupby(['leiden', 'batch']).size(). new_df = count_series.to_frame(name = 'size').reset_index(). # convert from multi index to pivot. constitution = new_df.pivot(index='leiden', columns='batch')['size']. # convert to %batch (but could be modified to show different things instead. perc_clust = np.array((constitution.T / np.sum(constitution.T, axis=0))). # keep track of the batch, cluster IDs so we can use them for plotting. clusters = adata.obs.leiden.cat.categories. batches = adata.obs.batch.cat.categories. # actual plotting; basically stacked barplots. # replace styling with scanpy defaults probably? fig, ax = plt.subplots(). ax.grid(False). ax.bar(clusters, perc_clust[0], 0.6, yerr=0, label=platy_names[0]). bottom = np.zeros(clusters.shape). for i, b in enume",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1573
https://github.com/scverse/scanpy/issues/1573:634,availability,consist,consisted,634,"Cluster content plot for integrated data; <!-- What kind of feature would you like to request? -->. - [ ] Additional function parameters / changed functionality / changed defaults? - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`? - [x] New plotting function: A kind of plot you would like to seein `sc.pl`? - [ ] External tools: Do you know an existing package that should go into `sc.external.*`? - [ ] Other? <!-- Please describe your wishes below: -->. I am integrating multiple datasets. In the integrated object I wanted to see for each cluster the percentage of each dataset it consisted of (also see [this question](https://scanpy.discourse.group/t/cluster-statistics/134) in the scanpy discourse group). I wrote code for it on my own, but am not sure which part of `sc.pl` you want it to go to, so here is the code for your consideration:. ```python. import matplotlib.pyplot as plt. import scanpy as sc. import numpy as np. # given integrated object adata, clustered via the leiden algorithm and. # with the batch ID in the 'batch' slot, and a collection of batch_names:. # count the number of occurrences of each batch ID for each cluster ID. count_series = adata.obs.groupby(['leiden', 'batch']).size(). new_df = count_series.to_frame(name = 'size').reset_index(). # convert from multi index to pivot. constitution = new_df.pivot(index='leiden', columns='batch')['size']. # convert to %batch (but could be modified to show different things instead. perc_clust = np.array((constitution.T / np.sum(constitution.T, axis=0))). # keep track of the batch, cluster IDs so we can use them for plotting. clusters = adata.obs.leiden.cat.categories. batches = adata.obs.batch.cat.categories. # actual plotting; basically stacked barplots. # replace styling with scanpy defaults probably? fig, ax = plt.subplots(). ax.grid(False). ax.bar(clusters, perc_clust[0], 0.6, yerr=0, label=platy_names[0]). bottom = np.zeros(clusters.shape). for i, b in enume",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1573
https://github.com/scverse/scanpy/issues/1573:706,availability,cluster,cluster-statistics,706,"Cluster content plot for integrated data; <!-- What kind of feature would you like to request? -->. - [ ] Additional function parameters / changed functionality / changed defaults? - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`? - [x] New plotting function: A kind of plot you would like to seein `sc.pl`? - [ ] External tools: Do you know an existing package that should go into `sc.external.*`? - [ ] Other? <!-- Please describe your wishes below: -->. I am integrating multiple datasets. In the integrated object I wanted to see for each cluster the percentage of each dataset it consisted of (also see [this question](https://scanpy.discourse.group/t/cluster-statistics/134) in the scanpy discourse group). I wrote code for it on my own, but am not sure which part of `sc.pl` you want it to go to, so here is the code for your consideration:. ```python. import matplotlib.pyplot as plt. import scanpy as sc. import numpy as np. # given integrated object adata, clustered via the leiden algorithm and. # with the batch ID in the 'batch' slot, and a collection of batch_names:. # count the number of occurrences of each batch ID for each cluster ID. count_series = adata.obs.groupby(['leiden', 'batch']).size(). new_df = count_series.to_frame(name = 'size').reset_index(). # convert from multi index to pivot. constitution = new_df.pivot(index='leiden', columns='batch')['size']. # convert to %batch (but could be modified to show different things instead. perc_clust = np.array((constitution.T / np.sum(constitution.T, axis=0))). # keep track of the batch, cluster IDs so we can use them for plotting. clusters = adata.obs.leiden.cat.categories. batches = adata.obs.batch.cat.categories. # actual plotting; basically stacked barplots. # replace styling with scanpy defaults probably? fig, ax = plt.subplots(). ax.grid(False). ax.bar(clusters, perc_clust[0], 0.6, yerr=0, label=platy_names[0]). bottom = np.zeros(clusters.shape). for i, b in enume",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1573
https://github.com/scverse/scanpy/issues/1573:1016,availability,cluster,clustered,1016,"for integrated data; <!-- What kind of feature would you like to request? -->. - [ ] Additional function parameters / changed functionality / changed defaults? - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`? - [x] New plotting function: A kind of plot you would like to seein `sc.pl`? - [ ] External tools: Do you know an existing package that should go into `sc.external.*`? - [ ] Other? <!-- Please describe your wishes below: -->. I am integrating multiple datasets. In the integrated object I wanted to see for each cluster the percentage of each dataset it consisted of (also see [this question](https://scanpy.discourse.group/t/cluster-statistics/134) in the scanpy discourse group). I wrote code for it on my own, but am not sure which part of `sc.pl` you want it to go to, so here is the code for your consideration:. ```python. import matplotlib.pyplot as plt. import scanpy as sc. import numpy as np. # given integrated object adata, clustered via the leiden algorithm and. # with the batch ID in the 'batch' slot, and a collection of batch_names:. # count the number of occurrences of each batch ID for each cluster ID. count_series = adata.obs.groupby(['leiden', 'batch']).size(). new_df = count_series.to_frame(name = 'size').reset_index(). # convert from multi index to pivot. constitution = new_df.pivot(index='leiden', columns='batch')['size']. # convert to %batch (but could be modified to show different things instead. perc_clust = np.array((constitution.T / np.sum(constitution.T, axis=0))). # keep track of the batch, cluster IDs so we can use them for plotting. clusters = adata.obs.leiden.cat.categories. batches = adata.obs.batch.cat.categories. # actual plotting; basically stacked barplots. # replace styling with scanpy defaults probably? fig, ax = plt.subplots(). ax.grid(False). ax.bar(clusters, perc_clust[0], 0.6, yerr=0, label=platy_names[0]). bottom = np.zeros(clusters.shape). for i, b in enumerate(batches):. ax.ba",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1573
https://github.com/scverse/scanpy/issues/1573:1091,availability,slo,slot,1091,"? -->. - [ ] Additional function parameters / changed functionality / changed defaults? - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`? - [x] New plotting function: A kind of plot you would like to seein `sc.pl`? - [ ] External tools: Do you know an existing package that should go into `sc.external.*`? - [ ] Other? <!-- Please describe your wishes below: -->. I am integrating multiple datasets. In the integrated object I wanted to see for each cluster the percentage of each dataset it consisted of (also see [this question](https://scanpy.discourse.group/t/cluster-statistics/134) in the scanpy discourse group). I wrote code for it on my own, but am not sure which part of `sc.pl` you want it to go to, so here is the code for your consideration:. ```python. import matplotlib.pyplot as plt. import scanpy as sc. import numpy as np. # given integrated object adata, clustered via the leiden algorithm and. # with the batch ID in the 'batch' slot, and a collection of batch_names:. # count the number of occurrences of each batch ID for each cluster ID. count_series = adata.obs.groupby(['leiden', 'batch']).size(). new_df = count_series.to_frame(name = 'size').reset_index(). # convert from multi index to pivot. constitution = new_df.pivot(index='leiden', columns='batch')['size']. # convert to %batch (but could be modified to show different things instead. perc_clust = np.array((constitution.T / np.sum(constitution.T, axis=0))). # keep track of the batch, cluster IDs so we can use them for plotting. clusters = adata.obs.leiden.cat.categories. batches = adata.obs.batch.cat.categories. # actual plotting; basically stacked barplots. # replace styling with scanpy defaults probably? fig, ax = plt.subplots(). ax.grid(False). ax.bar(clusters, perc_clust[0], 0.6, yerr=0, label=platy_names[0]). bottom = np.zeros(clusters.shape). for i, b in enumerate(batches):. ax.bar(clusters, perc_clust[i], 0.6, bottom=bottom, yerr=0, label=batch_names",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1573
https://github.com/scverse/scanpy/issues/1573:1191,availability,cluster,cluster,1191,"ysis tool: A simple analysis tool you have been using and are missing in `sc.tools`? - [x] New plotting function: A kind of plot you would like to seein `sc.pl`? - [ ] External tools: Do you know an existing package that should go into `sc.external.*`? - [ ] Other? <!-- Please describe your wishes below: -->. I am integrating multiple datasets. In the integrated object I wanted to see for each cluster the percentage of each dataset it consisted of (also see [this question](https://scanpy.discourse.group/t/cluster-statistics/134) in the scanpy discourse group). I wrote code for it on my own, but am not sure which part of `sc.pl` you want it to go to, so here is the code for your consideration:. ```python. import matplotlib.pyplot as plt. import scanpy as sc. import numpy as np. # given integrated object adata, clustered via the leiden algorithm and. # with the batch ID in the 'batch' slot, and a collection of batch_names:. # count the number of occurrences of each batch ID for each cluster ID. count_series = adata.obs.groupby(['leiden', 'batch']).size(). new_df = count_series.to_frame(name = 'size').reset_index(). # convert from multi index to pivot. constitution = new_df.pivot(index='leiden', columns='batch')['size']. # convert to %batch (but could be modified to show different things instead. perc_clust = np.array((constitution.T / np.sum(constitution.T, axis=0))). # keep track of the batch, cluster IDs so we can use them for plotting. clusters = adata.obs.leiden.cat.categories. batches = adata.obs.batch.cat.categories. # actual plotting; basically stacked barplots. # replace styling with scanpy defaults probably? fig, ax = plt.subplots(). ax.grid(False). ax.bar(clusters, perc_clust[0], 0.6, yerr=0, label=platy_names[0]). bottom = np.zeros(clusters.shape). for i, b in enumerate(batches):. ax.bar(clusters, perc_clust[i], 0.6, bottom=bottom, yerr=0, label=batch_names[i]). bottom += perc_clust[I]. . # place legend not on top of the plot. ax.legend(loc='right', bbox_to",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1573
https://github.com/scverse/scanpy/issues/1573:1611,availability,cluster,cluster,1611,": Do you know an existing package that should go into `sc.external.*`? - [ ] Other? <!-- Please describe your wishes below: -->. I am integrating multiple datasets. In the integrated object I wanted to see for each cluster the percentage of each dataset it consisted of (also see [this question](https://scanpy.discourse.group/t/cluster-statistics/134) in the scanpy discourse group). I wrote code for it on my own, but am not sure which part of `sc.pl` you want it to go to, so here is the code for your consideration:. ```python. import matplotlib.pyplot as plt. import scanpy as sc. import numpy as np. # given integrated object adata, clustered via the leiden algorithm and. # with the batch ID in the 'batch' slot, and a collection of batch_names:. # count the number of occurrences of each batch ID for each cluster ID. count_series = adata.obs.groupby(['leiden', 'batch']).size(). new_df = count_series.to_frame(name = 'size').reset_index(). # convert from multi index to pivot. constitution = new_df.pivot(index='leiden', columns='batch')['size']. # convert to %batch (but could be modified to show different things instead. perc_clust = np.array((constitution.T / np.sum(constitution.T, axis=0))). # keep track of the batch, cluster IDs so we can use them for plotting. clusters = adata.obs.leiden.cat.categories. batches = adata.obs.batch.cat.categories. # actual plotting; basically stacked barplots. # replace styling with scanpy defaults probably? fig, ax = plt.subplots(). ax.grid(False). ax.bar(clusters, perc_clust[0], 0.6, yerr=0, label=platy_names[0]). bottom = np.zeros(clusters.shape). for i, b in enumerate(batches):. ax.bar(clusters, perc_clust[i], 0.6, bottom=bottom, yerr=0, label=batch_names[i]). bottom += perc_clust[I]. . # place legend not on top of the plot. ax.legend(loc='right', bbox_to_anchor=(1.2, 0.5));. ```. End result should look something like this. ![image](https://user-images.githubusercontent.com/1651067/104041019-ac463480-51d8-11eb-93ee-267deb4aa3f9.png).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1573
https://github.com/scverse/scanpy/issues/1573:1656,availability,cluster,clusters,1656,": Do you know an existing package that should go into `sc.external.*`? - [ ] Other? <!-- Please describe your wishes below: -->. I am integrating multiple datasets. In the integrated object I wanted to see for each cluster the percentage of each dataset it consisted of (also see [this question](https://scanpy.discourse.group/t/cluster-statistics/134) in the scanpy discourse group). I wrote code for it on my own, but am not sure which part of `sc.pl` you want it to go to, so here is the code for your consideration:. ```python. import matplotlib.pyplot as plt. import scanpy as sc. import numpy as np. # given integrated object adata, clustered via the leiden algorithm and. # with the batch ID in the 'batch' slot, and a collection of batch_names:. # count the number of occurrences of each batch ID for each cluster ID. count_series = adata.obs.groupby(['leiden', 'batch']).size(). new_df = count_series.to_frame(name = 'size').reset_index(). # convert from multi index to pivot. constitution = new_df.pivot(index='leiden', columns='batch')['size']. # convert to %batch (but could be modified to show different things instead. perc_clust = np.array((constitution.T / np.sum(constitution.T, axis=0))). # keep track of the batch, cluster IDs so we can use them for plotting. clusters = adata.obs.leiden.cat.categories. batches = adata.obs.batch.cat.categories. # actual plotting; basically stacked barplots. # replace styling with scanpy defaults probably? fig, ax = plt.subplots(). ax.grid(False). ax.bar(clusters, perc_clust[0], 0.6, yerr=0, label=platy_names[0]). bottom = np.zeros(clusters.shape). for i, b in enumerate(batches):. ax.bar(clusters, perc_clust[i], 0.6, bottom=bottom, yerr=0, label=batch_names[i]). bottom += perc_clust[I]. . # place legend not on top of the plot. ax.legend(loc='right', bbox_to_anchor=(1.2, 0.5));. ```. End result should look something like this. ![image](https://user-images.githubusercontent.com/1651067/104041019-ac463480-51d8-11eb-93ee-267deb4aa3f9.png).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1573
https://github.com/scverse/scanpy/issues/1573:1887,availability,cluster,clusters,1887,": Do you know an existing package that should go into `sc.external.*`? - [ ] Other? <!-- Please describe your wishes below: -->. I am integrating multiple datasets. In the integrated object I wanted to see for each cluster the percentage of each dataset it consisted of (also see [this question](https://scanpy.discourse.group/t/cluster-statistics/134) in the scanpy discourse group). I wrote code for it on my own, but am not sure which part of `sc.pl` you want it to go to, so here is the code for your consideration:. ```python. import matplotlib.pyplot as plt. import scanpy as sc. import numpy as np. # given integrated object adata, clustered via the leiden algorithm and. # with the batch ID in the 'batch' slot, and a collection of batch_names:. # count the number of occurrences of each batch ID for each cluster ID. count_series = adata.obs.groupby(['leiden', 'batch']).size(). new_df = count_series.to_frame(name = 'size').reset_index(). # convert from multi index to pivot. constitution = new_df.pivot(index='leiden', columns='batch')['size']. # convert to %batch (but could be modified to show different things instead. perc_clust = np.array((constitution.T / np.sum(constitution.T, axis=0))). # keep track of the batch, cluster IDs so we can use them for plotting. clusters = adata.obs.leiden.cat.categories. batches = adata.obs.batch.cat.categories. # actual plotting; basically stacked barplots. # replace styling with scanpy defaults probably? fig, ax = plt.subplots(). ax.grid(False). ax.bar(clusters, perc_clust[0], 0.6, yerr=0, label=platy_names[0]). bottom = np.zeros(clusters.shape). for i, b in enumerate(batches):. ax.bar(clusters, perc_clust[i], 0.6, bottom=bottom, yerr=0, label=batch_names[i]). bottom += perc_clust[I]. . # place legend not on top of the plot. ax.legend(loc='right', bbox_to_anchor=(1.2, 0.5));. ```. End result should look something like this. ![image](https://user-images.githubusercontent.com/1651067/104041019-ac463480-51d8-11eb-93ee-267deb4aa3f9.png).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1573
https://github.com/scverse/scanpy/issues/1573:1966,availability,cluster,clusters,1966,": Do you know an existing package that should go into `sc.external.*`? - [ ] Other? <!-- Please describe your wishes below: -->. I am integrating multiple datasets. In the integrated object I wanted to see for each cluster the percentage of each dataset it consisted of (also see [this question](https://scanpy.discourse.group/t/cluster-statistics/134) in the scanpy discourse group). I wrote code for it on my own, but am not sure which part of `sc.pl` you want it to go to, so here is the code for your consideration:. ```python. import matplotlib.pyplot as plt. import scanpy as sc. import numpy as np. # given integrated object adata, clustered via the leiden algorithm and. # with the batch ID in the 'batch' slot, and a collection of batch_names:. # count the number of occurrences of each batch ID for each cluster ID. count_series = adata.obs.groupby(['leiden', 'batch']).size(). new_df = count_series.to_frame(name = 'size').reset_index(). # convert from multi index to pivot. constitution = new_df.pivot(index='leiden', columns='batch')['size']. # convert to %batch (but could be modified to show different things instead. perc_clust = np.array((constitution.T / np.sum(constitution.T, axis=0))). # keep track of the batch, cluster IDs so we can use them for plotting. clusters = adata.obs.leiden.cat.categories. batches = adata.obs.batch.cat.categories. # actual plotting; basically stacked barplots. # replace styling with scanpy defaults probably? fig, ax = plt.subplots(). ax.grid(False). ax.bar(clusters, perc_clust[0], 0.6, yerr=0, label=platy_names[0]). bottom = np.zeros(clusters.shape). for i, b in enumerate(batches):. ax.bar(clusters, perc_clust[i], 0.6, bottom=bottom, yerr=0, label=batch_names[i]). bottom += perc_clust[I]. . # place legend not on top of the plot. ax.legend(loc='right', bbox_to_anchor=(1.2, 0.5));. ```. End result should look something like this. ![image](https://user-images.githubusercontent.com/1651067/104041019-ac463480-51d8-11eb-93ee-267deb4aa3f9.png).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1573
https://github.com/scverse/scanpy/issues/1573:2023,availability,cluster,clusters,2023,": Do you know an existing package that should go into `sc.external.*`? - [ ] Other? <!-- Please describe your wishes below: -->. I am integrating multiple datasets. In the integrated object I wanted to see for each cluster the percentage of each dataset it consisted of (also see [this question](https://scanpy.discourse.group/t/cluster-statistics/134) in the scanpy discourse group). I wrote code for it on my own, but am not sure which part of `sc.pl` you want it to go to, so here is the code for your consideration:. ```python. import matplotlib.pyplot as plt. import scanpy as sc. import numpy as np. # given integrated object adata, clustered via the leiden algorithm and. # with the batch ID in the 'batch' slot, and a collection of batch_names:. # count the number of occurrences of each batch ID for each cluster ID. count_series = adata.obs.groupby(['leiden', 'batch']).size(). new_df = count_series.to_frame(name = 'size').reset_index(). # convert from multi index to pivot. constitution = new_df.pivot(index='leiden', columns='batch')['size']. # convert to %batch (but could be modified to show different things instead. perc_clust = np.array((constitution.T / np.sum(constitution.T, axis=0))). # keep track of the batch, cluster IDs so we can use them for plotting. clusters = adata.obs.leiden.cat.categories. batches = adata.obs.batch.cat.categories. # actual plotting; basically stacked barplots. # replace styling with scanpy defaults probably? fig, ax = plt.subplots(). ax.grid(False). ax.bar(clusters, perc_clust[0], 0.6, yerr=0, label=platy_names[0]). bottom = np.zeros(clusters.shape). for i, b in enumerate(batches):. ax.bar(clusters, perc_clust[i], 0.6, bottom=bottom, yerr=0, label=batch_names[i]). bottom += perc_clust[I]. . # place legend not on top of the plot. ax.legend(loc='right', bbox_to_anchor=(1.2, 0.5));. ```. End result should look something like this. ![image](https://user-images.githubusercontent.com/1651067/104041019-ac463480-51d8-11eb-93ee-267deb4aa3f9.png).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1573
https://github.com/scverse/scanpy/issues/1573:0,deployability,Cluster,Cluster,0,"Cluster content plot for integrated data; <!-- What kind of feature would you like to request? -->. - [ ] Additional function parameters / changed functionality / changed defaults? - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`? - [x] New plotting function: A kind of plot you would like to seein `sc.pl`? - [ ] External tools: Do you know an existing package that should go into `sc.external.*`? - [ ] Other? <!-- Please describe your wishes below: -->. I am integrating multiple datasets. In the integrated object I wanted to see for each cluster the percentage of each dataset it consisted of (also see [this question](https://scanpy.discourse.group/t/cluster-statistics/134) in the scanpy discourse group). I wrote code for it on my own, but am not sure which part of `sc.pl` you want it to go to, so here is the code for your consideration:. ```python. import matplotlib.pyplot as plt. import scanpy as sc. import numpy as np. # given integrated object adata, clustered via the leiden algorithm and. # with the batch ID in the 'batch' slot, and a collection of batch_names:. # count the number of occurrences of each batch ID for each cluster ID. count_series = adata.obs.groupby(['leiden', 'batch']).size(). new_df = count_series.to_frame(name = 'size').reset_index(). # convert from multi index to pivot. constitution = new_df.pivot(index='leiden', columns='batch')['size']. # convert to %batch (but could be modified to show different things instead. perc_clust = np.array((constitution.T / np.sum(constitution.T, axis=0))). # keep track of the batch, cluster IDs so we can use them for plotting. clusters = adata.obs.leiden.cat.categories. batches = adata.obs.batch.cat.categories. # actual plotting; basically stacked barplots. # replace styling with scanpy defaults probably? fig, ax = plt.subplots(). ax.grid(False). ax.bar(clusters, perc_clust[0], 0.6, yerr=0, label=platy_names[0]). bottom = np.zeros(clusters.shape). for i, b in enume",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1573
https://github.com/scverse/scanpy/issues/1573:25,deployability,integr,integrated,25,"Cluster content plot for integrated data; <!-- What kind of feature would you like to request? -->. - [ ] Additional function parameters / changed functionality / changed defaults? - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`? - [x] New plotting function: A kind of plot you would like to seein `sc.pl`? - [ ] External tools: Do you know an existing package that should go into `sc.external.*`? - [ ] Other? <!-- Please describe your wishes below: -->. I am integrating multiple datasets. In the integrated object I wanted to see for each cluster the percentage of each dataset it consisted of (also see [this question](https://scanpy.discourse.group/t/cluster-statistics/134) in the scanpy discourse group). I wrote code for it on my own, but am not sure which part of `sc.pl` you want it to go to, so here is the code for your consideration:. ```python. import matplotlib.pyplot as plt. import scanpy as sc. import numpy as np. # given integrated object adata, clustered via the leiden algorithm and. # with the batch ID in the 'batch' slot, and a collection of batch_names:. # count the number of occurrences of each batch ID for each cluster ID. count_series = adata.obs.groupby(['leiden', 'batch']).size(). new_df = count_series.to_frame(name = 'size').reset_index(). # convert from multi index to pivot. constitution = new_df.pivot(index='leiden', columns='batch')['size']. # convert to %batch (but could be modified to show different things instead. perc_clust = np.array((constitution.T / np.sum(constitution.T, axis=0))). # keep track of the batch, cluster IDs so we can use them for plotting. clusters = adata.obs.leiden.cat.categories. batches = adata.obs.batch.cat.categories. # actual plotting; basically stacked barplots. # replace styling with scanpy defaults probably? fig, ax = plt.subplots(). ax.grid(False). ax.bar(clusters, perc_clust[0], 0.6, yerr=0, label=platy_names[0]). bottom = np.zeros(clusters.shape). for i, b in enume",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1573
https://github.com/scverse/scanpy/issues/1573:511,deployability,integr,integrating,511,"Cluster content plot for integrated data; <!-- What kind of feature would you like to request? -->. - [ ] Additional function parameters / changed functionality / changed defaults? - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`? - [x] New plotting function: A kind of plot you would like to seein `sc.pl`? - [ ] External tools: Do you know an existing package that should go into `sc.external.*`? - [ ] Other? <!-- Please describe your wishes below: -->. I am integrating multiple datasets. In the integrated object I wanted to see for each cluster the percentage of each dataset it consisted of (also see [this question](https://scanpy.discourse.group/t/cluster-statistics/134) in the scanpy discourse group). I wrote code for it on my own, but am not sure which part of `sc.pl` you want it to go to, so here is the code for your consideration:. ```python. import matplotlib.pyplot as plt. import scanpy as sc. import numpy as np. # given integrated object adata, clustered via the leiden algorithm and. # with the batch ID in the 'batch' slot, and a collection of batch_names:. # count the number of occurrences of each batch ID for each cluster ID. count_series = adata.obs.groupby(['leiden', 'batch']).size(). new_df = count_series.to_frame(name = 'size').reset_index(). # convert from multi index to pivot. constitution = new_df.pivot(index='leiden', columns='batch')['size']. # convert to %batch (but could be modified to show different things instead. perc_clust = np.array((constitution.T / np.sum(constitution.T, axis=0))). # keep track of the batch, cluster IDs so we can use them for plotting. clusters = adata.obs.leiden.cat.categories. batches = adata.obs.batch.cat.categories. # actual plotting; basically stacked barplots. # replace styling with scanpy defaults probably? fig, ax = plt.subplots(). ax.grid(False). ax.bar(clusters, perc_clust[0], 0.6, yerr=0, label=platy_names[0]). bottom = np.zeros(clusters.shape). for i, b in enume",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1573
https://github.com/scverse/scanpy/issues/1573:549,deployability,integr,integrated,549,"Cluster content plot for integrated data; <!-- What kind of feature would you like to request? -->. - [ ] Additional function parameters / changed functionality / changed defaults? - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`? - [x] New plotting function: A kind of plot you would like to seein `sc.pl`? - [ ] External tools: Do you know an existing package that should go into `sc.external.*`? - [ ] Other? <!-- Please describe your wishes below: -->. I am integrating multiple datasets. In the integrated object I wanted to see for each cluster the percentage of each dataset it consisted of (also see [this question](https://scanpy.discourse.group/t/cluster-statistics/134) in the scanpy discourse group). I wrote code for it on my own, but am not sure which part of `sc.pl` you want it to go to, so here is the code for your consideration:. ```python. import matplotlib.pyplot as plt. import scanpy as sc. import numpy as np. # given integrated object adata, clustered via the leiden algorithm and. # with the batch ID in the 'batch' slot, and a collection of batch_names:. # count the number of occurrences of each batch ID for each cluster ID. count_series = adata.obs.groupby(['leiden', 'batch']).size(). new_df = count_series.to_frame(name = 'size').reset_index(). # convert from multi index to pivot. constitution = new_df.pivot(index='leiden', columns='batch')['size']. # convert to %batch (but could be modified to show different things instead. perc_clust = np.array((constitution.T / np.sum(constitution.T, axis=0))). # keep track of the batch, cluster IDs so we can use them for plotting. clusters = adata.obs.leiden.cat.categories. batches = adata.obs.batch.cat.categories. # actual plotting; basically stacked barplots. # replace styling with scanpy defaults probably? fig, ax = plt.subplots(). ax.grid(False). ax.bar(clusters, perc_clust[0], 0.6, yerr=0, label=platy_names[0]). bottom = np.zeros(clusters.shape). for i, b in enume",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1573
https://github.com/scverse/scanpy/issues/1573:592,deployability,cluster,cluster,592,"Cluster content plot for integrated data; <!-- What kind of feature would you like to request? -->. - [ ] Additional function parameters / changed functionality / changed defaults? - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`? - [x] New plotting function: A kind of plot you would like to seein `sc.pl`? - [ ] External tools: Do you know an existing package that should go into `sc.external.*`? - [ ] Other? <!-- Please describe your wishes below: -->. I am integrating multiple datasets. In the integrated object I wanted to see for each cluster the percentage of each dataset it consisted of (also see [this question](https://scanpy.discourse.group/t/cluster-statistics/134) in the scanpy discourse group). I wrote code for it on my own, but am not sure which part of `sc.pl` you want it to go to, so here is the code for your consideration:. ```python. import matplotlib.pyplot as plt. import scanpy as sc. import numpy as np. # given integrated object adata, clustered via the leiden algorithm and. # with the batch ID in the 'batch' slot, and a collection of batch_names:. # count the number of occurrences of each batch ID for each cluster ID. count_series = adata.obs.groupby(['leiden', 'batch']).size(). new_df = count_series.to_frame(name = 'size').reset_index(). # convert from multi index to pivot. constitution = new_df.pivot(index='leiden', columns='batch')['size']. # convert to %batch (but could be modified to show different things instead. perc_clust = np.array((constitution.T / np.sum(constitution.T, axis=0))). # keep track of the batch, cluster IDs so we can use them for plotting. clusters = adata.obs.leiden.cat.categories. batches = adata.obs.batch.cat.categories. # actual plotting; basically stacked barplots. # replace styling with scanpy defaults probably? fig, ax = plt.subplots(). ax.grid(False). ax.bar(clusters, perc_clust[0], 0.6, yerr=0, label=platy_names[0]). bottom = np.zeros(clusters.shape). for i, b in enume",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1573
https://github.com/scverse/scanpy/issues/1573:706,deployability,cluster,cluster-statistics,706,"Cluster content plot for integrated data; <!-- What kind of feature would you like to request? -->. - [ ] Additional function parameters / changed functionality / changed defaults? - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`? - [x] New plotting function: A kind of plot you would like to seein `sc.pl`? - [ ] External tools: Do you know an existing package that should go into `sc.external.*`? - [ ] Other? <!-- Please describe your wishes below: -->. I am integrating multiple datasets. In the integrated object I wanted to see for each cluster the percentage of each dataset it consisted of (also see [this question](https://scanpy.discourse.group/t/cluster-statistics/134) in the scanpy discourse group). I wrote code for it on my own, but am not sure which part of `sc.pl` you want it to go to, so here is the code for your consideration:. ```python. import matplotlib.pyplot as plt. import scanpy as sc. import numpy as np. # given integrated object adata, clustered via the leiden algorithm and. # with the batch ID in the 'batch' slot, and a collection of batch_names:. # count the number of occurrences of each batch ID for each cluster ID. count_series = adata.obs.groupby(['leiden', 'batch']).size(). new_df = count_series.to_frame(name = 'size').reset_index(). # convert from multi index to pivot. constitution = new_df.pivot(index='leiden', columns='batch')['size']. # convert to %batch (but could be modified to show different things instead. perc_clust = np.array((constitution.T / np.sum(constitution.T, axis=0))). # keep track of the batch, cluster IDs so we can use them for plotting. clusters = adata.obs.leiden.cat.categories. batches = adata.obs.batch.cat.categories. # actual plotting; basically stacked barplots. # replace styling with scanpy defaults probably? fig, ax = plt.subplots(). ax.grid(False). ax.bar(clusters, perc_clust[0], 0.6, yerr=0, label=platy_names[0]). bottom = np.zeros(clusters.shape). for i, b in enume",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1573
https://github.com/scverse/scanpy/issues/1573:991,deployability,integr,integrated,991,"Cluster content plot for integrated data; <!-- What kind of feature would you like to request? -->. - [ ] Additional function parameters / changed functionality / changed defaults? - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`? - [x] New plotting function: A kind of plot you would like to seein `sc.pl`? - [ ] External tools: Do you know an existing package that should go into `sc.external.*`? - [ ] Other? <!-- Please describe your wishes below: -->. I am integrating multiple datasets. In the integrated object I wanted to see for each cluster the percentage of each dataset it consisted of (also see [this question](https://scanpy.discourse.group/t/cluster-statistics/134) in the scanpy discourse group). I wrote code for it on my own, but am not sure which part of `sc.pl` you want it to go to, so here is the code for your consideration:. ```python. import matplotlib.pyplot as plt. import scanpy as sc. import numpy as np. # given integrated object adata, clustered via the leiden algorithm and. # with the batch ID in the 'batch' slot, and a collection of batch_names:. # count the number of occurrences of each batch ID for each cluster ID. count_series = adata.obs.groupby(['leiden', 'batch']).size(). new_df = count_series.to_frame(name = 'size').reset_index(). # convert from multi index to pivot. constitution = new_df.pivot(index='leiden', columns='batch')['size']. # convert to %batch (but could be modified to show different things instead. perc_clust = np.array((constitution.T / np.sum(constitution.T, axis=0))). # keep track of the batch, cluster IDs so we can use them for plotting. clusters = adata.obs.leiden.cat.categories. batches = adata.obs.batch.cat.categories. # actual plotting; basically stacked barplots. # replace styling with scanpy defaults probably? fig, ax = plt.subplots(). ax.grid(False). ax.bar(clusters, perc_clust[0], 0.6, yerr=0, label=platy_names[0]). bottom = np.zeros(clusters.shape). for i, b in enume",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1573
https://github.com/scverse/scanpy/issues/1573:1016,deployability,cluster,clustered,1016,"for integrated data; <!-- What kind of feature would you like to request? -->. - [ ] Additional function parameters / changed functionality / changed defaults? - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`? - [x] New plotting function: A kind of plot you would like to seein `sc.pl`? - [ ] External tools: Do you know an existing package that should go into `sc.external.*`? - [ ] Other? <!-- Please describe your wishes below: -->. I am integrating multiple datasets. In the integrated object I wanted to see for each cluster the percentage of each dataset it consisted of (also see [this question](https://scanpy.discourse.group/t/cluster-statistics/134) in the scanpy discourse group). I wrote code for it on my own, but am not sure which part of `sc.pl` you want it to go to, so here is the code for your consideration:. ```python. import matplotlib.pyplot as plt. import scanpy as sc. import numpy as np. # given integrated object adata, clustered via the leiden algorithm and. # with the batch ID in the 'batch' slot, and a collection of batch_names:. # count the number of occurrences of each batch ID for each cluster ID. count_series = adata.obs.groupby(['leiden', 'batch']).size(). new_df = count_series.to_frame(name = 'size').reset_index(). # convert from multi index to pivot. constitution = new_df.pivot(index='leiden', columns='batch')['size']. # convert to %batch (but could be modified to show different things instead. perc_clust = np.array((constitution.T / np.sum(constitution.T, axis=0))). # keep track of the batch, cluster IDs so we can use them for plotting. clusters = adata.obs.leiden.cat.categories. batches = adata.obs.batch.cat.categories. # actual plotting; basically stacked barplots. # replace styling with scanpy defaults probably? fig, ax = plt.subplots(). ax.grid(False). ax.bar(clusters, perc_clust[0], 0.6, yerr=0, label=platy_names[0]). bottom = np.zeros(clusters.shape). for i, b in enumerate(batches):. ax.ba",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1573
https://github.com/scverse/scanpy/issues/1573:1191,deployability,cluster,cluster,1191,"ysis tool: A simple analysis tool you have been using and are missing in `sc.tools`? - [x] New plotting function: A kind of plot you would like to seein `sc.pl`? - [ ] External tools: Do you know an existing package that should go into `sc.external.*`? - [ ] Other? <!-- Please describe your wishes below: -->. I am integrating multiple datasets. In the integrated object I wanted to see for each cluster the percentage of each dataset it consisted of (also see [this question](https://scanpy.discourse.group/t/cluster-statistics/134) in the scanpy discourse group). I wrote code for it on my own, but am not sure which part of `sc.pl` you want it to go to, so here is the code for your consideration:. ```python. import matplotlib.pyplot as plt. import scanpy as sc. import numpy as np. # given integrated object adata, clustered via the leiden algorithm and. # with the batch ID in the 'batch' slot, and a collection of batch_names:. # count the number of occurrences of each batch ID for each cluster ID. count_series = adata.obs.groupby(['leiden', 'batch']).size(). new_df = count_series.to_frame(name = 'size').reset_index(). # convert from multi index to pivot. constitution = new_df.pivot(index='leiden', columns='batch')['size']. # convert to %batch (but could be modified to show different things instead. perc_clust = np.array((constitution.T / np.sum(constitution.T, axis=0))). # keep track of the batch, cluster IDs so we can use them for plotting. clusters = adata.obs.leiden.cat.categories. batches = adata.obs.batch.cat.categories. # actual plotting; basically stacked barplots. # replace styling with scanpy defaults probably? fig, ax = plt.subplots(). ax.grid(False). ax.bar(clusters, perc_clust[0], 0.6, yerr=0, label=platy_names[0]). bottom = np.zeros(clusters.shape). for i, b in enumerate(batches):. ax.bar(clusters, perc_clust[i], 0.6, bottom=bottom, yerr=0, label=batch_names[i]). bottom += perc_clust[I]. . # place legend not on top of the plot. ax.legend(loc='right', bbox_to",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1573
https://github.com/scverse/scanpy/issues/1573:1611,deployability,cluster,cluster,1611,": Do you know an existing package that should go into `sc.external.*`? - [ ] Other? <!-- Please describe your wishes below: -->. I am integrating multiple datasets. In the integrated object I wanted to see for each cluster the percentage of each dataset it consisted of (also see [this question](https://scanpy.discourse.group/t/cluster-statistics/134) in the scanpy discourse group). I wrote code for it on my own, but am not sure which part of `sc.pl` you want it to go to, so here is the code for your consideration:. ```python. import matplotlib.pyplot as plt. import scanpy as sc. import numpy as np. # given integrated object adata, clustered via the leiden algorithm and. # with the batch ID in the 'batch' slot, and a collection of batch_names:. # count the number of occurrences of each batch ID for each cluster ID. count_series = adata.obs.groupby(['leiden', 'batch']).size(). new_df = count_series.to_frame(name = 'size').reset_index(). # convert from multi index to pivot. constitution = new_df.pivot(index='leiden', columns='batch')['size']. # convert to %batch (but could be modified to show different things instead. perc_clust = np.array((constitution.T / np.sum(constitution.T, axis=0))). # keep track of the batch, cluster IDs so we can use them for plotting. clusters = adata.obs.leiden.cat.categories. batches = adata.obs.batch.cat.categories. # actual plotting; basically stacked barplots. # replace styling with scanpy defaults probably? fig, ax = plt.subplots(). ax.grid(False). ax.bar(clusters, perc_clust[0], 0.6, yerr=0, label=platy_names[0]). bottom = np.zeros(clusters.shape). for i, b in enumerate(batches):. ax.bar(clusters, perc_clust[i], 0.6, bottom=bottom, yerr=0, label=batch_names[i]). bottom += perc_clust[I]. . # place legend not on top of the plot. ax.legend(loc='right', bbox_to_anchor=(1.2, 0.5));. ```. End result should look something like this. ![image](https://user-images.githubusercontent.com/1651067/104041019-ac463480-51d8-11eb-93ee-267deb4aa3f9.png).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1573
https://github.com/scverse/scanpy/issues/1573:1656,deployability,cluster,clusters,1656,": Do you know an existing package that should go into `sc.external.*`? - [ ] Other? <!-- Please describe your wishes below: -->. I am integrating multiple datasets. In the integrated object I wanted to see for each cluster the percentage of each dataset it consisted of (also see [this question](https://scanpy.discourse.group/t/cluster-statistics/134) in the scanpy discourse group). I wrote code for it on my own, but am not sure which part of `sc.pl` you want it to go to, so here is the code for your consideration:. ```python. import matplotlib.pyplot as plt. import scanpy as sc. import numpy as np. # given integrated object adata, clustered via the leiden algorithm and. # with the batch ID in the 'batch' slot, and a collection of batch_names:. # count the number of occurrences of each batch ID for each cluster ID. count_series = adata.obs.groupby(['leiden', 'batch']).size(). new_df = count_series.to_frame(name = 'size').reset_index(). # convert from multi index to pivot. constitution = new_df.pivot(index='leiden', columns='batch')['size']. # convert to %batch (but could be modified to show different things instead. perc_clust = np.array((constitution.T / np.sum(constitution.T, axis=0))). # keep track of the batch, cluster IDs so we can use them for plotting. clusters = adata.obs.leiden.cat.categories. batches = adata.obs.batch.cat.categories. # actual plotting; basically stacked barplots. # replace styling with scanpy defaults probably? fig, ax = plt.subplots(). ax.grid(False). ax.bar(clusters, perc_clust[0], 0.6, yerr=0, label=platy_names[0]). bottom = np.zeros(clusters.shape). for i, b in enumerate(batches):. ax.bar(clusters, perc_clust[i], 0.6, bottom=bottom, yerr=0, label=batch_names[i]). bottom += perc_clust[I]. . # place legend not on top of the plot. ax.legend(loc='right', bbox_to_anchor=(1.2, 0.5));. ```. End result should look something like this. ![image](https://user-images.githubusercontent.com/1651067/104041019-ac463480-51d8-11eb-93ee-267deb4aa3f9.png).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1573
https://github.com/scverse/scanpy/issues/1573:1771,deployability,stack,stacked,1771,": Do you know an existing package that should go into `sc.external.*`? - [ ] Other? <!-- Please describe your wishes below: -->. I am integrating multiple datasets. In the integrated object I wanted to see for each cluster the percentage of each dataset it consisted of (also see [this question](https://scanpy.discourse.group/t/cluster-statistics/134) in the scanpy discourse group). I wrote code for it on my own, but am not sure which part of `sc.pl` you want it to go to, so here is the code for your consideration:. ```python. import matplotlib.pyplot as plt. import scanpy as sc. import numpy as np. # given integrated object adata, clustered via the leiden algorithm and. # with the batch ID in the 'batch' slot, and a collection of batch_names:. # count the number of occurrences of each batch ID for each cluster ID. count_series = adata.obs.groupby(['leiden', 'batch']).size(). new_df = count_series.to_frame(name = 'size').reset_index(). # convert from multi index to pivot. constitution = new_df.pivot(index='leiden', columns='batch')['size']. # convert to %batch (but could be modified to show different things instead. perc_clust = np.array((constitution.T / np.sum(constitution.T, axis=0))). # keep track of the batch, cluster IDs so we can use them for plotting. clusters = adata.obs.leiden.cat.categories. batches = adata.obs.batch.cat.categories. # actual plotting; basically stacked barplots. # replace styling with scanpy defaults probably? fig, ax = plt.subplots(). ax.grid(False). ax.bar(clusters, perc_clust[0], 0.6, yerr=0, label=platy_names[0]). bottom = np.zeros(clusters.shape). for i, b in enumerate(batches):. ax.bar(clusters, perc_clust[i], 0.6, bottom=bottom, yerr=0, label=batch_names[i]). bottom += perc_clust[I]. . # place legend not on top of the plot. ax.legend(loc='right', bbox_to_anchor=(1.2, 0.5));. ```. End result should look something like this. ![image](https://user-images.githubusercontent.com/1651067/104041019-ac463480-51d8-11eb-93ee-267deb4aa3f9.png).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1573
https://github.com/scverse/scanpy/issues/1573:1887,deployability,cluster,clusters,1887,": Do you know an existing package that should go into `sc.external.*`? - [ ] Other? <!-- Please describe your wishes below: -->. I am integrating multiple datasets. In the integrated object I wanted to see for each cluster the percentage of each dataset it consisted of (also see [this question](https://scanpy.discourse.group/t/cluster-statistics/134) in the scanpy discourse group). I wrote code for it on my own, but am not sure which part of `sc.pl` you want it to go to, so here is the code for your consideration:. ```python. import matplotlib.pyplot as plt. import scanpy as sc. import numpy as np. # given integrated object adata, clustered via the leiden algorithm and. # with the batch ID in the 'batch' slot, and a collection of batch_names:. # count the number of occurrences of each batch ID for each cluster ID. count_series = adata.obs.groupby(['leiden', 'batch']).size(). new_df = count_series.to_frame(name = 'size').reset_index(). # convert from multi index to pivot. constitution = new_df.pivot(index='leiden', columns='batch')['size']. # convert to %batch (but could be modified to show different things instead. perc_clust = np.array((constitution.T / np.sum(constitution.T, axis=0))). # keep track of the batch, cluster IDs so we can use them for plotting. clusters = adata.obs.leiden.cat.categories. batches = adata.obs.batch.cat.categories. # actual plotting; basically stacked barplots. # replace styling with scanpy defaults probably? fig, ax = plt.subplots(). ax.grid(False). ax.bar(clusters, perc_clust[0], 0.6, yerr=0, label=platy_names[0]). bottom = np.zeros(clusters.shape). for i, b in enumerate(batches):. ax.bar(clusters, perc_clust[i], 0.6, bottom=bottom, yerr=0, label=batch_names[i]). bottom += perc_clust[I]. . # place legend not on top of the plot. ax.legend(loc='right', bbox_to_anchor=(1.2, 0.5));. ```. End result should look something like this. ![image](https://user-images.githubusercontent.com/1651067/104041019-ac463480-51d8-11eb-93ee-267deb4aa3f9.png).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1573
https://github.com/scverse/scanpy/issues/1573:1966,deployability,cluster,clusters,1966,": Do you know an existing package that should go into `sc.external.*`? - [ ] Other? <!-- Please describe your wishes below: -->. I am integrating multiple datasets. In the integrated object I wanted to see for each cluster the percentage of each dataset it consisted of (also see [this question](https://scanpy.discourse.group/t/cluster-statistics/134) in the scanpy discourse group). I wrote code for it on my own, but am not sure which part of `sc.pl` you want it to go to, so here is the code for your consideration:. ```python. import matplotlib.pyplot as plt. import scanpy as sc. import numpy as np. # given integrated object adata, clustered via the leiden algorithm and. # with the batch ID in the 'batch' slot, and a collection of batch_names:. # count the number of occurrences of each batch ID for each cluster ID. count_series = adata.obs.groupby(['leiden', 'batch']).size(). new_df = count_series.to_frame(name = 'size').reset_index(). # convert from multi index to pivot. constitution = new_df.pivot(index='leiden', columns='batch')['size']. # convert to %batch (but could be modified to show different things instead. perc_clust = np.array((constitution.T / np.sum(constitution.T, axis=0))). # keep track of the batch, cluster IDs so we can use them for plotting. clusters = adata.obs.leiden.cat.categories. batches = adata.obs.batch.cat.categories. # actual plotting; basically stacked barplots. # replace styling with scanpy defaults probably? fig, ax = plt.subplots(). ax.grid(False). ax.bar(clusters, perc_clust[0], 0.6, yerr=0, label=platy_names[0]). bottom = np.zeros(clusters.shape). for i, b in enumerate(batches):. ax.bar(clusters, perc_clust[i], 0.6, bottom=bottom, yerr=0, label=batch_names[i]). bottom += perc_clust[I]. . # place legend not on top of the plot. ax.legend(loc='right', bbox_to_anchor=(1.2, 0.5));. ```. End result should look something like this. ![image](https://user-images.githubusercontent.com/1651067/104041019-ac463480-51d8-11eb-93ee-267deb4aa3f9.png).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1573
https://github.com/scverse/scanpy/issues/1573:2023,deployability,cluster,clusters,2023,": Do you know an existing package that should go into `sc.external.*`? - [ ] Other? <!-- Please describe your wishes below: -->. I am integrating multiple datasets. In the integrated object I wanted to see for each cluster the percentage of each dataset it consisted of (also see [this question](https://scanpy.discourse.group/t/cluster-statistics/134) in the scanpy discourse group). I wrote code for it on my own, but am not sure which part of `sc.pl` you want it to go to, so here is the code for your consideration:. ```python. import matplotlib.pyplot as plt. import scanpy as sc. import numpy as np. # given integrated object adata, clustered via the leiden algorithm and. # with the batch ID in the 'batch' slot, and a collection of batch_names:. # count the number of occurrences of each batch ID for each cluster ID. count_series = adata.obs.groupby(['leiden', 'batch']).size(). new_df = count_series.to_frame(name = 'size').reset_index(). # convert from multi index to pivot. constitution = new_df.pivot(index='leiden', columns='batch')['size']. # convert to %batch (but could be modified to show different things instead. perc_clust = np.array((constitution.T / np.sum(constitution.T, axis=0))). # keep track of the batch, cluster IDs so we can use them for plotting. clusters = adata.obs.leiden.cat.categories. batches = adata.obs.batch.cat.categories. # actual plotting; basically stacked barplots. # replace styling with scanpy defaults probably? fig, ax = plt.subplots(). ax.grid(False). ax.bar(clusters, perc_clust[0], 0.6, yerr=0, label=platy_names[0]). bottom = np.zeros(clusters.shape). for i, b in enumerate(batches):. ax.bar(clusters, perc_clust[i], 0.6, bottom=bottom, yerr=0, label=batch_names[i]). bottom += perc_clust[I]. . # place legend not on top of the plot. ax.legend(loc='right', bbox_to_anchor=(1.2, 0.5));. ```. End result should look something like this. ![image](https://user-images.githubusercontent.com/1651067/104041019-ac463480-51d8-11eb-93ee-267deb4aa3f9.png).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1573
https://github.com/scverse/scanpy/issues/1573:25,integrability,integr,integrated,25,"Cluster content plot for integrated data; <!-- What kind of feature would you like to request? -->. - [ ] Additional function parameters / changed functionality / changed defaults? - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`? - [x] New plotting function: A kind of plot you would like to seein `sc.pl`? - [ ] External tools: Do you know an existing package that should go into `sc.external.*`? - [ ] Other? <!-- Please describe your wishes below: -->. I am integrating multiple datasets. In the integrated object I wanted to see for each cluster the percentage of each dataset it consisted of (also see [this question](https://scanpy.discourse.group/t/cluster-statistics/134) in the scanpy discourse group). I wrote code for it on my own, but am not sure which part of `sc.pl` you want it to go to, so here is the code for your consideration:. ```python. import matplotlib.pyplot as plt. import scanpy as sc. import numpy as np. # given integrated object adata, clustered via the leiden algorithm and. # with the batch ID in the 'batch' slot, and a collection of batch_names:. # count the number of occurrences of each batch ID for each cluster ID. count_series = adata.obs.groupby(['leiden', 'batch']).size(). new_df = count_series.to_frame(name = 'size').reset_index(). # convert from multi index to pivot. constitution = new_df.pivot(index='leiden', columns='batch')['size']. # convert to %batch (but could be modified to show different things instead. perc_clust = np.array((constitution.T / np.sum(constitution.T, axis=0))). # keep track of the batch, cluster IDs so we can use them for plotting. clusters = adata.obs.leiden.cat.categories. batches = adata.obs.batch.cat.categories. # actual plotting; basically stacked barplots. # replace styling with scanpy defaults probably? fig, ax = plt.subplots(). ax.grid(False). ax.bar(clusters, perc_clust[0], 0.6, yerr=0, label=platy_names[0]). bottom = np.zeros(clusters.shape). for i, b in enume",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1573
https://github.com/scverse/scanpy/issues/1573:511,integrability,integr,integrating,511,"Cluster content plot for integrated data; <!-- What kind of feature would you like to request? -->. - [ ] Additional function parameters / changed functionality / changed defaults? - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`? - [x] New plotting function: A kind of plot you would like to seein `sc.pl`? - [ ] External tools: Do you know an existing package that should go into `sc.external.*`? - [ ] Other? <!-- Please describe your wishes below: -->. I am integrating multiple datasets. In the integrated object I wanted to see for each cluster the percentage of each dataset it consisted of (also see [this question](https://scanpy.discourse.group/t/cluster-statistics/134) in the scanpy discourse group). I wrote code for it on my own, but am not sure which part of `sc.pl` you want it to go to, so here is the code for your consideration:. ```python. import matplotlib.pyplot as plt. import scanpy as sc. import numpy as np. # given integrated object adata, clustered via the leiden algorithm and. # with the batch ID in the 'batch' slot, and a collection of batch_names:. # count the number of occurrences of each batch ID for each cluster ID. count_series = adata.obs.groupby(['leiden', 'batch']).size(). new_df = count_series.to_frame(name = 'size').reset_index(). # convert from multi index to pivot. constitution = new_df.pivot(index='leiden', columns='batch')['size']. # convert to %batch (but could be modified to show different things instead. perc_clust = np.array((constitution.T / np.sum(constitution.T, axis=0))). # keep track of the batch, cluster IDs so we can use them for plotting. clusters = adata.obs.leiden.cat.categories. batches = adata.obs.batch.cat.categories. # actual plotting; basically stacked barplots. # replace styling with scanpy defaults probably? fig, ax = plt.subplots(). ax.grid(False). ax.bar(clusters, perc_clust[0], 0.6, yerr=0, label=platy_names[0]). bottom = np.zeros(clusters.shape). for i, b in enume",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1573
https://github.com/scverse/scanpy/issues/1573:549,integrability,integr,integrated,549,"Cluster content plot for integrated data; <!-- What kind of feature would you like to request? -->. - [ ] Additional function parameters / changed functionality / changed defaults? - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`? - [x] New plotting function: A kind of plot you would like to seein `sc.pl`? - [ ] External tools: Do you know an existing package that should go into `sc.external.*`? - [ ] Other? <!-- Please describe your wishes below: -->. I am integrating multiple datasets. In the integrated object I wanted to see for each cluster the percentage of each dataset it consisted of (also see [this question](https://scanpy.discourse.group/t/cluster-statistics/134) in the scanpy discourse group). I wrote code for it on my own, but am not sure which part of `sc.pl` you want it to go to, so here is the code for your consideration:. ```python. import matplotlib.pyplot as plt. import scanpy as sc. import numpy as np. # given integrated object adata, clustered via the leiden algorithm and. # with the batch ID in the 'batch' slot, and a collection of batch_names:. # count the number of occurrences of each batch ID for each cluster ID. count_series = adata.obs.groupby(['leiden', 'batch']).size(). new_df = count_series.to_frame(name = 'size').reset_index(). # convert from multi index to pivot. constitution = new_df.pivot(index='leiden', columns='batch')['size']. # convert to %batch (but could be modified to show different things instead. perc_clust = np.array((constitution.T / np.sum(constitution.T, axis=0))). # keep track of the batch, cluster IDs so we can use them for plotting. clusters = adata.obs.leiden.cat.categories. batches = adata.obs.batch.cat.categories. # actual plotting; basically stacked barplots. # replace styling with scanpy defaults probably? fig, ax = plt.subplots(). ax.grid(False). ax.bar(clusters, perc_clust[0], 0.6, yerr=0, label=platy_names[0]). bottom = np.zeros(clusters.shape). for i, b in enume",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1573
https://github.com/scverse/scanpy/issues/1573:991,integrability,integr,integrated,991,"Cluster content plot for integrated data; <!-- What kind of feature would you like to request? -->. - [ ] Additional function parameters / changed functionality / changed defaults? - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`? - [x] New plotting function: A kind of plot you would like to seein `sc.pl`? - [ ] External tools: Do you know an existing package that should go into `sc.external.*`? - [ ] Other? <!-- Please describe your wishes below: -->. I am integrating multiple datasets. In the integrated object I wanted to see for each cluster the percentage of each dataset it consisted of (also see [this question](https://scanpy.discourse.group/t/cluster-statistics/134) in the scanpy discourse group). I wrote code for it on my own, but am not sure which part of `sc.pl` you want it to go to, so here is the code for your consideration:. ```python. import matplotlib.pyplot as plt. import scanpy as sc. import numpy as np. # given integrated object adata, clustered via the leiden algorithm and. # with the batch ID in the 'batch' slot, and a collection of batch_names:. # count the number of occurrences of each batch ID for each cluster ID. count_series = adata.obs.groupby(['leiden', 'batch']).size(). new_df = count_series.to_frame(name = 'size').reset_index(). # convert from multi index to pivot. constitution = new_df.pivot(index='leiden', columns='batch')['size']. # convert to %batch (but could be modified to show different things instead. perc_clust = np.array((constitution.T / np.sum(constitution.T, axis=0))). # keep track of the batch, cluster IDs so we can use them for plotting. clusters = adata.obs.leiden.cat.categories. batches = adata.obs.batch.cat.categories. # actual plotting; basically stacked barplots. # replace styling with scanpy defaults probably? fig, ax = plt.subplots(). ax.grid(False). ax.bar(clusters, perc_clust[0], 0.6, yerr=0, label=platy_names[0]). bottom = np.zeros(clusters.shape). for i, b in enume",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1573
https://github.com/scverse/scanpy/issues/1573:1067,integrability,batch,batch,1067,"uld you like to request? -->. - [ ] Additional function parameters / changed functionality / changed defaults? - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`? - [x] New plotting function: A kind of plot you would like to seein `sc.pl`? - [ ] External tools: Do you know an existing package that should go into `sc.external.*`? - [ ] Other? <!-- Please describe your wishes below: -->. I am integrating multiple datasets. In the integrated object I wanted to see for each cluster the percentage of each dataset it consisted of (also see [this question](https://scanpy.discourse.group/t/cluster-statistics/134) in the scanpy discourse group). I wrote code for it on my own, but am not sure which part of `sc.pl` you want it to go to, so here is the code for your consideration:. ```python. import matplotlib.pyplot as plt. import scanpy as sc. import numpy as np. # given integrated object adata, clustered via the leiden algorithm and. # with the batch ID in the 'batch' slot, and a collection of batch_names:. # count the number of occurrences of each batch ID for each cluster ID. count_series = adata.obs.groupby(['leiden', 'batch']).size(). new_df = count_series.to_frame(name = 'size').reset_index(). # convert from multi index to pivot. constitution = new_df.pivot(index='leiden', columns='batch')['size']. # convert to %batch (but could be modified to show different things instead. perc_clust = np.array((constitution.T / np.sum(constitution.T, axis=0))). # keep track of the batch, cluster IDs so we can use them for plotting. clusters = adata.obs.leiden.cat.categories. batches = adata.obs.batch.cat.categories. # actual plotting; basically stacked barplots. # replace styling with scanpy defaults probably? fig, ax = plt.subplots(). ax.grid(False). ax.bar(clusters, perc_clust[0], 0.6, yerr=0, label=platy_names[0]). bottom = np.zeros(clusters.shape). for i, b in enumerate(batches):. ax.bar(clusters, perc_clust[i], 0.6, bottom=bottom, ye",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1573
https://github.com/scverse/scanpy/issues/1573:1084,integrability,batch,batch,1084,"equest? -->. - [ ] Additional function parameters / changed functionality / changed defaults? - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`? - [x] New plotting function: A kind of plot you would like to seein `sc.pl`? - [ ] External tools: Do you know an existing package that should go into `sc.external.*`? - [ ] Other? <!-- Please describe your wishes below: -->. I am integrating multiple datasets. In the integrated object I wanted to see for each cluster the percentage of each dataset it consisted of (also see [this question](https://scanpy.discourse.group/t/cluster-statistics/134) in the scanpy discourse group). I wrote code for it on my own, but am not sure which part of `sc.pl` you want it to go to, so here is the code for your consideration:. ```python. import matplotlib.pyplot as plt. import scanpy as sc. import numpy as np. # given integrated object adata, clustered via the leiden algorithm and. # with the batch ID in the 'batch' slot, and a collection of batch_names:. # count the number of occurrences of each batch ID for each cluster ID. count_series = adata.obs.groupby(['leiden', 'batch']).size(). new_df = count_series.to_frame(name = 'size').reset_index(). # convert from multi index to pivot. constitution = new_df.pivot(index='leiden', columns='batch')['size']. # convert to %batch (but could be modified to show different things instead. perc_clust = np.array((constitution.T / np.sum(constitution.T, axis=0))). # keep track of the batch, cluster IDs so we can use them for plotting. clusters = adata.obs.leiden.cat.categories. batches = adata.obs.batch.cat.categories. # actual plotting; basically stacked barplots. # replace styling with scanpy defaults probably? fig, ax = plt.subplots(). ax.grid(False). ax.bar(clusters, perc_clust[0], 0.6, yerr=0, label=platy_names[0]). bottom = np.zeros(clusters.shape). for i, b in enumerate(batches):. ax.bar(clusters, perc_clust[i], 0.6, bottom=bottom, yerr=0, label=batch",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1573
https://github.com/scverse/scanpy/issues/1573:1173,integrability,batch,batch,1173,"lts? - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`? - [x] New plotting function: A kind of plot you would like to seein `sc.pl`? - [ ] External tools: Do you know an existing package that should go into `sc.external.*`? - [ ] Other? <!-- Please describe your wishes below: -->. I am integrating multiple datasets. In the integrated object I wanted to see for each cluster the percentage of each dataset it consisted of (also see [this question](https://scanpy.discourse.group/t/cluster-statistics/134) in the scanpy discourse group). I wrote code for it on my own, but am not sure which part of `sc.pl` you want it to go to, so here is the code for your consideration:. ```python. import matplotlib.pyplot as plt. import scanpy as sc. import numpy as np. # given integrated object adata, clustered via the leiden algorithm and. # with the batch ID in the 'batch' slot, and a collection of batch_names:. # count the number of occurrences of each batch ID for each cluster ID. count_series = adata.obs.groupby(['leiden', 'batch']).size(). new_df = count_series.to_frame(name = 'size').reset_index(). # convert from multi index to pivot. constitution = new_df.pivot(index='leiden', columns='batch')['size']. # convert to %batch (but could be modified to show different things instead. perc_clust = np.array((constitution.T / np.sum(constitution.T, axis=0))). # keep track of the batch, cluster IDs so we can use them for plotting. clusters = adata.obs.leiden.cat.categories. batches = adata.obs.batch.cat.categories. # actual plotting; basically stacked barplots. # replace styling with scanpy defaults probably? fig, ax = plt.subplots(). ax.grid(False). ax.bar(clusters, perc_clust[0], 0.6, yerr=0, label=platy_names[0]). bottom = np.zeros(clusters.shape). for i, b in enumerate(batches):. ax.bar(clusters, perc_clust[i], 0.6, bottom=bottom, yerr=0, label=batch_names[i]). bottom += perc_clust[I]. . # place legend not on top of the plot. ax.legend(l",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1573
https://github.com/scverse/scanpy/issues/1573:1248,integrability,batch,batch,1248,"d are missing in `sc.tools`? - [x] New plotting function: A kind of plot you would like to seein `sc.pl`? - [ ] External tools: Do you know an existing package that should go into `sc.external.*`? - [ ] Other? <!-- Please describe your wishes below: -->. I am integrating multiple datasets. In the integrated object I wanted to see for each cluster the percentage of each dataset it consisted of (also see [this question](https://scanpy.discourse.group/t/cluster-statistics/134) in the scanpy discourse group). I wrote code for it on my own, but am not sure which part of `sc.pl` you want it to go to, so here is the code for your consideration:. ```python. import matplotlib.pyplot as plt. import scanpy as sc. import numpy as np. # given integrated object adata, clustered via the leiden algorithm and. # with the batch ID in the 'batch' slot, and a collection of batch_names:. # count the number of occurrences of each batch ID for each cluster ID. count_series = adata.obs.groupby(['leiden', 'batch']).size(). new_df = count_series.to_frame(name = 'size').reset_index(). # convert from multi index to pivot. constitution = new_df.pivot(index='leiden', columns='batch')['size']. # convert to %batch (but could be modified to show different things instead. perc_clust = np.array((constitution.T / np.sum(constitution.T, axis=0))). # keep track of the batch, cluster IDs so we can use them for plotting. clusters = adata.obs.leiden.cat.categories. batches = adata.obs.batch.cat.categories. # actual plotting; basically stacked barplots. # replace styling with scanpy defaults probably? fig, ax = plt.subplots(). ax.grid(False). ax.bar(clusters, perc_clust[0], 0.6, yerr=0, label=platy_names[0]). bottom = np.zeros(clusters.shape). for i, b in enumerate(batches):. ax.bar(clusters, perc_clust[i], 0.6, bottom=bottom, yerr=0, label=batch_names[i]). bottom += perc_clust[I]. . # place legend not on top of the plot. ax.legend(loc='right', bbox_to_anchor=(1.2, 0.5));. ```. End result should look someth",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1573
https://github.com/scverse/scanpy/issues/1573:1416,integrability,batch,batch,1416,": Do you know an existing package that should go into `sc.external.*`? - [ ] Other? <!-- Please describe your wishes below: -->. I am integrating multiple datasets. In the integrated object I wanted to see for each cluster the percentage of each dataset it consisted of (also see [this question](https://scanpy.discourse.group/t/cluster-statistics/134) in the scanpy discourse group). I wrote code for it on my own, but am not sure which part of `sc.pl` you want it to go to, so here is the code for your consideration:. ```python. import matplotlib.pyplot as plt. import scanpy as sc. import numpy as np. # given integrated object adata, clustered via the leiden algorithm and. # with the batch ID in the 'batch' slot, and a collection of batch_names:. # count the number of occurrences of each batch ID for each cluster ID. count_series = adata.obs.groupby(['leiden', 'batch']).size(). new_df = count_series.to_frame(name = 'size').reset_index(). # convert from multi index to pivot. constitution = new_df.pivot(index='leiden', columns='batch')['size']. # convert to %batch (but could be modified to show different things instead. perc_clust = np.array((constitution.T / np.sum(constitution.T, axis=0))). # keep track of the batch, cluster IDs so we can use them for plotting. clusters = adata.obs.leiden.cat.categories. batches = adata.obs.batch.cat.categories. # actual plotting; basically stacked barplots. # replace styling with scanpy defaults probably? fig, ax = plt.subplots(). ax.grid(False). ax.bar(clusters, perc_clust[0], 0.6, yerr=0, label=platy_names[0]). bottom = np.zeros(clusters.shape). for i, b in enumerate(batches):. ax.bar(clusters, perc_clust[i], 0.6, bottom=bottom, yerr=0, label=batch_names[i]). bottom += perc_clust[I]. . # place legend not on top of the plot. ax.legend(loc='right', bbox_to_anchor=(1.2, 0.5));. ```. End result should look something like this. ![image](https://user-images.githubusercontent.com/1651067/104041019-ac463480-51d8-11eb-93ee-267deb4aa3f9.png).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1573
https://github.com/scverse/scanpy/issues/1573:1447,integrability,batch,batch,1447,": Do you know an existing package that should go into `sc.external.*`? - [ ] Other? <!-- Please describe your wishes below: -->. I am integrating multiple datasets. In the integrated object I wanted to see for each cluster the percentage of each dataset it consisted of (also see [this question](https://scanpy.discourse.group/t/cluster-statistics/134) in the scanpy discourse group). I wrote code for it on my own, but am not sure which part of `sc.pl` you want it to go to, so here is the code for your consideration:. ```python. import matplotlib.pyplot as plt. import scanpy as sc. import numpy as np. # given integrated object adata, clustered via the leiden algorithm and. # with the batch ID in the 'batch' slot, and a collection of batch_names:. # count the number of occurrences of each batch ID for each cluster ID. count_series = adata.obs.groupby(['leiden', 'batch']).size(). new_df = count_series.to_frame(name = 'size').reset_index(). # convert from multi index to pivot. constitution = new_df.pivot(index='leiden', columns='batch')['size']. # convert to %batch (but could be modified to show different things instead. perc_clust = np.array((constitution.T / np.sum(constitution.T, axis=0))). # keep track of the batch, cluster IDs so we can use them for plotting. clusters = adata.obs.leiden.cat.categories. batches = adata.obs.batch.cat.categories. # actual plotting; basically stacked barplots. # replace styling with scanpy defaults probably? fig, ax = plt.subplots(). ax.grid(False). ax.bar(clusters, perc_clust[0], 0.6, yerr=0, label=platy_names[0]). bottom = np.zeros(clusters.shape). for i, b in enumerate(batches):. ax.bar(clusters, perc_clust[i], 0.6, bottom=bottom, yerr=0, label=batch_names[i]). bottom += perc_clust[I]. . # place legend not on top of the plot. ax.legend(loc='right', bbox_to_anchor=(1.2, 0.5));. ```. End result should look something like this. ![image](https://user-images.githubusercontent.com/1651067/104041019-ac463480-51d8-11eb-93ee-267deb4aa3f9.png).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1573
https://github.com/scverse/scanpy/issues/1573:1604,integrability,batch,batch,1604,": Do you know an existing package that should go into `sc.external.*`? - [ ] Other? <!-- Please describe your wishes below: -->. I am integrating multiple datasets. In the integrated object I wanted to see for each cluster the percentage of each dataset it consisted of (also see [this question](https://scanpy.discourse.group/t/cluster-statistics/134) in the scanpy discourse group). I wrote code for it on my own, but am not sure which part of `sc.pl` you want it to go to, so here is the code for your consideration:. ```python. import matplotlib.pyplot as plt. import scanpy as sc. import numpy as np. # given integrated object adata, clustered via the leiden algorithm and. # with the batch ID in the 'batch' slot, and a collection of batch_names:. # count the number of occurrences of each batch ID for each cluster ID. count_series = adata.obs.groupby(['leiden', 'batch']).size(). new_df = count_series.to_frame(name = 'size').reset_index(). # convert from multi index to pivot. constitution = new_df.pivot(index='leiden', columns='batch')['size']. # convert to %batch (but could be modified to show different things instead. perc_clust = np.array((constitution.T / np.sum(constitution.T, axis=0))). # keep track of the batch, cluster IDs so we can use them for plotting. clusters = adata.obs.leiden.cat.categories. batches = adata.obs.batch.cat.categories. # actual plotting; basically stacked barplots. # replace styling with scanpy defaults probably? fig, ax = plt.subplots(). ax.grid(False). ax.bar(clusters, perc_clust[0], 0.6, yerr=0, label=platy_names[0]). bottom = np.zeros(clusters.shape). for i, b in enumerate(batches):. ax.bar(clusters, perc_clust[i], 0.6, bottom=bottom, yerr=0, label=batch_names[i]). bottom += perc_clust[I]. . # place legend not on top of the plot. ax.legend(loc='right', bbox_to_anchor=(1.2, 0.5));. ```. End result should look something like this. ![image](https://user-images.githubusercontent.com/1651067/104041019-ac463480-51d8-11eb-93ee-267deb4aa3f9.png).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1573
https://github.com/scverse/scanpy/issues/1573:1700,integrability,batch,batches,1700,": Do you know an existing package that should go into `sc.external.*`? - [ ] Other? <!-- Please describe your wishes below: -->. I am integrating multiple datasets. In the integrated object I wanted to see for each cluster the percentage of each dataset it consisted of (also see [this question](https://scanpy.discourse.group/t/cluster-statistics/134) in the scanpy discourse group). I wrote code for it on my own, but am not sure which part of `sc.pl` you want it to go to, so here is the code for your consideration:. ```python. import matplotlib.pyplot as plt. import scanpy as sc. import numpy as np. # given integrated object adata, clustered via the leiden algorithm and. # with the batch ID in the 'batch' slot, and a collection of batch_names:. # count the number of occurrences of each batch ID for each cluster ID. count_series = adata.obs.groupby(['leiden', 'batch']).size(). new_df = count_series.to_frame(name = 'size').reset_index(). # convert from multi index to pivot. constitution = new_df.pivot(index='leiden', columns='batch')['size']. # convert to %batch (but could be modified to show different things instead. perc_clust = np.array((constitution.T / np.sum(constitution.T, axis=0))). # keep track of the batch, cluster IDs so we can use them for plotting. clusters = adata.obs.leiden.cat.categories. batches = adata.obs.batch.cat.categories. # actual plotting; basically stacked barplots. # replace styling with scanpy defaults probably? fig, ax = plt.subplots(). ax.grid(False). ax.bar(clusters, perc_clust[0], 0.6, yerr=0, label=platy_names[0]). bottom = np.zeros(clusters.shape). for i, b in enumerate(batches):. ax.bar(clusters, perc_clust[i], 0.6, bottom=bottom, yerr=0, label=batch_names[i]). bottom += perc_clust[I]. . # place legend not on top of the plot. ax.legend(loc='right', bbox_to_anchor=(1.2, 0.5));. ```. End result should look something like this. ![image](https://user-images.githubusercontent.com/1651067/104041019-ac463480-51d8-11eb-93ee-267deb4aa3f9.png).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1573
https://github.com/scverse/scanpy/issues/1573:1720,integrability,batch,batch,1720,": Do you know an existing package that should go into `sc.external.*`? - [ ] Other? <!-- Please describe your wishes below: -->. I am integrating multiple datasets. In the integrated object I wanted to see for each cluster the percentage of each dataset it consisted of (also see [this question](https://scanpy.discourse.group/t/cluster-statistics/134) in the scanpy discourse group). I wrote code for it on my own, but am not sure which part of `sc.pl` you want it to go to, so here is the code for your consideration:. ```python. import matplotlib.pyplot as plt. import scanpy as sc. import numpy as np. # given integrated object adata, clustered via the leiden algorithm and. # with the batch ID in the 'batch' slot, and a collection of batch_names:. # count the number of occurrences of each batch ID for each cluster ID. count_series = adata.obs.groupby(['leiden', 'batch']).size(). new_df = count_series.to_frame(name = 'size').reset_index(). # convert from multi index to pivot. constitution = new_df.pivot(index='leiden', columns='batch')['size']. # convert to %batch (but could be modified to show different things instead. perc_clust = np.array((constitution.T / np.sum(constitution.T, axis=0))). # keep track of the batch, cluster IDs so we can use them for plotting. clusters = adata.obs.leiden.cat.categories. batches = adata.obs.batch.cat.categories. # actual plotting; basically stacked barplots. # replace styling with scanpy defaults probably? fig, ax = plt.subplots(). ax.grid(False). ax.bar(clusters, perc_clust[0], 0.6, yerr=0, label=platy_names[0]). bottom = np.zeros(clusters.shape). for i, b in enumerate(batches):. ax.bar(clusters, perc_clust[i], 0.6, bottom=bottom, yerr=0, label=batch_names[i]). bottom += perc_clust[I]. . # place legend not on top of the plot. ax.legend(loc='right', bbox_to_anchor=(1.2, 0.5));. ```. End result should look something like this. ![image](https://user-images.githubusercontent.com/1651067/104041019-ac463480-51d8-11eb-93ee-267deb4aa3f9.png).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1573
https://github.com/scverse/scanpy/issues/1573:1852,integrability,sub,subplots,1852,": Do you know an existing package that should go into `sc.external.*`? - [ ] Other? <!-- Please describe your wishes below: -->. I am integrating multiple datasets. In the integrated object I wanted to see for each cluster the percentage of each dataset it consisted of (also see [this question](https://scanpy.discourse.group/t/cluster-statistics/134) in the scanpy discourse group). I wrote code for it on my own, but am not sure which part of `sc.pl` you want it to go to, so here is the code for your consideration:. ```python. import matplotlib.pyplot as plt. import scanpy as sc. import numpy as np. # given integrated object adata, clustered via the leiden algorithm and. # with the batch ID in the 'batch' slot, and a collection of batch_names:. # count the number of occurrences of each batch ID for each cluster ID. count_series = adata.obs.groupby(['leiden', 'batch']).size(). new_df = count_series.to_frame(name = 'size').reset_index(). # convert from multi index to pivot. constitution = new_df.pivot(index='leiden', columns='batch')['size']. # convert to %batch (but could be modified to show different things instead. perc_clust = np.array((constitution.T / np.sum(constitution.T, axis=0))). # keep track of the batch, cluster IDs so we can use them for plotting. clusters = adata.obs.leiden.cat.categories. batches = adata.obs.batch.cat.categories. # actual plotting; basically stacked barplots. # replace styling with scanpy defaults probably? fig, ax = plt.subplots(). ax.grid(False). ax.bar(clusters, perc_clust[0], 0.6, yerr=0, label=platy_names[0]). bottom = np.zeros(clusters.shape). for i, b in enumerate(batches):. ax.bar(clusters, perc_clust[i], 0.6, bottom=bottom, yerr=0, label=batch_names[i]). bottom += perc_clust[I]. . # place legend not on top of the plot. ax.legend(loc='right', bbox_to_anchor=(1.2, 0.5));. ```. End result should look something like this. ![image](https://user-images.githubusercontent.com/1651067/104041019-ac463480-51d8-11eb-93ee-267deb4aa3f9.png).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1573
https://github.com/scverse/scanpy/issues/1573:2005,integrability,batch,batches,2005,": Do you know an existing package that should go into `sc.external.*`? - [ ] Other? <!-- Please describe your wishes below: -->. I am integrating multiple datasets. In the integrated object I wanted to see for each cluster the percentage of each dataset it consisted of (also see [this question](https://scanpy.discourse.group/t/cluster-statistics/134) in the scanpy discourse group). I wrote code for it on my own, but am not sure which part of `sc.pl` you want it to go to, so here is the code for your consideration:. ```python. import matplotlib.pyplot as plt. import scanpy as sc. import numpy as np. # given integrated object adata, clustered via the leiden algorithm and. # with the batch ID in the 'batch' slot, and a collection of batch_names:. # count the number of occurrences of each batch ID for each cluster ID. count_series = adata.obs.groupby(['leiden', 'batch']).size(). new_df = count_series.to_frame(name = 'size').reset_index(). # convert from multi index to pivot. constitution = new_df.pivot(index='leiden', columns='batch')['size']. # convert to %batch (but could be modified to show different things instead. perc_clust = np.array((constitution.T / np.sum(constitution.T, axis=0))). # keep track of the batch, cluster IDs so we can use them for plotting. clusters = adata.obs.leiden.cat.categories. batches = adata.obs.batch.cat.categories. # actual plotting; basically stacked barplots. # replace styling with scanpy defaults probably? fig, ax = plt.subplots(). ax.grid(False). ax.bar(clusters, perc_clust[0], 0.6, yerr=0, label=platy_names[0]). bottom = np.zeros(clusters.shape). for i, b in enumerate(batches):. ax.bar(clusters, perc_clust[i], 0.6, bottom=bottom, yerr=0, label=batch_names[i]). bottom += perc_clust[I]. . # place legend not on top of the plot. ax.legend(loc='right', bbox_to_anchor=(1.2, 0.5));. ```. End result should look something like this. ![image](https://user-images.githubusercontent.com/1651067/104041019-ac463480-51d8-11eb-93ee-267deb4aa3f9.png).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1573
https://github.com/scverse/scanpy/issues/1573:25,interoperability,integr,integrated,25,"Cluster content plot for integrated data; <!-- What kind of feature would you like to request? -->. - [ ] Additional function parameters / changed functionality / changed defaults? - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`? - [x] New plotting function: A kind of plot you would like to seein `sc.pl`? - [ ] External tools: Do you know an existing package that should go into `sc.external.*`? - [ ] Other? <!-- Please describe your wishes below: -->. I am integrating multiple datasets. In the integrated object I wanted to see for each cluster the percentage of each dataset it consisted of (also see [this question](https://scanpy.discourse.group/t/cluster-statistics/134) in the scanpy discourse group). I wrote code for it on my own, but am not sure which part of `sc.pl` you want it to go to, so here is the code for your consideration:. ```python. import matplotlib.pyplot as plt. import scanpy as sc. import numpy as np. # given integrated object adata, clustered via the leiden algorithm and. # with the batch ID in the 'batch' slot, and a collection of batch_names:. # count the number of occurrences of each batch ID for each cluster ID. count_series = adata.obs.groupby(['leiden', 'batch']).size(). new_df = count_series.to_frame(name = 'size').reset_index(). # convert from multi index to pivot. constitution = new_df.pivot(index='leiden', columns='batch')['size']. # convert to %batch (but could be modified to show different things instead. perc_clust = np.array((constitution.T / np.sum(constitution.T, axis=0))). # keep track of the batch, cluster IDs so we can use them for plotting. clusters = adata.obs.leiden.cat.categories. batches = adata.obs.batch.cat.categories. # actual plotting; basically stacked barplots. # replace styling with scanpy defaults probably? fig, ax = plt.subplots(). ax.grid(False). ax.bar(clusters, perc_clust[0], 0.6, yerr=0, label=platy_names[0]). bottom = np.zeros(clusters.shape). for i, b in enume",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1573
https://github.com/scverse/scanpy/issues/1573:511,interoperability,integr,integrating,511,"Cluster content plot for integrated data; <!-- What kind of feature would you like to request? -->. - [ ] Additional function parameters / changed functionality / changed defaults? - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`? - [x] New plotting function: A kind of plot you would like to seein `sc.pl`? - [ ] External tools: Do you know an existing package that should go into `sc.external.*`? - [ ] Other? <!-- Please describe your wishes below: -->. I am integrating multiple datasets. In the integrated object I wanted to see for each cluster the percentage of each dataset it consisted of (also see [this question](https://scanpy.discourse.group/t/cluster-statistics/134) in the scanpy discourse group). I wrote code for it on my own, but am not sure which part of `sc.pl` you want it to go to, so here is the code for your consideration:. ```python. import matplotlib.pyplot as plt. import scanpy as sc. import numpy as np. # given integrated object adata, clustered via the leiden algorithm and. # with the batch ID in the 'batch' slot, and a collection of batch_names:. # count the number of occurrences of each batch ID for each cluster ID. count_series = adata.obs.groupby(['leiden', 'batch']).size(). new_df = count_series.to_frame(name = 'size').reset_index(). # convert from multi index to pivot. constitution = new_df.pivot(index='leiden', columns='batch')['size']. # convert to %batch (but could be modified to show different things instead. perc_clust = np.array((constitution.T / np.sum(constitution.T, axis=0))). # keep track of the batch, cluster IDs so we can use them for plotting. clusters = adata.obs.leiden.cat.categories. batches = adata.obs.batch.cat.categories. # actual plotting; basically stacked barplots. # replace styling with scanpy defaults probably? fig, ax = plt.subplots(). ax.grid(False). ax.bar(clusters, perc_clust[0], 0.6, yerr=0, label=platy_names[0]). bottom = np.zeros(clusters.shape). for i, b in enume",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1573
https://github.com/scverse/scanpy/issues/1573:549,interoperability,integr,integrated,549,"Cluster content plot for integrated data; <!-- What kind of feature would you like to request? -->. - [ ] Additional function parameters / changed functionality / changed defaults? - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`? - [x] New plotting function: A kind of plot you would like to seein `sc.pl`? - [ ] External tools: Do you know an existing package that should go into `sc.external.*`? - [ ] Other? <!-- Please describe your wishes below: -->. I am integrating multiple datasets. In the integrated object I wanted to see for each cluster the percentage of each dataset it consisted of (also see [this question](https://scanpy.discourse.group/t/cluster-statistics/134) in the scanpy discourse group). I wrote code for it on my own, but am not sure which part of `sc.pl` you want it to go to, so here is the code for your consideration:. ```python. import matplotlib.pyplot as plt. import scanpy as sc. import numpy as np. # given integrated object adata, clustered via the leiden algorithm and. # with the batch ID in the 'batch' slot, and a collection of batch_names:. # count the number of occurrences of each batch ID for each cluster ID. count_series = adata.obs.groupby(['leiden', 'batch']).size(). new_df = count_series.to_frame(name = 'size').reset_index(). # convert from multi index to pivot. constitution = new_df.pivot(index='leiden', columns='batch')['size']. # convert to %batch (but could be modified to show different things instead. perc_clust = np.array((constitution.T / np.sum(constitution.T, axis=0))). # keep track of the batch, cluster IDs so we can use them for plotting. clusters = adata.obs.leiden.cat.categories. batches = adata.obs.batch.cat.categories. # actual plotting; basically stacked barplots. # replace styling with scanpy defaults probably? fig, ax = plt.subplots(). ax.grid(False). ax.bar(clusters, perc_clust[0], 0.6, yerr=0, label=platy_names[0]). bottom = np.zeros(clusters.shape). for i, b in enume",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1573
https://github.com/scverse/scanpy/issues/1573:991,interoperability,integr,integrated,991,"Cluster content plot for integrated data; <!-- What kind of feature would you like to request? -->. - [ ] Additional function parameters / changed functionality / changed defaults? - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`? - [x] New plotting function: A kind of plot you would like to seein `sc.pl`? - [ ] External tools: Do you know an existing package that should go into `sc.external.*`? - [ ] Other? <!-- Please describe your wishes below: -->. I am integrating multiple datasets. In the integrated object I wanted to see for each cluster the percentage of each dataset it consisted of (also see [this question](https://scanpy.discourse.group/t/cluster-statistics/134) in the scanpy discourse group). I wrote code for it on my own, but am not sure which part of `sc.pl` you want it to go to, so here is the code for your consideration:. ```python. import matplotlib.pyplot as plt. import scanpy as sc. import numpy as np. # given integrated object adata, clustered via the leiden algorithm and. # with the batch ID in the 'batch' slot, and a collection of batch_names:. # count the number of occurrences of each batch ID for each cluster ID. count_series = adata.obs.groupby(['leiden', 'batch']).size(). new_df = count_series.to_frame(name = 'size').reset_index(). # convert from multi index to pivot. constitution = new_df.pivot(index='leiden', columns='batch')['size']. # convert to %batch (but could be modified to show different things instead. perc_clust = np.array((constitution.T / np.sum(constitution.T, axis=0))). # keep track of the batch, cluster IDs so we can use them for plotting. clusters = adata.obs.leiden.cat.categories. batches = adata.obs.batch.cat.categories. # actual plotting; basically stacked barplots. # replace styling with scanpy defaults probably? fig, ax = plt.subplots(). ax.grid(False). ax.bar(clusters, perc_clust[0], 0.6, yerr=0, label=platy_names[0]). bottom = np.zeros(clusters.shape). for i, b in enume",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1573
https://github.com/scverse/scanpy/issues/1573:25,modifiability,integr,integrated,25,"Cluster content plot for integrated data; <!-- What kind of feature would you like to request? -->. - [ ] Additional function parameters / changed functionality / changed defaults? - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`? - [x] New plotting function: A kind of plot you would like to seein `sc.pl`? - [ ] External tools: Do you know an existing package that should go into `sc.external.*`? - [ ] Other? <!-- Please describe your wishes below: -->. I am integrating multiple datasets. In the integrated object I wanted to see for each cluster the percentage of each dataset it consisted of (also see [this question](https://scanpy.discourse.group/t/cluster-statistics/134) in the scanpy discourse group). I wrote code for it on my own, but am not sure which part of `sc.pl` you want it to go to, so here is the code for your consideration:. ```python. import matplotlib.pyplot as plt. import scanpy as sc. import numpy as np. # given integrated object adata, clustered via the leiden algorithm and. # with the batch ID in the 'batch' slot, and a collection of batch_names:. # count the number of occurrences of each batch ID for each cluster ID. count_series = adata.obs.groupby(['leiden', 'batch']).size(). new_df = count_series.to_frame(name = 'size').reset_index(). # convert from multi index to pivot. constitution = new_df.pivot(index='leiden', columns='batch')['size']. # convert to %batch (but could be modified to show different things instead. perc_clust = np.array((constitution.T / np.sum(constitution.T, axis=0))). # keep track of the batch, cluster IDs so we can use them for plotting. clusters = adata.obs.leiden.cat.categories. batches = adata.obs.batch.cat.categories. # actual plotting; basically stacked barplots. # replace styling with scanpy defaults probably? fig, ax = plt.subplots(). ax.grid(False). ax.bar(clusters, perc_clust[0], 0.6, yerr=0, label=platy_names[0]). bottom = np.zeros(clusters.shape). for i, b in enume",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1573
https://github.com/scverse/scanpy/issues/1573:126,modifiability,paramet,parameters,126,"Cluster content plot for integrated data; <!-- What kind of feature would you like to request? -->. - [ ] Additional function parameters / changed functionality / changed defaults? - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`? - [x] New plotting function: A kind of plot you would like to seein `sc.pl`? - [ ] External tools: Do you know an existing package that should go into `sc.external.*`? - [ ] Other? <!-- Please describe your wishes below: -->. I am integrating multiple datasets. In the integrated object I wanted to see for each cluster the percentage of each dataset it consisted of (also see [this question](https://scanpy.discourse.group/t/cluster-statistics/134) in the scanpy discourse group). I wrote code for it on my own, but am not sure which part of `sc.pl` you want it to go to, so here is the code for your consideration:. ```python. import matplotlib.pyplot as plt. import scanpy as sc. import numpy as np. # given integrated object adata, clustered via the leiden algorithm and. # with the batch ID in the 'batch' slot, and a collection of batch_names:. # count the number of occurrences of each batch ID for each cluster ID. count_series = adata.obs.groupby(['leiden', 'batch']).size(). new_df = count_series.to_frame(name = 'size').reset_index(). # convert from multi index to pivot. constitution = new_df.pivot(index='leiden', columns='batch')['size']. # convert to %batch (but could be modified to show different things instead. perc_clust = np.array((constitution.T / np.sum(constitution.T, axis=0))). # keep track of the batch, cluster IDs so we can use them for plotting. clusters = adata.obs.leiden.cat.categories. batches = adata.obs.batch.cat.categories. # actual plotting; basically stacked barplots. # replace styling with scanpy defaults probably? fig, ax = plt.subplots(). ax.grid(False). ax.bar(clusters, perc_clust[0], 0.6, yerr=0, label=platy_names[0]). bottom = np.zeros(clusters.shape). for i, b in enume",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1573
https://github.com/scverse/scanpy/issues/1573:403,modifiability,pac,package,403,"Cluster content plot for integrated data; <!-- What kind of feature would you like to request? -->. - [ ] Additional function parameters / changed functionality / changed defaults? - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`? - [x] New plotting function: A kind of plot you would like to seein `sc.pl`? - [ ] External tools: Do you know an existing package that should go into `sc.external.*`? - [ ] Other? <!-- Please describe your wishes below: -->. I am integrating multiple datasets. In the integrated object I wanted to see for each cluster the percentage of each dataset it consisted of (also see [this question](https://scanpy.discourse.group/t/cluster-statistics/134) in the scanpy discourse group). I wrote code for it on my own, but am not sure which part of `sc.pl` you want it to go to, so here is the code for your consideration:. ```python. import matplotlib.pyplot as plt. import scanpy as sc. import numpy as np. # given integrated object adata, clustered via the leiden algorithm and. # with the batch ID in the 'batch' slot, and a collection of batch_names:. # count the number of occurrences of each batch ID for each cluster ID. count_series = adata.obs.groupby(['leiden', 'batch']).size(). new_df = count_series.to_frame(name = 'size').reset_index(). # convert from multi index to pivot. constitution = new_df.pivot(index='leiden', columns='batch')['size']. # convert to %batch (but could be modified to show different things instead. perc_clust = np.array((constitution.T / np.sum(constitution.T, axis=0))). # keep track of the batch, cluster IDs so we can use them for plotting. clusters = adata.obs.leiden.cat.categories. batches = adata.obs.batch.cat.categories. # actual plotting; basically stacked barplots. # replace styling with scanpy defaults probably? fig, ax = plt.subplots(). ax.grid(False). ax.bar(clusters, perc_clust[0], 0.6, yerr=0, label=platy_names[0]). bottom = np.zeros(clusters.shape). for i, b in enume",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1573
https://github.com/scverse/scanpy/issues/1573:511,modifiability,integr,integrating,511,"Cluster content plot for integrated data; <!-- What kind of feature would you like to request? -->. - [ ] Additional function parameters / changed functionality / changed defaults? - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`? - [x] New plotting function: A kind of plot you would like to seein `sc.pl`? - [ ] External tools: Do you know an existing package that should go into `sc.external.*`? - [ ] Other? <!-- Please describe your wishes below: -->. I am integrating multiple datasets. In the integrated object I wanted to see for each cluster the percentage of each dataset it consisted of (also see [this question](https://scanpy.discourse.group/t/cluster-statistics/134) in the scanpy discourse group). I wrote code for it on my own, but am not sure which part of `sc.pl` you want it to go to, so here is the code for your consideration:. ```python. import matplotlib.pyplot as plt. import scanpy as sc. import numpy as np. # given integrated object adata, clustered via the leiden algorithm and. # with the batch ID in the 'batch' slot, and a collection of batch_names:. # count the number of occurrences of each batch ID for each cluster ID. count_series = adata.obs.groupby(['leiden', 'batch']).size(). new_df = count_series.to_frame(name = 'size').reset_index(). # convert from multi index to pivot. constitution = new_df.pivot(index='leiden', columns='batch')['size']. # convert to %batch (but could be modified to show different things instead. perc_clust = np.array((constitution.T / np.sum(constitution.T, axis=0))). # keep track of the batch, cluster IDs so we can use them for plotting. clusters = adata.obs.leiden.cat.categories. batches = adata.obs.batch.cat.categories. # actual plotting; basically stacked barplots. # replace styling with scanpy defaults probably? fig, ax = plt.subplots(). ax.grid(False). ax.bar(clusters, perc_clust[0], 0.6, yerr=0, label=platy_names[0]). bottom = np.zeros(clusters.shape). for i, b in enume",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1573
https://github.com/scverse/scanpy/issues/1573:549,modifiability,integr,integrated,549,"Cluster content plot for integrated data; <!-- What kind of feature would you like to request? -->. - [ ] Additional function parameters / changed functionality / changed defaults? - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`? - [x] New plotting function: A kind of plot you would like to seein `sc.pl`? - [ ] External tools: Do you know an existing package that should go into `sc.external.*`? - [ ] Other? <!-- Please describe your wishes below: -->. I am integrating multiple datasets. In the integrated object I wanted to see for each cluster the percentage of each dataset it consisted of (also see [this question](https://scanpy.discourse.group/t/cluster-statistics/134) in the scanpy discourse group). I wrote code for it on my own, but am not sure which part of `sc.pl` you want it to go to, so here is the code for your consideration:. ```python. import matplotlib.pyplot as plt. import scanpy as sc. import numpy as np. # given integrated object adata, clustered via the leiden algorithm and. # with the batch ID in the 'batch' slot, and a collection of batch_names:. # count the number of occurrences of each batch ID for each cluster ID. count_series = adata.obs.groupby(['leiden', 'batch']).size(). new_df = count_series.to_frame(name = 'size').reset_index(). # convert from multi index to pivot. constitution = new_df.pivot(index='leiden', columns='batch')['size']. # convert to %batch (but could be modified to show different things instead. perc_clust = np.array((constitution.T / np.sum(constitution.T, axis=0))). # keep track of the batch, cluster IDs so we can use them for plotting. clusters = adata.obs.leiden.cat.categories. batches = adata.obs.batch.cat.categories. # actual plotting; basically stacked barplots. # replace styling with scanpy defaults probably? fig, ax = plt.subplots(). ax.grid(False). ax.bar(clusters, perc_clust[0], 0.6, yerr=0, label=platy_names[0]). bottom = np.zeros(clusters.shape). for i, b in enume",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1573
https://github.com/scverse/scanpy/issues/1573:991,modifiability,integr,integrated,991,"Cluster content plot for integrated data; <!-- What kind of feature would you like to request? -->. - [ ] Additional function parameters / changed functionality / changed defaults? - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`? - [x] New plotting function: A kind of plot you would like to seein `sc.pl`? - [ ] External tools: Do you know an existing package that should go into `sc.external.*`? - [ ] Other? <!-- Please describe your wishes below: -->. I am integrating multiple datasets. In the integrated object I wanted to see for each cluster the percentage of each dataset it consisted of (also see [this question](https://scanpy.discourse.group/t/cluster-statistics/134) in the scanpy discourse group). I wrote code for it on my own, but am not sure which part of `sc.pl` you want it to go to, so here is the code for your consideration:. ```python. import matplotlib.pyplot as plt. import scanpy as sc. import numpy as np. # given integrated object adata, clustered via the leiden algorithm and. # with the batch ID in the 'batch' slot, and a collection of batch_names:. # count the number of occurrences of each batch ID for each cluster ID. count_series = adata.obs.groupby(['leiden', 'batch']).size(). new_df = count_series.to_frame(name = 'size').reset_index(). # convert from multi index to pivot. constitution = new_df.pivot(index='leiden', columns='batch')['size']. # convert to %batch (but could be modified to show different things instead. perc_clust = np.array((constitution.T / np.sum(constitution.T, axis=0))). # keep track of the batch, cluster IDs so we can use them for plotting. clusters = adata.obs.leiden.cat.categories. batches = adata.obs.batch.cat.categories. # actual plotting; basically stacked barplots. # replace styling with scanpy defaults probably? fig, ax = plt.subplots(). ax.grid(False). ax.bar(clusters, perc_clust[0], 0.6, yerr=0, label=platy_names[0]). bottom = np.zeros(clusters.shape). for i, b in enume",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1573
https://github.com/scverse/scanpy/issues/1573:8,performance,content,content,8,"Cluster content plot for integrated data; <!-- What kind of feature would you like to request? -->. - [ ] Additional function parameters / changed functionality / changed defaults? - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`? - [x] New plotting function: A kind of plot you would like to seein `sc.pl`? - [ ] External tools: Do you know an existing package that should go into `sc.external.*`? - [ ] Other? <!-- Please describe your wishes below: -->. I am integrating multiple datasets. In the integrated object I wanted to see for each cluster the percentage of each dataset it consisted of (also see [this question](https://scanpy.discourse.group/t/cluster-statistics/134) in the scanpy discourse group). I wrote code for it on my own, but am not sure which part of `sc.pl` you want it to go to, so here is the code for your consideration:. ```python. import matplotlib.pyplot as plt. import scanpy as sc. import numpy as np. # given integrated object adata, clustered via the leiden algorithm and. # with the batch ID in the 'batch' slot, and a collection of batch_names:. # count the number of occurrences of each batch ID for each cluster ID. count_series = adata.obs.groupby(['leiden', 'batch']).size(). new_df = count_series.to_frame(name = 'size').reset_index(). # convert from multi index to pivot. constitution = new_df.pivot(index='leiden', columns='batch')['size']. # convert to %batch (but could be modified to show different things instead. perc_clust = np.array((constitution.T / np.sum(constitution.T, axis=0))). # keep track of the batch, cluster IDs so we can use them for plotting. clusters = adata.obs.leiden.cat.categories. batches = adata.obs.batch.cat.categories. # actual plotting; basically stacked barplots. # replace styling with scanpy defaults probably? fig, ax = plt.subplots(). ax.grid(False). ax.bar(clusters, perc_clust[0], 0.6, yerr=0, label=platy_names[0]). bottom = np.zeros(clusters.shape). for i, b in enume",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1573
https://github.com/scverse/scanpy/issues/1573:1067,performance,batch,batch,1067,"uld you like to request? -->. - [ ] Additional function parameters / changed functionality / changed defaults? - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`? - [x] New plotting function: A kind of plot you would like to seein `sc.pl`? - [ ] External tools: Do you know an existing package that should go into `sc.external.*`? - [ ] Other? <!-- Please describe your wishes below: -->. I am integrating multiple datasets. In the integrated object I wanted to see for each cluster the percentage of each dataset it consisted of (also see [this question](https://scanpy.discourse.group/t/cluster-statistics/134) in the scanpy discourse group). I wrote code for it on my own, but am not sure which part of `sc.pl` you want it to go to, so here is the code for your consideration:. ```python. import matplotlib.pyplot as plt. import scanpy as sc. import numpy as np. # given integrated object adata, clustered via the leiden algorithm and. # with the batch ID in the 'batch' slot, and a collection of batch_names:. # count the number of occurrences of each batch ID for each cluster ID. count_series = adata.obs.groupby(['leiden', 'batch']).size(). new_df = count_series.to_frame(name = 'size').reset_index(). # convert from multi index to pivot. constitution = new_df.pivot(index='leiden', columns='batch')['size']. # convert to %batch (but could be modified to show different things instead. perc_clust = np.array((constitution.T / np.sum(constitution.T, axis=0))). # keep track of the batch, cluster IDs so we can use them for plotting. clusters = adata.obs.leiden.cat.categories. batches = adata.obs.batch.cat.categories. # actual plotting; basically stacked barplots. # replace styling with scanpy defaults probably? fig, ax = plt.subplots(). ax.grid(False). ax.bar(clusters, perc_clust[0], 0.6, yerr=0, label=platy_names[0]). bottom = np.zeros(clusters.shape). for i, b in enumerate(batches):. ax.bar(clusters, perc_clust[i], 0.6, bottom=bottom, ye",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1573
https://github.com/scverse/scanpy/issues/1573:1084,performance,batch,batch,1084,"equest? -->. - [ ] Additional function parameters / changed functionality / changed defaults? - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`? - [x] New plotting function: A kind of plot you would like to seein `sc.pl`? - [ ] External tools: Do you know an existing package that should go into `sc.external.*`? - [ ] Other? <!-- Please describe your wishes below: -->. I am integrating multiple datasets. In the integrated object I wanted to see for each cluster the percentage of each dataset it consisted of (also see [this question](https://scanpy.discourse.group/t/cluster-statistics/134) in the scanpy discourse group). I wrote code for it on my own, but am not sure which part of `sc.pl` you want it to go to, so here is the code for your consideration:. ```python. import matplotlib.pyplot as plt. import scanpy as sc. import numpy as np. # given integrated object adata, clustered via the leiden algorithm and. # with the batch ID in the 'batch' slot, and a collection of batch_names:. # count the number of occurrences of each batch ID for each cluster ID. count_series = adata.obs.groupby(['leiden', 'batch']).size(). new_df = count_series.to_frame(name = 'size').reset_index(). # convert from multi index to pivot. constitution = new_df.pivot(index='leiden', columns='batch')['size']. # convert to %batch (but could be modified to show different things instead. perc_clust = np.array((constitution.T / np.sum(constitution.T, axis=0))). # keep track of the batch, cluster IDs so we can use them for plotting. clusters = adata.obs.leiden.cat.categories. batches = adata.obs.batch.cat.categories. # actual plotting; basically stacked barplots. # replace styling with scanpy defaults probably? fig, ax = plt.subplots(). ax.grid(False). ax.bar(clusters, perc_clust[0], 0.6, yerr=0, label=platy_names[0]). bottom = np.zeros(clusters.shape). for i, b in enumerate(batches):. ax.bar(clusters, perc_clust[i], 0.6, bottom=bottom, yerr=0, label=batch",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1573
https://github.com/scverse/scanpy/issues/1573:1173,performance,batch,batch,1173,"lts? - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`? - [x] New plotting function: A kind of plot you would like to seein `sc.pl`? - [ ] External tools: Do you know an existing package that should go into `sc.external.*`? - [ ] Other? <!-- Please describe your wishes below: -->. I am integrating multiple datasets. In the integrated object I wanted to see for each cluster the percentage of each dataset it consisted of (also see [this question](https://scanpy.discourse.group/t/cluster-statistics/134) in the scanpy discourse group). I wrote code for it on my own, but am not sure which part of `sc.pl` you want it to go to, so here is the code for your consideration:. ```python. import matplotlib.pyplot as plt. import scanpy as sc. import numpy as np. # given integrated object adata, clustered via the leiden algorithm and. # with the batch ID in the 'batch' slot, and a collection of batch_names:. # count the number of occurrences of each batch ID for each cluster ID. count_series = adata.obs.groupby(['leiden', 'batch']).size(). new_df = count_series.to_frame(name = 'size').reset_index(). # convert from multi index to pivot. constitution = new_df.pivot(index='leiden', columns='batch')['size']. # convert to %batch (but could be modified to show different things instead. perc_clust = np.array((constitution.T / np.sum(constitution.T, axis=0))). # keep track of the batch, cluster IDs so we can use them for plotting. clusters = adata.obs.leiden.cat.categories. batches = adata.obs.batch.cat.categories. # actual plotting; basically stacked barplots. # replace styling with scanpy defaults probably? fig, ax = plt.subplots(). ax.grid(False). ax.bar(clusters, perc_clust[0], 0.6, yerr=0, label=platy_names[0]). bottom = np.zeros(clusters.shape). for i, b in enumerate(batches):. ax.bar(clusters, perc_clust[i], 0.6, bottom=bottom, yerr=0, label=batch_names[i]). bottom += perc_clust[I]. . # place legend not on top of the plot. ax.legend(l",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1573
https://github.com/scverse/scanpy/issues/1573:1248,performance,batch,batch,1248,"d are missing in `sc.tools`? - [x] New plotting function: A kind of plot you would like to seein `sc.pl`? - [ ] External tools: Do you know an existing package that should go into `sc.external.*`? - [ ] Other? <!-- Please describe your wishes below: -->. I am integrating multiple datasets. In the integrated object I wanted to see for each cluster the percentage of each dataset it consisted of (also see [this question](https://scanpy.discourse.group/t/cluster-statistics/134) in the scanpy discourse group). I wrote code for it on my own, but am not sure which part of `sc.pl` you want it to go to, so here is the code for your consideration:. ```python. import matplotlib.pyplot as plt. import scanpy as sc. import numpy as np. # given integrated object adata, clustered via the leiden algorithm and. # with the batch ID in the 'batch' slot, and a collection of batch_names:. # count the number of occurrences of each batch ID for each cluster ID. count_series = adata.obs.groupby(['leiden', 'batch']).size(). new_df = count_series.to_frame(name = 'size').reset_index(). # convert from multi index to pivot. constitution = new_df.pivot(index='leiden', columns='batch')['size']. # convert to %batch (but could be modified to show different things instead. perc_clust = np.array((constitution.T / np.sum(constitution.T, axis=0))). # keep track of the batch, cluster IDs so we can use them for plotting. clusters = adata.obs.leiden.cat.categories. batches = adata.obs.batch.cat.categories. # actual plotting; basically stacked barplots. # replace styling with scanpy defaults probably? fig, ax = plt.subplots(). ax.grid(False). ax.bar(clusters, perc_clust[0], 0.6, yerr=0, label=platy_names[0]). bottom = np.zeros(clusters.shape). for i, b in enumerate(batches):. ax.bar(clusters, perc_clust[i], 0.6, bottom=bottom, yerr=0, label=batch_names[i]). bottom += perc_clust[I]. . # place legend not on top of the plot. ax.legend(loc='right', bbox_to_anchor=(1.2, 0.5));. ```. End result should look someth",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1573
https://github.com/scverse/scanpy/issues/1573:1416,performance,batch,batch,1416,": Do you know an existing package that should go into `sc.external.*`? - [ ] Other? <!-- Please describe your wishes below: -->. I am integrating multiple datasets. In the integrated object I wanted to see for each cluster the percentage of each dataset it consisted of (also see [this question](https://scanpy.discourse.group/t/cluster-statistics/134) in the scanpy discourse group). I wrote code for it on my own, but am not sure which part of `sc.pl` you want it to go to, so here is the code for your consideration:. ```python. import matplotlib.pyplot as plt. import scanpy as sc. import numpy as np. # given integrated object adata, clustered via the leiden algorithm and. # with the batch ID in the 'batch' slot, and a collection of batch_names:. # count the number of occurrences of each batch ID for each cluster ID. count_series = adata.obs.groupby(['leiden', 'batch']).size(). new_df = count_series.to_frame(name = 'size').reset_index(). # convert from multi index to pivot. constitution = new_df.pivot(index='leiden', columns='batch')['size']. # convert to %batch (but could be modified to show different things instead. perc_clust = np.array((constitution.T / np.sum(constitution.T, axis=0))). # keep track of the batch, cluster IDs so we can use them for plotting. clusters = adata.obs.leiden.cat.categories. batches = adata.obs.batch.cat.categories. # actual plotting; basically stacked barplots. # replace styling with scanpy defaults probably? fig, ax = plt.subplots(). ax.grid(False). ax.bar(clusters, perc_clust[0], 0.6, yerr=0, label=platy_names[0]). bottom = np.zeros(clusters.shape). for i, b in enumerate(batches):. ax.bar(clusters, perc_clust[i], 0.6, bottom=bottom, yerr=0, label=batch_names[i]). bottom += perc_clust[I]. . # place legend not on top of the plot. ax.legend(loc='right', bbox_to_anchor=(1.2, 0.5));. ```. End result should look something like this. ![image](https://user-images.githubusercontent.com/1651067/104041019-ac463480-51d8-11eb-93ee-267deb4aa3f9.png).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1573
https://github.com/scverse/scanpy/issues/1573:1447,performance,batch,batch,1447,": Do you know an existing package that should go into `sc.external.*`? - [ ] Other? <!-- Please describe your wishes below: -->. I am integrating multiple datasets. In the integrated object I wanted to see for each cluster the percentage of each dataset it consisted of (also see [this question](https://scanpy.discourse.group/t/cluster-statistics/134) in the scanpy discourse group). I wrote code for it on my own, but am not sure which part of `sc.pl` you want it to go to, so here is the code for your consideration:. ```python. import matplotlib.pyplot as plt. import scanpy as sc. import numpy as np. # given integrated object adata, clustered via the leiden algorithm and. # with the batch ID in the 'batch' slot, and a collection of batch_names:. # count the number of occurrences of each batch ID for each cluster ID. count_series = adata.obs.groupby(['leiden', 'batch']).size(). new_df = count_series.to_frame(name = 'size').reset_index(). # convert from multi index to pivot. constitution = new_df.pivot(index='leiden', columns='batch')['size']. # convert to %batch (but could be modified to show different things instead. perc_clust = np.array((constitution.T / np.sum(constitution.T, axis=0))). # keep track of the batch, cluster IDs so we can use them for plotting. clusters = adata.obs.leiden.cat.categories. batches = adata.obs.batch.cat.categories. # actual plotting; basically stacked barplots. # replace styling with scanpy defaults probably? fig, ax = plt.subplots(). ax.grid(False). ax.bar(clusters, perc_clust[0], 0.6, yerr=0, label=platy_names[0]). bottom = np.zeros(clusters.shape). for i, b in enumerate(batches):. ax.bar(clusters, perc_clust[i], 0.6, bottom=bottom, yerr=0, label=batch_names[i]). bottom += perc_clust[I]. . # place legend not on top of the plot. ax.legend(loc='right', bbox_to_anchor=(1.2, 0.5));. ```. End result should look something like this. ![image](https://user-images.githubusercontent.com/1651067/104041019-ac463480-51d8-11eb-93ee-267deb4aa3f9.png).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1573
https://github.com/scverse/scanpy/issues/1573:1604,performance,batch,batch,1604,": Do you know an existing package that should go into `sc.external.*`? - [ ] Other? <!-- Please describe your wishes below: -->. I am integrating multiple datasets. In the integrated object I wanted to see for each cluster the percentage of each dataset it consisted of (also see [this question](https://scanpy.discourse.group/t/cluster-statistics/134) in the scanpy discourse group). I wrote code for it on my own, but am not sure which part of `sc.pl` you want it to go to, so here is the code for your consideration:. ```python. import matplotlib.pyplot as plt. import scanpy as sc. import numpy as np. # given integrated object adata, clustered via the leiden algorithm and. # with the batch ID in the 'batch' slot, and a collection of batch_names:. # count the number of occurrences of each batch ID for each cluster ID. count_series = adata.obs.groupby(['leiden', 'batch']).size(). new_df = count_series.to_frame(name = 'size').reset_index(). # convert from multi index to pivot. constitution = new_df.pivot(index='leiden', columns='batch')['size']. # convert to %batch (but could be modified to show different things instead. perc_clust = np.array((constitution.T / np.sum(constitution.T, axis=0))). # keep track of the batch, cluster IDs so we can use them for plotting. clusters = adata.obs.leiden.cat.categories. batches = adata.obs.batch.cat.categories. # actual plotting; basically stacked barplots. # replace styling with scanpy defaults probably? fig, ax = plt.subplots(). ax.grid(False). ax.bar(clusters, perc_clust[0], 0.6, yerr=0, label=platy_names[0]). bottom = np.zeros(clusters.shape). for i, b in enumerate(batches):. ax.bar(clusters, perc_clust[i], 0.6, bottom=bottom, yerr=0, label=batch_names[i]). bottom += perc_clust[I]. . # place legend not on top of the plot. ax.legend(loc='right', bbox_to_anchor=(1.2, 0.5));. ```. End result should look something like this. ![image](https://user-images.githubusercontent.com/1651067/104041019-ac463480-51d8-11eb-93ee-267deb4aa3f9.png).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1573
https://github.com/scverse/scanpy/issues/1573:1700,performance,batch,batches,1700,": Do you know an existing package that should go into `sc.external.*`? - [ ] Other? <!-- Please describe your wishes below: -->. I am integrating multiple datasets. In the integrated object I wanted to see for each cluster the percentage of each dataset it consisted of (also see [this question](https://scanpy.discourse.group/t/cluster-statistics/134) in the scanpy discourse group). I wrote code for it on my own, but am not sure which part of `sc.pl` you want it to go to, so here is the code for your consideration:. ```python. import matplotlib.pyplot as plt. import scanpy as sc. import numpy as np. # given integrated object adata, clustered via the leiden algorithm and. # with the batch ID in the 'batch' slot, and a collection of batch_names:. # count the number of occurrences of each batch ID for each cluster ID. count_series = adata.obs.groupby(['leiden', 'batch']).size(). new_df = count_series.to_frame(name = 'size').reset_index(). # convert from multi index to pivot. constitution = new_df.pivot(index='leiden', columns='batch')['size']. # convert to %batch (but could be modified to show different things instead. perc_clust = np.array((constitution.T / np.sum(constitution.T, axis=0))). # keep track of the batch, cluster IDs so we can use them for plotting. clusters = adata.obs.leiden.cat.categories. batches = adata.obs.batch.cat.categories. # actual plotting; basically stacked barplots. # replace styling with scanpy defaults probably? fig, ax = plt.subplots(). ax.grid(False). ax.bar(clusters, perc_clust[0], 0.6, yerr=0, label=platy_names[0]). bottom = np.zeros(clusters.shape). for i, b in enumerate(batches):. ax.bar(clusters, perc_clust[i], 0.6, bottom=bottom, yerr=0, label=batch_names[i]). bottom += perc_clust[I]. . # place legend not on top of the plot. ax.legend(loc='right', bbox_to_anchor=(1.2, 0.5));. ```. End result should look something like this. ![image](https://user-images.githubusercontent.com/1651067/104041019-ac463480-51d8-11eb-93ee-267deb4aa3f9.png).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1573
https://github.com/scverse/scanpy/issues/1573:1720,performance,batch,batch,1720,": Do you know an existing package that should go into `sc.external.*`? - [ ] Other? <!-- Please describe your wishes below: -->. I am integrating multiple datasets. In the integrated object I wanted to see for each cluster the percentage of each dataset it consisted of (also see [this question](https://scanpy.discourse.group/t/cluster-statistics/134) in the scanpy discourse group). I wrote code for it on my own, but am not sure which part of `sc.pl` you want it to go to, so here is the code for your consideration:. ```python. import matplotlib.pyplot as plt. import scanpy as sc. import numpy as np. # given integrated object adata, clustered via the leiden algorithm and. # with the batch ID in the 'batch' slot, and a collection of batch_names:. # count the number of occurrences of each batch ID for each cluster ID. count_series = adata.obs.groupby(['leiden', 'batch']).size(). new_df = count_series.to_frame(name = 'size').reset_index(). # convert from multi index to pivot. constitution = new_df.pivot(index='leiden', columns='batch')['size']. # convert to %batch (but could be modified to show different things instead. perc_clust = np.array((constitution.T / np.sum(constitution.T, axis=0))). # keep track of the batch, cluster IDs so we can use them for plotting. clusters = adata.obs.leiden.cat.categories. batches = adata.obs.batch.cat.categories. # actual plotting; basically stacked barplots. # replace styling with scanpy defaults probably? fig, ax = plt.subplots(). ax.grid(False). ax.bar(clusters, perc_clust[0], 0.6, yerr=0, label=platy_names[0]). bottom = np.zeros(clusters.shape). for i, b in enumerate(batches):. ax.bar(clusters, perc_clust[i], 0.6, bottom=bottom, yerr=0, label=batch_names[i]). bottom += perc_clust[I]. . # place legend not on top of the plot. ax.legend(loc='right', bbox_to_anchor=(1.2, 0.5));. ```. End result should look something like this. ![image](https://user-images.githubusercontent.com/1651067/104041019-ac463480-51d8-11eb-93ee-267deb4aa3f9.png).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1573
https://github.com/scverse/scanpy/issues/1573:2005,performance,batch,batches,2005,": Do you know an existing package that should go into `sc.external.*`? - [ ] Other? <!-- Please describe your wishes below: -->. I am integrating multiple datasets. In the integrated object I wanted to see for each cluster the percentage of each dataset it consisted of (also see [this question](https://scanpy.discourse.group/t/cluster-statistics/134) in the scanpy discourse group). I wrote code for it on my own, but am not sure which part of `sc.pl` you want it to go to, so here is the code for your consideration:. ```python. import matplotlib.pyplot as plt. import scanpy as sc. import numpy as np. # given integrated object adata, clustered via the leiden algorithm and. # with the batch ID in the 'batch' slot, and a collection of batch_names:. # count the number of occurrences of each batch ID for each cluster ID. count_series = adata.obs.groupby(['leiden', 'batch']).size(). new_df = count_series.to_frame(name = 'size').reset_index(). # convert from multi index to pivot. constitution = new_df.pivot(index='leiden', columns='batch')['size']. # convert to %batch (but could be modified to show different things instead. perc_clust = np.array((constitution.T / np.sum(constitution.T, axis=0))). # keep track of the batch, cluster IDs so we can use them for plotting. clusters = adata.obs.leiden.cat.categories. batches = adata.obs.batch.cat.categories. # actual plotting; basically stacked barplots. # replace styling with scanpy defaults probably? fig, ax = plt.subplots(). ax.grid(False). ax.bar(clusters, perc_clust[0], 0.6, yerr=0, label=platy_names[0]). bottom = np.zeros(clusters.shape). for i, b in enumerate(batches):. ax.bar(clusters, perc_clust[i], 0.6, bottom=bottom, yerr=0, label=batch_names[i]). bottom += perc_clust[I]. . # place legend not on top of the plot. ax.legend(loc='right', bbox_to_anchor=(1.2, 0.5));. ```. End result should look something like this. ![image](https://user-images.githubusercontent.com/1651067/104041019-ac463480-51d8-11eb-93ee-267deb4aa3f9.png).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1573
https://github.com/scverse/scanpy/issues/1573:25,reliability,integr,integrated,25,"Cluster content plot for integrated data; <!-- What kind of feature would you like to request? -->. - [ ] Additional function parameters / changed functionality / changed defaults? - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`? - [x] New plotting function: A kind of plot you would like to seein `sc.pl`? - [ ] External tools: Do you know an existing package that should go into `sc.external.*`? - [ ] Other? <!-- Please describe your wishes below: -->. I am integrating multiple datasets. In the integrated object I wanted to see for each cluster the percentage of each dataset it consisted of (also see [this question](https://scanpy.discourse.group/t/cluster-statistics/134) in the scanpy discourse group). I wrote code for it on my own, but am not sure which part of `sc.pl` you want it to go to, so here is the code for your consideration:. ```python. import matplotlib.pyplot as plt. import scanpy as sc. import numpy as np. # given integrated object adata, clustered via the leiden algorithm and. # with the batch ID in the 'batch' slot, and a collection of batch_names:. # count the number of occurrences of each batch ID for each cluster ID. count_series = adata.obs.groupby(['leiden', 'batch']).size(). new_df = count_series.to_frame(name = 'size').reset_index(). # convert from multi index to pivot. constitution = new_df.pivot(index='leiden', columns='batch')['size']. # convert to %batch (but could be modified to show different things instead. perc_clust = np.array((constitution.T / np.sum(constitution.T, axis=0))). # keep track of the batch, cluster IDs so we can use them for plotting. clusters = adata.obs.leiden.cat.categories. batches = adata.obs.batch.cat.categories. # actual plotting; basically stacked barplots. # replace styling with scanpy defaults probably? fig, ax = plt.subplots(). ax.grid(False). ax.bar(clusters, perc_clust[0], 0.6, yerr=0, label=platy_names[0]). bottom = np.zeros(clusters.shape). for i, b in enume",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1573
https://github.com/scverse/scanpy/issues/1573:511,reliability,integr,integrating,511,"Cluster content plot for integrated data; <!-- What kind of feature would you like to request? -->. - [ ] Additional function parameters / changed functionality / changed defaults? - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`? - [x] New plotting function: A kind of plot you would like to seein `sc.pl`? - [ ] External tools: Do you know an existing package that should go into `sc.external.*`? - [ ] Other? <!-- Please describe your wishes below: -->. I am integrating multiple datasets. In the integrated object I wanted to see for each cluster the percentage of each dataset it consisted of (also see [this question](https://scanpy.discourse.group/t/cluster-statistics/134) in the scanpy discourse group). I wrote code for it on my own, but am not sure which part of `sc.pl` you want it to go to, so here is the code for your consideration:. ```python. import matplotlib.pyplot as plt. import scanpy as sc. import numpy as np. # given integrated object adata, clustered via the leiden algorithm and. # with the batch ID in the 'batch' slot, and a collection of batch_names:. # count the number of occurrences of each batch ID for each cluster ID. count_series = adata.obs.groupby(['leiden', 'batch']).size(). new_df = count_series.to_frame(name = 'size').reset_index(). # convert from multi index to pivot. constitution = new_df.pivot(index='leiden', columns='batch')['size']. # convert to %batch (but could be modified to show different things instead. perc_clust = np.array((constitution.T / np.sum(constitution.T, axis=0))). # keep track of the batch, cluster IDs so we can use them for plotting. clusters = adata.obs.leiden.cat.categories. batches = adata.obs.batch.cat.categories. # actual plotting; basically stacked barplots. # replace styling with scanpy defaults probably? fig, ax = plt.subplots(). ax.grid(False). ax.bar(clusters, perc_clust[0], 0.6, yerr=0, label=platy_names[0]). bottom = np.zeros(clusters.shape). for i, b in enume",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1573
https://github.com/scverse/scanpy/issues/1573:549,reliability,integr,integrated,549,"Cluster content plot for integrated data; <!-- What kind of feature would you like to request? -->. - [ ] Additional function parameters / changed functionality / changed defaults? - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`? - [x] New plotting function: A kind of plot you would like to seein `sc.pl`? - [ ] External tools: Do you know an existing package that should go into `sc.external.*`? - [ ] Other? <!-- Please describe your wishes below: -->. I am integrating multiple datasets. In the integrated object I wanted to see for each cluster the percentage of each dataset it consisted of (also see [this question](https://scanpy.discourse.group/t/cluster-statistics/134) in the scanpy discourse group). I wrote code for it on my own, but am not sure which part of `sc.pl` you want it to go to, so here is the code for your consideration:. ```python. import matplotlib.pyplot as plt. import scanpy as sc. import numpy as np. # given integrated object adata, clustered via the leiden algorithm and. # with the batch ID in the 'batch' slot, and a collection of batch_names:. # count the number of occurrences of each batch ID for each cluster ID. count_series = adata.obs.groupby(['leiden', 'batch']).size(). new_df = count_series.to_frame(name = 'size').reset_index(). # convert from multi index to pivot. constitution = new_df.pivot(index='leiden', columns='batch')['size']. # convert to %batch (but could be modified to show different things instead. perc_clust = np.array((constitution.T / np.sum(constitution.T, axis=0))). # keep track of the batch, cluster IDs so we can use them for plotting. clusters = adata.obs.leiden.cat.categories. batches = adata.obs.batch.cat.categories. # actual plotting; basically stacked barplots. # replace styling with scanpy defaults probably? fig, ax = plt.subplots(). ax.grid(False). ax.bar(clusters, perc_clust[0], 0.6, yerr=0, label=platy_names[0]). bottom = np.zeros(clusters.shape). for i, b in enume",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1573
https://github.com/scverse/scanpy/issues/1573:991,reliability,integr,integrated,991,"Cluster content plot for integrated data; <!-- What kind of feature would you like to request? -->. - [ ] Additional function parameters / changed functionality / changed defaults? - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`? - [x] New plotting function: A kind of plot you would like to seein `sc.pl`? - [ ] External tools: Do you know an existing package that should go into `sc.external.*`? - [ ] Other? <!-- Please describe your wishes below: -->. I am integrating multiple datasets. In the integrated object I wanted to see for each cluster the percentage of each dataset it consisted of (also see [this question](https://scanpy.discourse.group/t/cluster-statistics/134) in the scanpy discourse group). I wrote code for it on my own, but am not sure which part of `sc.pl` you want it to go to, so here is the code for your consideration:. ```python. import matplotlib.pyplot as plt. import scanpy as sc. import numpy as np. # given integrated object adata, clustered via the leiden algorithm and. # with the batch ID in the 'batch' slot, and a collection of batch_names:. # count the number of occurrences of each batch ID for each cluster ID. count_series = adata.obs.groupby(['leiden', 'batch']).size(). new_df = count_series.to_frame(name = 'size').reset_index(). # convert from multi index to pivot. constitution = new_df.pivot(index='leiden', columns='batch')['size']. # convert to %batch (but could be modified to show different things instead. perc_clust = np.array((constitution.T / np.sum(constitution.T, axis=0))). # keep track of the batch, cluster IDs so we can use them for plotting. clusters = adata.obs.leiden.cat.categories. batches = adata.obs.batch.cat.categories. # actual plotting; basically stacked barplots. # replace styling with scanpy defaults probably? fig, ax = plt.subplots(). ax.grid(False). ax.bar(clusters, perc_clust[0], 0.6, yerr=0, label=platy_names[0]). bottom = np.zeros(clusters.shape). for i, b in enume",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1573
https://github.com/scverse/scanpy/issues/1573:1091,reliability,slo,slot,1091,"? -->. - [ ] Additional function parameters / changed functionality / changed defaults? - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`? - [x] New plotting function: A kind of plot you would like to seein `sc.pl`? - [ ] External tools: Do you know an existing package that should go into `sc.external.*`? - [ ] Other? <!-- Please describe your wishes below: -->. I am integrating multiple datasets. In the integrated object I wanted to see for each cluster the percentage of each dataset it consisted of (also see [this question](https://scanpy.discourse.group/t/cluster-statistics/134) in the scanpy discourse group). I wrote code for it on my own, but am not sure which part of `sc.pl` you want it to go to, so here is the code for your consideration:. ```python. import matplotlib.pyplot as plt. import scanpy as sc. import numpy as np. # given integrated object adata, clustered via the leiden algorithm and. # with the batch ID in the 'batch' slot, and a collection of batch_names:. # count the number of occurrences of each batch ID for each cluster ID. count_series = adata.obs.groupby(['leiden', 'batch']).size(). new_df = count_series.to_frame(name = 'size').reset_index(). # convert from multi index to pivot. constitution = new_df.pivot(index='leiden', columns='batch')['size']. # convert to %batch (but could be modified to show different things instead. perc_clust = np.array((constitution.T / np.sum(constitution.T, axis=0))). # keep track of the batch, cluster IDs so we can use them for plotting. clusters = adata.obs.leiden.cat.categories. batches = adata.obs.batch.cat.categories. # actual plotting; basically stacked barplots. # replace styling with scanpy defaults probably? fig, ax = plt.subplots(). ax.grid(False). ax.bar(clusters, perc_clust[0], 0.6, yerr=0, label=platy_names[0]). bottom = np.zeros(clusters.shape). for i, b in enumerate(batches):. ax.bar(clusters, perc_clust[i], 0.6, bottom=bottom, yerr=0, label=batch_names",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1573
https://github.com/scverse/scanpy/issues/1573:25,security,integr,integrated,25,"Cluster content plot for integrated data; <!-- What kind of feature would you like to request? -->. - [ ] Additional function parameters / changed functionality / changed defaults? - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`? - [x] New plotting function: A kind of plot you would like to seein `sc.pl`? - [ ] External tools: Do you know an existing package that should go into `sc.external.*`? - [ ] Other? <!-- Please describe your wishes below: -->. I am integrating multiple datasets. In the integrated object I wanted to see for each cluster the percentage of each dataset it consisted of (also see [this question](https://scanpy.discourse.group/t/cluster-statistics/134) in the scanpy discourse group). I wrote code for it on my own, but am not sure which part of `sc.pl` you want it to go to, so here is the code for your consideration:. ```python. import matplotlib.pyplot as plt. import scanpy as sc. import numpy as np. # given integrated object adata, clustered via the leiden algorithm and. # with the batch ID in the 'batch' slot, and a collection of batch_names:. # count the number of occurrences of each batch ID for each cluster ID. count_series = adata.obs.groupby(['leiden', 'batch']).size(). new_df = count_series.to_frame(name = 'size').reset_index(). # convert from multi index to pivot. constitution = new_df.pivot(index='leiden', columns='batch')['size']. # convert to %batch (but could be modified to show different things instead. perc_clust = np.array((constitution.T / np.sum(constitution.T, axis=0))). # keep track of the batch, cluster IDs so we can use them for plotting. clusters = adata.obs.leiden.cat.categories. batches = adata.obs.batch.cat.categories. # actual plotting; basically stacked barplots. # replace styling with scanpy defaults probably? fig, ax = plt.subplots(). ax.grid(False). ax.bar(clusters, perc_clust[0], 0.6, yerr=0, label=platy_names[0]). bottom = np.zeros(clusters.shape). for i, b in enume",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1573
https://github.com/scverse/scanpy/issues/1573:511,security,integr,integrating,511,"Cluster content plot for integrated data; <!-- What kind of feature would you like to request? -->. - [ ] Additional function parameters / changed functionality / changed defaults? - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`? - [x] New plotting function: A kind of plot you would like to seein `sc.pl`? - [ ] External tools: Do you know an existing package that should go into `sc.external.*`? - [ ] Other? <!-- Please describe your wishes below: -->. I am integrating multiple datasets. In the integrated object I wanted to see for each cluster the percentage of each dataset it consisted of (also see [this question](https://scanpy.discourse.group/t/cluster-statistics/134) in the scanpy discourse group). I wrote code for it on my own, but am not sure which part of `sc.pl` you want it to go to, so here is the code for your consideration:. ```python. import matplotlib.pyplot as plt. import scanpy as sc. import numpy as np. # given integrated object adata, clustered via the leiden algorithm and. # with the batch ID in the 'batch' slot, and a collection of batch_names:. # count the number of occurrences of each batch ID for each cluster ID. count_series = adata.obs.groupby(['leiden', 'batch']).size(). new_df = count_series.to_frame(name = 'size').reset_index(). # convert from multi index to pivot. constitution = new_df.pivot(index='leiden', columns='batch')['size']. # convert to %batch (but could be modified to show different things instead. perc_clust = np.array((constitution.T / np.sum(constitution.T, axis=0))). # keep track of the batch, cluster IDs so we can use them for plotting. clusters = adata.obs.leiden.cat.categories. batches = adata.obs.batch.cat.categories. # actual plotting; basically stacked barplots. # replace styling with scanpy defaults probably? fig, ax = plt.subplots(). ax.grid(False). ax.bar(clusters, perc_clust[0], 0.6, yerr=0, label=platy_names[0]). bottom = np.zeros(clusters.shape). for i, b in enume",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1573
https://github.com/scverse/scanpy/issues/1573:549,security,integr,integrated,549,"Cluster content plot for integrated data; <!-- What kind of feature would you like to request? -->. - [ ] Additional function parameters / changed functionality / changed defaults? - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`? - [x] New plotting function: A kind of plot you would like to seein `sc.pl`? - [ ] External tools: Do you know an existing package that should go into `sc.external.*`? - [ ] Other? <!-- Please describe your wishes below: -->. I am integrating multiple datasets. In the integrated object I wanted to see for each cluster the percentage of each dataset it consisted of (also see [this question](https://scanpy.discourse.group/t/cluster-statistics/134) in the scanpy discourse group). I wrote code for it on my own, but am not sure which part of `sc.pl` you want it to go to, so here is the code for your consideration:. ```python. import matplotlib.pyplot as plt. import scanpy as sc. import numpy as np. # given integrated object adata, clustered via the leiden algorithm and. # with the batch ID in the 'batch' slot, and a collection of batch_names:. # count the number of occurrences of each batch ID for each cluster ID. count_series = adata.obs.groupby(['leiden', 'batch']).size(). new_df = count_series.to_frame(name = 'size').reset_index(). # convert from multi index to pivot. constitution = new_df.pivot(index='leiden', columns='batch')['size']. # convert to %batch (but could be modified to show different things instead. perc_clust = np.array((constitution.T / np.sum(constitution.T, axis=0))). # keep track of the batch, cluster IDs so we can use them for plotting. clusters = adata.obs.leiden.cat.categories. batches = adata.obs.batch.cat.categories. # actual plotting; basically stacked barplots. # replace styling with scanpy defaults probably? fig, ax = plt.subplots(). ax.grid(False). ax.bar(clusters, perc_clust[0], 0.6, yerr=0, label=platy_names[0]). bottom = np.zeros(clusters.shape). for i, b in enume",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1573
https://github.com/scverse/scanpy/issues/1573:991,security,integr,integrated,991,"Cluster content plot for integrated data; <!-- What kind of feature would you like to request? -->. - [ ] Additional function parameters / changed functionality / changed defaults? - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`? - [x] New plotting function: A kind of plot you would like to seein `sc.pl`? - [ ] External tools: Do you know an existing package that should go into `sc.external.*`? - [ ] Other? <!-- Please describe your wishes below: -->. I am integrating multiple datasets. In the integrated object I wanted to see for each cluster the percentage of each dataset it consisted of (also see [this question](https://scanpy.discourse.group/t/cluster-statistics/134) in the scanpy discourse group). I wrote code for it on my own, but am not sure which part of `sc.pl` you want it to go to, so here is the code for your consideration:. ```python. import matplotlib.pyplot as plt. import scanpy as sc. import numpy as np. # given integrated object adata, clustered via the leiden algorithm and. # with the batch ID in the 'batch' slot, and a collection of batch_names:. # count the number of occurrences of each batch ID for each cluster ID. count_series = adata.obs.groupby(['leiden', 'batch']).size(). new_df = count_series.to_frame(name = 'size').reset_index(). # convert from multi index to pivot. constitution = new_df.pivot(index='leiden', columns='batch')['size']. # convert to %batch (but could be modified to show different things instead. perc_clust = np.array((constitution.T / np.sum(constitution.T, axis=0))). # keep track of the batch, cluster IDs so we can use them for plotting. clusters = adata.obs.leiden.cat.categories. batches = adata.obs.batch.cat.categories. # actual plotting; basically stacked barplots. # replace styling with scanpy defaults probably? fig, ax = plt.subplots(). ax.grid(False). ax.bar(clusters, perc_clust[0], 0.6, yerr=0, label=platy_names[0]). bottom = np.zeros(clusters.shape). for i, b in enume",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1573
https://github.com/scverse/scanpy/issues/1573:1467,security,modif,modified,1467,": Do you know an existing package that should go into `sc.external.*`? - [ ] Other? <!-- Please describe your wishes below: -->. I am integrating multiple datasets. In the integrated object I wanted to see for each cluster the percentage of each dataset it consisted of (also see [this question](https://scanpy.discourse.group/t/cluster-statistics/134) in the scanpy discourse group). I wrote code for it on my own, but am not sure which part of `sc.pl` you want it to go to, so here is the code for your consideration:. ```python. import matplotlib.pyplot as plt. import scanpy as sc. import numpy as np. # given integrated object adata, clustered via the leiden algorithm and. # with the batch ID in the 'batch' slot, and a collection of batch_names:. # count the number of occurrences of each batch ID for each cluster ID. count_series = adata.obs.groupby(['leiden', 'batch']).size(). new_df = count_series.to_frame(name = 'size').reset_index(). # convert from multi index to pivot. constitution = new_df.pivot(index='leiden', columns='batch')['size']. # convert to %batch (but could be modified to show different things instead. perc_clust = np.array((constitution.T / np.sum(constitution.T, axis=0))). # keep track of the batch, cluster IDs so we can use them for plotting. clusters = adata.obs.leiden.cat.categories. batches = adata.obs.batch.cat.categories. # actual plotting; basically stacked barplots. # replace styling with scanpy defaults probably? fig, ax = plt.subplots(). ax.grid(False). ax.bar(clusters, perc_clust[0], 0.6, yerr=0, label=platy_names[0]). bottom = np.zeros(clusters.shape). for i, b in enumerate(batches):. ax.bar(clusters, perc_clust[i], 0.6, bottom=bottom, yerr=0, label=batch_names[i]). bottom += perc_clust[I]. . # place legend not on top of the plot. ax.legend(loc='right', bbox_to_anchor=(1.2, 0.5));. ```. End result should look something like this. ![image](https://user-images.githubusercontent.com/1651067/104041019-ac463480-51d8-11eb-93ee-267deb4aa3f9.png).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1573
https://github.com/scverse/scanpy/issues/1573:25,testability,integr,integrated,25,"Cluster content plot for integrated data; <!-- What kind of feature would you like to request? -->. - [ ] Additional function parameters / changed functionality / changed defaults? - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`? - [x] New plotting function: A kind of plot you would like to seein `sc.pl`? - [ ] External tools: Do you know an existing package that should go into `sc.external.*`? - [ ] Other? <!-- Please describe your wishes below: -->. I am integrating multiple datasets. In the integrated object I wanted to see for each cluster the percentage of each dataset it consisted of (also see [this question](https://scanpy.discourse.group/t/cluster-statistics/134) in the scanpy discourse group). I wrote code for it on my own, but am not sure which part of `sc.pl` you want it to go to, so here is the code for your consideration:. ```python. import matplotlib.pyplot as plt. import scanpy as sc. import numpy as np. # given integrated object adata, clustered via the leiden algorithm and. # with the batch ID in the 'batch' slot, and a collection of batch_names:. # count the number of occurrences of each batch ID for each cluster ID. count_series = adata.obs.groupby(['leiden', 'batch']).size(). new_df = count_series.to_frame(name = 'size').reset_index(). # convert from multi index to pivot. constitution = new_df.pivot(index='leiden', columns='batch')['size']. # convert to %batch (but could be modified to show different things instead. perc_clust = np.array((constitution.T / np.sum(constitution.T, axis=0))). # keep track of the batch, cluster IDs so we can use them for plotting. clusters = adata.obs.leiden.cat.categories. batches = adata.obs.batch.cat.categories. # actual plotting; basically stacked barplots. # replace styling with scanpy defaults probably? fig, ax = plt.subplots(). ax.grid(False). ax.bar(clusters, perc_clust[0], 0.6, yerr=0, label=platy_names[0]). bottom = np.zeros(clusters.shape). for i, b in enume",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1573
https://github.com/scverse/scanpy/issues/1573:208,testability,simpl,simple,208,"Cluster content plot for integrated data; <!-- What kind of feature would you like to request? -->. - [ ] Additional function parameters / changed functionality / changed defaults? - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`? - [x] New plotting function: A kind of plot you would like to seein `sc.pl`? - [ ] External tools: Do you know an existing package that should go into `sc.external.*`? - [ ] Other? <!-- Please describe your wishes below: -->. I am integrating multiple datasets. In the integrated object I wanted to see for each cluster the percentage of each dataset it consisted of (also see [this question](https://scanpy.discourse.group/t/cluster-statistics/134) in the scanpy discourse group). I wrote code for it on my own, but am not sure which part of `sc.pl` you want it to go to, so here is the code for your consideration:. ```python. import matplotlib.pyplot as plt. import scanpy as sc. import numpy as np. # given integrated object adata, clustered via the leiden algorithm and. # with the batch ID in the 'batch' slot, and a collection of batch_names:. # count the number of occurrences of each batch ID for each cluster ID. count_series = adata.obs.groupby(['leiden', 'batch']).size(). new_df = count_series.to_frame(name = 'size').reset_index(). # convert from multi index to pivot. constitution = new_df.pivot(index='leiden', columns='batch')['size']. # convert to %batch (but could be modified to show different things instead. perc_clust = np.array((constitution.T / np.sum(constitution.T, axis=0))). # keep track of the batch, cluster IDs so we can use them for plotting. clusters = adata.obs.leiden.cat.categories. batches = adata.obs.batch.cat.categories. # actual plotting; basically stacked barplots. # replace styling with scanpy defaults probably? fig, ax = plt.subplots(). ax.grid(False). ax.bar(clusters, perc_clust[0], 0.6, yerr=0, label=platy_names[0]). bottom = np.zeros(clusters.shape). for i, b in enume",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1573
https://github.com/scverse/scanpy/issues/1573:511,testability,integr,integrating,511,"Cluster content plot for integrated data; <!-- What kind of feature would you like to request? -->. - [ ] Additional function parameters / changed functionality / changed defaults? - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`? - [x] New plotting function: A kind of plot you would like to seein `sc.pl`? - [ ] External tools: Do you know an existing package that should go into `sc.external.*`? - [ ] Other? <!-- Please describe your wishes below: -->. I am integrating multiple datasets. In the integrated object I wanted to see for each cluster the percentage of each dataset it consisted of (also see [this question](https://scanpy.discourse.group/t/cluster-statistics/134) in the scanpy discourse group). I wrote code for it on my own, but am not sure which part of `sc.pl` you want it to go to, so here is the code for your consideration:. ```python. import matplotlib.pyplot as plt. import scanpy as sc. import numpy as np. # given integrated object adata, clustered via the leiden algorithm and. # with the batch ID in the 'batch' slot, and a collection of batch_names:. # count the number of occurrences of each batch ID for each cluster ID. count_series = adata.obs.groupby(['leiden', 'batch']).size(). new_df = count_series.to_frame(name = 'size').reset_index(). # convert from multi index to pivot. constitution = new_df.pivot(index='leiden', columns='batch')['size']. # convert to %batch (but could be modified to show different things instead. perc_clust = np.array((constitution.T / np.sum(constitution.T, axis=0))). # keep track of the batch, cluster IDs so we can use them for plotting. clusters = adata.obs.leiden.cat.categories. batches = adata.obs.batch.cat.categories. # actual plotting; basically stacked barplots. # replace styling with scanpy defaults probably? fig, ax = plt.subplots(). ax.grid(False). ax.bar(clusters, perc_clust[0], 0.6, yerr=0, label=platy_names[0]). bottom = np.zeros(clusters.shape). for i, b in enume",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1573
https://github.com/scverse/scanpy/issues/1573:549,testability,integr,integrated,549,"Cluster content plot for integrated data; <!-- What kind of feature would you like to request? -->. - [ ] Additional function parameters / changed functionality / changed defaults? - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`? - [x] New plotting function: A kind of plot you would like to seein `sc.pl`? - [ ] External tools: Do you know an existing package that should go into `sc.external.*`? - [ ] Other? <!-- Please describe your wishes below: -->. I am integrating multiple datasets. In the integrated object I wanted to see for each cluster the percentage of each dataset it consisted of (also see [this question](https://scanpy.discourse.group/t/cluster-statistics/134) in the scanpy discourse group). I wrote code for it on my own, but am not sure which part of `sc.pl` you want it to go to, so here is the code for your consideration:. ```python. import matplotlib.pyplot as plt. import scanpy as sc. import numpy as np. # given integrated object adata, clustered via the leiden algorithm and. # with the batch ID in the 'batch' slot, and a collection of batch_names:. # count the number of occurrences of each batch ID for each cluster ID. count_series = adata.obs.groupby(['leiden', 'batch']).size(). new_df = count_series.to_frame(name = 'size').reset_index(). # convert from multi index to pivot. constitution = new_df.pivot(index='leiden', columns='batch')['size']. # convert to %batch (but could be modified to show different things instead. perc_clust = np.array((constitution.T / np.sum(constitution.T, axis=0))). # keep track of the batch, cluster IDs so we can use them for plotting. clusters = adata.obs.leiden.cat.categories. batches = adata.obs.batch.cat.categories. # actual plotting; basically stacked barplots. # replace styling with scanpy defaults probably? fig, ax = plt.subplots(). ax.grid(False). ax.bar(clusters, perc_clust[0], 0.6, yerr=0, label=platy_names[0]). bottom = np.zeros(clusters.shape). for i, b in enume",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1573
https://github.com/scverse/scanpy/issues/1573:991,testability,integr,integrated,991,"Cluster content plot for integrated data; <!-- What kind of feature would you like to request? -->. - [ ] Additional function parameters / changed functionality / changed defaults? - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`? - [x] New plotting function: A kind of plot you would like to seein `sc.pl`? - [ ] External tools: Do you know an existing package that should go into `sc.external.*`? - [ ] Other? <!-- Please describe your wishes below: -->. I am integrating multiple datasets. In the integrated object I wanted to see for each cluster the percentage of each dataset it consisted of (also see [this question](https://scanpy.discourse.group/t/cluster-statistics/134) in the scanpy discourse group). I wrote code for it on my own, but am not sure which part of `sc.pl` you want it to go to, so here is the code for your consideration:. ```python. import matplotlib.pyplot as plt. import scanpy as sc. import numpy as np. # given integrated object adata, clustered via the leiden algorithm and. # with the batch ID in the 'batch' slot, and a collection of batch_names:. # count the number of occurrences of each batch ID for each cluster ID. count_series = adata.obs.groupby(['leiden', 'batch']).size(). new_df = count_series.to_frame(name = 'size').reset_index(). # convert from multi index to pivot. constitution = new_df.pivot(index='leiden', columns='batch')['size']. # convert to %batch (but could be modified to show different things instead. perc_clust = np.array((constitution.T / np.sum(constitution.T, axis=0))). # keep track of the batch, cluster IDs so we can use them for plotting. clusters = adata.obs.leiden.cat.categories. batches = adata.obs.batch.cat.categories. # actual plotting; basically stacked barplots. # replace styling with scanpy defaults probably? fig, ax = plt.subplots(). ax.grid(False). ax.bar(clusters, perc_clust[0], 0.6, yerr=0, label=platy_names[0]). bottom = np.zeros(clusters.shape). for i, b in enume",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1573
https://github.com/scverse/scanpy/issues/1573:200,usability,tool,tool,200,"Cluster content plot for integrated data; <!-- What kind of feature would you like to request? -->. - [ ] Additional function parameters / changed functionality / changed defaults? - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`? - [x] New plotting function: A kind of plot you would like to seein `sc.pl`? - [ ] External tools: Do you know an existing package that should go into `sc.external.*`? - [ ] Other? <!-- Please describe your wishes below: -->. I am integrating multiple datasets. In the integrated object I wanted to see for each cluster the percentage of each dataset it consisted of (also see [this question](https://scanpy.discourse.group/t/cluster-statistics/134) in the scanpy discourse group). I wrote code for it on my own, but am not sure which part of `sc.pl` you want it to go to, so here is the code for your consideration:. ```python. import matplotlib.pyplot as plt. import scanpy as sc. import numpy as np. # given integrated object adata, clustered via the leiden algorithm and. # with the batch ID in the 'batch' slot, and a collection of batch_names:. # count the number of occurrences of each batch ID for each cluster ID. count_series = adata.obs.groupby(['leiden', 'batch']).size(). new_df = count_series.to_frame(name = 'size').reset_index(). # convert from multi index to pivot. constitution = new_df.pivot(index='leiden', columns='batch')['size']. # convert to %batch (but could be modified to show different things instead. perc_clust = np.array((constitution.T / np.sum(constitution.T, axis=0))). # keep track of the batch, cluster IDs so we can use them for plotting. clusters = adata.obs.leiden.cat.categories. batches = adata.obs.batch.cat.categories. # actual plotting; basically stacked barplots. # replace styling with scanpy defaults probably? fig, ax = plt.subplots(). ax.grid(False). ax.bar(clusters, perc_clust[0], 0.6, yerr=0, label=platy_names[0]). bottom = np.zeros(clusters.shape). for i, b in enume",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1573
https://github.com/scverse/scanpy/issues/1573:208,usability,simpl,simple,208,"Cluster content plot for integrated data; <!-- What kind of feature would you like to request? -->. - [ ] Additional function parameters / changed functionality / changed defaults? - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`? - [x] New plotting function: A kind of plot you would like to seein `sc.pl`? - [ ] External tools: Do you know an existing package that should go into `sc.external.*`? - [ ] Other? <!-- Please describe your wishes below: -->. I am integrating multiple datasets. In the integrated object I wanted to see for each cluster the percentage of each dataset it consisted of (also see [this question](https://scanpy.discourse.group/t/cluster-statistics/134) in the scanpy discourse group). I wrote code for it on my own, but am not sure which part of `sc.pl` you want it to go to, so here is the code for your consideration:. ```python. import matplotlib.pyplot as plt. import scanpy as sc. import numpy as np. # given integrated object adata, clustered via the leiden algorithm and. # with the batch ID in the 'batch' slot, and a collection of batch_names:. # count the number of occurrences of each batch ID for each cluster ID. count_series = adata.obs.groupby(['leiden', 'batch']).size(). new_df = count_series.to_frame(name = 'size').reset_index(). # convert from multi index to pivot. constitution = new_df.pivot(index='leiden', columns='batch')['size']. # convert to %batch (but could be modified to show different things instead. perc_clust = np.array((constitution.T / np.sum(constitution.T, axis=0))). # keep track of the batch, cluster IDs so we can use them for plotting. clusters = adata.obs.leiden.cat.categories. batches = adata.obs.batch.cat.categories. # actual plotting; basically stacked barplots. # replace styling with scanpy defaults probably? fig, ax = plt.subplots(). ax.grid(False). ax.bar(clusters, perc_clust[0], 0.6, yerr=0, label=platy_names[0]). bottom = np.zeros(clusters.shape). for i, b in enume",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1573
https://github.com/scverse/scanpy/issues/1573:224,usability,tool,tool,224,"Cluster content plot for integrated data; <!-- What kind of feature would you like to request? -->. - [ ] Additional function parameters / changed functionality / changed defaults? - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`? - [x] New plotting function: A kind of plot you would like to seein `sc.pl`? - [ ] External tools: Do you know an existing package that should go into `sc.external.*`? - [ ] Other? <!-- Please describe your wishes below: -->. I am integrating multiple datasets. In the integrated object I wanted to see for each cluster the percentage of each dataset it consisted of (also see [this question](https://scanpy.discourse.group/t/cluster-statistics/134) in the scanpy discourse group). I wrote code for it on my own, but am not sure which part of `sc.pl` you want it to go to, so here is the code for your consideration:. ```python. import matplotlib.pyplot as plt. import scanpy as sc. import numpy as np. # given integrated object adata, clustered via the leiden algorithm and. # with the batch ID in the 'batch' slot, and a collection of batch_names:. # count the number of occurrences of each batch ID for each cluster ID. count_series = adata.obs.groupby(['leiden', 'batch']).size(). new_df = count_series.to_frame(name = 'size').reset_index(). # convert from multi index to pivot. constitution = new_df.pivot(index='leiden', columns='batch')['size']. # convert to %batch (but could be modified to show different things instead. perc_clust = np.array((constitution.T / np.sum(constitution.T, axis=0))). # keep track of the batch, cluster IDs so we can use them for plotting. clusters = adata.obs.leiden.cat.categories. batches = adata.obs.batch.cat.categories. # actual plotting; basically stacked barplots. # replace styling with scanpy defaults probably? fig, ax = plt.subplots(). ax.grid(False). ax.bar(clusters, perc_clust[0], 0.6, yerr=0, label=platy_names[0]). bottom = np.zeros(clusters.shape). for i, b in enume",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1573
https://github.com/scverse/scanpy/issues/1573:272,usability,tool,tools,272,"Cluster content plot for integrated data; <!-- What kind of feature would you like to request? -->. - [ ] Additional function parameters / changed functionality / changed defaults? - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`? - [x] New plotting function: A kind of plot you would like to seein `sc.pl`? - [ ] External tools: Do you know an existing package that should go into `sc.external.*`? - [ ] Other? <!-- Please describe your wishes below: -->. I am integrating multiple datasets. In the integrated object I wanted to see for each cluster the percentage of each dataset it consisted of (also see [this question](https://scanpy.discourse.group/t/cluster-statistics/134) in the scanpy discourse group). I wrote code for it on my own, but am not sure which part of `sc.pl` you want it to go to, so here is the code for your consideration:. ```python. import matplotlib.pyplot as plt. import scanpy as sc. import numpy as np. # given integrated object adata, clustered via the leiden algorithm and. # with the batch ID in the 'batch' slot, and a collection of batch_names:. # count the number of occurrences of each batch ID for each cluster ID. count_series = adata.obs.groupby(['leiden', 'batch']).size(). new_df = count_series.to_frame(name = 'size').reset_index(). # convert from multi index to pivot. constitution = new_df.pivot(index='leiden', columns='batch')['size']. # convert to %batch (but could be modified to show different things instead. perc_clust = np.array((constitution.T / np.sum(constitution.T, axis=0))). # keep track of the batch, cluster IDs so we can use them for plotting. clusters = adata.obs.leiden.cat.categories. batches = adata.obs.batch.cat.categories. # actual plotting; basically stacked barplots. # replace styling with scanpy defaults probably? fig, ax = plt.subplots(). ax.grid(False). ax.bar(clusters, perc_clust[0], 0.6, yerr=0, label=platy_names[0]). bottom = np.zeros(clusters.shape). for i, b in enume",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1573
https://github.com/scverse/scanpy/issues/1573:372,usability,tool,tools,372,"Cluster content plot for integrated data; <!-- What kind of feature would you like to request? -->. - [ ] Additional function parameters / changed functionality / changed defaults? - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`? - [x] New plotting function: A kind of plot you would like to seein `sc.pl`? - [ ] External tools: Do you know an existing package that should go into `sc.external.*`? - [ ] Other? <!-- Please describe your wishes below: -->. I am integrating multiple datasets. In the integrated object I wanted to see for each cluster the percentage of each dataset it consisted of (also see [this question](https://scanpy.discourse.group/t/cluster-statistics/134) in the scanpy discourse group). I wrote code for it on my own, but am not sure which part of `sc.pl` you want it to go to, so here is the code for your consideration:. ```python. import matplotlib.pyplot as plt. import scanpy as sc. import numpy as np. # given integrated object adata, clustered via the leiden algorithm and. # with the batch ID in the 'batch' slot, and a collection of batch_names:. # count the number of occurrences of each batch ID for each cluster ID. count_series = adata.obs.groupby(['leiden', 'batch']).size(). new_df = count_series.to_frame(name = 'size').reset_index(). # convert from multi index to pivot. constitution = new_df.pivot(index='leiden', columns='batch')['size']. # convert to %batch (but could be modified to show different things instead. perc_clust = np.array((constitution.T / np.sum(constitution.T, axis=0))). # keep track of the batch, cluster IDs so we can use them for plotting. clusters = adata.obs.leiden.cat.categories. batches = adata.obs.batch.cat.categories. # actual plotting; basically stacked barplots. # replace styling with scanpy defaults probably? fig, ax = plt.subplots(). ax.grid(False). ax.bar(clusters, perc_clust[0], 0.6, yerr=0, label=platy_names[0]). bottom = np.zeros(clusters.shape). for i, b in enume",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1573
https://github.com/scverse/scanpy/issues/1573:634,usability,consist,consisted,634,"Cluster content plot for integrated data; <!-- What kind of feature would you like to request? -->. - [ ] Additional function parameters / changed functionality / changed defaults? - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`? - [x] New plotting function: A kind of plot you would like to seein `sc.pl`? - [ ] External tools: Do you know an existing package that should go into `sc.external.*`? - [ ] Other? <!-- Please describe your wishes below: -->. I am integrating multiple datasets. In the integrated object I wanted to see for each cluster the percentage of each dataset it consisted of (also see [this question](https://scanpy.discourse.group/t/cluster-statistics/134) in the scanpy discourse group). I wrote code for it on my own, but am not sure which part of `sc.pl` you want it to go to, so here is the code for your consideration:. ```python. import matplotlib.pyplot as plt. import scanpy as sc. import numpy as np. # given integrated object adata, clustered via the leiden algorithm and. # with the batch ID in the 'batch' slot, and a collection of batch_names:. # count the number of occurrences of each batch ID for each cluster ID. count_series = adata.obs.groupby(['leiden', 'batch']).size(). new_df = count_series.to_frame(name = 'size').reset_index(). # convert from multi index to pivot. constitution = new_df.pivot(index='leiden', columns='batch')['size']. # convert to %batch (but could be modified to show different things instead. perc_clust = np.array((constitution.T / np.sum(constitution.T, axis=0))). # keep track of the batch, cluster IDs so we can use them for plotting. clusters = adata.obs.leiden.cat.categories. batches = adata.obs.batch.cat.categories. # actual plotting; basically stacked barplots. # replace styling with scanpy defaults probably? fig, ax = plt.subplots(). ax.grid(False). ax.bar(clusters, perc_clust[0], 0.6, yerr=0, label=platy_names[0]). bottom = np.zeros(clusters.shape). for i, b in enume",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1573
https://github.com/scverse/scanpy/issues/1573:2283,usability,user,user-images,2283,": Do you know an existing package that should go into `sc.external.*`? - [ ] Other? <!-- Please describe your wishes below: -->. I am integrating multiple datasets. In the integrated object I wanted to see for each cluster the percentage of each dataset it consisted of (also see [this question](https://scanpy.discourse.group/t/cluster-statistics/134) in the scanpy discourse group). I wrote code for it on my own, but am not sure which part of `sc.pl` you want it to go to, so here is the code for your consideration:. ```python. import matplotlib.pyplot as plt. import scanpy as sc. import numpy as np. # given integrated object adata, clustered via the leiden algorithm and. # with the batch ID in the 'batch' slot, and a collection of batch_names:. # count the number of occurrences of each batch ID for each cluster ID. count_series = adata.obs.groupby(['leiden', 'batch']).size(). new_df = count_series.to_frame(name = 'size').reset_index(). # convert from multi index to pivot. constitution = new_df.pivot(index='leiden', columns='batch')['size']. # convert to %batch (but could be modified to show different things instead. perc_clust = np.array((constitution.T / np.sum(constitution.T, axis=0))). # keep track of the batch, cluster IDs so we can use them for plotting. clusters = adata.obs.leiden.cat.categories. batches = adata.obs.batch.cat.categories. # actual plotting; basically stacked barplots. # replace styling with scanpy defaults probably? fig, ax = plt.subplots(). ax.grid(False). ax.bar(clusters, perc_clust[0], 0.6, yerr=0, label=platy_names[0]). bottom = np.zeros(clusters.shape). for i, b in enumerate(batches):. ax.bar(clusters, perc_clust[i], 0.6, bottom=bottom, yerr=0, label=batch_names[i]). bottom += perc_clust[I]. . # place legend not on top of the plot. ax.legend(loc='right', bbox_to_anchor=(1.2, 0.5));. ```. End result should look something like this. ![image](https://user-images.githubusercontent.com/1651067/104041019-ac463480-51d8-11eb-93ee-267deb4aa3f9.png).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1573
https://github.com/scverse/scanpy/issues/1574:51,deployability,integr,integration,51,Adding cell2location to the list of scRNA->spatial integration methods; Hi! I am wondering if you could add https://github.com/BayraktarLab/cell2location to your list of scRNA->spatial integration methods (https://scanpy-tutorials.readthedocs.io/en/latest/spatial/integration-scanorama.html). Thanks!,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1574
https://github.com/scverse/scanpy/issues/1574:185,deployability,integr,integration,185,Adding cell2location to the list of scRNA->spatial integration methods; Hi! I am wondering if you could add https://github.com/BayraktarLab/cell2location to your list of scRNA->spatial integration methods (https://scanpy-tutorials.readthedocs.io/en/latest/spatial/integration-scanorama.html). Thanks!,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1574
https://github.com/scverse/scanpy/issues/1574:264,deployability,integr,integration-scanorama,264,Adding cell2location to the list of scRNA->spatial integration methods; Hi! I am wondering if you could add https://github.com/BayraktarLab/cell2location to your list of scRNA->spatial integration methods (https://scanpy-tutorials.readthedocs.io/en/latest/spatial/integration-scanorama.html). Thanks!,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1574
https://github.com/scverse/scanpy/issues/1574:51,integrability,integr,integration,51,Adding cell2location to the list of scRNA->spatial integration methods; Hi! I am wondering if you could add https://github.com/BayraktarLab/cell2location to your list of scRNA->spatial integration methods (https://scanpy-tutorials.readthedocs.io/en/latest/spatial/integration-scanorama.html). Thanks!,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1574
https://github.com/scverse/scanpy/issues/1574:185,integrability,integr,integration,185,Adding cell2location to the list of scRNA->spatial integration methods; Hi! I am wondering if you could add https://github.com/BayraktarLab/cell2location to your list of scRNA->spatial integration methods (https://scanpy-tutorials.readthedocs.io/en/latest/spatial/integration-scanorama.html). Thanks!,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1574
https://github.com/scverse/scanpy/issues/1574:264,integrability,integr,integration-scanorama,264,Adding cell2location to the list of scRNA->spatial integration methods; Hi! I am wondering if you could add https://github.com/BayraktarLab/cell2location to your list of scRNA->spatial integration methods (https://scanpy-tutorials.readthedocs.io/en/latest/spatial/integration-scanorama.html). Thanks!,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1574
https://github.com/scverse/scanpy/issues/1574:51,interoperability,integr,integration,51,Adding cell2location to the list of scRNA->spatial integration methods; Hi! I am wondering if you could add https://github.com/BayraktarLab/cell2location to your list of scRNA->spatial integration methods (https://scanpy-tutorials.readthedocs.io/en/latest/spatial/integration-scanorama.html). Thanks!,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1574
https://github.com/scverse/scanpy/issues/1574:185,interoperability,integr,integration,185,Adding cell2location to the list of scRNA->spatial integration methods; Hi! I am wondering if you could add https://github.com/BayraktarLab/cell2location to your list of scRNA->spatial integration methods (https://scanpy-tutorials.readthedocs.io/en/latest/spatial/integration-scanorama.html). Thanks!,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1574
https://github.com/scverse/scanpy/issues/1574:264,interoperability,integr,integration-scanorama,264,Adding cell2location to the list of scRNA->spatial integration methods; Hi! I am wondering if you could add https://github.com/BayraktarLab/cell2location to your list of scRNA->spatial integration methods (https://scanpy-tutorials.readthedocs.io/en/latest/spatial/integration-scanorama.html). Thanks!,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1574
https://github.com/scverse/scanpy/issues/1574:51,modifiability,integr,integration,51,Adding cell2location to the list of scRNA->spatial integration methods; Hi! I am wondering if you could add https://github.com/BayraktarLab/cell2location to your list of scRNA->spatial integration methods (https://scanpy-tutorials.readthedocs.io/en/latest/spatial/integration-scanorama.html). Thanks!,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1574
https://github.com/scverse/scanpy/issues/1574:185,modifiability,integr,integration,185,Adding cell2location to the list of scRNA->spatial integration methods; Hi! I am wondering if you could add https://github.com/BayraktarLab/cell2location to your list of scRNA->spatial integration methods (https://scanpy-tutorials.readthedocs.io/en/latest/spatial/integration-scanorama.html). Thanks!,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1574
https://github.com/scverse/scanpy/issues/1574:264,modifiability,integr,integration-scanorama,264,Adding cell2location to the list of scRNA->spatial integration methods; Hi! I am wondering if you could add https://github.com/BayraktarLab/cell2location to your list of scRNA->spatial integration methods (https://scanpy-tutorials.readthedocs.io/en/latest/spatial/integration-scanorama.html). Thanks!,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1574
https://github.com/scverse/scanpy/issues/1574:51,reliability,integr,integration,51,Adding cell2location to the list of scRNA->spatial integration methods; Hi! I am wondering if you could add https://github.com/BayraktarLab/cell2location to your list of scRNA->spatial integration methods (https://scanpy-tutorials.readthedocs.io/en/latest/spatial/integration-scanorama.html). Thanks!,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1574
https://github.com/scverse/scanpy/issues/1574:185,reliability,integr,integration,185,Adding cell2location to the list of scRNA->spatial integration methods; Hi! I am wondering if you could add https://github.com/BayraktarLab/cell2location to your list of scRNA->spatial integration methods (https://scanpy-tutorials.readthedocs.io/en/latest/spatial/integration-scanorama.html). Thanks!,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1574
https://github.com/scverse/scanpy/issues/1574:264,reliability,integr,integration-scanorama,264,Adding cell2location to the list of scRNA->spatial integration methods; Hi! I am wondering if you could add https://github.com/BayraktarLab/cell2location to your list of scRNA->spatial integration methods (https://scanpy-tutorials.readthedocs.io/en/latest/spatial/integration-scanorama.html). Thanks!,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1574
https://github.com/scverse/scanpy/issues/1574:51,security,integr,integration,51,Adding cell2location to the list of scRNA->spatial integration methods; Hi! I am wondering if you could add https://github.com/BayraktarLab/cell2location to your list of scRNA->spatial integration methods (https://scanpy-tutorials.readthedocs.io/en/latest/spatial/integration-scanorama.html). Thanks!,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1574
https://github.com/scverse/scanpy/issues/1574:185,security,integr,integration,185,Adding cell2location to the list of scRNA->spatial integration methods; Hi! I am wondering if you could add https://github.com/BayraktarLab/cell2location to your list of scRNA->spatial integration methods (https://scanpy-tutorials.readthedocs.io/en/latest/spatial/integration-scanorama.html). Thanks!,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1574
https://github.com/scverse/scanpy/issues/1574:264,security,integr,integration-scanorama,264,Adding cell2location to the list of scRNA->spatial integration methods; Hi! I am wondering if you could add https://github.com/BayraktarLab/cell2location to your list of scRNA->spatial integration methods (https://scanpy-tutorials.readthedocs.io/en/latest/spatial/integration-scanorama.html). Thanks!,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1574
https://github.com/scverse/scanpy/issues/1574:51,testability,integr,integration,51,Adding cell2location to the list of scRNA->spatial integration methods; Hi! I am wondering if you could add https://github.com/BayraktarLab/cell2location to your list of scRNA->spatial integration methods (https://scanpy-tutorials.readthedocs.io/en/latest/spatial/integration-scanorama.html). Thanks!,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1574
https://github.com/scverse/scanpy/issues/1574:185,testability,integr,integration,185,Adding cell2location to the list of scRNA->spatial integration methods; Hi! I am wondering if you could add https://github.com/BayraktarLab/cell2location to your list of scRNA->spatial integration methods (https://scanpy-tutorials.readthedocs.io/en/latest/spatial/integration-scanorama.html). Thanks!,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1574
https://github.com/scverse/scanpy/issues/1574:264,testability,integr,integration-scanorama,264,Adding cell2location to the list of scRNA->spatial integration methods; Hi! I am wondering if you could add https://github.com/BayraktarLab/cell2location to your list of scRNA->spatial integration methods (https://scanpy-tutorials.readthedocs.io/en/latest/spatial/integration-scanorama.html). Thanks!,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1574
https://github.com/scverse/scanpy/pull/1575:75,deployability,version,versioning,75,"Add verioning/ backporting docs; Added some docs for maintainers about how versioning works, and how we should be handling bug fixes.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1575
https://github.com/scverse/scanpy/pull/1575:75,integrability,version,versioning,75,"Add verioning/ backporting docs; Added some docs for maintainers about how versioning works, and how we should be handling bug fixes.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1575
https://github.com/scverse/scanpy/pull/1575:53,modifiability,maintain,maintainers,53,"Add verioning/ backporting docs; Added some docs for maintainers about how versioning works, and how we should be handling bug fixes.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1575
https://github.com/scverse/scanpy/pull/1575:75,modifiability,version,versioning,75,"Add verioning/ backporting docs; Added some docs for maintainers about how versioning works, and how we should be handling bug fixes.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1575
https://github.com/scverse/scanpy/pull/1575:53,safety,maintain,maintainers,53,"Add verioning/ backporting docs; Added some docs for maintainers about how versioning works, and how we should be handling bug fixes.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1575
https://github.com/scverse/scanpy/issues/1576:33,deployability,stack,stackoverflow,33,Diff coverage; @ivirshup https://stackoverflow.com/a/57222267/247482. `azurepipelines-coverage.yml`. ```yaml. coverage:. status:. diff:. target: 40%. ```,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1576
https://github.com/scverse/scanpy/issues/1576:5,testability,coverag,coverage,5,Diff coverage; @ivirshup https://stackoverflow.com/a/57222267/247482. `azurepipelines-coverage.yml`. ```yaml. coverage:. status:. diff:. target: 40%. ```,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1576
https://github.com/scverse/scanpy/issues/1576:86,testability,coverag,coverage,86,Diff coverage; @ivirshup https://stackoverflow.com/a/57222267/247482. `azurepipelines-coverage.yml`. ```yaml. coverage:. status:. diff:. target: 40%. ```,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1576
https://github.com/scverse/scanpy/issues/1576:110,testability,coverag,coverage,110,Diff coverage; @ivirshup https://stackoverflow.com/a/57222267/247482. `azurepipelines-coverage.yml`. ```yaml. coverage:. status:. diff:. target: 40%. ```,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1576
https://github.com/scverse/scanpy/issues/1576:121,usability,statu,status,121,Diff coverage; @ivirshup https://stackoverflow.com/a/57222267/247482. `azurepipelines-coverage.yml`. ```yaml. coverage:. status:. diff:. target: 40%. ```,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1576
https://github.com/scverse/scanpy/pull/1577:0,deployability,Updat,Update,0,Update ecosystem.rst; Added MiCV as a full pipeline in the external ecosystem docs. We recently built this tool and hope it will be useful to others trying to jump into this analysis space. Please let me know if there are any questions or issues with including this in the docs. Thank you for all that you're doing with this project! For extra information:. https://micv.works. https://github.com/Cai-Lab-at-University-of-Michigan/MiCV. https://www.biorxiv.org/content/10.1101/2020.07.02.184549v2. <!-- . Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1577
https://github.com/scverse/scanpy/pull/1577:43,deployability,pipelin,pipeline,43,Update ecosystem.rst; Added MiCV as a full pipeline in the external ecosystem docs. We recently built this tool and hope it will be useful to others trying to jump into this analysis space. Please let me know if there are any questions or issues with including this in the docs. Thank you for all that you're doing with this project! For extra information:. https://micv.works. https://github.com/Cai-Lab-at-University-of-Michigan/MiCV. https://www.biorxiv.org/content/10.1101/2020.07.02.184549v2. <!-- . Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1577
https://github.com/scverse/scanpy/pull/1577:43,integrability,pipelin,pipeline,43,Update ecosystem.rst; Added MiCV as a full pipeline in the external ecosystem docs. We recently built this tool and hope it will be useful to others trying to jump into this analysis space. Please let me know if there are any questions or issues with including this in the docs. Thank you for all that you're doing with this project! For extra information:. https://micv.works. https://github.com/Cai-Lab-at-University-of-Michigan/MiCV. https://www.biorxiv.org/content/10.1101/2020.07.02.184549v2. <!-- . Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1577
https://github.com/scverse/scanpy/pull/1577:461,performance,content,content,461,Update ecosystem.rst; Added MiCV as a full pipeline in the external ecosystem docs. We recently built this tool and hope it will be useful to others trying to jump into this analysis space. Please let me know if there are any questions or issues with including this in the docs. Thank you for all that you're doing with this project! For extra information:. https://micv.works. https://github.com/Cai-Lab-at-University-of-Michigan/MiCV. https://www.biorxiv.org/content/10.1101/2020.07.02.184549v2. <!-- . Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1577
https://github.com/scverse/scanpy/pull/1577:0,safety,Updat,Update,0,Update ecosystem.rst; Added MiCV as a full pipeline in the external ecosystem docs. We recently built this tool and hope it will be useful to others trying to jump into this analysis space. Please let me know if there are any questions or issues with including this in the docs. Thank you for all that you're doing with this project! For extra information:. https://micv.works. https://github.com/Cai-Lab-at-University-of-Michigan/MiCV. https://www.biorxiv.org/content/10.1101/2020.07.02.184549v2. <!-- . Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1577
https://github.com/scverse/scanpy/pull/1577:718,safety,review,review,718,Update ecosystem.rst; Added MiCV as a full pipeline in the external ecosystem docs. We recently built this tool and hope it will be useful to others trying to jump into this analysis space. Please let me know if there are any questions or issues with including this in the docs. Thank you for all that you're doing with this project! For extra information:. https://micv.works. https://github.com/Cai-Lab-at-University-of-Michigan/MiCV. https://www.biorxiv.org/content/10.1101/2020.07.02.184549v2. <!-- . Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1577
https://github.com/scverse/scanpy/pull/1577:0,security,Updat,Update,0,Update ecosystem.rst; Added MiCV as a full pipeline in the external ecosystem docs. We recently built this tool and hope it will be useful to others trying to jump into this analysis space. Please let me know if there are any questions or issues with including this in the docs. Thank you for all that you're doing with this project! For extra information:. https://micv.works. https://github.com/Cai-Lab-at-University-of-Michigan/MiCV. https://www.biorxiv.org/content/10.1101/2020.07.02.184549v2. <!-- . Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1577
https://github.com/scverse/scanpy/pull/1577:718,testability,review,review,718,Update ecosystem.rst; Added MiCV as a full pipeline in the external ecosystem docs. We recently built this tool and hope it will be useful to others trying to jump into this analysis space. Please let me know if there are any questions or issues with including this in the docs. Thank you for all that you're doing with this project! For extra information:. https://micv.works. https://github.com/Cai-Lab-at-University-of-Michigan/MiCV. https://www.biorxiv.org/content/10.1101/2020.07.02.184549v2. <!-- . Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1577
https://github.com/scverse/scanpy/pull/1577:107,usability,tool,tool,107,Update ecosystem.rst; Added MiCV as a full pipeline in the external ecosystem docs. We recently built this tool and hope it will be useful to others trying to jump into this analysis space. Please let me know if there are any questions or issues with including this in the docs. Thank you for all that you're doing with this project! For extra information:. https://micv.works. https://github.com/Cai-Lab-at-University-of-Michigan/MiCV. https://www.biorxiv.org/content/10.1101/2020.07.02.184549v2. <!-- . Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1577
https://github.com/scverse/scanpy/pull/1577:569,usability,guid,guidelines,569,Update ecosystem.rst; Added MiCV as a full pipeline in the external ecosystem docs. We recently built this tool and hope it will be useful to others trying to jump into this analysis space. Please let me know if there are any questions or issues with including this in the docs. Thank you for all that you're doing with this project! For extra information:. https://micv.works. https://github.com/Cai-Lab-at-University-of-Michigan/MiCV. https://www.biorxiv.org/content/10.1101/2020.07.02.184549v2. <!-- . Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1577
https://github.com/scverse/scanpy/pull/1577:600,usability,guid,guide,600,Update ecosystem.rst; Added MiCV as a full pipeline in the external ecosystem docs. We recently built this tool and hope it will be useful to others trying to jump into this analysis space. Please let me know if there are any questions or issues with including this in the docs. Thank you for all that you're doing with this project! For extra information:. https://micv.works. https://github.com/Cai-Lab-at-University-of-Michigan/MiCV. https://www.biorxiv.org/content/10.1101/2020.07.02.184549v2. <!-- . Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1577
https://github.com/scverse/scanpy/pull/1577:696,usability,workflow,workflow,696,Update ecosystem.rst; Added MiCV as a full pipeline in the external ecosystem docs. We recently built this tool and hope it will be useful to others trying to jump into this analysis space. Please let me know if there are any questions or issues with including this in the docs. Thank you for all that you're doing with this project! For extra information:. https://micv.works. https://github.com/Cai-Lab-at-University-of-Michigan/MiCV. https://www.biorxiv.org/content/10.1101/2020.07.02.184549v2. <!-- . Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1577
https://github.com/scverse/scanpy/issues/1578:89,availability,down,downstream,89,"shared high variable genes; Hi,. Is it necessary to use only high variable genes for the downstream analysis ? If an examperiment includes many batches, then each batch will give a different set of high variable genes, how to determine the shared high variable genes (intersection or union) when integrating the batches ? Does scany have any fucntion to get the shared genes ?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1578
https://github.com/scverse/scanpy/issues/1578:296,deployability,integr,integrating,296,"shared high variable genes; Hi,. Is it necessary to use only high variable genes for the downstream analysis ? If an examperiment includes many batches, then each batch will give a different set of high variable genes, how to determine the shared high variable genes (intersection or union) when integrating the batches ? Does scany have any fucntion to get the shared genes ?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1578
https://github.com/scverse/scanpy/issues/1578:144,integrability,batch,batches,144,"shared high variable genes; Hi,. Is it necessary to use only high variable genes for the downstream analysis ? If an examperiment includes many batches, then each batch will give a different set of high variable genes, how to determine the shared high variable genes (intersection or union) when integrating the batches ? Does scany have any fucntion to get the shared genes ?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1578
https://github.com/scverse/scanpy/issues/1578:163,integrability,batch,batch,163,"shared high variable genes; Hi,. Is it necessary to use only high variable genes for the downstream analysis ? If an examperiment includes many batches, then each batch will give a different set of high variable genes, how to determine the shared high variable genes (intersection or union) when integrating the batches ? Does scany have any fucntion to get the shared genes ?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1578
https://github.com/scverse/scanpy/issues/1578:296,integrability,integr,integrating,296,"shared high variable genes; Hi,. Is it necessary to use only high variable genes for the downstream analysis ? If an examperiment includes many batches, then each batch will give a different set of high variable genes, how to determine the shared high variable genes (intersection or union) when integrating the batches ? Does scany have any fucntion to get the shared genes ?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1578
https://github.com/scverse/scanpy/issues/1578:312,integrability,batch,batches,312,"shared high variable genes; Hi,. Is it necessary to use only high variable genes for the downstream analysis ? If an examperiment includes many batches, then each batch will give a different set of high variable genes, how to determine the shared high variable genes (intersection or union) when integrating the batches ? Does scany have any fucntion to get the shared genes ?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1578
https://github.com/scverse/scanpy/issues/1578:0,interoperability,share,shared,0,"shared high variable genes; Hi,. Is it necessary to use only high variable genes for the downstream analysis ? If an examperiment includes many batches, then each batch will give a different set of high variable genes, how to determine the shared high variable genes (intersection or union) when integrating the batches ? Does scany have any fucntion to get the shared genes ?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1578
https://github.com/scverse/scanpy/issues/1578:240,interoperability,share,shared,240,"shared high variable genes; Hi,. Is it necessary to use only high variable genes for the downstream analysis ? If an examperiment includes many batches, then each batch will give a different set of high variable genes, how to determine the shared high variable genes (intersection or union) when integrating the batches ? Does scany have any fucntion to get the shared genes ?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1578
https://github.com/scverse/scanpy/issues/1578:296,interoperability,integr,integrating,296,"shared high variable genes; Hi,. Is it necessary to use only high variable genes for the downstream analysis ? If an examperiment includes many batches, then each batch will give a different set of high variable genes, how to determine the shared high variable genes (intersection or union) when integrating the batches ? Does scany have any fucntion to get the shared genes ?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1578
https://github.com/scverse/scanpy/issues/1578:362,interoperability,share,shared,362,"shared high variable genes; Hi,. Is it necessary to use only high variable genes for the downstream analysis ? If an examperiment includes many batches, then each batch will give a different set of high variable genes, how to determine the shared high variable genes (intersection or union) when integrating the batches ? Does scany have any fucntion to get the shared genes ?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1578
https://github.com/scverse/scanpy/issues/1578:12,modifiability,variab,variable,12,"shared high variable genes; Hi,. Is it necessary to use only high variable genes for the downstream analysis ? If an examperiment includes many batches, then each batch will give a different set of high variable genes, how to determine the shared high variable genes (intersection or union) when integrating the batches ? Does scany have any fucntion to get the shared genes ?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1578
https://github.com/scverse/scanpy/issues/1578:66,modifiability,variab,variable,66,"shared high variable genes; Hi,. Is it necessary to use only high variable genes for the downstream analysis ? If an examperiment includes many batches, then each batch will give a different set of high variable genes, how to determine the shared high variable genes (intersection or union) when integrating the batches ? Does scany have any fucntion to get the shared genes ?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1578
https://github.com/scverse/scanpy/issues/1578:203,modifiability,variab,variable,203,"shared high variable genes; Hi,. Is it necessary to use only high variable genes for the downstream analysis ? If an examperiment includes many batches, then each batch will give a different set of high variable genes, how to determine the shared high variable genes (intersection or union) when integrating the batches ? Does scany have any fucntion to get the shared genes ?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1578
https://github.com/scverse/scanpy/issues/1578:252,modifiability,variab,variable,252,"shared high variable genes; Hi,. Is it necessary to use only high variable genes for the downstream analysis ? If an examperiment includes many batches, then each batch will give a different set of high variable genes, how to determine the shared high variable genes (intersection or union) when integrating the batches ? Does scany have any fucntion to get the shared genes ?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1578
https://github.com/scverse/scanpy/issues/1578:296,modifiability,integr,integrating,296,"shared high variable genes; Hi,. Is it necessary to use only high variable genes for the downstream analysis ? If an examperiment includes many batches, then each batch will give a different set of high variable genes, how to determine the shared high variable genes (intersection or union) when integrating the batches ? Does scany have any fucntion to get the shared genes ?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1578
https://github.com/scverse/scanpy/issues/1578:144,performance,batch,batches,144,"shared high variable genes; Hi,. Is it necessary to use only high variable genes for the downstream analysis ? If an examperiment includes many batches, then each batch will give a different set of high variable genes, how to determine the shared high variable genes (intersection or union) when integrating the batches ? Does scany have any fucntion to get the shared genes ?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1578
https://github.com/scverse/scanpy/issues/1578:163,performance,batch,batch,163,"shared high variable genes; Hi,. Is it necessary to use only high variable genes for the downstream analysis ? If an examperiment includes many batches, then each batch will give a different set of high variable genes, how to determine the shared high variable genes (intersection or union) when integrating the batches ? Does scany have any fucntion to get the shared genes ?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1578
https://github.com/scverse/scanpy/issues/1578:312,performance,batch,batches,312,"shared high variable genes; Hi,. Is it necessary to use only high variable genes for the downstream analysis ? If an examperiment includes many batches, then each batch will give a different set of high variable genes, how to determine the shared high variable genes (intersection or union) when integrating the batches ? Does scany have any fucntion to get the shared genes ?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1578
https://github.com/scverse/scanpy/issues/1578:296,reliability,integr,integrating,296,"shared high variable genes; Hi,. Is it necessary to use only high variable genes for the downstream analysis ? If an examperiment includes many batches, then each batch will give a different set of high variable genes, how to determine the shared high variable genes (intersection or union) when integrating the batches ? Does scany have any fucntion to get the shared genes ?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1578
https://github.com/scverse/scanpy/issues/1578:322,reliability,Doe,Does,322,"shared high variable genes; Hi,. Is it necessary to use only high variable genes for the downstream analysis ? If an examperiment includes many batches, then each batch will give a different set of high variable genes, how to determine the shared high variable genes (intersection or union) when integrating the batches ? Does scany have any fucntion to get the shared genes ?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1578
https://github.com/scverse/scanpy/issues/1578:296,security,integr,integrating,296,"shared high variable genes; Hi,. Is it necessary to use only high variable genes for the downstream analysis ? If an examperiment includes many batches, then each batch will give a different set of high variable genes, how to determine the shared high variable genes (intersection or union) when integrating the batches ? Does scany have any fucntion to get the shared genes ?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1578
https://github.com/scverse/scanpy/issues/1578:296,testability,integr,integrating,296,"shared high variable genes; Hi,. Is it necessary to use only high variable genes for the downstream analysis ? If an examperiment includes many batches, then each batch will give a different set of high variable genes, how to determine the shared high variable genes (intersection or union) when integrating the batches ? Does scany have any fucntion to get the shared genes ?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1578
https://github.com/scverse/scanpy/issues/1579:0,availability,Error,Error,0,"Error in `sc.tl.umap`; - [ ] I have checked that this issue has not already been reported. - [ ] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. ### Minimal code sample (that we can copy&paste without having any data). I am learning the example of [Integrating data using ingest and BBKNN](https://scanpy-tutorials.readthedocs.io/en/latest/integrating-data-using-ingest.html). When I run code . ```python. sc.tl.umap(adata_ref). ```. I get. ```pytb. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-13-0e548e19df3a> in <module>. 1 sc.pp.pca(adata_ref). 2 sc.pp.neighbors(adata_ref). ----> 3 sc.tl.umap(adata_ref). ~/miniconda3/envs/tf/lib/python3.6/site-packages/scanpy/tools/_umap.py in umap(adata, min_dist, spread, n_components, maxiter, alpha, gamma, negative_sample_rate, init_pos, random_state, a, b, copy, method, neighbors_key). 171 neigh_params.get('metric', 'euclidean'),. 172 neigh_params.get('metric_kwds', {}),. --> 173 verbose=settings.verbosity > 3,. 174 ). 175 elif method == 'rapids':. TypeError: simplicial_set_embedding() missing 3 required positional arguments: 'densmap', 'densmap_kwds', and 'output_dens'. ```. #### Versions. <details>. scanpy==1.6.0 anndata==0.7.5 umap==0.5.0 numpy==1.19.5 scipy==1.5.4 pandas==1.1.5 scikit-learn==0.24.0 statsmodels==0.12.1 python-igraph==0.8.3 leidenalg==0.8.3. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1579
https://github.com/scverse/scanpy/issues/1579:144,deployability,version,version,144,"Error in `sc.tl.umap`; - [ ] I have checked that this issue has not already been reported. - [ ] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. ### Minimal code sample (that we can copy&paste without having any data). I am learning the example of [Integrating data using ingest and BBKNN](https://scanpy-tutorials.readthedocs.io/en/latest/integrating-data-using-ingest.html). When I run code . ```python. sc.tl.umap(adata_ref). ```. I get. ```pytb. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-13-0e548e19df3a> in <module>. 1 sc.pp.pca(adata_ref). 2 sc.pp.neighbors(adata_ref). ----> 3 sc.tl.umap(adata_ref). ~/miniconda3/envs/tf/lib/python3.6/site-packages/scanpy/tools/_umap.py in umap(adata, min_dist, spread, n_components, maxiter, alpha, gamma, negative_sample_rate, init_pos, random_state, a, b, copy, method, neighbors_key). 171 neigh_params.get('metric', 'euclidean'),. 172 neigh_params.get('metric_kwds', {}),. --> 173 verbose=settings.verbosity > 3,. 174 ). 175 elif method == 'rapids':. TypeError: simplicial_set_embedding() missing 3 required positional arguments: 'densmap', 'densmap_kwds', and 'output_dens'. ```. #### Versions. <details>. scanpy==1.6.0 anndata==0.7.5 umap==0.5.0 numpy==1.19.5 scipy==1.5.4 pandas==1.1.5 scikit-learn==0.24.0 statsmodels==0.12.1 python-igraph==0.8.3 leidenalg==0.8.3. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1579
https://github.com/scverse/scanpy/issues/1579:539,deployability,Integr,Integrating,539,"Error in `sc.tl.umap`; - [ ] I have checked that this issue has not already been reported. - [ ] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. ### Minimal code sample (that we can copy&paste without having any data). I am learning the example of [Integrating data using ingest and BBKNN](https://scanpy-tutorials.readthedocs.io/en/latest/integrating-data-using-ingest.html). When I run code . ```python. sc.tl.umap(adata_ref). ```. I get. ```pytb. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-13-0e548e19df3a> in <module>. 1 sc.pp.pca(adata_ref). 2 sc.pp.neighbors(adata_ref). ----> 3 sc.tl.umap(adata_ref). ~/miniconda3/envs/tf/lib/python3.6/site-packages/scanpy/tools/_umap.py in umap(adata, min_dist, spread, n_components, maxiter, alpha, gamma, negative_sample_rate, init_pos, random_state, a, b, copy, method, neighbors_key). 171 neigh_params.get('metric', 'euclidean'),. 172 neigh_params.get('metric_kwds', {}),. --> 173 verbose=settings.verbosity > 3,. 174 ). 175 elif method == 'rapids':. TypeError: simplicial_set_embedding() missing 3 required positional arguments: 'densmap', 'densmap_kwds', and 'output_dens'. ```. #### Versions. <details>. scanpy==1.6.0 anndata==0.7.5 umap==0.5.0 numpy==1.19.5 scipy==1.5.4 pandas==1.1.5 scikit-learn==0.24.0 statsmodels==0.12.1 python-igraph==0.8.3 leidenalg==0.8.3. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1579
https://github.com/scverse/scanpy/issues/1579:630,deployability,integr,integrating-data-using-ingest,630,"Error in `sc.tl.umap`; - [ ] I have checked that this issue has not already been reported. - [ ] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. ### Minimal code sample (that we can copy&paste without having any data). I am learning the example of [Integrating data using ingest and BBKNN](https://scanpy-tutorials.readthedocs.io/en/latest/integrating-data-using-ingest.html). When I run code . ```python. sc.tl.umap(adata_ref). ```. I get. ```pytb. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-13-0e548e19df3a> in <module>. 1 sc.pp.pca(adata_ref). 2 sc.pp.neighbors(adata_ref). ----> 3 sc.tl.umap(adata_ref). ~/miniconda3/envs/tf/lib/python3.6/site-packages/scanpy/tools/_umap.py in umap(adata, min_dist, spread, n_components, maxiter, alpha, gamma, negative_sample_rate, init_pos, random_state, a, b, copy, method, neighbors_key). 171 neigh_params.get('metric', 'euclidean'),. 172 neigh_params.get('metric_kwds', {}),. --> 173 verbose=settings.verbosity > 3,. 174 ). 175 elif method == 'rapids':. TypeError: simplicial_set_embedding() missing 3 required positional arguments: 'densmap', 'densmap_kwds', and 'output_dens'. ```. #### Versions. <details>. scanpy==1.6.0 anndata==0.7.5 umap==0.5.0 numpy==1.19.5 scipy==1.5.4 pandas==1.1.5 scikit-learn==0.24.0 statsmodels==0.12.1 python-igraph==0.8.3 leidenalg==0.8.3. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1579
https://github.com/scverse/scanpy/issues/1579:898,deployability,modul,module,898,"Error in `sc.tl.umap`; - [ ] I have checked that this issue has not already been reported. - [ ] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. ### Minimal code sample (that we can copy&paste without having any data). I am learning the example of [Integrating data using ingest and BBKNN](https://scanpy-tutorials.readthedocs.io/en/latest/integrating-data-using-ingest.html). When I run code . ```python. sc.tl.umap(adata_ref). ```. I get. ```pytb. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-13-0e548e19df3a> in <module>. 1 sc.pp.pca(adata_ref). 2 sc.pp.neighbors(adata_ref). ----> 3 sc.tl.umap(adata_ref). ~/miniconda3/envs/tf/lib/python3.6/site-packages/scanpy/tools/_umap.py in umap(adata, min_dist, spread, n_components, maxiter, alpha, gamma, negative_sample_rate, init_pos, random_state, a, b, copy, method, neighbors_key). 171 neigh_params.get('metric', 'euclidean'),. 172 neigh_params.get('metric_kwds', {}),. --> 173 verbose=settings.verbosity > 3,. 174 ). 175 elif method == 'rapids':. TypeError: simplicial_set_embedding() missing 3 required positional arguments: 'densmap', 'densmap_kwds', and 'output_dens'. ```. #### Versions. <details>. scanpy==1.6.0 anndata==0.7.5 umap==0.5.0 numpy==1.19.5 scipy==1.5.4 pandas==1.1.5 scikit-learn==0.24.0 statsmodels==0.12.1 python-igraph==0.8.3 leidenalg==0.8.3. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1579
https://github.com/scverse/scanpy/issues/1579:1516,deployability,Version,Versions,1516,"Error in `sc.tl.umap`; - [ ] I have checked that this issue has not already been reported. - [ ] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. ### Minimal code sample (that we can copy&paste without having any data). I am learning the example of [Integrating data using ingest and BBKNN](https://scanpy-tutorials.readthedocs.io/en/latest/integrating-data-using-ingest.html). When I run code . ```python. sc.tl.umap(adata_ref). ```. I get. ```pytb. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-13-0e548e19df3a> in <module>. 1 sc.pp.pca(adata_ref). 2 sc.pp.neighbors(adata_ref). ----> 3 sc.tl.umap(adata_ref). ~/miniconda3/envs/tf/lib/python3.6/site-packages/scanpy/tools/_umap.py in umap(adata, min_dist, spread, n_components, maxiter, alpha, gamma, negative_sample_rate, init_pos, random_state, a, b, copy, method, neighbors_key). 171 neigh_params.get('metric', 'euclidean'),. 172 neigh_params.get('metric_kwds', {}),. --> 173 verbose=settings.verbosity > 3,. 174 ). 175 elif method == 'rapids':. TypeError: simplicial_set_embedding() missing 3 required positional arguments: 'densmap', 'densmap_kwds', and 'output_dens'. ```. #### Versions. <details>. scanpy==1.6.0 anndata==0.7.5 umap==0.5.0 numpy==1.19.5 scipy==1.5.4 pandas==1.1.5 scikit-learn==0.24.0 statsmodels==0.12.1 python-igraph==0.8.3 leidenalg==0.8.3. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1579
https://github.com/scverse/scanpy/issues/1579:144,integrability,version,version,144,"Error in `sc.tl.umap`; - [ ] I have checked that this issue has not already been reported. - [ ] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. ### Minimal code sample (that we can copy&paste without having any data). I am learning the example of [Integrating data using ingest and BBKNN](https://scanpy-tutorials.readthedocs.io/en/latest/integrating-data-using-ingest.html). When I run code . ```python. sc.tl.umap(adata_ref). ```. I get. ```pytb. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-13-0e548e19df3a> in <module>. 1 sc.pp.pca(adata_ref). 2 sc.pp.neighbors(adata_ref). ----> 3 sc.tl.umap(adata_ref). ~/miniconda3/envs/tf/lib/python3.6/site-packages/scanpy/tools/_umap.py in umap(adata, min_dist, spread, n_components, maxiter, alpha, gamma, negative_sample_rate, init_pos, random_state, a, b, copy, method, neighbors_key). 171 neigh_params.get('metric', 'euclidean'),. 172 neigh_params.get('metric_kwds', {}),. --> 173 verbose=settings.verbosity > 3,. 174 ). 175 elif method == 'rapids':. TypeError: simplicial_set_embedding() missing 3 required positional arguments: 'densmap', 'densmap_kwds', and 'output_dens'. ```. #### Versions. <details>. scanpy==1.6.0 anndata==0.7.5 umap==0.5.0 numpy==1.19.5 scipy==1.5.4 pandas==1.1.5 scikit-learn==0.24.0 statsmodels==0.12.1 python-igraph==0.8.3 leidenalg==0.8.3. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1579
https://github.com/scverse/scanpy/issues/1579:539,integrability,Integr,Integrating,539,"Error in `sc.tl.umap`; - [ ] I have checked that this issue has not already been reported. - [ ] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. ### Minimal code sample (that we can copy&paste without having any data). I am learning the example of [Integrating data using ingest and BBKNN](https://scanpy-tutorials.readthedocs.io/en/latest/integrating-data-using-ingest.html). When I run code . ```python. sc.tl.umap(adata_ref). ```. I get. ```pytb. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-13-0e548e19df3a> in <module>. 1 sc.pp.pca(adata_ref). 2 sc.pp.neighbors(adata_ref). ----> 3 sc.tl.umap(adata_ref). ~/miniconda3/envs/tf/lib/python3.6/site-packages/scanpy/tools/_umap.py in umap(adata, min_dist, spread, n_components, maxiter, alpha, gamma, negative_sample_rate, init_pos, random_state, a, b, copy, method, neighbors_key). 171 neigh_params.get('metric', 'euclidean'),. 172 neigh_params.get('metric_kwds', {}),. --> 173 verbose=settings.verbosity > 3,. 174 ). 175 elif method == 'rapids':. TypeError: simplicial_set_embedding() missing 3 required positional arguments: 'densmap', 'densmap_kwds', and 'output_dens'. ```. #### Versions. <details>. scanpy==1.6.0 anndata==0.7.5 umap==0.5.0 numpy==1.19.5 scipy==1.5.4 pandas==1.1.5 scikit-learn==0.24.0 statsmodels==0.12.1 python-igraph==0.8.3 leidenalg==0.8.3. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1579
https://github.com/scverse/scanpy/issues/1579:630,integrability,integr,integrating-data-using-ingest,630,"Error in `sc.tl.umap`; - [ ] I have checked that this issue has not already been reported. - [ ] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. ### Minimal code sample (that we can copy&paste without having any data). I am learning the example of [Integrating data using ingest and BBKNN](https://scanpy-tutorials.readthedocs.io/en/latest/integrating-data-using-ingest.html). When I run code . ```python. sc.tl.umap(adata_ref). ```. I get. ```pytb. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-13-0e548e19df3a> in <module>. 1 sc.pp.pca(adata_ref). 2 sc.pp.neighbors(adata_ref). ----> 3 sc.tl.umap(adata_ref). ~/miniconda3/envs/tf/lib/python3.6/site-packages/scanpy/tools/_umap.py in umap(adata, min_dist, spread, n_components, maxiter, alpha, gamma, negative_sample_rate, init_pos, random_state, a, b, copy, method, neighbors_key). 171 neigh_params.get('metric', 'euclidean'),. 172 neigh_params.get('metric_kwds', {}),. --> 173 verbose=settings.verbosity > 3,. 174 ). 175 elif method == 'rapids':. TypeError: simplicial_set_embedding() missing 3 required positional arguments: 'densmap', 'densmap_kwds', and 'output_dens'. ```. #### Versions. <details>. scanpy==1.6.0 anndata==0.7.5 umap==0.5.0 numpy==1.19.5 scipy==1.5.4 pandas==1.1.5 scikit-learn==0.24.0 statsmodels==0.12.1 python-igraph==0.8.3 leidenalg==0.8.3. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1579
https://github.com/scverse/scanpy/issues/1579:1516,integrability,Version,Versions,1516,"Error in `sc.tl.umap`; - [ ] I have checked that this issue has not already been reported. - [ ] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. ### Minimal code sample (that we can copy&paste without having any data). I am learning the example of [Integrating data using ingest and BBKNN](https://scanpy-tutorials.readthedocs.io/en/latest/integrating-data-using-ingest.html). When I run code . ```python. sc.tl.umap(adata_ref). ```. I get. ```pytb. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-13-0e548e19df3a> in <module>. 1 sc.pp.pca(adata_ref). 2 sc.pp.neighbors(adata_ref). ----> 3 sc.tl.umap(adata_ref). ~/miniconda3/envs/tf/lib/python3.6/site-packages/scanpy/tools/_umap.py in umap(adata, min_dist, spread, n_components, maxiter, alpha, gamma, negative_sample_rate, init_pos, random_state, a, b, copy, method, neighbors_key). 171 neigh_params.get('metric', 'euclidean'),. 172 neigh_params.get('metric_kwds', {}),. --> 173 verbose=settings.verbosity > 3,. 174 ). 175 elif method == 'rapids':. TypeError: simplicial_set_embedding() missing 3 required positional arguments: 'densmap', 'densmap_kwds', and 'output_dens'. ```. #### Versions. <details>. scanpy==1.6.0 anndata==0.7.5 umap==0.5.0 numpy==1.19.5 scipy==1.5.4 pandas==1.1.5 scikit-learn==0.24.0 statsmodels==0.12.1 python-igraph==0.8.3 leidenalg==0.8.3. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1579
https://github.com/scverse/scanpy/issues/1579:539,interoperability,Integr,Integrating,539,"Error in `sc.tl.umap`; - [ ] I have checked that this issue has not already been reported. - [ ] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. ### Minimal code sample (that we can copy&paste without having any data). I am learning the example of [Integrating data using ingest and BBKNN](https://scanpy-tutorials.readthedocs.io/en/latest/integrating-data-using-ingest.html). When I run code . ```python. sc.tl.umap(adata_ref). ```. I get. ```pytb. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-13-0e548e19df3a> in <module>. 1 sc.pp.pca(adata_ref). 2 sc.pp.neighbors(adata_ref). ----> 3 sc.tl.umap(adata_ref). ~/miniconda3/envs/tf/lib/python3.6/site-packages/scanpy/tools/_umap.py in umap(adata, min_dist, spread, n_components, maxiter, alpha, gamma, negative_sample_rate, init_pos, random_state, a, b, copy, method, neighbors_key). 171 neigh_params.get('metric', 'euclidean'),. 172 neigh_params.get('metric_kwds', {}),. --> 173 verbose=settings.verbosity > 3,. 174 ). 175 elif method == 'rapids':. TypeError: simplicial_set_embedding() missing 3 required positional arguments: 'densmap', 'densmap_kwds', and 'output_dens'. ```. #### Versions. <details>. scanpy==1.6.0 anndata==0.7.5 umap==0.5.0 numpy==1.19.5 scipy==1.5.4 pandas==1.1.5 scikit-learn==0.24.0 statsmodels==0.12.1 python-igraph==0.8.3 leidenalg==0.8.3. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1579
https://github.com/scverse/scanpy/issues/1579:630,interoperability,integr,integrating-data-using-ingest,630,"Error in `sc.tl.umap`; - [ ] I have checked that this issue has not already been reported. - [ ] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. ### Minimal code sample (that we can copy&paste without having any data). I am learning the example of [Integrating data using ingest and BBKNN](https://scanpy-tutorials.readthedocs.io/en/latest/integrating-data-using-ingest.html). When I run code . ```python. sc.tl.umap(adata_ref). ```. I get. ```pytb. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-13-0e548e19df3a> in <module>. 1 sc.pp.pca(adata_ref). 2 sc.pp.neighbors(adata_ref). ----> 3 sc.tl.umap(adata_ref). ~/miniconda3/envs/tf/lib/python3.6/site-packages/scanpy/tools/_umap.py in umap(adata, min_dist, spread, n_components, maxiter, alpha, gamma, negative_sample_rate, init_pos, random_state, a, b, copy, method, neighbors_key). 171 neigh_params.get('metric', 'euclidean'),. 172 neigh_params.get('metric_kwds', {}),. --> 173 verbose=settings.verbosity > 3,. 174 ). 175 elif method == 'rapids':. TypeError: simplicial_set_embedding() missing 3 required positional arguments: 'densmap', 'densmap_kwds', and 'output_dens'. ```. #### Versions. <details>. scanpy==1.6.0 anndata==0.7.5 umap==0.5.0 numpy==1.19.5 scipy==1.5.4 pandas==1.1.5 scikit-learn==0.24.0 statsmodels==0.12.1 python-igraph==0.8.3 leidenalg==0.8.3. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1579
https://github.com/scverse/scanpy/issues/1579:144,modifiability,version,version,144,"Error in `sc.tl.umap`; - [ ] I have checked that this issue has not already been reported. - [ ] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. ### Minimal code sample (that we can copy&paste without having any data). I am learning the example of [Integrating data using ingest and BBKNN](https://scanpy-tutorials.readthedocs.io/en/latest/integrating-data-using-ingest.html). When I run code . ```python. sc.tl.umap(adata_ref). ```. I get. ```pytb. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-13-0e548e19df3a> in <module>. 1 sc.pp.pca(adata_ref). 2 sc.pp.neighbors(adata_ref). ----> 3 sc.tl.umap(adata_ref). ~/miniconda3/envs/tf/lib/python3.6/site-packages/scanpy/tools/_umap.py in umap(adata, min_dist, spread, n_components, maxiter, alpha, gamma, negative_sample_rate, init_pos, random_state, a, b, copy, method, neighbors_key). 171 neigh_params.get('metric', 'euclidean'),. 172 neigh_params.get('metric_kwds', {}),. --> 173 verbose=settings.verbosity > 3,. 174 ). 175 elif method == 'rapids':. TypeError: simplicial_set_embedding() missing 3 required positional arguments: 'densmap', 'densmap_kwds', and 'output_dens'. ```. #### Versions. <details>. scanpy==1.6.0 anndata==0.7.5 umap==0.5.0 numpy==1.19.5 scipy==1.5.4 pandas==1.1.5 scikit-learn==0.24.0 statsmodels==0.12.1 python-igraph==0.8.3 leidenalg==0.8.3. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1579
https://github.com/scverse/scanpy/issues/1579:539,modifiability,Integr,Integrating,539,"Error in `sc.tl.umap`; - [ ] I have checked that this issue has not already been reported. - [ ] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. ### Minimal code sample (that we can copy&paste without having any data). I am learning the example of [Integrating data using ingest and BBKNN](https://scanpy-tutorials.readthedocs.io/en/latest/integrating-data-using-ingest.html). When I run code . ```python. sc.tl.umap(adata_ref). ```. I get. ```pytb. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-13-0e548e19df3a> in <module>. 1 sc.pp.pca(adata_ref). 2 sc.pp.neighbors(adata_ref). ----> 3 sc.tl.umap(adata_ref). ~/miniconda3/envs/tf/lib/python3.6/site-packages/scanpy/tools/_umap.py in umap(adata, min_dist, spread, n_components, maxiter, alpha, gamma, negative_sample_rate, init_pos, random_state, a, b, copy, method, neighbors_key). 171 neigh_params.get('metric', 'euclidean'),. 172 neigh_params.get('metric_kwds', {}),. --> 173 verbose=settings.verbosity > 3,. 174 ). 175 elif method == 'rapids':. TypeError: simplicial_set_embedding() missing 3 required positional arguments: 'densmap', 'densmap_kwds', and 'output_dens'. ```. #### Versions. <details>. scanpy==1.6.0 anndata==0.7.5 umap==0.5.0 numpy==1.19.5 scipy==1.5.4 pandas==1.1.5 scikit-learn==0.24.0 statsmodels==0.12.1 python-igraph==0.8.3 leidenalg==0.8.3. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1579
https://github.com/scverse/scanpy/issues/1579:630,modifiability,integr,integrating-data-using-ingest,630,"Error in `sc.tl.umap`; - [ ] I have checked that this issue has not already been reported. - [ ] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. ### Minimal code sample (that we can copy&paste without having any data). I am learning the example of [Integrating data using ingest and BBKNN](https://scanpy-tutorials.readthedocs.io/en/latest/integrating-data-using-ingest.html). When I run code . ```python. sc.tl.umap(adata_ref). ```. I get. ```pytb. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-13-0e548e19df3a> in <module>. 1 sc.pp.pca(adata_ref). 2 sc.pp.neighbors(adata_ref). ----> 3 sc.tl.umap(adata_ref). ~/miniconda3/envs/tf/lib/python3.6/site-packages/scanpy/tools/_umap.py in umap(adata, min_dist, spread, n_components, maxiter, alpha, gamma, negative_sample_rate, init_pos, random_state, a, b, copy, method, neighbors_key). 171 neigh_params.get('metric', 'euclidean'),. 172 neigh_params.get('metric_kwds', {}),. --> 173 verbose=settings.verbosity > 3,. 174 ). 175 elif method == 'rapids':. TypeError: simplicial_set_embedding() missing 3 required positional arguments: 'densmap', 'densmap_kwds', and 'output_dens'. ```. #### Versions. <details>. scanpy==1.6.0 anndata==0.7.5 umap==0.5.0 numpy==1.19.5 scipy==1.5.4 pandas==1.1.5 scikit-learn==0.24.0 statsmodels==0.12.1 python-igraph==0.8.3 leidenalg==0.8.3. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1579
https://github.com/scverse/scanpy/issues/1579:898,modifiability,modul,module,898,"Error in `sc.tl.umap`; - [ ] I have checked that this issue has not already been reported. - [ ] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. ### Minimal code sample (that we can copy&paste without having any data). I am learning the example of [Integrating data using ingest and BBKNN](https://scanpy-tutorials.readthedocs.io/en/latest/integrating-data-using-ingest.html). When I run code . ```python. sc.tl.umap(adata_ref). ```. I get. ```pytb. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-13-0e548e19df3a> in <module>. 1 sc.pp.pca(adata_ref). 2 sc.pp.neighbors(adata_ref). ----> 3 sc.tl.umap(adata_ref). ~/miniconda3/envs/tf/lib/python3.6/site-packages/scanpy/tools/_umap.py in umap(adata, min_dist, spread, n_components, maxiter, alpha, gamma, negative_sample_rate, init_pos, random_state, a, b, copy, method, neighbors_key). 171 neigh_params.get('metric', 'euclidean'),. 172 neigh_params.get('metric_kwds', {}),. --> 173 verbose=settings.verbosity > 3,. 174 ). 175 elif method == 'rapids':. TypeError: simplicial_set_embedding() missing 3 required positional arguments: 'densmap', 'densmap_kwds', and 'output_dens'. ```. #### Versions. <details>. scanpy==1.6.0 anndata==0.7.5 umap==0.5.0 numpy==1.19.5 scipy==1.5.4 pandas==1.1.5 scikit-learn==0.24.0 statsmodels==0.12.1 python-igraph==0.8.3 leidenalg==0.8.3. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1579
https://github.com/scverse/scanpy/issues/1579:1032,modifiability,pac,packages,1032,"Error in `sc.tl.umap`; - [ ] I have checked that this issue has not already been reported. - [ ] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. ### Minimal code sample (that we can copy&paste without having any data). I am learning the example of [Integrating data using ingest and BBKNN](https://scanpy-tutorials.readthedocs.io/en/latest/integrating-data-using-ingest.html). When I run code . ```python. sc.tl.umap(adata_ref). ```. I get. ```pytb. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-13-0e548e19df3a> in <module>. 1 sc.pp.pca(adata_ref). 2 sc.pp.neighbors(adata_ref). ----> 3 sc.tl.umap(adata_ref). ~/miniconda3/envs/tf/lib/python3.6/site-packages/scanpy/tools/_umap.py in umap(adata, min_dist, spread, n_components, maxiter, alpha, gamma, negative_sample_rate, init_pos, random_state, a, b, copy, method, neighbors_key). 171 neigh_params.get('metric', 'euclidean'),. 172 neigh_params.get('metric_kwds', {}),. --> 173 verbose=settings.verbosity > 3,. 174 ). 175 elif method == 'rapids':. TypeError: simplicial_set_embedding() missing 3 required positional arguments: 'densmap', 'densmap_kwds', and 'output_dens'. ```. #### Versions. <details>. scanpy==1.6.0 anndata==0.7.5 umap==0.5.0 numpy==1.19.5 scipy==1.5.4 pandas==1.1.5 scikit-learn==0.24.0 statsmodels==0.12.1 python-igraph==0.8.3 leidenalg==0.8.3. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1579
https://github.com/scverse/scanpy/issues/1579:1516,modifiability,Version,Versions,1516,"Error in `sc.tl.umap`; - [ ] I have checked that this issue has not already been reported. - [ ] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. ### Minimal code sample (that we can copy&paste without having any data). I am learning the example of [Integrating data using ingest and BBKNN](https://scanpy-tutorials.readthedocs.io/en/latest/integrating-data-using-ingest.html). When I run code . ```python. sc.tl.umap(adata_ref). ```. I get. ```pytb. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-13-0e548e19df3a> in <module>. 1 sc.pp.pca(adata_ref). 2 sc.pp.neighbors(adata_ref). ----> 3 sc.tl.umap(adata_ref). ~/miniconda3/envs/tf/lib/python3.6/site-packages/scanpy/tools/_umap.py in umap(adata, min_dist, spread, n_components, maxiter, alpha, gamma, negative_sample_rate, init_pos, random_state, a, b, copy, method, neighbors_key). 171 neigh_params.get('metric', 'euclidean'),. 172 neigh_params.get('metric_kwds', {}),. --> 173 verbose=settings.verbosity > 3,. 174 ). 175 elif method == 'rapids':. TypeError: simplicial_set_embedding() missing 3 required positional arguments: 'densmap', 'densmap_kwds', and 'output_dens'. ```. #### Versions. <details>. scanpy==1.6.0 anndata==0.7.5 umap==0.5.0 numpy==1.19.5 scipy==1.5.4 pandas==1.1.5 scikit-learn==0.24.0 statsmodels==0.12.1 python-igraph==0.8.3 leidenalg==0.8.3. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1579
https://github.com/scverse/scanpy/issues/1579:0,performance,Error,Error,0,"Error in `sc.tl.umap`; - [ ] I have checked that this issue has not already been reported. - [ ] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. ### Minimal code sample (that we can copy&paste without having any data). I am learning the example of [Integrating data using ingest and BBKNN](https://scanpy-tutorials.readthedocs.io/en/latest/integrating-data-using-ingest.html). When I run code . ```python. sc.tl.umap(adata_ref). ```. I get. ```pytb. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-13-0e548e19df3a> in <module>. 1 sc.pp.pca(adata_ref). 2 sc.pp.neighbors(adata_ref). ----> 3 sc.tl.umap(adata_ref). ~/miniconda3/envs/tf/lib/python3.6/site-packages/scanpy/tools/_umap.py in umap(adata, min_dist, spread, n_components, maxiter, alpha, gamma, negative_sample_rate, init_pos, random_state, a, b, copy, method, neighbors_key). 171 neigh_params.get('metric', 'euclidean'),. 172 neigh_params.get('metric_kwds', {}),. --> 173 verbose=settings.verbosity > 3,. 174 ). 175 elif method == 'rapids':. TypeError: simplicial_set_embedding() missing 3 required positional arguments: 'densmap', 'densmap_kwds', and 'output_dens'. ```. #### Versions. <details>. scanpy==1.6.0 anndata==0.7.5 umap==0.5.0 numpy==1.19.5 scipy==1.5.4 pandas==1.1.5 scikit-learn==0.24.0 statsmodels==0.12.1 python-igraph==0.8.3 leidenalg==0.8.3. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1579
https://github.com/scverse/scanpy/issues/1579:539,reliability,Integr,Integrating,539,"Error in `sc.tl.umap`; - [ ] I have checked that this issue has not already been reported. - [ ] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. ### Minimal code sample (that we can copy&paste without having any data). I am learning the example of [Integrating data using ingest and BBKNN](https://scanpy-tutorials.readthedocs.io/en/latest/integrating-data-using-ingest.html). When I run code . ```python. sc.tl.umap(adata_ref). ```. I get. ```pytb. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-13-0e548e19df3a> in <module>. 1 sc.pp.pca(adata_ref). 2 sc.pp.neighbors(adata_ref). ----> 3 sc.tl.umap(adata_ref). ~/miniconda3/envs/tf/lib/python3.6/site-packages/scanpy/tools/_umap.py in umap(adata, min_dist, spread, n_components, maxiter, alpha, gamma, negative_sample_rate, init_pos, random_state, a, b, copy, method, neighbors_key). 171 neigh_params.get('metric', 'euclidean'),. 172 neigh_params.get('metric_kwds', {}),. --> 173 verbose=settings.verbosity > 3,. 174 ). 175 elif method == 'rapids':. TypeError: simplicial_set_embedding() missing 3 required positional arguments: 'densmap', 'densmap_kwds', and 'output_dens'. ```. #### Versions. <details>. scanpy==1.6.0 anndata==0.7.5 umap==0.5.0 numpy==1.19.5 scipy==1.5.4 pandas==1.1.5 scikit-learn==0.24.0 statsmodels==0.12.1 python-igraph==0.8.3 leidenalg==0.8.3. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1579
https://github.com/scverse/scanpy/issues/1579:630,reliability,integr,integrating-data-using-ingest,630,"Error in `sc.tl.umap`; - [ ] I have checked that this issue has not already been reported. - [ ] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. ### Minimal code sample (that we can copy&paste without having any data). I am learning the example of [Integrating data using ingest and BBKNN](https://scanpy-tutorials.readthedocs.io/en/latest/integrating-data-using-ingest.html). When I run code . ```python. sc.tl.umap(adata_ref). ```. I get. ```pytb. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-13-0e548e19df3a> in <module>. 1 sc.pp.pca(adata_ref). 2 sc.pp.neighbors(adata_ref). ----> 3 sc.tl.umap(adata_ref). ~/miniconda3/envs/tf/lib/python3.6/site-packages/scanpy/tools/_umap.py in umap(adata, min_dist, spread, n_components, maxiter, alpha, gamma, negative_sample_rate, init_pos, random_state, a, b, copy, method, neighbors_key). 171 neigh_params.get('metric', 'euclidean'),. 172 neigh_params.get('metric_kwds', {}),. --> 173 verbose=settings.verbosity > 3,. 174 ). 175 elif method == 'rapids':. TypeError: simplicial_set_embedding() missing 3 required positional arguments: 'densmap', 'densmap_kwds', and 'output_dens'. ```. #### Versions. <details>. scanpy==1.6.0 anndata==0.7.5 umap==0.5.0 numpy==1.19.5 scipy==1.5.4 pandas==1.1.5 scikit-learn==0.24.0 statsmodels==0.12.1 python-igraph==0.8.3 leidenalg==0.8.3. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1579
https://github.com/scverse/scanpy/issues/1579:0,safety,Error,Error,0,"Error in `sc.tl.umap`; - [ ] I have checked that this issue has not already been reported. - [ ] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. ### Minimal code sample (that we can copy&paste without having any data). I am learning the example of [Integrating data using ingest and BBKNN](https://scanpy-tutorials.readthedocs.io/en/latest/integrating-data-using-ingest.html). When I run code . ```python. sc.tl.umap(adata_ref). ```. I get. ```pytb. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-13-0e548e19df3a> in <module>. 1 sc.pp.pca(adata_ref). 2 sc.pp.neighbors(adata_ref). ----> 3 sc.tl.umap(adata_ref). ~/miniconda3/envs/tf/lib/python3.6/site-packages/scanpy/tools/_umap.py in umap(adata, min_dist, spread, n_components, maxiter, alpha, gamma, negative_sample_rate, init_pos, random_state, a, b, copy, method, neighbors_key). 171 neigh_params.get('metric', 'euclidean'),. 172 neigh_params.get('metric_kwds', {}),. --> 173 verbose=settings.verbosity > 3,. 174 ). 175 elif method == 'rapids':. TypeError: simplicial_set_embedding() missing 3 required positional arguments: 'densmap', 'densmap_kwds', and 'output_dens'. ```. #### Versions. <details>. scanpy==1.6.0 anndata==0.7.5 umap==0.5.0 numpy==1.19.5 scipy==1.5.4 pandas==1.1.5 scikit-learn==0.24.0 statsmodels==0.12.1 python-igraph==0.8.3 leidenalg==0.8.3. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1579
https://github.com/scverse/scanpy/issues/1579:871,safety,input,input-,871,"Error in `sc.tl.umap`; - [ ] I have checked that this issue has not already been reported. - [ ] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. ### Minimal code sample (that we can copy&paste without having any data). I am learning the example of [Integrating data using ingest and BBKNN](https://scanpy-tutorials.readthedocs.io/en/latest/integrating-data-using-ingest.html). When I run code . ```python. sc.tl.umap(adata_ref). ```. I get. ```pytb. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-13-0e548e19df3a> in <module>. 1 sc.pp.pca(adata_ref). 2 sc.pp.neighbors(adata_ref). ----> 3 sc.tl.umap(adata_ref). ~/miniconda3/envs/tf/lib/python3.6/site-packages/scanpy/tools/_umap.py in umap(adata, min_dist, spread, n_components, maxiter, alpha, gamma, negative_sample_rate, init_pos, random_state, a, b, copy, method, neighbors_key). 171 neigh_params.get('metric', 'euclidean'),. 172 neigh_params.get('metric_kwds', {}),. --> 173 verbose=settings.verbosity > 3,. 174 ). 175 elif method == 'rapids':. TypeError: simplicial_set_embedding() missing 3 required positional arguments: 'densmap', 'densmap_kwds', and 'output_dens'. ```. #### Versions. <details>. scanpy==1.6.0 anndata==0.7.5 umap==0.5.0 numpy==1.19.5 scipy==1.5.4 pandas==1.1.5 scikit-learn==0.24.0 statsmodels==0.12.1 python-igraph==0.8.3 leidenalg==0.8.3. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1579
https://github.com/scverse/scanpy/issues/1579:898,safety,modul,module,898,"Error in `sc.tl.umap`; - [ ] I have checked that this issue has not already been reported. - [ ] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. ### Minimal code sample (that we can copy&paste without having any data). I am learning the example of [Integrating data using ingest and BBKNN](https://scanpy-tutorials.readthedocs.io/en/latest/integrating-data-using-ingest.html). When I run code . ```python. sc.tl.umap(adata_ref). ```. I get. ```pytb. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-13-0e548e19df3a> in <module>. 1 sc.pp.pca(adata_ref). 2 sc.pp.neighbors(adata_ref). ----> 3 sc.tl.umap(adata_ref). ~/miniconda3/envs/tf/lib/python3.6/site-packages/scanpy/tools/_umap.py in umap(adata, min_dist, spread, n_components, maxiter, alpha, gamma, negative_sample_rate, init_pos, random_state, a, b, copy, method, neighbors_key). 171 neigh_params.get('metric', 'euclidean'),. 172 neigh_params.get('metric_kwds', {}),. --> 173 verbose=settings.verbosity > 3,. 174 ). 175 elif method == 'rapids':. TypeError: simplicial_set_embedding() missing 3 required positional arguments: 'densmap', 'densmap_kwds', and 'output_dens'. ```. #### Versions. <details>. scanpy==1.6.0 anndata==0.7.5 umap==0.5.0 numpy==1.19.5 scipy==1.5.4 pandas==1.1.5 scikit-learn==0.24.0 statsmodels==0.12.1 python-igraph==0.8.3 leidenalg==0.8.3. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1579
https://github.com/scverse/scanpy/issues/1579:539,security,Integr,Integrating,539,"Error in `sc.tl.umap`; - [ ] I have checked that this issue has not already been reported. - [ ] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. ### Minimal code sample (that we can copy&paste without having any data). I am learning the example of [Integrating data using ingest and BBKNN](https://scanpy-tutorials.readthedocs.io/en/latest/integrating-data-using-ingest.html). When I run code . ```python. sc.tl.umap(adata_ref). ```. I get. ```pytb. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-13-0e548e19df3a> in <module>. 1 sc.pp.pca(adata_ref). 2 sc.pp.neighbors(adata_ref). ----> 3 sc.tl.umap(adata_ref). ~/miniconda3/envs/tf/lib/python3.6/site-packages/scanpy/tools/_umap.py in umap(adata, min_dist, spread, n_components, maxiter, alpha, gamma, negative_sample_rate, init_pos, random_state, a, b, copy, method, neighbors_key). 171 neigh_params.get('metric', 'euclidean'),. 172 neigh_params.get('metric_kwds', {}),. --> 173 verbose=settings.verbosity > 3,. 174 ). 175 elif method == 'rapids':. TypeError: simplicial_set_embedding() missing 3 required positional arguments: 'densmap', 'densmap_kwds', and 'output_dens'. ```. #### Versions. <details>. scanpy==1.6.0 anndata==0.7.5 umap==0.5.0 numpy==1.19.5 scipy==1.5.4 pandas==1.1.5 scikit-learn==0.24.0 statsmodels==0.12.1 python-igraph==0.8.3 leidenalg==0.8.3. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1579
https://github.com/scverse/scanpy/issues/1579:630,security,integr,integrating-data-using-ingest,630,"Error in `sc.tl.umap`; - [ ] I have checked that this issue has not already been reported. - [ ] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. ### Minimal code sample (that we can copy&paste without having any data). I am learning the example of [Integrating data using ingest and BBKNN](https://scanpy-tutorials.readthedocs.io/en/latest/integrating-data-using-ingest.html). When I run code . ```python. sc.tl.umap(adata_ref). ```. I get. ```pytb. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-13-0e548e19df3a> in <module>. 1 sc.pp.pca(adata_ref). 2 sc.pp.neighbors(adata_ref). ----> 3 sc.tl.umap(adata_ref). ~/miniconda3/envs/tf/lib/python3.6/site-packages/scanpy/tools/_umap.py in umap(adata, min_dist, spread, n_components, maxiter, alpha, gamma, negative_sample_rate, init_pos, random_state, a, b, copy, method, neighbors_key). 171 neigh_params.get('metric', 'euclidean'),. 172 neigh_params.get('metric_kwds', {}),. --> 173 verbose=settings.verbosity > 3,. 174 ). 175 elif method == 'rapids':. TypeError: simplicial_set_embedding() missing 3 required positional arguments: 'densmap', 'densmap_kwds', and 'output_dens'. ```. #### Versions. <details>. scanpy==1.6.0 anndata==0.7.5 umap==0.5.0 numpy==1.19.5 scipy==1.5.4 pandas==1.1.5 scikit-learn==0.24.0 statsmodels==0.12.1 python-igraph==0.8.3 leidenalg==0.8.3. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1579
https://github.com/scverse/scanpy/issues/1579:539,testability,Integr,Integrating,539,"Error in `sc.tl.umap`; - [ ] I have checked that this issue has not already been reported. - [ ] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. ### Minimal code sample (that we can copy&paste without having any data). I am learning the example of [Integrating data using ingest and BBKNN](https://scanpy-tutorials.readthedocs.io/en/latest/integrating-data-using-ingest.html). When I run code . ```python. sc.tl.umap(adata_ref). ```. I get. ```pytb. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-13-0e548e19df3a> in <module>. 1 sc.pp.pca(adata_ref). 2 sc.pp.neighbors(adata_ref). ----> 3 sc.tl.umap(adata_ref). ~/miniconda3/envs/tf/lib/python3.6/site-packages/scanpy/tools/_umap.py in umap(adata, min_dist, spread, n_components, maxiter, alpha, gamma, negative_sample_rate, init_pos, random_state, a, b, copy, method, neighbors_key). 171 neigh_params.get('metric', 'euclidean'),. 172 neigh_params.get('metric_kwds', {}),. --> 173 verbose=settings.verbosity > 3,. 174 ). 175 elif method == 'rapids':. TypeError: simplicial_set_embedding() missing 3 required positional arguments: 'densmap', 'densmap_kwds', and 'output_dens'. ```. #### Versions. <details>. scanpy==1.6.0 anndata==0.7.5 umap==0.5.0 numpy==1.19.5 scipy==1.5.4 pandas==1.1.5 scikit-learn==0.24.0 statsmodels==0.12.1 python-igraph==0.8.3 leidenalg==0.8.3. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1579
https://github.com/scverse/scanpy/issues/1579:630,testability,integr,integrating-data-using-ingest,630,"Error in `sc.tl.umap`; - [ ] I have checked that this issue has not already been reported. - [ ] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. ### Minimal code sample (that we can copy&paste without having any data). I am learning the example of [Integrating data using ingest and BBKNN](https://scanpy-tutorials.readthedocs.io/en/latest/integrating-data-using-ingest.html). When I run code . ```python. sc.tl.umap(adata_ref). ```. I get. ```pytb. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-13-0e548e19df3a> in <module>. 1 sc.pp.pca(adata_ref). 2 sc.pp.neighbors(adata_ref). ----> 3 sc.tl.umap(adata_ref). ~/miniconda3/envs/tf/lib/python3.6/site-packages/scanpy/tools/_umap.py in umap(adata, min_dist, spread, n_components, maxiter, alpha, gamma, negative_sample_rate, init_pos, random_state, a, b, copy, method, neighbors_key). 171 neigh_params.get('metric', 'euclidean'),. 172 neigh_params.get('metric_kwds', {}),. --> 173 verbose=settings.verbosity > 3,. 174 ). 175 elif method == 'rapids':. TypeError: simplicial_set_embedding() missing 3 required positional arguments: 'densmap', 'densmap_kwds', and 'output_dens'. ```. #### Versions. <details>. scanpy==1.6.0 anndata==0.7.5 umap==0.5.0 numpy==1.19.5 scipy==1.5.4 pandas==1.1.5 scikit-learn==0.24.0 statsmodels==0.12.1 python-igraph==0.8.3 leidenalg==0.8.3. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1579
https://github.com/scverse/scanpy/issues/1579:827,testability,Trace,Traceback,827,"Error in `sc.tl.umap`; - [ ] I have checked that this issue has not already been reported. - [ ] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. ### Minimal code sample (that we can copy&paste without having any data). I am learning the example of [Integrating data using ingest and BBKNN](https://scanpy-tutorials.readthedocs.io/en/latest/integrating-data-using-ingest.html). When I run code . ```python. sc.tl.umap(adata_ref). ```. I get. ```pytb. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-13-0e548e19df3a> in <module>. 1 sc.pp.pca(adata_ref). 2 sc.pp.neighbors(adata_ref). ----> 3 sc.tl.umap(adata_ref). ~/miniconda3/envs/tf/lib/python3.6/site-packages/scanpy/tools/_umap.py in umap(adata, min_dist, spread, n_components, maxiter, alpha, gamma, negative_sample_rate, init_pos, random_state, a, b, copy, method, neighbors_key). 171 neigh_params.get('metric', 'euclidean'),. 172 neigh_params.get('metric_kwds', {}),. --> 173 verbose=settings.verbosity > 3,. 174 ). 175 elif method == 'rapids':. TypeError: simplicial_set_embedding() missing 3 required positional arguments: 'densmap', 'densmap_kwds', and 'output_dens'. ```. #### Versions. <details>. scanpy==1.6.0 anndata==0.7.5 umap==0.5.0 numpy==1.19.5 scipy==1.5.4 pandas==1.1.5 scikit-learn==0.24.0 statsmodels==0.12.1 python-igraph==0.8.3 leidenalg==0.8.3. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1579
https://github.com/scverse/scanpy/issues/1579:0,usability,Error,Error,0,"Error in `sc.tl.umap`; - [ ] I have checked that this issue has not already been reported. - [ ] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. ### Minimal code sample (that we can copy&paste without having any data). I am learning the example of [Integrating data using ingest and BBKNN](https://scanpy-tutorials.readthedocs.io/en/latest/integrating-data-using-ingest.html). When I run code . ```python. sc.tl.umap(adata_ref). ```. I get. ```pytb. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-13-0e548e19df3a> in <module>. 1 sc.pp.pca(adata_ref). 2 sc.pp.neighbors(adata_ref). ----> 3 sc.tl.umap(adata_ref). ~/miniconda3/envs/tf/lib/python3.6/site-packages/scanpy/tools/_umap.py in umap(adata, min_dist, spread, n_components, maxiter, alpha, gamma, negative_sample_rate, init_pos, random_state, a, b, copy, method, neighbors_key). 171 neigh_params.get('metric', 'euclidean'),. 172 neigh_params.get('metric_kwds', {}),. --> 173 verbose=settings.verbosity > 3,. 174 ). 175 elif method == 'rapids':. TypeError: simplicial_set_embedding() missing 3 required positional arguments: 'densmap', 'densmap_kwds', and 'output_dens'. ```. #### Versions. <details>. scanpy==1.6.0 anndata==0.7.5 umap==0.5.0 numpy==1.19.5 scipy==1.5.4 pandas==1.1.5 scikit-learn==0.24.0 statsmodels==0.12.1 python-igraph==0.8.3 leidenalg==0.8.3. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1579
https://github.com/scverse/scanpy/issues/1579:104,usability,confirm,confirmed,104,"Error in `sc.tl.umap`; - [ ] I have checked that this issue has not already been reported. - [ ] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. ### Minimal code sample (that we can copy&paste without having any data). I am learning the example of [Integrating data using ingest and BBKNN](https://scanpy-tutorials.readthedocs.io/en/latest/integrating-data-using-ingest.html). When I run code . ```python. sc.tl.umap(adata_ref). ```. I get. ```pytb. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-13-0e548e19df3a> in <module>. 1 sc.pp.pca(adata_ref). 2 sc.pp.neighbors(adata_ref). ----> 3 sc.tl.umap(adata_ref). ~/miniconda3/envs/tf/lib/python3.6/site-packages/scanpy/tools/_umap.py in umap(adata, min_dist, spread, n_components, maxiter, alpha, gamma, negative_sample_rate, init_pos, random_state, a, b, copy, method, neighbors_key). 171 neigh_params.get('metric', 'euclidean'),. 172 neigh_params.get('metric_kwds', {}),. --> 173 verbose=settings.verbosity > 3,. 174 ). 175 elif method == 'rapids':. TypeError: simplicial_set_embedding() missing 3 required positional arguments: 'densmap', 'densmap_kwds', and 'output_dens'. ```. #### Versions. <details>. scanpy==1.6.0 anndata==0.7.5 umap==0.5.0 numpy==1.19.5 scipy==1.5.4 pandas==1.1.5 scikit-learn==0.24.0 statsmodels==0.12.1 python-igraph==0.8.3 leidenalg==0.8.3. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1579
https://github.com/scverse/scanpy/issues/1579:187,usability,confirm,confirmed,187,"Error in `sc.tl.umap`; - [ ] I have checked that this issue has not already been reported. - [ ] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. ### Minimal code sample (that we can copy&paste without having any data). I am learning the example of [Integrating data using ingest and BBKNN](https://scanpy-tutorials.readthedocs.io/en/latest/integrating-data-using-ingest.html). When I run code . ```python. sc.tl.umap(adata_ref). ```. I get. ```pytb. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-13-0e548e19df3a> in <module>. 1 sc.pp.pca(adata_ref). 2 sc.pp.neighbors(adata_ref). ----> 3 sc.tl.umap(adata_ref). ~/miniconda3/envs/tf/lib/python3.6/site-packages/scanpy/tools/_umap.py in umap(adata, min_dist, spread, n_components, maxiter, alpha, gamma, negative_sample_rate, init_pos, random_state, a, b, copy, method, neighbors_key). 171 neigh_params.get('metric', 'euclidean'),. 172 neigh_params.get('metric_kwds', {}),. --> 173 verbose=settings.verbosity > 3,. 174 ). 175 elif method == 'rapids':. TypeError: simplicial_set_embedding() missing 3 required positional arguments: 'densmap', 'densmap_kwds', and 'output_dens'. ```. #### Versions. <details>. scanpy==1.6.0 anndata==0.7.5 umap==0.5.0 numpy==1.19.5 scipy==1.5.4 pandas==1.1.5 scikit-learn==0.24.0 statsmodels==0.12.1 python-igraph==0.8.3 leidenalg==0.8.3. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1579
https://github.com/scverse/scanpy/issues/1579:278,usability,guid,guide,278,"Error in `sc.tl.umap`; - [ ] I have checked that this issue has not already been reported. - [ ] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. ### Minimal code sample (that we can copy&paste without having any data). I am learning the example of [Integrating data using ingest and BBKNN](https://scanpy-tutorials.readthedocs.io/en/latest/integrating-data-using-ingest.html). When I run code . ```python. sc.tl.umap(adata_ref). ```. I get. ```pytb. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-13-0e548e19df3a> in <module>. 1 sc.pp.pca(adata_ref). 2 sc.pp.neighbors(adata_ref). ----> 3 sc.tl.umap(adata_ref). ~/miniconda3/envs/tf/lib/python3.6/site-packages/scanpy/tools/_umap.py in umap(adata, min_dist, spread, n_components, maxiter, alpha, gamma, negative_sample_rate, init_pos, random_state, a, b, copy, method, neighbors_key). 171 neigh_params.get('metric', 'euclidean'),. 172 neigh_params.get('metric_kwds', {}),. --> 173 verbose=settings.verbosity > 3,. 174 ). 175 elif method == 'rapids':. TypeError: simplicial_set_embedding() missing 3 required positional arguments: 'densmap', 'densmap_kwds', and 'output_dens'. ```. #### Versions. <details>. scanpy==1.6.0 anndata==0.7.5 umap==0.5.0 numpy==1.19.5 scipy==1.5.4 pandas==1.1.5 scikit-learn==0.24.0 statsmodels==0.12.1 python-igraph==0.8.3 leidenalg==0.8.3. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1579
https://github.com/scverse/scanpy/issues/1579:333,usability,minim,minimal-bug-reports,333,"Error in `sc.tl.umap`; - [ ] I have checked that this issue has not already been reported. - [ ] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. ### Minimal code sample (that we can copy&paste without having any data). I am learning the example of [Integrating data using ingest and BBKNN](https://scanpy-tutorials.readthedocs.io/en/latest/integrating-data-using-ingest.html). When I run code . ```python. sc.tl.umap(adata_ref). ```. I get. ```pytb. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-13-0e548e19df3a> in <module>. 1 sc.pp.pca(adata_ref). 2 sc.pp.neighbors(adata_ref). ----> 3 sc.tl.umap(adata_ref). ~/miniconda3/envs/tf/lib/python3.6/site-packages/scanpy/tools/_umap.py in umap(adata, min_dist, spread, n_components, maxiter, alpha, gamma, negative_sample_rate, init_pos, random_state, a, b, copy, method, neighbors_key). 171 neigh_params.get('metric', 'euclidean'),. 172 neigh_params.get('metric_kwds', {}),. --> 173 verbose=settings.verbosity > 3,. 174 ). 175 elif method == 'rapids':. TypeError: simplicial_set_embedding() missing 3 required positional arguments: 'densmap', 'densmap_kwds', and 'output_dens'. ```. #### Versions. <details>. scanpy==1.6.0 anndata==0.7.5 umap==0.5.0 numpy==1.19.5 scipy==1.5.4 pandas==1.1.5 scikit-learn==0.24.0 statsmodels==0.12.1 python-igraph==0.8.3 leidenalg==0.8.3. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1579
https://github.com/scverse/scanpy/issues/1579:439,usability,Minim,Minimal,439,"Error in `sc.tl.umap`; - [ ] I have checked that this issue has not already been reported. - [ ] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. ### Minimal code sample (that we can copy&paste without having any data). I am learning the example of [Integrating data using ingest and BBKNN](https://scanpy-tutorials.readthedocs.io/en/latest/integrating-data-using-ingest.html). When I run code . ```python. sc.tl.umap(adata_ref). ```. I get. ```pytb. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-13-0e548e19df3a> in <module>. 1 sc.pp.pca(adata_ref). 2 sc.pp.neighbors(adata_ref). ----> 3 sc.tl.umap(adata_ref). ~/miniconda3/envs/tf/lib/python3.6/site-packages/scanpy/tools/_umap.py in umap(adata, min_dist, spread, n_components, maxiter, alpha, gamma, negative_sample_rate, init_pos, random_state, a, b, copy, method, neighbors_key). 171 neigh_params.get('metric', 'euclidean'),. 172 neigh_params.get('metric_kwds', {}),. --> 173 verbose=settings.verbosity > 3,. 174 ). 175 elif method == 'rapids':. TypeError: simplicial_set_embedding() missing 3 required positional arguments: 'densmap', 'densmap_kwds', and 'output_dens'. ```. #### Versions. <details>. scanpy==1.6.0 anndata==0.7.5 umap==0.5.0 numpy==1.19.5 scipy==1.5.4 pandas==1.1.5 scikit-learn==0.24.0 statsmodels==0.12.1 python-igraph==0.8.3 leidenalg==0.8.3. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1579
https://github.com/scverse/scanpy/issues/1579:514,usability,learn,learning,514,"Error in `sc.tl.umap`; - [ ] I have checked that this issue has not already been reported. - [ ] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. ### Minimal code sample (that we can copy&paste without having any data). I am learning the example of [Integrating data using ingest and BBKNN](https://scanpy-tutorials.readthedocs.io/en/latest/integrating-data-using-ingest.html). When I run code . ```python. sc.tl.umap(adata_ref). ```. I get. ```pytb. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-13-0e548e19df3a> in <module>. 1 sc.pp.pca(adata_ref). 2 sc.pp.neighbors(adata_ref). ----> 3 sc.tl.umap(adata_ref). ~/miniconda3/envs/tf/lib/python3.6/site-packages/scanpy/tools/_umap.py in umap(adata, min_dist, spread, n_components, maxiter, alpha, gamma, negative_sample_rate, init_pos, random_state, a, b, copy, method, neighbors_key). 171 neigh_params.get('metric', 'euclidean'),. 172 neigh_params.get('metric_kwds', {}),. --> 173 verbose=settings.verbosity > 3,. 174 ). 175 elif method == 'rapids':. TypeError: simplicial_set_embedding() missing 3 required positional arguments: 'densmap', 'densmap_kwds', and 'output_dens'. ```. #### Versions. <details>. scanpy==1.6.0 anndata==0.7.5 umap==0.5.0 numpy==1.19.5 scipy==1.5.4 pandas==1.1.5 scikit-learn==0.24.0 statsmodels==0.12.1 python-igraph==0.8.3 leidenalg==0.8.3. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1579
https://github.com/scverse/scanpy/issues/1579:871,usability,input,input-,871,"Error in `sc.tl.umap`; - [ ] I have checked that this issue has not already been reported. - [ ] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. ### Minimal code sample (that we can copy&paste without having any data). I am learning the example of [Integrating data using ingest and BBKNN](https://scanpy-tutorials.readthedocs.io/en/latest/integrating-data-using-ingest.html). When I run code . ```python. sc.tl.umap(adata_ref). ```. I get. ```pytb. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-13-0e548e19df3a> in <module>. 1 sc.pp.pca(adata_ref). 2 sc.pp.neighbors(adata_ref). ----> 3 sc.tl.umap(adata_ref). ~/miniconda3/envs/tf/lib/python3.6/site-packages/scanpy/tools/_umap.py in umap(adata, min_dist, spread, n_components, maxiter, alpha, gamma, negative_sample_rate, init_pos, random_state, a, b, copy, method, neighbors_key). 171 neigh_params.get('metric', 'euclidean'),. 172 neigh_params.get('metric_kwds', {}),. --> 173 verbose=settings.verbosity > 3,. 174 ). 175 elif method == 'rapids':. TypeError: simplicial_set_embedding() missing 3 required positional arguments: 'densmap', 'densmap_kwds', and 'output_dens'. ```. #### Versions. <details>. scanpy==1.6.0 anndata==0.7.5 umap==0.5.0 numpy==1.19.5 scipy==1.5.4 pandas==1.1.5 scikit-learn==0.24.0 statsmodels==0.12.1 python-igraph==0.8.3 leidenalg==0.8.3. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1579
https://github.com/scverse/scanpy/issues/1579:1048,usability,tool,tools,1048,"Error in `sc.tl.umap`; - [ ] I have checked that this issue has not already been reported. - [ ] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. ### Minimal code sample (that we can copy&paste without having any data). I am learning the example of [Integrating data using ingest and BBKNN](https://scanpy-tutorials.readthedocs.io/en/latest/integrating-data-using-ingest.html). When I run code . ```python. sc.tl.umap(adata_ref). ```. I get. ```pytb. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-13-0e548e19df3a> in <module>. 1 sc.pp.pca(adata_ref). 2 sc.pp.neighbors(adata_ref). ----> 3 sc.tl.umap(adata_ref). ~/miniconda3/envs/tf/lib/python3.6/site-packages/scanpy/tools/_umap.py in umap(adata, min_dist, spread, n_components, maxiter, alpha, gamma, negative_sample_rate, init_pos, random_state, a, b, copy, method, neighbors_key). 171 neigh_params.get('metric', 'euclidean'),. 172 neigh_params.get('metric_kwds', {}),. --> 173 verbose=settings.verbosity > 3,. 174 ). 175 elif method == 'rapids':. TypeError: simplicial_set_embedding() missing 3 required positional arguments: 'densmap', 'densmap_kwds', and 'output_dens'. ```. #### Versions. <details>. scanpy==1.6.0 anndata==0.7.5 umap==0.5.0 numpy==1.19.5 scipy==1.5.4 pandas==1.1.5 scikit-learn==0.24.0 statsmodels==0.12.1 python-igraph==0.8.3 leidenalg==0.8.3. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1579
https://github.com/scverse/scanpy/issues/1579:1626,usability,learn,learn,1626,"Error in `sc.tl.umap`; - [ ] I have checked that this issue has not already been reported. - [ ] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. ### Minimal code sample (that we can copy&paste without having any data). I am learning the example of [Integrating data using ingest and BBKNN](https://scanpy-tutorials.readthedocs.io/en/latest/integrating-data-using-ingest.html). When I run code . ```python. sc.tl.umap(adata_ref). ```. I get. ```pytb. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-13-0e548e19df3a> in <module>. 1 sc.pp.pca(adata_ref). 2 sc.pp.neighbors(adata_ref). ----> 3 sc.tl.umap(adata_ref). ~/miniconda3/envs/tf/lib/python3.6/site-packages/scanpy/tools/_umap.py in umap(adata, min_dist, spread, n_components, maxiter, alpha, gamma, negative_sample_rate, init_pos, random_state, a, b, copy, method, neighbors_key). 171 neigh_params.get('metric', 'euclidean'),. 172 neigh_params.get('metric_kwds', {}),. --> 173 verbose=settings.verbosity > 3,. 174 ). 175 elif method == 'rapids':. TypeError: simplicial_set_embedding() missing 3 required positional arguments: 'densmap', 'densmap_kwds', and 'output_dens'. ```. #### Versions. <details>. scanpy==1.6.0 anndata==0.7.5 umap==0.5.0 numpy==1.19.5 scipy==1.5.4 pandas==1.1.5 scikit-learn==0.24.0 statsmodels==0.12.1 python-igraph==0.8.3 leidenalg==0.8.3. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1579
https://github.com/scverse/scanpy/pull/1580:129,deployability,updat,update,129,"Fix empty img isaac; Some changes to @giovp #1512, just pushing here so they are visible. Still needs going through the tests to update offsets, and some doc tweaks (behaviour of `na_color`, what `spot_size` is, move `scale_factor` to be `spatial` only).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1580
https://github.com/scverse/scanpy/pull/1580:120,safety,test,tests,120,"Fix empty img isaac; Some changes to @giovp #1512, just pushing here so they are visible. Still needs going through the tests to update offsets, and some doc tweaks (behaviour of `na_color`, what `spot_size` is, move `scale_factor` to be `spatial` only).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1580
https://github.com/scverse/scanpy/pull/1580:129,safety,updat,update,129,"Fix empty img isaac; Some changes to @giovp #1512, just pushing here so they are visible. Still needs going through the tests to update offsets, and some doc tweaks (behaviour of `na_color`, what `spot_size` is, move `scale_factor` to be `spatial` only).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1580
https://github.com/scverse/scanpy/pull/1580:129,security,updat,update,129,"Fix empty img isaac; Some changes to @giovp #1512, just pushing here so they are visible. Still needs going through the tests to update offsets, and some doc tweaks (behaviour of `na_color`, what `spot_size` is, move `scale_factor` to be `spatial` only).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1580
https://github.com/scverse/scanpy/pull/1580:120,testability,test,tests,120,"Fix empty img isaac; Some changes to @giovp #1512, just pushing here so they are visible. Still needs going through the tests to update offsets, and some doc tweaks (behaviour of `na_color`, what `spot_size` is, move `scale_factor` to be `spatial` only).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1580
https://github.com/scverse/scanpy/pull/1580:166,usability,behavi,behaviour,166,"Fix empty img isaac; Some changes to @giovp #1512, just pushing here so they are visible. Still needs going through the tests to update offsets, and some doc tweaks (behaviour of `na_color`, what `spot_size` is, move `scale_factor` to be `spatial` only).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1580
https://github.com/scverse/scanpy/issues/1581:128,usability,user,user-images,128,"Examples sections headings are wrong; From the `rank_genes_groups_matrixplot` docs:. <img width=""1117"" alt=""image"" src=""https://user-images.githubusercontent.com/8238804/104416321-b9439900-55c7-11eb-8690-f93312bef864.png"">. The `Examples` heading is just showing up as text, should be styled.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1581
https://github.com/scverse/scanpy/pull/1582:102,deployability,build,build,102,"Prep 1.7.0rc1; Prepping 1.7.0rc1, might not get made tonight unless numpy's docs go back up so we can build ours  . https://github.com/numpy/numpy.org/issues/383",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1582
https://github.com/scverse/scanpy/pull/1583:1560,availability,operat,operations,1560,"Allow plots to use adata.obs index as groupby; I simplified the `_prepare_dataframe code` by using `sc.get.df`. However, this change uncovered two issues with `sc.get.obs_df` that I have now addressed in this PR. . The most relevant is the case when the call to `sc.get.obs_df` contains keys with duplicates (e.g. `keys=['gene1', 'gene1']`). This case is not rare as for example in `sc.pl.dotplot` the same gene can be visualized several times, which requires calling `sc.get.obs_df` with keys that contain duplications. An example is when `sc.pl.rank_genes_groups_dotplot` is called and, frequently, the same gene appears as top up-regulated for more than one category. To address this, `sc.get.obs_df` removes all duplicates (which correspond to DataFrame columns) and after the DataFrame is complete, the duplicates are added back. A second problem was for non-unique adata.obs indices which should be a rare situation. However, it turns out that one of the test adata object used in `test_plotting` have this issue. Also, for the goal of this PR (allow adata.obs.index as groupby option) it could be expected that the index may not be unique. . In general, non unique obs indices are ok as long as `.obs` DataFrame is not joined or merge based on index. However, because internally in `sc.get.obs_df` the DataFrames are merged using `adata.obs.index` this non-unique indices caused an increase in rows due to multiple matching. To fix this, the code now checks for unique index, and if it is not unique then a temporary index is added to allow proper join operations and then the non-unique index is put back.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1583
https://github.com/scverse/scanpy/pull/1583:278,deployability,contain,contains,278,"Allow plots to use adata.obs index as groupby; I simplified the `_prepare_dataframe code` by using `sc.get.df`. However, this change uncovered two issues with `sc.get.obs_df` that I have now addressed in this PR. . The most relevant is the case when the call to `sc.get.obs_df` contains keys with duplicates (e.g. `keys=['gene1', 'gene1']`). This case is not rare as for example in `sc.pl.dotplot` the same gene can be visualized several times, which requires calling `sc.get.obs_df` with keys that contain duplications. An example is when `sc.pl.rank_genes_groups_dotplot` is called and, frequently, the same gene appears as top up-regulated for more than one category. To address this, `sc.get.obs_df` removes all duplicates (which correspond to DataFrame columns) and after the DataFrame is complete, the duplicates are added back. A second problem was for non-unique adata.obs indices which should be a rare situation. However, it turns out that one of the test adata object used in `test_plotting` have this issue. Also, for the goal of this PR (allow adata.obs.index as groupby option) it could be expected that the index may not be unique. . In general, non unique obs indices are ok as long as `.obs` DataFrame is not joined or merge based on index. However, because internally in `sc.get.obs_df` the DataFrames are merged using `adata.obs.index` this non-unique indices caused an increase in rows due to multiple matching. To fix this, the code now checks for unique index, and if it is not unique then a temporary index is added to allow proper join operations and then the non-unique index is put back.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1583
https://github.com/scverse/scanpy/pull/1583:499,deployability,contain,contain,499,"Allow plots to use adata.obs index as groupby; I simplified the `_prepare_dataframe code` by using `sc.get.df`. However, this change uncovered two issues with `sc.get.obs_df` that I have now addressed in this PR. . The most relevant is the case when the call to `sc.get.obs_df` contains keys with duplicates (e.g. `keys=['gene1', 'gene1']`). This case is not rare as for example in `sc.pl.dotplot` the same gene can be visualized several times, which requires calling `sc.get.obs_df` with keys that contain duplications. An example is when `sc.pl.rank_genes_groups_dotplot` is called and, frequently, the same gene appears as top up-regulated for more than one category. To address this, `sc.get.obs_df` removes all duplicates (which correspond to DataFrame columns) and after the DataFrame is complete, the duplicates are added back. A second problem was for non-unique adata.obs indices which should be a rare situation. However, it turns out that one of the test adata object used in `test_plotting` have this issue. Also, for the goal of this PR (allow adata.obs.index as groupby option) it could be expected that the index may not be unique. . In general, non unique obs indices are ok as long as `.obs` DataFrame is not joined or merge based on index. However, because internally in `sc.get.obs_df` the DataFrames are merged using `adata.obs.index` this non-unique indices caused an increase in rows due to multiple matching. To fix this, the code now checks for unique index, and if it is not unique then a temporary index is added to allow proper join operations and then the non-unique index is put back.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1583
https://github.com/scverse/scanpy/pull/1583:438,performance,time,times,438,"Allow plots to use adata.obs index as groupby; I simplified the `_prepare_dataframe code` by using `sc.get.df`. However, this change uncovered two issues with `sc.get.obs_df` that I have now addressed in this PR. . The most relevant is the case when the call to `sc.get.obs_df` contains keys with duplicates (e.g. `keys=['gene1', 'gene1']`). This case is not rare as for example in `sc.pl.dotplot` the same gene can be visualized several times, which requires calling `sc.get.obs_df` with keys that contain duplications. An example is when `sc.pl.rank_genes_groups_dotplot` is called and, frequently, the same gene appears as top up-regulated for more than one category. To address this, `sc.get.obs_df` removes all duplicates (which correspond to DataFrame columns) and after the DataFrame is complete, the duplicates are added back. A second problem was for non-unique adata.obs indices which should be a rare situation. However, it turns out that one of the test adata object used in `test_plotting` have this issue. Also, for the goal of this PR (allow adata.obs.index as groupby option) it could be expected that the index may not be unique. . In general, non unique obs indices are ok as long as `.obs` DataFrame is not joined or merge based on index. However, because internally in `sc.get.obs_df` the DataFrames are merged using `adata.obs.index` this non-unique indices caused an increase in rows due to multiple matching. To fix this, the code now checks for unique index, and if it is not unique then a temporary index is added to allow proper join operations and then the non-unique index is put back.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1583
https://github.com/scverse/scanpy/pull/1583:794,safety,compl,complete,794,"Allow plots to use adata.obs index as groupby; I simplified the `_prepare_dataframe code` by using `sc.get.df`. However, this change uncovered two issues with `sc.get.obs_df` that I have now addressed in this PR. . The most relevant is the case when the call to `sc.get.obs_df` contains keys with duplicates (e.g. `keys=['gene1', 'gene1']`). This case is not rare as for example in `sc.pl.dotplot` the same gene can be visualized several times, which requires calling `sc.get.obs_df` with keys that contain duplications. An example is when `sc.pl.rank_genes_groups_dotplot` is called and, frequently, the same gene appears as top up-regulated for more than one category. To address this, `sc.get.obs_df` removes all duplicates (which correspond to DataFrame columns) and after the DataFrame is complete, the duplicates are added back. A second problem was for non-unique adata.obs indices which should be a rare situation. However, it turns out that one of the test adata object used in `test_plotting` have this issue. Also, for the goal of this PR (allow adata.obs.index as groupby option) it could be expected that the index may not be unique. . In general, non unique obs indices are ok as long as `.obs` DataFrame is not joined or merge based on index. However, because internally in `sc.get.obs_df` the DataFrames are merged using `adata.obs.index` this non-unique indices caused an increase in rows due to multiple matching. To fix this, the code now checks for unique index, and if it is not unique then a temporary index is added to allow proper join operations and then the non-unique index is put back.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1583
https://github.com/scverse/scanpy/pull/1583:961,safety,test,test,961,"Allow plots to use adata.obs index as groupby; I simplified the `_prepare_dataframe code` by using `sc.get.df`. However, this change uncovered two issues with `sc.get.obs_df` that I have now addressed in this PR. . The most relevant is the case when the call to `sc.get.obs_df` contains keys with duplicates (e.g. `keys=['gene1', 'gene1']`). This case is not rare as for example in `sc.pl.dotplot` the same gene can be visualized several times, which requires calling `sc.get.obs_df` with keys that contain duplications. An example is when `sc.pl.rank_genes_groups_dotplot` is called and, frequently, the same gene appears as top up-regulated for more than one category. To address this, `sc.get.obs_df` removes all duplicates (which correspond to DataFrame columns) and after the DataFrame is complete, the duplicates are added back. A second problem was for non-unique adata.obs indices which should be a rare situation. However, it turns out that one of the test adata object used in `test_plotting` have this issue. Also, for the goal of this PR (allow adata.obs.index as groupby option) it could be expected that the index may not be unique. . In general, non unique obs indices are ok as long as `.obs` DataFrame is not joined or merge based on index. However, because internally in `sc.get.obs_df` the DataFrames are merged using `adata.obs.index` this non-unique indices caused an increase in rows due to multiple matching. To fix this, the code now checks for unique index, and if it is not unique then a temporary index is added to allow proper join operations and then the non-unique index is put back.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1583
https://github.com/scverse/scanpy/pull/1583:794,security,compl,complete,794,"Allow plots to use adata.obs index as groupby; I simplified the `_prepare_dataframe code` by using `sc.get.df`. However, this change uncovered two issues with `sc.get.obs_df` that I have now addressed in this PR. . The most relevant is the case when the call to `sc.get.obs_df` contains keys with duplicates (e.g. `keys=['gene1', 'gene1']`). This case is not rare as for example in `sc.pl.dotplot` the same gene can be visualized several times, which requires calling `sc.get.obs_df` with keys that contain duplications. An example is when `sc.pl.rank_genes_groups_dotplot` is called and, frequently, the same gene appears as top up-regulated for more than one category. To address this, `sc.get.obs_df` removes all duplicates (which correspond to DataFrame columns) and after the DataFrame is complete, the duplicates are added back. A second problem was for non-unique adata.obs indices which should be a rare situation. However, it turns out that one of the test adata object used in `test_plotting` have this issue. Also, for the goal of this PR (allow adata.obs.index as groupby option) it could be expected that the index may not be unique. . In general, non unique obs indices are ok as long as `.obs` DataFrame is not joined or merge based on index. However, because internally in `sc.get.obs_df` the DataFrames are merged using `adata.obs.index` this non-unique indices caused an increase in rows due to multiple matching. To fix this, the code now checks for unique index, and if it is not unique then a temporary index is added to allow proper join operations and then the non-unique index is put back.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1583
https://github.com/scverse/scanpy/pull/1583:49,testability,simpl,simplified,49,"Allow plots to use adata.obs index as groupby; I simplified the `_prepare_dataframe code` by using `sc.get.df`. However, this change uncovered two issues with `sc.get.obs_df` that I have now addressed in this PR. . The most relevant is the case when the call to `sc.get.obs_df` contains keys with duplicates (e.g. `keys=['gene1', 'gene1']`). This case is not rare as for example in `sc.pl.dotplot` the same gene can be visualized several times, which requires calling `sc.get.obs_df` with keys that contain duplications. An example is when `sc.pl.rank_genes_groups_dotplot` is called and, frequently, the same gene appears as top up-regulated for more than one category. To address this, `sc.get.obs_df` removes all duplicates (which correspond to DataFrame columns) and after the DataFrame is complete, the duplicates are added back. A second problem was for non-unique adata.obs indices which should be a rare situation. However, it turns out that one of the test adata object used in `test_plotting` have this issue. Also, for the goal of this PR (allow adata.obs.index as groupby option) it could be expected that the index may not be unique. . In general, non unique obs indices are ok as long as `.obs` DataFrame is not joined or merge based on index. However, because internally in `sc.get.obs_df` the DataFrames are merged using `adata.obs.index` this non-unique indices caused an increase in rows due to multiple matching. To fix this, the code now checks for unique index, and if it is not unique then a temporary index is added to allow proper join operations and then the non-unique index is put back.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1583
https://github.com/scverse/scanpy/pull/1583:961,testability,test,test,961,"Allow plots to use adata.obs index as groupby; I simplified the `_prepare_dataframe code` by using `sc.get.df`. However, this change uncovered two issues with `sc.get.obs_df` that I have now addressed in this PR. . The most relevant is the case when the call to `sc.get.obs_df` contains keys with duplicates (e.g. `keys=['gene1', 'gene1']`). This case is not rare as for example in `sc.pl.dotplot` the same gene can be visualized several times, which requires calling `sc.get.obs_df` with keys that contain duplications. An example is when `sc.pl.rank_genes_groups_dotplot` is called and, frequently, the same gene appears as top up-regulated for more than one category. To address this, `sc.get.obs_df` removes all duplicates (which correspond to DataFrame columns) and after the DataFrame is complete, the duplicates are added back. A second problem was for non-unique adata.obs indices which should be a rare situation. However, it turns out that one of the test adata object used in `test_plotting` have this issue. Also, for the goal of this PR (allow adata.obs.index as groupby option) it could be expected that the index may not be unique. . In general, non unique obs indices are ok as long as `.obs` DataFrame is not joined or merge based on index. However, because internally in `sc.get.obs_df` the DataFrames are merged using `adata.obs.index` this non-unique indices caused an increase in rows due to multiple matching. To fix this, the code now checks for unique index, and if it is not unique then a temporary index is added to allow proper join operations and then the non-unique index is put back.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1583
https://github.com/scverse/scanpy/pull/1583:49,usability,simpl,simplified,49,"Allow plots to use adata.obs index as groupby; I simplified the `_prepare_dataframe code` by using `sc.get.df`. However, this change uncovered two issues with `sc.get.obs_df` that I have now addressed in this PR. . The most relevant is the case when the call to `sc.get.obs_df` contains keys with duplicates (e.g. `keys=['gene1', 'gene1']`). This case is not rare as for example in `sc.pl.dotplot` the same gene can be visualized several times, which requires calling `sc.get.obs_df` with keys that contain duplications. An example is when `sc.pl.rank_genes_groups_dotplot` is called and, frequently, the same gene appears as top up-regulated for more than one category. To address this, `sc.get.obs_df` removes all duplicates (which correspond to DataFrame columns) and after the DataFrame is complete, the duplicates are added back. A second problem was for non-unique adata.obs indices which should be a rare situation. However, it turns out that one of the test adata object used in `test_plotting` have this issue. Also, for the goal of this PR (allow adata.obs.index as groupby option) it could be expected that the index may not be unique. . In general, non unique obs indices are ok as long as `.obs` DataFrame is not joined or merge based on index. However, because internally in `sc.get.obs_df` the DataFrames are merged using `adata.obs.index` this non-unique indices caused an increase in rows due to multiple matching. To fix this, the code now checks for unique index, and if it is not unique then a temporary index is added to allow proper join operations and then the non-unique index is put back.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1583
https://github.com/scverse/scanpy/pull/1583:419,usability,visual,visualized,419,"Allow plots to use adata.obs index as groupby; I simplified the `_prepare_dataframe code` by using `sc.get.df`. However, this change uncovered two issues with `sc.get.obs_df` that I have now addressed in this PR. . The most relevant is the case when the call to `sc.get.obs_df` contains keys with duplicates (e.g. `keys=['gene1', 'gene1']`). This case is not rare as for example in `sc.pl.dotplot` the same gene can be visualized several times, which requires calling `sc.get.obs_df` with keys that contain duplications. An example is when `sc.pl.rank_genes_groups_dotplot` is called and, frequently, the same gene appears as top up-regulated for more than one category. To address this, `sc.get.obs_df` removes all duplicates (which correspond to DataFrame columns) and after the DataFrame is complete, the duplicates are added back. A second problem was for non-unique adata.obs indices which should be a rare situation. However, it turns out that one of the test adata object used in `test_plotting` have this issue. Also, for the goal of this PR (allow adata.obs.index as groupby option) it could be expected that the index may not be unique. . In general, non unique obs indices are ok as long as `.obs` DataFrame is not joined or merge based on index. However, because internally in `sc.get.obs_df` the DataFrames are merged using `adata.obs.index` this non-unique indices caused an increase in rows due to multiple matching. To fix this, the code now checks for unique index, and if it is not unique then a temporary index is added to allow proper join operations and then the non-unique index is put back.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1583
https://github.com/scverse/scanpy/pull/1584:0,deployability,updat,updated,0,updated test that works with pandas 1.2 #1562; This PR addresses #1562. A test requiring sorting was updated such that it works the same with pandas<1.2.0 or pandas>=1.2.0. The issue was caused by the test having identical values to sort.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1584
https://github.com/scverse/scanpy/pull/1584:101,deployability,updat,updated,101,updated test that works with pandas 1.2 #1562; This PR addresses #1562. A test requiring sorting was updated such that it works the same with pandas<1.2.0 or pandas>=1.2.0. The issue was caused by the test having identical values to sort.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1584
https://github.com/scverse/scanpy/pull/1584:0,safety,updat,updated,0,updated test that works with pandas 1.2 #1562; This PR addresses #1562. A test requiring sorting was updated such that it works the same with pandas<1.2.0 or pandas>=1.2.0. The issue was caused by the test having identical values to sort.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1584
https://github.com/scverse/scanpy/pull/1584:8,safety,test,test,8,updated test that works with pandas 1.2 #1562; This PR addresses #1562. A test requiring sorting was updated such that it works the same with pandas<1.2.0 or pandas>=1.2.0. The issue was caused by the test having identical values to sort.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1584
https://github.com/scverse/scanpy/pull/1584:74,safety,test,test,74,updated test that works with pandas 1.2 #1562; This PR addresses #1562. A test requiring sorting was updated such that it works the same with pandas<1.2.0 or pandas>=1.2.0. The issue was caused by the test having identical values to sort.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1584
https://github.com/scverse/scanpy/pull/1584:101,safety,updat,updated,101,updated test that works with pandas 1.2 #1562; This PR addresses #1562. A test requiring sorting was updated such that it works the same with pandas<1.2.0 or pandas>=1.2.0. The issue was caused by the test having identical values to sort.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1584
https://github.com/scverse/scanpy/pull/1584:201,safety,test,test,201,updated test that works with pandas 1.2 #1562; This PR addresses #1562. A test requiring sorting was updated such that it works the same with pandas<1.2.0 or pandas>=1.2.0. The issue was caused by the test having identical values to sort.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1584
https://github.com/scverse/scanpy/pull/1584:0,security,updat,updated,0,updated test that works with pandas 1.2 #1562; This PR addresses #1562. A test requiring sorting was updated such that it works the same with pandas<1.2.0 or pandas>=1.2.0. The issue was caused by the test having identical values to sort.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1584
https://github.com/scverse/scanpy/pull/1584:101,security,updat,updated,101,updated test that works with pandas 1.2 #1562; This PR addresses #1562. A test requiring sorting was updated such that it works the same with pandas<1.2.0 or pandas>=1.2.0. The issue was caused by the test having identical values to sort.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1584
https://github.com/scverse/scanpy/pull/1584:213,security,ident,identical,213,updated test that works with pandas 1.2 #1562; This PR addresses #1562. A test requiring sorting was updated such that it works the same with pandas<1.2.0 or pandas>=1.2.0. The issue was caused by the test having identical values to sort.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1584
https://github.com/scverse/scanpy/pull/1584:8,testability,test,test,8,updated test that works with pandas 1.2 #1562; This PR addresses #1562. A test requiring sorting was updated such that it works the same with pandas<1.2.0 or pandas>=1.2.0. The issue was caused by the test having identical values to sort.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1584
https://github.com/scverse/scanpy/pull/1584:74,testability,test,test,74,updated test that works with pandas 1.2 #1562; This PR addresses #1562. A test requiring sorting was updated such that it works the same with pandas<1.2.0 or pandas>=1.2.0. The issue was caused by the test having identical values to sort.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1584
https://github.com/scverse/scanpy/pull/1584:201,testability,test,test,201,updated test that works with pandas 1.2 #1562; This PR addresses #1562. A test requiring sorting was updated such that it works the same with pandas<1.2.0 or pandas>=1.2.0. The issue was caused by the test having identical values to sort.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1584
https://github.com/scverse/scanpy/issues/1585:263,availability,error,error,263,"Add build and `twine check` CI task; `twine check` is not great at telling you why it's failing. It would be easier to figure out what caused the break if we were continuously checking for this. Inspired by finding out that `authors` can't have new lines, via an error that says `long_description` can't have section headings (which definitely isn't true).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1585
https://github.com/scverse/scanpy/issues/1585:4,deployability,build,build,4,"Add build and `twine check` CI task; `twine check` is not great at telling you why it's failing. It would be easier to figure out what caused the break if we were continuously checking for this. Inspired by finding out that `authors` can't have new lines, via an error that says `long_description` can't have section headings (which definitely isn't true).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1585
https://github.com/scverse/scanpy/issues/1585:88,deployability,fail,failing,88,"Add build and `twine check` CI task; `twine check` is not great at telling you why it's failing. It would be easier to figure out what caused the break if we were continuously checking for this. Inspired by finding out that `authors` can't have new lines, via an error that says `long_description` can't have section headings (which definitely isn't true).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1585
https://github.com/scverse/scanpy/issues/1585:163,deployability,continu,continuously,163,"Add build and `twine check` CI task; `twine check` is not great at telling you why it's failing. It would be easier to figure out what caused the break if we were continuously checking for this. Inspired by finding out that `authors` can't have new lines, via an error that says `long_description` can't have section headings (which definitely isn't true).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1585
https://github.com/scverse/scanpy/issues/1585:263,performance,error,error,263,"Add build and `twine check` CI task; `twine check` is not great at telling you why it's failing. It would be easier to figure out what caused the break if we were continuously checking for this. Inspired by finding out that `authors` can't have new lines, via an error that says `long_description` can't have section headings (which definitely isn't true).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1585
https://github.com/scverse/scanpy/issues/1585:88,reliability,fail,failing,88,"Add build and `twine check` CI task; `twine check` is not great at telling you why it's failing. It would be easier to figure out what caused the break if we were continuously checking for this. Inspired by finding out that `authors` can't have new lines, via an error that says `long_description` can't have section headings (which definitely isn't true).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1585
https://github.com/scverse/scanpy/issues/1585:263,safety,error,error,263,"Add build and `twine check` CI task; `twine check` is not great at telling you why it's failing. It would be easier to figure out what caused the break if we were continuously checking for this. Inspired by finding out that `authors` can't have new lines, via an error that says `long_description` can't have section headings (which definitely isn't true).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1585
https://github.com/scverse/scanpy/issues/1585:225,security,auth,authors,225,"Add build and `twine check` CI task; `twine check` is not great at telling you why it's failing. It would be easier to figure out what caused the break if we were continuously checking for this. Inspired by finding out that `authors` can't have new lines, via an error that says `long_description` can't have section headings (which definitely isn't true).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1585
https://github.com/scverse/scanpy/issues/1585:263,usability,error,error,263,"Add build and `twine check` CI task; `twine check` is not great at telling you why it's failing. It would be easier to figure out what caused the break if we were continuously checking for this. Inspired by finding out that `authors` can't have new lines, via an error that says `long_description` can't have section headings (which definitely isn't true).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1585
https://github.com/scverse/scanpy/pull/1586:5,deployability,releas,release,5,"Prep release again; Ran into issues making release (PyPi didn't like the upload), trying again",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1586
https://github.com/scverse/scanpy/pull/1586:43,deployability,releas,release,43,"Prep release again; Ran into issues making release (PyPi didn't like the upload), trying again",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1586
https://github.com/scverse/scanpy/pull/1587:7,deployability,fail,failing,7,"Attach failing plots to CI results; If plots fail on CI, this should let us see the expected, actual, and diff through azure. ~~Hopefully~~ It works! I think this could make debugging plotting issues much easier. Here's an example of what the results look like:. <img width=""1090"" alt=""image"" src=""https://user-images.githubusercontent.com/8238804/104561575-b5814680-569b-11eb-8d1e-a9971affc645.png"">. Current issue, if tests are run in parallel, this does not work (https://github.com/pytest-dev/pytest-nunit/issues/40), which is not an immediate problem for CI, but limits applicability for local usage. I believe this is an issue with this particular pytest plugin, not necessarily this strategy.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1587
https://github.com/scverse/scanpy/pull/1587:45,deployability,fail,fail,45,"Attach failing plots to CI results; If plots fail on CI, this should let us see the expected, actual, and diff through azure. ~~Hopefully~~ It works! I think this could make debugging plotting issues much easier. Here's an example of what the results look like:. <img width=""1090"" alt=""image"" src=""https://user-images.githubusercontent.com/8238804/104561575-b5814680-569b-11eb-8d1e-a9971affc645.png"">. Current issue, if tests are run in parallel, this does not work (https://github.com/pytest-dev/pytest-nunit/issues/40), which is not an immediate problem for CI, but limits applicability for local usage. I believe this is an issue with this particular pytest plugin, not necessarily this strategy.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1587
https://github.com/scverse/scanpy/pull/1587:402,energy efficiency,Current,Current,402,"Attach failing plots to CI results; If plots fail on CI, this should let us see the expected, actual, and diff through azure. ~~Hopefully~~ It works! I think this could make debugging plotting issues much easier. Here's an example of what the results look like:. <img width=""1090"" alt=""image"" src=""https://user-images.githubusercontent.com/8238804/104561575-b5814680-569b-11eb-8d1e-a9971affc645.png"">. Current issue, if tests are run in parallel, this does not work (https://github.com/pytest-dev/pytest-nunit/issues/40), which is not an immediate problem for CI, but limits applicability for local usage. I believe this is an issue with this particular pytest plugin, not necessarily this strategy.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1587
https://github.com/scverse/scanpy/pull/1587:661,interoperability,plug,plugin,661,"Attach failing plots to CI results; If plots fail on CI, this should let us see the expected, actual, and diff through azure. ~~Hopefully~~ It works! I think this could make debugging plotting issues much easier. Here's an example of what the results look like:. <img width=""1090"" alt=""image"" src=""https://user-images.githubusercontent.com/8238804/104561575-b5814680-569b-11eb-8d1e-a9971affc645.png"">. Current issue, if tests are run in parallel, this does not work (https://github.com/pytest-dev/pytest-nunit/issues/40), which is not an immediate problem for CI, but limits applicability for local usage. I believe this is an issue with this particular pytest plugin, not necessarily this strategy.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1587
https://github.com/scverse/scanpy/pull/1587:437,performance,parallel,parallel,437,"Attach failing plots to CI results; If plots fail on CI, this should let us see the expected, actual, and diff through azure. ~~Hopefully~~ It works! I think this could make debugging plotting issues much easier. Here's an example of what the results look like:. <img width=""1090"" alt=""image"" src=""https://user-images.githubusercontent.com/8238804/104561575-b5814680-569b-11eb-8d1e-a9971affc645.png"">. Current issue, if tests are run in parallel, this does not work (https://github.com/pytest-dev/pytest-nunit/issues/40), which is not an immediate problem for CI, but limits applicability for local usage. I believe this is an issue with this particular pytest plugin, not necessarily this strategy.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1587
https://github.com/scverse/scanpy/pull/1587:7,reliability,fail,failing,7,"Attach failing plots to CI results; If plots fail on CI, this should let us see the expected, actual, and diff through azure. ~~Hopefully~~ It works! I think this could make debugging plotting issues much easier. Here's an example of what the results look like:. <img width=""1090"" alt=""image"" src=""https://user-images.githubusercontent.com/8238804/104561575-b5814680-569b-11eb-8d1e-a9971affc645.png"">. Current issue, if tests are run in parallel, this does not work (https://github.com/pytest-dev/pytest-nunit/issues/40), which is not an immediate problem for CI, but limits applicability for local usage. I believe this is an issue with this particular pytest plugin, not necessarily this strategy.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1587
https://github.com/scverse/scanpy/pull/1587:45,reliability,fail,fail,45,"Attach failing plots to CI results; If plots fail on CI, this should let us see the expected, actual, and diff through azure. ~~Hopefully~~ It works! I think this could make debugging plotting issues much easier. Here's an example of what the results look like:. <img width=""1090"" alt=""image"" src=""https://user-images.githubusercontent.com/8238804/104561575-b5814680-569b-11eb-8d1e-a9971affc645.png"">. Current issue, if tests are run in parallel, this does not work (https://github.com/pytest-dev/pytest-nunit/issues/40), which is not an immediate problem for CI, but limits applicability for local usage. I believe this is an issue with this particular pytest plugin, not necessarily this strategy.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1587
https://github.com/scverse/scanpy/pull/1587:452,reliability,doe,does,452,"Attach failing plots to CI results; If plots fail on CI, this should let us see the expected, actual, and diff through azure. ~~Hopefully~~ It works! I think this could make debugging plotting issues much easier. Here's an example of what the results look like:. <img width=""1090"" alt=""image"" src=""https://user-images.githubusercontent.com/8238804/104561575-b5814680-569b-11eb-8d1e-a9971affc645.png"">. Current issue, if tests are run in parallel, this does not work (https://github.com/pytest-dev/pytest-nunit/issues/40), which is not an immediate problem for CI, but limits applicability for local usage. I believe this is an issue with this particular pytest plugin, not necessarily this strategy.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1587
https://github.com/scverse/scanpy/pull/1587:420,safety,test,tests,420,"Attach failing plots to CI results; If plots fail on CI, this should let us see the expected, actual, and diff through azure. ~~Hopefully~~ It works! I think this could make debugging plotting issues much easier. Here's an example of what the results look like:. <img width=""1090"" alt=""image"" src=""https://user-images.githubusercontent.com/8238804/104561575-b5814680-569b-11eb-8d1e-a9971affc645.png"">. Current issue, if tests are run in parallel, this does not work (https://github.com/pytest-dev/pytest-nunit/issues/40), which is not an immediate problem for CI, but limits applicability for local usage. I believe this is an issue with this particular pytest plugin, not necessarily this strategy.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1587
https://github.com/scverse/scanpy/pull/1587:420,testability,test,tests,420,"Attach failing plots to CI results; If plots fail on CI, this should let us see the expected, actual, and diff through azure. ~~Hopefully~~ It works! I think this could make debugging plotting issues much easier. Here's an example of what the results look like:. <img width=""1090"" alt=""image"" src=""https://user-images.githubusercontent.com/8238804/104561575-b5814680-569b-11eb-8d1e-a9971affc645.png"">. Current issue, if tests are run in parallel, this does not work (https://github.com/pytest-dev/pytest-nunit/issues/40), which is not an immediate problem for CI, but limits applicability for local usage. I believe this is an issue with this particular pytest plugin, not necessarily this strategy.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1587
https://github.com/scverse/scanpy/pull/1587:306,usability,user,user-images,306,"Attach failing plots to CI results; If plots fail on CI, this should let us see the expected, actual, and diff through azure. ~~Hopefully~~ It works! I think this could make debugging plotting issues much easier. Here's an example of what the results look like:. <img width=""1090"" alt=""image"" src=""https://user-images.githubusercontent.com/8238804/104561575-b5814680-569b-11eb-8d1e-a9971affc645.png"">. Current issue, if tests are run in parallel, this does not work (https://github.com/pytest-dev/pytest-nunit/issues/40), which is not an immediate problem for CI, but limits applicability for local usage. I believe this is an issue with this particular pytest plugin, not necessarily this strategy.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1587
https://github.com/scverse/scanpy/issues/1588:27,availability,cluster,cluster,27,"Default plotting order for cluster annotations; <!-- What kind of feature would you like to request? -->. - [x] Additional function parameters / changed functionality / changed defaults? As this has come up several times (in my own work and in issue reports such as #891), I was wondering if it might be a good idea to have `sort_order=False` (or even better to use random ordering) as default for plotting any clustering result. That would require `sc.tl.louvain()` and `sc.tl.leiden()` to store a flag in `adata.uns` stating which columns are clustering columns, and `sc.pl.embedding()` to set `sort_order` according to this flag. . I would argue it's inherently wrong to plot a particular cluster on top when looking at clustering results as this complicates the interpretation of a visualization and leads to misinterpretations when we have overlapping embedding locations of cells. What do you think @ivirshup @fidelram?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1588
https://github.com/scverse/scanpy/issues/1588:411,availability,cluster,clustering,411,"Default plotting order for cluster annotations; <!-- What kind of feature would you like to request? -->. - [x] Additional function parameters / changed functionality / changed defaults? As this has come up several times (in my own work and in issue reports such as #891), I was wondering if it might be a good idea to have `sort_order=False` (or even better to use random ordering) as default for plotting any clustering result. That would require `sc.tl.louvain()` and `sc.tl.leiden()` to store a flag in `adata.uns` stating which columns are clustering columns, and `sc.pl.embedding()` to set `sort_order` according to this flag. . I would argue it's inherently wrong to plot a particular cluster on top when looking at clustering results as this complicates the interpretation of a visualization and leads to misinterpretations when we have overlapping embedding locations of cells. What do you think @ivirshup @fidelram?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1588
https://github.com/scverse/scanpy/issues/1588:545,availability,cluster,clustering,545,"Default plotting order for cluster annotations; <!-- What kind of feature would you like to request? -->. - [x] Additional function parameters / changed functionality / changed defaults? As this has come up several times (in my own work and in issue reports such as #891), I was wondering if it might be a good idea to have `sort_order=False` (or even better to use random ordering) as default for plotting any clustering result. That would require `sc.tl.louvain()` and `sc.tl.leiden()` to store a flag in `adata.uns` stating which columns are clustering columns, and `sc.pl.embedding()` to set `sort_order` according to this flag. . I would argue it's inherently wrong to plot a particular cluster on top when looking at clustering results as this complicates the interpretation of a visualization and leads to misinterpretations when we have overlapping embedding locations of cells. What do you think @ivirshup @fidelram?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1588
https://github.com/scverse/scanpy/issues/1588:692,availability,cluster,cluster,692,"Default plotting order for cluster annotations; <!-- What kind of feature would you like to request? -->. - [x] Additional function parameters / changed functionality / changed defaults? As this has come up several times (in my own work and in issue reports such as #891), I was wondering if it might be a good idea to have `sort_order=False` (or even better to use random ordering) as default for plotting any clustering result. That would require `sc.tl.louvain()` and `sc.tl.leiden()` to store a flag in `adata.uns` stating which columns are clustering columns, and `sc.pl.embedding()` to set `sort_order` according to this flag. . I would argue it's inherently wrong to plot a particular cluster on top when looking at clustering results as this complicates the interpretation of a visualization and leads to misinterpretations when we have overlapping embedding locations of cells. What do you think @ivirshup @fidelram?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1588
https://github.com/scverse/scanpy/issues/1588:723,availability,cluster,clustering,723,"Default plotting order for cluster annotations; <!-- What kind of feature would you like to request? -->. - [x] Additional function parameters / changed functionality / changed defaults? As this has come up several times (in my own work and in issue reports such as #891), I was wondering if it might be a good idea to have `sort_order=False` (or even better to use random ordering) as default for plotting any clustering result. That would require `sc.tl.louvain()` and `sc.tl.leiden()` to store a flag in `adata.uns` stating which columns are clustering columns, and `sc.pl.embedding()` to set `sort_order` according to this flag. . I would argue it's inherently wrong to plot a particular cluster on top when looking at clustering results as this complicates the interpretation of a visualization and leads to misinterpretations when we have overlapping embedding locations of cells. What do you think @ivirshup @fidelram?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1588
https://github.com/scverse/scanpy/issues/1588:27,deployability,cluster,cluster,27,"Default plotting order for cluster annotations; <!-- What kind of feature would you like to request? -->. - [x] Additional function parameters / changed functionality / changed defaults? As this has come up several times (in my own work and in issue reports such as #891), I was wondering if it might be a good idea to have `sort_order=False` (or even better to use random ordering) as default for plotting any clustering result. That would require `sc.tl.louvain()` and `sc.tl.leiden()` to store a flag in `adata.uns` stating which columns are clustering columns, and `sc.pl.embedding()` to set `sort_order` according to this flag. . I would argue it's inherently wrong to plot a particular cluster on top when looking at clustering results as this complicates the interpretation of a visualization and leads to misinterpretations when we have overlapping embedding locations of cells. What do you think @ivirshup @fidelram?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1588
https://github.com/scverse/scanpy/issues/1588:411,deployability,cluster,clustering,411,"Default plotting order for cluster annotations; <!-- What kind of feature would you like to request? -->. - [x] Additional function parameters / changed functionality / changed defaults? As this has come up several times (in my own work and in issue reports such as #891), I was wondering if it might be a good idea to have `sort_order=False` (or even better to use random ordering) as default for plotting any clustering result. That would require `sc.tl.louvain()` and `sc.tl.leiden()` to store a flag in `adata.uns` stating which columns are clustering columns, and `sc.pl.embedding()` to set `sort_order` according to this flag. . I would argue it's inherently wrong to plot a particular cluster on top when looking at clustering results as this complicates the interpretation of a visualization and leads to misinterpretations when we have overlapping embedding locations of cells. What do you think @ivirshup @fidelram?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1588
https://github.com/scverse/scanpy/issues/1588:545,deployability,cluster,clustering,545,"Default plotting order for cluster annotations; <!-- What kind of feature would you like to request? -->. - [x] Additional function parameters / changed functionality / changed defaults? As this has come up several times (in my own work and in issue reports such as #891), I was wondering if it might be a good idea to have `sort_order=False` (or even better to use random ordering) as default for plotting any clustering result. That would require `sc.tl.louvain()` and `sc.tl.leiden()` to store a flag in `adata.uns` stating which columns are clustering columns, and `sc.pl.embedding()` to set `sort_order` according to this flag. . I would argue it's inherently wrong to plot a particular cluster on top when looking at clustering results as this complicates the interpretation of a visualization and leads to misinterpretations when we have overlapping embedding locations of cells. What do you think @ivirshup @fidelram?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1588
https://github.com/scverse/scanpy/issues/1588:692,deployability,cluster,cluster,692,"Default plotting order for cluster annotations; <!-- What kind of feature would you like to request? -->. - [x] Additional function parameters / changed functionality / changed defaults? As this has come up several times (in my own work and in issue reports such as #891), I was wondering if it might be a good idea to have `sort_order=False` (or even better to use random ordering) as default for plotting any clustering result. That would require `sc.tl.louvain()` and `sc.tl.leiden()` to store a flag in `adata.uns` stating which columns are clustering columns, and `sc.pl.embedding()` to set `sort_order` according to this flag. . I would argue it's inherently wrong to plot a particular cluster on top when looking at clustering results as this complicates the interpretation of a visualization and leads to misinterpretations when we have overlapping embedding locations of cells. What do you think @ivirshup @fidelram?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1588
https://github.com/scverse/scanpy/issues/1588:723,deployability,cluster,clustering,723,"Default plotting order for cluster annotations; <!-- What kind of feature would you like to request? -->. - [x] Additional function parameters / changed functionality / changed defaults? As this has come up several times (in my own work and in issue reports such as #891), I was wondering if it might be a good idea to have `sort_order=False` (or even better to use random ordering) as default for plotting any clustering result. That would require `sc.tl.louvain()` and `sc.tl.leiden()` to store a flag in `adata.uns` stating which columns are clustering columns, and `sc.pl.embedding()` to set `sort_order` according to this flag. . I would argue it's inherently wrong to plot a particular cluster on top when looking at clustering results as this complicates the interpretation of a visualization and leads to misinterpretations when we have overlapping embedding locations of cells. What do you think @ivirshup @fidelram?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1588
https://github.com/scverse/scanpy/issues/1588:132,modifiability,paramet,parameters,132,"Default plotting order for cluster annotations; <!-- What kind of feature would you like to request? -->. - [x] Additional function parameters / changed functionality / changed defaults? As this has come up several times (in my own work and in issue reports such as #891), I was wondering if it might be a good idea to have `sort_order=False` (or even better to use random ordering) as default for plotting any clustering result. That would require `sc.tl.louvain()` and `sc.tl.leiden()` to store a flag in `adata.uns` stating which columns are clustering columns, and `sc.pl.embedding()` to set `sort_order` according to this flag. . I would argue it's inherently wrong to plot a particular cluster on top when looking at clustering results as this complicates the interpretation of a visualization and leads to misinterpretations when we have overlapping embedding locations of cells. What do you think @ivirshup @fidelram?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1588
https://github.com/scverse/scanpy/issues/1588:215,performance,time,times,215,"Default plotting order for cluster annotations; <!-- What kind of feature would you like to request? -->. - [x] Additional function parameters / changed functionality / changed defaults? As this has come up several times (in my own work and in issue reports such as #891), I was wondering if it might be a good idea to have `sort_order=False` (or even better to use random ordering) as default for plotting any clustering result. That would require `sc.tl.louvain()` and `sc.tl.leiden()` to store a flag in `adata.uns` stating which columns are clustering columns, and `sc.pl.embedding()` to set `sort_order` according to this flag. . I would argue it's inherently wrong to plot a particular cluster on top when looking at clustering results as this complicates the interpretation of a visualization and leads to misinterpretations when we have overlapping embedding locations of cells. What do you think @ivirshup @fidelram?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1588
https://github.com/scverse/scanpy/issues/1588:750,safety,compl,complicates,750,"Default plotting order for cluster annotations; <!-- What kind of feature would you like to request? -->. - [x] Additional function parameters / changed functionality / changed defaults? As this has come up several times (in my own work and in issue reports such as #891), I was wondering if it might be a good idea to have `sort_order=False` (or even better to use random ordering) as default for plotting any clustering result. That would require `sc.tl.louvain()` and `sc.tl.leiden()` to store a flag in `adata.uns` stating which columns are clustering columns, and `sc.pl.embedding()` to set `sort_order` according to this flag. . I would argue it's inherently wrong to plot a particular cluster on top when looking at clustering results as this complicates the interpretation of a visualization and leads to misinterpretations when we have overlapping embedding locations of cells. What do you think @ivirshup @fidelram?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1588
https://github.com/scverse/scanpy/issues/1588:750,security,compl,complicates,750,"Default plotting order for cluster annotations; <!-- What kind of feature would you like to request? -->. - [x] Additional function parameters / changed functionality / changed defaults? As this has come up several times (in my own work and in issue reports such as #891), I was wondering if it might be a good idea to have `sort_order=False` (or even better to use random ordering) as default for plotting any clustering result. That would require `sc.tl.louvain()` and `sc.tl.leiden()` to store a flag in `adata.uns` stating which columns are clustering columns, and `sc.pl.embedding()` to set `sort_order` according to this flag. . I would argue it's inherently wrong to plot a particular cluster on top when looking at clustering results as this complicates the interpretation of a visualization and leads to misinterpretations when we have overlapping embedding locations of cells. What do you think @ivirshup @fidelram?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1588
https://github.com/scverse/scanpy/issues/1588:786,usability,visual,visualization,786,"Default plotting order for cluster annotations; <!-- What kind of feature would you like to request? -->. - [x] Additional function parameters / changed functionality / changed defaults? As this has come up several times (in my own work and in issue reports such as #891), I was wondering if it might be a good idea to have `sort_order=False` (or even better to use random ordering) as default for plotting any clustering result. That would require `sc.tl.louvain()` and `sc.tl.leiden()` to store a flag in `adata.uns` stating which columns are clustering columns, and `sc.pl.embedding()` to set `sort_order` according to this flag. . I would argue it's inherently wrong to plot a particular cluster on top when looking at clustering results as this complicates the interpretation of a visualization and leads to misinterpretations when we have overlapping embedding locations of cells. What do you think @ivirshup @fidelram?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1588
https://github.com/scverse/scanpy/pull/1589:305,energy efficiency,current,currently,305,"fix sc.tl.umap for umap-0.5; @Koncopd, this seems to work as a bare minimum for making `sc.tl.umap` work for `umap-0.5`. `ingest` still does not work, and that definitely looks complicated to fix. What would you say to adding a line like. ```python. if umap.__version__ >= 0.5:. raise ImportError(""Ingest currently require umap-learn < 0.5""). ```. We could then remove the pin, and free up user's environments.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1589
https://github.com/scverse/scanpy/pull/1589:136,reliability,doe,does,136,"fix sc.tl.umap for umap-0.5; @Koncopd, this seems to work as a bare minimum for making `sc.tl.umap` work for `umap-0.5`. `ingest` still does not work, and that definitely looks complicated to fix. What would you say to adding a line like. ```python. if umap.__version__ >= 0.5:. raise ImportError(""Ingest currently require umap-learn < 0.5""). ```. We could then remove the pin, and free up user's environments.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1589
https://github.com/scverse/scanpy/pull/1589:177,safety,compl,complicated,177,"fix sc.tl.umap for umap-0.5; @Koncopd, this seems to work as a bare minimum for making `sc.tl.umap` work for `umap-0.5`. `ingest` still does not work, and that definitely looks complicated to fix. What would you say to adding a line like. ```python. if umap.__version__ >= 0.5:. raise ImportError(""Ingest currently require umap-learn < 0.5""). ```. We could then remove the pin, and free up user's environments.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1589
https://github.com/scverse/scanpy/pull/1589:177,security,compl,complicated,177,"fix sc.tl.umap for umap-0.5; @Koncopd, this seems to work as a bare minimum for making `sc.tl.umap` work for `umap-0.5`. `ingest` still does not work, and that definitely looks complicated to fix. What would you say to adding a line like. ```python. if umap.__version__ >= 0.5:. raise ImportError(""Ingest currently require umap-learn < 0.5""). ```. We could then remove the pin, and free up user's environments.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1589
https://github.com/scverse/scanpy/pull/1589:68,usability,minim,minimum,68,"fix sc.tl.umap for umap-0.5; @Koncopd, this seems to work as a bare minimum for making `sc.tl.umap` work for `umap-0.5`. `ingest` still does not work, and that definitely looks complicated to fix. What would you say to adding a line like. ```python. if umap.__version__ >= 0.5:. raise ImportError(""Ingest currently require umap-learn < 0.5""). ```. We could then remove the pin, and free up user's environments.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1589
https://github.com/scverse/scanpy/pull/1589:328,usability,learn,learn,328,"fix sc.tl.umap for umap-0.5; @Koncopd, this seems to work as a bare minimum for making `sc.tl.umap` work for `umap-0.5`. `ingest` still does not work, and that definitely looks complicated to fix. What would you say to adding a line like. ```python. if umap.__version__ >= 0.5:. raise ImportError(""Ingest currently require umap-learn < 0.5""). ```. We could then remove the pin, and free up user's environments.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1589
https://github.com/scverse/scanpy/pull/1589:390,usability,user,user,390,"fix sc.tl.umap for umap-0.5; @Koncopd, this seems to work as a bare minimum for making `sc.tl.umap` work for `umap-0.5`. `ingest` still does not work, and that definitely looks complicated to fix. What would you say to adding a line like. ```python. if umap.__version__ >= 0.5:. raise ImportError(""Ingest currently require umap-learn < 0.5""). ```. We could then remove the pin, and free up user's environments.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1589
https://github.com/scverse/scanpy/issues/1590:482,availability,error,error,482,"Bug when running PCA in chunked mode; - [x] I have checked that this issue has not already been reported. - [ ] I have confirmed this bug exists on the latest version of scanpy. - [x] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. ```python. sc.tl.pca(adata, svd_solver='arpack', use_highly_variable=True, chunked=True, chunk_size=1000). ```. ```pytb. TypeError: unsupported operand type(s) for -: 'datetime.datetime' and 'int'. ```. I believe the error is due overwriting of `start` variable. It is declared in [line 116](https://github.com/theislab/scanpy/blob/master/scanpy/preprocessing/_pca.py#L116) but is overwritten by loop variable of same name in [line 172](https://github.com/theislab/scanpy/blob/master/scanpy/preprocessing/_pca.py#L172). #### Versions. <details>. -----. anndata 0.7.5. scanpy 1.6.0. sinfo 0.3.1. -----. PIL 8.1.0. anndata 0.7.5. asciitree NA. cffi 1.14.3. cloudpickle 1.6.0. constants NA. cycler 0.10.0. cython_runtime NA. dask 2020.12.0. dateutil 2.8.1. fasteners NA. get_version 2.1. h5py 3.1.0. highs_wrapper NA. igraph 0.8.3. joblib 1.0.0. kiwisolver 1.3.1. legacy_api_wrap 1.2. leidenalg 0.8.3. llvmlite 0.35.0. matplotlib 3.3.3. mpl_toolkits NA. msgpack 1.0.2. natsort 7.1.0. numba 0.52.0. numcodecs 0.7.2. numexpr 2.7.2. numpy 1.19.4. packaging 20.8. pandas 1.2.0. pkg_resources NA. psutil 5.8.0. pyparsing 2.4.7. pytz 2020.5. scanpy 1.6.0. scipy 1.6.0. setuptools_scm NA. sinfo 0.3.1. six 1.15.0. sklearn 0.24.0. sparse 0.11.2. tables 3.6.1. tblib 1.7.0. texttable 1.6.3. tlz 0.11.1. toolz 0.11.1. typing_extensions NA. yaml 5.3.1. zarr 2.6.1. -----. Python 3.8.5 (default, Sep 4 2020, 07:30:14) [GCC 7.3.0]. Linux-3.10.0-1062.4.1.el7.x86_64-x86_64-with-glibc2.10. 2 logical CPU cores, x86_64. -----. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1590
https://github.com/scverse/scanpy/issues/1590:159,deployability,version,version,159,"Bug when running PCA in chunked mode; - [x] I have checked that this issue has not already been reported. - [ ] I have confirmed this bug exists on the latest version of scanpy. - [x] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. ```python. sc.tl.pca(adata, svd_solver='arpack', use_highly_variable=True, chunked=True, chunk_size=1000). ```. ```pytb. TypeError: unsupported operand type(s) for -: 'datetime.datetime' and 'int'. ```. I believe the error is due overwriting of `start` variable. It is declared in [line 116](https://github.com/theislab/scanpy/blob/master/scanpy/preprocessing/_pca.py#L116) but is overwritten by loop variable of same name in [line 172](https://github.com/theislab/scanpy/blob/master/scanpy/preprocessing/_pca.py#L172). #### Versions. <details>. -----. anndata 0.7.5. scanpy 1.6.0. sinfo 0.3.1. -----. PIL 8.1.0. anndata 0.7.5. asciitree NA. cffi 1.14.3. cloudpickle 1.6.0. constants NA. cycler 0.10.0. cython_runtime NA. dask 2020.12.0. dateutil 2.8.1. fasteners NA. get_version 2.1. h5py 3.1.0. highs_wrapper NA. igraph 0.8.3. joblib 1.0.0. kiwisolver 1.3.1. legacy_api_wrap 1.2. leidenalg 0.8.3. llvmlite 0.35.0. matplotlib 3.3.3. mpl_toolkits NA. msgpack 1.0.2. natsort 7.1.0. numba 0.52.0. numcodecs 0.7.2. numexpr 2.7.2. numpy 1.19.4. packaging 20.8. pandas 1.2.0. pkg_resources NA. psutil 5.8.0. pyparsing 2.4.7. pytz 2020.5. scanpy 1.6.0. scipy 1.6.0. setuptools_scm NA. sinfo 0.3.1. six 1.15.0. sklearn 0.24.0. sparse 0.11.2. tables 3.6.1. tblib 1.7.0. texttable 1.6.3. tlz 0.11.1. toolz 0.11.1. typing_extensions NA. yaml 5.3.1. zarr 2.6.1. -----. Python 3.8.5 (default, Sep 4 2020, 07:30:14) [GCC 7.3.0]. Linux-3.10.0-1062.4.1.el7.x86_64-x86_64-with-glibc2.10. 2 logical CPU cores, x86_64. -----. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1590
https://github.com/scverse/scanpy/issues/1590:790,deployability,Version,Versions,790,"Bug when running PCA in chunked mode; - [x] I have checked that this issue has not already been reported. - [ ] I have confirmed this bug exists on the latest version of scanpy. - [x] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. ```python. sc.tl.pca(adata, svd_solver='arpack', use_highly_variable=True, chunked=True, chunk_size=1000). ```. ```pytb. TypeError: unsupported operand type(s) for -: 'datetime.datetime' and 'int'. ```. I believe the error is due overwriting of `start` variable. It is declared in [line 116](https://github.com/theislab/scanpy/blob/master/scanpy/preprocessing/_pca.py#L116) but is overwritten by loop variable of same name in [line 172](https://github.com/theislab/scanpy/blob/master/scanpy/preprocessing/_pca.py#L172). #### Versions. <details>. -----. anndata 0.7.5. scanpy 1.6.0. sinfo 0.3.1. -----. PIL 8.1.0. anndata 0.7.5. asciitree NA. cffi 1.14.3. cloudpickle 1.6.0. constants NA. cycler 0.10.0. cython_runtime NA. dask 2020.12.0. dateutil 2.8.1. fasteners NA. get_version 2.1. h5py 3.1.0. highs_wrapper NA. igraph 0.8.3. joblib 1.0.0. kiwisolver 1.3.1. legacy_api_wrap 1.2. leidenalg 0.8.3. llvmlite 0.35.0. matplotlib 3.3.3. mpl_toolkits NA. msgpack 1.0.2. natsort 7.1.0. numba 0.52.0. numcodecs 0.7.2. numexpr 2.7.2. numpy 1.19.4. packaging 20.8. pandas 1.2.0. pkg_resources NA. psutil 5.8.0. pyparsing 2.4.7. pytz 2020.5. scanpy 1.6.0. scipy 1.6.0. setuptools_scm NA. sinfo 0.3.1. six 1.15.0. sklearn 0.24.0. sparse 0.11.2. tables 3.6.1. tblib 1.7.0. texttable 1.6.3. tlz 0.11.1. toolz 0.11.1. typing_extensions NA. yaml 5.3.1. zarr 2.6.1. -----. Python 3.8.5 (default, Sep 4 2020, 07:30:14) [GCC 7.3.0]. Linux-3.10.0-1062.4.1.el7.x86_64-x86_64-with-glibc2.10. 2 logical CPU cores, x86_64. -----. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1590
https://github.com/scverse/scanpy/issues/1590:1739,deployability,log,logical,1739,"Bug when running PCA in chunked mode; - [x] I have checked that this issue has not already been reported. - [ ] I have confirmed this bug exists on the latest version of scanpy. - [x] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. ```python. sc.tl.pca(adata, svd_solver='arpack', use_highly_variable=True, chunked=True, chunk_size=1000). ```. ```pytb. TypeError: unsupported operand type(s) for -: 'datetime.datetime' and 'int'. ```. I believe the error is due overwriting of `start` variable. It is declared in [line 116](https://github.com/theislab/scanpy/blob/master/scanpy/preprocessing/_pca.py#L116) but is overwritten by loop variable of same name in [line 172](https://github.com/theislab/scanpy/blob/master/scanpy/preprocessing/_pca.py#L172). #### Versions. <details>. -----. anndata 0.7.5. scanpy 1.6.0. sinfo 0.3.1. -----. PIL 8.1.0. anndata 0.7.5. asciitree NA. cffi 1.14.3. cloudpickle 1.6.0. constants NA. cycler 0.10.0. cython_runtime NA. dask 2020.12.0. dateutil 2.8.1. fasteners NA. get_version 2.1. h5py 3.1.0. highs_wrapper NA. igraph 0.8.3. joblib 1.0.0. kiwisolver 1.3.1. legacy_api_wrap 1.2. leidenalg 0.8.3. llvmlite 0.35.0. matplotlib 3.3.3. mpl_toolkits NA. msgpack 1.0.2. natsort 7.1.0. numba 0.52.0. numcodecs 0.7.2. numexpr 2.7.2. numpy 1.19.4. packaging 20.8. pandas 1.2.0. pkg_resources NA. psutil 5.8.0. pyparsing 2.4.7. pytz 2020.5. scanpy 1.6.0. scipy 1.6.0. setuptools_scm NA. sinfo 0.3.1. six 1.15.0. sklearn 0.24.0. sparse 0.11.2. tables 3.6.1. tblib 1.7.0. texttable 1.6.3. tlz 0.11.1. toolz 0.11.1. typing_extensions NA. yaml 5.3.1. zarr 2.6.1. -----. Python 3.8.5 (default, Sep 4 2020, 07:30:14) [GCC 7.3.0]. Linux-3.10.0-1062.4.1.el7.x86_64-x86_64-with-glibc2.10. 2 logical CPU cores, x86_64. -----. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1590
https://github.com/scverse/scanpy/issues/1590:920,energy efficiency,cloud,cloudpickle,920,"Bug when running PCA in chunked mode; - [x] I have checked that this issue has not already been reported. - [ ] I have confirmed this bug exists on the latest version of scanpy. - [x] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. ```python. sc.tl.pca(adata, svd_solver='arpack', use_highly_variable=True, chunked=True, chunk_size=1000). ```. ```pytb. TypeError: unsupported operand type(s) for -: 'datetime.datetime' and 'int'. ```. I believe the error is due overwriting of `start` variable. It is declared in [line 116](https://github.com/theislab/scanpy/blob/master/scanpy/preprocessing/_pca.py#L116) but is overwritten by loop variable of same name in [line 172](https://github.com/theislab/scanpy/blob/master/scanpy/preprocessing/_pca.py#L172). #### Versions. <details>. -----. anndata 0.7.5. scanpy 1.6.0. sinfo 0.3.1. -----. PIL 8.1.0. anndata 0.7.5. asciitree NA. cffi 1.14.3. cloudpickle 1.6.0. constants NA. cycler 0.10.0. cython_runtime NA. dask 2020.12.0. dateutil 2.8.1. fasteners NA. get_version 2.1. h5py 3.1.0. highs_wrapper NA. igraph 0.8.3. joblib 1.0.0. kiwisolver 1.3.1. legacy_api_wrap 1.2. leidenalg 0.8.3. llvmlite 0.35.0. matplotlib 3.3.3. mpl_toolkits NA. msgpack 1.0.2. natsort 7.1.0. numba 0.52.0. numcodecs 0.7.2. numexpr 2.7.2. numpy 1.19.4. packaging 20.8. pandas 1.2.0. pkg_resources NA. psutil 5.8.0. pyparsing 2.4.7. pytz 2020.5. scanpy 1.6.0. scipy 1.6.0. setuptools_scm NA. sinfo 0.3.1. six 1.15.0. sklearn 0.24.0. sparse 0.11.2. tables 3.6.1. tblib 1.7.0. texttable 1.6.3. tlz 0.11.1. toolz 0.11.1. typing_extensions NA. yaml 5.3.1. zarr 2.6.1. -----. Python 3.8.5 (default, Sep 4 2020, 07:30:14) [GCC 7.3.0]. Linux-3.10.0-1062.4.1.el7.x86_64-x86_64-with-glibc2.10. 2 logical CPU cores, x86_64. -----. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1590
https://github.com/scverse/scanpy/issues/1590:1747,energy efficiency,CPU,CPU,1747,"Bug when running PCA in chunked mode; - [x] I have checked that this issue has not already been reported. - [ ] I have confirmed this bug exists on the latest version of scanpy. - [x] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. ```python. sc.tl.pca(adata, svd_solver='arpack', use_highly_variable=True, chunked=True, chunk_size=1000). ```. ```pytb. TypeError: unsupported operand type(s) for -: 'datetime.datetime' and 'int'. ```. I believe the error is due overwriting of `start` variable. It is declared in [line 116](https://github.com/theislab/scanpy/blob/master/scanpy/preprocessing/_pca.py#L116) but is overwritten by loop variable of same name in [line 172](https://github.com/theislab/scanpy/blob/master/scanpy/preprocessing/_pca.py#L172). #### Versions. <details>. -----. anndata 0.7.5. scanpy 1.6.0. sinfo 0.3.1. -----. PIL 8.1.0. anndata 0.7.5. asciitree NA. cffi 1.14.3. cloudpickle 1.6.0. constants NA. cycler 0.10.0. cython_runtime NA. dask 2020.12.0. dateutil 2.8.1. fasteners NA. get_version 2.1. h5py 3.1.0. highs_wrapper NA. igraph 0.8.3. joblib 1.0.0. kiwisolver 1.3.1. legacy_api_wrap 1.2. leidenalg 0.8.3. llvmlite 0.35.0. matplotlib 3.3.3. mpl_toolkits NA. msgpack 1.0.2. natsort 7.1.0. numba 0.52.0. numcodecs 0.7.2. numexpr 2.7.2. numpy 1.19.4. packaging 20.8. pandas 1.2.0. pkg_resources NA. psutil 5.8.0. pyparsing 2.4.7. pytz 2020.5. scanpy 1.6.0. scipy 1.6.0. setuptools_scm NA. sinfo 0.3.1. six 1.15.0. sklearn 0.24.0. sparse 0.11.2. tables 3.6.1. tblib 1.7.0. texttable 1.6.3. tlz 0.11.1. toolz 0.11.1. typing_extensions NA. yaml 5.3.1. zarr 2.6.1. -----. Python 3.8.5 (default, Sep 4 2020, 07:30:14) [GCC 7.3.0]. Linux-3.10.0-1062.4.1.el7.x86_64-x86_64-with-glibc2.10. 2 logical CPU cores, x86_64. -----. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1590
https://github.com/scverse/scanpy/issues/1590:1751,energy efficiency,core,cores,1751,"Bug when running PCA in chunked mode; - [x] I have checked that this issue has not already been reported. - [ ] I have confirmed this bug exists on the latest version of scanpy. - [x] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. ```python. sc.tl.pca(adata, svd_solver='arpack', use_highly_variable=True, chunked=True, chunk_size=1000). ```. ```pytb. TypeError: unsupported operand type(s) for -: 'datetime.datetime' and 'int'. ```. I believe the error is due overwriting of `start` variable. It is declared in [line 116](https://github.com/theislab/scanpy/blob/master/scanpy/preprocessing/_pca.py#L116) but is overwritten by loop variable of same name in [line 172](https://github.com/theislab/scanpy/blob/master/scanpy/preprocessing/_pca.py#L172). #### Versions. <details>. -----. anndata 0.7.5. scanpy 1.6.0. sinfo 0.3.1. -----. PIL 8.1.0. anndata 0.7.5. asciitree NA. cffi 1.14.3. cloudpickle 1.6.0. constants NA. cycler 0.10.0. cython_runtime NA. dask 2020.12.0. dateutil 2.8.1. fasteners NA. get_version 2.1. h5py 3.1.0. highs_wrapper NA. igraph 0.8.3. joblib 1.0.0. kiwisolver 1.3.1. legacy_api_wrap 1.2. leidenalg 0.8.3. llvmlite 0.35.0. matplotlib 3.3.3. mpl_toolkits NA. msgpack 1.0.2. natsort 7.1.0. numba 0.52.0. numcodecs 0.7.2. numexpr 2.7.2. numpy 1.19.4. packaging 20.8. pandas 1.2.0. pkg_resources NA. psutil 5.8.0. pyparsing 2.4.7. pytz 2020.5. scanpy 1.6.0. scipy 1.6.0. setuptools_scm NA. sinfo 0.3.1. six 1.15.0. sklearn 0.24.0. sparse 0.11.2. tables 3.6.1. tblib 1.7.0. texttable 1.6.3. tlz 0.11.1. toolz 0.11.1. typing_extensions NA. yaml 5.3.1. zarr 2.6.1. -----. Python 3.8.5 (default, Sep 4 2020, 07:30:14) [GCC 7.3.0]. Linux-3.10.0-1062.4.1.el7.x86_64-x86_64-with-glibc2.10. 2 logical CPU cores, x86_64. -----. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1590
https://github.com/scverse/scanpy/issues/1590:159,integrability,version,version,159,"Bug when running PCA in chunked mode; - [x] I have checked that this issue has not already been reported. - [ ] I have confirmed this bug exists on the latest version of scanpy. - [x] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. ```python. sc.tl.pca(adata, svd_solver='arpack', use_highly_variable=True, chunked=True, chunk_size=1000). ```. ```pytb. TypeError: unsupported operand type(s) for -: 'datetime.datetime' and 'int'. ```. I believe the error is due overwriting of `start` variable. It is declared in [line 116](https://github.com/theislab/scanpy/blob/master/scanpy/preprocessing/_pca.py#L116) but is overwritten by loop variable of same name in [line 172](https://github.com/theislab/scanpy/blob/master/scanpy/preprocessing/_pca.py#L172). #### Versions. <details>. -----. anndata 0.7.5. scanpy 1.6.0. sinfo 0.3.1. -----. PIL 8.1.0. anndata 0.7.5. asciitree NA. cffi 1.14.3. cloudpickle 1.6.0. constants NA. cycler 0.10.0. cython_runtime NA. dask 2020.12.0. dateutil 2.8.1. fasteners NA. get_version 2.1. h5py 3.1.0. highs_wrapper NA. igraph 0.8.3. joblib 1.0.0. kiwisolver 1.3.1. legacy_api_wrap 1.2. leidenalg 0.8.3. llvmlite 0.35.0. matplotlib 3.3.3. mpl_toolkits NA. msgpack 1.0.2. natsort 7.1.0. numba 0.52.0. numcodecs 0.7.2. numexpr 2.7.2. numpy 1.19.4. packaging 20.8. pandas 1.2.0. pkg_resources NA. psutil 5.8.0. pyparsing 2.4.7. pytz 2020.5. scanpy 1.6.0. scipy 1.6.0. setuptools_scm NA. sinfo 0.3.1. six 1.15.0. sklearn 0.24.0. sparse 0.11.2. tables 3.6.1. tblib 1.7.0. texttable 1.6.3. tlz 0.11.1. toolz 0.11.1. typing_extensions NA. yaml 5.3.1. zarr 2.6.1. -----. Python 3.8.5 (default, Sep 4 2020, 07:30:14) [GCC 7.3.0]. Linux-3.10.0-1062.4.1.el7.x86_64-x86_64-with-glibc2.10. 2 logical CPU cores, x86_64. -----. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1590
https://github.com/scverse/scanpy/issues/1590:790,integrability,Version,Versions,790,"Bug when running PCA in chunked mode; - [x] I have checked that this issue has not already been reported. - [ ] I have confirmed this bug exists on the latest version of scanpy. - [x] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. ```python. sc.tl.pca(adata, svd_solver='arpack', use_highly_variable=True, chunked=True, chunk_size=1000). ```. ```pytb. TypeError: unsupported operand type(s) for -: 'datetime.datetime' and 'int'. ```. I believe the error is due overwriting of `start` variable. It is declared in [line 116](https://github.com/theislab/scanpy/blob/master/scanpy/preprocessing/_pca.py#L116) but is overwritten by loop variable of same name in [line 172](https://github.com/theislab/scanpy/blob/master/scanpy/preprocessing/_pca.py#L172). #### Versions. <details>. -----. anndata 0.7.5. scanpy 1.6.0. sinfo 0.3.1. -----. PIL 8.1.0. anndata 0.7.5. asciitree NA. cffi 1.14.3. cloudpickle 1.6.0. constants NA. cycler 0.10.0. cython_runtime NA. dask 2020.12.0. dateutil 2.8.1. fasteners NA. get_version 2.1. h5py 3.1.0. highs_wrapper NA. igraph 0.8.3. joblib 1.0.0. kiwisolver 1.3.1. legacy_api_wrap 1.2. leidenalg 0.8.3. llvmlite 0.35.0. matplotlib 3.3.3. mpl_toolkits NA. msgpack 1.0.2. natsort 7.1.0. numba 0.52.0. numcodecs 0.7.2. numexpr 2.7.2. numpy 1.19.4. packaging 20.8. pandas 1.2.0. pkg_resources NA. psutil 5.8.0. pyparsing 2.4.7. pytz 2020.5. scanpy 1.6.0. scipy 1.6.0. setuptools_scm NA. sinfo 0.3.1. six 1.15.0. sklearn 0.24.0. sparse 0.11.2. tables 3.6.1. tblib 1.7.0. texttable 1.6.3. tlz 0.11.1. toolz 0.11.1. typing_extensions NA. yaml 5.3.1. zarr 2.6.1. -----. Python 3.8.5 (default, Sep 4 2020, 07:30:14) [GCC 7.3.0]. Linux-3.10.0-1062.4.1.el7.x86_64-x86_64-with-glibc2.10. 2 logical CPU cores, x86_64. -----. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1590
https://github.com/scverse/scanpy/issues/1590:159,modifiability,version,version,159,"Bug when running PCA in chunked mode; - [x] I have checked that this issue has not already been reported. - [ ] I have confirmed this bug exists on the latest version of scanpy. - [x] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. ```python. sc.tl.pca(adata, svd_solver='arpack', use_highly_variable=True, chunked=True, chunk_size=1000). ```. ```pytb. TypeError: unsupported operand type(s) for -: 'datetime.datetime' and 'int'. ```. I believe the error is due overwriting of `start` variable. It is declared in [line 116](https://github.com/theislab/scanpy/blob/master/scanpy/preprocessing/_pca.py#L116) but is overwritten by loop variable of same name in [line 172](https://github.com/theislab/scanpy/blob/master/scanpy/preprocessing/_pca.py#L172). #### Versions. <details>. -----. anndata 0.7.5. scanpy 1.6.0. sinfo 0.3.1. -----. PIL 8.1.0. anndata 0.7.5. asciitree NA. cffi 1.14.3. cloudpickle 1.6.0. constants NA. cycler 0.10.0. cython_runtime NA. dask 2020.12.0. dateutil 2.8.1. fasteners NA. get_version 2.1. h5py 3.1.0. highs_wrapper NA. igraph 0.8.3. joblib 1.0.0. kiwisolver 1.3.1. legacy_api_wrap 1.2. leidenalg 0.8.3. llvmlite 0.35.0. matplotlib 3.3.3. mpl_toolkits NA. msgpack 1.0.2. natsort 7.1.0. numba 0.52.0. numcodecs 0.7.2. numexpr 2.7.2. numpy 1.19.4. packaging 20.8. pandas 1.2.0. pkg_resources NA. psutil 5.8.0. pyparsing 2.4.7. pytz 2020.5. scanpy 1.6.0. scipy 1.6.0. setuptools_scm NA. sinfo 0.3.1. six 1.15.0. sklearn 0.24.0. sparse 0.11.2. tables 3.6.1. tblib 1.7.0. texttable 1.6.3. tlz 0.11.1. toolz 0.11.1. typing_extensions NA. yaml 5.3.1. zarr 2.6.1. -----. Python 3.8.5 (default, Sep 4 2020, 07:30:14) [GCC 7.3.0]. Linux-3.10.0-1062.4.1.el7.x86_64-x86_64-with-glibc2.10. 2 logical CPU cores, x86_64. -----. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1590
https://github.com/scverse/scanpy/issues/1590:518,modifiability,variab,variable,518,"Bug when running PCA in chunked mode; - [x] I have checked that this issue has not already been reported. - [ ] I have confirmed this bug exists on the latest version of scanpy. - [x] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. ```python. sc.tl.pca(adata, svd_solver='arpack', use_highly_variable=True, chunked=True, chunk_size=1000). ```. ```pytb. TypeError: unsupported operand type(s) for -: 'datetime.datetime' and 'int'. ```. I believe the error is due overwriting of `start` variable. It is declared in [line 116](https://github.com/theislab/scanpy/blob/master/scanpy/preprocessing/_pca.py#L116) but is overwritten by loop variable of same name in [line 172](https://github.com/theislab/scanpy/blob/master/scanpy/preprocessing/_pca.py#L172). #### Versions. <details>. -----. anndata 0.7.5. scanpy 1.6.0. sinfo 0.3.1. -----. PIL 8.1.0. anndata 0.7.5. asciitree NA. cffi 1.14.3. cloudpickle 1.6.0. constants NA. cycler 0.10.0. cython_runtime NA. dask 2020.12.0. dateutil 2.8.1. fasteners NA. get_version 2.1. h5py 3.1.0. highs_wrapper NA. igraph 0.8.3. joblib 1.0.0. kiwisolver 1.3.1. legacy_api_wrap 1.2. leidenalg 0.8.3. llvmlite 0.35.0. matplotlib 3.3.3. mpl_toolkits NA. msgpack 1.0.2. natsort 7.1.0. numba 0.52.0. numcodecs 0.7.2. numexpr 2.7.2. numpy 1.19.4. packaging 20.8. pandas 1.2.0. pkg_resources NA. psutil 5.8.0. pyparsing 2.4.7. pytz 2020.5. scanpy 1.6.0. scipy 1.6.0. setuptools_scm NA. sinfo 0.3.1. six 1.15.0. sklearn 0.24.0. sparse 0.11.2. tables 3.6.1. tblib 1.7.0. texttable 1.6.3. tlz 0.11.1. toolz 0.11.1. typing_extensions NA. yaml 5.3.1. zarr 2.6.1. -----. Python 3.8.5 (default, Sep 4 2020, 07:30:14) [GCC 7.3.0]. Linux-3.10.0-1062.4.1.el7.x86_64-x86_64-with-glibc2.10. 2 logical CPU cores, x86_64. -----. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1590
https://github.com/scverse/scanpy/issues/1590:666,modifiability,variab,variable,666,"Bug when running PCA in chunked mode; - [x] I have checked that this issue has not already been reported. - [ ] I have confirmed this bug exists on the latest version of scanpy. - [x] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. ```python. sc.tl.pca(adata, svd_solver='arpack', use_highly_variable=True, chunked=True, chunk_size=1000). ```. ```pytb. TypeError: unsupported operand type(s) for -: 'datetime.datetime' and 'int'. ```. I believe the error is due overwriting of `start` variable. It is declared in [line 116](https://github.com/theislab/scanpy/blob/master/scanpy/preprocessing/_pca.py#L116) but is overwritten by loop variable of same name in [line 172](https://github.com/theislab/scanpy/blob/master/scanpy/preprocessing/_pca.py#L172). #### Versions. <details>. -----. anndata 0.7.5. scanpy 1.6.0. sinfo 0.3.1. -----. PIL 8.1.0. anndata 0.7.5. asciitree NA. cffi 1.14.3. cloudpickle 1.6.0. constants NA. cycler 0.10.0. cython_runtime NA. dask 2020.12.0. dateutil 2.8.1. fasteners NA. get_version 2.1. h5py 3.1.0. highs_wrapper NA. igraph 0.8.3. joblib 1.0.0. kiwisolver 1.3.1. legacy_api_wrap 1.2. leidenalg 0.8.3. llvmlite 0.35.0. matplotlib 3.3.3. mpl_toolkits NA. msgpack 1.0.2. natsort 7.1.0. numba 0.52.0. numcodecs 0.7.2. numexpr 2.7.2. numpy 1.19.4. packaging 20.8. pandas 1.2.0. pkg_resources NA. psutil 5.8.0. pyparsing 2.4.7. pytz 2020.5. scanpy 1.6.0. scipy 1.6.0. setuptools_scm NA. sinfo 0.3.1. six 1.15.0. sklearn 0.24.0. sparse 0.11.2. tables 3.6.1. tblib 1.7.0. texttable 1.6.3. tlz 0.11.1. toolz 0.11.1. typing_extensions NA. yaml 5.3.1. zarr 2.6.1. -----. Python 3.8.5 (default, Sep 4 2020, 07:30:14) [GCC 7.3.0]. Linux-3.10.0-1062.4.1.el7.x86_64-x86_64-with-glibc2.10. 2 logical CPU cores, x86_64. -----. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1590
https://github.com/scverse/scanpy/issues/1590:790,modifiability,Version,Versions,790,"Bug when running PCA in chunked mode; - [x] I have checked that this issue has not already been reported. - [ ] I have confirmed this bug exists on the latest version of scanpy. - [x] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. ```python. sc.tl.pca(adata, svd_solver='arpack', use_highly_variable=True, chunked=True, chunk_size=1000). ```. ```pytb. TypeError: unsupported operand type(s) for -: 'datetime.datetime' and 'int'. ```. I believe the error is due overwriting of `start` variable. It is declared in [line 116](https://github.com/theislab/scanpy/blob/master/scanpy/preprocessing/_pca.py#L116) but is overwritten by loop variable of same name in [line 172](https://github.com/theislab/scanpy/blob/master/scanpy/preprocessing/_pca.py#L172). #### Versions. <details>. -----. anndata 0.7.5. scanpy 1.6.0. sinfo 0.3.1. -----. PIL 8.1.0. anndata 0.7.5. asciitree NA. cffi 1.14.3. cloudpickle 1.6.0. constants NA. cycler 0.10.0. cython_runtime NA. dask 2020.12.0. dateutil 2.8.1. fasteners NA. get_version 2.1. h5py 3.1.0. highs_wrapper NA. igraph 0.8.3. joblib 1.0.0. kiwisolver 1.3.1. legacy_api_wrap 1.2. leidenalg 0.8.3. llvmlite 0.35.0. matplotlib 3.3.3. mpl_toolkits NA. msgpack 1.0.2. natsort 7.1.0. numba 0.52.0. numcodecs 0.7.2. numexpr 2.7.2. numpy 1.19.4. packaging 20.8. pandas 1.2.0. pkg_resources NA. psutil 5.8.0. pyparsing 2.4.7. pytz 2020.5. scanpy 1.6.0. scipy 1.6.0. setuptools_scm NA. sinfo 0.3.1. six 1.15.0. sklearn 0.24.0. sparse 0.11.2. tables 3.6.1. tblib 1.7.0. texttable 1.6.3. tlz 0.11.1. toolz 0.11.1. typing_extensions NA. yaml 5.3.1. zarr 2.6.1. -----. Python 3.8.5 (default, Sep 4 2020, 07:30:14) [GCC 7.3.0]. Linux-3.10.0-1062.4.1.el7.x86_64-x86_64-with-glibc2.10. 2 logical CPU cores, x86_64. -----. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1590
https://github.com/scverse/scanpy/issues/1590:1306,modifiability,pac,packaging,1306,"Bug when running PCA in chunked mode; - [x] I have checked that this issue has not already been reported. - [ ] I have confirmed this bug exists on the latest version of scanpy. - [x] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. ```python. sc.tl.pca(adata, svd_solver='arpack', use_highly_variable=True, chunked=True, chunk_size=1000). ```. ```pytb. TypeError: unsupported operand type(s) for -: 'datetime.datetime' and 'int'. ```. I believe the error is due overwriting of `start` variable. It is declared in [line 116](https://github.com/theislab/scanpy/blob/master/scanpy/preprocessing/_pca.py#L116) but is overwritten by loop variable of same name in [line 172](https://github.com/theislab/scanpy/blob/master/scanpy/preprocessing/_pca.py#L172). #### Versions. <details>. -----. anndata 0.7.5. scanpy 1.6.0. sinfo 0.3.1. -----. PIL 8.1.0. anndata 0.7.5. asciitree NA. cffi 1.14.3. cloudpickle 1.6.0. constants NA. cycler 0.10.0. cython_runtime NA. dask 2020.12.0. dateutil 2.8.1. fasteners NA. get_version 2.1. h5py 3.1.0. highs_wrapper NA. igraph 0.8.3. joblib 1.0.0. kiwisolver 1.3.1. legacy_api_wrap 1.2. leidenalg 0.8.3. llvmlite 0.35.0. matplotlib 3.3.3. mpl_toolkits NA. msgpack 1.0.2. natsort 7.1.0. numba 0.52.0. numcodecs 0.7.2. numexpr 2.7.2. numpy 1.19.4. packaging 20.8. pandas 1.2.0. pkg_resources NA. psutil 5.8.0. pyparsing 2.4.7. pytz 2020.5. scanpy 1.6.0. scipy 1.6.0. setuptools_scm NA. sinfo 0.3.1. six 1.15.0. sklearn 0.24.0. sparse 0.11.2. tables 3.6.1. tblib 1.7.0. texttable 1.6.3. tlz 0.11.1. toolz 0.11.1. typing_extensions NA. yaml 5.3.1. zarr 2.6.1. -----. Python 3.8.5 (default, Sep 4 2020, 07:30:14) [GCC 7.3.0]. Linux-3.10.0-1062.4.1.el7.x86_64-x86_64-with-glibc2.10. 2 logical CPU cores, x86_64. -----. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1590
https://github.com/scverse/scanpy/issues/1590:482,performance,error,error,482,"Bug when running PCA in chunked mode; - [x] I have checked that this issue has not already been reported. - [ ] I have confirmed this bug exists on the latest version of scanpy. - [x] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. ```python. sc.tl.pca(adata, svd_solver='arpack', use_highly_variable=True, chunked=True, chunk_size=1000). ```. ```pytb. TypeError: unsupported operand type(s) for -: 'datetime.datetime' and 'int'. ```. I believe the error is due overwriting of `start` variable. It is declared in [line 116](https://github.com/theislab/scanpy/blob/master/scanpy/preprocessing/_pca.py#L116) but is overwritten by loop variable of same name in [line 172](https://github.com/theislab/scanpy/blob/master/scanpy/preprocessing/_pca.py#L172). #### Versions. <details>. -----. anndata 0.7.5. scanpy 1.6.0. sinfo 0.3.1. -----. PIL 8.1.0. anndata 0.7.5. asciitree NA. cffi 1.14.3. cloudpickle 1.6.0. constants NA. cycler 0.10.0. cython_runtime NA. dask 2020.12.0. dateutil 2.8.1. fasteners NA. get_version 2.1. h5py 3.1.0. highs_wrapper NA. igraph 0.8.3. joblib 1.0.0. kiwisolver 1.3.1. legacy_api_wrap 1.2. leidenalg 0.8.3. llvmlite 0.35.0. matplotlib 3.3.3. mpl_toolkits NA. msgpack 1.0.2. natsort 7.1.0. numba 0.52.0. numcodecs 0.7.2. numexpr 2.7.2. numpy 1.19.4. packaging 20.8. pandas 1.2.0. pkg_resources NA. psutil 5.8.0. pyparsing 2.4.7. pytz 2020.5. scanpy 1.6.0. scipy 1.6.0. setuptools_scm NA. sinfo 0.3.1. six 1.15.0. sklearn 0.24.0. sparse 0.11.2. tables 3.6.1. tblib 1.7.0. texttable 1.6.3. tlz 0.11.1. toolz 0.11.1. typing_extensions NA. yaml 5.3.1. zarr 2.6.1. -----. Python 3.8.5 (default, Sep 4 2020, 07:30:14) [GCC 7.3.0]. Linux-3.10.0-1062.4.1.el7.x86_64-x86_64-with-glibc2.10. 2 logical CPU cores, x86_64. -----. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1590
https://github.com/scverse/scanpy/issues/1590:1747,performance,CPU,CPU,1747,"Bug when running PCA in chunked mode; - [x] I have checked that this issue has not already been reported. - [ ] I have confirmed this bug exists on the latest version of scanpy. - [x] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. ```python. sc.tl.pca(adata, svd_solver='arpack', use_highly_variable=True, chunked=True, chunk_size=1000). ```. ```pytb. TypeError: unsupported operand type(s) for -: 'datetime.datetime' and 'int'. ```. I believe the error is due overwriting of `start` variable. It is declared in [line 116](https://github.com/theislab/scanpy/blob/master/scanpy/preprocessing/_pca.py#L116) but is overwritten by loop variable of same name in [line 172](https://github.com/theislab/scanpy/blob/master/scanpy/preprocessing/_pca.py#L172). #### Versions. <details>. -----. anndata 0.7.5. scanpy 1.6.0. sinfo 0.3.1. -----. PIL 8.1.0. anndata 0.7.5. asciitree NA. cffi 1.14.3. cloudpickle 1.6.0. constants NA. cycler 0.10.0. cython_runtime NA. dask 2020.12.0. dateutil 2.8.1. fasteners NA. get_version 2.1. h5py 3.1.0. highs_wrapper NA. igraph 0.8.3. joblib 1.0.0. kiwisolver 1.3.1. legacy_api_wrap 1.2. leidenalg 0.8.3. llvmlite 0.35.0. matplotlib 3.3.3. mpl_toolkits NA. msgpack 1.0.2. natsort 7.1.0. numba 0.52.0. numcodecs 0.7.2. numexpr 2.7.2. numpy 1.19.4. packaging 20.8. pandas 1.2.0. pkg_resources NA. psutil 5.8.0. pyparsing 2.4.7. pytz 2020.5. scanpy 1.6.0. scipy 1.6.0. setuptools_scm NA. sinfo 0.3.1. six 1.15.0. sklearn 0.24.0. sparse 0.11.2. tables 3.6.1. tblib 1.7.0. texttable 1.6.3. tlz 0.11.1. toolz 0.11.1. typing_extensions NA. yaml 5.3.1. zarr 2.6.1. -----. Python 3.8.5 (default, Sep 4 2020, 07:30:14) [GCC 7.3.0]. Linux-3.10.0-1062.4.1.el7.x86_64-x86_64-with-glibc2.10. 2 logical CPU cores, x86_64. -----. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1590
https://github.com/scverse/scanpy/issues/1590:482,safety,error,error,482,"Bug when running PCA in chunked mode; - [x] I have checked that this issue has not already been reported. - [ ] I have confirmed this bug exists on the latest version of scanpy. - [x] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. ```python. sc.tl.pca(adata, svd_solver='arpack', use_highly_variable=True, chunked=True, chunk_size=1000). ```. ```pytb. TypeError: unsupported operand type(s) for -: 'datetime.datetime' and 'int'. ```. I believe the error is due overwriting of `start` variable. It is declared in [line 116](https://github.com/theislab/scanpy/blob/master/scanpy/preprocessing/_pca.py#L116) but is overwritten by loop variable of same name in [line 172](https://github.com/theislab/scanpy/blob/master/scanpy/preprocessing/_pca.py#L172). #### Versions. <details>. -----. anndata 0.7.5. scanpy 1.6.0. sinfo 0.3.1. -----. PIL 8.1.0. anndata 0.7.5. asciitree NA. cffi 1.14.3. cloudpickle 1.6.0. constants NA. cycler 0.10.0. cython_runtime NA. dask 2020.12.0. dateutil 2.8.1. fasteners NA. get_version 2.1. h5py 3.1.0. highs_wrapper NA. igraph 0.8.3. joblib 1.0.0. kiwisolver 1.3.1. legacy_api_wrap 1.2. leidenalg 0.8.3. llvmlite 0.35.0. matplotlib 3.3.3. mpl_toolkits NA. msgpack 1.0.2. natsort 7.1.0. numba 0.52.0. numcodecs 0.7.2. numexpr 2.7.2. numpy 1.19.4. packaging 20.8. pandas 1.2.0. pkg_resources NA. psutil 5.8.0. pyparsing 2.4.7. pytz 2020.5. scanpy 1.6.0. scipy 1.6.0. setuptools_scm NA. sinfo 0.3.1. six 1.15.0. sklearn 0.24.0. sparse 0.11.2. tables 3.6.1. tblib 1.7.0. texttable 1.6.3. tlz 0.11.1. toolz 0.11.1. typing_extensions NA. yaml 5.3.1. zarr 2.6.1. -----. Python 3.8.5 (default, Sep 4 2020, 07:30:14) [GCC 7.3.0]. Linux-3.10.0-1062.4.1.el7.x86_64-x86_64-with-glibc2.10. 2 logical CPU cores, x86_64. -----. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1590
https://github.com/scverse/scanpy/issues/1590:1739,safety,log,logical,1739,"Bug when running PCA in chunked mode; - [x] I have checked that this issue has not already been reported. - [ ] I have confirmed this bug exists on the latest version of scanpy. - [x] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. ```python. sc.tl.pca(adata, svd_solver='arpack', use_highly_variable=True, chunked=True, chunk_size=1000). ```. ```pytb. TypeError: unsupported operand type(s) for -: 'datetime.datetime' and 'int'. ```. I believe the error is due overwriting of `start` variable. It is declared in [line 116](https://github.com/theislab/scanpy/blob/master/scanpy/preprocessing/_pca.py#L116) but is overwritten by loop variable of same name in [line 172](https://github.com/theislab/scanpy/blob/master/scanpy/preprocessing/_pca.py#L172). #### Versions. <details>. -----. anndata 0.7.5. scanpy 1.6.0. sinfo 0.3.1. -----. PIL 8.1.0. anndata 0.7.5. asciitree NA. cffi 1.14.3. cloudpickle 1.6.0. constants NA. cycler 0.10.0. cython_runtime NA. dask 2020.12.0. dateutil 2.8.1. fasteners NA. get_version 2.1. h5py 3.1.0. highs_wrapper NA. igraph 0.8.3. joblib 1.0.0. kiwisolver 1.3.1. legacy_api_wrap 1.2. leidenalg 0.8.3. llvmlite 0.35.0. matplotlib 3.3.3. mpl_toolkits NA. msgpack 1.0.2. natsort 7.1.0. numba 0.52.0. numcodecs 0.7.2. numexpr 2.7.2. numpy 1.19.4. packaging 20.8. pandas 1.2.0. pkg_resources NA. psutil 5.8.0. pyparsing 2.4.7. pytz 2020.5. scanpy 1.6.0. scipy 1.6.0. setuptools_scm NA. sinfo 0.3.1. six 1.15.0. sklearn 0.24.0. sparse 0.11.2. tables 3.6.1. tblib 1.7.0. texttable 1.6.3. tlz 0.11.1. toolz 0.11.1. typing_extensions NA. yaml 5.3.1. zarr 2.6.1. -----. Python 3.8.5 (default, Sep 4 2020, 07:30:14) [GCC 7.3.0]. Linux-3.10.0-1062.4.1.el7.x86_64-x86_64-with-glibc2.10. 2 logical CPU cores, x86_64. -----. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1590
https://github.com/scverse/scanpy/issues/1590:1739,security,log,logical,1739,"Bug when running PCA in chunked mode; - [x] I have checked that this issue has not already been reported. - [ ] I have confirmed this bug exists on the latest version of scanpy. - [x] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. ```python. sc.tl.pca(adata, svd_solver='arpack', use_highly_variable=True, chunked=True, chunk_size=1000). ```. ```pytb. TypeError: unsupported operand type(s) for -: 'datetime.datetime' and 'int'. ```. I believe the error is due overwriting of `start` variable. It is declared in [line 116](https://github.com/theislab/scanpy/blob/master/scanpy/preprocessing/_pca.py#L116) but is overwritten by loop variable of same name in [line 172](https://github.com/theislab/scanpy/blob/master/scanpy/preprocessing/_pca.py#L172). #### Versions. <details>. -----. anndata 0.7.5. scanpy 1.6.0. sinfo 0.3.1. -----. PIL 8.1.0. anndata 0.7.5. asciitree NA. cffi 1.14.3. cloudpickle 1.6.0. constants NA. cycler 0.10.0. cython_runtime NA. dask 2020.12.0. dateutil 2.8.1. fasteners NA. get_version 2.1. h5py 3.1.0. highs_wrapper NA. igraph 0.8.3. joblib 1.0.0. kiwisolver 1.3.1. legacy_api_wrap 1.2. leidenalg 0.8.3. llvmlite 0.35.0. matplotlib 3.3.3. mpl_toolkits NA. msgpack 1.0.2. natsort 7.1.0. numba 0.52.0. numcodecs 0.7.2. numexpr 2.7.2. numpy 1.19.4. packaging 20.8. pandas 1.2.0. pkg_resources NA. psutil 5.8.0. pyparsing 2.4.7. pytz 2020.5. scanpy 1.6.0. scipy 1.6.0. setuptools_scm NA. sinfo 0.3.1. six 1.15.0. sklearn 0.24.0. sparse 0.11.2. tables 3.6.1. tblib 1.7.0. texttable 1.6.3. tlz 0.11.1. toolz 0.11.1. typing_extensions NA. yaml 5.3.1. zarr 2.6.1. -----. Python 3.8.5 (default, Sep 4 2020, 07:30:14) [GCC 7.3.0]. Linux-3.10.0-1062.4.1.el7.x86_64-x86_64-with-glibc2.10. 2 logical CPU cores, x86_64. -----. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1590
https://github.com/scverse/scanpy/issues/1590:1739,testability,log,logical,1739,"Bug when running PCA in chunked mode; - [x] I have checked that this issue has not already been reported. - [ ] I have confirmed this bug exists on the latest version of scanpy. - [x] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. ```python. sc.tl.pca(adata, svd_solver='arpack', use_highly_variable=True, chunked=True, chunk_size=1000). ```. ```pytb. TypeError: unsupported operand type(s) for -: 'datetime.datetime' and 'int'. ```. I believe the error is due overwriting of `start` variable. It is declared in [line 116](https://github.com/theislab/scanpy/blob/master/scanpy/preprocessing/_pca.py#L116) but is overwritten by loop variable of same name in [line 172](https://github.com/theislab/scanpy/blob/master/scanpy/preprocessing/_pca.py#L172). #### Versions. <details>. -----. anndata 0.7.5. scanpy 1.6.0. sinfo 0.3.1. -----. PIL 8.1.0. anndata 0.7.5. asciitree NA. cffi 1.14.3. cloudpickle 1.6.0. constants NA. cycler 0.10.0. cython_runtime NA. dask 2020.12.0. dateutil 2.8.1. fasteners NA. get_version 2.1. h5py 3.1.0. highs_wrapper NA. igraph 0.8.3. joblib 1.0.0. kiwisolver 1.3.1. legacy_api_wrap 1.2. leidenalg 0.8.3. llvmlite 0.35.0. matplotlib 3.3.3. mpl_toolkits NA. msgpack 1.0.2. natsort 7.1.0. numba 0.52.0. numcodecs 0.7.2. numexpr 2.7.2. numpy 1.19.4. packaging 20.8. pandas 1.2.0. pkg_resources NA. psutil 5.8.0. pyparsing 2.4.7. pytz 2020.5. scanpy 1.6.0. scipy 1.6.0. setuptools_scm NA. sinfo 0.3.1. six 1.15.0. sklearn 0.24.0. sparse 0.11.2. tables 3.6.1. tblib 1.7.0. texttable 1.6.3. tlz 0.11.1. toolz 0.11.1. typing_extensions NA. yaml 5.3.1. zarr 2.6.1. -----. Python 3.8.5 (default, Sep 4 2020, 07:30:14) [GCC 7.3.0]. Linux-3.10.0-1062.4.1.el7.x86_64-x86_64-with-glibc2.10. 2 logical CPU cores, x86_64. -----. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1590
https://github.com/scverse/scanpy/issues/1590:119,usability,confirm,confirmed,119,"Bug when running PCA in chunked mode; - [x] I have checked that this issue has not already been reported. - [ ] I have confirmed this bug exists on the latest version of scanpy. - [x] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. ```python. sc.tl.pca(adata, svd_solver='arpack', use_highly_variable=True, chunked=True, chunk_size=1000). ```. ```pytb. TypeError: unsupported operand type(s) for -: 'datetime.datetime' and 'int'. ```. I believe the error is due overwriting of `start` variable. It is declared in [line 116](https://github.com/theislab/scanpy/blob/master/scanpy/preprocessing/_pca.py#L116) but is overwritten by loop variable of same name in [line 172](https://github.com/theislab/scanpy/blob/master/scanpy/preprocessing/_pca.py#L172). #### Versions. <details>. -----. anndata 0.7.5. scanpy 1.6.0. sinfo 0.3.1. -----. PIL 8.1.0. anndata 0.7.5. asciitree NA. cffi 1.14.3. cloudpickle 1.6.0. constants NA. cycler 0.10.0. cython_runtime NA. dask 2020.12.0. dateutil 2.8.1. fasteners NA. get_version 2.1. h5py 3.1.0. highs_wrapper NA. igraph 0.8.3. joblib 1.0.0. kiwisolver 1.3.1. legacy_api_wrap 1.2. leidenalg 0.8.3. llvmlite 0.35.0. matplotlib 3.3.3. mpl_toolkits NA. msgpack 1.0.2. natsort 7.1.0. numba 0.52.0. numcodecs 0.7.2. numexpr 2.7.2. numpy 1.19.4. packaging 20.8. pandas 1.2.0. pkg_resources NA. psutil 5.8.0. pyparsing 2.4.7. pytz 2020.5. scanpy 1.6.0. scipy 1.6.0. setuptools_scm NA. sinfo 0.3.1. six 1.15.0. sklearn 0.24.0. sparse 0.11.2. tables 3.6.1. tblib 1.7.0. texttable 1.6.3. tlz 0.11.1. toolz 0.11.1. typing_extensions NA. yaml 5.3.1. zarr 2.6.1. -----. Python 3.8.5 (default, Sep 4 2020, 07:30:14) [GCC 7.3.0]. Linux-3.10.0-1062.4.1.el7.x86_64-x86_64-with-glibc2.10. 2 logical CPU cores, x86_64. -----. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1590
https://github.com/scverse/scanpy/issues/1590:202,usability,confirm,confirmed,202,"Bug when running PCA in chunked mode; - [x] I have checked that this issue has not already been reported. - [ ] I have confirmed this bug exists on the latest version of scanpy. - [x] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. ```python. sc.tl.pca(adata, svd_solver='arpack', use_highly_variable=True, chunked=True, chunk_size=1000). ```. ```pytb. TypeError: unsupported operand type(s) for -: 'datetime.datetime' and 'int'. ```. I believe the error is due overwriting of `start` variable. It is declared in [line 116](https://github.com/theislab/scanpy/blob/master/scanpy/preprocessing/_pca.py#L116) but is overwritten by loop variable of same name in [line 172](https://github.com/theislab/scanpy/blob/master/scanpy/preprocessing/_pca.py#L172). #### Versions. <details>. -----. anndata 0.7.5. scanpy 1.6.0. sinfo 0.3.1. -----. PIL 8.1.0. anndata 0.7.5. asciitree NA. cffi 1.14.3. cloudpickle 1.6.0. constants NA. cycler 0.10.0. cython_runtime NA. dask 2020.12.0. dateutil 2.8.1. fasteners NA. get_version 2.1. h5py 3.1.0. highs_wrapper NA. igraph 0.8.3. joblib 1.0.0. kiwisolver 1.3.1. legacy_api_wrap 1.2. leidenalg 0.8.3. llvmlite 0.35.0. matplotlib 3.3.3. mpl_toolkits NA. msgpack 1.0.2. natsort 7.1.0. numba 0.52.0. numcodecs 0.7.2. numexpr 2.7.2. numpy 1.19.4. packaging 20.8. pandas 1.2.0. pkg_resources NA. psutil 5.8.0. pyparsing 2.4.7. pytz 2020.5. scanpy 1.6.0. scipy 1.6.0. setuptools_scm NA. sinfo 0.3.1. six 1.15.0. sklearn 0.24.0. sparse 0.11.2. tables 3.6.1. tblib 1.7.0. texttable 1.6.3. tlz 0.11.1. toolz 0.11.1. typing_extensions NA. yaml 5.3.1. zarr 2.6.1. -----. Python 3.8.5 (default, Sep 4 2020, 07:30:14) [GCC 7.3.0]. Linux-3.10.0-1062.4.1.el7.x86_64-x86_64-with-glibc2.10. 2 logical CPU cores, x86_64. -----. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1590
https://github.com/scverse/scanpy/issues/1590:482,usability,error,error,482,"Bug when running PCA in chunked mode; - [x] I have checked that this issue has not already been reported. - [ ] I have confirmed this bug exists on the latest version of scanpy. - [x] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. ```python. sc.tl.pca(adata, svd_solver='arpack', use_highly_variable=True, chunked=True, chunk_size=1000). ```. ```pytb. TypeError: unsupported operand type(s) for -: 'datetime.datetime' and 'int'. ```. I believe the error is due overwriting of `start` variable. It is declared in [line 116](https://github.com/theislab/scanpy/blob/master/scanpy/preprocessing/_pca.py#L116) but is overwritten by loop variable of same name in [line 172](https://github.com/theislab/scanpy/blob/master/scanpy/preprocessing/_pca.py#L172). #### Versions. <details>. -----. anndata 0.7.5. scanpy 1.6.0. sinfo 0.3.1. -----. PIL 8.1.0. anndata 0.7.5. asciitree NA. cffi 1.14.3. cloudpickle 1.6.0. constants NA. cycler 0.10.0. cython_runtime NA. dask 2020.12.0. dateutil 2.8.1. fasteners NA. get_version 2.1. h5py 3.1.0. highs_wrapper NA. igraph 0.8.3. joblib 1.0.0. kiwisolver 1.3.1. legacy_api_wrap 1.2. leidenalg 0.8.3. llvmlite 0.35.0. matplotlib 3.3.3. mpl_toolkits NA. msgpack 1.0.2. natsort 7.1.0. numba 0.52.0. numcodecs 0.7.2. numexpr 2.7.2. numpy 1.19.4. packaging 20.8. pandas 1.2.0. pkg_resources NA. psutil 5.8.0. pyparsing 2.4.7. pytz 2020.5. scanpy 1.6.0. scipy 1.6.0. setuptools_scm NA. sinfo 0.3.1. six 1.15.0. sklearn 0.24.0. sparse 0.11.2. tables 3.6.1. tblib 1.7.0. texttable 1.6.3. tlz 0.11.1. toolz 0.11.1. typing_extensions NA. yaml 5.3.1. zarr 2.6.1. -----. Python 3.8.5 (default, Sep 4 2020, 07:30:14) [GCC 7.3.0]. Linux-3.10.0-1062.4.1.el7.x86_64-x86_64-with-glibc2.10. 2 logical CPU cores, x86_64. -----. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1590
https://github.com/scverse/scanpy/issues/1590:1556,usability,tool,toolz,1556,"Bug when running PCA in chunked mode; - [x] I have checked that this issue has not already been reported. - [ ] I have confirmed this bug exists on the latest version of scanpy. - [x] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. ```python. sc.tl.pca(adata, svd_solver='arpack', use_highly_variable=True, chunked=True, chunk_size=1000). ```. ```pytb. TypeError: unsupported operand type(s) for -: 'datetime.datetime' and 'int'. ```. I believe the error is due overwriting of `start` variable. It is declared in [line 116](https://github.com/theislab/scanpy/blob/master/scanpy/preprocessing/_pca.py#L116) but is overwritten by loop variable of same name in [line 172](https://github.com/theislab/scanpy/blob/master/scanpy/preprocessing/_pca.py#L172). #### Versions. <details>. -----. anndata 0.7.5. scanpy 1.6.0. sinfo 0.3.1. -----. PIL 8.1.0. anndata 0.7.5. asciitree NA. cffi 1.14.3. cloudpickle 1.6.0. constants NA. cycler 0.10.0. cython_runtime NA. dask 2020.12.0. dateutil 2.8.1. fasteners NA. get_version 2.1. h5py 3.1.0. highs_wrapper NA. igraph 0.8.3. joblib 1.0.0. kiwisolver 1.3.1. legacy_api_wrap 1.2. leidenalg 0.8.3. llvmlite 0.35.0. matplotlib 3.3.3. mpl_toolkits NA. msgpack 1.0.2. natsort 7.1.0. numba 0.52.0. numcodecs 0.7.2. numexpr 2.7.2. numpy 1.19.4. packaging 20.8. pandas 1.2.0. pkg_resources NA. psutil 5.8.0. pyparsing 2.4.7. pytz 2020.5. scanpy 1.6.0. scipy 1.6.0. setuptools_scm NA. sinfo 0.3.1. six 1.15.0. sklearn 0.24.0. sparse 0.11.2. tables 3.6.1. tblib 1.7.0. texttable 1.6.3. tlz 0.11.1. toolz 0.11.1. typing_extensions NA. yaml 5.3.1. zarr 2.6.1. -----. Python 3.8.5 (default, Sep 4 2020, 07:30:14) [GCC 7.3.0]. Linux-3.10.0-1062.4.1.el7.x86_64-x86_64-with-glibc2.10. 2 logical CPU cores, x86_64. -----. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1590
https://github.com/scverse/scanpy/issues/1591:1844,availability,sli,slightly,1844,"e left (fig. 1). sc.pl.heatmap(a, var_names=a.var_names[:5], groupby=""foo"", swap_axes=True) # see the tb (if there's only 1 category, in the group, it doesn't crash). a.X[:16, :] = 10 + np.random.RandomState(42).normal(size=(16, 50)). a.X[16:32, :] = -10 + np.random.RandomState(42).normal(size=(16, 50)). a.obs['foo'].iloc[:16, :] = 0. a.obs['foo'].iloc[16:32, :] = 1. a.obs['foo'].iloc[32:, :] = 2. # wrong label-color mapping. sc.pl.heatmap(a, var_names=a.var_names[:50], groupby=""foo"", swap_axes=False, dendrogram=True) # fig. 2. sc.pl.heatmap(a, var_names=a.var_names[:50], groupby=""foo"", swap_axes=False, dendrogram=False) # fig. 3. # but not when there are colors in `.uns. a.uns['foo_colors'] = [(1, 0, 0), (0, 1, 0), (0, 0, 1)]. sc.pl.heatmap(a, var_names=a.var_names[:50], groupby=""foo"", swap_axes=False, dendrogram=True) # fig. 4. sc.pl.heatmap(a, var_names=a.var_names[:50], groupby=""foo"", swap_axes=False, dendrogram=False) # fig. 5. # xlabels are not centered + horizntal lines are slightly shifted downwards (really have to zoom in). sc.pl.heatmap(a, var_names=a.var_names, groupby=""foo"", swap_axes=False, figsize=(50, 50)). ```. If you look closely (e.g. between fig. 2 and fig.3, or fig. 4 and fig. 5), the within group order is also broken. ```pytb. TypeError Traceback (most recent call last). <ipython-input-56-f1ba710dac43> in <module>. 9 . 10 sc.pl.heatmap(a, var_names=a.var_names[:5], groupby=None, swap_axes=True) # is shifted to the left (fig. 1). ---> 11 sc.pl.heatmap(a, var_names=a.var_names[:5], groupby=""foo"", swap_axes=True) . 12 . 13 a.X[:16, :] = 0. ~/.miniconda3/envs/cellrank/lib/python3.8/site-packages/scanpy/plotting/_anndata.py in heatmap(adata, var_names, groupby, use_raw, log, num_categories, dendrogram, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, standard_scale, swap_axes, show_gene_labels, show, save, figsize, **kwds). 1220 groupby_cmap,. 1221 norm,. -> 1222 ) = _plot_categories_as_colorblocks(. 1223 groupby_ax, ob",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1591
https://github.com/scverse/scanpy/issues/1591:1861,availability,down,downwards,1861,"c.pl.heatmap(a, var_names=a.var_names[:5], groupby=""foo"", swap_axes=True) # see the tb (if there's only 1 category, in the group, it doesn't crash). a.X[:16, :] = 10 + np.random.RandomState(42).normal(size=(16, 50)). a.X[16:32, :] = -10 + np.random.RandomState(42).normal(size=(16, 50)). a.obs['foo'].iloc[:16, :] = 0. a.obs['foo'].iloc[16:32, :] = 1. a.obs['foo'].iloc[32:, :] = 2. # wrong label-color mapping. sc.pl.heatmap(a, var_names=a.var_names[:50], groupby=""foo"", swap_axes=False, dendrogram=True) # fig. 2. sc.pl.heatmap(a, var_names=a.var_names[:50], groupby=""foo"", swap_axes=False, dendrogram=False) # fig. 3. # but not when there are colors in `.uns. a.uns['foo_colors'] = [(1, 0, 0), (0, 1, 0), (0, 0, 1)]. sc.pl.heatmap(a, var_names=a.var_names[:50], groupby=""foo"", swap_axes=False, dendrogram=True) # fig. 4. sc.pl.heatmap(a, var_names=a.var_names[:50], groupby=""foo"", swap_axes=False, dendrogram=False) # fig. 5. # xlabels are not centered + horizntal lines are slightly shifted downwards (really have to zoom in). sc.pl.heatmap(a, var_names=a.var_names, groupby=""foo"", swap_axes=False, figsize=(50, 50)). ```. If you look closely (e.g. between fig. 2 and fig.3, or fig. 4 and fig. 5), the within group order is also broken. ```pytb. TypeError Traceback (most recent call last). <ipython-input-56-f1ba710dac43> in <module>. 9 . 10 sc.pl.heatmap(a, var_names=a.var_names[:5], groupby=None, swap_axes=True) # is shifted to the left (fig. 1). ---> 11 sc.pl.heatmap(a, var_names=a.var_names[:5], groupby=""foo"", swap_axes=True) . 12 . 13 a.X[:16, :] = 0. ~/.miniconda3/envs/cellrank/lib/python3.8/site-packages/scanpy/plotting/_anndata.py in heatmap(adata, var_names, groupby, use_raw, log, num_categories, dendrogram, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, standard_scale, swap_axes, show_gene_labels, show, save, figsize, **kwds). 1220 groupby_cmap,. 1221 norm,. -> 1222 ) = _plot_categories_as_colorblocks(. 1223 groupby_ax, obs_tidy, colors=gro",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1591
https://github.com/scverse/scanpy/issues/1591:159,deployability,version,version,159,"Heatmap inconsistent order/swap_axes; - [x] I have checked that this issue has not already been reported. - [x] I have confirmed this bug exists on the latest version of scanpy. - [x] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. ### Minimal code sample (that we can copy&paste without having any data). ```python. import numpy as np. import pandas as pd. import scanpy as sc. from anndata import AnnData. a = AnnData(np.random.RandomState(42).normal(size=(50, 50)),. obs={""foo"": pd.Categorical(np.random.choice([0, 1, 2], size=50))}). sc.pl.heatmap(a, var_names=a.var_names[:5], groupby=None, swap_axes=True) # is shifted to the left (fig. 1). sc.pl.heatmap(a, var_names=a.var_names[:5], groupby=""foo"", swap_axes=True) # see the tb (if there's only 1 category, in the group, it doesn't crash). a.X[:16, :] = 10 + np.random.RandomState(42).normal(size=(16, 50)). a.X[16:32, :] = -10 + np.random.RandomState(42).normal(size=(16, 50)). a.obs['foo'].iloc[:16, :] = 0. a.obs['foo'].iloc[16:32, :] = 1. a.obs['foo'].iloc[32:, :] = 2. # wrong label-color mapping. sc.pl.heatmap(a, var_names=a.var_names[:50], groupby=""foo"", swap_axes=False, dendrogram=True) # fig. 2. sc.pl.heatmap(a, var_names=a.var_names[:50], groupby=""foo"", swap_axes=False, dendrogram=False) # fig. 3. # but not when there are colors in `.uns. a.uns['foo_colors'] = [(1, 0, 0), (0, 1, 0), (0, 0, 1)]. sc.pl.heatmap(a, var_names=a.var_names[:50], groupby=""foo"", swap_axes=False, dendrogram=True) # fig. 4. sc.pl.heatmap(a, var_names=a.var_names[:50], groupby=""foo"", swap_axes=False, dendrogram=False) # fig. 5. # xlabels are not centered + horizntal lines are slightly shifted downwards (really have to zoom in). sc.pl.heatmap(a, var_names=a.var_names, groupby=""foo"", swap_axes=False, figsize=(50, 50)). ```. If you ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1591
https://github.com/scverse/scanpy/issues/1591:2197,deployability,modul,module,2197,"oc[16:32, :] = 1. a.obs['foo'].iloc[32:, :] = 2. # wrong label-color mapping. sc.pl.heatmap(a, var_names=a.var_names[:50], groupby=""foo"", swap_axes=False, dendrogram=True) # fig. 2. sc.pl.heatmap(a, var_names=a.var_names[:50], groupby=""foo"", swap_axes=False, dendrogram=False) # fig. 3. # but not when there are colors in `.uns. a.uns['foo_colors'] = [(1, 0, 0), (0, 1, 0), (0, 0, 1)]. sc.pl.heatmap(a, var_names=a.var_names[:50], groupby=""foo"", swap_axes=False, dendrogram=True) # fig. 4. sc.pl.heatmap(a, var_names=a.var_names[:50], groupby=""foo"", swap_axes=False, dendrogram=False) # fig. 5. # xlabels are not centered + horizntal lines are slightly shifted downwards (really have to zoom in). sc.pl.heatmap(a, var_names=a.var_names, groupby=""foo"", swap_axes=False, figsize=(50, 50)). ```. If you look closely (e.g. between fig. 2 and fig.3, or fig. 4 and fig. 5), the within group order is also broken. ```pytb. TypeError Traceback (most recent call last). <ipython-input-56-f1ba710dac43> in <module>. 9 . 10 sc.pl.heatmap(a, var_names=a.var_names[:5], groupby=None, swap_axes=True) # is shifted to the left (fig. 1). ---> 11 sc.pl.heatmap(a, var_names=a.var_names[:5], groupby=""foo"", swap_axes=True) . 12 . 13 a.X[:16, :] = 0. ~/.miniconda3/envs/cellrank/lib/python3.8/site-packages/scanpy/plotting/_anndata.py in heatmap(adata, var_names, groupby, use_raw, log, num_categories, dendrogram, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, standard_scale, swap_axes, show_gene_labels, show, save, figsize, **kwds). 1220 groupby_cmap,. 1221 norm,. -> 1222 ) = _plot_categories_as_colorblocks(. 1223 groupby_ax, obs_tidy, colors=groupby_colors, orientation='bottom'. 1224 ). ~/.miniconda3/envs/cellrank/lib/python3.8/site-packages/scanpy/plotting/_anndata.py in _plot_categories_as_colorblocks(groupby_ax, obs_tidy, colors, orientation, cmap_name). 2361 if len(labels) > 1:. 2362 groupby_ax.set_xticks(ticks). -> 2363 if max([len(x) for x in labels]) < 3:. 2364 # if",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1591
https://github.com/scverse/scanpy/issues/1591:2563,deployability,log,log,2563,", 1, 0), (0, 0, 1)]. sc.pl.heatmap(a, var_names=a.var_names[:50], groupby=""foo"", swap_axes=False, dendrogram=True) # fig. 4. sc.pl.heatmap(a, var_names=a.var_names[:50], groupby=""foo"", swap_axes=False, dendrogram=False) # fig. 5. # xlabels are not centered + horizntal lines are slightly shifted downwards (really have to zoom in). sc.pl.heatmap(a, var_names=a.var_names, groupby=""foo"", swap_axes=False, figsize=(50, 50)). ```. If you look closely (e.g. between fig. 2 and fig.3, or fig. 4 and fig. 5), the within group order is also broken. ```pytb. TypeError Traceback (most recent call last). <ipython-input-56-f1ba710dac43> in <module>. 9 . 10 sc.pl.heatmap(a, var_names=a.var_names[:5], groupby=None, swap_axes=True) # is shifted to the left (fig. 1). ---> 11 sc.pl.heatmap(a, var_names=a.var_names[:5], groupby=""foo"", swap_axes=True) . 12 . 13 a.X[:16, :] = 0. ~/.miniconda3/envs/cellrank/lib/python3.8/site-packages/scanpy/plotting/_anndata.py in heatmap(adata, var_names, groupby, use_raw, log, num_categories, dendrogram, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, standard_scale, swap_axes, show_gene_labels, show, save, figsize, **kwds). 1220 groupby_cmap,. 1221 norm,. -> 1222 ) = _plot_categories_as_colorblocks(. 1223 groupby_ax, obs_tidy, colors=groupby_colors, orientation='bottom'. 1224 ). ~/.miniconda3/envs/cellrank/lib/python3.8/site-packages/scanpy/plotting/_anndata.py in _plot_categories_as_colorblocks(groupby_ax, obs_tidy, colors, orientation, cmap_name). 2361 if len(labels) > 1:. 2362 groupby_ax.set_xticks(ticks). -> 2363 if max([len(x) for x in labels]) < 3:. 2364 # if the labels are small do not rotate them. 2365 rotation = 0. ~/.miniconda3/envs/cellrank/lib/python3.8/site-packages/scanpy/plotting/_anndata.py in <listcomp>(.0). 2361 if len(labels) > 1:. 2362 groupby_ax.set_xticks(ticks). -> 2363 if max([len(x) for x in labels]) < 3:. 2364 # if the labels are small do not rotate them. 2365 rotation = 0. TypeError: object of ty",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1591
https://github.com/scverse/scanpy/issues/1591:4320,deployability,Version,Versions,4320,plotting/_anndata.py in <listcomp>(.0). 2361 if len(labels) > 1:. 2362 groupby_ax.set_xticks(ticks). -> 2363 if max([len(x) for x in labels]) < 3:. 2364 # if the labels are small do not rotate them. 2365 rotation = 0. TypeError: object of type 'int' has no len(). ```. #### Figures. Fig. 1. ![f0](https://user-images.githubusercontent.com/46717574/104822526-6544e880-5843-11eb-8324-261e84eb45e4.png). Fig. 2. ![f1](https://user-images.githubusercontent.com/46717574/104822449-d637d080-5842-11eb-853d-bb115cd014aa.png). Fig. 3. ![f2](https://user-images.githubusercontent.com/46717574/104822452-d9cb5780-5842-11eb-8606-f4a9a5c97893.png). Fig. 4. ![f3](https://user-images.githubusercontent.com/46717574/104822453-dcc64800-5842-11eb-827c-90db0525d4d4.png). Fig. 5. ![f4](https://user-images.githubusercontent.com/46717574/104822455-dfc13880-5842-11eb-9286-655b2210b182.png). Fig. 6. ![f6](https://user-images.githubusercontent.com/46717574/104822796-958d8680-5845-11eb-82e2-4b30597c6722.png). #### Versions. <details>. -----. anndata 0.7.5. scanpy 1.7.0rc2.dev1+g2a123065. sinfo 0.3.1. -----. PIL 8.0.1. anndata 0.7.5. asciitree NA. backcall 0.2.0. cairo 1.20.0. cffi 1.14.4. cloudpickle 1.6.0. colorama 0.4.4. coverage 5.3. cycler 0.10.0. cython_runtime NA. dask 2020.12.0. dateutil 2.8.1. decorator 4.4.2. fasteners NA. future_fstrings NA. get_version 2.1. h5py 2.10.0. igraph 0.8.3. ipykernel 5.4.2. ipython_genutils 0.2.0. jedi 0.17.2. joblib 0.17.0. kiwisolver 1.3.1. legacy_api_wrap 1.2. leidenalg 0.8.1. llvmlite 0.34.0. louvain 0.6.1. matplotlib 3.3.3. monotonic NA. mpl_toolkits NA. msgpack 1.0.0. natsort 7.0.1. numba 0.51.2. numcodecs 0.7.2. numexpr 2.7.1. numpy 1.19.4. packaging 20.8. pandas 1.1.4. parso 0.7.1. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. prompt_toolkit 3.0.8. psutil 5.7.3. ptyprocess 0.6.0. pygments 2.7.3. pyparsing 2.4.7. pytz 2020.5. ruamel NA. scanpy 1.7.0rc2.dev1+g2a123065. scipy 1.5.4. setuptools_scm NA. sinfo 0.3.1. six 1.15.0. sklearn 0.23.2. sparse 0.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1591
https://github.com/scverse/scanpy/issues/1591:5737,deployability,log,logical,5737,"-bb115cd014aa.png). Fig. 3. ![f2](https://user-images.githubusercontent.com/46717574/104822452-d9cb5780-5842-11eb-8606-f4a9a5c97893.png). Fig. 4. ![f3](https://user-images.githubusercontent.com/46717574/104822453-dcc64800-5842-11eb-827c-90db0525d4d4.png). Fig. 5. ![f4](https://user-images.githubusercontent.com/46717574/104822455-dfc13880-5842-11eb-9286-655b2210b182.png). Fig. 6. ![f6](https://user-images.githubusercontent.com/46717574/104822796-958d8680-5845-11eb-82e2-4b30597c6722.png). #### Versions. <details>. -----. anndata 0.7.5. scanpy 1.7.0rc2.dev1+g2a123065. sinfo 0.3.1. -----. PIL 8.0.1. anndata 0.7.5. asciitree NA. backcall 0.2.0. cairo 1.20.0. cffi 1.14.4. cloudpickle 1.6.0. colorama 0.4.4. coverage 5.3. cycler 0.10.0. cython_runtime NA. dask 2020.12.0. dateutil 2.8.1. decorator 4.4.2. fasteners NA. future_fstrings NA. get_version 2.1. h5py 2.10.0. igraph 0.8.3. ipykernel 5.4.2. ipython_genutils 0.2.0. jedi 0.17.2. joblib 0.17.0. kiwisolver 1.3.1. legacy_api_wrap 1.2. leidenalg 0.8.1. llvmlite 0.34.0. louvain 0.6.1. matplotlib 3.3.3. monotonic NA. mpl_toolkits NA. msgpack 1.0.0. natsort 7.0.1. numba 0.51.2. numcodecs 0.7.2. numexpr 2.7.1. numpy 1.19.4. packaging 20.8. pandas 1.1.4. parso 0.7.1. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. prompt_toolkit 3.0.8. psutil 5.7.3. ptyprocess 0.6.0. pygments 2.7.3. pyparsing 2.4.7. pytz 2020.5. ruamel NA. scanpy 1.7.0rc2.dev1+g2a123065. scipy 1.5.4. setuptools_scm NA. sinfo 0.3.1. six 1.15.0. sklearn 0.23.2. sparse 0.11.2. sphinxcontrib NA. storemagic NA. tables 3.6.1. tblib 1.7.0. texttable 1.6.3. tlz 0.11.1. toolz 0.11.1. tornado 6.1. traitlets 5.0.5. typing_extensions NA. wcwidth 0.2.5. yaml 5.3.1. zarr 2.6.1. zmq 20.0.0. zope NA. -----. IPython 7.19.0. jupyter_client 6.1.7. jupyter_core 4.7.0. notebook 6.1.5. -----. Python 3.8.5 (default, Sep 4 2020, 07:30:14) [GCC 7.3.0]. Linux-5.10.0-1-amd64-x86_64-with-glibc2.10. 8 logical CPU cores. -----. Session information updated at 2021-01-16 21:28. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1591
https://github.com/scverse/scanpy/issues/1591:5783,deployability,updat,updated,5783,"-bb115cd014aa.png). Fig. 3. ![f2](https://user-images.githubusercontent.com/46717574/104822452-d9cb5780-5842-11eb-8606-f4a9a5c97893.png). Fig. 4. ![f3](https://user-images.githubusercontent.com/46717574/104822453-dcc64800-5842-11eb-827c-90db0525d4d4.png). Fig. 5. ![f4](https://user-images.githubusercontent.com/46717574/104822455-dfc13880-5842-11eb-9286-655b2210b182.png). Fig. 6. ![f6](https://user-images.githubusercontent.com/46717574/104822796-958d8680-5845-11eb-82e2-4b30597c6722.png). #### Versions. <details>. -----. anndata 0.7.5. scanpy 1.7.0rc2.dev1+g2a123065. sinfo 0.3.1. -----. PIL 8.0.1. anndata 0.7.5. asciitree NA. backcall 0.2.0. cairo 1.20.0. cffi 1.14.4. cloudpickle 1.6.0. colorama 0.4.4. coverage 5.3. cycler 0.10.0. cython_runtime NA. dask 2020.12.0. dateutil 2.8.1. decorator 4.4.2. fasteners NA. future_fstrings NA. get_version 2.1. h5py 2.10.0. igraph 0.8.3. ipykernel 5.4.2. ipython_genutils 0.2.0. jedi 0.17.2. joblib 0.17.0. kiwisolver 1.3.1. legacy_api_wrap 1.2. leidenalg 0.8.1. llvmlite 0.34.0. louvain 0.6.1. matplotlib 3.3.3. monotonic NA. mpl_toolkits NA. msgpack 1.0.0. natsort 7.0.1. numba 0.51.2. numcodecs 0.7.2. numexpr 2.7.1. numpy 1.19.4. packaging 20.8. pandas 1.1.4. parso 0.7.1. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. prompt_toolkit 3.0.8. psutil 5.7.3. ptyprocess 0.6.0. pygments 2.7.3. pyparsing 2.4.7. pytz 2020.5. ruamel NA. scanpy 1.7.0rc2.dev1+g2a123065. scipy 1.5.4. setuptools_scm NA. sinfo 0.3.1. six 1.15.0. sklearn 0.23.2. sparse 0.11.2. sphinxcontrib NA. storemagic NA. tables 3.6.1. tblib 1.7.0. texttable 1.6.3. tlz 0.11.1. toolz 0.11.1. tornado 6.1. traitlets 5.0.5. typing_extensions NA. wcwidth 0.2.5. yaml 5.3.1. zarr 2.6.1. zmq 20.0.0. zope NA. -----. IPython 7.19.0. jupyter_client 6.1.7. jupyter_core 4.7.0. notebook 6.1.5. -----. Python 3.8.5 (default, Sep 4 2020, 07:30:14) [GCC 7.3.0]. Linux-5.10.0-1-amd64-x86_64-with-glibc2.10. 8 logical CPU cores. -----. Session information updated at 2021-01-16 21:28. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1591
https://github.com/scverse/scanpy/issues/1591:0,energy efficiency,Heat,Heatmap,0,"Heatmap inconsistent order/swap_axes; - [x] I have checked that this issue has not already been reported. - [x] I have confirmed this bug exists on the latest version of scanpy. - [x] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. ### Minimal code sample (that we can copy&paste without having any data). ```python. import numpy as np. import pandas as pd. import scanpy as sc. from anndata import AnnData. a = AnnData(np.random.RandomState(42).normal(size=(50, 50)),. obs={""foo"": pd.Categorical(np.random.choice([0, 1, 2], size=50))}). sc.pl.heatmap(a, var_names=a.var_names[:5], groupby=None, swap_axes=True) # is shifted to the left (fig. 1). sc.pl.heatmap(a, var_names=a.var_names[:5], groupby=""foo"", swap_axes=True) # see the tb (if there's only 1 category, in the group, it doesn't crash). a.X[:16, :] = 10 + np.random.RandomState(42).normal(size=(16, 50)). a.X[16:32, :] = -10 + np.random.RandomState(42).normal(size=(16, 50)). a.obs['foo'].iloc[:16, :] = 0. a.obs['foo'].iloc[16:32, :] = 1. a.obs['foo'].iloc[32:, :] = 2. # wrong label-color mapping. sc.pl.heatmap(a, var_names=a.var_names[:50], groupby=""foo"", swap_axes=False, dendrogram=True) # fig. 2. sc.pl.heatmap(a, var_names=a.var_names[:50], groupby=""foo"", swap_axes=False, dendrogram=False) # fig. 3. # but not when there are colors in `.uns. a.uns['foo_colors'] = [(1, 0, 0), (0, 1, 0), (0, 0, 1)]. sc.pl.heatmap(a, var_names=a.var_names[:50], groupby=""foo"", swap_axes=False, dendrogram=True) # fig. 4. sc.pl.heatmap(a, var_names=a.var_names[:50], groupby=""foo"", swap_axes=False, dendrogram=False) # fig. 5. # xlabels are not centered + horizntal lines are slightly shifted downwards (really have to zoom in). sc.pl.heatmap(a, var_names=a.var_names, groupby=""foo"", swap_axes=False, figsize=(50, 50)). ```. If you ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1591
https://github.com/scverse/scanpy/issues/1591:762,energy efficiency,heat,heatmap,762,"Heatmap inconsistent order/swap_axes; - [x] I have checked that this issue has not already been reported. - [x] I have confirmed this bug exists on the latest version of scanpy. - [x] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. ### Minimal code sample (that we can copy&paste without having any data). ```python. import numpy as np. import pandas as pd. import scanpy as sc. from anndata import AnnData. a = AnnData(np.random.RandomState(42).normal(size=(50, 50)),. obs={""foo"": pd.Categorical(np.random.choice([0, 1, 2], size=50))}). sc.pl.heatmap(a, var_names=a.var_names[:5], groupby=None, swap_axes=True) # is shifted to the left (fig. 1). sc.pl.heatmap(a, var_names=a.var_names[:5], groupby=""foo"", swap_axes=True) # see the tb (if there's only 1 category, in the group, it doesn't crash). a.X[:16, :] = 10 + np.random.RandomState(42).normal(size=(16, 50)). a.X[16:32, :] = -10 + np.random.RandomState(42).normal(size=(16, 50)). a.obs['foo'].iloc[:16, :] = 0. a.obs['foo'].iloc[16:32, :] = 1. a.obs['foo'].iloc[32:, :] = 2. # wrong label-color mapping. sc.pl.heatmap(a, var_names=a.var_names[:50], groupby=""foo"", swap_axes=False, dendrogram=True) # fig. 2. sc.pl.heatmap(a, var_names=a.var_names[:50], groupby=""foo"", swap_axes=False, dendrogram=False) # fig. 3. # but not when there are colors in `.uns. a.uns['foo_colors'] = [(1, 0, 0), (0, 1, 0), (0, 0, 1)]. sc.pl.heatmap(a, var_names=a.var_names[:50], groupby=""foo"", swap_axes=False, dendrogram=True) # fig. 4. sc.pl.heatmap(a, var_names=a.var_names[:50], groupby=""foo"", swap_axes=False, dendrogram=False) # fig. 5. # xlabels are not centered + horizntal lines are slightly shifted downwards (really have to zoom in). sc.pl.heatmap(a, var_names=a.var_names, groupby=""foo"", swap_axes=False, figsize=(50, 50)). ```. If you ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1591
https://github.com/scverse/scanpy/issues/1591:871,energy efficiency,heat,heatmap,871,"Heatmap inconsistent order/swap_axes; - [x] I have checked that this issue has not already been reported. - [x] I have confirmed this bug exists on the latest version of scanpy. - [x] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. ### Minimal code sample (that we can copy&paste without having any data). ```python. import numpy as np. import pandas as pd. import scanpy as sc. from anndata import AnnData. a = AnnData(np.random.RandomState(42).normal(size=(50, 50)),. obs={""foo"": pd.Categorical(np.random.choice([0, 1, 2], size=50))}). sc.pl.heatmap(a, var_names=a.var_names[:5], groupby=None, swap_axes=True) # is shifted to the left (fig. 1). sc.pl.heatmap(a, var_names=a.var_names[:5], groupby=""foo"", swap_axes=True) # see the tb (if there's only 1 category, in the group, it doesn't crash). a.X[:16, :] = 10 + np.random.RandomState(42).normal(size=(16, 50)). a.X[16:32, :] = -10 + np.random.RandomState(42).normal(size=(16, 50)). a.obs['foo'].iloc[:16, :] = 0. a.obs['foo'].iloc[16:32, :] = 1. a.obs['foo'].iloc[32:, :] = 2. # wrong label-color mapping. sc.pl.heatmap(a, var_names=a.var_names[:50], groupby=""foo"", swap_axes=False, dendrogram=True) # fig. 2. sc.pl.heatmap(a, var_names=a.var_names[:50], groupby=""foo"", swap_axes=False, dendrogram=False) # fig. 3. # but not when there are colors in `.uns. a.uns['foo_colors'] = [(1, 0, 0), (0, 1, 0), (0, 0, 1)]. sc.pl.heatmap(a, var_names=a.var_names[:50], groupby=""foo"", swap_axes=False, dendrogram=True) # fig. 4. sc.pl.heatmap(a, var_names=a.var_names[:50], groupby=""foo"", swap_axes=False, dendrogram=False) # fig. 5. # xlabels are not centered + horizntal lines are slightly shifted downwards (really have to zoom in). sc.pl.heatmap(a, var_names=a.var_names, groupby=""foo"", swap_axes=False, figsize=(50, 50)). ```. If you ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1591
https://github.com/scverse/scanpy/issues/1591:1284,energy efficiency,heat,heatmap,1284,"this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. ### Minimal code sample (that we can copy&paste without having any data). ```python. import numpy as np. import pandas as pd. import scanpy as sc. from anndata import AnnData. a = AnnData(np.random.RandomState(42).normal(size=(50, 50)),. obs={""foo"": pd.Categorical(np.random.choice([0, 1, 2], size=50))}). sc.pl.heatmap(a, var_names=a.var_names[:5], groupby=None, swap_axes=True) # is shifted to the left (fig. 1). sc.pl.heatmap(a, var_names=a.var_names[:5], groupby=""foo"", swap_axes=True) # see the tb (if there's only 1 category, in the group, it doesn't crash). a.X[:16, :] = 10 + np.random.RandomState(42).normal(size=(16, 50)). a.X[16:32, :] = -10 + np.random.RandomState(42).normal(size=(16, 50)). a.obs['foo'].iloc[:16, :] = 0. a.obs['foo'].iloc[16:32, :] = 1. a.obs['foo'].iloc[32:, :] = 2. # wrong label-color mapping. sc.pl.heatmap(a, var_names=a.var_names[:50], groupby=""foo"", swap_axes=False, dendrogram=True) # fig. 2. sc.pl.heatmap(a, var_names=a.var_names[:50], groupby=""foo"", swap_axes=False, dendrogram=False) # fig. 3. # but not when there are colors in `.uns. a.uns['foo_colors'] = [(1, 0, 0), (0, 1, 0), (0, 0, 1)]. sc.pl.heatmap(a, var_names=a.var_names[:50], groupby=""foo"", swap_axes=False, dendrogram=True) # fig. 4. sc.pl.heatmap(a, var_names=a.var_names[:50], groupby=""foo"", swap_axes=False, dendrogram=False) # fig. 5. # xlabels are not centered + horizntal lines are slightly shifted downwards (really have to zoom in). sc.pl.heatmap(a, var_names=a.var_names, groupby=""foo"", swap_axes=False, figsize=(50, 50)). ```. If you look closely (e.g. between fig. 2 and fig.3, or fig. 4 and fig. 5), the within group order is also broken. ```pytb. TypeError Traceback (most recent call last). <ipython-input-56-f1ba710dac43> in <module>. 9 . 10 sc.pl.heatmap(a, var_names=a.var_names[:5], groupby=None, swap_axes=True) #",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1591
https://github.com/scverse/scanpy/issues/1591:1388,energy efficiency,heat,heatmap,1388,"e the necessary information for us to reproduce your bug. ### Minimal code sample (that we can copy&paste without having any data). ```python. import numpy as np. import pandas as pd. import scanpy as sc. from anndata import AnnData. a = AnnData(np.random.RandomState(42).normal(size=(50, 50)),. obs={""foo"": pd.Categorical(np.random.choice([0, 1, 2], size=50))}). sc.pl.heatmap(a, var_names=a.var_names[:5], groupby=None, swap_axes=True) # is shifted to the left (fig. 1). sc.pl.heatmap(a, var_names=a.var_names[:5], groupby=""foo"", swap_axes=True) # see the tb (if there's only 1 category, in the group, it doesn't crash). a.X[:16, :] = 10 + np.random.RandomState(42).normal(size=(16, 50)). a.X[16:32, :] = -10 + np.random.RandomState(42).normal(size=(16, 50)). a.obs['foo'].iloc[:16, :] = 0. a.obs['foo'].iloc[16:32, :] = 1. a.obs['foo'].iloc[32:, :] = 2. # wrong label-color mapping. sc.pl.heatmap(a, var_names=a.var_names[:50], groupby=""foo"", swap_axes=False, dendrogram=True) # fig. 2. sc.pl.heatmap(a, var_names=a.var_names[:50], groupby=""foo"", swap_axes=False, dendrogram=False) # fig. 3. # but not when there are colors in `.uns. a.uns['foo_colors'] = [(1, 0, 0), (0, 1, 0), (0, 0, 1)]. sc.pl.heatmap(a, var_names=a.var_names[:50], groupby=""foo"", swap_axes=False, dendrogram=True) # fig. 4. sc.pl.heatmap(a, var_names=a.var_names[:50], groupby=""foo"", swap_axes=False, dendrogram=False) # fig. 5. # xlabels are not centered + horizntal lines are slightly shifted downwards (really have to zoom in). sc.pl.heatmap(a, var_names=a.var_names, groupby=""foo"", swap_axes=False, figsize=(50, 50)). ```. If you look closely (e.g. between fig. 2 and fig.3, or fig. 4 and fig. 5), the within group order is also broken. ```pytb. TypeError Traceback (most recent call last). <ipython-input-56-f1ba710dac43> in <module>. 9 . 10 sc.pl.heatmap(a, var_names=a.var_names[:5], groupby=None, swap_axes=True) # is shifted to the left (fig. 1). ---> 11 sc.pl.heatmap(a, var_names=a.var_names[:5], groupby=""foo"", swa",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1591
https://github.com/scverse/scanpy/issues/1591:1592,energy efficiency,heat,heatmap,1592," from anndata import AnnData. a = AnnData(np.random.RandomState(42).normal(size=(50, 50)),. obs={""foo"": pd.Categorical(np.random.choice([0, 1, 2], size=50))}). sc.pl.heatmap(a, var_names=a.var_names[:5], groupby=None, swap_axes=True) # is shifted to the left (fig. 1). sc.pl.heatmap(a, var_names=a.var_names[:5], groupby=""foo"", swap_axes=True) # see the tb (if there's only 1 category, in the group, it doesn't crash). a.X[:16, :] = 10 + np.random.RandomState(42).normal(size=(16, 50)). a.X[16:32, :] = -10 + np.random.RandomState(42).normal(size=(16, 50)). a.obs['foo'].iloc[:16, :] = 0. a.obs['foo'].iloc[16:32, :] = 1. a.obs['foo'].iloc[32:, :] = 2. # wrong label-color mapping. sc.pl.heatmap(a, var_names=a.var_names[:50], groupby=""foo"", swap_axes=False, dendrogram=True) # fig. 2. sc.pl.heatmap(a, var_names=a.var_names[:50], groupby=""foo"", swap_axes=False, dendrogram=False) # fig. 3. # but not when there are colors in `.uns. a.uns['foo_colors'] = [(1, 0, 0), (0, 1, 0), (0, 0, 1)]. sc.pl.heatmap(a, var_names=a.var_names[:50], groupby=""foo"", swap_axes=False, dendrogram=True) # fig. 4. sc.pl.heatmap(a, var_names=a.var_names[:50], groupby=""foo"", swap_axes=False, dendrogram=False) # fig. 5. # xlabels are not centered + horizntal lines are slightly shifted downwards (really have to zoom in). sc.pl.heatmap(a, var_names=a.var_names, groupby=""foo"", swap_axes=False, figsize=(50, 50)). ```. If you look closely (e.g. between fig. 2 and fig.3, or fig. 4 and fig. 5), the within group order is also broken. ```pytb. TypeError Traceback (most recent call last). <ipython-input-56-f1ba710dac43> in <module>. 9 . 10 sc.pl.heatmap(a, var_names=a.var_names[:5], groupby=None, swap_axes=True) # is shifted to the left (fig. 1). ---> 11 sc.pl.heatmap(a, var_names=a.var_names[:5], groupby=""foo"", swap_axes=True) . 12 . 13 a.X[:16, :] = 0. ~/.miniconda3/envs/cellrank/lib/python3.8/site-packages/scanpy/plotting/_anndata.py in heatmap(adata, var_names, groupby, use_raw, log, num_categories, dendrogram, ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1591
https://github.com/scverse/scanpy/issues/1591:1696,energy efficiency,heat,heatmap,1696,"pd.Categorical(np.random.choice([0, 1, 2], size=50))}). sc.pl.heatmap(a, var_names=a.var_names[:5], groupby=None, swap_axes=True) # is shifted to the left (fig. 1). sc.pl.heatmap(a, var_names=a.var_names[:5], groupby=""foo"", swap_axes=True) # see the tb (if there's only 1 category, in the group, it doesn't crash). a.X[:16, :] = 10 + np.random.RandomState(42).normal(size=(16, 50)). a.X[16:32, :] = -10 + np.random.RandomState(42).normal(size=(16, 50)). a.obs['foo'].iloc[:16, :] = 0. a.obs['foo'].iloc[16:32, :] = 1. a.obs['foo'].iloc[32:, :] = 2. # wrong label-color mapping. sc.pl.heatmap(a, var_names=a.var_names[:50], groupby=""foo"", swap_axes=False, dendrogram=True) # fig. 2. sc.pl.heatmap(a, var_names=a.var_names[:50], groupby=""foo"", swap_axes=False, dendrogram=False) # fig. 3. # but not when there are colors in `.uns. a.uns['foo_colors'] = [(1, 0, 0), (0, 1, 0), (0, 0, 1)]. sc.pl.heatmap(a, var_names=a.var_names[:50], groupby=""foo"", swap_axes=False, dendrogram=True) # fig. 4. sc.pl.heatmap(a, var_names=a.var_names[:50], groupby=""foo"", swap_axes=False, dendrogram=False) # fig. 5. # xlabels are not centered + horizntal lines are slightly shifted downwards (really have to zoom in). sc.pl.heatmap(a, var_names=a.var_names, groupby=""foo"", swap_axes=False, figsize=(50, 50)). ```. If you look closely (e.g. between fig. 2 and fig.3, or fig. 4 and fig. 5), the within group order is also broken. ```pytb. TypeError Traceback (most recent call last). <ipython-input-56-f1ba710dac43> in <module>. 9 . 10 sc.pl.heatmap(a, var_names=a.var_names[:5], groupby=None, swap_axes=True) # is shifted to the left (fig. 1). ---> 11 sc.pl.heatmap(a, var_names=a.var_names[:5], groupby=""foo"", swap_axes=True) . 12 . 13 a.X[:16, :] = 0. ~/.miniconda3/envs/cellrank/lib/python3.8/site-packages/scanpy/plotting/_anndata.py in heatmap(adata, var_names, groupby, use_raw, log, num_categories, dendrogram, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, standard_scale, swap_axe",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1591
https://github.com/scverse/scanpy/issues/1591:1903,energy efficiency,heat,heatmap,1903,", groupby=""foo"", swap_axes=True) # see the tb (if there's only 1 category, in the group, it doesn't crash). a.X[:16, :] = 10 + np.random.RandomState(42).normal(size=(16, 50)). a.X[16:32, :] = -10 + np.random.RandomState(42).normal(size=(16, 50)). a.obs['foo'].iloc[:16, :] = 0. a.obs['foo'].iloc[16:32, :] = 1. a.obs['foo'].iloc[32:, :] = 2. # wrong label-color mapping. sc.pl.heatmap(a, var_names=a.var_names[:50], groupby=""foo"", swap_axes=False, dendrogram=True) # fig. 2. sc.pl.heatmap(a, var_names=a.var_names[:50], groupby=""foo"", swap_axes=False, dendrogram=False) # fig. 3. # but not when there are colors in `.uns. a.uns['foo_colors'] = [(1, 0, 0), (0, 1, 0), (0, 0, 1)]. sc.pl.heatmap(a, var_names=a.var_names[:50], groupby=""foo"", swap_axes=False, dendrogram=True) # fig. 4. sc.pl.heatmap(a, var_names=a.var_names[:50], groupby=""foo"", swap_axes=False, dendrogram=False) # fig. 5. # xlabels are not centered + horizntal lines are slightly shifted downwards (really have to zoom in). sc.pl.heatmap(a, var_names=a.var_names, groupby=""foo"", swap_axes=False, figsize=(50, 50)). ```. If you look closely (e.g. between fig. 2 and fig.3, or fig. 4 and fig. 5), the within group order is also broken. ```pytb. TypeError Traceback (most recent call last). <ipython-input-56-f1ba710dac43> in <module>. 9 . 10 sc.pl.heatmap(a, var_names=a.var_names[:5], groupby=None, swap_axes=True) # is shifted to the left (fig. 1). ---> 11 sc.pl.heatmap(a, var_names=a.var_names[:5], groupby=""foo"", swap_axes=True) . 12 . 13 a.X[:16, :] = 0. ~/.miniconda3/envs/cellrank/lib/python3.8/site-packages/scanpy/plotting/_anndata.py in heatmap(adata, var_names, groupby, use_raw, log, num_categories, dendrogram, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, standard_scale, swap_axes, show_gene_labels, show, save, figsize, **kwds). 1220 groupby_cmap,. 1221 norm,. -> 1222 ) = _plot_categories_as_colorblocks(. 1223 groupby_ax, obs_tidy, colors=groupby_colors, orientation='bottom'. 1224 )",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1591
https://github.com/scverse/scanpy/issues/1591:2219,energy efficiency,heat,heatmap,2219,"['foo'].iloc[32:, :] = 2. # wrong label-color mapping. sc.pl.heatmap(a, var_names=a.var_names[:50], groupby=""foo"", swap_axes=False, dendrogram=True) # fig. 2. sc.pl.heatmap(a, var_names=a.var_names[:50], groupby=""foo"", swap_axes=False, dendrogram=False) # fig. 3. # but not when there are colors in `.uns. a.uns['foo_colors'] = [(1, 0, 0), (0, 1, 0), (0, 0, 1)]. sc.pl.heatmap(a, var_names=a.var_names[:50], groupby=""foo"", swap_axes=False, dendrogram=True) # fig. 4. sc.pl.heatmap(a, var_names=a.var_names[:50], groupby=""foo"", swap_axes=False, dendrogram=False) # fig. 5. # xlabels are not centered + horizntal lines are slightly shifted downwards (really have to zoom in). sc.pl.heatmap(a, var_names=a.var_names, groupby=""foo"", swap_axes=False, figsize=(50, 50)). ```. If you look closely (e.g. between fig. 2 and fig.3, or fig. 4 and fig. 5), the within group order is also broken. ```pytb. TypeError Traceback (most recent call last). <ipython-input-56-f1ba710dac43> in <module>. 9 . 10 sc.pl.heatmap(a, var_names=a.var_names[:5], groupby=None, swap_axes=True) # is shifted to the left (fig. 1). ---> 11 sc.pl.heatmap(a, var_names=a.var_names[:5], groupby=""foo"", swap_axes=True) . 12 . 13 a.X[:16, :] = 0. ~/.miniconda3/envs/cellrank/lib/python3.8/site-packages/scanpy/plotting/_anndata.py in heatmap(adata, var_names, groupby, use_raw, log, num_categories, dendrogram, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, standard_scale, swap_axes, show_gene_labels, show, save, figsize, **kwds). 1220 groupby_cmap,. 1221 norm,. -> 1222 ) = _plot_categories_as_colorblocks(. 1223 groupby_ax, obs_tidy, colors=groupby_colors, orientation='bottom'. 1224 ). ~/.miniconda3/envs/cellrank/lib/python3.8/site-packages/scanpy/plotting/_anndata.py in _plot_categories_as_colorblocks(groupby_ax, obs_tidy, colors, orientation, cmap_name). 2361 if len(labels) > 1:. 2362 groupby_ax.set_xticks(ticks). -> 2363 if max([len(x) for x in labels]) < 3:. 2364 # if the labels are small d",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1591
https://github.com/scverse/scanpy/issues/1591:2336,energy efficiency,heat,heatmap,2336,"ap_axes=False, dendrogram=True) # fig. 2. sc.pl.heatmap(a, var_names=a.var_names[:50], groupby=""foo"", swap_axes=False, dendrogram=False) # fig. 3. # but not when there are colors in `.uns. a.uns['foo_colors'] = [(1, 0, 0), (0, 1, 0), (0, 0, 1)]. sc.pl.heatmap(a, var_names=a.var_names[:50], groupby=""foo"", swap_axes=False, dendrogram=True) # fig. 4. sc.pl.heatmap(a, var_names=a.var_names[:50], groupby=""foo"", swap_axes=False, dendrogram=False) # fig. 5. # xlabels are not centered + horizntal lines are slightly shifted downwards (really have to zoom in). sc.pl.heatmap(a, var_names=a.var_names, groupby=""foo"", swap_axes=False, figsize=(50, 50)). ```. If you look closely (e.g. between fig. 2 and fig.3, or fig. 4 and fig. 5), the within group order is also broken. ```pytb. TypeError Traceback (most recent call last). <ipython-input-56-f1ba710dac43> in <module>. 9 . 10 sc.pl.heatmap(a, var_names=a.var_names[:5], groupby=None, swap_axes=True) # is shifted to the left (fig. 1). ---> 11 sc.pl.heatmap(a, var_names=a.var_names[:5], groupby=""foo"", swap_axes=True) . 12 . 13 a.X[:16, :] = 0. ~/.miniconda3/envs/cellrank/lib/python3.8/site-packages/scanpy/plotting/_anndata.py in heatmap(adata, var_names, groupby, use_raw, log, num_categories, dendrogram, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, standard_scale, swap_axes, show_gene_labels, show, save, figsize, **kwds). 1220 groupby_cmap,. 1221 norm,. -> 1222 ) = _plot_categories_as_colorblocks(. 1223 groupby_ax, obs_tidy, colors=groupby_colors, orientation='bottom'. 1224 ). ~/.miniconda3/envs/cellrank/lib/python3.8/site-packages/scanpy/plotting/_anndata.py in _plot_categories_as_colorblocks(groupby_ax, obs_tidy, colors, orientation, cmap_name). 2361 if len(labels) > 1:. 2362 groupby_ax.set_xticks(ticks). -> 2363 if max([len(x) for x in labels]) < 3:. 2364 # if the labels are small do not rotate them. 2365 rotation = 0. ~/.miniconda3/envs/cellrank/lib/python3.8/site-packages/scanpy/plotting/_anndat",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1591
https://github.com/scverse/scanpy/issues/1591:2519,energy efficiency,heat,heatmap,2519,".uns. a.uns['foo_colors'] = [(1, 0, 0), (0, 1, 0), (0, 0, 1)]. sc.pl.heatmap(a, var_names=a.var_names[:50], groupby=""foo"", swap_axes=False, dendrogram=True) # fig. 4. sc.pl.heatmap(a, var_names=a.var_names[:50], groupby=""foo"", swap_axes=False, dendrogram=False) # fig. 5. # xlabels are not centered + horizntal lines are slightly shifted downwards (really have to zoom in). sc.pl.heatmap(a, var_names=a.var_names, groupby=""foo"", swap_axes=False, figsize=(50, 50)). ```. If you look closely (e.g. between fig. 2 and fig.3, or fig. 4 and fig. 5), the within group order is also broken. ```pytb. TypeError Traceback (most recent call last). <ipython-input-56-f1ba710dac43> in <module>. 9 . 10 sc.pl.heatmap(a, var_names=a.var_names[:5], groupby=None, swap_axes=True) # is shifted to the left (fig. 1). ---> 11 sc.pl.heatmap(a, var_names=a.var_names[:5], groupby=""foo"", swap_axes=True) . 12 . 13 a.X[:16, :] = 0. ~/.miniconda3/envs/cellrank/lib/python3.8/site-packages/scanpy/plotting/_anndata.py in heatmap(adata, var_names, groupby, use_raw, log, num_categories, dendrogram, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, standard_scale, swap_axes, show_gene_labels, show, save, figsize, **kwds). 1220 groupby_cmap,. 1221 norm,. -> 1222 ) = _plot_categories_as_colorblocks(. 1223 groupby_ax, obs_tidy, colors=groupby_colors, orientation='bottom'. 1224 ). ~/.miniconda3/envs/cellrank/lib/python3.8/site-packages/scanpy/plotting/_anndata.py in _plot_categories_as_colorblocks(groupby_ax, obs_tidy, colors, orientation, cmap_name). 2361 if len(labels) > 1:. 2362 groupby_ax.set_xticks(ticks). -> 2363 if max([len(x) for x in labels]) < 3:. 2364 # if the labels are small do not rotate them. 2365 rotation = 0. ~/.miniconda3/envs/cellrank/lib/python3.8/site-packages/scanpy/plotting/_anndata.py in <listcomp>(.0). 2361 if len(labels) > 1:. 2362 groupby_ax.set_xticks(ticks). -> 2363 if max([len(x) for x in labels]) < 3:. 2364 # if the labels are small do not rotate them. ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1591
https://github.com/scverse/scanpy/issues/1591:4498,energy efficiency,cloud,cloudpickle,4498,o not rotate them. 2365 rotation = 0. TypeError: object of type 'int' has no len(). ```. #### Figures. Fig. 1. ![f0](https://user-images.githubusercontent.com/46717574/104822526-6544e880-5843-11eb-8324-261e84eb45e4.png). Fig. 2. ![f1](https://user-images.githubusercontent.com/46717574/104822449-d637d080-5842-11eb-853d-bb115cd014aa.png). Fig. 3. ![f2](https://user-images.githubusercontent.com/46717574/104822452-d9cb5780-5842-11eb-8606-f4a9a5c97893.png). Fig. 4. ![f3](https://user-images.githubusercontent.com/46717574/104822453-dcc64800-5842-11eb-827c-90db0525d4d4.png). Fig. 5. ![f4](https://user-images.githubusercontent.com/46717574/104822455-dfc13880-5842-11eb-9286-655b2210b182.png). Fig. 6. ![f6](https://user-images.githubusercontent.com/46717574/104822796-958d8680-5845-11eb-82e2-4b30597c6722.png). #### Versions. <details>. -----. anndata 0.7.5. scanpy 1.7.0rc2.dev1+g2a123065. sinfo 0.3.1. -----. PIL 8.0.1. anndata 0.7.5. asciitree NA. backcall 0.2.0. cairo 1.20.0. cffi 1.14.4. cloudpickle 1.6.0. colorama 0.4.4. coverage 5.3. cycler 0.10.0. cython_runtime NA. dask 2020.12.0. dateutil 2.8.1. decorator 4.4.2. fasteners NA. future_fstrings NA. get_version 2.1. h5py 2.10.0. igraph 0.8.3. ipykernel 5.4.2. ipython_genutils 0.2.0. jedi 0.17.2. joblib 0.17.0. kiwisolver 1.3.1. legacy_api_wrap 1.2. leidenalg 0.8.1. llvmlite 0.34.0. louvain 0.6.1. matplotlib 3.3.3. monotonic NA. mpl_toolkits NA. msgpack 1.0.0. natsort 7.0.1. numba 0.51.2. numcodecs 0.7.2. numexpr 2.7.1. numpy 1.19.4. packaging 20.8. pandas 1.1.4. parso 0.7.1. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. prompt_toolkit 3.0.8. psutil 5.7.3. ptyprocess 0.6.0. pygments 2.7.3. pyparsing 2.4.7. pytz 2020.5. ruamel NA. scanpy 1.7.0rc2.dev1+g2a123065. scipy 1.5.4. setuptools_scm NA. sinfo 0.3.1. six 1.15.0. sklearn 0.23.2. sparse 0.11.2. sphinxcontrib NA. storemagic NA. tables 3.6.1. tblib 1.7.0. texttable 1.6.3. tlz 0.11.1. toolz 0.11.1. tornado 6.1. traitlets 5.0.5. typing_extensions NA. wcwidth 0.2.5. yaml,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1591
https://github.com/scverse/scanpy/issues/1591:5745,energy efficiency,CPU,CPU,5745,"-bb115cd014aa.png). Fig. 3. ![f2](https://user-images.githubusercontent.com/46717574/104822452-d9cb5780-5842-11eb-8606-f4a9a5c97893.png). Fig. 4. ![f3](https://user-images.githubusercontent.com/46717574/104822453-dcc64800-5842-11eb-827c-90db0525d4d4.png). Fig. 5. ![f4](https://user-images.githubusercontent.com/46717574/104822455-dfc13880-5842-11eb-9286-655b2210b182.png). Fig. 6. ![f6](https://user-images.githubusercontent.com/46717574/104822796-958d8680-5845-11eb-82e2-4b30597c6722.png). #### Versions. <details>. -----. anndata 0.7.5. scanpy 1.7.0rc2.dev1+g2a123065. sinfo 0.3.1. -----. PIL 8.0.1. anndata 0.7.5. asciitree NA. backcall 0.2.0. cairo 1.20.0. cffi 1.14.4. cloudpickle 1.6.0. colorama 0.4.4. coverage 5.3. cycler 0.10.0. cython_runtime NA. dask 2020.12.0. dateutil 2.8.1. decorator 4.4.2. fasteners NA. future_fstrings NA. get_version 2.1. h5py 2.10.0. igraph 0.8.3. ipykernel 5.4.2. ipython_genutils 0.2.0. jedi 0.17.2. joblib 0.17.0. kiwisolver 1.3.1. legacy_api_wrap 1.2. leidenalg 0.8.1. llvmlite 0.34.0. louvain 0.6.1. matplotlib 3.3.3. monotonic NA. mpl_toolkits NA. msgpack 1.0.0. natsort 7.0.1. numba 0.51.2. numcodecs 0.7.2. numexpr 2.7.1. numpy 1.19.4. packaging 20.8. pandas 1.1.4. parso 0.7.1. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. prompt_toolkit 3.0.8. psutil 5.7.3. ptyprocess 0.6.0. pygments 2.7.3. pyparsing 2.4.7. pytz 2020.5. ruamel NA. scanpy 1.7.0rc2.dev1+g2a123065. scipy 1.5.4. setuptools_scm NA. sinfo 0.3.1. six 1.15.0. sklearn 0.23.2. sparse 0.11.2. sphinxcontrib NA. storemagic NA. tables 3.6.1. tblib 1.7.0. texttable 1.6.3. tlz 0.11.1. toolz 0.11.1. tornado 6.1. traitlets 5.0.5. typing_extensions NA. wcwidth 0.2.5. yaml 5.3.1. zarr 2.6.1. zmq 20.0.0. zope NA. -----. IPython 7.19.0. jupyter_client 6.1.7. jupyter_core 4.7.0. notebook 6.1.5. -----. Python 3.8.5 (default, Sep 4 2020, 07:30:14) [GCC 7.3.0]. Linux-5.10.0-1-amd64-x86_64-with-glibc2.10. 8 logical CPU cores. -----. Session information updated at 2021-01-16 21:28. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1591
https://github.com/scverse/scanpy/issues/1591:5749,energy efficiency,core,cores,5749,"-bb115cd014aa.png). Fig. 3. ![f2](https://user-images.githubusercontent.com/46717574/104822452-d9cb5780-5842-11eb-8606-f4a9a5c97893.png). Fig. 4. ![f3](https://user-images.githubusercontent.com/46717574/104822453-dcc64800-5842-11eb-827c-90db0525d4d4.png). Fig. 5. ![f4](https://user-images.githubusercontent.com/46717574/104822455-dfc13880-5842-11eb-9286-655b2210b182.png). Fig. 6. ![f6](https://user-images.githubusercontent.com/46717574/104822796-958d8680-5845-11eb-82e2-4b30597c6722.png). #### Versions. <details>. -----. anndata 0.7.5. scanpy 1.7.0rc2.dev1+g2a123065. sinfo 0.3.1. -----. PIL 8.0.1. anndata 0.7.5. asciitree NA. backcall 0.2.0. cairo 1.20.0. cffi 1.14.4. cloudpickle 1.6.0. colorama 0.4.4. coverage 5.3. cycler 0.10.0. cython_runtime NA. dask 2020.12.0. dateutil 2.8.1. decorator 4.4.2. fasteners NA. future_fstrings NA. get_version 2.1. h5py 2.10.0. igraph 0.8.3. ipykernel 5.4.2. ipython_genutils 0.2.0. jedi 0.17.2. joblib 0.17.0. kiwisolver 1.3.1. legacy_api_wrap 1.2. leidenalg 0.8.1. llvmlite 0.34.0. louvain 0.6.1. matplotlib 3.3.3. monotonic NA. mpl_toolkits NA. msgpack 1.0.0. natsort 7.0.1. numba 0.51.2. numcodecs 0.7.2. numexpr 2.7.1. numpy 1.19.4. packaging 20.8. pandas 1.1.4. parso 0.7.1. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. prompt_toolkit 3.0.8. psutil 5.7.3. ptyprocess 0.6.0. pygments 2.7.3. pyparsing 2.4.7. pytz 2020.5. ruamel NA. scanpy 1.7.0rc2.dev1+g2a123065. scipy 1.5.4. setuptools_scm NA. sinfo 0.3.1. six 1.15.0. sklearn 0.23.2. sparse 0.11.2. sphinxcontrib NA. storemagic NA. tables 3.6.1. tblib 1.7.0. texttable 1.6.3. tlz 0.11.1. toolz 0.11.1. tornado 6.1. traitlets 5.0.5. typing_extensions NA. wcwidth 0.2.5. yaml 5.3.1. zarr 2.6.1. zmq 20.0.0. zope NA. -----. IPython 7.19.0. jupyter_client 6.1.7. jupyter_core 4.7.0. notebook 6.1.5. -----. Python 3.8.5 (default, Sep 4 2020, 07:30:14) [GCC 7.3.0]. Linux-5.10.0-1-amd64-x86_64-with-glibc2.10. 8 logical CPU cores. -----. Session information updated at 2021-01-16 21:28. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1591
https://github.com/scverse/scanpy/issues/1591:159,integrability,version,version,159,"Heatmap inconsistent order/swap_axes; - [x] I have checked that this issue has not already been reported. - [x] I have confirmed this bug exists on the latest version of scanpy. - [x] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. ### Minimal code sample (that we can copy&paste without having any data). ```python. import numpy as np. import pandas as pd. import scanpy as sc. from anndata import AnnData. a = AnnData(np.random.RandomState(42).normal(size=(50, 50)),. obs={""foo"": pd.Categorical(np.random.choice([0, 1, 2], size=50))}). sc.pl.heatmap(a, var_names=a.var_names[:5], groupby=None, swap_axes=True) # is shifted to the left (fig. 1). sc.pl.heatmap(a, var_names=a.var_names[:5], groupby=""foo"", swap_axes=True) # see the tb (if there's only 1 category, in the group, it doesn't crash). a.X[:16, :] = 10 + np.random.RandomState(42).normal(size=(16, 50)). a.X[16:32, :] = -10 + np.random.RandomState(42).normal(size=(16, 50)). a.obs['foo'].iloc[:16, :] = 0. a.obs['foo'].iloc[16:32, :] = 1. a.obs['foo'].iloc[32:, :] = 2. # wrong label-color mapping. sc.pl.heatmap(a, var_names=a.var_names[:50], groupby=""foo"", swap_axes=False, dendrogram=True) # fig. 2. sc.pl.heatmap(a, var_names=a.var_names[:50], groupby=""foo"", swap_axes=False, dendrogram=False) # fig. 3. # but not when there are colors in `.uns. a.uns['foo_colors'] = [(1, 0, 0), (0, 1, 0), (0, 0, 1)]. sc.pl.heatmap(a, var_names=a.var_names[:50], groupby=""foo"", swap_axes=False, dendrogram=True) # fig. 4. sc.pl.heatmap(a, var_names=a.var_names[:50], groupby=""foo"", swap_axes=False, dendrogram=False) # fig. 5. # xlabels are not centered + horizntal lines are slightly shifted downwards (really have to zoom in). sc.pl.heatmap(a, var_names=a.var_names, groupby=""foo"", swap_axes=False, figsize=(50, 50)). ```. If you ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1591
https://github.com/scverse/scanpy/issues/1591:4320,integrability,Version,Versions,4320,plotting/_anndata.py in <listcomp>(.0). 2361 if len(labels) > 1:. 2362 groupby_ax.set_xticks(ticks). -> 2363 if max([len(x) for x in labels]) < 3:. 2364 # if the labels are small do not rotate them. 2365 rotation = 0. TypeError: object of type 'int' has no len(). ```. #### Figures. Fig. 1. ![f0](https://user-images.githubusercontent.com/46717574/104822526-6544e880-5843-11eb-8324-261e84eb45e4.png). Fig. 2. ![f1](https://user-images.githubusercontent.com/46717574/104822449-d637d080-5842-11eb-853d-bb115cd014aa.png). Fig. 3. ![f2](https://user-images.githubusercontent.com/46717574/104822452-d9cb5780-5842-11eb-8606-f4a9a5c97893.png). Fig. 4. ![f3](https://user-images.githubusercontent.com/46717574/104822453-dcc64800-5842-11eb-827c-90db0525d4d4.png). Fig. 5. ![f4](https://user-images.githubusercontent.com/46717574/104822455-dfc13880-5842-11eb-9286-655b2210b182.png). Fig. 6. ![f6](https://user-images.githubusercontent.com/46717574/104822796-958d8680-5845-11eb-82e2-4b30597c6722.png). #### Versions. <details>. -----. anndata 0.7.5. scanpy 1.7.0rc2.dev1+g2a123065. sinfo 0.3.1. -----. PIL 8.0.1. anndata 0.7.5. asciitree NA. backcall 0.2.0. cairo 1.20.0. cffi 1.14.4. cloudpickle 1.6.0. colorama 0.4.4. coverage 5.3. cycler 0.10.0. cython_runtime NA. dask 2020.12.0. dateutil 2.8.1. decorator 4.4.2. fasteners NA. future_fstrings NA. get_version 2.1. h5py 2.10.0. igraph 0.8.3. ipykernel 5.4.2. ipython_genutils 0.2.0. jedi 0.17.2. joblib 0.17.0. kiwisolver 1.3.1. legacy_api_wrap 1.2. leidenalg 0.8.1. llvmlite 0.34.0. louvain 0.6.1. matplotlib 3.3.3. monotonic NA. mpl_toolkits NA. msgpack 1.0.0. natsort 7.0.1. numba 0.51.2. numcodecs 0.7.2. numexpr 2.7.1. numpy 1.19.4. packaging 20.8. pandas 1.1.4. parso 0.7.1. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. prompt_toolkit 3.0.8. psutil 5.7.3. ptyprocess 0.6.0. pygments 2.7.3. pyparsing 2.4.7. pytz 2020.5. ruamel NA. scanpy 1.7.0rc2.dev1+g2a123065. scipy 1.5.4. setuptools_scm NA. sinfo 0.3.1. six 1.15.0. sklearn 0.23.2. sparse 0.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1591
https://github.com/scverse/scanpy/issues/1591:159,modifiability,version,version,159,"Heatmap inconsistent order/swap_axes; - [x] I have checked that this issue has not already been reported. - [x] I have confirmed this bug exists on the latest version of scanpy. - [x] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. ### Minimal code sample (that we can copy&paste without having any data). ```python. import numpy as np. import pandas as pd. import scanpy as sc. from anndata import AnnData. a = AnnData(np.random.RandomState(42).normal(size=(50, 50)),. obs={""foo"": pd.Categorical(np.random.choice([0, 1, 2], size=50))}). sc.pl.heatmap(a, var_names=a.var_names[:5], groupby=None, swap_axes=True) # is shifted to the left (fig. 1). sc.pl.heatmap(a, var_names=a.var_names[:5], groupby=""foo"", swap_axes=True) # see the tb (if there's only 1 category, in the group, it doesn't crash). a.X[:16, :] = 10 + np.random.RandomState(42).normal(size=(16, 50)). a.X[16:32, :] = -10 + np.random.RandomState(42).normal(size=(16, 50)). a.obs['foo'].iloc[:16, :] = 0. a.obs['foo'].iloc[16:32, :] = 1. a.obs['foo'].iloc[32:, :] = 2. # wrong label-color mapping. sc.pl.heatmap(a, var_names=a.var_names[:50], groupby=""foo"", swap_axes=False, dendrogram=True) # fig. 2. sc.pl.heatmap(a, var_names=a.var_names[:50], groupby=""foo"", swap_axes=False, dendrogram=False) # fig. 3. # but not when there are colors in `.uns. a.uns['foo_colors'] = [(1, 0, 0), (0, 1, 0), (0, 0, 1)]. sc.pl.heatmap(a, var_names=a.var_names[:50], groupby=""foo"", swap_axes=False, dendrogram=True) # fig. 4. sc.pl.heatmap(a, var_names=a.var_names[:50], groupby=""foo"", swap_axes=False, dendrogram=False) # fig. 5. # xlabels are not centered + horizntal lines are slightly shifted downwards (really have to zoom in). sc.pl.heatmap(a, var_names=a.var_names, groupby=""foo"", swap_axes=False, figsize=(50, 50)). ```. If you ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1591
https://github.com/scverse/scanpy/issues/1591:2197,modifiability,modul,module,2197,"oc[16:32, :] = 1. a.obs['foo'].iloc[32:, :] = 2. # wrong label-color mapping. sc.pl.heatmap(a, var_names=a.var_names[:50], groupby=""foo"", swap_axes=False, dendrogram=True) # fig. 2. sc.pl.heatmap(a, var_names=a.var_names[:50], groupby=""foo"", swap_axes=False, dendrogram=False) # fig. 3. # but not when there are colors in `.uns. a.uns['foo_colors'] = [(1, 0, 0), (0, 1, 0), (0, 0, 1)]. sc.pl.heatmap(a, var_names=a.var_names[:50], groupby=""foo"", swap_axes=False, dendrogram=True) # fig. 4. sc.pl.heatmap(a, var_names=a.var_names[:50], groupby=""foo"", swap_axes=False, dendrogram=False) # fig. 5. # xlabels are not centered + horizntal lines are slightly shifted downwards (really have to zoom in). sc.pl.heatmap(a, var_names=a.var_names, groupby=""foo"", swap_axes=False, figsize=(50, 50)). ```. If you look closely (e.g. between fig. 2 and fig.3, or fig. 4 and fig. 5), the within group order is also broken. ```pytb. TypeError Traceback (most recent call last). <ipython-input-56-f1ba710dac43> in <module>. 9 . 10 sc.pl.heatmap(a, var_names=a.var_names[:5], groupby=None, swap_axes=True) # is shifted to the left (fig. 1). ---> 11 sc.pl.heatmap(a, var_names=a.var_names[:5], groupby=""foo"", swap_axes=True) . 12 . 13 a.X[:16, :] = 0. ~/.miniconda3/envs/cellrank/lib/python3.8/site-packages/scanpy/plotting/_anndata.py in heatmap(adata, var_names, groupby, use_raw, log, num_categories, dendrogram, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, standard_scale, swap_axes, show_gene_labels, show, save, figsize, **kwds). 1220 groupby_cmap,. 1221 norm,. -> 1222 ) = _plot_categories_as_colorblocks(. 1223 groupby_ax, obs_tidy, colors=groupby_colors, orientation='bottom'. 1224 ). ~/.miniconda3/envs/cellrank/lib/python3.8/site-packages/scanpy/plotting/_anndata.py in _plot_categories_as_colorblocks(groupby_ax, obs_tidy, colors, orientation, cmap_name). 2361 if len(labels) > 1:. 2362 groupby_ax.set_xticks(ticks). -> 2363 if max([len(x) for x in labels]) < 3:. 2364 # if",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1591
https://github.com/scverse/scanpy/issues/1591:2479,modifiability,pac,packages,2479," 3. # but not when there are colors in `.uns. a.uns['foo_colors'] = [(1, 0, 0), (0, 1, 0), (0, 0, 1)]. sc.pl.heatmap(a, var_names=a.var_names[:50], groupby=""foo"", swap_axes=False, dendrogram=True) # fig. 4. sc.pl.heatmap(a, var_names=a.var_names[:50], groupby=""foo"", swap_axes=False, dendrogram=False) # fig. 5. # xlabels are not centered + horizntal lines are slightly shifted downwards (really have to zoom in). sc.pl.heatmap(a, var_names=a.var_names, groupby=""foo"", swap_axes=False, figsize=(50, 50)). ```. If you look closely (e.g. between fig. 2 and fig.3, or fig. 4 and fig. 5), the within group order is also broken. ```pytb. TypeError Traceback (most recent call last). <ipython-input-56-f1ba710dac43> in <module>. 9 . 10 sc.pl.heatmap(a, var_names=a.var_names[:5], groupby=None, swap_axes=True) # is shifted to the left (fig. 1). ---> 11 sc.pl.heatmap(a, var_names=a.var_names[:5], groupby=""foo"", swap_axes=True) . 12 . 13 a.X[:16, :] = 0. ~/.miniconda3/envs/cellrank/lib/python3.8/site-packages/scanpy/plotting/_anndata.py in heatmap(adata, var_names, groupby, use_raw, log, num_categories, dendrogram, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, standard_scale, swap_axes, show_gene_labels, show, save, figsize, **kwds). 1220 groupby_cmap,. 1221 norm,. -> 1222 ) = _plot_categories_as_colorblocks(. 1223 groupby_ax, obs_tidy, colors=groupby_colors, orientation='bottom'. 1224 ). ~/.miniconda3/envs/cellrank/lib/python3.8/site-packages/scanpy/plotting/_anndata.py in _plot_categories_as_colorblocks(groupby_ax, obs_tidy, colors, orientation, cmap_name). 2361 if len(labels) > 1:. 2362 groupby_ax.set_xticks(ticks). -> 2363 if max([len(x) for x in labels]) < 3:. 2364 # if the labels are small do not rotate them. 2365 rotation = 0. ~/.miniconda3/envs/cellrank/lib/python3.8/site-packages/scanpy/plotting/_anndata.py in <listcomp>(.0). 2361 if len(labels) > 1:. 2362 groupby_ax.set_xticks(ticks). -> 2363 if max([len(x) for x in labels]) < 3:. 2364 # if t",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1591
https://github.com/scverse/scanpy/issues/1591:2669,modifiability,layer,layer,2669,"m=True) # fig. 4. sc.pl.heatmap(a, var_names=a.var_names[:50], groupby=""foo"", swap_axes=False, dendrogram=False) # fig. 5. # xlabels are not centered + horizntal lines are slightly shifted downwards (really have to zoom in). sc.pl.heatmap(a, var_names=a.var_names, groupby=""foo"", swap_axes=False, figsize=(50, 50)). ```. If you look closely (e.g. between fig. 2 and fig.3, or fig. 4 and fig. 5), the within group order is also broken. ```pytb. TypeError Traceback (most recent call last). <ipython-input-56-f1ba710dac43> in <module>. 9 . 10 sc.pl.heatmap(a, var_names=a.var_names[:5], groupby=None, swap_axes=True) # is shifted to the left (fig. 1). ---> 11 sc.pl.heatmap(a, var_names=a.var_names[:5], groupby=""foo"", swap_axes=True) . 12 . 13 a.X[:16, :] = 0. ~/.miniconda3/envs/cellrank/lib/python3.8/site-packages/scanpy/plotting/_anndata.py in heatmap(adata, var_names, groupby, use_raw, log, num_categories, dendrogram, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, standard_scale, swap_axes, show_gene_labels, show, save, figsize, **kwds). 1220 groupby_cmap,. 1221 norm,. -> 1222 ) = _plot_categories_as_colorblocks(. 1223 groupby_ax, obs_tidy, colors=groupby_colors, orientation='bottom'. 1224 ). ~/.miniconda3/envs/cellrank/lib/python3.8/site-packages/scanpy/plotting/_anndata.py in _plot_categories_as_colorblocks(groupby_ax, obs_tidy, colors, orientation, cmap_name). 2361 if len(labels) > 1:. 2362 groupby_ax.set_xticks(ticks). -> 2363 if max([len(x) for x in labels]) < 3:. 2364 # if the labels are small do not rotate them. 2365 rotation = 0. ~/.miniconda3/envs/cellrank/lib/python3.8/site-packages/scanpy/plotting/_anndata.py in <listcomp>(.0). 2361 if len(labels) > 1:. 2362 groupby_ax.set_xticks(ticks). -> 2363 if max([len(x) for x in labels]) < 3:. 2364 # if the labels are small do not rotate them. 2365 rotation = 0. TypeError: object of type 'int' has no len(). ```. #### Figures. Fig. 1. ![f0](https://user-images.githubusercontent.com/46717574/",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1591
https://github.com/scverse/scanpy/issues/1591:2956,modifiability,pac,packages,2956,"s=False, figsize=(50, 50)). ```. If you look closely (e.g. between fig. 2 and fig.3, or fig. 4 and fig. 5), the within group order is also broken. ```pytb. TypeError Traceback (most recent call last). <ipython-input-56-f1ba710dac43> in <module>. 9 . 10 sc.pl.heatmap(a, var_names=a.var_names[:5], groupby=None, swap_axes=True) # is shifted to the left (fig. 1). ---> 11 sc.pl.heatmap(a, var_names=a.var_names[:5], groupby=""foo"", swap_axes=True) . 12 . 13 a.X[:16, :] = 0. ~/.miniconda3/envs/cellrank/lib/python3.8/site-packages/scanpy/plotting/_anndata.py in heatmap(adata, var_names, groupby, use_raw, log, num_categories, dendrogram, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, standard_scale, swap_axes, show_gene_labels, show, save, figsize, **kwds). 1220 groupby_cmap,. 1221 norm,. -> 1222 ) = _plot_categories_as_colorblocks(. 1223 groupby_ax, obs_tidy, colors=groupby_colors, orientation='bottom'. 1224 ). ~/.miniconda3/envs/cellrank/lib/python3.8/site-packages/scanpy/plotting/_anndata.py in _plot_categories_as_colorblocks(groupby_ax, obs_tidy, colors, orientation, cmap_name). 2361 if len(labels) > 1:. 2362 groupby_ax.set_xticks(ticks). -> 2363 if max([len(x) for x in labels]) < 3:. 2364 # if the labels are small do not rotate them. 2365 rotation = 0. ~/.miniconda3/envs/cellrank/lib/python3.8/site-packages/scanpy/plotting/_anndata.py in <listcomp>(.0). 2361 if len(labels) > 1:. 2362 groupby_ax.set_xticks(ticks). -> 2363 if max([len(x) for x in labels]) < 3:. 2364 # if the labels are small do not rotate them. 2365 rotation = 0. TypeError: object of type 'int' has no len(). ```. #### Figures. Fig. 1. ![f0](https://user-images.githubusercontent.com/46717574/104822526-6544e880-5843-11eb-8324-261e84eb45e4.png). Fig. 2. ![f1](https://user-images.githubusercontent.com/46717574/104822449-d637d080-5842-11eb-853d-bb115cd014aa.png). Fig. 3. ![f2](https://user-images.githubusercontent.com/46717574/104822452-d9cb5780-5842-11eb-8606-f4a9a5c97893.png).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1591
https://github.com/scverse/scanpy/issues/1591:3308,modifiability,pac,packages,3308,"(fig. 1). ---> 11 sc.pl.heatmap(a, var_names=a.var_names[:5], groupby=""foo"", swap_axes=True) . 12 . 13 a.X[:16, :] = 0. ~/.miniconda3/envs/cellrank/lib/python3.8/site-packages/scanpy/plotting/_anndata.py in heatmap(adata, var_names, groupby, use_raw, log, num_categories, dendrogram, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, standard_scale, swap_axes, show_gene_labels, show, save, figsize, **kwds). 1220 groupby_cmap,. 1221 norm,. -> 1222 ) = _plot_categories_as_colorblocks(. 1223 groupby_ax, obs_tidy, colors=groupby_colors, orientation='bottom'. 1224 ). ~/.miniconda3/envs/cellrank/lib/python3.8/site-packages/scanpy/plotting/_anndata.py in _plot_categories_as_colorblocks(groupby_ax, obs_tidy, colors, orientation, cmap_name). 2361 if len(labels) > 1:. 2362 groupby_ax.set_xticks(ticks). -> 2363 if max([len(x) for x in labels]) < 3:. 2364 # if the labels are small do not rotate them. 2365 rotation = 0. ~/.miniconda3/envs/cellrank/lib/python3.8/site-packages/scanpy/plotting/_anndata.py in <listcomp>(.0). 2361 if len(labels) > 1:. 2362 groupby_ax.set_xticks(ticks). -> 2363 if max([len(x) for x in labels]) < 3:. 2364 # if the labels are small do not rotate them. 2365 rotation = 0. TypeError: object of type 'int' has no len(). ```. #### Figures. Fig. 1. ![f0](https://user-images.githubusercontent.com/46717574/104822526-6544e880-5843-11eb-8324-261e84eb45e4.png). Fig. 2. ![f1](https://user-images.githubusercontent.com/46717574/104822449-d637d080-5842-11eb-853d-bb115cd014aa.png). Fig. 3. ![f2](https://user-images.githubusercontent.com/46717574/104822452-d9cb5780-5842-11eb-8606-f4a9a5c97893.png). Fig. 4. ![f3](https://user-images.githubusercontent.com/46717574/104822453-dcc64800-5842-11eb-827c-90db0525d4d4.png). Fig. 5. ![f4](https://user-images.githubusercontent.com/46717574/104822455-dfc13880-5842-11eb-9286-655b2210b182.png). Fig. 6. ![f6](https://user-images.githubusercontent.com/46717574/104822796-958d8680-5845-11eb-82e2-4b30597c6722.png",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1591
https://github.com/scverse/scanpy/issues/1591:4320,modifiability,Version,Versions,4320,plotting/_anndata.py in <listcomp>(.0). 2361 if len(labels) > 1:. 2362 groupby_ax.set_xticks(ticks). -> 2363 if max([len(x) for x in labels]) < 3:. 2364 # if the labels are small do not rotate them. 2365 rotation = 0. TypeError: object of type 'int' has no len(). ```. #### Figures. Fig. 1. ![f0](https://user-images.githubusercontent.com/46717574/104822526-6544e880-5843-11eb-8324-261e84eb45e4.png). Fig. 2. ![f1](https://user-images.githubusercontent.com/46717574/104822449-d637d080-5842-11eb-853d-bb115cd014aa.png). Fig. 3. ![f2](https://user-images.githubusercontent.com/46717574/104822452-d9cb5780-5842-11eb-8606-f4a9a5c97893.png). Fig. 4. ![f3](https://user-images.githubusercontent.com/46717574/104822453-dcc64800-5842-11eb-827c-90db0525d4d4.png). Fig. 5. ![f4](https://user-images.githubusercontent.com/46717574/104822455-dfc13880-5842-11eb-9286-655b2210b182.png). Fig. 6. ![f6](https://user-images.githubusercontent.com/46717574/104822796-958d8680-5845-11eb-82e2-4b30597c6722.png). #### Versions. <details>. -----. anndata 0.7.5. scanpy 1.7.0rc2.dev1+g2a123065. sinfo 0.3.1. -----. PIL 8.0.1. anndata 0.7.5. asciitree NA. backcall 0.2.0. cairo 1.20.0. cffi 1.14.4. cloudpickle 1.6.0. colorama 0.4.4. coverage 5.3. cycler 0.10.0. cython_runtime NA. dask 2020.12.0. dateutil 2.8.1. decorator 4.4.2. fasteners NA. future_fstrings NA. get_version 2.1. h5py 2.10.0. igraph 0.8.3. ipykernel 5.4.2. ipython_genutils 0.2.0. jedi 0.17.2. joblib 0.17.0. kiwisolver 1.3.1. legacy_api_wrap 1.2. leidenalg 0.8.1. llvmlite 0.34.0. louvain 0.6.1. matplotlib 3.3.3. monotonic NA. mpl_toolkits NA. msgpack 1.0.0. natsort 7.0.1. numba 0.51.2. numcodecs 0.7.2. numexpr 2.7.1. numpy 1.19.4. packaging 20.8. pandas 1.1.4. parso 0.7.1. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. prompt_toolkit 3.0.8. psutil 5.7.3. ptyprocess 0.6.0. pygments 2.7.3. pyparsing 2.4.7. pytz 2020.5. ruamel NA. scanpy 1.7.0rc2.dev1+g2a123065. scipy 1.5.4. setuptools_scm NA. sinfo 0.3.1. six 1.15.0. sklearn 0.23.2. sparse 0.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1591
https://github.com/scverse/scanpy/issues/1591:4613,modifiability,deco,decorator,4613,0](https://user-images.githubusercontent.com/46717574/104822526-6544e880-5843-11eb-8324-261e84eb45e4.png). Fig. 2. ![f1](https://user-images.githubusercontent.com/46717574/104822449-d637d080-5842-11eb-853d-bb115cd014aa.png). Fig. 3. ![f2](https://user-images.githubusercontent.com/46717574/104822452-d9cb5780-5842-11eb-8606-f4a9a5c97893.png). Fig. 4. ![f3](https://user-images.githubusercontent.com/46717574/104822453-dcc64800-5842-11eb-827c-90db0525d4d4.png). Fig. 5. ![f4](https://user-images.githubusercontent.com/46717574/104822455-dfc13880-5842-11eb-9286-655b2210b182.png). Fig. 6. ![f6](https://user-images.githubusercontent.com/46717574/104822796-958d8680-5845-11eb-82e2-4b30597c6722.png). #### Versions. <details>. -----. anndata 0.7.5. scanpy 1.7.0rc2.dev1+g2a123065. sinfo 0.3.1. -----. PIL 8.0.1. anndata 0.7.5. asciitree NA. backcall 0.2.0. cairo 1.20.0. cffi 1.14.4. cloudpickle 1.6.0. colorama 0.4.4. coverage 5.3. cycler 0.10.0. cython_runtime NA. dask 2020.12.0. dateutil 2.8.1. decorator 4.4.2. fasteners NA. future_fstrings NA. get_version 2.1. h5py 2.10.0. igraph 0.8.3. ipykernel 5.4.2. ipython_genutils 0.2.0. jedi 0.17.2. joblib 0.17.0. kiwisolver 1.3.1. legacy_api_wrap 1.2. leidenalg 0.8.1. llvmlite 0.34.0. louvain 0.6.1. matplotlib 3.3.3. monotonic NA. mpl_toolkits NA. msgpack 1.0.0. natsort 7.0.1. numba 0.51.2. numcodecs 0.7.2. numexpr 2.7.1. numpy 1.19.4. packaging 20.8. pandas 1.1.4. parso 0.7.1. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. prompt_toolkit 3.0.8. psutil 5.7.3. ptyprocess 0.6.0. pygments 2.7.3. pyparsing 2.4.7. pytz 2020.5. ruamel NA. scanpy 1.7.0rc2.dev1+g2a123065. scipy 1.5.4. setuptools_scm NA. sinfo 0.3.1. six 1.15.0. sklearn 0.23.2. sparse 0.11.2. sphinxcontrib NA. storemagic NA. tables 3.6.1. tblib 1.7.0. texttable 1.6.3. tlz 0.11.1. toolz 0.11.1. tornado 6.1. traitlets 5.0.5. typing_extensions NA. wcwidth 0.2.5. yaml 5.3.1. zarr 2.6.1. zmq 20.0.0. zope NA. -----. IPython 7.19.0. jupyter_client 6.1.7. jupyter_core 4.7.0. notebook,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1591
https://github.com/scverse/scanpy/issues/1591:5004,modifiability,pac,packaging,5004,"-bb115cd014aa.png). Fig. 3. ![f2](https://user-images.githubusercontent.com/46717574/104822452-d9cb5780-5842-11eb-8606-f4a9a5c97893.png). Fig. 4. ![f3](https://user-images.githubusercontent.com/46717574/104822453-dcc64800-5842-11eb-827c-90db0525d4d4.png). Fig. 5. ![f4](https://user-images.githubusercontent.com/46717574/104822455-dfc13880-5842-11eb-9286-655b2210b182.png). Fig. 6. ![f6](https://user-images.githubusercontent.com/46717574/104822796-958d8680-5845-11eb-82e2-4b30597c6722.png). #### Versions. <details>. -----. anndata 0.7.5. scanpy 1.7.0rc2.dev1+g2a123065. sinfo 0.3.1. -----. PIL 8.0.1. anndata 0.7.5. asciitree NA. backcall 0.2.0. cairo 1.20.0. cffi 1.14.4. cloudpickle 1.6.0. colorama 0.4.4. coverage 5.3. cycler 0.10.0. cython_runtime NA. dask 2020.12.0. dateutil 2.8.1. decorator 4.4.2. fasteners NA. future_fstrings NA. get_version 2.1. h5py 2.10.0. igraph 0.8.3. ipykernel 5.4.2. ipython_genutils 0.2.0. jedi 0.17.2. joblib 0.17.0. kiwisolver 1.3.1. legacy_api_wrap 1.2. leidenalg 0.8.1. llvmlite 0.34.0. louvain 0.6.1. matplotlib 3.3.3. monotonic NA. mpl_toolkits NA. msgpack 1.0.0. natsort 7.0.1. numba 0.51.2. numcodecs 0.7.2. numexpr 2.7.1. numpy 1.19.4. packaging 20.8. pandas 1.1.4. parso 0.7.1. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. prompt_toolkit 3.0.8. psutil 5.7.3. ptyprocess 0.6.0. pygments 2.7.3. pyparsing 2.4.7. pytz 2020.5. ruamel NA. scanpy 1.7.0rc2.dev1+g2a123065. scipy 1.5.4. setuptools_scm NA. sinfo 0.3.1. six 1.15.0. sklearn 0.23.2. sparse 0.11.2. sphinxcontrib NA. storemagic NA. tables 3.6.1. tblib 1.7.0. texttable 1.6.3. tlz 0.11.1. toolz 0.11.1. tornado 6.1. traitlets 5.0.5. typing_extensions NA. wcwidth 0.2.5. yaml 5.3.1. zarr 2.6.1. zmq 20.0.0. zope NA. -----. IPython 7.19.0. jupyter_client 6.1.7. jupyter_core 4.7.0. notebook 6.1.5. -----. Python 3.8.5 (default, Sep 4 2020, 07:30:14) [GCC 7.3.0]. Linux-5.10.0-1-amd64-x86_64-with-glibc2.10. 8 logical CPU cores. -----. Session information updated at 2021-01-16 21:28. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1591
https://github.com/scverse/scanpy/issues/1591:5745,performance,CPU,CPU,5745,"-bb115cd014aa.png). Fig. 3. ![f2](https://user-images.githubusercontent.com/46717574/104822452-d9cb5780-5842-11eb-8606-f4a9a5c97893.png). Fig. 4. ![f3](https://user-images.githubusercontent.com/46717574/104822453-dcc64800-5842-11eb-827c-90db0525d4d4.png). Fig. 5. ![f4](https://user-images.githubusercontent.com/46717574/104822455-dfc13880-5842-11eb-9286-655b2210b182.png). Fig. 6. ![f6](https://user-images.githubusercontent.com/46717574/104822796-958d8680-5845-11eb-82e2-4b30597c6722.png). #### Versions. <details>. -----. anndata 0.7.5. scanpy 1.7.0rc2.dev1+g2a123065. sinfo 0.3.1. -----. PIL 8.0.1. anndata 0.7.5. asciitree NA. backcall 0.2.0. cairo 1.20.0. cffi 1.14.4. cloudpickle 1.6.0. colorama 0.4.4. coverage 5.3. cycler 0.10.0. cython_runtime NA. dask 2020.12.0. dateutil 2.8.1. decorator 4.4.2. fasteners NA. future_fstrings NA. get_version 2.1. h5py 2.10.0. igraph 0.8.3. ipykernel 5.4.2. ipython_genutils 0.2.0. jedi 0.17.2. joblib 0.17.0. kiwisolver 1.3.1. legacy_api_wrap 1.2. leidenalg 0.8.1. llvmlite 0.34.0. louvain 0.6.1. matplotlib 3.3.3. monotonic NA. mpl_toolkits NA. msgpack 1.0.0. natsort 7.0.1. numba 0.51.2. numcodecs 0.7.2. numexpr 2.7.1. numpy 1.19.4. packaging 20.8. pandas 1.1.4. parso 0.7.1. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. prompt_toolkit 3.0.8. psutil 5.7.3. ptyprocess 0.6.0. pygments 2.7.3. pyparsing 2.4.7. pytz 2020.5. ruamel NA. scanpy 1.7.0rc2.dev1+g2a123065. scipy 1.5.4. setuptools_scm NA. sinfo 0.3.1. six 1.15.0. sklearn 0.23.2. sparse 0.11.2. sphinxcontrib NA. storemagic NA. tables 3.6.1. tblib 1.7.0. texttable 1.6.3. tlz 0.11.1. toolz 0.11.1. tornado 6.1. traitlets 5.0.5. typing_extensions NA. wcwidth 0.2.5. yaml 5.3.1. zarr 2.6.1. zmq 20.0.0. zope NA. -----. IPython 7.19.0. jupyter_client 6.1.7. jupyter_core 4.7.0. notebook 6.1.5. -----. Python 3.8.5 (default, Sep 4 2020, 07:30:14) [GCC 7.3.0]. Linux-5.10.0-1-amd64-x86_64-with-glibc2.10. 8 logical CPU cores. -----. Session information updated at 2021-01-16 21:28. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1591
https://github.com/scverse/scanpy/issues/1591:999,reliability,doe,doesn,999,"atmap inconsistent order/swap_axes; - [x] I have checked that this issue has not already been reported. - [x] I have confirmed this bug exists on the latest version of scanpy. - [x] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. ### Minimal code sample (that we can copy&paste without having any data). ```python. import numpy as np. import pandas as pd. import scanpy as sc. from anndata import AnnData. a = AnnData(np.random.RandomState(42).normal(size=(50, 50)),. obs={""foo"": pd.Categorical(np.random.choice([0, 1, 2], size=50))}). sc.pl.heatmap(a, var_names=a.var_names[:5], groupby=None, swap_axes=True) # is shifted to the left (fig. 1). sc.pl.heatmap(a, var_names=a.var_names[:5], groupby=""foo"", swap_axes=True) # see the tb (if there's only 1 category, in the group, it doesn't crash). a.X[:16, :] = 10 + np.random.RandomState(42).normal(size=(16, 50)). a.X[16:32, :] = -10 + np.random.RandomState(42).normal(size=(16, 50)). a.obs['foo'].iloc[:16, :] = 0. a.obs['foo'].iloc[16:32, :] = 1. a.obs['foo'].iloc[32:, :] = 2. # wrong label-color mapping. sc.pl.heatmap(a, var_names=a.var_names[:50], groupby=""foo"", swap_axes=False, dendrogram=True) # fig. 2. sc.pl.heatmap(a, var_names=a.var_names[:50], groupby=""foo"", swap_axes=False, dendrogram=False) # fig. 3. # but not when there are colors in `.uns. a.uns['foo_colors'] = [(1, 0, 0), (0, 1, 0), (0, 0, 1)]. sc.pl.heatmap(a, var_names=a.var_names[:50], groupby=""foo"", swap_axes=False, dendrogram=True) # fig. 4. sc.pl.heatmap(a, var_names=a.var_names[:50], groupby=""foo"", swap_axes=False, dendrogram=False) # fig. 5. # xlabels are not centered + horizntal lines are slightly shifted downwards (really have to zoom in). sc.pl.heatmap(a, var_names=a.var_names, groupby=""foo"", swap_axes=False, figsize=(50, 50)). ```. If you lo",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1591
https://github.com/scverse/scanpy/issues/1591:1844,reliability,sli,slightly,1844,"e left (fig. 1). sc.pl.heatmap(a, var_names=a.var_names[:5], groupby=""foo"", swap_axes=True) # see the tb (if there's only 1 category, in the group, it doesn't crash). a.X[:16, :] = 10 + np.random.RandomState(42).normal(size=(16, 50)). a.X[16:32, :] = -10 + np.random.RandomState(42).normal(size=(16, 50)). a.obs['foo'].iloc[:16, :] = 0. a.obs['foo'].iloc[16:32, :] = 1. a.obs['foo'].iloc[32:, :] = 2. # wrong label-color mapping. sc.pl.heatmap(a, var_names=a.var_names[:50], groupby=""foo"", swap_axes=False, dendrogram=True) # fig. 2. sc.pl.heatmap(a, var_names=a.var_names[:50], groupby=""foo"", swap_axes=False, dendrogram=False) # fig. 3. # but not when there are colors in `.uns. a.uns['foo_colors'] = [(1, 0, 0), (0, 1, 0), (0, 0, 1)]. sc.pl.heatmap(a, var_names=a.var_names[:50], groupby=""foo"", swap_axes=False, dendrogram=True) # fig. 4. sc.pl.heatmap(a, var_names=a.var_names[:50], groupby=""foo"", swap_axes=False, dendrogram=False) # fig. 5. # xlabels are not centered + horizntal lines are slightly shifted downwards (really have to zoom in). sc.pl.heatmap(a, var_names=a.var_names, groupby=""foo"", swap_axes=False, figsize=(50, 50)). ```. If you look closely (e.g. between fig. 2 and fig.3, or fig. 4 and fig. 5), the within group order is also broken. ```pytb. TypeError Traceback (most recent call last). <ipython-input-56-f1ba710dac43> in <module>. 9 . 10 sc.pl.heatmap(a, var_names=a.var_names[:5], groupby=None, swap_axes=True) # is shifted to the left (fig. 1). ---> 11 sc.pl.heatmap(a, var_names=a.var_names[:5], groupby=""foo"", swap_axes=True) . 12 . 13 a.X[:16, :] = 0. ~/.miniconda3/envs/cellrank/lib/python3.8/site-packages/scanpy/plotting/_anndata.py in heatmap(adata, var_names, groupby, use_raw, log, num_categories, dendrogram, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, standard_scale, swap_axes, show_gene_labels, show, save, figsize, **kwds). 1220 groupby_cmap,. 1221 norm,. -> 1222 ) = _plot_categories_as_colorblocks(. 1223 groupby_ax, ob",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1591
https://github.com/scverse/scanpy/issues/1591:2170,safety,input,input-,2170,"16, :] = 0. a.obs['foo'].iloc[16:32, :] = 1. a.obs['foo'].iloc[32:, :] = 2. # wrong label-color mapping. sc.pl.heatmap(a, var_names=a.var_names[:50], groupby=""foo"", swap_axes=False, dendrogram=True) # fig. 2. sc.pl.heatmap(a, var_names=a.var_names[:50], groupby=""foo"", swap_axes=False, dendrogram=False) # fig. 3. # but not when there are colors in `.uns. a.uns['foo_colors'] = [(1, 0, 0), (0, 1, 0), (0, 0, 1)]. sc.pl.heatmap(a, var_names=a.var_names[:50], groupby=""foo"", swap_axes=False, dendrogram=True) # fig. 4. sc.pl.heatmap(a, var_names=a.var_names[:50], groupby=""foo"", swap_axes=False, dendrogram=False) # fig. 5. # xlabels are not centered + horizntal lines are slightly shifted downwards (really have to zoom in). sc.pl.heatmap(a, var_names=a.var_names, groupby=""foo"", swap_axes=False, figsize=(50, 50)). ```. If you look closely (e.g. between fig. 2 and fig.3, or fig. 4 and fig. 5), the within group order is also broken. ```pytb. TypeError Traceback (most recent call last). <ipython-input-56-f1ba710dac43> in <module>. 9 . 10 sc.pl.heatmap(a, var_names=a.var_names[:5], groupby=None, swap_axes=True) # is shifted to the left (fig. 1). ---> 11 sc.pl.heatmap(a, var_names=a.var_names[:5], groupby=""foo"", swap_axes=True) . 12 . 13 a.X[:16, :] = 0. ~/.miniconda3/envs/cellrank/lib/python3.8/site-packages/scanpy/plotting/_anndata.py in heatmap(adata, var_names, groupby, use_raw, log, num_categories, dendrogram, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, standard_scale, swap_axes, show_gene_labels, show, save, figsize, **kwds). 1220 groupby_cmap,. 1221 norm,. -> 1222 ) = _plot_categories_as_colorblocks(. 1223 groupby_ax, obs_tidy, colors=groupby_colors, orientation='bottom'. 1224 ). ~/.miniconda3/envs/cellrank/lib/python3.8/site-packages/scanpy/plotting/_anndata.py in _plot_categories_as_colorblocks(groupby_ax, obs_tidy, colors, orientation, cmap_name). 2361 if len(labels) > 1:. 2362 groupby_ax.set_xticks(ticks). -> 2363 if max([len(x) for x ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1591
https://github.com/scverse/scanpy/issues/1591:2197,safety,modul,module,2197,"oc[16:32, :] = 1. a.obs['foo'].iloc[32:, :] = 2. # wrong label-color mapping. sc.pl.heatmap(a, var_names=a.var_names[:50], groupby=""foo"", swap_axes=False, dendrogram=True) # fig. 2. sc.pl.heatmap(a, var_names=a.var_names[:50], groupby=""foo"", swap_axes=False, dendrogram=False) # fig. 3. # but not when there are colors in `.uns. a.uns['foo_colors'] = [(1, 0, 0), (0, 1, 0), (0, 0, 1)]. sc.pl.heatmap(a, var_names=a.var_names[:50], groupby=""foo"", swap_axes=False, dendrogram=True) # fig. 4. sc.pl.heatmap(a, var_names=a.var_names[:50], groupby=""foo"", swap_axes=False, dendrogram=False) # fig. 5. # xlabels are not centered + horizntal lines are slightly shifted downwards (really have to zoom in). sc.pl.heatmap(a, var_names=a.var_names, groupby=""foo"", swap_axes=False, figsize=(50, 50)). ```. If you look closely (e.g. between fig. 2 and fig.3, or fig. 4 and fig. 5), the within group order is also broken. ```pytb. TypeError Traceback (most recent call last). <ipython-input-56-f1ba710dac43> in <module>. 9 . 10 sc.pl.heatmap(a, var_names=a.var_names[:5], groupby=None, swap_axes=True) # is shifted to the left (fig. 1). ---> 11 sc.pl.heatmap(a, var_names=a.var_names[:5], groupby=""foo"", swap_axes=True) . 12 . 13 a.X[:16, :] = 0. ~/.miniconda3/envs/cellrank/lib/python3.8/site-packages/scanpy/plotting/_anndata.py in heatmap(adata, var_names, groupby, use_raw, log, num_categories, dendrogram, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, standard_scale, swap_axes, show_gene_labels, show, save, figsize, **kwds). 1220 groupby_cmap,. 1221 norm,. -> 1222 ) = _plot_categories_as_colorblocks(. 1223 groupby_ax, obs_tidy, colors=groupby_colors, orientation='bottom'. 1224 ). ~/.miniconda3/envs/cellrank/lib/python3.8/site-packages/scanpy/plotting/_anndata.py in _plot_categories_as_colorblocks(groupby_ax, obs_tidy, colors, orientation, cmap_name). 2361 if len(labels) > 1:. 2362 groupby_ax.set_xticks(ticks). -> 2363 if max([len(x) for x in labels]) < 3:. 2364 # if",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1591
https://github.com/scverse/scanpy/issues/1591:2563,safety,log,log,2563,", 1, 0), (0, 0, 1)]. sc.pl.heatmap(a, var_names=a.var_names[:50], groupby=""foo"", swap_axes=False, dendrogram=True) # fig. 4. sc.pl.heatmap(a, var_names=a.var_names[:50], groupby=""foo"", swap_axes=False, dendrogram=False) # fig. 5. # xlabels are not centered + horizntal lines are slightly shifted downwards (really have to zoom in). sc.pl.heatmap(a, var_names=a.var_names, groupby=""foo"", swap_axes=False, figsize=(50, 50)). ```. If you look closely (e.g. between fig. 2 and fig.3, or fig. 4 and fig. 5), the within group order is also broken. ```pytb. TypeError Traceback (most recent call last). <ipython-input-56-f1ba710dac43> in <module>. 9 . 10 sc.pl.heatmap(a, var_names=a.var_names[:5], groupby=None, swap_axes=True) # is shifted to the left (fig. 1). ---> 11 sc.pl.heatmap(a, var_names=a.var_names[:5], groupby=""foo"", swap_axes=True) . 12 . 13 a.X[:16, :] = 0. ~/.miniconda3/envs/cellrank/lib/python3.8/site-packages/scanpy/plotting/_anndata.py in heatmap(adata, var_names, groupby, use_raw, log, num_categories, dendrogram, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, standard_scale, swap_axes, show_gene_labels, show, save, figsize, **kwds). 1220 groupby_cmap,. 1221 norm,. -> 1222 ) = _plot_categories_as_colorblocks(. 1223 groupby_ax, obs_tidy, colors=groupby_colors, orientation='bottom'. 1224 ). ~/.miniconda3/envs/cellrank/lib/python3.8/site-packages/scanpy/plotting/_anndata.py in _plot_categories_as_colorblocks(groupby_ax, obs_tidy, colors, orientation, cmap_name). 2361 if len(labels) > 1:. 2362 groupby_ax.set_xticks(ticks). -> 2363 if max([len(x) for x in labels]) < 3:. 2364 # if the labels are small do not rotate them. 2365 rotation = 0. ~/.miniconda3/envs/cellrank/lib/python3.8/site-packages/scanpy/plotting/_anndata.py in <listcomp>(.0). 2361 if len(labels) > 1:. 2362 groupby_ax.set_xticks(ticks). -> 2363 if max([len(x) for x in labels]) < 3:. 2364 # if the labels are small do not rotate them. 2365 rotation = 0. TypeError: object of ty",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1591
https://github.com/scverse/scanpy/issues/1591:5737,safety,log,logical,5737,"-bb115cd014aa.png). Fig. 3. ![f2](https://user-images.githubusercontent.com/46717574/104822452-d9cb5780-5842-11eb-8606-f4a9a5c97893.png). Fig. 4. ![f3](https://user-images.githubusercontent.com/46717574/104822453-dcc64800-5842-11eb-827c-90db0525d4d4.png). Fig. 5. ![f4](https://user-images.githubusercontent.com/46717574/104822455-dfc13880-5842-11eb-9286-655b2210b182.png). Fig. 6. ![f6](https://user-images.githubusercontent.com/46717574/104822796-958d8680-5845-11eb-82e2-4b30597c6722.png). #### Versions. <details>. -----. anndata 0.7.5. scanpy 1.7.0rc2.dev1+g2a123065. sinfo 0.3.1. -----. PIL 8.0.1. anndata 0.7.5. asciitree NA. backcall 0.2.0. cairo 1.20.0. cffi 1.14.4. cloudpickle 1.6.0. colorama 0.4.4. coverage 5.3. cycler 0.10.0. cython_runtime NA. dask 2020.12.0. dateutil 2.8.1. decorator 4.4.2. fasteners NA. future_fstrings NA. get_version 2.1. h5py 2.10.0. igraph 0.8.3. ipykernel 5.4.2. ipython_genutils 0.2.0. jedi 0.17.2. joblib 0.17.0. kiwisolver 1.3.1. legacy_api_wrap 1.2. leidenalg 0.8.1. llvmlite 0.34.0. louvain 0.6.1. matplotlib 3.3.3. monotonic NA. mpl_toolkits NA. msgpack 1.0.0. natsort 7.0.1. numba 0.51.2. numcodecs 0.7.2. numexpr 2.7.1. numpy 1.19.4. packaging 20.8. pandas 1.1.4. parso 0.7.1. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. prompt_toolkit 3.0.8. psutil 5.7.3. ptyprocess 0.6.0. pygments 2.7.3. pyparsing 2.4.7. pytz 2020.5. ruamel NA. scanpy 1.7.0rc2.dev1+g2a123065. scipy 1.5.4. setuptools_scm NA. sinfo 0.3.1. six 1.15.0. sklearn 0.23.2. sparse 0.11.2. sphinxcontrib NA. storemagic NA. tables 3.6.1. tblib 1.7.0. texttable 1.6.3. tlz 0.11.1. toolz 0.11.1. tornado 6.1. traitlets 5.0.5. typing_extensions NA. wcwidth 0.2.5. yaml 5.3.1. zarr 2.6.1. zmq 20.0.0. zope NA. -----. IPython 7.19.0. jupyter_client 6.1.7. jupyter_core 4.7.0. notebook 6.1.5. -----. Python 3.8.5 (default, Sep 4 2020, 07:30:14) [GCC 7.3.0]. Linux-5.10.0-1-amd64-x86_64-with-glibc2.10. 8 logical CPU cores. -----. Session information updated at 2021-01-16 21:28. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1591
https://github.com/scverse/scanpy/issues/1591:5783,safety,updat,updated,5783,"-bb115cd014aa.png). Fig. 3. ![f2](https://user-images.githubusercontent.com/46717574/104822452-d9cb5780-5842-11eb-8606-f4a9a5c97893.png). Fig. 4. ![f3](https://user-images.githubusercontent.com/46717574/104822453-dcc64800-5842-11eb-827c-90db0525d4d4.png). Fig. 5. ![f4](https://user-images.githubusercontent.com/46717574/104822455-dfc13880-5842-11eb-9286-655b2210b182.png). Fig. 6. ![f6](https://user-images.githubusercontent.com/46717574/104822796-958d8680-5845-11eb-82e2-4b30597c6722.png). #### Versions. <details>. -----. anndata 0.7.5. scanpy 1.7.0rc2.dev1+g2a123065. sinfo 0.3.1. -----. PIL 8.0.1. anndata 0.7.5. asciitree NA. backcall 0.2.0. cairo 1.20.0. cffi 1.14.4. cloudpickle 1.6.0. colorama 0.4.4. coverage 5.3. cycler 0.10.0. cython_runtime NA. dask 2020.12.0. dateutil 2.8.1. decorator 4.4.2. fasteners NA. future_fstrings NA. get_version 2.1. h5py 2.10.0. igraph 0.8.3. ipykernel 5.4.2. ipython_genutils 0.2.0. jedi 0.17.2. joblib 0.17.0. kiwisolver 1.3.1. legacy_api_wrap 1.2. leidenalg 0.8.1. llvmlite 0.34.0. louvain 0.6.1. matplotlib 3.3.3. monotonic NA. mpl_toolkits NA. msgpack 1.0.0. natsort 7.0.1. numba 0.51.2. numcodecs 0.7.2. numexpr 2.7.1. numpy 1.19.4. packaging 20.8. pandas 1.1.4. parso 0.7.1. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. prompt_toolkit 3.0.8. psutil 5.7.3. ptyprocess 0.6.0. pygments 2.7.3. pyparsing 2.4.7. pytz 2020.5. ruamel NA. scanpy 1.7.0rc2.dev1+g2a123065. scipy 1.5.4. setuptools_scm NA. sinfo 0.3.1. six 1.15.0. sklearn 0.23.2. sparse 0.11.2. sphinxcontrib NA. storemagic NA. tables 3.6.1. tblib 1.7.0. texttable 1.6.3. tlz 0.11.1. toolz 0.11.1. tornado 6.1. traitlets 5.0.5. typing_extensions NA. wcwidth 0.2.5. yaml 5.3.1. zarr 2.6.1. zmq 20.0.0. zope NA. -----. IPython 7.19.0. jupyter_client 6.1.7. jupyter_core 4.7.0. notebook 6.1.5. -----. Python 3.8.5 (default, Sep 4 2020, 07:30:14) [GCC 7.3.0]. Linux-5.10.0-1-amd64-x86_64-with-glibc2.10. 8 logical CPU cores. -----. Session information updated at 2021-01-16 21:28. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1591
https://github.com/scverse/scanpy/issues/1591:2563,security,log,log,2563,", 1, 0), (0, 0, 1)]. sc.pl.heatmap(a, var_names=a.var_names[:50], groupby=""foo"", swap_axes=False, dendrogram=True) # fig. 4. sc.pl.heatmap(a, var_names=a.var_names[:50], groupby=""foo"", swap_axes=False, dendrogram=False) # fig. 5. # xlabels are not centered + horizntal lines are slightly shifted downwards (really have to zoom in). sc.pl.heatmap(a, var_names=a.var_names, groupby=""foo"", swap_axes=False, figsize=(50, 50)). ```. If you look closely (e.g. between fig. 2 and fig.3, or fig. 4 and fig. 5), the within group order is also broken. ```pytb. TypeError Traceback (most recent call last). <ipython-input-56-f1ba710dac43> in <module>. 9 . 10 sc.pl.heatmap(a, var_names=a.var_names[:5], groupby=None, swap_axes=True) # is shifted to the left (fig. 1). ---> 11 sc.pl.heatmap(a, var_names=a.var_names[:5], groupby=""foo"", swap_axes=True) . 12 . 13 a.X[:16, :] = 0. ~/.miniconda3/envs/cellrank/lib/python3.8/site-packages/scanpy/plotting/_anndata.py in heatmap(adata, var_names, groupby, use_raw, log, num_categories, dendrogram, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, standard_scale, swap_axes, show_gene_labels, show, save, figsize, **kwds). 1220 groupby_cmap,. 1221 norm,. -> 1222 ) = _plot_categories_as_colorblocks(. 1223 groupby_ax, obs_tidy, colors=groupby_colors, orientation='bottom'. 1224 ). ~/.miniconda3/envs/cellrank/lib/python3.8/site-packages/scanpy/plotting/_anndata.py in _plot_categories_as_colorblocks(groupby_ax, obs_tidy, colors, orientation, cmap_name). 2361 if len(labels) > 1:. 2362 groupby_ax.set_xticks(ticks). -> 2363 if max([len(x) for x in labels]) < 3:. 2364 # if the labels are small do not rotate them. 2365 rotation = 0. ~/.miniconda3/envs/cellrank/lib/python3.8/site-packages/scanpy/plotting/_anndata.py in <listcomp>(.0). 2361 if len(labels) > 1:. 2362 groupby_ax.set_xticks(ticks). -> 2363 if max([len(x) for x in labels]) < 3:. 2364 # if the labels are small do not rotate them. 2365 rotation = 0. TypeError: object of ty",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1591
https://github.com/scverse/scanpy/issues/1591:3229,security,rotat,rotate,3229,"r_names=a.var_names[:5], groupby=None, swap_axes=True) # is shifted to the left (fig. 1). ---> 11 sc.pl.heatmap(a, var_names=a.var_names[:5], groupby=""foo"", swap_axes=True) . 12 . 13 a.X[:16, :] = 0. ~/.miniconda3/envs/cellrank/lib/python3.8/site-packages/scanpy/plotting/_anndata.py in heatmap(adata, var_names, groupby, use_raw, log, num_categories, dendrogram, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, standard_scale, swap_axes, show_gene_labels, show, save, figsize, **kwds). 1220 groupby_cmap,. 1221 norm,. -> 1222 ) = _plot_categories_as_colorblocks(. 1223 groupby_ax, obs_tidy, colors=groupby_colors, orientation='bottom'. 1224 ). ~/.miniconda3/envs/cellrank/lib/python3.8/site-packages/scanpy/plotting/_anndata.py in _plot_categories_as_colorblocks(groupby_ax, obs_tidy, colors, orientation, cmap_name). 2361 if len(labels) > 1:. 2362 groupby_ax.set_xticks(ticks). -> 2363 if max([len(x) for x in labels]) < 3:. 2364 # if the labels are small do not rotate them. 2365 rotation = 0. ~/.miniconda3/envs/cellrank/lib/python3.8/site-packages/scanpy/plotting/_anndata.py in <listcomp>(.0). 2361 if len(labels) > 1:. 2362 groupby_ax.set_xticks(ticks). -> 2363 if max([len(x) for x in labels]) < 3:. 2364 # if the labels are small do not rotate them. 2365 rotation = 0. TypeError: object of type 'int' has no len(). ```. #### Figures. Fig. 1. ![f0](https://user-images.githubusercontent.com/46717574/104822526-6544e880-5843-11eb-8324-261e84eb45e4.png). Fig. 2. ![f1](https://user-images.githubusercontent.com/46717574/104822449-d637d080-5842-11eb-853d-bb115cd014aa.png). Fig. 3. ![f2](https://user-images.githubusercontent.com/46717574/104822452-d9cb5780-5842-11eb-8606-f4a9a5c97893.png). Fig. 4. ![f3](https://user-images.githubusercontent.com/46717574/104822453-dcc64800-5842-11eb-827c-90db0525d4d4.png). Fig. 5. ![f4](https://user-images.githubusercontent.com/46717574/104822455-dfc13880-5842-11eb-9286-655b2210b182.png). Fig. 6. ![f6](https://user-images.g",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1591
https://github.com/scverse/scanpy/issues/1591:3247,security,rotat,rotation,3247,"[:5], groupby=None, swap_axes=True) # is shifted to the left (fig. 1). ---> 11 sc.pl.heatmap(a, var_names=a.var_names[:5], groupby=""foo"", swap_axes=True) . 12 . 13 a.X[:16, :] = 0. ~/.miniconda3/envs/cellrank/lib/python3.8/site-packages/scanpy/plotting/_anndata.py in heatmap(adata, var_names, groupby, use_raw, log, num_categories, dendrogram, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, standard_scale, swap_axes, show_gene_labels, show, save, figsize, **kwds). 1220 groupby_cmap,. 1221 norm,. -> 1222 ) = _plot_categories_as_colorblocks(. 1223 groupby_ax, obs_tidy, colors=groupby_colors, orientation='bottom'. 1224 ). ~/.miniconda3/envs/cellrank/lib/python3.8/site-packages/scanpy/plotting/_anndata.py in _plot_categories_as_colorblocks(groupby_ax, obs_tidy, colors, orientation, cmap_name). 2361 if len(labels) > 1:. 2362 groupby_ax.set_xticks(ticks). -> 2363 if max([len(x) for x in labels]) < 3:. 2364 # if the labels are small do not rotate them. 2365 rotation = 0. ~/.miniconda3/envs/cellrank/lib/python3.8/site-packages/scanpy/plotting/_anndata.py in <listcomp>(.0). 2361 if len(labels) > 1:. 2362 groupby_ax.set_xticks(ticks). -> 2363 if max([len(x) for x in labels]) < 3:. 2364 # if the labels are small do not rotate them. 2365 rotation = 0. TypeError: object of type 'int' has no len(). ```. #### Figures. Fig. 1. ![f0](https://user-images.githubusercontent.com/46717574/104822526-6544e880-5843-11eb-8324-261e84eb45e4.png). Fig. 2. ![f1](https://user-images.githubusercontent.com/46717574/104822449-d637d080-5842-11eb-853d-bb115cd014aa.png). Fig. 3. ![f2](https://user-images.githubusercontent.com/46717574/104822452-d9cb5780-5842-11eb-8606-f4a9a5c97893.png). Fig. 4. ![f3](https://user-images.githubusercontent.com/46717574/104822453-dcc64800-5842-11eb-827c-90db0525d4d4.png). Fig. 5. ![f4](https://user-images.githubusercontent.com/46717574/104822455-dfc13880-5842-11eb-9286-655b2210b182.png). Fig. 6. ![f6](https://user-images.githubusercontent.co",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1591
https://github.com/scverse/scanpy/issues/1591:3510,security,rotat,rotate,3510,"py in heatmap(adata, var_names, groupby, use_raw, log, num_categories, dendrogram, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, standard_scale, swap_axes, show_gene_labels, show, save, figsize, **kwds). 1220 groupby_cmap,. 1221 norm,. -> 1222 ) = _plot_categories_as_colorblocks(. 1223 groupby_ax, obs_tidy, colors=groupby_colors, orientation='bottom'. 1224 ). ~/.miniconda3/envs/cellrank/lib/python3.8/site-packages/scanpy/plotting/_anndata.py in _plot_categories_as_colorblocks(groupby_ax, obs_tidy, colors, orientation, cmap_name). 2361 if len(labels) > 1:. 2362 groupby_ax.set_xticks(ticks). -> 2363 if max([len(x) for x in labels]) < 3:. 2364 # if the labels are small do not rotate them. 2365 rotation = 0. ~/.miniconda3/envs/cellrank/lib/python3.8/site-packages/scanpy/plotting/_anndata.py in <listcomp>(.0). 2361 if len(labels) > 1:. 2362 groupby_ax.set_xticks(ticks). -> 2363 if max([len(x) for x in labels]) < 3:. 2364 # if the labels are small do not rotate them. 2365 rotation = 0. TypeError: object of type 'int' has no len(). ```. #### Figures. Fig. 1. ![f0](https://user-images.githubusercontent.com/46717574/104822526-6544e880-5843-11eb-8324-261e84eb45e4.png). Fig. 2. ![f1](https://user-images.githubusercontent.com/46717574/104822449-d637d080-5842-11eb-853d-bb115cd014aa.png). Fig. 3. ![f2](https://user-images.githubusercontent.com/46717574/104822452-d9cb5780-5842-11eb-8606-f4a9a5c97893.png). Fig. 4. ![f3](https://user-images.githubusercontent.com/46717574/104822453-dcc64800-5842-11eb-827c-90db0525d4d4.png). Fig. 5. ![f4](https://user-images.githubusercontent.com/46717574/104822455-dfc13880-5842-11eb-9286-655b2210b182.png). Fig. 6. ![f6](https://user-images.githubusercontent.com/46717574/104822796-958d8680-5845-11eb-82e2-4b30597c6722.png). #### Versions. <details>. -----. anndata 0.7.5. scanpy 1.7.0rc2.dev1+g2a123065. sinfo 0.3.1. -----. PIL 8.0.1. anndata 0.7.5. asciitree NA. backcall 0.2.0. cairo 1.20.0. cffi 1.14.4. cloudpickle 1.6",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1591
https://github.com/scverse/scanpy/issues/1591:3528,security,rotat,rotation,3528,", var_names, groupby, use_raw, log, num_categories, dendrogram, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, standard_scale, swap_axes, show_gene_labels, show, save, figsize, **kwds). 1220 groupby_cmap,. 1221 norm,. -> 1222 ) = _plot_categories_as_colorblocks(. 1223 groupby_ax, obs_tidy, colors=groupby_colors, orientation='bottom'. 1224 ). ~/.miniconda3/envs/cellrank/lib/python3.8/site-packages/scanpy/plotting/_anndata.py in _plot_categories_as_colorblocks(groupby_ax, obs_tidy, colors, orientation, cmap_name). 2361 if len(labels) > 1:. 2362 groupby_ax.set_xticks(ticks). -> 2363 if max([len(x) for x in labels]) < 3:. 2364 # if the labels are small do not rotate them. 2365 rotation = 0. ~/.miniconda3/envs/cellrank/lib/python3.8/site-packages/scanpy/plotting/_anndata.py in <listcomp>(.0). 2361 if len(labels) > 1:. 2362 groupby_ax.set_xticks(ticks). -> 2363 if max([len(x) for x in labels]) < 3:. 2364 # if the labels are small do not rotate them. 2365 rotation = 0. TypeError: object of type 'int' has no len(). ```. #### Figures. Fig. 1. ![f0](https://user-images.githubusercontent.com/46717574/104822526-6544e880-5843-11eb-8324-261e84eb45e4.png). Fig. 2. ![f1](https://user-images.githubusercontent.com/46717574/104822449-d637d080-5842-11eb-853d-bb115cd014aa.png). Fig. 3. ![f2](https://user-images.githubusercontent.com/46717574/104822452-d9cb5780-5842-11eb-8606-f4a9a5c97893.png). Fig. 4. ![f3](https://user-images.githubusercontent.com/46717574/104822453-dcc64800-5842-11eb-827c-90db0525d4d4.png). Fig. 5. ![f4](https://user-images.githubusercontent.com/46717574/104822455-dfc13880-5842-11eb-9286-655b2210b182.png). Fig. 6. ![f6](https://user-images.githubusercontent.com/46717574/104822796-958d8680-5845-11eb-82e2-4b30597c6722.png). #### Versions. <details>. -----. anndata 0.7.5. scanpy 1.7.0rc2.dev1+g2a123065. sinfo 0.3.1. -----. PIL 8.0.1. anndata 0.7.5. asciitree NA. backcall 0.2.0. cairo 1.20.0. cffi 1.14.4. cloudpickle 1.6.0. colorama 0.4.4.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1591
https://github.com/scverse/scanpy/issues/1591:5737,security,log,logical,5737,"-bb115cd014aa.png). Fig. 3. ![f2](https://user-images.githubusercontent.com/46717574/104822452-d9cb5780-5842-11eb-8606-f4a9a5c97893.png). Fig. 4. ![f3](https://user-images.githubusercontent.com/46717574/104822453-dcc64800-5842-11eb-827c-90db0525d4d4.png). Fig. 5. ![f4](https://user-images.githubusercontent.com/46717574/104822455-dfc13880-5842-11eb-9286-655b2210b182.png). Fig. 6. ![f6](https://user-images.githubusercontent.com/46717574/104822796-958d8680-5845-11eb-82e2-4b30597c6722.png). #### Versions. <details>. -----. anndata 0.7.5. scanpy 1.7.0rc2.dev1+g2a123065. sinfo 0.3.1. -----. PIL 8.0.1. anndata 0.7.5. asciitree NA. backcall 0.2.0. cairo 1.20.0. cffi 1.14.4. cloudpickle 1.6.0. colorama 0.4.4. coverage 5.3. cycler 0.10.0. cython_runtime NA. dask 2020.12.0. dateutil 2.8.1. decorator 4.4.2. fasteners NA. future_fstrings NA. get_version 2.1. h5py 2.10.0. igraph 0.8.3. ipykernel 5.4.2. ipython_genutils 0.2.0. jedi 0.17.2. joblib 0.17.0. kiwisolver 1.3.1. legacy_api_wrap 1.2. leidenalg 0.8.1. llvmlite 0.34.0. louvain 0.6.1. matplotlib 3.3.3. monotonic NA. mpl_toolkits NA. msgpack 1.0.0. natsort 7.0.1. numba 0.51.2. numcodecs 0.7.2. numexpr 2.7.1. numpy 1.19.4. packaging 20.8. pandas 1.1.4. parso 0.7.1. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. prompt_toolkit 3.0.8. psutil 5.7.3. ptyprocess 0.6.0. pygments 2.7.3. pyparsing 2.4.7. pytz 2020.5. ruamel NA. scanpy 1.7.0rc2.dev1+g2a123065. scipy 1.5.4. setuptools_scm NA. sinfo 0.3.1. six 1.15.0. sklearn 0.23.2. sparse 0.11.2. sphinxcontrib NA. storemagic NA. tables 3.6.1. tblib 1.7.0. texttable 1.6.3. tlz 0.11.1. toolz 0.11.1. tornado 6.1. traitlets 5.0.5. typing_extensions NA. wcwidth 0.2.5. yaml 5.3.1. zarr 2.6.1. zmq 20.0.0. zope NA. -----. IPython 7.19.0. jupyter_client 6.1.7. jupyter_core 4.7.0. notebook 6.1.5. -----. Python 3.8.5 (default, Sep 4 2020, 07:30:14) [GCC 7.3.0]. Linux-5.10.0-1-amd64-x86_64-with-glibc2.10. 8 logical CPU cores. -----. Session information updated at 2021-01-16 21:28. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1591
https://github.com/scverse/scanpy/issues/1591:5763,security,Session,Session,5763,"-bb115cd014aa.png). Fig. 3. ![f2](https://user-images.githubusercontent.com/46717574/104822452-d9cb5780-5842-11eb-8606-f4a9a5c97893.png). Fig. 4. ![f3](https://user-images.githubusercontent.com/46717574/104822453-dcc64800-5842-11eb-827c-90db0525d4d4.png). Fig. 5. ![f4](https://user-images.githubusercontent.com/46717574/104822455-dfc13880-5842-11eb-9286-655b2210b182.png). Fig. 6. ![f6](https://user-images.githubusercontent.com/46717574/104822796-958d8680-5845-11eb-82e2-4b30597c6722.png). #### Versions. <details>. -----. anndata 0.7.5. scanpy 1.7.0rc2.dev1+g2a123065. sinfo 0.3.1. -----. PIL 8.0.1. anndata 0.7.5. asciitree NA. backcall 0.2.0. cairo 1.20.0. cffi 1.14.4. cloudpickle 1.6.0. colorama 0.4.4. coverage 5.3. cycler 0.10.0. cython_runtime NA. dask 2020.12.0. dateutil 2.8.1. decorator 4.4.2. fasteners NA. future_fstrings NA. get_version 2.1. h5py 2.10.0. igraph 0.8.3. ipykernel 5.4.2. ipython_genutils 0.2.0. jedi 0.17.2. joblib 0.17.0. kiwisolver 1.3.1. legacy_api_wrap 1.2. leidenalg 0.8.1. llvmlite 0.34.0. louvain 0.6.1. matplotlib 3.3.3. monotonic NA. mpl_toolkits NA. msgpack 1.0.0. natsort 7.0.1. numba 0.51.2. numcodecs 0.7.2. numexpr 2.7.1. numpy 1.19.4. packaging 20.8. pandas 1.1.4. parso 0.7.1. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. prompt_toolkit 3.0.8. psutil 5.7.3. ptyprocess 0.6.0. pygments 2.7.3. pyparsing 2.4.7. pytz 2020.5. ruamel NA. scanpy 1.7.0rc2.dev1+g2a123065. scipy 1.5.4. setuptools_scm NA. sinfo 0.3.1. six 1.15.0. sklearn 0.23.2. sparse 0.11.2. sphinxcontrib NA. storemagic NA. tables 3.6.1. tblib 1.7.0. texttable 1.6.3. tlz 0.11.1. toolz 0.11.1. tornado 6.1. traitlets 5.0.5. typing_extensions NA. wcwidth 0.2.5. yaml 5.3.1. zarr 2.6.1. zmq 20.0.0. zope NA. -----. IPython 7.19.0. jupyter_client 6.1.7. jupyter_core 4.7.0. notebook 6.1.5. -----. Python 3.8.5 (default, Sep 4 2020, 07:30:14) [GCC 7.3.0]. Linux-5.10.0-1-amd64-x86_64-with-glibc2.10. 8 logical CPU cores. -----. Session information updated at 2021-01-16 21:28. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1591
https://github.com/scverse/scanpy/issues/1591:5783,security,updat,updated,5783,"-bb115cd014aa.png). Fig. 3. ![f2](https://user-images.githubusercontent.com/46717574/104822452-d9cb5780-5842-11eb-8606-f4a9a5c97893.png). Fig. 4. ![f3](https://user-images.githubusercontent.com/46717574/104822453-dcc64800-5842-11eb-827c-90db0525d4d4.png). Fig. 5. ![f4](https://user-images.githubusercontent.com/46717574/104822455-dfc13880-5842-11eb-9286-655b2210b182.png). Fig. 6. ![f6](https://user-images.githubusercontent.com/46717574/104822796-958d8680-5845-11eb-82e2-4b30597c6722.png). #### Versions. <details>. -----. anndata 0.7.5. scanpy 1.7.0rc2.dev1+g2a123065. sinfo 0.3.1. -----. PIL 8.0.1. anndata 0.7.5. asciitree NA. backcall 0.2.0. cairo 1.20.0. cffi 1.14.4. cloudpickle 1.6.0. colorama 0.4.4. coverage 5.3. cycler 0.10.0. cython_runtime NA. dask 2020.12.0. dateutil 2.8.1. decorator 4.4.2. fasteners NA. future_fstrings NA. get_version 2.1. h5py 2.10.0. igraph 0.8.3. ipykernel 5.4.2. ipython_genutils 0.2.0. jedi 0.17.2. joblib 0.17.0. kiwisolver 1.3.1. legacy_api_wrap 1.2. leidenalg 0.8.1. llvmlite 0.34.0. louvain 0.6.1. matplotlib 3.3.3. monotonic NA. mpl_toolkits NA. msgpack 1.0.0. natsort 7.0.1. numba 0.51.2. numcodecs 0.7.2. numexpr 2.7.1. numpy 1.19.4. packaging 20.8. pandas 1.1.4. parso 0.7.1. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. prompt_toolkit 3.0.8. psutil 5.7.3. ptyprocess 0.6.0. pygments 2.7.3. pyparsing 2.4.7. pytz 2020.5. ruamel NA. scanpy 1.7.0rc2.dev1+g2a123065. scipy 1.5.4. setuptools_scm NA. sinfo 0.3.1. six 1.15.0. sklearn 0.23.2. sparse 0.11.2. sphinxcontrib NA. storemagic NA. tables 3.6.1. tblib 1.7.0. texttable 1.6.3. tlz 0.11.1. toolz 0.11.1. tornado 6.1. traitlets 5.0.5. typing_extensions NA. wcwidth 0.2.5. yaml 5.3.1. zarr 2.6.1. zmq 20.0.0. zope NA. -----. IPython 7.19.0. jupyter_client 6.1.7. jupyter_core 4.7.0. notebook 6.1.5. -----. Python 3.8.5 (default, Sep 4 2020, 07:30:14) [GCC 7.3.0]. Linux-5.10.0-1-amd64-x86_64-with-glibc2.10. 8 logical CPU cores. -----. Session information updated at 2021-01-16 21:28. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1591
https://github.com/scverse/scanpy/issues/1591:2126,testability,Trace,Traceback,2126,"normal(size=(16, 50)). a.obs['foo'].iloc[:16, :] = 0. a.obs['foo'].iloc[16:32, :] = 1. a.obs['foo'].iloc[32:, :] = 2. # wrong label-color mapping. sc.pl.heatmap(a, var_names=a.var_names[:50], groupby=""foo"", swap_axes=False, dendrogram=True) # fig. 2. sc.pl.heatmap(a, var_names=a.var_names[:50], groupby=""foo"", swap_axes=False, dendrogram=False) # fig. 3. # but not when there are colors in `.uns. a.uns['foo_colors'] = [(1, 0, 0), (0, 1, 0), (0, 0, 1)]. sc.pl.heatmap(a, var_names=a.var_names[:50], groupby=""foo"", swap_axes=False, dendrogram=True) # fig. 4. sc.pl.heatmap(a, var_names=a.var_names[:50], groupby=""foo"", swap_axes=False, dendrogram=False) # fig. 5. # xlabels are not centered + horizntal lines are slightly shifted downwards (really have to zoom in). sc.pl.heatmap(a, var_names=a.var_names, groupby=""foo"", swap_axes=False, figsize=(50, 50)). ```. If you look closely (e.g. between fig. 2 and fig.3, or fig. 4 and fig. 5), the within group order is also broken. ```pytb. TypeError Traceback (most recent call last). <ipython-input-56-f1ba710dac43> in <module>. 9 . 10 sc.pl.heatmap(a, var_names=a.var_names[:5], groupby=None, swap_axes=True) # is shifted to the left (fig. 1). ---> 11 sc.pl.heatmap(a, var_names=a.var_names[:5], groupby=""foo"", swap_axes=True) . 12 . 13 a.X[:16, :] = 0. ~/.miniconda3/envs/cellrank/lib/python3.8/site-packages/scanpy/plotting/_anndata.py in heatmap(adata, var_names, groupby, use_raw, log, num_categories, dendrogram, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, standard_scale, swap_axes, show_gene_labels, show, save, figsize, **kwds). 1220 groupby_cmap,. 1221 norm,. -> 1222 ) = _plot_categories_as_colorblocks(. 1223 groupby_ax, obs_tidy, colors=groupby_colors, orientation='bottom'. 1224 ). ~/.miniconda3/envs/cellrank/lib/python3.8/site-packages/scanpy/plotting/_anndata.py in _plot_categories_as_colorblocks(groupby_ax, obs_tidy, colors, orientation, cmap_name). 2361 if len(labels) > 1:. 2362 groupby_ax.set_xt",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1591
https://github.com/scverse/scanpy/issues/1591:2563,testability,log,log,2563,", 1, 0), (0, 0, 1)]. sc.pl.heatmap(a, var_names=a.var_names[:50], groupby=""foo"", swap_axes=False, dendrogram=True) # fig. 4. sc.pl.heatmap(a, var_names=a.var_names[:50], groupby=""foo"", swap_axes=False, dendrogram=False) # fig. 5. # xlabels are not centered + horizntal lines are slightly shifted downwards (really have to zoom in). sc.pl.heatmap(a, var_names=a.var_names, groupby=""foo"", swap_axes=False, figsize=(50, 50)). ```. If you look closely (e.g. between fig. 2 and fig.3, or fig. 4 and fig. 5), the within group order is also broken. ```pytb. TypeError Traceback (most recent call last). <ipython-input-56-f1ba710dac43> in <module>. 9 . 10 sc.pl.heatmap(a, var_names=a.var_names[:5], groupby=None, swap_axes=True) # is shifted to the left (fig. 1). ---> 11 sc.pl.heatmap(a, var_names=a.var_names[:5], groupby=""foo"", swap_axes=True) . 12 . 13 a.X[:16, :] = 0. ~/.miniconda3/envs/cellrank/lib/python3.8/site-packages/scanpy/plotting/_anndata.py in heatmap(adata, var_names, groupby, use_raw, log, num_categories, dendrogram, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, standard_scale, swap_axes, show_gene_labels, show, save, figsize, **kwds). 1220 groupby_cmap,. 1221 norm,. -> 1222 ) = _plot_categories_as_colorblocks(. 1223 groupby_ax, obs_tidy, colors=groupby_colors, orientation='bottom'. 1224 ). ~/.miniconda3/envs/cellrank/lib/python3.8/site-packages/scanpy/plotting/_anndata.py in _plot_categories_as_colorblocks(groupby_ax, obs_tidy, colors, orientation, cmap_name). 2361 if len(labels) > 1:. 2362 groupby_ax.set_xticks(ticks). -> 2363 if max([len(x) for x in labels]) < 3:. 2364 # if the labels are small do not rotate them. 2365 rotation = 0. ~/.miniconda3/envs/cellrank/lib/python3.8/site-packages/scanpy/plotting/_anndata.py in <listcomp>(.0). 2361 if len(labels) > 1:. 2362 groupby_ax.set_xticks(ticks). -> 2363 if max([len(x) for x in labels]) < 3:. 2364 # if the labels are small do not rotate them. 2365 rotation = 0. TypeError: object of ty",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1591
https://github.com/scverse/scanpy/issues/1591:4533,testability,coverag,coverage,4533,= 0. TypeError: object of type 'int' has no len(). ```. #### Figures. Fig. 1. ![f0](https://user-images.githubusercontent.com/46717574/104822526-6544e880-5843-11eb-8324-261e84eb45e4.png). Fig. 2. ![f1](https://user-images.githubusercontent.com/46717574/104822449-d637d080-5842-11eb-853d-bb115cd014aa.png). Fig. 3. ![f2](https://user-images.githubusercontent.com/46717574/104822452-d9cb5780-5842-11eb-8606-f4a9a5c97893.png). Fig. 4. ![f3](https://user-images.githubusercontent.com/46717574/104822453-dcc64800-5842-11eb-827c-90db0525d4d4.png). Fig. 5. ![f4](https://user-images.githubusercontent.com/46717574/104822455-dfc13880-5842-11eb-9286-655b2210b182.png). Fig. 6. ![f6](https://user-images.githubusercontent.com/46717574/104822796-958d8680-5845-11eb-82e2-4b30597c6722.png). #### Versions. <details>. -----. anndata 0.7.5. scanpy 1.7.0rc2.dev1+g2a123065. sinfo 0.3.1. -----. PIL 8.0.1. anndata 0.7.5. asciitree NA. backcall 0.2.0. cairo 1.20.0. cffi 1.14.4. cloudpickle 1.6.0. colorama 0.4.4. coverage 5.3. cycler 0.10.0. cython_runtime NA. dask 2020.12.0. dateutil 2.8.1. decorator 4.4.2. fasteners NA. future_fstrings NA. get_version 2.1. h5py 2.10.0. igraph 0.8.3. ipykernel 5.4.2. ipython_genutils 0.2.0. jedi 0.17.2. joblib 0.17.0. kiwisolver 1.3.1. legacy_api_wrap 1.2. leidenalg 0.8.1. llvmlite 0.34.0. louvain 0.6.1. matplotlib 3.3.3. monotonic NA. mpl_toolkits NA. msgpack 1.0.0. natsort 7.0.1. numba 0.51.2. numcodecs 0.7.2. numexpr 2.7.1. numpy 1.19.4. packaging 20.8. pandas 1.1.4. parso 0.7.1. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. prompt_toolkit 3.0.8. psutil 5.7.3. ptyprocess 0.6.0. pygments 2.7.3. pyparsing 2.4.7. pytz 2020.5. ruamel NA. scanpy 1.7.0rc2.dev1+g2a123065. scipy 1.5.4. setuptools_scm NA. sinfo 0.3.1. six 1.15.0. sklearn 0.23.2. sparse 0.11.2. sphinxcontrib NA. storemagic NA. tables 3.6.1. tblib 1.7.0. texttable 1.6.3. tlz 0.11.1. toolz 0.11.1. tornado 6.1. traitlets 5.0.5. typing_extensions NA. wcwidth 0.2.5. yaml 5.3.1. zarr 2.6.1. zmq 20.0.0. z,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1591
https://github.com/scverse/scanpy/issues/1591:5737,testability,log,logical,5737,"-bb115cd014aa.png). Fig. 3. ![f2](https://user-images.githubusercontent.com/46717574/104822452-d9cb5780-5842-11eb-8606-f4a9a5c97893.png). Fig. 4. ![f3](https://user-images.githubusercontent.com/46717574/104822453-dcc64800-5842-11eb-827c-90db0525d4d4.png). Fig. 5. ![f4](https://user-images.githubusercontent.com/46717574/104822455-dfc13880-5842-11eb-9286-655b2210b182.png). Fig. 6. ![f6](https://user-images.githubusercontent.com/46717574/104822796-958d8680-5845-11eb-82e2-4b30597c6722.png). #### Versions. <details>. -----. anndata 0.7.5. scanpy 1.7.0rc2.dev1+g2a123065. sinfo 0.3.1. -----. PIL 8.0.1. anndata 0.7.5. asciitree NA. backcall 0.2.0. cairo 1.20.0. cffi 1.14.4. cloudpickle 1.6.0. colorama 0.4.4. coverage 5.3. cycler 0.10.0. cython_runtime NA. dask 2020.12.0. dateutil 2.8.1. decorator 4.4.2. fasteners NA. future_fstrings NA. get_version 2.1. h5py 2.10.0. igraph 0.8.3. ipykernel 5.4.2. ipython_genutils 0.2.0. jedi 0.17.2. joblib 0.17.0. kiwisolver 1.3.1. legacy_api_wrap 1.2. leidenalg 0.8.1. llvmlite 0.34.0. louvain 0.6.1. matplotlib 3.3.3. monotonic NA. mpl_toolkits NA. msgpack 1.0.0. natsort 7.0.1. numba 0.51.2. numcodecs 0.7.2. numexpr 2.7.1. numpy 1.19.4. packaging 20.8. pandas 1.1.4. parso 0.7.1. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. prompt_toolkit 3.0.8. psutil 5.7.3. ptyprocess 0.6.0. pygments 2.7.3. pyparsing 2.4.7. pytz 2020.5. ruamel NA. scanpy 1.7.0rc2.dev1+g2a123065. scipy 1.5.4. setuptools_scm NA. sinfo 0.3.1. six 1.15.0. sklearn 0.23.2. sparse 0.11.2. sphinxcontrib NA. storemagic NA. tables 3.6.1. tblib 1.7.0. texttable 1.6.3. tlz 0.11.1. toolz 0.11.1. tornado 6.1. traitlets 5.0.5. typing_extensions NA. wcwidth 0.2.5. yaml 5.3.1. zarr 2.6.1. zmq 20.0.0. zope NA. -----. IPython 7.19.0. jupyter_client 6.1.7. jupyter_core 4.7.0. notebook 6.1.5. -----. Python 3.8.5 (default, Sep 4 2020, 07:30:14) [GCC 7.3.0]. Linux-5.10.0-1-amd64-x86_64-with-glibc2.10. 8 logical CPU cores. -----. Session information updated at 2021-01-16 21:28. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1591
https://github.com/scverse/scanpy/issues/1591:119,usability,confirm,confirmed,119,"Heatmap inconsistent order/swap_axes; - [x] I have checked that this issue has not already been reported. - [x] I have confirmed this bug exists on the latest version of scanpy. - [x] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. ### Minimal code sample (that we can copy&paste without having any data). ```python. import numpy as np. import pandas as pd. import scanpy as sc. from anndata import AnnData. a = AnnData(np.random.RandomState(42).normal(size=(50, 50)),. obs={""foo"": pd.Categorical(np.random.choice([0, 1, 2], size=50))}). sc.pl.heatmap(a, var_names=a.var_names[:5], groupby=None, swap_axes=True) # is shifted to the left (fig. 1). sc.pl.heatmap(a, var_names=a.var_names[:5], groupby=""foo"", swap_axes=True) # see the tb (if there's only 1 category, in the group, it doesn't crash). a.X[:16, :] = 10 + np.random.RandomState(42).normal(size=(16, 50)). a.X[16:32, :] = -10 + np.random.RandomState(42).normal(size=(16, 50)). a.obs['foo'].iloc[:16, :] = 0. a.obs['foo'].iloc[16:32, :] = 1. a.obs['foo'].iloc[32:, :] = 2. # wrong label-color mapping. sc.pl.heatmap(a, var_names=a.var_names[:50], groupby=""foo"", swap_axes=False, dendrogram=True) # fig. 2. sc.pl.heatmap(a, var_names=a.var_names[:50], groupby=""foo"", swap_axes=False, dendrogram=False) # fig. 3. # but not when there are colors in `.uns. a.uns['foo_colors'] = [(1, 0, 0), (0, 1, 0), (0, 0, 1)]. sc.pl.heatmap(a, var_names=a.var_names[:50], groupby=""foo"", swap_axes=False, dendrogram=True) # fig. 4. sc.pl.heatmap(a, var_names=a.var_names[:50], groupby=""foo"", swap_axes=False, dendrogram=False) # fig. 5. # xlabels are not centered + horizntal lines are slightly shifted downwards (really have to zoom in). sc.pl.heatmap(a, var_names=a.var_names, groupby=""foo"", swap_axes=False, figsize=(50, 50)). ```. If you ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1591
https://github.com/scverse/scanpy/issues/1591:202,usability,confirm,confirmed,202,"Heatmap inconsistent order/swap_axes; - [x] I have checked that this issue has not already been reported. - [x] I have confirmed this bug exists on the latest version of scanpy. - [x] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. ### Minimal code sample (that we can copy&paste without having any data). ```python. import numpy as np. import pandas as pd. import scanpy as sc. from anndata import AnnData. a = AnnData(np.random.RandomState(42).normal(size=(50, 50)),. obs={""foo"": pd.Categorical(np.random.choice([0, 1, 2], size=50))}). sc.pl.heatmap(a, var_names=a.var_names[:5], groupby=None, swap_axes=True) # is shifted to the left (fig. 1). sc.pl.heatmap(a, var_names=a.var_names[:5], groupby=""foo"", swap_axes=True) # see the tb (if there's only 1 category, in the group, it doesn't crash). a.X[:16, :] = 10 + np.random.RandomState(42).normal(size=(16, 50)). a.X[16:32, :] = -10 + np.random.RandomState(42).normal(size=(16, 50)). a.obs['foo'].iloc[:16, :] = 0. a.obs['foo'].iloc[16:32, :] = 1. a.obs['foo'].iloc[32:, :] = 2. # wrong label-color mapping. sc.pl.heatmap(a, var_names=a.var_names[:50], groupby=""foo"", swap_axes=False, dendrogram=True) # fig. 2. sc.pl.heatmap(a, var_names=a.var_names[:50], groupby=""foo"", swap_axes=False, dendrogram=False) # fig. 3. # but not when there are colors in `.uns. a.uns['foo_colors'] = [(1, 0, 0), (0, 1, 0), (0, 0, 1)]. sc.pl.heatmap(a, var_names=a.var_names[:50], groupby=""foo"", swap_axes=False, dendrogram=True) # fig. 4. sc.pl.heatmap(a, var_names=a.var_names[:50], groupby=""foo"", swap_axes=False, dendrogram=False) # fig. 5. # xlabels are not centered + horizntal lines are slightly shifted downwards (really have to zoom in). sc.pl.heatmap(a, var_names=a.var_names, groupby=""foo"", swap_axes=False, figsize=(50, 50)). ```. If you ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1591
https://github.com/scverse/scanpy/issues/1591:293,usability,guid,guide,293,"Heatmap inconsistent order/swap_axes; - [x] I have checked that this issue has not already been reported. - [x] I have confirmed this bug exists on the latest version of scanpy. - [x] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. ### Minimal code sample (that we can copy&paste without having any data). ```python. import numpy as np. import pandas as pd. import scanpy as sc. from anndata import AnnData. a = AnnData(np.random.RandomState(42).normal(size=(50, 50)),. obs={""foo"": pd.Categorical(np.random.choice([0, 1, 2], size=50))}). sc.pl.heatmap(a, var_names=a.var_names[:5], groupby=None, swap_axes=True) # is shifted to the left (fig. 1). sc.pl.heatmap(a, var_names=a.var_names[:5], groupby=""foo"", swap_axes=True) # see the tb (if there's only 1 category, in the group, it doesn't crash). a.X[:16, :] = 10 + np.random.RandomState(42).normal(size=(16, 50)). a.X[16:32, :] = -10 + np.random.RandomState(42).normal(size=(16, 50)). a.obs['foo'].iloc[:16, :] = 0. a.obs['foo'].iloc[16:32, :] = 1. a.obs['foo'].iloc[32:, :] = 2. # wrong label-color mapping. sc.pl.heatmap(a, var_names=a.var_names[:50], groupby=""foo"", swap_axes=False, dendrogram=True) # fig. 2. sc.pl.heatmap(a, var_names=a.var_names[:50], groupby=""foo"", swap_axes=False, dendrogram=False) # fig. 3. # but not when there are colors in `.uns. a.uns['foo_colors'] = [(1, 0, 0), (0, 1, 0), (0, 0, 1)]. sc.pl.heatmap(a, var_names=a.var_names[:50], groupby=""foo"", swap_axes=False, dendrogram=True) # fig. 4. sc.pl.heatmap(a, var_names=a.var_names[:50], groupby=""foo"", swap_axes=False, dendrogram=False) # fig. 5. # xlabels are not centered + horizntal lines are slightly shifted downwards (really have to zoom in). sc.pl.heatmap(a, var_names=a.var_names, groupby=""foo"", swap_axes=False, figsize=(50, 50)). ```. If you ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1591
https://github.com/scverse/scanpy/issues/1591:348,usability,minim,minimal-bug-reports,348,"Heatmap inconsistent order/swap_axes; - [x] I have checked that this issue has not already been reported. - [x] I have confirmed this bug exists on the latest version of scanpy. - [x] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. ### Minimal code sample (that we can copy&paste without having any data). ```python. import numpy as np. import pandas as pd. import scanpy as sc. from anndata import AnnData. a = AnnData(np.random.RandomState(42).normal(size=(50, 50)),. obs={""foo"": pd.Categorical(np.random.choice([0, 1, 2], size=50))}). sc.pl.heatmap(a, var_names=a.var_names[:5], groupby=None, swap_axes=True) # is shifted to the left (fig. 1). sc.pl.heatmap(a, var_names=a.var_names[:5], groupby=""foo"", swap_axes=True) # see the tb (if there's only 1 category, in the group, it doesn't crash). a.X[:16, :] = 10 + np.random.RandomState(42).normal(size=(16, 50)). a.X[16:32, :] = -10 + np.random.RandomState(42).normal(size=(16, 50)). a.obs['foo'].iloc[:16, :] = 0. a.obs['foo'].iloc[16:32, :] = 1. a.obs['foo'].iloc[32:, :] = 2. # wrong label-color mapping. sc.pl.heatmap(a, var_names=a.var_names[:50], groupby=""foo"", swap_axes=False, dendrogram=True) # fig. 2. sc.pl.heatmap(a, var_names=a.var_names[:50], groupby=""foo"", swap_axes=False, dendrogram=False) # fig. 3. # but not when there are colors in `.uns. a.uns['foo_colors'] = [(1, 0, 0), (0, 1, 0), (0, 0, 1)]. sc.pl.heatmap(a, var_names=a.var_names[:50], groupby=""foo"", swap_axes=False, dendrogram=True) # fig. 4. sc.pl.heatmap(a, var_names=a.var_names[:50], groupby=""foo"", swap_axes=False, dendrogram=False) # fig. 5. # xlabels are not centered + horizntal lines are slightly shifted downwards (really have to zoom in). sc.pl.heatmap(a, var_names=a.var_names, groupby=""foo"", swap_axes=False, figsize=(50, 50)). ```. If you ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1591
https://github.com/scverse/scanpy/issues/1591:454,usability,Minim,Minimal,454,"Heatmap inconsistent order/swap_axes; - [x] I have checked that this issue has not already been reported. - [x] I have confirmed this bug exists on the latest version of scanpy. - [x] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. ### Minimal code sample (that we can copy&paste without having any data). ```python. import numpy as np. import pandas as pd. import scanpy as sc. from anndata import AnnData. a = AnnData(np.random.RandomState(42).normal(size=(50, 50)),. obs={""foo"": pd.Categorical(np.random.choice([0, 1, 2], size=50))}). sc.pl.heatmap(a, var_names=a.var_names[:5], groupby=None, swap_axes=True) # is shifted to the left (fig. 1). sc.pl.heatmap(a, var_names=a.var_names[:5], groupby=""foo"", swap_axes=True) # see the tb (if there's only 1 category, in the group, it doesn't crash). a.X[:16, :] = 10 + np.random.RandomState(42).normal(size=(16, 50)). a.X[16:32, :] = -10 + np.random.RandomState(42).normal(size=(16, 50)). a.obs['foo'].iloc[:16, :] = 0. a.obs['foo'].iloc[16:32, :] = 1. a.obs['foo'].iloc[32:, :] = 2. # wrong label-color mapping. sc.pl.heatmap(a, var_names=a.var_names[:50], groupby=""foo"", swap_axes=False, dendrogram=True) # fig. 2. sc.pl.heatmap(a, var_names=a.var_names[:50], groupby=""foo"", swap_axes=False, dendrogram=False) # fig. 3. # but not when there are colors in `.uns. a.uns['foo_colors'] = [(1, 0, 0), (0, 1, 0), (0, 0, 1)]. sc.pl.heatmap(a, var_names=a.var_names[:50], groupby=""foo"", swap_axes=False, dendrogram=True) # fig. 4. sc.pl.heatmap(a, var_names=a.var_names[:50], groupby=""foo"", swap_axes=False, dendrogram=False) # fig. 5. # xlabels are not centered + horizntal lines are slightly shifted downwards (really have to zoom in). sc.pl.heatmap(a, var_names=a.var_names, groupby=""foo"", swap_axes=False, figsize=(50, 50)). ```. If you ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1591
https://github.com/scverse/scanpy/issues/1591:2005,usability,close,closely,2005,"ash). a.X[:16, :] = 10 + np.random.RandomState(42).normal(size=(16, 50)). a.X[16:32, :] = -10 + np.random.RandomState(42).normal(size=(16, 50)). a.obs['foo'].iloc[:16, :] = 0. a.obs['foo'].iloc[16:32, :] = 1. a.obs['foo'].iloc[32:, :] = 2. # wrong label-color mapping. sc.pl.heatmap(a, var_names=a.var_names[:50], groupby=""foo"", swap_axes=False, dendrogram=True) # fig. 2. sc.pl.heatmap(a, var_names=a.var_names[:50], groupby=""foo"", swap_axes=False, dendrogram=False) # fig. 3. # but not when there are colors in `.uns. a.uns['foo_colors'] = [(1, 0, 0), (0, 1, 0), (0, 0, 1)]. sc.pl.heatmap(a, var_names=a.var_names[:50], groupby=""foo"", swap_axes=False, dendrogram=True) # fig. 4. sc.pl.heatmap(a, var_names=a.var_names[:50], groupby=""foo"", swap_axes=False, dendrogram=False) # fig. 5. # xlabels are not centered + horizntal lines are slightly shifted downwards (really have to zoom in). sc.pl.heatmap(a, var_names=a.var_names, groupby=""foo"", swap_axes=False, figsize=(50, 50)). ```. If you look closely (e.g. between fig. 2 and fig.3, or fig. 4 and fig. 5), the within group order is also broken. ```pytb. TypeError Traceback (most recent call last). <ipython-input-56-f1ba710dac43> in <module>. 9 . 10 sc.pl.heatmap(a, var_names=a.var_names[:5], groupby=None, swap_axes=True) # is shifted to the left (fig. 1). ---> 11 sc.pl.heatmap(a, var_names=a.var_names[:5], groupby=""foo"", swap_axes=True) . 12 . 13 a.X[:16, :] = 0. ~/.miniconda3/envs/cellrank/lib/python3.8/site-packages/scanpy/plotting/_anndata.py in heatmap(adata, var_names, groupby, use_raw, log, num_categories, dendrogram, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, standard_scale, swap_axes, show_gene_labels, show, save, figsize, **kwds). 1220 groupby_cmap,. 1221 norm,. -> 1222 ) = _plot_categories_as_colorblocks(. 1223 groupby_ax, obs_tidy, colors=groupby_colors, orientation='bottom'. 1224 ). ~/.miniconda3/envs/cellrank/lib/python3.8/site-packages/scanpy/plotting/_anndata.py in _plot_categor",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1591
https://github.com/scverse/scanpy/issues/1591:2170,usability,input,input-,2170,"16, :] = 0. a.obs['foo'].iloc[16:32, :] = 1. a.obs['foo'].iloc[32:, :] = 2. # wrong label-color mapping. sc.pl.heatmap(a, var_names=a.var_names[:50], groupby=""foo"", swap_axes=False, dendrogram=True) # fig. 2. sc.pl.heatmap(a, var_names=a.var_names[:50], groupby=""foo"", swap_axes=False, dendrogram=False) # fig. 3. # but not when there are colors in `.uns. a.uns['foo_colors'] = [(1, 0, 0), (0, 1, 0), (0, 0, 1)]. sc.pl.heatmap(a, var_names=a.var_names[:50], groupby=""foo"", swap_axes=False, dendrogram=True) # fig. 4. sc.pl.heatmap(a, var_names=a.var_names[:50], groupby=""foo"", swap_axes=False, dendrogram=False) # fig. 5. # xlabels are not centered + horizntal lines are slightly shifted downwards (really have to zoom in). sc.pl.heatmap(a, var_names=a.var_names, groupby=""foo"", swap_axes=False, figsize=(50, 50)). ```. If you look closely (e.g. between fig. 2 and fig.3, or fig. 4 and fig. 5), the within group order is also broken. ```pytb. TypeError Traceback (most recent call last). <ipython-input-56-f1ba710dac43> in <module>. 9 . 10 sc.pl.heatmap(a, var_names=a.var_names[:5], groupby=None, swap_axes=True) # is shifted to the left (fig. 1). ---> 11 sc.pl.heatmap(a, var_names=a.var_names[:5], groupby=""foo"", swap_axes=True) . 12 . 13 a.X[:16, :] = 0. ~/.miniconda3/envs/cellrank/lib/python3.8/site-packages/scanpy/plotting/_anndata.py in heatmap(adata, var_names, groupby, use_raw, log, num_categories, dendrogram, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, standard_scale, swap_axes, show_gene_labels, show, save, figsize, **kwds). 1220 groupby_cmap,. 1221 norm,. -> 1222 ) = _plot_categories_as_colorblocks(. 1223 groupby_ax, obs_tidy, colors=groupby_colors, orientation='bottom'. 1224 ). ~/.miniconda3/envs/cellrank/lib/python3.8/site-packages/scanpy/plotting/_anndata.py in _plot_categories_as_colorblocks(groupby_ax, obs_tidy, colors, orientation, cmap_name). 2361 if len(labels) > 1:. 2362 groupby_ax.set_xticks(ticks). -> 2363 if max([len(x) for x ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1591
https://github.com/scverse/scanpy/issues/1591:3629,usability,user,user-images,3629,"group_labels, var_group_rotation, layer, standard_scale, swap_axes, show_gene_labels, show, save, figsize, **kwds). 1220 groupby_cmap,. 1221 norm,. -> 1222 ) = _plot_categories_as_colorblocks(. 1223 groupby_ax, obs_tidy, colors=groupby_colors, orientation='bottom'. 1224 ). ~/.miniconda3/envs/cellrank/lib/python3.8/site-packages/scanpy/plotting/_anndata.py in _plot_categories_as_colorblocks(groupby_ax, obs_tidy, colors, orientation, cmap_name). 2361 if len(labels) > 1:. 2362 groupby_ax.set_xticks(ticks). -> 2363 if max([len(x) for x in labels]) < 3:. 2364 # if the labels are small do not rotate them. 2365 rotation = 0. ~/.miniconda3/envs/cellrank/lib/python3.8/site-packages/scanpy/plotting/_anndata.py in <listcomp>(.0). 2361 if len(labels) > 1:. 2362 groupby_ax.set_xticks(ticks). -> 2363 if max([len(x) for x in labels]) < 3:. 2364 # if the labels are small do not rotate them. 2365 rotation = 0. TypeError: object of type 'int' has no len(). ```. #### Figures. Fig. 1. ![f0](https://user-images.githubusercontent.com/46717574/104822526-6544e880-5843-11eb-8324-261e84eb45e4.png). Fig. 2. ![f1](https://user-images.githubusercontent.com/46717574/104822449-d637d080-5842-11eb-853d-bb115cd014aa.png). Fig. 3. ![f2](https://user-images.githubusercontent.com/46717574/104822452-d9cb5780-5842-11eb-8606-f4a9a5c97893.png). Fig. 4. ![f3](https://user-images.githubusercontent.com/46717574/104822453-dcc64800-5842-11eb-827c-90db0525d4d4.png). Fig. 5. ![f4](https://user-images.githubusercontent.com/46717574/104822455-dfc13880-5842-11eb-9286-655b2210b182.png). Fig. 6. ![f6](https://user-images.githubusercontent.com/46717574/104822796-958d8680-5845-11eb-82e2-4b30597c6722.png). #### Versions. <details>. -----. anndata 0.7.5. scanpy 1.7.0rc2.dev1+g2a123065. sinfo 0.3.1. -----. PIL 8.0.1. anndata 0.7.5. asciitree NA. backcall 0.2.0. cairo 1.20.0. cffi 1.14.4. cloudpickle 1.6.0. colorama 0.4.4. coverage 5.3. cycler 0.10.0. cython_runtime NA. dask 2020.12.0. dateutil 2.8.1. decorator 4.4.2. faste",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1591
https://github.com/scverse/scanpy/issues/1591:3747,usability,user,user-images,3747,"20 groupby_cmap,. 1221 norm,. -> 1222 ) = _plot_categories_as_colorblocks(. 1223 groupby_ax, obs_tidy, colors=groupby_colors, orientation='bottom'. 1224 ). ~/.miniconda3/envs/cellrank/lib/python3.8/site-packages/scanpy/plotting/_anndata.py in _plot_categories_as_colorblocks(groupby_ax, obs_tidy, colors, orientation, cmap_name). 2361 if len(labels) > 1:. 2362 groupby_ax.set_xticks(ticks). -> 2363 if max([len(x) for x in labels]) < 3:. 2364 # if the labels are small do not rotate them. 2365 rotation = 0. ~/.miniconda3/envs/cellrank/lib/python3.8/site-packages/scanpy/plotting/_anndata.py in <listcomp>(.0). 2361 if len(labels) > 1:. 2362 groupby_ax.set_xticks(ticks). -> 2363 if max([len(x) for x in labels]) < 3:. 2364 # if the labels are small do not rotate them. 2365 rotation = 0. TypeError: object of type 'int' has no len(). ```. #### Figures. Fig. 1. ![f0](https://user-images.githubusercontent.com/46717574/104822526-6544e880-5843-11eb-8324-261e84eb45e4.png). Fig. 2. ![f1](https://user-images.githubusercontent.com/46717574/104822449-d637d080-5842-11eb-853d-bb115cd014aa.png). Fig. 3. ![f2](https://user-images.githubusercontent.com/46717574/104822452-d9cb5780-5842-11eb-8606-f4a9a5c97893.png). Fig. 4. ![f3](https://user-images.githubusercontent.com/46717574/104822453-dcc64800-5842-11eb-827c-90db0525d4d4.png). Fig. 5. ![f4](https://user-images.githubusercontent.com/46717574/104822455-dfc13880-5842-11eb-9286-655b2210b182.png). Fig. 6. ![f6](https://user-images.githubusercontent.com/46717574/104822796-958d8680-5845-11eb-82e2-4b30597c6722.png). #### Versions. <details>. -----. anndata 0.7.5. scanpy 1.7.0rc2.dev1+g2a123065. sinfo 0.3.1. -----. PIL 8.0.1. anndata 0.7.5. asciitree NA. backcall 0.2.0. cairo 1.20.0. cffi 1.14.4. cloudpickle 1.6.0. colorama 0.4.4. coverage 5.3. cycler 0.10.0. cython_runtime NA. dask 2020.12.0. dateutil 2.8.1. decorator 4.4.2. fasteners NA. future_fstrings NA. get_version 2.1. h5py 2.10.0. igraph 0.8.3. ipykernel 5.4.2. ipython_genutils 0.2.0. jedi",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1591
https://github.com/scverse/scanpy/issues/1591:3865,usability,user,user-images,3865,"colors, orientation='bottom'. 1224 ). ~/.miniconda3/envs/cellrank/lib/python3.8/site-packages/scanpy/plotting/_anndata.py in _plot_categories_as_colorblocks(groupby_ax, obs_tidy, colors, orientation, cmap_name). 2361 if len(labels) > 1:. 2362 groupby_ax.set_xticks(ticks). -> 2363 if max([len(x) for x in labels]) < 3:. 2364 # if the labels are small do not rotate them. 2365 rotation = 0. ~/.miniconda3/envs/cellrank/lib/python3.8/site-packages/scanpy/plotting/_anndata.py in <listcomp>(.0). 2361 if len(labels) > 1:. 2362 groupby_ax.set_xticks(ticks). -> 2363 if max([len(x) for x in labels]) < 3:. 2364 # if the labels are small do not rotate them. 2365 rotation = 0. TypeError: object of type 'int' has no len(). ```. #### Figures. Fig. 1. ![f0](https://user-images.githubusercontent.com/46717574/104822526-6544e880-5843-11eb-8324-261e84eb45e4.png). Fig. 2. ![f1](https://user-images.githubusercontent.com/46717574/104822449-d637d080-5842-11eb-853d-bb115cd014aa.png). Fig. 3. ![f2](https://user-images.githubusercontent.com/46717574/104822452-d9cb5780-5842-11eb-8606-f4a9a5c97893.png). Fig. 4. ![f3](https://user-images.githubusercontent.com/46717574/104822453-dcc64800-5842-11eb-827c-90db0525d4d4.png). Fig. 5. ![f4](https://user-images.githubusercontent.com/46717574/104822455-dfc13880-5842-11eb-9286-655b2210b182.png). Fig. 6. ![f6](https://user-images.githubusercontent.com/46717574/104822796-958d8680-5845-11eb-82e2-4b30597c6722.png). #### Versions. <details>. -----. anndata 0.7.5. scanpy 1.7.0rc2.dev1+g2a123065. sinfo 0.3.1. -----. PIL 8.0.1. anndata 0.7.5. asciitree NA. backcall 0.2.0. cairo 1.20.0. cffi 1.14.4. cloudpickle 1.6.0. colorama 0.4.4. coverage 5.3. cycler 0.10.0. cython_runtime NA. dask 2020.12.0. dateutil 2.8.1. decorator 4.4.2. fasteners NA. future_fstrings NA. get_version 2.1. h5py 2.10.0. igraph 0.8.3. ipykernel 5.4.2. ipython_genutils 0.2.0. jedi 0.17.2. joblib 0.17.0. kiwisolver 1.3.1. legacy_api_wrap 1.2. leidenalg 0.8.1. llvmlite 0.34.0. louvain 0.6.1. matplo",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1591
https://github.com/scverse/scanpy/issues/1591:3983,usability,user,user-images,3983,".py in _plot_categories_as_colorblocks(groupby_ax, obs_tidy, colors, orientation, cmap_name). 2361 if len(labels) > 1:. 2362 groupby_ax.set_xticks(ticks). -> 2363 if max([len(x) for x in labels]) < 3:. 2364 # if the labels are small do not rotate them. 2365 rotation = 0. ~/.miniconda3/envs/cellrank/lib/python3.8/site-packages/scanpy/plotting/_anndata.py in <listcomp>(.0). 2361 if len(labels) > 1:. 2362 groupby_ax.set_xticks(ticks). -> 2363 if max([len(x) for x in labels]) < 3:. 2364 # if the labels are small do not rotate them. 2365 rotation = 0. TypeError: object of type 'int' has no len(). ```. #### Figures. Fig. 1. ![f0](https://user-images.githubusercontent.com/46717574/104822526-6544e880-5843-11eb-8324-261e84eb45e4.png). Fig. 2. ![f1](https://user-images.githubusercontent.com/46717574/104822449-d637d080-5842-11eb-853d-bb115cd014aa.png). Fig. 3. ![f2](https://user-images.githubusercontent.com/46717574/104822452-d9cb5780-5842-11eb-8606-f4a9a5c97893.png). Fig. 4. ![f3](https://user-images.githubusercontent.com/46717574/104822453-dcc64800-5842-11eb-827c-90db0525d4d4.png). Fig. 5. ![f4](https://user-images.githubusercontent.com/46717574/104822455-dfc13880-5842-11eb-9286-655b2210b182.png). Fig. 6. ![f6](https://user-images.githubusercontent.com/46717574/104822796-958d8680-5845-11eb-82e2-4b30597c6722.png). #### Versions. <details>. -----. anndata 0.7.5. scanpy 1.7.0rc2.dev1+g2a123065. sinfo 0.3.1. -----. PIL 8.0.1. anndata 0.7.5. asciitree NA. backcall 0.2.0. cairo 1.20.0. cffi 1.14.4. cloudpickle 1.6.0. colorama 0.4.4. coverage 5.3. cycler 0.10.0. cython_runtime NA. dask 2020.12.0. dateutil 2.8.1. decorator 4.4.2. fasteners NA. future_fstrings NA. get_version 2.1. h5py 2.10.0. igraph 0.8.3. ipykernel 5.4.2. ipython_genutils 0.2.0. jedi 0.17.2. joblib 0.17.0. kiwisolver 1.3.1. legacy_api_wrap 1.2. leidenalg 0.8.1. llvmlite 0.34.0. louvain 0.6.1. matplotlib 3.3.3. monotonic NA. mpl_toolkits NA. msgpack 1.0.0. natsort 7.0.1. numba 0.51.2. numcodecs 0.7.2. numexpr 2.7.1.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1591
https://github.com/scverse/scanpy/issues/1591:4101,usability,user,user-images,4101,. 2362 groupby_ax.set_xticks(ticks). -> 2363 if max([len(x) for x in labels]) < 3:. 2364 # if the labels are small do not rotate them. 2365 rotation = 0. ~/.miniconda3/envs/cellrank/lib/python3.8/site-packages/scanpy/plotting/_anndata.py in <listcomp>(.0). 2361 if len(labels) > 1:. 2362 groupby_ax.set_xticks(ticks). -> 2363 if max([len(x) for x in labels]) < 3:. 2364 # if the labels are small do not rotate them. 2365 rotation = 0. TypeError: object of type 'int' has no len(). ```. #### Figures. Fig. 1. ![f0](https://user-images.githubusercontent.com/46717574/104822526-6544e880-5843-11eb-8324-261e84eb45e4.png). Fig. 2. ![f1](https://user-images.githubusercontent.com/46717574/104822449-d637d080-5842-11eb-853d-bb115cd014aa.png). Fig. 3. ![f2](https://user-images.githubusercontent.com/46717574/104822452-d9cb5780-5842-11eb-8606-f4a9a5c97893.png). Fig. 4. ![f3](https://user-images.githubusercontent.com/46717574/104822453-dcc64800-5842-11eb-827c-90db0525d4d4.png). Fig. 5. ![f4](https://user-images.githubusercontent.com/46717574/104822455-dfc13880-5842-11eb-9286-655b2210b182.png). Fig. 6. ![f6](https://user-images.githubusercontent.com/46717574/104822796-958d8680-5845-11eb-82e2-4b30597c6722.png). #### Versions. <details>. -----. anndata 0.7.5. scanpy 1.7.0rc2.dev1+g2a123065. sinfo 0.3.1. -----. PIL 8.0.1. anndata 0.7.5. asciitree NA. backcall 0.2.0. cairo 1.20.0. cffi 1.14.4. cloudpickle 1.6.0. colorama 0.4.4. coverage 5.3. cycler 0.10.0. cython_runtime NA. dask 2020.12.0. dateutil 2.8.1. decorator 4.4.2. fasteners NA. future_fstrings NA. get_version 2.1. h5py 2.10.0. igraph 0.8.3. ipykernel 5.4.2. ipython_genutils 0.2.0. jedi 0.17.2. joblib 0.17.0. kiwisolver 1.3.1. legacy_api_wrap 1.2. leidenalg 0.8.1. llvmlite 0.34.0. louvain 0.6.1. matplotlib 3.3.3. monotonic NA. mpl_toolkits NA. msgpack 1.0.0. natsort 7.0.1. numba 0.51.2. numcodecs 0.7.2. numexpr 2.7.1. numpy 1.19.4. packaging 20.8. pandas 1.1.4. parso 0.7.1. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. prompt_t,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1591
https://github.com/scverse/scanpy/issues/1591:4219,usability,user,user-images,4219,not rotate them. 2365 rotation = 0. ~/.miniconda3/envs/cellrank/lib/python3.8/site-packages/scanpy/plotting/_anndata.py in <listcomp>(.0). 2361 if len(labels) > 1:. 2362 groupby_ax.set_xticks(ticks). -> 2363 if max([len(x) for x in labels]) < 3:. 2364 # if the labels are small do not rotate them. 2365 rotation = 0. TypeError: object of type 'int' has no len(). ```. #### Figures. Fig. 1. ![f0](https://user-images.githubusercontent.com/46717574/104822526-6544e880-5843-11eb-8324-261e84eb45e4.png). Fig. 2. ![f1](https://user-images.githubusercontent.com/46717574/104822449-d637d080-5842-11eb-853d-bb115cd014aa.png). Fig. 3. ![f2](https://user-images.githubusercontent.com/46717574/104822452-d9cb5780-5842-11eb-8606-f4a9a5c97893.png). Fig. 4. ![f3](https://user-images.githubusercontent.com/46717574/104822453-dcc64800-5842-11eb-827c-90db0525d4d4.png). Fig. 5. ![f4](https://user-images.githubusercontent.com/46717574/104822455-dfc13880-5842-11eb-9286-655b2210b182.png). Fig. 6. ![f6](https://user-images.githubusercontent.com/46717574/104822796-958d8680-5845-11eb-82e2-4b30597c6722.png). #### Versions. <details>. -----. anndata 0.7.5. scanpy 1.7.0rc2.dev1+g2a123065. sinfo 0.3.1. -----. PIL 8.0.1. anndata 0.7.5. asciitree NA. backcall 0.2.0. cairo 1.20.0. cffi 1.14.4. cloudpickle 1.6.0. colorama 0.4.4. coverage 5.3. cycler 0.10.0. cython_runtime NA. dask 2020.12.0. dateutil 2.8.1. decorator 4.4.2. fasteners NA. future_fstrings NA. get_version 2.1. h5py 2.10.0. igraph 0.8.3. ipykernel 5.4.2. ipython_genutils 0.2.0. jedi 0.17.2. joblib 0.17.0. kiwisolver 1.3.1. legacy_api_wrap 1.2. leidenalg 0.8.1. llvmlite 0.34.0. louvain 0.6.1. matplotlib 3.3.3. monotonic NA. mpl_toolkits NA. msgpack 1.0.0. natsort 7.0.1. numba 0.51.2. numcodecs 0.7.2. numexpr 2.7.1. numpy 1.19.4. packaging 20.8. pandas 1.1.4. parso 0.7.1. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. prompt_toolkit 3.0.8. psutil 5.7.3. ptyprocess 0.6.0. pygments 2.7.3. pyparsing 2.4.7. pytz 2020.5. ruamel NA. scanpy 1.7.0rc2,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1591
https://github.com/scverse/scanpy/issues/1591:5419,usability,tool,toolz,5419,"-bb115cd014aa.png). Fig. 3. ![f2](https://user-images.githubusercontent.com/46717574/104822452-d9cb5780-5842-11eb-8606-f4a9a5c97893.png). Fig. 4. ![f3](https://user-images.githubusercontent.com/46717574/104822453-dcc64800-5842-11eb-827c-90db0525d4d4.png). Fig. 5. ![f4](https://user-images.githubusercontent.com/46717574/104822455-dfc13880-5842-11eb-9286-655b2210b182.png). Fig. 6. ![f6](https://user-images.githubusercontent.com/46717574/104822796-958d8680-5845-11eb-82e2-4b30597c6722.png). #### Versions. <details>. -----. anndata 0.7.5. scanpy 1.7.0rc2.dev1+g2a123065. sinfo 0.3.1. -----. PIL 8.0.1. anndata 0.7.5. asciitree NA. backcall 0.2.0. cairo 1.20.0. cffi 1.14.4. cloudpickle 1.6.0. colorama 0.4.4. coverage 5.3. cycler 0.10.0. cython_runtime NA. dask 2020.12.0. dateutil 2.8.1. decorator 4.4.2. fasteners NA. future_fstrings NA. get_version 2.1. h5py 2.10.0. igraph 0.8.3. ipykernel 5.4.2. ipython_genutils 0.2.0. jedi 0.17.2. joblib 0.17.0. kiwisolver 1.3.1. legacy_api_wrap 1.2. leidenalg 0.8.1. llvmlite 0.34.0. louvain 0.6.1. matplotlib 3.3.3. monotonic NA. mpl_toolkits NA. msgpack 1.0.0. natsort 7.0.1. numba 0.51.2. numcodecs 0.7.2. numexpr 2.7.1. numpy 1.19.4. packaging 20.8. pandas 1.1.4. parso 0.7.1. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. prompt_toolkit 3.0.8. psutil 5.7.3. ptyprocess 0.6.0. pygments 2.7.3. pyparsing 2.4.7. pytz 2020.5. ruamel NA. scanpy 1.7.0rc2.dev1+g2a123065. scipy 1.5.4. setuptools_scm NA. sinfo 0.3.1. six 1.15.0. sklearn 0.23.2. sparse 0.11.2. sphinxcontrib NA. storemagic NA. tables 3.6.1. tblib 1.7.0. texttable 1.6.3. tlz 0.11.1. toolz 0.11.1. tornado 6.1. traitlets 5.0.5. typing_extensions NA. wcwidth 0.2.5. yaml 5.3.1. zarr 2.6.1. zmq 20.0.0. zope NA. -----. IPython 7.19.0. jupyter_client 6.1.7. jupyter_core 4.7.0. notebook 6.1.5. -----. Python 3.8.5 (default, Sep 4 2020, 07:30:14) [GCC 7.3.0]. Linux-5.10.0-1-amd64-x86_64-with-glibc2.10. 8 logical CPU cores. -----. Session information updated at 2021-01-16 21:28. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1591
https://github.com/scverse/scanpy/pull/1592:33,safety,test,tests,33,Fix chunked pca; Fixes #1590 and tests that chunked pca works,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1592
https://github.com/scverse/scanpy/pull/1592:33,testability,test,tests,33,Fix chunked pca; Fixes #1590 and tests that chunked pca works,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1592
https://github.com/scverse/scanpy/pull/1595:0,deployability,Updat,Update,0,"Update sam params; Supersedes #1540, just adding a release note. (I didn't want to push to your master branch @atarashansky)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1595
https://github.com/scverse/scanpy/pull/1595:51,deployability,releas,release,51,"Update sam params; Supersedes #1540, just adding a release note. (I didn't want to push to your master branch @atarashansky)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1595
https://github.com/scverse/scanpy/pull/1595:0,safety,Updat,Update,0,"Update sam params; Supersedes #1540, just adding a release note. (I didn't want to push to your master branch @atarashansky)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1595
https://github.com/scverse/scanpy/pull/1595:0,security,Updat,Update,0,"Update sam params; Supersedes #1540, just adding a release note. (I didn't want to push to your master branch @atarashansky)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1595
https://github.com/scverse/scanpy/pull/1596:262,availability,slo,slow,262,"Speedup embedding plots when len(cluster_colors) < len(cluster_categories); If values of color dict are not unique, `categorical.map(non_unique)` returns a string array, which caused `to_hex` to be called on each element, not just the categories. This was quite slow. Using a dataset of 13million cells, with 38 clusters but only 20 colors, benchmarking the following line . ```python. sc.pl.umap(adata, color=""louvain""). ```. On master:. ```. CPU times: user 12.3 s, sys: 187 ms, total: 12.4 s. Wall time: 12.4 s. ```. This pr:. ```. CPU times: user 6.82 s, sys: 149 ms, total: 6.97 s. Wall time: 6.97 s. ```.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1596
https://github.com/scverse/scanpy/pull/1596:312,availability,cluster,clusters,312,"Speedup embedding plots when len(cluster_colors) < len(cluster_categories); If values of color dict are not unique, `categorical.map(non_unique)` returns a string array, which caused `to_hex` to be called on each element, not just the categories. This was quite slow. Using a dataset of 13million cells, with 38 clusters but only 20 colors, benchmarking the following line . ```python. sc.pl.umap(adata, color=""louvain""). ```. On master:. ```. CPU times: user 12.3 s, sys: 187 ms, total: 12.4 s. Wall time: 12.4 s. ```. This pr:. ```. CPU times: user 6.82 s, sys: 149 ms, total: 6.97 s. Wall time: 6.97 s. ```.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1596
https://github.com/scverse/scanpy/pull/1596:312,deployability,cluster,clusters,312,"Speedup embedding plots when len(cluster_colors) < len(cluster_categories); If values of color dict are not unique, `categorical.map(non_unique)` returns a string array, which caused `to_hex` to be called on each element, not just the categories. This was quite slow. Using a dataset of 13million cells, with 38 clusters but only 20 colors, benchmarking the following line . ```python. sc.pl.umap(adata, color=""louvain""). ```. On master:. ```. CPU times: user 12.3 s, sys: 187 ms, total: 12.4 s. Wall time: 12.4 s. ```. This pr:. ```. CPU times: user 6.82 s, sys: 149 ms, total: 6.97 s. Wall time: 6.97 s. ```.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1596
https://github.com/scverse/scanpy/pull/1596:444,energy efficiency,CPU,CPU,444,"Speedup embedding plots when len(cluster_colors) < len(cluster_categories); If values of color dict are not unique, `categorical.map(non_unique)` returns a string array, which caused `to_hex` to be called on each element, not just the categories. This was quite slow. Using a dataset of 13million cells, with 38 clusters but only 20 colors, benchmarking the following line . ```python. sc.pl.umap(adata, color=""louvain""). ```. On master:. ```. CPU times: user 12.3 s, sys: 187 ms, total: 12.4 s. Wall time: 12.4 s. ```. This pr:. ```. CPU times: user 6.82 s, sys: 149 ms, total: 6.97 s. Wall time: 6.97 s. ```.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1596
https://github.com/scverse/scanpy/pull/1596:535,energy efficiency,CPU,CPU,535,"Speedup embedding plots when len(cluster_colors) < len(cluster_categories); If values of color dict are not unique, `categorical.map(non_unique)` returns a string array, which caused `to_hex` to be called on each element, not just the categories. This was quite slow. Using a dataset of 13million cells, with 38 clusters but only 20 colors, benchmarking the following line . ```python. sc.pl.umap(adata, color=""louvain""). ```. On master:. ```. CPU times: user 12.3 s, sys: 187 ms, total: 12.4 s. Wall time: 12.4 s. ```. This pr:. ```. CPU times: user 6.82 s, sys: 149 ms, total: 6.97 s. Wall time: 6.97 s. ```.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1596
https://github.com/scverse/scanpy/pull/1596:444,performance,CPU,CPU,444,"Speedup embedding plots when len(cluster_colors) < len(cluster_categories); If values of color dict are not unique, `categorical.map(non_unique)` returns a string array, which caused `to_hex` to be called on each element, not just the categories. This was quite slow. Using a dataset of 13million cells, with 38 clusters but only 20 colors, benchmarking the following line . ```python. sc.pl.umap(adata, color=""louvain""). ```. On master:. ```. CPU times: user 12.3 s, sys: 187 ms, total: 12.4 s. Wall time: 12.4 s. ```. This pr:. ```. CPU times: user 6.82 s, sys: 149 ms, total: 6.97 s. Wall time: 6.97 s. ```.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1596
https://github.com/scverse/scanpy/pull/1596:448,performance,time,times,448,"Speedup embedding plots when len(cluster_colors) < len(cluster_categories); If values of color dict are not unique, `categorical.map(non_unique)` returns a string array, which caused `to_hex` to be called on each element, not just the categories. This was quite slow. Using a dataset of 13million cells, with 38 clusters but only 20 colors, benchmarking the following line . ```python. sc.pl.umap(adata, color=""louvain""). ```. On master:. ```. CPU times: user 12.3 s, sys: 187 ms, total: 12.4 s. Wall time: 12.4 s. ```. This pr:. ```. CPU times: user 6.82 s, sys: 149 ms, total: 6.97 s. Wall time: 6.97 s. ```.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1596
https://github.com/scverse/scanpy/pull/1596:501,performance,time,time,501,"Speedup embedding plots when len(cluster_colors) < len(cluster_categories); If values of color dict are not unique, `categorical.map(non_unique)` returns a string array, which caused `to_hex` to be called on each element, not just the categories. This was quite slow. Using a dataset of 13million cells, with 38 clusters but only 20 colors, benchmarking the following line . ```python. sc.pl.umap(adata, color=""louvain""). ```. On master:. ```. CPU times: user 12.3 s, sys: 187 ms, total: 12.4 s. Wall time: 12.4 s. ```. This pr:. ```. CPU times: user 6.82 s, sys: 149 ms, total: 6.97 s. Wall time: 6.97 s. ```.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1596
https://github.com/scverse/scanpy/pull/1596:535,performance,CPU,CPU,535,"Speedup embedding plots when len(cluster_colors) < len(cluster_categories); If values of color dict are not unique, `categorical.map(non_unique)` returns a string array, which caused `to_hex` to be called on each element, not just the categories. This was quite slow. Using a dataset of 13million cells, with 38 clusters but only 20 colors, benchmarking the following line . ```python. sc.pl.umap(adata, color=""louvain""). ```. On master:. ```. CPU times: user 12.3 s, sys: 187 ms, total: 12.4 s. Wall time: 12.4 s. ```. This pr:. ```. CPU times: user 6.82 s, sys: 149 ms, total: 6.97 s. Wall time: 6.97 s. ```.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1596
https://github.com/scverse/scanpy/pull/1596:539,performance,time,times,539,"Speedup embedding plots when len(cluster_colors) < len(cluster_categories); If values of color dict are not unique, `categorical.map(non_unique)` returns a string array, which caused `to_hex` to be called on each element, not just the categories. This was quite slow. Using a dataset of 13million cells, with 38 clusters but only 20 colors, benchmarking the following line . ```python. sc.pl.umap(adata, color=""louvain""). ```. On master:. ```. CPU times: user 12.3 s, sys: 187 ms, total: 12.4 s. Wall time: 12.4 s. ```. This pr:. ```. CPU times: user 6.82 s, sys: 149 ms, total: 6.97 s. Wall time: 6.97 s. ```.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1596
https://github.com/scverse/scanpy/pull/1596:592,performance,time,time,592,"Speedup embedding plots when len(cluster_colors) < len(cluster_categories); If values of color dict are not unique, `categorical.map(non_unique)` returns a string array, which caused `to_hex` to be called on each element, not just the categories. This was quite slow. Using a dataset of 13million cells, with 38 clusters but only 20 colors, benchmarking the following line . ```python. sc.pl.umap(adata, color=""louvain""). ```. On master:. ```. CPU times: user 12.3 s, sys: 187 ms, total: 12.4 s. Wall time: 12.4 s. ```. This pr:. ```. CPU times: user 6.82 s, sys: 149 ms, total: 6.97 s. Wall time: 6.97 s. ```.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1596
https://github.com/scverse/scanpy/pull/1596:262,reliability,slo,slow,262,"Speedup embedding plots when len(cluster_colors) < len(cluster_categories); If values of color dict are not unique, `categorical.map(non_unique)` returns a string array, which caused `to_hex` to be called on each element, not just the categories. This was quite slow. Using a dataset of 13million cells, with 38 clusters but only 20 colors, benchmarking the following line . ```python. sc.pl.umap(adata, color=""louvain""). ```. On master:. ```. CPU times: user 12.3 s, sys: 187 ms, total: 12.4 s. Wall time: 12.4 s. ```. This pr:. ```. CPU times: user 6.82 s, sys: 149 ms, total: 6.97 s. Wall time: 6.97 s. ```.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1596
https://github.com/scverse/scanpy/pull/1596:455,usability,user,user,455,"Speedup embedding plots when len(cluster_colors) < len(cluster_categories); If values of color dict are not unique, `categorical.map(non_unique)` returns a string array, which caused `to_hex` to be called on each element, not just the categories. This was quite slow. Using a dataset of 13million cells, with 38 clusters but only 20 colors, benchmarking the following line . ```python. sc.pl.umap(adata, color=""louvain""). ```. On master:. ```. CPU times: user 12.3 s, sys: 187 ms, total: 12.4 s. Wall time: 12.4 s. ```. This pr:. ```. CPU times: user 6.82 s, sys: 149 ms, total: 6.97 s. Wall time: 6.97 s. ```.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1596
https://github.com/scverse/scanpy/pull/1596:546,usability,user,user,546,"Speedup embedding plots when len(cluster_colors) < len(cluster_categories); If values of color dict are not unique, `categorical.map(non_unique)` returns a string array, which caused `to_hex` to be called on each element, not just the categories. This was quite slow. Using a dataset of 13million cells, with 38 clusters but only 20 colors, benchmarking the following line . ```python. sc.pl.umap(adata, color=""louvain""). ```. On master:. ```. CPU times: user 12.3 s, sys: 187 ms, total: 12.4 s. Wall time: 12.4 s. ```. This pr:. ```. CPU times: user 6.82 s, sys: 149 ms, total: 6.97 s. Wall time: 6.97 s. ```.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1596
https://github.com/scverse/scanpy/pull/1597:35,deployability,Updat,Update,35,Backport PR #1595 on branch 1.7.x (Update sam params); Backport PR #1595: Update sam params,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1597
https://github.com/scverse/scanpy/pull/1597:74,deployability,Updat,Update,74,Backport PR #1595 on branch 1.7.x (Update sam params); Backport PR #1595: Update sam params,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1597
https://github.com/scverse/scanpy/pull/1597:35,safety,Updat,Update,35,Backport PR #1595 on branch 1.7.x (Update sam params); Backport PR #1595: Update sam params,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1597
https://github.com/scverse/scanpy/pull/1597:74,safety,Updat,Update,74,Backport PR #1595 on branch 1.7.x (Update sam params); Backport PR #1595: Update sam params,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1597
https://github.com/scverse/scanpy/pull/1597:35,security,Updat,Update,35,Backport PR #1595 on branch 1.7.x (Update sam params); Backport PR #1595: Update sam params,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1597
https://github.com/scverse/scanpy/pull/1597:74,security,Updat,Update,74,Backport PR #1595 on branch 1.7.x (Update sam params); Backport PR #1595: Update sam params,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1597
https://github.com/scverse/scanpy/issues/1599:546,availability,error,error,546,"KeyError in score_genes_cell_cycle; - [ ] I have checked that this issue has not already been reported. - [ ] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. Hello everyone, I am trying to calculate cell cycle score for my mouse data and encountering this error. I have already converted the gene name to upper case letters to read from the cell cycle genes. please help me to resolve this issue. Thank you very much. ### Minimal code sample (that we can copy&paste without having any data). ```python. ## Mouse. folder = ""D:/Pawandeep/Alldatasets/Macosko_cell_cycle_genes.txt"". cc_genes = pd.read_table(folder, delimiter='\t'). #cc_genes = pd.read_table(Macosko_cell_cycle_genes, delimiter='\t'). s_genes = cc_genes['S'].dropna(). g2m_genes = cc_genes['G2.M'].dropna(). s_genes_mm = [gene.lower().upper() for gene in s_genes]. g2m_genes_mm = [gene.lower().upper() for gene in g2m_genes]. s_genes_mm_ens = adata.var.index[np.in1d(adata.var.index, s_genes_mm)]. g2m_genes_mm_ens = adata.var.index[np.in1d(adata.var.index, g2m_genes_mm)]. sc.tl.score_genes_cell_cycle(adata, s_genes=s_genes_mm_ens, g2m_genes=s_genes_mm_ens). ```. ```pytb. KeyError Traceback (most recent call last). <ipython-input-63-57c51b3902c0> in <module>. 11 s_genes_mm_ens = adata.var.index[np.in1d(adata.var.index, s_genes_mm)]. 12 g2m_genes_mm_ens = adata.var.index[np.in1d(adata.var.index, g2m_genes_mm)]. ---> 13 sc.tl.score_genes_cell_cycle(adata, s_genes=s_genes_mm_ens, g2m_genes=s_genes_mm_ens). ~\anaconda3\lib\site-packages\scanpy\tools\_score_genes.py in score_genes_cell_cycle(adata, s_genes, g2m_genes, copy, **kwargs). 247 ctrl_size = min(len(s_genes), len(g2m_genes)). 248 # add s-score. --> 249 score_genes(adata, gene_lis",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1599
https://github.com/scverse/scanpy/issues/1599:157,deployability,version,version,157,"KeyError in score_genes_cell_cycle; - [ ] I have checked that this issue has not already been reported. - [ ] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. Hello everyone, I am trying to calculate cell cycle score for my mouse data and encountering this error. I have already converted the gene name to upper case letters to read from the cell cycle genes. please help me to resolve this issue. Thank you very much. ### Minimal code sample (that we can copy&paste without having any data). ```python. ## Mouse. folder = ""D:/Pawandeep/Alldatasets/Macosko_cell_cycle_genes.txt"". cc_genes = pd.read_table(folder, delimiter='\t'). #cc_genes = pd.read_table(Macosko_cell_cycle_genes, delimiter='\t'). s_genes = cc_genes['S'].dropna(). g2m_genes = cc_genes['G2.M'].dropna(). s_genes_mm = [gene.lower().upper() for gene in s_genes]. g2m_genes_mm = [gene.lower().upper() for gene in g2m_genes]. s_genes_mm_ens = adata.var.index[np.in1d(adata.var.index, s_genes_mm)]. g2m_genes_mm_ens = adata.var.index[np.in1d(adata.var.index, g2m_genes_mm)]. sc.tl.score_genes_cell_cycle(adata, s_genes=s_genes_mm_ens, g2m_genes=s_genes_mm_ens). ```. ```pytb. KeyError Traceback (most recent call last). <ipython-input-63-57c51b3902c0> in <module>. 11 s_genes_mm_ens = adata.var.index[np.in1d(adata.var.index, s_genes_mm)]. 12 g2m_genes_mm_ens = adata.var.index[np.in1d(adata.var.index, g2m_genes_mm)]. ---> 13 sc.tl.score_genes_cell_cycle(adata, s_genes=s_genes_mm_ens, g2m_genes=s_genes_mm_ens). ~\anaconda3\lib\site-packages\scanpy\tools\_score_genes.py in score_genes_cell_cycle(adata, s_genes, g2m_genes, copy, **kwargs). 247 ctrl_size = min(len(s_genes), len(g2m_genes)). 248 # add s-score. --> 249 score_genes(adata, gene_lis",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1599
https://github.com/scverse/scanpy/issues/1599:1508,deployability,modul,module,1508,"y mouse data and encountering this error. I have already converted the gene name to upper case letters to read from the cell cycle genes. please help me to resolve this issue. Thank you very much. ### Minimal code sample (that we can copy&paste without having any data). ```python. ## Mouse. folder = ""D:/Pawandeep/Alldatasets/Macosko_cell_cycle_genes.txt"". cc_genes = pd.read_table(folder, delimiter='\t'). #cc_genes = pd.read_table(Macosko_cell_cycle_genes, delimiter='\t'). s_genes = cc_genes['S'].dropna(). g2m_genes = cc_genes['G2.M'].dropna(). s_genes_mm = [gene.lower().upper() for gene in s_genes]. g2m_genes_mm = [gene.lower().upper() for gene in g2m_genes]. s_genes_mm_ens = adata.var.index[np.in1d(adata.var.index, s_genes_mm)]. g2m_genes_mm_ens = adata.var.index[np.in1d(adata.var.index, g2m_genes_mm)]. sc.tl.score_genes_cell_cycle(adata, s_genes=s_genes_mm_ens, g2m_genes=s_genes_mm_ens). ```. ```pytb. KeyError Traceback (most recent call last). <ipython-input-63-57c51b3902c0> in <module>. 11 s_genes_mm_ens = adata.var.index[np.in1d(adata.var.index, s_genes_mm)]. 12 g2m_genes_mm_ens = adata.var.index[np.in1d(adata.var.index, g2m_genes_mm)]. ---> 13 sc.tl.score_genes_cell_cycle(adata, s_genes=s_genes_mm_ens, g2m_genes=s_genes_mm_ens). ~\anaconda3\lib\site-packages\scanpy\tools\_score_genes.py in score_genes_cell_cycle(adata, s_genes, g2m_genes, copy, **kwargs). 247 ctrl_size = min(len(s_genes), len(g2m_genes)). 248 # add s-score. --> 249 score_genes(adata, gene_list=s_genes, score_name='S_score', ctrl_size=ctrl_size, **kwargs). 250 # add g2m-score. 251 score_genes(adata, gene_list=g2m_genes, score_name='G2M_score', ctrl_size=ctrl_size, **kwargs). ~\anaconda3\lib\site-packages\scanpy\tools\_score_genes.py in score_genes(adata, gene_list, ctrl_size, gene_pool, n_bins, score_name, random_state, copy, use_raw). 153 gene_list = list(gene_list). 154 . --> 155 X_list = _adata[:, gene_list].X. 156 if issparse(X_list):. 157 X_list = np.array(_sparse_nanmean(X_list, axis=1)).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1599
https://github.com/scverse/scanpy/issues/1599:5164,deployability,Version,Versions,5164,"ndices."". KeyError: ""Values ['USP1', 'NSUN3', 'PHTF2', 'CCDC84', 'NUP160', 'RRM1', 'CENPM', 'SLC38A2', 'ABCC5', 'REEP1', 'DYNC1LI2', 'BMI1', 'RBBP8', 'MYCBP2', 'NFE2L2', 'GCLM', 'DNA2', 'OSGIN2', 'MAP3K2', 'BLM', 'PRIM1', 'RSRC2', 'CREBZF', 'E2F8', 'CDKN2AIP', 'ASF1B', 'CCDC150', 'BRIP1', 'MAN1A2', 'ZWINT', 'PKMYT1', 'ZBED5', 'PHTF1', 'ATAD2', 'CASP2', 'BRCA1', 'HELLS', 'CALD1', 'ORC3', 'SVIP', 'ABHD10', 'SAP30BP', 'RAD51', 'TTLL7', 'UBL3', 'DNAJB4', 'NEAT1', 'KAT2A', 'OGT', 'CENPQ', 'RAD51AP1', 'TOP2A', 'CDC7', 'CRLS1', 'DEPDC7', 'EZH2', 'CDCA5', 'TYMS', 'FANCA', 'PTAR1', 'RMI1', 'CDC45', 'UBE2T', 'POLA1', 'FANCI', 'KAT2B', 'MCM8', 'SRSF5', 'COQ9', 'NT5DC1', 'BIVM', 'RHOBTB3', 'CHML', 'CALM2', 'NAB1', 'LMO4', 'SP1', 'INTS7', 'RFC2', 'ESCO2', 'MASTL', 'DHFR', 'FEN1', 'DSCC1', 'CCDC14', 'MBD4', 'LYRM7', 'NRD1', 'RAD18', 'EXO1', 'PHIP', 'RPA2', 'BBS2', 'CERS6', 'CPNE8', 'RRM2', 'DONSON', 'H1F0', 'EIF4EBP2'], from ['USP1', 'NSUN3', 'PHTF2', 'CCDC84', 'NUP160', 'RRM1', 'CENPM', 'SLC38A2', 'ABCC5', 'REEP1', 'DYNC1LI2', 'BMI1', 'RBBP8', 'MYCBP2', 'NFE2L2', 'GCLM', 'DNA2', 'OSGIN2', 'MAP3K2', 'BLM', 'PRIM1', 'RSRC2', 'CREBZF', 'E2F8', 'CDKN2AIP', 'ASF1B', 'CCDC150', 'BRIP1', 'MAN1A2', 'ZWINT', 'PKMYT1', 'ZBED5', 'PHTF1', 'ATAD2', 'CASP2', 'BRCA1', 'HELLS', 'CALD1', 'ORC3', 'SVIP', 'ABHD10', 'SAP30BP', 'RAD51', 'TTLL7', 'UBL3', 'DNAJB4', 'NEAT1', 'KAT2A', 'OGT', 'CENPQ', 'RAD51AP1', 'TOP2A', 'CDC7', 'CRLS1', 'DEPDC7', 'EZH2', 'CDCA5', 'TYMS', 'FANCA', 'PTAR1', 'RMI1', 'CDC45', 'UBE2T', 'POLA1', 'FANCI', 'KAT2B', 'MCM8', 'SRSF5', 'COQ9', 'NT5DC1', 'BIVM', 'RHOBTB3', 'CHML', 'CALM2', 'NAB1', 'LMO4', 'SP1', 'INTS7', 'RFC2', 'ESCO2', 'MASTL', 'DHFR', 'FEN1', 'DSCC1', 'CCDC14', 'MBD4', 'LYRM7', 'NRD1', 'RAD18', 'EXO1', 'PHIP', 'RPA2', 'BBS2', 'CERS6', 'CPNE8', 'RRM2', 'DONSON', 'H1F0', 'EIF4EBP2'], are not valid obs/ var names or indices."". ```. #### Versions. <details>. [Paste the output of scanpy.logging.print_versions() leaving a blank line after the details tag]. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1599
https://github.com/scverse/scanpy/issues/1599:5213,deployability,log,logging,5213,"ndices."". KeyError: ""Values ['USP1', 'NSUN3', 'PHTF2', 'CCDC84', 'NUP160', 'RRM1', 'CENPM', 'SLC38A2', 'ABCC5', 'REEP1', 'DYNC1LI2', 'BMI1', 'RBBP8', 'MYCBP2', 'NFE2L2', 'GCLM', 'DNA2', 'OSGIN2', 'MAP3K2', 'BLM', 'PRIM1', 'RSRC2', 'CREBZF', 'E2F8', 'CDKN2AIP', 'ASF1B', 'CCDC150', 'BRIP1', 'MAN1A2', 'ZWINT', 'PKMYT1', 'ZBED5', 'PHTF1', 'ATAD2', 'CASP2', 'BRCA1', 'HELLS', 'CALD1', 'ORC3', 'SVIP', 'ABHD10', 'SAP30BP', 'RAD51', 'TTLL7', 'UBL3', 'DNAJB4', 'NEAT1', 'KAT2A', 'OGT', 'CENPQ', 'RAD51AP1', 'TOP2A', 'CDC7', 'CRLS1', 'DEPDC7', 'EZH2', 'CDCA5', 'TYMS', 'FANCA', 'PTAR1', 'RMI1', 'CDC45', 'UBE2T', 'POLA1', 'FANCI', 'KAT2B', 'MCM8', 'SRSF5', 'COQ9', 'NT5DC1', 'BIVM', 'RHOBTB3', 'CHML', 'CALM2', 'NAB1', 'LMO4', 'SP1', 'INTS7', 'RFC2', 'ESCO2', 'MASTL', 'DHFR', 'FEN1', 'DSCC1', 'CCDC14', 'MBD4', 'LYRM7', 'NRD1', 'RAD18', 'EXO1', 'PHIP', 'RPA2', 'BBS2', 'CERS6', 'CPNE8', 'RRM2', 'DONSON', 'H1F0', 'EIF4EBP2'], from ['USP1', 'NSUN3', 'PHTF2', 'CCDC84', 'NUP160', 'RRM1', 'CENPM', 'SLC38A2', 'ABCC5', 'REEP1', 'DYNC1LI2', 'BMI1', 'RBBP8', 'MYCBP2', 'NFE2L2', 'GCLM', 'DNA2', 'OSGIN2', 'MAP3K2', 'BLM', 'PRIM1', 'RSRC2', 'CREBZF', 'E2F8', 'CDKN2AIP', 'ASF1B', 'CCDC150', 'BRIP1', 'MAN1A2', 'ZWINT', 'PKMYT1', 'ZBED5', 'PHTF1', 'ATAD2', 'CASP2', 'BRCA1', 'HELLS', 'CALD1', 'ORC3', 'SVIP', 'ABHD10', 'SAP30BP', 'RAD51', 'TTLL7', 'UBL3', 'DNAJB4', 'NEAT1', 'KAT2A', 'OGT', 'CENPQ', 'RAD51AP1', 'TOP2A', 'CDC7', 'CRLS1', 'DEPDC7', 'EZH2', 'CDCA5', 'TYMS', 'FANCA', 'PTAR1', 'RMI1', 'CDC45', 'UBE2T', 'POLA1', 'FANCI', 'KAT2B', 'MCM8', 'SRSF5', 'COQ9', 'NT5DC1', 'BIVM', 'RHOBTB3', 'CHML', 'CALM2', 'NAB1', 'LMO4', 'SP1', 'INTS7', 'RFC2', 'ESCO2', 'MASTL', 'DHFR', 'FEN1', 'DSCC1', 'CCDC14', 'MBD4', 'LYRM7', 'NRD1', 'RAD18', 'EXO1', 'PHIP', 'RPA2', 'BBS2', 'CERS6', 'CPNE8', 'RRM2', 'DONSON', 'H1F0', 'EIF4EBP2'], are not valid obs/ var names or indices."". ```. #### Versions. <details>. [Paste the output of scanpy.logging.print_versions() leaving a blank line after the details tag]. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1599
https://github.com/scverse/scanpy/issues/1599:157,integrability,version,version,157,"KeyError in score_genes_cell_cycle; - [ ] I have checked that this issue has not already been reported. - [ ] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. Hello everyone, I am trying to calculate cell cycle score for my mouse data and encountering this error. I have already converted the gene name to upper case letters to read from the cell cycle genes. please help me to resolve this issue. Thank you very much. ### Minimal code sample (that we can copy&paste without having any data). ```python. ## Mouse. folder = ""D:/Pawandeep/Alldatasets/Macosko_cell_cycle_genes.txt"". cc_genes = pd.read_table(folder, delimiter='\t'). #cc_genes = pd.read_table(Macosko_cell_cycle_genes, delimiter='\t'). s_genes = cc_genes['S'].dropna(). g2m_genes = cc_genes['G2.M'].dropna(). s_genes_mm = [gene.lower().upper() for gene in s_genes]. g2m_genes_mm = [gene.lower().upper() for gene in g2m_genes]. s_genes_mm_ens = adata.var.index[np.in1d(adata.var.index, s_genes_mm)]. g2m_genes_mm_ens = adata.var.index[np.in1d(adata.var.index, g2m_genes_mm)]. sc.tl.score_genes_cell_cycle(adata, s_genes=s_genes_mm_ens, g2m_genes=s_genes_mm_ens). ```. ```pytb. KeyError Traceback (most recent call last). <ipython-input-63-57c51b3902c0> in <module>. 11 s_genes_mm_ens = adata.var.index[np.in1d(adata.var.index, s_genes_mm)]. 12 g2m_genes_mm_ens = adata.var.index[np.in1d(adata.var.index, g2m_genes_mm)]. ---> 13 sc.tl.score_genes_cell_cycle(adata, s_genes=s_genes_mm_ens, g2m_genes=s_genes_mm_ens). ~\anaconda3\lib\site-packages\scanpy\tools\_score_genes.py in score_genes_cell_cycle(adata, s_genes, g2m_genes, copy, **kwargs). 247 ctrl_size = min(len(s_genes), len(g2m_genes)). 248 # add s-score. --> 249 score_genes(adata, gene_lis",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1599
https://github.com/scverse/scanpy/issues/1599:5164,integrability,Version,Versions,5164,"ndices."". KeyError: ""Values ['USP1', 'NSUN3', 'PHTF2', 'CCDC84', 'NUP160', 'RRM1', 'CENPM', 'SLC38A2', 'ABCC5', 'REEP1', 'DYNC1LI2', 'BMI1', 'RBBP8', 'MYCBP2', 'NFE2L2', 'GCLM', 'DNA2', 'OSGIN2', 'MAP3K2', 'BLM', 'PRIM1', 'RSRC2', 'CREBZF', 'E2F8', 'CDKN2AIP', 'ASF1B', 'CCDC150', 'BRIP1', 'MAN1A2', 'ZWINT', 'PKMYT1', 'ZBED5', 'PHTF1', 'ATAD2', 'CASP2', 'BRCA1', 'HELLS', 'CALD1', 'ORC3', 'SVIP', 'ABHD10', 'SAP30BP', 'RAD51', 'TTLL7', 'UBL3', 'DNAJB4', 'NEAT1', 'KAT2A', 'OGT', 'CENPQ', 'RAD51AP1', 'TOP2A', 'CDC7', 'CRLS1', 'DEPDC7', 'EZH2', 'CDCA5', 'TYMS', 'FANCA', 'PTAR1', 'RMI1', 'CDC45', 'UBE2T', 'POLA1', 'FANCI', 'KAT2B', 'MCM8', 'SRSF5', 'COQ9', 'NT5DC1', 'BIVM', 'RHOBTB3', 'CHML', 'CALM2', 'NAB1', 'LMO4', 'SP1', 'INTS7', 'RFC2', 'ESCO2', 'MASTL', 'DHFR', 'FEN1', 'DSCC1', 'CCDC14', 'MBD4', 'LYRM7', 'NRD1', 'RAD18', 'EXO1', 'PHIP', 'RPA2', 'BBS2', 'CERS6', 'CPNE8', 'RRM2', 'DONSON', 'H1F0', 'EIF4EBP2'], from ['USP1', 'NSUN3', 'PHTF2', 'CCDC84', 'NUP160', 'RRM1', 'CENPM', 'SLC38A2', 'ABCC5', 'REEP1', 'DYNC1LI2', 'BMI1', 'RBBP8', 'MYCBP2', 'NFE2L2', 'GCLM', 'DNA2', 'OSGIN2', 'MAP3K2', 'BLM', 'PRIM1', 'RSRC2', 'CREBZF', 'E2F8', 'CDKN2AIP', 'ASF1B', 'CCDC150', 'BRIP1', 'MAN1A2', 'ZWINT', 'PKMYT1', 'ZBED5', 'PHTF1', 'ATAD2', 'CASP2', 'BRCA1', 'HELLS', 'CALD1', 'ORC3', 'SVIP', 'ABHD10', 'SAP30BP', 'RAD51', 'TTLL7', 'UBL3', 'DNAJB4', 'NEAT1', 'KAT2A', 'OGT', 'CENPQ', 'RAD51AP1', 'TOP2A', 'CDC7', 'CRLS1', 'DEPDC7', 'EZH2', 'CDCA5', 'TYMS', 'FANCA', 'PTAR1', 'RMI1', 'CDC45', 'UBE2T', 'POLA1', 'FANCI', 'KAT2B', 'MCM8', 'SRSF5', 'COQ9', 'NT5DC1', 'BIVM', 'RHOBTB3', 'CHML', 'CALM2', 'NAB1', 'LMO4', 'SP1', 'INTS7', 'RFC2', 'ESCO2', 'MASTL', 'DHFR', 'FEN1', 'DSCC1', 'CCDC14', 'MBD4', 'LYRM7', 'NRD1', 'RAD18', 'EXO1', 'PHIP', 'RPA2', 'BBS2', 'CERS6', 'CPNE8', 'RRM2', 'DONSON', 'H1F0', 'EIF4EBP2'], are not valid obs/ var names or indices."". ```. #### Versions. <details>. [Paste the output of scanpy.logging.print_versions() leaving a blank line after the details tag]. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1599
https://github.com/scverse/scanpy/issues/1599:157,modifiability,version,version,157,"KeyError in score_genes_cell_cycle; - [ ] I have checked that this issue has not already been reported. - [ ] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. Hello everyone, I am trying to calculate cell cycle score for my mouse data and encountering this error. I have already converted the gene name to upper case letters to read from the cell cycle genes. please help me to resolve this issue. Thank you very much. ### Minimal code sample (that we can copy&paste without having any data). ```python. ## Mouse. folder = ""D:/Pawandeep/Alldatasets/Macosko_cell_cycle_genes.txt"". cc_genes = pd.read_table(folder, delimiter='\t'). #cc_genes = pd.read_table(Macosko_cell_cycle_genes, delimiter='\t'). s_genes = cc_genes['S'].dropna(). g2m_genes = cc_genes['G2.M'].dropna(). s_genes_mm = [gene.lower().upper() for gene in s_genes]. g2m_genes_mm = [gene.lower().upper() for gene in g2m_genes]. s_genes_mm_ens = adata.var.index[np.in1d(adata.var.index, s_genes_mm)]. g2m_genes_mm_ens = adata.var.index[np.in1d(adata.var.index, g2m_genes_mm)]. sc.tl.score_genes_cell_cycle(adata, s_genes=s_genes_mm_ens, g2m_genes=s_genes_mm_ens). ```. ```pytb. KeyError Traceback (most recent call last). <ipython-input-63-57c51b3902c0> in <module>. 11 s_genes_mm_ens = adata.var.index[np.in1d(adata.var.index, s_genes_mm)]. 12 g2m_genes_mm_ens = adata.var.index[np.in1d(adata.var.index, g2m_genes_mm)]. ---> 13 sc.tl.score_genes_cell_cycle(adata, s_genes=s_genes_mm_ens, g2m_genes=s_genes_mm_ens). ~\anaconda3\lib\site-packages\scanpy\tools\_score_genes.py in score_genes_cell_cycle(adata, s_genes, g2m_genes, copy, **kwargs). 247 ctrl_size = min(len(s_genes), len(g2m_genes)). 248 # add s-score. --> 249 score_genes(adata, gene_lis",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1599
https://github.com/scverse/scanpy/issues/1599:1508,modifiability,modul,module,1508,"y mouse data and encountering this error. I have already converted the gene name to upper case letters to read from the cell cycle genes. please help me to resolve this issue. Thank you very much. ### Minimal code sample (that we can copy&paste without having any data). ```python. ## Mouse. folder = ""D:/Pawandeep/Alldatasets/Macosko_cell_cycle_genes.txt"". cc_genes = pd.read_table(folder, delimiter='\t'). #cc_genes = pd.read_table(Macosko_cell_cycle_genes, delimiter='\t'). s_genes = cc_genes['S'].dropna(). g2m_genes = cc_genes['G2.M'].dropna(). s_genes_mm = [gene.lower().upper() for gene in s_genes]. g2m_genes_mm = [gene.lower().upper() for gene in g2m_genes]. s_genes_mm_ens = adata.var.index[np.in1d(adata.var.index, s_genes_mm)]. g2m_genes_mm_ens = adata.var.index[np.in1d(adata.var.index, g2m_genes_mm)]. sc.tl.score_genes_cell_cycle(adata, s_genes=s_genes_mm_ens, g2m_genes=s_genes_mm_ens). ```. ```pytb. KeyError Traceback (most recent call last). <ipython-input-63-57c51b3902c0> in <module>. 11 s_genes_mm_ens = adata.var.index[np.in1d(adata.var.index, s_genes_mm)]. 12 g2m_genes_mm_ens = adata.var.index[np.in1d(adata.var.index, g2m_genes_mm)]. ---> 13 sc.tl.score_genes_cell_cycle(adata, s_genes=s_genes_mm_ens, g2m_genes=s_genes_mm_ens). ~\anaconda3\lib\site-packages\scanpy\tools\_score_genes.py in score_genes_cell_cycle(adata, s_genes, g2m_genes, copy, **kwargs). 247 ctrl_size = min(len(s_genes), len(g2m_genes)). 248 # add s-score. --> 249 score_genes(adata, gene_list=s_genes, score_name='S_score', ctrl_size=ctrl_size, **kwargs). 250 # add g2m-score. 251 score_genes(adata, gene_list=g2m_genes, score_name='G2M_score', ctrl_size=ctrl_size, **kwargs). ~\anaconda3\lib\site-packages\scanpy\tools\_score_genes.py in score_genes(adata, gene_list, ctrl_size, gene_pool, n_bins, score_name, random_state, copy, use_raw). 153 gene_list = list(gene_list). 154 . --> 155 X_list = _adata[:, gene_list].X. 156 if issparse(X_list):. 157 X_list = np.array(_sparse_nanmean(X_list, axis=1)).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1599
https://github.com/scverse/scanpy/issues/1599:1787,modifiability,pac,packages,1787,". ## Mouse. folder = ""D:/Pawandeep/Alldatasets/Macosko_cell_cycle_genes.txt"". cc_genes = pd.read_table(folder, delimiter='\t'). #cc_genes = pd.read_table(Macosko_cell_cycle_genes, delimiter='\t'). s_genes = cc_genes['S'].dropna(). g2m_genes = cc_genes['G2.M'].dropna(). s_genes_mm = [gene.lower().upper() for gene in s_genes]. g2m_genes_mm = [gene.lower().upper() for gene in g2m_genes]. s_genes_mm_ens = adata.var.index[np.in1d(adata.var.index, s_genes_mm)]. g2m_genes_mm_ens = adata.var.index[np.in1d(adata.var.index, g2m_genes_mm)]. sc.tl.score_genes_cell_cycle(adata, s_genes=s_genes_mm_ens, g2m_genes=s_genes_mm_ens). ```. ```pytb. KeyError Traceback (most recent call last). <ipython-input-63-57c51b3902c0> in <module>. 11 s_genes_mm_ens = adata.var.index[np.in1d(adata.var.index, s_genes_mm)]. 12 g2m_genes_mm_ens = adata.var.index[np.in1d(adata.var.index, g2m_genes_mm)]. ---> 13 sc.tl.score_genes_cell_cycle(adata, s_genes=s_genes_mm_ens, g2m_genes=s_genes_mm_ens). ~\anaconda3\lib\site-packages\scanpy\tools\_score_genes.py in score_genes_cell_cycle(adata, s_genes, g2m_genes, copy, **kwargs). 247 ctrl_size = min(len(s_genes), len(g2m_genes)). 248 # add s-score. --> 249 score_genes(adata, gene_list=s_genes, score_name='S_score', ctrl_size=ctrl_size, **kwargs). 250 # add g2m-score. 251 score_genes(adata, gene_list=g2m_genes, score_name='G2M_score', ctrl_size=ctrl_size, **kwargs). ~\anaconda3\lib\site-packages\scanpy\tools\_score_genes.py in score_genes(adata, gene_list, ctrl_size, gene_pool, n_bins, score_name, random_state, copy, use_raw). 153 gene_list = list(gene_list). 154 . --> 155 X_list = _adata[:, gene_list].X. 156 if issparse(X_list):. 157 X_list = np.array(_sparse_nanmean(X_list, axis=1)).flatten(). ~\anaconda3\lib\site-packages\anndata\_core\raw.py in __getitem__(self, index). 99 . 100 def __getitem__(self, index):. --> 101 oidx, vidx = self._normalize_indices(index). 102 . 103 # To preserve two dimensional shape. ~\anaconda3\lib\site-packages\anndata\_core\raw.p",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1599
https://github.com/scverse/scanpy/issues/1599:2207,modifiability,pac,packages,2207,"[np.in1d(adata.var.index, s_genes_mm)]. g2m_genes_mm_ens = adata.var.index[np.in1d(adata.var.index, g2m_genes_mm)]. sc.tl.score_genes_cell_cycle(adata, s_genes=s_genes_mm_ens, g2m_genes=s_genes_mm_ens). ```. ```pytb. KeyError Traceback (most recent call last). <ipython-input-63-57c51b3902c0> in <module>. 11 s_genes_mm_ens = adata.var.index[np.in1d(adata.var.index, s_genes_mm)]. 12 g2m_genes_mm_ens = adata.var.index[np.in1d(adata.var.index, g2m_genes_mm)]. ---> 13 sc.tl.score_genes_cell_cycle(adata, s_genes=s_genes_mm_ens, g2m_genes=s_genes_mm_ens). ~\anaconda3\lib\site-packages\scanpy\tools\_score_genes.py in score_genes_cell_cycle(adata, s_genes, g2m_genes, copy, **kwargs). 247 ctrl_size = min(len(s_genes), len(g2m_genes)). 248 # add s-score. --> 249 score_genes(adata, gene_list=s_genes, score_name='S_score', ctrl_size=ctrl_size, **kwargs). 250 # add g2m-score. 251 score_genes(adata, gene_list=g2m_genes, score_name='G2M_score', ctrl_size=ctrl_size, **kwargs). ~\anaconda3\lib\site-packages\scanpy\tools\_score_genes.py in score_genes(adata, gene_list, ctrl_size, gene_pool, n_bins, score_name, random_state, copy, use_raw). 153 gene_list = list(gene_list). 154 . --> 155 X_list = _adata[:, gene_list].X. 156 if issparse(X_list):. 157 X_list = np.array(_sparse_nanmean(X_list, axis=1)).flatten(). ~\anaconda3\lib\site-packages\anndata\_core\raw.py in __getitem__(self, index). 99 . 100 def __getitem__(self, index):. --> 101 oidx, vidx = self._normalize_indices(index). 102 . 103 # To preserve two dimensional shape. ~\anaconda3\lib\site-packages\anndata\_core\raw.py in _normalize_indices(self, packed_index). 159 obs, var = unpack_index(packed_index). 160 obs = _normalize_index(obs, self._adata.obs_names). --> 161 var = _normalize_index(var, self.var_names). 162 return obs, var. 163 . ~\anaconda3\lib\site-packages\anndata\_core\index.py in _normalize_index(indexer, index). 98 if np.any(positions < 0):. 99 not_found = indexer[positions < 0]. --> 100 raise KeyError(. 101 f""Values",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1599
https://github.com/scverse/scanpy/issues/1599:2543,modifiability,pac,packages,2543,"index[np.in1d(adata.var.index, s_genes_mm)]. 12 g2m_genes_mm_ens = adata.var.index[np.in1d(adata.var.index, g2m_genes_mm)]. ---> 13 sc.tl.score_genes_cell_cycle(adata, s_genes=s_genes_mm_ens, g2m_genes=s_genes_mm_ens). ~\anaconda3\lib\site-packages\scanpy\tools\_score_genes.py in score_genes_cell_cycle(adata, s_genes, g2m_genes, copy, **kwargs). 247 ctrl_size = min(len(s_genes), len(g2m_genes)). 248 # add s-score. --> 249 score_genes(adata, gene_list=s_genes, score_name='S_score', ctrl_size=ctrl_size, **kwargs). 250 # add g2m-score. 251 score_genes(adata, gene_list=g2m_genes, score_name='G2M_score', ctrl_size=ctrl_size, **kwargs). ~\anaconda3\lib\site-packages\scanpy\tools\_score_genes.py in score_genes(adata, gene_list, ctrl_size, gene_pool, n_bins, score_name, random_state, copy, use_raw). 153 gene_list = list(gene_list). 154 . --> 155 X_list = _adata[:, gene_list].X. 156 if issparse(X_list):. 157 X_list = np.array(_sparse_nanmean(X_list, axis=1)).flatten(). ~\anaconda3\lib\site-packages\anndata\_core\raw.py in __getitem__(self, index). 99 . 100 def __getitem__(self, index):. --> 101 oidx, vidx = self._normalize_indices(index). 102 . 103 # To preserve two dimensional shape. ~\anaconda3\lib\site-packages\anndata\_core\raw.py in _normalize_indices(self, packed_index). 159 obs, var = unpack_index(packed_index). 160 obs = _normalize_index(obs, self._adata.obs_names). --> 161 var = _normalize_index(var, self.var_names). 162 return obs, var. 163 . ~\anaconda3\lib\site-packages\anndata\_core\index.py in _normalize_index(indexer, index). 98 if np.any(positions < 0):. 99 not_found = indexer[positions < 0]. --> 100 raise KeyError(. 101 f""Values {list(not_found)}, from {list(indexer)}, "". 102 ""are not valid obs/ var names or indices."". KeyError: ""Values ['USP1', 'NSUN3', 'PHTF2', 'CCDC84', 'NUP160', 'RRM1', 'CENPM', 'SLC38A2', 'ABCC5', 'REEP1', 'DYNC1LI2', 'BMI1', 'RBBP8', 'MYCBP2', 'NFE2L2', 'GCLM', 'DNA2', 'OSGIN2', 'MAP3K2', 'BLM', 'PRIM1', 'RSRC2', 'CREBZF', 'E2F8', 'CDK",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1599
https://github.com/scverse/scanpy/issues/1599:2763,modifiability,pac,packages,2763,"\anaconda3\lib\site-packages\scanpy\tools\_score_genes.py in score_genes_cell_cycle(adata, s_genes, g2m_genes, copy, **kwargs). 247 ctrl_size = min(len(s_genes), len(g2m_genes)). 248 # add s-score. --> 249 score_genes(adata, gene_list=s_genes, score_name='S_score', ctrl_size=ctrl_size, **kwargs). 250 # add g2m-score. 251 score_genes(adata, gene_list=g2m_genes, score_name='G2M_score', ctrl_size=ctrl_size, **kwargs). ~\anaconda3\lib\site-packages\scanpy\tools\_score_genes.py in score_genes(adata, gene_list, ctrl_size, gene_pool, n_bins, score_name, random_state, copy, use_raw). 153 gene_list = list(gene_list). 154 . --> 155 X_list = _adata[:, gene_list].X. 156 if issparse(X_list):. 157 X_list = np.array(_sparse_nanmean(X_list, axis=1)).flatten(). ~\anaconda3\lib\site-packages\anndata\_core\raw.py in __getitem__(self, index). 99 . 100 def __getitem__(self, index):. --> 101 oidx, vidx = self._normalize_indices(index). 102 . 103 # To preserve two dimensional shape. ~\anaconda3\lib\site-packages\anndata\_core\raw.py in _normalize_indices(self, packed_index). 159 obs, var = unpack_index(packed_index). 160 obs = _normalize_index(obs, self._adata.obs_names). --> 161 var = _normalize_index(var, self.var_names). 162 return obs, var. 163 . ~\anaconda3\lib\site-packages\anndata\_core\index.py in _normalize_index(indexer, index). 98 if np.any(positions < 0):. 99 not_found = indexer[positions < 0]. --> 100 raise KeyError(. 101 f""Values {list(not_found)}, from {list(indexer)}, "". 102 ""are not valid obs/ var names or indices."". KeyError: ""Values ['USP1', 'NSUN3', 'PHTF2', 'CCDC84', 'NUP160', 'RRM1', 'CENPM', 'SLC38A2', 'ABCC5', 'REEP1', 'DYNC1LI2', 'BMI1', 'RBBP8', 'MYCBP2', 'NFE2L2', 'GCLM', 'DNA2', 'OSGIN2', 'MAP3K2', 'BLM', 'PRIM1', 'RSRC2', 'CREBZF', 'E2F8', 'CDKN2AIP', 'ASF1B', 'CCDC150', 'BRIP1', 'MAN1A2', 'ZWINT', 'PKMYT1', 'ZBED5', 'PHTF1', 'ATAD2', 'CASP2', 'BRCA1', 'HELLS', 'CALD1', 'ORC3', 'SVIP', 'ABHD10', 'SAP30BP', 'RAD51', 'TTLL7', 'UBL3', 'DNAJB4', 'NEAT1', 'KAT2A', ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1599
https://github.com/scverse/scanpy/issues/1599:3036,modifiability,pac,packages,3036,"ze=ctrl_size, **kwargs). 250 # add g2m-score. 251 score_genes(adata, gene_list=g2m_genes, score_name='G2M_score', ctrl_size=ctrl_size, **kwargs). ~\anaconda3\lib\site-packages\scanpy\tools\_score_genes.py in score_genes(adata, gene_list, ctrl_size, gene_pool, n_bins, score_name, random_state, copy, use_raw). 153 gene_list = list(gene_list). 154 . --> 155 X_list = _adata[:, gene_list].X. 156 if issparse(X_list):. 157 X_list = np.array(_sparse_nanmean(X_list, axis=1)).flatten(). ~\anaconda3\lib\site-packages\anndata\_core\raw.py in __getitem__(self, index). 99 . 100 def __getitem__(self, index):. --> 101 oidx, vidx = self._normalize_indices(index). 102 . 103 # To preserve two dimensional shape. ~\anaconda3\lib\site-packages\anndata\_core\raw.py in _normalize_indices(self, packed_index). 159 obs, var = unpack_index(packed_index). 160 obs = _normalize_index(obs, self._adata.obs_names). --> 161 var = _normalize_index(var, self.var_names). 162 return obs, var. 163 . ~\anaconda3\lib\site-packages\anndata\_core\index.py in _normalize_index(indexer, index). 98 if np.any(positions < 0):. 99 not_found = indexer[positions < 0]. --> 100 raise KeyError(. 101 f""Values {list(not_found)}, from {list(indexer)}, "". 102 ""are not valid obs/ var names or indices."". KeyError: ""Values ['USP1', 'NSUN3', 'PHTF2', 'CCDC84', 'NUP160', 'RRM1', 'CENPM', 'SLC38A2', 'ABCC5', 'REEP1', 'DYNC1LI2', 'BMI1', 'RBBP8', 'MYCBP2', 'NFE2L2', 'GCLM', 'DNA2', 'OSGIN2', 'MAP3K2', 'BLM', 'PRIM1', 'RSRC2', 'CREBZF', 'E2F8', 'CDKN2AIP', 'ASF1B', 'CCDC150', 'BRIP1', 'MAN1A2', 'ZWINT', 'PKMYT1', 'ZBED5', 'PHTF1', 'ATAD2', 'CASP2', 'BRCA1', 'HELLS', 'CALD1', 'ORC3', 'SVIP', 'ABHD10', 'SAP30BP', 'RAD51', 'TTLL7', 'UBL3', 'DNAJB4', 'NEAT1', 'KAT2A', 'OGT', 'CENPQ', 'RAD51AP1', 'TOP2A', 'CDC7', 'CRLS1', 'DEPDC7', 'EZH2', 'CDCA5', 'TYMS', 'FANCA', 'PTAR1', 'RMI1', 'CDC45', 'UBE2T', 'POLA1', 'FANCI', 'KAT2B', 'MCM8', 'SRSF5', 'COQ9', 'NT5DC1', 'BIVM', 'RHOBTB3', 'CHML', 'CALM2', 'NAB1', 'LMO4', 'SP1', 'INTS7', 'RFC2', 'E",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1599
https://github.com/scverse/scanpy/issues/1599:5164,modifiability,Version,Versions,5164,"ndices."". KeyError: ""Values ['USP1', 'NSUN3', 'PHTF2', 'CCDC84', 'NUP160', 'RRM1', 'CENPM', 'SLC38A2', 'ABCC5', 'REEP1', 'DYNC1LI2', 'BMI1', 'RBBP8', 'MYCBP2', 'NFE2L2', 'GCLM', 'DNA2', 'OSGIN2', 'MAP3K2', 'BLM', 'PRIM1', 'RSRC2', 'CREBZF', 'E2F8', 'CDKN2AIP', 'ASF1B', 'CCDC150', 'BRIP1', 'MAN1A2', 'ZWINT', 'PKMYT1', 'ZBED5', 'PHTF1', 'ATAD2', 'CASP2', 'BRCA1', 'HELLS', 'CALD1', 'ORC3', 'SVIP', 'ABHD10', 'SAP30BP', 'RAD51', 'TTLL7', 'UBL3', 'DNAJB4', 'NEAT1', 'KAT2A', 'OGT', 'CENPQ', 'RAD51AP1', 'TOP2A', 'CDC7', 'CRLS1', 'DEPDC7', 'EZH2', 'CDCA5', 'TYMS', 'FANCA', 'PTAR1', 'RMI1', 'CDC45', 'UBE2T', 'POLA1', 'FANCI', 'KAT2B', 'MCM8', 'SRSF5', 'COQ9', 'NT5DC1', 'BIVM', 'RHOBTB3', 'CHML', 'CALM2', 'NAB1', 'LMO4', 'SP1', 'INTS7', 'RFC2', 'ESCO2', 'MASTL', 'DHFR', 'FEN1', 'DSCC1', 'CCDC14', 'MBD4', 'LYRM7', 'NRD1', 'RAD18', 'EXO1', 'PHIP', 'RPA2', 'BBS2', 'CERS6', 'CPNE8', 'RRM2', 'DONSON', 'H1F0', 'EIF4EBP2'], from ['USP1', 'NSUN3', 'PHTF2', 'CCDC84', 'NUP160', 'RRM1', 'CENPM', 'SLC38A2', 'ABCC5', 'REEP1', 'DYNC1LI2', 'BMI1', 'RBBP8', 'MYCBP2', 'NFE2L2', 'GCLM', 'DNA2', 'OSGIN2', 'MAP3K2', 'BLM', 'PRIM1', 'RSRC2', 'CREBZF', 'E2F8', 'CDKN2AIP', 'ASF1B', 'CCDC150', 'BRIP1', 'MAN1A2', 'ZWINT', 'PKMYT1', 'ZBED5', 'PHTF1', 'ATAD2', 'CASP2', 'BRCA1', 'HELLS', 'CALD1', 'ORC3', 'SVIP', 'ABHD10', 'SAP30BP', 'RAD51', 'TTLL7', 'UBL3', 'DNAJB4', 'NEAT1', 'KAT2A', 'OGT', 'CENPQ', 'RAD51AP1', 'TOP2A', 'CDC7', 'CRLS1', 'DEPDC7', 'EZH2', 'CDCA5', 'TYMS', 'FANCA', 'PTAR1', 'RMI1', 'CDC45', 'UBE2T', 'POLA1', 'FANCI', 'KAT2B', 'MCM8', 'SRSF5', 'COQ9', 'NT5DC1', 'BIVM', 'RHOBTB3', 'CHML', 'CALM2', 'NAB1', 'LMO4', 'SP1', 'INTS7', 'RFC2', 'ESCO2', 'MASTL', 'DHFR', 'FEN1', 'DSCC1', 'CCDC14', 'MBD4', 'LYRM7', 'NRD1', 'RAD18', 'EXO1', 'PHIP', 'RPA2', 'BBS2', 'CERS6', 'CPNE8', 'RRM2', 'DONSON', 'H1F0', 'EIF4EBP2'], are not valid obs/ var names or indices."". ```. #### Versions. <details>. [Paste the output of scanpy.logging.print_versions() leaving a blank line after the details tag]. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1599
https://github.com/scverse/scanpy/issues/1599:546,performance,error,error,546,"KeyError in score_genes_cell_cycle; - [ ] I have checked that this issue has not already been reported. - [ ] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. Hello everyone, I am trying to calculate cell cycle score for my mouse data and encountering this error. I have already converted the gene name to upper case letters to read from the cell cycle genes. please help me to resolve this issue. Thank you very much. ### Minimal code sample (that we can copy&paste without having any data). ```python. ## Mouse. folder = ""D:/Pawandeep/Alldatasets/Macosko_cell_cycle_genes.txt"". cc_genes = pd.read_table(folder, delimiter='\t'). #cc_genes = pd.read_table(Macosko_cell_cycle_genes, delimiter='\t'). s_genes = cc_genes['S'].dropna(). g2m_genes = cc_genes['G2.M'].dropna(). s_genes_mm = [gene.lower().upper() for gene in s_genes]. g2m_genes_mm = [gene.lower().upper() for gene in g2m_genes]. s_genes_mm_ens = adata.var.index[np.in1d(adata.var.index, s_genes_mm)]. g2m_genes_mm_ens = adata.var.index[np.in1d(adata.var.index, g2m_genes_mm)]. sc.tl.score_genes_cell_cycle(adata, s_genes=s_genes_mm_ens, g2m_genes=s_genes_mm_ens). ```. ```pytb. KeyError Traceback (most recent call last). <ipython-input-63-57c51b3902c0> in <module>. 11 s_genes_mm_ens = adata.var.index[np.in1d(adata.var.index, s_genes_mm)]. 12 g2m_genes_mm_ens = adata.var.index[np.in1d(adata.var.index, g2m_genes_mm)]. ---> 13 sc.tl.score_genes_cell_cycle(adata, s_genes=s_genes_mm_ens, g2m_genes=s_genes_mm_ens). ~\anaconda3\lib\site-packages\scanpy\tools\_score_genes.py in score_genes_cell_cycle(adata, s_genes, g2m_genes, copy, **kwargs). 247 ctrl_size = min(len(s_genes), len(g2m_genes)). 248 # add s-score. --> 249 score_genes(adata, gene_lis",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1599
https://github.com/scverse/scanpy/issues/1599:546,safety,error,error,546,"KeyError in score_genes_cell_cycle; - [ ] I have checked that this issue has not already been reported. - [ ] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. Hello everyone, I am trying to calculate cell cycle score for my mouse data and encountering this error. I have already converted the gene name to upper case letters to read from the cell cycle genes. please help me to resolve this issue. Thank you very much. ### Minimal code sample (that we can copy&paste without having any data). ```python. ## Mouse. folder = ""D:/Pawandeep/Alldatasets/Macosko_cell_cycle_genes.txt"". cc_genes = pd.read_table(folder, delimiter='\t'). #cc_genes = pd.read_table(Macosko_cell_cycle_genes, delimiter='\t'). s_genes = cc_genes['S'].dropna(). g2m_genes = cc_genes['G2.M'].dropna(). s_genes_mm = [gene.lower().upper() for gene in s_genes]. g2m_genes_mm = [gene.lower().upper() for gene in g2m_genes]. s_genes_mm_ens = adata.var.index[np.in1d(adata.var.index, s_genes_mm)]. g2m_genes_mm_ens = adata.var.index[np.in1d(adata.var.index, g2m_genes_mm)]. sc.tl.score_genes_cell_cycle(adata, s_genes=s_genes_mm_ens, g2m_genes=s_genes_mm_ens). ```. ```pytb. KeyError Traceback (most recent call last). <ipython-input-63-57c51b3902c0> in <module>. 11 s_genes_mm_ens = adata.var.index[np.in1d(adata.var.index, s_genes_mm)]. 12 g2m_genes_mm_ens = adata.var.index[np.in1d(adata.var.index, g2m_genes_mm)]. ---> 13 sc.tl.score_genes_cell_cycle(adata, s_genes=s_genes_mm_ens, g2m_genes=s_genes_mm_ens). ~\anaconda3\lib\site-packages\scanpy\tools\_score_genes.py in score_genes_cell_cycle(adata, s_genes, g2m_genes, copy, **kwargs). 247 ctrl_size = min(len(s_genes), len(g2m_genes)). 248 # add s-score. --> 249 score_genes(adata, gene_lis",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1599
https://github.com/scverse/scanpy/issues/1599:1481,safety,input,input-,1481,"late cell cycle score for my mouse data and encountering this error. I have already converted the gene name to upper case letters to read from the cell cycle genes. please help me to resolve this issue. Thank you very much. ### Minimal code sample (that we can copy&paste without having any data). ```python. ## Mouse. folder = ""D:/Pawandeep/Alldatasets/Macosko_cell_cycle_genes.txt"". cc_genes = pd.read_table(folder, delimiter='\t'). #cc_genes = pd.read_table(Macosko_cell_cycle_genes, delimiter='\t'). s_genes = cc_genes['S'].dropna(). g2m_genes = cc_genes['G2.M'].dropna(). s_genes_mm = [gene.lower().upper() for gene in s_genes]. g2m_genes_mm = [gene.lower().upper() for gene in g2m_genes]. s_genes_mm_ens = adata.var.index[np.in1d(adata.var.index, s_genes_mm)]. g2m_genes_mm_ens = adata.var.index[np.in1d(adata.var.index, g2m_genes_mm)]. sc.tl.score_genes_cell_cycle(adata, s_genes=s_genes_mm_ens, g2m_genes=s_genes_mm_ens). ```. ```pytb. KeyError Traceback (most recent call last). <ipython-input-63-57c51b3902c0> in <module>. 11 s_genes_mm_ens = adata.var.index[np.in1d(adata.var.index, s_genes_mm)]. 12 g2m_genes_mm_ens = adata.var.index[np.in1d(adata.var.index, g2m_genes_mm)]. ---> 13 sc.tl.score_genes_cell_cycle(adata, s_genes=s_genes_mm_ens, g2m_genes=s_genes_mm_ens). ~\anaconda3\lib\site-packages\scanpy\tools\_score_genes.py in score_genes_cell_cycle(adata, s_genes, g2m_genes, copy, **kwargs). 247 ctrl_size = min(len(s_genes), len(g2m_genes)). 248 # add s-score. --> 249 score_genes(adata, gene_list=s_genes, score_name='S_score', ctrl_size=ctrl_size, **kwargs). 250 # add g2m-score. 251 score_genes(adata, gene_list=g2m_genes, score_name='G2M_score', ctrl_size=ctrl_size, **kwargs). ~\anaconda3\lib\site-packages\scanpy\tools\_score_genes.py in score_genes(adata, gene_list, ctrl_size, gene_pool, n_bins, score_name, random_state, copy, use_raw). 153 gene_list = list(gene_list). 154 . --> 155 X_list = _adata[:, gene_list].X. 156 if issparse(X_list):. 157 X_list = np.array(_spars",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1599
https://github.com/scverse/scanpy/issues/1599:1508,safety,modul,module,1508,"y mouse data and encountering this error. I have already converted the gene name to upper case letters to read from the cell cycle genes. please help me to resolve this issue. Thank you very much. ### Minimal code sample (that we can copy&paste without having any data). ```python. ## Mouse. folder = ""D:/Pawandeep/Alldatasets/Macosko_cell_cycle_genes.txt"". cc_genes = pd.read_table(folder, delimiter='\t'). #cc_genes = pd.read_table(Macosko_cell_cycle_genes, delimiter='\t'). s_genes = cc_genes['S'].dropna(). g2m_genes = cc_genes['G2.M'].dropna(). s_genes_mm = [gene.lower().upper() for gene in s_genes]. g2m_genes_mm = [gene.lower().upper() for gene in g2m_genes]. s_genes_mm_ens = adata.var.index[np.in1d(adata.var.index, s_genes_mm)]. g2m_genes_mm_ens = adata.var.index[np.in1d(adata.var.index, g2m_genes_mm)]. sc.tl.score_genes_cell_cycle(adata, s_genes=s_genes_mm_ens, g2m_genes=s_genes_mm_ens). ```. ```pytb. KeyError Traceback (most recent call last). <ipython-input-63-57c51b3902c0> in <module>. 11 s_genes_mm_ens = adata.var.index[np.in1d(adata.var.index, s_genes_mm)]. 12 g2m_genes_mm_ens = adata.var.index[np.in1d(adata.var.index, g2m_genes_mm)]. ---> 13 sc.tl.score_genes_cell_cycle(adata, s_genes=s_genes_mm_ens, g2m_genes=s_genes_mm_ens). ~\anaconda3\lib\site-packages\scanpy\tools\_score_genes.py in score_genes_cell_cycle(adata, s_genes, g2m_genes, copy, **kwargs). 247 ctrl_size = min(len(s_genes), len(g2m_genes)). 248 # add s-score. --> 249 score_genes(adata, gene_list=s_genes, score_name='S_score', ctrl_size=ctrl_size, **kwargs). 250 # add g2m-score. 251 score_genes(adata, gene_list=g2m_genes, score_name='G2M_score', ctrl_size=ctrl_size, **kwargs). ~\anaconda3\lib\site-packages\scanpy\tools\_score_genes.py in score_genes(adata, gene_list, ctrl_size, gene_pool, n_bins, score_name, random_state, copy, use_raw). 153 gene_list = list(gene_list). 154 . --> 155 X_list = _adata[:, gene_list].X. 156 if issparse(X_list):. 157 X_list = np.array(_sparse_nanmean(X_list, axis=1)).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1599
https://github.com/scverse/scanpy/issues/1599:3269,safety,valid,valid,3269,"list, ctrl_size, gene_pool, n_bins, score_name, random_state, copy, use_raw). 153 gene_list = list(gene_list). 154 . --> 155 X_list = _adata[:, gene_list].X. 156 if issparse(X_list):. 157 X_list = np.array(_sparse_nanmean(X_list, axis=1)).flatten(). ~\anaconda3\lib\site-packages\anndata\_core\raw.py in __getitem__(self, index). 99 . 100 def __getitem__(self, index):. --> 101 oidx, vidx = self._normalize_indices(index). 102 . 103 # To preserve two dimensional shape. ~\anaconda3\lib\site-packages\anndata\_core\raw.py in _normalize_indices(self, packed_index). 159 obs, var = unpack_index(packed_index). 160 obs = _normalize_index(obs, self._adata.obs_names). --> 161 var = _normalize_index(var, self.var_names). 162 return obs, var. 163 . ~\anaconda3\lib\site-packages\anndata\_core\index.py in _normalize_index(indexer, index). 98 if np.any(positions < 0):. 99 not_found = indexer[positions < 0]. --> 100 raise KeyError(. 101 f""Values {list(not_found)}, from {list(indexer)}, "". 102 ""are not valid obs/ var names or indices."". KeyError: ""Values ['USP1', 'NSUN3', 'PHTF2', 'CCDC84', 'NUP160', 'RRM1', 'CENPM', 'SLC38A2', 'ABCC5', 'REEP1', 'DYNC1LI2', 'BMI1', 'RBBP8', 'MYCBP2', 'NFE2L2', 'GCLM', 'DNA2', 'OSGIN2', 'MAP3K2', 'BLM', 'PRIM1', 'RSRC2', 'CREBZF', 'E2F8', 'CDKN2AIP', 'ASF1B', 'CCDC150', 'BRIP1', 'MAN1A2', 'ZWINT', 'PKMYT1', 'ZBED5', 'PHTF1', 'ATAD2', 'CASP2', 'BRCA1', 'HELLS', 'CALD1', 'ORC3', 'SVIP', 'ABHD10', 'SAP30BP', 'RAD51', 'TTLL7', 'UBL3', 'DNAJB4', 'NEAT1', 'KAT2A', 'OGT', 'CENPQ', 'RAD51AP1', 'TOP2A', 'CDC7', 'CRLS1', 'DEPDC7', 'EZH2', 'CDCA5', 'TYMS', 'FANCA', 'PTAR1', 'RMI1', 'CDC45', 'UBE2T', 'POLA1', 'FANCI', 'KAT2B', 'MCM8', 'SRSF5', 'COQ9', 'NT5DC1', 'BIVM', 'RHOBTB3', 'CHML', 'CALM2', 'NAB1', 'LMO4', 'SP1', 'INTS7', 'RFC2', 'ESCO2', 'MASTL', 'DHFR', 'FEN1', 'DSCC1', 'CCDC14', 'MBD4', 'LYRM7', 'NRD1', 'RAD18', 'EXO1', 'PHIP', 'RPA2', 'BBS2', 'CERS6', 'CPNE8', 'RRM2', 'DONSON', 'H1F0', 'EIF4EBP2'], from ['USP1', 'NSUN3', 'PHTF2', 'CCDC84', 'NUP160', 'RRM1'",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1599
https://github.com/scverse/scanpy/issues/1599:5119,safety,valid,valid,5119,"ndices."". KeyError: ""Values ['USP1', 'NSUN3', 'PHTF2', 'CCDC84', 'NUP160', 'RRM1', 'CENPM', 'SLC38A2', 'ABCC5', 'REEP1', 'DYNC1LI2', 'BMI1', 'RBBP8', 'MYCBP2', 'NFE2L2', 'GCLM', 'DNA2', 'OSGIN2', 'MAP3K2', 'BLM', 'PRIM1', 'RSRC2', 'CREBZF', 'E2F8', 'CDKN2AIP', 'ASF1B', 'CCDC150', 'BRIP1', 'MAN1A2', 'ZWINT', 'PKMYT1', 'ZBED5', 'PHTF1', 'ATAD2', 'CASP2', 'BRCA1', 'HELLS', 'CALD1', 'ORC3', 'SVIP', 'ABHD10', 'SAP30BP', 'RAD51', 'TTLL7', 'UBL3', 'DNAJB4', 'NEAT1', 'KAT2A', 'OGT', 'CENPQ', 'RAD51AP1', 'TOP2A', 'CDC7', 'CRLS1', 'DEPDC7', 'EZH2', 'CDCA5', 'TYMS', 'FANCA', 'PTAR1', 'RMI1', 'CDC45', 'UBE2T', 'POLA1', 'FANCI', 'KAT2B', 'MCM8', 'SRSF5', 'COQ9', 'NT5DC1', 'BIVM', 'RHOBTB3', 'CHML', 'CALM2', 'NAB1', 'LMO4', 'SP1', 'INTS7', 'RFC2', 'ESCO2', 'MASTL', 'DHFR', 'FEN1', 'DSCC1', 'CCDC14', 'MBD4', 'LYRM7', 'NRD1', 'RAD18', 'EXO1', 'PHIP', 'RPA2', 'BBS2', 'CERS6', 'CPNE8', 'RRM2', 'DONSON', 'H1F0', 'EIF4EBP2'], from ['USP1', 'NSUN3', 'PHTF2', 'CCDC84', 'NUP160', 'RRM1', 'CENPM', 'SLC38A2', 'ABCC5', 'REEP1', 'DYNC1LI2', 'BMI1', 'RBBP8', 'MYCBP2', 'NFE2L2', 'GCLM', 'DNA2', 'OSGIN2', 'MAP3K2', 'BLM', 'PRIM1', 'RSRC2', 'CREBZF', 'E2F8', 'CDKN2AIP', 'ASF1B', 'CCDC150', 'BRIP1', 'MAN1A2', 'ZWINT', 'PKMYT1', 'ZBED5', 'PHTF1', 'ATAD2', 'CASP2', 'BRCA1', 'HELLS', 'CALD1', 'ORC3', 'SVIP', 'ABHD10', 'SAP30BP', 'RAD51', 'TTLL7', 'UBL3', 'DNAJB4', 'NEAT1', 'KAT2A', 'OGT', 'CENPQ', 'RAD51AP1', 'TOP2A', 'CDC7', 'CRLS1', 'DEPDC7', 'EZH2', 'CDCA5', 'TYMS', 'FANCA', 'PTAR1', 'RMI1', 'CDC45', 'UBE2T', 'POLA1', 'FANCI', 'KAT2B', 'MCM8', 'SRSF5', 'COQ9', 'NT5DC1', 'BIVM', 'RHOBTB3', 'CHML', 'CALM2', 'NAB1', 'LMO4', 'SP1', 'INTS7', 'RFC2', 'ESCO2', 'MASTL', 'DHFR', 'FEN1', 'DSCC1', 'CCDC14', 'MBD4', 'LYRM7', 'NRD1', 'RAD18', 'EXO1', 'PHIP', 'RPA2', 'BBS2', 'CERS6', 'CPNE8', 'RRM2', 'DONSON', 'H1F0', 'EIF4EBP2'], are not valid obs/ var names or indices."". ```. #### Versions. <details>. [Paste the output of scanpy.logging.print_versions() leaving a blank line after the details tag]. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1599
https://github.com/scverse/scanpy/issues/1599:5213,safety,log,logging,5213,"ndices."". KeyError: ""Values ['USP1', 'NSUN3', 'PHTF2', 'CCDC84', 'NUP160', 'RRM1', 'CENPM', 'SLC38A2', 'ABCC5', 'REEP1', 'DYNC1LI2', 'BMI1', 'RBBP8', 'MYCBP2', 'NFE2L2', 'GCLM', 'DNA2', 'OSGIN2', 'MAP3K2', 'BLM', 'PRIM1', 'RSRC2', 'CREBZF', 'E2F8', 'CDKN2AIP', 'ASF1B', 'CCDC150', 'BRIP1', 'MAN1A2', 'ZWINT', 'PKMYT1', 'ZBED5', 'PHTF1', 'ATAD2', 'CASP2', 'BRCA1', 'HELLS', 'CALD1', 'ORC3', 'SVIP', 'ABHD10', 'SAP30BP', 'RAD51', 'TTLL7', 'UBL3', 'DNAJB4', 'NEAT1', 'KAT2A', 'OGT', 'CENPQ', 'RAD51AP1', 'TOP2A', 'CDC7', 'CRLS1', 'DEPDC7', 'EZH2', 'CDCA5', 'TYMS', 'FANCA', 'PTAR1', 'RMI1', 'CDC45', 'UBE2T', 'POLA1', 'FANCI', 'KAT2B', 'MCM8', 'SRSF5', 'COQ9', 'NT5DC1', 'BIVM', 'RHOBTB3', 'CHML', 'CALM2', 'NAB1', 'LMO4', 'SP1', 'INTS7', 'RFC2', 'ESCO2', 'MASTL', 'DHFR', 'FEN1', 'DSCC1', 'CCDC14', 'MBD4', 'LYRM7', 'NRD1', 'RAD18', 'EXO1', 'PHIP', 'RPA2', 'BBS2', 'CERS6', 'CPNE8', 'RRM2', 'DONSON', 'H1F0', 'EIF4EBP2'], from ['USP1', 'NSUN3', 'PHTF2', 'CCDC84', 'NUP160', 'RRM1', 'CENPM', 'SLC38A2', 'ABCC5', 'REEP1', 'DYNC1LI2', 'BMI1', 'RBBP8', 'MYCBP2', 'NFE2L2', 'GCLM', 'DNA2', 'OSGIN2', 'MAP3K2', 'BLM', 'PRIM1', 'RSRC2', 'CREBZF', 'E2F8', 'CDKN2AIP', 'ASF1B', 'CCDC150', 'BRIP1', 'MAN1A2', 'ZWINT', 'PKMYT1', 'ZBED5', 'PHTF1', 'ATAD2', 'CASP2', 'BRCA1', 'HELLS', 'CALD1', 'ORC3', 'SVIP', 'ABHD10', 'SAP30BP', 'RAD51', 'TTLL7', 'UBL3', 'DNAJB4', 'NEAT1', 'KAT2A', 'OGT', 'CENPQ', 'RAD51AP1', 'TOP2A', 'CDC7', 'CRLS1', 'DEPDC7', 'EZH2', 'CDCA5', 'TYMS', 'FANCA', 'PTAR1', 'RMI1', 'CDC45', 'UBE2T', 'POLA1', 'FANCI', 'KAT2B', 'MCM8', 'SRSF5', 'COQ9', 'NT5DC1', 'BIVM', 'RHOBTB3', 'CHML', 'CALM2', 'NAB1', 'LMO4', 'SP1', 'INTS7', 'RFC2', 'ESCO2', 'MASTL', 'DHFR', 'FEN1', 'DSCC1', 'CCDC14', 'MBD4', 'LYRM7', 'NRD1', 'RAD18', 'EXO1', 'PHIP', 'RPA2', 'BBS2', 'CERS6', 'CPNE8', 'RRM2', 'DONSON', 'H1F0', 'EIF4EBP2'], are not valid obs/ var names or indices."". ```. #### Versions. <details>. [Paste the output of scanpy.logging.print_versions() leaving a blank line after the details tag]. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1599
https://github.com/scverse/scanpy/issues/1599:5213,security,log,logging,5213,"ndices."". KeyError: ""Values ['USP1', 'NSUN3', 'PHTF2', 'CCDC84', 'NUP160', 'RRM1', 'CENPM', 'SLC38A2', 'ABCC5', 'REEP1', 'DYNC1LI2', 'BMI1', 'RBBP8', 'MYCBP2', 'NFE2L2', 'GCLM', 'DNA2', 'OSGIN2', 'MAP3K2', 'BLM', 'PRIM1', 'RSRC2', 'CREBZF', 'E2F8', 'CDKN2AIP', 'ASF1B', 'CCDC150', 'BRIP1', 'MAN1A2', 'ZWINT', 'PKMYT1', 'ZBED5', 'PHTF1', 'ATAD2', 'CASP2', 'BRCA1', 'HELLS', 'CALD1', 'ORC3', 'SVIP', 'ABHD10', 'SAP30BP', 'RAD51', 'TTLL7', 'UBL3', 'DNAJB4', 'NEAT1', 'KAT2A', 'OGT', 'CENPQ', 'RAD51AP1', 'TOP2A', 'CDC7', 'CRLS1', 'DEPDC7', 'EZH2', 'CDCA5', 'TYMS', 'FANCA', 'PTAR1', 'RMI1', 'CDC45', 'UBE2T', 'POLA1', 'FANCI', 'KAT2B', 'MCM8', 'SRSF5', 'COQ9', 'NT5DC1', 'BIVM', 'RHOBTB3', 'CHML', 'CALM2', 'NAB1', 'LMO4', 'SP1', 'INTS7', 'RFC2', 'ESCO2', 'MASTL', 'DHFR', 'FEN1', 'DSCC1', 'CCDC14', 'MBD4', 'LYRM7', 'NRD1', 'RAD18', 'EXO1', 'PHIP', 'RPA2', 'BBS2', 'CERS6', 'CPNE8', 'RRM2', 'DONSON', 'H1F0', 'EIF4EBP2'], from ['USP1', 'NSUN3', 'PHTF2', 'CCDC84', 'NUP160', 'RRM1', 'CENPM', 'SLC38A2', 'ABCC5', 'REEP1', 'DYNC1LI2', 'BMI1', 'RBBP8', 'MYCBP2', 'NFE2L2', 'GCLM', 'DNA2', 'OSGIN2', 'MAP3K2', 'BLM', 'PRIM1', 'RSRC2', 'CREBZF', 'E2F8', 'CDKN2AIP', 'ASF1B', 'CCDC150', 'BRIP1', 'MAN1A2', 'ZWINT', 'PKMYT1', 'ZBED5', 'PHTF1', 'ATAD2', 'CASP2', 'BRCA1', 'HELLS', 'CALD1', 'ORC3', 'SVIP', 'ABHD10', 'SAP30BP', 'RAD51', 'TTLL7', 'UBL3', 'DNAJB4', 'NEAT1', 'KAT2A', 'OGT', 'CENPQ', 'RAD51AP1', 'TOP2A', 'CDC7', 'CRLS1', 'DEPDC7', 'EZH2', 'CDCA5', 'TYMS', 'FANCA', 'PTAR1', 'RMI1', 'CDC45', 'UBE2T', 'POLA1', 'FANCI', 'KAT2B', 'MCM8', 'SRSF5', 'COQ9', 'NT5DC1', 'BIVM', 'RHOBTB3', 'CHML', 'CALM2', 'NAB1', 'LMO4', 'SP1', 'INTS7', 'RFC2', 'ESCO2', 'MASTL', 'DHFR', 'FEN1', 'DSCC1', 'CCDC14', 'MBD4', 'LYRM7', 'NRD1', 'RAD18', 'EXO1', 'PHIP', 'RPA2', 'BBS2', 'CERS6', 'CPNE8', 'RRM2', 'DONSON', 'H1F0', 'EIF4EBP2'], are not valid obs/ var names or indices."". ```. #### Versions. <details>. [Paste the output of scanpy.logging.print_versions() leaving a blank line after the details tag]. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1599
https://github.com/scverse/scanpy/issues/1599:1437,testability,Trace,Traceback,1437," bug. Hello everyone, I am trying to calculate cell cycle score for my mouse data and encountering this error. I have already converted the gene name to upper case letters to read from the cell cycle genes. please help me to resolve this issue. Thank you very much. ### Minimal code sample (that we can copy&paste without having any data). ```python. ## Mouse. folder = ""D:/Pawandeep/Alldatasets/Macosko_cell_cycle_genes.txt"". cc_genes = pd.read_table(folder, delimiter='\t'). #cc_genes = pd.read_table(Macosko_cell_cycle_genes, delimiter='\t'). s_genes = cc_genes['S'].dropna(). g2m_genes = cc_genes['G2.M'].dropna(). s_genes_mm = [gene.lower().upper() for gene in s_genes]. g2m_genes_mm = [gene.lower().upper() for gene in g2m_genes]. s_genes_mm_ens = adata.var.index[np.in1d(adata.var.index, s_genes_mm)]. g2m_genes_mm_ens = adata.var.index[np.in1d(adata.var.index, g2m_genes_mm)]. sc.tl.score_genes_cell_cycle(adata, s_genes=s_genes_mm_ens, g2m_genes=s_genes_mm_ens). ```. ```pytb. KeyError Traceback (most recent call last). <ipython-input-63-57c51b3902c0> in <module>. 11 s_genes_mm_ens = adata.var.index[np.in1d(adata.var.index, s_genes_mm)]. 12 g2m_genes_mm_ens = adata.var.index[np.in1d(adata.var.index, g2m_genes_mm)]. ---> 13 sc.tl.score_genes_cell_cycle(adata, s_genes=s_genes_mm_ens, g2m_genes=s_genes_mm_ens). ~\anaconda3\lib\site-packages\scanpy\tools\_score_genes.py in score_genes_cell_cycle(adata, s_genes, g2m_genes, copy, **kwargs). 247 ctrl_size = min(len(s_genes), len(g2m_genes)). 248 # add s-score. --> 249 score_genes(adata, gene_list=s_genes, score_name='S_score', ctrl_size=ctrl_size, **kwargs). 250 # add g2m-score. 251 score_genes(adata, gene_list=g2m_genes, score_name='G2M_score', ctrl_size=ctrl_size, **kwargs). ~\anaconda3\lib\site-packages\scanpy\tools\_score_genes.py in score_genes(adata, gene_list, ctrl_size, gene_pool, n_bins, score_name, random_state, copy, use_raw). 153 gene_list = list(gene_list). 154 . --> 155 X_list = _adata[:, gene_list].X. 156 if isspa",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1599
https://github.com/scverse/scanpy/issues/1599:5213,testability,log,logging,5213,"ndices."". KeyError: ""Values ['USP1', 'NSUN3', 'PHTF2', 'CCDC84', 'NUP160', 'RRM1', 'CENPM', 'SLC38A2', 'ABCC5', 'REEP1', 'DYNC1LI2', 'BMI1', 'RBBP8', 'MYCBP2', 'NFE2L2', 'GCLM', 'DNA2', 'OSGIN2', 'MAP3K2', 'BLM', 'PRIM1', 'RSRC2', 'CREBZF', 'E2F8', 'CDKN2AIP', 'ASF1B', 'CCDC150', 'BRIP1', 'MAN1A2', 'ZWINT', 'PKMYT1', 'ZBED5', 'PHTF1', 'ATAD2', 'CASP2', 'BRCA1', 'HELLS', 'CALD1', 'ORC3', 'SVIP', 'ABHD10', 'SAP30BP', 'RAD51', 'TTLL7', 'UBL3', 'DNAJB4', 'NEAT1', 'KAT2A', 'OGT', 'CENPQ', 'RAD51AP1', 'TOP2A', 'CDC7', 'CRLS1', 'DEPDC7', 'EZH2', 'CDCA5', 'TYMS', 'FANCA', 'PTAR1', 'RMI1', 'CDC45', 'UBE2T', 'POLA1', 'FANCI', 'KAT2B', 'MCM8', 'SRSF5', 'COQ9', 'NT5DC1', 'BIVM', 'RHOBTB3', 'CHML', 'CALM2', 'NAB1', 'LMO4', 'SP1', 'INTS7', 'RFC2', 'ESCO2', 'MASTL', 'DHFR', 'FEN1', 'DSCC1', 'CCDC14', 'MBD4', 'LYRM7', 'NRD1', 'RAD18', 'EXO1', 'PHIP', 'RPA2', 'BBS2', 'CERS6', 'CPNE8', 'RRM2', 'DONSON', 'H1F0', 'EIF4EBP2'], from ['USP1', 'NSUN3', 'PHTF2', 'CCDC84', 'NUP160', 'RRM1', 'CENPM', 'SLC38A2', 'ABCC5', 'REEP1', 'DYNC1LI2', 'BMI1', 'RBBP8', 'MYCBP2', 'NFE2L2', 'GCLM', 'DNA2', 'OSGIN2', 'MAP3K2', 'BLM', 'PRIM1', 'RSRC2', 'CREBZF', 'E2F8', 'CDKN2AIP', 'ASF1B', 'CCDC150', 'BRIP1', 'MAN1A2', 'ZWINT', 'PKMYT1', 'ZBED5', 'PHTF1', 'ATAD2', 'CASP2', 'BRCA1', 'HELLS', 'CALD1', 'ORC3', 'SVIP', 'ABHD10', 'SAP30BP', 'RAD51', 'TTLL7', 'UBL3', 'DNAJB4', 'NEAT1', 'KAT2A', 'OGT', 'CENPQ', 'RAD51AP1', 'TOP2A', 'CDC7', 'CRLS1', 'DEPDC7', 'EZH2', 'CDCA5', 'TYMS', 'FANCA', 'PTAR1', 'RMI1', 'CDC45', 'UBE2T', 'POLA1', 'FANCI', 'KAT2B', 'MCM8', 'SRSF5', 'COQ9', 'NT5DC1', 'BIVM', 'RHOBTB3', 'CHML', 'CALM2', 'NAB1', 'LMO4', 'SP1', 'INTS7', 'RFC2', 'ESCO2', 'MASTL', 'DHFR', 'FEN1', 'DSCC1', 'CCDC14', 'MBD4', 'LYRM7', 'NRD1', 'RAD18', 'EXO1', 'PHIP', 'RPA2', 'BBS2', 'CERS6', 'CPNE8', 'RRM2', 'DONSON', 'H1F0', 'EIF4EBP2'], are not valid obs/ var names or indices."". ```. #### Versions. <details>. [Paste the output of scanpy.logging.print_versions() leaving a blank line after the details tag]. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1599
https://github.com/scverse/scanpy/issues/1599:117,usability,confirm,confirmed,117,"KeyError in score_genes_cell_cycle; - [ ] I have checked that this issue has not already been reported. - [ ] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. Hello everyone, I am trying to calculate cell cycle score for my mouse data and encountering this error. I have already converted the gene name to upper case letters to read from the cell cycle genes. please help me to resolve this issue. Thank you very much. ### Minimal code sample (that we can copy&paste without having any data). ```python. ## Mouse. folder = ""D:/Pawandeep/Alldatasets/Macosko_cell_cycle_genes.txt"". cc_genes = pd.read_table(folder, delimiter='\t'). #cc_genes = pd.read_table(Macosko_cell_cycle_genes, delimiter='\t'). s_genes = cc_genes['S'].dropna(). g2m_genes = cc_genes['G2.M'].dropna(). s_genes_mm = [gene.lower().upper() for gene in s_genes]. g2m_genes_mm = [gene.lower().upper() for gene in g2m_genes]. s_genes_mm_ens = adata.var.index[np.in1d(adata.var.index, s_genes_mm)]. g2m_genes_mm_ens = adata.var.index[np.in1d(adata.var.index, g2m_genes_mm)]. sc.tl.score_genes_cell_cycle(adata, s_genes=s_genes_mm_ens, g2m_genes=s_genes_mm_ens). ```. ```pytb. KeyError Traceback (most recent call last). <ipython-input-63-57c51b3902c0> in <module>. 11 s_genes_mm_ens = adata.var.index[np.in1d(adata.var.index, s_genes_mm)]. 12 g2m_genes_mm_ens = adata.var.index[np.in1d(adata.var.index, g2m_genes_mm)]. ---> 13 sc.tl.score_genes_cell_cycle(adata, s_genes=s_genes_mm_ens, g2m_genes=s_genes_mm_ens). ~\anaconda3\lib\site-packages\scanpy\tools\_score_genes.py in score_genes_cell_cycle(adata, s_genes, g2m_genes, copy, **kwargs). 247 ctrl_size = min(len(s_genes), len(g2m_genes)). 248 # add s-score. --> 249 score_genes(adata, gene_lis",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1599
https://github.com/scverse/scanpy/issues/1599:200,usability,confirm,confirmed,200,"KeyError in score_genes_cell_cycle; - [ ] I have checked that this issue has not already been reported. - [ ] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. Hello everyone, I am trying to calculate cell cycle score for my mouse data and encountering this error. I have already converted the gene name to upper case letters to read from the cell cycle genes. please help me to resolve this issue. Thank you very much. ### Minimal code sample (that we can copy&paste without having any data). ```python. ## Mouse. folder = ""D:/Pawandeep/Alldatasets/Macosko_cell_cycle_genes.txt"". cc_genes = pd.read_table(folder, delimiter='\t'). #cc_genes = pd.read_table(Macosko_cell_cycle_genes, delimiter='\t'). s_genes = cc_genes['S'].dropna(). g2m_genes = cc_genes['G2.M'].dropna(). s_genes_mm = [gene.lower().upper() for gene in s_genes]. g2m_genes_mm = [gene.lower().upper() for gene in g2m_genes]. s_genes_mm_ens = adata.var.index[np.in1d(adata.var.index, s_genes_mm)]. g2m_genes_mm_ens = adata.var.index[np.in1d(adata.var.index, g2m_genes_mm)]. sc.tl.score_genes_cell_cycle(adata, s_genes=s_genes_mm_ens, g2m_genes=s_genes_mm_ens). ```. ```pytb. KeyError Traceback (most recent call last). <ipython-input-63-57c51b3902c0> in <module>. 11 s_genes_mm_ens = adata.var.index[np.in1d(adata.var.index, s_genes_mm)]. 12 g2m_genes_mm_ens = adata.var.index[np.in1d(adata.var.index, g2m_genes_mm)]. ---> 13 sc.tl.score_genes_cell_cycle(adata, s_genes=s_genes_mm_ens, g2m_genes=s_genes_mm_ens). ~\anaconda3\lib\site-packages\scanpy\tools\_score_genes.py in score_genes_cell_cycle(adata, s_genes, g2m_genes, copy, **kwargs). 247 ctrl_size = min(len(s_genes), len(g2m_genes)). 248 # add s-score. --> 249 score_genes(adata, gene_lis",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1599
https://github.com/scverse/scanpy/issues/1599:291,usability,guid,guide,291,"KeyError in score_genes_cell_cycle; - [ ] I have checked that this issue has not already been reported. - [ ] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. Hello everyone, I am trying to calculate cell cycle score for my mouse data and encountering this error. I have already converted the gene name to upper case letters to read from the cell cycle genes. please help me to resolve this issue. Thank you very much. ### Minimal code sample (that we can copy&paste without having any data). ```python. ## Mouse. folder = ""D:/Pawandeep/Alldatasets/Macosko_cell_cycle_genes.txt"". cc_genes = pd.read_table(folder, delimiter='\t'). #cc_genes = pd.read_table(Macosko_cell_cycle_genes, delimiter='\t'). s_genes = cc_genes['S'].dropna(). g2m_genes = cc_genes['G2.M'].dropna(). s_genes_mm = [gene.lower().upper() for gene in s_genes]. g2m_genes_mm = [gene.lower().upper() for gene in g2m_genes]. s_genes_mm_ens = adata.var.index[np.in1d(adata.var.index, s_genes_mm)]. g2m_genes_mm_ens = adata.var.index[np.in1d(adata.var.index, g2m_genes_mm)]. sc.tl.score_genes_cell_cycle(adata, s_genes=s_genes_mm_ens, g2m_genes=s_genes_mm_ens). ```. ```pytb. KeyError Traceback (most recent call last). <ipython-input-63-57c51b3902c0> in <module>. 11 s_genes_mm_ens = adata.var.index[np.in1d(adata.var.index, s_genes_mm)]. 12 g2m_genes_mm_ens = adata.var.index[np.in1d(adata.var.index, g2m_genes_mm)]. ---> 13 sc.tl.score_genes_cell_cycle(adata, s_genes=s_genes_mm_ens, g2m_genes=s_genes_mm_ens). ~\anaconda3\lib\site-packages\scanpy\tools\_score_genes.py in score_genes_cell_cycle(adata, s_genes, g2m_genes, copy, **kwargs). 247 ctrl_size = min(len(s_genes), len(g2m_genes)). 248 # add s-score. --> 249 score_genes(adata, gene_lis",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1599
https://github.com/scverse/scanpy/issues/1599:346,usability,minim,minimal-bug-reports,346,"KeyError in score_genes_cell_cycle; - [ ] I have checked that this issue has not already been reported. - [ ] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. Hello everyone, I am trying to calculate cell cycle score for my mouse data and encountering this error. I have already converted the gene name to upper case letters to read from the cell cycle genes. please help me to resolve this issue. Thank you very much. ### Minimal code sample (that we can copy&paste without having any data). ```python. ## Mouse. folder = ""D:/Pawandeep/Alldatasets/Macosko_cell_cycle_genes.txt"". cc_genes = pd.read_table(folder, delimiter='\t'). #cc_genes = pd.read_table(Macosko_cell_cycle_genes, delimiter='\t'). s_genes = cc_genes['S'].dropna(). g2m_genes = cc_genes['G2.M'].dropna(). s_genes_mm = [gene.lower().upper() for gene in s_genes]. g2m_genes_mm = [gene.lower().upper() for gene in g2m_genes]. s_genes_mm_ens = adata.var.index[np.in1d(adata.var.index, s_genes_mm)]. g2m_genes_mm_ens = adata.var.index[np.in1d(adata.var.index, g2m_genes_mm)]. sc.tl.score_genes_cell_cycle(adata, s_genes=s_genes_mm_ens, g2m_genes=s_genes_mm_ens). ```. ```pytb. KeyError Traceback (most recent call last). <ipython-input-63-57c51b3902c0> in <module>. 11 s_genes_mm_ens = adata.var.index[np.in1d(adata.var.index, s_genes_mm)]. 12 g2m_genes_mm_ens = adata.var.index[np.in1d(adata.var.index, g2m_genes_mm)]. ---> 13 sc.tl.score_genes_cell_cycle(adata, s_genes=s_genes_mm_ens, g2m_genes=s_genes_mm_ens). ~\anaconda3\lib\site-packages\scanpy\tools\_score_genes.py in score_genes_cell_cycle(adata, s_genes, g2m_genes, copy, **kwargs). 247 ctrl_size = min(len(s_genes), len(g2m_genes)). 248 # add s-score. --> 249 score_genes(adata, gene_lis",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1599
https://github.com/scverse/scanpy/issues/1599:513,usability,mous,mouse,513,"KeyError in score_genes_cell_cycle; - [ ] I have checked that this issue has not already been reported. - [ ] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. Hello everyone, I am trying to calculate cell cycle score for my mouse data and encountering this error. I have already converted the gene name to upper case letters to read from the cell cycle genes. please help me to resolve this issue. Thank you very much. ### Minimal code sample (that we can copy&paste without having any data). ```python. ## Mouse. folder = ""D:/Pawandeep/Alldatasets/Macosko_cell_cycle_genes.txt"". cc_genes = pd.read_table(folder, delimiter='\t'). #cc_genes = pd.read_table(Macosko_cell_cycle_genes, delimiter='\t'). s_genes = cc_genes['S'].dropna(). g2m_genes = cc_genes['G2.M'].dropna(). s_genes_mm = [gene.lower().upper() for gene in s_genes]. g2m_genes_mm = [gene.lower().upper() for gene in g2m_genes]. s_genes_mm_ens = adata.var.index[np.in1d(adata.var.index, s_genes_mm)]. g2m_genes_mm_ens = adata.var.index[np.in1d(adata.var.index, g2m_genes_mm)]. sc.tl.score_genes_cell_cycle(adata, s_genes=s_genes_mm_ens, g2m_genes=s_genes_mm_ens). ```. ```pytb. KeyError Traceback (most recent call last). <ipython-input-63-57c51b3902c0> in <module>. 11 s_genes_mm_ens = adata.var.index[np.in1d(adata.var.index, s_genes_mm)]. 12 g2m_genes_mm_ens = adata.var.index[np.in1d(adata.var.index, g2m_genes_mm)]. ---> 13 sc.tl.score_genes_cell_cycle(adata, s_genes=s_genes_mm_ens, g2m_genes=s_genes_mm_ens). ~\anaconda3\lib\site-packages\scanpy\tools\_score_genes.py in score_genes_cell_cycle(adata, s_genes, g2m_genes, copy, **kwargs). 247 ctrl_size = min(len(s_genes), len(g2m_genes)). 248 # add s-score. --> 249 score_genes(adata, gene_lis",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1599
https://github.com/scverse/scanpy/issues/1599:546,usability,error,error,546,"KeyError in score_genes_cell_cycle; - [ ] I have checked that this issue has not already been reported. - [ ] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. Hello everyone, I am trying to calculate cell cycle score for my mouse data and encountering this error. I have already converted the gene name to upper case letters to read from the cell cycle genes. please help me to resolve this issue. Thank you very much. ### Minimal code sample (that we can copy&paste without having any data). ```python. ## Mouse. folder = ""D:/Pawandeep/Alldatasets/Macosko_cell_cycle_genes.txt"". cc_genes = pd.read_table(folder, delimiter='\t'). #cc_genes = pd.read_table(Macosko_cell_cycle_genes, delimiter='\t'). s_genes = cc_genes['S'].dropna(). g2m_genes = cc_genes['G2.M'].dropna(). s_genes_mm = [gene.lower().upper() for gene in s_genes]. g2m_genes_mm = [gene.lower().upper() for gene in g2m_genes]. s_genes_mm_ens = adata.var.index[np.in1d(adata.var.index, s_genes_mm)]. g2m_genes_mm_ens = adata.var.index[np.in1d(adata.var.index, g2m_genes_mm)]. sc.tl.score_genes_cell_cycle(adata, s_genes=s_genes_mm_ens, g2m_genes=s_genes_mm_ens). ```. ```pytb. KeyError Traceback (most recent call last). <ipython-input-63-57c51b3902c0> in <module>. 11 s_genes_mm_ens = adata.var.index[np.in1d(adata.var.index, s_genes_mm)]. 12 g2m_genes_mm_ens = adata.var.index[np.in1d(adata.var.index, g2m_genes_mm)]. ---> 13 sc.tl.score_genes_cell_cycle(adata, s_genes=s_genes_mm_ens, g2m_genes=s_genes_mm_ens). ~\anaconda3\lib\site-packages\scanpy\tools\_score_genes.py in score_genes_cell_cycle(adata, s_genes, g2m_genes, copy, **kwargs). 247 ctrl_size = min(len(s_genes), len(g2m_genes)). 248 # add s-score. --> 249 score_genes(adata, gene_lis",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1599
https://github.com/scverse/scanpy/issues/1599:656,usability,help,help,656,"KeyError in score_genes_cell_cycle; - [ ] I have checked that this issue has not already been reported. - [ ] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. Hello everyone, I am trying to calculate cell cycle score for my mouse data and encountering this error. I have already converted the gene name to upper case letters to read from the cell cycle genes. please help me to resolve this issue. Thank you very much. ### Minimal code sample (that we can copy&paste without having any data). ```python. ## Mouse. folder = ""D:/Pawandeep/Alldatasets/Macosko_cell_cycle_genes.txt"". cc_genes = pd.read_table(folder, delimiter='\t'). #cc_genes = pd.read_table(Macosko_cell_cycle_genes, delimiter='\t'). s_genes = cc_genes['S'].dropna(). g2m_genes = cc_genes['G2.M'].dropna(). s_genes_mm = [gene.lower().upper() for gene in s_genes]. g2m_genes_mm = [gene.lower().upper() for gene in g2m_genes]. s_genes_mm_ens = adata.var.index[np.in1d(adata.var.index, s_genes_mm)]. g2m_genes_mm_ens = adata.var.index[np.in1d(adata.var.index, g2m_genes_mm)]. sc.tl.score_genes_cell_cycle(adata, s_genes=s_genes_mm_ens, g2m_genes=s_genes_mm_ens). ```. ```pytb. KeyError Traceback (most recent call last). <ipython-input-63-57c51b3902c0> in <module>. 11 s_genes_mm_ens = adata.var.index[np.in1d(adata.var.index, s_genes_mm)]. 12 g2m_genes_mm_ens = adata.var.index[np.in1d(adata.var.index, g2m_genes_mm)]. ---> 13 sc.tl.score_genes_cell_cycle(adata, s_genes=s_genes_mm_ens, g2m_genes=s_genes_mm_ens). ~\anaconda3\lib\site-packages\scanpy\tools\_score_genes.py in score_genes_cell_cycle(adata, s_genes, g2m_genes, copy, **kwargs). 247 ctrl_size = min(len(s_genes), len(g2m_genes)). 248 # add s-score. --> 249 score_genes(adata, gene_lis",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1599
https://github.com/scverse/scanpy/issues/1599:712,usability,Minim,Minimal,712,"KeyError in score_genes_cell_cycle; - [ ] I have checked that this issue has not already been reported. - [ ] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. Hello everyone, I am trying to calculate cell cycle score for my mouse data and encountering this error. I have already converted the gene name to upper case letters to read from the cell cycle genes. please help me to resolve this issue. Thank you very much. ### Minimal code sample (that we can copy&paste without having any data). ```python. ## Mouse. folder = ""D:/Pawandeep/Alldatasets/Macosko_cell_cycle_genes.txt"". cc_genes = pd.read_table(folder, delimiter='\t'). #cc_genes = pd.read_table(Macosko_cell_cycle_genes, delimiter='\t'). s_genes = cc_genes['S'].dropna(). g2m_genes = cc_genes['G2.M'].dropna(). s_genes_mm = [gene.lower().upper() for gene in s_genes]. g2m_genes_mm = [gene.lower().upper() for gene in g2m_genes]. s_genes_mm_ens = adata.var.index[np.in1d(adata.var.index, s_genes_mm)]. g2m_genes_mm_ens = adata.var.index[np.in1d(adata.var.index, g2m_genes_mm)]. sc.tl.score_genes_cell_cycle(adata, s_genes=s_genes_mm_ens, g2m_genes=s_genes_mm_ens). ```. ```pytb. KeyError Traceback (most recent call last). <ipython-input-63-57c51b3902c0> in <module>. 11 s_genes_mm_ens = adata.var.index[np.in1d(adata.var.index, s_genes_mm)]. 12 g2m_genes_mm_ens = adata.var.index[np.in1d(adata.var.index, g2m_genes_mm)]. ---> 13 sc.tl.score_genes_cell_cycle(adata, s_genes=s_genes_mm_ens, g2m_genes=s_genes_mm_ens). ~\anaconda3\lib\site-packages\scanpy\tools\_score_genes.py in score_genes_cell_cycle(adata, s_genes, g2m_genes, copy, **kwargs). 247 ctrl_size = min(len(s_genes), len(g2m_genes)). 248 # add s-score. --> 249 score_genes(adata, gene_lis",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1599
https://github.com/scverse/scanpy/issues/1599:796,usability,Mous,Mouse,796,"KeyError in score_genes_cell_cycle; - [ ] I have checked that this issue has not already been reported. - [ ] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. Hello everyone, I am trying to calculate cell cycle score for my mouse data and encountering this error. I have already converted the gene name to upper case letters to read from the cell cycle genes. please help me to resolve this issue. Thank you very much. ### Minimal code sample (that we can copy&paste without having any data). ```python. ## Mouse. folder = ""D:/Pawandeep/Alldatasets/Macosko_cell_cycle_genes.txt"". cc_genes = pd.read_table(folder, delimiter='\t'). #cc_genes = pd.read_table(Macosko_cell_cycle_genes, delimiter='\t'). s_genes = cc_genes['S'].dropna(). g2m_genes = cc_genes['G2.M'].dropna(). s_genes_mm = [gene.lower().upper() for gene in s_genes]. g2m_genes_mm = [gene.lower().upper() for gene in g2m_genes]. s_genes_mm_ens = adata.var.index[np.in1d(adata.var.index, s_genes_mm)]. g2m_genes_mm_ens = adata.var.index[np.in1d(adata.var.index, g2m_genes_mm)]. sc.tl.score_genes_cell_cycle(adata, s_genes=s_genes_mm_ens, g2m_genes=s_genes_mm_ens). ```. ```pytb. KeyError Traceback (most recent call last). <ipython-input-63-57c51b3902c0> in <module>. 11 s_genes_mm_ens = adata.var.index[np.in1d(adata.var.index, s_genes_mm)]. 12 g2m_genes_mm_ens = adata.var.index[np.in1d(adata.var.index, g2m_genes_mm)]. ---> 13 sc.tl.score_genes_cell_cycle(adata, s_genes=s_genes_mm_ens, g2m_genes=s_genes_mm_ens). ~\anaconda3\lib\site-packages\scanpy\tools\_score_genes.py in score_genes_cell_cycle(adata, s_genes, g2m_genes, copy, **kwargs). 247 ctrl_size = min(len(s_genes), len(g2m_genes)). 248 # add s-score. --> 249 score_genes(adata, gene_lis",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1599
https://github.com/scverse/scanpy/issues/1599:1481,usability,input,input-,1481,"late cell cycle score for my mouse data and encountering this error. I have already converted the gene name to upper case letters to read from the cell cycle genes. please help me to resolve this issue. Thank you very much. ### Minimal code sample (that we can copy&paste without having any data). ```python. ## Mouse. folder = ""D:/Pawandeep/Alldatasets/Macosko_cell_cycle_genes.txt"". cc_genes = pd.read_table(folder, delimiter='\t'). #cc_genes = pd.read_table(Macosko_cell_cycle_genes, delimiter='\t'). s_genes = cc_genes['S'].dropna(). g2m_genes = cc_genes['G2.M'].dropna(). s_genes_mm = [gene.lower().upper() for gene in s_genes]. g2m_genes_mm = [gene.lower().upper() for gene in g2m_genes]. s_genes_mm_ens = adata.var.index[np.in1d(adata.var.index, s_genes_mm)]. g2m_genes_mm_ens = adata.var.index[np.in1d(adata.var.index, g2m_genes_mm)]. sc.tl.score_genes_cell_cycle(adata, s_genes=s_genes_mm_ens, g2m_genes=s_genes_mm_ens). ```. ```pytb. KeyError Traceback (most recent call last). <ipython-input-63-57c51b3902c0> in <module>. 11 s_genes_mm_ens = adata.var.index[np.in1d(adata.var.index, s_genes_mm)]. 12 g2m_genes_mm_ens = adata.var.index[np.in1d(adata.var.index, g2m_genes_mm)]. ---> 13 sc.tl.score_genes_cell_cycle(adata, s_genes=s_genes_mm_ens, g2m_genes=s_genes_mm_ens). ~\anaconda3\lib\site-packages\scanpy\tools\_score_genes.py in score_genes_cell_cycle(adata, s_genes, g2m_genes, copy, **kwargs). 247 ctrl_size = min(len(s_genes), len(g2m_genes)). 248 # add s-score. --> 249 score_genes(adata, gene_list=s_genes, score_name='S_score', ctrl_size=ctrl_size, **kwargs). 250 # add g2m-score. 251 score_genes(adata, gene_list=g2m_genes, score_name='G2M_score', ctrl_size=ctrl_size, **kwargs). ~\anaconda3\lib\site-packages\scanpy\tools\_score_genes.py in score_genes(adata, gene_list, ctrl_size, gene_pool, n_bins, score_name, random_state, copy, use_raw). 153 gene_list = list(gene_list). 154 . --> 155 X_list = _adata[:, gene_list].X. 156 if issparse(X_list):. 157 X_list = np.array(_spars",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1599
https://github.com/scverse/scanpy/issues/1599:1803,usability,tool,tools,1803,"der = ""D:/Pawandeep/Alldatasets/Macosko_cell_cycle_genes.txt"". cc_genes = pd.read_table(folder, delimiter='\t'). #cc_genes = pd.read_table(Macosko_cell_cycle_genes, delimiter='\t'). s_genes = cc_genes['S'].dropna(). g2m_genes = cc_genes['G2.M'].dropna(). s_genes_mm = [gene.lower().upper() for gene in s_genes]. g2m_genes_mm = [gene.lower().upper() for gene in g2m_genes]. s_genes_mm_ens = adata.var.index[np.in1d(adata.var.index, s_genes_mm)]. g2m_genes_mm_ens = adata.var.index[np.in1d(adata.var.index, g2m_genes_mm)]. sc.tl.score_genes_cell_cycle(adata, s_genes=s_genes_mm_ens, g2m_genes=s_genes_mm_ens). ```. ```pytb. KeyError Traceback (most recent call last). <ipython-input-63-57c51b3902c0> in <module>. 11 s_genes_mm_ens = adata.var.index[np.in1d(adata.var.index, s_genes_mm)]. 12 g2m_genes_mm_ens = adata.var.index[np.in1d(adata.var.index, g2m_genes_mm)]. ---> 13 sc.tl.score_genes_cell_cycle(adata, s_genes=s_genes_mm_ens, g2m_genes=s_genes_mm_ens). ~\anaconda3\lib\site-packages\scanpy\tools\_score_genes.py in score_genes_cell_cycle(adata, s_genes, g2m_genes, copy, **kwargs). 247 ctrl_size = min(len(s_genes), len(g2m_genes)). 248 # add s-score. --> 249 score_genes(adata, gene_list=s_genes, score_name='S_score', ctrl_size=ctrl_size, **kwargs). 250 # add g2m-score. 251 score_genes(adata, gene_list=g2m_genes, score_name='G2M_score', ctrl_size=ctrl_size, **kwargs). ~\anaconda3\lib\site-packages\scanpy\tools\_score_genes.py in score_genes(adata, gene_list, ctrl_size, gene_pool, n_bins, score_name, random_state, copy, use_raw). 153 gene_list = list(gene_list). 154 . --> 155 X_list = _adata[:, gene_list].X. 156 if issparse(X_list):. 157 X_list = np.array(_sparse_nanmean(X_list, axis=1)).flatten(). ~\anaconda3\lib\site-packages\anndata\_core\raw.py in __getitem__(self, index). 99 . 100 def __getitem__(self, index):. --> 101 oidx, vidx = self._normalize_indices(index). 102 . 103 # To preserve two dimensional shape. ~\anaconda3\lib\site-packages\anndata\_core\raw.py in _normalize",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1599
https://github.com/scverse/scanpy/issues/1599:2223,usability,tool,tools,2223,"var.index, s_genes_mm)]. g2m_genes_mm_ens = adata.var.index[np.in1d(adata.var.index, g2m_genes_mm)]. sc.tl.score_genes_cell_cycle(adata, s_genes=s_genes_mm_ens, g2m_genes=s_genes_mm_ens). ```. ```pytb. KeyError Traceback (most recent call last). <ipython-input-63-57c51b3902c0> in <module>. 11 s_genes_mm_ens = adata.var.index[np.in1d(adata.var.index, s_genes_mm)]. 12 g2m_genes_mm_ens = adata.var.index[np.in1d(adata.var.index, g2m_genes_mm)]. ---> 13 sc.tl.score_genes_cell_cycle(adata, s_genes=s_genes_mm_ens, g2m_genes=s_genes_mm_ens). ~\anaconda3\lib\site-packages\scanpy\tools\_score_genes.py in score_genes_cell_cycle(adata, s_genes, g2m_genes, copy, **kwargs). 247 ctrl_size = min(len(s_genes), len(g2m_genes)). 248 # add s-score. --> 249 score_genes(adata, gene_list=s_genes, score_name='S_score', ctrl_size=ctrl_size, **kwargs). 250 # add g2m-score. 251 score_genes(adata, gene_list=g2m_genes, score_name='G2M_score', ctrl_size=ctrl_size, **kwargs). ~\anaconda3\lib\site-packages\scanpy\tools\_score_genes.py in score_genes(adata, gene_list, ctrl_size, gene_pool, n_bins, score_name, random_state, copy, use_raw). 153 gene_list = list(gene_list). 154 . --> 155 X_list = _adata[:, gene_list].X. 156 if issparse(X_list):. 157 X_list = np.array(_sparse_nanmean(X_list, axis=1)).flatten(). ~\anaconda3\lib\site-packages\anndata\_core\raw.py in __getitem__(self, index). 99 . 100 def __getitem__(self, index):. --> 101 oidx, vidx = self._normalize_indices(index). 102 . 103 # To preserve two dimensional shape. ~\anaconda3\lib\site-packages\anndata\_core\raw.py in _normalize_indices(self, packed_index). 159 obs, var = unpack_index(packed_index). 160 obs = _normalize_index(obs, self._adata.obs_names). --> 161 var = _normalize_index(var, self.var_names). 162 return obs, var. 163 . ~\anaconda3\lib\site-packages\anndata\_core\index.py in _normalize_index(indexer, index). 98 if np.any(positions < 0):. 99 not_found = indexer[positions < 0]. --> 100 raise KeyError(. 101 f""Values {list(not_foun",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1599
https://github.com/scverse/scanpy/pull/1601:25,integrability,sub,subj,25,fix ingest for umap 0.5; subj. https://github.com/theislab/scanpy/issues/1509. `rp_forest` storage is still broken though.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1601
https://github.com/scverse/scanpy/pull/1602:93,deployability,build,build,93,"add twine check to azure CI + 3.8 matrix; Dear everyone,. This PR adds. 1. Python 3.8 to the build matrix. We were currently only testing vs 3.6 and 3.7. 3.10 is already coming up in April, so it is in my opinion time to use the more latest Python versions. If you feel like this adds too much to the Azure/CI bill, then I would suggest to remove 3.6. . 2. Solves #1585 . Signed-off-by: Zethson <lukas.heumos@posteo.net>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1602
https://github.com/scverse/scanpy/pull/1602:248,deployability,version,versions,248,"add twine check to azure CI + 3.8 matrix; Dear everyone,. This PR adds. 1. Python 3.8 to the build matrix. We were currently only testing vs 3.6 and 3.7. 3.10 is already coming up in April, so it is in my opinion time to use the more latest Python versions. If you feel like this adds too much to the Azure/CI bill, then I would suggest to remove 3.6. . 2. Solves #1585 . Signed-off-by: Zethson <lukas.heumos@posteo.net>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1602
https://github.com/scverse/scanpy/pull/1602:115,energy efficiency,current,currently,115,"add twine check to azure CI + 3.8 matrix; Dear everyone,. This PR adds. 1. Python 3.8 to the build matrix. We were currently only testing vs 3.6 and 3.7. 3.10 is already coming up in April, so it is in my opinion time to use the more latest Python versions. If you feel like this adds too much to the Azure/CI bill, then I would suggest to remove 3.6. . 2. Solves #1585 . Signed-off-by: Zethson <lukas.heumos@posteo.net>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1602
https://github.com/scverse/scanpy/pull/1602:248,integrability,version,versions,248,"add twine check to azure CI + 3.8 matrix; Dear everyone,. This PR adds. 1. Python 3.8 to the build matrix. We were currently only testing vs 3.6 and 3.7. 3.10 is already coming up in April, so it is in my opinion time to use the more latest Python versions. If you feel like this adds too much to the Azure/CI bill, then I would suggest to remove 3.6. . 2. Solves #1585 . Signed-off-by: Zethson <lukas.heumos@posteo.net>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1602
https://github.com/scverse/scanpy/pull/1602:248,modifiability,version,versions,248,"add twine check to azure CI + 3.8 matrix; Dear everyone,. This PR adds. 1. Python 3.8 to the build matrix. We were currently only testing vs 3.6 and 3.7. 3.10 is already coming up in April, so it is in my opinion time to use the more latest Python versions. If you feel like this adds too much to the Azure/CI bill, then I would suggest to remove 3.6. . 2. Solves #1585 . Signed-off-by: Zethson <lukas.heumos@posteo.net>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1602
https://github.com/scverse/scanpy/pull/1602:213,performance,time,time,213,"add twine check to azure CI + 3.8 matrix; Dear everyone,. This PR adds. 1. Python 3.8 to the build matrix. We were currently only testing vs 3.6 and 3.7. 3.10 is already coming up in April, so it is in my opinion time to use the more latest Python versions. If you feel like this adds too much to the Azure/CI bill, then I would suggest to remove 3.6. . 2. Solves #1585 . Signed-off-by: Zethson <lukas.heumos@posteo.net>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1602
https://github.com/scverse/scanpy/pull/1602:130,safety,test,testing,130,"add twine check to azure CI + 3.8 matrix; Dear everyone,. This PR adds. 1. Python 3.8 to the build matrix. We were currently only testing vs 3.6 and 3.7. 3.10 is already coming up in April, so it is in my opinion time to use the more latest Python versions. If you feel like this adds too much to the Azure/CI bill, then I would suggest to remove 3.6. . 2. Solves #1585 . Signed-off-by: Zethson <lukas.heumos@posteo.net>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1602
https://github.com/scverse/scanpy/pull/1602:372,security,Sign,Signed-off-by,372,"add twine check to azure CI + 3.8 matrix; Dear everyone,. This PR adds. 1. Python 3.8 to the build matrix. We were currently only testing vs 3.6 and 3.7. 3.10 is already coming up in April, so it is in my opinion time to use the more latest Python versions. If you feel like this adds too much to the Azure/CI bill, then I would suggest to remove 3.6. . 2. Solves #1585 . Signed-off-by: Zethson <lukas.heumos@posteo.net>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1602
https://github.com/scverse/scanpy/pull/1602:130,testability,test,testing,130,"add twine check to azure CI + 3.8 matrix; Dear everyone,. This PR adds. 1. Python 3.8 to the build matrix. We were currently only testing vs 3.6 and 3.7. 3.10 is already coming up in April, so it is in my opinion time to use the more latest Python versions. If you feel like this adds too much to the Azure/CI bill, then I would suggest to remove 3.6. . 2. Solves #1585 . Signed-off-by: Zethson <lukas.heumos@posteo.net>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1602
https://github.com/scverse/scanpy/pull/1603:85,availability,error,error,85,Address issues with heatmaps reported in #1591; This PR resolves the issues with:. * error when the type of a category is not `str`. * inconsistent color assigned to categories. * white space . * horizontal lines not well aligned. Missing. - [x] Tests,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1603
https://github.com/scverse/scanpy/pull/1603:20,energy efficiency,heat,heatmaps,20,Address issues with heatmaps reported in #1591; This PR resolves the issues with:. * error when the type of a category is not `str`. * inconsistent color assigned to categories. * white space . * horizontal lines not well aligned. Missing. - [x] Tests,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1603
https://github.com/scverse/scanpy/pull/1603:85,performance,error,error,85,Address issues with heatmaps reported in #1591; This PR resolves the issues with:. * error when the type of a category is not `str`. * inconsistent color assigned to categories. * white space . * horizontal lines not well aligned. Missing. - [x] Tests,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1603
https://github.com/scverse/scanpy/pull/1603:85,safety,error,error,85,Address issues with heatmaps reported in #1591; This PR resolves the issues with:. * error when the type of a category is not `str`. * inconsistent color assigned to categories. * white space . * horizontal lines not well aligned. Missing. - [x] Tests,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1603
https://github.com/scverse/scanpy/pull/1603:246,safety,Test,Tests,246,Address issues with heatmaps reported in #1591; This PR resolves the issues with:. * error when the type of a category is not `str`. * inconsistent color assigned to categories. * white space . * horizontal lines not well aligned. Missing. - [x] Tests,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1603
https://github.com/scverse/scanpy/pull/1603:246,testability,Test,Tests,246,Address issues with heatmaps reported in #1591; This PR resolves the issues with:. * error when the type of a category is not `str`. * inconsistent color assigned to categories. * white space . * horizontal lines not well aligned. Missing. - [x] Tests,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1603
https://github.com/scverse/scanpy/pull/1603:85,usability,error,error,85,Address issues with heatmaps reported in #1591; This PR resolves the issues with:. * error when the type of a category is not `str`. * inconsistent color assigned to categories. * white space . * horizontal lines not well aligned. Missing. - [x] Tests,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1603
https://github.com/scverse/scanpy/issues/1604:469,reliability,Doe,Does,469,"Show example plots as part of documentation; In seaborn, the docstring example plots are shown as part of the documentation. Would be great to have this as part of scanpy as this will motivate the creation of more examples as part of the docstring while keeping them up-to-date. . See for example: https://seaborn.pydata.org/generated/seaborn.catplot.html. ![image](https://user-images.githubusercontent.com/4964309/105457501-3a192980-5c87-11eb-8a17-050b0dc5b67b.png). Does anyone has experience on how to achieve this?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1604
https://github.com/scverse/scanpy/issues/1604:30,usability,document,documentation,30,"Show example plots as part of documentation; In seaborn, the docstring example plots are shown as part of the documentation. Would be great to have this as part of scanpy as this will motivate the creation of more examples as part of the docstring while keeping them up-to-date. . See for example: https://seaborn.pydata.org/generated/seaborn.catplot.html. ![image](https://user-images.githubusercontent.com/4964309/105457501-3a192980-5c87-11eb-8a17-050b0dc5b67b.png). Does anyone has experience on how to achieve this?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1604
https://github.com/scverse/scanpy/issues/1604:110,usability,document,documentation,110,"Show example plots as part of documentation; In seaborn, the docstring example plots are shown as part of the documentation. Would be great to have this as part of scanpy as this will motivate the creation of more examples as part of the docstring while keeping them up-to-date. . See for example: https://seaborn.pydata.org/generated/seaborn.catplot.html. ![image](https://user-images.githubusercontent.com/4964309/105457501-3a192980-5c87-11eb-8a17-050b0dc5b67b.png). Does anyone has experience on how to achieve this?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1604
https://github.com/scverse/scanpy/issues/1604:374,usability,user,user-images,374,"Show example plots as part of documentation; In seaborn, the docstring example plots are shown as part of the documentation. Would be great to have this as part of scanpy as this will motivate the creation of more examples as part of the docstring while keeping them up-to-date. . See for example: https://seaborn.pydata.org/generated/seaborn.catplot.html. ![image](https://user-images.githubusercontent.com/4964309/105457501-3a192980-5c87-11eb-8a17-050b0dc5b67b.png). Does anyone has experience on how to achieve this?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1604
https://github.com/scverse/scanpy/issues/1604:485,usability,experien,experience,485,"Show example plots as part of documentation; In seaborn, the docstring example plots are shown as part of the documentation. Would be great to have this as part of scanpy as this will motivate the creation of more examples as part of the docstring while keeping them up-to-date. . See for example: https://seaborn.pydata.org/generated/seaborn.catplot.html. ![image](https://user-images.githubusercontent.com/4964309/105457501-3a192980-5c87-11eb-8a17-050b0dc5b67b.png). Does anyone has experience on how to achieve this?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1604
https://github.com/scverse/scanpy/issues/1605:174,deployability,version,version,174,"PC2 has more variance than PC1 from scanpy.pp.pca; - [x ] I have checked that this issue has not already been reported. - [x ] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. Hey, I noticed that after scanpy.pp.pca, I got a result with PC2 variance ratio larger than PC1. What could possibly be the reason for this? ![image (10)](https://user-images.githubusercontent.com/16257776/105540366-1922f980-5cc4-11eb-8d58-d0ffde9cc456.png).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1605
https://github.com/scverse/scanpy/issues/1605:174,integrability,version,version,174,"PC2 has more variance than PC1 from scanpy.pp.pca; - [x ] I have checked that this issue has not already been reported. - [x ] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. Hey, I noticed that after scanpy.pp.pca, I got a result with PC2 variance ratio larger than PC1. What could possibly be the reason for this? ![image (10)](https://user-images.githubusercontent.com/16257776/105540366-1922f980-5cc4-11eb-8d58-d0ffde9cc456.png).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1605
https://github.com/scverse/scanpy/issues/1605:174,modifiability,version,version,174,"PC2 has more variance than PC1 from scanpy.pp.pca; - [x ] I have checked that this issue has not already been reported. - [x ] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. Hey, I noticed that after scanpy.pp.pca, I got a result with PC2 variance ratio larger than PC1. What could possibly be the reason for this? ![image (10)](https://user-images.githubusercontent.com/16257776/105540366-1922f980-5cc4-11eb-8d58-d0ffde9cc456.png).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1605
https://github.com/scverse/scanpy/issues/1605:134,usability,confirm,confirmed,134,"PC2 has more variance than PC1 from scanpy.pp.pca; - [x ] I have checked that this issue has not already been reported. - [x ] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. Hey, I noticed that after scanpy.pp.pca, I got a result with PC2 variance ratio larger than PC1. What could possibly be the reason for this? ![image (10)](https://user-images.githubusercontent.com/16257776/105540366-1922f980-5cc4-11eb-8d58-d0ffde9cc456.png).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1605
https://github.com/scverse/scanpy/issues/1605:217,usability,confirm,confirmed,217,"PC2 has more variance than PC1 from scanpy.pp.pca; - [x ] I have checked that this issue has not already been reported. - [x ] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. Hey, I noticed that after scanpy.pp.pca, I got a result with PC2 variance ratio larger than PC1. What could possibly be the reason for this? ![image (10)](https://user-images.githubusercontent.com/16257776/105540366-1922f980-5cc4-11eb-8d58-d0ffde9cc456.png).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1605
https://github.com/scverse/scanpy/issues/1605:443,usability,user,user-images,443,"PC2 has more variance than PC1 from scanpy.pp.pca; - [x ] I have checked that this issue has not already been reported. - [x ] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. Hey, I noticed that after scanpy.pp.pca, I got a result with PC2 variance ratio larger than PC1. What could possibly be the reason for this? ![image (10)](https://user-images.githubusercontent.com/16257776/105540366-1922f980-5cc4-11eb-8d58-d0ffde9cc456.png).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1605
https://github.com/scverse/scanpy/issues/1606:37,deployability,fail,fails,37,"Combat works for some covariates yet fails for others; - [ x] I have checked that this issue has not already been reported. - [ x] I have confirmed this bug exists on the latest version of scanpy. - [ x] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. ### Minimal code sample (that we can copy&paste without having any data). ```python. adata.obs['sex'].cat.categories.tolist(). ```. ```pytb. ['F', 'M', 'U']. ```. ```python. adata.obs['age_groups'].cat.categories.tolist(). ```. ```pytb. ['Old', 'YoungAdult', 'Pediatric', 'Fetal', 'NewBorn']. ```. ```python. sc.pp.combat(adata, key='384plate', covariates=['sex']). ```. ```pytb. Standardizing Data across genes. Found 34 batches. Found 1 categorical variables:. 	sex. Found 0 numerical variables:. 	. Fitting L/S model and finding priors. Finding parametric adjustments. /home/sinhar/.local/lib/python3.8/site-packages/scanpy/preprocessing/_combat.py:340: RuntimeWarning: divide by zero encountered in true_divide. (abs(g_new - g_old) / g_old).max(), (abs(d_new - d_old) / d_old).max(). Adjusting data. ```. ```python. sc.pp.combat(adata, key='384plate', covariates=['age_group']). ```. ```pytb. Standardizing Data across genes. Found 34 batches. Found 1 categorical variables:. 	age_group. Found 0 numerical variables:. 	. ---------------------------------------------------------------------------. LinAlgError Traceback (most recent call last). <timed eval> in <module>. ~/.local/lib/python3.8/site-packages/scanpy/preprocessing/_combat.py in combat(adata, key, covariates, inplace). 204 # standardize across genes using a pooled variance estimator. 205 logg.info(""Standardizing Data across genes.\n""). --> 206 s_data, design, var_pooled, stand_mean = _standardize_data(model, data, key). 207 . 208 # fitting the parameters on",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1606
https://github.com/scverse/scanpy/issues/1606:178,deployability,version,version,178,"Combat works for some covariates yet fails for others; - [ x] I have checked that this issue has not already been reported. - [ x] I have confirmed this bug exists on the latest version of scanpy. - [ x] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. ### Minimal code sample (that we can copy&paste without having any data). ```python. adata.obs['sex'].cat.categories.tolist(). ```. ```pytb. ['F', 'M', 'U']. ```. ```python. adata.obs['age_groups'].cat.categories.tolist(). ```. ```pytb. ['Old', 'YoungAdult', 'Pediatric', 'Fetal', 'NewBorn']. ```. ```python. sc.pp.combat(adata, key='384plate', covariates=['sex']). ```. ```pytb. Standardizing Data across genes. Found 34 batches. Found 1 categorical variables:. 	sex. Found 0 numerical variables:. 	. Fitting L/S model and finding priors. Finding parametric adjustments. /home/sinhar/.local/lib/python3.8/site-packages/scanpy/preprocessing/_combat.py:340: RuntimeWarning: divide by zero encountered in true_divide. (abs(g_new - g_old) / g_old).max(), (abs(d_new - d_old) / d_old).max(). Adjusting data. ```. ```python. sc.pp.combat(adata, key='384plate', covariates=['age_group']). ```. ```pytb. Standardizing Data across genes. Found 34 batches. Found 1 categorical variables:. 	age_group. Found 0 numerical variables:. 	. ---------------------------------------------------------------------------. LinAlgError Traceback (most recent call last). <timed eval> in <module>. ~/.local/lib/python3.8/site-packages/scanpy/preprocessing/_combat.py in combat(adata, key, covariates, inplace). 204 # standardize across genes using a pooled variance estimator. 205 logg.info(""Standardizing Data across genes.\n""). --> 206 s_data, design, var_pooled, stand_mean = _standardize_data(model, data, key). 207 . 208 # fitting the parameters on",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1606
https://github.com/scverse/scanpy/issues/1606:1636,deployability,modul,module,1636,"hon. adata.obs['age_groups'].cat.categories.tolist(). ```. ```pytb. ['Old', 'YoungAdult', 'Pediatric', 'Fetal', 'NewBorn']. ```. ```python. sc.pp.combat(adata, key='384plate', covariates=['sex']). ```. ```pytb. Standardizing Data across genes. Found 34 batches. Found 1 categorical variables:. 	sex. Found 0 numerical variables:. 	. Fitting L/S model and finding priors. Finding parametric adjustments. /home/sinhar/.local/lib/python3.8/site-packages/scanpy/preprocessing/_combat.py:340: RuntimeWarning: divide by zero encountered in true_divide. (abs(g_new - g_old) / g_old).max(), (abs(d_new - d_old) / d_old).max(). Adjusting data. ```. ```python. sc.pp.combat(adata, key='384plate', covariates=['age_group']). ```. ```pytb. Standardizing Data across genes. Found 34 batches. Found 1 categorical variables:. 	age_group. Found 0 numerical variables:. 	. ---------------------------------------------------------------------------. LinAlgError Traceback (most recent call last). <timed eval> in <module>. ~/.local/lib/python3.8/site-packages/scanpy/preprocessing/_combat.py in combat(adata, key, covariates, inplace). 204 # standardize across genes using a pooled variance estimator. 205 logg.info(""Standardizing Data across genes.\n""). --> 206 s_data, design, var_pooled, stand_mean = _standardize_data(model, data, key). 207 . 208 # fitting the parameters on the standardized data. ~/.local/lib/python3.8/site-packages/scanpy/preprocessing/_combat.py in _standardize_data(model, data, batch_key). 102 . 103 # compute pooled variance estimator. --> 104 B_hat = np.dot(np.dot(la.inv(np.dot(design.T, design)), design.T), data.T). 105 grand_mean = np.dot((n_batches / n_array).T, B_hat[:n_batch, :]). 106 var_pooled = (data - np.dot(design, B_hat).T) ** 2. <__array_function__ internals> in inv(*args, **kwargs). ~/.local/lib/python3.8/site-packages/numpy/linalg/linalg.py in inv(a). 544 signature = 'D->D' if isComplexType(t) else 'd->d'. 545 extobj = get_linalg_error_extobj(_raise_linalgerror_sing",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1606
https://github.com/scverse/scanpy/issues/1606:1828,deployability,log,logg,1828,"x']). ```. ```pytb. Standardizing Data across genes. Found 34 batches. Found 1 categorical variables:. 	sex. Found 0 numerical variables:. 	. Fitting L/S model and finding priors. Finding parametric adjustments. /home/sinhar/.local/lib/python3.8/site-packages/scanpy/preprocessing/_combat.py:340: RuntimeWarning: divide by zero encountered in true_divide. (abs(g_new - g_old) / g_old).max(), (abs(d_new - d_old) / d_old).max(). Adjusting data. ```. ```python. sc.pp.combat(adata, key='384plate', covariates=['age_group']). ```. ```pytb. Standardizing Data across genes. Found 34 batches. Found 1 categorical variables:. 	age_group. Found 0 numerical variables:. 	. ---------------------------------------------------------------------------. LinAlgError Traceback (most recent call last). <timed eval> in <module>. ~/.local/lib/python3.8/site-packages/scanpy/preprocessing/_combat.py in combat(adata, key, covariates, inplace). 204 # standardize across genes using a pooled variance estimator. 205 logg.info(""Standardizing Data across genes.\n""). --> 206 s_data, design, var_pooled, stand_mean = _standardize_data(model, data, key). 207 . 208 # fitting the parameters on the standardized data. ~/.local/lib/python3.8/site-packages/scanpy/preprocessing/_combat.py in _standardize_data(model, data, batch_key). 102 . 103 # compute pooled variance estimator. --> 104 B_hat = np.dot(np.dot(la.inv(np.dot(design.T, design)), design.T), data.T). 105 grand_mean = np.dot((n_batches / n_array).T, B_hat[:n_batch, :]). 106 var_pooled = (data - np.dot(design, B_hat).T) ** 2. <__array_function__ internals> in inv(*args, **kwargs). ~/.local/lib/python3.8/site-packages/numpy/linalg/linalg.py in inv(a). 544 signature = 'D->D' if isComplexType(t) else 'd->d'. 545 extobj = get_linalg_error_extobj(_raise_linalgerror_singular). --> 546 ainv = _umath_linalg.inv(a, signature=signature, extobj=extobj). 547 return wrap(ainv.astype(result_t, copy=False)). 548 . ~/.local/lib/python3.8/site-packages/numpy/linalg/lin",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1606
https://github.com/scverse/scanpy/issues/1606:3073,deployability,Version,Versions,3073,". Found 34 batches. Found 1 categorical variables:. 	age_group. Found 0 numerical variables:. 	. ---------------------------------------------------------------------------. LinAlgError Traceback (most recent call last). <timed eval> in <module>. ~/.local/lib/python3.8/site-packages/scanpy/preprocessing/_combat.py in combat(adata, key, covariates, inplace). 204 # standardize across genes using a pooled variance estimator. 205 logg.info(""Standardizing Data across genes.\n""). --> 206 s_data, design, var_pooled, stand_mean = _standardize_data(model, data, key). 207 . 208 # fitting the parameters on the standardized data. ~/.local/lib/python3.8/site-packages/scanpy/preprocessing/_combat.py in _standardize_data(model, data, batch_key). 102 . 103 # compute pooled variance estimator. --> 104 B_hat = np.dot(np.dot(la.inv(np.dot(design.T, design)), design.T), data.T). 105 grand_mean = np.dot((n_batches / n_array).T, B_hat[:n_batch, :]). 106 var_pooled = (data - np.dot(design, B_hat).T) ** 2. <__array_function__ internals> in inv(*args, **kwargs). ~/.local/lib/python3.8/site-packages/numpy/linalg/linalg.py in inv(a). 544 signature = 'D->D' if isComplexType(t) else 'd->d'. 545 extobj = get_linalg_error_extobj(_raise_linalgerror_singular). --> 546 ainv = _umath_linalg.inv(a, signature=signature, extobj=extobj). 547 return wrap(ainv.astype(result_t, copy=False)). 548 . ~/.local/lib/python3.8/site-packages/numpy/linalg/linalg.py in _raise_linalgerror_singular(err, flag). 86 . 87 def _raise_linalgerror_singular(err, flag):. ---> 88 raise LinAlgError(""Singular matrix""). 89 . 90 def _raise_linalgerror_nonposdef(err, flag):. LinAlgError: Singular matrix. ```. #### Versions. <details>. [Paste the output of scanpy.logging.print_versions() leaving a blank line after the details tag]. </details>. scanpy==1.7.0rc2.dev7+g57ec8a7e anndata==0.7.3 umap==0.4.6 numpy==1.19.5 scipy==1.6.0 pandas==1.2.1 scikit-learn==0.24.1 statsmodels==0.12.1 python-igraph==0.8.3 louvain==0.7.0 leidenalg==0.8.3.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1606
https://github.com/scverse/scanpy/issues/1606:3122,deployability,log,logging,3122,". Found 34 batches. Found 1 categorical variables:. 	age_group. Found 0 numerical variables:. 	. ---------------------------------------------------------------------------. LinAlgError Traceback (most recent call last). <timed eval> in <module>. ~/.local/lib/python3.8/site-packages/scanpy/preprocessing/_combat.py in combat(adata, key, covariates, inplace). 204 # standardize across genes using a pooled variance estimator. 205 logg.info(""Standardizing Data across genes.\n""). --> 206 s_data, design, var_pooled, stand_mean = _standardize_data(model, data, key). 207 . 208 # fitting the parameters on the standardized data. ~/.local/lib/python3.8/site-packages/scanpy/preprocessing/_combat.py in _standardize_data(model, data, batch_key). 102 . 103 # compute pooled variance estimator. --> 104 B_hat = np.dot(np.dot(la.inv(np.dot(design.T, design)), design.T), data.T). 105 grand_mean = np.dot((n_batches / n_array).T, B_hat[:n_batch, :]). 106 var_pooled = (data - np.dot(design, B_hat).T) ** 2. <__array_function__ internals> in inv(*args, **kwargs). ~/.local/lib/python3.8/site-packages/numpy/linalg/linalg.py in inv(a). 544 signature = 'D->D' if isComplexType(t) else 'd->d'. 545 extobj = get_linalg_error_extobj(_raise_linalgerror_singular). --> 546 ainv = _umath_linalg.inv(a, signature=signature, extobj=extobj). 547 return wrap(ainv.astype(result_t, copy=False)). 548 . ~/.local/lib/python3.8/site-packages/numpy/linalg/linalg.py in _raise_linalgerror_singular(err, flag). 86 . 87 def _raise_linalgerror_singular(err, flag):. ---> 88 raise LinAlgError(""Singular matrix""). 89 . 90 def _raise_linalgerror_nonposdef(err, flag):. LinAlgError: Singular matrix. ```. #### Versions. <details>. [Paste the output of scanpy.logging.print_versions() leaving a blank line after the details tag]. </details>. scanpy==1.7.0rc2.dev7+g57ec8a7e anndata==0.7.3 umap==0.4.6 numpy==1.19.5 scipy==1.6.0 pandas==1.2.1 scikit-learn==0.24.1 statsmodels==0.12.1 python-igraph==0.8.3 louvain==0.7.0 leidenalg==0.8.3.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1606
https://github.com/scverse/scanpy/issues/1606:984,energy efficiency,model,model,984,"Combat works for some covariates yet fails for others; - [ x] I have checked that this issue has not already been reported. - [ x] I have confirmed this bug exists on the latest version of scanpy. - [ x] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. ### Minimal code sample (that we can copy&paste without having any data). ```python. adata.obs['sex'].cat.categories.tolist(). ```. ```pytb. ['F', 'M', 'U']. ```. ```python. adata.obs['age_groups'].cat.categories.tolist(). ```. ```pytb. ['Old', 'YoungAdult', 'Pediatric', 'Fetal', 'NewBorn']. ```. ```python. sc.pp.combat(adata, key='384plate', covariates=['sex']). ```. ```pytb. Standardizing Data across genes. Found 34 batches. Found 1 categorical variables:. 	sex. Found 0 numerical variables:. 	. Fitting L/S model and finding priors. Finding parametric adjustments. /home/sinhar/.local/lib/python3.8/site-packages/scanpy/preprocessing/_combat.py:340: RuntimeWarning: divide by zero encountered in true_divide. (abs(g_new - g_old) / g_old).max(), (abs(d_new - d_old) / d_old).max(). Adjusting data. ```. ```python. sc.pp.combat(adata, key='384plate', covariates=['age_group']). ```. ```pytb. Standardizing Data across genes. Found 34 batches. Found 1 categorical variables:. 	age_group. Found 0 numerical variables:. 	. ---------------------------------------------------------------------------. LinAlgError Traceback (most recent call last). <timed eval> in <module>. ~/.local/lib/python3.8/site-packages/scanpy/preprocessing/_combat.py in combat(adata, key, covariates, inplace). 204 # standardize across genes using a pooled variance estimator. 205 logg.info(""Standardizing Data across genes.\n""). --> 206 s_data, design, var_pooled, stand_mean = _standardize_data(model, data, key). 207 . 208 # fitting the parameters on",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1606
https://github.com/scverse/scanpy/issues/1606:1813,energy efficiency,estimat,estimator,1813,"ariates=['sex']). ```. ```pytb. Standardizing Data across genes. Found 34 batches. Found 1 categorical variables:. 	sex. Found 0 numerical variables:. 	. Fitting L/S model and finding priors. Finding parametric adjustments. /home/sinhar/.local/lib/python3.8/site-packages/scanpy/preprocessing/_combat.py:340: RuntimeWarning: divide by zero encountered in true_divide. (abs(g_new - g_old) / g_old).max(), (abs(d_new - d_old) / d_old).max(). Adjusting data. ```. ```python. sc.pp.combat(adata, key='384plate', covariates=['age_group']). ```. ```pytb. Standardizing Data across genes. Found 34 batches. Found 1 categorical variables:. 	age_group. Found 0 numerical variables:. 	. ---------------------------------------------------------------------------. LinAlgError Traceback (most recent call last). <timed eval> in <module>. ~/.local/lib/python3.8/site-packages/scanpy/preprocessing/_combat.py in combat(adata, key, covariates, inplace). 204 # standardize across genes using a pooled variance estimator. 205 logg.info(""Standardizing Data across genes.\n""). --> 206 s_data, design, var_pooled, stand_mean = _standardize_data(model, data, key). 207 . 208 # fitting the parameters on the standardized data. ~/.local/lib/python3.8/site-packages/scanpy/preprocessing/_combat.py in _standardize_data(model, data, batch_key). 102 . 103 # compute pooled variance estimator. --> 104 B_hat = np.dot(np.dot(la.inv(np.dot(design.T, design)), design.T), data.T). 105 grand_mean = np.dot((n_batches / n_array).T, B_hat[:n_batch, :]). 106 var_pooled = (data - np.dot(design, B_hat).T) ** 2. <__array_function__ internals> in inv(*args, **kwargs). ~/.local/lib/python3.8/site-packages/numpy/linalg/linalg.py in inv(a). 544 signature = 'D->D' if isComplexType(t) else 'd->d'. 545 extobj = get_linalg_error_extobj(_raise_linalgerror_singular). --> 546 ainv = _umath_linalg.inv(a, signature=signature, extobj=extobj). 547 return wrap(ainv.astype(result_t, copy=False)). 548 . ~/.local/lib/python3.8/site-packages/nump",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1606
https://github.com/scverse/scanpy/issues/1606:1944,energy efficiency,model,model,1944,"numerical variables:. 	. Fitting L/S model and finding priors. Finding parametric adjustments. /home/sinhar/.local/lib/python3.8/site-packages/scanpy/preprocessing/_combat.py:340: RuntimeWarning: divide by zero encountered in true_divide. (abs(g_new - g_old) / g_old).max(), (abs(d_new - d_old) / d_old).max(). Adjusting data. ```. ```python. sc.pp.combat(adata, key='384plate', covariates=['age_group']). ```. ```pytb. Standardizing Data across genes. Found 34 batches. Found 1 categorical variables:. 	age_group. Found 0 numerical variables:. 	. ---------------------------------------------------------------------------. LinAlgError Traceback (most recent call last). <timed eval> in <module>. ~/.local/lib/python3.8/site-packages/scanpy/preprocessing/_combat.py in combat(adata, key, covariates, inplace). 204 # standardize across genes using a pooled variance estimator. 205 logg.info(""Standardizing Data across genes.\n""). --> 206 s_data, design, var_pooled, stand_mean = _standardize_data(model, data, key). 207 . 208 # fitting the parameters on the standardized data. ~/.local/lib/python3.8/site-packages/scanpy/preprocessing/_combat.py in _standardize_data(model, data, batch_key). 102 . 103 # compute pooled variance estimator. --> 104 B_hat = np.dot(np.dot(la.inv(np.dot(design.T, design)), design.T), data.T). 105 grand_mean = np.dot((n_batches / n_array).T, B_hat[:n_batch, :]). 106 var_pooled = (data - np.dot(design, B_hat).T) ** 2. <__array_function__ internals> in inv(*args, **kwargs). ~/.local/lib/python3.8/site-packages/numpy/linalg/linalg.py in inv(a). 544 signature = 'D->D' if isComplexType(t) else 'd->d'. 545 extobj = get_linalg_error_extobj(_raise_linalgerror_singular). --> 546 ainv = _umath_linalg.inv(a, signature=signature, extobj=extobj). 547 return wrap(ainv.astype(result_t, copy=False)). 548 . ~/.local/lib/python3.8/site-packages/numpy/linalg/linalg.py in _raise_linalgerror_singular(err, flag). 86 . 87 def _raise_linalgerror_singular(err, flag):. ---> 88 raise ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1606
https://github.com/scverse/scanpy/issues/1606:2114,energy efficiency,model,model,2114,"t.py:340: RuntimeWarning: divide by zero encountered in true_divide. (abs(g_new - g_old) / g_old).max(), (abs(d_new - d_old) / d_old).max(). Adjusting data. ```. ```python. sc.pp.combat(adata, key='384plate', covariates=['age_group']). ```. ```pytb. Standardizing Data across genes. Found 34 batches. Found 1 categorical variables:. 	age_group. Found 0 numerical variables:. 	. ---------------------------------------------------------------------------. LinAlgError Traceback (most recent call last). <timed eval> in <module>. ~/.local/lib/python3.8/site-packages/scanpy/preprocessing/_combat.py in combat(adata, key, covariates, inplace). 204 # standardize across genes using a pooled variance estimator. 205 logg.info(""Standardizing Data across genes.\n""). --> 206 s_data, design, var_pooled, stand_mean = _standardize_data(model, data, key). 207 . 208 # fitting the parameters on the standardized data. ~/.local/lib/python3.8/site-packages/scanpy/preprocessing/_combat.py in _standardize_data(model, data, batch_key). 102 . 103 # compute pooled variance estimator. --> 104 B_hat = np.dot(np.dot(la.inv(np.dot(design.T, design)), design.T), data.T). 105 grand_mean = np.dot((n_batches / n_array).T, B_hat[:n_batch, :]). 106 var_pooled = (data - np.dot(design, B_hat).T) ** 2. <__array_function__ internals> in inv(*args, **kwargs). ~/.local/lib/python3.8/site-packages/numpy/linalg/linalg.py in inv(a). 544 signature = 'D->D' if isComplexType(t) else 'd->d'. 545 extobj = get_linalg_error_extobj(_raise_linalgerror_singular). --> 546 ainv = _umath_linalg.inv(a, signature=signature, extobj=extobj). 547 return wrap(ainv.astype(result_t, copy=False)). 548 . ~/.local/lib/python3.8/site-packages/numpy/linalg/linalg.py in _raise_linalgerror_singular(err, flag). 86 . 87 def _raise_linalgerror_singular(err, flag):. ---> 88 raise LinAlgError(""Singular matrix""). 89 . 90 def _raise_linalgerror_nonposdef(err, flag):. LinAlgError: Singular matrix. ```. #### Versions. <details>. [Paste the output of sc",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1606
https://github.com/scverse/scanpy/issues/1606:2175,energy efficiency,estimat,estimator,2175,"vide. (abs(g_new - g_old) / g_old).max(), (abs(d_new - d_old) / d_old).max(). Adjusting data. ```. ```python. sc.pp.combat(adata, key='384plate', covariates=['age_group']). ```. ```pytb. Standardizing Data across genes. Found 34 batches. Found 1 categorical variables:. 	age_group. Found 0 numerical variables:. 	. ---------------------------------------------------------------------------. LinAlgError Traceback (most recent call last). <timed eval> in <module>. ~/.local/lib/python3.8/site-packages/scanpy/preprocessing/_combat.py in combat(adata, key, covariates, inplace). 204 # standardize across genes using a pooled variance estimator. 205 logg.info(""Standardizing Data across genes.\n""). --> 206 s_data, design, var_pooled, stand_mean = _standardize_data(model, data, key). 207 . 208 # fitting the parameters on the standardized data. ~/.local/lib/python3.8/site-packages/scanpy/preprocessing/_combat.py in _standardize_data(model, data, batch_key). 102 . 103 # compute pooled variance estimator. --> 104 B_hat = np.dot(np.dot(la.inv(np.dot(design.T, design)), design.T), data.T). 105 grand_mean = np.dot((n_batches / n_array).T, B_hat[:n_batch, :]). 106 var_pooled = (data - np.dot(design, B_hat).T) ** 2. <__array_function__ internals> in inv(*args, **kwargs). ~/.local/lib/python3.8/site-packages/numpy/linalg/linalg.py in inv(a). 544 signature = 'D->D' if isComplexType(t) else 'd->d'. 545 extobj = get_linalg_error_extobj(_raise_linalgerror_singular). --> 546 ainv = _umath_linalg.inv(a, signature=signature, extobj=extobj). 547 return wrap(ainv.astype(result_t, copy=False)). 548 . ~/.local/lib/python3.8/site-packages/numpy/linalg/linalg.py in _raise_linalgerror_singular(err, flag). 86 . 87 def _raise_linalgerror_singular(err, flag):. ---> 88 raise LinAlgError(""Singular matrix""). 89 . 90 def _raise_linalgerror_nonposdef(err, flag):. LinAlgError: Singular matrix. ```. #### Versions. <details>. [Paste the output of scanpy.logging.print_versions() leaving a blank line after the de",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1606
https://github.com/scverse/scanpy/issues/1606:178,integrability,version,version,178,"Combat works for some covariates yet fails for others; - [ x] I have checked that this issue has not already been reported. - [ x] I have confirmed this bug exists on the latest version of scanpy. - [ x] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. ### Minimal code sample (that we can copy&paste without having any data). ```python. adata.obs['sex'].cat.categories.tolist(). ```. ```pytb. ['F', 'M', 'U']. ```. ```python. adata.obs['age_groups'].cat.categories.tolist(). ```. ```pytb. ['Old', 'YoungAdult', 'Pediatric', 'Fetal', 'NewBorn']. ```. ```python. sc.pp.combat(adata, key='384plate', covariates=['sex']). ```. ```pytb. Standardizing Data across genes. Found 34 batches. Found 1 categorical variables:. 	sex. Found 0 numerical variables:. 	. Fitting L/S model and finding priors. Finding parametric adjustments. /home/sinhar/.local/lib/python3.8/site-packages/scanpy/preprocessing/_combat.py:340: RuntimeWarning: divide by zero encountered in true_divide. (abs(g_new - g_old) / g_old).max(), (abs(d_new - d_old) / d_old).max(). Adjusting data. ```. ```python. sc.pp.combat(adata, key='384plate', covariates=['age_group']). ```. ```pytb. Standardizing Data across genes. Found 34 batches. Found 1 categorical variables:. 	age_group. Found 0 numerical variables:. 	. ---------------------------------------------------------------------------. LinAlgError Traceback (most recent call last). <timed eval> in <module>. ~/.local/lib/python3.8/site-packages/scanpy/preprocessing/_combat.py in combat(adata, key, covariates, inplace). 204 # standardize across genes using a pooled variance estimator. 205 logg.info(""Standardizing Data across genes.\n""). --> 206 s_data, design, var_pooled, stand_mean = _standardize_data(model, data, key). 207 . 208 # fitting the parameters on",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1606
https://github.com/scverse/scanpy/issues/1606:850,integrability,Standardiz,Standardizing,850,"Combat works for some covariates yet fails for others; - [ x] I have checked that this issue has not already been reported. - [ x] I have confirmed this bug exists on the latest version of scanpy. - [ x] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. ### Minimal code sample (that we can copy&paste without having any data). ```python. adata.obs['sex'].cat.categories.tolist(). ```. ```pytb. ['F', 'M', 'U']. ```. ```python. adata.obs['age_groups'].cat.categories.tolist(). ```. ```pytb. ['Old', 'YoungAdult', 'Pediatric', 'Fetal', 'NewBorn']. ```. ```python. sc.pp.combat(adata, key='384plate', covariates=['sex']). ```. ```pytb. Standardizing Data across genes. Found 34 batches. Found 1 categorical variables:. 	sex. Found 0 numerical variables:. 	. Fitting L/S model and finding priors. Finding parametric adjustments. /home/sinhar/.local/lib/python3.8/site-packages/scanpy/preprocessing/_combat.py:340: RuntimeWarning: divide by zero encountered in true_divide. (abs(g_new - g_old) / g_old).max(), (abs(d_new - d_old) / d_old).max(). Adjusting data. ```. ```python. sc.pp.combat(adata, key='384plate', covariates=['age_group']). ```. ```pytb. Standardizing Data across genes. Found 34 batches. Found 1 categorical variables:. 	age_group. Found 0 numerical variables:. 	. ---------------------------------------------------------------------------. LinAlgError Traceback (most recent call last). <timed eval> in <module>. ~/.local/lib/python3.8/site-packages/scanpy/preprocessing/_combat.py in combat(adata, key, covariates, inplace). 204 # standardize across genes using a pooled variance estimator. 205 logg.info(""Standardizing Data across genes.\n""). --> 206 s_data, design, var_pooled, stand_mean = _standardize_data(model, data, key). 207 . 208 # fitting the parameters on",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1606
https://github.com/scverse/scanpy/issues/1606:892,integrability,batch,batches,892,"Combat works for some covariates yet fails for others; - [ x] I have checked that this issue has not already been reported. - [ x] I have confirmed this bug exists on the latest version of scanpy. - [ x] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. ### Minimal code sample (that we can copy&paste without having any data). ```python. adata.obs['sex'].cat.categories.tolist(). ```. ```pytb. ['F', 'M', 'U']. ```. ```python. adata.obs['age_groups'].cat.categories.tolist(). ```. ```pytb. ['Old', 'YoungAdult', 'Pediatric', 'Fetal', 'NewBorn']. ```. ```python. sc.pp.combat(adata, key='384plate', covariates=['sex']). ```. ```pytb. Standardizing Data across genes. Found 34 batches. Found 1 categorical variables:. 	sex. Found 0 numerical variables:. 	. Fitting L/S model and finding priors. Finding parametric adjustments. /home/sinhar/.local/lib/python3.8/site-packages/scanpy/preprocessing/_combat.py:340: RuntimeWarning: divide by zero encountered in true_divide. (abs(g_new - g_old) / g_old).max(), (abs(d_new - d_old) / d_old).max(). Adjusting data. ```. ```python. sc.pp.combat(adata, key='384plate', covariates=['age_group']). ```. ```pytb. Standardizing Data across genes. Found 34 batches. Found 1 categorical variables:. 	age_group. Found 0 numerical variables:. 	. ---------------------------------------------------------------------------. LinAlgError Traceback (most recent call last). <timed eval> in <module>. ~/.local/lib/python3.8/site-packages/scanpy/preprocessing/_combat.py in combat(adata, key, covariates, inplace). 204 # standardize across genes using a pooled variance estimator. 205 logg.info(""Standardizing Data across genes.\n""). --> 206 s_data, design, var_pooled, stand_mean = _standardize_data(model, data, key). 207 . 208 # fitting the parameters on",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1606
https://github.com/scverse/scanpy/issues/1606:1367,integrability,Standardiz,Standardizing,1367,"l-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. ### Minimal code sample (that we can copy&paste without having any data). ```python. adata.obs['sex'].cat.categories.tolist(). ```. ```pytb. ['F', 'M', 'U']. ```. ```python. adata.obs['age_groups'].cat.categories.tolist(). ```. ```pytb. ['Old', 'YoungAdult', 'Pediatric', 'Fetal', 'NewBorn']. ```. ```python. sc.pp.combat(adata, key='384plate', covariates=['sex']). ```. ```pytb. Standardizing Data across genes. Found 34 batches. Found 1 categorical variables:. 	sex. Found 0 numerical variables:. 	. Fitting L/S model and finding priors. Finding parametric adjustments. /home/sinhar/.local/lib/python3.8/site-packages/scanpy/preprocessing/_combat.py:340: RuntimeWarning: divide by zero encountered in true_divide. (abs(g_new - g_old) / g_old).max(), (abs(d_new - d_old) / d_old).max(). Adjusting data. ```. ```python. sc.pp.combat(adata, key='384plate', covariates=['age_group']). ```. ```pytb. Standardizing Data across genes. Found 34 batches. Found 1 categorical variables:. 	age_group. Found 0 numerical variables:. 	. ---------------------------------------------------------------------------. LinAlgError Traceback (most recent call last). <timed eval> in <module>. ~/.local/lib/python3.8/site-packages/scanpy/preprocessing/_combat.py in combat(adata, key, covariates, inplace). 204 # standardize across genes using a pooled variance estimator. 205 logg.info(""Standardizing Data across genes.\n""). --> 206 s_data, design, var_pooled, stand_mean = _standardize_data(model, data, key). 207 . 208 # fitting the parameters on the standardized data. ~/.local/lib/python3.8/site-packages/scanpy/preprocessing/_combat.py in _standardize_data(model, data, batch_key). 102 . 103 # compute pooled variance estimator. --> 104 B_hat = np.dot(np.dot(la.inv(np.dot(design.T, design)), design.T), data.T). 105 grand_mean = np.dot((n_batches / n_array).T, B_hat[:n_batch, :]). 106 var_pooled = (data - np.dot(de",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1606
https://github.com/scverse/scanpy/issues/1606:1409,integrability,batch,batches,1409," the necessary information for us to reproduce your bug. ### Minimal code sample (that we can copy&paste without having any data). ```python. adata.obs['sex'].cat.categories.tolist(). ```. ```pytb. ['F', 'M', 'U']. ```. ```python. adata.obs['age_groups'].cat.categories.tolist(). ```. ```pytb. ['Old', 'YoungAdult', 'Pediatric', 'Fetal', 'NewBorn']. ```. ```python. sc.pp.combat(adata, key='384plate', covariates=['sex']). ```. ```pytb. Standardizing Data across genes. Found 34 batches. Found 1 categorical variables:. 	sex. Found 0 numerical variables:. 	. Fitting L/S model and finding priors. Finding parametric adjustments. /home/sinhar/.local/lib/python3.8/site-packages/scanpy/preprocessing/_combat.py:340: RuntimeWarning: divide by zero encountered in true_divide. (abs(g_new - g_old) / g_old).max(), (abs(d_new - d_old) / d_old).max(). Adjusting data. ```. ```python. sc.pp.combat(adata, key='384plate', covariates=['age_group']). ```. ```pytb. Standardizing Data across genes. Found 34 batches. Found 1 categorical variables:. 	age_group. Found 0 numerical variables:. 	. ---------------------------------------------------------------------------. LinAlgError Traceback (most recent call last). <timed eval> in <module>. ~/.local/lib/python3.8/site-packages/scanpy/preprocessing/_combat.py in combat(adata, key, covariates, inplace). 204 # standardize across genes using a pooled variance estimator. 205 logg.info(""Standardizing Data across genes.\n""). --> 206 s_data, design, var_pooled, stand_mean = _standardize_data(model, data, key). 207 . 208 # fitting the parameters on the standardized data. ~/.local/lib/python3.8/site-packages/scanpy/preprocessing/_combat.py in _standardize_data(model, data, batch_key). 102 . 103 # compute pooled variance estimator. --> 104 B_hat = np.dot(np.dot(la.inv(np.dot(design.T, design)), design.T), data.T). 105 grand_mean = np.dot((n_batches / n_array).T, B_hat[:n_batch, :]). 106 var_pooled = (data - np.dot(design, B_hat).T) ** 2. <__array_function",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1606
https://github.com/scverse/scanpy/issues/1606:1764,integrability,standardiz,standardize,1764,"`python. sc.pp.combat(adata, key='384plate', covariates=['sex']). ```. ```pytb. Standardizing Data across genes. Found 34 batches. Found 1 categorical variables:. 	sex. Found 0 numerical variables:. 	. Fitting L/S model and finding priors. Finding parametric adjustments. /home/sinhar/.local/lib/python3.8/site-packages/scanpy/preprocessing/_combat.py:340: RuntimeWarning: divide by zero encountered in true_divide. (abs(g_new - g_old) / g_old).max(), (abs(d_new - d_old) / d_old).max(). Adjusting data. ```. ```python. sc.pp.combat(adata, key='384plate', covariates=['age_group']). ```. ```pytb. Standardizing Data across genes. Found 34 batches. Found 1 categorical variables:. 	age_group. Found 0 numerical variables:. 	. ---------------------------------------------------------------------------. LinAlgError Traceback (most recent call last). <timed eval> in <module>. ~/.local/lib/python3.8/site-packages/scanpy/preprocessing/_combat.py in combat(adata, key, covariates, inplace). 204 # standardize across genes using a pooled variance estimator. 205 logg.info(""Standardizing Data across genes.\n""). --> 206 s_data, design, var_pooled, stand_mean = _standardize_data(model, data, key). 207 . 208 # fitting the parameters on the standardized data. ~/.local/lib/python3.8/site-packages/scanpy/preprocessing/_combat.py in _standardize_data(model, data, batch_key). 102 . 103 # compute pooled variance estimator. --> 104 B_hat = np.dot(np.dot(la.inv(np.dot(design.T, design)), design.T), data.T). 105 grand_mean = np.dot((n_batches / n_array).T, B_hat[:n_batch, :]). 106 var_pooled = (data - np.dot(design, B_hat).T) ** 2. <__array_function__ internals> in inv(*args, **kwargs). ~/.local/lib/python3.8/site-packages/numpy/linalg/linalg.py in inv(a). 544 signature = 'D->D' if isComplexType(t) else 'd->d'. 545 extobj = get_linalg_error_extobj(_raise_linalgerror_singular). --> 546 ainv = _umath_linalg.inv(a, signature=signature, extobj=extobj). 547 return wrap(ainv.astype(result_t, copy=False)).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1606
https://github.com/scverse/scanpy/issues/1606:1839,integrability,Standardiz,Standardizing,1839,"tb. Standardizing Data across genes. Found 34 batches. Found 1 categorical variables:. 	sex. Found 0 numerical variables:. 	. Fitting L/S model and finding priors. Finding parametric adjustments. /home/sinhar/.local/lib/python3.8/site-packages/scanpy/preprocessing/_combat.py:340: RuntimeWarning: divide by zero encountered in true_divide. (abs(g_new - g_old) / g_old).max(), (abs(d_new - d_old) / d_old).max(). Adjusting data. ```. ```python. sc.pp.combat(adata, key='384plate', covariates=['age_group']). ```. ```pytb. Standardizing Data across genes. Found 34 batches. Found 1 categorical variables:. 	age_group. Found 0 numerical variables:. 	. ---------------------------------------------------------------------------. LinAlgError Traceback (most recent call last). <timed eval> in <module>. ~/.local/lib/python3.8/site-packages/scanpy/preprocessing/_combat.py in combat(adata, key, covariates, inplace). 204 # standardize across genes using a pooled variance estimator. 205 logg.info(""Standardizing Data across genes.\n""). --> 206 s_data, design, var_pooled, stand_mean = _standardize_data(model, data, key). 207 . 208 # fitting the parameters on the standardized data. ~/.local/lib/python3.8/site-packages/scanpy/preprocessing/_combat.py in _standardize_data(model, data, batch_key). 102 . 103 # compute pooled variance estimator. --> 104 B_hat = np.dot(np.dot(la.inv(np.dot(design.T, design)), design.T), data.T). 105 grand_mean = np.dot((n_batches / n_array).T, B_hat[:n_batch, :]). 106 var_pooled = (data - np.dot(design, B_hat).T) ** 2. <__array_function__ internals> in inv(*args, **kwargs). ~/.local/lib/python3.8/site-packages/numpy/linalg/linalg.py in inv(a). 544 signature = 'D->D' if isComplexType(t) else 'd->d'. 545 extobj = get_linalg_error_extobj(_raise_linalgerror_singular). --> 546 ainv = _umath_linalg.inv(a, signature=signature, extobj=extobj). 547 return wrap(ainv.astype(result_t, copy=False)). 548 . ~/.local/lib/python3.8/site-packages/numpy/linalg/linalg.py in _raise",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1606
https://github.com/scverse/scanpy/issues/1606:2005,integrability,standardiz,standardized,2005,"inding parametric adjustments. /home/sinhar/.local/lib/python3.8/site-packages/scanpy/preprocessing/_combat.py:340: RuntimeWarning: divide by zero encountered in true_divide. (abs(g_new - g_old) / g_old).max(), (abs(d_new - d_old) / d_old).max(). Adjusting data. ```. ```python. sc.pp.combat(adata, key='384plate', covariates=['age_group']). ```. ```pytb. Standardizing Data across genes. Found 34 batches. Found 1 categorical variables:. 	age_group. Found 0 numerical variables:. 	. ---------------------------------------------------------------------------. LinAlgError Traceback (most recent call last). <timed eval> in <module>. ~/.local/lib/python3.8/site-packages/scanpy/preprocessing/_combat.py in combat(adata, key, covariates, inplace). 204 # standardize across genes using a pooled variance estimator. 205 logg.info(""Standardizing Data across genes.\n""). --> 206 s_data, design, var_pooled, stand_mean = _standardize_data(model, data, key). 207 . 208 # fitting the parameters on the standardized data. ~/.local/lib/python3.8/site-packages/scanpy/preprocessing/_combat.py in _standardize_data(model, data, batch_key). 102 . 103 # compute pooled variance estimator. --> 104 B_hat = np.dot(np.dot(la.inv(np.dot(design.T, design)), design.T), data.T). 105 grand_mean = np.dot((n_batches / n_array).T, B_hat[:n_batch, :]). 106 var_pooled = (data - np.dot(design, B_hat).T) ** 2. <__array_function__ internals> in inv(*args, **kwargs). ~/.local/lib/python3.8/site-packages/numpy/linalg/linalg.py in inv(a). 544 signature = 'D->D' if isComplexType(t) else 'd->d'. 545 extobj = get_linalg_error_extobj(_raise_linalgerror_singular). --> 546 ainv = _umath_linalg.inv(a, signature=signature, extobj=extobj). 547 return wrap(ainv.astype(result_t, copy=False)). 548 . ~/.local/lib/python3.8/site-packages/numpy/linalg/linalg.py in _raise_linalgerror_singular(err, flag). 86 . 87 def _raise_linalgerror_singular(err, flag):. ---> 88 raise LinAlgError(""Singular matrix""). 89 . 90 def _raise_linalgerror_n",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1606
https://github.com/scverse/scanpy/issues/1606:2730,integrability,wrap,wrap,2730,". Found 34 batches. Found 1 categorical variables:. 	age_group. Found 0 numerical variables:. 	. ---------------------------------------------------------------------------. LinAlgError Traceback (most recent call last). <timed eval> in <module>. ~/.local/lib/python3.8/site-packages/scanpy/preprocessing/_combat.py in combat(adata, key, covariates, inplace). 204 # standardize across genes using a pooled variance estimator. 205 logg.info(""Standardizing Data across genes.\n""). --> 206 s_data, design, var_pooled, stand_mean = _standardize_data(model, data, key). 207 . 208 # fitting the parameters on the standardized data. ~/.local/lib/python3.8/site-packages/scanpy/preprocessing/_combat.py in _standardize_data(model, data, batch_key). 102 . 103 # compute pooled variance estimator. --> 104 B_hat = np.dot(np.dot(la.inv(np.dot(design.T, design)), design.T), data.T). 105 grand_mean = np.dot((n_batches / n_array).T, B_hat[:n_batch, :]). 106 var_pooled = (data - np.dot(design, B_hat).T) ** 2. <__array_function__ internals> in inv(*args, **kwargs). ~/.local/lib/python3.8/site-packages/numpy/linalg/linalg.py in inv(a). 544 signature = 'D->D' if isComplexType(t) else 'd->d'. 545 extobj = get_linalg_error_extobj(_raise_linalgerror_singular). --> 546 ainv = _umath_linalg.inv(a, signature=signature, extobj=extobj). 547 return wrap(ainv.astype(result_t, copy=False)). 548 . ~/.local/lib/python3.8/site-packages/numpy/linalg/linalg.py in _raise_linalgerror_singular(err, flag). 86 . 87 def _raise_linalgerror_singular(err, flag):. ---> 88 raise LinAlgError(""Singular matrix""). 89 . 90 def _raise_linalgerror_nonposdef(err, flag):. LinAlgError: Singular matrix. ```. #### Versions. <details>. [Paste the output of scanpy.logging.print_versions() leaving a blank line after the details tag]. </details>. scanpy==1.7.0rc2.dev7+g57ec8a7e anndata==0.7.3 umap==0.4.6 numpy==1.19.5 scipy==1.6.0 pandas==1.2.1 scikit-learn==0.24.1 statsmodels==0.12.1 python-igraph==0.8.3 louvain==0.7.0 leidenalg==0.8.3.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1606
https://github.com/scverse/scanpy/issues/1606:3073,integrability,Version,Versions,3073,". Found 34 batches. Found 1 categorical variables:. 	age_group. Found 0 numerical variables:. 	. ---------------------------------------------------------------------------. LinAlgError Traceback (most recent call last). <timed eval> in <module>. ~/.local/lib/python3.8/site-packages/scanpy/preprocessing/_combat.py in combat(adata, key, covariates, inplace). 204 # standardize across genes using a pooled variance estimator. 205 logg.info(""Standardizing Data across genes.\n""). --> 206 s_data, design, var_pooled, stand_mean = _standardize_data(model, data, key). 207 . 208 # fitting the parameters on the standardized data. ~/.local/lib/python3.8/site-packages/scanpy/preprocessing/_combat.py in _standardize_data(model, data, batch_key). 102 . 103 # compute pooled variance estimator. --> 104 B_hat = np.dot(np.dot(la.inv(np.dot(design.T, design)), design.T), data.T). 105 grand_mean = np.dot((n_batches / n_array).T, B_hat[:n_batch, :]). 106 var_pooled = (data - np.dot(design, B_hat).T) ** 2. <__array_function__ internals> in inv(*args, **kwargs). ~/.local/lib/python3.8/site-packages/numpy/linalg/linalg.py in inv(a). 544 signature = 'D->D' if isComplexType(t) else 'd->d'. 545 extobj = get_linalg_error_extobj(_raise_linalgerror_singular). --> 546 ainv = _umath_linalg.inv(a, signature=signature, extobj=extobj). 547 return wrap(ainv.astype(result_t, copy=False)). 548 . ~/.local/lib/python3.8/site-packages/numpy/linalg/linalg.py in _raise_linalgerror_singular(err, flag). 86 . 87 def _raise_linalgerror_singular(err, flag):. ---> 88 raise LinAlgError(""Singular matrix""). 89 . 90 def _raise_linalgerror_nonposdef(err, flag):. LinAlgError: Singular matrix. ```. #### Versions. <details>. [Paste the output of scanpy.logging.print_versions() leaving a blank line after the details tag]. </details>. scanpy==1.7.0rc2.dev7+g57ec8a7e anndata==0.7.3 umap==0.4.6 numpy==1.19.5 scipy==1.6.0 pandas==1.2.1 scikit-learn==0.24.1 statsmodels==0.12.1 python-igraph==0.8.3 louvain==0.7.0 leidenalg==0.8.3.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1606
https://github.com/scverse/scanpy/issues/1606:850,interoperability,Standard,Standardizing,850,"Combat works for some covariates yet fails for others; - [ x] I have checked that this issue has not already been reported. - [ x] I have confirmed this bug exists on the latest version of scanpy. - [ x] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. ### Minimal code sample (that we can copy&paste without having any data). ```python. adata.obs['sex'].cat.categories.tolist(). ```. ```pytb. ['F', 'M', 'U']. ```. ```python. adata.obs['age_groups'].cat.categories.tolist(). ```. ```pytb. ['Old', 'YoungAdult', 'Pediatric', 'Fetal', 'NewBorn']. ```. ```python. sc.pp.combat(adata, key='384plate', covariates=['sex']). ```. ```pytb. Standardizing Data across genes. Found 34 batches. Found 1 categorical variables:. 	sex. Found 0 numerical variables:. 	. Fitting L/S model and finding priors. Finding parametric adjustments. /home/sinhar/.local/lib/python3.8/site-packages/scanpy/preprocessing/_combat.py:340: RuntimeWarning: divide by zero encountered in true_divide. (abs(g_new - g_old) / g_old).max(), (abs(d_new - d_old) / d_old).max(). Adjusting data. ```. ```python. sc.pp.combat(adata, key='384plate', covariates=['age_group']). ```. ```pytb. Standardizing Data across genes. Found 34 batches. Found 1 categorical variables:. 	age_group. Found 0 numerical variables:. 	. ---------------------------------------------------------------------------. LinAlgError Traceback (most recent call last). <timed eval> in <module>. ~/.local/lib/python3.8/site-packages/scanpy/preprocessing/_combat.py in combat(adata, key, covariates, inplace). 204 # standardize across genes using a pooled variance estimator. 205 logg.info(""Standardizing Data across genes.\n""). --> 206 s_data, design, var_pooled, stand_mean = _standardize_data(model, data, key). 207 . 208 # fitting the parameters on",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1606
https://github.com/scverse/scanpy/issues/1606:1367,interoperability,Standard,Standardizing,1367,"l-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. ### Minimal code sample (that we can copy&paste without having any data). ```python. adata.obs['sex'].cat.categories.tolist(). ```. ```pytb. ['F', 'M', 'U']. ```. ```python. adata.obs['age_groups'].cat.categories.tolist(). ```. ```pytb. ['Old', 'YoungAdult', 'Pediatric', 'Fetal', 'NewBorn']. ```. ```python. sc.pp.combat(adata, key='384plate', covariates=['sex']). ```. ```pytb. Standardizing Data across genes. Found 34 batches. Found 1 categorical variables:. 	sex. Found 0 numerical variables:. 	. Fitting L/S model and finding priors. Finding parametric adjustments. /home/sinhar/.local/lib/python3.8/site-packages/scanpy/preprocessing/_combat.py:340: RuntimeWarning: divide by zero encountered in true_divide. (abs(g_new - g_old) / g_old).max(), (abs(d_new - d_old) / d_old).max(). Adjusting data. ```. ```python. sc.pp.combat(adata, key='384plate', covariates=['age_group']). ```. ```pytb. Standardizing Data across genes. Found 34 batches. Found 1 categorical variables:. 	age_group. Found 0 numerical variables:. 	. ---------------------------------------------------------------------------. LinAlgError Traceback (most recent call last). <timed eval> in <module>. ~/.local/lib/python3.8/site-packages/scanpy/preprocessing/_combat.py in combat(adata, key, covariates, inplace). 204 # standardize across genes using a pooled variance estimator. 205 logg.info(""Standardizing Data across genes.\n""). --> 206 s_data, design, var_pooled, stand_mean = _standardize_data(model, data, key). 207 . 208 # fitting the parameters on the standardized data. ~/.local/lib/python3.8/site-packages/scanpy/preprocessing/_combat.py in _standardize_data(model, data, batch_key). 102 . 103 # compute pooled variance estimator. --> 104 B_hat = np.dot(np.dot(la.inv(np.dot(design.T, design)), design.T), data.T). 105 grand_mean = np.dot((n_batches / n_array).T, B_hat[:n_batch, :]). 106 var_pooled = (data - np.dot(de",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1606
https://github.com/scverse/scanpy/issues/1606:1764,interoperability,standard,standardize,1764,"`python. sc.pp.combat(adata, key='384plate', covariates=['sex']). ```. ```pytb. Standardizing Data across genes. Found 34 batches. Found 1 categorical variables:. 	sex. Found 0 numerical variables:. 	. Fitting L/S model and finding priors. Finding parametric adjustments. /home/sinhar/.local/lib/python3.8/site-packages/scanpy/preprocessing/_combat.py:340: RuntimeWarning: divide by zero encountered in true_divide. (abs(g_new - g_old) / g_old).max(), (abs(d_new - d_old) / d_old).max(). Adjusting data. ```. ```python. sc.pp.combat(adata, key='384plate', covariates=['age_group']). ```. ```pytb. Standardizing Data across genes. Found 34 batches. Found 1 categorical variables:. 	age_group. Found 0 numerical variables:. 	. ---------------------------------------------------------------------------. LinAlgError Traceback (most recent call last). <timed eval> in <module>. ~/.local/lib/python3.8/site-packages/scanpy/preprocessing/_combat.py in combat(adata, key, covariates, inplace). 204 # standardize across genes using a pooled variance estimator. 205 logg.info(""Standardizing Data across genes.\n""). --> 206 s_data, design, var_pooled, stand_mean = _standardize_data(model, data, key). 207 . 208 # fitting the parameters on the standardized data. ~/.local/lib/python3.8/site-packages/scanpy/preprocessing/_combat.py in _standardize_data(model, data, batch_key). 102 . 103 # compute pooled variance estimator. --> 104 B_hat = np.dot(np.dot(la.inv(np.dot(design.T, design)), design.T), data.T). 105 grand_mean = np.dot((n_batches / n_array).T, B_hat[:n_batch, :]). 106 var_pooled = (data - np.dot(design, B_hat).T) ** 2. <__array_function__ internals> in inv(*args, **kwargs). ~/.local/lib/python3.8/site-packages/numpy/linalg/linalg.py in inv(a). 544 signature = 'D->D' if isComplexType(t) else 'd->d'. 545 extobj = get_linalg_error_extobj(_raise_linalgerror_singular). --> 546 ainv = _umath_linalg.inv(a, signature=signature, extobj=extobj). 547 return wrap(ainv.astype(result_t, copy=False)).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1606
https://github.com/scverse/scanpy/issues/1606:1839,interoperability,Standard,Standardizing,1839,"tb. Standardizing Data across genes. Found 34 batches. Found 1 categorical variables:. 	sex. Found 0 numerical variables:. 	. Fitting L/S model and finding priors. Finding parametric adjustments. /home/sinhar/.local/lib/python3.8/site-packages/scanpy/preprocessing/_combat.py:340: RuntimeWarning: divide by zero encountered in true_divide. (abs(g_new - g_old) / g_old).max(), (abs(d_new - d_old) / d_old).max(). Adjusting data. ```. ```python. sc.pp.combat(adata, key='384plate', covariates=['age_group']). ```. ```pytb. Standardizing Data across genes. Found 34 batches. Found 1 categorical variables:. 	age_group. Found 0 numerical variables:. 	. ---------------------------------------------------------------------------. LinAlgError Traceback (most recent call last). <timed eval> in <module>. ~/.local/lib/python3.8/site-packages/scanpy/preprocessing/_combat.py in combat(adata, key, covariates, inplace). 204 # standardize across genes using a pooled variance estimator. 205 logg.info(""Standardizing Data across genes.\n""). --> 206 s_data, design, var_pooled, stand_mean = _standardize_data(model, data, key). 207 . 208 # fitting the parameters on the standardized data. ~/.local/lib/python3.8/site-packages/scanpy/preprocessing/_combat.py in _standardize_data(model, data, batch_key). 102 . 103 # compute pooled variance estimator. --> 104 B_hat = np.dot(np.dot(la.inv(np.dot(design.T, design)), design.T), data.T). 105 grand_mean = np.dot((n_batches / n_array).T, B_hat[:n_batch, :]). 106 var_pooled = (data - np.dot(design, B_hat).T) ** 2. <__array_function__ internals> in inv(*args, **kwargs). ~/.local/lib/python3.8/site-packages/numpy/linalg/linalg.py in inv(a). 544 signature = 'D->D' if isComplexType(t) else 'd->d'. 545 extobj = get_linalg_error_extobj(_raise_linalgerror_singular). --> 546 ainv = _umath_linalg.inv(a, signature=signature, extobj=extobj). 547 return wrap(ainv.astype(result_t, copy=False)). 548 . ~/.local/lib/python3.8/site-packages/numpy/linalg/linalg.py in _raise",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1606
https://github.com/scverse/scanpy/issues/1606:2005,interoperability,standard,standardized,2005,"inding parametric adjustments. /home/sinhar/.local/lib/python3.8/site-packages/scanpy/preprocessing/_combat.py:340: RuntimeWarning: divide by zero encountered in true_divide. (abs(g_new - g_old) / g_old).max(), (abs(d_new - d_old) / d_old).max(). Adjusting data. ```. ```python. sc.pp.combat(adata, key='384plate', covariates=['age_group']). ```. ```pytb. Standardizing Data across genes. Found 34 batches. Found 1 categorical variables:. 	age_group. Found 0 numerical variables:. 	. ---------------------------------------------------------------------------. LinAlgError Traceback (most recent call last). <timed eval> in <module>. ~/.local/lib/python3.8/site-packages/scanpy/preprocessing/_combat.py in combat(adata, key, covariates, inplace). 204 # standardize across genes using a pooled variance estimator. 205 logg.info(""Standardizing Data across genes.\n""). --> 206 s_data, design, var_pooled, stand_mean = _standardize_data(model, data, key). 207 . 208 # fitting the parameters on the standardized data. ~/.local/lib/python3.8/site-packages/scanpy/preprocessing/_combat.py in _standardize_data(model, data, batch_key). 102 . 103 # compute pooled variance estimator. --> 104 B_hat = np.dot(np.dot(la.inv(np.dot(design.T, design)), design.T), data.T). 105 grand_mean = np.dot((n_batches / n_array).T, B_hat[:n_batch, :]). 106 var_pooled = (data - np.dot(design, B_hat).T) ** 2. <__array_function__ internals> in inv(*args, **kwargs). ~/.local/lib/python3.8/site-packages/numpy/linalg/linalg.py in inv(a). 544 signature = 'D->D' if isComplexType(t) else 'd->d'. 545 extobj = get_linalg_error_extobj(_raise_linalgerror_singular). --> 546 ainv = _umath_linalg.inv(a, signature=signature, extobj=extobj). 547 return wrap(ainv.astype(result_t, copy=False)). 548 . ~/.local/lib/python3.8/site-packages/numpy/linalg/linalg.py in _raise_linalgerror_singular(err, flag). 86 . 87 def _raise_linalgerror_singular(err, flag):. ---> 88 raise LinAlgError(""Singular matrix""). 89 . 90 def _raise_linalgerror_n",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1606
https://github.com/scverse/scanpy/issues/1606:178,modifiability,version,version,178,"Combat works for some covariates yet fails for others; - [ x] I have checked that this issue has not already been reported. - [ x] I have confirmed this bug exists on the latest version of scanpy. - [ x] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. ### Minimal code sample (that we can copy&paste without having any data). ```python. adata.obs['sex'].cat.categories.tolist(). ```. ```pytb. ['F', 'M', 'U']. ```. ```python. adata.obs['age_groups'].cat.categories.tolist(). ```. ```pytb. ['Old', 'YoungAdult', 'Pediatric', 'Fetal', 'NewBorn']. ```. ```python. sc.pp.combat(adata, key='384plate', covariates=['sex']). ```. ```pytb. Standardizing Data across genes. Found 34 batches. Found 1 categorical variables:. 	sex. Found 0 numerical variables:. 	. Fitting L/S model and finding priors. Finding parametric adjustments. /home/sinhar/.local/lib/python3.8/site-packages/scanpy/preprocessing/_combat.py:340: RuntimeWarning: divide by zero encountered in true_divide. (abs(g_new - g_old) / g_old).max(), (abs(d_new - d_old) / d_old).max(). Adjusting data. ```. ```python. sc.pp.combat(adata, key='384plate', covariates=['age_group']). ```. ```pytb. Standardizing Data across genes. Found 34 batches. Found 1 categorical variables:. 	age_group. Found 0 numerical variables:. 	. ---------------------------------------------------------------------------. LinAlgError Traceback (most recent call last). <timed eval> in <module>. ~/.local/lib/python3.8/site-packages/scanpy/preprocessing/_combat.py in combat(adata, key, covariates, inplace). 204 # standardize across genes using a pooled variance estimator. 205 logg.info(""Standardizing Data across genes.\n""). --> 206 s_data, design, var_pooled, stand_mean = _standardize_data(model, data, key). 207 . 208 # fitting the parameters on",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1606
https://github.com/scverse/scanpy/issues/1606:921,modifiability,variab,variables,921,"Combat works for some covariates yet fails for others; - [ x] I have checked that this issue has not already been reported. - [ x] I have confirmed this bug exists on the latest version of scanpy. - [ x] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. ### Minimal code sample (that we can copy&paste without having any data). ```python. adata.obs['sex'].cat.categories.tolist(). ```. ```pytb. ['F', 'M', 'U']. ```. ```python. adata.obs['age_groups'].cat.categories.tolist(). ```. ```pytb. ['Old', 'YoungAdult', 'Pediatric', 'Fetal', 'NewBorn']. ```. ```python. sc.pp.combat(adata, key='384plate', covariates=['sex']). ```. ```pytb. Standardizing Data across genes. Found 34 batches. Found 1 categorical variables:. 	sex. Found 0 numerical variables:. 	. Fitting L/S model and finding priors. Finding parametric adjustments. /home/sinhar/.local/lib/python3.8/site-packages/scanpy/preprocessing/_combat.py:340: RuntimeWarning: divide by zero encountered in true_divide. (abs(g_new - g_old) / g_old).max(), (abs(d_new - d_old) / d_old).max(). Adjusting data. ```. ```python. sc.pp.combat(adata, key='384plate', covariates=['age_group']). ```. ```pytb. Standardizing Data across genes. Found 34 batches. Found 1 categorical variables:. 	age_group. Found 0 numerical variables:. 	. ---------------------------------------------------------------------------. LinAlgError Traceback (most recent call last). <timed eval> in <module>. ~/.local/lib/python3.8/site-packages/scanpy/preprocessing/_combat.py in combat(adata, key, covariates, inplace). 204 # standardize across genes using a pooled variance estimator. 205 logg.info(""Standardizing Data across genes.\n""). --> 206 s_data, design, var_pooled, stand_mean = _standardize_data(model, data, key). 207 . 208 # fitting the parameters on",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1606
https://github.com/scverse/scanpy/issues/1606:957,modifiability,variab,variables,957,"Combat works for some covariates yet fails for others; - [ x] I have checked that this issue has not already been reported. - [ x] I have confirmed this bug exists on the latest version of scanpy. - [ x] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. ### Minimal code sample (that we can copy&paste without having any data). ```python. adata.obs['sex'].cat.categories.tolist(). ```. ```pytb. ['F', 'M', 'U']. ```. ```python. adata.obs['age_groups'].cat.categories.tolist(). ```. ```pytb. ['Old', 'YoungAdult', 'Pediatric', 'Fetal', 'NewBorn']. ```. ```python. sc.pp.combat(adata, key='384plate', covariates=['sex']). ```. ```pytb. Standardizing Data across genes. Found 34 batches. Found 1 categorical variables:. 	sex. Found 0 numerical variables:. 	. Fitting L/S model and finding priors. Finding parametric adjustments. /home/sinhar/.local/lib/python3.8/site-packages/scanpy/preprocessing/_combat.py:340: RuntimeWarning: divide by zero encountered in true_divide. (abs(g_new - g_old) / g_old).max(), (abs(d_new - d_old) / d_old).max(). Adjusting data. ```. ```python. sc.pp.combat(adata, key='384plate', covariates=['age_group']). ```. ```pytb. Standardizing Data across genes. Found 34 batches. Found 1 categorical variables:. 	age_group. Found 0 numerical variables:. 	. ---------------------------------------------------------------------------. LinAlgError Traceback (most recent call last). <timed eval> in <module>. ~/.local/lib/python3.8/site-packages/scanpy/preprocessing/_combat.py in combat(adata, key, covariates, inplace). 204 # standardize across genes using a pooled variance estimator. 205 logg.info(""Standardizing Data across genes.\n""). --> 206 s_data, design, var_pooled, stand_mean = _standardize_data(model, data, key). 207 . 208 # fitting the parameters on",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1606
https://github.com/scverse/scanpy/issues/1606:1018,modifiability,paramet,parametric,1018,"ovariates yet fails for others; - [ x] I have checked that this issue has not already been reported. - [ x] I have confirmed this bug exists on the latest version of scanpy. - [ x] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. ### Minimal code sample (that we can copy&paste without having any data). ```python. adata.obs['sex'].cat.categories.tolist(). ```. ```pytb. ['F', 'M', 'U']. ```. ```python. adata.obs['age_groups'].cat.categories.tolist(). ```. ```pytb. ['Old', 'YoungAdult', 'Pediatric', 'Fetal', 'NewBorn']. ```. ```python. sc.pp.combat(adata, key='384plate', covariates=['sex']). ```. ```pytb. Standardizing Data across genes. Found 34 batches. Found 1 categorical variables:. 	sex. Found 0 numerical variables:. 	. Fitting L/S model and finding priors. Finding parametric adjustments. /home/sinhar/.local/lib/python3.8/site-packages/scanpy/preprocessing/_combat.py:340: RuntimeWarning: divide by zero encountered in true_divide. (abs(g_new - g_old) / g_old).max(), (abs(d_new - d_old) / d_old).max(). Adjusting data. ```. ```python. sc.pp.combat(adata, key='384plate', covariates=['age_group']). ```. ```pytb. Standardizing Data across genes. Found 34 batches. Found 1 categorical variables:. 	age_group. Found 0 numerical variables:. 	. ---------------------------------------------------------------------------. LinAlgError Traceback (most recent call last). <timed eval> in <module>. ~/.local/lib/python3.8/site-packages/scanpy/preprocessing/_combat.py in combat(adata, key, covariates, inplace). 204 # standardize across genes using a pooled variance estimator. 205 logg.info(""Standardizing Data across genes.\n""). --> 206 s_data, design, var_pooled, stand_mean = _standardize_data(model, data, key). 207 . 208 # fitting the parameters on the standardized data.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1606
https://github.com/scverse/scanpy/issues/1606:1081,modifiability,pac,packages,1081,"s issue has not already been reported. - [ x] I have confirmed this bug exists on the latest version of scanpy. - [ x] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. ### Minimal code sample (that we can copy&paste without having any data). ```python. adata.obs['sex'].cat.categories.tolist(). ```. ```pytb. ['F', 'M', 'U']. ```. ```python. adata.obs['age_groups'].cat.categories.tolist(). ```. ```pytb. ['Old', 'YoungAdult', 'Pediatric', 'Fetal', 'NewBorn']. ```. ```python. sc.pp.combat(adata, key='384plate', covariates=['sex']). ```. ```pytb. Standardizing Data across genes. Found 34 batches. Found 1 categorical variables:. 	sex. Found 0 numerical variables:. 	. Fitting L/S model and finding priors. Finding parametric adjustments. /home/sinhar/.local/lib/python3.8/site-packages/scanpy/preprocessing/_combat.py:340: RuntimeWarning: divide by zero encountered in true_divide. (abs(g_new - g_old) / g_old).max(), (abs(d_new - d_old) / d_old).max(). Adjusting data. ```. ```python. sc.pp.combat(adata, key='384plate', covariates=['age_group']). ```. ```pytb. Standardizing Data across genes. Found 34 batches. Found 1 categorical variables:. 	age_group. Found 0 numerical variables:. 	. ---------------------------------------------------------------------------. LinAlgError Traceback (most recent call last). <timed eval> in <module>. ~/.local/lib/python3.8/site-packages/scanpy/preprocessing/_combat.py in combat(adata, key, covariates, inplace). 204 # standardize across genes using a pooled variance estimator. 205 logg.info(""Standardizing Data across genes.\n""). --> 206 s_data, design, var_pooled, stand_mean = _standardize_data(model, data, key). 207 . 208 # fitting the parameters on the standardized data. ~/.local/lib/python3.8/site-packages/scanpy/preprocessing/_co",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1606
https://github.com/scverse/scanpy/issues/1606:1438,modifiability,variab,variables,1438," us to reproduce your bug. ### Minimal code sample (that we can copy&paste without having any data). ```python. adata.obs['sex'].cat.categories.tolist(). ```. ```pytb. ['F', 'M', 'U']. ```. ```python. adata.obs['age_groups'].cat.categories.tolist(). ```. ```pytb. ['Old', 'YoungAdult', 'Pediatric', 'Fetal', 'NewBorn']. ```. ```python. sc.pp.combat(adata, key='384plate', covariates=['sex']). ```. ```pytb. Standardizing Data across genes. Found 34 batches. Found 1 categorical variables:. 	sex. Found 0 numerical variables:. 	. Fitting L/S model and finding priors. Finding parametric adjustments. /home/sinhar/.local/lib/python3.8/site-packages/scanpy/preprocessing/_combat.py:340: RuntimeWarning: divide by zero encountered in true_divide. (abs(g_new - g_old) / g_old).max(), (abs(d_new - d_old) / d_old).max(). Adjusting data. ```. ```python. sc.pp.combat(adata, key='384plate', covariates=['age_group']). ```. ```pytb. Standardizing Data across genes. Found 34 batches. Found 1 categorical variables:. 	age_group. Found 0 numerical variables:. 	. ---------------------------------------------------------------------------. LinAlgError Traceback (most recent call last). <timed eval> in <module>. ~/.local/lib/python3.8/site-packages/scanpy/preprocessing/_combat.py in combat(adata, key, covariates, inplace). 204 # standardize across genes using a pooled variance estimator. 205 logg.info(""Standardizing Data across genes.\n""). --> 206 s_data, design, var_pooled, stand_mean = _standardize_data(model, data, key). 207 . 208 # fitting the parameters on the standardized data. ~/.local/lib/python3.8/site-packages/scanpy/preprocessing/_combat.py in _standardize_data(model, data, batch_key). 102 . 103 # compute pooled variance estimator. --> 104 B_hat = np.dot(np.dot(la.inv(np.dot(design.T, design)), design.T), data.T). 105 grand_mean = np.dot((n_batches / n_array).T, B_hat[:n_batch, :]). 106 var_pooled = (data - np.dot(design, B_hat).T) ** 2. <__array_function__ internals> in inv(*args, **",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1606
https://github.com/scverse/scanpy/issues/1606:1480,modifiability,variab,variables,1480,"e sample (that we can copy&paste without having any data). ```python. adata.obs['sex'].cat.categories.tolist(). ```. ```pytb. ['F', 'M', 'U']. ```. ```python. adata.obs['age_groups'].cat.categories.tolist(). ```. ```pytb. ['Old', 'YoungAdult', 'Pediatric', 'Fetal', 'NewBorn']. ```. ```python. sc.pp.combat(adata, key='384plate', covariates=['sex']). ```. ```pytb. Standardizing Data across genes. Found 34 batches. Found 1 categorical variables:. 	sex. Found 0 numerical variables:. 	. Fitting L/S model and finding priors. Finding parametric adjustments. /home/sinhar/.local/lib/python3.8/site-packages/scanpy/preprocessing/_combat.py:340: RuntimeWarning: divide by zero encountered in true_divide. (abs(g_new - g_old) / g_old).max(), (abs(d_new - d_old) / d_old).max(). Adjusting data. ```. ```python. sc.pp.combat(adata, key='384plate', covariates=['age_group']). ```. ```pytb. Standardizing Data across genes. Found 34 batches. Found 1 categorical variables:. 	age_group. Found 0 numerical variables:. 	. ---------------------------------------------------------------------------. LinAlgError Traceback (most recent call last). <timed eval> in <module>. ~/.local/lib/python3.8/site-packages/scanpy/preprocessing/_combat.py in combat(adata, key, covariates, inplace). 204 # standardize across genes using a pooled variance estimator. 205 logg.info(""Standardizing Data across genes.\n""). --> 206 s_data, design, var_pooled, stand_mean = _standardize_data(model, data, key). 207 . 208 # fitting the parameters on the standardized data. ~/.local/lib/python3.8/site-packages/scanpy/preprocessing/_combat.py in _standardize_data(model, data, batch_key). 102 . 103 # compute pooled variance estimator. --> 104 B_hat = np.dot(np.dot(la.inv(np.dot(design.T, design)), design.T), data.T). 105 grand_mean = np.dot((n_batches / n_array).T, B_hat[:n_batch, :]). 106 var_pooled = (data - np.dot(design, B_hat).T) ** 2. <__array_function__ internals> in inv(*args, **kwargs). ~/.local/lib/python3.8/site-packa",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1606
https://github.com/scverse/scanpy/issues/1606:1636,modifiability,modul,module,1636,"hon. adata.obs['age_groups'].cat.categories.tolist(). ```. ```pytb. ['Old', 'YoungAdult', 'Pediatric', 'Fetal', 'NewBorn']. ```. ```python. sc.pp.combat(adata, key='384plate', covariates=['sex']). ```. ```pytb. Standardizing Data across genes. Found 34 batches. Found 1 categorical variables:. 	sex. Found 0 numerical variables:. 	. Fitting L/S model and finding priors. Finding parametric adjustments. /home/sinhar/.local/lib/python3.8/site-packages/scanpy/preprocessing/_combat.py:340: RuntimeWarning: divide by zero encountered in true_divide. (abs(g_new - g_old) / g_old).max(), (abs(d_new - d_old) / d_old).max(). Adjusting data. ```. ```python. sc.pp.combat(adata, key='384plate', covariates=['age_group']). ```. ```pytb. Standardizing Data across genes. Found 34 batches. Found 1 categorical variables:. 	age_group. Found 0 numerical variables:. 	. ---------------------------------------------------------------------------. LinAlgError Traceback (most recent call last). <timed eval> in <module>. ~/.local/lib/python3.8/site-packages/scanpy/preprocessing/_combat.py in combat(adata, key, covariates, inplace). 204 # standardize across genes using a pooled variance estimator. 205 logg.info(""Standardizing Data across genes.\n""). --> 206 s_data, design, var_pooled, stand_mean = _standardize_data(model, data, key). 207 . 208 # fitting the parameters on the standardized data. ~/.local/lib/python3.8/site-packages/scanpy/preprocessing/_combat.py in _standardize_data(model, data, batch_key). 102 . 103 # compute pooled variance estimator. --> 104 B_hat = np.dot(np.dot(la.inv(np.dot(design.T, design)), design.T), data.T). 105 grand_mean = np.dot((n_batches / n_array).T, B_hat[:n_batch, :]). 106 var_pooled = (data - np.dot(design, B_hat).T) ** 2. <__array_function__ internals> in inv(*args, **kwargs). ~/.local/lib/python3.8/site-packages/numpy/linalg/linalg.py in inv(a). 544 signature = 'D->D' if isComplexType(t) else 'd->d'. 545 extobj = get_linalg_error_extobj(_raise_linalgerror_sing",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1606
https://github.com/scverse/scanpy/issues/1606:1673,modifiability,pac,packages,1673,"ories.tolist(). ```. ```pytb. ['Old', 'YoungAdult', 'Pediatric', 'Fetal', 'NewBorn']. ```. ```python. sc.pp.combat(adata, key='384plate', covariates=['sex']). ```. ```pytb. Standardizing Data across genes. Found 34 batches. Found 1 categorical variables:. 	sex. Found 0 numerical variables:. 	. Fitting L/S model and finding priors. Finding parametric adjustments. /home/sinhar/.local/lib/python3.8/site-packages/scanpy/preprocessing/_combat.py:340: RuntimeWarning: divide by zero encountered in true_divide. (abs(g_new - g_old) / g_old).max(), (abs(d_new - d_old) / d_old).max(). Adjusting data. ```. ```python. sc.pp.combat(adata, key='384plate', covariates=['age_group']). ```. ```pytb. Standardizing Data across genes. Found 34 batches. Found 1 categorical variables:. 	age_group. Found 0 numerical variables:. 	. ---------------------------------------------------------------------------. LinAlgError Traceback (most recent call last). <timed eval> in <module>. ~/.local/lib/python3.8/site-packages/scanpy/preprocessing/_combat.py in combat(adata, key, covariates, inplace). 204 # standardize across genes using a pooled variance estimator. 205 logg.info(""Standardizing Data across genes.\n""). --> 206 s_data, design, var_pooled, stand_mean = _standardize_data(model, data, key). 207 . 208 # fitting the parameters on the standardized data. ~/.local/lib/python3.8/site-packages/scanpy/preprocessing/_combat.py in _standardize_data(model, data, batch_key). 102 . 103 # compute pooled variance estimator. --> 104 B_hat = np.dot(np.dot(la.inv(np.dot(design.T, design)), design.T), data.T). 105 grand_mean = np.dot((n_batches / n_array).T, B_hat[:n_batch, :]). 106 var_pooled = (data - np.dot(design, B_hat).T) ** 2. <__array_function__ internals> in inv(*args, **kwargs). ~/.local/lib/python3.8/site-packages/numpy/linalg/linalg.py in inv(a). 544 signature = 'D->D' if isComplexType(t) else 'd->d'. 545 extobj = get_linalg_error_extobj(_raise_linalgerror_singular). --> 546 ainv = _umath_linalg.in",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1606
https://github.com/scverse/scanpy/issues/1606:1987,modifiability,paramet,parameters,1987,"d finding priors. Finding parametric adjustments. /home/sinhar/.local/lib/python3.8/site-packages/scanpy/preprocessing/_combat.py:340: RuntimeWarning: divide by zero encountered in true_divide. (abs(g_new - g_old) / g_old).max(), (abs(d_new - d_old) / d_old).max(). Adjusting data. ```. ```python. sc.pp.combat(adata, key='384plate', covariates=['age_group']). ```. ```pytb. Standardizing Data across genes. Found 34 batches. Found 1 categorical variables:. 	age_group. Found 0 numerical variables:. 	. ---------------------------------------------------------------------------. LinAlgError Traceback (most recent call last). <timed eval> in <module>. ~/.local/lib/python3.8/site-packages/scanpy/preprocessing/_combat.py in combat(adata, key, covariates, inplace). 204 # standardize across genes using a pooled variance estimator. 205 logg.info(""Standardizing Data across genes.\n""). --> 206 s_data, design, var_pooled, stand_mean = _standardize_data(model, data, key). 207 . 208 # fitting the parameters on the standardized data. ~/.local/lib/python3.8/site-packages/scanpy/preprocessing/_combat.py in _standardize_data(model, data, batch_key). 102 . 103 # compute pooled variance estimator. --> 104 B_hat = np.dot(np.dot(la.inv(np.dot(design.T, design)), design.T), data.T). 105 grand_mean = np.dot((n_batches / n_array).T, B_hat[:n_batch, :]). 106 var_pooled = (data - np.dot(design, B_hat).T) ** 2. <__array_function__ internals> in inv(*args, **kwargs). ~/.local/lib/python3.8/site-packages/numpy/linalg/linalg.py in inv(a). 544 signature = 'D->D' if isComplexType(t) else 'd->d'. 545 extobj = get_linalg_error_extobj(_raise_linalgerror_singular). --> 546 ainv = _umath_linalg.inv(a, signature=signature, extobj=extobj). 547 return wrap(ainv.astype(result_t, copy=False)). 548 . ~/.local/lib/python3.8/site-packages/numpy/linalg/linalg.py in _raise_linalgerror_singular(err, flag). 86 . 87 def _raise_linalgerror_singular(err, flag):. ---> 88 raise LinAlgError(""Singular matrix""). 89 . 90 def _",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1606
https://github.com/scverse/scanpy/issues/1606:2052,modifiability,pac,packages,2052,"local/lib/python3.8/site-packages/scanpy/preprocessing/_combat.py:340: RuntimeWarning: divide by zero encountered in true_divide. (abs(g_new - g_old) / g_old).max(), (abs(d_new - d_old) / d_old).max(). Adjusting data. ```. ```python. sc.pp.combat(adata, key='384plate', covariates=['age_group']). ```. ```pytb. Standardizing Data across genes. Found 34 batches. Found 1 categorical variables:. 	age_group. Found 0 numerical variables:. 	. ---------------------------------------------------------------------------. LinAlgError Traceback (most recent call last). <timed eval> in <module>. ~/.local/lib/python3.8/site-packages/scanpy/preprocessing/_combat.py in combat(adata, key, covariates, inplace). 204 # standardize across genes using a pooled variance estimator. 205 logg.info(""Standardizing Data across genes.\n""). --> 206 s_data, design, var_pooled, stand_mean = _standardize_data(model, data, key). 207 . 208 # fitting the parameters on the standardized data. ~/.local/lib/python3.8/site-packages/scanpy/preprocessing/_combat.py in _standardize_data(model, data, batch_key). 102 . 103 # compute pooled variance estimator. --> 104 B_hat = np.dot(np.dot(la.inv(np.dot(design.T, design)), design.T), data.T). 105 grand_mean = np.dot((n_batches / n_array).T, B_hat[:n_batch, :]). 106 var_pooled = (data - np.dot(design, B_hat).T) ** 2. <__array_function__ internals> in inv(*args, **kwargs). ~/.local/lib/python3.8/site-packages/numpy/linalg/linalg.py in inv(a). 544 signature = 'D->D' if isComplexType(t) else 'd->d'. 545 extobj = get_linalg_error_extobj(_raise_linalgerror_singular). --> 546 ainv = _umath_linalg.inv(a, signature=signature, extobj=extobj). 547 return wrap(ainv.astype(result_t, copy=False)). 548 . ~/.local/lib/python3.8/site-packages/numpy/linalg/linalg.py in _raise_linalgerror_singular(err, flag). 86 . 87 def _raise_linalgerror_singular(err, flag):. ---> 88 raise LinAlgError(""Singular matrix""). 89 . 90 def _raise_linalgerror_nonposdef(err, flag):. LinAlgError: Singular m",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1606
https://github.com/scverse/scanpy/issues/1606:2480,modifiability,pac,packages,2480,". Found 34 batches. Found 1 categorical variables:. 	age_group. Found 0 numerical variables:. 	. ---------------------------------------------------------------------------. LinAlgError Traceback (most recent call last). <timed eval> in <module>. ~/.local/lib/python3.8/site-packages/scanpy/preprocessing/_combat.py in combat(adata, key, covariates, inplace). 204 # standardize across genes using a pooled variance estimator. 205 logg.info(""Standardizing Data across genes.\n""). --> 206 s_data, design, var_pooled, stand_mean = _standardize_data(model, data, key). 207 . 208 # fitting the parameters on the standardized data. ~/.local/lib/python3.8/site-packages/scanpy/preprocessing/_combat.py in _standardize_data(model, data, batch_key). 102 . 103 # compute pooled variance estimator. --> 104 B_hat = np.dot(np.dot(la.inv(np.dot(design.T, design)), design.T), data.T). 105 grand_mean = np.dot((n_batches / n_array).T, B_hat[:n_batch, :]). 106 var_pooled = (data - np.dot(design, B_hat).T) ** 2. <__array_function__ internals> in inv(*args, **kwargs). ~/.local/lib/python3.8/site-packages/numpy/linalg/linalg.py in inv(a). 544 signature = 'D->D' if isComplexType(t) else 'd->d'. 545 extobj = get_linalg_error_extobj(_raise_linalgerror_singular). --> 546 ainv = _umath_linalg.inv(a, signature=signature, extobj=extobj). 547 return wrap(ainv.astype(result_t, copy=False)). 548 . ~/.local/lib/python3.8/site-packages/numpy/linalg/linalg.py in _raise_linalgerror_singular(err, flag). 86 . 87 def _raise_linalgerror_singular(err, flag):. ---> 88 raise LinAlgError(""Singular matrix""). 89 . 90 def _raise_linalgerror_nonposdef(err, flag):. LinAlgError: Singular matrix. ```. #### Versions. <details>. [Paste the output of scanpy.logging.print_versions() leaving a blank line after the details tag]. </details>. scanpy==1.7.0rc2.dev7+g57ec8a7e anndata==0.7.3 umap==0.4.6 numpy==1.19.5 scipy==1.6.0 pandas==1.2.1 scikit-learn==0.24.1 statsmodels==0.12.1 python-igraph==0.8.3 louvain==0.7.0 leidenalg==0.8.3.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1606
https://github.com/scverse/scanpy/issues/1606:2805,modifiability,pac,packages,2805,". Found 34 batches. Found 1 categorical variables:. 	age_group. Found 0 numerical variables:. 	. ---------------------------------------------------------------------------. LinAlgError Traceback (most recent call last). <timed eval> in <module>. ~/.local/lib/python3.8/site-packages/scanpy/preprocessing/_combat.py in combat(adata, key, covariates, inplace). 204 # standardize across genes using a pooled variance estimator. 205 logg.info(""Standardizing Data across genes.\n""). --> 206 s_data, design, var_pooled, stand_mean = _standardize_data(model, data, key). 207 . 208 # fitting the parameters on the standardized data. ~/.local/lib/python3.8/site-packages/scanpy/preprocessing/_combat.py in _standardize_data(model, data, batch_key). 102 . 103 # compute pooled variance estimator. --> 104 B_hat = np.dot(np.dot(la.inv(np.dot(design.T, design)), design.T), data.T). 105 grand_mean = np.dot((n_batches / n_array).T, B_hat[:n_batch, :]). 106 var_pooled = (data - np.dot(design, B_hat).T) ** 2. <__array_function__ internals> in inv(*args, **kwargs). ~/.local/lib/python3.8/site-packages/numpy/linalg/linalg.py in inv(a). 544 signature = 'D->D' if isComplexType(t) else 'd->d'. 545 extobj = get_linalg_error_extobj(_raise_linalgerror_singular). --> 546 ainv = _umath_linalg.inv(a, signature=signature, extobj=extobj). 547 return wrap(ainv.astype(result_t, copy=False)). 548 . ~/.local/lib/python3.8/site-packages/numpy/linalg/linalg.py in _raise_linalgerror_singular(err, flag). 86 . 87 def _raise_linalgerror_singular(err, flag):. ---> 88 raise LinAlgError(""Singular matrix""). 89 . 90 def _raise_linalgerror_nonposdef(err, flag):. LinAlgError: Singular matrix. ```. #### Versions. <details>. [Paste the output of scanpy.logging.print_versions() leaving a blank line after the details tag]. </details>. scanpy==1.7.0rc2.dev7+g57ec8a7e anndata==0.7.3 umap==0.4.6 numpy==1.19.5 scipy==1.6.0 pandas==1.2.1 scikit-learn==0.24.1 statsmodels==0.12.1 python-igraph==0.8.3 louvain==0.7.0 leidenalg==0.8.3.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1606
https://github.com/scverse/scanpy/issues/1606:3073,modifiability,Version,Versions,3073,". Found 34 batches. Found 1 categorical variables:. 	age_group. Found 0 numerical variables:. 	. ---------------------------------------------------------------------------. LinAlgError Traceback (most recent call last). <timed eval> in <module>. ~/.local/lib/python3.8/site-packages/scanpy/preprocessing/_combat.py in combat(adata, key, covariates, inplace). 204 # standardize across genes using a pooled variance estimator. 205 logg.info(""Standardizing Data across genes.\n""). --> 206 s_data, design, var_pooled, stand_mean = _standardize_data(model, data, key). 207 . 208 # fitting the parameters on the standardized data. ~/.local/lib/python3.8/site-packages/scanpy/preprocessing/_combat.py in _standardize_data(model, data, batch_key). 102 . 103 # compute pooled variance estimator. --> 104 B_hat = np.dot(np.dot(la.inv(np.dot(design.T, design)), design.T), data.T). 105 grand_mean = np.dot((n_batches / n_array).T, B_hat[:n_batch, :]). 106 var_pooled = (data - np.dot(design, B_hat).T) ** 2. <__array_function__ internals> in inv(*args, **kwargs). ~/.local/lib/python3.8/site-packages/numpy/linalg/linalg.py in inv(a). 544 signature = 'D->D' if isComplexType(t) else 'd->d'. 545 extobj = get_linalg_error_extobj(_raise_linalgerror_singular). --> 546 ainv = _umath_linalg.inv(a, signature=signature, extobj=extobj). 547 return wrap(ainv.astype(result_t, copy=False)). 548 . ~/.local/lib/python3.8/site-packages/numpy/linalg/linalg.py in _raise_linalgerror_singular(err, flag). 86 . 87 def _raise_linalgerror_singular(err, flag):. ---> 88 raise LinAlgError(""Singular matrix""). 89 . 90 def _raise_linalgerror_nonposdef(err, flag):. LinAlgError: Singular matrix. ```. #### Versions. <details>. [Paste the output of scanpy.logging.print_versions() leaving a blank line after the details tag]. </details>. scanpy==1.7.0rc2.dev7+g57ec8a7e anndata==0.7.3 umap==0.4.6 numpy==1.19.5 scipy==1.6.0 pandas==1.2.1 scikit-learn==0.24.1 statsmodels==0.12.1 python-igraph==0.8.3 louvain==0.7.0 leidenalg==0.8.3.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1606
https://github.com/scverse/scanpy/issues/1606:892,performance,batch,batches,892,"Combat works for some covariates yet fails for others; - [ x] I have checked that this issue has not already been reported. - [ x] I have confirmed this bug exists on the latest version of scanpy. - [ x] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. ### Minimal code sample (that we can copy&paste without having any data). ```python. adata.obs['sex'].cat.categories.tolist(). ```. ```pytb. ['F', 'M', 'U']. ```. ```python. adata.obs['age_groups'].cat.categories.tolist(). ```. ```pytb. ['Old', 'YoungAdult', 'Pediatric', 'Fetal', 'NewBorn']. ```. ```python. sc.pp.combat(adata, key='384plate', covariates=['sex']). ```. ```pytb. Standardizing Data across genes. Found 34 batches. Found 1 categorical variables:. 	sex. Found 0 numerical variables:. 	. Fitting L/S model and finding priors. Finding parametric adjustments. /home/sinhar/.local/lib/python3.8/site-packages/scanpy/preprocessing/_combat.py:340: RuntimeWarning: divide by zero encountered in true_divide. (abs(g_new - g_old) / g_old).max(), (abs(d_new - d_old) / d_old).max(). Adjusting data. ```. ```python. sc.pp.combat(adata, key='384plate', covariates=['age_group']). ```. ```pytb. Standardizing Data across genes. Found 34 batches. Found 1 categorical variables:. 	age_group. Found 0 numerical variables:. 	. ---------------------------------------------------------------------------. LinAlgError Traceback (most recent call last). <timed eval> in <module>. ~/.local/lib/python3.8/site-packages/scanpy/preprocessing/_combat.py in combat(adata, key, covariates, inplace). 204 # standardize across genes using a pooled variance estimator. 205 logg.info(""Standardizing Data across genes.\n""). --> 206 s_data, design, var_pooled, stand_mean = _standardize_data(model, data, key). 207 . 208 # fitting the parameters on",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1606
https://github.com/scverse/scanpy/issues/1606:1409,performance,batch,batches,1409," the necessary information for us to reproduce your bug. ### Minimal code sample (that we can copy&paste without having any data). ```python. adata.obs['sex'].cat.categories.tolist(). ```. ```pytb. ['F', 'M', 'U']. ```. ```python. adata.obs['age_groups'].cat.categories.tolist(). ```. ```pytb. ['Old', 'YoungAdult', 'Pediatric', 'Fetal', 'NewBorn']. ```. ```python. sc.pp.combat(adata, key='384plate', covariates=['sex']). ```. ```pytb. Standardizing Data across genes. Found 34 batches. Found 1 categorical variables:. 	sex. Found 0 numerical variables:. 	. Fitting L/S model and finding priors. Finding parametric adjustments. /home/sinhar/.local/lib/python3.8/site-packages/scanpy/preprocessing/_combat.py:340: RuntimeWarning: divide by zero encountered in true_divide. (abs(g_new - g_old) / g_old).max(), (abs(d_new - d_old) / d_old).max(). Adjusting data. ```. ```python. sc.pp.combat(adata, key='384plate', covariates=['age_group']). ```. ```pytb. Standardizing Data across genes. Found 34 batches. Found 1 categorical variables:. 	age_group. Found 0 numerical variables:. 	. ---------------------------------------------------------------------------. LinAlgError Traceback (most recent call last). <timed eval> in <module>. ~/.local/lib/python3.8/site-packages/scanpy/preprocessing/_combat.py in combat(adata, key, covariates, inplace). 204 # standardize across genes using a pooled variance estimator. 205 logg.info(""Standardizing Data across genes.\n""). --> 206 s_data, design, var_pooled, stand_mean = _standardize_data(model, data, key). 207 . 208 # fitting the parameters on the standardized data. ~/.local/lib/python3.8/site-packages/scanpy/preprocessing/_combat.py in _standardize_data(model, data, batch_key). 102 . 103 # compute pooled variance estimator. --> 104 B_hat = np.dot(np.dot(la.inv(np.dot(design.T, design)), design.T), data.T). 105 grand_mean = np.dot((n_batches / n_array).T, B_hat[:n_batch, :]). 106 var_pooled = (data - np.dot(design, B_hat).T) ** 2. <__array_function",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1606
https://github.com/scverse/scanpy/issues/1606:1620,performance,time,timed,1620,"U']. ```. ```python. adata.obs['age_groups'].cat.categories.tolist(). ```. ```pytb. ['Old', 'YoungAdult', 'Pediatric', 'Fetal', 'NewBorn']. ```. ```python. sc.pp.combat(adata, key='384plate', covariates=['sex']). ```. ```pytb. Standardizing Data across genes. Found 34 batches. Found 1 categorical variables:. 	sex. Found 0 numerical variables:. 	. Fitting L/S model and finding priors. Finding parametric adjustments. /home/sinhar/.local/lib/python3.8/site-packages/scanpy/preprocessing/_combat.py:340: RuntimeWarning: divide by zero encountered in true_divide. (abs(g_new - g_old) / g_old).max(), (abs(d_new - d_old) / d_old).max(). Adjusting data. ```. ```python. sc.pp.combat(adata, key='384plate', covariates=['age_group']). ```. ```pytb. Standardizing Data across genes. Found 34 batches. Found 1 categorical variables:. 	age_group. Found 0 numerical variables:. 	. ---------------------------------------------------------------------------. LinAlgError Traceback (most recent call last). <timed eval> in <module>. ~/.local/lib/python3.8/site-packages/scanpy/preprocessing/_combat.py in combat(adata, key, covariates, inplace). 204 # standardize across genes using a pooled variance estimator. 205 logg.info(""Standardizing Data across genes.\n""). --> 206 s_data, design, var_pooled, stand_mean = _standardize_data(model, data, key). 207 . 208 # fitting the parameters on the standardized data. ~/.local/lib/python3.8/site-packages/scanpy/preprocessing/_combat.py in _standardize_data(model, data, batch_key). 102 . 103 # compute pooled variance estimator. --> 104 B_hat = np.dot(np.dot(la.inv(np.dot(design.T, design)), design.T), data.T). 105 grand_mean = np.dot((n_batches / n_array).T, B_hat[:n_batch, :]). 106 var_pooled = (data - np.dot(design, B_hat).T) ** 2. <__array_function__ internals> in inv(*args, **kwargs). ~/.local/lib/python3.8/site-packages/numpy/linalg/linalg.py in inv(a). 544 signature = 'D->D' if isComplexType(t) else 'd->d'. 545 extobj = get_linalg_error_extobj(_raise_",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1606
https://github.com/scverse/scanpy/issues/1606:37,reliability,fail,fails,37,"Combat works for some covariates yet fails for others; - [ x] I have checked that this issue has not already been reported. - [ x] I have confirmed this bug exists on the latest version of scanpy. - [ x] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. ### Minimal code sample (that we can copy&paste without having any data). ```python. adata.obs['sex'].cat.categories.tolist(). ```. ```pytb. ['F', 'M', 'U']. ```. ```python. adata.obs['age_groups'].cat.categories.tolist(). ```. ```pytb. ['Old', 'YoungAdult', 'Pediatric', 'Fetal', 'NewBorn']. ```. ```python. sc.pp.combat(adata, key='384plate', covariates=['sex']). ```. ```pytb. Standardizing Data across genes. Found 34 batches. Found 1 categorical variables:. 	sex. Found 0 numerical variables:. 	. Fitting L/S model and finding priors. Finding parametric adjustments. /home/sinhar/.local/lib/python3.8/site-packages/scanpy/preprocessing/_combat.py:340: RuntimeWarning: divide by zero encountered in true_divide. (abs(g_new - g_old) / g_old).max(), (abs(d_new - d_old) / d_old).max(). Adjusting data. ```. ```python. sc.pp.combat(adata, key='384plate', covariates=['age_group']). ```. ```pytb. Standardizing Data across genes. Found 34 batches. Found 1 categorical variables:. 	age_group. Found 0 numerical variables:. 	. ---------------------------------------------------------------------------. LinAlgError Traceback (most recent call last). <timed eval> in <module>. ~/.local/lib/python3.8/site-packages/scanpy/preprocessing/_combat.py in combat(adata, key, covariates, inplace). 204 # standardize across genes using a pooled variance estimator. 205 logg.info(""Standardizing Data across genes.\n""). --> 206 s_data, design, var_pooled, stand_mean = _standardize_data(model, data, key). 207 . 208 # fitting the parameters on",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1606
https://github.com/scverse/scanpy/issues/1606:1636,safety,modul,module,1636,"hon. adata.obs['age_groups'].cat.categories.tolist(). ```. ```pytb. ['Old', 'YoungAdult', 'Pediatric', 'Fetal', 'NewBorn']. ```. ```python. sc.pp.combat(adata, key='384plate', covariates=['sex']). ```. ```pytb. Standardizing Data across genes. Found 34 batches. Found 1 categorical variables:. 	sex. Found 0 numerical variables:. 	. Fitting L/S model and finding priors. Finding parametric adjustments. /home/sinhar/.local/lib/python3.8/site-packages/scanpy/preprocessing/_combat.py:340: RuntimeWarning: divide by zero encountered in true_divide. (abs(g_new - g_old) / g_old).max(), (abs(d_new - d_old) / d_old).max(). Adjusting data. ```. ```python. sc.pp.combat(adata, key='384plate', covariates=['age_group']). ```. ```pytb. Standardizing Data across genes. Found 34 batches. Found 1 categorical variables:. 	age_group. Found 0 numerical variables:. 	. ---------------------------------------------------------------------------. LinAlgError Traceback (most recent call last). <timed eval> in <module>. ~/.local/lib/python3.8/site-packages/scanpy/preprocessing/_combat.py in combat(adata, key, covariates, inplace). 204 # standardize across genes using a pooled variance estimator. 205 logg.info(""Standardizing Data across genes.\n""). --> 206 s_data, design, var_pooled, stand_mean = _standardize_data(model, data, key). 207 . 208 # fitting the parameters on the standardized data. ~/.local/lib/python3.8/site-packages/scanpy/preprocessing/_combat.py in _standardize_data(model, data, batch_key). 102 . 103 # compute pooled variance estimator. --> 104 B_hat = np.dot(np.dot(la.inv(np.dot(design.T, design)), design.T), data.T). 105 grand_mean = np.dot((n_batches / n_array).T, B_hat[:n_batch, :]). 106 var_pooled = (data - np.dot(design, B_hat).T) ** 2. <__array_function__ internals> in inv(*args, **kwargs). ~/.local/lib/python3.8/site-packages/numpy/linalg/linalg.py in inv(a). 544 signature = 'D->D' if isComplexType(t) else 'd->d'. 545 extobj = get_linalg_error_extobj(_raise_linalgerror_sing",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1606
https://github.com/scverse/scanpy/issues/1606:1828,safety,log,logg,1828,"x']). ```. ```pytb. Standardizing Data across genes. Found 34 batches. Found 1 categorical variables:. 	sex. Found 0 numerical variables:. 	. Fitting L/S model and finding priors. Finding parametric adjustments. /home/sinhar/.local/lib/python3.8/site-packages/scanpy/preprocessing/_combat.py:340: RuntimeWarning: divide by zero encountered in true_divide. (abs(g_new - g_old) / g_old).max(), (abs(d_new - d_old) / d_old).max(). Adjusting data. ```. ```python. sc.pp.combat(adata, key='384plate', covariates=['age_group']). ```. ```pytb. Standardizing Data across genes. Found 34 batches. Found 1 categorical variables:. 	age_group. Found 0 numerical variables:. 	. ---------------------------------------------------------------------------. LinAlgError Traceback (most recent call last). <timed eval> in <module>. ~/.local/lib/python3.8/site-packages/scanpy/preprocessing/_combat.py in combat(adata, key, covariates, inplace). 204 # standardize across genes using a pooled variance estimator. 205 logg.info(""Standardizing Data across genes.\n""). --> 206 s_data, design, var_pooled, stand_mean = _standardize_data(model, data, key). 207 . 208 # fitting the parameters on the standardized data. ~/.local/lib/python3.8/site-packages/scanpy/preprocessing/_combat.py in _standardize_data(model, data, batch_key). 102 . 103 # compute pooled variance estimator. --> 104 B_hat = np.dot(np.dot(la.inv(np.dot(design.T, design)), design.T), data.T). 105 grand_mean = np.dot((n_batches / n_array).T, B_hat[:n_batch, :]). 106 var_pooled = (data - np.dot(design, B_hat).T) ** 2. <__array_function__ internals> in inv(*args, **kwargs). ~/.local/lib/python3.8/site-packages/numpy/linalg/linalg.py in inv(a). 544 signature = 'D->D' if isComplexType(t) else 'd->d'. 545 extobj = get_linalg_error_extobj(_raise_linalgerror_singular). --> 546 ainv = _umath_linalg.inv(a, signature=signature, extobj=extobj). 547 return wrap(ainv.astype(result_t, copy=False)). 548 . ~/.local/lib/python3.8/site-packages/numpy/linalg/lin",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1606
https://github.com/scverse/scanpy/issues/1606:3122,safety,log,logging,3122,". Found 34 batches. Found 1 categorical variables:. 	age_group. Found 0 numerical variables:. 	. ---------------------------------------------------------------------------. LinAlgError Traceback (most recent call last). <timed eval> in <module>. ~/.local/lib/python3.8/site-packages/scanpy/preprocessing/_combat.py in combat(adata, key, covariates, inplace). 204 # standardize across genes using a pooled variance estimator. 205 logg.info(""Standardizing Data across genes.\n""). --> 206 s_data, design, var_pooled, stand_mean = _standardize_data(model, data, key). 207 . 208 # fitting the parameters on the standardized data. ~/.local/lib/python3.8/site-packages/scanpy/preprocessing/_combat.py in _standardize_data(model, data, batch_key). 102 . 103 # compute pooled variance estimator. --> 104 B_hat = np.dot(np.dot(la.inv(np.dot(design.T, design)), design.T), data.T). 105 grand_mean = np.dot((n_batches / n_array).T, B_hat[:n_batch, :]). 106 var_pooled = (data - np.dot(design, B_hat).T) ** 2. <__array_function__ internals> in inv(*args, **kwargs). ~/.local/lib/python3.8/site-packages/numpy/linalg/linalg.py in inv(a). 544 signature = 'D->D' if isComplexType(t) else 'd->d'. 545 extobj = get_linalg_error_extobj(_raise_linalgerror_singular). --> 546 ainv = _umath_linalg.inv(a, signature=signature, extobj=extobj). 547 return wrap(ainv.astype(result_t, copy=False)). 548 . ~/.local/lib/python3.8/site-packages/numpy/linalg/linalg.py in _raise_linalgerror_singular(err, flag). 86 . 87 def _raise_linalgerror_singular(err, flag):. ---> 88 raise LinAlgError(""Singular matrix""). 89 . 90 def _raise_linalgerror_nonposdef(err, flag):. LinAlgError: Singular matrix. ```. #### Versions. <details>. [Paste the output of scanpy.logging.print_versions() leaving a blank line after the details tag]. </details>. scanpy==1.7.0rc2.dev7+g57ec8a7e anndata==0.7.3 umap==0.4.6 numpy==1.19.5 scipy==1.6.0 pandas==1.2.1 scikit-learn==0.24.1 statsmodels==0.12.1 python-igraph==0.8.3 louvain==0.7.0 leidenalg==0.8.3.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1606
https://github.com/scverse/scanpy/issues/1606:984,security,model,model,984,"Combat works for some covariates yet fails for others; - [ x] I have checked that this issue has not already been reported. - [ x] I have confirmed this bug exists on the latest version of scanpy. - [ x] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. ### Minimal code sample (that we can copy&paste without having any data). ```python. adata.obs['sex'].cat.categories.tolist(). ```. ```pytb. ['F', 'M', 'U']. ```. ```python. adata.obs['age_groups'].cat.categories.tolist(). ```. ```pytb. ['Old', 'YoungAdult', 'Pediatric', 'Fetal', 'NewBorn']. ```. ```python. sc.pp.combat(adata, key='384plate', covariates=['sex']). ```. ```pytb. Standardizing Data across genes. Found 34 batches. Found 1 categorical variables:. 	sex. Found 0 numerical variables:. 	. Fitting L/S model and finding priors. Finding parametric adjustments. /home/sinhar/.local/lib/python3.8/site-packages/scanpy/preprocessing/_combat.py:340: RuntimeWarning: divide by zero encountered in true_divide. (abs(g_new - g_old) / g_old).max(), (abs(d_new - d_old) / d_old).max(). Adjusting data. ```. ```python. sc.pp.combat(adata, key='384plate', covariates=['age_group']). ```. ```pytb. Standardizing Data across genes. Found 34 batches. Found 1 categorical variables:. 	age_group. Found 0 numerical variables:. 	. ---------------------------------------------------------------------------. LinAlgError Traceback (most recent call last). <timed eval> in <module>. ~/.local/lib/python3.8/site-packages/scanpy/preprocessing/_combat.py in combat(adata, key, covariates, inplace). 204 # standardize across genes using a pooled variance estimator. 205 logg.info(""Standardizing Data across genes.\n""). --> 206 s_data, design, var_pooled, stand_mean = _standardize_data(model, data, key). 207 . 208 # fitting the parameters on",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1606
https://github.com/scverse/scanpy/issues/1606:1828,security,log,logg,1828,"x']). ```. ```pytb. Standardizing Data across genes. Found 34 batches. Found 1 categorical variables:. 	sex. Found 0 numerical variables:. 	. Fitting L/S model and finding priors. Finding parametric adjustments. /home/sinhar/.local/lib/python3.8/site-packages/scanpy/preprocessing/_combat.py:340: RuntimeWarning: divide by zero encountered in true_divide. (abs(g_new - g_old) / g_old).max(), (abs(d_new - d_old) / d_old).max(). Adjusting data. ```. ```python. sc.pp.combat(adata, key='384plate', covariates=['age_group']). ```. ```pytb. Standardizing Data across genes. Found 34 batches. Found 1 categorical variables:. 	age_group. Found 0 numerical variables:. 	. ---------------------------------------------------------------------------. LinAlgError Traceback (most recent call last). <timed eval> in <module>. ~/.local/lib/python3.8/site-packages/scanpy/preprocessing/_combat.py in combat(adata, key, covariates, inplace). 204 # standardize across genes using a pooled variance estimator. 205 logg.info(""Standardizing Data across genes.\n""). --> 206 s_data, design, var_pooled, stand_mean = _standardize_data(model, data, key). 207 . 208 # fitting the parameters on the standardized data. ~/.local/lib/python3.8/site-packages/scanpy/preprocessing/_combat.py in _standardize_data(model, data, batch_key). 102 . 103 # compute pooled variance estimator. --> 104 B_hat = np.dot(np.dot(la.inv(np.dot(design.T, design)), design.T), data.T). 105 grand_mean = np.dot((n_batches / n_array).T, B_hat[:n_batch, :]). 106 var_pooled = (data - np.dot(design, B_hat).T) ** 2. <__array_function__ internals> in inv(*args, **kwargs). ~/.local/lib/python3.8/site-packages/numpy/linalg/linalg.py in inv(a). 544 signature = 'D->D' if isComplexType(t) else 'd->d'. 545 extobj = get_linalg_error_extobj(_raise_linalgerror_singular). --> 546 ainv = _umath_linalg.inv(a, signature=signature, extobj=extobj). 547 return wrap(ainv.astype(result_t, copy=False)). 548 . ~/.local/lib/python3.8/site-packages/numpy/linalg/lin",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1606
https://github.com/scverse/scanpy/issues/1606:1944,security,model,model,1944,"numerical variables:. 	. Fitting L/S model and finding priors. Finding parametric adjustments. /home/sinhar/.local/lib/python3.8/site-packages/scanpy/preprocessing/_combat.py:340: RuntimeWarning: divide by zero encountered in true_divide. (abs(g_new - g_old) / g_old).max(), (abs(d_new - d_old) / d_old).max(). Adjusting data. ```. ```python. sc.pp.combat(adata, key='384plate', covariates=['age_group']). ```. ```pytb. Standardizing Data across genes. Found 34 batches. Found 1 categorical variables:. 	age_group. Found 0 numerical variables:. 	. ---------------------------------------------------------------------------. LinAlgError Traceback (most recent call last). <timed eval> in <module>. ~/.local/lib/python3.8/site-packages/scanpy/preprocessing/_combat.py in combat(adata, key, covariates, inplace). 204 # standardize across genes using a pooled variance estimator. 205 logg.info(""Standardizing Data across genes.\n""). --> 206 s_data, design, var_pooled, stand_mean = _standardize_data(model, data, key). 207 . 208 # fitting the parameters on the standardized data. ~/.local/lib/python3.8/site-packages/scanpy/preprocessing/_combat.py in _standardize_data(model, data, batch_key). 102 . 103 # compute pooled variance estimator. --> 104 B_hat = np.dot(np.dot(la.inv(np.dot(design.T, design)), design.T), data.T). 105 grand_mean = np.dot((n_batches / n_array).T, B_hat[:n_batch, :]). 106 var_pooled = (data - np.dot(design, B_hat).T) ** 2. <__array_function__ internals> in inv(*args, **kwargs). ~/.local/lib/python3.8/site-packages/numpy/linalg/linalg.py in inv(a). 544 signature = 'D->D' if isComplexType(t) else 'd->d'. 545 extobj = get_linalg_error_extobj(_raise_linalgerror_singular). --> 546 ainv = _umath_linalg.inv(a, signature=signature, extobj=extobj). 547 return wrap(ainv.astype(result_t, copy=False)). 548 . ~/.local/lib/python3.8/site-packages/numpy/linalg/linalg.py in _raise_linalgerror_singular(err, flag). 86 . 87 def _raise_linalgerror_singular(err, flag):. ---> 88 raise ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1606
https://github.com/scverse/scanpy/issues/1606:2114,security,model,model,2114,"t.py:340: RuntimeWarning: divide by zero encountered in true_divide. (abs(g_new - g_old) / g_old).max(), (abs(d_new - d_old) / d_old).max(). Adjusting data. ```. ```python. sc.pp.combat(adata, key='384plate', covariates=['age_group']). ```. ```pytb. Standardizing Data across genes. Found 34 batches. Found 1 categorical variables:. 	age_group. Found 0 numerical variables:. 	. ---------------------------------------------------------------------------. LinAlgError Traceback (most recent call last). <timed eval> in <module>. ~/.local/lib/python3.8/site-packages/scanpy/preprocessing/_combat.py in combat(adata, key, covariates, inplace). 204 # standardize across genes using a pooled variance estimator. 205 logg.info(""Standardizing Data across genes.\n""). --> 206 s_data, design, var_pooled, stand_mean = _standardize_data(model, data, key). 207 . 208 # fitting the parameters on the standardized data. ~/.local/lib/python3.8/site-packages/scanpy/preprocessing/_combat.py in _standardize_data(model, data, batch_key). 102 . 103 # compute pooled variance estimator. --> 104 B_hat = np.dot(np.dot(la.inv(np.dot(design.T, design)), design.T), data.T). 105 grand_mean = np.dot((n_batches / n_array).T, B_hat[:n_batch, :]). 106 var_pooled = (data - np.dot(design, B_hat).T) ** 2. <__array_function__ internals> in inv(*args, **kwargs). ~/.local/lib/python3.8/site-packages/numpy/linalg/linalg.py in inv(a). 544 signature = 'D->D' if isComplexType(t) else 'd->d'. 545 extobj = get_linalg_error_extobj(_raise_linalgerror_singular). --> 546 ainv = _umath_linalg.inv(a, signature=signature, extobj=extobj). 547 return wrap(ainv.astype(result_t, copy=False)). 548 . ~/.local/lib/python3.8/site-packages/numpy/linalg/linalg.py in _raise_linalgerror_singular(err, flag). 86 . 87 def _raise_linalgerror_singular(err, flag):. ---> 88 raise LinAlgError(""Singular matrix""). 89 . 90 def _raise_linalgerror_nonposdef(err, flag):. LinAlgError: Singular matrix. ```. #### Versions. <details>. [Paste the output of sc",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1606
https://github.com/scverse/scanpy/issues/1606:2527,security,sign,signature,2527,". Found 34 batches. Found 1 categorical variables:. 	age_group. Found 0 numerical variables:. 	. ---------------------------------------------------------------------------. LinAlgError Traceback (most recent call last). <timed eval> in <module>. ~/.local/lib/python3.8/site-packages/scanpy/preprocessing/_combat.py in combat(adata, key, covariates, inplace). 204 # standardize across genes using a pooled variance estimator. 205 logg.info(""Standardizing Data across genes.\n""). --> 206 s_data, design, var_pooled, stand_mean = _standardize_data(model, data, key). 207 . 208 # fitting the parameters on the standardized data. ~/.local/lib/python3.8/site-packages/scanpy/preprocessing/_combat.py in _standardize_data(model, data, batch_key). 102 . 103 # compute pooled variance estimator. --> 104 B_hat = np.dot(np.dot(la.inv(np.dot(design.T, design)), design.T), data.T). 105 grand_mean = np.dot((n_batches / n_array).T, B_hat[:n_batch, :]). 106 var_pooled = (data - np.dot(design, B_hat).T) ** 2. <__array_function__ internals> in inv(*args, **kwargs). ~/.local/lib/python3.8/site-packages/numpy/linalg/linalg.py in inv(a). 544 signature = 'D->D' if isComplexType(t) else 'd->d'. 545 extobj = get_linalg_error_extobj(_raise_linalgerror_singular). --> 546 ainv = _umath_linalg.inv(a, signature=signature, extobj=extobj). 547 return wrap(ainv.astype(result_t, copy=False)). 548 . ~/.local/lib/python3.8/site-packages/numpy/linalg/linalg.py in _raise_linalgerror_singular(err, flag). 86 . 87 def _raise_linalgerror_singular(err, flag):. ---> 88 raise LinAlgError(""Singular matrix""). 89 . 90 def _raise_linalgerror_nonposdef(err, flag):. LinAlgError: Singular matrix. ```. #### Versions. <details>. [Paste the output of scanpy.logging.print_versions() leaving a blank line after the details tag]. </details>. scanpy==1.7.0rc2.dev7+g57ec8a7e anndata==0.7.3 umap==0.4.6 numpy==1.19.5 scipy==1.6.0 pandas==1.2.1 scikit-learn==0.24.1 statsmodels==0.12.1 python-igraph==0.8.3 louvain==0.7.0 leidenalg==0.8.3.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1606
https://github.com/scverse/scanpy/issues/1606:2682,security,sign,signature,2682,". Found 34 batches. Found 1 categorical variables:. 	age_group. Found 0 numerical variables:. 	. ---------------------------------------------------------------------------. LinAlgError Traceback (most recent call last). <timed eval> in <module>. ~/.local/lib/python3.8/site-packages/scanpy/preprocessing/_combat.py in combat(adata, key, covariates, inplace). 204 # standardize across genes using a pooled variance estimator. 205 logg.info(""Standardizing Data across genes.\n""). --> 206 s_data, design, var_pooled, stand_mean = _standardize_data(model, data, key). 207 . 208 # fitting the parameters on the standardized data. ~/.local/lib/python3.8/site-packages/scanpy/preprocessing/_combat.py in _standardize_data(model, data, batch_key). 102 . 103 # compute pooled variance estimator. --> 104 B_hat = np.dot(np.dot(la.inv(np.dot(design.T, design)), design.T), data.T). 105 grand_mean = np.dot((n_batches / n_array).T, B_hat[:n_batch, :]). 106 var_pooled = (data - np.dot(design, B_hat).T) ** 2. <__array_function__ internals> in inv(*args, **kwargs). ~/.local/lib/python3.8/site-packages/numpy/linalg/linalg.py in inv(a). 544 signature = 'D->D' if isComplexType(t) else 'd->d'. 545 extobj = get_linalg_error_extobj(_raise_linalgerror_singular). --> 546 ainv = _umath_linalg.inv(a, signature=signature, extobj=extobj). 547 return wrap(ainv.astype(result_t, copy=False)). 548 . ~/.local/lib/python3.8/site-packages/numpy/linalg/linalg.py in _raise_linalgerror_singular(err, flag). 86 . 87 def _raise_linalgerror_singular(err, flag):. ---> 88 raise LinAlgError(""Singular matrix""). 89 . 90 def _raise_linalgerror_nonposdef(err, flag):. LinAlgError: Singular matrix. ```. #### Versions. <details>. [Paste the output of scanpy.logging.print_versions() leaving a blank line after the details tag]. </details>. scanpy==1.7.0rc2.dev7+g57ec8a7e anndata==0.7.3 umap==0.4.6 numpy==1.19.5 scipy==1.6.0 pandas==1.2.1 scikit-learn==0.24.1 statsmodels==0.12.1 python-igraph==0.8.3 louvain==0.7.0 leidenalg==0.8.3.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1606
https://github.com/scverse/scanpy/issues/1606:2692,security,sign,signature,2692,". Found 34 batches. Found 1 categorical variables:. 	age_group. Found 0 numerical variables:. 	. ---------------------------------------------------------------------------. LinAlgError Traceback (most recent call last). <timed eval> in <module>. ~/.local/lib/python3.8/site-packages/scanpy/preprocessing/_combat.py in combat(adata, key, covariates, inplace). 204 # standardize across genes using a pooled variance estimator. 205 logg.info(""Standardizing Data across genes.\n""). --> 206 s_data, design, var_pooled, stand_mean = _standardize_data(model, data, key). 207 . 208 # fitting the parameters on the standardized data. ~/.local/lib/python3.8/site-packages/scanpy/preprocessing/_combat.py in _standardize_data(model, data, batch_key). 102 . 103 # compute pooled variance estimator. --> 104 B_hat = np.dot(np.dot(la.inv(np.dot(design.T, design)), design.T), data.T). 105 grand_mean = np.dot((n_batches / n_array).T, B_hat[:n_batch, :]). 106 var_pooled = (data - np.dot(design, B_hat).T) ** 2. <__array_function__ internals> in inv(*args, **kwargs). ~/.local/lib/python3.8/site-packages/numpy/linalg/linalg.py in inv(a). 544 signature = 'D->D' if isComplexType(t) else 'd->d'. 545 extobj = get_linalg_error_extobj(_raise_linalgerror_singular). --> 546 ainv = _umath_linalg.inv(a, signature=signature, extobj=extobj). 547 return wrap(ainv.astype(result_t, copy=False)). 548 . ~/.local/lib/python3.8/site-packages/numpy/linalg/linalg.py in _raise_linalgerror_singular(err, flag). 86 . 87 def _raise_linalgerror_singular(err, flag):. ---> 88 raise LinAlgError(""Singular matrix""). 89 . 90 def _raise_linalgerror_nonposdef(err, flag):. LinAlgError: Singular matrix. ```. #### Versions. <details>. [Paste the output of scanpy.logging.print_versions() leaving a blank line after the details tag]. </details>. scanpy==1.7.0rc2.dev7+g57ec8a7e anndata==0.7.3 umap==0.4.6 numpy==1.19.5 scipy==1.6.0 pandas==1.2.1 scikit-learn==0.24.1 statsmodels==0.12.1 python-igraph==0.8.3 louvain==0.7.0 leidenalg==0.8.3.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1606
https://github.com/scverse/scanpy/issues/1606:3122,security,log,logging,3122,". Found 34 batches. Found 1 categorical variables:. 	age_group. Found 0 numerical variables:. 	. ---------------------------------------------------------------------------. LinAlgError Traceback (most recent call last). <timed eval> in <module>. ~/.local/lib/python3.8/site-packages/scanpy/preprocessing/_combat.py in combat(adata, key, covariates, inplace). 204 # standardize across genes using a pooled variance estimator. 205 logg.info(""Standardizing Data across genes.\n""). --> 206 s_data, design, var_pooled, stand_mean = _standardize_data(model, data, key). 207 . 208 # fitting the parameters on the standardized data. ~/.local/lib/python3.8/site-packages/scanpy/preprocessing/_combat.py in _standardize_data(model, data, batch_key). 102 . 103 # compute pooled variance estimator. --> 104 B_hat = np.dot(np.dot(la.inv(np.dot(design.T, design)), design.T), data.T). 105 grand_mean = np.dot((n_batches / n_array).T, B_hat[:n_batch, :]). 106 var_pooled = (data - np.dot(design, B_hat).T) ** 2. <__array_function__ internals> in inv(*args, **kwargs). ~/.local/lib/python3.8/site-packages/numpy/linalg/linalg.py in inv(a). 544 signature = 'D->D' if isComplexType(t) else 'd->d'. 545 extobj = get_linalg_error_extobj(_raise_linalgerror_singular). --> 546 ainv = _umath_linalg.inv(a, signature=signature, extobj=extobj). 547 return wrap(ainv.astype(result_t, copy=False)). 548 . ~/.local/lib/python3.8/site-packages/numpy/linalg/linalg.py in _raise_linalgerror_singular(err, flag). 86 . 87 def _raise_linalgerror_singular(err, flag):. ---> 88 raise LinAlgError(""Singular matrix""). 89 . 90 def _raise_linalgerror_nonposdef(err, flag):. LinAlgError: Singular matrix. ```. #### Versions. <details>. [Paste the output of scanpy.logging.print_versions() leaving a blank line after the details tag]. </details>. scanpy==1.7.0rc2.dev7+g57ec8a7e anndata==0.7.3 umap==0.4.6 numpy==1.19.5 scipy==1.6.0 pandas==1.2.1 scikit-learn==0.24.1 statsmodels==0.12.1 python-igraph==0.8.3 louvain==0.7.0 leidenalg==0.8.3.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1606
https://github.com/scverse/scanpy/issues/1606:1584,testability,Trace,Traceback,1584,"list(). ```. ```pytb. ['F', 'M', 'U']. ```. ```python. adata.obs['age_groups'].cat.categories.tolist(). ```. ```pytb. ['Old', 'YoungAdult', 'Pediatric', 'Fetal', 'NewBorn']. ```. ```python. sc.pp.combat(adata, key='384plate', covariates=['sex']). ```. ```pytb. Standardizing Data across genes. Found 34 batches. Found 1 categorical variables:. 	sex. Found 0 numerical variables:. 	. Fitting L/S model and finding priors. Finding parametric adjustments. /home/sinhar/.local/lib/python3.8/site-packages/scanpy/preprocessing/_combat.py:340: RuntimeWarning: divide by zero encountered in true_divide. (abs(g_new - g_old) / g_old).max(), (abs(d_new - d_old) / d_old).max(). Adjusting data. ```. ```python. sc.pp.combat(adata, key='384plate', covariates=['age_group']). ```. ```pytb. Standardizing Data across genes. Found 34 batches. Found 1 categorical variables:. 	age_group. Found 0 numerical variables:. 	. ---------------------------------------------------------------------------. LinAlgError Traceback (most recent call last). <timed eval> in <module>. ~/.local/lib/python3.8/site-packages/scanpy/preprocessing/_combat.py in combat(adata, key, covariates, inplace). 204 # standardize across genes using a pooled variance estimator. 205 logg.info(""Standardizing Data across genes.\n""). --> 206 s_data, design, var_pooled, stand_mean = _standardize_data(model, data, key). 207 . 208 # fitting the parameters on the standardized data. ~/.local/lib/python3.8/site-packages/scanpy/preprocessing/_combat.py in _standardize_data(model, data, batch_key). 102 . 103 # compute pooled variance estimator. --> 104 B_hat = np.dot(np.dot(la.inv(np.dot(design.T, design)), design.T), data.T). 105 grand_mean = np.dot((n_batches / n_array).T, B_hat[:n_batch, :]). 106 var_pooled = (data - np.dot(design, B_hat).T) ** 2. <__array_function__ internals> in inv(*args, **kwargs). ~/.local/lib/python3.8/site-packages/numpy/linalg/linalg.py in inv(a). 544 signature = 'D->D' if isComplexType(t) else 'd->d'. 545 extobj",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1606
https://github.com/scverse/scanpy/issues/1606:1828,testability,log,logg,1828,"x']). ```. ```pytb. Standardizing Data across genes. Found 34 batches. Found 1 categorical variables:. 	sex. Found 0 numerical variables:. 	. Fitting L/S model and finding priors. Finding parametric adjustments. /home/sinhar/.local/lib/python3.8/site-packages/scanpy/preprocessing/_combat.py:340: RuntimeWarning: divide by zero encountered in true_divide. (abs(g_new - g_old) / g_old).max(), (abs(d_new - d_old) / d_old).max(). Adjusting data. ```. ```python. sc.pp.combat(adata, key='384plate', covariates=['age_group']). ```. ```pytb. Standardizing Data across genes. Found 34 batches. Found 1 categorical variables:. 	age_group. Found 0 numerical variables:. 	. ---------------------------------------------------------------------------. LinAlgError Traceback (most recent call last). <timed eval> in <module>. ~/.local/lib/python3.8/site-packages/scanpy/preprocessing/_combat.py in combat(adata, key, covariates, inplace). 204 # standardize across genes using a pooled variance estimator. 205 logg.info(""Standardizing Data across genes.\n""). --> 206 s_data, design, var_pooled, stand_mean = _standardize_data(model, data, key). 207 . 208 # fitting the parameters on the standardized data. ~/.local/lib/python3.8/site-packages/scanpy/preprocessing/_combat.py in _standardize_data(model, data, batch_key). 102 . 103 # compute pooled variance estimator. --> 104 B_hat = np.dot(np.dot(la.inv(np.dot(design.T, design)), design.T), data.T). 105 grand_mean = np.dot((n_batches / n_array).T, B_hat[:n_batch, :]). 106 var_pooled = (data - np.dot(design, B_hat).T) ** 2. <__array_function__ internals> in inv(*args, **kwargs). ~/.local/lib/python3.8/site-packages/numpy/linalg/linalg.py in inv(a). 544 signature = 'D->D' if isComplexType(t) else 'd->d'. 545 extobj = get_linalg_error_extobj(_raise_linalgerror_singular). --> 546 ainv = _umath_linalg.inv(a, signature=signature, extobj=extobj). 547 return wrap(ainv.astype(result_t, copy=False)). 548 . ~/.local/lib/python3.8/site-packages/numpy/linalg/lin",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1606
https://github.com/scverse/scanpy/issues/1606:3122,testability,log,logging,3122,". Found 34 batches. Found 1 categorical variables:. 	age_group. Found 0 numerical variables:. 	. ---------------------------------------------------------------------------. LinAlgError Traceback (most recent call last). <timed eval> in <module>. ~/.local/lib/python3.8/site-packages/scanpy/preprocessing/_combat.py in combat(adata, key, covariates, inplace). 204 # standardize across genes using a pooled variance estimator. 205 logg.info(""Standardizing Data across genes.\n""). --> 206 s_data, design, var_pooled, stand_mean = _standardize_data(model, data, key). 207 . 208 # fitting the parameters on the standardized data. ~/.local/lib/python3.8/site-packages/scanpy/preprocessing/_combat.py in _standardize_data(model, data, batch_key). 102 . 103 # compute pooled variance estimator. --> 104 B_hat = np.dot(np.dot(la.inv(np.dot(design.T, design)), design.T), data.T). 105 grand_mean = np.dot((n_batches / n_array).T, B_hat[:n_batch, :]). 106 var_pooled = (data - np.dot(design, B_hat).T) ** 2. <__array_function__ internals> in inv(*args, **kwargs). ~/.local/lib/python3.8/site-packages/numpy/linalg/linalg.py in inv(a). 544 signature = 'D->D' if isComplexType(t) else 'd->d'. 545 extobj = get_linalg_error_extobj(_raise_linalgerror_singular). --> 546 ainv = _umath_linalg.inv(a, signature=signature, extobj=extobj). 547 return wrap(ainv.astype(result_t, copy=False)). 548 . ~/.local/lib/python3.8/site-packages/numpy/linalg/linalg.py in _raise_linalgerror_singular(err, flag). 86 . 87 def _raise_linalgerror_singular(err, flag):. ---> 88 raise LinAlgError(""Singular matrix""). 89 . 90 def _raise_linalgerror_nonposdef(err, flag):. LinAlgError: Singular matrix. ```. #### Versions. <details>. [Paste the output of scanpy.logging.print_versions() leaving a blank line after the details tag]. </details>. scanpy==1.7.0rc2.dev7+g57ec8a7e anndata==0.7.3 umap==0.4.6 numpy==1.19.5 scipy==1.6.0 pandas==1.2.1 scikit-learn==0.24.1 statsmodels==0.12.1 python-igraph==0.8.3 louvain==0.7.0 leidenalg==0.8.3.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1606
https://github.com/scverse/scanpy/issues/1606:138,usability,confirm,confirmed,138,"Combat works for some covariates yet fails for others; - [ x] I have checked that this issue has not already been reported. - [ x] I have confirmed this bug exists on the latest version of scanpy. - [ x] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. ### Minimal code sample (that we can copy&paste without having any data). ```python. adata.obs['sex'].cat.categories.tolist(). ```. ```pytb. ['F', 'M', 'U']. ```. ```python. adata.obs['age_groups'].cat.categories.tolist(). ```. ```pytb. ['Old', 'YoungAdult', 'Pediatric', 'Fetal', 'NewBorn']. ```. ```python. sc.pp.combat(adata, key='384plate', covariates=['sex']). ```. ```pytb. Standardizing Data across genes. Found 34 batches. Found 1 categorical variables:. 	sex. Found 0 numerical variables:. 	. Fitting L/S model and finding priors. Finding parametric adjustments. /home/sinhar/.local/lib/python3.8/site-packages/scanpy/preprocessing/_combat.py:340: RuntimeWarning: divide by zero encountered in true_divide. (abs(g_new - g_old) / g_old).max(), (abs(d_new - d_old) / d_old).max(). Adjusting data. ```. ```python. sc.pp.combat(adata, key='384plate', covariates=['age_group']). ```. ```pytb. Standardizing Data across genes. Found 34 batches. Found 1 categorical variables:. 	age_group. Found 0 numerical variables:. 	. ---------------------------------------------------------------------------. LinAlgError Traceback (most recent call last). <timed eval> in <module>. ~/.local/lib/python3.8/site-packages/scanpy/preprocessing/_combat.py in combat(adata, key, covariates, inplace). 204 # standardize across genes using a pooled variance estimator. 205 logg.info(""Standardizing Data across genes.\n""). --> 206 s_data, design, var_pooled, stand_mean = _standardize_data(model, data, key). 207 . 208 # fitting the parameters on",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1606
https://github.com/scverse/scanpy/issues/1606:222,usability,confirm,confirmed,222,"Combat works for some covariates yet fails for others; - [ x] I have checked that this issue has not already been reported. - [ x] I have confirmed this bug exists on the latest version of scanpy. - [ x] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. ### Minimal code sample (that we can copy&paste without having any data). ```python. adata.obs['sex'].cat.categories.tolist(). ```. ```pytb. ['F', 'M', 'U']. ```. ```python. adata.obs['age_groups'].cat.categories.tolist(). ```. ```pytb. ['Old', 'YoungAdult', 'Pediatric', 'Fetal', 'NewBorn']. ```. ```python. sc.pp.combat(adata, key='384plate', covariates=['sex']). ```. ```pytb. Standardizing Data across genes. Found 34 batches. Found 1 categorical variables:. 	sex. Found 0 numerical variables:. 	. Fitting L/S model and finding priors. Finding parametric adjustments. /home/sinhar/.local/lib/python3.8/site-packages/scanpy/preprocessing/_combat.py:340: RuntimeWarning: divide by zero encountered in true_divide. (abs(g_new - g_old) / g_old).max(), (abs(d_new - d_old) / d_old).max(). Adjusting data. ```. ```python. sc.pp.combat(adata, key='384plate', covariates=['age_group']). ```. ```pytb. Standardizing Data across genes. Found 34 batches. Found 1 categorical variables:. 	age_group. Found 0 numerical variables:. 	. ---------------------------------------------------------------------------. LinAlgError Traceback (most recent call last). <timed eval> in <module>. ~/.local/lib/python3.8/site-packages/scanpy/preprocessing/_combat.py in combat(adata, key, covariates, inplace). 204 # standardize across genes using a pooled variance estimator. 205 logg.info(""Standardizing Data across genes.\n""). --> 206 s_data, design, var_pooled, stand_mean = _standardize_data(model, data, key). 207 . 208 # fitting the parameters on",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1606
https://github.com/scverse/scanpy/issues/1606:313,usability,guid,guide,313,"Combat works for some covariates yet fails for others; - [ x] I have checked that this issue has not already been reported. - [ x] I have confirmed this bug exists on the latest version of scanpy. - [ x] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. ### Minimal code sample (that we can copy&paste without having any data). ```python. adata.obs['sex'].cat.categories.tolist(). ```. ```pytb. ['F', 'M', 'U']. ```. ```python. adata.obs['age_groups'].cat.categories.tolist(). ```. ```pytb. ['Old', 'YoungAdult', 'Pediatric', 'Fetal', 'NewBorn']. ```. ```python. sc.pp.combat(adata, key='384plate', covariates=['sex']). ```. ```pytb. Standardizing Data across genes. Found 34 batches. Found 1 categorical variables:. 	sex. Found 0 numerical variables:. 	. Fitting L/S model and finding priors. Finding parametric adjustments. /home/sinhar/.local/lib/python3.8/site-packages/scanpy/preprocessing/_combat.py:340: RuntimeWarning: divide by zero encountered in true_divide. (abs(g_new - g_old) / g_old).max(), (abs(d_new - d_old) / d_old).max(). Adjusting data. ```. ```python. sc.pp.combat(adata, key='384plate', covariates=['age_group']). ```. ```pytb. Standardizing Data across genes. Found 34 batches. Found 1 categorical variables:. 	age_group. Found 0 numerical variables:. 	. ---------------------------------------------------------------------------. LinAlgError Traceback (most recent call last). <timed eval> in <module>. ~/.local/lib/python3.8/site-packages/scanpy/preprocessing/_combat.py in combat(adata, key, covariates, inplace). 204 # standardize across genes using a pooled variance estimator. 205 logg.info(""Standardizing Data across genes.\n""). --> 206 s_data, design, var_pooled, stand_mean = _standardize_data(model, data, key). 207 . 208 # fitting the parameters on",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1606
https://github.com/scverse/scanpy/issues/1606:368,usability,minim,minimal-bug-reports,368,"Combat works for some covariates yet fails for others; - [ x] I have checked that this issue has not already been reported. - [ x] I have confirmed this bug exists on the latest version of scanpy. - [ x] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. ### Minimal code sample (that we can copy&paste without having any data). ```python. adata.obs['sex'].cat.categories.tolist(). ```. ```pytb. ['F', 'M', 'U']. ```. ```python. adata.obs['age_groups'].cat.categories.tolist(). ```. ```pytb. ['Old', 'YoungAdult', 'Pediatric', 'Fetal', 'NewBorn']. ```. ```python. sc.pp.combat(adata, key='384plate', covariates=['sex']). ```. ```pytb. Standardizing Data across genes. Found 34 batches. Found 1 categorical variables:. 	sex. Found 0 numerical variables:. 	. Fitting L/S model and finding priors. Finding parametric adjustments. /home/sinhar/.local/lib/python3.8/site-packages/scanpy/preprocessing/_combat.py:340: RuntimeWarning: divide by zero encountered in true_divide. (abs(g_new - g_old) / g_old).max(), (abs(d_new - d_old) / d_old).max(). Adjusting data. ```. ```python. sc.pp.combat(adata, key='384plate', covariates=['age_group']). ```. ```pytb. Standardizing Data across genes. Found 34 batches. Found 1 categorical variables:. 	age_group. Found 0 numerical variables:. 	. ---------------------------------------------------------------------------. LinAlgError Traceback (most recent call last). <timed eval> in <module>. ~/.local/lib/python3.8/site-packages/scanpy/preprocessing/_combat.py in combat(adata, key, covariates, inplace). 204 # standardize across genes using a pooled variance estimator. 205 logg.info(""Standardizing Data across genes.\n""). --> 206 s_data, design, var_pooled, stand_mean = _standardize_data(model, data, key). 207 . 208 # fitting the parameters on",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1606
https://github.com/scverse/scanpy/issues/1606:474,usability,Minim,Minimal,474,"Combat works for some covariates yet fails for others; - [ x] I have checked that this issue has not already been reported. - [ x] I have confirmed this bug exists on the latest version of scanpy. - [ x] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. ### Minimal code sample (that we can copy&paste without having any data). ```python. adata.obs['sex'].cat.categories.tolist(). ```. ```pytb. ['F', 'M', 'U']. ```. ```python. adata.obs['age_groups'].cat.categories.tolist(). ```. ```pytb. ['Old', 'YoungAdult', 'Pediatric', 'Fetal', 'NewBorn']. ```. ```python. sc.pp.combat(adata, key='384plate', covariates=['sex']). ```. ```pytb. Standardizing Data across genes. Found 34 batches. Found 1 categorical variables:. 	sex. Found 0 numerical variables:. 	. Fitting L/S model and finding priors. Finding parametric adjustments. /home/sinhar/.local/lib/python3.8/site-packages/scanpy/preprocessing/_combat.py:340: RuntimeWarning: divide by zero encountered in true_divide. (abs(g_new - g_old) / g_old).max(), (abs(d_new - d_old) / d_old).max(). Adjusting data. ```. ```python. sc.pp.combat(adata, key='384plate', covariates=['age_group']). ```. ```pytb. Standardizing Data across genes. Found 34 batches. Found 1 categorical variables:. 	age_group. Found 0 numerical variables:. 	. ---------------------------------------------------------------------------. LinAlgError Traceback (most recent call last). <timed eval> in <module>. ~/.local/lib/python3.8/site-packages/scanpy/preprocessing/_combat.py in combat(adata, key, covariates, inplace). 204 # standardize across genes using a pooled variance estimator. 205 logg.info(""Standardizing Data across genes.\n""). --> 206 s_data, design, var_pooled, stand_mean = _standardize_data(model, data, key). 207 . 208 # fitting the parameters on",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1606
https://github.com/scverse/scanpy/issues/1606:3311,usability,learn,learn,3311,". Found 34 batches. Found 1 categorical variables:. 	age_group. Found 0 numerical variables:. 	. ---------------------------------------------------------------------------. LinAlgError Traceback (most recent call last). <timed eval> in <module>. ~/.local/lib/python3.8/site-packages/scanpy/preprocessing/_combat.py in combat(adata, key, covariates, inplace). 204 # standardize across genes using a pooled variance estimator. 205 logg.info(""Standardizing Data across genes.\n""). --> 206 s_data, design, var_pooled, stand_mean = _standardize_data(model, data, key). 207 . 208 # fitting the parameters on the standardized data. ~/.local/lib/python3.8/site-packages/scanpy/preprocessing/_combat.py in _standardize_data(model, data, batch_key). 102 . 103 # compute pooled variance estimator. --> 104 B_hat = np.dot(np.dot(la.inv(np.dot(design.T, design)), design.T), data.T). 105 grand_mean = np.dot((n_batches / n_array).T, B_hat[:n_batch, :]). 106 var_pooled = (data - np.dot(design, B_hat).T) ** 2. <__array_function__ internals> in inv(*args, **kwargs). ~/.local/lib/python3.8/site-packages/numpy/linalg/linalg.py in inv(a). 544 signature = 'D->D' if isComplexType(t) else 'd->d'. 545 extobj = get_linalg_error_extobj(_raise_linalgerror_singular). --> 546 ainv = _umath_linalg.inv(a, signature=signature, extobj=extobj). 547 return wrap(ainv.astype(result_t, copy=False)). 548 . ~/.local/lib/python3.8/site-packages/numpy/linalg/linalg.py in _raise_linalgerror_singular(err, flag). 86 . 87 def _raise_linalgerror_singular(err, flag):. ---> 88 raise LinAlgError(""Singular matrix""). 89 . 90 def _raise_linalgerror_nonposdef(err, flag):. LinAlgError: Singular matrix. ```. #### Versions. <details>. [Paste the output of scanpy.logging.print_versions() leaving a blank line after the details tag]. </details>. scanpy==1.7.0rc2.dev7+g57ec8a7e anndata==0.7.3 umap==0.4.6 numpy==1.19.5 scipy==1.6.0 pandas==1.2.1 scikit-learn==0.24.1 statsmodels==0.12.1 python-igraph==0.8.3 louvain==0.7.0 leidenalg==0.8.3.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1606
https://github.com/scverse/scanpy/pull/1608:7,deployability,depend,dependency,7,"Remove dependency on scvelo for doc builds; scvelo docs have been changed so the url for the sphinx inventory is different. We also probably don't want to depend on scvelo's documentation for our doc builds, especially since it's pre 1.0, and we weren't really doing much with it. Should fix current doc build problems.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1608
https://github.com/scverse/scanpy/pull/1608:36,deployability,build,builds,36,"Remove dependency on scvelo for doc builds; scvelo docs have been changed so the url for the sphinx inventory is different. We also probably don't want to depend on scvelo's documentation for our doc builds, especially since it's pre 1.0, and we weren't really doing much with it. Should fix current doc build problems.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1608
https://github.com/scverse/scanpy/pull/1608:155,deployability,depend,depend,155,"Remove dependency on scvelo for doc builds; scvelo docs have been changed so the url for the sphinx inventory is different. We also probably don't want to depend on scvelo's documentation for our doc builds, especially since it's pre 1.0, and we weren't really doing much with it. Should fix current doc build problems.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1608
https://github.com/scverse/scanpy/pull/1608:200,deployability,build,builds,200,"Remove dependency on scvelo for doc builds; scvelo docs have been changed so the url for the sphinx inventory is different. We also probably don't want to depend on scvelo's documentation for our doc builds, especially since it's pre 1.0, and we weren't really doing much with it. Should fix current doc build problems.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1608
https://github.com/scverse/scanpy/pull/1608:304,deployability,build,build,304,"Remove dependency on scvelo for doc builds; scvelo docs have been changed so the url for the sphinx inventory is different. We also probably don't want to depend on scvelo's documentation for our doc builds, especially since it's pre 1.0, and we weren't really doing much with it. Should fix current doc build problems.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1608
https://github.com/scverse/scanpy/pull/1608:292,energy efficiency,current,current,292,"Remove dependency on scvelo for doc builds; scvelo docs have been changed so the url for the sphinx inventory is different. We also probably don't want to depend on scvelo's documentation for our doc builds, especially since it's pre 1.0, and we weren't really doing much with it. Should fix current doc build problems.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1608
https://github.com/scverse/scanpy/pull/1608:7,integrability,depend,dependency,7,"Remove dependency on scvelo for doc builds; scvelo docs have been changed so the url for the sphinx inventory is different. We also probably don't want to depend on scvelo's documentation for our doc builds, especially since it's pre 1.0, and we weren't really doing much with it. Should fix current doc build problems.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1608
https://github.com/scverse/scanpy/pull/1608:155,integrability,depend,depend,155,"Remove dependency on scvelo for doc builds; scvelo docs have been changed so the url for the sphinx inventory is different. We also probably don't want to depend on scvelo's documentation for our doc builds, especially since it's pre 1.0, and we weren't really doing much with it. Should fix current doc build problems.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1608
https://github.com/scverse/scanpy/pull/1608:7,modifiability,depend,dependency,7,"Remove dependency on scvelo for doc builds; scvelo docs have been changed so the url for the sphinx inventory is different. We also probably don't want to depend on scvelo's documentation for our doc builds, especially since it's pre 1.0, and we weren't really doing much with it. Should fix current doc build problems.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1608
https://github.com/scverse/scanpy/pull/1608:155,modifiability,depend,depend,155,"Remove dependency on scvelo for doc builds; scvelo docs have been changed so the url for the sphinx inventory is different. We also probably don't want to depend on scvelo's documentation for our doc builds, especially since it's pre 1.0, and we weren't really doing much with it. Should fix current doc build problems.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1608
https://github.com/scverse/scanpy/pull/1608:7,safety,depend,dependency,7,"Remove dependency on scvelo for doc builds; scvelo docs have been changed so the url for the sphinx inventory is different. We also probably don't want to depend on scvelo's documentation for our doc builds, especially since it's pre 1.0, and we weren't really doing much with it. Should fix current doc build problems.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1608
https://github.com/scverse/scanpy/pull/1608:155,safety,depend,depend,155,"Remove dependency on scvelo for doc builds; scvelo docs have been changed so the url for the sphinx inventory is different. We also probably don't want to depend on scvelo's documentation for our doc builds, especially since it's pre 1.0, and we weren't really doing much with it. Should fix current doc build problems.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1608
https://github.com/scverse/scanpy/pull/1608:7,testability,depend,dependency,7,"Remove dependency on scvelo for doc builds; scvelo docs have been changed so the url for the sphinx inventory is different. We also probably don't want to depend on scvelo's documentation for our doc builds, especially since it's pre 1.0, and we weren't really doing much with it. Should fix current doc build problems.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1608
https://github.com/scverse/scanpy/pull/1608:155,testability,depend,depend,155,"Remove dependency on scvelo for doc builds; scvelo docs have been changed so the url for the sphinx inventory is different. We also probably don't want to depend on scvelo's documentation for our doc builds, especially since it's pre 1.0, and we weren't really doing much with it. Should fix current doc build problems.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1608
https://github.com/scverse/scanpy/pull/1608:174,usability,document,documentation,174,"Remove dependency on scvelo for doc builds; scvelo docs have been changed so the url for the sphinx inventory is different. We also probably don't want to depend on scvelo's documentation for our doc builds, especially since it's pre 1.0, and we weren't really doing much with it. Should fix current doc build problems.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1608
https://github.com/scverse/scanpy/pull/1609:42,deployability,depend,dependency,42,Backport PR #1608 on branch 1.7.x (Remove dependency on scvelo for doc builds); Backport PR #1608: Remove dependency on scvelo for doc builds,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1609
https://github.com/scverse/scanpy/pull/1609:71,deployability,build,builds,71,Backport PR #1608 on branch 1.7.x (Remove dependency on scvelo for doc builds); Backport PR #1608: Remove dependency on scvelo for doc builds,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1609
https://github.com/scverse/scanpy/pull/1609:106,deployability,depend,dependency,106,Backport PR #1608 on branch 1.7.x (Remove dependency on scvelo for doc builds); Backport PR #1608: Remove dependency on scvelo for doc builds,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1609
https://github.com/scverse/scanpy/pull/1609:135,deployability,build,builds,135,Backport PR #1608 on branch 1.7.x (Remove dependency on scvelo for doc builds); Backport PR #1608: Remove dependency on scvelo for doc builds,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1609
https://github.com/scverse/scanpy/pull/1609:42,integrability,depend,dependency,42,Backport PR #1608 on branch 1.7.x (Remove dependency on scvelo for doc builds); Backport PR #1608: Remove dependency on scvelo for doc builds,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1609
https://github.com/scverse/scanpy/pull/1609:106,integrability,depend,dependency,106,Backport PR #1608 on branch 1.7.x (Remove dependency on scvelo for doc builds); Backport PR #1608: Remove dependency on scvelo for doc builds,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1609
https://github.com/scverse/scanpy/pull/1609:42,modifiability,depend,dependency,42,Backport PR #1608 on branch 1.7.x (Remove dependency on scvelo for doc builds); Backport PR #1608: Remove dependency on scvelo for doc builds,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1609
https://github.com/scverse/scanpy/pull/1609:106,modifiability,depend,dependency,106,Backport PR #1608 on branch 1.7.x (Remove dependency on scvelo for doc builds); Backport PR #1608: Remove dependency on scvelo for doc builds,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1609
https://github.com/scverse/scanpy/pull/1609:42,safety,depend,dependency,42,Backport PR #1608 on branch 1.7.x (Remove dependency on scvelo for doc builds); Backport PR #1608: Remove dependency on scvelo for doc builds,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1609
https://github.com/scverse/scanpy/pull/1609:106,safety,depend,dependency,106,Backport PR #1608 on branch 1.7.x (Remove dependency on scvelo for doc builds); Backport PR #1608: Remove dependency on scvelo for doc builds,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1609
https://github.com/scverse/scanpy/pull/1609:42,testability,depend,dependency,42,Backport PR #1608 on branch 1.7.x (Remove dependency on scvelo for doc builds); Backport PR #1608: Remove dependency on scvelo for doc builds,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1609
https://github.com/scverse/scanpy/pull/1609:106,testability,depend,dependency,106,Backport PR #1608 on branch 1.7.x (Remove dependency on scvelo for doc builds); Backport PR #1608: Remove dependency on scvelo for doc builds,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1609
https://github.com/scverse/scanpy/pull/1610:55,energy efficiency,heat,heatmaps,55,Backport PR #1603 on branch 1.7.x (Address issues with heatmaps reported in #1591); Backport PR #1603: Address issues with heatmaps reported in #1591,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1610
https://github.com/scverse/scanpy/pull/1610:123,energy efficiency,heat,heatmaps,123,Backport PR #1603 on branch 1.7.x (Address issues with heatmaps reported in #1591); Backport PR #1603: Address issues with heatmaps reported in #1591,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1610
https://github.com/scverse/scanpy/issues/1611:8,energy efficiency,heat,heatmap,8,"mutiple heatmap annotation; Hi,. Does scanpy support mutliple heatmap annotation now, just like this following heatmap, where ""cyl"" can be tissue source, ""mpg"" can be age and ""am"" can be time point, and so on. ![006-heatmap-heatmap-annotation-1](https://user-images.githubusercontent.com/29703450/105984627-60025c00-60d5-11eb-8d07-c9e446d4bd62.png).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1611
https://github.com/scverse/scanpy/issues/1611:62,energy efficiency,heat,heatmap,62,"mutiple heatmap annotation; Hi,. Does scanpy support mutliple heatmap annotation now, just like this following heatmap, where ""cyl"" can be tissue source, ""mpg"" can be age and ""am"" can be time point, and so on. ![006-heatmap-heatmap-annotation-1](https://user-images.githubusercontent.com/29703450/105984627-60025c00-60d5-11eb-8d07-c9e446d4bd62.png).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1611
https://github.com/scverse/scanpy/issues/1611:111,energy efficiency,heat,heatmap,111,"mutiple heatmap annotation; Hi,. Does scanpy support mutliple heatmap annotation now, just like this following heatmap, where ""cyl"" can be tissue source, ""mpg"" can be age and ""am"" can be time point, and so on. ![006-heatmap-heatmap-annotation-1](https://user-images.githubusercontent.com/29703450/105984627-60025c00-60d5-11eb-8d07-c9e446d4bd62.png).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1611
https://github.com/scverse/scanpy/issues/1611:216,energy efficiency,heat,heatmap-heatmap-annotation-,216,"mutiple heatmap annotation; Hi,. Does scanpy support mutliple heatmap annotation now, just like this following heatmap, where ""cyl"" can be tissue source, ""mpg"" can be age and ""am"" can be time point, and so on. ![006-heatmap-heatmap-annotation-1](https://user-images.githubusercontent.com/29703450/105984627-60025c00-60d5-11eb-8d07-c9e446d4bd62.png).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1611
https://github.com/scverse/scanpy/issues/1611:187,performance,time,time,187,"mutiple heatmap annotation; Hi,. Does scanpy support mutliple heatmap annotation now, just like this following heatmap, where ""cyl"" can be tissue source, ""mpg"" can be age and ""am"" can be time point, and so on. ![006-heatmap-heatmap-annotation-1](https://user-images.githubusercontent.com/29703450/105984627-60025c00-60d5-11eb-8d07-c9e446d4bd62.png).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1611
https://github.com/scverse/scanpy/issues/1611:33,reliability,Doe,Does,33,"mutiple heatmap annotation; Hi,. Does scanpy support mutliple heatmap annotation now, just like this following heatmap, where ""cyl"" can be tissue source, ""mpg"" can be age and ""am"" can be time point, and so on. ![006-heatmap-heatmap-annotation-1](https://user-images.githubusercontent.com/29703450/105984627-60025c00-60d5-11eb-8d07-c9e446d4bd62.png).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1611
https://github.com/scverse/scanpy/issues/1611:45,usability,support,support,45,"mutiple heatmap annotation; Hi,. Does scanpy support mutliple heatmap annotation now, just like this following heatmap, where ""cyl"" can be tissue source, ""mpg"" can be age and ""am"" can be time point, and so on. ![006-heatmap-heatmap-annotation-1](https://user-images.githubusercontent.com/29703450/105984627-60025c00-60d5-11eb-8d07-c9e446d4bd62.png).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1611
https://github.com/scverse/scanpy/issues/1611:254,usability,user,user-images,254,"mutiple heatmap annotation; Hi,. Does scanpy support mutliple heatmap annotation now, just like this following heatmap, where ""cyl"" can be tissue source, ""mpg"" can be age and ""am"" can be time point, and so on. ![006-heatmap-heatmap-annotation-1](https://user-images.githubusercontent.com/29703450/105984627-60025c00-60d5-11eb-8d07-c9e446d4bd62.png).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1611
https://github.com/scverse/scanpy/issues/1612:138,deployability,version,version,138,"Reproducibility with normalize_total; Hi,. We have encountered a bug regarding the reproducibility of the `normalize_total` in the latest version of `scanpy==1.6.11`. We also found this bug with `scanpy==1.4.4.post1`. This bug appears with an in-house dataset (containing 10x data and SMART-seq2 scaled data) and interestingly does not appear with the internal scanpy 10x 3k PBMC dataset. ### Minimal code sample. ```python. def check_equal_adata_X(adata1, adata2):. print((adata1.X!=adata2.X).nnz==0). tmp1 = adata1.X.todense(). tmp2 = adata2.X.todense(). print(np.array_equal(tmp1, tmp2)). print((tmp1 == tmp2).all()). adata_run1_concat = sc.read_h5ad(""adata1.h5ad""). adata_run2_concat = sc.read_h5ad(""adata2.h5ad""). check_equal_adata_X(adata_run1_concat, adata_run2_concat). sc.pp.normalize_total(adata_run1_concat). sc.pp.normalize_total(adata_run2_concat). check_equal_adata_X(adata_run1_concat, adata_run2_concat). ```. Output:. ```pytb. True. True. True. False. False. False. ```. Here is the code that fixes this bug. ```python. adata_run1_concat = sc.read_h5ad(""adata1.h5ad""). adata_run2_concat = sc.read_h5ad(""adata2.h5ad""). check_equal_adata_X(adata_run1_concat, adata_run2_concat). adata_run1_concat.X = adata_run1_concat.X.astype(np.float64). adata_run2_concat.X = adata_run2_concat.X.astype(np.float64). sc.pp.normalize_total(adata_run1_concat). sc.pp.normalize_total(adata_run2_concat). check_equal_adata_X(adata_run1_concat, adata_run2_concat). ```. Output:. ```pytb. True. True. True. True. True. True. ```. Like I said, we tested also the ""internal"" `scanpy` 10x 3k PBMC dataset. ```python. sc.datasets.pbmc3k(). pbmc3k_1 = sc.read_h5ad(""data/pbmc3k_raw.h5ad""). pbmc3k_2 = sc.read_h5ad(""data/pbmc3k_raw.h5ad""). check_equal_adata_X(pbmc3k_1, pbmc3k_2). sc.pp.normalize_total(pbmc3k_1). sc.pp.normalize_total(pbmc3k_2). check_equal_adata_X(pbmc3k_1, pbmc3k_2). ```. ```pytb. True. True. True. True. True. True. ```. #### Versions. <details>. -----. anndata 0.7.5. scanpy 1.6.1. sinfo ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1612
https://github.com/scverse/scanpy/issues/1612:261,deployability,contain,containing,261,"Reproducibility with normalize_total; Hi,. We have encountered a bug regarding the reproducibility of the `normalize_total` in the latest version of `scanpy==1.6.11`. We also found this bug with `scanpy==1.4.4.post1`. This bug appears with an in-house dataset (containing 10x data and SMART-seq2 scaled data) and interestingly does not appear with the internal scanpy 10x 3k PBMC dataset. ### Minimal code sample. ```python. def check_equal_adata_X(adata1, adata2):. print((adata1.X!=adata2.X).nnz==0). tmp1 = adata1.X.todense(). tmp2 = adata2.X.todense(). print(np.array_equal(tmp1, tmp2)). print((tmp1 == tmp2).all()). adata_run1_concat = sc.read_h5ad(""adata1.h5ad""). adata_run2_concat = sc.read_h5ad(""adata2.h5ad""). check_equal_adata_X(adata_run1_concat, adata_run2_concat). sc.pp.normalize_total(adata_run1_concat). sc.pp.normalize_total(adata_run2_concat). check_equal_adata_X(adata_run1_concat, adata_run2_concat). ```. Output:. ```pytb. True. True. True. False. False. False. ```. Here is the code that fixes this bug. ```python. adata_run1_concat = sc.read_h5ad(""adata1.h5ad""). adata_run2_concat = sc.read_h5ad(""adata2.h5ad""). check_equal_adata_X(adata_run1_concat, adata_run2_concat). adata_run1_concat.X = adata_run1_concat.X.astype(np.float64). adata_run2_concat.X = adata_run2_concat.X.astype(np.float64). sc.pp.normalize_total(adata_run1_concat). sc.pp.normalize_total(adata_run2_concat). check_equal_adata_X(adata_run1_concat, adata_run2_concat). ```. Output:. ```pytb. True. True. True. True. True. True. ```. Like I said, we tested also the ""internal"" `scanpy` 10x 3k PBMC dataset. ```python. sc.datasets.pbmc3k(). pbmc3k_1 = sc.read_h5ad(""data/pbmc3k_raw.h5ad""). pbmc3k_2 = sc.read_h5ad(""data/pbmc3k_raw.h5ad""). check_equal_adata_X(pbmc3k_1, pbmc3k_2). sc.pp.normalize_total(pbmc3k_1). sc.pp.normalize_total(pbmc3k_2). check_equal_adata_X(pbmc3k_1, pbmc3k_2). ```. ```pytb. True. True. True. True. True. True. ```. #### Versions. <details>. -----. anndata 0.7.5. scanpy 1.6.1. sinfo ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1612
https://github.com/scverse/scanpy/issues/1612:296,deployability,scale,scaled,296,"Reproducibility with normalize_total; Hi,. We have encountered a bug regarding the reproducibility of the `normalize_total` in the latest version of `scanpy==1.6.11`. We also found this bug with `scanpy==1.4.4.post1`. This bug appears with an in-house dataset (containing 10x data and SMART-seq2 scaled data) and interestingly does not appear with the internal scanpy 10x 3k PBMC dataset. ### Minimal code sample. ```python. def check_equal_adata_X(adata1, adata2):. print((adata1.X!=adata2.X).nnz==0). tmp1 = adata1.X.todense(). tmp2 = adata2.X.todense(). print(np.array_equal(tmp1, tmp2)). print((tmp1 == tmp2).all()). adata_run1_concat = sc.read_h5ad(""adata1.h5ad""). adata_run2_concat = sc.read_h5ad(""adata2.h5ad""). check_equal_adata_X(adata_run1_concat, adata_run2_concat). sc.pp.normalize_total(adata_run1_concat). sc.pp.normalize_total(adata_run2_concat). check_equal_adata_X(adata_run1_concat, adata_run2_concat). ```. Output:. ```pytb. True. True. True. False. False. False. ```. Here is the code that fixes this bug. ```python. adata_run1_concat = sc.read_h5ad(""adata1.h5ad""). adata_run2_concat = sc.read_h5ad(""adata2.h5ad""). check_equal_adata_X(adata_run1_concat, adata_run2_concat). adata_run1_concat.X = adata_run1_concat.X.astype(np.float64). adata_run2_concat.X = adata_run2_concat.X.astype(np.float64). sc.pp.normalize_total(adata_run1_concat). sc.pp.normalize_total(adata_run2_concat). check_equal_adata_X(adata_run1_concat, adata_run2_concat). ```. Output:. ```pytb. True. True. True. True. True. True. ```. Like I said, we tested also the ""internal"" `scanpy` 10x 3k PBMC dataset. ```python. sc.datasets.pbmc3k(). pbmc3k_1 = sc.read_h5ad(""data/pbmc3k_raw.h5ad""). pbmc3k_2 = sc.read_h5ad(""data/pbmc3k_raw.h5ad""). check_equal_adata_X(pbmc3k_1, pbmc3k_2). sc.pp.normalize_total(pbmc3k_1). sc.pp.normalize_total(pbmc3k_2). check_equal_adata_X(pbmc3k_1, pbmc3k_2). ```. ```pytb. True. True. True. True. True. True. ```. #### Versions. <details>. -----. anndata 0.7.5. scanpy 1.6.1. sinfo ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1612
https://github.com/scverse/scanpy/issues/1612:1937,deployability,Version,Versions,1937,"b. True. True. True. False. False. False. ```. Here is the code that fixes this bug. ```python. adata_run1_concat = sc.read_h5ad(""adata1.h5ad""). adata_run2_concat = sc.read_h5ad(""adata2.h5ad""). check_equal_adata_X(adata_run1_concat, adata_run2_concat). adata_run1_concat.X = adata_run1_concat.X.astype(np.float64). adata_run2_concat.X = adata_run2_concat.X.astype(np.float64). sc.pp.normalize_total(adata_run1_concat). sc.pp.normalize_total(adata_run2_concat). check_equal_adata_X(adata_run1_concat, adata_run2_concat). ```. Output:. ```pytb. True. True. True. True. True. True. ```. Like I said, we tested also the ""internal"" `scanpy` 10x 3k PBMC dataset. ```python. sc.datasets.pbmc3k(). pbmc3k_1 = sc.read_h5ad(""data/pbmc3k_raw.h5ad""). pbmc3k_2 = sc.read_h5ad(""data/pbmc3k_raw.h5ad""). check_equal_adata_X(pbmc3k_1, pbmc3k_2). sc.pp.normalize_total(pbmc3k_1). sc.pp.normalize_total(pbmc3k_2). check_equal_adata_X(pbmc3k_1, pbmc3k_2). ```. ```pytb. True. True. True. True. True. True. ```. #### Versions. <details>. -----. anndata 0.7.5. scanpy 1.6.1. sinfo 0.3.1. -----. anndata 0.7.5. backcall 0.1.0. cycler 0.10.0. cython_runtime NA. dateutil 2.8.1. decorator 4.4.2. get_version 2.1. h5py 2.10.0. importlib_metadata 1.6.0. ipykernel 5.2.1. ipython_genutils 0.2.0. ipywidgets 7.5.1. jedi 0.17.0. joblib 0.14.1. kiwisolver 1.2.0. legacy_api_wrap 1.2. llvmlite 0.31.0. matplotlib 3.2.1. mpl_toolkits NA. natsort 7.0.1. numba 0.48.0. numexpr 2.7.1. numpy 1.18.2. packaging 20.3. pandas 1.0.3. parso 0.7.0. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. prompt_toolkit 3.0.5. ptyprocess 0.6.0. pygments 2.6.1. pyparsing 2.4.7. pytz 2019.3. scanpy 1.6.1. scipy 1.4.1. setuptools_scm NA. sinfo 0.3.1. six 1.14.0. sklearn 0.22.2.post1. storemagic NA. tables 3.6.1. tornado 6.0.4. tqdm 4.45.0. traitlets 4.3.3. wcwidth NA. zipp NA. zmq 19.0.0. -----. IPython 7.13.0. jupyter_client 6.1.3. jupyter_core 4.6.3. notebook 6.0.3. -----. Python 3.7.6 | packaged by conda-forge | (default, Mar 23 2020, 23:0",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1612
https://github.com/scverse/scanpy/issues/1612:3031,deployability,log,logical,3031,"d(""adata2.h5ad""). check_equal_adata_X(adata_run1_concat, adata_run2_concat). adata_run1_concat.X = adata_run1_concat.X.astype(np.float64). adata_run2_concat.X = adata_run2_concat.X.astype(np.float64). sc.pp.normalize_total(adata_run1_concat). sc.pp.normalize_total(adata_run2_concat). check_equal_adata_X(adata_run1_concat, adata_run2_concat). ```. Output:. ```pytb. True. True. True. True. True. True. ```. Like I said, we tested also the ""internal"" `scanpy` 10x 3k PBMC dataset. ```python. sc.datasets.pbmc3k(). pbmc3k_1 = sc.read_h5ad(""data/pbmc3k_raw.h5ad""). pbmc3k_2 = sc.read_h5ad(""data/pbmc3k_raw.h5ad""). check_equal_adata_X(pbmc3k_1, pbmc3k_2). sc.pp.normalize_total(pbmc3k_1). sc.pp.normalize_total(pbmc3k_2). check_equal_adata_X(pbmc3k_1, pbmc3k_2). ```. ```pytb. True. True. True. True. True. True. ```. #### Versions. <details>. -----. anndata 0.7.5. scanpy 1.6.1. sinfo 0.3.1. -----. anndata 0.7.5. backcall 0.1.0. cycler 0.10.0. cython_runtime NA. dateutil 2.8.1. decorator 4.4.2. get_version 2.1. h5py 2.10.0. importlib_metadata 1.6.0. ipykernel 5.2.1. ipython_genutils 0.2.0. ipywidgets 7.5.1. jedi 0.17.0. joblib 0.14.1. kiwisolver 1.2.0. legacy_api_wrap 1.2. llvmlite 0.31.0. matplotlib 3.2.1. mpl_toolkits NA. natsort 7.0.1. numba 0.48.0. numexpr 2.7.1. numpy 1.18.2. packaging 20.3. pandas 1.0.3. parso 0.7.0. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. prompt_toolkit 3.0.5. ptyprocess 0.6.0. pygments 2.6.1. pyparsing 2.4.7. pytz 2019.3. scanpy 1.6.1. scipy 1.4.1. setuptools_scm NA. sinfo 0.3.1. six 1.14.0. sklearn 0.22.2.post1. storemagic NA. tables 3.6.1. tornado 6.0.4. tqdm 4.45.0. traitlets 4.3.3. wcwidth NA. zipp NA. zmq 19.0.0. -----. IPython 7.13.0. jupyter_client 6.1.3. jupyter_core 4.6.3. notebook 6.0.3. -----. Python 3.7.6 | packaged by conda-forge | (default, Mar 23 2020, 23:03:20) [GCC 7.3.0]. Linux-3.10.0-1127.18.2.el7.x86_64-x86_64-with-centos-7.8.2003-Core. 36 logical CPU cores. -----. Session information updated at 2021-01-27 17:16. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1612
https://github.com/scverse/scanpy/issues/1612:3077,deployability,updat,updated,3077,"d(""adata2.h5ad""). check_equal_adata_X(adata_run1_concat, adata_run2_concat). adata_run1_concat.X = adata_run1_concat.X.astype(np.float64). adata_run2_concat.X = adata_run2_concat.X.astype(np.float64). sc.pp.normalize_total(adata_run1_concat). sc.pp.normalize_total(adata_run2_concat). check_equal_adata_X(adata_run1_concat, adata_run2_concat). ```. Output:. ```pytb. True. True. True. True. True. True. ```. Like I said, we tested also the ""internal"" `scanpy` 10x 3k PBMC dataset. ```python. sc.datasets.pbmc3k(). pbmc3k_1 = sc.read_h5ad(""data/pbmc3k_raw.h5ad""). pbmc3k_2 = sc.read_h5ad(""data/pbmc3k_raw.h5ad""). check_equal_adata_X(pbmc3k_1, pbmc3k_2). sc.pp.normalize_total(pbmc3k_1). sc.pp.normalize_total(pbmc3k_2). check_equal_adata_X(pbmc3k_1, pbmc3k_2). ```. ```pytb. True. True. True. True. True. True. ```. #### Versions. <details>. -----. anndata 0.7.5. scanpy 1.6.1. sinfo 0.3.1. -----. anndata 0.7.5. backcall 0.1.0. cycler 0.10.0. cython_runtime NA. dateutil 2.8.1. decorator 4.4.2. get_version 2.1. h5py 2.10.0. importlib_metadata 1.6.0. ipykernel 5.2.1. ipython_genutils 0.2.0. ipywidgets 7.5.1. jedi 0.17.0. joblib 0.14.1. kiwisolver 1.2.0. legacy_api_wrap 1.2. llvmlite 0.31.0. matplotlib 3.2.1. mpl_toolkits NA. natsort 7.0.1. numba 0.48.0. numexpr 2.7.1. numpy 1.18.2. packaging 20.3. pandas 1.0.3. parso 0.7.0. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. prompt_toolkit 3.0.5. ptyprocess 0.6.0. pygments 2.6.1. pyparsing 2.4.7. pytz 2019.3. scanpy 1.6.1. scipy 1.4.1. setuptools_scm NA. sinfo 0.3.1. six 1.14.0. sklearn 0.22.2.post1. storemagic NA. tables 3.6.1. tornado 6.0.4. tqdm 4.45.0. traitlets 4.3.3. wcwidth NA. zipp NA. zmq 19.0.0. -----. IPython 7.13.0. jupyter_client 6.1.3. jupyter_core 4.6.3. notebook 6.0.3. -----. Python 3.7.6 | packaged by conda-forge | (default, Mar 23 2020, 23:03:20) [GCC 7.3.0]. Linux-3.10.0-1127.18.2.el7.x86_64-x86_64-with-centos-7.8.2003-Core. 36 logical CPU cores. -----. Session information updated at 2021-01-27 17:16. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1612
https://github.com/scverse/scanpy/issues/1612:296,energy efficiency,scale,scaled,296,"Reproducibility with normalize_total; Hi,. We have encountered a bug regarding the reproducibility of the `normalize_total` in the latest version of `scanpy==1.6.11`. We also found this bug with `scanpy==1.4.4.post1`. This bug appears with an in-house dataset (containing 10x data and SMART-seq2 scaled data) and interestingly does not appear with the internal scanpy 10x 3k PBMC dataset. ### Minimal code sample. ```python. def check_equal_adata_X(adata1, adata2):. print((adata1.X!=adata2.X).nnz==0). tmp1 = adata1.X.todense(). tmp2 = adata2.X.todense(). print(np.array_equal(tmp1, tmp2)). print((tmp1 == tmp2).all()). adata_run1_concat = sc.read_h5ad(""adata1.h5ad""). adata_run2_concat = sc.read_h5ad(""adata2.h5ad""). check_equal_adata_X(adata_run1_concat, adata_run2_concat). sc.pp.normalize_total(adata_run1_concat). sc.pp.normalize_total(adata_run2_concat). check_equal_adata_X(adata_run1_concat, adata_run2_concat). ```. Output:. ```pytb. True. True. True. False. False. False. ```. Here is the code that fixes this bug. ```python. adata_run1_concat = sc.read_h5ad(""adata1.h5ad""). adata_run2_concat = sc.read_h5ad(""adata2.h5ad""). check_equal_adata_X(adata_run1_concat, adata_run2_concat). adata_run1_concat.X = adata_run1_concat.X.astype(np.float64). adata_run2_concat.X = adata_run2_concat.X.astype(np.float64). sc.pp.normalize_total(adata_run1_concat). sc.pp.normalize_total(adata_run2_concat). check_equal_adata_X(adata_run1_concat, adata_run2_concat). ```. Output:. ```pytb. True. True. True. True. True. True. ```. Like I said, we tested also the ""internal"" `scanpy` 10x 3k PBMC dataset. ```python. sc.datasets.pbmc3k(). pbmc3k_1 = sc.read_h5ad(""data/pbmc3k_raw.h5ad""). pbmc3k_2 = sc.read_h5ad(""data/pbmc3k_raw.h5ad""). check_equal_adata_X(pbmc3k_1, pbmc3k_2). sc.pp.normalize_total(pbmc3k_1). sc.pp.normalize_total(pbmc3k_2). check_equal_adata_X(pbmc3k_1, pbmc3k_2). ```. ```pytb. True. True. True. True. True. True. ```. #### Versions. <details>. -----. anndata 0.7.5. scanpy 1.6.1. sinfo ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1612
https://github.com/scverse/scanpy/issues/1612:3022,energy efficiency,Core,Core,3022,"d(""adata2.h5ad""). check_equal_adata_X(adata_run1_concat, adata_run2_concat). adata_run1_concat.X = adata_run1_concat.X.astype(np.float64). adata_run2_concat.X = adata_run2_concat.X.astype(np.float64). sc.pp.normalize_total(adata_run1_concat). sc.pp.normalize_total(adata_run2_concat). check_equal_adata_X(adata_run1_concat, adata_run2_concat). ```. Output:. ```pytb. True. True. True. True. True. True. ```. Like I said, we tested also the ""internal"" `scanpy` 10x 3k PBMC dataset. ```python. sc.datasets.pbmc3k(). pbmc3k_1 = sc.read_h5ad(""data/pbmc3k_raw.h5ad""). pbmc3k_2 = sc.read_h5ad(""data/pbmc3k_raw.h5ad""). check_equal_adata_X(pbmc3k_1, pbmc3k_2). sc.pp.normalize_total(pbmc3k_1). sc.pp.normalize_total(pbmc3k_2). check_equal_adata_X(pbmc3k_1, pbmc3k_2). ```. ```pytb. True. True. True. True. True. True. ```. #### Versions. <details>. -----. anndata 0.7.5. scanpy 1.6.1. sinfo 0.3.1. -----. anndata 0.7.5. backcall 0.1.0. cycler 0.10.0. cython_runtime NA. dateutil 2.8.1. decorator 4.4.2. get_version 2.1. h5py 2.10.0. importlib_metadata 1.6.0. ipykernel 5.2.1. ipython_genutils 0.2.0. ipywidgets 7.5.1. jedi 0.17.0. joblib 0.14.1. kiwisolver 1.2.0. legacy_api_wrap 1.2. llvmlite 0.31.0. matplotlib 3.2.1. mpl_toolkits NA. natsort 7.0.1. numba 0.48.0. numexpr 2.7.1. numpy 1.18.2. packaging 20.3. pandas 1.0.3. parso 0.7.0. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. prompt_toolkit 3.0.5. ptyprocess 0.6.0. pygments 2.6.1. pyparsing 2.4.7. pytz 2019.3. scanpy 1.6.1. scipy 1.4.1. setuptools_scm NA. sinfo 0.3.1. six 1.14.0. sklearn 0.22.2.post1. storemagic NA. tables 3.6.1. tornado 6.0.4. tqdm 4.45.0. traitlets 4.3.3. wcwidth NA. zipp NA. zmq 19.0.0. -----. IPython 7.13.0. jupyter_client 6.1.3. jupyter_core 4.6.3. notebook 6.0.3. -----. Python 3.7.6 | packaged by conda-forge | (default, Mar 23 2020, 23:03:20) [GCC 7.3.0]. Linux-3.10.0-1127.18.2.el7.x86_64-x86_64-with-centos-7.8.2003-Core. 36 logical CPU cores. -----. Session information updated at 2021-01-27 17:16. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1612
https://github.com/scverse/scanpy/issues/1612:3039,energy efficiency,CPU,CPU,3039,"d(""adata2.h5ad""). check_equal_adata_X(adata_run1_concat, adata_run2_concat). adata_run1_concat.X = adata_run1_concat.X.astype(np.float64). adata_run2_concat.X = adata_run2_concat.X.astype(np.float64). sc.pp.normalize_total(adata_run1_concat). sc.pp.normalize_total(adata_run2_concat). check_equal_adata_X(adata_run1_concat, adata_run2_concat). ```. Output:. ```pytb. True. True. True. True. True. True. ```. Like I said, we tested also the ""internal"" `scanpy` 10x 3k PBMC dataset. ```python. sc.datasets.pbmc3k(). pbmc3k_1 = sc.read_h5ad(""data/pbmc3k_raw.h5ad""). pbmc3k_2 = sc.read_h5ad(""data/pbmc3k_raw.h5ad""). check_equal_adata_X(pbmc3k_1, pbmc3k_2). sc.pp.normalize_total(pbmc3k_1). sc.pp.normalize_total(pbmc3k_2). check_equal_adata_X(pbmc3k_1, pbmc3k_2). ```. ```pytb. True. True. True. True. True. True. ```. #### Versions. <details>. -----. anndata 0.7.5. scanpy 1.6.1. sinfo 0.3.1. -----. anndata 0.7.5. backcall 0.1.0. cycler 0.10.0. cython_runtime NA. dateutil 2.8.1. decorator 4.4.2. get_version 2.1. h5py 2.10.0. importlib_metadata 1.6.0. ipykernel 5.2.1. ipython_genutils 0.2.0. ipywidgets 7.5.1. jedi 0.17.0. joblib 0.14.1. kiwisolver 1.2.0. legacy_api_wrap 1.2. llvmlite 0.31.0. matplotlib 3.2.1. mpl_toolkits NA. natsort 7.0.1. numba 0.48.0. numexpr 2.7.1. numpy 1.18.2. packaging 20.3. pandas 1.0.3. parso 0.7.0. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. prompt_toolkit 3.0.5. ptyprocess 0.6.0. pygments 2.6.1. pyparsing 2.4.7. pytz 2019.3. scanpy 1.6.1. scipy 1.4.1. setuptools_scm NA. sinfo 0.3.1. six 1.14.0. sklearn 0.22.2.post1. storemagic NA. tables 3.6.1. tornado 6.0.4. tqdm 4.45.0. traitlets 4.3.3. wcwidth NA. zipp NA. zmq 19.0.0. -----. IPython 7.13.0. jupyter_client 6.1.3. jupyter_core 4.6.3. notebook 6.0.3. -----. Python 3.7.6 | packaged by conda-forge | (default, Mar 23 2020, 23:03:20) [GCC 7.3.0]. Linux-3.10.0-1127.18.2.el7.x86_64-x86_64-with-centos-7.8.2003-Core. 36 logical CPU cores. -----. Session information updated at 2021-01-27 17:16. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1612
https://github.com/scverse/scanpy/issues/1612:3043,energy efficiency,core,cores,3043,"d(""adata2.h5ad""). check_equal_adata_X(adata_run1_concat, adata_run2_concat). adata_run1_concat.X = adata_run1_concat.X.astype(np.float64). adata_run2_concat.X = adata_run2_concat.X.astype(np.float64). sc.pp.normalize_total(adata_run1_concat). sc.pp.normalize_total(adata_run2_concat). check_equal_adata_X(adata_run1_concat, adata_run2_concat). ```. Output:. ```pytb. True. True. True. True. True. True. ```. Like I said, we tested also the ""internal"" `scanpy` 10x 3k PBMC dataset. ```python. sc.datasets.pbmc3k(). pbmc3k_1 = sc.read_h5ad(""data/pbmc3k_raw.h5ad""). pbmc3k_2 = sc.read_h5ad(""data/pbmc3k_raw.h5ad""). check_equal_adata_X(pbmc3k_1, pbmc3k_2). sc.pp.normalize_total(pbmc3k_1). sc.pp.normalize_total(pbmc3k_2). check_equal_adata_X(pbmc3k_1, pbmc3k_2). ```. ```pytb. True. True. True. True. True. True. ```. #### Versions. <details>. -----. anndata 0.7.5. scanpy 1.6.1. sinfo 0.3.1. -----. anndata 0.7.5. backcall 0.1.0. cycler 0.10.0. cython_runtime NA. dateutil 2.8.1. decorator 4.4.2. get_version 2.1. h5py 2.10.0. importlib_metadata 1.6.0. ipykernel 5.2.1. ipython_genutils 0.2.0. ipywidgets 7.5.1. jedi 0.17.0. joblib 0.14.1. kiwisolver 1.2.0. legacy_api_wrap 1.2. llvmlite 0.31.0. matplotlib 3.2.1. mpl_toolkits NA. natsort 7.0.1. numba 0.48.0. numexpr 2.7.1. numpy 1.18.2. packaging 20.3. pandas 1.0.3. parso 0.7.0. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. prompt_toolkit 3.0.5. ptyprocess 0.6.0. pygments 2.6.1. pyparsing 2.4.7. pytz 2019.3. scanpy 1.6.1. scipy 1.4.1. setuptools_scm NA. sinfo 0.3.1. six 1.14.0. sklearn 0.22.2.post1. storemagic NA. tables 3.6.1. tornado 6.0.4. tqdm 4.45.0. traitlets 4.3.3. wcwidth NA. zipp NA. zmq 19.0.0. -----. IPython 7.13.0. jupyter_client 6.1.3. jupyter_core 4.6.3. notebook 6.0.3. -----. Python 3.7.6 | packaged by conda-forge | (default, Mar 23 2020, 23:03:20) [GCC 7.3.0]. Linux-3.10.0-1127.18.2.el7.x86_64-x86_64-with-centos-7.8.2003-Core. 36 logical CPU cores. -----. Session information updated at 2021-01-27 17:16. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1612
https://github.com/scverse/scanpy/issues/1612:138,integrability,version,version,138,"Reproducibility with normalize_total; Hi,. We have encountered a bug regarding the reproducibility of the `normalize_total` in the latest version of `scanpy==1.6.11`. We also found this bug with `scanpy==1.4.4.post1`. This bug appears with an in-house dataset (containing 10x data and SMART-seq2 scaled data) and interestingly does not appear with the internal scanpy 10x 3k PBMC dataset. ### Minimal code sample. ```python. def check_equal_adata_X(adata1, adata2):. print((adata1.X!=adata2.X).nnz==0). tmp1 = adata1.X.todense(). tmp2 = adata2.X.todense(). print(np.array_equal(tmp1, tmp2)). print((tmp1 == tmp2).all()). adata_run1_concat = sc.read_h5ad(""adata1.h5ad""). adata_run2_concat = sc.read_h5ad(""adata2.h5ad""). check_equal_adata_X(adata_run1_concat, adata_run2_concat). sc.pp.normalize_total(adata_run1_concat). sc.pp.normalize_total(adata_run2_concat). check_equal_adata_X(adata_run1_concat, adata_run2_concat). ```. Output:. ```pytb. True. True. True. False. False. False. ```. Here is the code that fixes this bug. ```python. adata_run1_concat = sc.read_h5ad(""adata1.h5ad""). adata_run2_concat = sc.read_h5ad(""adata2.h5ad""). check_equal_adata_X(adata_run1_concat, adata_run2_concat). adata_run1_concat.X = adata_run1_concat.X.astype(np.float64). adata_run2_concat.X = adata_run2_concat.X.astype(np.float64). sc.pp.normalize_total(adata_run1_concat). sc.pp.normalize_total(adata_run2_concat). check_equal_adata_X(adata_run1_concat, adata_run2_concat). ```. Output:. ```pytb. True. True. True. True. True. True. ```. Like I said, we tested also the ""internal"" `scanpy` 10x 3k PBMC dataset. ```python. sc.datasets.pbmc3k(). pbmc3k_1 = sc.read_h5ad(""data/pbmc3k_raw.h5ad""). pbmc3k_2 = sc.read_h5ad(""data/pbmc3k_raw.h5ad""). check_equal_adata_X(pbmc3k_1, pbmc3k_2). sc.pp.normalize_total(pbmc3k_1). sc.pp.normalize_total(pbmc3k_2). check_equal_adata_X(pbmc3k_1, pbmc3k_2). ```. ```pytb. True. True. True. True. True. True. ```. #### Versions. <details>. -----. anndata 0.7.5. scanpy 1.6.1. sinfo ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1612
https://github.com/scverse/scanpy/issues/1612:1937,integrability,Version,Versions,1937,"b. True. True. True. False. False. False. ```. Here is the code that fixes this bug. ```python. adata_run1_concat = sc.read_h5ad(""adata1.h5ad""). adata_run2_concat = sc.read_h5ad(""adata2.h5ad""). check_equal_adata_X(adata_run1_concat, adata_run2_concat). adata_run1_concat.X = adata_run1_concat.X.astype(np.float64). adata_run2_concat.X = adata_run2_concat.X.astype(np.float64). sc.pp.normalize_total(adata_run1_concat). sc.pp.normalize_total(adata_run2_concat). check_equal_adata_X(adata_run1_concat, adata_run2_concat). ```. Output:. ```pytb. True. True. True. True. True. True. ```. Like I said, we tested also the ""internal"" `scanpy` 10x 3k PBMC dataset. ```python. sc.datasets.pbmc3k(). pbmc3k_1 = sc.read_h5ad(""data/pbmc3k_raw.h5ad""). pbmc3k_2 = sc.read_h5ad(""data/pbmc3k_raw.h5ad""). check_equal_adata_X(pbmc3k_1, pbmc3k_2). sc.pp.normalize_total(pbmc3k_1). sc.pp.normalize_total(pbmc3k_2). check_equal_adata_X(pbmc3k_1, pbmc3k_2). ```. ```pytb. True. True. True. True. True. True. ```. #### Versions. <details>. -----. anndata 0.7.5. scanpy 1.6.1. sinfo 0.3.1. -----. anndata 0.7.5. backcall 0.1.0. cycler 0.10.0. cython_runtime NA. dateutil 2.8.1. decorator 4.4.2. get_version 2.1. h5py 2.10.0. importlib_metadata 1.6.0. ipykernel 5.2.1. ipython_genutils 0.2.0. ipywidgets 7.5.1. jedi 0.17.0. joblib 0.14.1. kiwisolver 1.2.0. legacy_api_wrap 1.2. llvmlite 0.31.0. matplotlib 3.2.1. mpl_toolkits NA. natsort 7.0.1. numba 0.48.0. numexpr 2.7.1. numpy 1.18.2. packaging 20.3. pandas 1.0.3. parso 0.7.0. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. prompt_toolkit 3.0.5. ptyprocess 0.6.0. pygments 2.6.1. pyparsing 2.4.7. pytz 2019.3. scanpy 1.6.1. scipy 1.4.1. setuptools_scm NA. sinfo 0.3.1. six 1.14.0. sklearn 0.22.2.post1. storemagic NA. tables 3.6.1. tornado 6.0.4. tqdm 4.45.0. traitlets 4.3.3. wcwidth NA. zipp NA. zmq 19.0.0. -----. IPython 7.13.0. jupyter_client 6.1.3. jupyter_core 4.6.3. notebook 6.0.3. -----. Python 3.7.6 | packaged by conda-forge | (default, Mar 23 2020, 23:0",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1612
https://github.com/scverse/scanpy/issues/1612:138,modifiability,version,version,138,"Reproducibility with normalize_total; Hi,. We have encountered a bug regarding the reproducibility of the `normalize_total` in the latest version of `scanpy==1.6.11`. We also found this bug with `scanpy==1.4.4.post1`. This bug appears with an in-house dataset (containing 10x data and SMART-seq2 scaled data) and interestingly does not appear with the internal scanpy 10x 3k PBMC dataset. ### Minimal code sample. ```python. def check_equal_adata_X(adata1, adata2):. print((adata1.X!=adata2.X).nnz==0). tmp1 = adata1.X.todense(). tmp2 = adata2.X.todense(). print(np.array_equal(tmp1, tmp2)). print((tmp1 == tmp2).all()). adata_run1_concat = sc.read_h5ad(""adata1.h5ad""). adata_run2_concat = sc.read_h5ad(""adata2.h5ad""). check_equal_adata_X(adata_run1_concat, adata_run2_concat). sc.pp.normalize_total(adata_run1_concat). sc.pp.normalize_total(adata_run2_concat). check_equal_adata_X(adata_run1_concat, adata_run2_concat). ```. Output:. ```pytb. True. True. True. False. False. False. ```. Here is the code that fixes this bug. ```python. adata_run1_concat = sc.read_h5ad(""adata1.h5ad""). adata_run2_concat = sc.read_h5ad(""adata2.h5ad""). check_equal_adata_X(adata_run1_concat, adata_run2_concat). adata_run1_concat.X = adata_run1_concat.X.astype(np.float64). adata_run2_concat.X = adata_run2_concat.X.astype(np.float64). sc.pp.normalize_total(adata_run1_concat). sc.pp.normalize_total(adata_run2_concat). check_equal_adata_X(adata_run1_concat, adata_run2_concat). ```. Output:. ```pytb. True. True. True. True. True. True. ```. Like I said, we tested also the ""internal"" `scanpy` 10x 3k PBMC dataset. ```python. sc.datasets.pbmc3k(). pbmc3k_1 = sc.read_h5ad(""data/pbmc3k_raw.h5ad""). pbmc3k_2 = sc.read_h5ad(""data/pbmc3k_raw.h5ad""). check_equal_adata_X(pbmc3k_1, pbmc3k_2). sc.pp.normalize_total(pbmc3k_1). sc.pp.normalize_total(pbmc3k_2). check_equal_adata_X(pbmc3k_1, pbmc3k_2). ```. ```pytb. True. True. True. True. True. True. ```. #### Versions. <details>. -----. anndata 0.7.5. scanpy 1.6.1. sinfo ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1612
https://github.com/scverse/scanpy/issues/1612:296,modifiability,scal,scaled,296,"Reproducibility with normalize_total; Hi,. We have encountered a bug regarding the reproducibility of the `normalize_total` in the latest version of `scanpy==1.6.11`. We also found this bug with `scanpy==1.4.4.post1`. This bug appears with an in-house dataset (containing 10x data and SMART-seq2 scaled data) and interestingly does not appear with the internal scanpy 10x 3k PBMC dataset. ### Minimal code sample. ```python. def check_equal_adata_X(adata1, adata2):. print((adata1.X!=adata2.X).nnz==0). tmp1 = adata1.X.todense(). tmp2 = adata2.X.todense(). print(np.array_equal(tmp1, tmp2)). print((tmp1 == tmp2).all()). adata_run1_concat = sc.read_h5ad(""adata1.h5ad""). adata_run2_concat = sc.read_h5ad(""adata2.h5ad""). check_equal_adata_X(adata_run1_concat, adata_run2_concat). sc.pp.normalize_total(adata_run1_concat). sc.pp.normalize_total(adata_run2_concat). check_equal_adata_X(adata_run1_concat, adata_run2_concat). ```. Output:. ```pytb. True. True. True. False. False. False. ```. Here is the code that fixes this bug. ```python. adata_run1_concat = sc.read_h5ad(""adata1.h5ad""). adata_run2_concat = sc.read_h5ad(""adata2.h5ad""). check_equal_adata_X(adata_run1_concat, adata_run2_concat). adata_run1_concat.X = adata_run1_concat.X.astype(np.float64). adata_run2_concat.X = adata_run2_concat.X.astype(np.float64). sc.pp.normalize_total(adata_run1_concat). sc.pp.normalize_total(adata_run2_concat). check_equal_adata_X(adata_run1_concat, adata_run2_concat). ```. Output:. ```pytb. True. True. True. True. True. True. ```. Like I said, we tested also the ""internal"" `scanpy` 10x 3k PBMC dataset. ```python. sc.datasets.pbmc3k(). pbmc3k_1 = sc.read_h5ad(""data/pbmc3k_raw.h5ad""). pbmc3k_2 = sc.read_h5ad(""data/pbmc3k_raw.h5ad""). check_equal_adata_X(pbmc3k_1, pbmc3k_2). sc.pp.normalize_total(pbmc3k_1). sc.pp.normalize_total(pbmc3k_2). check_equal_adata_X(pbmc3k_1, pbmc3k_2). ```. ```pytb. True. True. True. True. True. True. ```. #### Versions. <details>. -----. anndata 0.7.5. scanpy 1.6.1. sinfo ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1612
https://github.com/scverse/scanpy/issues/1612:1937,modifiability,Version,Versions,1937,"b. True. True. True. False. False. False. ```. Here is the code that fixes this bug. ```python. adata_run1_concat = sc.read_h5ad(""adata1.h5ad""). adata_run2_concat = sc.read_h5ad(""adata2.h5ad""). check_equal_adata_X(adata_run1_concat, adata_run2_concat). adata_run1_concat.X = adata_run1_concat.X.astype(np.float64). adata_run2_concat.X = adata_run2_concat.X.astype(np.float64). sc.pp.normalize_total(adata_run1_concat). sc.pp.normalize_total(adata_run2_concat). check_equal_adata_X(adata_run1_concat, adata_run2_concat). ```. Output:. ```pytb. True. True. True. True. True. True. ```. Like I said, we tested also the ""internal"" `scanpy` 10x 3k PBMC dataset. ```python. sc.datasets.pbmc3k(). pbmc3k_1 = sc.read_h5ad(""data/pbmc3k_raw.h5ad""). pbmc3k_2 = sc.read_h5ad(""data/pbmc3k_raw.h5ad""). check_equal_adata_X(pbmc3k_1, pbmc3k_2). sc.pp.normalize_total(pbmc3k_1). sc.pp.normalize_total(pbmc3k_2). check_equal_adata_X(pbmc3k_1, pbmc3k_2). ```. ```pytb. True. True. True. True. True. True. ```. #### Versions. <details>. -----. anndata 0.7.5. scanpy 1.6.1. sinfo 0.3.1. -----. anndata 0.7.5. backcall 0.1.0. cycler 0.10.0. cython_runtime NA. dateutil 2.8.1. decorator 4.4.2. get_version 2.1. h5py 2.10.0. importlib_metadata 1.6.0. ipykernel 5.2.1. ipython_genutils 0.2.0. ipywidgets 7.5.1. jedi 0.17.0. joblib 0.14.1. kiwisolver 1.2.0. legacy_api_wrap 1.2. llvmlite 0.31.0. matplotlib 3.2.1. mpl_toolkits NA. natsort 7.0.1. numba 0.48.0. numexpr 2.7.1. numpy 1.18.2. packaging 20.3. pandas 1.0.3. parso 0.7.0. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. prompt_toolkit 3.0.5. ptyprocess 0.6.0. pygments 2.6.1. pyparsing 2.4.7. pytz 2019.3. scanpy 1.6.1. scipy 1.4.1. setuptools_scm NA. sinfo 0.3.1. six 1.14.0. sklearn 0.22.2.post1. storemagic NA. tables 3.6.1. tornado 6.0.4. tqdm 4.45.0. traitlets 4.3.3. wcwidth NA. zipp NA. zmq 19.0.0. -----. IPython 7.13.0. jupyter_client 6.1.3. jupyter_core 4.6.3. notebook 6.0.3. -----. Python 3.7.6 | packaged by conda-forge | (default, Mar 23 2020, 23:0",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1612
https://github.com/scverse/scanpy/issues/1612:2095,modifiability,deco,decorator,2095,"cat = sc.read_h5ad(""adata2.h5ad""). check_equal_adata_X(adata_run1_concat, adata_run2_concat). adata_run1_concat.X = adata_run1_concat.X.astype(np.float64). adata_run2_concat.X = adata_run2_concat.X.astype(np.float64). sc.pp.normalize_total(adata_run1_concat). sc.pp.normalize_total(adata_run2_concat). check_equal_adata_X(adata_run1_concat, adata_run2_concat). ```. Output:. ```pytb. True. True. True. True. True. True. ```. Like I said, we tested also the ""internal"" `scanpy` 10x 3k PBMC dataset. ```python. sc.datasets.pbmc3k(). pbmc3k_1 = sc.read_h5ad(""data/pbmc3k_raw.h5ad""). pbmc3k_2 = sc.read_h5ad(""data/pbmc3k_raw.h5ad""). check_equal_adata_X(pbmc3k_1, pbmc3k_2). sc.pp.normalize_total(pbmc3k_1). sc.pp.normalize_total(pbmc3k_2). check_equal_adata_X(pbmc3k_1, pbmc3k_2). ```. ```pytb. True. True. True. True. True. True. ```. #### Versions. <details>. -----. anndata 0.7.5. scanpy 1.6.1. sinfo 0.3.1. -----. anndata 0.7.5. backcall 0.1.0. cycler 0.10.0. cython_runtime NA. dateutil 2.8.1. decorator 4.4.2. get_version 2.1. h5py 2.10.0. importlib_metadata 1.6.0. ipykernel 5.2.1. ipython_genutils 0.2.0. ipywidgets 7.5.1. jedi 0.17.0. joblib 0.14.1. kiwisolver 1.2.0. legacy_api_wrap 1.2. llvmlite 0.31.0. matplotlib 3.2.1. mpl_toolkits NA. natsort 7.0.1. numba 0.48.0. numexpr 2.7.1. numpy 1.18.2. packaging 20.3. pandas 1.0.3. parso 0.7.0. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. prompt_toolkit 3.0.5. ptyprocess 0.6.0. pygments 2.6.1. pyparsing 2.4.7. pytz 2019.3. scanpy 1.6.1. scipy 1.4.1. setuptools_scm NA. sinfo 0.3.1. six 1.14.0. sklearn 0.22.2.post1. storemagic NA. tables 3.6.1. tornado 6.0.4. tqdm 4.45.0. traitlets 4.3.3. wcwidth NA. zipp NA. zmq 19.0.0. -----. IPython 7.13.0. jupyter_client 6.1.3. jupyter_core 4.6.3. notebook 6.0.3. -----. Python 3.7.6 | packaged by conda-forge | (default, Mar 23 2020, 23:03:20) [GCC 7.3.0]. Linux-3.10.0-1127.18.2.el7.x86_64-x86_64-with-centos-7.8.2003-Core. 36 logical CPU cores. -----. Session information updated at 2021-01-27 1",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1612
https://github.com/scverse/scanpy/issues/1612:2404,modifiability,pac,packaging,2404,"d(""adata2.h5ad""). check_equal_adata_X(adata_run1_concat, adata_run2_concat). adata_run1_concat.X = adata_run1_concat.X.astype(np.float64). adata_run2_concat.X = adata_run2_concat.X.astype(np.float64). sc.pp.normalize_total(adata_run1_concat). sc.pp.normalize_total(adata_run2_concat). check_equal_adata_X(adata_run1_concat, adata_run2_concat). ```. Output:. ```pytb. True. True. True. True. True. True. ```. Like I said, we tested also the ""internal"" `scanpy` 10x 3k PBMC dataset. ```python. sc.datasets.pbmc3k(). pbmc3k_1 = sc.read_h5ad(""data/pbmc3k_raw.h5ad""). pbmc3k_2 = sc.read_h5ad(""data/pbmc3k_raw.h5ad""). check_equal_adata_X(pbmc3k_1, pbmc3k_2). sc.pp.normalize_total(pbmc3k_1). sc.pp.normalize_total(pbmc3k_2). check_equal_adata_X(pbmc3k_1, pbmc3k_2). ```. ```pytb. True. True. True. True. True. True. ```. #### Versions. <details>. -----. anndata 0.7.5. scanpy 1.6.1. sinfo 0.3.1. -----. anndata 0.7.5. backcall 0.1.0. cycler 0.10.0. cython_runtime NA. dateutil 2.8.1. decorator 4.4.2. get_version 2.1. h5py 2.10.0. importlib_metadata 1.6.0. ipykernel 5.2.1. ipython_genutils 0.2.0. ipywidgets 7.5.1. jedi 0.17.0. joblib 0.14.1. kiwisolver 1.2.0. legacy_api_wrap 1.2. llvmlite 0.31.0. matplotlib 3.2.1. mpl_toolkits NA. natsort 7.0.1. numba 0.48.0. numexpr 2.7.1. numpy 1.18.2. packaging 20.3. pandas 1.0.3. parso 0.7.0. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. prompt_toolkit 3.0.5. ptyprocess 0.6.0. pygments 2.6.1. pyparsing 2.4.7. pytz 2019.3. scanpy 1.6.1. scipy 1.4.1. setuptools_scm NA. sinfo 0.3.1. six 1.14.0. sklearn 0.22.2.post1. storemagic NA. tables 3.6.1. tornado 6.0.4. tqdm 4.45.0. traitlets 4.3.3. wcwidth NA. zipp NA. zmq 19.0.0. -----. IPython 7.13.0. jupyter_client 6.1.3. jupyter_core 4.6.3. notebook 6.0.3. -----. Python 3.7.6 | packaged by conda-forge | (default, Mar 23 2020, 23:03:20) [GCC 7.3.0]. Linux-3.10.0-1127.18.2.el7.x86_64-x86_64-with-centos-7.8.2003-Core. 36 logical CPU cores. -----. Session information updated at 2021-01-27 17:16. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1612
https://github.com/scverse/scanpy/issues/1612:2888,modifiability,pac,packaged,2888,"d(""adata2.h5ad""). check_equal_adata_X(adata_run1_concat, adata_run2_concat). adata_run1_concat.X = adata_run1_concat.X.astype(np.float64). adata_run2_concat.X = adata_run2_concat.X.astype(np.float64). sc.pp.normalize_total(adata_run1_concat). sc.pp.normalize_total(adata_run2_concat). check_equal_adata_X(adata_run1_concat, adata_run2_concat). ```. Output:. ```pytb. True. True. True. True. True. True. ```. Like I said, we tested also the ""internal"" `scanpy` 10x 3k PBMC dataset. ```python. sc.datasets.pbmc3k(). pbmc3k_1 = sc.read_h5ad(""data/pbmc3k_raw.h5ad""). pbmc3k_2 = sc.read_h5ad(""data/pbmc3k_raw.h5ad""). check_equal_adata_X(pbmc3k_1, pbmc3k_2). sc.pp.normalize_total(pbmc3k_1). sc.pp.normalize_total(pbmc3k_2). check_equal_adata_X(pbmc3k_1, pbmc3k_2). ```. ```pytb. True. True. True. True. True. True. ```. #### Versions. <details>. -----. anndata 0.7.5. scanpy 1.6.1. sinfo 0.3.1. -----. anndata 0.7.5. backcall 0.1.0. cycler 0.10.0. cython_runtime NA. dateutil 2.8.1. decorator 4.4.2. get_version 2.1. h5py 2.10.0. importlib_metadata 1.6.0. ipykernel 5.2.1. ipython_genutils 0.2.0. ipywidgets 7.5.1. jedi 0.17.0. joblib 0.14.1. kiwisolver 1.2.0. legacy_api_wrap 1.2. llvmlite 0.31.0. matplotlib 3.2.1. mpl_toolkits NA. natsort 7.0.1. numba 0.48.0. numexpr 2.7.1. numpy 1.18.2. packaging 20.3. pandas 1.0.3. parso 0.7.0. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. prompt_toolkit 3.0.5. ptyprocess 0.6.0. pygments 2.6.1. pyparsing 2.4.7. pytz 2019.3. scanpy 1.6.1. scipy 1.4.1. setuptools_scm NA. sinfo 0.3.1. six 1.14.0. sklearn 0.22.2.post1. storemagic NA. tables 3.6.1. tornado 6.0.4. tqdm 4.45.0. traitlets 4.3.3. wcwidth NA. zipp NA. zmq 19.0.0. -----. IPython 7.13.0. jupyter_client 6.1.3. jupyter_core 4.6.3. notebook 6.0.3. -----. Python 3.7.6 | packaged by conda-forge | (default, Mar 23 2020, 23:03:20) [GCC 7.3.0]. Linux-3.10.0-1127.18.2.el7.x86_64-x86_64-with-centos-7.8.2003-Core. 36 logical CPU cores. -----. Session information updated at 2021-01-27 17:16. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1612
https://github.com/scverse/scanpy/issues/1612:296,performance,scale,scaled,296,"Reproducibility with normalize_total; Hi,. We have encountered a bug regarding the reproducibility of the `normalize_total` in the latest version of `scanpy==1.6.11`. We also found this bug with `scanpy==1.4.4.post1`. This bug appears with an in-house dataset (containing 10x data and SMART-seq2 scaled data) and interestingly does not appear with the internal scanpy 10x 3k PBMC dataset. ### Minimal code sample. ```python. def check_equal_adata_X(adata1, adata2):. print((adata1.X!=adata2.X).nnz==0). tmp1 = adata1.X.todense(). tmp2 = adata2.X.todense(). print(np.array_equal(tmp1, tmp2)). print((tmp1 == tmp2).all()). adata_run1_concat = sc.read_h5ad(""adata1.h5ad""). adata_run2_concat = sc.read_h5ad(""adata2.h5ad""). check_equal_adata_X(adata_run1_concat, adata_run2_concat). sc.pp.normalize_total(adata_run1_concat). sc.pp.normalize_total(adata_run2_concat). check_equal_adata_X(adata_run1_concat, adata_run2_concat). ```. Output:. ```pytb. True. True. True. False. False. False. ```. Here is the code that fixes this bug. ```python. adata_run1_concat = sc.read_h5ad(""adata1.h5ad""). adata_run2_concat = sc.read_h5ad(""adata2.h5ad""). check_equal_adata_X(adata_run1_concat, adata_run2_concat). adata_run1_concat.X = adata_run1_concat.X.astype(np.float64). adata_run2_concat.X = adata_run2_concat.X.astype(np.float64). sc.pp.normalize_total(adata_run1_concat). sc.pp.normalize_total(adata_run2_concat). check_equal_adata_X(adata_run1_concat, adata_run2_concat). ```. Output:. ```pytb. True. True. True. True. True. True. ```. Like I said, we tested also the ""internal"" `scanpy` 10x 3k PBMC dataset. ```python. sc.datasets.pbmc3k(). pbmc3k_1 = sc.read_h5ad(""data/pbmc3k_raw.h5ad""). pbmc3k_2 = sc.read_h5ad(""data/pbmc3k_raw.h5ad""). check_equal_adata_X(pbmc3k_1, pbmc3k_2). sc.pp.normalize_total(pbmc3k_1). sc.pp.normalize_total(pbmc3k_2). check_equal_adata_X(pbmc3k_1, pbmc3k_2). ```. ```pytb. True. True. True. True. True. True. ```. #### Versions. <details>. -----. anndata 0.7.5. scanpy 1.6.1. sinfo ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1612
https://github.com/scverse/scanpy/issues/1612:3039,performance,CPU,CPU,3039,"d(""adata2.h5ad""). check_equal_adata_X(adata_run1_concat, adata_run2_concat). adata_run1_concat.X = adata_run1_concat.X.astype(np.float64). adata_run2_concat.X = adata_run2_concat.X.astype(np.float64). sc.pp.normalize_total(adata_run1_concat). sc.pp.normalize_total(adata_run2_concat). check_equal_adata_X(adata_run1_concat, adata_run2_concat). ```. Output:. ```pytb. True. True. True. True. True. True. ```. Like I said, we tested also the ""internal"" `scanpy` 10x 3k PBMC dataset. ```python. sc.datasets.pbmc3k(). pbmc3k_1 = sc.read_h5ad(""data/pbmc3k_raw.h5ad""). pbmc3k_2 = sc.read_h5ad(""data/pbmc3k_raw.h5ad""). check_equal_adata_X(pbmc3k_1, pbmc3k_2). sc.pp.normalize_total(pbmc3k_1). sc.pp.normalize_total(pbmc3k_2). check_equal_adata_X(pbmc3k_1, pbmc3k_2). ```. ```pytb. True. True. True. True. True. True. ```. #### Versions. <details>. -----. anndata 0.7.5. scanpy 1.6.1. sinfo 0.3.1. -----. anndata 0.7.5. backcall 0.1.0. cycler 0.10.0. cython_runtime NA. dateutil 2.8.1. decorator 4.4.2. get_version 2.1. h5py 2.10.0. importlib_metadata 1.6.0. ipykernel 5.2.1. ipython_genutils 0.2.0. ipywidgets 7.5.1. jedi 0.17.0. joblib 0.14.1. kiwisolver 1.2.0. legacy_api_wrap 1.2. llvmlite 0.31.0. matplotlib 3.2.1. mpl_toolkits NA. natsort 7.0.1. numba 0.48.0. numexpr 2.7.1. numpy 1.18.2. packaging 20.3. pandas 1.0.3. parso 0.7.0. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. prompt_toolkit 3.0.5. ptyprocess 0.6.0. pygments 2.6.1. pyparsing 2.4.7. pytz 2019.3. scanpy 1.6.1. scipy 1.4.1. setuptools_scm NA. sinfo 0.3.1. six 1.14.0. sklearn 0.22.2.post1. storemagic NA. tables 3.6.1. tornado 6.0.4. tqdm 4.45.0. traitlets 4.3.3. wcwidth NA. zipp NA. zmq 19.0.0. -----. IPython 7.13.0. jupyter_client 6.1.3. jupyter_core 4.6.3. notebook 6.0.3. -----. Python 3.7.6 | packaged by conda-forge | (default, Mar 23 2020, 23:03:20) [GCC 7.3.0]. Linux-3.10.0-1127.18.2.el7.x86_64-x86_64-with-centos-7.8.2003-Core. 36 logical CPU cores. -----. Session information updated at 2021-01-27 17:16. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1612
https://github.com/scverse/scanpy/issues/1612:327,reliability,doe,does,327,"Reproducibility with normalize_total; Hi,. We have encountered a bug regarding the reproducibility of the `normalize_total` in the latest version of `scanpy==1.6.11`. We also found this bug with `scanpy==1.4.4.post1`. This bug appears with an in-house dataset (containing 10x data and SMART-seq2 scaled data) and interestingly does not appear with the internal scanpy 10x 3k PBMC dataset. ### Minimal code sample. ```python. def check_equal_adata_X(adata1, adata2):. print((adata1.X!=adata2.X).nnz==0). tmp1 = adata1.X.todense(). tmp2 = adata2.X.todense(). print(np.array_equal(tmp1, tmp2)). print((tmp1 == tmp2).all()). adata_run1_concat = sc.read_h5ad(""adata1.h5ad""). adata_run2_concat = sc.read_h5ad(""adata2.h5ad""). check_equal_adata_X(adata_run1_concat, adata_run2_concat). sc.pp.normalize_total(adata_run1_concat). sc.pp.normalize_total(adata_run2_concat). check_equal_adata_X(adata_run1_concat, adata_run2_concat). ```. Output:. ```pytb. True. True. True. False. False. False. ```. Here is the code that fixes this bug. ```python. adata_run1_concat = sc.read_h5ad(""adata1.h5ad""). adata_run2_concat = sc.read_h5ad(""adata2.h5ad""). check_equal_adata_X(adata_run1_concat, adata_run2_concat). adata_run1_concat.X = adata_run1_concat.X.astype(np.float64). adata_run2_concat.X = adata_run2_concat.X.astype(np.float64). sc.pp.normalize_total(adata_run1_concat). sc.pp.normalize_total(adata_run2_concat). check_equal_adata_X(adata_run1_concat, adata_run2_concat). ```. Output:. ```pytb. True. True. True. True. True. True. ```. Like I said, we tested also the ""internal"" `scanpy` 10x 3k PBMC dataset. ```python. sc.datasets.pbmc3k(). pbmc3k_1 = sc.read_h5ad(""data/pbmc3k_raw.h5ad""). pbmc3k_2 = sc.read_h5ad(""data/pbmc3k_raw.h5ad""). check_equal_adata_X(pbmc3k_1, pbmc3k_2). sc.pp.normalize_total(pbmc3k_1). sc.pp.normalize_total(pbmc3k_2). check_equal_adata_X(pbmc3k_1, pbmc3k_2). ```. ```pytb. True. True. True. True. True. True. ```. #### Versions. <details>. -----. anndata 0.7.5. scanpy 1.6.1. sinfo ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1612
https://github.com/scverse/scanpy/issues/1612:1541,safety,test,tested,1541,"X.todense(). print(np.array_equal(tmp1, tmp2)). print((tmp1 == tmp2).all()). adata_run1_concat = sc.read_h5ad(""adata1.h5ad""). adata_run2_concat = sc.read_h5ad(""adata2.h5ad""). check_equal_adata_X(adata_run1_concat, adata_run2_concat). sc.pp.normalize_total(adata_run1_concat). sc.pp.normalize_total(adata_run2_concat). check_equal_adata_X(adata_run1_concat, adata_run2_concat). ```. Output:. ```pytb. True. True. True. False. False. False. ```. Here is the code that fixes this bug. ```python. adata_run1_concat = sc.read_h5ad(""adata1.h5ad""). adata_run2_concat = sc.read_h5ad(""adata2.h5ad""). check_equal_adata_X(adata_run1_concat, adata_run2_concat). adata_run1_concat.X = adata_run1_concat.X.astype(np.float64). adata_run2_concat.X = adata_run2_concat.X.astype(np.float64). sc.pp.normalize_total(adata_run1_concat). sc.pp.normalize_total(adata_run2_concat). check_equal_adata_X(adata_run1_concat, adata_run2_concat). ```. Output:. ```pytb. True. True. True. True. True. True. ```. Like I said, we tested also the ""internal"" `scanpy` 10x 3k PBMC dataset. ```python. sc.datasets.pbmc3k(). pbmc3k_1 = sc.read_h5ad(""data/pbmc3k_raw.h5ad""). pbmc3k_2 = sc.read_h5ad(""data/pbmc3k_raw.h5ad""). check_equal_adata_X(pbmc3k_1, pbmc3k_2). sc.pp.normalize_total(pbmc3k_1). sc.pp.normalize_total(pbmc3k_2). check_equal_adata_X(pbmc3k_1, pbmc3k_2). ```. ```pytb. True. True. True. True. True. True. ```. #### Versions. <details>. -----. anndata 0.7.5. scanpy 1.6.1. sinfo 0.3.1. -----. anndata 0.7.5. backcall 0.1.0. cycler 0.10.0. cython_runtime NA. dateutil 2.8.1. decorator 4.4.2. get_version 2.1. h5py 2.10.0. importlib_metadata 1.6.0. ipykernel 5.2.1. ipython_genutils 0.2.0. ipywidgets 7.5.1. jedi 0.17.0. joblib 0.14.1. kiwisolver 1.2.0. legacy_api_wrap 1.2. llvmlite 0.31.0. matplotlib 3.2.1. mpl_toolkits NA. natsort 7.0.1. numba 0.48.0. numexpr 2.7.1. numpy 1.18.2. packaging 20.3. pandas 1.0.3. parso 0.7.0. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. prompt_toolkit 3.0.5. ptyprocess 0.6.0. pygme",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1612
https://github.com/scverse/scanpy/issues/1612:3031,safety,log,logical,3031,"d(""adata2.h5ad""). check_equal_adata_X(adata_run1_concat, adata_run2_concat). adata_run1_concat.X = adata_run1_concat.X.astype(np.float64). adata_run2_concat.X = adata_run2_concat.X.astype(np.float64). sc.pp.normalize_total(adata_run1_concat). sc.pp.normalize_total(adata_run2_concat). check_equal_adata_X(adata_run1_concat, adata_run2_concat). ```. Output:. ```pytb. True. True. True. True. True. True. ```. Like I said, we tested also the ""internal"" `scanpy` 10x 3k PBMC dataset. ```python. sc.datasets.pbmc3k(). pbmc3k_1 = sc.read_h5ad(""data/pbmc3k_raw.h5ad""). pbmc3k_2 = sc.read_h5ad(""data/pbmc3k_raw.h5ad""). check_equal_adata_X(pbmc3k_1, pbmc3k_2). sc.pp.normalize_total(pbmc3k_1). sc.pp.normalize_total(pbmc3k_2). check_equal_adata_X(pbmc3k_1, pbmc3k_2). ```. ```pytb. True. True. True. True. True. True. ```. #### Versions. <details>. -----. anndata 0.7.5. scanpy 1.6.1. sinfo 0.3.1. -----. anndata 0.7.5. backcall 0.1.0. cycler 0.10.0. cython_runtime NA. dateutil 2.8.1. decorator 4.4.2. get_version 2.1. h5py 2.10.0. importlib_metadata 1.6.0. ipykernel 5.2.1. ipython_genutils 0.2.0. ipywidgets 7.5.1. jedi 0.17.0. joblib 0.14.1. kiwisolver 1.2.0. legacy_api_wrap 1.2. llvmlite 0.31.0. matplotlib 3.2.1. mpl_toolkits NA. natsort 7.0.1. numba 0.48.0. numexpr 2.7.1. numpy 1.18.2. packaging 20.3. pandas 1.0.3. parso 0.7.0. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. prompt_toolkit 3.0.5. ptyprocess 0.6.0. pygments 2.6.1. pyparsing 2.4.7. pytz 2019.3. scanpy 1.6.1. scipy 1.4.1. setuptools_scm NA. sinfo 0.3.1. six 1.14.0. sklearn 0.22.2.post1. storemagic NA. tables 3.6.1. tornado 6.0.4. tqdm 4.45.0. traitlets 4.3.3. wcwidth NA. zipp NA. zmq 19.0.0. -----. IPython 7.13.0. jupyter_client 6.1.3. jupyter_core 4.6.3. notebook 6.0.3. -----. Python 3.7.6 | packaged by conda-forge | (default, Mar 23 2020, 23:03:20) [GCC 7.3.0]. Linux-3.10.0-1127.18.2.el7.x86_64-x86_64-with-centos-7.8.2003-Core. 36 logical CPU cores. -----. Session information updated at 2021-01-27 17:16. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1612
https://github.com/scverse/scanpy/issues/1612:3077,safety,updat,updated,3077,"d(""adata2.h5ad""). check_equal_adata_X(adata_run1_concat, adata_run2_concat). adata_run1_concat.X = adata_run1_concat.X.astype(np.float64). adata_run2_concat.X = adata_run2_concat.X.astype(np.float64). sc.pp.normalize_total(adata_run1_concat). sc.pp.normalize_total(adata_run2_concat). check_equal_adata_X(adata_run1_concat, adata_run2_concat). ```. Output:. ```pytb. True. True. True. True. True. True. ```. Like I said, we tested also the ""internal"" `scanpy` 10x 3k PBMC dataset. ```python. sc.datasets.pbmc3k(). pbmc3k_1 = sc.read_h5ad(""data/pbmc3k_raw.h5ad""). pbmc3k_2 = sc.read_h5ad(""data/pbmc3k_raw.h5ad""). check_equal_adata_X(pbmc3k_1, pbmc3k_2). sc.pp.normalize_total(pbmc3k_1). sc.pp.normalize_total(pbmc3k_2). check_equal_adata_X(pbmc3k_1, pbmc3k_2). ```. ```pytb. True. True. True. True. True. True. ```. #### Versions. <details>. -----. anndata 0.7.5. scanpy 1.6.1. sinfo 0.3.1. -----. anndata 0.7.5. backcall 0.1.0. cycler 0.10.0. cython_runtime NA. dateutil 2.8.1. decorator 4.4.2. get_version 2.1. h5py 2.10.0. importlib_metadata 1.6.0. ipykernel 5.2.1. ipython_genutils 0.2.0. ipywidgets 7.5.1. jedi 0.17.0. joblib 0.14.1. kiwisolver 1.2.0. legacy_api_wrap 1.2. llvmlite 0.31.0. matplotlib 3.2.1. mpl_toolkits NA. natsort 7.0.1. numba 0.48.0. numexpr 2.7.1. numpy 1.18.2. packaging 20.3. pandas 1.0.3. parso 0.7.0. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. prompt_toolkit 3.0.5. ptyprocess 0.6.0. pygments 2.6.1. pyparsing 2.4.7. pytz 2019.3. scanpy 1.6.1. scipy 1.4.1. setuptools_scm NA. sinfo 0.3.1. six 1.14.0. sklearn 0.22.2.post1. storemagic NA. tables 3.6.1. tornado 6.0.4. tqdm 4.45.0. traitlets 4.3.3. wcwidth NA. zipp NA. zmq 19.0.0. -----. IPython 7.13.0. jupyter_client 6.1.3. jupyter_core 4.6.3. notebook 6.0.3. -----. Python 3.7.6 | packaged by conda-forge | (default, Mar 23 2020, 23:03:20) [GCC 7.3.0]. Linux-3.10.0-1127.18.2.el7.x86_64-x86_64-with-centos-7.8.2003-Core. 36 logical CPU cores. -----. Session information updated at 2021-01-27 17:16. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1612
https://github.com/scverse/scanpy/issues/1612:3031,security,log,logical,3031,"d(""adata2.h5ad""). check_equal_adata_X(adata_run1_concat, adata_run2_concat). adata_run1_concat.X = adata_run1_concat.X.astype(np.float64). adata_run2_concat.X = adata_run2_concat.X.astype(np.float64). sc.pp.normalize_total(adata_run1_concat). sc.pp.normalize_total(adata_run2_concat). check_equal_adata_X(adata_run1_concat, adata_run2_concat). ```. Output:. ```pytb. True. True. True. True. True. True. ```. Like I said, we tested also the ""internal"" `scanpy` 10x 3k PBMC dataset. ```python. sc.datasets.pbmc3k(). pbmc3k_1 = sc.read_h5ad(""data/pbmc3k_raw.h5ad""). pbmc3k_2 = sc.read_h5ad(""data/pbmc3k_raw.h5ad""). check_equal_adata_X(pbmc3k_1, pbmc3k_2). sc.pp.normalize_total(pbmc3k_1). sc.pp.normalize_total(pbmc3k_2). check_equal_adata_X(pbmc3k_1, pbmc3k_2). ```. ```pytb. True. True. True. True. True. True. ```. #### Versions. <details>. -----. anndata 0.7.5. scanpy 1.6.1. sinfo 0.3.1. -----. anndata 0.7.5. backcall 0.1.0. cycler 0.10.0. cython_runtime NA. dateutil 2.8.1. decorator 4.4.2. get_version 2.1. h5py 2.10.0. importlib_metadata 1.6.0. ipykernel 5.2.1. ipython_genutils 0.2.0. ipywidgets 7.5.1. jedi 0.17.0. joblib 0.14.1. kiwisolver 1.2.0. legacy_api_wrap 1.2. llvmlite 0.31.0. matplotlib 3.2.1. mpl_toolkits NA. natsort 7.0.1. numba 0.48.0. numexpr 2.7.1. numpy 1.18.2. packaging 20.3. pandas 1.0.3. parso 0.7.0. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. prompt_toolkit 3.0.5. ptyprocess 0.6.0. pygments 2.6.1. pyparsing 2.4.7. pytz 2019.3. scanpy 1.6.1. scipy 1.4.1. setuptools_scm NA. sinfo 0.3.1. six 1.14.0. sklearn 0.22.2.post1. storemagic NA. tables 3.6.1. tornado 6.0.4. tqdm 4.45.0. traitlets 4.3.3. wcwidth NA. zipp NA. zmq 19.0.0. -----. IPython 7.13.0. jupyter_client 6.1.3. jupyter_core 4.6.3. notebook 6.0.3. -----. Python 3.7.6 | packaged by conda-forge | (default, Mar 23 2020, 23:03:20) [GCC 7.3.0]. Linux-3.10.0-1127.18.2.el7.x86_64-x86_64-with-centos-7.8.2003-Core. 36 logical CPU cores. -----. Session information updated at 2021-01-27 17:16. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1612
https://github.com/scverse/scanpy/issues/1612:3057,security,Session,Session,3057,"d(""adata2.h5ad""). check_equal_adata_X(adata_run1_concat, adata_run2_concat). adata_run1_concat.X = adata_run1_concat.X.astype(np.float64). adata_run2_concat.X = adata_run2_concat.X.astype(np.float64). sc.pp.normalize_total(adata_run1_concat). sc.pp.normalize_total(adata_run2_concat). check_equal_adata_X(adata_run1_concat, adata_run2_concat). ```. Output:. ```pytb. True. True. True. True. True. True. ```. Like I said, we tested also the ""internal"" `scanpy` 10x 3k PBMC dataset. ```python. sc.datasets.pbmc3k(). pbmc3k_1 = sc.read_h5ad(""data/pbmc3k_raw.h5ad""). pbmc3k_2 = sc.read_h5ad(""data/pbmc3k_raw.h5ad""). check_equal_adata_X(pbmc3k_1, pbmc3k_2). sc.pp.normalize_total(pbmc3k_1). sc.pp.normalize_total(pbmc3k_2). check_equal_adata_X(pbmc3k_1, pbmc3k_2). ```. ```pytb. True. True. True. True. True. True. ```. #### Versions. <details>. -----. anndata 0.7.5. scanpy 1.6.1. sinfo 0.3.1. -----. anndata 0.7.5. backcall 0.1.0. cycler 0.10.0. cython_runtime NA. dateutil 2.8.1. decorator 4.4.2. get_version 2.1. h5py 2.10.0. importlib_metadata 1.6.0. ipykernel 5.2.1. ipython_genutils 0.2.0. ipywidgets 7.5.1. jedi 0.17.0. joblib 0.14.1. kiwisolver 1.2.0. legacy_api_wrap 1.2. llvmlite 0.31.0. matplotlib 3.2.1. mpl_toolkits NA. natsort 7.0.1. numba 0.48.0. numexpr 2.7.1. numpy 1.18.2. packaging 20.3. pandas 1.0.3. parso 0.7.0. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. prompt_toolkit 3.0.5. ptyprocess 0.6.0. pygments 2.6.1. pyparsing 2.4.7. pytz 2019.3. scanpy 1.6.1. scipy 1.4.1. setuptools_scm NA. sinfo 0.3.1. six 1.14.0. sklearn 0.22.2.post1. storemagic NA. tables 3.6.1. tornado 6.0.4. tqdm 4.45.0. traitlets 4.3.3. wcwidth NA. zipp NA. zmq 19.0.0. -----. IPython 7.13.0. jupyter_client 6.1.3. jupyter_core 4.6.3. notebook 6.0.3. -----. Python 3.7.6 | packaged by conda-forge | (default, Mar 23 2020, 23:03:20) [GCC 7.3.0]. Linux-3.10.0-1127.18.2.el7.x86_64-x86_64-with-centos-7.8.2003-Core. 36 logical CPU cores. -----. Session information updated at 2021-01-27 17:16. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1612
https://github.com/scverse/scanpy/issues/1612:3077,security,updat,updated,3077,"d(""adata2.h5ad""). check_equal_adata_X(adata_run1_concat, adata_run2_concat). adata_run1_concat.X = adata_run1_concat.X.astype(np.float64). adata_run2_concat.X = adata_run2_concat.X.astype(np.float64). sc.pp.normalize_total(adata_run1_concat). sc.pp.normalize_total(adata_run2_concat). check_equal_adata_X(adata_run1_concat, adata_run2_concat). ```. Output:. ```pytb. True. True. True. True. True. True. ```. Like I said, we tested also the ""internal"" `scanpy` 10x 3k PBMC dataset. ```python. sc.datasets.pbmc3k(). pbmc3k_1 = sc.read_h5ad(""data/pbmc3k_raw.h5ad""). pbmc3k_2 = sc.read_h5ad(""data/pbmc3k_raw.h5ad""). check_equal_adata_X(pbmc3k_1, pbmc3k_2). sc.pp.normalize_total(pbmc3k_1). sc.pp.normalize_total(pbmc3k_2). check_equal_adata_X(pbmc3k_1, pbmc3k_2). ```. ```pytb. True. True. True. True. True. True. ```. #### Versions. <details>. -----. anndata 0.7.5. scanpy 1.6.1. sinfo 0.3.1. -----. anndata 0.7.5. backcall 0.1.0. cycler 0.10.0. cython_runtime NA. dateutil 2.8.1. decorator 4.4.2. get_version 2.1. h5py 2.10.0. importlib_metadata 1.6.0. ipykernel 5.2.1. ipython_genutils 0.2.0. ipywidgets 7.5.1. jedi 0.17.0. joblib 0.14.1. kiwisolver 1.2.0. legacy_api_wrap 1.2. llvmlite 0.31.0. matplotlib 3.2.1. mpl_toolkits NA. natsort 7.0.1. numba 0.48.0. numexpr 2.7.1. numpy 1.18.2. packaging 20.3. pandas 1.0.3. parso 0.7.0. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. prompt_toolkit 3.0.5. ptyprocess 0.6.0. pygments 2.6.1. pyparsing 2.4.7. pytz 2019.3. scanpy 1.6.1. scipy 1.4.1. setuptools_scm NA. sinfo 0.3.1. six 1.14.0. sklearn 0.22.2.post1. storemagic NA. tables 3.6.1. tornado 6.0.4. tqdm 4.45.0. traitlets 4.3.3. wcwidth NA. zipp NA. zmq 19.0.0. -----. IPython 7.13.0. jupyter_client 6.1.3. jupyter_core 4.6.3. notebook 6.0.3. -----. Python 3.7.6 | packaged by conda-forge | (default, Mar 23 2020, 23:03:20) [GCC 7.3.0]. Linux-3.10.0-1127.18.2.el7.x86_64-x86_64-with-centos-7.8.2003-Core. 36 logical CPU cores. -----. Session information updated at 2021-01-27 17:16. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1612
https://github.com/scverse/scanpy/issues/1612:1541,testability,test,tested,1541,"X.todense(). print(np.array_equal(tmp1, tmp2)). print((tmp1 == tmp2).all()). adata_run1_concat = sc.read_h5ad(""adata1.h5ad""). adata_run2_concat = sc.read_h5ad(""adata2.h5ad""). check_equal_adata_X(adata_run1_concat, adata_run2_concat). sc.pp.normalize_total(adata_run1_concat). sc.pp.normalize_total(adata_run2_concat). check_equal_adata_X(adata_run1_concat, adata_run2_concat). ```. Output:. ```pytb. True. True. True. False. False. False. ```. Here is the code that fixes this bug. ```python. adata_run1_concat = sc.read_h5ad(""adata1.h5ad""). adata_run2_concat = sc.read_h5ad(""adata2.h5ad""). check_equal_adata_X(adata_run1_concat, adata_run2_concat). adata_run1_concat.X = adata_run1_concat.X.astype(np.float64). adata_run2_concat.X = adata_run2_concat.X.astype(np.float64). sc.pp.normalize_total(adata_run1_concat). sc.pp.normalize_total(adata_run2_concat). check_equal_adata_X(adata_run1_concat, adata_run2_concat). ```. Output:. ```pytb. True. True. True. True. True. True. ```. Like I said, we tested also the ""internal"" `scanpy` 10x 3k PBMC dataset. ```python. sc.datasets.pbmc3k(). pbmc3k_1 = sc.read_h5ad(""data/pbmc3k_raw.h5ad""). pbmc3k_2 = sc.read_h5ad(""data/pbmc3k_raw.h5ad""). check_equal_adata_X(pbmc3k_1, pbmc3k_2). sc.pp.normalize_total(pbmc3k_1). sc.pp.normalize_total(pbmc3k_2). check_equal_adata_X(pbmc3k_1, pbmc3k_2). ```. ```pytb. True. True. True. True. True. True. ```. #### Versions. <details>. -----. anndata 0.7.5. scanpy 1.6.1. sinfo 0.3.1. -----. anndata 0.7.5. backcall 0.1.0. cycler 0.10.0. cython_runtime NA. dateutil 2.8.1. decorator 4.4.2. get_version 2.1. h5py 2.10.0. importlib_metadata 1.6.0. ipykernel 5.2.1. ipython_genutils 0.2.0. ipywidgets 7.5.1. jedi 0.17.0. joblib 0.14.1. kiwisolver 1.2.0. legacy_api_wrap 1.2. llvmlite 0.31.0. matplotlib 3.2.1. mpl_toolkits NA. natsort 7.0.1. numba 0.48.0. numexpr 2.7.1. numpy 1.18.2. packaging 20.3. pandas 1.0.3. parso 0.7.0. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. prompt_toolkit 3.0.5. ptyprocess 0.6.0. pygme",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1612
https://github.com/scverse/scanpy/issues/1612:3031,testability,log,logical,3031,"d(""adata2.h5ad""). check_equal_adata_X(adata_run1_concat, adata_run2_concat). adata_run1_concat.X = adata_run1_concat.X.astype(np.float64). adata_run2_concat.X = adata_run2_concat.X.astype(np.float64). sc.pp.normalize_total(adata_run1_concat). sc.pp.normalize_total(adata_run2_concat). check_equal_adata_X(adata_run1_concat, adata_run2_concat). ```. Output:. ```pytb. True. True. True. True. True. True. ```. Like I said, we tested also the ""internal"" `scanpy` 10x 3k PBMC dataset. ```python. sc.datasets.pbmc3k(). pbmc3k_1 = sc.read_h5ad(""data/pbmc3k_raw.h5ad""). pbmc3k_2 = sc.read_h5ad(""data/pbmc3k_raw.h5ad""). check_equal_adata_X(pbmc3k_1, pbmc3k_2). sc.pp.normalize_total(pbmc3k_1). sc.pp.normalize_total(pbmc3k_2). check_equal_adata_X(pbmc3k_1, pbmc3k_2). ```. ```pytb. True. True. True. True. True. True. ```. #### Versions. <details>. -----. anndata 0.7.5. scanpy 1.6.1. sinfo 0.3.1. -----. anndata 0.7.5. backcall 0.1.0. cycler 0.10.0. cython_runtime NA. dateutil 2.8.1. decorator 4.4.2. get_version 2.1. h5py 2.10.0. importlib_metadata 1.6.0. ipykernel 5.2.1. ipython_genutils 0.2.0. ipywidgets 7.5.1. jedi 0.17.0. joblib 0.14.1. kiwisolver 1.2.0. legacy_api_wrap 1.2. llvmlite 0.31.0. matplotlib 3.2.1. mpl_toolkits NA. natsort 7.0.1. numba 0.48.0. numexpr 2.7.1. numpy 1.18.2. packaging 20.3. pandas 1.0.3. parso 0.7.0. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. prompt_toolkit 3.0.5. ptyprocess 0.6.0. pygments 2.6.1. pyparsing 2.4.7. pytz 2019.3. scanpy 1.6.1. scipy 1.4.1. setuptools_scm NA. sinfo 0.3.1. six 1.14.0. sklearn 0.22.2.post1. storemagic NA. tables 3.6.1. tornado 6.0.4. tqdm 4.45.0. traitlets 4.3.3. wcwidth NA. zipp NA. zmq 19.0.0. -----. IPython 7.13.0. jupyter_client 6.1.3. jupyter_core 4.6.3. notebook 6.0.3. -----. Python 3.7.6 | packaged by conda-forge | (default, Mar 23 2020, 23:03:20) [GCC 7.3.0]. Linux-3.10.0-1127.18.2.el7.x86_64-x86_64-with-centos-7.8.2003-Core. 36 logical CPU cores. -----. Session information updated at 2021-01-27 17:16. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1612
https://github.com/scverse/scanpy/issues/1612:393,usability,Minim,Minimal,393,"Reproducibility with normalize_total; Hi,. We have encountered a bug regarding the reproducibility of the `normalize_total` in the latest version of `scanpy==1.6.11`. We also found this bug with `scanpy==1.4.4.post1`. This bug appears with an in-house dataset (containing 10x data and SMART-seq2 scaled data) and interestingly does not appear with the internal scanpy 10x 3k PBMC dataset. ### Minimal code sample. ```python. def check_equal_adata_X(adata1, adata2):. print((adata1.X!=adata2.X).nnz==0). tmp1 = adata1.X.todense(). tmp2 = adata2.X.todense(). print(np.array_equal(tmp1, tmp2)). print((tmp1 == tmp2).all()). adata_run1_concat = sc.read_h5ad(""adata1.h5ad""). adata_run2_concat = sc.read_h5ad(""adata2.h5ad""). check_equal_adata_X(adata_run1_concat, adata_run2_concat). sc.pp.normalize_total(adata_run1_concat). sc.pp.normalize_total(adata_run2_concat). check_equal_adata_X(adata_run1_concat, adata_run2_concat). ```. Output:. ```pytb. True. True. True. False. False. False. ```. Here is the code that fixes this bug. ```python. adata_run1_concat = sc.read_h5ad(""adata1.h5ad""). adata_run2_concat = sc.read_h5ad(""adata2.h5ad""). check_equal_adata_X(adata_run1_concat, adata_run2_concat). adata_run1_concat.X = adata_run1_concat.X.astype(np.float64). adata_run2_concat.X = adata_run2_concat.X.astype(np.float64). sc.pp.normalize_total(adata_run1_concat). sc.pp.normalize_total(adata_run2_concat). check_equal_adata_X(adata_run1_concat, adata_run2_concat). ```. Output:. ```pytb. True. True. True. True. True. True. ```. Like I said, we tested also the ""internal"" `scanpy` 10x 3k PBMC dataset. ```python. sc.datasets.pbmc3k(). pbmc3k_1 = sc.read_h5ad(""data/pbmc3k_raw.h5ad""). pbmc3k_2 = sc.read_h5ad(""data/pbmc3k_raw.h5ad""). check_equal_adata_X(pbmc3k_1, pbmc3k_2). sc.pp.normalize_total(pbmc3k_1). sc.pp.normalize_total(pbmc3k_2). check_equal_adata_X(pbmc3k_1, pbmc3k_2). ```. ```pytb. True. True. True. True. True. True. ```. #### Versions. <details>. -----. anndata 0.7.5. scanpy 1.6.1. sinfo ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1612
https://github.com/scverse/scanpy/issues/1613:19,deployability,version,version,19,"Report pynndescent version in sc.logging.print_header; Hi,. Thank you for the great tool. I think this is not a bug. . Recently I upgraded some packages and found my results were different from the previous runs. I figured out that it is caused by different versions of `pynndescent` (0.4.7 vs 0.5.1), which is recommended to use in UMAP. So I think `pynndescent` should be included in the output of `sc.logging.print_header()`. #### Versions. <details>. -----. anndata 0.7.5. scanpy 1.6.1. sinfo 0.3.1. -----. PIL 8.1.0. anndata 0.7.5. constants NA. cycler 0.10.0. cython_runtime NA. dateutil 2.8.1. get_version 2.1. h5py 3.1.0. highs_wrapper NA. igraph 0.8.3. joblib 1.0.0. kiwisolver 1.3.1. legacy_api_wrap 1.2. leidenalg 0.8.3. llvmlite 0.35.0. louvain 0.7.0. matplotlib 3.3.3. mpl_toolkits NA. natsort 7.1.1. numba 0.52.0. numexpr 2.7.2. numpy 1.19.5. packaging 20.8. pandas 1.2.1. pkg_resources NA. pynndescent 0.5.1. pyparsing 2.4.7. pytz 2020.5. scanpy 1.6.1. scipy 1.6.0. setuptools_scm NA. sinfo 0.3.1. six 1.15.0. sklearn 0.24.1. statsmodels 0.12.1. tables 3.6.1. texttable 1.6.3. umap 0.4.6. -----. Python 3.8.5 (default, Sep 4 2020, 07:30:14) [GCC 7.3.0]. Linux-3.10.0-1160.11.1.el7.x86_64-x86_64-with-glibc2.10. 40 logical CPU cores, x86_64. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1613
https://github.com/scverse/scanpy/issues/1613:33,deployability,log,logging,33,"Report pynndescent version in sc.logging.print_header; Hi,. Thank you for the great tool. I think this is not a bug. . Recently I upgraded some packages and found my results were different from the previous runs. I figured out that it is caused by different versions of `pynndescent` (0.4.7 vs 0.5.1), which is recommended to use in UMAP. So I think `pynndescent` should be included in the output of `sc.logging.print_header()`. #### Versions. <details>. -----. anndata 0.7.5. scanpy 1.6.1. sinfo 0.3.1. -----. PIL 8.1.0. anndata 0.7.5. constants NA. cycler 0.10.0. cython_runtime NA. dateutil 2.8.1. get_version 2.1. h5py 3.1.0. highs_wrapper NA. igraph 0.8.3. joblib 1.0.0. kiwisolver 1.3.1. legacy_api_wrap 1.2. leidenalg 0.8.3. llvmlite 0.35.0. louvain 0.7.0. matplotlib 3.3.3. mpl_toolkits NA. natsort 7.1.1. numba 0.52.0. numexpr 2.7.2. numpy 1.19.5. packaging 20.8. pandas 1.2.1. pkg_resources NA. pynndescent 0.5.1. pyparsing 2.4.7. pytz 2020.5. scanpy 1.6.1. scipy 1.6.0. setuptools_scm NA. sinfo 0.3.1. six 1.15.0. sklearn 0.24.1. statsmodels 0.12.1. tables 3.6.1. texttable 1.6.3. umap 0.4.6. -----. Python 3.8.5 (default, Sep 4 2020, 07:30:14) [GCC 7.3.0]. Linux-3.10.0-1160.11.1.el7.x86_64-x86_64-with-glibc2.10. 40 logical CPU cores, x86_64. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1613
https://github.com/scverse/scanpy/issues/1613:130,deployability,upgrad,upgraded,130,"Report pynndescent version in sc.logging.print_header; Hi,. Thank you for the great tool. I think this is not a bug. . Recently I upgraded some packages and found my results were different from the previous runs. I figured out that it is caused by different versions of `pynndescent` (0.4.7 vs 0.5.1), which is recommended to use in UMAP. So I think `pynndescent` should be included in the output of `sc.logging.print_header()`. #### Versions. <details>. -----. anndata 0.7.5. scanpy 1.6.1. sinfo 0.3.1. -----. PIL 8.1.0. anndata 0.7.5. constants NA. cycler 0.10.0. cython_runtime NA. dateutil 2.8.1. get_version 2.1. h5py 3.1.0. highs_wrapper NA. igraph 0.8.3. joblib 1.0.0. kiwisolver 1.3.1. legacy_api_wrap 1.2. leidenalg 0.8.3. llvmlite 0.35.0. louvain 0.7.0. matplotlib 3.3.3. mpl_toolkits NA. natsort 7.1.1. numba 0.52.0. numexpr 2.7.2. numpy 1.19.5. packaging 20.8. pandas 1.2.1. pkg_resources NA. pynndescent 0.5.1. pyparsing 2.4.7. pytz 2020.5. scanpy 1.6.1. scipy 1.6.0. setuptools_scm NA. sinfo 0.3.1. six 1.15.0. sklearn 0.24.1. statsmodels 0.12.1. tables 3.6.1. texttable 1.6.3. umap 0.4.6. -----. Python 3.8.5 (default, Sep 4 2020, 07:30:14) [GCC 7.3.0]. Linux-3.10.0-1160.11.1.el7.x86_64-x86_64-with-glibc2.10. 40 logical CPU cores, x86_64. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1613
https://github.com/scverse/scanpy/issues/1613:258,deployability,version,versions,258,"Report pynndescent version in sc.logging.print_header; Hi,. Thank you for the great tool. I think this is not a bug. . Recently I upgraded some packages and found my results were different from the previous runs. I figured out that it is caused by different versions of `pynndescent` (0.4.7 vs 0.5.1), which is recommended to use in UMAP. So I think `pynndescent` should be included in the output of `sc.logging.print_header()`. #### Versions. <details>. -----. anndata 0.7.5. scanpy 1.6.1. sinfo 0.3.1. -----. PIL 8.1.0. anndata 0.7.5. constants NA. cycler 0.10.0. cython_runtime NA. dateutil 2.8.1. get_version 2.1. h5py 3.1.0. highs_wrapper NA. igraph 0.8.3. joblib 1.0.0. kiwisolver 1.3.1. legacy_api_wrap 1.2. leidenalg 0.8.3. llvmlite 0.35.0. louvain 0.7.0. matplotlib 3.3.3. mpl_toolkits NA. natsort 7.1.1. numba 0.52.0. numexpr 2.7.2. numpy 1.19.5. packaging 20.8. pandas 1.2.1. pkg_resources NA. pynndescent 0.5.1. pyparsing 2.4.7. pytz 2020.5. scanpy 1.6.1. scipy 1.6.0. setuptools_scm NA. sinfo 0.3.1. six 1.15.0. sklearn 0.24.1. statsmodels 0.12.1. tables 3.6.1. texttable 1.6.3. umap 0.4.6. -----. Python 3.8.5 (default, Sep 4 2020, 07:30:14) [GCC 7.3.0]. Linux-3.10.0-1160.11.1.el7.x86_64-x86_64-with-glibc2.10. 40 logical CPU cores, x86_64. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1613
https://github.com/scverse/scanpy/issues/1613:404,deployability,log,logging,404,"Report pynndescent version in sc.logging.print_header; Hi,. Thank you for the great tool. I think this is not a bug. . Recently I upgraded some packages and found my results were different from the previous runs. I figured out that it is caused by different versions of `pynndescent` (0.4.7 vs 0.5.1), which is recommended to use in UMAP. So I think `pynndescent` should be included in the output of `sc.logging.print_header()`. #### Versions. <details>. -----. anndata 0.7.5. scanpy 1.6.1. sinfo 0.3.1. -----. PIL 8.1.0. anndata 0.7.5. constants NA. cycler 0.10.0. cython_runtime NA. dateutil 2.8.1. get_version 2.1. h5py 3.1.0. highs_wrapper NA. igraph 0.8.3. joblib 1.0.0. kiwisolver 1.3.1. legacy_api_wrap 1.2. leidenalg 0.8.3. llvmlite 0.35.0. louvain 0.7.0. matplotlib 3.3.3. mpl_toolkits NA. natsort 7.1.1. numba 0.52.0. numexpr 2.7.2. numpy 1.19.5. packaging 20.8. pandas 1.2.1. pkg_resources NA. pynndescent 0.5.1. pyparsing 2.4.7. pytz 2020.5. scanpy 1.6.1. scipy 1.6.0. setuptools_scm NA. sinfo 0.3.1. six 1.15.0. sklearn 0.24.1. statsmodels 0.12.1. tables 3.6.1. texttable 1.6.3. umap 0.4.6. -----. Python 3.8.5 (default, Sep 4 2020, 07:30:14) [GCC 7.3.0]. Linux-3.10.0-1160.11.1.el7.x86_64-x86_64-with-glibc2.10. 40 logical CPU cores, x86_64. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1613
https://github.com/scverse/scanpy/issues/1613:434,deployability,Version,Versions,434,"Report pynndescent version in sc.logging.print_header; Hi,. Thank you for the great tool. I think this is not a bug. . Recently I upgraded some packages and found my results were different from the previous runs. I figured out that it is caused by different versions of `pynndescent` (0.4.7 vs 0.5.1), which is recommended to use in UMAP. So I think `pynndescent` should be included in the output of `sc.logging.print_header()`. #### Versions. <details>. -----. anndata 0.7.5. scanpy 1.6.1. sinfo 0.3.1. -----. PIL 8.1.0. anndata 0.7.5. constants NA. cycler 0.10.0. cython_runtime NA. dateutil 2.8.1. get_version 2.1. h5py 3.1.0. highs_wrapper NA. igraph 0.8.3. joblib 1.0.0. kiwisolver 1.3.1. legacy_api_wrap 1.2. leidenalg 0.8.3. llvmlite 0.35.0. louvain 0.7.0. matplotlib 3.3.3. mpl_toolkits NA. natsort 7.1.1. numba 0.52.0. numexpr 2.7.2. numpy 1.19.5. packaging 20.8. pandas 1.2.1. pkg_resources NA. pynndescent 0.5.1. pyparsing 2.4.7. pytz 2020.5. scanpy 1.6.1. scipy 1.6.0. setuptools_scm NA. sinfo 0.3.1. six 1.15.0. sklearn 0.24.1. statsmodels 0.12.1. tables 3.6.1. texttable 1.6.3. umap 0.4.6. -----. Python 3.8.5 (default, Sep 4 2020, 07:30:14) [GCC 7.3.0]. Linux-3.10.0-1160.11.1.el7.x86_64-x86_64-with-glibc2.10. 40 logical CPU cores, x86_64. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1613
https://github.com/scverse/scanpy/issues/1613:1229,deployability,log,logical,1229,"Report pynndescent version in sc.logging.print_header; Hi,. Thank you for the great tool. I think this is not a bug. . Recently I upgraded some packages and found my results were different from the previous runs. I figured out that it is caused by different versions of `pynndescent` (0.4.7 vs 0.5.1), which is recommended to use in UMAP. So I think `pynndescent` should be included in the output of `sc.logging.print_header()`. #### Versions. <details>. -----. anndata 0.7.5. scanpy 1.6.1. sinfo 0.3.1. -----. PIL 8.1.0. anndata 0.7.5. constants NA. cycler 0.10.0. cython_runtime NA. dateutil 2.8.1. get_version 2.1. h5py 3.1.0. highs_wrapper NA. igraph 0.8.3. joblib 1.0.0. kiwisolver 1.3.1. legacy_api_wrap 1.2. leidenalg 0.8.3. llvmlite 0.35.0. louvain 0.7.0. matplotlib 3.3.3. mpl_toolkits NA. natsort 7.1.1. numba 0.52.0. numexpr 2.7.2. numpy 1.19.5. packaging 20.8. pandas 1.2.1. pkg_resources NA. pynndescent 0.5.1. pyparsing 2.4.7. pytz 2020.5. scanpy 1.6.1. scipy 1.6.0. setuptools_scm NA. sinfo 0.3.1. six 1.15.0. sklearn 0.24.1. statsmodels 0.12.1. tables 3.6.1. texttable 1.6.3. umap 0.4.6. -----. Python 3.8.5 (default, Sep 4 2020, 07:30:14) [GCC 7.3.0]. Linux-3.10.0-1160.11.1.el7.x86_64-x86_64-with-glibc2.10. 40 logical CPU cores, x86_64. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1613
https://github.com/scverse/scanpy/issues/1613:1237,energy efficiency,CPU,CPU,1237,"Report pynndescent version in sc.logging.print_header; Hi,. Thank you for the great tool. I think this is not a bug. . Recently I upgraded some packages and found my results were different from the previous runs. I figured out that it is caused by different versions of `pynndescent` (0.4.7 vs 0.5.1), which is recommended to use in UMAP. So I think `pynndescent` should be included in the output of `sc.logging.print_header()`. #### Versions. <details>. -----. anndata 0.7.5. scanpy 1.6.1. sinfo 0.3.1. -----. PIL 8.1.0. anndata 0.7.5. constants NA. cycler 0.10.0. cython_runtime NA. dateutil 2.8.1. get_version 2.1. h5py 3.1.0. highs_wrapper NA. igraph 0.8.3. joblib 1.0.0. kiwisolver 1.3.1. legacy_api_wrap 1.2. leidenalg 0.8.3. llvmlite 0.35.0. louvain 0.7.0. matplotlib 3.3.3. mpl_toolkits NA. natsort 7.1.1. numba 0.52.0. numexpr 2.7.2. numpy 1.19.5. packaging 20.8. pandas 1.2.1. pkg_resources NA. pynndescent 0.5.1. pyparsing 2.4.7. pytz 2020.5. scanpy 1.6.1. scipy 1.6.0. setuptools_scm NA. sinfo 0.3.1. six 1.15.0. sklearn 0.24.1. statsmodels 0.12.1. tables 3.6.1. texttable 1.6.3. umap 0.4.6. -----. Python 3.8.5 (default, Sep 4 2020, 07:30:14) [GCC 7.3.0]. Linux-3.10.0-1160.11.1.el7.x86_64-x86_64-with-glibc2.10. 40 logical CPU cores, x86_64. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1613
https://github.com/scverse/scanpy/issues/1613:1241,energy efficiency,core,cores,1241,"Report pynndescent version in sc.logging.print_header; Hi,. Thank you for the great tool. I think this is not a bug. . Recently I upgraded some packages and found my results were different from the previous runs. I figured out that it is caused by different versions of `pynndescent` (0.4.7 vs 0.5.1), which is recommended to use in UMAP. So I think `pynndescent` should be included in the output of `sc.logging.print_header()`. #### Versions. <details>. -----. anndata 0.7.5. scanpy 1.6.1. sinfo 0.3.1. -----. PIL 8.1.0. anndata 0.7.5. constants NA. cycler 0.10.0. cython_runtime NA. dateutil 2.8.1. get_version 2.1. h5py 3.1.0. highs_wrapper NA. igraph 0.8.3. joblib 1.0.0. kiwisolver 1.3.1. legacy_api_wrap 1.2. leidenalg 0.8.3. llvmlite 0.35.0. louvain 0.7.0. matplotlib 3.3.3. mpl_toolkits NA. natsort 7.1.1. numba 0.52.0. numexpr 2.7.2. numpy 1.19.5. packaging 20.8. pandas 1.2.1. pkg_resources NA. pynndescent 0.5.1. pyparsing 2.4.7. pytz 2020.5. scanpy 1.6.1. scipy 1.6.0. setuptools_scm NA. sinfo 0.3.1. six 1.15.0. sklearn 0.24.1. statsmodels 0.12.1. tables 3.6.1. texttable 1.6.3. umap 0.4.6. -----. Python 3.8.5 (default, Sep 4 2020, 07:30:14) [GCC 7.3.0]. Linux-3.10.0-1160.11.1.el7.x86_64-x86_64-with-glibc2.10. 40 logical CPU cores, x86_64. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1613
https://github.com/scverse/scanpy/issues/1613:19,integrability,version,version,19,"Report pynndescent version in sc.logging.print_header; Hi,. Thank you for the great tool. I think this is not a bug. . Recently I upgraded some packages and found my results were different from the previous runs. I figured out that it is caused by different versions of `pynndescent` (0.4.7 vs 0.5.1), which is recommended to use in UMAP. So I think `pynndescent` should be included in the output of `sc.logging.print_header()`. #### Versions. <details>. -----. anndata 0.7.5. scanpy 1.6.1. sinfo 0.3.1. -----. PIL 8.1.0. anndata 0.7.5. constants NA. cycler 0.10.0. cython_runtime NA. dateutil 2.8.1. get_version 2.1. h5py 3.1.0. highs_wrapper NA. igraph 0.8.3. joblib 1.0.0. kiwisolver 1.3.1. legacy_api_wrap 1.2. leidenalg 0.8.3. llvmlite 0.35.0. louvain 0.7.0. matplotlib 3.3.3. mpl_toolkits NA. natsort 7.1.1. numba 0.52.0. numexpr 2.7.2. numpy 1.19.5. packaging 20.8. pandas 1.2.1. pkg_resources NA. pynndescent 0.5.1. pyparsing 2.4.7. pytz 2020.5. scanpy 1.6.1. scipy 1.6.0. setuptools_scm NA. sinfo 0.3.1. six 1.15.0. sklearn 0.24.1. statsmodels 0.12.1. tables 3.6.1. texttable 1.6.3. umap 0.4.6. -----. Python 3.8.5 (default, Sep 4 2020, 07:30:14) [GCC 7.3.0]. Linux-3.10.0-1160.11.1.el7.x86_64-x86_64-with-glibc2.10. 40 logical CPU cores, x86_64. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1613
https://github.com/scverse/scanpy/issues/1613:258,integrability,version,versions,258,"Report pynndescent version in sc.logging.print_header; Hi,. Thank you for the great tool. I think this is not a bug. . Recently I upgraded some packages and found my results were different from the previous runs. I figured out that it is caused by different versions of `pynndescent` (0.4.7 vs 0.5.1), which is recommended to use in UMAP. So I think `pynndescent` should be included in the output of `sc.logging.print_header()`. #### Versions. <details>. -----. anndata 0.7.5. scanpy 1.6.1. sinfo 0.3.1. -----. PIL 8.1.0. anndata 0.7.5. constants NA. cycler 0.10.0. cython_runtime NA. dateutil 2.8.1. get_version 2.1. h5py 3.1.0. highs_wrapper NA. igraph 0.8.3. joblib 1.0.0. kiwisolver 1.3.1. legacy_api_wrap 1.2. leidenalg 0.8.3. llvmlite 0.35.0. louvain 0.7.0. matplotlib 3.3.3. mpl_toolkits NA. natsort 7.1.1. numba 0.52.0. numexpr 2.7.2. numpy 1.19.5. packaging 20.8. pandas 1.2.1. pkg_resources NA. pynndescent 0.5.1. pyparsing 2.4.7. pytz 2020.5. scanpy 1.6.1. scipy 1.6.0. setuptools_scm NA. sinfo 0.3.1. six 1.15.0. sklearn 0.24.1. statsmodels 0.12.1. tables 3.6.1. texttable 1.6.3. umap 0.4.6. -----. Python 3.8.5 (default, Sep 4 2020, 07:30:14) [GCC 7.3.0]. Linux-3.10.0-1160.11.1.el7.x86_64-x86_64-with-glibc2.10. 40 logical CPU cores, x86_64. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1613
https://github.com/scverse/scanpy/issues/1613:434,integrability,Version,Versions,434,"Report pynndescent version in sc.logging.print_header; Hi,. Thank you for the great tool. I think this is not a bug. . Recently I upgraded some packages and found my results were different from the previous runs. I figured out that it is caused by different versions of `pynndescent` (0.4.7 vs 0.5.1), which is recommended to use in UMAP. So I think `pynndescent` should be included in the output of `sc.logging.print_header()`. #### Versions. <details>. -----. anndata 0.7.5. scanpy 1.6.1. sinfo 0.3.1. -----. PIL 8.1.0. anndata 0.7.5. constants NA. cycler 0.10.0. cython_runtime NA. dateutil 2.8.1. get_version 2.1. h5py 3.1.0. highs_wrapper NA. igraph 0.8.3. joblib 1.0.0. kiwisolver 1.3.1. legacy_api_wrap 1.2. leidenalg 0.8.3. llvmlite 0.35.0. louvain 0.7.0. matplotlib 3.3.3. mpl_toolkits NA. natsort 7.1.1. numba 0.52.0. numexpr 2.7.2. numpy 1.19.5. packaging 20.8. pandas 1.2.1. pkg_resources NA. pynndescent 0.5.1. pyparsing 2.4.7. pytz 2020.5. scanpy 1.6.1. scipy 1.6.0. setuptools_scm NA. sinfo 0.3.1. six 1.15.0. sklearn 0.24.1. statsmodels 0.12.1. tables 3.6.1. texttable 1.6.3. umap 0.4.6. -----. Python 3.8.5 (default, Sep 4 2020, 07:30:14) [GCC 7.3.0]. Linux-3.10.0-1160.11.1.el7.x86_64-x86_64-with-glibc2.10. 40 logical CPU cores, x86_64. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1613
https://github.com/scverse/scanpy/issues/1613:19,modifiability,version,version,19,"Report pynndescent version in sc.logging.print_header; Hi,. Thank you for the great tool. I think this is not a bug. . Recently I upgraded some packages and found my results were different from the previous runs. I figured out that it is caused by different versions of `pynndescent` (0.4.7 vs 0.5.1), which is recommended to use in UMAP. So I think `pynndescent` should be included in the output of `sc.logging.print_header()`. #### Versions. <details>. -----. anndata 0.7.5. scanpy 1.6.1. sinfo 0.3.1. -----. PIL 8.1.0. anndata 0.7.5. constants NA. cycler 0.10.0. cython_runtime NA. dateutil 2.8.1. get_version 2.1. h5py 3.1.0. highs_wrapper NA. igraph 0.8.3. joblib 1.0.0. kiwisolver 1.3.1. legacy_api_wrap 1.2. leidenalg 0.8.3. llvmlite 0.35.0. louvain 0.7.0. matplotlib 3.3.3. mpl_toolkits NA. natsort 7.1.1. numba 0.52.0. numexpr 2.7.2. numpy 1.19.5. packaging 20.8. pandas 1.2.1. pkg_resources NA. pynndescent 0.5.1. pyparsing 2.4.7. pytz 2020.5. scanpy 1.6.1. scipy 1.6.0. setuptools_scm NA. sinfo 0.3.1. six 1.15.0. sklearn 0.24.1. statsmodels 0.12.1. tables 3.6.1. texttable 1.6.3. umap 0.4.6. -----. Python 3.8.5 (default, Sep 4 2020, 07:30:14) [GCC 7.3.0]. Linux-3.10.0-1160.11.1.el7.x86_64-x86_64-with-glibc2.10. 40 logical CPU cores, x86_64. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1613
https://github.com/scverse/scanpy/issues/1613:130,modifiability,upgrad,upgraded,130,"Report pynndescent version in sc.logging.print_header; Hi,. Thank you for the great tool. I think this is not a bug. . Recently I upgraded some packages and found my results were different from the previous runs. I figured out that it is caused by different versions of `pynndescent` (0.4.7 vs 0.5.1), which is recommended to use in UMAP. So I think `pynndescent` should be included in the output of `sc.logging.print_header()`. #### Versions. <details>. -----. anndata 0.7.5. scanpy 1.6.1. sinfo 0.3.1. -----. PIL 8.1.0. anndata 0.7.5. constants NA. cycler 0.10.0. cython_runtime NA. dateutil 2.8.1. get_version 2.1. h5py 3.1.0. highs_wrapper NA. igraph 0.8.3. joblib 1.0.0. kiwisolver 1.3.1. legacy_api_wrap 1.2. leidenalg 0.8.3. llvmlite 0.35.0. louvain 0.7.0. matplotlib 3.3.3. mpl_toolkits NA. natsort 7.1.1. numba 0.52.0. numexpr 2.7.2. numpy 1.19.5. packaging 20.8. pandas 1.2.1. pkg_resources NA. pynndescent 0.5.1. pyparsing 2.4.7. pytz 2020.5. scanpy 1.6.1. scipy 1.6.0. setuptools_scm NA. sinfo 0.3.1. six 1.15.0. sklearn 0.24.1. statsmodels 0.12.1. tables 3.6.1. texttable 1.6.3. umap 0.4.6. -----. Python 3.8.5 (default, Sep 4 2020, 07:30:14) [GCC 7.3.0]. Linux-3.10.0-1160.11.1.el7.x86_64-x86_64-with-glibc2.10. 40 logical CPU cores, x86_64. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1613
https://github.com/scverse/scanpy/issues/1613:144,modifiability,pac,packages,144,"Report pynndescent version in sc.logging.print_header; Hi,. Thank you for the great tool. I think this is not a bug. . Recently I upgraded some packages and found my results were different from the previous runs. I figured out that it is caused by different versions of `pynndescent` (0.4.7 vs 0.5.1), which is recommended to use in UMAP. So I think `pynndescent` should be included in the output of `sc.logging.print_header()`. #### Versions. <details>. -----. anndata 0.7.5. scanpy 1.6.1. sinfo 0.3.1. -----. PIL 8.1.0. anndata 0.7.5. constants NA. cycler 0.10.0. cython_runtime NA. dateutil 2.8.1. get_version 2.1. h5py 3.1.0. highs_wrapper NA. igraph 0.8.3. joblib 1.0.0. kiwisolver 1.3.1. legacy_api_wrap 1.2. leidenalg 0.8.3. llvmlite 0.35.0. louvain 0.7.0. matplotlib 3.3.3. mpl_toolkits NA. natsort 7.1.1. numba 0.52.0. numexpr 2.7.2. numpy 1.19.5. packaging 20.8. pandas 1.2.1. pkg_resources NA. pynndescent 0.5.1. pyparsing 2.4.7. pytz 2020.5. scanpy 1.6.1. scipy 1.6.0. setuptools_scm NA. sinfo 0.3.1. six 1.15.0. sklearn 0.24.1. statsmodels 0.12.1. tables 3.6.1. texttable 1.6.3. umap 0.4.6. -----. Python 3.8.5 (default, Sep 4 2020, 07:30:14) [GCC 7.3.0]. Linux-3.10.0-1160.11.1.el7.x86_64-x86_64-with-glibc2.10. 40 logical CPU cores, x86_64. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1613
https://github.com/scverse/scanpy/issues/1613:258,modifiability,version,versions,258,"Report pynndescent version in sc.logging.print_header; Hi,. Thank you for the great tool. I think this is not a bug. . Recently I upgraded some packages and found my results were different from the previous runs. I figured out that it is caused by different versions of `pynndescent` (0.4.7 vs 0.5.1), which is recommended to use in UMAP. So I think `pynndescent` should be included in the output of `sc.logging.print_header()`. #### Versions. <details>. -----. anndata 0.7.5. scanpy 1.6.1. sinfo 0.3.1. -----. PIL 8.1.0. anndata 0.7.5. constants NA. cycler 0.10.0. cython_runtime NA. dateutil 2.8.1. get_version 2.1. h5py 3.1.0. highs_wrapper NA. igraph 0.8.3. joblib 1.0.0. kiwisolver 1.3.1. legacy_api_wrap 1.2. leidenalg 0.8.3. llvmlite 0.35.0. louvain 0.7.0. matplotlib 3.3.3. mpl_toolkits NA. natsort 7.1.1. numba 0.52.0. numexpr 2.7.2. numpy 1.19.5. packaging 20.8. pandas 1.2.1. pkg_resources NA. pynndescent 0.5.1. pyparsing 2.4.7. pytz 2020.5. scanpy 1.6.1. scipy 1.6.0. setuptools_scm NA. sinfo 0.3.1. six 1.15.0. sklearn 0.24.1. statsmodels 0.12.1. tables 3.6.1. texttable 1.6.3. umap 0.4.6. -----. Python 3.8.5 (default, Sep 4 2020, 07:30:14) [GCC 7.3.0]. Linux-3.10.0-1160.11.1.el7.x86_64-x86_64-with-glibc2.10. 40 logical CPU cores, x86_64. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1613
https://github.com/scverse/scanpy/issues/1613:434,modifiability,Version,Versions,434,"Report pynndescent version in sc.logging.print_header; Hi,. Thank you for the great tool. I think this is not a bug. . Recently I upgraded some packages and found my results were different from the previous runs. I figured out that it is caused by different versions of `pynndescent` (0.4.7 vs 0.5.1), which is recommended to use in UMAP. So I think `pynndescent` should be included in the output of `sc.logging.print_header()`. #### Versions. <details>. -----. anndata 0.7.5. scanpy 1.6.1. sinfo 0.3.1. -----. PIL 8.1.0. anndata 0.7.5. constants NA. cycler 0.10.0. cython_runtime NA. dateutil 2.8.1. get_version 2.1. h5py 3.1.0. highs_wrapper NA. igraph 0.8.3. joblib 1.0.0. kiwisolver 1.3.1. legacy_api_wrap 1.2. leidenalg 0.8.3. llvmlite 0.35.0. louvain 0.7.0. matplotlib 3.3.3. mpl_toolkits NA. natsort 7.1.1. numba 0.52.0. numexpr 2.7.2. numpy 1.19.5. packaging 20.8. pandas 1.2.1. pkg_resources NA. pynndescent 0.5.1. pyparsing 2.4.7. pytz 2020.5. scanpy 1.6.1. scipy 1.6.0. setuptools_scm NA. sinfo 0.3.1. six 1.15.0. sklearn 0.24.1. statsmodels 0.12.1. tables 3.6.1. texttable 1.6.3. umap 0.4.6. -----. Python 3.8.5 (default, Sep 4 2020, 07:30:14) [GCC 7.3.0]. Linux-3.10.0-1160.11.1.el7.x86_64-x86_64-with-glibc2.10. 40 logical CPU cores, x86_64. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1613
https://github.com/scverse/scanpy/issues/1613:857,modifiability,pac,packaging,857,"Report pynndescent version in sc.logging.print_header; Hi,. Thank you for the great tool. I think this is not a bug. . Recently I upgraded some packages and found my results were different from the previous runs. I figured out that it is caused by different versions of `pynndescent` (0.4.7 vs 0.5.1), which is recommended to use in UMAP. So I think `pynndescent` should be included in the output of `sc.logging.print_header()`. #### Versions. <details>. -----. anndata 0.7.5. scanpy 1.6.1. sinfo 0.3.1. -----. PIL 8.1.0. anndata 0.7.5. constants NA. cycler 0.10.0. cython_runtime NA. dateutil 2.8.1. get_version 2.1. h5py 3.1.0. highs_wrapper NA. igraph 0.8.3. joblib 1.0.0. kiwisolver 1.3.1. legacy_api_wrap 1.2. leidenalg 0.8.3. llvmlite 0.35.0. louvain 0.7.0. matplotlib 3.3.3. mpl_toolkits NA. natsort 7.1.1. numba 0.52.0. numexpr 2.7.2. numpy 1.19.5. packaging 20.8. pandas 1.2.1. pkg_resources NA. pynndescent 0.5.1. pyparsing 2.4.7. pytz 2020.5. scanpy 1.6.1. scipy 1.6.0. setuptools_scm NA. sinfo 0.3.1. six 1.15.0. sklearn 0.24.1. statsmodels 0.12.1. tables 3.6.1. texttable 1.6.3. umap 0.4.6. -----. Python 3.8.5 (default, Sep 4 2020, 07:30:14) [GCC 7.3.0]. Linux-3.10.0-1160.11.1.el7.x86_64-x86_64-with-glibc2.10. 40 logical CPU cores, x86_64. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1613
https://github.com/scverse/scanpy/issues/1613:1237,performance,CPU,CPU,1237,"Report pynndescent version in sc.logging.print_header; Hi,. Thank you for the great tool. I think this is not a bug. . Recently I upgraded some packages and found my results were different from the previous runs. I figured out that it is caused by different versions of `pynndescent` (0.4.7 vs 0.5.1), which is recommended to use in UMAP. So I think `pynndescent` should be included in the output of `sc.logging.print_header()`. #### Versions. <details>. -----. anndata 0.7.5. scanpy 1.6.1. sinfo 0.3.1. -----. PIL 8.1.0. anndata 0.7.5. constants NA. cycler 0.10.0. cython_runtime NA. dateutil 2.8.1. get_version 2.1. h5py 3.1.0. highs_wrapper NA. igraph 0.8.3. joblib 1.0.0. kiwisolver 1.3.1. legacy_api_wrap 1.2. leidenalg 0.8.3. llvmlite 0.35.0. louvain 0.7.0. matplotlib 3.3.3. mpl_toolkits NA. natsort 7.1.1. numba 0.52.0. numexpr 2.7.2. numpy 1.19.5. packaging 20.8. pandas 1.2.1. pkg_resources NA. pynndescent 0.5.1. pyparsing 2.4.7. pytz 2020.5. scanpy 1.6.1. scipy 1.6.0. setuptools_scm NA. sinfo 0.3.1. six 1.15.0. sklearn 0.24.1. statsmodels 0.12.1. tables 3.6.1. texttable 1.6.3. umap 0.4.6. -----. Python 3.8.5 (default, Sep 4 2020, 07:30:14) [GCC 7.3.0]. Linux-3.10.0-1160.11.1.el7.x86_64-x86_64-with-glibc2.10. 40 logical CPU cores, x86_64. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1613
https://github.com/scverse/scanpy/issues/1613:33,safety,log,logging,33,"Report pynndescent version in sc.logging.print_header; Hi,. Thank you for the great tool. I think this is not a bug. . Recently I upgraded some packages and found my results were different from the previous runs. I figured out that it is caused by different versions of `pynndescent` (0.4.7 vs 0.5.1), which is recommended to use in UMAP. So I think `pynndescent` should be included in the output of `sc.logging.print_header()`. #### Versions. <details>. -----. anndata 0.7.5. scanpy 1.6.1. sinfo 0.3.1. -----. PIL 8.1.0. anndata 0.7.5. constants NA. cycler 0.10.0. cython_runtime NA. dateutil 2.8.1. get_version 2.1. h5py 3.1.0. highs_wrapper NA. igraph 0.8.3. joblib 1.0.0. kiwisolver 1.3.1. legacy_api_wrap 1.2. leidenalg 0.8.3. llvmlite 0.35.0. louvain 0.7.0. matplotlib 3.3.3. mpl_toolkits NA. natsort 7.1.1. numba 0.52.0. numexpr 2.7.2. numpy 1.19.5. packaging 20.8. pandas 1.2.1. pkg_resources NA. pynndescent 0.5.1. pyparsing 2.4.7. pytz 2020.5. scanpy 1.6.1. scipy 1.6.0. setuptools_scm NA. sinfo 0.3.1. six 1.15.0. sklearn 0.24.1. statsmodels 0.12.1. tables 3.6.1. texttable 1.6.3. umap 0.4.6. -----. Python 3.8.5 (default, Sep 4 2020, 07:30:14) [GCC 7.3.0]. Linux-3.10.0-1160.11.1.el7.x86_64-x86_64-with-glibc2.10. 40 logical CPU cores, x86_64. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1613
https://github.com/scverse/scanpy/issues/1613:404,safety,log,logging,404,"Report pynndescent version in sc.logging.print_header; Hi,. Thank you for the great tool. I think this is not a bug. . Recently I upgraded some packages and found my results were different from the previous runs. I figured out that it is caused by different versions of `pynndescent` (0.4.7 vs 0.5.1), which is recommended to use in UMAP. So I think `pynndescent` should be included in the output of `sc.logging.print_header()`. #### Versions. <details>. -----. anndata 0.7.5. scanpy 1.6.1. sinfo 0.3.1. -----. PIL 8.1.0. anndata 0.7.5. constants NA. cycler 0.10.0. cython_runtime NA. dateutil 2.8.1. get_version 2.1. h5py 3.1.0. highs_wrapper NA. igraph 0.8.3. joblib 1.0.0. kiwisolver 1.3.1. legacy_api_wrap 1.2. leidenalg 0.8.3. llvmlite 0.35.0. louvain 0.7.0. matplotlib 3.3.3. mpl_toolkits NA. natsort 7.1.1. numba 0.52.0. numexpr 2.7.2. numpy 1.19.5. packaging 20.8. pandas 1.2.1. pkg_resources NA. pynndescent 0.5.1. pyparsing 2.4.7. pytz 2020.5. scanpy 1.6.1. scipy 1.6.0. setuptools_scm NA. sinfo 0.3.1. six 1.15.0. sklearn 0.24.1. statsmodels 0.12.1. tables 3.6.1. texttable 1.6.3. umap 0.4.6. -----. Python 3.8.5 (default, Sep 4 2020, 07:30:14) [GCC 7.3.0]. Linux-3.10.0-1160.11.1.el7.x86_64-x86_64-with-glibc2.10. 40 logical CPU cores, x86_64. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1613
https://github.com/scverse/scanpy/issues/1613:1229,safety,log,logical,1229,"Report pynndescent version in sc.logging.print_header; Hi,. Thank you for the great tool. I think this is not a bug. . Recently I upgraded some packages and found my results were different from the previous runs. I figured out that it is caused by different versions of `pynndescent` (0.4.7 vs 0.5.1), which is recommended to use in UMAP. So I think `pynndescent` should be included in the output of `sc.logging.print_header()`. #### Versions. <details>. -----. anndata 0.7.5. scanpy 1.6.1. sinfo 0.3.1. -----. PIL 8.1.0. anndata 0.7.5. constants NA. cycler 0.10.0. cython_runtime NA. dateutil 2.8.1. get_version 2.1. h5py 3.1.0. highs_wrapper NA. igraph 0.8.3. joblib 1.0.0. kiwisolver 1.3.1. legacy_api_wrap 1.2. leidenalg 0.8.3. llvmlite 0.35.0. louvain 0.7.0. matplotlib 3.3.3. mpl_toolkits NA. natsort 7.1.1. numba 0.52.0. numexpr 2.7.2. numpy 1.19.5. packaging 20.8. pandas 1.2.1. pkg_resources NA. pynndescent 0.5.1. pyparsing 2.4.7. pytz 2020.5. scanpy 1.6.1. scipy 1.6.0. setuptools_scm NA. sinfo 0.3.1. six 1.15.0. sklearn 0.24.1. statsmodels 0.12.1. tables 3.6.1. texttable 1.6.3. umap 0.4.6. -----. Python 3.8.5 (default, Sep 4 2020, 07:30:14) [GCC 7.3.0]. Linux-3.10.0-1160.11.1.el7.x86_64-x86_64-with-glibc2.10. 40 logical CPU cores, x86_64. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1613
https://github.com/scverse/scanpy/issues/1613:33,security,log,logging,33,"Report pynndescent version in sc.logging.print_header; Hi,. Thank you for the great tool. I think this is not a bug. . Recently I upgraded some packages and found my results were different from the previous runs. I figured out that it is caused by different versions of `pynndescent` (0.4.7 vs 0.5.1), which is recommended to use in UMAP. So I think `pynndescent` should be included in the output of `sc.logging.print_header()`. #### Versions. <details>. -----. anndata 0.7.5. scanpy 1.6.1. sinfo 0.3.1. -----. PIL 8.1.0. anndata 0.7.5. constants NA. cycler 0.10.0. cython_runtime NA. dateutil 2.8.1. get_version 2.1. h5py 3.1.0. highs_wrapper NA. igraph 0.8.3. joblib 1.0.0. kiwisolver 1.3.1. legacy_api_wrap 1.2. leidenalg 0.8.3. llvmlite 0.35.0. louvain 0.7.0. matplotlib 3.3.3. mpl_toolkits NA. natsort 7.1.1. numba 0.52.0. numexpr 2.7.2. numpy 1.19.5. packaging 20.8. pandas 1.2.1. pkg_resources NA. pynndescent 0.5.1. pyparsing 2.4.7. pytz 2020.5. scanpy 1.6.1. scipy 1.6.0. setuptools_scm NA. sinfo 0.3.1. six 1.15.0. sklearn 0.24.1. statsmodels 0.12.1. tables 3.6.1. texttable 1.6.3. umap 0.4.6. -----. Python 3.8.5 (default, Sep 4 2020, 07:30:14) [GCC 7.3.0]. Linux-3.10.0-1160.11.1.el7.x86_64-x86_64-with-glibc2.10. 40 logical CPU cores, x86_64. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1613
https://github.com/scverse/scanpy/issues/1613:404,security,log,logging,404,"Report pynndescent version in sc.logging.print_header; Hi,. Thank you for the great tool. I think this is not a bug. . Recently I upgraded some packages and found my results were different from the previous runs. I figured out that it is caused by different versions of `pynndescent` (0.4.7 vs 0.5.1), which is recommended to use in UMAP. So I think `pynndescent` should be included in the output of `sc.logging.print_header()`. #### Versions. <details>. -----. anndata 0.7.5. scanpy 1.6.1. sinfo 0.3.1. -----. PIL 8.1.0. anndata 0.7.5. constants NA. cycler 0.10.0. cython_runtime NA. dateutil 2.8.1. get_version 2.1. h5py 3.1.0. highs_wrapper NA. igraph 0.8.3. joblib 1.0.0. kiwisolver 1.3.1. legacy_api_wrap 1.2. leidenalg 0.8.3. llvmlite 0.35.0. louvain 0.7.0. matplotlib 3.3.3. mpl_toolkits NA. natsort 7.1.1. numba 0.52.0. numexpr 2.7.2. numpy 1.19.5. packaging 20.8. pandas 1.2.1. pkg_resources NA. pynndescent 0.5.1. pyparsing 2.4.7. pytz 2020.5. scanpy 1.6.1. scipy 1.6.0. setuptools_scm NA. sinfo 0.3.1. six 1.15.0. sklearn 0.24.1. statsmodels 0.12.1. tables 3.6.1. texttable 1.6.3. umap 0.4.6. -----. Python 3.8.5 (default, Sep 4 2020, 07:30:14) [GCC 7.3.0]. Linux-3.10.0-1160.11.1.el7.x86_64-x86_64-with-glibc2.10. 40 logical CPU cores, x86_64. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1613
https://github.com/scverse/scanpy/issues/1613:1229,security,log,logical,1229,"Report pynndescent version in sc.logging.print_header; Hi,. Thank you for the great tool. I think this is not a bug. . Recently I upgraded some packages and found my results were different from the previous runs. I figured out that it is caused by different versions of `pynndescent` (0.4.7 vs 0.5.1), which is recommended to use in UMAP. So I think `pynndescent` should be included in the output of `sc.logging.print_header()`. #### Versions. <details>. -----. anndata 0.7.5. scanpy 1.6.1. sinfo 0.3.1. -----. PIL 8.1.0. anndata 0.7.5. constants NA. cycler 0.10.0. cython_runtime NA. dateutil 2.8.1. get_version 2.1. h5py 3.1.0. highs_wrapper NA. igraph 0.8.3. joblib 1.0.0. kiwisolver 1.3.1. legacy_api_wrap 1.2. leidenalg 0.8.3. llvmlite 0.35.0. louvain 0.7.0. matplotlib 3.3.3. mpl_toolkits NA. natsort 7.1.1. numba 0.52.0. numexpr 2.7.2. numpy 1.19.5. packaging 20.8. pandas 1.2.1. pkg_resources NA. pynndescent 0.5.1. pyparsing 2.4.7. pytz 2020.5. scanpy 1.6.1. scipy 1.6.0. setuptools_scm NA. sinfo 0.3.1. six 1.15.0. sklearn 0.24.1. statsmodels 0.12.1. tables 3.6.1. texttable 1.6.3. umap 0.4.6. -----. Python 3.8.5 (default, Sep 4 2020, 07:30:14) [GCC 7.3.0]. Linux-3.10.0-1160.11.1.el7.x86_64-x86_64-with-glibc2.10. 40 logical CPU cores, x86_64. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1613
https://github.com/scverse/scanpy/issues/1613:33,testability,log,logging,33,"Report pynndescent version in sc.logging.print_header; Hi,. Thank you for the great tool. I think this is not a bug. . Recently I upgraded some packages and found my results were different from the previous runs. I figured out that it is caused by different versions of `pynndescent` (0.4.7 vs 0.5.1), which is recommended to use in UMAP. So I think `pynndescent` should be included in the output of `sc.logging.print_header()`. #### Versions. <details>. -----. anndata 0.7.5. scanpy 1.6.1. sinfo 0.3.1. -----. PIL 8.1.0. anndata 0.7.5. constants NA. cycler 0.10.0. cython_runtime NA. dateutil 2.8.1. get_version 2.1. h5py 3.1.0. highs_wrapper NA. igraph 0.8.3. joblib 1.0.0. kiwisolver 1.3.1. legacy_api_wrap 1.2. leidenalg 0.8.3. llvmlite 0.35.0. louvain 0.7.0. matplotlib 3.3.3. mpl_toolkits NA. natsort 7.1.1. numba 0.52.0. numexpr 2.7.2. numpy 1.19.5. packaging 20.8. pandas 1.2.1. pkg_resources NA. pynndescent 0.5.1. pyparsing 2.4.7. pytz 2020.5. scanpy 1.6.1. scipy 1.6.0. setuptools_scm NA. sinfo 0.3.1. six 1.15.0. sklearn 0.24.1. statsmodels 0.12.1. tables 3.6.1. texttable 1.6.3. umap 0.4.6. -----. Python 3.8.5 (default, Sep 4 2020, 07:30:14) [GCC 7.3.0]. Linux-3.10.0-1160.11.1.el7.x86_64-x86_64-with-glibc2.10. 40 logical CPU cores, x86_64. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1613
https://github.com/scverse/scanpy/issues/1613:404,testability,log,logging,404,"Report pynndescent version in sc.logging.print_header; Hi,. Thank you for the great tool. I think this is not a bug. . Recently I upgraded some packages and found my results were different from the previous runs. I figured out that it is caused by different versions of `pynndescent` (0.4.7 vs 0.5.1), which is recommended to use in UMAP. So I think `pynndescent` should be included in the output of `sc.logging.print_header()`. #### Versions. <details>. -----. anndata 0.7.5. scanpy 1.6.1. sinfo 0.3.1. -----. PIL 8.1.0. anndata 0.7.5. constants NA. cycler 0.10.0. cython_runtime NA. dateutil 2.8.1. get_version 2.1. h5py 3.1.0. highs_wrapper NA. igraph 0.8.3. joblib 1.0.0. kiwisolver 1.3.1. legacy_api_wrap 1.2. leidenalg 0.8.3. llvmlite 0.35.0. louvain 0.7.0. matplotlib 3.3.3. mpl_toolkits NA. natsort 7.1.1. numba 0.52.0. numexpr 2.7.2. numpy 1.19.5. packaging 20.8. pandas 1.2.1. pkg_resources NA. pynndescent 0.5.1. pyparsing 2.4.7. pytz 2020.5. scanpy 1.6.1. scipy 1.6.0. setuptools_scm NA. sinfo 0.3.1. six 1.15.0. sklearn 0.24.1. statsmodels 0.12.1. tables 3.6.1. texttable 1.6.3. umap 0.4.6. -----. Python 3.8.5 (default, Sep 4 2020, 07:30:14) [GCC 7.3.0]. Linux-3.10.0-1160.11.1.el7.x86_64-x86_64-with-glibc2.10. 40 logical CPU cores, x86_64. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1613
https://github.com/scverse/scanpy/issues/1613:1229,testability,log,logical,1229,"Report pynndescent version in sc.logging.print_header; Hi,. Thank you for the great tool. I think this is not a bug. . Recently I upgraded some packages and found my results were different from the previous runs. I figured out that it is caused by different versions of `pynndescent` (0.4.7 vs 0.5.1), which is recommended to use in UMAP. So I think `pynndescent` should be included in the output of `sc.logging.print_header()`. #### Versions. <details>. -----. anndata 0.7.5. scanpy 1.6.1. sinfo 0.3.1. -----. PIL 8.1.0. anndata 0.7.5. constants NA. cycler 0.10.0. cython_runtime NA. dateutil 2.8.1. get_version 2.1. h5py 3.1.0. highs_wrapper NA. igraph 0.8.3. joblib 1.0.0. kiwisolver 1.3.1. legacy_api_wrap 1.2. leidenalg 0.8.3. llvmlite 0.35.0. louvain 0.7.0. matplotlib 3.3.3. mpl_toolkits NA. natsort 7.1.1. numba 0.52.0. numexpr 2.7.2. numpy 1.19.5. packaging 20.8. pandas 1.2.1. pkg_resources NA. pynndescent 0.5.1. pyparsing 2.4.7. pytz 2020.5. scanpy 1.6.1. scipy 1.6.0. setuptools_scm NA. sinfo 0.3.1. six 1.15.0. sklearn 0.24.1. statsmodels 0.12.1. tables 3.6.1. texttable 1.6.3. umap 0.4.6. -----. Python 3.8.5 (default, Sep 4 2020, 07:30:14) [GCC 7.3.0]. Linux-3.10.0-1160.11.1.el7.x86_64-x86_64-with-glibc2.10. 40 logical CPU cores, x86_64. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1613
https://github.com/scverse/scanpy/issues/1613:84,usability,tool,tool,84,"Report pynndescent version in sc.logging.print_header; Hi,. Thank you for the great tool. I think this is not a bug. . Recently I upgraded some packages and found my results were different from the previous runs. I figured out that it is caused by different versions of `pynndescent` (0.4.7 vs 0.5.1), which is recommended to use in UMAP. So I think `pynndescent` should be included in the output of `sc.logging.print_header()`. #### Versions. <details>. -----. anndata 0.7.5. scanpy 1.6.1. sinfo 0.3.1. -----. PIL 8.1.0. anndata 0.7.5. constants NA. cycler 0.10.0. cython_runtime NA. dateutil 2.8.1. get_version 2.1. h5py 3.1.0. highs_wrapper NA. igraph 0.8.3. joblib 1.0.0. kiwisolver 1.3.1. legacy_api_wrap 1.2. leidenalg 0.8.3. llvmlite 0.35.0. louvain 0.7.0. matplotlib 3.3.3. mpl_toolkits NA. natsort 7.1.1. numba 0.52.0. numexpr 2.7.2. numpy 1.19.5. packaging 20.8. pandas 1.2.1. pkg_resources NA. pynndescent 0.5.1. pyparsing 2.4.7. pytz 2020.5. scanpy 1.6.1. scipy 1.6.0. setuptools_scm NA. sinfo 0.3.1. six 1.15.0. sklearn 0.24.1. statsmodels 0.12.1. tables 3.6.1. texttable 1.6.3. umap 0.4.6. -----. Python 3.8.5 (default, Sep 4 2020, 07:30:14) [GCC 7.3.0]. Linux-3.10.0-1160.11.1.el7.x86_64-x86_64-with-glibc2.10. 40 logical CPU cores, x86_64. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1613
https://github.com/scverse/scanpy/pull/1614:619,availability,ping,ping,619,fixes dendrogram issue #1228; <!-- . Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. The PR uses `1-correlation` as distance matrix to compute the dendrogram as suggested in #1288 and mentioned in https://github.com/theislab/squidpy/pull/236. I opted for the minimal changes to the code. Other solution would be to use `scipy.spatial.distance.pdist` that allows a larger number of distance metrics. I am open for a discussion on this topic (ping @michalk8),MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1614
https://github.com/scverse/scanpy/pull/1614:612,integrability,topic,topic,612,fixes dendrogram issue #1228; <!-- . Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. The PR uses `1-correlation` as distance matrix to compute the dendrogram as suggested in #1288 and mentioned in https://github.com/theislab/squidpy/pull/236. I opted for the minimal changes to the code. Other solution would be to use `scipy.spatial.distance.pdist` that allows a larger number of distance metrics. I am open for a discussion on this topic (ping @michalk8),MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1614
https://github.com/scverse/scanpy/pull/1614:250,safety,review,review,250,fixes dendrogram issue #1228; <!-- . Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. The PR uses `1-correlation` as distance matrix to compute the dendrogram as suggested in #1288 and mentioned in https://github.com/theislab/squidpy/pull/236. I opted for the minimal changes to the code. Other solution would be to use `scipy.spatial.distance.pdist` that allows a larger number of distance metrics. I am open for a discussion on this topic (ping @michalk8),MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1614
https://github.com/scverse/scanpy/pull/1614:250,testability,review,review,250,fixes dendrogram issue #1228; <!-- . Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. The PR uses `1-correlation` as distance matrix to compute the dendrogram as suggested in #1288 and mentioned in https://github.com/theislab/squidpy/pull/236. I opted for the minimal changes to the code. Other solution would be to use `scipy.spatial.distance.pdist` that allows a larger number of distance metrics. I am open for a discussion on this topic (ping @michalk8),MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1614
https://github.com/scverse/scanpy/pull/1614:101,usability,guid,guidelines,101,fixes dendrogram issue #1228; <!-- . Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. The PR uses `1-correlation` as distance matrix to compute the dendrogram as suggested in #1288 and mentioned in https://github.com/theislab/squidpy/pull/236. I opted for the minimal changes to the code. Other solution would be to use `scipy.spatial.distance.pdist` that allows a larger number of distance metrics. I am open for a discussion on this topic (ping @michalk8),MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1614
https://github.com/scverse/scanpy/pull/1614:132,usability,guid,guide,132,fixes dendrogram issue #1228; <!-- . Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. The PR uses `1-correlation` as distance matrix to compute the dendrogram as suggested in #1288 and mentioned in https://github.com/theislab/squidpy/pull/236. I opted for the minimal changes to the code. Other solution would be to use `scipy.spatial.distance.pdist` that allows a larger number of distance metrics. I am open for a discussion on this topic (ping @michalk8),MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1614
https://github.com/scverse/scanpy/pull/1614:228,usability,workflow,workflow,228,fixes dendrogram issue #1228; <!-- . Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. The PR uses `1-correlation` as distance matrix to compute the dendrogram as suggested in #1288 and mentioned in https://github.com/theislab/squidpy/pull/236. I opted for the minimal changes to the code. Other solution would be to use `scipy.spatial.distance.pdist` that allows a larger number of distance metrics. I am open for a discussion on this topic (ping @michalk8),MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1614
https://github.com/scverse/scanpy/pull/1614:437,usability,minim,minimal,437,fixes dendrogram issue #1228; <!-- . Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. The PR uses `1-correlation` as distance matrix to compute the dendrogram as suggested in #1288 and mentioned in https://github.com/theislab/squidpy/pull/236. I opted for the minimal changes to the code. Other solution would be to use `scipy.spatial.distance.pdist` that allows a larger number of distance metrics. I am open for a discussion on this topic (ping @michalk8),MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1614
https://github.com/scverse/scanpy/pull/1615:4,interoperability,format,formatting,4,fix formatting; <!-- . Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1615
https://github.com/scverse/scanpy/pull/1615:236,safety,review,review,236,fix formatting; <!-- . Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1615
https://github.com/scverse/scanpy/pull/1615:236,testability,review,review,236,fix formatting; <!-- . Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1615
https://github.com/scverse/scanpy/pull/1615:87,usability,guid,guidelines,87,fix formatting; <!-- . Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1615
https://github.com/scverse/scanpy/pull/1615:118,usability,guid,guide,118,fix formatting; <!-- . Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1615
https://github.com/scverse/scanpy/pull/1615:214,usability,workflow,workflow,214,fix formatting; <!-- . Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1615
https://github.com/scverse/scanpy/issues/1619:193,energy efficiency,cool,cool,193,densmap; This seems related to the new Densmap feature https://umap-learn.readthedocs.io/en/latest/densmap_demo.html (see https://www.biorxiv.org/content/10.1101/2020.05.12.077776v1). Would be cool to support it in scanpy. _Originally posted by @gokceneraslan in https://github.com/theislab/scanpy/issues/1509#issuecomment-748156450_,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1619
https://github.com/scverse/scanpy/issues/1619:146,performance,content,content,146,densmap; This seems related to the new Densmap feature https://umap-learn.readthedocs.io/en/latest/densmap_demo.html (see https://www.biorxiv.org/content/10.1101/2020.05.12.077776v1). Would be cool to support it in scanpy. _Originally posted by @gokceneraslan in https://github.com/theislab/scanpy/issues/1509#issuecomment-748156450_,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1619
https://github.com/scverse/scanpy/issues/1619:68,usability,learn,learn,68,densmap; This seems related to the new Densmap feature https://umap-learn.readthedocs.io/en/latest/densmap_demo.html (see https://www.biorxiv.org/content/10.1101/2020.05.12.077776v1). Would be cool to support it in scanpy. _Originally posted by @gokceneraslan in https://github.com/theislab/scanpy/issues/1509#issuecomment-748156450_,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1619
https://github.com/scverse/scanpy/issues/1619:201,usability,support,support,201,densmap; This seems related to the new Densmap feature https://umap-learn.readthedocs.io/en/latest/densmap_demo.html (see https://www.biorxiv.org/content/10.1101/2020.05.12.077776v1). Would be cool to support it in scanpy. _Originally posted by @gokceneraslan in https://github.com/theislab/scanpy/issues/1509#issuecomment-748156450_,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1619
https://github.com/scverse/scanpy/pull/1620:109,deployability,pipelin,pipelines,109,Add pip cache to CI; Should speed up CI. Based on [these docs](https://docs.microsoft.com/en-us/azure/devops/pipelines/release/caching?view=azure-devops#pythonpip),MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1620
https://github.com/scverse/scanpy/pull/1620:119,deployability,releas,release,119,Add pip cache to CI; Should speed up CI. Based on [these docs](https://docs.microsoft.com/en-us/azure/devops/pipelines/release/caching?view=azure-devops#pythonpip),MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1620
https://github.com/scverse/scanpy/pull/1620:109,integrability,pipelin,pipelines,109,Add pip cache to CI; Should speed up CI. Based on [these docs](https://docs.microsoft.com/en-us/azure/devops/pipelines/release/caching?view=azure-devops#pythonpip),MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1620
https://github.com/scverse/scanpy/pull/1620:8,performance,cach,cache,8,Add pip cache to CI; Should speed up CI. Based on [these docs](https://docs.microsoft.com/en-us/azure/devops/pipelines/release/caching?view=azure-devops#pythonpip),MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1620
https://github.com/scverse/scanpy/pull/1620:127,performance,cach,caching,127,Add pip cache to CI; Should speed up CI. Based on [these docs](https://docs.microsoft.com/en-us/azure/devops/pipelines/release/caching?view=azure-devops#pythonpip),MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1620
https://github.com/scverse/scanpy/pull/1621:46,deployability,updat,updates,46,"Get cleanup; Addendum to #1583, realized some updates hadn't been propagated to `var_df` so I've added those. Made it harder to forget these in the future by sharing the code. Additionally cleaned up the `get` namespace.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1621
https://github.com/scverse/scanpy/pull/1621:46,safety,updat,updates,46,"Get cleanup; Addendum to #1583, realized some updates hadn't been propagated to `var_df` so I've added those. Made it harder to forget these in the future by sharing the code. Additionally cleaned up the `get` namespace.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1621
https://github.com/scverse/scanpy/pull/1621:46,security,updat,updates,46,"Get cleanup; Addendum to #1583, realized some updates hadn't been propagated to `var_df` so I've added those. Made it harder to forget these in the future by sharing the code. Additionally cleaned up the `get` namespace.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1621
https://github.com/scverse/scanpy/pull/1623:0,deployability,Releas,Release,0,Release note for #1583 and update release date; prep 1.7.0,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1623
https://github.com/scverse/scanpy/pull/1623:27,deployability,updat,update,27,Release note for #1583 and update release date; prep 1.7.0,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1623
https://github.com/scverse/scanpy/pull/1623:34,deployability,releas,release,34,Release note for #1583 and update release date; prep 1.7.0,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1623
https://github.com/scverse/scanpy/pull/1623:27,safety,updat,update,27,Release note for #1583 and update release date; prep 1.7.0,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1623
https://github.com/scverse/scanpy/pull/1623:27,security,updat,update,27,Release note for #1583 and update release date; prep 1.7.0,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1623
https://github.com/scverse/scanpy/pull/1624:35,deployability,Releas,Release,35,Backport PR #1623 on branch 1.7.x (Release note for #1583 and update release date); Backport PR #1623: Release note for #1583 and update release date,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1624
https://github.com/scverse/scanpy/pull/1624:62,deployability,updat,update,62,Backport PR #1623 on branch 1.7.x (Release note for #1583 and update release date); Backport PR #1623: Release note for #1583 and update release date,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1624
https://github.com/scverse/scanpy/pull/1624:69,deployability,releas,release,69,Backport PR #1623 on branch 1.7.x (Release note for #1583 and update release date); Backport PR #1623: Release note for #1583 and update release date,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1624
https://github.com/scverse/scanpy/pull/1624:103,deployability,Releas,Release,103,Backport PR #1623 on branch 1.7.x (Release note for #1583 and update release date); Backport PR #1623: Release note for #1583 and update release date,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1624
https://github.com/scverse/scanpy/pull/1624:130,deployability,updat,update,130,Backport PR #1623 on branch 1.7.x (Release note for #1583 and update release date); Backport PR #1623: Release note for #1583 and update release date,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1624
https://github.com/scverse/scanpy/pull/1624:137,deployability,releas,release,137,Backport PR #1623 on branch 1.7.x (Release note for #1583 and update release date); Backport PR #1623: Release note for #1583 and update release date,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1624
https://github.com/scverse/scanpy/pull/1624:62,safety,updat,update,62,Backport PR #1623 on branch 1.7.x (Release note for #1583 and update release date); Backport PR #1623: Release note for #1583 and update release date,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1624
https://github.com/scverse/scanpy/pull/1624:130,safety,updat,update,130,Backport PR #1623 on branch 1.7.x (Release note for #1583 and update release date); Backport PR #1623: Release note for #1583 and update release date,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1624
https://github.com/scverse/scanpy/pull/1624:62,security,updat,update,62,Backport PR #1623 on branch 1.7.x (Release note for #1583 and update release date); Backport PR #1623: Release note for #1583 and update release date,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1624
https://github.com/scverse/scanpy/pull/1624:130,security,updat,update,130,Backport PR #1623 on branch 1.7.x (Release note for #1583 and update release date); Backport PR #1623: Release note for #1583 and update release date,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1624
https://github.com/scverse/scanpy/issues/1625:409,availability,down,down,409,"gene ranking produces completely wrong results on the most recent tool/library combinations on specific dataset; - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. # Introduction. Hi,. so this is a weird one and I could not track it down yet. The Schillerlab people have a workstation known as ""agando"". On this workstation the full environment is installed globally and shared by all users. I am looking to change that. # The issue. When calculating the `sc.tl.marker_gene_overlap` I get the expected and reasonable results on the agando environment, but completely rubbish results when running the same code with a fresh Conda environment and the latest dependencies installed. ![image](https://user-images.githubusercontent.com/21954664/106739402-659dfb80-6619-11eb-84f1-e75abfa6167d.png). Top = new, trash results. Bottom = old=agando expected results. The old environment has:. ```. scanpy==1.6.1.dev110+gb4234d81 anndata==0.7.4 umap==0.4.6 numpy==1.19.0 scipy==1.5.1 pandas==1.1.5 scikit-learn==0.23.1 statsmodels==0.12.1 python-igraph==0.8.0 louvain==0.6.1 leidenalg==0.8.3. ```. The new environment has . ```. scanpy==1.6.1 anndata==0.7.5 umap==0.4.6 numpy==1.20.0 scipy==1.6.0 pandas==1.2.1 scikit-learn==0.24.1 statsmodels==0.12.1 python-igraph==0.8.3 louvain==0.7.0 leidenalg==0.8.3. ```. Full new conda environment:. ```. name: single_cell_analysis. channels:. - defaults. dependencies:. - _libgcc_mutex=0.1=main. - argon2-cffi=20.1.0=py37h7b6447c_1. - async_generator=1.10=py37h28b3542_0. - attrs=20.3.0=pyhd3eb1b0_0. - backcall=0.2.0=pyhd3eb1b0_0. - bleach=3.3.0=pyhd3eb1b0_0. - ca-certificates=2021.1.19=h06a4308_0. - certifi=2020.12.5=py37h06a4308_0. - cffi=1.14.4=py37h261ae71_0. - dbus=1.13.18=hb2f20db_0. - decorator=4.4.2=pyhd3eb1b0_0. - defusedxml=0.6.0=py_0. - entrypoints=0.3=py37_0. - expat=2.2.10=he",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1625
https://github.com/scverse/scanpy/issues/1625:5668,availability,avail,available,5668,"3.4.0. - joblib==1.0.0. - kiwisolver==1.3.1. - legacy-api-wrap==1.2. - leidenalg==0.8.3. - llvmlite==0.35.0. - loompy==3.0.6. - louvain==0.7.0. - matplotlib==3.3.4. - natsort==7.1.1. - networkx==2.5. - numba==0.52.0. - numexpr==2.7.2. - numpy==1.20.0. - numpy-groupies==0.9.13. - pandas==1.2.1. - patsy==0.5.1. - pillow==8.1.0. - python-igraph==0.8.3. - pytz==2021.1. - scanpy==1.6.1. - scikit-learn==0.24.1. - scipy==1.6.0. - scvelo==0.2.2. - seaborn==0.11.1. - setuptools-scm==5.0.1. - sinfo==0.3.1. - statsmodels==0.12.1. - stdlib-list==0.8.0. - tables==3.6.1. - texttable==1.6.3. - threadpoolctl==2.1.0. - tqdm==4.56.0. - typing-extensions==3.7.4.3. - umap-learn==0.4.6. ```. I can reproduce the issue with a Docker container that only has the minimal conda environment above. Additionally, I already tried installing the exact same dependency versions in the new environment, but got the same results. . If you need access to the data and the container please contact me and I will make it available to you. The data is already at the ICB cluster. Code:. ```. from os import path. import numpy as np. import matplotlib.pyplot as plt. import scanpy as sc. import scanpy.external as sce. from os import listdir. import pandas as pd. import seaborn as sb. import datetime, time. import scvelo as scv. from matplotlib.colors import LinearSegmentedColormap. #Define a nice colour map for gene expression. from matplotlib import colors. def timestamp():. ts = time.time(). st = datetime.datetime.fromtimestamp(ts).strftime('%d-%m-%Y %H:%M:%S'). return st. # Exporting folder. folder = ""/output"". sc.settings.figdir = folder + ""Plots/"". sc.set_figure_params(vector_friendly = True, dpi=300). sc.settings.verbosity = 3 # verbosity: errors (0), warnings (1), info (2), hints (3). sc.logging.print_version_and_date(). sc.logging.print_header(). adata = sc.read(""/data/190924_Recreated_Virus_Object_regressed.h5ad""). #adata.write(folder + ""190924_Recreated_Virus_Object_regressed.h5ad""). sc.tl.louvain(adat",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1625
https://github.com/scverse/scanpy/issues/1625:5717,availability,cluster,cluster,5717,"egacy-api-wrap==1.2. - leidenalg==0.8.3. - llvmlite==0.35.0. - loompy==3.0.6. - louvain==0.7.0. - matplotlib==3.3.4. - natsort==7.1.1. - networkx==2.5. - numba==0.52.0. - numexpr==2.7.2. - numpy==1.20.0. - numpy-groupies==0.9.13. - pandas==1.2.1. - patsy==0.5.1. - pillow==8.1.0. - python-igraph==0.8.3. - pytz==2021.1. - scanpy==1.6.1. - scikit-learn==0.24.1. - scipy==1.6.0. - scvelo==0.2.2. - seaborn==0.11.1. - setuptools-scm==5.0.1. - sinfo==0.3.1. - statsmodels==0.12.1. - stdlib-list==0.8.0. - tables==3.6.1. - texttable==1.6.3. - threadpoolctl==2.1.0. - tqdm==4.56.0. - typing-extensions==3.7.4.3. - umap-learn==0.4.6. ```. I can reproduce the issue with a Docker container that only has the minimal conda environment above. Additionally, I already tried installing the exact same dependency versions in the new environment, but got the same results. . If you need access to the data and the container please contact me and I will make it available to you. The data is already at the ICB cluster. Code:. ```. from os import path. import numpy as np. import matplotlib.pyplot as plt. import scanpy as sc. import scanpy.external as sce. from os import listdir. import pandas as pd. import seaborn as sb. import datetime, time. import scvelo as scv. from matplotlib.colors import LinearSegmentedColormap. #Define a nice colour map for gene expression. from matplotlib import colors. def timestamp():. ts = time.time(). st = datetime.datetime.fromtimestamp(ts).strftime('%d-%m-%Y %H:%M:%S'). return st. # Exporting folder. folder = ""/output"". sc.settings.figdir = folder + ""Plots/"". sc.set_figure_params(vector_friendly = True, dpi=300). sc.settings.verbosity = 3 # verbosity: errors (0), warnings (1), info (2), hints (3). sc.logging.print_version_and_date(). sc.logging.print_header(). adata = sc.read(""/data/190924_Recreated_Virus_Object_regressed.h5ad""). #adata.write(folder + ""190924_Recreated_Virus_Object_regressed.h5ad""). sc.tl.louvain(adata, resolution = 4, key_added = ""louvain_2""). sc.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1625
https://github.com/scverse/scanpy/issues/1625:6402,availability,error,errors,6402,"at only has the minimal conda environment above. Additionally, I already tried installing the exact same dependency versions in the new environment, but got the same results. . If you need access to the data and the container please contact me and I will make it available to you. The data is already at the ICB cluster. Code:. ```. from os import path. import numpy as np. import matplotlib.pyplot as plt. import scanpy as sc. import scanpy.external as sce. from os import listdir. import pandas as pd. import seaborn as sb. import datetime, time. import scvelo as scv. from matplotlib.colors import LinearSegmentedColormap. #Define a nice colour map for gene expression. from matplotlib import colors. def timestamp():. ts = time.time(). st = datetime.datetime.fromtimestamp(ts).strftime('%d-%m-%Y %H:%M:%S'). return st. # Exporting folder. folder = ""/output"". sc.settings.figdir = folder + ""Plots/"". sc.set_figure_params(vector_friendly = True, dpi=300). sc.settings.verbosity = 3 # verbosity: errors (0), warnings (1), info (2), hints (3). sc.logging.print_version_and_date(). sc.logging.print_header(). adata = sc.read(""/data/190924_Recreated_Virus_Object_regressed.h5ad""). #adata.write(folder + ""190924_Recreated_Virus_Object_regressed.h5ad""). sc.tl.louvain(adata, resolution = 4, key_added = ""louvain_2""). sc.tl.louvain(adata, resolution = 5, key_added = ""louvain_3""). sc.tl.louvain(adata, resolution = 6, key_added = ""louvain_4""). sc.tl.louvain(adata, resolution = 7, key_added = ""louvain_5""). sc.pl.umap(adata, color = [""louvain_2"", ""louvain_3"", ""louvain_4"", ""louvain_5""], wspace = 0.45). #select resolution. print(adata.obs[""louvain_5""].value_counts()). sc.tl.rank_genes_groups(adata, groupby = ""louvain_5""). # read all arkers table from known annotated data. marker_folder = ""/marker/"". marker_table = pd.read_csv(marker_folder + ""Particle_AllMarkers.txt"", sep = ""\t"", index_col = None). marker_table.head(2). ## Restrict to Foldchange and P value. marker_table = marker_table[(marker_tabl",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1625
https://github.com/scverse/scanpy/issues/1625:7536,availability,cluster,cluster,7536,"ms(vector_friendly = True, dpi=300). sc.settings.verbosity = 3 # verbosity: errors (0), warnings (1), info (2), hints (3). sc.logging.print_version_and_date(). sc.logging.print_header(). adata = sc.read(""/data/190924_Recreated_Virus_Object_regressed.h5ad""). #adata.write(folder + ""190924_Recreated_Virus_Object_regressed.h5ad""). sc.tl.louvain(adata, resolution = 4, key_added = ""louvain_2""). sc.tl.louvain(adata, resolution = 5, key_added = ""louvain_3""). sc.tl.louvain(adata, resolution = 6, key_added = ""louvain_4""). sc.tl.louvain(adata, resolution = 7, key_added = ""louvain_5""). sc.pl.umap(adata, color = [""louvain_2"", ""louvain_3"", ""louvain_4"", ""louvain_5""], wspace = 0.45). #select resolution. print(adata.obs[""louvain_5""].value_counts()). sc.tl.rank_genes_groups(adata, groupby = ""louvain_5""). # read all arkers table from known annotated data. marker_folder = ""/marker/"". marker_table = pd.read_csv(marker_folder + ""Particle_AllMarkers.txt"", sep = ""\t"", index_col = None). marker_table.head(2). ## Restrict to Foldchange and P value. marker_table = marker_table[(marker_table.logfoldchange > 2) & (marker_table.pval_adj < 0.05)].copy(). print(marker_table.shape). marker = dict(). for ct in marker_table[""cluster""].unique():. tmp = marker_table[marker_table[""cluster""] == ct]. marker[ct] = tmp.gene.values. ## Look at a predefined list of cell types and the corresponding marker genes in order to annotate our clusters. cell_annotation = sc.tl.marker_gene_overlap(adata, marker, key = 'rank_genes_groups', normalize = 'reference'). fig, ax = plt.subplots(figsize = (30, 10)). sb.heatmap(cell_annotation, cbar = False, annot = True). fig.savefig('/data/heatmap.png'). ```. Everything works smoothly and the results look fine until the marker_gene_overlap. A similar issue was observed by another ICB member who is trying to verify that their new environment still works. Any pointers strongly appreciated, since we consider this a severe issue blocking new analyses with the most recent packages.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1625
https://github.com/scverse/scanpy/issues/1625:7590,availability,cluster,cluster,7590,"ms(vector_friendly = True, dpi=300). sc.settings.verbosity = 3 # verbosity: errors (0), warnings (1), info (2), hints (3). sc.logging.print_version_and_date(). sc.logging.print_header(). adata = sc.read(""/data/190924_Recreated_Virus_Object_regressed.h5ad""). #adata.write(folder + ""190924_Recreated_Virus_Object_regressed.h5ad""). sc.tl.louvain(adata, resolution = 4, key_added = ""louvain_2""). sc.tl.louvain(adata, resolution = 5, key_added = ""louvain_3""). sc.tl.louvain(adata, resolution = 6, key_added = ""louvain_4""). sc.tl.louvain(adata, resolution = 7, key_added = ""louvain_5""). sc.pl.umap(adata, color = [""louvain_2"", ""louvain_3"", ""louvain_4"", ""louvain_5""], wspace = 0.45). #select resolution. print(adata.obs[""louvain_5""].value_counts()). sc.tl.rank_genes_groups(adata, groupby = ""louvain_5""). # read all arkers table from known annotated data. marker_folder = ""/marker/"". marker_table = pd.read_csv(marker_folder + ""Particle_AllMarkers.txt"", sep = ""\t"", index_col = None). marker_table.head(2). ## Restrict to Foldchange and P value. marker_table = marker_table[(marker_table.logfoldchange > 2) & (marker_table.pval_adj < 0.05)].copy(). print(marker_table.shape). marker = dict(). for ct in marker_table[""cluster""].unique():. tmp = marker_table[marker_table[""cluster""] == ct]. marker[ct] = tmp.gene.values. ## Look at a predefined list of cell types and the corresponding marker genes in order to annotate our clusters. cell_annotation = sc.tl.marker_gene_overlap(adata, marker, key = 'rank_genes_groups', normalize = 'reference'). fig, ax = plt.subplots(figsize = (30, 10)). sb.heatmap(cell_annotation, cbar = False, annot = True). fig.savefig('/data/heatmap.png'). ```. Everything works smoothly and the results look fine until the marker_gene_overlap. A similar issue was observed by another ICB member who is trying to verify that their new environment still works. Any pointers strongly appreciated, since we consider this a severe issue blocking new analyses with the most recent packages.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1625
https://github.com/scverse/scanpy/issues/1625:7741,availability,cluster,clusters,7741,"ms(vector_friendly = True, dpi=300). sc.settings.verbosity = 3 # verbosity: errors (0), warnings (1), info (2), hints (3). sc.logging.print_version_and_date(). sc.logging.print_header(). adata = sc.read(""/data/190924_Recreated_Virus_Object_regressed.h5ad""). #adata.write(folder + ""190924_Recreated_Virus_Object_regressed.h5ad""). sc.tl.louvain(adata, resolution = 4, key_added = ""louvain_2""). sc.tl.louvain(adata, resolution = 5, key_added = ""louvain_3""). sc.tl.louvain(adata, resolution = 6, key_added = ""louvain_4""). sc.tl.louvain(adata, resolution = 7, key_added = ""louvain_5""). sc.pl.umap(adata, color = [""louvain_2"", ""louvain_3"", ""louvain_4"", ""louvain_5""], wspace = 0.45). #select resolution. print(adata.obs[""louvain_5""].value_counts()). sc.tl.rank_genes_groups(adata, groupby = ""louvain_5""). # read all arkers table from known annotated data. marker_folder = ""/marker/"". marker_table = pd.read_csv(marker_folder + ""Particle_AllMarkers.txt"", sep = ""\t"", index_col = None). marker_table.head(2). ## Restrict to Foldchange and P value. marker_table = marker_table[(marker_table.logfoldchange > 2) & (marker_table.pval_adj < 0.05)].copy(). print(marker_table.shape). marker = dict(). for ct in marker_table[""cluster""].unique():. tmp = marker_table[marker_table[""cluster""] == ct]. marker[ct] = tmp.gene.values. ## Look at a predefined list of cell types and the corresponding marker genes in order to annotate our clusters. cell_annotation = sc.tl.marker_gene_overlap(adata, marker, key = 'rank_genes_groups', normalize = 'reference'). fig, ax = plt.subplots(figsize = (30, 10)). sb.heatmap(cell_annotation, cbar = False, annot = True). fig.savefig('/data/heatmap.png'). ```. Everything works smoothly and the results look fine until the marker_gene_overlap. A similar issue was observed by another ICB member who is trying to verify that their new environment still works. Any pointers strongly appreciated, since we consider this a severe issue blocking new analyses with the most recent packages.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1625
https://github.com/scverse/scanpy/issues/1625:234,deployability,version,version,234,"gene ranking produces completely wrong results on the most recent tool/library combinations on specific dataset; - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. # Introduction. Hi,. so this is a weird one and I could not track it down yet. The Schillerlab people have a workstation known as ""agando"". On this workstation the full environment is installed globally and shared by all users. I am looking to change that. # The issue. When calculating the `sc.tl.marker_gene_overlap` I get the expected and reasonable results on the agando environment, but completely rubbish results when running the same code with a fresh Conda environment and the latest dependencies installed. ![image](https://user-images.githubusercontent.com/21954664/106739402-659dfb80-6619-11eb-84f1-e75abfa6167d.png). Top = new, trash results. Bottom = old=agando expected results. The old environment has:. ```. scanpy==1.6.1.dev110+gb4234d81 anndata==0.7.4 umap==0.4.6 numpy==1.19.0 scipy==1.5.1 pandas==1.1.5 scikit-learn==0.23.1 statsmodels==0.12.1 python-igraph==0.8.0 louvain==0.6.1 leidenalg==0.8.3. ```. The new environment has . ```. scanpy==1.6.1 anndata==0.7.5 umap==0.4.6 numpy==1.20.0 scipy==1.6.0 pandas==1.2.1 scikit-learn==0.24.1 statsmodels==0.12.1 python-igraph==0.8.3 louvain==0.7.0 leidenalg==0.8.3. ```. Full new conda environment:. ```. name: single_cell_analysis. channels:. - defaults. dependencies:. - _libgcc_mutex=0.1=main. - argon2-cffi=20.1.0=py37h7b6447c_1. - async_generator=1.10=py37h28b3542_0. - attrs=20.3.0=pyhd3eb1b0_0. - backcall=0.2.0=pyhd3eb1b0_0. - bleach=3.3.0=pyhd3eb1b0_0. - ca-certificates=2021.1.19=h06a4308_0. - certifi=2020.12.5=py37h06a4308_0. - cffi=1.14.4=py37h261ae71_0. - dbus=1.13.18=hb2f20db_0. - decorator=4.4.2=pyhd3eb1b0_0. - defusedxml=0.6.0=py_0. - entrypoints=0.3=py37_0. - expat=2.2.10=he",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1625
https://github.com/scverse/scanpy/issues/1625:524,deployability,instal,installed,524,"gene ranking produces completely wrong results on the most recent tool/library combinations on specific dataset; - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. # Introduction. Hi,. so this is a weird one and I could not track it down yet. The Schillerlab people have a workstation known as ""agando"". On this workstation the full environment is installed globally and shared by all users. I am looking to change that. # The issue. When calculating the `sc.tl.marker_gene_overlap` I get the expected and reasonable results on the agando environment, but completely rubbish results when running the same code with a fresh Conda environment and the latest dependencies installed. ![image](https://user-images.githubusercontent.com/21954664/106739402-659dfb80-6619-11eb-84f1-e75abfa6167d.png). Top = new, trash results. Bottom = old=agando expected results. The old environment has:. ```. scanpy==1.6.1.dev110+gb4234d81 anndata==0.7.4 umap==0.4.6 numpy==1.19.0 scipy==1.5.1 pandas==1.1.5 scikit-learn==0.23.1 statsmodels==0.12.1 python-igraph==0.8.0 louvain==0.6.1 leidenalg==0.8.3. ```. The new environment has . ```. scanpy==1.6.1 anndata==0.7.5 umap==0.4.6 numpy==1.20.0 scipy==1.6.0 pandas==1.2.1 scikit-learn==0.24.1 statsmodels==0.12.1 python-igraph==0.8.3 louvain==0.7.0 leidenalg==0.8.3. ```. Full new conda environment:. ```. name: single_cell_analysis. channels:. - defaults. dependencies:. - _libgcc_mutex=0.1=main. - argon2-cffi=20.1.0=py37h7b6447c_1. - async_generator=1.10=py37h28b3542_0. - attrs=20.3.0=pyhd3eb1b0_0. - backcall=0.2.0=pyhd3eb1b0_0. - bleach=3.3.0=pyhd3eb1b0_0. - ca-certificates=2021.1.19=h06a4308_0. - certifi=2020.12.5=py37h06a4308_0. - cffi=1.14.4=py37h261ae71_0. - dbus=1.13.18=hb2f20db_0. - decorator=4.4.2=pyhd3eb1b0_0. - defusedxml=0.6.0=py_0. - entrypoints=0.3=py37_0. - expat=2.2.10=he",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1625
https://github.com/scverse/scanpy/issues/1625:832,deployability,depend,dependencies,832,"gene ranking produces completely wrong results on the most recent tool/library combinations on specific dataset; - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. # Introduction. Hi,. so this is a weird one and I could not track it down yet. The Schillerlab people have a workstation known as ""agando"". On this workstation the full environment is installed globally and shared by all users. I am looking to change that. # The issue. When calculating the `sc.tl.marker_gene_overlap` I get the expected and reasonable results on the agando environment, but completely rubbish results when running the same code with a fresh Conda environment and the latest dependencies installed. ![image](https://user-images.githubusercontent.com/21954664/106739402-659dfb80-6619-11eb-84f1-e75abfa6167d.png). Top = new, trash results. Bottom = old=agando expected results. The old environment has:. ```. scanpy==1.6.1.dev110+gb4234d81 anndata==0.7.4 umap==0.4.6 numpy==1.19.0 scipy==1.5.1 pandas==1.1.5 scikit-learn==0.23.1 statsmodels==0.12.1 python-igraph==0.8.0 louvain==0.6.1 leidenalg==0.8.3. ```. The new environment has . ```. scanpy==1.6.1 anndata==0.7.5 umap==0.4.6 numpy==1.20.0 scipy==1.6.0 pandas==1.2.1 scikit-learn==0.24.1 statsmodels==0.12.1 python-igraph==0.8.3 louvain==0.7.0 leidenalg==0.8.3. ```. Full new conda environment:. ```. name: single_cell_analysis. channels:. - defaults. dependencies:. - _libgcc_mutex=0.1=main. - argon2-cffi=20.1.0=py37h7b6447c_1. - async_generator=1.10=py37h28b3542_0. - attrs=20.3.0=pyhd3eb1b0_0. - backcall=0.2.0=pyhd3eb1b0_0. - bleach=3.3.0=pyhd3eb1b0_0. - ca-certificates=2021.1.19=h06a4308_0. - certifi=2020.12.5=py37h06a4308_0. - cffi=1.14.4=py37h261ae71_0. - dbus=1.13.18=hb2f20db_0. - decorator=4.4.2=pyhd3eb1b0_0. - defusedxml=0.6.0=py_0. - entrypoints=0.3=py37_0. - expat=2.2.10=he",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1625
https://github.com/scverse/scanpy/issues/1625:845,deployability,instal,installed,845,"gene ranking produces completely wrong results on the most recent tool/library combinations on specific dataset; - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. # Introduction. Hi,. so this is a weird one and I could not track it down yet. The Schillerlab people have a workstation known as ""agando"". On this workstation the full environment is installed globally and shared by all users. I am looking to change that. # The issue. When calculating the `sc.tl.marker_gene_overlap` I get the expected and reasonable results on the agando environment, but completely rubbish results when running the same code with a fresh Conda environment and the latest dependencies installed. ![image](https://user-images.githubusercontent.com/21954664/106739402-659dfb80-6619-11eb-84f1-e75abfa6167d.png). Top = new, trash results. Bottom = old=agando expected results. The old environment has:. ```. scanpy==1.6.1.dev110+gb4234d81 anndata==0.7.4 umap==0.4.6 numpy==1.19.0 scipy==1.5.1 pandas==1.1.5 scikit-learn==0.23.1 statsmodels==0.12.1 python-igraph==0.8.0 louvain==0.6.1 leidenalg==0.8.3. ```. The new environment has . ```. scanpy==1.6.1 anndata==0.7.5 umap==0.4.6 numpy==1.20.0 scipy==1.6.0 pandas==1.2.1 scikit-learn==0.24.1 statsmodels==0.12.1 python-igraph==0.8.3 louvain==0.7.0 leidenalg==0.8.3. ```. Full new conda environment:. ```. name: single_cell_analysis. channels:. - defaults. dependencies:. - _libgcc_mutex=0.1=main. - argon2-cffi=20.1.0=py37h7b6447c_1. - async_generator=1.10=py37h28b3542_0. - attrs=20.3.0=pyhd3eb1b0_0. - backcall=0.2.0=pyhd3eb1b0_0. - bleach=3.3.0=pyhd3eb1b0_0. - ca-certificates=2021.1.19=h06a4308_0. - certifi=2020.12.5=py37h06a4308_0. - cffi=1.14.4=py37h261ae71_0. - dbus=1.13.18=hb2f20db_0. - decorator=4.4.2=pyhd3eb1b0_0. - defusedxml=0.6.0=py_0. - entrypoints=0.3=py37_0. - expat=2.2.10=he",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1625
https://github.com/scverse/scanpy/issues/1625:1561,deployability,depend,dependencies,1561," I am looking to change that. # The issue. When calculating the `sc.tl.marker_gene_overlap` I get the expected and reasonable results on the agando environment, but completely rubbish results when running the same code with a fresh Conda environment and the latest dependencies installed. ![image](https://user-images.githubusercontent.com/21954664/106739402-659dfb80-6619-11eb-84f1-e75abfa6167d.png). Top = new, trash results. Bottom = old=agando expected results. The old environment has:. ```. scanpy==1.6.1.dev110+gb4234d81 anndata==0.7.4 umap==0.4.6 numpy==1.19.0 scipy==1.5.1 pandas==1.1.5 scikit-learn==0.23.1 statsmodels==0.12.1 python-igraph==0.8.0 louvain==0.6.1 leidenalg==0.8.3. ```. The new environment has . ```. scanpy==1.6.1 anndata==0.7.5 umap==0.4.6 numpy==1.20.0 scipy==1.6.0 pandas==1.2.1 scikit-learn==0.24.1 statsmodels==0.12.1 python-igraph==0.8.3 louvain==0.7.0 leidenalg==0.8.3. ```. Full new conda environment:. ```. name: single_cell_analysis. channels:. - defaults. dependencies:. - _libgcc_mutex=0.1=main. - argon2-cffi=20.1.0=py37h7b6447c_1. - async_generator=1.10=py37h28b3542_0. - attrs=20.3.0=pyhd3eb1b0_0. - backcall=0.2.0=pyhd3eb1b0_0. - bleach=3.3.0=pyhd3eb1b0_0. - ca-certificates=2021.1.19=h06a4308_0. - certifi=2020.12.5=py37h06a4308_0. - cffi=1.14.4=py37h261ae71_0. - dbus=1.13.18=hb2f20db_0. - decorator=4.4.2=pyhd3eb1b0_0. - defusedxml=0.6.0=py_0. - entrypoints=0.3=py37_0. - expat=2.2.10=he6710b0_2. - fontconfig=2.13.0=h9420a91_0. - freetype=2.10.4=h5ab3b9f_0. - glib=2.66.1=h92f7085_0. - gst-plugins-base=1.14.0=h8213a91_2. - gstreamer=1.14.0=h28cd5cc_2. - icu=58.2=he6710b0_3. - importlib_metadata=2.0.0=1. - ipykernel=5.3.4=py37h5ca1d4c_0. - ipython=7.20.0=py37hb070fc8_1. - ipython_genutils=0.2.0=pyhd3eb1b0_1. - ipywidgets=7.6.3=pyhd3eb1b0_1. - jedi=0.17.0=py37_0. - jinja2=2.11.3=pyhd3eb1b0_0. - jpeg=9b=h024ee3a_2. - jsonschema=3.2.0=py_2. - jupyter=1.0.0=py37_7. - jupyter_client=6.1.7=py_0. - jupyter_console=6.2.0=py_0. - jupyter_core=4.7.1=py37h",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1625
https://github.com/scverse/scanpy/issues/1625:4622,deployability,version,version,4622,"0.8=0. - ptyprocess=0.7.0=pyhd3eb1b0_2. - pycparser=2.20=py_2. - pygments=2.7.4=pyhd3eb1b0_0. - pyparsing=2.4.7=pyhd3eb1b0_0. - pyqt=5.9.2=py37h05f1152_2. - pyrsistent=0.17.3=py37h7b6447c_0. - python=3.7.9=h7579374_0. - python-dateutil=2.8.1=pyhd3eb1b0_0. - pyzmq=20.0.0=py37h2531618_1. - qt=5.9.7=h5867ecd_1. - qtconsole=4.7.7=py_0. - qtpy=1.9.0=py_0. - readline=8.1=h27cfd23_0. - send2trash=1.5.0=pyhd3eb1b0_1. - setuptools=52.0.0=py37h06a4308_0. - sip=4.19.8=py37hf484d3e_0. - six=1.15.0=py37h06a4308_0. - sqlite=3.33.0=h62c20be_0. - terminado=0.9.2=py37h06a4308_0. - testpath=0.4.4=pyhd3eb1b0_0. - tk=8.6.10=hbc83047_0. - tornado=6.1=py37h27cfd23_0. - traitlets=5.0.5=pyhd3eb1b0_0. - wcwidth=0.2.5=py_0. - webencodings=0.5.1=py37_1. - wheel=0.36.2=pyhd3eb1b0_0. - widgetsnbextension=3.5.1=py37_0. - xz=5.2.5=h7b6447c_0. - zeromq=4.3.3=he6710b0_3. - zipp=3.4.0=pyhd3eb1b0_0. - zlib=1.2.11=h7b6447c_3. - pip:. - anndata==0.7.5. - cached-property==1.5.2. - click==7.1.2. - cycler==0.10.0. - get-version==2.1. - h5py==3.1.0. - importlib-metadata==3.4.0. - joblib==1.0.0. - kiwisolver==1.3.1. - legacy-api-wrap==1.2. - leidenalg==0.8.3. - llvmlite==0.35.0. - loompy==3.0.6. - louvain==0.7.0. - matplotlib==3.3.4. - natsort==7.1.1. - networkx==2.5. - numba==0.52.0. - numexpr==2.7.2. - numpy==1.20.0. - numpy-groupies==0.9.13. - pandas==1.2.1. - patsy==0.5.1. - pillow==8.1.0. - python-igraph==0.8.3. - pytz==2021.1. - scanpy==1.6.1. - scikit-learn==0.24.1. - scipy==1.6.0. - scvelo==0.2.2. - seaborn==0.11.1. - setuptools-scm==5.0.1. - sinfo==0.3.1. - statsmodels==0.12.1. - stdlib-list==0.8.0. - tables==3.6.1. - texttable==1.6.3. - threadpoolctl==2.1.0. - tqdm==4.56.0. - typing-extensions==3.7.4.3. - umap-learn==0.4.6. ```. I can reproduce the issue with a Docker container that only has the minimal conda environment above. Additionally, I already tried installing the exact same dependency versions in the new environment, but got the same results. . If you need access to the data and the conta",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1625
https://github.com/scverse/scanpy/issues/1625:4727,deployability,api,api-wrap,4727,"=2.4.7=pyhd3eb1b0_0. - pyqt=5.9.2=py37h05f1152_2. - pyrsistent=0.17.3=py37h7b6447c_0. - python=3.7.9=h7579374_0. - python-dateutil=2.8.1=pyhd3eb1b0_0. - pyzmq=20.0.0=py37h2531618_1. - qt=5.9.7=h5867ecd_1. - qtconsole=4.7.7=py_0. - qtpy=1.9.0=py_0. - readline=8.1=h27cfd23_0. - send2trash=1.5.0=pyhd3eb1b0_1. - setuptools=52.0.0=py37h06a4308_0. - sip=4.19.8=py37hf484d3e_0. - six=1.15.0=py37h06a4308_0. - sqlite=3.33.0=h62c20be_0. - terminado=0.9.2=py37h06a4308_0. - testpath=0.4.4=pyhd3eb1b0_0. - tk=8.6.10=hbc83047_0. - tornado=6.1=py37h27cfd23_0. - traitlets=5.0.5=pyhd3eb1b0_0. - wcwidth=0.2.5=py_0. - webencodings=0.5.1=py37_1. - wheel=0.36.2=pyhd3eb1b0_0. - widgetsnbextension=3.5.1=py37_0. - xz=5.2.5=h7b6447c_0. - zeromq=4.3.3=he6710b0_3. - zipp=3.4.0=pyhd3eb1b0_0. - zlib=1.2.11=h7b6447c_3. - pip:. - anndata==0.7.5. - cached-property==1.5.2. - click==7.1.2. - cycler==0.10.0. - get-version==2.1. - h5py==3.1.0. - importlib-metadata==3.4.0. - joblib==1.0.0. - kiwisolver==1.3.1. - legacy-api-wrap==1.2. - leidenalg==0.8.3. - llvmlite==0.35.0. - loompy==3.0.6. - louvain==0.7.0. - matplotlib==3.3.4. - natsort==7.1.1. - networkx==2.5. - numba==0.52.0. - numexpr==2.7.2. - numpy==1.20.0. - numpy-groupies==0.9.13. - pandas==1.2.1. - patsy==0.5.1. - pillow==8.1.0. - python-igraph==0.8.3. - pytz==2021.1. - scanpy==1.6.1. - scikit-learn==0.24.1. - scipy==1.6.0. - scvelo==0.2.2. - seaborn==0.11.1. - setuptools-scm==5.0.1. - sinfo==0.3.1. - statsmodels==0.12.1. - stdlib-list==0.8.0. - tables==3.6.1. - texttable==1.6.3. - threadpoolctl==2.1.0. - tqdm==4.56.0. - typing-extensions==3.7.4.3. - umap-learn==0.4.6. ```. I can reproduce the issue with a Docker container that only has the minimal conda environment above. Additionally, I already tried installing the exact same dependency versions in the new environment, but got the same results. . If you need access to the data and the container please contact me and I will make it available to you. The data is already at the ICB cluster. Code:",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1625
https://github.com/scverse/scanpy/issues/1625:5393,deployability,contain,container,5393,"etsnbextension=3.5.1=py37_0. - xz=5.2.5=h7b6447c_0. - zeromq=4.3.3=he6710b0_3. - zipp=3.4.0=pyhd3eb1b0_0. - zlib=1.2.11=h7b6447c_3. - pip:. - anndata==0.7.5. - cached-property==1.5.2. - click==7.1.2. - cycler==0.10.0. - get-version==2.1. - h5py==3.1.0. - importlib-metadata==3.4.0. - joblib==1.0.0. - kiwisolver==1.3.1. - legacy-api-wrap==1.2. - leidenalg==0.8.3. - llvmlite==0.35.0. - loompy==3.0.6. - louvain==0.7.0. - matplotlib==3.3.4. - natsort==7.1.1. - networkx==2.5. - numba==0.52.0. - numexpr==2.7.2. - numpy==1.20.0. - numpy-groupies==0.9.13. - pandas==1.2.1. - patsy==0.5.1. - pillow==8.1.0. - python-igraph==0.8.3. - pytz==2021.1. - scanpy==1.6.1. - scikit-learn==0.24.1. - scipy==1.6.0. - scvelo==0.2.2. - seaborn==0.11.1. - setuptools-scm==5.0.1. - sinfo==0.3.1. - statsmodels==0.12.1. - stdlib-list==0.8.0. - tables==3.6.1. - texttable==1.6.3. - threadpoolctl==2.1.0. - tqdm==4.56.0. - typing-extensions==3.7.4.3. - umap-learn==0.4.6. ```. I can reproduce the issue with a Docker container that only has the minimal conda environment above. Additionally, I already tried installing the exact same dependency versions in the new environment, but got the same results. . If you need access to the data and the container please contact me and I will make it available to you. The data is already at the ICB cluster. Code:. ```. from os import path. import numpy as np. import matplotlib.pyplot as plt. import scanpy as sc. import scanpy.external as sce. from os import listdir. import pandas as pd. import seaborn as sb. import datetime, time. import scvelo as scv. from matplotlib.colors import LinearSegmentedColormap. #Define a nice colour map for gene expression. from matplotlib import colors. def timestamp():. ts = time.time(). st = datetime.datetime.fromtimestamp(ts).strftime('%d-%m-%Y %H:%M:%S'). return st. # Exporting folder. folder = ""/output"". sc.settings.figdir = folder + ""Plots/"". sc.set_figure_params(vector_friendly = True, dpi=300). sc.settings.verbosity = 3 # verbosi",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1625
https://github.com/scverse/scanpy/issues/1625:5484,deployability,instal,installing,5484,"=pyhd3eb1b0_0. - zlib=1.2.11=h7b6447c_3. - pip:. - anndata==0.7.5. - cached-property==1.5.2. - click==7.1.2. - cycler==0.10.0. - get-version==2.1. - h5py==3.1.0. - importlib-metadata==3.4.0. - joblib==1.0.0. - kiwisolver==1.3.1. - legacy-api-wrap==1.2. - leidenalg==0.8.3. - llvmlite==0.35.0. - loompy==3.0.6. - louvain==0.7.0. - matplotlib==3.3.4. - natsort==7.1.1. - networkx==2.5. - numba==0.52.0. - numexpr==2.7.2. - numpy==1.20.0. - numpy-groupies==0.9.13. - pandas==1.2.1. - patsy==0.5.1. - pillow==8.1.0. - python-igraph==0.8.3. - pytz==2021.1. - scanpy==1.6.1. - scikit-learn==0.24.1. - scipy==1.6.0. - scvelo==0.2.2. - seaborn==0.11.1. - setuptools-scm==5.0.1. - sinfo==0.3.1. - statsmodels==0.12.1. - stdlib-list==0.8.0. - tables==3.6.1. - texttable==1.6.3. - threadpoolctl==2.1.0. - tqdm==4.56.0. - typing-extensions==3.7.4.3. - umap-learn==0.4.6. ```. I can reproduce the issue with a Docker container that only has the minimal conda environment above. Additionally, I already tried installing the exact same dependency versions in the new environment, but got the same results. . If you need access to the data and the container please contact me and I will make it available to you. The data is already at the ICB cluster. Code:. ```. from os import path. import numpy as np. import matplotlib.pyplot as plt. import scanpy as sc. import scanpy.external as sce. from os import listdir. import pandas as pd. import seaborn as sb. import datetime, time. import scvelo as scv. from matplotlib.colors import LinearSegmentedColormap. #Define a nice colour map for gene expression. from matplotlib import colors. def timestamp():. ts = time.time(). st = datetime.datetime.fromtimestamp(ts).strftime('%d-%m-%Y %H:%M:%S'). return st. # Exporting folder. folder = ""/output"". sc.settings.figdir = folder + ""Plots/"". sc.set_figure_params(vector_friendly = True, dpi=300). sc.settings.verbosity = 3 # verbosity: errors (0), warnings (1), info (2), hints (3). sc.logging.print_version_and_date(). sc.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1625
https://github.com/scverse/scanpy/issues/1625:5510,deployability,depend,dependency,5510,"11=h7b6447c_3. - pip:. - anndata==0.7.5. - cached-property==1.5.2. - click==7.1.2. - cycler==0.10.0. - get-version==2.1. - h5py==3.1.0. - importlib-metadata==3.4.0. - joblib==1.0.0. - kiwisolver==1.3.1. - legacy-api-wrap==1.2. - leidenalg==0.8.3. - llvmlite==0.35.0. - loompy==3.0.6. - louvain==0.7.0. - matplotlib==3.3.4. - natsort==7.1.1. - networkx==2.5. - numba==0.52.0. - numexpr==2.7.2. - numpy==1.20.0. - numpy-groupies==0.9.13. - pandas==1.2.1. - patsy==0.5.1. - pillow==8.1.0. - python-igraph==0.8.3. - pytz==2021.1. - scanpy==1.6.1. - scikit-learn==0.24.1. - scipy==1.6.0. - scvelo==0.2.2. - seaborn==0.11.1. - setuptools-scm==5.0.1. - sinfo==0.3.1. - statsmodels==0.12.1. - stdlib-list==0.8.0. - tables==3.6.1. - texttable==1.6.3. - threadpoolctl==2.1.0. - tqdm==4.56.0. - typing-extensions==3.7.4.3. - umap-learn==0.4.6. ```. I can reproduce the issue with a Docker container that only has the minimal conda environment above. Additionally, I already tried installing the exact same dependency versions in the new environment, but got the same results. . If you need access to the data and the container please contact me and I will make it available to you. The data is already at the ICB cluster. Code:. ```. from os import path. import numpy as np. import matplotlib.pyplot as plt. import scanpy as sc. import scanpy.external as sce. from os import listdir. import pandas as pd. import seaborn as sb. import datetime, time. import scvelo as scv. from matplotlib.colors import LinearSegmentedColormap. #Define a nice colour map for gene expression. from matplotlib import colors. def timestamp():. ts = time.time(). st = datetime.datetime.fromtimestamp(ts).strftime('%d-%m-%Y %H:%M:%S'). return st. # Exporting folder. folder = ""/output"". sc.settings.figdir = folder + ""Plots/"". sc.set_figure_params(vector_friendly = True, dpi=300). sc.settings.verbosity = 3 # verbosity: errors (0), warnings (1), info (2), hints (3). sc.logging.print_version_and_date(). sc.logging.print_header(). ad",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1625
https://github.com/scverse/scanpy/issues/1625:5521,deployability,version,versions,5521,"c_3. - pip:. - anndata==0.7.5. - cached-property==1.5.2. - click==7.1.2. - cycler==0.10.0. - get-version==2.1. - h5py==3.1.0. - importlib-metadata==3.4.0. - joblib==1.0.0. - kiwisolver==1.3.1. - legacy-api-wrap==1.2. - leidenalg==0.8.3. - llvmlite==0.35.0. - loompy==3.0.6. - louvain==0.7.0. - matplotlib==3.3.4. - natsort==7.1.1. - networkx==2.5. - numba==0.52.0. - numexpr==2.7.2. - numpy==1.20.0. - numpy-groupies==0.9.13. - pandas==1.2.1. - patsy==0.5.1. - pillow==8.1.0. - python-igraph==0.8.3. - pytz==2021.1. - scanpy==1.6.1. - scikit-learn==0.24.1. - scipy==1.6.0. - scvelo==0.2.2. - seaborn==0.11.1. - setuptools-scm==5.0.1. - sinfo==0.3.1. - statsmodels==0.12.1. - stdlib-list==0.8.0. - tables==3.6.1. - texttable==1.6.3. - threadpoolctl==2.1.0. - tqdm==4.56.0. - typing-extensions==3.7.4.3. - umap-learn==0.4.6. ```. I can reproduce the issue with a Docker container that only has the minimal conda environment above. Additionally, I already tried installing the exact same dependency versions in the new environment, but got the same results. . If you need access to the data and the container please contact me and I will make it available to you. The data is already at the ICB cluster. Code:. ```. from os import path. import numpy as np. import matplotlib.pyplot as plt. import scanpy as sc. import scanpy.external as sce. from os import listdir. import pandas as pd. import seaborn as sb. import datetime, time. import scvelo as scv. from matplotlib.colors import LinearSegmentedColormap. #Define a nice colour map for gene expression. from matplotlib import colors. def timestamp():. ts = time.time(). st = datetime.datetime.fromtimestamp(ts).strftime('%d-%m-%Y %H:%M:%S'). return st. # Exporting folder. folder = ""/output"". sc.settings.figdir = folder + ""Plots/"". sc.set_figure_params(vector_friendly = True, dpi=300). sc.settings.verbosity = 3 # verbosity: errors (0), warnings (1), info (2), hints (3). sc.logging.print_version_and_date(). sc.logging.print_header(). adata = sc.r",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1625
https://github.com/scverse/scanpy/issues/1625:5621,deployability,contain,container,5621,"ion==2.1. - h5py==3.1.0. - importlib-metadata==3.4.0. - joblib==1.0.0. - kiwisolver==1.3.1. - legacy-api-wrap==1.2. - leidenalg==0.8.3. - llvmlite==0.35.0. - loompy==3.0.6. - louvain==0.7.0. - matplotlib==3.3.4. - natsort==7.1.1. - networkx==2.5. - numba==0.52.0. - numexpr==2.7.2. - numpy==1.20.0. - numpy-groupies==0.9.13. - pandas==1.2.1. - patsy==0.5.1. - pillow==8.1.0. - python-igraph==0.8.3. - pytz==2021.1. - scanpy==1.6.1. - scikit-learn==0.24.1. - scipy==1.6.0. - scvelo==0.2.2. - seaborn==0.11.1. - setuptools-scm==5.0.1. - sinfo==0.3.1. - statsmodels==0.12.1. - stdlib-list==0.8.0. - tables==3.6.1. - texttable==1.6.3. - threadpoolctl==2.1.0. - tqdm==4.56.0. - typing-extensions==3.7.4.3. - umap-learn==0.4.6. ```. I can reproduce the issue with a Docker container that only has the minimal conda environment above. Additionally, I already tried installing the exact same dependency versions in the new environment, but got the same results. . If you need access to the data and the container please contact me and I will make it available to you. The data is already at the ICB cluster. Code:. ```. from os import path. import numpy as np. import matplotlib.pyplot as plt. import scanpy as sc. import scanpy.external as sce. from os import listdir. import pandas as pd. import seaborn as sb. import datetime, time. import scvelo as scv. from matplotlib.colors import LinearSegmentedColormap. #Define a nice colour map for gene expression. from matplotlib import colors. def timestamp():. ts = time.time(). st = datetime.datetime.fromtimestamp(ts).strftime('%d-%m-%Y %H:%M:%S'). return st. # Exporting folder. folder = ""/output"". sc.settings.figdir = folder + ""Plots/"". sc.set_figure_params(vector_friendly = True, dpi=300). sc.settings.verbosity = 3 # verbosity: errors (0), warnings (1), info (2), hints (3). sc.logging.print_version_and_date(). sc.logging.print_header(). adata = sc.read(""/data/190924_Recreated_Virus_Object_regressed.h5ad""). #adata.write(folder + ""190924_Recreated_Vi",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1625
https://github.com/scverse/scanpy/issues/1625:5717,deployability,cluster,cluster,5717,"egacy-api-wrap==1.2. - leidenalg==0.8.3. - llvmlite==0.35.0. - loompy==3.0.6. - louvain==0.7.0. - matplotlib==3.3.4. - natsort==7.1.1. - networkx==2.5. - numba==0.52.0. - numexpr==2.7.2. - numpy==1.20.0. - numpy-groupies==0.9.13. - pandas==1.2.1. - patsy==0.5.1. - pillow==8.1.0. - python-igraph==0.8.3. - pytz==2021.1. - scanpy==1.6.1. - scikit-learn==0.24.1. - scipy==1.6.0. - scvelo==0.2.2. - seaborn==0.11.1. - setuptools-scm==5.0.1. - sinfo==0.3.1. - statsmodels==0.12.1. - stdlib-list==0.8.0. - tables==3.6.1. - texttable==1.6.3. - threadpoolctl==2.1.0. - tqdm==4.56.0. - typing-extensions==3.7.4.3. - umap-learn==0.4.6. ```. I can reproduce the issue with a Docker container that only has the minimal conda environment above. Additionally, I already tried installing the exact same dependency versions in the new environment, but got the same results. . If you need access to the data and the container please contact me and I will make it available to you. The data is already at the ICB cluster. Code:. ```. from os import path. import numpy as np. import matplotlib.pyplot as plt. import scanpy as sc. import scanpy.external as sce. from os import listdir. import pandas as pd. import seaborn as sb. import datetime, time. import scvelo as scv. from matplotlib.colors import LinearSegmentedColormap. #Define a nice colour map for gene expression. from matplotlib import colors. def timestamp():. ts = time.time(). st = datetime.datetime.fromtimestamp(ts).strftime('%d-%m-%Y %H:%M:%S'). return st. # Exporting folder. folder = ""/output"". sc.settings.figdir = folder + ""Plots/"". sc.set_figure_params(vector_friendly = True, dpi=300). sc.settings.verbosity = 3 # verbosity: errors (0), warnings (1), info (2), hints (3). sc.logging.print_version_and_date(). sc.logging.print_header(). adata = sc.read(""/data/190924_Recreated_Virus_Object_regressed.h5ad""). #adata.write(folder + ""190924_Recreated_Virus_Object_regressed.h5ad""). sc.tl.louvain(adata, resolution = 4, key_added = ""louvain_2""). sc.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1625
https://github.com/scverse/scanpy/issues/1625:6452,deployability,log,logging,6452,"ditionally, I already tried installing the exact same dependency versions in the new environment, but got the same results. . If you need access to the data and the container please contact me and I will make it available to you. The data is already at the ICB cluster. Code:. ```. from os import path. import numpy as np. import matplotlib.pyplot as plt. import scanpy as sc. import scanpy.external as sce. from os import listdir. import pandas as pd. import seaborn as sb. import datetime, time. import scvelo as scv. from matplotlib.colors import LinearSegmentedColormap. #Define a nice colour map for gene expression. from matplotlib import colors. def timestamp():. ts = time.time(). st = datetime.datetime.fromtimestamp(ts).strftime('%d-%m-%Y %H:%M:%S'). return st. # Exporting folder. folder = ""/output"". sc.settings.figdir = folder + ""Plots/"". sc.set_figure_params(vector_friendly = True, dpi=300). sc.settings.verbosity = 3 # verbosity: errors (0), warnings (1), info (2), hints (3). sc.logging.print_version_and_date(). sc.logging.print_header(). adata = sc.read(""/data/190924_Recreated_Virus_Object_regressed.h5ad""). #adata.write(folder + ""190924_Recreated_Virus_Object_regressed.h5ad""). sc.tl.louvain(adata, resolution = 4, key_added = ""louvain_2""). sc.tl.louvain(adata, resolution = 5, key_added = ""louvain_3""). sc.tl.louvain(adata, resolution = 6, key_added = ""louvain_4""). sc.tl.louvain(adata, resolution = 7, key_added = ""louvain_5""). sc.pl.umap(adata, color = [""louvain_2"", ""louvain_3"", ""louvain_4"", ""louvain_5""], wspace = 0.45). #select resolution. print(adata.obs[""louvain_5""].value_counts()). sc.tl.rank_genes_groups(adata, groupby = ""louvain_5""). # read all arkers table from known annotated data. marker_folder = ""/marker/"". marker_table = pd.read_csv(marker_folder + ""Particle_AllMarkers.txt"", sep = ""\t"", index_col = None). marker_table.head(2). ## Restrict to Foldchange and P value. marker_table = marker_table[(marker_table.logfoldchange > 2) & (marker_table.pval_adj < 0.0",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1625
https://github.com/scverse/scanpy/issues/1625:6489,deployability,log,logging,6489,"g the exact same dependency versions in the new environment, but got the same results. . If you need access to the data and the container please contact me and I will make it available to you. The data is already at the ICB cluster. Code:. ```. from os import path. import numpy as np. import matplotlib.pyplot as plt. import scanpy as sc. import scanpy.external as sce. from os import listdir. import pandas as pd. import seaborn as sb. import datetime, time. import scvelo as scv. from matplotlib.colors import LinearSegmentedColormap. #Define a nice colour map for gene expression. from matplotlib import colors. def timestamp():. ts = time.time(). st = datetime.datetime.fromtimestamp(ts).strftime('%d-%m-%Y %H:%M:%S'). return st. # Exporting folder. folder = ""/output"". sc.settings.figdir = folder + ""Plots/"". sc.set_figure_params(vector_friendly = True, dpi=300). sc.settings.verbosity = 3 # verbosity: errors (0), warnings (1), info (2), hints (3). sc.logging.print_version_and_date(). sc.logging.print_header(). adata = sc.read(""/data/190924_Recreated_Virus_Object_regressed.h5ad""). #adata.write(folder + ""190924_Recreated_Virus_Object_regressed.h5ad""). sc.tl.louvain(adata, resolution = 4, key_added = ""louvain_2""). sc.tl.louvain(adata, resolution = 5, key_added = ""louvain_3""). sc.tl.louvain(adata, resolution = 6, key_added = ""louvain_4""). sc.tl.louvain(adata, resolution = 7, key_added = ""louvain_5""). sc.pl.umap(adata, color = [""louvain_2"", ""louvain_3"", ""louvain_4"", ""louvain_5""], wspace = 0.45). #select resolution. print(adata.obs[""louvain_5""].value_counts()). sc.tl.rank_genes_groups(adata, groupby = ""louvain_5""). # read all arkers table from known annotated data. marker_folder = ""/marker/"". marker_table = pd.read_csv(marker_folder + ""Particle_AllMarkers.txt"", sep = ""\t"", index_col = None). marker_table.head(2). ## Restrict to Foldchange and P value. marker_table = marker_table[(marker_table.logfoldchange > 2) & (marker_table.pval_adj < 0.05)].copy(). print(marker_table.shape)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1625
https://github.com/scverse/scanpy/issues/1625:7407,deployability,log,logfoldchange,7407,"ms(vector_friendly = True, dpi=300). sc.settings.verbosity = 3 # verbosity: errors (0), warnings (1), info (2), hints (3). sc.logging.print_version_and_date(). sc.logging.print_header(). adata = sc.read(""/data/190924_Recreated_Virus_Object_regressed.h5ad""). #adata.write(folder + ""190924_Recreated_Virus_Object_regressed.h5ad""). sc.tl.louvain(adata, resolution = 4, key_added = ""louvain_2""). sc.tl.louvain(adata, resolution = 5, key_added = ""louvain_3""). sc.tl.louvain(adata, resolution = 6, key_added = ""louvain_4""). sc.tl.louvain(adata, resolution = 7, key_added = ""louvain_5""). sc.pl.umap(adata, color = [""louvain_2"", ""louvain_3"", ""louvain_4"", ""louvain_5""], wspace = 0.45). #select resolution. print(adata.obs[""louvain_5""].value_counts()). sc.tl.rank_genes_groups(adata, groupby = ""louvain_5""). # read all arkers table from known annotated data. marker_folder = ""/marker/"". marker_table = pd.read_csv(marker_folder + ""Particle_AllMarkers.txt"", sep = ""\t"", index_col = None). marker_table.head(2). ## Restrict to Foldchange and P value. marker_table = marker_table[(marker_table.logfoldchange > 2) & (marker_table.pval_adj < 0.05)].copy(). print(marker_table.shape). marker = dict(). for ct in marker_table[""cluster""].unique():. tmp = marker_table[marker_table[""cluster""] == ct]. marker[ct] = tmp.gene.values. ## Look at a predefined list of cell types and the corresponding marker genes in order to annotate our clusters. cell_annotation = sc.tl.marker_gene_overlap(adata, marker, key = 'rank_genes_groups', normalize = 'reference'). fig, ax = plt.subplots(figsize = (30, 10)). sb.heatmap(cell_annotation, cbar = False, annot = True). fig.savefig('/data/heatmap.png'). ```. Everything works smoothly and the results look fine until the marker_gene_overlap. A similar issue was observed by another ICB member who is trying to verify that their new environment still works. Any pointers strongly appreciated, since we consider this a severe issue blocking new analyses with the most recent packages.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1625
https://github.com/scverse/scanpy/issues/1625:7536,deployability,cluster,cluster,7536,"ms(vector_friendly = True, dpi=300). sc.settings.verbosity = 3 # verbosity: errors (0), warnings (1), info (2), hints (3). sc.logging.print_version_and_date(). sc.logging.print_header(). adata = sc.read(""/data/190924_Recreated_Virus_Object_regressed.h5ad""). #adata.write(folder + ""190924_Recreated_Virus_Object_regressed.h5ad""). sc.tl.louvain(adata, resolution = 4, key_added = ""louvain_2""). sc.tl.louvain(adata, resolution = 5, key_added = ""louvain_3""). sc.tl.louvain(adata, resolution = 6, key_added = ""louvain_4""). sc.tl.louvain(adata, resolution = 7, key_added = ""louvain_5""). sc.pl.umap(adata, color = [""louvain_2"", ""louvain_3"", ""louvain_4"", ""louvain_5""], wspace = 0.45). #select resolution. print(adata.obs[""louvain_5""].value_counts()). sc.tl.rank_genes_groups(adata, groupby = ""louvain_5""). # read all arkers table from known annotated data. marker_folder = ""/marker/"". marker_table = pd.read_csv(marker_folder + ""Particle_AllMarkers.txt"", sep = ""\t"", index_col = None). marker_table.head(2). ## Restrict to Foldchange and P value. marker_table = marker_table[(marker_table.logfoldchange > 2) & (marker_table.pval_adj < 0.05)].copy(). print(marker_table.shape). marker = dict(). for ct in marker_table[""cluster""].unique():. tmp = marker_table[marker_table[""cluster""] == ct]. marker[ct] = tmp.gene.values. ## Look at a predefined list of cell types and the corresponding marker genes in order to annotate our clusters. cell_annotation = sc.tl.marker_gene_overlap(adata, marker, key = 'rank_genes_groups', normalize = 'reference'). fig, ax = plt.subplots(figsize = (30, 10)). sb.heatmap(cell_annotation, cbar = False, annot = True). fig.savefig('/data/heatmap.png'). ```. Everything works smoothly and the results look fine until the marker_gene_overlap. A similar issue was observed by another ICB member who is trying to verify that their new environment still works. Any pointers strongly appreciated, since we consider this a severe issue blocking new analyses with the most recent packages.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1625
https://github.com/scverse/scanpy/issues/1625:7590,deployability,cluster,cluster,7590,"ms(vector_friendly = True, dpi=300). sc.settings.verbosity = 3 # verbosity: errors (0), warnings (1), info (2), hints (3). sc.logging.print_version_and_date(). sc.logging.print_header(). adata = sc.read(""/data/190924_Recreated_Virus_Object_regressed.h5ad""). #adata.write(folder + ""190924_Recreated_Virus_Object_regressed.h5ad""). sc.tl.louvain(adata, resolution = 4, key_added = ""louvain_2""). sc.tl.louvain(adata, resolution = 5, key_added = ""louvain_3""). sc.tl.louvain(adata, resolution = 6, key_added = ""louvain_4""). sc.tl.louvain(adata, resolution = 7, key_added = ""louvain_5""). sc.pl.umap(adata, color = [""louvain_2"", ""louvain_3"", ""louvain_4"", ""louvain_5""], wspace = 0.45). #select resolution. print(adata.obs[""louvain_5""].value_counts()). sc.tl.rank_genes_groups(adata, groupby = ""louvain_5""). # read all arkers table from known annotated data. marker_folder = ""/marker/"". marker_table = pd.read_csv(marker_folder + ""Particle_AllMarkers.txt"", sep = ""\t"", index_col = None). marker_table.head(2). ## Restrict to Foldchange and P value. marker_table = marker_table[(marker_table.logfoldchange > 2) & (marker_table.pval_adj < 0.05)].copy(). print(marker_table.shape). marker = dict(). for ct in marker_table[""cluster""].unique():. tmp = marker_table[marker_table[""cluster""] == ct]. marker[ct] = tmp.gene.values. ## Look at a predefined list of cell types and the corresponding marker genes in order to annotate our clusters. cell_annotation = sc.tl.marker_gene_overlap(adata, marker, key = 'rank_genes_groups', normalize = 'reference'). fig, ax = plt.subplots(figsize = (30, 10)). sb.heatmap(cell_annotation, cbar = False, annot = True). fig.savefig('/data/heatmap.png'). ```. Everything works smoothly and the results look fine until the marker_gene_overlap. A similar issue was observed by another ICB member who is trying to verify that their new environment still works. Any pointers strongly appreciated, since we consider this a severe issue blocking new analyses with the most recent packages.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1625
https://github.com/scverse/scanpy/issues/1625:7741,deployability,cluster,clusters,7741,"ms(vector_friendly = True, dpi=300). sc.settings.verbosity = 3 # verbosity: errors (0), warnings (1), info (2), hints (3). sc.logging.print_version_and_date(). sc.logging.print_header(). adata = sc.read(""/data/190924_Recreated_Virus_Object_regressed.h5ad""). #adata.write(folder + ""190924_Recreated_Virus_Object_regressed.h5ad""). sc.tl.louvain(adata, resolution = 4, key_added = ""louvain_2""). sc.tl.louvain(adata, resolution = 5, key_added = ""louvain_3""). sc.tl.louvain(adata, resolution = 6, key_added = ""louvain_4""). sc.tl.louvain(adata, resolution = 7, key_added = ""louvain_5""). sc.pl.umap(adata, color = [""louvain_2"", ""louvain_3"", ""louvain_4"", ""louvain_5""], wspace = 0.45). #select resolution. print(adata.obs[""louvain_5""].value_counts()). sc.tl.rank_genes_groups(adata, groupby = ""louvain_5""). # read all arkers table from known annotated data. marker_folder = ""/marker/"". marker_table = pd.read_csv(marker_folder + ""Particle_AllMarkers.txt"", sep = ""\t"", index_col = None). marker_table.head(2). ## Restrict to Foldchange and P value. marker_table = marker_table[(marker_table.logfoldchange > 2) & (marker_table.pval_adj < 0.05)].copy(). print(marker_table.shape). marker = dict(). for ct in marker_table[""cluster""].unique():. tmp = marker_table[marker_table[""cluster""] == ct]. marker[ct] = tmp.gene.values. ## Look at a predefined list of cell types and the corresponding marker genes in order to annotate our clusters. cell_annotation = sc.tl.marker_gene_overlap(adata, marker, key = 'rank_genes_groups', normalize = 'reference'). fig, ax = plt.subplots(figsize = (30, 10)). sb.heatmap(cell_annotation, cbar = False, annot = True). fig.savefig('/data/heatmap.png'). ```. Everything works smoothly and the results look fine until the marker_gene_overlap. A similar issue was observed by another ICB member who is trying to verify that their new environment still works. Any pointers strongly appreciated, since we consider this a severe issue blocking new analyses with the most recent packages.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1625
https://github.com/scverse/scanpy/issues/1625:8106,deployability,observ,observed,8106,"ms(vector_friendly = True, dpi=300). sc.settings.verbosity = 3 # verbosity: errors (0), warnings (1), info (2), hints (3). sc.logging.print_version_and_date(). sc.logging.print_header(). adata = sc.read(""/data/190924_Recreated_Virus_Object_regressed.h5ad""). #adata.write(folder + ""190924_Recreated_Virus_Object_regressed.h5ad""). sc.tl.louvain(adata, resolution = 4, key_added = ""louvain_2""). sc.tl.louvain(adata, resolution = 5, key_added = ""louvain_3""). sc.tl.louvain(adata, resolution = 6, key_added = ""louvain_4""). sc.tl.louvain(adata, resolution = 7, key_added = ""louvain_5""). sc.pl.umap(adata, color = [""louvain_2"", ""louvain_3"", ""louvain_4"", ""louvain_5""], wspace = 0.45). #select resolution. print(adata.obs[""louvain_5""].value_counts()). sc.tl.rank_genes_groups(adata, groupby = ""louvain_5""). # read all arkers table from known annotated data. marker_folder = ""/marker/"". marker_table = pd.read_csv(marker_folder + ""Particle_AllMarkers.txt"", sep = ""\t"", index_col = None). marker_table.head(2). ## Restrict to Foldchange and P value. marker_table = marker_table[(marker_table.logfoldchange > 2) & (marker_table.pval_adj < 0.05)].copy(). print(marker_table.shape). marker = dict(). for ct in marker_table[""cluster""].unique():. tmp = marker_table[marker_table[""cluster""] == ct]. marker[ct] = tmp.gene.values. ## Look at a predefined list of cell types and the corresponding marker genes in order to annotate our clusters. cell_annotation = sc.tl.marker_gene_overlap(adata, marker, key = 'rank_genes_groups', normalize = 'reference'). fig, ax = plt.subplots(figsize = (30, 10)). sb.heatmap(cell_annotation, cbar = False, annot = True). fig.savefig('/data/heatmap.png'). ```. Everything works smoothly and the results look fine until the marker_gene_overlap. A similar issue was observed by another ICB member who is trying to verify that their new environment still works. Any pointers strongly appreciated, since we consider this a severe issue blocking new analyses with the most recent packages.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1625
https://github.com/scverse/scanpy/issues/1625:7910,energy efficiency,heat,heatmap,7910,"ms(vector_friendly = True, dpi=300). sc.settings.verbosity = 3 # verbosity: errors (0), warnings (1), info (2), hints (3). sc.logging.print_version_and_date(). sc.logging.print_header(). adata = sc.read(""/data/190924_Recreated_Virus_Object_regressed.h5ad""). #adata.write(folder + ""190924_Recreated_Virus_Object_regressed.h5ad""). sc.tl.louvain(adata, resolution = 4, key_added = ""louvain_2""). sc.tl.louvain(adata, resolution = 5, key_added = ""louvain_3""). sc.tl.louvain(adata, resolution = 6, key_added = ""louvain_4""). sc.tl.louvain(adata, resolution = 7, key_added = ""louvain_5""). sc.pl.umap(adata, color = [""louvain_2"", ""louvain_3"", ""louvain_4"", ""louvain_5""], wspace = 0.45). #select resolution. print(adata.obs[""louvain_5""].value_counts()). sc.tl.rank_genes_groups(adata, groupby = ""louvain_5""). # read all arkers table from known annotated data. marker_folder = ""/marker/"". marker_table = pd.read_csv(marker_folder + ""Particle_AllMarkers.txt"", sep = ""\t"", index_col = None). marker_table.head(2). ## Restrict to Foldchange and P value. marker_table = marker_table[(marker_table.logfoldchange > 2) & (marker_table.pval_adj < 0.05)].copy(). print(marker_table.shape). marker = dict(). for ct in marker_table[""cluster""].unique():. tmp = marker_table[marker_table[""cluster""] == ct]. marker[ct] = tmp.gene.values. ## Look at a predefined list of cell types and the corresponding marker genes in order to annotate our clusters. cell_annotation = sc.tl.marker_gene_overlap(adata, marker, key = 'rank_genes_groups', normalize = 'reference'). fig, ax = plt.subplots(figsize = (30, 10)). sb.heatmap(cell_annotation, cbar = False, annot = True). fig.savefig('/data/heatmap.png'). ```. Everything works smoothly and the results look fine until the marker_gene_overlap. A similar issue was observed by another ICB member who is trying to verify that their new environment still works. Any pointers strongly appreciated, since we consider this a severe issue blocking new analyses with the most recent packages.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1625
https://github.com/scverse/scanpy/issues/1625:7983,energy efficiency,heat,heatmap,7983,"ms(vector_friendly = True, dpi=300). sc.settings.verbosity = 3 # verbosity: errors (0), warnings (1), info (2), hints (3). sc.logging.print_version_and_date(). sc.logging.print_header(). adata = sc.read(""/data/190924_Recreated_Virus_Object_regressed.h5ad""). #adata.write(folder + ""190924_Recreated_Virus_Object_regressed.h5ad""). sc.tl.louvain(adata, resolution = 4, key_added = ""louvain_2""). sc.tl.louvain(adata, resolution = 5, key_added = ""louvain_3""). sc.tl.louvain(adata, resolution = 6, key_added = ""louvain_4""). sc.tl.louvain(adata, resolution = 7, key_added = ""louvain_5""). sc.pl.umap(adata, color = [""louvain_2"", ""louvain_3"", ""louvain_4"", ""louvain_5""], wspace = 0.45). #select resolution. print(adata.obs[""louvain_5""].value_counts()). sc.tl.rank_genes_groups(adata, groupby = ""louvain_5""). # read all arkers table from known annotated data. marker_folder = ""/marker/"". marker_table = pd.read_csv(marker_folder + ""Particle_AllMarkers.txt"", sep = ""\t"", index_col = None). marker_table.head(2). ## Restrict to Foldchange and P value. marker_table = marker_table[(marker_table.logfoldchange > 2) & (marker_table.pval_adj < 0.05)].copy(). print(marker_table.shape). marker = dict(). for ct in marker_table[""cluster""].unique():. tmp = marker_table[marker_table[""cluster""] == ct]. marker[ct] = tmp.gene.values. ## Look at a predefined list of cell types and the corresponding marker genes in order to annotate our clusters. cell_annotation = sc.tl.marker_gene_overlap(adata, marker, key = 'rank_genes_groups', normalize = 'reference'). fig, ax = plt.subplots(figsize = (30, 10)). sb.heatmap(cell_annotation, cbar = False, annot = True). fig.savefig('/data/heatmap.png'). ```. Everything works smoothly and the results look fine until the marker_gene_overlap. A similar issue was observed by another ICB member who is trying to verify that their new environment still works. Any pointers strongly appreciated, since we consider this a severe issue blocking new analyses with the most recent packages.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1625
https://github.com/scverse/scanpy/issues/1625:234,integrability,version,version,234,"gene ranking produces completely wrong results on the most recent tool/library combinations on specific dataset; - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. # Introduction. Hi,. so this is a weird one and I could not track it down yet. The Schillerlab people have a workstation known as ""agando"". On this workstation the full environment is installed globally and shared by all users. I am looking to change that. # The issue. When calculating the `sc.tl.marker_gene_overlap` I get the expected and reasonable results on the agando environment, but completely rubbish results when running the same code with a fresh Conda environment and the latest dependencies installed. ![image](https://user-images.githubusercontent.com/21954664/106739402-659dfb80-6619-11eb-84f1-e75abfa6167d.png). Top = new, trash results. Bottom = old=agando expected results. The old environment has:. ```. scanpy==1.6.1.dev110+gb4234d81 anndata==0.7.4 umap==0.4.6 numpy==1.19.0 scipy==1.5.1 pandas==1.1.5 scikit-learn==0.23.1 statsmodels==0.12.1 python-igraph==0.8.0 louvain==0.6.1 leidenalg==0.8.3. ```. The new environment has . ```. scanpy==1.6.1 anndata==0.7.5 umap==0.4.6 numpy==1.20.0 scipy==1.6.0 pandas==1.2.1 scikit-learn==0.24.1 statsmodels==0.12.1 python-igraph==0.8.3 louvain==0.7.0 leidenalg==0.8.3. ```. Full new conda environment:. ```. name: single_cell_analysis. channels:. - defaults. dependencies:. - _libgcc_mutex=0.1=main. - argon2-cffi=20.1.0=py37h7b6447c_1. - async_generator=1.10=py37h28b3542_0. - attrs=20.3.0=pyhd3eb1b0_0. - backcall=0.2.0=pyhd3eb1b0_0. - bleach=3.3.0=pyhd3eb1b0_0. - ca-certificates=2021.1.19=h06a4308_0. - certifi=2020.12.5=py37h06a4308_0. - cffi=1.14.4=py37h261ae71_0. - dbus=1.13.18=hb2f20db_0. - decorator=4.4.2=pyhd3eb1b0_0. - defusedxml=0.6.0=py_0. - entrypoints=0.3=py37_0. - expat=2.2.10=he",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1625
https://github.com/scverse/scanpy/issues/1625:832,integrability,depend,dependencies,832,"gene ranking produces completely wrong results on the most recent tool/library combinations on specific dataset; - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. # Introduction. Hi,. so this is a weird one and I could not track it down yet. The Schillerlab people have a workstation known as ""agando"". On this workstation the full environment is installed globally and shared by all users. I am looking to change that. # The issue. When calculating the `sc.tl.marker_gene_overlap` I get the expected and reasonable results on the agando environment, but completely rubbish results when running the same code with a fresh Conda environment and the latest dependencies installed. ![image](https://user-images.githubusercontent.com/21954664/106739402-659dfb80-6619-11eb-84f1-e75abfa6167d.png). Top = new, trash results. Bottom = old=agando expected results. The old environment has:. ```. scanpy==1.6.1.dev110+gb4234d81 anndata==0.7.4 umap==0.4.6 numpy==1.19.0 scipy==1.5.1 pandas==1.1.5 scikit-learn==0.23.1 statsmodels==0.12.1 python-igraph==0.8.0 louvain==0.6.1 leidenalg==0.8.3. ```. The new environment has . ```. scanpy==1.6.1 anndata==0.7.5 umap==0.4.6 numpy==1.20.0 scipy==1.6.0 pandas==1.2.1 scikit-learn==0.24.1 statsmodels==0.12.1 python-igraph==0.8.3 louvain==0.7.0 leidenalg==0.8.3. ```. Full new conda environment:. ```. name: single_cell_analysis. channels:. - defaults. dependencies:. - _libgcc_mutex=0.1=main. - argon2-cffi=20.1.0=py37h7b6447c_1. - async_generator=1.10=py37h28b3542_0. - attrs=20.3.0=pyhd3eb1b0_0. - backcall=0.2.0=pyhd3eb1b0_0. - bleach=3.3.0=pyhd3eb1b0_0. - ca-certificates=2021.1.19=h06a4308_0. - certifi=2020.12.5=py37h06a4308_0. - cffi=1.14.4=py37h261ae71_0. - dbus=1.13.18=hb2f20db_0. - decorator=4.4.2=pyhd3eb1b0_0. - defusedxml=0.6.0=py_0. - entrypoints=0.3=py37_0. - expat=2.2.10=he",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1625
https://github.com/scverse/scanpy/issues/1625:1561,integrability,depend,dependencies,1561," I am looking to change that. # The issue. When calculating the `sc.tl.marker_gene_overlap` I get the expected and reasonable results on the agando environment, but completely rubbish results when running the same code with a fresh Conda environment and the latest dependencies installed. ![image](https://user-images.githubusercontent.com/21954664/106739402-659dfb80-6619-11eb-84f1-e75abfa6167d.png). Top = new, trash results. Bottom = old=agando expected results. The old environment has:. ```. scanpy==1.6.1.dev110+gb4234d81 anndata==0.7.4 umap==0.4.6 numpy==1.19.0 scipy==1.5.1 pandas==1.1.5 scikit-learn==0.23.1 statsmodels==0.12.1 python-igraph==0.8.0 louvain==0.6.1 leidenalg==0.8.3. ```. The new environment has . ```. scanpy==1.6.1 anndata==0.7.5 umap==0.4.6 numpy==1.20.0 scipy==1.6.0 pandas==1.2.1 scikit-learn==0.24.1 statsmodels==0.12.1 python-igraph==0.8.3 louvain==0.7.0 leidenalg==0.8.3. ```. Full new conda environment:. ```. name: single_cell_analysis. channels:. - defaults. dependencies:. - _libgcc_mutex=0.1=main. - argon2-cffi=20.1.0=py37h7b6447c_1. - async_generator=1.10=py37h28b3542_0. - attrs=20.3.0=pyhd3eb1b0_0. - backcall=0.2.0=pyhd3eb1b0_0. - bleach=3.3.0=pyhd3eb1b0_0. - ca-certificates=2021.1.19=h06a4308_0. - certifi=2020.12.5=py37h06a4308_0. - cffi=1.14.4=py37h261ae71_0. - dbus=1.13.18=hb2f20db_0. - decorator=4.4.2=pyhd3eb1b0_0. - defusedxml=0.6.0=py_0. - entrypoints=0.3=py37_0. - expat=2.2.10=he6710b0_2. - fontconfig=2.13.0=h9420a91_0. - freetype=2.10.4=h5ab3b9f_0. - glib=2.66.1=h92f7085_0. - gst-plugins-base=1.14.0=h8213a91_2. - gstreamer=1.14.0=h28cd5cc_2. - icu=58.2=he6710b0_3. - importlib_metadata=2.0.0=1. - ipykernel=5.3.4=py37h5ca1d4c_0. - ipython=7.20.0=py37hb070fc8_1. - ipython_genutils=0.2.0=pyhd3eb1b0_1. - ipywidgets=7.6.3=pyhd3eb1b0_1. - jedi=0.17.0=py37_0. - jinja2=2.11.3=pyhd3eb1b0_0. - jpeg=9b=h024ee3a_2. - jsonschema=3.2.0=py_2. - jupyter=1.0.0=py37_7. - jupyter_client=6.1.7=py_0. - jupyter_console=6.2.0=py_0. - jupyter_core=4.7.1=py37h",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1625
https://github.com/scverse/scanpy/issues/1625:4622,integrability,version,version,4622,"0.8=0. - ptyprocess=0.7.0=pyhd3eb1b0_2. - pycparser=2.20=py_2. - pygments=2.7.4=pyhd3eb1b0_0. - pyparsing=2.4.7=pyhd3eb1b0_0. - pyqt=5.9.2=py37h05f1152_2. - pyrsistent=0.17.3=py37h7b6447c_0. - python=3.7.9=h7579374_0. - python-dateutil=2.8.1=pyhd3eb1b0_0. - pyzmq=20.0.0=py37h2531618_1. - qt=5.9.7=h5867ecd_1. - qtconsole=4.7.7=py_0. - qtpy=1.9.0=py_0. - readline=8.1=h27cfd23_0. - send2trash=1.5.0=pyhd3eb1b0_1. - setuptools=52.0.0=py37h06a4308_0. - sip=4.19.8=py37hf484d3e_0. - six=1.15.0=py37h06a4308_0. - sqlite=3.33.0=h62c20be_0. - terminado=0.9.2=py37h06a4308_0. - testpath=0.4.4=pyhd3eb1b0_0. - tk=8.6.10=hbc83047_0. - tornado=6.1=py37h27cfd23_0. - traitlets=5.0.5=pyhd3eb1b0_0. - wcwidth=0.2.5=py_0. - webencodings=0.5.1=py37_1. - wheel=0.36.2=pyhd3eb1b0_0. - widgetsnbextension=3.5.1=py37_0. - xz=5.2.5=h7b6447c_0. - zeromq=4.3.3=he6710b0_3. - zipp=3.4.0=pyhd3eb1b0_0. - zlib=1.2.11=h7b6447c_3. - pip:. - anndata==0.7.5. - cached-property==1.5.2. - click==7.1.2. - cycler==0.10.0. - get-version==2.1. - h5py==3.1.0. - importlib-metadata==3.4.0. - joblib==1.0.0. - kiwisolver==1.3.1. - legacy-api-wrap==1.2. - leidenalg==0.8.3. - llvmlite==0.35.0. - loompy==3.0.6. - louvain==0.7.0. - matplotlib==3.3.4. - natsort==7.1.1. - networkx==2.5. - numba==0.52.0. - numexpr==2.7.2. - numpy==1.20.0. - numpy-groupies==0.9.13. - pandas==1.2.1. - patsy==0.5.1. - pillow==8.1.0. - python-igraph==0.8.3. - pytz==2021.1. - scanpy==1.6.1. - scikit-learn==0.24.1. - scipy==1.6.0. - scvelo==0.2.2. - seaborn==0.11.1. - setuptools-scm==5.0.1. - sinfo==0.3.1. - statsmodels==0.12.1. - stdlib-list==0.8.0. - tables==3.6.1. - texttable==1.6.3. - threadpoolctl==2.1.0. - tqdm==4.56.0. - typing-extensions==3.7.4.3. - umap-learn==0.4.6. ```. I can reproduce the issue with a Docker container that only has the minimal conda environment above. Additionally, I already tried installing the exact same dependency versions in the new environment, but got the same results. . If you need access to the data and the conta",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1625
https://github.com/scverse/scanpy/issues/1625:4727,integrability,api,api-wrap,4727,"=2.4.7=pyhd3eb1b0_0. - pyqt=5.9.2=py37h05f1152_2. - pyrsistent=0.17.3=py37h7b6447c_0. - python=3.7.9=h7579374_0. - python-dateutil=2.8.1=pyhd3eb1b0_0. - pyzmq=20.0.0=py37h2531618_1. - qt=5.9.7=h5867ecd_1. - qtconsole=4.7.7=py_0. - qtpy=1.9.0=py_0. - readline=8.1=h27cfd23_0. - send2trash=1.5.0=pyhd3eb1b0_1. - setuptools=52.0.0=py37h06a4308_0. - sip=4.19.8=py37hf484d3e_0. - six=1.15.0=py37h06a4308_0. - sqlite=3.33.0=h62c20be_0. - terminado=0.9.2=py37h06a4308_0. - testpath=0.4.4=pyhd3eb1b0_0. - tk=8.6.10=hbc83047_0. - tornado=6.1=py37h27cfd23_0. - traitlets=5.0.5=pyhd3eb1b0_0. - wcwidth=0.2.5=py_0. - webencodings=0.5.1=py37_1. - wheel=0.36.2=pyhd3eb1b0_0. - widgetsnbextension=3.5.1=py37_0. - xz=5.2.5=h7b6447c_0. - zeromq=4.3.3=he6710b0_3. - zipp=3.4.0=pyhd3eb1b0_0. - zlib=1.2.11=h7b6447c_3. - pip:. - anndata==0.7.5. - cached-property==1.5.2. - click==7.1.2. - cycler==0.10.0. - get-version==2.1. - h5py==3.1.0. - importlib-metadata==3.4.0. - joblib==1.0.0. - kiwisolver==1.3.1. - legacy-api-wrap==1.2. - leidenalg==0.8.3. - llvmlite==0.35.0. - loompy==3.0.6. - louvain==0.7.0. - matplotlib==3.3.4. - natsort==7.1.1. - networkx==2.5. - numba==0.52.0. - numexpr==2.7.2. - numpy==1.20.0. - numpy-groupies==0.9.13. - pandas==1.2.1. - patsy==0.5.1. - pillow==8.1.0. - python-igraph==0.8.3. - pytz==2021.1. - scanpy==1.6.1. - scikit-learn==0.24.1. - scipy==1.6.0. - scvelo==0.2.2. - seaborn==0.11.1. - setuptools-scm==5.0.1. - sinfo==0.3.1. - statsmodels==0.12.1. - stdlib-list==0.8.0. - tables==3.6.1. - texttable==1.6.3. - threadpoolctl==2.1.0. - tqdm==4.56.0. - typing-extensions==3.7.4.3. - umap-learn==0.4.6. ```. I can reproduce the issue with a Docker container that only has the minimal conda environment above. Additionally, I already tried installing the exact same dependency versions in the new environment, but got the same results. . If you need access to the data and the container please contact me and I will make it available to you. The data is already at the ICB cluster. Code:",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1625
https://github.com/scverse/scanpy/issues/1625:5510,integrability,depend,dependency,5510,"11=h7b6447c_3. - pip:. - anndata==0.7.5. - cached-property==1.5.2. - click==7.1.2. - cycler==0.10.0. - get-version==2.1. - h5py==3.1.0. - importlib-metadata==3.4.0. - joblib==1.0.0. - kiwisolver==1.3.1. - legacy-api-wrap==1.2. - leidenalg==0.8.3. - llvmlite==0.35.0. - loompy==3.0.6. - louvain==0.7.0. - matplotlib==3.3.4. - natsort==7.1.1. - networkx==2.5. - numba==0.52.0. - numexpr==2.7.2. - numpy==1.20.0. - numpy-groupies==0.9.13. - pandas==1.2.1. - patsy==0.5.1. - pillow==8.1.0. - python-igraph==0.8.3. - pytz==2021.1. - scanpy==1.6.1. - scikit-learn==0.24.1. - scipy==1.6.0. - scvelo==0.2.2. - seaborn==0.11.1. - setuptools-scm==5.0.1. - sinfo==0.3.1. - statsmodels==0.12.1. - stdlib-list==0.8.0. - tables==3.6.1. - texttable==1.6.3. - threadpoolctl==2.1.0. - tqdm==4.56.0. - typing-extensions==3.7.4.3. - umap-learn==0.4.6. ```. I can reproduce the issue with a Docker container that only has the minimal conda environment above. Additionally, I already tried installing the exact same dependency versions in the new environment, but got the same results. . If you need access to the data and the container please contact me and I will make it available to you. The data is already at the ICB cluster. Code:. ```. from os import path. import numpy as np. import matplotlib.pyplot as plt. import scanpy as sc. import scanpy.external as sce. from os import listdir. import pandas as pd. import seaborn as sb. import datetime, time. import scvelo as scv. from matplotlib.colors import LinearSegmentedColormap. #Define a nice colour map for gene expression. from matplotlib import colors. def timestamp():. ts = time.time(). st = datetime.datetime.fromtimestamp(ts).strftime('%d-%m-%Y %H:%M:%S'). return st. # Exporting folder. folder = ""/output"". sc.settings.figdir = folder + ""Plots/"". sc.set_figure_params(vector_friendly = True, dpi=300). sc.settings.verbosity = 3 # verbosity: errors (0), warnings (1), info (2), hints (3). sc.logging.print_version_and_date(). sc.logging.print_header(). ad",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1625
https://github.com/scverse/scanpy/issues/1625:5521,integrability,version,versions,5521,"c_3. - pip:. - anndata==0.7.5. - cached-property==1.5.2. - click==7.1.2. - cycler==0.10.0. - get-version==2.1. - h5py==3.1.0. - importlib-metadata==3.4.0. - joblib==1.0.0. - kiwisolver==1.3.1. - legacy-api-wrap==1.2. - leidenalg==0.8.3. - llvmlite==0.35.0. - loompy==3.0.6. - louvain==0.7.0. - matplotlib==3.3.4. - natsort==7.1.1. - networkx==2.5. - numba==0.52.0. - numexpr==2.7.2. - numpy==1.20.0. - numpy-groupies==0.9.13. - pandas==1.2.1. - patsy==0.5.1. - pillow==8.1.0. - python-igraph==0.8.3. - pytz==2021.1. - scanpy==1.6.1. - scikit-learn==0.24.1. - scipy==1.6.0. - scvelo==0.2.2. - seaborn==0.11.1. - setuptools-scm==5.0.1. - sinfo==0.3.1. - statsmodels==0.12.1. - stdlib-list==0.8.0. - tables==3.6.1. - texttable==1.6.3. - threadpoolctl==2.1.0. - tqdm==4.56.0. - typing-extensions==3.7.4.3. - umap-learn==0.4.6. ```. I can reproduce the issue with a Docker container that only has the minimal conda environment above. Additionally, I already tried installing the exact same dependency versions in the new environment, but got the same results. . If you need access to the data and the container please contact me and I will make it available to you. The data is already at the ICB cluster. Code:. ```. from os import path. import numpy as np. import matplotlib.pyplot as plt. import scanpy as sc. import scanpy.external as sce. from os import listdir. import pandas as pd. import seaborn as sb. import datetime, time. import scvelo as scv. from matplotlib.colors import LinearSegmentedColormap. #Define a nice colour map for gene expression. from matplotlib import colors. def timestamp():. ts = time.time(). st = datetime.datetime.fromtimestamp(ts).strftime('%d-%m-%Y %H:%M:%S'). return st. # Exporting folder. folder = ""/output"". sc.settings.figdir = folder + ""Plots/"". sc.set_figure_params(vector_friendly = True, dpi=300). sc.settings.verbosity = 3 # verbosity: errors (0), warnings (1), info (2), hints (3). sc.logging.print_version_and_date(). sc.logging.print_header(). adata = sc.r",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1625
https://github.com/scverse/scanpy/issues/1625:7877,integrability,sub,subplots,7877,"ms(vector_friendly = True, dpi=300). sc.settings.verbosity = 3 # verbosity: errors (0), warnings (1), info (2), hints (3). sc.logging.print_version_and_date(). sc.logging.print_header(). adata = sc.read(""/data/190924_Recreated_Virus_Object_regressed.h5ad""). #adata.write(folder + ""190924_Recreated_Virus_Object_regressed.h5ad""). sc.tl.louvain(adata, resolution = 4, key_added = ""louvain_2""). sc.tl.louvain(adata, resolution = 5, key_added = ""louvain_3""). sc.tl.louvain(adata, resolution = 6, key_added = ""louvain_4""). sc.tl.louvain(adata, resolution = 7, key_added = ""louvain_5""). sc.pl.umap(adata, color = [""louvain_2"", ""louvain_3"", ""louvain_4"", ""louvain_5""], wspace = 0.45). #select resolution. print(adata.obs[""louvain_5""].value_counts()). sc.tl.rank_genes_groups(adata, groupby = ""louvain_5""). # read all arkers table from known annotated data. marker_folder = ""/marker/"". marker_table = pd.read_csv(marker_folder + ""Particle_AllMarkers.txt"", sep = ""\t"", index_col = None). marker_table.head(2). ## Restrict to Foldchange and P value. marker_table = marker_table[(marker_table.logfoldchange > 2) & (marker_table.pval_adj < 0.05)].copy(). print(marker_table.shape). marker = dict(). for ct in marker_table[""cluster""].unique():. tmp = marker_table[marker_table[""cluster""] == ct]. marker[ct] = tmp.gene.values. ## Look at a predefined list of cell types and the corresponding marker genes in order to annotate our clusters. cell_annotation = sc.tl.marker_gene_overlap(adata, marker, key = 'rank_genes_groups', normalize = 'reference'). fig, ax = plt.subplots(figsize = (30, 10)). sb.heatmap(cell_annotation, cbar = False, annot = True). fig.savefig('/data/heatmap.png'). ```. Everything works smoothly and the results look fine until the marker_gene_overlap. A similar issue was observed by another ICB member who is trying to verify that their new environment still works. Any pointers strongly appreciated, since we consider this a severe issue blocking new analyses with the most recent packages.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1625
https://github.com/scverse/scanpy/issues/1625:95,interoperability,specif,specific,95,"gene ranking produces completely wrong results on the most recent tool/library combinations on specific dataset; - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. # Introduction. Hi,. so this is a weird one and I could not track it down yet. The Schillerlab people have a workstation known as ""agando"". On this workstation the full environment is installed globally and shared by all users. I am looking to change that. # The issue. When calculating the `sc.tl.marker_gene_overlap` I get the expected and reasonable results on the agando environment, but completely rubbish results when running the same code with a fresh Conda environment and the latest dependencies installed. ![image](https://user-images.githubusercontent.com/21954664/106739402-659dfb80-6619-11eb-84f1-e75abfa6167d.png). Top = new, trash results. Bottom = old=agando expected results. The old environment has:. ```. scanpy==1.6.1.dev110+gb4234d81 anndata==0.7.4 umap==0.4.6 numpy==1.19.0 scipy==1.5.1 pandas==1.1.5 scikit-learn==0.23.1 statsmodels==0.12.1 python-igraph==0.8.0 louvain==0.6.1 leidenalg==0.8.3. ```. The new environment has . ```. scanpy==1.6.1 anndata==0.7.5 umap==0.4.6 numpy==1.20.0 scipy==1.6.0 pandas==1.2.1 scikit-learn==0.24.1 statsmodels==0.12.1 python-igraph==0.8.3 louvain==0.7.0 leidenalg==0.8.3. ```. Full new conda environment:. ```. name: single_cell_analysis. channels:. - defaults. dependencies:. - _libgcc_mutex=0.1=main. - argon2-cffi=20.1.0=py37h7b6447c_1. - async_generator=1.10=py37h28b3542_0. - attrs=20.3.0=pyhd3eb1b0_0. - backcall=0.2.0=pyhd3eb1b0_0. - bleach=3.3.0=pyhd3eb1b0_0. - ca-certificates=2021.1.19=h06a4308_0. - certifi=2020.12.5=py37h06a4308_0. - cffi=1.14.4=py37h261ae71_0. - dbus=1.13.18=hb2f20db_0. - decorator=4.4.2=pyhd3eb1b0_0. - defusedxml=0.6.0=py_0. - entrypoints=0.3=py37_0. - expat=2.2.10=he",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1625
https://github.com/scverse/scanpy/issues/1625:547,interoperability,share,shared,547,"gene ranking produces completely wrong results on the most recent tool/library combinations on specific dataset; - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. # Introduction. Hi,. so this is a weird one and I could not track it down yet. The Schillerlab people have a workstation known as ""agando"". On this workstation the full environment is installed globally and shared by all users. I am looking to change that. # The issue. When calculating the `sc.tl.marker_gene_overlap` I get the expected and reasonable results on the agando environment, but completely rubbish results when running the same code with a fresh Conda environment and the latest dependencies installed. ![image](https://user-images.githubusercontent.com/21954664/106739402-659dfb80-6619-11eb-84f1-e75abfa6167d.png). Top = new, trash results. Bottom = old=agando expected results. The old environment has:. ```. scanpy==1.6.1.dev110+gb4234d81 anndata==0.7.4 umap==0.4.6 numpy==1.19.0 scipy==1.5.1 pandas==1.1.5 scikit-learn==0.23.1 statsmodels==0.12.1 python-igraph==0.8.0 louvain==0.6.1 leidenalg==0.8.3. ```. The new environment has . ```. scanpy==1.6.1 anndata==0.7.5 umap==0.4.6 numpy==1.20.0 scipy==1.6.0 pandas==1.2.1 scikit-learn==0.24.1 statsmodels==0.12.1 python-igraph==0.8.3 louvain==0.7.0 leidenalg==0.8.3. ```. Full new conda environment:. ```. name: single_cell_analysis. channels:. - defaults. dependencies:. - _libgcc_mutex=0.1=main. - argon2-cffi=20.1.0=py37h7b6447c_1. - async_generator=1.10=py37h28b3542_0. - attrs=20.3.0=pyhd3eb1b0_0. - backcall=0.2.0=pyhd3eb1b0_0. - bleach=3.3.0=pyhd3eb1b0_0. - ca-certificates=2021.1.19=h06a4308_0. - certifi=2020.12.5=py37h06a4308_0. - cffi=1.14.4=py37h261ae71_0. - dbus=1.13.18=hb2f20db_0. - decorator=4.4.2=pyhd3eb1b0_0. - defusedxml=0.6.0=py_0. - entrypoints=0.3=py37_0. - expat=2.2.10=he",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1625
https://github.com/scverse/scanpy/issues/1625:2104,interoperability,plug,plugins-base,2104,umap==0.4.6 numpy==1.19.0 scipy==1.5.1 pandas==1.1.5 scikit-learn==0.23.1 statsmodels==0.12.1 python-igraph==0.8.0 louvain==0.6.1 leidenalg==0.8.3. ```. The new environment has . ```. scanpy==1.6.1 anndata==0.7.5 umap==0.4.6 numpy==1.20.0 scipy==1.6.0 pandas==1.2.1 scikit-learn==0.24.1 statsmodels==0.12.1 python-igraph==0.8.3 louvain==0.7.0 leidenalg==0.8.3. ```. Full new conda environment:. ```. name: single_cell_analysis. channels:. - defaults. dependencies:. - _libgcc_mutex=0.1=main. - argon2-cffi=20.1.0=py37h7b6447c_1. - async_generator=1.10=py37h28b3542_0. - attrs=20.3.0=pyhd3eb1b0_0. - backcall=0.2.0=pyhd3eb1b0_0. - bleach=3.3.0=pyhd3eb1b0_0. - ca-certificates=2021.1.19=h06a4308_0. - certifi=2020.12.5=py37h06a4308_0. - cffi=1.14.4=py37h261ae71_0. - dbus=1.13.18=hb2f20db_0. - decorator=4.4.2=pyhd3eb1b0_0. - defusedxml=0.6.0=py_0. - entrypoints=0.3=py37_0. - expat=2.2.10=he6710b0_2. - fontconfig=2.13.0=h9420a91_0. - freetype=2.10.4=h5ab3b9f_0. - glib=2.66.1=h92f7085_0. - gst-plugins-base=1.14.0=h8213a91_2. - gstreamer=1.14.0=h28cd5cc_2. - icu=58.2=he6710b0_3. - importlib_metadata=2.0.0=1. - ipykernel=5.3.4=py37h5ca1d4c_0. - ipython=7.20.0=py37hb070fc8_1. - ipython_genutils=0.2.0=pyhd3eb1b0_1. - ipywidgets=7.6.3=pyhd3eb1b0_1. - jedi=0.17.0=py37_0. - jinja2=2.11.3=pyhd3eb1b0_0. - jpeg=9b=h024ee3a_2. - jsonschema=3.2.0=py_2. - jupyter=1.0.0=py37_7. - jupyter_client=6.1.7=py_0. - jupyter_console=6.2.0=py_0. - jupyter_core=4.7.1=py37h06a4308_0. - jupyterlab_pygments=0.1.2=py_0. - jupyterlab_widgets=1.0.0=pyhd3eb1b0_1. - ld_impl_linux-64=2.33.1=h53a641e_7. - libedit=3.1.20191231=h14c3975_1. - libffi=3.3=he6710b0_2. - libgcc-ng=9.1.0=hdf63c60_0. - libpng=1.6.37=hbc83047_0. - libsodium=1.0.18=h7b6447c_0. - libstdcxx-ng=9.1.0=hdf63c60_0. - libuuid=1.0.3=h1bed415_2. - libxcb=1.14=h7b6447c_0. - libxml2=2.9.10=hb55368b_3. - markupsafe=1.1.1=py37h14c3975_1. - mistune=0.8.4=py37h14c3975_1001. - nb_conda=2.2.1=py37_0. - nb_conda_kernels=2.3.1=py37h06a4308_0. - nbclient=0.5.1=p,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1625
https://github.com/scverse/scanpy/issues/1625:4727,interoperability,api,api-wrap,4727,"=2.4.7=pyhd3eb1b0_0. - pyqt=5.9.2=py37h05f1152_2. - pyrsistent=0.17.3=py37h7b6447c_0. - python=3.7.9=h7579374_0. - python-dateutil=2.8.1=pyhd3eb1b0_0. - pyzmq=20.0.0=py37h2531618_1. - qt=5.9.7=h5867ecd_1. - qtconsole=4.7.7=py_0. - qtpy=1.9.0=py_0. - readline=8.1=h27cfd23_0. - send2trash=1.5.0=pyhd3eb1b0_1. - setuptools=52.0.0=py37h06a4308_0. - sip=4.19.8=py37hf484d3e_0. - six=1.15.0=py37h06a4308_0. - sqlite=3.33.0=h62c20be_0. - terminado=0.9.2=py37h06a4308_0. - testpath=0.4.4=pyhd3eb1b0_0. - tk=8.6.10=hbc83047_0. - tornado=6.1=py37h27cfd23_0. - traitlets=5.0.5=pyhd3eb1b0_0. - wcwidth=0.2.5=py_0. - webencodings=0.5.1=py37_1. - wheel=0.36.2=pyhd3eb1b0_0. - widgetsnbextension=3.5.1=py37_0. - xz=5.2.5=h7b6447c_0. - zeromq=4.3.3=he6710b0_3. - zipp=3.4.0=pyhd3eb1b0_0. - zlib=1.2.11=h7b6447c_3. - pip:. - anndata==0.7.5. - cached-property==1.5.2. - click==7.1.2. - cycler==0.10.0. - get-version==2.1. - h5py==3.1.0. - importlib-metadata==3.4.0. - joblib==1.0.0. - kiwisolver==1.3.1. - legacy-api-wrap==1.2. - leidenalg==0.8.3. - llvmlite==0.35.0. - loompy==3.0.6. - louvain==0.7.0. - matplotlib==3.3.4. - natsort==7.1.1. - networkx==2.5. - numba==0.52.0. - numexpr==2.7.2. - numpy==1.20.0. - numpy-groupies==0.9.13. - pandas==1.2.1. - patsy==0.5.1. - pillow==8.1.0. - python-igraph==0.8.3. - pytz==2021.1. - scanpy==1.6.1. - scikit-learn==0.24.1. - scipy==1.6.0. - scvelo==0.2.2. - seaborn==0.11.1. - setuptools-scm==5.0.1. - sinfo==0.3.1. - statsmodels==0.12.1. - stdlib-list==0.8.0. - tables==3.6.1. - texttable==1.6.3. - threadpoolctl==2.1.0. - tqdm==4.56.0. - typing-extensions==3.7.4.3. - umap-learn==0.4.6. ```. I can reproduce the issue with a Docker container that only has the minimal conda environment above. Additionally, I already tried installing the exact same dependency versions in the new environment, but got the same results. . If you need access to the data and the container please contact me and I will make it available to you. The data is already at the ICB cluster. Code:",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1625
https://github.com/scverse/scanpy/issues/1625:234,modifiability,version,version,234,"gene ranking produces completely wrong results on the most recent tool/library combinations on specific dataset; - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. # Introduction. Hi,. so this is a weird one and I could not track it down yet. The Schillerlab people have a workstation known as ""agando"". On this workstation the full environment is installed globally and shared by all users. I am looking to change that. # The issue. When calculating the `sc.tl.marker_gene_overlap` I get the expected and reasonable results on the agando environment, but completely rubbish results when running the same code with a fresh Conda environment and the latest dependencies installed. ![image](https://user-images.githubusercontent.com/21954664/106739402-659dfb80-6619-11eb-84f1-e75abfa6167d.png). Top = new, trash results. Bottom = old=agando expected results. The old environment has:. ```. scanpy==1.6.1.dev110+gb4234d81 anndata==0.7.4 umap==0.4.6 numpy==1.19.0 scipy==1.5.1 pandas==1.1.5 scikit-learn==0.23.1 statsmodels==0.12.1 python-igraph==0.8.0 louvain==0.6.1 leidenalg==0.8.3. ```. The new environment has . ```. scanpy==1.6.1 anndata==0.7.5 umap==0.4.6 numpy==1.20.0 scipy==1.6.0 pandas==1.2.1 scikit-learn==0.24.1 statsmodels==0.12.1 python-igraph==0.8.3 louvain==0.7.0 leidenalg==0.8.3. ```. Full new conda environment:. ```. name: single_cell_analysis. channels:. - defaults. dependencies:. - _libgcc_mutex=0.1=main. - argon2-cffi=20.1.0=py37h7b6447c_1. - async_generator=1.10=py37h28b3542_0. - attrs=20.3.0=pyhd3eb1b0_0. - backcall=0.2.0=pyhd3eb1b0_0. - bleach=3.3.0=pyhd3eb1b0_0. - ca-certificates=2021.1.19=h06a4308_0. - certifi=2020.12.5=py37h06a4308_0. - cffi=1.14.4=py37h261ae71_0. - dbus=1.13.18=hb2f20db_0. - decorator=4.4.2=pyhd3eb1b0_0. - defusedxml=0.6.0=py_0. - entrypoints=0.3=py37_0. - expat=2.2.10=he",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1625
https://github.com/scverse/scanpy/issues/1625:832,modifiability,depend,dependencies,832,"gene ranking produces completely wrong results on the most recent tool/library combinations on specific dataset; - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. # Introduction. Hi,. so this is a weird one and I could not track it down yet. The Schillerlab people have a workstation known as ""agando"". On this workstation the full environment is installed globally and shared by all users. I am looking to change that. # The issue. When calculating the `sc.tl.marker_gene_overlap` I get the expected and reasonable results on the agando environment, but completely rubbish results when running the same code with a fresh Conda environment and the latest dependencies installed. ![image](https://user-images.githubusercontent.com/21954664/106739402-659dfb80-6619-11eb-84f1-e75abfa6167d.png). Top = new, trash results. Bottom = old=agando expected results. The old environment has:. ```. scanpy==1.6.1.dev110+gb4234d81 anndata==0.7.4 umap==0.4.6 numpy==1.19.0 scipy==1.5.1 pandas==1.1.5 scikit-learn==0.23.1 statsmodels==0.12.1 python-igraph==0.8.0 louvain==0.6.1 leidenalg==0.8.3. ```. The new environment has . ```. scanpy==1.6.1 anndata==0.7.5 umap==0.4.6 numpy==1.20.0 scipy==1.6.0 pandas==1.2.1 scikit-learn==0.24.1 statsmodels==0.12.1 python-igraph==0.8.3 louvain==0.7.0 leidenalg==0.8.3. ```. Full new conda environment:. ```. name: single_cell_analysis. channels:. - defaults. dependencies:. - _libgcc_mutex=0.1=main. - argon2-cffi=20.1.0=py37h7b6447c_1. - async_generator=1.10=py37h28b3542_0. - attrs=20.3.0=pyhd3eb1b0_0. - backcall=0.2.0=pyhd3eb1b0_0. - bleach=3.3.0=pyhd3eb1b0_0. - ca-certificates=2021.1.19=h06a4308_0. - certifi=2020.12.5=py37h06a4308_0. - cffi=1.14.4=py37h261ae71_0. - dbus=1.13.18=hb2f20db_0. - decorator=4.4.2=pyhd3eb1b0_0. - defusedxml=0.6.0=py_0. - entrypoints=0.3=py37_0. - expat=2.2.10=he",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1625
https://github.com/scverse/scanpy/issues/1625:1561,modifiability,depend,dependencies,1561," I am looking to change that. # The issue. When calculating the `sc.tl.marker_gene_overlap` I get the expected and reasonable results on the agando environment, but completely rubbish results when running the same code with a fresh Conda environment and the latest dependencies installed. ![image](https://user-images.githubusercontent.com/21954664/106739402-659dfb80-6619-11eb-84f1-e75abfa6167d.png). Top = new, trash results. Bottom = old=agando expected results. The old environment has:. ```. scanpy==1.6.1.dev110+gb4234d81 anndata==0.7.4 umap==0.4.6 numpy==1.19.0 scipy==1.5.1 pandas==1.1.5 scikit-learn==0.23.1 statsmodels==0.12.1 python-igraph==0.8.0 louvain==0.6.1 leidenalg==0.8.3. ```. The new environment has . ```. scanpy==1.6.1 anndata==0.7.5 umap==0.4.6 numpy==1.20.0 scipy==1.6.0 pandas==1.2.1 scikit-learn==0.24.1 statsmodels==0.12.1 python-igraph==0.8.3 louvain==0.7.0 leidenalg==0.8.3. ```. Full new conda environment:. ```. name: single_cell_analysis. channels:. - defaults. dependencies:. - _libgcc_mutex=0.1=main. - argon2-cffi=20.1.0=py37h7b6447c_1. - async_generator=1.10=py37h28b3542_0. - attrs=20.3.0=pyhd3eb1b0_0. - backcall=0.2.0=pyhd3eb1b0_0. - bleach=3.3.0=pyhd3eb1b0_0. - ca-certificates=2021.1.19=h06a4308_0. - certifi=2020.12.5=py37h06a4308_0. - cffi=1.14.4=py37h261ae71_0. - dbus=1.13.18=hb2f20db_0. - decorator=4.4.2=pyhd3eb1b0_0. - defusedxml=0.6.0=py_0. - entrypoints=0.3=py37_0. - expat=2.2.10=he6710b0_2. - fontconfig=2.13.0=h9420a91_0. - freetype=2.10.4=h5ab3b9f_0. - glib=2.66.1=h92f7085_0. - gst-plugins-base=1.14.0=h8213a91_2. - gstreamer=1.14.0=h28cd5cc_2. - icu=58.2=he6710b0_3. - importlib_metadata=2.0.0=1. - ipykernel=5.3.4=py37h5ca1d4c_0. - ipython=7.20.0=py37hb070fc8_1. - ipython_genutils=0.2.0=pyhd3eb1b0_1. - ipywidgets=7.6.3=pyhd3eb1b0_1. - jedi=0.17.0=py37_0. - jinja2=2.11.3=pyhd3eb1b0_0. - jpeg=9b=h024ee3a_2. - jsonschema=3.2.0=py_2. - jupyter=1.0.0=py37_7. - jupyter_client=6.1.7=py_0. - jupyter_console=6.2.0=py_0. - jupyter_core=4.7.1=py37h",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1625
https://github.com/scverse/scanpy/issues/1625:1902,modifiability,deco,decorator,1902,"21954664/106739402-659dfb80-6619-11eb-84f1-e75abfa6167d.png). Top = new, trash results. Bottom = old=agando expected results. The old environment has:. ```. scanpy==1.6.1.dev110+gb4234d81 anndata==0.7.4 umap==0.4.6 numpy==1.19.0 scipy==1.5.1 pandas==1.1.5 scikit-learn==0.23.1 statsmodels==0.12.1 python-igraph==0.8.0 louvain==0.6.1 leidenalg==0.8.3. ```. The new environment has . ```. scanpy==1.6.1 anndata==0.7.5 umap==0.4.6 numpy==1.20.0 scipy==1.6.0 pandas==1.2.1 scikit-learn==0.24.1 statsmodels==0.12.1 python-igraph==0.8.3 louvain==0.7.0 leidenalg==0.8.3. ```. Full new conda environment:. ```. name: single_cell_analysis. channels:. - defaults. dependencies:. - _libgcc_mutex=0.1=main. - argon2-cffi=20.1.0=py37h7b6447c_1. - async_generator=1.10=py37h28b3542_0. - attrs=20.3.0=pyhd3eb1b0_0. - backcall=0.2.0=pyhd3eb1b0_0. - bleach=3.3.0=pyhd3eb1b0_0. - ca-certificates=2021.1.19=h06a4308_0. - certifi=2020.12.5=py37h06a4308_0. - cffi=1.14.4=py37h261ae71_0. - dbus=1.13.18=hb2f20db_0. - decorator=4.4.2=pyhd3eb1b0_0. - defusedxml=0.6.0=py_0. - entrypoints=0.3=py37_0. - expat=2.2.10=he6710b0_2. - fontconfig=2.13.0=h9420a91_0. - freetype=2.10.4=h5ab3b9f_0. - glib=2.66.1=h92f7085_0. - gst-plugins-base=1.14.0=h8213a91_2. - gstreamer=1.14.0=h28cd5cc_2. - icu=58.2=he6710b0_3. - importlib_metadata=2.0.0=1. - ipykernel=5.3.4=py37h5ca1d4c_0. - ipython=7.20.0=py37hb070fc8_1. - ipython_genutils=0.2.0=pyhd3eb1b0_1. - ipywidgets=7.6.3=pyhd3eb1b0_1. - jedi=0.17.0=py37_0. - jinja2=2.11.3=pyhd3eb1b0_0. - jpeg=9b=h024ee3a_2. - jsonschema=3.2.0=py_2. - jupyter=1.0.0=py37_7. - jupyter_client=6.1.7=py_0. - jupyter_console=6.2.0=py_0. - jupyter_core=4.7.1=py37h06a4308_0. - jupyterlab_pygments=0.1.2=py_0. - jupyterlab_widgets=1.0.0=pyhd3eb1b0_1. - ld_impl_linux-64=2.33.1=h53a641e_7. - libedit=3.1.20191231=h14c3975_1. - libffi=3.3=he6710b0_2. - libgcc-ng=9.1.0=hdf63c60_0. - libpng=1.6.37=hbc83047_0. - libsodium=1.0.18=h7b6447c_0. - libstdcxx-ng=9.1.0=hdf63c60_0. - libuuid=1.0.3=h1bed415_2. - libx",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1625
https://github.com/scverse/scanpy/issues/1625:3297,modifiability,pac,packaging,3297,ils=0.2.0=pyhd3eb1b0_1. - ipywidgets=7.6.3=pyhd3eb1b0_1. - jedi=0.17.0=py37_0. - jinja2=2.11.3=pyhd3eb1b0_0. - jpeg=9b=h024ee3a_2. - jsonschema=3.2.0=py_2. - jupyter=1.0.0=py37_7. - jupyter_client=6.1.7=py_0. - jupyter_console=6.2.0=py_0. - jupyter_core=4.7.1=py37h06a4308_0. - jupyterlab_pygments=0.1.2=py_0. - jupyterlab_widgets=1.0.0=pyhd3eb1b0_1. - ld_impl_linux-64=2.33.1=h53a641e_7. - libedit=3.1.20191231=h14c3975_1. - libffi=3.3=he6710b0_2. - libgcc-ng=9.1.0=hdf63c60_0. - libpng=1.6.37=hbc83047_0. - libsodium=1.0.18=h7b6447c_0. - libstdcxx-ng=9.1.0=hdf63c60_0. - libuuid=1.0.3=h1bed415_2. - libxcb=1.14=h7b6447c_0. - libxml2=2.9.10=hb55368b_3. - markupsafe=1.1.1=py37h14c3975_1. - mistune=0.8.4=py37h14c3975_1001. - nb_conda=2.2.1=py37_0. - nb_conda_kernels=2.3.1=py37h06a4308_0. - nbclient=0.5.1=py_0. - nbconvert=6.0.7=py37_0. - nbformat=5.1.2=pyhd3eb1b0_1. - ncurses=6.2=he6710b0_1. - nest-asyncio=1.4.3=pyhd3eb1b0_0. - notebook=6.2.0=py37h06a4308_0. - openssl=1.1.1i=h27cfd23_0. - packaging=20.9=pyhd3eb1b0_0. - pandoc=2.11=hb0f4dca_0. - pandocfilters=1.4.3=py37h06a4308_1. - parso=0.8.1=pyhd3eb1b0_0. - pcre=8.44=he6710b0_0. - pexpect=4.8.0=pyhd3eb1b0_3. - pickleshare=0.7.5=pyhd3eb1b0_1003. - pip=20.3.3=py37h06a4308_0. - prometheus_client=0.9.0=pyhd3eb1b0_0. - prompt-toolkit=3.0.8=py_0. - prompt_toolkit=3.0.8=0. - ptyprocess=0.7.0=pyhd3eb1b0_2. - pycparser=2.20=py_2. - pygments=2.7.4=pyhd3eb1b0_0. - pyparsing=2.4.7=pyhd3eb1b0_0. - pyqt=5.9.2=py37h05f1152_2. - pyrsistent=0.17.3=py37h7b6447c_0. - python=3.7.9=h7579374_0. - python-dateutil=2.8.1=pyhd3eb1b0_0. - pyzmq=20.0.0=py37h2531618_1. - qt=5.9.7=h5867ecd_1. - qtconsole=4.7.7=py_0. - qtpy=1.9.0=py_0. - readline=8.1=h27cfd23_0. - send2trash=1.5.0=pyhd3eb1b0_1. - setuptools=52.0.0=py37h06a4308_0. - sip=4.19.8=py37hf484d3e_0. - six=1.15.0=py37h06a4308_0. - sqlite=3.33.0=h62c20be_0. - terminado=0.9.2=py37h06a4308_0. - testpath=0.4.4=pyhd3eb1b0_0. - tk=8.6.10=hbc83047_0. - tornado=6.1=py37h27cfd23_0. - traitlets=5.0.5=pyhd,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1625
https://github.com/scverse/scanpy/issues/1625:4622,modifiability,version,version,4622,"0.8=0. - ptyprocess=0.7.0=pyhd3eb1b0_2. - pycparser=2.20=py_2. - pygments=2.7.4=pyhd3eb1b0_0. - pyparsing=2.4.7=pyhd3eb1b0_0. - pyqt=5.9.2=py37h05f1152_2. - pyrsistent=0.17.3=py37h7b6447c_0. - python=3.7.9=h7579374_0. - python-dateutil=2.8.1=pyhd3eb1b0_0. - pyzmq=20.0.0=py37h2531618_1. - qt=5.9.7=h5867ecd_1. - qtconsole=4.7.7=py_0. - qtpy=1.9.0=py_0. - readline=8.1=h27cfd23_0. - send2trash=1.5.0=pyhd3eb1b0_1. - setuptools=52.0.0=py37h06a4308_0. - sip=4.19.8=py37hf484d3e_0. - six=1.15.0=py37h06a4308_0. - sqlite=3.33.0=h62c20be_0. - terminado=0.9.2=py37h06a4308_0. - testpath=0.4.4=pyhd3eb1b0_0. - tk=8.6.10=hbc83047_0. - tornado=6.1=py37h27cfd23_0. - traitlets=5.0.5=pyhd3eb1b0_0. - wcwidth=0.2.5=py_0. - webencodings=0.5.1=py37_1. - wheel=0.36.2=pyhd3eb1b0_0. - widgetsnbextension=3.5.1=py37_0. - xz=5.2.5=h7b6447c_0. - zeromq=4.3.3=he6710b0_3. - zipp=3.4.0=pyhd3eb1b0_0. - zlib=1.2.11=h7b6447c_3. - pip:. - anndata==0.7.5. - cached-property==1.5.2. - click==7.1.2. - cycler==0.10.0. - get-version==2.1. - h5py==3.1.0. - importlib-metadata==3.4.0. - joblib==1.0.0. - kiwisolver==1.3.1. - legacy-api-wrap==1.2. - leidenalg==0.8.3. - llvmlite==0.35.0. - loompy==3.0.6. - louvain==0.7.0. - matplotlib==3.3.4. - natsort==7.1.1. - networkx==2.5. - numba==0.52.0. - numexpr==2.7.2. - numpy==1.20.0. - numpy-groupies==0.9.13. - pandas==1.2.1. - patsy==0.5.1. - pillow==8.1.0. - python-igraph==0.8.3. - pytz==2021.1. - scanpy==1.6.1. - scikit-learn==0.24.1. - scipy==1.6.0. - scvelo==0.2.2. - seaborn==0.11.1. - setuptools-scm==5.0.1. - sinfo==0.3.1. - statsmodels==0.12.1. - stdlib-list==0.8.0. - tables==3.6.1. - texttable==1.6.3. - threadpoolctl==2.1.0. - tqdm==4.56.0. - typing-extensions==3.7.4.3. - umap-learn==0.4.6. ```. I can reproduce the issue with a Docker container that only has the minimal conda environment above. Additionally, I already tried installing the exact same dependency versions in the new environment, but got the same results. . If you need access to the data and the conta",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1625
https://github.com/scverse/scanpy/issues/1625:5306,modifiability,extens,extensions,5306," - wcwidth=0.2.5=py_0. - webencodings=0.5.1=py37_1. - wheel=0.36.2=pyhd3eb1b0_0. - widgetsnbextension=3.5.1=py37_0. - xz=5.2.5=h7b6447c_0. - zeromq=4.3.3=he6710b0_3. - zipp=3.4.0=pyhd3eb1b0_0. - zlib=1.2.11=h7b6447c_3. - pip:. - anndata==0.7.5. - cached-property==1.5.2. - click==7.1.2. - cycler==0.10.0. - get-version==2.1. - h5py==3.1.0. - importlib-metadata==3.4.0. - joblib==1.0.0. - kiwisolver==1.3.1. - legacy-api-wrap==1.2. - leidenalg==0.8.3. - llvmlite==0.35.0. - loompy==3.0.6. - louvain==0.7.0. - matplotlib==3.3.4. - natsort==7.1.1. - networkx==2.5. - numba==0.52.0. - numexpr==2.7.2. - numpy==1.20.0. - numpy-groupies==0.9.13. - pandas==1.2.1. - patsy==0.5.1. - pillow==8.1.0. - python-igraph==0.8.3. - pytz==2021.1. - scanpy==1.6.1. - scikit-learn==0.24.1. - scipy==1.6.0. - scvelo==0.2.2. - seaborn==0.11.1. - setuptools-scm==5.0.1. - sinfo==0.3.1. - statsmodels==0.12.1. - stdlib-list==0.8.0. - tables==3.6.1. - texttable==1.6.3. - threadpoolctl==2.1.0. - tqdm==4.56.0. - typing-extensions==3.7.4.3. - umap-learn==0.4.6. ```. I can reproduce the issue with a Docker container that only has the minimal conda environment above. Additionally, I already tried installing the exact same dependency versions in the new environment, but got the same results. . If you need access to the data and the container please contact me and I will make it available to you. The data is already at the ICB cluster. Code:. ```. from os import path. import numpy as np. import matplotlib.pyplot as plt. import scanpy as sc. import scanpy.external as sce. from os import listdir. import pandas as pd. import seaborn as sb. import datetime, time. import scvelo as scv. from matplotlib.colors import LinearSegmentedColormap. #Define a nice colour map for gene expression. from matplotlib import colors. def timestamp():. ts = time.time(). st = datetime.datetime.fromtimestamp(ts).strftime('%d-%m-%Y %H:%M:%S'). return st. # Exporting folder. folder = ""/output"". sc.settings.figdir = folder + ""Plots/"". sc.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1625
https://github.com/scverse/scanpy/issues/1625:5510,modifiability,depend,dependency,5510,"11=h7b6447c_3. - pip:. - anndata==0.7.5. - cached-property==1.5.2. - click==7.1.2. - cycler==0.10.0. - get-version==2.1. - h5py==3.1.0. - importlib-metadata==3.4.0. - joblib==1.0.0. - kiwisolver==1.3.1. - legacy-api-wrap==1.2. - leidenalg==0.8.3. - llvmlite==0.35.0. - loompy==3.0.6. - louvain==0.7.0. - matplotlib==3.3.4. - natsort==7.1.1. - networkx==2.5. - numba==0.52.0. - numexpr==2.7.2. - numpy==1.20.0. - numpy-groupies==0.9.13. - pandas==1.2.1. - patsy==0.5.1. - pillow==8.1.0. - python-igraph==0.8.3. - pytz==2021.1. - scanpy==1.6.1. - scikit-learn==0.24.1. - scipy==1.6.0. - scvelo==0.2.2. - seaborn==0.11.1. - setuptools-scm==5.0.1. - sinfo==0.3.1. - statsmodels==0.12.1. - stdlib-list==0.8.0. - tables==3.6.1. - texttable==1.6.3. - threadpoolctl==2.1.0. - tqdm==4.56.0. - typing-extensions==3.7.4.3. - umap-learn==0.4.6. ```. I can reproduce the issue with a Docker container that only has the minimal conda environment above. Additionally, I already tried installing the exact same dependency versions in the new environment, but got the same results. . If you need access to the data and the container please contact me and I will make it available to you. The data is already at the ICB cluster. Code:. ```. from os import path. import numpy as np. import matplotlib.pyplot as plt. import scanpy as sc. import scanpy.external as sce. from os import listdir. import pandas as pd. import seaborn as sb. import datetime, time. import scvelo as scv. from matplotlib.colors import LinearSegmentedColormap. #Define a nice colour map for gene expression. from matplotlib import colors. def timestamp():. ts = time.time(). st = datetime.datetime.fromtimestamp(ts).strftime('%d-%m-%Y %H:%M:%S'). return st. # Exporting folder. folder = ""/output"". sc.settings.figdir = folder + ""Plots/"". sc.set_figure_params(vector_friendly = True, dpi=300). sc.settings.verbosity = 3 # verbosity: errors (0), warnings (1), info (2), hints (3). sc.logging.print_version_and_date(). sc.logging.print_header(). ad",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1625
https://github.com/scverse/scanpy/issues/1625:5521,modifiability,version,versions,5521,"c_3. - pip:. - anndata==0.7.5. - cached-property==1.5.2. - click==7.1.2. - cycler==0.10.0. - get-version==2.1. - h5py==3.1.0. - importlib-metadata==3.4.0. - joblib==1.0.0. - kiwisolver==1.3.1. - legacy-api-wrap==1.2. - leidenalg==0.8.3. - llvmlite==0.35.0. - loompy==3.0.6. - louvain==0.7.0. - matplotlib==3.3.4. - natsort==7.1.1. - networkx==2.5. - numba==0.52.0. - numexpr==2.7.2. - numpy==1.20.0. - numpy-groupies==0.9.13. - pandas==1.2.1. - patsy==0.5.1. - pillow==8.1.0. - python-igraph==0.8.3. - pytz==2021.1. - scanpy==1.6.1. - scikit-learn==0.24.1. - scipy==1.6.0. - scvelo==0.2.2. - seaborn==0.11.1. - setuptools-scm==5.0.1. - sinfo==0.3.1. - statsmodels==0.12.1. - stdlib-list==0.8.0. - tables==3.6.1. - texttable==1.6.3. - threadpoolctl==2.1.0. - tqdm==4.56.0. - typing-extensions==3.7.4.3. - umap-learn==0.4.6. ```. I can reproduce the issue with a Docker container that only has the minimal conda environment above. Additionally, I already tried installing the exact same dependency versions in the new environment, but got the same results. . If you need access to the data and the container please contact me and I will make it available to you. The data is already at the ICB cluster. Code:. ```. from os import path. import numpy as np. import matplotlib.pyplot as plt. import scanpy as sc. import scanpy.external as sce. from os import listdir. import pandas as pd. import seaborn as sb. import datetime, time. import scvelo as scv. from matplotlib.colors import LinearSegmentedColormap. #Define a nice colour map for gene expression. from matplotlib import colors. def timestamp():. ts = time.time(). st = datetime.datetime.fromtimestamp(ts).strftime('%d-%m-%Y %H:%M:%S'). return st. # Exporting folder. folder = ""/output"". sc.settings.figdir = folder + ""Plots/"". sc.set_figure_params(vector_friendly = True, dpi=300). sc.settings.verbosity = 3 # verbosity: errors (0), warnings (1), info (2), hints (3). sc.logging.print_version_and_date(). sc.logging.print_header(). adata = sc.r",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1625
https://github.com/scverse/scanpy/issues/1625:8317,modifiability,pac,packages,8317,"ms(vector_friendly = True, dpi=300). sc.settings.verbosity = 3 # verbosity: errors (0), warnings (1), info (2), hints (3). sc.logging.print_version_and_date(). sc.logging.print_header(). adata = sc.read(""/data/190924_Recreated_Virus_Object_regressed.h5ad""). #adata.write(folder + ""190924_Recreated_Virus_Object_regressed.h5ad""). sc.tl.louvain(adata, resolution = 4, key_added = ""louvain_2""). sc.tl.louvain(adata, resolution = 5, key_added = ""louvain_3""). sc.tl.louvain(adata, resolution = 6, key_added = ""louvain_4""). sc.tl.louvain(adata, resolution = 7, key_added = ""louvain_5""). sc.pl.umap(adata, color = [""louvain_2"", ""louvain_3"", ""louvain_4"", ""louvain_5""], wspace = 0.45). #select resolution. print(adata.obs[""louvain_5""].value_counts()). sc.tl.rank_genes_groups(adata, groupby = ""louvain_5""). # read all arkers table from known annotated data. marker_folder = ""/marker/"". marker_table = pd.read_csv(marker_folder + ""Particle_AllMarkers.txt"", sep = ""\t"", index_col = None). marker_table.head(2). ## Restrict to Foldchange and P value. marker_table = marker_table[(marker_table.logfoldchange > 2) & (marker_table.pval_adj < 0.05)].copy(). print(marker_table.shape). marker = dict(). for ct in marker_table[""cluster""].unique():. tmp = marker_table[marker_table[""cluster""] == ct]. marker[ct] = tmp.gene.values. ## Look at a predefined list of cell types and the corresponding marker genes in order to annotate our clusters. cell_annotation = sc.tl.marker_gene_overlap(adata, marker, key = 'rank_genes_groups', normalize = 'reference'). fig, ax = plt.subplots(figsize = (30, 10)). sb.heatmap(cell_annotation, cbar = False, annot = True). fig.savefig('/data/heatmap.png'). ```. Everything works smoothly and the results look fine until the marker_gene_overlap. A similar issue was observed by another ICB member who is trying to verify that their new environment still works. Any pointers strongly appreciated, since we consider this a severe issue blocking new analyses with the most recent packages.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1625
https://github.com/scverse/scanpy/issues/1625:4558,performance,cach,cached-property,4558,"hd3eb1b0_0. - prompt-toolkit=3.0.8=py_0. - prompt_toolkit=3.0.8=0. - ptyprocess=0.7.0=pyhd3eb1b0_2. - pycparser=2.20=py_2. - pygments=2.7.4=pyhd3eb1b0_0. - pyparsing=2.4.7=pyhd3eb1b0_0. - pyqt=5.9.2=py37h05f1152_2. - pyrsistent=0.17.3=py37h7b6447c_0. - python=3.7.9=h7579374_0. - python-dateutil=2.8.1=pyhd3eb1b0_0. - pyzmq=20.0.0=py37h2531618_1. - qt=5.9.7=h5867ecd_1. - qtconsole=4.7.7=py_0. - qtpy=1.9.0=py_0. - readline=8.1=h27cfd23_0. - send2trash=1.5.0=pyhd3eb1b0_1. - setuptools=52.0.0=py37h06a4308_0. - sip=4.19.8=py37hf484d3e_0. - six=1.15.0=py37h06a4308_0. - sqlite=3.33.0=h62c20be_0. - terminado=0.9.2=py37h06a4308_0. - testpath=0.4.4=pyhd3eb1b0_0. - tk=8.6.10=hbc83047_0. - tornado=6.1=py37h27cfd23_0. - traitlets=5.0.5=pyhd3eb1b0_0. - wcwidth=0.2.5=py_0. - webencodings=0.5.1=py37_1. - wheel=0.36.2=pyhd3eb1b0_0. - widgetsnbextension=3.5.1=py37_0. - xz=5.2.5=h7b6447c_0. - zeromq=4.3.3=he6710b0_3. - zipp=3.4.0=pyhd3eb1b0_0. - zlib=1.2.11=h7b6447c_3. - pip:. - anndata==0.7.5. - cached-property==1.5.2. - click==7.1.2. - cycler==0.10.0. - get-version==2.1. - h5py==3.1.0. - importlib-metadata==3.4.0. - joblib==1.0.0. - kiwisolver==1.3.1. - legacy-api-wrap==1.2. - leidenalg==0.8.3. - llvmlite==0.35.0. - loompy==3.0.6. - louvain==0.7.0. - matplotlib==3.3.4. - natsort==7.1.1. - networkx==2.5. - numba==0.52.0. - numexpr==2.7.2. - numpy==1.20.0. - numpy-groupies==0.9.13. - pandas==1.2.1. - patsy==0.5.1. - pillow==8.1.0. - python-igraph==0.8.3. - pytz==2021.1. - scanpy==1.6.1. - scikit-learn==0.24.1. - scipy==1.6.0. - scvelo==0.2.2. - seaborn==0.11.1. - setuptools-scm==5.0.1. - sinfo==0.3.1. - statsmodels==0.12.1. - stdlib-list==0.8.0. - tables==3.6.1. - texttable==1.6.3. - threadpoolctl==2.1.0. - tqdm==4.56.0. - typing-extensions==3.7.4.3. - umap-learn==0.4.6. ```. I can reproduce the issue with a Docker container that only has the minimal conda environment above. Additionally, I already tried installing the exact same dependency versions in the new environment, but got the ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1625
https://github.com/scverse/scanpy/issues/1625:4858,performance,network,networkx,4858,"2.8.1=pyhd3eb1b0_0. - pyzmq=20.0.0=py37h2531618_1. - qt=5.9.7=h5867ecd_1. - qtconsole=4.7.7=py_0. - qtpy=1.9.0=py_0. - readline=8.1=h27cfd23_0. - send2trash=1.5.0=pyhd3eb1b0_1. - setuptools=52.0.0=py37h06a4308_0. - sip=4.19.8=py37hf484d3e_0. - six=1.15.0=py37h06a4308_0. - sqlite=3.33.0=h62c20be_0. - terminado=0.9.2=py37h06a4308_0. - testpath=0.4.4=pyhd3eb1b0_0. - tk=8.6.10=hbc83047_0. - tornado=6.1=py37h27cfd23_0. - traitlets=5.0.5=pyhd3eb1b0_0. - wcwidth=0.2.5=py_0. - webencodings=0.5.1=py37_1. - wheel=0.36.2=pyhd3eb1b0_0. - widgetsnbextension=3.5.1=py37_0. - xz=5.2.5=h7b6447c_0. - zeromq=4.3.3=he6710b0_3. - zipp=3.4.0=pyhd3eb1b0_0. - zlib=1.2.11=h7b6447c_3. - pip:. - anndata==0.7.5. - cached-property==1.5.2. - click==7.1.2. - cycler==0.10.0. - get-version==2.1. - h5py==3.1.0. - importlib-metadata==3.4.0. - joblib==1.0.0. - kiwisolver==1.3.1. - legacy-api-wrap==1.2. - leidenalg==0.8.3. - llvmlite==0.35.0. - loompy==3.0.6. - louvain==0.7.0. - matplotlib==3.3.4. - natsort==7.1.1. - networkx==2.5. - numba==0.52.0. - numexpr==2.7.2. - numpy==1.20.0. - numpy-groupies==0.9.13. - pandas==1.2.1. - patsy==0.5.1. - pillow==8.1.0. - python-igraph==0.8.3. - pytz==2021.1. - scanpy==1.6.1. - scikit-learn==0.24.1. - scipy==1.6.0. - scvelo==0.2.2. - seaborn==0.11.1. - setuptools-scm==5.0.1. - sinfo==0.3.1. - statsmodels==0.12.1. - stdlib-list==0.8.0. - tables==3.6.1. - texttable==1.6.3. - threadpoolctl==2.1.0. - tqdm==4.56.0. - typing-extensions==3.7.4.3. - umap-learn==0.4.6. ```. I can reproduce the issue with a Docker container that only has the minimal conda environment above. Additionally, I already tried installing the exact same dependency versions in the new environment, but got the same results. . If you need access to the data and the container please contact me and I will make it available to you. The data is already at the ICB cluster. Code:. ```. from os import path. import numpy as np. import matplotlib.pyplot as plt. import scanpy as sc. import scanpy.external as sce",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1625
https://github.com/scverse/scanpy/issues/1625:5948,performance,time,time,5948," - pandas==1.2.1. - patsy==0.5.1. - pillow==8.1.0. - python-igraph==0.8.3. - pytz==2021.1. - scanpy==1.6.1. - scikit-learn==0.24.1. - scipy==1.6.0. - scvelo==0.2.2. - seaborn==0.11.1. - setuptools-scm==5.0.1. - sinfo==0.3.1. - statsmodels==0.12.1. - stdlib-list==0.8.0. - tables==3.6.1. - texttable==1.6.3. - threadpoolctl==2.1.0. - tqdm==4.56.0. - typing-extensions==3.7.4.3. - umap-learn==0.4.6. ```. I can reproduce the issue with a Docker container that only has the minimal conda environment above. Additionally, I already tried installing the exact same dependency versions in the new environment, but got the same results. . If you need access to the data and the container please contact me and I will make it available to you. The data is already at the ICB cluster. Code:. ```. from os import path. import numpy as np. import matplotlib.pyplot as plt. import scanpy as sc. import scanpy.external as sce. from os import listdir. import pandas as pd. import seaborn as sb. import datetime, time. import scvelo as scv. from matplotlib.colors import LinearSegmentedColormap. #Define a nice colour map for gene expression. from matplotlib import colors. def timestamp():. ts = time.time(). st = datetime.datetime.fromtimestamp(ts).strftime('%d-%m-%Y %H:%M:%S'). return st. # Exporting folder. folder = ""/output"". sc.settings.figdir = folder + ""Plots/"". sc.set_figure_params(vector_friendly = True, dpi=300). sc.settings.verbosity = 3 # verbosity: errors (0), warnings (1), info (2), hints (3). sc.logging.print_version_and_date(). sc.logging.print_header(). adata = sc.read(""/data/190924_Recreated_Virus_Object_regressed.h5ad""). #adata.write(folder + ""190924_Recreated_Virus_Object_regressed.h5ad""). sc.tl.louvain(adata, resolution = 4, key_added = ""louvain_2""). sc.tl.louvain(adata, resolution = 5, key_added = ""louvain_3""). sc.tl.louvain(adata, resolution = 6, key_added = ""louvain_4""). sc.tl.louvain(adata, resolution = 7, key_added = ""louvain_5""). sc.pl.umap(adata, color = [""louvain_2"", ""lo",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1625
https://github.com/scverse/scanpy/issues/1625:6113,performance,time,timestamp,6113,"eaborn==0.11.1. - setuptools-scm==5.0.1. - sinfo==0.3.1. - statsmodels==0.12.1. - stdlib-list==0.8.0. - tables==3.6.1. - texttable==1.6.3. - threadpoolctl==2.1.0. - tqdm==4.56.0. - typing-extensions==3.7.4.3. - umap-learn==0.4.6. ```. I can reproduce the issue with a Docker container that only has the minimal conda environment above. Additionally, I already tried installing the exact same dependency versions in the new environment, but got the same results. . If you need access to the data and the container please contact me and I will make it available to you. The data is already at the ICB cluster. Code:. ```. from os import path. import numpy as np. import matplotlib.pyplot as plt. import scanpy as sc. import scanpy.external as sce. from os import listdir. import pandas as pd. import seaborn as sb. import datetime, time. import scvelo as scv. from matplotlib.colors import LinearSegmentedColormap. #Define a nice colour map for gene expression. from matplotlib import colors. def timestamp():. ts = time.time(). st = datetime.datetime.fromtimestamp(ts).strftime('%d-%m-%Y %H:%M:%S'). return st. # Exporting folder. folder = ""/output"". sc.settings.figdir = folder + ""Plots/"". sc.set_figure_params(vector_friendly = True, dpi=300). sc.settings.verbosity = 3 # verbosity: errors (0), warnings (1), info (2), hints (3). sc.logging.print_version_and_date(). sc.logging.print_header(). adata = sc.read(""/data/190924_Recreated_Virus_Object_regressed.h5ad""). #adata.write(folder + ""190924_Recreated_Virus_Object_regressed.h5ad""). sc.tl.louvain(adata, resolution = 4, key_added = ""louvain_2""). sc.tl.louvain(adata, resolution = 5, key_added = ""louvain_3""). sc.tl.louvain(adata, resolution = 6, key_added = ""louvain_4""). sc.tl.louvain(adata, resolution = 7, key_added = ""louvain_5""). sc.pl.umap(adata, color = [""louvain_2"", ""louvain_3"", ""louvain_4"", ""louvain_5""], wspace = 0.45). #select resolution. print(adata.obs[""louvain_5""].value_counts()). sc.tl.rank_genes_groups(adata, groupby = ""louvain",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1625
https://github.com/scverse/scanpy/issues/1625:6132,performance,time,time,6132,"- setuptools-scm==5.0.1. - sinfo==0.3.1. - statsmodels==0.12.1. - stdlib-list==0.8.0. - tables==3.6.1. - texttable==1.6.3. - threadpoolctl==2.1.0. - tqdm==4.56.0. - typing-extensions==3.7.4.3. - umap-learn==0.4.6. ```. I can reproduce the issue with a Docker container that only has the minimal conda environment above. Additionally, I already tried installing the exact same dependency versions in the new environment, but got the same results. . If you need access to the data and the container please contact me and I will make it available to you. The data is already at the ICB cluster. Code:. ```. from os import path. import numpy as np. import matplotlib.pyplot as plt. import scanpy as sc. import scanpy.external as sce. from os import listdir. import pandas as pd. import seaborn as sb. import datetime, time. import scvelo as scv. from matplotlib.colors import LinearSegmentedColormap. #Define a nice colour map for gene expression. from matplotlib import colors. def timestamp():. ts = time.time(). st = datetime.datetime.fromtimestamp(ts).strftime('%d-%m-%Y %H:%M:%S'). return st. # Exporting folder. folder = ""/output"". sc.settings.figdir = folder + ""Plots/"". sc.set_figure_params(vector_friendly = True, dpi=300). sc.settings.verbosity = 3 # verbosity: errors (0), warnings (1), info (2), hints (3). sc.logging.print_version_and_date(). sc.logging.print_header(). adata = sc.read(""/data/190924_Recreated_Virus_Object_regressed.h5ad""). #adata.write(folder + ""190924_Recreated_Virus_Object_regressed.h5ad""). sc.tl.louvain(adata, resolution = 4, key_added = ""louvain_2""). sc.tl.louvain(adata, resolution = 5, key_added = ""louvain_3""). sc.tl.louvain(adata, resolution = 6, key_added = ""louvain_4""). sc.tl.louvain(adata, resolution = 7, key_added = ""louvain_5""). sc.pl.umap(adata, color = [""louvain_2"", ""louvain_3"", ""louvain_4"", ""louvain_5""], wspace = 0.45). #select resolution. print(adata.obs[""louvain_5""].value_counts()). sc.tl.rank_genes_groups(adata, groupby = ""louvain_5""). # read all",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1625
https://github.com/scverse/scanpy/issues/1625:6137,performance,time,time,6137,"uptools-scm==5.0.1. - sinfo==0.3.1. - statsmodels==0.12.1. - stdlib-list==0.8.0. - tables==3.6.1. - texttable==1.6.3. - threadpoolctl==2.1.0. - tqdm==4.56.0. - typing-extensions==3.7.4.3. - umap-learn==0.4.6. ```. I can reproduce the issue with a Docker container that only has the minimal conda environment above. Additionally, I already tried installing the exact same dependency versions in the new environment, but got the same results. . If you need access to the data and the container please contact me and I will make it available to you. The data is already at the ICB cluster. Code:. ```. from os import path. import numpy as np. import matplotlib.pyplot as plt. import scanpy as sc. import scanpy.external as sce. from os import listdir. import pandas as pd. import seaborn as sb. import datetime, time. import scvelo as scv. from matplotlib.colors import LinearSegmentedColormap. #Define a nice colour map for gene expression. from matplotlib import colors. def timestamp():. ts = time.time(). st = datetime.datetime.fromtimestamp(ts).strftime('%d-%m-%Y %H:%M:%S'). return st. # Exporting folder. folder = ""/output"". sc.settings.figdir = folder + ""Plots/"". sc.set_figure_params(vector_friendly = True, dpi=300). sc.settings.verbosity = 3 # verbosity: errors (0), warnings (1), info (2), hints (3). sc.logging.print_version_and_date(). sc.logging.print_header(). adata = sc.read(""/data/190924_Recreated_Virus_Object_regressed.h5ad""). #adata.write(folder + ""190924_Recreated_Virus_Object_regressed.h5ad""). sc.tl.louvain(adata, resolution = 4, key_added = ""louvain_2""). sc.tl.louvain(adata, resolution = 5, key_added = ""louvain_3""). sc.tl.louvain(adata, resolution = 6, key_added = ""louvain_4""). sc.tl.louvain(adata, resolution = 7, key_added = ""louvain_5""). sc.pl.umap(adata, color = [""louvain_2"", ""louvain_3"", ""louvain_4"", ""louvain_5""], wspace = 0.45). #select resolution. print(adata.obs[""louvain_5""].value_counts()). sc.tl.rank_genes_groups(adata, groupby = ""louvain_5""). # read all arke",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1625
https://github.com/scverse/scanpy/issues/1625:6402,performance,error,errors,6402,"at only has the minimal conda environment above. Additionally, I already tried installing the exact same dependency versions in the new environment, but got the same results. . If you need access to the data and the container please contact me and I will make it available to you. The data is already at the ICB cluster. Code:. ```. from os import path. import numpy as np. import matplotlib.pyplot as plt. import scanpy as sc. import scanpy.external as sce. from os import listdir. import pandas as pd. import seaborn as sb. import datetime, time. import scvelo as scv. from matplotlib.colors import LinearSegmentedColormap. #Define a nice colour map for gene expression. from matplotlib import colors. def timestamp():. ts = time.time(). st = datetime.datetime.fromtimestamp(ts).strftime('%d-%m-%Y %H:%M:%S'). return st. # Exporting folder. folder = ""/output"". sc.settings.figdir = folder + ""Plots/"". sc.set_figure_params(vector_friendly = True, dpi=300). sc.settings.verbosity = 3 # verbosity: errors (0), warnings (1), info (2), hints (3). sc.logging.print_version_and_date(). sc.logging.print_header(). adata = sc.read(""/data/190924_Recreated_Virus_Object_regressed.h5ad""). #adata.write(folder + ""190924_Recreated_Virus_Object_regressed.h5ad""). sc.tl.louvain(adata, resolution = 4, key_added = ""louvain_2""). sc.tl.louvain(adata, resolution = 5, key_added = ""louvain_3""). sc.tl.louvain(adata, resolution = 6, key_added = ""louvain_4""). sc.tl.louvain(adata, resolution = 7, key_added = ""louvain_5""). sc.pl.umap(adata, color = [""louvain_2"", ""louvain_3"", ""louvain_4"", ""louvain_5""], wspace = 0.45). #select resolution. print(adata.obs[""louvain_5""].value_counts()). sc.tl.rank_genes_groups(adata, groupby = ""louvain_5""). # read all arkers table from known annotated data. marker_folder = ""/marker/"". marker_table = pd.read_csv(marker_folder + ""Particle_AllMarkers.txt"", sep = ""\t"", index_col = None). marker_table.head(2). ## Restrict to Foldchange and P value. marker_table = marker_table[(marker_tabl",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1625
https://github.com/scverse/scanpy/issues/1625:5668,reliability,availab,available,5668,"3.4.0. - joblib==1.0.0. - kiwisolver==1.3.1. - legacy-api-wrap==1.2. - leidenalg==0.8.3. - llvmlite==0.35.0. - loompy==3.0.6. - louvain==0.7.0. - matplotlib==3.3.4. - natsort==7.1.1. - networkx==2.5. - numba==0.52.0. - numexpr==2.7.2. - numpy==1.20.0. - numpy-groupies==0.9.13. - pandas==1.2.1. - patsy==0.5.1. - pillow==8.1.0. - python-igraph==0.8.3. - pytz==2021.1. - scanpy==1.6.1. - scikit-learn==0.24.1. - scipy==1.6.0. - scvelo==0.2.2. - seaborn==0.11.1. - setuptools-scm==5.0.1. - sinfo==0.3.1. - statsmodels==0.12.1. - stdlib-list==0.8.0. - tables==3.6.1. - texttable==1.6.3. - threadpoolctl==2.1.0. - tqdm==4.56.0. - typing-extensions==3.7.4.3. - umap-learn==0.4.6. ```. I can reproduce the issue with a Docker container that only has the minimal conda environment above. Additionally, I already tried installing the exact same dependency versions in the new environment, but got the same results. . If you need access to the data and the container please contact me and I will make it available to you. The data is already at the ICB cluster. Code:. ```. from os import path. import numpy as np. import matplotlib.pyplot as plt. import scanpy as sc. import scanpy.external as sce. from os import listdir. import pandas as pd. import seaborn as sb. import datetime, time. import scvelo as scv. from matplotlib.colors import LinearSegmentedColormap. #Define a nice colour map for gene expression. from matplotlib import colors. def timestamp():. ts = time.time(). st = datetime.datetime.fromtimestamp(ts).strftime('%d-%m-%Y %H:%M:%S'). return st. # Exporting folder. folder = ""/output"". sc.settings.figdir = folder + ""Plots/"". sc.set_figure_params(vector_friendly = True, dpi=300). sc.settings.verbosity = 3 # verbosity: errors (0), warnings (1), info (2), hints (3). sc.logging.print_version_and_date(). sc.logging.print_header(). adata = sc.read(""/data/190924_Recreated_Virus_Object_regressed.h5ad""). #adata.write(folder + ""190924_Recreated_Virus_Object_regressed.h5ad""). sc.tl.louvain(adat",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1625
https://github.com/scverse/scanpy/issues/1625:22,safety,compl,completely,22,"gene ranking produces completely wrong results on the most recent tool/library combinations on specific dataset; - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. # Introduction. Hi,. so this is a weird one and I could not track it down yet. The Schillerlab people have a workstation known as ""agando"". On this workstation the full environment is installed globally and shared by all users. I am looking to change that. # The issue. When calculating the `sc.tl.marker_gene_overlap` I get the expected and reasonable results on the agando environment, but completely rubbish results when running the same code with a fresh Conda environment and the latest dependencies installed. ![image](https://user-images.githubusercontent.com/21954664/106739402-659dfb80-6619-11eb-84f1-e75abfa6167d.png). Top = new, trash results. Bottom = old=agando expected results. The old environment has:. ```. scanpy==1.6.1.dev110+gb4234d81 anndata==0.7.4 umap==0.4.6 numpy==1.19.0 scipy==1.5.1 pandas==1.1.5 scikit-learn==0.23.1 statsmodels==0.12.1 python-igraph==0.8.0 louvain==0.6.1 leidenalg==0.8.3. ```. The new environment has . ```. scanpy==1.6.1 anndata==0.7.5 umap==0.4.6 numpy==1.20.0 scipy==1.6.0 pandas==1.2.1 scikit-learn==0.24.1 statsmodels==0.12.1 python-igraph==0.8.3 louvain==0.7.0 leidenalg==0.8.3. ```. Full new conda environment:. ```. name: single_cell_analysis. channels:. - defaults. dependencies:. - _libgcc_mutex=0.1=main. - argon2-cffi=20.1.0=py37h7b6447c_1. - async_generator=1.10=py37h28b3542_0. - attrs=20.3.0=pyhd3eb1b0_0. - backcall=0.2.0=pyhd3eb1b0_0. - bleach=3.3.0=pyhd3eb1b0_0. - ca-certificates=2021.1.19=h06a4308_0. - certifi=2020.12.5=py37h06a4308_0. - cffi=1.14.4=py37h261ae71_0. - dbus=1.13.18=hb2f20db_0. - decorator=4.4.2=pyhd3eb1b0_0. - defusedxml=0.6.0=py_0. - entrypoints=0.3=py37_0. - expat=2.2.10=he",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1625
https://github.com/scverse/scanpy/issues/1625:732,safety,compl,completely,732,"gene ranking produces completely wrong results on the most recent tool/library combinations on specific dataset; - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. # Introduction. Hi,. so this is a weird one and I could not track it down yet. The Schillerlab people have a workstation known as ""agando"". On this workstation the full environment is installed globally and shared by all users. I am looking to change that. # The issue. When calculating the `sc.tl.marker_gene_overlap` I get the expected and reasonable results on the agando environment, but completely rubbish results when running the same code with a fresh Conda environment and the latest dependencies installed. ![image](https://user-images.githubusercontent.com/21954664/106739402-659dfb80-6619-11eb-84f1-e75abfa6167d.png). Top = new, trash results. Bottom = old=agando expected results. The old environment has:. ```. scanpy==1.6.1.dev110+gb4234d81 anndata==0.7.4 umap==0.4.6 numpy==1.19.0 scipy==1.5.1 pandas==1.1.5 scikit-learn==0.23.1 statsmodels==0.12.1 python-igraph==0.8.0 louvain==0.6.1 leidenalg==0.8.3. ```. The new environment has . ```. scanpy==1.6.1 anndata==0.7.5 umap==0.4.6 numpy==1.20.0 scipy==1.6.0 pandas==1.2.1 scikit-learn==0.24.1 statsmodels==0.12.1 python-igraph==0.8.3 louvain==0.7.0 leidenalg==0.8.3. ```. Full new conda environment:. ```. name: single_cell_analysis. channels:. - defaults. dependencies:. - _libgcc_mutex=0.1=main. - argon2-cffi=20.1.0=py37h7b6447c_1. - async_generator=1.10=py37h28b3542_0. - attrs=20.3.0=pyhd3eb1b0_0. - backcall=0.2.0=pyhd3eb1b0_0. - bleach=3.3.0=pyhd3eb1b0_0. - ca-certificates=2021.1.19=h06a4308_0. - certifi=2020.12.5=py37h06a4308_0. - cffi=1.14.4=py37h261ae71_0. - dbus=1.13.18=hb2f20db_0. - decorator=4.4.2=pyhd3eb1b0_0. - defusedxml=0.6.0=py_0. - entrypoints=0.3=py37_0. - expat=2.2.10=he",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1625
https://github.com/scverse/scanpy/issues/1625:832,safety,depend,dependencies,832,"gene ranking produces completely wrong results on the most recent tool/library combinations on specific dataset; - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. # Introduction. Hi,. so this is a weird one and I could not track it down yet. The Schillerlab people have a workstation known as ""agando"". On this workstation the full environment is installed globally and shared by all users. I am looking to change that. # The issue. When calculating the `sc.tl.marker_gene_overlap` I get the expected and reasonable results on the agando environment, but completely rubbish results when running the same code with a fresh Conda environment and the latest dependencies installed. ![image](https://user-images.githubusercontent.com/21954664/106739402-659dfb80-6619-11eb-84f1-e75abfa6167d.png). Top = new, trash results. Bottom = old=agando expected results. The old environment has:. ```. scanpy==1.6.1.dev110+gb4234d81 anndata==0.7.4 umap==0.4.6 numpy==1.19.0 scipy==1.5.1 pandas==1.1.5 scikit-learn==0.23.1 statsmodels==0.12.1 python-igraph==0.8.0 louvain==0.6.1 leidenalg==0.8.3. ```. The new environment has . ```. scanpy==1.6.1 anndata==0.7.5 umap==0.4.6 numpy==1.20.0 scipy==1.6.0 pandas==1.2.1 scikit-learn==0.24.1 statsmodels==0.12.1 python-igraph==0.8.3 louvain==0.7.0 leidenalg==0.8.3. ```. Full new conda environment:. ```. name: single_cell_analysis. channels:. - defaults. dependencies:. - _libgcc_mutex=0.1=main. - argon2-cffi=20.1.0=py37h7b6447c_1. - async_generator=1.10=py37h28b3542_0. - attrs=20.3.0=pyhd3eb1b0_0. - backcall=0.2.0=pyhd3eb1b0_0. - bleach=3.3.0=pyhd3eb1b0_0. - ca-certificates=2021.1.19=h06a4308_0. - certifi=2020.12.5=py37h06a4308_0. - cffi=1.14.4=py37h261ae71_0. - dbus=1.13.18=hb2f20db_0. - decorator=4.4.2=pyhd3eb1b0_0. - defusedxml=0.6.0=py_0. - entrypoints=0.3=py37_0. - expat=2.2.10=he",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1625
https://github.com/scverse/scanpy/issues/1625:1561,safety,depend,dependencies,1561," I am looking to change that. # The issue. When calculating the `sc.tl.marker_gene_overlap` I get the expected and reasonable results on the agando environment, but completely rubbish results when running the same code with a fresh Conda environment and the latest dependencies installed. ![image](https://user-images.githubusercontent.com/21954664/106739402-659dfb80-6619-11eb-84f1-e75abfa6167d.png). Top = new, trash results. Bottom = old=agando expected results. The old environment has:. ```. scanpy==1.6.1.dev110+gb4234d81 anndata==0.7.4 umap==0.4.6 numpy==1.19.0 scipy==1.5.1 pandas==1.1.5 scikit-learn==0.23.1 statsmodels==0.12.1 python-igraph==0.8.0 louvain==0.6.1 leidenalg==0.8.3. ```. The new environment has . ```. scanpy==1.6.1 anndata==0.7.5 umap==0.4.6 numpy==1.20.0 scipy==1.6.0 pandas==1.2.1 scikit-learn==0.24.1 statsmodels==0.12.1 python-igraph==0.8.3 louvain==0.7.0 leidenalg==0.8.3. ```. Full new conda environment:. ```. name: single_cell_analysis. channels:. - defaults. dependencies:. - _libgcc_mutex=0.1=main. - argon2-cffi=20.1.0=py37h7b6447c_1. - async_generator=1.10=py37h28b3542_0. - attrs=20.3.0=pyhd3eb1b0_0. - backcall=0.2.0=pyhd3eb1b0_0. - bleach=3.3.0=pyhd3eb1b0_0. - ca-certificates=2021.1.19=h06a4308_0. - certifi=2020.12.5=py37h06a4308_0. - cffi=1.14.4=py37h261ae71_0. - dbus=1.13.18=hb2f20db_0. - decorator=4.4.2=pyhd3eb1b0_0. - defusedxml=0.6.0=py_0. - entrypoints=0.3=py37_0. - expat=2.2.10=he6710b0_2. - fontconfig=2.13.0=h9420a91_0. - freetype=2.10.4=h5ab3b9f_0. - glib=2.66.1=h92f7085_0. - gst-plugins-base=1.14.0=h8213a91_2. - gstreamer=1.14.0=h28cd5cc_2. - icu=58.2=he6710b0_3. - importlib_metadata=2.0.0=1. - ipykernel=5.3.4=py37h5ca1d4c_0. - ipython=7.20.0=py37hb070fc8_1. - ipython_genutils=0.2.0=pyhd3eb1b0_1. - ipywidgets=7.6.3=pyhd3eb1b0_1. - jedi=0.17.0=py37_0. - jinja2=2.11.3=pyhd3eb1b0_0. - jpeg=9b=h024ee3a_2. - jsonschema=3.2.0=py_2. - jupyter=1.0.0=py37_7. - jupyter_client=6.1.7=py_0. - jupyter_console=6.2.0=py_0. - jupyter_core=4.7.1=py37h",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1625
https://github.com/scverse/scanpy/issues/1625:4197,safety,test,testpath,4197,est-asyncio=1.4.3=pyhd3eb1b0_0. - notebook=6.2.0=py37h06a4308_0. - openssl=1.1.1i=h27cfd23_0. - packaging=20.9=pyhd3eb1b0_0. - pandoc=2.11=hb0f4dca_0. - pandocfilters=1.4.3=py37h06a4308_1. - parso=0.8.1=pyhd3eb1b0_0. - pcre=8.44=he6710b0_0. - pexpect=4.8.0=pyhd3eb1b0_3. - pickleshare=0.7.5=pyhd3eb1b0_1003. - pip=20.3.3=py37h06a4308_0. - prometheus_client=0.9.0=pyhd3eb1b0_0. - prompt-toolkit=3.0.8=py_0. - prompt_toolkit=3.0.8=0. - ptyprocess=0.7.0=pyhd3eb1b0_2. - pycparser=2.20=py_2. - pygments=2.7.4=pyhd3eb1b0_0. - pyparsing=2.4.7=pyhd3eb1b0_0. - pyqt=5.9.2=py37h05f1152_2. - pyrsistent=0.17.3=py37h7b6447c_0. - python=3.7.9=h7579374_0. - python-dateutil=2.8.1=pyhd3eb1b0_0. - pyzmq=20.0.0=py37h2531618_1. - qt=5.9.7=h5867ecd_1. - qtconsole=4.7.7=py_0. - qtpy=1.9.0=py_0. - readline=8.1=h27cfd23_0. - send2trash=1.5.0=pyhd3eb1b0_1. - setuptools=52.0.0=py37h06a4308_0. - sip=4.19.8=py37hf484d3e_0. - six=1.15.0=py37h06a4308_0. - sqlite=3.33.0=h62c20be_0. - terminado=0.9.2=py37h06a4308_0. - testpath=0.4.4=pyhd3eb1b0_0. - tk=8.6.10=hbc83047_0. - tornado=6.1=py37h27cfd23_0. - traitlets=5.0.5=pyhd3eb1b0_0. - wcwidth=0.2.5=py_0. - webencodings=0.5.1=py37_1. - wheel=0.36.2=pyhd3eb1b0_0. - widgetsnbextension=3.5.1=py37_0. - xz=5.2.5=h7b6447c_0. - zeromq=4.3.3=he6710b0_3. - zipp=3.4.0=pyhd3eb1b0_0. - zlib=1.2.11=h7b6447c_3. - pip:. - anndata==0.7.5. - cached-property==1.5.2. - click==7.1.2. - cycler==0.10.0. - get-version==2.1. - h5py==3.1.0. - importlib-metadata==3.4.0. - joblib==1.0.0. - kiwisolver==1.3.1. - legacy-api-wrap==1.2. - leidenalg==0.8.3. - llvmlite==0.35.0. - loompy==3.0.6. - louvain==0.7.0. - matplotlib==3.3.4. - natsort==7.1.1. - networkx==2.5. - numba==0.52.0. - numexpr==2.7.2. - numpy==1.20.0. - numpy-groupies==0.9.13. - pandas==1.2.1. - patsy==0.5.1. - pillow==8.1.0. - python-igraph==0.8.3. - pytz==2021.1. - scanpy==1.6.1. - scikit-learn==0.24.1. - scipy==1.6.0. - scvelo==0.2.2. - seaborn==0.11.1. - setuptools-scm==5.0.1. - sinfo==0.3.1. - statsmodels==0.12.1. - s,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1625
https://github.com/scverse/scanpy/issues/1625:5510,safety,depend,dependency,5510,"11=h7b6447c_3. - pip:. - anndata==0.7.5. - cached-property==1.5.2. - click==7.1.2. - cycler==0.10.0. - get-version==2.1. - h5py==3.1.0. - importlib-metadata==3.4.0. - joblib==1.0.0. - kiwisolver==1.3.1. - legacy-api-wrap==1.2. - leidenalg==0.8.3. - llvmlite==0.35.0. - loompy==3.0.6. - louvain==0.7.0. - matplotlib==3.3.4. - natsort==7.1.1. - networkx==2.5. - numba==0.52.0. - numexpr==2.7.2. - numpy==1.20.0. - numpy-groupies==0.9.13. - pandas==1.2.1. - patsy==0.5.1. - pillow==8.1.0. - python-igraph==0.8.3. - pytz==2021.1. - scanpy==1.6.1. - scikit-learn==0.24.1. - scipy==1.6.0. - scvelo==0.2.2. - seaborn==0.11.1. - setuptools-scm==5.0.1. - sinfo==0.3.1. - statsmodels==0.12.1. - stdlib-list==0.8.0. - tables==3.6.1. - texttable==1.6.3. - threadpoolctl==2.1.0. - tqdm==4.56.0. - typing-extensions==3.7.4.3. - umap-learn==0.4.6. ```. I can reproduce the issue with a Docker container that only has the minimal conda environment above. Additionally, I already tried installing the exact same dependency versions in the new environment, but got the same results. . If you need access to the data and the container please contact me and I will make it available to you. The data is already at the ICB cluster. Code:. ```. from os import path. import numpy as np. import matplotlib.pyplot as plt. import scanpy as sc. import scanpy.external as sce. from os import listdir. import pandas as pd. import seaborn as sb. import datetime, time. import scvelo as scv. from matplotlib.colors import LinearSegmentedColormap. #Define a nice colour map for gene expression. from matplotlib import colors. def timestamp():. ts = time.time(). st = datetime.datetime.fromtimestamp(ts).strftime('%d-%m-%Y %H:%M:%S'). return st. # Exporting folder. folder = ""/output"". sc.settings.figdir = folder + ""Plots/"". sc.set_figure_params(vector_friendly = True, dpi=300). sc.settings.verbosity = 3 # verbosity: errors (0), warnings (1), info (2), hints (3). sc.logging.print_version_and_date(). sc.logging.print_header(). ad",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1625
https://github.com/scverse/scanpy/issues/1625:5668,safety,avail,available,5668,"3.4.0. - joblib==1.0.0. - kiwisolver==1.3.1. - legacy-api-wrap==1.2. - leidenalg==0.8.3. - llvmlite==0.35.0. - loompy==3.0.6. - louvain==0.7.0. - matplotlib==3.3.4. - natsort==7.1.1. - networkx==2.5. - numba==0.52.0. - numexpr==2.7.2. - numpy==1.20.0. - numpy-groupies==0.9.13. - pandas==1.2.1. - patsy==0.5.1. - pillow==8.1.0. - python-igraph==0.8.3. - pytz==2021.1. - scanpy==1.6.1. - scikit-learn==0.24.1. - scipy==1.6.0. - scvelo==0.2.2. - seaborn==0.11.1. - setuptools-scm==5.0.1. - sinfo==0.3.1. - statsmodels==0.12.1. - stdlib-list==0.8.0. - tables==3.6.1. - texttable==1.6.3. - threadpoolctl==2.1.0. - tqdm==4.56.0. - typing-extensions==3.7.4.3. - umap-learn==0.4.6. ```. I can reproduce the issue with a Docker container that only has the minimal conda environment above. Additionally, I already tried installing the exact same dependency versions in the new environment, but got the same results. . If you need access to the data and the container please contact me and I will make it available to you. The data is already at the ICB cluster. Code:. ```. from os import path. import numpy as np. import matplotlib.pyplot as plt. import scanpy as sc. import scanpy.external as sce. from os import listdir. import pandas as pd. import seaborn as sb. import datetime, time. import scvelo as scv. from matplotlib.colors import LinearSegmentedColormap. #Define a nice colour map for gene expression. from matplotlib import colors. def timestamp():. ts = time.time(). st = datetime.datetime.fromtimestamp(ts).strftime('%d-%m-%Y %H:%M:%S'). return st. # Exporting folder. folder = ""/output"". sc.settings.figdir = folder + ""Plots/"". sc.set_figure_params(vector_friendly = True, dpi=300). sc.settings.verbosity = 3 # verbosity: errors (0), warnings (1), info (2), hints (3). sc.logging.print_version_and_date(). sc.logging.print_header(). adata = sc.read(""/data/190924_Recreated_Virus_Object_regressed.h5ad""). #adata.write(folder + ""190924_Recreated_Virus_Object_regressed.h5ad""). sc.tl.louvain(adat",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1625
https://github.com/scverse/scanpy/issues/1625:6402,safety,error,errors,6402,"at only has the minimal conda environment above. Additionally, I already tried installing the exact same dependency versions in the new environment, but got the same results. . If you need access to the data and the container please contact me and I will make it available to you. The data is already at the ICB cluster. Code:. ```. from os import path. import numpy as np. import matplotlib.pyplot as plt. import scanpy as sc. import scanpy.external as sce. from os import listdir. import pandas as pd. import seaborn as sb. import datetime, time. import scvelo as scv. from matplotlib.colors import LinearSegmentedColormap. #Define a nice colour map for gene expression. from matplotlib import colors. def timestamp():. ts = time.time(). st = datetime.datetime.fromtimestamp(ts).strftime('%d-%m-%Y %H:%M:%S'). return st. # Exporting folder. folder = ""/output"". sc.settings.figdir = folder + ""Plots/"". sc.set_figure_params(vector_friendly = True, dpi=300). sc.settings.verbosity = 3 # verbosity: errors (0), warnings (1), info (2), hints (3). sc.logging.print_version_and_date(). sc.logging.print_header(). adata = sc.read(""/data/190924_Recreated_Virus_Object_regressed.h5ad""). #adata.write(folder + ""190924_Recreated_Virus_Object_regressed.h5ad""). sc.tl.louvain(adata, resolution = 4, key_added = ""louvain_2""). sc.tl.louvain(adata, resolution = 5, key_added = ""louvain_3""). sc.tl.louvain(adata, resolution = 6, key_added = ""louvain_4""). sc.tl.louvain(adata, resolution = 7, key_added = ""louvain_5""). sc.pl.umap(adata, color = [""louvain_2"", ""louvain_3"", ""louvain_4"", ""louvain_5""], wspace = 0.45). #select resolution. print(adata.obs[""louvain_5""].value_counts()). sc.tl.rank_genes_groups(adata, groupby = ""louvain_5""). # read all arkers table from known annotated data. marker_folder = ""/marker/"". marker_table = pd.read_csv(marker_folder + ""Particle_AllMarkers.txt"", sep = ""\t"", index_col = None). marker_table.head(2). ## Restrict to Foldchange and P value. marker_table = marker_table[(marker_tabl",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1625
https://github.com/scverse/scanpy/issues/1625:6452,safety,log,logging,6452,"ditionally, I already tried installing the exact same dependency versions in the new environment, but got the same results. . If you need access to the data and the container please contact me and I will make it available to you. The data is already at the ICB cluster. Code:. ```. from os import path. import numpy as np. import matplotlib.pyplot as plt. import scanpy as sc. import scanpy.external as sce. from os import listdir. import pandas as pd. import seaborn as sb. import datetime, time. import scvelo as scv. from matplotlib.colors import LinearSegmentedColormap. #Define a nice colour map for gene expression. from matplotlib import colors. def timestamp():. ts = time.time(). st = datetime.datetime.fromtimestamp(ts).strftime('%d-%m-%Y %H:%M:%S'). return st. # Exporting folder. folder = ""/output"". sc.settings.figdir = folder + ""Plots/"". sc.set_figure_params(vector_friendly = True, dpi=300). sc.settings.verbosity = 3 # verbosity: errors (0), warnings (1), info (2), hints (3). sc.logging.print_version_and_date(). sc.logging.print_header(). adata = sc.read(""/data/190924_Recreated_Virus_Object_regressed.h5ad""). #adata.write(folder + ""190924_Recreated_Virus_Object_regressed.h5ad""). sc.tl.louvain(adata, resolution = 4, key_added = ""louvain_2""). sc.tl.louvain(adata, resolution = 5, key_added = ""louvain_3""). sc.tl.louvain(adata, resolution = 6, key_added = ""louvain_4""). sc.tl.louvain(adata, resolution = 7, key_added = ""louvain_5""). sc.pl.umap(adata, color = [""louvain_2"", ""louvain_3"", ""louvain_4"", ""louvain_5""], wspace = 0.45). #select resolution. print(adata.obs[""louvain_5""].value_counts()). sc.tl.rank_genes_groups(adata, groupby = ""louvain_5""). # read all arkers table from known annotated data. marker_folder = ""/marker/"". marker_table = pd.read_csv(marker_folder + ""Particle_AllMarkers.txt"", sep = ""\t"", index_col = None). marker_table.head(2). ## Restrict to Foldchange and P value. marker_table = marker_table[(marker_table.logfoldchange > 2) & (marker_table.pval_adj < 0.0",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1625
https://github.com/scverse/scanpy/issues/1625:6489,safety,log,logging,6489,"g the exact same dependency versions in the new environment, but got the same results. . If you need access to the data and the container please contact me and I will make it available to you. The data is already at the ICB cluster. Code:. ```. from os import path. import numpy as np. import matplotlib.pyplot as plt. import scanpy as sc. import scanpy.external as sce. from os import listdir. import pandas as pd. import seaborn as sb. import datetime, time. import scvelo as scv. from matplotlib.colors import LinearSegmentedColormap. #Define a nice colour map for gene expression. from matplotlib import colors. def timestamp():. ts = time.time(). st = datetime.datetime.fromtimestamp(ts).strftime('%d-%m-%Y %H:%M:%S'). return st. # Exporting folder. folder = ""/output"". sc.settings.figdir = folder + ""Plots/"". sc.set_figure_params(vector_friendly = True, dpi=300). sc.settings.verbosity = 3 # verbosity: errors (0), warnings (1), info (2), hints (3). sc.logging.print_version_and_date(). sc.logging.print_header(). adata = sc.read(""/data/190924_Recreated_Virus_Object_regressed.h5ad""). #adata.write(folder + ""190924_Recreated_Virus_Object_regressed.h5ad""). sc.tl.louvain(adata, resolution = 4, key_added = ""louvain_2""). sc.tl.louvain(adata, resolution = 5, key_added = ""louvain_3""). sc.tl.louvain(adata, resolution = 6, key_added = ""louvain_4""). sc.tl.louvain(adata, resolution = 7, key_added = ""louvain_5""). sc.pl.umap(adata, color = [""louvain_2"", ""louvain_3"", ""louvain_4"", ""louvain_5""], wspace = 0.45). #select resolution. print(adata.obs[""louvain_5""].value_counts()). sc.tl.rank_genes_groups(adata, groupby = ""louvain_5""). # read all arkers table from known annotated data. marker_folder = ""/marker/"". marker_table = pd.read_csv(marker_folder + ""Particle_AllMarkers.txt"", sep = ""\t"", index_col = None). marker_table.head(2). ## Restrict to Foldchange and P value. marker_table = marker_table[(marker_table.logfoldchange > 2) & (marker_table.pval_adj < 0.05)].copy(). print(marker_table.shape)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1625
https://github.com/scverse/scanpy/issues/1625:7407,safety,log,logfoldchange,7407,"ms(vector_friendly = True, dpi=300). sc.settings.verbosity = 3 # verbosity: errors (0), warnings (1), info (2), hints (3). sc.logging.print_version_and_date(). sc.logging.print_header(). adata = sc.read(""/data/190924_Recreated_Virus_Object_regressed.h5ad""). #adata.write(folder + ""190924_Recreated_Virus_Object_regressed.h5ad""). sc.tl.louvain(adata, resolution = 4, key_added = ""louvain_2""). sc.tl.louvain(adata, resolution = 5, key_added = ""louvain_3""). sc.tl.louvain(adata, resolution = 6, key_added = ""louvain_4""). sc.tl.louvain(adata, resolution = 7, key_added = ""louvain_5""). sc.pl.umap(adata, color = [""louvain_2"", ""louvain_3"", ""louvain_4"", ""louvain_5""], wspace = 0.45). #select resolution. print(adata.obs[""louvain_5""].value_counts()). sc.tl.rank_genes_groups(adata, groupby = ""louvain_5""). # read all arkers table from known annotated data. marker_folder = ""/marker/"". marker_table = pd.read_csv(marker_folder + ""Particle_AllMarkers.txt"", sep = ""\t"", index_col = None). marker_table.head(2). ## Restrict to Foldchange and P value. marker_table = marker_table[(marker_table.logfoldchange > 2) & (marker_table.pval_adj < 0.05)].copy(). print(marker_table.shape). marker = dict(). for ct in marker_table[""cluster""].unique():. tmp = marker_table[marker_table[""cluster""] == ct]. marker[ct] = tmp.gene.values. ## Look at a predefined list of cell types and the corresponding marker genes in order to annotate our clusters. cell_annotation = sc.tl.marker_gene_overlap(adata, marker, key = 'rank_genes_groups', normalize = 'reference'). fig, ax = plt.subplots(figsize = (30, 10)). sb.heatmap(cell_annotation, cbar = False, annot = True). fig.savefig('/data/heatmap.png'). ```. Everything works smoothly and the results look fine until the marker_gene_overlap. A similar issue was observed by another ICB member who is trying to verify that their new environment still works. Any pointers strongly appreciated, since we consider this a severe issue blocking new analyses with the most recent packages.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1625
https://github.com/scverse/scanpy/issues/1625:22,security,compl,completely,22,"gene ranking produces completely wrong results on the most recent tool/library combinations on specific dataset; - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. # Introduction. Hi,. so this is a weird one and I could not track it down yet. The Schillerlab people have a workstation known as ""agando"". On this workstation the full environment is installed globally and shared by all users. I am looking to change that. # The issue. When calculating the `sc.tl.marker_gene_overlap` I get the expected and reasonable results on the agando environment, but completely rubbish results when running the same code with a fresh Conda environment and the latest dependencies installed. ![image](https://user-images.githubusercontent.com/21954664/106739402-659dfb80-6619-11eb-84f1-e75abfa6167d.png). Top = new, trash results. Bottom = old=agando expected results. The old environment has:. ```. scanpy==1.6.1.dev110+gb4234d81 anndata==0.7.4 umap==0.4.6 numpy==1.19.0 scipy==1.5.1 pandas==1.1.5 scikit-learn==0.23.1 statsmodels==0.12.1 python-igraph==0.8.0 louvain==0.6.1 leidenalg==0.8.3. ```. The new environment has . ```. scanpy==1.6.1 anndata==0.7.5 umap==0.4.6 numpy==1.20.0 scipy==1.6.0 pandas==1.2.1 scikit-learn==0.24.1 statsmodels==0.12.1 python-igraph==0.8.3 louvain==0.7.0 leidenalg==0.8.3. ```. Full new conda environment:. ```. name: single_cell_analysis. channels:. - defaults. dependencies:. - _libgcc_mutex=0.1=main. - argon2-cffi=20.1.0=py37h7b6447c_1. - async_generator=1.10=py37h28b3542_0. - attrs=20.3.0=pyhd3eb1b0_0. - backcall=0.2.0=pyhd3eb1b0_0. - bleach=3.3.0=pyhd3eb1b0_0. - ca-certificates=2021.1.19=h06a4308_0. - certifi=2020.12.5=py37h06a4308_0. - cffi=1.14.4=py37h261ae71_0. - dbus=1.13.18=hb2f20db_0. - decorator=4.4.2=pyhd3eb1b0_0. - defusedxml=0.6.0=py_0. - entrypoints=0.3=py37_0. - expat=2.2.10=he",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1625
https://github.com/scverse/scanpy/issues/1625:732,security,compl,completely,732,"gene ranking produces completely wrong results on the most recent tool/library combinations on specific dataset; - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. # Introduction. Hi,. so this is a weird one and I could not track it down yet. The Schillerlab people have a workstation known as ""agando"". On this workstation the full environment is installed globally and shared by all users. I am looking to change that. # The issue. When calculating the `sc.tl.marker_gene_overlap` I get the expected and reasonable results on the agando environment, but completely rubbish results when running the same code with a fresh Conda environment and the latest dependencies installed. ![image](https://user-images.githubusercontent.com/21954664/106739402-659dfb80-6619-11eb-84f1-e75abfa6167d.png). Top = new, trash results. Bottom = old=agando expected results. The old environment has:. ```. scanpy==1.6.1.dev110+gb4234d81 anndata==0.7.4 umap==0.4.6 numpy==1.19.0 scipy==1.5.1 pandas==1.1.5 scikit-learn==0.23.1 statsmodels==0.12.1 python-igraph==0.8.0 louvain==0.6.1 leidenalg==0.8.3. ```. The new environment has . ```. scanpy==1.6.1 anndata==0.7.5 umap==0.4.6 numpy==1.20.0 scipy==1.6.0 pandas==1.2.1 scikit-learn==0.24.1 statsmodels==0.12.1 python-igraph==0.8.3 louvain==0.7.0 leidenalg==0.8.3. ```. Full new conda environment:. ```. name: single_cell_analysis. channels:. - defaults. dependencies:. - _libgcc_mutex=0.1=main. - argon2-cffi=20.1.0=py37h7b6447c_1. - async_generator=1.10=py37h28b3542_0. - attrs=20.3.0=pyhd3eb1b0_0. - backcall=0.2.0=pyhd3eb1b0_0. - bleach=3.3.0=pyhd3eb1b0_0. - ca-certificates=2021.1.19=h06a4308_0. - certifi=2020.12.5=py37h06a4308_0. - cffi=1.14.4=py37h261ae71_0. - dbus=1.13.18=hb2f20db_0. - decorator=4.4.2=pyhd3eb1b0_0. - defusedxml=0.6.0=py_0. - entrypoints=0.3=py37_0. - expat=2.2.10=he",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1625
https://github.com/scverse/scanpy/issues/1625:1772,security,certif,certificates,1772,"me code with a fresh Conda environment and the latest dependencies installed. ![image](https://user-images.githubusercontent.com/21954664/106739402-659dfb80-6619-11eb-84f1-e75abfa6167d.png). Top = new, trash results. Bottom = old=agando expected results. The old environment has:. ```. scanpy==1.6.1.dev110+gb4234d81 anndata==0.7.4 umap==0.4.6 numpy==1.19.0 scipy==1.5.1 pandas==1.1.5 scikit-learn==0.23.1 statsmodels==0.12.1 python-igraph==0.8.0 louvain==0.6.1 leidenalg==0.8.3. ```. The new environment has . ```. scanpy==1.6.1 anndata==0.7.5 umap==0.4.6 numpy==1.20.0 scipy==1.6.0 pandas==1.2.1 scikit-learn==0.24.1 statsmodels==0.12.1 python-igraph==0.8.3 louvain==0.7.0 leidenalg==0.8.3. ```. Full new conda environment:. ```. name: single_cell_analysis. channels:. - defaults. dependencies:. - _libgcc_mutex=0.1=main. - argon2-cffi=20.1.0=py37h7b6447c_1. - async_generator=1.10=py37h28b3542_0. - attrs=20.3.0=pyhd3eb1b0_0. - backcall=0.2.0=pyhd3eb1b0_0. - bleach=3.3.0=pyhd3eb1b0_0. - ca-certificates=2021.1.19=h06a4308_0. - certifi=2020.12.5=py37h06a4308_0. - cffi=1.14.4=py37h261ae71_0. - dbus=1.13.18=hb2f20db_0. - decorator=4.4.2=pyhd3eb1b0_0. - defusedxml=0.6.0=py_0. - entrypoints=0.3=py37_0. - expat=2.2.10=he6710b0_2. - fontconfig=2.13.0=h9420a91_0. - freetype=2.10.4=h5ab3b9f_0. - glib=2.66.1=h92f7085_0. - gst-plugins-base=1.14.0=h8213a91_2. - gstreamer=1.14.0=h28cd5cc_2. - icu=58.2=he6710b0_3. - importlib_metadata=2.0.0=1. - ipykernel=5.3.4=py37h5ca1d4c_0. - ipython=7.20.0=py37hb070fc8_1. - ipython_genutils=0.2.0=pyhd3eb1b0_1. - ipywidgets=7.6.3=pyhd3eb1b0_1. - jedi=0.17.0=py37_0. - jinja2=2.11.3=pyhd3eb1b0_0. - jpeg=9b=h024ee3a_2. - jsonschema=3.2.0=py_2. - jupyter=1.0.0=py37_7. - jupyter_client=6.1.7=py_0. - jupyter_console=6.2.0=py_0. - jupyter_core=4.7.1=py37h06a4308_0. - jupyterlab_pygments=0.1.2=py_0. - jupyterlab_widgets=1.0.0=pyhd3eb1b0_1. - ld_impl_linux-64=2.33.1=h53a641e_7. - libedit=3.1.20191231=h14c3975_1. - libffi=3.3=he6710b0_2. - libgcc-ng=9.1.0=hdf63c60_",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1625
https://github.com/scverse/scanpy/issues/1625:1809,security,certif,certifi,1809,"ent and the latest dependencies installed. ![image](https://user-images.githubusercontent.com/21954664/106739402-659dfb80-6619-11eb-84f1-e75abfa6167d.png). Top = new, trash results. Bottom = old=agando expected results. The old environment has:. ```. scanpy==1.6.1.dev110+gb4234d81 anndata==0.7.4 umap==0.4.6 numpy==1.19.0 scipy==1.5.1 pandas==1.1.5 scikit-learn==0.23.1 statsmodels==0.12.1 python-igraph==0.8.0 louvain==0.6.1 leidenalg==0.8.3. ```. The new environment has . ```. scanpy==1.6.1 anndata==0.7.5 umap==0.4.6 numpy==1.20.0 scipy==1.6.0 pandas==1.2.1 scikit-learn==0.24.1 statsmodels==0.12.1 python-igraph==0.8.3 louvain==0.7.0 leidenalg==0.8.3. ```. Full new conda environment:. ```. name: single_cell_analysis. channels:. - defaults. dependencies:. - _libgcc_mutex=0.1=main. - argon2-cffi=20.1.0=py37h7b6447c_1. - async_generator=1.10=py37h28b3542_0. - attrs=20.3.0=pyhd3eb1b0_0. - backcall=0.2.0=pyhd3eb1b0_0. - bleach=3.3.0=pyhd3eb1b0_0. - ca-certificates=2021.1.19=h06a4308_0. - certifi=2020.12.5=py37h06a4308_0. - cffi=1.14.4=py37h261ae71_0. - dbus=1.13.18=hb2f20db_0. - decorator=4.4.2=pyhd3eb1b0_0. - defusedxml=0.6.0=py_0. - entrypoints=0.3=py37_0. - expat=2.2.10=he6710b0_2. - fontconfig=2.13.0=h9420a91_0. - freetype=2.10.4=h5ab3b9f_0. - glib=2.66.1=h92f7085_0. - gst-plugins-base=1.14.0=h8213a91_2. - gstreamer=1.14.0=h28cd5cc_2. - icu=58.2=he6710b0_3. - importlib_metadata=2.0.0=1. - ipykernel=5.3.4=py37h5ca1d4c_0. - ipython=7.20.0=py37hb070fc8_1. - ipython_genutils=0.2.0=pyhd3eb1b0_1. - ipywidgets=7.6.3=pyhd3eb1b0_1. - jedi=0.17.0=py37_0. - jinja2=2.11.3=pyhd3eb1b0_0. - jpeg=9b=h024ee3a_2. - jsonschema=3.2.0=py_2. - jupyter=1.0.0=py37_7. - jupyter_client=6.1.7=py_0. - jupyter_console=6.2.0=py_0. - jupyter_core=4.7.1=py37h06a4308_0. - jupyterlab_pygments=0.1.2=py_0. - jupyterlab_widgets=1.0.0=pyhd3eb1b0_1. - ld_impl_linux-64=2.33.1=h53a641e_7. - libedit=3.1.20191231=h14c3975_1. - libffi=3.3=he6710b0_2. - libgcc-ng=9.1.0=hdf63c60_0. - libpng=1.6.37=hbc83047_0. - li",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1625
https://github.com/scverse/scanpy/issues/1625:4858,security,network,networkx,4858,"2.8.1=pyhd3eb1b0_0. - pyzmq=20.0.0=py37h2531618_1. - qt=5.9.7=h5867ecd_1. - qtconsole=4.7.7=py_0. - qtpy=1.9.0=py_0. - readline=8.1=h27cfd23_0. - send2trash=1.5.0=pyhd3eb1b0_1. - setuptools=52.0.0=py37h06a4308_0. - sip=4.19.8=py37hf484d3e_0. - six=1.15.0=py37h06a4308_0. - sqlite=3.33.0=h62c20be_0. - terminado=0.9.2=py37h06a4308_0. - testpath=0.4.4=pyhd3eb1b0_0. - tk=8.6.10=hbc83047_0. - tornado=6.1=py37h27cfd23_0. - traitlets=5.0.5=pyhd3eb1b0_0. - wcwidth=0.2.5=py_0. - webencodings=0.5.1=py37_1. - wheel=0.36.2=pyhd3eb1b0_0. - widgetsnbextension=3.5.1=py37_0. - xz=5.2.5=h7b6447c_0. - zeromq=4.3.3=he6710b0_3. - zipp=3.4.0=pyhd3eb1b0_0. - zlib=1.2.11=h7b6447c_3. - pip:. - anndata==0.7.5. - cached-property==1.5.2. - click==7.1.2. - cycler==0.10.0. - get-version==2.1. - h5py==3.1.0. - importlib-metadata==3.4.0. - joblib==1.0.0. - kiwisolver==1.3.1. - legacy-api-wrap==1.2. - leidenalg==0.8.3. - llvmlite==0.35.0. - loompy==3.0.6. - louvain==0.7.0. - matplotlib==3.3.4. - natsort==7.1.1. - networkx==2.5. - numba==0.52.0. - numexpr==2.7.2. - numpy==1.20.0. - numpy-groupies==0.9.13. - pandas==1.2.1. - patsy==0.5.1. - pillow==8.1.0. - python-igraph==0.8.3. - pytz==2021.1. - scanpy==1.6.1. - scikit-learn==0.24.1. - scipy==1.6.0. - scvelo==0.2.2. - seaborn==0.11.1. - setuptools-scm==5.0.1. - sinfo==0.3.1. - statsmodels==0.12.1. - stdlib-list==0.8.0. - tables==3.6.1. - texttable==1.6.3. - threadpoolctl==2.1.0. - tqdm==4.56.0. - typing-extensions==3.7.4.3. - umap-learn==0.4.6. ```. I can reproduce the issue with a Docker container that only has the minimal conda environment above. Additionally, I already tried installing the exact same dependency versions in the new environment, but got the same results. . If you need access to the data and the container please contact me and I will make it available to you. The data is already at the ICB cluster. Code:. ```. from os import path. import numpy as np. import matplotlib.pyplot as plt. import scanpy as sc. import scanpy.external as sce",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1625
https://github.com/scverse/scanpy/issues/1625:5594,security,access,access,5594," - cycler==0.10.0. - get-version==2.1. - h5py==3.1.0. - importlib-metadata==3.4.0. - joblib==1.0.0. - kiwisolver==1.3.1. - legacy-api-wrap==1.2. - leidenalg==0.8.3. - llvmlite==0.35.0. - loompy==3.0.6. - louvain==0.7.0. - matplotlib==3.3.4. - natsort==7.1.1. - networkx==2.5. - numba==0.52.0. - numexpr==2.7.2. - numpy==1.20.0. - numpy-groupies==0.9.13. - pandas==1.2.1. - patsy==0.5.1. - pillow==8.1.0. - python-igraph==0.8.3. - pytz==2021.1. - scanpy==1.6.1. - scikit-learn==0.24.1. - scipy==1.6.0. - scvelo==0.2.2. - seaborn==0.11.1. - setuptools-scm==5.0.1. - sinfo==0.3.1. - statsmodels==0.12.1. - stdlib-list==0.8.0. - tables==3.6.1. - texttable==1.6.3. - threadpoolctl==2.1.0. - tqdm==4.56.0. - typing-extensions==3.7.4.3. - umap-learn==0.4.6. ```. I can reproduce the issue with a Docker container that only has the minimal conda environment above. Additionally, I already tried installing the exact same dependency versions in the new environment, but got the same results. . If you need access to the data and the container please contact me and I will make it available to you. The data is already at the ICB cluster. Code:. ```. from os import path. import numpy as np. import matplotlib.pyplot as plt. import scanpy as sc. import scanpy.external as sce. from os import listdir. import pandas as pd. import seaborn as sb. import datetime, time. import scvelo as scv. from matplotlib.colors import LinearSegmentedColormap. #Define a nice colour map for gene expression. from matplotlib import colors. def timestamp():. ts = time.time(). st = datetime.datetime.fromtimestamp(ts).strftime('%d-%m-%Y %H:%M:%S'). return st. # Exporting folder. folder = ""/output"". sc.settings.figdir = folder + ""Plots/"". sc.set_figure_params(vector_friendly = True, dpi=300). sc.settings.verbosity = 3 # verbosity: errors (0), warnings (1), info (2), hints (3). sc.logging.print_version_and_date(). sc.logging.print_header(). adata = sc.read(""/data/190924_Recreated_Virus_Object_regressed.h5ad""). #adata.write(",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1625
https://github.com/scverse/scanpy/issues/1625:5668,security,availab,available,5668,"3.4.0. - joblib==1.0.0. - kiwisolver==1.3.1. - legacy-api-wrap==1.2. - leidenalg==0.8.3. - llvmlite==0.35.0. - loompy==3.0.6. - louvain==0.7.0. - matplotlib==3.3.4. - natsort==7.1.1. - networkx==2.5. - numba==0.52.0. - numexpr==2.7.2. - numpy==1.20.0. - numpy-groupies==0.9.13. - pandas==1.2.1. - patsy==0.5.1. - pillow==8.1.0. - python-igraph==0.8.3. - pytz==2021.1. - scanpy==1.6.1. - scikit-learn==0.24.1. - scipy==1.6.0. - scvelo==0.2.2. - seaborn==0.11.1. - setuptools-scm==5.0.1. - sinfo==0.3.1. - statsmodels==0.12.1. - stdlib-list==0.8.0. - tables==3.6.1. - texttable==1.6.3. - threadpoolctl==2.1.0. - tqdm==4.56.0. - typing-extensions==3.7.4.3. - umap-learn==0.4.6. ```. I can reproduce the issue with a Docker container that only has the minimal conda environment above. Additionally, I already tried installing the exact same dependency versions in the new environment, but got the same results. . If you need access to the data and the container please contact me and I will make it available to you. The data is already at the ICB cluster. Code:. ```. from os import path. import numpy as np. import matplotlib.pyplot as plt. import scanpy as sc. import scanpy.external as sce. from os import listdir. import pandas as pd. import seaborn as sb. import datetime, time. import scvelo as scv. from matplotlib.colors import LinearSegmentedColormap. #Define a nice colour map for gene expression. from matplotlib import colors. def timestamp():. ts = time.time(). st = datetime.datetime.fromtimestamp(ts).strftime('%d-%m-%Y %H:%M:%S'). return st. # Exporting folder. folder = ""/output"". sc.settings.figdir = folder + ""Plots/"". sc.set_figure_params(vector_friendly = True, dpi=300). sc.settings.verbosity = 3 # verbosity: errors (0), warnings (1), info (2), hints (3). sc.logging.print_version_and_date(). sc.logging.print_header(). adata = sc.read(""/data/190924_Recreated_Virus_Object_regressed.h5ad""). #adata.write(folder + ""190924_Recreated_Virus_Object_regressed.h5ad""). sc.tl.louvain(adat",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1625
https://github.com/scverse/scanpy/issues/1625:6452,security,log,logging,6452,"ditionally, I already tried installing the exact same dependency versions in the new environment, but got the same results. . If you need access to the data and the container please contact me and I will make it available to you. The data is already at the ICB cluster. Code:. ```. from os import path. import numpy as np. import matplotlib.pyplot as plt. import scanpy as sc. import scanpy.external as sce. from os import listdir. import pandas as pd. import seaborn as sb. import datetime, time. import scvelo as scv. from matplotlib.colors import LinearSegmentedColormap. #Define a nice colour map for gene expression. from matplotlib import colors. def timestamp():. ts = time.time(). st = datetime.datetime.fromtimestamp(ts).strftime('%d-%m-%Y %H:%M:%S'). return st. # Exporting folder. folder = ""/output"". sc.settings.figdir = folder + ""Plots/"". sc.set_figure_params(vector_friendly = True, dpi=300). sc.settings.verbosity = 3 # verbosity: errors (0), warnings (1), info (2), hints (3). sc.logging.print_version_and_date(). sc.logging.print_header(). adata = sc.read(""/data/190924_Recreated_Virus_Object_regressed.h5ad""). #adata.write(folder + ""190924_Recreated_Virus_Object_regressed.h5ad""). sc.tl.louvain(adata, resolution = 4, key_added = ""louvain_2""). sc.tl.louvain(adata, resolution = 5, key_added = ""louvain_3""). sc.tl.louvain(adata, resolution = 6, key_added = ""louvain_4""). sc.tl.louvain(adata, resolution = 7, key_added = ""louvain_5""). sc.pl.umap(adata, color = [""louvain_2"", ""louvain_3"", ""louvain_4"", ""louvain_5""], wspace = 0.45). #select resolution. print(adata.obs[""louvain_5""].value_counts()). sc.tl.rank_genes_groups(adata, groupby = ""louvain_5""). # read all arkers table from known annotated data. marker_folder = ""/marker/"". marker_table = pd.read_csv(marker_folder + ""Particle_AllMarkers.txt"", sep = ""\t"", index_col = None). marker_table.head(2). ## Restrict to Foldchange and P value. marker_table = marker_table[(marker_table.logfoldchange > 2) & (marker_table.pval_adj < 0.0",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1625
https://github.com/scverse/scanpy/issues/1625:6489,security,log,logging,6489,"g the exact same dependency versions in the new environment, but got the same results. . If you need access to the data and the container please contact me and I will make it available to you. The data is already at the ICB cluster. Code:. ```. from os import path. import numpy as np. import matplotlib.pyplot as plt. import scanpy as sc. import scanpy.external as sce. from os import listdir. import pandas as pd. import seaborn as sb. import datetime, time. import scvelo as scv. from matplotlib.colors import LinearSegmentedColormap. #Define a nice colour map for gene expression. from matplotlib import colors. def timestamp():. ts = time.time(). st = datetime.datetime.fromtimestamp(ts).strftime('%d-%m-%Y %H:%M:%S'). return st. # Exporting folder. folder = ""/output"". sc.settings.figdir = folder + ""Plots/"". sc.set_figure_params(vector_friendly = True, dpi=300). sc.settings.verbosity = 3 # verbosity: errors (0), warnings (1), info (2), hints (3). sc.logging.print_version_and_date(). sc.logging.print_header(). adata = sc.read(""/data/190924_Recreated_Virus_Object_regressed.h5ad""). #adata.write(folder + ""190924_Recreated_Virus_Object_regressed.h5ad""). sc.tl.louvain(adata, resolution = 4, key_added = ""louvain_2""). sc.tl.louvain(adata, resolution = 5, key_added = ""louvain_3""). sc.tl.louvain(adata, resolution = 6, key_added = ""louvain_4""). sc.tl.louvain(adata, resolution = 7, key_added = ""louvain_5""). sc.pl.umap(adata, color = [""louvain_2"", ""louvain_3"", ""louvain_4"", ""louvain_5""], wspace = 0.45). #select resolution. print(adata.obs[""louvain_5""].value_counts()). sc.tl.rank_genes_groups(adata, groupby = ""louvain_5""). # read all arkers table from known annotated data. marker_folder = ""/marker/"". marker_table = pd.read_csv(marker_folder + ""Particle_AllMarkers.txt"", sep = ""\t"", index_col = None). marker_table.head(2). ## Restrict to Foldchange and P value. marker_table = marker_table[(marker_table.logfoldchange > 2) & (marker_table.pval_adj < 0.05)].copy(). print(marker_table.shape)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1625
https://github.com/scverse/scanpy/issues/1625:7407,security,log,logfoldchange,7407,"ms(vector_friendly = True, dpi=300). sc.settings.verbosity = 3 # verbosity: errors (0), warnings (1), info (2), hints (3). sc.logging.print_version_and_date(). sc.logging.print_header(). adata = sc.read(""/data/190924_Recreated_Virus_Object_regressed.h5ad""). #adata.write(folder + ""190924_Recreated_Virus_Object_regressed.h5ad""). sc.tl.louvain(adata, resolution = 4, key_added = ""louvain_2""). sc.tl.louvain(adata, resolution = 5, key_added = ""louvain_3""). sc.tl.louvain(adata, resolution = 6, key_added = ""louvain_4""). sc.tl.louvain(adata, resolution = 7, key_added = ""louvain_5""). sc.pl.umap(adata, color = [""louvain_2"", ""louvain_3"", ""louvain_4"", ""louvain_5""], wspace = 0.45). #select resolution. print(adata.obs[""louvain_5""].value_counts()). sc.tl.rank_genes_groups(adata, groupby = ""louvain_5""). # read all arkers table from known annotated data. marker_folder = ""/marker/"". marker_table = pd.read_csv(marker_folder + ""Particle_AllMarkers.txt"", sep = ""\t"", index_col = None). marker_table.head(2). ## Restrict to Foldchange and P value. marker_table = marker_table[(marker_table.logfoldchange > 2) & (marker_table.pval_adj < 0.05)].copy(). print(marker_table.shape). marker = dict(). for ct in marker_table[""cluster""].unique():. tmp = marker_table[marker_table[""cluster""] == ct]. marker[ct] = tmp.gene.values. ## Look at a predefined list of cell types and the corresponding marker genes in order to annotate our clusters. cell_annotation = sc.tl.marker_gene_overlap(adata, marker, key = 'rank_genes_groups', normalize = 'reference'). fig, ax = plt.subplots(figsize = (30, 10)). sb.heatmap(cell_annotation, cbar = False, annot = True). fig.savefig('/data/heatmap.png'). ```. Everything works smoothly and the results look fine until the marker_gene_overlap. A similar issue was observed by another ICB member who is trying to verify that their new environment still works. Any pointers strongly appreciated, since we consider this a severe issue blocking new analyses with the most recent packages.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1625
https://github.com/scverse/scanpy/issues/1625:832,testability,depend,dependencies,832,"gene ranking produces completely wrong results on the most recent tool/library combinations on specific dataset; - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. # Introduction. Hi,. so this is a weird one and I could not track it down yet. The Schillerlab people have a workstation known as ""agando"". On this workstation the full environment is installed globally and shared by all users. I am looking to change that. # The issue. When calculating the `sc.tl.marker_gene_overlap` I get the expected and reasonable results on the agando environment, but completely rubbish results when running the same code with a fresh Conda environment and the latest dependencies installed. ![image](https://user-images.githubusercontent.com/21954664/106739402-659dfb80-6619-11eb-84f1-e75abfa6167d.png). Top = new, trash results. Bottom = old=agando expected results. The old environment has:. ```. scanpy==1.6.1.dev110+gb4234d81 anndata==0.7.4 umap==0.4.6 numpy==1.19.0 scipy==1.5.1 pandas==1.1.5 scikit-learn==0.23.1 statsmodels==0.12.1 python-igraph==0.8.0 louvain==0.6.1 leidenalg==0.8.3. ```. The new environment has . ```. scanpy==1.6.1 anndata==0.7.5 umap==0.4.6 numpy==1.20.0 scipy==1.6.0 pandas==1.2.1 scikit-learn==0.24.1 statsmodels==0.12.1 python-igraph==0.8.3 louvain==0.7.0 leidenalg==0.8.3. ```. Full new conda environment:. ```. name: single_cell_analysis. channels:. - defaults. dependencies:. - _libgcc_mutex=0.1=main. - argon2-cffi=20.1.0=py37h7b6447c_1. - async_generator=1.10=py37h28b3542_0. - attrs=20.3.0=pyhd3eb1b0_0. - backcall=0.2.0=pyhd3eb1b0_0. - bleach=3.3.0=pyhd3eb1b0_0. - ca-certificates=2021.1.19=h06a4308_0. - certifi=2020.12.5=py37h06a4308_0. - cffi=1.14.4=py37h261ae71_0. - dbus=1.13.18=hb2f20db_0. - decorator=4.4.2=pyhd3eb1b0_0. - defusedxml=0.6.0=py_0. - entrypoints=0.3=py37_0. - expat=2.2.10=he",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1625
https://github.com/scverse/scanpy/issues/1625:1561,testability,depend,dependencies,1561," I am looking to change that. # The issue. When calculating the `sc.tl.marker_gene_overlap` I get the expected and reasonable results on the agando environment, but completely rubbish results when running the same code with a fresh Conda environment and the latest dependencies installed. ![image](https://user-images.githubusercontent.com/21954664/106739402-659dfb80-6619-11eb-84f1-e75abfa6167d.png). Top = new, trash results. Bottom = old=agando expected results. The old environment has:. ```. scanpy==1.6.1.dev110+gb4234d81 anndata==0.7.4 umap==0.4.6 numpy==1.19.0 scipy==1.5.1 pandas==1.1.5 scikit-learn==0.23.1 statsmodels==0.12.1 python-igraph==0.8.0 louvain==0.6.1 leidenalg==0.8.3. ```. The new environment has . ```. scanpy==1.6.1 anndata==0.7.5 umap==0.4.6 numpy==1.20.0 scipy==1.6.0 pandas==1.2.1 scikit-learn==0.24.1 statsmodels==0.12.1 python-igraph==0.8.3 louvain==0.7.0 leidenalg==0.8.3. ```. Full new conda environment:. ```. name: single_cell_analysis. channels:. - defaults. dependencies:. - _libgcc_mutex=0.1=main. - argon2-cffi=20.1.0=py37h7b6447c_1. - async_generator=1.10=py37h28b3542_0. - attrs=20.3.0=pyhd3eb1b0_0. - backcall=0.2.0=pyhd3eb1b0_0. - bleach=3.3.0=pyhd3eb1b0_0. - ca-certificates=2021.1.19=h06a4308_0. - certifi=2020.12.5=py37h06a4308_0. - cffi=1.14.4=py37h261ae71_0. - dbus=1.13.18=hb2f20db_0. - decorator=4.4.2=pyhd3eb1b0_0. - defusedxml=0.6.0=py_0. - entrypoints=0.3=py37_0. - expat=2.2.10=he6710b0_2. - fontconfig=2.13.0=h9420a91_0. - freetype=2.10.4=h5ab3b9f_0. - glib=2.66.1=h92f7085_0. - gst-plugins-base=1.14.0=h8213a91_2. - gstreamer=1.14.0=h28cd5cc_2. - icu=58.2=he6710b0_3. - importlib_metadata=2.0.0=1. - ipykernel=5.3.4=py37h5ca1d4c_0. - ipython=7.20.0=py37hb070fc8_1. - ipython_genutils=0.2.0=pyhd3eb1b0_1. - ipywidgets=7.6.3=pyhd3eb1b0_1. - jedi=0.17.0=py37_0. - jinja2=2.11.3=pyhd3eb1b0_0. - jpeg=9b=h024ee3a_2. - jsonschema=3.2.0=py_2. - jupyter=1.0.0=py37_7. - jupyter_client=6.1.7=py_0. - jupyter_console=6.2.0=py_0. - jupyter_core=4.7.1=py37h",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1625
https://github.com/scverse/scanpy/issues/1625:4197,testability,test,testpath,4197,est-asyncio=1.4.3=pyhd3eb1b0_0. - notebook=6.2.0=py37h06a4308_0. - openssl=1.1.1i=h27cfd23_0. - packaging=20.9=pyhd3eb1b0_0. - pandoc=2.11=hb0f4dca_0. - pandocfilters=1.4.3=py37h06a4308_1. - parso=0.8.1=pyhd3eb1b0_0. - pcre=8.44=he6710b0_0. - pexpect=4.8.0=pyhd3eb1b0_3. - pickleshare=0.7.5=pyhd3eb1b0_1003. - pip=20.3.3=py37h06a4308_0. - prometheus_client=0.9.0=pyhd3eb1b0_0. - prompt-toolkit=3.0.8=py_0. - prompt_toolkit=3.0.8=0. - ptyprocess=0.7.0=pyhd3eb1b0_2. - pycparser=2.20=py_2. - pygments=2.7.4=pyhd3eb1b0_0. - pyparsing=2.4.7=pyhd3eb1b0_0. - pyqt=5.9.2=py37h05f1152_2. - pyrsistent=0.17.3=py37h7b6447c_0. - python=3.7.9=h7579374_0. - python-dateutil=2.8.1=pyhd3eb1b0_0. - pyzmq=20.0.0=py37h2531618_1. - qt=5.9.7=h5867ecd_1. - qtconsole=4.7.7=py_0. - qtpy=1.9.0=py_0. - readline=8.1=h27cfd23_0. - send2trash=1.5.0=pyhd3eb1b0_1. - setuptools=52.0.0=py37h06a4308_0. - sip=4.19.8=py37hf484d3e_0. - six=1.15.0=py37h06a4308_0. - sqlite=3.33.0=h62c20be_0. - terminado=0.9.2=py37h06a4308_0. - testpath=0.4.4=pyhd3eb1b0_0. - tk=8.6.10=hbc83047_0. - tornado=6.1=py37h27cfd23_0. - traitlets=5.0.5=pyhd3eb1b0_0. - wcwidth=0.2.5=py_0. - webencodings=0.5.1=py37_1. - wheel=0.36.2=pyhd3eb1b0_0. - widgetsnbextension=3.5.1=py37_0. - xz=5.2.5=h7b6447c_0. - zeromq=4.3.3=he6710b0_3. - zipp=3.4.0=pyhd3eb1b0_0. - zlib=1.2.11=h7b6447c_3. - pip:. - anndata==0.7.5. - cached-property==1.5.2. - click==7.1.2. - cycler==0.10.0. - get-version==2.1. - h5py==3.1.0. - importlib-metadata==3.4.0. - joblib==1.0.0. - kiwisolver==1.3.1. - legacy-api-wrap==1.2. - leidenalg==0.8.3. - llvmlite==0.35.0. - loompy==3.0.6. - louvain==0.7.0. - matplotlib==3.3.4. - natsort==7.1.1. - networkx==2.5. - numba==0.52.0. - numexpr==2.7.2. - numpy==1.20.0. - numpy-groupies==0.9.13. - pandas==1.2.1. - patsy==0.5.1. - pillow==8.1.0. - python-igraph==0.8.3. - pytz==2021.1. - scanpy==1.6.1. - scikit-learn==0.24.1. - scipy==1.6.0. - scvelo==0.2.2. - seaborn==0.11.1. - setuptools-scm==5.0.1. - sinfo==0.3.1. - statsmodels==0.12.1. - s,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1625
https://github.com/scverse/scanpy/issues/1625:5510,testability,depend,dependency,5510,"11=h7b6447c_3. - pip:. - anndata==0.7.5. - cached-property==1.5.2. - click==7.1.2. - cycler==0.10.0. - get-version==2.1. - h5py==3.1.0. - importlib-metadata==3.4.0. - joblib==1.0.0. - kiwisolver==1.3.1. - legacy-api-wrap==1.2. - leidenalg==0.8.3. - llvmlite==0.35.0. - loompy==3.0.6. - louvain==0.7.0. - matplotlib==3.3.4. - natsort==7.1.1. - networkx==2.5. - numba==0.52.0. - numexpr==2.7.2. - numpy==1.20.0. - numpy-groupies==0.9.13. - pandas==1.2.1. - patsy==0.5.1. - pillow==8.1.0. - python-igraph==0.8.3. - pytz==2021.1. - scanpy==1.6.1. - scikit-learn==0.24.1. - scipy==1.6.0. - scvelo==0.2.2. - seaborn==0.11.1. - setuptools-scm==5.0.1. - sinfo==0.3.1. - statsmodels==0.12.1. - stdlib-list==0.8.0. - tables==3.6.1. - texttable==1.6.3. - threadpoolctl==2.1.0. - tqdm==4.56.0. - typing-extensions==3.7.4.3. - umap-learn==0.4.6. ```. I can reproduce the issue with a Docker container that only has the minimal conda environment above. Additionally, I already tried installing the exact same dependency versions in the new environment, but got the same results. . If you need access to the data and the container please contact me and I will make it available to you. The data is already at the ICB cluster. Code:. ```. from os import path. import numpy as np. import matplotlib.pyplot as plt. import scanpy as sc. import scanpy.external as sce. from os import listdir. import pandas as pd. import seaborn as sb. import datetime, time. import scvelo as scv. from matplotlib.colors import LinearSegmentedColormap. #Define a nice colour map for gene expression. from matplotlib import colors. def timestamp():. ts = time.time(). st = datetime.datetime.fromtimestamp(ts).strftime('%d-%m-%Y %H:%M:%S'). return st. # Exporting folder. folder = ""/output"". sc.settings.figdir = folder + ""Plots/"". sc.set_figure_params(vector_friendly = True, dpi=300). sc.settings.verbosity = 3 # verbosity: errors (0), warnings (1), info (2), hints (3). sc.logging.print_version_and_date(). sc.logging.print_header(). ad",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1625
https://github.com/scverse/scanpy/issues/1625:6452,testability,log,logging,6452,"ditionally, I already tried installing the exact same dependency versions in the new environment, but got the same results. . If you need access to the data and the container please contact me and I will make it available to you. The data is already at the ICB cluster. Code:. ```. from os import path. import numpy as np. import matplotlib.pyplot as plt. import scanpy as sc. import scanpy.external as sce. from os import listdir. import pandas as pd. import seaborn as sb. import datetime, time. import scvelo as scv. from matplotlib.colors import LinearSegmentedColormap. #Define a nice colour map for gene expression. from matplotlib import colors. def timestamp():. ts = time.time(). st = datetime.datetime.fromtimestamp(ts).strftime('%d-%m-%Y %H:%M:%S'). return st. # Exporting folder. folder = ""/output"". sc.settings.figdir = folder + ""Plots/"". sc.set_figure_params(vector_friendly = True, dpi=300). sc.settings.verbosity = 3 # verbosity: errors (0), warnings (1), info (2), hints (3). sc.logging.print_version_and_date(). sc.logging.print_header(). adata = sc.read(""/data/190924_Recreated_Virus_Object_regressed.h5ad""). #adata.write(folder + ""190924_Recreated_Virus_Object_regressed.h5ad""). sc.tl.louvain(adata, resolution = 4, key_added = ""louvain_2""). sc.tl.louvain(adata, resolution = 5, key_added = ""louvain_3""). sc.tl.louvain(adata, resolution = 6, key_added = ""louvain_4""). sc.tl.louvain(adata, resolution = 7, key_added = ""louvain_5""). sc.pl.umap(adata, color = [""louvain_2"", ""louvain_3"", ""louvain_4"", ""louvain_5""], wspace = 0.45). #select resolution. print(adata.obs[""louvain_5""].value_counts()). sc.tl.rank_genes_groups(adata, groupby = ""louvain_5""). # read all arkers table from known annotated data. marker_folder = ""/marker/"". marker_table = pd.read_csv(marker_folder + ""Particle_AllMarkers.txt"", sep = ""\t"", index_col = None). marker_table.head(2). ## Restrict to Foldchange and P value. marker_table = marker_table[(marker_table.logfoldchange > 2) & (marker_table.pval_adj < 0.0",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1625
https://github.com/scverse/scanpy/issues/1625:6489,testability,log,logging,6489,"g the exact same dependency versions in the new environment, but got the same results. . If you need access to the data and the container please contact me and I will make it available to you. The data is already at the ICB cluster. Code:. ```. from os import path. import numpy as np. import matplotlib.pyplot as plt. import scanpy as sc. import scanpy.external as sce. from os import listdir. import pandas as pd. import seaborn as sb. import datetime, time. import scvelo as scv. from matplotlib.colors import LinearSegmentedColormap. #Define a nice colour map for gene expression. from matplotlib import colors. def timestamp():. ts = time.time(). st = datetime.datetime.fromtimestamp(ts).strftime('%d-%m-%Y %H:%M:%S'). return st. # Exporting folder. folder = ""/output"". sc.settings.figdir = folder + ""Plots/"". sc.set_figure_params(vector_friendly = True, dpi=300). sc.settings.verbosity = 3 # verbosity: errors (0), warnings (1), info (2), hints (3). sc.logging.print_version_and_date(). sc.logging.print_header(). adata = sc.read(""/data/190924_Recreated_Virus_Object_regressed.h5ad""). #adata.write(folder + ""190924_Recreated_Virus_Object_regressed.h5ad""). sc.tl.louvain(adata, resolution = 4, key_added = ""louvain_2""). sc.tl.louvain(adata, resolution = 5, key_added = ""louvain_3""). sc.tl.louvain(adata, resolution = 6, key_added = ""louvain_4""). sc.tl.louvain(adata, resolution = 7, key_added = ""louvain_5""). sc.pl.umap(adata, color = [""louvain_2"", ""louvain_3"", ""louvain_4"", ""louvain_5""], wspace = 0.45). #select resolution. print(adata.obs[""louvain_5""].value_counts()). sc.tl.rank_genes_groups(adata, groupby = ""louvain_5""). # read all arkers table from known annotated data. marker_folder = ""/marker/"". marker_table = pd.read_csv(marker_folder + ""Particle_AllMarkers.txt"", sep = ""\t"", index_col = None). marker_table.head(2). ## Restrict to Foldchange and P value. marker_table = marker_table[(marker_table.logfoldchange > 2) & (marker_table.pval_adj < 0.05)].copy(). print(marker_table.shape)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1625
https://github.com/scverse/scanpy/issues/1625:7407,testability,log,logfoldchange,7407,"ms(vector_friendly = True, dpi=300). sc.settings.verbosity = 3 # verbosity: errors (0), warnings (1), info (2), hints (3). sc.logging.print_version_and_date(). sc.logging.print_header(). adata = sc.read(""/data/190924_Recreated_Virus_Object_regressed.h5ad""). #adata.write(folder + ""190924_Recreated_Virus_Object_regressed.h5ad""). sc.tl.louvain(adata, resolution = 4, key_added = ""louvain_2""). sc.tl.louvain(adata, resolution = 5, key_added = ""louvain_3""). sc.tl.louvain(adata, resolution = 6, key_added = ""louvain_4""). sc.tl.louvain(adata, resolution = 7, key_added = ""louvain_5""). sc.pl.umap(adata, color = [""louvain_2"", ""louvain_3"", ""louvain_4"", ""louvain_5""], wspace = 0.45). #select resolution. print(adata.obs[""louvain_5""].value_counts()). sc.tl.rank_genes_groups(adata, groupby = ""louvain_5""). # read all arkers table from known annotated data. marker_folder = ""/marker/"". marker_table = pd.read_csv(marker_folder + ""Particle_AllMarkers.txt"", sep = ""\t"", index_col = None). marker_table.head(2). ## Restrict to Foldchange and P value. marker_table = marker_table[(marker_table.logfoldchange > 2) & (marker_table.pval_adj < 0.05)].copy(). print(marker_table.shape). marker = dict(). for ct in marker_table[""cluster""].unique():. tmp = marker_table[marker_table[""cluster""] == ct]. marker[ct] = tmp.gene.values. ## Look at a predefined list of cell types and the corresponding marker genes in order to annotate our clusters. cell_annotation = sc.tl.marker_gene_overlap(adata, marker, key = 'rank_genes_groups', normalize = 'reference'). fig, ax = plt.subplots(figsize = (30, 10)). sb.heatmap(cell_annotation, cbar = False, annot = True). fig.savefig('/data/heatmap.png'). ```. Everything works smoothly and the results look fine until the marker_gene_overlap. A similar issue was observed by another ICB member who is trying to verify that their new environment still works. Any pointers strongly appreciated, since we consider this a severe issue blocking new analyses with the most recent packages.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1625
https://github.com/scverse/scanpy/issues/1625:8106,testability,observ,observed,8106,"ms(vector_friendly = True, dpi=300). sc.settings.verbosity = 3 # verbosity: errors (0), warnings (1), info (2), hints (3). sc.logging.print_version_and_date(). sc.logging.print_header(). adata = sc.read(""/data/190924_Recreated_Virus_Object_regressed.h5ad""). #adata.write(folder + ""190924_Recreated_Virus_Object_regressed.h5ad""). sc.tl.louvain(adata, resolution = 4, key_added = ""louvain_2""). sc.tl.louvain(adata, resolution = 5, key_added = ""louvain_3""). sc.tl.louvain(adata, resolution = 6, key_added = ""louvain_4""). sc.tl.louvain(adata, resolution = 7, key_added = ""louvain_5""). sc.pl.umap(adata, color = [""louvain_2"", ""louvain_3"", ""louvain_4"", ""louvain_5""], wspace = 0.45). #select resolution. print(adata.obs[""louvain_5""].value_counts()). sc.tl.rank_genes_groups(adata, groupby = ""louvain_5""). # read all arkers table from known annotated data. marker_folder = ""/marker/"". marker_table = pd.read_csv(marker_folder + ""Particle_AllMarkers.txt"", sep = ""\t"", index_col = None). marker_table.head(2). ## Restrict to Foldchange and P value. marker_table = marker_table[(marker_table.logfoldchange > 2) & (marker_table.pval_adj < 0.05)].copy(). print(marker_table.shape). marker = dict(). for ct in marker_table[""cluster""].unique():. tmp = marker_table[marker_table[""cluster""] == ct]. marker[ct] = tmp.gene.values. ## Look at a predefined list of cell types and the corresponding marker genes in order to annotate our clusters. cell_annotation = sc.tl.marker_gene_overlap(adata, marker, key = 'rank_genes_groups', normalize = 'reference'). fig, ax = plt.subplots(figsize = (30, 10)). sb.heatmap(cell_annotation, cbar = False, annot = True). fig.savefig('/data/heatmap.png'). ```. Everything works smoothly and the results look fine until the marker_gene_overlap. A similar issue was observed by another ICB member who is trying to verify that their new environment still works. Any pointers strongly appreciated, since we consider this a severe issue blocking new analyses with the most recent packages.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1625
https://github.com/scverse/scanpy/issues/1625:8154,testability,verif,verify,8154,"ms(vector_friendly = True, dpi=300). sc.settings.verbosity = 3 # verbosity: errors (0), warnings (1), info (2), hints (3). sc.logging.print_version_and_date(). sc.logging.print_header(). adata = sc.read(""/data/190924_Recreated_Virus_Object_regressed.h5ad""). #adata.write(folder + ""190924_Recreated_Virus_Object_regressed.h5ad""). sc.tl.louvain(adata, resolution = 4, key_added = ""louvain_2""). sc.tl.louvain(adata, resolution = 5, key_added = ""louvain_3""). sc.tl.louvain(adata, resolution = 6, key_added = ""louvain_4""). sc.tl.louvain(adata, resolution = 7, key_added = ""louvain_5""). sc.pl.umap(adata, color = [""louvain_2"", ""louvain_3"", ""louvain_4"", ""louvain_5""], wspace = 0.45). #select resolution. print(adata.obs[""louvain_5""].value_counts()). sc.tl.rank_genes_groups(adata, groupby = ""louvain_5""). # read all arkers table from known annotated data. marker_folder = ""/marker/"". marker_table = pd.read_csv(marker_folder + ""Particle_AllMarkers.txt"", sep = ""\t"", index_col = None). marker_table.head(2). ## Restrict to Foldchange and P value. marker_table = marker_table[(marker_table.logfoldchange > 2) & (marker_table.pval_adj < 0.05)].copy(). print(marker_table.shape). marker = dict(). for ct in marker_table[""cluster""].unique():. tmp = marker_table[marker_table[""cluster""] == ct]. marker[ct] = tmp.gene.values. ## Look at a predefined list of cell types and the corresponding marker genes in order to annotate our clusters. cell_annotation = sc.tl.marker_gene_overlap(adata, marker, key = 'rank_genes_groups', normalize = 'reference'). fig, ax = plt.subplots(figsize = (30, 10)). sb.heatmap(cell_annotation, cbar = False, annot = True). fig.savefig('/data/heatmap.png'). ```. Everything works smoothly and the results look fine until the marker_gene_overlap. A similar issue was observed by another ICB member who is trying to verify that their new environment still works. Any pointers strongly appreciated, since we consider this a severe issue blocking new analyses with the most recent packages.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1625
https://github.com/scverse/scanpy/issues/1625:66,usability,tool,tool,66,"gene ranking produces completely wrong results on the most recent tool/library combinations on specific dataset; - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. # Introduction. Hi,. so this is a weird one and I could not track it down yet. The Schillerlab people have a workstation known as ""agando"". On this workstation the full environment is installed globally and shared by all users. I am looking to change that. # The issue. When calculating the `sc.tl.marker_gene_overlap` I get the expected and reasonable results on the agando environment, but completely rubbish results when running the same code with a fresh Conda environment and the latest dependencies installed. ![image](https://user-images.githubusercontent.com/21954664/106739402-659dfb80-6619-11eb-84f1-e75abfa6167d.png). Top = new, trash results. Bottom = old=agando expected results. The old environment has:. ```. scanpy==1.6.1.dev110+gb4234d81 anndata==0.7.4 umap==0.4.6 numpy==1.19.0 scipy==1.5.1 pandas==1.1.5 scikit-learn==0.23.1 statsmodels==0.12.1 python-igraph==0.8.0 louvain==0.6.1 leidenalg==0.8.3. ```. The new environment has . ```. scanpy==1.6.1 anndata==0.7.5 umap==0.4.6 numpy==1.20.0 scipy==1.6.0 pandas==1.2.1 scikit-learn==0.24.1 statsmodels==0.12.1 python-igraph==0.8.3 louvain==0.7.0 leidenalg==0.8.3. ```. Full new conda environment:. ```. name: single_cell_analysis. channels:. - defaults. dependencies:. - _libgcc_mutex=0.1=main. - argon2-cffi=20.1.0=py37h7b6447c_1. - async_generator=1.10=py37h28b3542_0. - attrs=20.3.0=pyhd3eb1b0_0. - backcall=0.2.0=pyhd3eb1b0_0. - bleach=3.3.0=pyhd3eb1b0_0. - ca-certificates=2021.1.19=h06a4308_0. - certifi=2020.12.5=py37h06a4308_0. - cffi=1.14.4=py37h261ae71_0. - dbus=1.13.18=hb2f20db_0. - decorator=4.4.2=pyhd3eb1b0_0. - defusedxml=0.6.0=py_0. - entrypoints=0.3=py37_0. - expat=2.2.10=he",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1625
https://github.com/scverse/scanpy/issues/1625:194,usability,confirm,confirmed,194,"gene ranking produces completely wrong results on the most recent tool/library combinations on specific dataset; - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. # Introduction. Hi,. so this is a weird one and I could not track it down yet. The Schillerlab people have a workstation known as ""agando"". On this workstation the full environment is installed globally and shared by all users. I am looking to change that. # The issue. When calculating the `sc.tl.marker_gene_overlap` I get the expected and reasonable results on the agando environment, but completely rubbish results when running the same code with a fresh Conda environment and the latest dependencies installed. ![image](https://user-images.githubusercontent.com/21954664/106739402-659dfb80-6619-11eb-84f1-e75abfa6167d.png). Top = new, trash results. Bottom = old=agando expected results. The old environment has:. ```. scanpy==1.6.1.dev110+gb4234d81 anndata==0.7.4 umap==0.4.6 numpy==1.19.0 scipy==1.5.1 pandas==1.1.5 scikit-learn==0.23.1 statsmodels==0.12.1 python-igraph==0.8.0 louvain==0.6.1 leidenalg==0.8.3. ```. The new environment has . ```. scanpy==1.6.1 anndata==0.7.5 umap==0.4.6 numpy==1.20.0 scipy==1.6.0 pandas==1.2.1 scikit-learn==0.24.1 statsmodels==0.12.1 python-igraph==0.8.3 louvain==0.7.0 leidenalg==0.8.3. ```. Full new conda environment:. ```. name: single_cell_analysis. channels:. - defaults. dependencies:. - _libgcc_mutex=0.1=main. - argon2-cffi=20.1.0=py37h7b6447c_1. - async_generator=1.10=py37h28b3542_0. - attrs=20.3.0=pyhd3eb1b0_0. - backcall=0.2.0=pyhd3eb1b0_0. - bleach=3.3.0=pyhd3eb1b0_0. - ca-certificates=2021.1.19=h06a4308_0. - certifi=2020.12.5=py37h06a4308_0. - cffi=1.14.4=py37h261ae71_0. - dbus=1.13.18=hb2f20db_0. - decorator=4.4.2=pyhd3eb1b0_0. - defusedxml=0.6.0=py_0. - entrypoints=0.3=py37_0. - expat=2.2.10=he",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1625
https://github.com/scverse/scanpy/issues/1625:277,usability,confirm,confirmed,277,"gene ranking produces completely wrong results on the most recent tool/library combinations on specific dataset; - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. # Introduction. Hi,. so this is a weird one and I could not track it down yet. The Schillerlab people have a workstation known as ""agando"". On this workstation the full environment is installed globally and shared by all users. I am looking to change that. # The issue. When calculating the `sc.tl.marker_gene_overlap` I get the expected and reasonable results on the agando environment, but completely rubbish results when running the same code with a fresh Conda environment and the latest dependencies installed. ![image](https://user-images.githubusercontent.com/21954664/106739402-659dfb80-6619-11eb-84f1-e75abfa6167d.png). Top = new, trash results. Bottom = old=agando expected results. The old environment has:. ```. scanpy==1.6.1.dev110+gb4234d81 anndata==0.7.4 umap==0.4.6 numpy==1.19.0 scipy==1.5.1 pandas==1.1.5 scikit-learn==0.23.1 statsmodels==0.12.1 python-igraph==0.8.0 louvain==0.6.1 leidenalg==0.8.3. ```. The new environment has . ```. scanpy==1.6.1 anndata==0.7.5 umap==0.4.6 numpy==1.20.0 scipy==1.6.0 pandas==1.2.1 scikit-learn==0.24.1 statsmodels==0.12.1 python-igraph==0.8.3 louvain==0.7.0 leidenalg==0.8.3. ```. Full new conda environment:. ```. name: single_cell_analysis. channels:. - defaults. dependencies:. - _libgcc_mutex=0.1=main. - argon2-cffi=20.1.0=py37h7b6447c_1. - async_generator=1.10=py37h28b3542_0. - attrs=20.3.0=pyhd3eb1b0_0. - backcall=0.2.0=pyhd3eb1b0_0. - bleach=3.3.0=pyhd3eb1b0_0. - ca-certificates=2021.1.19=h06a4308_0. - certifi=2020.12.5=py37h06a4308_0. - cffi=1.14.4=py37h261ae71_0. - dbus=1.13.18=hb2f20db_0. - decorator=4.4.2=pyhd3eb1b0_0. - defusedxml=0.6.0=py_0. - entrypoints=0.3=py37_0. - expat=2.2.10=he",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1625
https://github.com/scverse/scanpy/issues/1625:561,usability,user,users,561,"gene ranking produces completely wrong results on the most recent tool/library combinations on specific dataset; - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. # Introduction. Hi,. so this is a weird one and I could not track it down yet. The Schillerlab people have a workstation known as ""agando"". On this workstation the full environment is installed globally and shared by all users. I am looking to change that. # The issue. When calculating the `sc.tl.marker_gene_overlap` I get the expected and reasonable results on the agando environment, but completely rubbish results when running the same code with a fresh Conda environment and the latest dependencies installed. ![image](https://user-images.githubusercontent.com/21954664/106739402-659dfb80-6619-11eb-84f1-e75abfa6167d.png). Top = new, trash results. Bottom = old=agando expected results. The old environment has:. ```. scanpy==1.6.1.dev110+gb4234d81 anndata==0.7.4 umap==0.4.6 numpy==1.19.0 scipy==1.5.1 pandas==1.1.5 scikit-learn==0.23.1 statsmodels==0.12.1 python-igraph==0.8.0 louvain==0.6.1 leidenalg==0.8.3. ```. The new environment has . ```. scanpy==1.6.1 anndata==0.7.5 umap==0.4.6 numpy==1.20.0 scipy==1.6.0 pandas==1.2.1 scikit-learn==0.24.1 statsmodels==0.12.1 python-igraph==0.8.3 louvain==0.7.0 leidenalg==0.8.3. ```. Full new conda environment:. ```. name: single_cell_analysis. channels:. - defaults. dependencies:. - _libgcc_mutex=0.1=main. - argon2-cffi=20.1.0=py37h7b6447c_1. - async_generator=1.10=py37h28b3542_0. - attrs=20.3.0=pyhd3eb1b0_0. - backcall=0.2.0=pyhd3eb1b0_0. - bleach=3.3.0=pyhd3eb1b0_0. - ca-certificates=2021.1.19=h06a4308_0. - certifi=2020.12.5=py37h06a4308_0. - cffi=1.14.4=py37h261ae71_0. - dbus=1.13.18=hb2f20db_0. - decorator=4.4.2=pyhd3eb1b0_0. - defusedxml=0.6.0=py_0. - entrypoints=0.3=py37_0. - expat=2.2.10=he",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1625
https://github.com/scverse/scanpy/issues/1625:873,usability,user,user-images,873,"gene ranking produces completely wrong results on the most recent tool/library combinations on specific dataset; - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. # Introduction. Hi,. so this is a weird one and I could not track it down yet. The Schillerlab people have a workstation known as ""agando"". On this workstation the full environment is installed globally and shared by all users. I am looking to change that. # The issue. When calculating the `sc.tl.marker_gene_overlap` I get the expected and reasonable results on the agando environment, but completely rubbish results when running the same code with a fresh Conda environment and the latest dependencies installed. ![image](https://user-images.githubusercontent.com/21954664/106739402-659dfb80-6619-11eb-84f1-e75abfa6167d.png). Top = new, trash results. Bottom = old=agando expected results. The old environment has:. ```. scanpy==1.6.1.dev110+gb4234d81 anndata==0.7.4 umap==0.4.6 numpy==1.19.0 scipy==1.5.1 pandas==1.1.5 scikit-learn==0.23.1 statsmodels==0.12.1 python-igraph==0.8.0 louvain==0.6.1 leidenalg==0.8.3. ```. The new environment has . ```. scanpy==1.6.1 anndata==0.7.5 umap==0.4.6 numpy==1.20.0 scipy==1.6.0 pandas==1.2.1 scikit-learn==0.24.1 statsmodels==0.12.1 python-igraph==0.8.3 louvain==0.7.0 leidenalg==0.8.3. ```. Full new conda environment:. ```. name: single_cell_analysis. channels:. - defaults. dependencies:. - _libgcc_mutex=0.1=main. - argon2-cffi=20.1.0=py37h7b6447c_1. - async_generator=1.10=py37h28b3542_0. - attrs=20.3.0=pyhd3eb1b0_0. - backcall=0.2.0=pyhd3eb1b0_0. - bleach=3.3.0=pyhd3eb1b0_0. - ca-certificates=2021.1.19=h06a4308_0. - certifi=2020.12.5=py37h06a4308_0. - cffi=1.14.4=py37h261ae71_0. - dbus=1.13.18=hb2f20db_0. - decorator=4.4.2=pyhd3eb1b0_0. - defusedxml=0.6.0=py_0. - entrypoints=0.3=py37_0. - expat=2.2.10=he",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1625
https://github.com/scverse/scanpy/issues/1625:1170,usability,learn,learn,1170,"ported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. # Introduction. Hi,. so this is a weird one and I could not track it down yet. The Schillerlab people have a workstation known as ""agando"". On this workstation the full environment is installed globally and shared by all users. I am looking to change that. # The issue. When calculating the `sc.tl.marker_gene_overlap` I get the expected and reasonable results on the agando environment, but completely rubbish results when running the same code with a fresh Conda environment and the latest dependencies installed. ![image](https://user-images.githubusercontent.com/21954664/106739402-659dfb80-6619-11eb-84f1-e75abfa6167d.png). Top = new, trash results. Bottom = old=agando expected results. The old environment has:. ```. scanpy==1.6.1.dev110+gb4234d81 anndata==0.7.4 umap==0.4.6 numpy==1.19.0 scipy==1.5.1 pandas==1.1.5 scikit-learn==0.23.1 statsmodels==0.12.1 python-igraph==0.8.0 louvain==0.6.1 leidenalg==0.8.3. ```. The new environment has . ```. scanpy==1.6.1 anndata==0.7.5 umap==0.4.6 numpy==1.20.0 scipy==1.6.0 pandas==1.2.1 scikit-learn==0.24.1 statsmodels==0.12.1 python-igraph==0.8.3 louvain==0.7.0 leidenalg==0.8.3. ```. Full new conda environment:. ```. name: single_cell_analysis. channels:. - defaults. dependencies:. - _libgcc_mutex=0.1=main. - argon2-cffi=20.1.0=py37h7b6447c_1. - async_generator=1.10=py37h28b3542_0. - attrs=20.3.0=pyhd3eb1b0_0. - backcall=0.2.0=pyhd3eb1b0_0. - bleach=3.3.0=pyhd3eb1b0_0. - ca-certificates=2021.1.19=h06a4308_0. - certifi=2020.12.5=py37h06a4308_0. - cffi=1.14.4=py37h261ae71_0. - dbus=1.13.18=hb2f20db_0. - decorator=4.4.2=pyhd3eb1b0_0. - defusedxml=0.6.0=py_0. - entrypoints=0.3=py37_0. - expat=2.2.10=he6710b0_2. - fontconfig=2.13.0=h9420a91_0. - freetype=2.10.4=h5ab3b9f_0. - glib=2.66.1=h92f7085_0. - gst-plugins-base=1.14.0=h8213a91_2. - gstreamer=1.14.0=h28cd5cc_2. - icu=",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1625
https://github.com/scverse/scanpy/issues/1625:1383,usability,learn,learn,1383,"d I could not track it down yet. The Schillerlab people have a workstation known as ""agando"". On this workstation the full environment is installed globally and shared by all users. I am looking to change that. # The issue. When calculating the `sc.tl.marker_gene_overlap` I get the expected and reasonable results on the agando environment, but completely rubbish results when running the same code with a fresh Conda environment and the latest dependencies installed. ![image](https://user-images.githubusercontent.com/21954664/106739402-659dfb80-6619-11eb-84f1-e75abfa6167d.png). Top = new, trash results. Bottom = old=agando expected results. The old environment has:. ```. scanpy==1.6.1.dev110+gb4234d81 anndata==0.7.4 umap==0.4.6 numpy==1.19.0 scipy==1.5.1 pandas==1.1.5 scikit-learn==0.23.1 statsmodels==0.12.1 python-igraph==0.8.0 louvain==0.6.1 leidenalg==0.8.3. ```. The new environment has . ```. scanpy==1.6.1 anndata==0.7.5 umap==0.4.6 numpy==1.20.0 scipy==1.6.0 pandas==1.2.1 scikit-learn==0.24.1 statsmodels==0.12.1 python-igraph==0.8.3 louvain==0.7.0 leidenalg==0.8.3. ```. Full new conda environment:. ```. name: single_cell_analysis. channels:. - defaults. dependencies:. - _libgcc_mutex=0.1=main. - argon2-cffi=20.1.0=py37h7b6447c_1. - async_generator=1.10=py37h28b3542_0. - attrs=20.3.0=pyhd3eb1b0_0. - backcall=0.2.0=pyhd3eb1b0_0. - bleach=3.3.0=pyhd3eb1b0_0. - ca-certificates=2021.1.19=h06a4308_0. - certifi=2020.12.5=py37h06a4308_0. - cffi=1.14.4=py37h261ae71_0. - dbus=1.13.18=hb2f20db_0. - decorator=4.4.2=pyhd3eb1b0_0. - defusedxml=0.6.0=py_0. - entrypoints=0.3=py37_0. - expat=2.2.10=he6710b0_2. - fontconfig=2.13.0=h9420a91_0. - freetype=2.10.4=h5ab3b9f_0. - glib=2.66.1=h92f7085_0. - gst-plugins-base=1.14.0=h8213a91_2. - gstreamer=1.14.0=h28cd5cc_2. - icu=58.2=he6710b0_3. - importlib_metadata=2.0.0=1. - ipykernel=5.3.4=py37h5ca1d4c_0. - ipython=7.20.0=py37hb070fc8_1. - ipython_genutils=0.2.0=pyhd3eb1b0_1. - ipywidgets=7.6.3=pyhd3eb1b0_1. - jedi=0.17.0=py37_0. - jin",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1625
https://github.com/scverse/scanpy/issues/1625:3587,usability,tool,toolkit,3587,pygments=0.1.2=py_0. - jupyterlab_widgets=1.0.0=pyhd3eb1b0_1. - ld_impl_linux-64=2.33.1=h53a641e_7. - libedit=3.1.20191231=h14c3975_1. - libffi=3.3=he6710b0_2. - libgcc-ng=9.1.0=hdf63c60_0. - libpng=1.6.37=hbc83047_0. - libsodium=1.0.18=h7b6447c_0. - libstdcxx-ng=9.1.0=hdf63c60_0. - libuuid=1.0.3=h1bed415_2. - libxcb=1.14=h7b6447c_0. - libxml2=2.9.10=hb55368b_3. - markupsafe=1.1.1=py37h14c3975_1. - mistune=0.8.4=py37h14c3975_1001. - nb_conda=2.2.1=py37_0. - nb_conda_kernels=2.3.1=py37h06a4308_0. - nbclient=0.5.1=py_0. - nbconvert=6.0.7=py37_0. - nbformat=5.1.2=pyhd3eb1b0_1. - ncurses=6.2=he6710b0_1. - nest-asyncio=1.4.3=pyhd3eb1b0_0. - notebook=6.2.0=py37h06a4308_0. - openssl=1.1.1i=h27cfd23_0. - packaging=20.9=pyhd3eb1b0_0. - pandoc=2.11=hb0f4dca_0. - pandocfilters=1.4.3=py37h06a4308_1. - parso=0.8.1=pyhd3eb1b0_0. - pcre=8.44=he6710b0_0. - pexpect=4.8.0=pyhd3eb1b0_3. - pickleshare=0.7.5=pyhd3eb1b0_1003. - pip=20.3.3=py37h06a4308_0. - prometheus_client=0.9.0=pyhd3eb1b0_0. - prompt-toolkit=3.0.8=py_0. - prompt_toolkit=3.0.8=0. - ptyprocess=0.7.0=pyhd3eb1b0_2. - pycparser=2.20=py_2. - pygments=2.7.4=pyhd3eb1b0_0. - pyparsing=2.4.7=pyhd3eb1b0_0. - pyqt=5.9.2=py37h05f1152_2. - pyrsistent=0.17.3=py37h7b6447c_0. - python=3.7.9=h7579374_0. - python-dateutil=2.8.1=pyhd3eb1b0_0. - pyzmq=20.0.0=py37h2531618_1. - qt=5.9.7=h5867ecd_1. - qtconsole=4.7.7=py_0. - qtpy=1.9.0=py_0. - readline=8.1=h27cfd23_0. - send2trash=1.5.0=pyhd3eb1b0_1. - setuptools=52.0.0=py37h06a4308_0. - sip=4.19.8=py37hf484d3e_0. - six=1.15.0=py37h06a4308_0. - sqlite=3.33.0=h62c20be_0. - terminado=0.9.2=py37h06a4308_0. - testpath=0.4.4=pyhd3eb1b0_0. - tk=8.6.10=hbc83047_0. - tornado=6.1=py37h27cfd23_0. - traitlets=5.0.5=pyhd3eb1b0_0. - wcwidth=0.2.5=py_0. - webencodings=0.5.1=py37_1. - wheel=0.36.2=pyhd3eb1b0_0. - widgetsnbextension=3.5.1=py37_0. - xz=5.2.5=h7b6447c_0. - zeromq=4.3.3=he6710b0_3. - zipp=3.4.0=pyhd3eb1b0_0. - zlib=1.2.11=h7b6447c_3. - pip:. - anndata==0.7.5. - cached-property==1.5.2. - click==,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1625
https://github.com/scverse/scanpy/issues/1625:4394,usability,widget,widgetsnbextension,4394,=pyhd3eb1b0_0. - pcre=8.44=he6710b0_0. - pexpect=4.8.0=pyhd3eb1b0_3. - pickleshare=0.7.5=pyhd3eb1b0_1003. - pip=20.3.3=py37h06a4308_0. - prometheus_client=0.9.0=pyhd3eb1b0_0. - prompt-toolkit=3.0.8=py_0. - prompt_toolkit=3.0.8=0. - ptyprocess=0.7.0=pyhd3eb1b0_2. - pycparser=2.20=py_2. - pygments=2.7.4=pyhd3eb1b0_0. - pyparsing=2.4.7=pyhd3eb1b0_0. - pyqt=5.9.2=py37h05f1152_2. - pyrsistent=0.17.3=py37h7b6447c_0. - python=3.7.9=h7579374_0. - python-dateutil=2.8.1=pyhd3eb1b0_0. - pyzmq=20.0.0=py37h2531618_1. - qt=5.9.7=h5867ecd_1. - qtconsole=4.7.7=py_0. - qtpy=1.9.0=py_0. - readline=8.1=h27cfd23_0. - send2trash=1.5.0=pyhd3eb1b0_1. - setuptools=52.0.0=py37h06a4308_0. - sip=4.19.8=py37hf484d3e_0. - six=1.15.0=py37h06a4308_0. - sqlite=3.33.0=h62c20be_0. - terminado=0.9.2=py37h06a4308_0. - testpath=0.4.4=pyhd3eb1b0_0. - tk=8.6.10=hbc83047_0. - tornado=6.1=py37h27cfd23_0. - traitlets=5.0.5=pyhd3eb1b0_0. - wcwidth=0.2.5=py_0. - webencodings=0.5.1=py37_1. - wheel=0.36.2=pyhd3eb1b0_0. - widgetsnbextension=3.5.1=py37_0. - xz=5.2.5=h7b6447c_0. - zeromq=4.3.3=he6710b0_3. - zipp=3.4.0=pyhd3eb1b0_0. - zlib=1.2.11=h7b6447c_3. - pip:. - anndata==0.7.5. - cached-property==1.5.2. - click==7.1.2. - cycler==0.10.0. - get-version==2.1. - h5py==3.1.0. - importlib-metadata==3.4.0. - joblib==1.0.0. - kiwisolver==1.3.1. - legacy-api-wrap==1.2. - leidenalg==0.8.3. - llvmlite==0.35.0. - loompy==3.0.6. - louvain==0.7.0. - matplotlib==3.3.4. - natsort==7.1.1. - networkx==2.5. - numba==0.52.0. - numexpr==2.7.2. - numpy==1.20.0. - numpy-groupies==0.9.13. - pandas==1.2.1. - patsy==0.5.1. - pillow==8.1.0. - python-igraph==0.8.3. - pytz==2021.1. - scanpy==1.6.1. - scikit-learn==0.24.1. - scipy==1.6.0. - scvelo==0.2.2. - seaborn==0.11.1. - setuptools-scm==5.0.1. - sinfo==0.3.1. - statsmodels==0.12.1. - stdlib-list==0.8.0. - tables==3.6.1. - texttable==1.6.3. - threadpoolctl==2.1.0. - tqdm==4.56.0. - typing-extensions==3.7.4.3. - umap-learn==0.4.6. ```. I can reproduce the issue with a Docker container ,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1625
https://github.com/scverse/scanpy/issues/1625:5067,usability,learn,learn,5067,"8_0. - sip=4.19.8=py37hf484d3e_0. - six=1.15.0=py37h06a4308_0. - sqlite=3.33.0=h62c20be_0. - terminado=0.9.2=py37h06a4308_0. - testpath=0.4.4=pyhd3eb1b0_0. - tk=8.6.10=hbc83047_0. - tornado=6.1=py37h27cfd23_0. - traitlets=5.0.5=pyhd3eb1b0_0. - wcwidth=0.2.5=py_0. - webencodings=0.5.1=py37_1. - wheel=0.36.2=pyhd3eb1b0_0. - widgetsnbextension=3.5.1=py37_0. - xz=5.2.5=h7b6447c_0. - zeromq=4.3.3=he6710b0_3. - zipp=3.4.0=pyhd3eb1b0_0. - zlib=1.2.11=h7b6447c_3. - pip:. - anndata==0.7.5. - cached-property==1.5.2. - click==7.1.2. - cycler==0.10.0. - get-version==2.1. - h5py==3.1.0. - importlib-metadata==3.4.0. - joblib==1.0.0. - kiwisolver==1.3.1. - legacy-api-wrap==1.2. - leidenalg==0.8.3. - llvmlite==0.35.0. - loompy==3.0.6. - louvain==0.7.0. - matplotlib==3.3.4. - natsort==7.1.1. - networkx==2.5. - numba==0.52.0. - numexpr==2.7.2. - numpy==1.20.0. - numpy-groupies==0.9.13. - pandas==1.2.1. - patsy==0.5.1. - pillow==8.1.0. - python-igraph==0.8.3. - pytz==2021.1. - scanpy==1.6.1. - scikit-learn==0.24.1. - scipy==1.6.0. - scvelo==0.2.2. - seaborn==0.11.1. - setuptools-scm==5.0.1. - sinfo==0.3.1. - statsmodels==0.12.1. - stdlib-list==0.8.0. - tables==3.6.1. - texttable==1.6.3. - threadpoolctl==2.1.0. - tqdm==4.56.0. - typing-extensions==3.7.4.3. - umap-learn==0.4.6. ```. I can reproduce the issue with a Docker container that only has the minimal conda environment above. Additionally, I already tried installing the exact same dependency versions in the new environment, but got the same results. . If you need access to the data and the container please contact me and I will make it available to you. The data is already at the ICB cluster. Code:. ```. from os import path. import numpy as np. import matplotlib.pyplot as plt. import scanpy as sc. import scanpy.external as sce. from os import listdir. import pandas as pd. import seaborn as sb. import datetime, time. import scvelo as scv. from matplotlib.colors import LinearSegmentedColormap. #Define a nice colour map for gene expr",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1625
https://github.com/scverse/scanpy/issues/1625:5334,usability,learn,learn,5334,"ebencodings=0.5.1=py37_1. - wheel=0.36.2=pyhd3eb1b0_0. - widgetsnbextension=3.5.1=py37_0. - xz=5.2.5=h7b6447c_0. - zeromq=4.3.3=he6710b0_3. - zipp=3.4.0=pyhd3eb1b0_0. - zlib=1.2.11=h7b6447c_3. - pip:. - anndata==0.7.5. - cached-property==1.5.2. - click==7.1.2. - cycler==0.10.0. - get-version==2.1. - h5py==3.1.0. - importlib-metadata==3.4.0. - joblib==1.0.0. - kiwisolver==1.3.1. - legacy-api-wrap==1.2. - leidenalg==0.8.3. - llvmlite==0.35.0. - loompy==3.0.6. - louvain==0.7.0. - matplotlib==3.3.4. - natsort==7.1.1. - networkx==2.5. - numba==0.52.0. - numexpr==2.7.2. - numpy==1.20.0. - numpy-groupies==0.9.13. - pandas==1.2.1. - patsy==0.5.1. - pillow==8.1.0. - python-igraph==0.8.3. - pytz==2021.1. - scanpy==1.6.1. - scikit-learn==0.24.1. - scipy==1.6.0. - scvelo==0.2.2. - seaborn==0.11.1. - setuptools-scm==5.0.1. - sinfo==0.3.1. - statsmodels==0.12.1. - stdlib-list==0.8.0. - tables==3.6.1. - texttable==1.6.3. - threadpoolctl==2.1.0. - tqdm==4.56.0. - typing-extensions==3.7.4.3. - umap-learn==0.4.6. ```. I can reproduce the issue with a Docker container that only has the minimal conda environment above. Additionally, I already tried installing the exact same dependency versions in the new environment, but got the same results. . If you need access to the data and the container please contact me and I will make it available to you. The data is already at the ICB cluster. Code:. ```. from os import path. import numpy as np. import matplotlib.pyplot as plt. import scanpy as sc. import scanpy.external as sce. from os import listdir. import pandas as pd. import seaborn as sb. import datetime, time. import scvelo as scv. from matplotlib.colors import LinearSegmentedColormap. #Define a nice colour map for gene expression. from matplotlib import colors. def timestamp():. ts = time.time(). st = datetime.datetime.fromtimestamp(ts).strftime('%d-%m-%Y %H:%M:%S'). return st. # Exporting folder. folder = ""/output"". sc.settings.figdir = folder + ""Plots/"". sc.set_figure_params(vector_f",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1625
https://github.com/scverse/scanpy/issues/1625:5421,usability,minim,minimal,5421,". - xz=5.2.5=h7b6447c_0. - zeromq=4.3.3=he6710b0_3. - zipp=3.4.0=pyhd3eb1b0_0. - zlib=1.2.11=h7b6447c_3. - pip:. - anndata==0.7.5. - cached-property==1.5.2. - click==7.1.2. - cycler==0.10.0. - get-version==2.1. - h5py==3.1.0. - importlib-metadata==3.4.0. - joblib==1.0.0. - kiwisolver==1.3.1. - legacy-api-wrap==1.2. - leidenalg==0.8.3. - llvmlite==0.35.0. - loompy==3.0.6. - louvain==0.7.0. - matplotlib==3.3.4. - natsort==7.1.1. - networkx==2.5. - numba==0.52.0. - numexpr==2.7.2. - numpy==1.20.0. - numpy-groupies==0.9.13. - pandas==1.2.1. - patsy==0.5.1. - pillow==8.1.0. - python-igraph==0.8.3. - pytz==2021.1. - scanpy==1.6.1. - scikit-learn==0.24.1. - scipy==1.6.0. - scvelo==0.2.2. - seaborn==0.11.1. - setuptools-scm==5.0.1. - sinfo==0.3.1. - statsmodels==0.12.1. - stdlib-list==0.8.0. - tables==3.6.1. - texttable==1.6.3. - threadpoolctl==2.1.0. - tqdm==4.56.0. - typing-extensions==3.7.4.3. - umap-learn==0.4.6. ```. I can reproduce the issue with a Docker container that only has the minimal conda environment above. Additionally, I already tried installing the exact same dependency versions in the new environment, but got the same results. . If you need access to the data and the container please contact me and I will make it available to you. The data is already at the ICB cluster. Code:. ```. from os import path. import numpy as np. import matplotlib.pyplot as plt. import scanpy as sc. import scanpy.external as sce. from os import listdir. import pandas as pd. import seaborn as sb. import datetime, time. import scvelo as scv. from matplotlib.colors import LinearSegmentedColormap. #Define a nice colour map for gene expression. from matplotlib import colors. def timestamp():. ts = time.time(). st = datetime.datetime.fromtimestamp(ts).strftime('%d-%m-%Y %H:%M:%S'). return st. # Exporting folder. folder = ""/output"". sc.settings.figdir = folder + ""Plots/"". sc.set_figure_params(vector_friendly = True, dpi=300). sc.settings.verbosity = 3 # verbosity: errors (0), warnings (1",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1625
https://github.com/scverse/scanpy/issues/1625:6402,usability,error,errors,6402,"at only has the minimal conda environment above. Additionally, I already tried installing the exact same dependency versions in the new environment, but got the same results. . If you need access to the data and the container please contact me and I will make it available to you. The data is already at the ICB cluster. Code:. ```. from os import path. import numpy as np. import matplotlib.pyplot as plt. import scanpy as sc. import scanpy.external as sce. from os import listdir. import pandas as pd. import seaborn as sb. import datetime, time. import scvelo as scv. from matplotlib.colors import LinearSegmentedColormap. #Define a nice colour map for gene expression. from matplotlib import colors. def timestamp():. ts = time.time(). st = datetime.datetime.fromtimestamp(ts).strftime('%d-%m-%Y %H:%M:%S'). return st. # Exporting folder. folder = ""/output"". sc.settings.figdir = folder + ""Plots/"". sc.set_figure_params(vector_friendly = True, dpi=300). sc.settings.verbosity = 3 # verbosity: errors (0), warnings (1), info (2), hints (3). sc.logging.print_version_and_date(). sc.logging.print_header(). adata = sc.read(""/data/190924_Recreated_Virus_Object_regressed.h5ad""). #adata.write(folder + ""190924_Recreated_Virus_Object_regressed.h5ad""). sc.tl.louvain(adata, resolution = 4, key_added = ""louvain_2""). sc.tl.louvain(adata, resolution = 5, key_added = ""louvain_3""). sc.tl.louvain(adata, resolution = 6, key_added = ""louvain_4""). sc.tl.louvain(adata, resolution = 7, key_added = ""louvain_5""). sc.pl.umap(adata, color = [""louvain_2"", ""louvain_3"", ""louvain_4"", ""louvain_5""], wspace = 0.45). #select resolution. print(adata.obs[""louvain_5""].value_counts()). sc.tl.rank_genes_groups(adata, groupby = ""louvain_5""). # read all arkers table from known annotated data. marker_folder = ""/marker/"". marker_table = pd.read_csv(marker_folder + ""Particle_AllMarkers.txt"", sep = ""\t"", index_col = None). marker_table.head(2). ## Restrict to Foldchange and P value. marker_table = marker_table[(marker_tabl",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1625
https://github.com/scverse/scanpy/issues/1625:6438,usability,hint,hints,6438,"nment above. Additionally, I already tried installing the exact same dependency versions in the new environment, but got the same results. . If you need access to the data and the container please contact me and I will make it available to you. The data is already at the ICB cluster. Code:. ```. from os import path. import numpy as np. import matplotlib.pyplot as plt. import scanpy as sc. import scanpy.external as sce. from os import listdir. import pandas as pd. import seaborn as sb. import datetime, time. import scvelo as scv. from matplotlib.colors import LinearSegmentedColormap. #Define a nice colour map for gene expression. from matplotlib import colors. def timestamp():. ts = time.time(). st = datetime.datetime.fromtimestamp(ts).strftime('%d-%m-%Y %H:%M:%S'). return st. # Exporting folder. folder = ""/output"". sc.settings.figdir = folder + ""Plots/"". sc.set_figure_params(vector_friendly = True, dpi=300). sc.settings.verbosity = 3 # verbosity: errors (0), warnings (1), info (2), hints (3). sc.logging.print_version_and_date(). sc.logging.print_header(). adata = sc.read(""/data/190924_Recreated_Virus_Object_regressed.h5ad""). #adata.write(folder + ""190924_Recreated_Virus_Object_regressed.h5ad""). sc.tl.louvain(adata, resolution = 4, key_added = ""louvain_2""). sc.tl.louvain(adata, resolution = 5, key_added = ""louvain_3""). sc.tl.louvain(adata, resolution = 6, key_added = ""louvain_4""). sc.tl.louvain(adata, resolution = 7, key_added = ""louvain_5""). sc.pl.umap(adata, color = [""louvain_2"", ""louvain_3"", ""louvain_4"", ""louvain_5""], wspace = 0.45). #select resolution. print(adata.obs[""louvain_5""].value_counts()). sc.tl.rank_genes_groups(adata, groupby = ""louvain_5""). # read all arkers table from known annotated data. marker_folder = ""/marker/"". marker_table = pd.read_csv(marker_folder + ""Particle_AllMarkers.txt"", sep = ""\t"", index_col = None). marker_table.head(2). ## Restrict to Foldchange and P value. marker_table = marker_table[(marker_table.logfoldchange > 2) & (marker_table",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1625
https://github.com/scverse/scanpy/issues/1626:63,interoperability,specif,specifically,63,"What is the random number generator used by scanpy?; Hi,. I am specifically trying to test something with random_state in sc.tl.umap, and would like to get more info on what random number generator is used. The docs there say that np.random is used if random_state=None, but does not explicitly comment on what the random number generator is otherwise. Could you please help clarify this? Thank you! Byron. <!--.  If you need help using Scanpy, please ask in https://scanpy.discourse.group/ instead . If you want to know about design decisions and the like, please ask below:. -->. ...",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1626
https://github.com/scverse/scanpy/issues/1626:529,modifiability,design decis,design decisions,529,"What is the random number generator used by scanpy?; Hi,. I am specifically trying to test something with random_state in sc.tl.umap, and would like to get more info on what random number generator is used. The docs there say that np.random is used if random_state=None, but does not explicitly comment on what the random number generator is otherwise. Could you please help clarify this? Thank you! Byron. <!--.  If you need help using Scanpy, please ask in https://scanpy.discourse.group/ instead . If you want to know about design decisions and the like, please ask below:. -->. ...",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1626
https://github.com/scverse/scanpy/issues/1626:275,reliability,doe,does,275,"What is the random number generator used by scanpy?; Hi,. I am specifically trying to test something with random_state in sc.tl.umap, and would like to get more info on what random number generator is used. The docs there say that np.random is used if random_state=None, but does not explicitly comment on what the random number generator is otherwise. Could you please help clarify this? Thank you! Byron. <!--.  If you need help using Scanpy, please ask in https://scanpy.discourse.group/ instead . If you want to know about design decisions and the like, please ask below:. -->. ...",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1626
https://github.com/scverse/scanpy/issues/1626:86,safety,test,test,86,"What is the random number generator used by scanpy?; Hi,. I am specifically trying to test something with random_state in sc.tl.umap, and would like to get more info on what random number generator is used. The docs there say that np.random is used if random_state=None, but does not explicitly comment on what the random number generator is otherwise. Could you please help clarify this? Thank you! Byron. <!--.  If you need help using Scanpy, please ask in https://scanpy.discourse.group/ instead . If you want to know about design decisions and the like, please ask below:. -->. ...",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1626
https://github.com/scverse/scanpy/issues/1626:86,testability,test,test,86,"What is the random number generator used by scanpy?; Hi,. I am specifically trying to test something with random_state in sc.tl.umap, and would like to get more info on what random number generator is used. The docs there say that np.random is used if random_state=None, but does not explicitly comment on what the random number generator is otherwise. Could you please help clarify this? Thank you! Byron. <!--.  If you need help using Scanpy, please ask in https://scanpy.discourse.group/ instead . If you want to know about design decisions and the like, please ask below:. -->. ...",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1626
https://github.com/scverse/scanpy/issues/1626:370,usability,help,help,370,"What is the random number generator used by scanpy?; Hi,. I am specifically trying to test something with random_state in sc.tl.umap, and would like to get more info on what random number generator is used. The docs there say that np.random is used if random_state=None, but does not explicitly comment on what the random number generator is otherwise. Could you please help clarify this? Thank you! Byron. <!--.  If you need help using Scanpy, please ask in https://scanpy.discourse.group/ instead . If you want to know about design decisions and the like, please ask below:. -->. ...",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1626
https://github.com/scverse/scanpy/issues/1626:427,usability,help,help,427,"What is the random number generator used by scanpy?; Hi,. I am specifically trying to test something with random_state in sc.tl.umap, and would like to get more info on what random number generator is used. The docs there say that np.random is used if random_state=None, but does not explicitly comment on what the random number generator is otherwise. Could you please help clarify this? Thank you! Byron. <!--.  If you need help using Scanpy, please ask in https://scanpy.discourse.group/ instead . If you want to know about design decisions and the like, please ask below:. -->. ...",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1626
https://github.com/scverse/scanpy/issues/1627:15,deployability,api,api,15,"Remove `scanpy.api`; Finally get around to deleting `scanpy.api`. It's occasionally causing issues, makes docs more complicated and has been deprecated for a while.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1627
https://github.com/scverse/scanpy/issues/1627:60,deployability,api,api,60,"Remove `scanpy.api`; Finally get around to deleting `scanpy.api`. It's occasionally causing issues, makes docs more complicated and has been deprecated for a while.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1627
https://github.com/scverse/scanpy/issues/1627:15,integrability,api,api,15,"Remove `scanpy.api`; Finally get around to deleting `scanpy.api`. It's occasionally causing issues, makes docs more complicated and has been deprecated for a while.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1627
https://github.com/scverse/scanpy/issues/1627:60,integrability,api,api,60,"Remove `scanpy.api`; Finally get around to deleting `scanpy.api`. It's occasionally causing issues, makes docs more complicated and has been deprecated for a while.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1627
https://github.com/scverse/scanpy/issues/1627:15,interoperability,api,api,15,"Remove `scanpy.api`; Finally get around to deleting `scanpy.api`. It's occasionally causing issues, makes docs more complicated and has been deprecated for a while.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1627
https://github.com/scverse/scanpy/issues/1627:60,interoperability,api,api,60,"Remove `scanpy.api`; Finally get around to deleting `scanpy.api`. It's occasionally causing issues, makes docs more complicated and has been deprecated for a while.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1627
https://github.com/scverse/scanpy/issues/1627:116,safety,compl,complicated,116,"Remove `scanpy.api`; Finally get around to deleting `scanpy.api`. It's occasionally causing issues, makes docs more complicated and has been deprecated for a while.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1627
https://github.com/scverse/scanpy/issues/1627:116,security,compl,complicated,116,"Remove `scanpy.api`; Finally get around to deleting `scanpy.api`. It's occasionally causing issues, makes docs more complicated and has been deprecated for a while.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1627
https://github.com/scverse/scanpy/pull/1628:0,deployability,Releas,Release,0,"Release notes reorganization; @flying-sheep @falexwolf this is a first stab at reorganizing the release notes to facilitate bugfix releases. Using the example of the current release cycle:. The idea is that PRs which are bug fixes would have their change added to the `1.7.1.rst` release notes. New features are added to `1.8.0.rst` (which will be added in a follow up PR, since this pr will be back ported to the `1.7.x` branch). This way we don't mix all changes on master in one file, meaning we don't have to figure out what changes are included when making a release, as this information has been collected ahead of time. What I'd like to happen, is that `latest` builds will show changes for unreleased `1.7.1` and `1.8.0`, while the the `1.7.x` branch just shows `1.7.1`. ## TODO. - [x] Figure out how exactly release-latest works. What are the latest additions for ""Latest build"" vs. ""Stable"" and how should they differ. - `release-latest.rst` will differ between stable and dev branches. - [x] Why isn't release latest being added right to `docs/index.rst`. - [x] Process for adding new release note files should be added to dev docs. - [x] Fix links to PAGA website",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1628
https://github.com/scverse/scanpy/pull/1628:96,deployability,releas,release,96,"Release notes reorganization; @flying-sheep @falexwolf this is a first stab at reorganizing the release notes to facilitate bugfix releases. Using the example of the current release cycle:. The idea is that PRs which are bug fixes would have their change added to the `1.7.1.rst` release notes. New features are added to `1.8.0.rst` (which will be added in a follow up PR, since this pr will be back ported to the `1.7.x` branch). This way we don't mix all changes on master in one file, meaning we don't have to figure out what changes are included when making a release, as this information has been collected ahead of time. What I'd like to happen, is that `latest` builds will show changes for unreleased `1.7.1` and `1.8.0`, while the the `1.7.x` branch just shows `1.7.1`. ## TODO. - [x] Figure out how exactly release-latest works. What are the latest additions for ""Latest build"" vs. ""Stable"" and how should they differ. - `release-latest.rst` will differ between stable and dev branches. - [x] Why isn't release latest being added right to `docs/index.rst`. - [x] Process for adding new release note files should be added to dev docs. - [x] Fix links to PAGA website",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1628
https://github.com/scverse/scanpy/pull/1628:131,deployability,releas,releases,131,"Release notes reorganization; @flying-sheep @falexwolf this is a first stab at reorganizing the release notes to facilitate bugfix releases. Using the example of the current release cycle:. The idea is that PRs which are bug fixes would have their change added to the `1.7.1.rst` release notes. New features are added to `1.8.0.rst` (which will be added in a follow up PR, since this pr will be back ported to the `1.7.x` branch). This way we don't mix all changes on master in one file, meaning we don't have to figure out what changes are included when making a release, as this information has been collected ahead of time. What I'd like to happen, is that `latest` builds will show changes for unreleased `1.7.1` and `1.8.0`, while the the `1.7.x` branch just shows `1.7.1`. ## TODO. - [x] Figure out how exactly release-latest works. What are the latest additions for ""Latest build"" vs. ""Stable"" and how should they differ. - `release-latest.rst` will differ between stable and dev branches. - [x] Why isn't release latest being added right to `docs/index.rst`. - [x] Process for adding new release note files should be added to dev docs. - [x] Fix links to PAGA website",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1628
https://github.com/scverse/scanpy/pull/1628:174,deployability,releas,release,174,"Release notes reorganization; @flying-sheep @falexwolf this is a first stab at reorganizing the release notes to facilitate bugfix releases. Using the example of the current release cycle:. The idea is that PRs which are bug fixes would have their change added to the `1.7.1.rst` release notes. New features are added to `1.8.0.rst` (which will be added in a follow up PR, since this pr will be back ported to the `1.7.x` branch). This way we don't mix all changes on master in one file, meaning we don't have to figure out what changes are included when making a release, as this information has been collected ahead of time. What I'd like to happen, is that `latest` builds will show changes for unreleased `1.7.1` and `1.8.0`, while the the `1.7.x` branch just shows `1.7.1`. ## TODO. - [x] Figure out how exactly release-latest works. What are the latest additions for ""Latest build"" vs. ""Stable"" and how should they differ. - `release-latest.rst` will differ between stable and dev branches. - [x] Why isn't release latest being added right to `docs/index.rst`. - [x] Process for adding new release note files should be added to dev docs. - [x] Fix links to PAGA website",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1628
https://github.com/scverse/scanpy/pull/1628:280,deployability,releas,release,280,"Release notes reorganization; @flying-sheep @falexwolf this is a first stab at reorganizing the release notes to facilitate bugfix releases. Using the example of the current release cycle:. The idea is that PRs which are bug fixes would have their change added to the `1.7.1.rst` release notes. New features are added to `1.8.0.rst` (which will be added in a follow up PR, since this pr will be back ported to the `1.7.x` branch). This way we don't mix all changes on master in one file, meaning we don't have to figure out what changes are included when making a release, as this information has been collected ahead of time. What I'd like to happen, is that `latest` builds will show changes for unreleased `1.7.1` and `1.8.0`, while the the `1.7.x` branch just shows `1.7.1`. ## TODO. - [x] Figure out how exactly release-latest works. What are the latest additions for ""Latest build"" vs. ""Stable"" and how should they differ. - `release-latest.rst` will differ between stable and dev branches. - [x] Why isn't release latest being added right to `docs/index.rst`. - [x] Process for adding new release note files should be added to dev docs. - [x] Fix links to PAGA website",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1628
https://github.com/scverse/scanpy/pull/1628:564,deployability,releas,release,564,"Release notes reorganization; @flying-sheep @falexwolf this is a first stab at reorganizing the release notes to facilitate bugfix releases. Using the example of the current release cycle:. The idea is that PRs which are bug fixes would have their change added to the `1.7.1.rst` release notes. New features are added to `1.8.0.rst` (which will be added in a follow up PR, since this pr will be back ported to the `1.7.x` branch). This way we don't mix all changes on master in one file, meaning we don't have to figure out what changes are included when making a release, as this information has been collected ahead of time. What I'd like to happen, is that `latest` builds will show changes for unreleased `1.7.1` and `1.8.0`, while the the `1.7.x` branch just shows `1.7.1`. ## TODO. - [x] Figure out how exactly release-latest works. What are the latest additions for ""Latest build"" vs. ""Stable"" and how should they differ. - `release-latest.rst` will differ between stable and dev branches. - [x] Why isn't release latest being added right to `docs/index.rst`. - [x] Process for adding new release note files should be added to dev docs. - [x] Fix links to PAGA website",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1628
https://github.com/scverse/scanpy/pull/1628:669,deployability,build,builds,669,"Release notes reorganization; @flying-sheep @falexwolf this is a first stab at reorganizing the release notes to facilitate bugfix releases. Using the example of the current release cycle:. The idea is that PRs which are bug fixes would have their change added to the `1.7.1.rst` release notes. New features are added to `1.8.0.rst` (which will be added in a follow up PR, since this pr will be back ported to the `1.7.x` branch). This way we don't mix all changes on master in one file, meaning we don't have to figure out what changes are included when making a release, as this information has been collected ahead of time. What I'd like to happen, is that `latest` builds will show changes for unreleased `1.7.1` and `1.8.0`, while the the `1.7.x` branch just shows `1.7.1`. ## TODO. - [x] Figure out how exactly release-latest works. What are the latest additions for ""Latest build"" vs. ""Stable"" and how should they differ. - `release-latest.rst` will differ between stable and dev branches. - [x] Why isn't release latest being added right to `docs/index.rst`. - [x] Process for adding new release note files should be added to dev docs. - [x] Fix links to PAGA website",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1628
https://github.com/scverse/scanpy/pull/1628:817,deployability,releas,release-latest,817,"Release notes reorganization; @flying-sheep @falexwolf this is a first stab at reorganizing the release notes to facilitate bugfix releases. Using the example of the current release cycle:. The idea is that PRs which are bug fixes would have their change added to the `1.7.1.rst` release notes. New features are added to `1.8.0.rst` (which will be added in a follow up PR, since this pr will be back ported to the `1.7.x` branch). This way we don't mix all changes on master in one file, meaning we don't have to figure out what changes are included when making a release, as this information has been collected ahead of time. What I'd like to happen, is that `latest` builds will show changes for unreleased `1.7.1` and `1.8.0`, while the the `1.7.x` branch just shows `1.7.1`. ## TODO. - [x] Figure out how exactly release-latest works. What are the latest additions for ""Latest build"" vs. ""Stable"" and how should they differ. - `release-latest.rst` will differ between stable and dev branches. - [x] Why isn't release latest being added right to `docs/index.rst`. - [x] Process for adding new release note files should be added to dev docs. - [x] Fix links to PAGA website",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1628
https://github.com/scverse/scanpy/pull/1628:881,deployability,build,build,881,"Release notes reorganization; @flying-sheep @falexwolf this is a first stab at reorganizing the release notes to facilitate bugfix releases. Using the example of the current release cycle:. The idea is that PRs which are bug fixes would have their change added to the `1.7.1.rst` release notes. New features are added to `1.8.0.rst` (which will be added in a follow up PR, since this pr will be back ported to the `1.7.x` branch). This way we don't mix all changes on master in one file, meaning we don't have to figure out what changes are included when making a release, as this information has been collected ahead of time. What I'd like to happen, is that `latest` builds will show changes for unreleased `1.7.1` and `1.8.0`, while the the `1.7.x` branch just shows `1.7.1`. ## TODO. - [x] Figure out how exactly release-latest works. What are the latest additions for ""Latest build"" vs. ""Stable"" and how should they differ. - `release-latest.rst` will differ between stable and dev branches. - [x] Why isn't release latest being added right to `docs/index.rst`. - [x] Process for adding new release note files should be added to dev docs. - [x] Fix links to PAGA website",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1628
https://github.com/scverse/scanpy/pull/1628:932,deployability,releas,release-latest,932,"Release notes reorganization; @flying-sheep @falexwolf this is a first stab at reorganizing the release notes to facilitate bugfix releases. Using the example of the current release cycle:. The idea is that PRs which are bug fixes would have their change added to the `1.7.1.rst` release notes. New features are added to `1.8.0.rst` (which will be added in a follow up PR, since this pr will be back ported to the `1.7.x` branch). This way we don't mix all changes on master in one file, meaning we don't have to figure out what changes are included when making a release, as this information has been collected ahead of time. What I'd like to happen, is that `latest` builds will show changes for unreleased `1.7.1` and `1.8.0`, while the the `1.7.x` branch just shows `1.7.1`. ## TODO. - [x] Figure out how exactly release-latest works. What are the latest additions for ""Latest build"" vs. ""Stable"" and how should they differ. - `release-latest.rst` will differ between stable and dev branches. - [x] Why isn't release latest being added right to `docs/index.rst`. - [x] Process for adding new release note files should be added to dev docs. - [x] Fix links to PAGA website",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1628
https://github.com/scverse/scanpy/pull/1628:1013,deployability,releas,release,1013,"Release notes reorganization; @flying-sheep @falexwolf this is a first stab at reorganizing the release notes to facilitate bugfix releases. Using the example of the current release cycle:. The idea is that PRs which are bug fixes would have their change added to the `1.7.1.rst` release notes. New features are added to `1.8.0.rst` (which will be added in a follow up PR, since this pr will be back ported to the `1.7.x` branch). This way we don't mix all changes on master in one file, meaning we don't have to figure out what changes are included when making a release, as this information has been collected ahead of time. What I'd like to happen, is that `latest` builds will show changes for unreleased `1.7.1` and `1.8.0`, while the the `1.7.x` branch just shows `1.7.1`. ## TODO. - [x] Figure out how exactly release-latest works. What are the latest additions for ""Latest build"" vs. ""Stable"" and how should they differ. - `release-latest.rst` will differ between stable and dev branches. - [x] Why isn't release latest being added right to `docs/index.rst`. - [x] Process for adding new release note files should be added to dev docs. - [x] Fix links to PAGA website",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1628
https://github.com/scverse/scanpy/pull/1628:1096,deployability,releas,release,1096,"Release notes reorganization; @flying-sheep @falexwolf this is a first stab at reorganizing the release notes to facilitate bugfix releases. Using the example of the current release cycle:. The idea is that PRs which are bug fixes would have their change added to the `1.7.1.rst` release notes. New features are added to `1.8.0.rst` (which will be added in a follow up PR, since this pr will be back ported to the `1.7.x` branch). This way we don't mix all changes on master in one file, meaning we don't have to figure out what changes are included when making a release, as this information has been collected ahead of time. What I'd like to happen, is that `latest` builds will show changes for unreleased `1.7.1` and `1.8.0`, while the the `1.7.x` branch just shows `1.7.1`. ## TODO. - [x] Figure out how exactly release-latest works. What are the latest additions for ""Latest build"" vs. ""Stable"" and how should they differ. - `release-latest.rst` will differ between stable and dev branches. - [x] Why isn't release latest being added right to `docs/index.rst`. - [x] Process for adding new release note files should be added to dev docs. - [x] Fix links to PAGA website",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1628
https://github.com/scverse/scanpy/pull/1628:166,energy efficiency,current,current,166,"Release notes reorganization; @flying-sheep @falexwolf this is a first stab at reorganizing the release notes to facilitate bugfix releases. Using the example of the current release cycle:. The idea is that PRs which are bug fixes would have their change added to the `1.7.1.rst` release notes. New features are added to `1.8.0.rst` (which will be added in a follow up PR, since this pr will be back ported to the `1.7.x` branch). This way we don't mix all changes on master in one file, meaning we don't have to figure out what changes are included when making a release, as this information has been collected ahead of time. What I'd like to happen, is that `latest` builds will show changes for unreleased `1.7.1` and `1.8.0`, while the the `1.7.x` branch just shows `1.7.1`. ## TODO. - [x] Figure out how exactly release-latest works. What are the latest additions for ""Latest build"" vs. ""Stable"" and how should they differ. - `release-latest.rst` will differ between stable and dev branches. - [x] Why isn't release latest being added right to `docs/index.rst`. - [x] Process for adding new release note files should be added to dev docs. - [x] Fix links to PAGA website",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1628
https://github.com/scverse/scanpy/pull/1628:621,performance,time,time,621,"Release notes reorganization; @flying-sheep @falexwolf this is a first stab at reorganizing the release notes to facilitate bugfix releases. Using the example of the current release cycle:. The idea is that PRs which are bug fixes would have their change added to the `1.7.1.rst` release notes. New features are added to `1.8.0.rst` (which will be added in a follow up PR, since this pr will be back ported to the `1.7.x` branch). This way we don't mix all changes on master in one file, meaning we don't have to figure out what changes are included when making a release, as this information has been collected ahead of time. What I'd like to happen, is that `latest` builds will show changes for unreleased `1.7.1` and `1.8.0`, while the the `1.7.x` branch just shows `1.7.1`. ## TODO. - [x] Figure out how exactly release-latest works. What are the latest additions for ""Latest build"" vs. ""Stable"" and how should they differ. - `release-latest.rst` will differ between stable and dev branches. - [x] Why isn't release latest being added right to `docs/index.rst`. - [x] Process for adding new release note files should be added to dev docs. - [x] Fix links to PAGA website",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1628
https://github.com/scverse/scanpy/issues/1629:976,availability,sli,slightly,976,"Interpretation and improving the sc.tl.score_genes functionality; <!-- What kind of feature would you like to request? -->. - [ ] New analysis tool: Calculate enrichment of a pathway across Conditions. - [ ] New plotting function: Distribution of gene-set score across Conditions. . <!-- Please describe your wishes below: -->. Hi, . I really like your implementation of sc.tl.score_genes, which enables to extract biological information based on prior knowledge or to follow-up on genes in the DEgene analysis. . I ran the function with different gene sets on my dataset. The Conditions (of one cell-type) are colored in the density plots below. Currently I am not sure with the interpretation of the results, here I would like to hear your thoughts and maybe some improvements. In both cases >30 genes with medium/high expression are included. . In Plot1, gene-set A is enriched in Condition green compared to the other conditions by comparing the medians. The scores are slightly negative, so I assume that the background-set is higher abundant in those Conditions. Which might indicate that gene-set B is depleted in the other conditions. . ![Plot1](https://user-images.githubusercontent.com/62027756/107199676-0106e600-69f7-11eb-93de-8739ad6dd497.png). In Plot 2, we see that gene-set B is depleted in purple. There might be however a slight enrichment in the other Conditions. . ![Plot2](https://user-images.githubusercontent.com/62027756/107199686-03694000-69f7-11eb-95a8-051dd9b31aea.png). 1. Can we infer from such an analysis how much a pathway is upregulated? (e.g. by calculating the FC of the mean?) . It would be great to conclude for example, that Pathway X is 30% more active, in condition Y. . 2. How does in your opinion class-imbalance affect the analysis? For example, Condition A has 10 samples, while for Condition B,C.. I only have 3 each? . 3. I am happy to provide the code for the density distributions to visualise the results of the gene-set-score function. . Looking fo",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1629
https://github.com/scverse/scanpy/issues/1629:1342,availability,sli,slight,1342,"he sc.tl.score_genes functionality; <!-- What kind of feature would you like to request? -->. - [ ] New analysis tool: Calculate enrichment of a pathway across Conditions. - [ ] New plotting function: Distribution of gene-set score across Conditions. . <!-- Please describe your wishes below: -->. Hi, . I really like your implementation of sc.tl.score_genes, which enables to extract biological information based on prior knowledge or to follow-up on genes in the DEgene analysis. . I ran the function with different gene sets on my dataset. The Conditions (of one cell-type) are colored in the density plots below. Currently I am not sure with the interpretation of the results, here I would like to hear your thoughts and maybe some improvements. In both cases >30 genes with medium/high expression are included. . In Plot1, gene-set A is enriched in Condition green compared to the other conditions by comparing the medians. The scores are slightly negative, so I assume that the background-set is higher abundant in those Conditions. Which might indicate that gene-set B is depleted in the other conditions. . ![Plot1](https://user-images.githubusercontent.com/62027756/107199676-0106e600-69f7-11eb-93de-8739ad6dd497.png). In Plot 2, we see that gene-set B is depleted in purple. There might be however a slight enrichment in the other Conditions. . ![Plot2](https://user-images.githubusercontent.com/62027756/107199686-03694000-69f7-11eb-95a8-051dd9b31aea.png). 1. Can we infer from such an analysis how much a pathway is upregulated? (e.g. by calculating the FC of the mean?) . It would be great to conclude for example, that Pathway X is 30% more active, in condition Y. . 2. How does in your opinion class-imbalance affect the analysis? For example, Condition A has 10 samples, while for Condition B,C.. I only have 3 each? . 3. I am happy to provide the code for the density distributions to visualise the results of the gene-set-score function. . Looking forward to hear your thoughts! .",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1629
https://github.com/scverse/scanpy/issues/1629:647,energy efficiency,Current,Currently,647,"Interpretation and improving the sc.tl.score_genes functionality; <!-- What kind of feature would you like to request? -->. - [ ] New analysis tool: Calculate enrichment of a pathway across Conditions. - [ ] New plotting function: Distribution of gene-set score across Conditions. . <!-- Please describe your wishes below: -->. Hi, . I really like your implementation of sc.tl.score_genes, which enables to extract biological information based on prior knowledge or to follow-up on genes in the DEgene analysis. . I ran the function with different gene sets on my dataset. The Conditions (of one cell-type) are colored in the density plots below. Currently I am not sure with the interpretation of the results, here I would like to hear your thoughts and maybe some improvements. In both cases >30 genes with medium/high expression are included. . In Plot1, gene-set A is enriched in Condition green compared to the other conditions by comparing the medians. The scores are slightly negative, so I assume that the background-set is higher abundant in those Conditions. Which might indicate that gene-set B is depleted in the other conditions. . ![Plot1](https://user-images.githubusercontent.com/62027756/107199676-0106e600-69f7-11eb-93de-8739ad6dd497.png). In Plot 2, we see that gene-set B is depleted in purple. There might be however a slight enrichment in the other Conditions. . ![Plot2](https://user-images.githubusercontent.com/62027756/107199686-03694000-69f7-11eb-95a8-051dd9b31aea.png). 1. Can we infer from such an analysis how much a pathway is upregulated? (e.g. by calculating the FC of the mean?) . It would be great to conclude for example, that Pathway X is 30% more active, in condition Y. . 2. How does in your opinion class-imbalance affect the analysis? For example, Condition A has 10 samples, while for Condition B,C.. I only have 3 each? . 3. I am happy to provide the code for the density distributions to visualise the results of the gene-set-score function. . Looking fo",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1629
https://github.com/scverse/scanpy/issues/1629:895,energy efficiency,green,green,895,"Interpretation and improving the sc.tl.score_genes functionality; <!-- What kind of feature would you like to request? -->. - [ ] New analysis tool: Calculate enrichment of a pathway across Conditions. - [ ] New plotting function: Distribution of gene-set score across Conditions. . <!-- Please describe your wishes below: -->. Hi, . I really like your implementation of sc.tl.score_genes, which enables to extract biological information based on prior knowledge or to follow-up on genes in the DEgene analysis. . I ran the function with different gene sets on my dataset. The Conditions (of one cell-type) are colored in the density plots below. Currently I am not sure with the interpretation of the results, here I would like to hear your thoughts and maybe some improvements. In both cases >30 genes with medium/high expression are included. . In Plot1, gene-set A is enriched in Condition green compared to the other conditions by comparing the medians. The scores are slightly negative, so I assume that the background-set is higher abundant in those Conditions. Which might indicate that gene-set B is depleted in the other conditions. . ![Plot1](https://user-images.githubusercontent.com/62027756/107199676-0106e600-69f7-11eb-93de-8739ad6dd497.png). In Plot 2, we see that gene-set B is depleted in purple. There might be however a slight enrichment in the other Conditions. . ![Plot2](https://user-images.githubusercontent.com/62027756/107199686-03694000-69f7-11eb-95a8-051dd9b31aea.png). 1. Can we infer from such an analysis how much a pathway is upregulated? (e.g. by calculating the FC of the mean?) . It would be great to conclude for example, that Pathway X is 30% more active, in condition Y. . 2. How does in your opinion class-imbalance affect the analysis? For example, Condition A has 10 samples, while for Condition B,C.. I only have 3 each? . 3. I am happy to provide the code for the density distributions to visualise the results of the gene-set-score function. . Looking fo",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1629
https://github.com/scverse/scanpy/issues/1629:231,interoperability,Distribut,Distribution,231,"Interpretation and improving the sc.tl.score_genes functionality; <!-- What kind of feature would you like to request? -->. - [ ] New analysis tool: Calculate enrichment of a pathway across Conditions. - [ ] New plotting function: Distribution of gene-set score across Conditions. . <!-- Please describe your wishes below: -->. Hi, . I really like your implementation of sc.tl.score_genes, which enables to extract biological information based on prior knowledge or to follow-up on genes in the DEgene analysis. . I ran the function with different gene sets on my dataset. The Conditions (of one cell-type) are colored in the density plots below. Currently I am not sure with the interpretation of the results, here I would like to hear your thoughts and maybe some improvements. In both cases >30 genes with medium/high expression are included. . In Plot1, gene-set A is enriched in Condition green compared to the other conditions by comparing the medians. The scores are slightly negative, so I assume that the background-set is higher abundant in those Conditions. Which might indicate that gene-set B is depleted in the other conditions. . ![Plot1](https://user-images.githubusercontent.com/62027756/107199676-0106e600-69f7-11eb-93de-8739ad6dd497.png). In Plot 2, we see that gene-set B is depleted in purple. There might be however a slight enrichment in the other Conditions. . ![Plot2](https://user-images.githubusercontent.com/62027756/107199686-03694000-69f7-11eb-95a8-051dd9b31aea.png). 1. Can we infer from such an analysis how much a pathway is upregulated? (e.g. by calculating the FC of the mean?) . It would be great to conclude for example, that Pathway X is 30% more active, in condition Y. . 2. How does in your opinion class-imbalance affect the analysis? For example, Condition A has 10 samples, while for Condition B,C.. I only have 3 each? . 3. I am happy to provide the code for the density distributions to visualise the results of the gene-set-score function. . Looking fo",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1629
https://github.com/scverse/scanpy/issues/1629:1917,interoperability,distribut,distributions,1917,"he sc.tl.score_genes functionality; <!-- What kind of feature would you like to request? -->. - [ ] New analysis tool: Calculate enrichment of a pathway across Conditions. - [ ] New plotting function: Distribution of gene-set score across Conditions. . <!-- Please describe your wishes below: -->. Hi, . I really like your implementation of sc.tl.score_genes, which enables to extract biological information based on prior knowledge or to follow-up on genes in the DEgene analysis. . I ran the function with different gene sets on my dataset. The Conditions (of one cell-type) are colored in the density plots below. Currently I am not sure with the interpretation of the results, here I would like to hear your thoughts and maybe some improvements. In both cases >30 genes with medium/high expression are included. . In Plot1, gene-set A is enriched in Condition green compared to the other conditions by comparing the medians. The scores are slightly negative, so I assume that the background-set is higher abundant in those Conditions. Which might indicate that gene-set B is depleted in the other conditions. . ![Plot1](https://user-images.githubusercontent.com/62027756/107199676-0106e600-69f7-11eb-93de-8739ad6dd497.png). In Plot 2, we see that gene-set B is depleted in purple. There might be however a slight enrichment in the other Conditions. . ![Plot2](https://user-images.githubusercontent.com/62027756/107199686-03694000-69f7-11eb-95a8-051dd9b31aea.png). 1. Can we infer from such an analysis how much a pathway is upregulated? (e.g. by calculating the FC of the mean?) . It would be great to conclude for example, that Pathway X is 30% more active, in condition Y. . 2. How does in your opinion class-imbalance affect the analysis? For example, Condition A has 10 samples, while for Condition B,C.. I only have 3 each? . 3. I am happy to provide the code for the density distributions to visualise the results of the gene-set-score function. . Looking forward to hear your thoughts! .",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1629
https://github.com/scverse/scanpy/issues/1629:976,reliability,sli,slightly,976,"Interpretation and improving the sc.tl.score_genes functionality; <!-- What kind of feature would you like to request? -->. - [ ] New analysis tool: Calculate enrichment of a pathway across Conditions. - [ ] New plotting function: Distribution of gene-set score across Conditions. . <!-- Please describe your wishes below: -->. Hi, . I really like your implementation of sc.tl.score_genes, which enables to extract biological information based on prior knowledge or to follow-up on genes in the DEgene analysis. . I ran the function with different gene sets on my dataset. The Conditions (of one cell-type) are colored in the density plots below. Currently I am not sure with the interpretation of the results, here I would like to hear your thoughts and maybe some improvements. In both cases >30 genes with medium/high expression are included. . In Plot1, gene-set A is enriched in Condition green compared to the other conditions by comparing the medians. The scores are slightly negative, so I assume that the background-set is higher abundant in those Conditions. Which might indicate that gene-set B is depleted in the other conditions. . ![Plot1](https://user-images.githubusercontent.com/62027756/107199676-0106e600-69f7-11eb-93de-8739ad6dd497.png). In Plot 2, we see that gene-set B is depleted in purple. There might be however a slight enrichment in the other Conditions. . ![Plot2](https://user-images.githubusercontent.com/62027756/107199686-03694000-69f7-11eb-95a8-051dd9b31aea.png). 1. Can we infer from such an analysis how much a pathway is upregulated? (e.g. by calculating the FC of the mean?) . It would be great to conclude for example, that Pathway X is 30% more active, in condition Y. . 2. How does in your opinion class-imbalance affect the analysis? For example, Condition A has 10 samples, while for Condition B,C.. I only have 3 each? . 3. I am happy to provide the code for the density distributions to visualise the results of the gene-set-score function. . Looking fo",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1629
https://github.com/scverse/scanpy/issues/1629:1342,reliability,sli,slight,1342,"he sc.tl.score_genes functionality; <!-- What kind of feature would you like to request? -->. - [ ] New analysis tool: Calculate enrichment of a pathway across Conditions. - [ ] New plotting function: Distribution of gene-set score across Conditions. . <!-- Please describe your wishes below: -->. Hi, . I really like your implementation of sc.tl.score_genes, which enables to extract biological information based on prior knowledge or to follow-up on genes in the DEgene analysis. . I ran the function with different gene sets on my dataset. The Conditions (of one cell-type) are colored in the density plots below. Currently I am not sure with the interpretation of the results, here I would like to hear your thoughts and maybe some improvements. In both cases >30 genes with medium/high expression are included. . In Plot1, gene-set A is enriched in Condition green compared to the other conditions by comparing the medians. The scores are slightly negative, so I assume that the background-set is higher abundant in those Conditions. Which might indicate that gene-set B is depleted in the other conditions. . ![Plot1](https://user-images.githubusercontent.com/62027756/107199676-0106e600-69f7-11eb-93de-8739ad6dd497.png). In Plot 2, we see that gene-set B is depleted in purple. There might be however a slight enrichment in the other Conditions. . ![Plot2](https://user-images.githubusercontent.com/62027756/107199686-03694000-69f7-11eb-95a8-051dd9b31aea.png). 1. Can we infer from such an analysis how much a pathway is upregulated? (e.g. by calculating the FC of the mean?) . It would be great to conclude for example, that Pathway X is 30% more active, in condition Y. . 2. How does in your opinion class-imbalance affect the analysis? For example, Condition A has 10 samples, while for Condition B,C.. I only have 3 each? . 3. I am happy to provide the code for the density distributions to visualise the results of the gene-set-score function. . Looking forward to hear your thoughts! .",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1629
https://github.com/scverse/scanpy/issues/1629:1720,reliability,doe,does,1720,"he sc.tl.score_genes functionality; <!-- What kind of feature would you like to request? -->. - [ ] New analysis tool: Calculate enrichment of a pathway across Conditions. - [ ] New plotting function: Distribution of gene-set score across Conditions. . <!-- Please describe your wishes below: -->. Hi, . I really like your implementation of sc.tl.score_genes, which enables to extract biological information based on prior knowledge or to follow-up on genes in the DEgene analysis. . I ran the function with different gene sets on my dataset. The Conditions (of one cell-type) are colored in the density plots below. Currently I am not sure with the interpretation of the results, here I would like to hear your thoughts and maybe some improvements. In both cases >30 genes with medium/high expression are included. . In Plot1, gene-set A is enriched in Condition green compared to the other conditions by comparing the medians. The scores are slightly negative, so I assume that the background-set is higher abundant in those Conditions. Which might indicate that gene-set B is depleted in the other conditions. . ![Plot1](https://user-images.githubusercontent.com/62027756/107199676-0106e600-69f7-11eb-93de-8739ad6dd497.png). In Plot 2, we see that gene-set B is depleted in purple. There might be however a slight enrichment in the other Conditions. . ![Plot2](https://user-images.githubusercontent.com/62027756/107199686-03694000-69f7-11eb-95a8-051dd9b31aea.png). 1. Can we infer from such an analysis how much a pathway is upregulated? (e.g. by calculating the FC of the mean?) . It would be great to conclude for example, that Pathway X is 30% more active, in condition Y. . 2. How does in your opinion class-imbalance affect the analysis? For example, Condition A has 10 samples, while for Condition B,C.. I only have 3 each? . 3. I am happy to provide the code for the density distributions to visualise the results of the gene-set-score function. . Looking forward to hear your thoughts! .",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1629
https://github.com/scverse/scanpy/issues/1629:143,usability,tool,tool,143,"Interpretation and improving the sc.tl.score_genes functionality; <!-- What kind of feature would you like to request? -->. - [ ] New analysis tool: Calculate enrichment of a pathway across Conditions. - [ ] New plotting function: Distribution of gene-set score across Conditions. . <!-- Please describe your wishes below: -->. Hi, . I really like your implementation of sc.tl.score_genes, which enables to extract biological information based on prior knowledge or to follow-up on genes in the DEgene analysis. . I ran the function with different gene sets on my dataset. The Conditions (of one cell-type) are colored in the density plots below. Currently I am not sure with the interpretation of the results, here I would like to hear your thoughts and maybe some improvements. In both cases >30 genes with medium/high expression are included. . In Plot1, gene-set A is enriched in Condition green compared to the other conditions by comparing the medians. The scores are slightly negative, so I assume that the background-set is higher abundant in those Conditions. Which might indicate that gene-set B is depleted in the other conditions. . ![Plot1](https://user-images.githubusercontent.com/62027756/107199676-0106e600-69f7-11eb-93de-8739ad6dd497.png). In Plot 2, we see that gene-set B is depleted in purple. There might be however a slight enrichment in the other Conditions. . ![Plot2](https://user-images.githubusercontent.com/62027756/107199686-03694000-69f7-11eb-95a8-051dd9b31aea.png). 1. Can we infer from such an analysis how much a pathway is upregulated? (e.g. by calculating the FC of the mean?) . It would be great to conclude for example, that Pathway X is 30% more active, in condition Y. . 2. How does in your opinion class-imbalance affect the analysis? For example, Condition A has 10 samples, while for Condition B,C.. I only have 3 each? . 3. I am happy to provide the code for the density distributions to visualise the results of the gene-set-score function. . Looking fo",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1629
https://github.com/scverse/scanpy/issues/1629:1083,usability,indicat,indicate,1083,"he sc.tl.score_genes functionality; <!-- What kind of feature would you like to request? -->. - [ ] New analysis tool: Calculate enrichment of a pathway across Conditions. - [ ] New plotting function: Distribution of gene-set score across Conditions. . <!-- Please describe your wishes below: -->. Hi, . I really like your implementation of sc.tl.score_genes, which enables to extract biological information based on prior knowledge or to follow-up on genes in the DEgene analysis. . I ran the function with different gene sets on my dataset. The Conditions (of one cell-type) are colored in the density plots below. Currently I am not sure with the interpretation of the results, here I would like to hear your thoughts and maybe some improvements. In both cases >30 genes with medium/high expression are included. . In Plot1, gene-set A is enriched in Condition green compared to the other conditions by comparing the medians. The scores are slightly negative, so I assume that the background-set is higher abundant in those Conditions. Which might indicate that gene-set B is depleted in the other conditions. . ![Plot1](https://user-images.githubusercontent.com/62027756/107199676-0106e600-69f7-11eb-93de-8739ad6dd497.png). In Plot 2, we see that gene-set B is depleted in purple. There might be however a slight enrichment in the other Conditions. . ![Plot2](https://user-images.githubusercontent.com/62027756/107199686-03694000-69f7-11eb-95a8-051dd9b31aea.png). 1. Can we infer from such an analysis how much a pathway is upregulated? (e.g. by calculating the FC of the mean?) . It would be great to conclude for example, that Pathway X is 30% more active, in condition Y. . 2. How does in your opinion class-imbalance affect the analysis? For example, Condition A has 10 samples, while for Condition B,C.. I only have 3 each? . 3. I am happy to provide the code for the density distributions to visualise the results of the gene-set-score function. . Looking forward to hear your thoughts! .",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1629
https://github.com/scverse/scanpy/issues/1629:1164,usability,user,user-images,1164,"he sc.tl.score_genes functionality; <!-- What kind of feature would you like to request? -->. - [ ] New analysis tool: Calculate enrichment of a pathway across Conditions. - [ ] New plotting function: Distribution of gene-set score across Conditions. . <!-- Please describe your wishes below: -->. Hi, . I really like your implementation of sc.tl.score_genes, which enables to extract biological information based on prior knowledge or to follow-up on genes in the DEgene analysis. . I ran the function with different gene sets on my dataset. The Conditions (of one cell-type) are colored in the density plots below. Currently I am not sure with the interpretation of the results, here I would like to hear your thoughts and maybe some improvements. In both cases >30 genes with medium/high expression are included. . In Plot1, gene-set A is enriched in Condition green compared to the other conditions by comparing the medians. The scores are slightly negative, so I assume that the background-set is higher abundant in those Conditions. Which might indicate that gene-set B is depleted in the other conditions. . ![Plot1](https://user-images.githubusercontent.com/62027756/107199676-0106e600-69f7-11eb-93de-8739ad6dd497.png). In Plot 2, we see that gene-set B is depleted in purple. There might be however a slight enrichment in the other Conditions. . ![Plot2](https://user-images.githubusercontent.com/62027756/107199686-03694000-69f7-11eb-95a8-051dd9b31aea.png). 1. Can we infer from such an analysis how much a pathway is upregulated? (e.g. by calculating the FC of the mean?) . It would be great to conclude for example, that Pathway X is 30% more active, in condition Y. . 2. How does in your opinion class-imbalance affect the analysis? For example, Condition A has 10 samples, while for Condition B,C.. I only have 3 each? . 3. I am happy to provide the code for the density distributions to visualise the results of the gene-set-score function. . Looking forward to hear your thoughts! .",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1629
https://github.com/scverse/scanpy/issues/1629:1404,usability,user,user-images,1404,"he sc.tl.score_genes functionality; <!-- What kind of feature would you like to request? -->. - [ ] New analysis tool: Calculate enrichment of a pathway across Conditions. - [ ] New plotting function: Distribution of gene-set score across Conditions. . <!-- Please describe your wishes below: -->. Hi, . I really like your implementation of sc.tl.score_genes, which enables to extract biological information based on prior knowledge or to follow-up on genes in the DEgene analysis. . I ran the function with different gene sets on my dataset. The Conditions (of one cell-type) are colored in the density plots below. Currently I am not sure with the interpretation of the results, here I would like to hear your thoughts and maybe some improvements. In both cases >30 genes with medium/high expression are included. . In Plot1, gene-set A is enriched in Condition green compared to the other conditions by comparing the medians. The scores are slightly negative, so I assume that the background-set is higher abundant in those Conditions. Which might indicate that gene-set B is depleted in the other conditions. . ![Plot1](https://user-images.githubusercontent.com/62027756/107199676-0106e600-69f7-11eb-93de-8739ad6dd497.png). In Plot 2, we see that gene-set B is depleted in purple. There might be however a slight enrichment in the other Conditions. . ![Plot2](https://user-images.githubusercontent.com/62027756/107199686-03694000-69f7-11eb-95a8-051dd9b31aea.png). 1. Can we infer from such an analysis how much a pathway is upregulated? (e.g. by calculating the FC of the mean?) . It would be great to conclude for example, that Pathway X is 30% more active, in condition Y. . 2. How does in your opinion class-imbalance affect the analysis? For example, Condition A has 10 samples, while for Condition B,C.. I only have 3 each? . 3. I am happy to provide the code for the density distributions to visualise the results of the gene-set-score function. . Looking forward to hear your thoughts! .",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1629
https://github.com/scverse/scanpy/issues/1629:1934,usability,visual,visualise,1934,"he sc.tl.score_genes functionality; <!-- What kind of feature would you like to request? -->. - [ ] New analysis tool: Calculate enrichment of a pathway across Conditions. - [ ] New plotting function: Distribution of gene-set score across Conditions. . <!-- Please describe your wishes below: -->. Hi, . I really like your implementation of sc.tl.score_genes, which enables to extract biological information based on prior knowledge or to follow-up on genes in the DEgene analysis. . I ran the function with different gene sets on my dataset. The Conditions (of one cell-type) are colored in the density plots below. Currently I am not sure with the interpretation of the results, here I would like to hear your thoughts and maybe some improvements. In both cases >30 genes with medium/high expression are included. . In Plot1, gene-set A is enriched in Condition green compared to the other conditions by comparing the medians. The scores are slightly negative, so I assume that the background-set is higher abundant in those Conditions. Which might indicate that gene-set B is depleted in the other conditions. . ![Plot1](https://user-images.githubusercontent.com/62027756/107199676-0106e600-69f7-11eb-93de-8739ad6dd497.png). In Plot 2, we see that gene-set B is depleted in purple. There might be however a slight enrichment in the other Conditions. . ![Plot2](https://user-images.githubusercontent.com/62027756/107199686-03694000-69f7-11eb-95a8-051dd9b31aea.png). 1. Can we infer from such an analysis how much a pathway is upregulated? (e.g. by calculating the FC of the mean?) . It would be great to conclude for example, that Pathway X is 30% more active, in condition Y. . 2. How does in your opinion class-imbalance affect the analysis? For example, Condition A has 10 samples, while for Condition B,C.. I only have 3 each? . 3. I am happy to provide the code for the density distributions to visualise the results of the gene-set-score function. . Looking forward to hear your thoughts! .",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1629
https://github.com/scverse/scanpy/issues/1630:560,deployability,build,buildReference,560,"Question about vst in highly_variable_genes; <!--.  If you need help using Scanpy, please ask in https://scanpy.discourse.group/ instead . If you want to know about design decisions and the like, please ask below:. -->. Hi, . When running `highly_variable_genes `with `flavor='seurat_v3'`, the method expects raw counts. But in Seurat tutorials e.g. [here](https://satijalab.org/seurat/archive/v3.0/pbmc3k_tutorial.html) or in [Symphony](https://github.com/immunogenomics/symphony) in their code [here](https://github.com/immunogenomics/symphony/blob/main/R/buildReference.R), they run the method on normalized counts. Could you please clarify this a little? Thanks.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1630
https://github.com/scverse/scanpy/issues/1630:167,modifiability,design decis,design decisions,167,"Question about vst in highly_variable_genes; <!--.  If you need help using Scanpy, please ask in https://scanpy.discourse.group/ instead . If you want to know about design decisions and the like, please ask below:. -->. Hi, . When running `highly_variable_genes `with `flavor='seurat_v3'`, the method expects raw counts. But in Seurat tutorials e.g. [here](https://satijalab.org/seurat/archive/v3.0/pbmc3k_tutorial.html) or in [Symphony](https://github.com/immunogenomics/symphony) in their code [here](https://github.com/immunogenomics/symphony/blob/main/R/buildReference.R), they run the method on normalized counts. Could you please clarify this a little? Thanks.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1630
https://github.com/scverse/scanpy/issues/1630:65,usability,help,help,65,"Question about vst in highly_variable_genes; <!--.  If you need help using Scanpy, please ask in https://scanpy.discourse.group/ instead . If you want to know about design decisions and the like, please ask below:. -->. Hi, . When running `highly_variable_genes `with `flavor='seurat_v3'`, the method expects raw counts. But in Seurat tutorials e.g. [here](https://satijalab.org/seurat/archive/v3.0/pbmc3k_tutorial.html) or in [Symphony](https://github.com/immunogenomics/symphony) in their code [here](https://github.com/immunogenomics/symphony/blob/main/R/buildReference.R), they run the method on normalized counts. Could you please clarify this a little? Thanks.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1630
https://github.com/scverse/scanpy/issues/1631:452,availability,avail,available,452,"Adding CellO to Scanpy; <!--.  If you need help using Scanpy, please ask in https://scanpy.discourse.group/ instead . If you want to know about design decisions and the like, please ask below:. -->. Hi, . I am hoping to open a pull request soon to add [CellO](https://www.cell.com/iscience/fulltext/S2589-0042(20)31110-X), a cell type classification tool, to Scanpy's external API. I notice that there currently are no cell type classification tools available in Scanpy's external API. . I am wondering if there is an explicit reason no cell type classifiers have been added to date? For example, I noticed some debate regarding how/whether to include expression imputation into Scanpy [https://github.com/theislab/scanpy/issues/189](https://github.com/theislab/scanpy/issues/189), and I just want to make sure there is no such reason why cell type classifiers have not been included yet. Thank you! Matt",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1631
https://github.com/scverse/scanpy/issues/1631:379,deployability,API,API,379,"Adding CellO to Scanpy; <!--.  If you need help using Scanpy, please ask in https://scanpy.discourse.group/ instead . If you want to know about design decisions and the like, please ask below:. -->. Hi, . I am hoping to open a pull request soon to add [CellO](https://www.cell.com/iscience/fulltext/S2589-0042(20)31110-X), a cell type classification tool, to Scanpy's external API. I notice that there currently are no cell type classification tools available in Scanpy's external API. . I am wondering if there is an explicit reason no cell type classifiers have been added to date? For example, I noticed some debate regarding how/whether to include expression imputation into Scanpy [https://github.com/theislab/scanpy/issues/189](https://github.com/theislab/scanpy/issues/189), and I just want to make sure there is no such reason why cell type classifiers have not been included yet. Thank you! Matt",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1631
https://github.com/scverse/scanpy/issues/1631:483,deployability,API,API,483,"Adding CellO to Scanpy; <!--.  If you need help using Scanpy, please ask in https://scanpy.discourse.group/ instead . If you want to know about design decisions and the like, please ask below:. -->. Hi, . I am hoping to open a pull request soon to add [CellO](https://www.cell.com/iscience/fulltext/S2589-0042(20)31110-X), a cell type classification tool, to Scanpy's external API. I notice that there currently are no cell type classification tools available in Scanpy's external API. . I am wondering if there is an explicit reason no cell type classifiers have been added to date? For example, I noticed some debate regarding how/whether to include expression imputation into Scanpy [https://github.com/theislab/scanpy/issues/189](https://github.com/theislab/scanpy/issues/189), and I just want to make sure there is no such reason why cell type classifiers have not been included yet. Thank you! Matt",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1631
https://github.com/scverse/scanpy/issues/1631:404,energy efficiency,current,currently,404,"Adding CellO to Scanpy; <!--.  If you need help using Scanpy, please ask in https://scanpy.discourse.group/ instead . If you want to know about design decisions and the like, please ask below:. -->. Hi, . I am hoping to open a pull request soon to add [CellO](https://www.cell.com/iscience/fulltext/S2589-0042(20)31110-X), a cell type classification tool, to Scanpy's external API. I notice that there currently are no cell type classification tools available in Scanpy's external API. . I am wondering if there is an explicit reason no cell type classifiers have been added to date? For example, I noticed some debate regarding how/whether to include expression imputation into Scanpy [https://github.com/theislab/scanpy/issues/189](https://github.com/theislab/scanpy/issues/189), and I just want to make sure there is no such reason why cell type classifiers have not been included yet. Thank you! Matt",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1631
https://github.com/scverse/scanpy/issues/1631:379,integrability,API,API,379,"Adding CellO to Scanpy; <!--.  If you need help using Scanpy, please ask in https://scanpy.discourse.group/ instead . If you want to know about design decisions and the like, please ask below:. -->. Hi, . I am hoping to open a pull request soon to add [CellO](https://www.cell.com/iscience/fulltext/S2589-0042(20)31110-X), a cell type classification tool, to Scanpy's external API. I notice that there currently are no cell type classification tools available in Scanpy's external API. . I am wondering if there is an explicit reason no cell type classifiers have been added to date? For example, I noticed some debate regarding how/whether to include expression imputation into Scanpy [https://github.com/theislab/scanpy/issues/189](https://github.com/theislab/scanpy/issues/189), and I just want to make sure there is no such reason why cell type classifiers have not been included yet. Thank you! Matt",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1631
https://github.com/scverse/scanpy/issues/1631:483,integrability,API,API,483,"Adding CellO to Scanpy; <!--.  If you need help using Scanpy, please ask in https://scanpy.discourse.group/ instead . If you want to know about design decisions and the like, please ask below:. -->. Hi, . I am hoping to open a pull request soon to add [CellO](https://www.cell.com/iscience/fulltext/S2589-0042(20)31110-X), a cell type classification tool, to Scanpy's external API. I notice that there currently are no cell type classification tools available in Scanpy's external API. . I am wondering if there is an explicit reason no cell type classifiers have been added to date? For example, I noticed some debate regarding how/whether to include expression imputation into Scanpy [https://github.com/theislab/scanpy/issues/189](https://github.com/theislab/scanpy/issues/189), and I just want to make sure there is no such reason why cell type classifiers have not been included yet. Thank you! Matt",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1631
https://github.com/scverse/scanpy/issues/1631:379,interoperability,API,API,379,"Adding CellO to Scanpy; <!--.  If you need help using Scanpy, please ask in https://scanpy.discourse.group/ instead . If you want to know about design decisions and the like, please ask below:. -->. Hi, . I am hoping to open a pull request soon to add [CellO](https://www.cell.com/iscience/fulltext/S2589-0042(20)31110-X), a cell type classification tool, to Scanpy's external API. I notice that there currently are no cell type classification tools available in Scanpy's external API. . I am wondering if there is an explicit reason no cell type classifiers have been added to date? For example, I noticed some debate regarding how/whether to include expression imputation into Scanpy [https://github.com/theislab/scanpy/issues/189](https://github.com/theislab/scanpy/issues/189), and I just want to make sure there is no such reason why cell type classifiers have not been included yet. Thank you! Matt",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1631
https://github.com/scverse/scanpy/issues/1631:483,interoperability,API,API,483,"Adding CellO to Scanpy; <!--.  If you need help using Scanpy, please ask in https://scanpy.discourse.group/ instead . If you want to know about design decisions and the like, please ask below:. -->. Hi, . I am hoping to open a pull request soon to add [CellO](https://www.cell.com/iscience/fulltext/S2589-0042(20)31110-X), a cell type classification tool, to Scanpy's external API. I notice that there currently are no cell type classification tools available in Scanpy's external API. . I am wondering if there is an explicit reason no cell type classifiers have been added to date? For example, I noticed some debate regarding how/whether to include expression imputation into Scanpy [https://github.com/theislab/scanpy/issues/189](https://github.com/theislab/scanpy/issues/189), and I just want to make sure there is no such reason why cell type classifiers have not been included yet. Thank you! Matt",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1631
https://github.com/scverse/scanpy/issues/1631:146,modifiability,design decis,design decisions,146,"Adding CellO to Scanpy; <!--.  If you need help using Scanpy, please ask in https://scanpy.discourse.group/ instead . If you want to know about design decisions and the like, please ask below:. -->. Hi, . I am hoping to open a pull request soon to add [CellO](https://www.cell.com/iscience/fulltext/S2589-0042(20)31110-X), a cell type classification tool, to Scanpy's external API. I notice that there currently are no cell type classification tools available in Scanpy's external API. . I am wondering if there is an explicit reason no cell type classifiers have been added to date? For example, I noticed some debate regarding how/whether to include expression imputation into Scanpy [https://github.com/theislab/scanpy/issues/189](https://github.com/theislab/scanpy/issues/189), and I just want to make sure there is no such reason why cell type classifiers have not been included yet. Thank you! Matt",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1631
https://github.com/scverse/scanpy/issues/1631:452,reliability,availab,available,452,"Adding CellO to Scanpy; <!--.  If you need help using Scanpy, please ask in https://scanpy.discourse.group/ instead . If you want to know about design decisions and the like, please ask below:. -->. Hi, . I am hoping to open a pull request soon to add [CellO](https://www.cell.com/iscience/fulltext/S2589-0042(20)31110-X), a cell type classification tool, to Scanpy's external API. I notice that there currently are no cell type classification tools available in Scanpy's external API. . I am wondering if there is an explicit reason no cell type classifiers have been added to date? For example, I noticed some debate regarding how/whether to include expression imputation into Scanpy [https://github.com/theislab/scanpy/issues/189](https://github.com/theislab/scanpy/issues/189), and I just want to make sure there is no such reason why cell type classifiers have not been included yet. Thank you! Matt",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1631
https://github.com/scverse/scanpy/issues/1631:452,safety,avail,available,452,"Adding CellO to Scanpy; <!--.  If you need help using Scanpy, please ask in https://scanpy.discourse.group/ instead . If you want to know about design decisions and the like, please ask below:. -->. Hi, . I am hoping to open a pull request soon to add [CellO](https://www.cell.com/iscience/fulltext/S2589-0042(20)31110-X), a cell type classification tool, to Scanpy's external API. I notice that there currently are no cell type classification tools available in Scanpy's external API. . I am wondering if there is an explicit reason no cell type classifiers have been added to date? For example, I noticed some debate regarding how/whether to include expression imputation into Scanpy [https://github.com/theislab/scanpy/issues/189](https://github.com/theislab/scanpy/issues/189), and I just want to make sure there is no such reason why cell type classifiers have not been included yet. Thank you! Matt",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1631
https://github.com/scverse/scanpy/issues/1631:452,security,availab,available,452,"Adding CellO to Scanpy; <!--.  If you need help using Scanpy, please ask in https://scanpy.discourse.group/ instead . If you want to know about design decisions and the like, please ask below:. -->. Hi, . I am hoping to open a pull request soon to add [CellO](https://www.cell.com/iscience/fulltext/S2589-0042(20)31110-X), a cell type classification tool, to Scanpy's external API. I notice that there currently are no cell type classification tools available in Scanpy's external API. . I am wondering if there is an explicit reason no cell type classifiers have been added to date? For example, I noticed some debate regarding how/whether to include expression imputation into Scanpy [https://github.com/theislab/scanpy/issues/189](https://github.com/theislab/scanpy/issues/189), and I just want to make sure there is no such reason why cell type classifiers have not been included yet. Thank you! Matt",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1631
https://github.com/scverse/scanpy/issues/1631:44,usability,help,help,44,"Adding CellO to Scanpy; <!--.  If you need help using Scanpy, please ask in https://scanpy.discourse.group/ instead . If you want to know about design decisions and the like, please ask below:. -->. Hi, . I am hoping to open a pull request soon to add [CellO](https://www.cell.com/iscience/fulltext/S2589-0042(20)31110-X), a cell type classification tool, to Scanpy's external API. I notice that there currently are no cell type classification tools available in Scanpy's external API. . I am wondering if there is an explicit reason no cell type classifiers have been added to date? For example, I noticed some debate regarding how/whether to include expression imputation into Scanpy [https://github.com/theislab/scanpy/issues/189](https://github.com/theislab/scanpy/issues/189), and I just want to make sure there is no such reason why cell type classifiers have not been included yet. Thank you! Matt",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1631
https://github.com/scverse/scanpy/issues/1631:352,usability,tool,tool,352,"Adding CellO to Scanpy; <!--.  If you need help using Scanpy, please ask in https://scanpy.discourse.group/ instead . If you want to know about design decisions and the like, please ask below:. -->. Hi, . I am hoping to open a pull request soon to add [CellO](https://www.cell.com/iscience/fulltext/S2589-0042(20)31110-X), a cell type classification tool, to Scanpy's external API. I notice that there currently are no cell type classification tools available in Scanpy's external API. . I am wondering if there is an explicit reason no cell type classifiers have been added to date? For example, I noticed some debate regarding how/whether to include expression imputation into Scanpy [https://github.com/theislab/scanpy/issues/189](https://github.com/theislab/scanpy/issues/189), and I just want to make sure there is no such reason why cell type classifiers have not been included yet. Thank you! Matt",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1631
https://github.com/scverse/scanpy/issues/1631:446,usability,tool,tools,446,"Adding CellO to Scanpy; <!--.  If you need help using Scanpy, please ask in https://scanpy.discourse.group/ instead . If you want to know about design decisions and the like, please ask below:. -->. Hi, . I am hoping to open a pull request soon to add [CellO](https://www.cell.com/iscience/fulltext/S2589-0042(20)31110-X), a cell type classification tool, to Scanpy's external API. I notice that there currently are no cell type classification tools available in Scanpy's external API. . I am wondering if there is an explicit reason no cell type classifiers have been added to date? For example, I noticed some debate regarding how/whether to include expression imputation into Scanpy [https://github.com/theislab/scanpy/issues/189](https://github.com/theislab/scanpy/issues/189), and I just want to make sure there is no such reason why cell type classifiers have not been included yet. Thank you! Matt",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1631
https://github.com/scverse/scanpy/pull/1632:940,availability,cluster,clustermap,940,"Plotting examples; Re #1604, @fidelram, I've got the sphinx extension working for examples (at least locally) here. You can check out a simple example for `sc.pl.embedding`. Does this solve the problem you were running into? <details>. <summary> Functions which need examples </summary>. Functions in `sc.pl`. - [ ] Embedding plots. - [ ] `sc.pl.embedding`. - [ ] `sc.pl.draw_graph`. - [ ] `sc.pl.diffmap`. - [ ] `sc.pl.pca`. - [ ] `sc.pl.tsne`. - [ ] `sc.pl.umap`. - [ ] `sc.pl.spatial`. - [x] `sc.pl.embedding_density`. - [ ] PCA specific. - [ ] `sc.pl.pca_loadings`. - [ ] `sc.pl.pca_overview`. - [ ] `sc.pl.pca_scatter`. - [ ] `sc.pl.pca_variance_ratio`. - [ ] PAGA. - [ ] `sc.pl.paga`. - [ ] `sc.pl.paga_adjacency`. - [ ] `sc.pl.paga_compare`. - [ ] `sc.pl.paga_path`. - [ ] DPT pseudotime. - [ ] `sc.pl.dpt_groups_pseudotime`. - [ ] `sc.pl.dpt_timeseries`. - [ ] Groupby. - [x] `sc.pl.dotplot`. - [ ] `sc.pl.matrixplot`. - [ ] `sc.pl.clustermap`. - [ ] `sc.pl.heatmap`. - [ ] `sc.pl.dendrogram`. - [ ] `sc.pl.stacked_violin`. - [ ] `sc.pl.tracksplot`. - [ ] `sc.pl.violin`. - [ ] Preprocessing. - [ ] `sc.pl.filter_genes_dispersion`. - [ ] `sc.pl.highest_expr_genes`. - [ ] `sc.pl.highly_variable_genes`. - [ ] DE. - [ ] `sc.pl.rank_genes_groups`. - [ ] `sc.pl.rank_genes_groups_dotplot`. - [ ] `sc.pl.rank_genes_groups_heatmap`. - [ ] `sc.pl.rank_genes_groups_matrixplot`. - [ ] `sc.pl.rank_genes_groups_stacked_violin`. - [ ] `sc.pl.rank_genes_groups_tracksplot`. - [ ] `sc.pl.rank_genes_groups_violin`. - [ ] Misc/ to be classified. - [ ] `sc.pl.ranking`. - [ ] `sc.pl.scatter`. - [ ] `sc.pl.sim`. - [ ] `sc.pl.correlation_matrix`. - [ ] `sc.pl.matrix`. - [ ] Time series (???). - [ ] `sc.pl.timeseries`. - [ ] `sc.pl.timeseries_as_heatmap`. - [ ] `sc.pl.timeseries_subplot`. Other functions. - [x] `sc.pp.calculate_qc_metrics`. - [x] `sc.tl.embedding_density`. - [ ] `sc.get.obs_df`. - [ ] `sc.get.rank_genes_groups_df`. </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1632
https://github.com/scverse/scanpy/pull/1632:940,deployability,cluster,clustermap,940,"Plotting examples; Re #1604, @fidelram, I've got the sphinx extension working for examples (at least locally) here. You can check out a simple example for `sc.pl.embedding`. Does this solve the problem you were running into? <details>. <summary> Functions which need examples </summary>. Functions in `sc.pl`. - [ ] Embedding plots. - [ ] `sc.pl.embedding`. - [ ] `sc.pl.draw_graph`. - [ ] `sc.pl.diffmap`. - [ ] `sc.pl.pca`. - [ ] `sc.pl.tsne`. - [ ] `sc.pl.umap`. - [ ] `sc.pl.spatial`. - [x] `sc.pl.embedding_density`. - [ ] PCA specific. - [ ] `sc.pl.pca_loadings`. - [ ] `sc.pl.pca_overview`. - [ ] `sc.pl.pca_scatter`. - [ ] `sc.pl.pca_variance_ratio`. - [ ] PAGA. - [ ] `sc.pl.paga`. - [ ] `sc.pl.paga_adjacency`. - [ ] `sc.pl.paga_compare`. - [ ] `sc.pl.paga_path`. - [ ] DPT pseudotime. - [ ] `sc.pl.dpt_groups_pseudotime`. - [ ] `sc.pl.dpt_timeseries`. - [ ] Groupby. - [x] `sc.pl.dotplot`. - [ ] `sc.pl.matrixplot`. - [ ] `sc.pl.clustermap`. - [ ] `sc.pl.heatmap`. - [ ] `sc.pl.dendrogram`. - [ ] `sc.pl.stacked_violin`. - [ ] `sc.pl.tracksplot`. - [ ] `sc.pl.violin`. - [ ] Preprocessing. - [ ] `sc.pl.filter_genes_dispersion`. - [ ] `sc.pl.highest_expr_genes`. - [ ] `sc.pl.highly_variable_genes`. - [ ] DE. - [ ] `sc.pl.rank_genes_groups`. - [ ] `sc.pl.rank_genes_groups_dotplot`. - [ ] `sc.pl.rank_genes_groups_heatmap`. - [ ] `sc.pl.rank_genes_groups_matrixplot`. - [ ] `sc.pl.rank_genes_groups_stacked_violin`. - [ ] `sc.pl.rank_genes_groups_tracksplot`. - [ ] `sc.pl.rank_genes_groups_violin`. - [ ] Misc/ to be classified. - [ ] `sc.pl.ranking`. - [ ] `sc.pl.scatter`. - [ ] `sc.pl.sim`. - [ ] `sc.pl.correlation_matrix`. - [ ] `sc.pl.matrix`. - [ ] Time series (???). - [ ] `sc.pl.timeseries`. - [ ] `sc.pl.timeseries_as_heatmap`. - [ ] `sc.pl.timeseries_subplot`. Other functions. - [x] `sc.pp.calculate_qc_metrics`. - [x] `sc.tl.embedding_density`. - [ ] `sc.get.obs_df`. - [ ] `sc.get.rank_genes_groups_df`. </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1632
https://github.com/scverse/scanpy/pull/1632:966,energy efficiency,heat,heatmap,966,"Plotting examples; Re #1604, @fidelram, I've got the sphinx extension working for examples (at least locally) here. You can check out a simple example for `sc.pl.embedding`. Does this solve the problem you were running into? <details>. <summary> Functions which need examples </summary>. Functions in `sc.pl`. - [ ] Embedding plots. - [ ] `sc.pl.embedding`. - [ ] `sc.pl.draw_graph`. - [ ] `sc.pl.diffmap`. - [ ] `sc.pl.pca`. - [ ] `sc.pl.tsne`. - [ ] `sc.pl.umap`. - [ ] `sc.pl.spatial`. - [x] `sc.pl.embedding_density`. - [ ] PCA specific. - [ ] `sc.pl.pca_loadings`. - [ ] `sc.pl.pca_overview`. - [ ] `sc.pl.pca_scatter`. - [ ] `sc.pl.pca_variance_ratio`. - [ ] PAGA. - [ ] `sc.pl.paga`. - [ ] `sc.pl.paga_adjacency`. - [ ] `sc.pl.paga_compare`. - [ ] `sc.pl.paga_path`. - [ ] DPT pseudotime. - [ ] `sc.pl.dpt_groups_pseudotime`. - [ ] `sc.pl.dpt_timeseries`. - [ ] Groupby. - [x] `sc.pl.dotplot`. - [ ] `sc.pl.matrixplot`. - [ ] `sc.pl.clustermap`. - [ ] `sc.pl.heatmap`. - [ ] `sc.pl.dendrogram`. - [ ] `sc.pl.stacked_violin`. - [ ] `sc.pl.tracksplot`. - [ ] `sc.pl.violin`. - [ ] Preprocessing. - [ ] `sc.pl.filter_genes_dispersion`. - [ ] `sc.pl.highest_expr_genes`. - [ ] `sc.pl.highly_variable_genes`. - [ ] DE. - [ ] `sc.pl.rank_genes_groups`. - [ ] `sc.pl.rank_genes_groups_dotplot`. - [ ] `sc.pl.rank_genes_groups_heatmap`. - [ ] `sc.pl.rank_genes_groups_matrixplot`. - [ ] `sc.pl.rank_genes_groups_stacked_violin`. - [ ] `sc.pl.rank_genes_groups_tracksplot`. - [ ] `sc.pl.rank_genes_groups_violin`. - [ ] Misc/ to be classified. - [ ] `sc.pl.ranking`. - [ ] `sc.pl.scatter`. - [ ] `sc.pl.sim`. - [ ] `sc.pl.correlation_matrix`. - [ ] `sc.pl.matrix`. - [ ] Time series (???). - [ ] `sc.pl.timeseries`. - [ ] `sc.pl.timeseries_as_heatmap`. - [ ] `sc.pl.timeseries_subplot`. Other functions. - [x] `sc.pp.calculate_qc_metrics`. - [x] `sc.tl.embedding_density`. - [ ] `sc.get.obs_df`. - [ ] `sc.get.rank_genes_groups_df`. </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1632
https://github.com/scverse/scanpy/pull/1632:532,interoperability,specif,specific,532,"Plotting examples; Re #1604, @fidelram, I've got the sphinx extension working for examples (at least locally) here. You can check out a simple example for `sc.pl.embedding`. Does this solve the problem you were running into? <details>. <summary> Functions which need examples </summary>. Functions in `sc.pl`. - [ ] Embedding plots. - [ ] `sc.pl.embedding`. - [ ] `sc.pl.draw_graph`. - [ ] `sc.pl.diffmap`. - [ ] `sc.pl.pca`. - [ ] `sc.pl.tsne`. - [ ] `sc.pl.umap`. - [ ] `sc.pl.spatial`. - [x] `sc.pl.embedding_density`. - [ ] PCA specific. - [ ] `sc.pl.pca_loadings`. - [ ] `sc.pl.pca_overview`. - [ ] `sc.pl.pca_scatter`. - [ ] `sc.pl.pca_variance_ratio`. - [ ] PAGA. - [ ] `sc.pl.paga`. - [ ] `sc.pl.paga_adjacency`. - [ ] `sc.pl.paga_compare`. - [ ] `sc.pl.paga_path`. - [ ] DPT pseudotime. - [ ] `sc.pl.dpt_groups_pseudotime`. - [ ] `sc.pl.dpt_timeseries`. - [ ] Groupby. - [x] `sc.pl.dotplot`. - [ ] `sc.pl.matrixplot`. - [ ] `sc.pl.clustermap`. - [ ] `sc.pl.heatmap`. - [ ] `sc.pl.dendrogram`. - [ ] `sc.pl.stacked_violin`. - [ ] `sc.pl.tracksplot`. - [ ] `sc.pl.violin`. - [ ] Preprocessing. - [ ] `sc.pl.filter_genes_dispersion`. - [ ] `sc.pl.highest_expr_genes`. - [ ] `sc.pl.highly_variable_genes`. - [ ] DE. - [ ] `sc.pl.rank_genes_groups`. - [ ] `sc.pl.rank_genes_groups_dotplot`. - [ ] `sc.pl.rank_genes_groups_heatmap`. - [ ] `sc.pl.rank_genes_groups_matrixplot`. - [ ] `sc.pl.rank_genes_groups_stacked_violin`. - [ ] `sc.pl.rank_genes_groups_tracksplot`. - [ ] `sc.pl.rank_genes_groups_violin`. - [ ] Misc/ to be classified. - [ ] `sc.pl.ranking`. - [ ] `sc.pl.scatter`. - [ ] `sc.pl.sim`. - [ ] `sc.pl.correlation_matrix`. - [ ] `sc.pl.matrix`. - [ ] Time series (???). - [ ] `sc.pl.timeseries`. - [ ] `sc.pl.timeseries_as_heatmap`. - [ ] `sc.pl.timeseries_subplot`. Other functions. - [x] `sc.pp.calculate_qc_metrics`. - [x] `sc.tl.embedding_density`. - [ ] `sc.get.obs_df`. - [ ] `sc.get.rank_genes_groups_df`. </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1632
https://github.com/scverse/scanpy/pull/1632:60,modifiability,extens,extension,60,"Plotting examples; Re #1604, @fidelram, I've got the sphinx extension working for examples (at least locally) here. You can check out a simple example for `sc.pl.embedding`. Does this solve the problem you were running into? <details>. <summary> Functions which need examples </summary>. Functions in `sc.pl`. - [ ] Embedding plots. - [ ] `sc.pl.embedding`. - [ ] `sc.pl.draw_graph`. - [ ] `sc.pl.diffmap`. - [ ] `sc.pl.pca`. - [ ] `sc.pl.tsne`. - [ ] `sc.pl.umap`. - [ ] `sc.pl.spatial`. - [x] `sc.pl.embedding_density`. - [ ] PCA specific. - [ ] `sc.pl.pca_loadings`. - [ ] `sc.pl.pca_overview`. - [ ] `sc.pl.pca_scatter`. - [ ] `sc.pl.pca_variance_ratio`. - [ ] PAGA. - [ ] `sc.pl.paga`. - [ ] `sc.pl.paga_adjacency`. - [ ] `sc.pl.paga_compare`. - [ ] `sc.pl.paga_path`. - [ ] DPT pseudotime. - [ ] `sc.pl.dpt_groups_pseudotime`. - [ ] `sc.pl.dpt_timeseries`. - [ ] Groupby. - [x] `sc.pl.dotplot`. - [ ] `sc.pl.matrixplot`. - [ ] `sc.pl.clustermap`. - [ ] `sc.pl.heatmap`. - [ ] `sc.pl.dendrogram`. - [ ] `sc.pl.stacked_violin`. - [ ] `sc.pl.tracksplot`. - [ ] `sc.pl.violin`. - [ ] Preprocessing. - [ ] `sc.pl.filter_genes_dispersion`. - [ ] `sc.pl.highest_expr_genes`. - [ ] `sc.pl.highly_variable_genes`. - [ ] DE. - [ ] `sc.pl.rank_genes_groups`. - [ ] `sc.pl.rank_genes_groups_dotplot`. - [ ] `sc.pl.rank_genes_groups_heatmap`. - [ ] `sc.pl.rank_genes_groups_matrixplot`. - [ ] `sc.pl.rank_genes_groups_stacked_violin`. - [ ] `sc.pl.rank_genes_groups_tracksplot`. - [ ] `sc.pl.rank_genes_groups_violin`. - [ ] Misc/ to be classified. - [ ] `sc.pl.ranking`. - [ ] `sc.pl.scatter`. - [ ] `sc.pl.sim`. - [ ] `sc.pl.correlation_matrix`. - [ ] `sc.pl.matrix`. - [ ] Time series (???). - [ ] `sc.pl.timeseries`. - [ ] `sc.pl.timeseries_as_heatmap`. - [ ] `sc.pl.timeseries_subplot`. Other functions. - [x] `sc.pp.calculate_qc_metrics`. - [x] `sc.tl.embedding_density`. - [ ] `sc.get.obs_df`. - [ ] `sc.get.rank_genes_groups_df`. </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1632
https://github.com/scverse/scanpy/pull/1632:1669,performance,Time,Time,1669,"Plotting examples; Re #1604, @fidelram, I've got the sphinx extension working for examples (at least locally) here. You can check out a simple example for `sc.pl.embedding`. Does this solve the problem you were running into? <details>. <summary> Functions which need examples </summary>. Functions in `sc.pl`. - [ ] Embedding plots. - [ ] `sc.pl.embedding`. - [ ] `sc.pl.draw_graph`. - [ ] `sc.pl.diffmap`. - [ ] `sc.pl.pca`. - [ ] `sc.pl.tsne`. - [ ] `sc.pl.umap`. - [ ] `sc.pl.spatial`. - [x] `sc.pl.embedding_density`. - [ ] PCA specific. - [ ] `sc.pl.pca_loadings`. - [ ] `sc.pl.pca_overview`. - [ ] `sc.pl.pca_scatter`. - [ ] `sc.pl.pca_variance_ratio`. - [ ] PAGA. - [ ] `sc.pl.paga`. - [ ] `sc.pl.paga_adjacency`. - [ ] `sc.pl.paga_compare`. - [ ] `sc.pl.paga_path`. - [ ] DPT pseudotime. - [ ] `sc.pl.dpt_groups_pseudotime`. - [ ] `sc.pl.dpt_timeseries`. - [ ] Groupby. - [x] `sc.pl.dotplot`. - [ ] `sc.pl.matrixplot`. - [ ] `sc.pl.clustermap`. - [ ] `sc.pl.heatmap`. - [ ] `sc.pl.dendrogram`. - [ ] `sc.pl.stacked_violin`. - [ ] `sc.pl.tracksplot`. - [ ] `sc.pl.violin`. - [ ] Preprocessing. - [ ] `sc.pl.filter_genes_dispersion`. - [ ] `sc.pl.highest_expr_genes`. - [ ] `sc.pl.highly_variable_genes`. - [ ] DE. - [ ] `sc.pl.rank_genes_groups`. - [ ] `sc.pl.rank_genes_groups_dotplot`. - [ ] `sc.pl.rank_genes_groups_heatmap`. - [ ] `sc.pl.rank_genes_groups_matrixplot`. - [ ] `sc.pl.rank_genes_groups_stacked_violin`. - [ ] `sc.pl.rank_genes_groups_tracksplot`. - [ ] `sc.pl.rank_genes_groups_violin`. - [ ] Misc/ to be classified. - [ ] `sc.pl.ranking`. - [ ] `sc.pl.scatter`. - [ ] `sc.pl.sim`. - [ ] `sc.pl.correlation_matrix`. - [ ] `sc.pl.matrix`. - [ ] Time series (???). - [ ] `sc.pl.timeseries`. - [ ] `sc.pl.timeseries_as_heatmap`. - [ ] `sc.pl.timeseries_subplot`. Other functions. - [x] `sc.pp.calculate_qc_metrics`. - [x] `sc.tl.embedding_density`. - [ ] `sc.get.obs_df`. - [ ] `sc.get.rank_genes_groups_df`. </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1632
https://github.com/scverse/scanpy/pull/1632:1701,performance,time,timeseries,1701,"Plotting examples; Re #1604, @fidelram, I've got the sphinx extension working for examples (at least locally) here. You can check out a simple example for `sc.pl.embedding`. Does this solve the problem you were running into? <details>. <summary> Functions which need examples </summary>. Functions in `sc.pl`. - [ ] Embedding plots. - [ ] `sc.pl.embedding`. - [ ] `sc.pl.draw_graph`. - [ ] `sc.pl.diffmap`. - [ ] `sc.pl.pca`. - [ ] `sc.pl.tsne`. - [ ] `sc.pl.umap`. - [ ] `sc.pl.spatial`. - [x] `sc.pl.embedding_density`. - [ ] PCA specific. - [ ] `sc.pl.pca_loadings`. - [ ] `sc.pl.pca_overview`. - [ ] `sc.pl.pca_scatter`. - [ ] `sc.pl.pca_variance_ratio`. - [ ] PAGA. - [ ] `sc.pl.paga`. - [ ] `sc.pl.paga_adjacency`. - [ ] `sc.pl.paga_compare`. - [ ] `sc.pl.paga_path`. - [ ] DPT pseudotime. - [ ] `sc.pl.dpt_groups_pseudotime`. - [ ] `sc.pl.dpt_timeseries`. - [ ] Groupby. - [x] `sc.pl.dotplot`. - [ ] `sc.pl.matrixplot`. - [ ] `sc.pl.clustermap`. - [ ] `sc.pl.heatmap`. - [ ] `sc.pl.dendrogram`. - [ ] `sc.pl.stacked_violin`. - [ ] `sc.pl.tracksplot`. - [ ] `sc.pl.violin`. - [ ] Preprocessing. - [ ] `sc.pl.filter_genes_dispersion`. - [ ] `sc.pl.highest_expr_genes`. - [ ] `sc.pl.highly_variable_genes`. - [ ] DE. - [ ] `sc.pl.rank_genes_groups`. - [ ] `sc.pl.rank_genes_groups_dotplot`. - [ ] `sc.pl.rank_genes_groups_heatmap`. - [ ] `sc.pl.rank_genes_groups_matrixplot`. - [ ] `sc.pl.rank_genes_groups_stacked_violin`. - [ ] `sc.pl.rank_genes_groups_tracksplot`. - [ ] `sc.pl.rank_genes_groups_violin`. - [ ] Misc/ to be classified. - [ ] `sc.pl.ranking`. - [ ] `sc.pl.scatter`. - [ ] `sc.pl.sim`. - [ ] `sc.pl.correlation_matrix`. - [ ] `sc.pl.matrix`. - [ ] Time series (???). - [ ] `sc.pl.timeseries`. - [ ] `sc.pl.timeseries_as_heatmap`. - [ ] `sc.pl.timeseries_subplot`. Other functions. - [x] `sc.pp.calculate_qc_metrics`. - [x] `sc.tl.embedding_density`. - [ ] `sc.get.obs_df`. - [ ] `sc.get.rank_genes_groups_df`. </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1632
https://github.com/scverse/scanpy/pull/1632:174,reliability,Doe,Does,174,"Plotting examples; Re #1604, @fidelram, I've got the sphinx extension working for examples (at least locally) here. You can check out a simple example for `sc.pl.embedding`. Does this solve the problem you were running into? <details>. <summary> Functions which need examples </summary>. Functions in `sc.pl`. - [ ] Embedding plots. - [ ] `sc.pl.embedding`. - [ ] `sc.pl.draw_graph`. - [ ] `sc.pl.diffmap`. - [ ] `sc.pl.pca`. - [ ] `sc.pl.tsne`. - [ ] `sc.pl.umap`. - [ ] `sc.pl.spatial`. - [x] `sc.pl.embedding_density`. - [ ] PCA specific. - [ ] `sc.pl.pca_loadings`. - [ ] `sc.pl.pca_overview`. - [ ] `sc.pl.pca_scatter`. - [ ] `sc.pl.pca_variance_ratio`. - [ ] PAGA. - [ ] `sc.pl.paga`. - [ ] `sc.pl.paga_adjacency`. - [ ] `sc.pl.paga_compare`. - [ ] `sc.pl.paga_path`. - [ ] DPT pseudotime. - [ ] `sc.pl.dpt_groups_pseudotime`. - [ ] `sc.pl.dpt_timeseries`. - [ ] Groupby. - [x] `sc.pl.dotplot`. - [ ] `sc.pl.matrixplot`. - [ ] `sc.pl.clustermap`. - [ ] `sc.pl.heatmap`. - [ ] `sc.pl.dendrogram`. - [ ] `sc.pl.stacked_violin`. - [ ] `sc.pl.tracksplot`. - [ ] `sc.pl.violin`. - [ ] Preprocessing. - [ ] `sc.pl.filter_genes_dispersion`. - [ ] `sc.pl.highest_expr_genes`. - [ ] `sc.pl.highly_variable_genes`. - [ ] DE. - [ ] `sc.pl.rank_genes_groups`. - [ ] `sc.pl.rank_genes_groups_dotplot`. - [ ] `sc.pl.rank_genes_groups_heatmap`. - [ ] `sc.pl.rank_genes_groups_matrixplot`. - [ ] `sc.pl.rank_genes_groups_stacked_violin`. - [ ] `sc.pl.rank_genes_groups_tracksplot`. - [ ] `sc.pl.rank_genes_groups_violin`. - [ ] Misc/ to be classified. - [ ] `sc.pl.ranking`. - [ ] `sc.pl.scatter`. - [ ] `sc.pl.sim`. - [ ] `sc.pl.correlation_matrix`. - [ ] `sc.pl.matrix`. - [ ] Time series (???). - [ ] `sc.pl.timeseries`. - [ ] `sc.pl.timeseries_as_heatmap`. - [ ] `sc.pl.timeseries_subplot`. Other functions. - [x] `sc.pp.calculate_qc_metrics`. - [x] `sc.tl.embedding_density`. - [ ] `sc.get.obs_df`. - [ ] `sc.get.rank_genes_groups_df`. </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1632
https://github.com/scverse/scanpy/pull/1632:136,testability,simpl,simple,136,"Plotting examples; Re #1604, @fidelram, I've got the sphinx extension working for examples (at least locally) here. You can check out a simple example for `sc.pl.embedding`. Does this solve the problem you were running into? <details>. <summary> Functions which need examples </summary>. Functions in `sc.pl`. - [ ] Embedding plots. - [ ] `sc.pl.embedding`. - [ ] `sc.pl.draw_graph`. - [ ] `sc.pl.diffmap`. - [ ] `sc.pl.pca`. - [ ] `sc.pl.tsne`. - [ ] `sc.pl.umap`. - [ ] `sc.pl.spatial`. - [x] `sc.pl.embedding_density`. - [ ] PCA specific. - [ ] `sc.pl.pca_loadings`. - [ ] `sc.pl.pca_overview`. - [ ] `sc.pl.pca_scatter`. - [ ] `sc.pl.pca_variance_ratio`. - [ ] PAGA. - [ ] `sc.pl.paga`. - [ ] `sc.pl.paga_adjacency`. - [ ] `sc.pl.paga_compare`. - [ ] `sc.pl.paga_path`. - [ ] DPT pseudotime. - [ ] `sc.pl.dpt_groups_pseudotime`. - [ ] `sc.pl.dpt_timeseries`. - [ ] Groupby. - [x] `sc.pl.dotplot`. - [ ] `sc.pl.matrixplot`. - [ ] `sc.pl.clustermap`. - [ ] `sc.pl.heatmap`. - [ ] `sc.pl.dendrogram`. - [ ] `sc.pl.stacked_violin`. - [ ] `sc.pl.tracksplot`. - [ ] `sc.pl.violin`. - [ ] Preprocessing. - [ ] `sc.pl.filter_genes_dispersion`. - [ ] `sc.pl.highest_expr_genes`. - [ ] `sc.pl.highly_variable_genes`. - [ ] DE. - [ ] `sc.pl.rank_genes_groups`. - [ ] `sc.pl.rank_genes_groups_dotplot`. - [ ] `sc.pl.rank_genes_groups_heatmap`. - [ ] `sc.pl.rank_genes_groups_matrixplot`. - [ ] `sc.pl.rank_genes_groups_stacked_violin`. - [ ] `sc.pl.rank_genes_groups_tracksplot`. - [ ] `sc.pl.rank_genes_groups_violin`. - [ ] Misc/ to be classified. - [ ] `sc.pl.ranking`. - [ ] `sc.pl.scatter`. - [ ] `sc.pl.sim`. - [ ] `sc.pl.correlation_matrix`. - [ ] `sc.pl.matrix`. - [ ] Time series (???). - [ ] `sc.pl.timeseries`. - [ ] `sc.pl.timeseries_as_heatmap`. - [ ] `sc.pl.timeseries_subplot`. Other functions. - [x] `sc.pp.calculate_qc_metrics`. - [x] `sc.tl.embedding_density`. - [ ] `sc.get.obs_df`. - [ ] `sc.get.rank_genes_groups_df`. </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1632
https://github.com/scverse/scanpy/pull/1632:136,usability,simpl,simple,136,"Plotting examples; Re #1604, @fidelram, I've got the sphinx extension working for examples (at least locally) here. You can check out a simple example for `sc.pl.embedding`. Does this solve the problem you were running into? <details>. <summary> Functions which need examples </summary>. Functions in `sc.pl`. - [ ] Embedding plots. - [ ] `sc.pl.embedding`. - [ ] `sc.pl.draw_graph`. - [ ] `sc.pl.diffmap`. - [ ] `sc.pl.pca`. - [ ] `sc.pl.tsne`. - [ ] `sc.pl.umap`. - [ ] `sc.pl.spatial`. - [x] `sc.pl.embedding_density`. - [ ] PCA specific. - [ ] `sc.pl.pca_loadings`. - [ ] `sc.pl.pca_overview`. - [ ] `sc.pl.pca_scatter`. - [ ] `sc.pl.pca_variance_ratio`. - [ ] PAGA. - [ ] `sc.pl.paga`. - [ ] `sc.pl.paga_adjacency`. - [ ] `sc.pl.paga_compare`. - [ ] `sc.pl.paga_path`. - [ ] DPT pseudotime. - [ ] `sc.pl.dpt_groups_pseudotime`. - [ ] `sc.pl.dpt_timeseries`. - [ ] Groupby. - [x] `sc.pl.dotplot`. - [ ] `sc.pl.matrixplot`. - [ ] `sc.pl.clustermap`. - [ ] `sc.pl.heatmap`. - [ ] `sc.pl.dendrogram`. - [ ] `sc.pl.stacked_violin`. - [ ] `sc.pl.tracksplot`. - [ ] `sc.pl.violin`. - [ ] Preprocessing. - [ ] `sc.pl.filter_genes_dispersion`. - [ ] `sc.pl.highest_expr_genes`. - [ ] `sc.pl.highly_variable_genes`. - [ ] DE. - [ ] `sc.pl.rank_genes_groups`. - [ ] `sc.pl.rank_genes_groups_dotplot`. - [ ] `sc.pl.rank_genes_groups_heatmap`. - [ ] `sc.pl.rank_genes_groups_matrixplot`. - [ ] `sc.pl.rank_genes_groups_stacked_violin`. - [ ] `sc.pl.rank_genes_groups_tracksplot`. - [ ] `sc.pl.rank_genes_groups_violin`. - [ ] Misc/ to be classified. - [ ] `sc.pl.ranking`. - [ ] `sc.pl.scatter`. - [ ] `sc.pl.sim`. - [ ] `sc.pl.correlation_matrix`. - [ ] `sc.pl.matrix`. - [ ] Time series (???). - [ ] `sc.pl.timeseries`. - [ ] `sc.pl.timeseries_as_heatmap`. - [ ] `sc.pl.timeseries_subplot`. Other functions. - [x] `sc.pp.calculate_qc_metrics`. - [x] `sc.tl.embedding_density`. - [ ] `sc.get.obs_df`. - [ ] `sc.get.rank_genes_groups_df`. </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1632
https://github.com/scverse/scanpy/issues/1633:1645,availability,down,downside,1645,"lt colormap shown by #1632 Other methods that set default parameters are also affected like `.add_totals()`. The following example should show the dots using the `Reds` colormap, but instead it uses the `winter` colormap because the second call sets the color map to `winter` if not given. This double call happens because when `sc.pl.dotplot()` is used (instead of `sc.pl.DotPlot`), internally a call to `.style()` is made and a subsequent explicit calls to `.style()` is required to tune the parameters as suggested in the documentation. ```python. adata = sc.datasets.pbmc68k_reduced(). markers = ['C1QA', 'PSAP', 'CD79A', 'CD79B', 'CST3', 'LYZ']. sc.pl.DotPlot(adata, markers, groupby='bulk_labels').style(cmap='Reds').style(dot_edge_color='black').show(). ```. ![image](https://user-images.githubusercontent.com/4964309/107354555-9628de00-6ace-11eb-9eb8-c0baaa80b1f6.png). The problem is caused by the current implementation of `sc.pl.Dotplot.style()` that set the default parameters as:. ```. def style(. self,. cmap: str = DEFAULT_COLORMAP,. color_on: Optional[Literal['dot', 'square']] = DEFAULT_COLOR_ON,. dot_max: Optional[float] = DEFAULT_DOT_MAX,. dot_min: Optional[float] = DEFAULT_DOT_MIN,. ..... ```. Where DEFAULT_* are the default values defined at the beginning of the file (see https://github.com/theislab/scanpy/blob/master/scanpy/plotting/_dotplot.py#L84) . What is nice about this is that the documentation clearly shows the default values. The downside is that optional values are assigned a default value that rewrites previous calls to style. Ideally, all optional values should be `None`, then is easy to know if a new value is passed or a previous call has already set a value. But, doing so will remove the defaults from the documentation. @flying-sheep suggested to use a code he wrote to add default annotations to the documentation. https://github.com/theislab/scanpydoc/blob/875b441212830678cf9fc81c52f5af29bbb8715f/scanpydoc/elegant_typehints/formatting.py#L101-L107.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1633
https://github.com/scverse/scanpy/issues/1633:145,deployability,observ,observed,145,"DotPlot `.style()` resets previous value assignments; Multiple calls to `sc.pl.Dotplot.style()` cause resetting of parameters. This issue can be observed as a change in the default colormap shown by #1632 Other methods that set default parameters are also affected like `.add_totals()`. The following example should show the dots using the `Reds` colormap, but instead it uses the `winter` colormap because the second call sets the color map to `winter` if not given. This double call happens because when `sc.pl.dotplot()` is used (instead of `sc.pl.DotPlot`), internally a call to `.style()` is made and a subsequent explicit calls to `.style()` is required to tune the parameters as suggested in the documentation. ```python. adata = sc.datasets.pbmc68k_reduced(). markers = ['C1QA', 'PSAP', 'CD79A', 'CD79B', 'CST3', 'LYZ']. sc.pl.DotPlot(adata, markers, groupby='bulk_labels').style(cmap='Reds').style(dot_edge_color='black').show(). ```. ![image](https://user-images.githubusercontent.com/4964309/107354555-9628de00-6ace-11eb-9eb8-c0baaa80b1f6.png). The problem is caused by the current implementation of `sc.pl.Dotplot.style()` that set the default parameters as:. ```. def style(. self,. cmap: str = DEFAULT_COLORMAP,. color_on: Optional[Literal['dot', 'square']] = DEFAULT_COLOR_ON,. dot_max: Optional[float] = DEFAULT_DOT_MAX,. dot_min: Optional[float] = DEFAULT_DOT_MIN,. ..... ```. Where DEFAULT_* are the default values defined at the beginning of the file (see https://github.com/theislab/scanpy/blob/master/scanpy/plotting/_dotplot.py#L84) . What is nice about this is that the documentation clearly shows the default values. The downside is that optional values are assigned a default value that rewrites previous calls to style. Ideally, all optional values should be `None`, then is easy to know if a new value is passed or a previous call has already set a value. But, doing so will remove the defaults from the documentation. @flying-sheep suggested to use a code he wrote to add ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1633
https://github.com/scverse/scanpy/issues/1633:1085,energy efficiency,current,current,1085,"yle()` cause resetting of parameters. This issue can be observed as a change in the default colormap shown by #1632 Other methods that set default parameters are also affected like `.add_totals()`. The following example should show the dots using the `Reds` colormap, but instead it uses the `winter` colormap because the second call sets the color map to `winter` if not given. This double call happens because when `sc.pl.dotplot()` is used (instead of `sc.pl.DotPlot`), internally a call to `.style()` is made and a subsequent explicit calls to `.style()` is required to tune the parameters as suggested in the documentation. ```python. adata = sc.datasets.pbmc68k_reduced(). markers = ['C1QA', 'PSAP', 'CD79A', 'CD79B', 'CST3', 'LYZ']. sc.pl.DotPlot(adata, markers, groupby='bulk_labels').style(cmap='Reds').style(dot_edge_color='black').show(). ```. ![image](https://user-images.githubusercontent.com/4964309/107354555-9628de00-6ace-11eb-9eb8-c0baaa80b1f6.png). The problem is caused by the current implementation of `sc.pl.Dotplot.style()` that set the default parameters as:. ```. def style(. self,. cmap: str = DEFAULT_COLORMAP,. color_on: Optional[Literal['dot', 'square']] = DEFAULT_COLOR_ON,. dot_max: Optional[float] = DEFAULT_DOT_MAX,. dot_min: Optional[float] = DEFAULT_DOT_MIN,. ..... ```. Where DEFAULT_* are the default values defined at the beginning of the file (see https://github.com/theislab/scanpy/blob/master/scanpy/plotting/_dotplot.py#L84) . What is nice about this is that the documentation clearly shows the default values. The downside is that optional values are assigned a default value that rewrites previous calls to style. Ideally, all optional values should be `None`, then is easy to know if a new value is passed or a previous call has already set a value. But, doing so will remove the defaults from the documentation. @flying-sheep suggested to use a code he wrote to add default annotations to the documentation. https://github.com/theislab/scanpydoc/blob/875b",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1633
https://github.com/scverse/scanpy/issues/1633:608,integrability,sub,subsequent,608,"DotPlot `.style()` resets previous value assignments; Multiple calls to `sc.pl.Dotplot.style()` cause resetting of parameters. This issue can be observed as a change in the default colormap shown by #1632 Other methods that set default parameters are also affected like `.add_totals()`. The following example should show the dots using the `Reds` colormap, but instead it uses the `winter` colormap because the second call sets the color map to `winter` if not given. This double call happens because when `sc.pl.dotplot()` is used (instead of `sc.pl.DotPlot`), internally a call to `.style()` is made and a subsequent explicit calls to `.style()` is required to tune the parameters as suggested in the documentation. ```python. adata = sc.datasets.pbmc68k_reduced(). markers = ['C1QA', 'PSAP', 'CD79A', 'CD79B', 'CST3', 'LYZ']. sc.pl.DotPlot(adata, markers, groupby='bulk_labels').style(cmap='Reds').style(dot_edge_color='black').show(). ```. ![image](https://user-images.githubusercontent.com/4964309/107354555-9628de00-6ace-11eb-9eb8-c0baaa80b1f6.png). The problem is caused by the current implementation of `sc.pl.Dotplot.style()` that set the default parameters as:. ```. def style(. self,. cmap: str = DEFAULT_COLORMAP,. color_on: Optional[Literal['dot', 'square']] = DEFAULT_COLOR_ON,. dot_max: Optional[float] = DEFAULT_DOT_MAX,. dot_min: Optional[float] = DEFAULT_DOT_MIN,. ..... ```. Where DEFAULT_* are the default values defined at the beginning of the file (see https://github.com/theislab/scanpy/blob/master/scanpy/plotting/_dotplot.py#L84) . What is nice about this is that the documentation clearly shows the default values. The downside is that optional values are assigned a default value that rewrites previous calls to style. Ideally, all optional values should be `None`, then is easy to know if a new value is passed or a previous call has already set a value. But, doing so will remove the defaults from the documentation. @flying-sheep suggested to use a code he wrote to add ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1633
https://github.com/scverse/scanpy/issues/1633:2154,interoperability,format,formatting,2154,"lt colormap shown by #1632 Other methods that set default parameters are also affected like `.add_totals()`. The following example should show the dots using the `Reds` colormap, but instead it uses the `winter` colormap because the second call sets the color map to `winter` if not given. This double call happens because when `sc.pl.dotplot()` is used (instead of `sc.pl.DotPlot`), internally a call to `.style()` is made and a subsequent explicit calls to `.style()` is required to tune the parameters as suggested in the documentation. ```python. adata = sc.datasets.pbmc68k_reduced(). markers = ['C1QA', 'PSAP', 'CD79A', 'CD79B', 'CST3', 'LYZ']. sc.pl.DotPlot(adata, markers, groupby='bulk_labels').style(cmap='Reds').style(dot_edge_color='black').show(). ```. ![image](https://user-images.githubusercontent.com/4964309/107354555-9628de00-6ace-11eb-9eb8-c0baaa80b1f6.png). The problem is caused by the current implementation of `sc.pl.Dotplot.style()` that set the default parameters as:. ```. def style(. self,. cmap: str = DEFAULT_COLORMAP,. color_on: Optional[Literal['dot', 'square']] = DEFAULT_COLOR_ON,. dot_max: Optional[float] = DEFAULT_DOT_MAX,. dot_min: Optional[float] = DEFAULT_DOT_MIN,. ..... ```. Where DEFAULT_* are the default values defined at the beginning of the file (see https://github.com/theislab/scanpy/blob/master/scanpy/plotting/_dotplot.py#L84) . What is nice about this is that the documentation clearly shows the default values. The downside is that optional values are assigned a default value that rewrites previous calls to style. Ideally, all optional values should be `None`, then is easy to know if a new value is passed or a previous call has already set a value. But, doing so will remove the defaults from the documentation. @flying-sheep suggested to use a code he wrote to add default annotations to the documentation. https://github.com/theislab/scanpydoc/blob/875b441212830678cf9fc81c52f5af29bbb8715f/scanpydoc/elegant_typehints/formatting.py#L101-L107.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1633
https://github.com/scverse/scanpy/issues/1633:115,modifiability,paramet,parameters,115,"DotPlot `.style()` resets previous value assignments; Multiple calls to `sc.pl.Dotplot.style()` cause resetting of parameters. This issue can be observed as a change in the default colormap shown by #1632 Other methods that set default parameters are also affected like `.add_totals()`. The following example should show the dots using the `Reds` colormap, but instead it uses the `winter` colormap because the second call sets the color map to `winter` if not given. This double call happens because when `sc.pl.dotplot()` is used (instead of `sc.pl.DotPlot`), internally a call to `.style()` is made and a subsequent explicit calls to `.style()` is required to tune the parameters as suggested in the documentation. ```python. adata = sc.datasets.pbmc68k_reduced(). markers = ['C1QA', 'PSAP', 'CD79A', 'CD79B', 'CST3', 'LYZ']. sc.pl.DotPlot(adata, markers, groupby='bulk_labels').style(cmap='Reds').style(dot_edge_color='black').show(). ```. ![image](https://user-images.githubusercontent.com/4964309/107354555-9628de00-6ace-11eb-9eb8-c0baaa80b1f6.png). The problem is caused by the current implementation of `sc.pl.Dotplot.style()` that set the default parameters as:. ```. def style(. self,. cmap: str = DEFAULT_COLORMAP,. color_on: Optional[Literal['dot', 'square']] = DEFAULT_COLOR_ON,. dot_max: Optional[float] = DEFAULT_DOT_MAX,. dot_min: Optional[float] = DEFAULT_DOT_MIN,. ..... ```. Where DEFAULT_* are the default values defined at the beginning of the file (see https://github.com/theislab/scanpy/blob/master/scanpy/plotting/_dotplot.py#L84) . What is nice about this is that the documentation clearly shows the default values. The downside is that optional values are assigned a default value that rewrites previous calls to style. Ideally, all optional values should be `None`, then is easy to know if a new value is passed or a previous call has already set a value. But, doing so will remove the defaults from the documentation. @flying-sheep suggested to use a code he wrote to add ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1633
https://github.com/scverse/scanpy/issues/1633:236,modifiability,paramet,parameters,236,"DotPlot `.style()` resets previous value assignments; Multiple calls to `sc.pl.Dotplot.style()` cause resetting of parameters. This issue can be observed as a change in the default colormap shown by #1632 Other methods that set default parameters are also affected like `.add_totals()`. The following example should show the dots using the `Reds` colormap, but instead it uses the `winter` colormap because the second call sets the color map to `winter` if not given. This double call happens because when `sc.pl.dotplot()` is used (instead of `sc.pl.DotPlot`), internally a call to `.style()` is made and a subsequent explicit calls to `.style()` is required to tune the parameters as suggested in the documentation. ```python. adata = sc.datasets.pbmc68k_reduced(). markers = ['C1QA', 'PSAP', 'CD79A', 'CD79B', 'CST3', 'LYZ']. sc.pl.DotPlot(adata, markers, groupby='bulk_labels').style(cmap='Reds').style(dot_edge_color='black').show(). ```. ![image](https://user-images.githubusercontent.com/4964309/107354555-9628de00-6ace-11eb-9eb8-c0baaa80b1f6.png). The problem is caused by the current implementation of `sc.pl.Dotplot.style()` that set the default parameters as:. ```. def style(. self,. cmap: str = DEFAULT_COLORMAP,. color_on: Optional[Literal['dot', 'square']] = DEFAULT_COLOR_ON,. dot_max: Optional[float] = DEFAULT_DOT_MAX,. dot_min: Optional[float] = DEFAULT_DOT_MIN,. ..... ```. Where DEFAULT_* are the default values defined at the beginning of the file (see https://github.com/theislab/scanpy/blob/master/scanpy/plotting/_dotplot.py#L84) . What is nice about this is that the documentation clearly shows the default values. The downside is that optional values are assigned a default value that rewrites previous calls to style. Ideally, all optional values should be `None`, then is easy to know if a new value is passed or a previous call has already set a value. But, doing so will remove the defaults from the documentation. @flying-sheep suggested to use a code he wrote to add ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1633
https://github.com/scverse/scanpy/issues/1633:672,modifiability,paramet,parameters,672,"DotPlot `.style()` resets previous value assignments; Multiple calls to `sc.pl.Dotplot.style()` cause resetting of parameters. This issue can be observed as a change in the default colormap shown by #1632 Other methods that set default parameters are also affected like `.add_totals()`. The following example should show the dots using the `Reds` colormap, but instead it uses the `winter` colormap because the second call sets the color map to `winter` if not given. This double call happens because when `sc.pl.dotplot()` is used (instead of `sc.pl.DotPlot`), internally a call to `.style()` is made and a subsequent explicit calls to `.style()` is required to tune the parameters as suggested in the documentation. ```python. adata = sc.datasets.pbmc68k_reduced(). markers = ['C1QA', 'PSAP', 'CD79A', 'CD79B', 'CST3', 'LYZ']. sc.pl.DotPlot(adata, markers, groupby='bulk_labels').style(cmap='Reds').style(dot_edge_color='black').show(). ```. ![image](https://user-images.githubusercontent.com/4964309/107354555-9628de00-6ace-11eb-9eb8-c0baaa80b1f6.png). The problem is caused by the current implementation of `sc.pl.Dotplot.style()` that set the default parameters as:. ```. def style(. self,. cmap: str = DEFAULT_COLORMAP,. color_on: Optional[Literal['dot', 'square']] = DEFAULT_COLOR_ON,. dot_max: Optional[float] = DEFAULT_DOT_MAX,. dot_min: Optional[float] = DEFAULT_DOT_MIN,. ..... ```. Where DEFAULT_* are the default values defined at the beginning of the file (see https://github.com/theislab/scanpy/blob/master/scanpy/plotting/_dotplot.py#L84) . What is nice about this is that the documentation clearly shows the default values. The downside is that optional values are assigned a default value that rewrites previous calls to style. Ideally, all optional values should be `None`, then is easy to know if a new value is passed or a previous call has already set a value. But, doing so will remove the defaults from the documentation. @flying-sheep suggested to use a code he wrote to add ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1633
https://github.com/scverse/scanpy/issues/1633:1156,modifiability,paramet,parameters,1156,"ange in the default colormap shown by #1632 Other methods that set default parameters are also affected like `.add_totals()`. The following example should show the dots using the `Reds` colormap, but instead it uses the `winter` colormap because the second call sets the color map to `winter` if not given. This double call happens because when `sc.pl.dotplot()` is used (instead of `sc.pl.DotPlot`), internally a call to `.style()` is made and a subsequent explicit calls to `.style()` is required to tune the parameters as suggested in the documentation. ```python. adata = sc.datasets.pbmc68k_reduced(). markers = ['C1QA', 'PSAP', 'CD79A', 'CD79B', 'CST3', 'LYZ']. sc.pl.DotPlot(adata, markers, groupby='bulk_labels').style(cmap='Reds').style(dot_edge_color='black').show(). ```. ![image](https://user-images.githubusercontent.com/4964309/107354555-9628de00-6ace-11eb-9eb8-c0baaa80b1f6.png). The problem is caused by the current implementation of `sc.pl.Dotplot.style()` that set the default parameters as:. ```. def style(. self,. cmap: str = DEFAULT_COLORMAP,. color_on: Optional[Literal['dot', 'square']] = DEFAULT_COLOR_ON,. dot_max: Optional[float] = DEFAULT_DOT_MAX,. dot_min: Optional[float] = DEFAULT_DOT_MIN,. ..... ```. Where DEFAULT_* are the default values defined at the beginning of the file (see https://github.com/theislab/scanpy/blob/master/scanpy/plotting/_dotplot.py#L84) . What is nice about this is that the documentation clearly shows the default values. The downside is that optional values are assigned a default value that rewrites previous calls to style. Ideally, all optional values should be `None`, then is easy to know if a new value is passed or a previous call has already set a value. But, doing so will remove the defaults from the documentation. @flying-sheep suggested to use a code he wrote to add default annotations to the documentation. https://github.com/theislab/scanpydoc/blob/875b441212830678cf9fc81c52f5af29bbb8715f/scanpydoc/elegant_typehints/formatt",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1633
https://github.com/scverse/scanpy/issues/1633:663,performance,tune,tune,663,"DotPlot `.style()` resets previous value assignments; Multiple calls to `sc.pl.Dotplot.style()` cause resetting of parameters. This issue can be observed as a change in the default colormap shown by #1632 Other methods that set default parameters are also affected like `.add_totals()`. The following example should show the dots using the `Reds` colormap, but instead it uses the `winter` colormap because the second call sets the color map to `winter` if not given. This double call happens because when `sc.pl.dotplot()` is used (instead of `sc.pl.DotPlot`), internally a call to `.style()` is made and a subsequent explicit calls to `.style()` is required to tune the parameters as suggested in the documentation. ```python. adata = sc.datasets.pbmc68k_reduced(). markers = ['C1QA', 'PSAP', 'CD79A', 'CD79B', 'CST3', 'LYZ']. sc.pl.DotPlot(adata, markers, groupby='bulk_labels').style(cmap='Reds').style(dot_edge_color='black').show(). ```. ![image](https://user-images.githubusercontent.com/4964309/107354555-9628de00-6ace-11eb-9eb8-c0baaa80b1f6.png). The problem is caused by the current implementation of `sc.pl.Dotplot.style()` that set the default parameters as:. ```. def style(. self,. cmap: str = DEFAULT_COLORMAP,. color_on: Optional[Literal['dot', 'square']] = DEFAULT_COLOR_ON,. dot_max: Optional[float] = DEFAULT_DOT_MAX,. dot_min: Optional[float] = DEFAULT_DOT_MIN,. ..... ```. Where DEFAULT_* are the default values defined at the beginning of the file (see https://github.com/theislab/scanpy/blob/master/scanpy/plotting/_dotplot.py#L84) . What is nice about this is that the documentation clearly shows the default values. The downside is that optional values are assigned a default value that rewrites previous calls to style. Ideally, all optional values should be `None`, then is easy to know if a new value is passed or a previous call has already set a value. But, doing so will remove the defaults from the documentation. @flying-sheep suggested to use a code he wrote to add ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1633
https://github.com/scverse/scanpy/issues/1633:145,testability,observ,observed,145,"DotPlot `.style()` resets previous value assignments; Multiple calls to `sc.pl.Dotplot.style()` cause resetting of parameters. This issue can be observed as a change in the default colormap shown by #1632 Other methods that set default parameters are also affected like `.add_totals()`. The following example should show the dots using the `Reds` colormap, but instead it uses the `winter` colormap because the second call sets the color map to `winter` if not given. This double call happens because when `sc.pl.dotplot()` is used (instead of `sc.pl.DotPlot`), internally a call to `.style()` is made and a subsequent explicit calls to `.style()` is required to tune the parameters as suggested in the documentation. ```python. adata = sc.datasets.pbmc68k_reduced(). markers = ['C1QA', 'PSAP', 'CD79A', 'CD79B', 'CST3', 'LYZ']. sc.pl.DotPlot(adata, markers, groupby='bulk_labels').style(cmap='Reds').style(dot_edge_color='black').show(). ```. ![image](https://user-images.githubusercontent.com/4964309/107354555-9628de00-6ace-11eb-9eb8-c0baaa80b1f6.png). The problem is caused by the current implementation of `sc.pl.Dotplot.style()` that set the default parameters as:. ```. def style(. self,. cmap: str = DEFAULT_COLORMAP,. color_on: Optional[Literal['dot', 'square']] = DEFAULT_COLOR_ON,. dot_max: Optional[float] = DEFAULT_DOT_MAX,. dot_min: Optional[float] = DEFAULT_DOT_MIN,. ..... ```. Where DEFAULT_* are the default values defined at the beginning of the file (see https://github.com/theislab/scanpy/blob/master/scanpy/plotting/_dotplot.py#L84) . What is nice about this is that the documentation clearly shows the default values. The downside is that optional values are assigned a default value that rewrites previous calls to style. Ideally, all optional values should be `None`, then is easy to know if a new value is passed or a previous call has already set a value. But, doing so will remove the defaults from the documentation. @flying-sheep suggested to use a code he wrote to add ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1633
https://github.com/scverse/scanpy/issues/1633:703,usability,document,documentation,703,"DotPlot `.style()` resets previous value assignments; Multiple calls to `sc.pl.Dotplot.style()` cause resetting of parameters. This issue can be observed as a change in the default colormap shown by #1632 Other methods that set default parameters are also affected like `.add_totals()`. The following example should show the dots using the `Reds` colormap, but instead it uses the `winter` colormap because the second call sets the color map to `winter` if not given. This double call happens because when `sc.pl.dotplot()` is used (instead of `sc.pl.DotPlot`), internally a call to `.style()` is made and a subsequent explicit calls to `.style()` is required to tune the parameters as suggested in the documentation. ```python. adata = sc.datasets.pbmc68k_reduced(). markers = ['C1QA', 'PSAP', 'CD79A', 'CD79B', 'CST3', 'LYZ']. sc.pl.DotPlot(adata, markers, groupby='bulk_labels').style(cmap='Reds').style(dot_edge_color='black').show(). ```. ![image](https://user-images.githubusercontent.com/4964309/107354555-9628de00-6ace-11eb-9eb8-c0baaa80b1f6.png). The problem is caused by the current implementation of `sc.pl.Dotplot.style()` that set the default parameters as:. ```. def style(. self,. cmap: str = DEFAULT_COLORMAP,. color_on: Optional[Literal['dot', 'square']] = DEFAULT_COLOR_ON,. dot_max: Optional[float] = DEFAULT_DOT_MAX,. dot_min: Optional[float] = DEFAULT_DOT_MIN,. ..... ```. Where DEFAULT_* are the default values defined at the beginning of the file (see https://github.com/theislab/scanpy/blob/master/scanpy/plotting/_dotplot.py#L84) . What is nice about this is that the documentation clearly shows the default values. The downside is that optional values are assigned a default value that rewrites previous calls to style. Ideally, all optional values should be `None`, then is easy to know if a new value is passed or a previous call has already set a value. But, doing so will remove the defaults from the documentation. @flying-sheep suggested to use a code he wrote to add ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1633
https://github.com/scverse/scanpy/issues/1633:961,usability,user,user-images,961,"DotPlot `.style()` resets previous value assignments; Multiple calls to `sc.pl.Dotplot.style()` cause resetting of parameters. This issue can be observed as a change in the default colormap shown by #1632 Other methods that set default parameters are also affected like `.add_totals()`. The following example should show the dots using the `Reds` colormap, but instead it uses the `winter` colormap because the second call sets the color map to `winter` if not given. This double call happens because when `sc.pl.dotplot()` is used (instead of `sc.pl.DotPlot`), internally a call to `.style()` is made and a subsequent explicit calls to `.style()` is required to tune the parameters as suggested in the documentation. ```python. adata = sc.datasets.pbmc68k_reduced(). markers = ['C1QA', 'PSAP', 'CD79A', 'CD79B', 'CST3', 'LYZ']. sc.pl.DotPlot(adata, markers, groupby='bulk_labels').style(cmap='Reds').style(dot_edge_color='black').show(). ```. ![image](https://user-images.githubusercontent.com/4964309/107354555-9628de00-6ace-11eb-9eb8-c0baaa80b1f6.png). The problem is caused by the current implementation of `sc.pl.Dotplot.style()` that set the default parameters as:. ```. def style(. self,. cmap: str = DEFAULT_COLORMAP,. color_on: Optional[Literal['dot', 'square']] = DEFAULT_COLOR_ON,. dot_max: Optional[float] = DEFAULT_DOT_MAX,. dot_min: Optional[float] = DEFAULT_DOT_MIN,. ..... ```. Where DEFAULT_* are the default values defined at the beginning of the file (see https://github.com/theislab/scanpy/blob/master/scanpy/plotting/_dotplot.py#L84) . What is nice about this is that the documentation clearly shows the default values. The downside is that optional values are assigned a default value that rewrites previous calls to style. Ideally, all optional values should be `None`, then is easy to know if a new value is passed or a previous call has already set a value. But, doing so will remove the defaults from the documentation. @flying-sheep suggested to use a code he wrote to add ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1633
https://github.com/scverse/scanpy/issues/1633:1593,usability,document,documentation,1593,"lt colormap shown by #1632 Other methods that set default parameters are also affected like `.add_totals()`. The following example should show the dots using the `Reds` colormap, but instead it uses the `winter` colormap because the second call sets the color map to `winter` if not given. This double call happens because when `sc.pl.dotplot()` is used (instead of `sc.pl.DotPlot`), internally a call to `.style()` is made and a subsequent explicit calls to `.style()` is required to tune the parameters as suggested in the documentation. ```python. adata = sc.datasets.pbmc68k_reduced(). markers = ['C1QA', 'PSAP', 'CD79A', 'CD79B', 'CST3', 'LYZ']. sc.pl.DotPlot(adata, markers, groupby='bulk_labels').style(cmap='Reds').style(dot_edge_color='black').show(). ```. ![image](https://user-images.githubusercontent.com/4964309/107354555-9628de00-6ace-11eb-9eb8-c0baaa80b1f6.png). The problem is caused by the current implementation of `sc.pl.Dotplot.style()` that set the default parameters as:. ```. def style(. self,. cmap: str = DEFAULT_COLORMAP,. color_on: Optional[Literal['dot', 'square']] = DEFAULT_COLOR_ON,. dot_max: Optional[float] = DEFAULT_DOT_MAX,. dot_min: Optional[float] = DEFAULT_DOT_MIN,. ..... ```. Where DEFAULT_* are the default values defined at the beginning of the file (see https://github.com/theislab/scanpy/blob/master/scanpy/plotting/_dotplot.py#L84) . What is nice about this is that the documentation clearly shows the default values. The downside is that optional values are assigned a default value that rewrites previous calls to style. Ideally, all optional values should be `None`, then is easy to know if a new value is passed or a previous call has already set a value. But, doing so will remove the defaults from the documentation. @flying-sheep suggested to use a code he wrote to add default annotations to the documentation. https://github.com/theislab/scanpydoc/blob/875b441212830678cf9fc81c52f5af29bbb8715f/scanpydoc/elegant_typehints/formatting.py#L101-L107.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1633
https://github.com/scverse/scanpy/issues/1633:1607,usability,clear,clearly,1607,"lt colormap shown by #1632 Other methods that set default parameters are also affected like `.add_totals()`. The following example should show the dots using the `Reds` colormap, but instead it uses the `winter` colormap because the second call sets the color map to `winter` if not given. This double call happens because when `sc.pl.dotplot()` is used (instead of `sc.pl.DotPlot`), internally a call to `.style()` is made and a subsequent explicit calls to `.style()` is required to tune the parameters as suggested in the documentation. ```python. adata = sc.datasets.pbmc68k_reduced(). markers = ['C1QA', 'PSAP', 'CD79A', 'CD79B', 'CST3', 'LYZ']. sc.pl.DotPlot(adata, markers, groupby='bulk_labels').style(cmap='Reds').style(dot_edge_color='black').show(). ```. ![image](https://user-images.githubusercontent.com/4964309/107354555-9628de00-6ace-11eb-9eb8-c0baaa80b1f6.png). The problem is caused by the current implementation of `sc.pl.Dotplot.style()` that set the default parameters as:. ```. def style(. self,. cmap: str = DEFAULT_COLORMAP,. color_on: Optional[Literal['dot', 'square']] = DEFAULT_COLOR_ON,. dot_max: Optional[float] = DEFAULT_DOT_MAX,. dot_min: Optional[float] = DEFAULT_DOT_MIN,. ..... ```. Where DEFAULT_* are the default values defined at the beginning of the file (see https://github.com/theislab/scanpy/blob/master/scanpy/plotting/_dotplot.py#L84) . What is nice about this is that the documentation clearly shows the default values. The downside is that optional values are assigned a default value that rewrites previous calls to style. Ideally, all optional values should be `None`, then is easy to know if a new value is passed or a previous call has already set a value. But, doing so will remove the defaults from the documentation. @flying-sheep suggested to use a code he wrote to add default annotations to the documentation. https://github.com/theislab/scanpydoc/blob/875b441212830678cf9fc81c52f5af29bbb8715f/scanpydoc/elegant_typehints/formatting.py#L101-L107.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1633
https://github.com/scverse/scanpy/issues/1633:1931,usability,document,documentation,1931,"lt colormap shown by #1632 Other methods that set default parameters are also affected like `.add_totals()`. The following example should show the dots using the `Reds` colormap, but instead it uses the `winter` colormap because the second call sets the color map to `winter` if not given. This double call happens because when `sc.pl.dotplot()` is used (instead of `sc.pl.DotPlot`), internally a call to `.style()` is made and a subsequent explicit calls to `.style()` is required to tune the parameters as suggested in the documentation. ```python. adata = sc.datasets.pbmc68k_reduced(). markers = ['C1QA', 'PSAP', 'CD79A', 'CD79B', 'CST3', 'LYZ']. sc.pl.DotPlot(adata, markers, groupby='bulk_labels').style(cmap='Reds').style(dot_edge_color='black').show(). ```. ![image](https://user-images.githubusercontent.com/4964309/107354555-9628de00-6ace-11eb-9eb8-c0baaa80b1f6.png). The problem is caused by the current implementation of `sc.pl.Dotplot.style()` that set the default parameters as:. ```. def style(. self,. cmap: str = DEFAULT_COLORMAP,. color_on: Optional[Literal['dot', 'square']] = DEFAULT_COLOR_ON,. dot_max: Optional[float] = DEFAULT_DOT_MAX,. dot_min: Optional[float] = DEFAULT_DOT_MIN,. ..... ```. Where DEFAULT_* are the default values defined at the beginning of the file (see https://github.com/theislab/scanpy/blob/master/scanpy/plotting/_dotplot.py#L84) . What is nice about this is that the documentation clearly shows the default values. The downside is that optional values are assigned a default value that rewrites previous calls to style. Ideally, all optional values should be `None`, then is easy to know if a new value is passed or a previous call has already set a value. But, doing so will remove the defaults from the documentation. @flying-sheep suggested to use a code he wrote to add default annotations to the documentation. https://github.com/theislab/scanpydoc/blob/875b441212830678cf9fc81c52f5af29bbb8715f/scanpydoc/elegant_typehints/formatting.py#L101-L107.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1633
https://github.com/scverse/scanpy/issues/1633:2027,usability,document,documentation,2027,"lt colormap shown by #1632 Other methods that set default parameters are also affected like `.add_totals()`. The following example should show the dots using the `Reds` colormap, but instead it uses the `winter` colormap because the second call sets the color map to `winter` if not given. This double call happens because when `sc.pl.dotplot()` is used (instead of `sc.pl.DotPlot`), internally a call to `.style()` is made and a subsequent explicit calls to `.style()` is required to tune the parameters as suggested in the documentation. ```python. adata = sc.datasets.pbmc68k_reduced(). markers = ['C1QA', 'PSAP', 'CD79A', 'CD79B', 'CST3', 'LYZ']. sc.pl.DotPlot(adata, markers, groupby='bulk_labels').style(cmap='Reds').style(dot_edge_color='black').show(). ```. ![image](https://user-images.githubusercontent.com/4964309/107354555-9628de00-6ace-11eb-9eb8-c0baaa80b1f6.png). The problem is caused by the current implementation of `sc.pl.Dotplot.style()` that set the default parameters as:. ```. def style(. self,. cmap: str = DEFAULT_COLORMAP,. color_on: Optional[Literal['dot', 'square']] = DEFAULT_COLOR_ON,. dot_max: Optional[float] = DEFAULT_DOT_MAX,. dot_min: Optional[float] = DEFAULT_DOT_MIN,. ..... ```. Where DEFAULT_* are the default values defined at the beginning of the file (see https://github.com/theislab/scanpy/blob/master/scanpy/plotting/_dotplot.py#L84) . What is nice about this is that the documentation clearly shows the default values. The downside is that optional values are assigned a default value that rewrites previous calls to style. Ideally, all optional values should be `None`, then is easy to know if a new value is passed or a previous call has already set a value. But, doing so will remove the defaults from the documentation. @flying-sheep suggested to use a code he wrote to add default annotations to the documentation. https://github.com/theislab/scanpydoc/blob/875b441212830678cf9fc81c52f5af29bbb8715f/scanpydoc/elegant_typehints/formatting.py#L101-L107.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1633
https://github.com/scverse/scanpy/issues/1634:25,availability,error,error,25,"sc.get.obs_df() gives an error when `obsm_keys` is given and keys are not given; Calling `sc.get.obs_df()` without the `keys` parameter causes an error:. ```python. adata = sc.datasets.pbmc68k_reduced(). sc.get.obs_df(adata, obsm_keys=[('X_umap', 0,)]). ```. ```pytb. ~/scanpy/scanpy/get/get.py in obs_df(adata, keys, obsm_keys, layer, gene_symbols, use_raw). 301 . 302 # reorder columns to given order (including duplicates keys if present). --> 303 df = df[keys]. 304 for k, idx in obsm_keys:. 305 added_k = f""{k}-{idx}"". KeyError: (). ```. Also, if `keys` is not a list, the object returned is not a pandas dataframe but a Series object in which the last row is the obsm values. In other words, instead of adding a column to a dataframe with the obsm values, a row is added to a pandas Series. . ```python. adata = sc.datasets.pbmc68k_reduced(). sc.get.obs_df(adata, obsm_keys=[('X_umap', 0,)], keys='CST3'). ```. ```. index. AAAGCCTGGCTAAC-1 0.281. AAATTCGATGCACA-1 -0.176. AACACGTGGTCTTT-1 -0.818. AAGTGCACGTGCTA-1 -0.818. ACACGAACGGAGTG-1 0.854. ... . TGTGAGTGCTTTAC-8 -0.069. TGTTACTGGCGATT-8 -0.818. TTCAGTACCGGGAA-8 -0.818. TTGAGGTGGAGAGC-8 0.428. X_umap-0 [-1.9918625454649166, -3.2486919412134108, -3.... Name: CST3, Length: 701, dtype: object. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1634
https://github.com/scverse/scanpy/issues/1634:146,availability,error,error,146,"sc.get.obs_df() gives an error when `obsm_keys` is given and keys are not given; Calling `sc.get.obs_df()` without the `keys` parameter causes an error:. ```python. adata = sc.datasets.pbmc68k_reduced(). sc.get.obs_df(adata, obsm_keys=[('X_umap', 0,)]). ```. ```pytb. ~/scanpy/scanpy/get/get.py in obs_df(adata, keys, obsm_keys, layer, gene_symbols, use_raw). 301 . 302 # reorder columns to given order (including duplicates keys if present). --> 303 df = df[keys]. 304 for k, idx in obsm_keys:. 305 added_k = f""{k}-{idx}"". KeyError: (). ```. Also, if `keys` is not a list, the object returned is not a pandas dataframe but a Series object in which the last row is the obsm values. In other words, instead of adding a column to a dataframe with the obsm values, a row is added to a pandas Series. . ```python. adata = sc.datasets.pbmc68k_reduced(). sc.get.obs_df(adata, obsm_keys=[('X_umap', 0,)], keys='CST3'). ```. ```. index. AAAGCCTGGCTAAC-1 0.281. AAATTCGATGCACA-1 -0.176. AACACGTGGTCTTT-1 -0.818. AAGTGCACGTGCTA-1 -0.818. ACACGAACGGAGTG-1 0.854. ... . TGTGAGTGCTTTAC-8 -0.069. TGTTACTGGCGATT-8 -0.818. TTCAGTACCGGGAA-8 -0.818. TTGAGGTGGAGAGC-8 0.428. X_umap-0 [-1.9918625454649166, -3.2486919412134108, -3.... Name: CST3, Length: 701, dtype: object. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1634
https://github.com/scverse/scanpy/issues/1634:126,modifiability,paramet,parameter,126,"sc.get.obs_df() gives an error when `obsm_keys` is given and keys are not given; Calling `sc.get.obs_df()` without the `keys` parameter causes an error:. ```python. adata = sc.datasets.pbmc68k_reduced(). sc.get.obs_df(adata, obsm_keys=[('X_umap', 0,)]). ```. ```pytb. ~/scanpy/scanpy/get/get.py in obs_df(adata, keys, obsm_keys, layer, gene_symbols, use_raw). 301 . 302 # reorder columns to given order (including duplicates keys if present). --> 303 df = df[keys]. 304 for k, idx in obsm_keys:. 305 added_k = f""{k}-{idx}"". KeyError: (). ```. Also, if `keys` is not a list, the object returned is not a pandas dataframe but a Series object in which the last row is the obsm values. In other words, instead of adding a column to a dataframe with the obsm values, a row is added to a pandas Series. . ```python. adata = sc.datasets.pbmc68k_reduced(). sc.get.obs_df(adata, obsm_keys=[('X_umap', 0,)], keys='CST3'). ```. ```. index. AAAGCCTGGCTAAC-1 0.281. AAATTCGATGCACA-1 -0.176. AACACGTGGTCTTT-1 -0.818. AAGTGCACGTGCTA-1 -0.818. ACACGAACGGAGTG-1 0.854. ... . TGTGAGTGCTTTAC-8 -0.069. TGTTACTGGCGATT-8 -0.818. TTCAGTACCGGGAA-8 -0.818. TTGAGGTGGAGAGC-8 0.428. X_umap-0 [-1.9918625454649166, -3.2486919412134108, -3.... Name: CST3, Length: 701, dtype: object. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1634
https://github.com/scverse/scanpy/issues/1634:329,modifiability,layer,layer,329,"sc.get.obs_df() gives an error when `obsm_keys` is given and keys are not given; Calling `sc.get.obs_df()` without the `keys` parameter causes an error:. ```python. adata = sc.datasets.pbmc68k_reduced(). sc.get.obs_df(adata, obsm_keys=[('X_umap', 0,)]). ```. ```pytb. ~/scanpy/scanpy/get/get.py in obs_df(adata, keys, obsm_keys, layer, gene_symbols, use_raw). 301 . 302 # reorder columns to given order (including duplicates keys if present). --> 303 df = df[keys]. 304 for k, idx in obsm_keys:. 305 added_k = f""{k}-{idx}"". KeyError: (). ```. Also, if `keys` is not a list, the object returned is not a pandas dataframe but a Series object in which the last row is the obsm values. In other words, instead of adding a column to a dataframe with the obsm values, a row is added to a pandas Series. . ```python. adata = sc.datasets.pbmc68k_reduced(). sc.get.obs_df(adata, obsm_keys=[('X_umap', 0,)], keys='CST3'). ```. ```. index. AAAGCCTGGCTAAC-1 0.281. AAATTCGATGCACA-1 -0.176. AACACGTGGTCTTT-1 -0.818. AAGTGCACGTGCTA-1 -0.818. ACACGAACGGAGTG-1 0.854. ... . TGTGAGTGCTTTAC-8 -0.069. TGTTACTGGCGATT-8 -0.818. TTCAGTACCGGGAA-8 -0.818. TTGAGGTGGAGAGC-8 0.428. X_umap-0 [-1.9918625454649166, -3.2486919412134108, -3.... Name: CST3, Length: 701, dtype: object. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1634
https://github.com/scverse/scanpy/issues/1634:25,performance,error,error,25,"sc.get.obs_df() gives an error when `obsm_keys` is given and keys are not given; Calling `sc.get.obs_df()` without the `keys` parameter causes an error:. ```python. adata = sc.datasets.pbmc68k_reduced(). sc.get.obs_df(adata, obsm_keys=[('X_umap', 0,)]). ```. ```pytb. ~/scanpy/scanpy/get/get.py in obs_df(adata, keys, obsm_keys, layer, gene_symbols, use_raw). 301 . 302 # reorder columns to given order (including duplicates keys if present). --> 303 df = df[keys]. 304 for k, idx in obsm_keys:. 305 added_k = f""{k}-{idx}"". KeyError: (). ```. Also, if `keys` is not a list, the object returned is not a pandas dataframe but a Series object in which the last row is the obsm values. In other words, instead of adding a column to a dataframe with the obsm values, a row is added to a pandas Series. . ```python. adata = sc.datasets.pbmc68k_reduced(). sc.get.obs_df(adata, obsm_keys=[('X_umap', 0,)], keys='CST3'). ```. ```. index. AAAGCCTGGCTAAC-1 0.281. AAATTCGATGCACA-1 -0.176. AACACGTGGTCTTT-1 -0.818. AAGTGCACGTGCTA-1 -0.818. ACACGAACGGAGTG-1 0.854. ... . TGTGAGTGCTTTAC-8 -0.069. TGTTACTGGCGATT-8 -0.818. TTCAGTACCGGGAA-8 -0.818. TTGAGGTGGAGAGC-8 0.428. X_umap-0 [-1.9918625454649166, -3.2486919412134108, -3.... Name: CST3, Length: 701, dtype: object. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1634
https://github.com/scverse/scanpy/issues/1634:146,performance,error,error,146,"sc.get.obs_df() gives an error when `obsm_keys` is given and keys are not given; Calling `sc.get.obs_df()` without the `keys` parameter causes an error:. ```python. adata = sc.datasets.pbmc68k_reduced(). sc.get.obs_df(adata, obsm_keys=[('X_umap', 0,)]). ```. ```pytb. ~/scanpy/scanpy/get/get.py in obs_df(adata, keys, obsm_keys, layer, gene_symbols, use_raw). 301 . 302 # reorder columns to given order (including duplicates keys if present). --> 303 df = df[keys]. 304 for k, idx in obsm_keys:. 305 added_k = f""{k}-{idx}"". KeyError: (). ```. Also, if `keys` is not a list, the object returned is not a pandas dataframe but a Series object in which the last row is the obsm values. In other words, instead of adding a column to a dataframe with the obsm values, a row is added to a pandas Series. . ```python. adata = sc.datasets.pbmc68k_reduced(). sc.get.obs_df(adata, obsm_keys=[('X_umap', 0,)], keys='CST3'). ```. ```. index. AAAGCCTGGCTAAC-1 0.281. AAATTCGATGCACA-1 -0.176. AACACGTGGTCTTT-1 -0.818. AAGTGCACGTGCTA-1 -0.818. ACACGAACGGAGTG-1 0.854. ... . TGTGAGTGCTTTAC-8 -0.069. TGTTACTGGCGATT-8 -0.818. TTCAGTACCGGGAA-8 -0.818. TTGAGGTGGAGAGC-8 0.428. X_umap-0 [-1.9918625454649166, -3.2486919412134108, -3.... Name: CST3, Length: 701, dtype: object. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1634
https://github.com/scverse/scanpy/issues/1634:25,safety,error,error,25,"sc.get.obs_df() gives an error when `obsm_keys` is given and keys are not given; Calling `sc.get.obs_df()` without the `keys` parameter causes an error:. ```python. adata = sc.datasets.pbmc68k_reduced(). sc.get.obs_df(adata, obsm_keys=[('X_umap', 0,)]). ```. ```pytb. ~/scanpy/scanpy/get/get.py in obs_df(adata, keys, obsm_keys, layer, gene_symbols, use_raw). 301 . 302 # reorder columns to given order (including duplicates keys if present). --> 303 df = df[keys]. 304 for k, idx in obsm_keys:. 305 added_k = f""{k}-{idx}"". KeyError: (). ```. Also, if `keys` is not a list, the object returned is not a pandas dataframe but a Series object in which the last row is the obsm values. In other words, instead of adding a column to a dataframe with the obsm values, a row is added to a pandas Series. . ```python. adata = sc.datasets.pbmc68k_reduced(). sc.get.obs_df(adata, obsm_keys=[('X_umap', 0,)], keys='CST3'). ```. ```. index. AAAGCCTGGCTAAC-1 0.281. AAATTCGATGCACA-1 -0.176. AACACGTGGTCTTT-1 -0.818. AAGTGCACGTGCTA-1 -0.818. ACACGAACGGAGTG-1 0.854. ... . TGTGAGTGCTTTAC-8 -0.069. TGTTACTGGCGATT-8 -0.818. TTCAGTACCGGGAA-8 -0.818. TTGAGGTGGAGAGC-8 0.428. X_umap-0 [-1.9918625454649166, -3.2486919412134108, -3.... Name: CST3, Length: 701, dtype: object. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1634
https://github.com/scverse/scanpy/issues/1634:146,safety,error,error,146,"sc.get.obs_df() gives an error when `obsm_keys` is given and keys are not given; Calling `sc.get.obs_df()` without the `keys` parameter causes an error:. ```python. adata = sc.datasets.pbmc68k_reduced(). sc.get.obs_df(adata, obsm_keys=[('X_umap', 0,)]). ```. ```pytb. ~/scanpy/scanpy/get/get.py in obs_df(adata, keys, obsm_keys, layer, gene_symbols, use_raw). 301 . 302 # reorder columns to given order (including duplicates keys if present). --> 303 df = df[keys]. 304 for k, idx in obsm_keys:. 305 added_k = f""{k}-{idx}"". KeyError: (). ```. Also, if `keys` is not a list, the object returned is not a pandas dataframe but a Series object in which the last row is the obsm values. In other words, instead of adding a column to a dataframe with the obsm values, a row is added to a pandas Series. . ```python. adata = sc.datasets.pbmc68k_reduced(). sc.get.obs_df(adata, obsm_keys=[('X_umap', 0,)], keys='CST3'). ```. ```. index. AAAGCCTGGCTAAC-1 0.281. AAATTCGATGCACA-1 -0.176. AACACGTGGTCTTT-1 -0.818. AAGTGCACGTGCTA-1 -0.818. ACACGAACGGAGTG-1 0.854. ... . TGTGAGTGCTTTAC-8 -0.069. TGTTACTGGCGATT-8 -0.818. TTCAGTACCGGGAA-8 -0.818. TTGAGGTGGAGAGC-8 0.428. X_umap-0 [-1.9918625454649166, -3.2486919412134108, -3.... Name: CST3, Length: 701, dtype: object. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1634
https://github.com/scverse/scanpy/issues/1634:25,usability,error,error,25,"sc.get.obs_df() gives an error when `obsm_keys` is given and keys are not given; Calling `sc.get.obs_df()` without the `keys` parameter causes an error:. ```python. adata = sc.datasets.pbmc68k_reduced(). sc.get.obs_df(adata, obsm_keys=[('X_umap', 0,)]). ```. ```pytb. ~/scanpy/scanpy/get/get.py in obs_df(adata, keys, obsm_keys, layer, gene_symbols, use_raw). 301 . 302 # reorder columns to given order (including duplicates keys if present). --> 303 df = df[keys]. 304 for k, idx in obsm_keys:. 305 added_k = f""{k}-{idx}"". KeyError: (). ```. Also, if `keys` is not a list, the object returned is not a pandas dataframe but a Series object in which the last row is the obsm values. In other words, instead of adding a column to a dataframe with the obsm values, a row is added to a pandas Series. . ```python. adata = sc.datasets.pbmc68k_reduced(). sc.get.obs_df(adata, obsm_keys=[('X_umap', 0,)], keys='CST3'). ```. ```. index. AAAGCCTGGCTAAC-1 0.281. AAATTCGATGCACA-1 -0.176. AACACGTGGTCTTT-1 -0.818. AAGTGCACGTGCTA-1 -0.818. ACACGAACGGAGTG-1 0.854. ... . TGTGAGTGCTTTAC-8 -0.069. TGTTACTGGCGATT-8 -0.818. TTCAGTACCGGGAA-8 -0.818. TTGAGGTGGAGAGC-8 0.428. X_umap-0 [-1.9918625454649166, -3.2486919412134108, -3.... Name: CST3, Length: 701, dtype: object. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1634
https://github.com/scverse/scanpy/issues/1634:146,usability,error,error,146,"sc.get.obs_df() gives an error when `obsm_keys` is given and keys are not given; Calling `sc.get.obs_df()` without the `keys` parameter causes an error:. ```python. adata = sc.datasets.pbmc68k_reduced(). sc.get.obs_df(adata, obsm_keys=[('X_umap', 0,)]). ```. ```pytb. ~/scanpy/scanpy/get/get.py in obs_df(adata, keys, obsm_keys, layer, gene_symbols, use_raw). 301 . 302 # reorder columns to given order (including duplicates keys if present). --> 303 df = df[keys]. 304 for k, idx in obsm_keys:. 305 added_k = f""{k}-{idx}"". KeyError: (). ```. Also, if `keys` is not a list, the object returned is not a pandas dataframe but a Series object in which the last row is the obsm values. In other words, instead of adding a column to a dataframe with the obsm values, a row is added to a pandas Series. . ```python. adata = sc.datasets.pbmc68k_reduced(). sc.get.obs_df(adata, obsm_keys=[('X_umap', 0,)], keys='CST3'). ```. ```. index. AAAGCCTGGCTAAC-1 0.281. AAATTCGATGCACA-1 -0.176. AACACGTGGTCTTT-1 -0.818. AAGTGCACGTGCTA-1 -0.818. ACACGAACGGAGTG-1 0.854. ... . TGTGAGTGCTTTAC-8 -0.069. TGTTACTGGCGATT-8 -0.818. TTCAGTACCGGGAA-8 -0.818. TTGAGGTGGAGAGC-8 0.428. X_umap-0 [-1.9918625454649166, -3.2486919412134108, -3.... Name: CST3, Length: 701, dtype: object. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1634
https://github.com/scverse/scanpy/issues/1636:551,deployability,modul,module,551,"dotplot cannot set gene_symbols; I would like to change the var name when plotting dotplot by setting the `gene_symbols` to the desired name. But this generates the NameError. Below is an example using the scanpy built-in dataset. ```python. adata = sc.datasets.pbmc68k_reduced(). ax = sc.pl.dotplot(adata, 'C1QA', groupby=['bulk_labels'], swap_axes=False, gene_symbols='TEST'). ```. ```pytb. ---------------------------------------------------------------------------. NameError Traceback (most recent call last). <ipython-input-45-206578ef4fd5> in <module>. 1 adata = sc.datasets.pbmc68k_reduced(). ----> 2 ax = sc.pl.dotplot(adata, 'C1QA', groupby=['bulk_labels'], swap_axes=False, gene_symbols='TEST'). 3 . ~/anaconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/_dotplot.py in dotplot(adata, var_names, groupby, use_raw, log, num_categories, expression_cutoff, mean_only_expressed, cmap, dot_max, dot_min, standard_scale, smallest_dot, title, colorbar_title, size_title, figsize, dendrogram, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, swap_axes, dot_color_df, show, save, ax, return_fig, **kwds). 930 dot_color_df=dot_color_df,. 931 ax=ax,. --> 932 **kwds,. 933 ). 934 . ~/anaconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/_dotplot.py in __init__(self, adata, var_names, groupby, use_raw, log, num_categories, categories_order, title, figsize, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, expression_cutoff, mean_only_expressed, standard_scale, dot_color_df, dot_size_df, ax, **kwds). 142 layer=layer,. 143 ax=ax,. --> 144 **kwds,. 145 ). 146 . ~/anaconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/_baseplot_class.py in __init__(self, adata, var_names, groupby, use_raw, log, num_categories, categories_order, title, figsize, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, ax, **kwds). 111 num_categories,. 112 layer=layer,. --> 113 gene_symbols=gene",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1636
https://github.com/scverse/scanpy/issues/1636:837,deployability,log,log,837,"dotplot cannot set gene_symbols; I would like to change the var name when plotting dotplot by setting the `gene_symbols` to the desired name. But this generates the NameError. Below is an example using the scanpy built-in dataset. ```python. adata = sc.datasets.pbmc68k_reduced(). ax = sc.pl.dotplot(adata, 'C1QA', groupby=['bulk_labels'], swap_axes=False, gene_symbols='TEST'). ```. ```pytb. ---------------------------------------------------------------------------. NameError Traceback (most recent call last). <ipython-input-45-206578ef4fd5> in <module>. 1 adata = sc.datasets.pbmc68k_reduced(). ----> 2 ax = sc.pl.dotplot(adata, 'C1QA', groupby=['bulk_labels'], swap_axes=False, gene_symbols='TEST'). 3 . ~/anaconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/_dotplot.py in dotplot(adata, var_names, groupby, use_raw, log, num_categories, expression_cutoff, mean_only_expressed, cmap, dot_max, dot_min, standard_scale, smallest_dot, title, colorbar_title, size_title, figsize, dendrogram, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, swap_axes, dot_color_df, show, save, ax, return_fig, **kwds). 930 dot_color_df=dot_color_df,. 931 ax=ax,. --> 932 **kwds,. 933 ). 934 . ~/anaconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/_dotplot.py in __init__(self, adata, var_names, groupby, use_raw, log, num_categories, categories_order, title, figsize, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, expression_cutoff, mean_only_expressed, standard_scale, dot_color_df, dot_size_df, ax, **kwds). 142 layer=layer,. 143 ax=ax,. --> 144 **kwds,. 145 ). 146 . ~/anaconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/_baseplot_class.py in __init__(self, adata, var_names, groupby, use_raw, log, num_categories, categories_order, title, figsize, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, ax, **kwds). 111 num_categories,. 112 layer=layer,. --> 113 gene_symbols=gene",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1636
https://github.com/scverse/scanpy/issues/1636:1357,deployability,log,log,1357,"ne_symbols='TEST'). ```. ```pytb. ---------------------------------------------------------------------------. NameError Traceback (most recent call last). <ipython-input-45-206578ef4fd5> in <module>. 1 adata = sc.datasets.pbmc68k_reduced(). ----> 2 ax = sc.pl.dotplot(adata, 'C1QA', groupby=['bulk_labels'], swap_axes=False, gene_symbols='TEST'). 3 . ~/anaconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/_dotplot.py in dotplot(adata, var_names, groupby, use_raw, log, num_categories, expression_cutoff, mean_only_expressed, cmap, dot_max, dot_min, standard_scale, smallest_dot, title, colorbar_title, size_title, figsize, dendrogram, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, swap_axes, dot_color_df, show, save, ax, return_fig, **kwds). 930 dot_color_df=dot_color_df,. 931 ax=ax,. --> 932 **kwds,. 933 ). 934 . ~/anaconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/_dotplot.py in __init__(self, adata, var_names, groupby, use_raw, log, num_categories, categories_order, title, figsize, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, expression_cutoff, mean_only_expressed, standard_scale, dot_color_df, dot_size_df, ax, **kwds). 142 layer=layer,. 143 ax=ax,. --> 144 **kwds,. 145 ). 146 . ~/anaconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/_baseplot_class.py in __init__(self, adata, var_names, groupby, use_raw, log, num_categories, categories_order, title, figsize, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, ax, **kwds). 111 num_categories,. 112 layer=layer,. --> 113 gene_symbols=gene_symbols,. 114 ). 115 . ~/anaconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/_anndata.py in _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer, gene_symbols). 1837 # translate the column names to the symbol names. 1838 obs_tidy.rename(. -> 1839 columns={var_names[x]: symbols[x] for x in range(len(var_names))},. 1",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1636
https://github.com/scverse/scanpy/issues/1636:1788,deployability,log,log,1788,"in dotplot(adata, var_names, groupby, use_raw, log, num_categories, expression_cutoff, mean_only_expressed, cmap, dot_max, dot_min, standard_scale, smallest_dot, title, colorbar_title, size_title, figsize, dendrogram, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, swap_axes, dot_color_df, show, save, ax, return_fig, **kwds). 930 dot_color_df=dot_color_df,. 931 ax=ax,. --> 932 **kwds,. 933 ). 934 . ~/anaconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/_dotplot.py in __init__(self, adata, var_names, groupby, use_raw, log, num_categories, categories_order, title, figsize, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, expression_cutoff, mean_only_expressed, standard_scale, dot_color_df, dot_size_df, ax, **kwds). 142 layer=layer,. 143 ax=ax,. --> 144 **kwds,. 145 ). 146 . ~/anaconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/_baseplot_class.py in __init__(self, adata, var_names, groupby, use_raw, log, num_categories, categories_order, title, figsize, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, ax, **kwds). 111 num_categories,. 112 layer=layer,. --> 113 gene_symbols=gene_symbols,. 114 ). 115 . ~/anaconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/_anndata.py in _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer, gene_symbols). 1837 # translate the column names to the symbol names. 1838 obs_tidy.rename(. -> 1839 columns={var_names[x]: symbols[x] for x in range(len(var_names))},. 1840 inplace=True,. 1841 ). ~/anaconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/_anndata.py in <dictcomp>(.0). 1837 # translate the column names to the symbol names. 1838 obs_tidy.rename(. -> 1839 columns={var_names[x]: symbols[x] for x in range(len(var_names))},. 1840 inplace=True,. 1841 ). NameError: free variable 'symbols' referenced before assignment in enclosing scope. ```. #### Versions. <details>. scanpy==1",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1636
https://github.com/scverse/scanpy/issues/1636:2161,deployability,log,log,2161,"tle, size_title, figsize, dendrogram, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, swap_axes, dot_color_df, show, save, ax, return_fig, **kwds). 930 dot_color_df=dot_color_df,. 931 ax=ax,. --> 932 **kwds,. 933 ). 934 . ~/anaconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/_dotplot.py in __init__(self, adata, var_names, groupby, use_raw, log, num_categories, categories_order, title, figsize, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, expression_cutoff, mean_only_expressed, standard_scale, dot_color_df, dot_size_df, ax, **kwds). 142 layer=layer,. 143 ax=ax,. --> 144 **kwds,. 145 ). 146 . ~/anaconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/_baseplot_class.py in __init__(self, adata, var_names, groupby, use_raw, log, num_categories, categories_order, title, figsize, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, ax, **kwds). 111 num_categories,. 112 layer=layer,. --> 113 gene_symbols=gene_symbols,. 114 ). 115 . ~/anaconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/_anndata.py in _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer, gene_symbols). 1837 # translate the column names to the symbol names. 1838 obs_tidy.rename(. -> 1839 columns={var_names[x]: symbols[x] for x in range(len(var_names))},. 1840 inplace=True,. 1841 ). ~/anaconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/_anndata.py in <dictcomp>(.0). 1837 # translate the column names to the symbol names. 1838 obs_tidy.rename(. -> 1839 columns={var_names[x]: symbols[x] for x in range(len(var_names))},. 1840 inplace=True,. 1841 ). NameError: free variable 'symbols' referenced before assignment in enclosing scope. ```. #### Versions. <details>. scanpy==1.6.0 anndata==0.7.4 umap==0.3.10 numpy==1.18.4 scipy==1.4.1 pandas==1.0.3 scikit-learn==0.21.3 statsmodels==0.10.1 python-igraph==0.8.2 louvain==0.6.1 leidenalg==0.8.0. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1636
https://github.com/scverse/scanpy/issues/1636:2760,deployability,Version,Versions,2760,"tle, size_title, figsize, dendrogram, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, swap_axes, dot_color_df, show, save, ax, return_fig, **kwds). 930 dot_color_df=dot_color_df,. 931 ax=ax,. --> 932 **kwds,. 933 ). 934 . ~/anaconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/_dotplot.py in __init__(self, adata, var_names, groupby, use_raw, log, num_categories, categories_order, title, figsize, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, expression_cutoff, mean_only_expressed, standard_scale, dot_color_df, dot_size_df, ax, **kwds). 142 layer=layer,. 143 ax=ax,. --> 144 **kwds,. 145 ). 146 . ~/anaconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/_baseplot_class.py in __init__(self, adata, var_names, groupby, use_raw, log, num_categories, categories_order, title, figsize, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, ax, **kwds). 111 num_categories,. 112 layer=layer,. --> 113 gene_symbols=gene_symbols,. 114 ). 115 . ~/anaconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/_anndata.py in _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer, gene_symbols). 1837 # translate the column names to the symbol names. 1838 obs_tidy.rename(. -> 1839 columns={var_names[x]: symbols[x] for x in range(len(var_names))},. 1840 inplace=True,. 1841 ). ~/anaconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/_anndata.py in <dictcomp>(.0). 1837 # translate the column names to the symbol names. 1838 obs_tidy.rename(. -> 1839 columns={var_names[x]: symbols[x] for x in range(len(var_names))},. 1840 inplace=True,. 1841 ). NameError: free variable 'symbols' referenced before assignment in enclosing scope. ```. #### Versions. <details>. scanpy==1.6.0 anndata==0.7.4 umap==0.3.10 numpy==1.18.4 scipy==1.4.1 pandas==1.0.3 scikit-learn==0.21.3 statsmodels==0.10.1 python-igraph==0.8.2 louvain==0.6.1 leidenalg==0.8.0. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1636
https://github.com/scverse/scanpy/issues/1636:2211,integrability,translat,translate,2211,"tle, size_title, figsize, dendrogram, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, swap_axes, dot_color_df, show, save, ax, return_fig, **kwds). 930 dot_color_df=dot_color_df,. 931 ax=ax,. --> 932 **kwds,. 933 ). 934 . ~/anaconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/_dotplot.py in __init__(self, adata, var_names, groupby, use_raw, log, num_categories, categories_order, title, figsize, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, expression_cutoff, mean_only_expressed, standard_scale, dot_color_df, dot_size_df, ax, **kwds). 142 layer=layer,. 143 ax=ax,. --> 144 **kwds,. 145 ). 146 . ~/anaconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/_baseplot_class.py in __init__(self, adata, var_names, groupby, use_raw, log, num_categories, categories_order, title, figsize, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, ax, **kwds). 111 num_categories,. 112 layer=layer,. --> 113 gene_symbols=gene_symbols,. 114 ). 115 . ~/anaconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/_anndata.py in _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer, gene_symbols). 1837 # translate the column names to the symbol names. 1838 obs_tidy.rename(. -> 1839 columns={var_names[x]: symbols[x] for x in range(len(var_names))},. 1840 inplace=True,. 1841 ). ~/anaconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/_anndata.py in <dictcomp>(.0). 1837 # translate the column names to the symbol names. 1838 obs_tidy.rename(. -> 1839 columns={var_names[x]: symbols[x] for x in range(len(var_names))},. 1840 inplace=True,. 1841 ). NameError: free variable 'symbols' referenced before assignment in enclosing scope. ```. #### Versions. <details>. scanpy==1.6.0 anndata==0.7.4 umap==0.3.10 numpy==1.18.4 scipy==1.4.1 pandas==1.0.3 scikit-learn==0.21.3 statsmodels==0.10.1 python-igraph==0.8.2 louvain==0.6.1 leidenalg==0.8.0. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1636
https://github.com/scverse/scanpy/issues/1636:2491,integrability,translat,translate,2491,"tle, size_title, figsize, dendrogram, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, swap_axes, dot_color_df, show, save, ax, return_fig, **kwds). 930 dot_color_df=dot_color_df,. 931 ax=ax,. --> 932 **kwds,. 933 ). 934 . ~/anaconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/_dotplot.py in __init__(self, adata, var_names, groupby, use_raw, log, num_categories, categories_order, title, figsize, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, expression_cutoff, mean_only_expressed, standard_scale, dot_color_df, dot_size_df, ax, **kwds). 142 layer=layer,. 143 ax=ax,. --> 144 **kwds,. 145 ). 146 . ~/anaconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/_baseplot_class.py in __init__(self, adata, var_names, groupby, use_raw, log, num_categories, categories_order, title, figsize, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, ax, **kwds). 111 num_categories,. 112 layer=layer,. --> 113 gene_symbols=gene_symbols,. 114 ). 115 . ~/anaconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/_anndata.py in _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer, gene_symbols). 1837 # translate the column names to the symbol names. 1838 obs_tidy.rename(. -> 1839 columns={var_names[x]: symbols[x] for x in range(len(var_names))},. 1840 inplace=True,. 1841 ). ~/anaconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/_anndata.py in <dictcomp>(.0). 1837 # translate the column names to the symbol names. 1838 obs_tidy.rename(. -> 1839 columns={var_names[x]: symbols[x] for x in range(len(var_names))},. 1840 inplace=True,. 1841 ). NameError: free variable 'symbols' referenced before assignment in enclosing scope. ```. #### Versions. <details>. scanpy==1.6.0 anndata==0.7.4 umap==0.3.10 numpy==1.18.4 scipy==1.4.1 pandas==1.0.3 scikit-learn==0.21.3 statsmodels==0.10.1 python-igraph==0.8.2 louvain==0.6.1 leidenalg==0.8.0. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1636
https://github.com/scverse/scanpy/issues/1636:2760,integrability,Version,Versions,2760,"tle, size_title, figsize, dendrogram, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, swap_axes, dot_color_df, show, save, ax, return_fig, **kwds). 930 dot_color_df=dot_color_df,. 931 ax=ax,. --> 932 **kwds,. 933 ). 934 . ~/anaconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/_dotplot.py in __init__(self, adata, var_names, groupby, use_raw, log, num_categories, categories_order, title, figsize, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, expression_cutoff, mean_only_expressed, standard_scale, dot_color_df, dot_size_df, ax, **kwds). 142 layer=layer,. 143 ax=ax,. --> 144 **kwds,. 145 ). 146 . ~/anaconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/_baseplot_class.py in __init__(self, adata, var_names, groupby, use_raw, log, num_categories, categories_order, title, figsize, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, ax, **kwds). 111 num_categories,. 112 layer=layer,. --> 113 gene_symbols=gene_symbols,. 114 ). 115 . ~/anaconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/_anndata.py in _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer, gene_symbols). 1837 # translate the column names to the symbol names. 1838 obs_tidy.rename(. -> 1839 columns={var_names[x]: symbols[x] for x in range(len(var_names))},. 1840 inplace=True,. 1841 ). ~/anaconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/_anndata.py in <dictcomp>(.0). 1837 # translate the column names to the symbol names. 1838 obs_tidy.rename(. -> 1839 columns={var_names[x]: symbols[x] for x in range(len(var_names))},. 1840 inplace=True,. 1841 ). NameError: free variable 'symbols' referenced before assignment in enclosing scope. ```. #### Versions. <details>. scanpy==1.6.0 anndata==0.7.4 umap==0.3.10 numpy==1.18.4 scipy==1.4.1 pandas==1.0.3 scikit-learn==0.21.3 statsmodels==0.10.1 python-igraph==0.8.2 louvain==0.6.1 leidenalg==0.8.0. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1636
https://github.com/scverse/scanpy/issues/1636:2211,interoperability,translat,translate,2211,"tle, size_title, figsize, dendrogram, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, swap_axes, dot_color_df, show, save, ax, return_fig, **kwds). 930 dot_color_df=dot_color_df,. 931 ax=ax,. --> 932 **kwds,. 933 ). 934 . ~/anaconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/_dotplot.py in __init__(self, adata, var_names, groupby, use_raw, log, num_categories, categories_order, title, figsize, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, expression_cutoff, mean_only_expressed, standard_scale, dot_color_df, dot_size_df, ax, **kwds). 142 layer=layer,. 143 ax=ax,. --> 144 **kwds,. 145 ). 146 . ~/anaconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/_baseplot_class.py in __init__(self, adata, var_names, groupby, use_raw, log, num_categories, categories_order, title, figsize, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, ax, **kwds). 111 num_categories,. 112 layer=layer,. --> 113 gene_symbols=gene_symbols,. 114 ). 115 . ~/anaconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/_anndata.py in _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer, gene_symbols). 1837 # translate the column names to the symbol names. 1838 obs_tidy.rename(. -> 1839 columns={var_names[x]: symbols[x] for x in range(len(var_names))},. 1840 inplace=True,. 1841 ). ~/anaconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/_anndata.py in <dictcomp>(.0). 1837 # translate the column names to the symbol names. 1838 obs_tidy.rename(. -> 1839 columns={var_names[x]: symbols[x] for x in range(len(var_names))},. 1840 inplace=True,. 1841 ). NameError: free variable 'symbols' referenced before assignment in enclosing scope. ```. #### Versions. <details>. scanpy==1.6.0 anndata==0.7.4 umap==0.3.10 numpy==1.18.4 scipy==1.4.1 pandas==1.0.3 scikit-learn==0.21.3 statsmodels==0.10.1 python-igraph==0.8.2 louvain==0.6.1 leidenalg==0.8.0. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1636
https://github.com/scverse/scanpy/issues/1636:2491,interoperability,translat,translate,2491,"tle, size_title, figsize, dendrogram, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, swap_axes, dot_color_df, show, save, ax, return_fig, **kwds). 930 dot_color_df=dot_color_df,. 931 ax=ax,. --> 932 **kwds,. 933 ). 934 . ~/anaconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/_dotplot.py in __init__(self, adata, var_names, groupby, use_raw, log, num_categories, categories_order, title, figsize, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, expression_cutoff, mean_only_expressed, standard_scale, dot_color_df, dot_size_df, ax, **kwds). 142 layer=layer,. 143 ax=ax,. --> 144 **kwds,. 145 ). 146 . ~/anaconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/_baseplot_class.py in __init__(self, adata, var_names, groupby, use_raw, log, num_categories, categories_order, title, figsize, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, ax, **kwds). 111 num_categories,. 112 layer=layer,. --> 113 gene_symbols=gene_symbols,. 114 ). 115 . ~/anaconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/_anndata.py in _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer, gene_symbols). 1837 # translate the column names to the symbol names. 1838 obs_tidy.rename(. -> 1839 columns={var_names[x]: symbols[x] for x in range(len(var_names))},. 1840 inplace=True,. 1841 ). ~/anaconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/_anndata.py in <dictcomp>(.0). 1837 # translate the column names to the symbol names. 1838 obs_tidy.rename(. -> 1839 columns={var_names[x]: symbols[x] for x in range(len(var_names))},. 1840 inplace=True,. 1841 ). NameError: free variable 'symbols' referenced before assignment in enclosing scope. ```. #### Versions. <details>. scanpy==1.6.0 anndata==0.7.4 umap==0.3.10 numpy==1.18.4 scipy==1.4.1 pandas==1.0.3 scikit-learn==0.21.3 statsmodels==0.10.1 python-igraph==0.8.2 louvain==0.6.1 leidenalg==0.8.0. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1636
https://github.com/scverse/scanpy/issues/1636:551,modifiability,modul,module,551,"dotplot cannot set gene_symbols; I would like to change the var name when plotting dotplot by setting the `gene_symbols` to the desired name. But this generates the NameError. Below is an example using the scanpy built-in dataset. ```python. adata = sc.datasets.pbmc68k_reduced(). ax = sc.pl.dotplot(adata, 'C1QA', groupby=['bulk_labels'], swap_axes=False, gene_symbols='TEST'). ```. ```pytb. ---------------------------------------------------------------------------. NameError Traceback (most recent call last). <ipython-input-45-206578ef4fd5> in <module>. 1 adata = sc.datasets.pbmc68k_reduced(). ----> 2 ax = sc.pl.dotplot(adata, 'C1QA', groupby=['bulk_labels'], swap_axes=False, gene_symbols='TEST'). 3 . ~/anaconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/_dotplot.py in dotplot(adata, var_names, groupby, use_raw, log, num_categories, expression_cutoff, mean_only_expressed, cmap, dot_max, dot_min, standard_scale, smallest_dot, title, colorbar_title, size_title, figsize, dendrogram, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, swap_axes, dot_color_df, show, save, ax, return_fig, **kwds). 930 dot_color_df=dot_color_df,. 931 ax=ax,. --> 932 **kwds,. 933 ). 934 . ~/anaconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/_dotplot.py in __init__(self, adata, var_names, groupby, use_raw, log, num_categories, categories_order, title, figsize, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, expression_cutoff, mean_only_expressed, standard_scale, dot_color_df, dot_size_df, ax, **kwds). 142 layer=layer,. 143 ax=ax,. --> 144 **kwds,. 145 ). 146 . ~/anaconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/_baseplot_class.py in __init__(self, adata, var_names, groupby, use_raw, log, num_categories, categories_order, title, figsize, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, ax, **kwds). 111 num_categories,. 112 layer=layer,. --> 113 gene_symbols=gene",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1636
https://github.com/scverse/scanpy/issues/1636:753,modifiability,pac,packages,753,"dotplot cannot set gene_symbols; I would like to change the var name when plotting dotplot by setting the `gene_symbols` to the desired name. But this generates the NameError. Below is an example using the scanpy built-in dataset. ```python. adata = sc.datasets.pbmc68k_reduced(). ax = sc.pl.dotplot(adata, 'C1QA', groupby=['bulk_labels'], swap_axes=False, gene_symbols='TEST'). ```. ```pytb. ---------------------------------------------------------------------------. NameError Traceback (most recent call last). <ipython-input-45-206578ef4fd5> in <module>. 1 adata = sc.datasets.pbmc68k_reduced(). ----> 2 ax = sc.pl.dotplot(adata, 'C1QA', groupby=['bulk_labels'], swap_axes=False, gene_symbols='TEST'). 3 . ~/anaconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/_dotplot.py in dotplot(adata, var_names, groupby, use_raw, log, num_categories, expression_cutoff, mean_only_expressed, cmap, dot_max, dot_min, standard_scale, smallest_dot, title, colorbar_title, size_title, figsize, dendrogram, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, swap_axes, dot_color_df, show, save, ax, return_fig, **kwds). 930 dot_color_df=dot_color_df,. 931 ax=ax,. --> 932 **kwds,. 933 ). 934 . ~/anaconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/_dotplot.py in __init__(self, adata, var_names, groupby, use_raw, log, num_categories, categories_order, title, figsize, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, expression_cutoff, mean_only_expressed, standard_scale, dot_color_df, dot_size_df, ax, **kwds). 142 layer=layer,. 143 ax=ax,. --> 144 **kwds,. 145 ). 146 . ~/anaconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/_baseplot_class.py in __init__(self, adata, var_names, groupby, use_raw, log, num_categories, categories_order, title, figsize, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, ax, **kwds). 111 num_categories,. 112 layer=layer,. --> 113 gene_symbols=gene",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1636
https://github.com/scverse/scanpy/issues/1636:1081,modifiability,layer,layer,1081,"otplot by setting the `gene_symbols` to the desired name. But this generates the NameError. Below is an example using the scanpy built-in dataset. ```python. adata = sc.datasets.pbmc68k_reduced(). ax = sc.pl.dotplot(adata, 'C1QA', groupby=['bulk_labels'], swap_axes=False, gene_symbols='TEST'). ```. ```pytb. ---------------------------------------------------------------------------. NameError Traceback (most recent call last). <ipython-input-45-206578ef4fd5> in <module>. 1 adata = sc.datasets.pbmc68k_reduced(). ----> 2 ax = sc.pl.dotplot(adata, 'C1QA', groupby=['bulk_labels'], swap_axes=False, gene_symbols='TEST'). 3 . ~/anaconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/_dotplot.py in dotplot(adata, var_names, groupby, use_raw, log, num_categories, expression_cutoff, mean_only_expressed, cmap, dot_max, dot_min, standard_scale, smallest_dot, title, colorbar_title, size_title, figsize, dendrogram, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, swap_axes, dot_color_df, show, save, ax, return_fig, **kwds). 930 dot_color_df=dot_color_df,. 931 ax=ax,. --> 932 **kwds,. 933 ). 934 . ~/anaconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/_dotplot.py in __init__(self, adata, var_names, groupby, use_raw, log, num_categories, categories_order, title, figsize, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, expression_cutoff, mean_only_expressed, standard_scale, dot_color_df, dot_size_df, ax, **kwds). 142 layer=layer,. 143 ax=ax,. --> 144 **kwds,. 145 ). 146 . ~/anaconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/_baseplot_class.py in __init__(self, adata, var_names, groupby, use_raw, log, num_categories, categories_order, title, figsize, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, ax, **kwds). 111 num_categories,. 112 layer=layer,. --> 113 gene_symbols=gene_symbols,. 114 ). 115 . ~/anaconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/pl",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1636
https://github.com/scverse/scanpy/issues/1636:1266,modifiability,pac,packages,1266,"reduced(). ax = sc.pl.dotplot(adata, 'C1QA', groupby=['bulk_labels'], swap_axes=False, gene_symbols='TEST'). ```. ```pytb. ---------------------------------------------------------------------------. NameError Traceback (most recent call last). <ipython-input-45-206578ef4fd5> in <module>. 1 adata = sc.datasets.pbmc68k_reduced(). ----> 2 ax = sc.pl.dotplot(adata, 'C1QA', groupby=['bulk_labels'], swap_axes=False, gene_symbols='TEST'). 3 . ~/anaconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/_dotplot.py in dotplot(adata, var_names, groupby, use_raw, log, num_categories, expression_cutoff, mean_only_expressed, cmap, dot_max, dot_min, standard_scale, smallest_dot, title, colorbar_title, size_title, figsize, dendrogram, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, swap_axes, dot_color_df, show, save, ax, return_fig, **kwds). 930 dot_color_df=dot_color_df,. 931 ax=ax,. --> 932 **kwds,. 933 ). 934 . ~/anaconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/_dotplot.py in __init__(self, adata, var_names, groupby, use_raw, log, num_categories, categories_order, title, figsize, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, expression_cutoff, mean_only_expressed, standard_scale, dot_color_df, dot_size_df, ax, **kwds). 142 layer=layer,. 143 ax=ax,. --> 144 **kwds,. 145 ). 146 . ~/anaconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/_baseplot_class.py in __init__(self, adata, var_names, groupby, use_raw, log, num_categories, categories_order, title, figsize, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, ax, **kwds). 111 num_categories,. 112 layer=layer,. --> 113 gene_symbols=gene_symbols,. 114 ). 115 . ~/anaconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/_anndata.py in _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer, gene_symbols). 1837 # translate the column names to the symbol names. 1838 obs_ti",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1636
https://github.com/scverse/scanpy/issues/1636:1485,modifiability,layer,layer,1485,"k (most recent call last). <ipython-input-45-206578ef4fd5> in <module>. 1 adata = sc.datasets.pbmc68k_reduced(). ----> 2 ax = sc.pl.dotplot(adata, 'C1QA', groupby=['bulk_labels'], swap_axes=False, gene_symbols='TEST'). 3 . ~/anaconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/_dotplot.py in dotplot(adata, var_names, groupby, use_raw, log, num_categories, expression_cutoff, mean_only_expressed, cmap, dot_max, dot_min, standard_scale, smallest_dot, title, colorbar_title, size_title, figsize, dendrogram, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, swap_axes, dot_color_df, show, save, ax, return_fig, **kwds). 930 dot_color_df=dot_color_df,. 931 ax=ax,. --> 932 **kwds,. 933 ). 934 . ~/anaconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/_dotplot.py in __init__(self, adata, var_names, groupby, use_raw, log, num_categories, categories_order, title, figsize, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, expression_cutoff, mean_only_expressed, standard_scale, dot_color_df, dot_size_df, ax, **kwds). 142 layer=layer,. 143 ax=ax,. --> 144 **kwds,. 145 ). 146 . ~/anaconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/_baseplot_class.py in __init__(self, adata, var_names, groupby, use_raw, log, num_categories, categories_order, title, figsize, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, ax, **kwds). 111 num_categories,. 112 layer=layer,. --> 113 gene_symbols=gene_symbols,. 114 ). 115 . ~/anaconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/_anndata.py in _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer, gene_symbols). 1837 # translate the column names to the symbol names. 1838 obs_tidy.rename(. -> 1839 columns={var_names[x]: symbols[x] for x in range(len(var_names))},. 1840 inplace=True,. 1841 ). ~/anaconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/_anndata.py in <dictcomp>(.0). 1837",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1636
https://github.com/scverse/scanpy/issues/1636:1592,modifiability,layer,layer,1592,"ed(). ----> 2 ax = sc.pl.dotplot(adata, 'C1QA', groupby=['bulk_labels'], swap_axes=False, gene_symbols='TEST'). 3 . ~/anaconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/_dotplot.py in dotplot(adata, var_names, groupby, use_raw, log, num_categories, expression_cutoff, mean_only_expressed, cmap, dot_max, dot_min, standard_scale, smallest_dot, title, colorbar_title, size_title, figsize, dendrogram, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, swap_axes, dot_color_df, show, save, ax, return_fig, **kwds). 930 dot_color_df=dot_color_df,. 931 ax=ax,. --> 932 **kwds,. 933 ). 934 . ~/anaconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/_dotplot.py in __init__(self, adata, var_names, groupby, use_raw, log, num_categories, categories_order, title, figsize, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, expression_cutoff, mean_only_expressed, standard_scale, dot_color_df, dot_size_df, ax, **kwds). 142 layer=layer,. 143 ax=ax,. --> 144 **kwds,. 145 ). 146 . ~/anaconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/_baseplot_class.py in __init__(self, adata, var_names, groupby, use_raw, log, num_categories, categories_order, title, figsize, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, ax, **kwds). 111 num_categories,. 112 layer=layer,. --> 113 gene_symbols=gene_symbols,. 114 ). 115 . ~/anaconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/_anndata.py in _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer, gene_symbols). 1837 # translate the column names to the symbol names. 1838 obs_tidy.rename(. -> 1839 columns={var_names[x]: symbols[x] for x in range(len(var_names))},. 1840 inplace=True,. 1841 ). ~/anaconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/_anndata.py in <dictcomp>(.0). 1837 # translate the column names to the symbol names. 1838 obs_tidy.rename(. -> 1839 columns={var_names[x]: sy",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1636
https://github.com/scverse/scanpy/issues/1636:1598,modifiability,layer,layer,1598,"----> 2 ax = sc.pl.dotplot(adata, 'C1QA', groupby=['bulk_labels'], swap_axes=False, gene_symbols='TEST'). 3 . ~/anaconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/_dotplot.py in dotplot(adata, var_names, groupby, use_raw, log, num_categories, expression_cutoff, mean_only_expressed, cmap, dot_max, dot_min, standard_scale, smallest_dot, title, colorbar_title, size_title, figsize, dendrogram, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, swap_axes, dot_color_df, show, save, ax, return_fig, **kwds). 930 dot_color_df=dot_color_df,. 931 ax=ax,. --> 932 **kwds,. 933 ). 934 . ~/anaconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/_dotplot.py in __init__(self, adata, var_names, groupby, use_raw, log, num_categories, categories_order, title, figsize, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, expression_cutoff, mean_only_expressed, standard_scale, dot_color_df, dot_size_df, ax, **kwds). 142 layer=layer,. 143 ax=ax,. --> 144 **kwds,. 145 ). 146 . ~/anaconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/_baseplot_class.py in __init__(self, adata, var_names, groupby, use_raw, log, num_categories, categories_order, title, figsize, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, ax, **kwds). 111 num_categories,. 112 layer=layer,. --> 113 gene_symbols=gene_symbols,. 114 ). 115 . ~/anaconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/_anndata.py in _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer, gene_symbols). 1837 # translate the column names to the symbol names. 1838 obs_tidy.rename(. -> 1839 columns={var_names[x]: symbols[x] for x in range(len(var_names))},. 1840 inplace=True,. 1841 ). ~/anaconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/_anndata.py in <dictcomp>(.0). 1837 # translate the column names to the symbol names. 1838 obs_tidy.rename(. -> 1839 columns={var_names[x]: symbols[",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1636
https://github.com/scverse/scanpy/issues/1636:1690,modifiability,pac,packages,1690,"ols='TEST'). 3 . ~/anaconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/_dotplot.py in dotplot(adata, var_names, groupby, use_raw, log, num_categories, expression_cutoff, mean_only_expressed, cmap, dot_max, dot_min, standard_scale, smallest_dot, title, colorbar_title, size_title, figsize, dendrogram, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, swap_axes, dot_color_df, show, save, ax, return_fig, **kwds). 930 dot_color_df=dot_color_df,. 931 ax=ax,. --> 932 **kwds,. 933 ). 934 . ~/anaconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/_dotplot.py in __init__(self, adata, var_names, groupby, use_raw, log, num_categories, categories_order, title, figsize, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, expression_cutoff, mean_only_expressed, standard_scale, dot_color_df, dot_size_df, ax, **kwds). 142 layer=layer,. 143 ax=ax,. --> 144 **kwds,. 145 ). 146 . ~/anaconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/_baseplot_class.py in __init__(self, adata, var_names, groupby, use_raw, log, num_categories, categories_order, title, figsize, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, ax, **kwds). 111 num_categories,. 112 layer=layer,. --> 113 gene_symbols=gene_symbols,. 114 ). 115 . ~/anaconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/_anndata.py in _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer, gene_symbols). 1837 # translate the column names to the symbol names. 1838 obs_tidy.rename(. -> 1839 columns={var_names[x]: symbols[x] for x in range(len(var_names))},. 1840 inplace=True,. 1841 ). ~/anaconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/_anndata.py in <dictcomp>(.0). 1837 # translate the column names to the symbol names. 1838 obs_tidy.rename(. -> 1839 columns={var_names[x]: symbols[x] for x in range(len(var_names))},. 1840 inplace=True,. 1841 ). NameError: free variable 'sy",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1636
https://github.com/scverse/scanpy/issues/1636:1916,modifiability,layer,layer,1916,"n, standard_scale, smallest_dot, title, colorbar_title, size_title, figsize, dendrogram, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, swap_axes, dot_color_df, show, save, ax, return_fig, **kwds). 930 dot_color_df=dot_color_df,. 931 ax=ax,. --> 932 **kwds,. 933 ). 934 . ~/anaconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/_dotplot.py in __init__(self, adata, var_names, groupby, use_raw, log, num_categories, categories_order, title, figsize, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, expression_cutoff, mean_only_expressed, standard_scale, dot_color_df, dot_size_df, ax, **kwds). 142 layer=layer,. 143 ax=ax,. --> 144 **kwds,. 145 ). 146 . ~/anaconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/_baseplot_class.py in __init__(self, adata, var_names, groupby, use_raw, log, num_categories, categories_order, title, figsize, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, ax, **kwds). 111 num_categories,. 112 layer=layer,. --> 113 gene_symbols=gene_symbols,. 114 ). 115 . ~/anaconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/_anndata.py in _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer, gene_symbols). 1837 # translate the column names to the symbol names. 1838 obs_tidy.rename(. -> 1839 columns={var_names[x]: symbols[x] for x in range(len(var_names))},. 1840 inplace=True,. 1841 ). ~/anaconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/_anndata.py in <dictcomp>(.0). 1837 # translate the column names to the symbol names. 1838 obs_tidy.rename(. -> 1839 columns={var_names[x]: symbols[x] for x in range(len(var_names))},. 1840 inplace=True,. 1841 ). NameError: free variable 'symbols' referenced before assignment in enclosing scope. ```. #### Versions. <details>. scanpy==1.6.0 anndata==0.7.4 umap==0.3.10 numpy==1.18.4 scipy==1.4.1 pandas==1.0.3 scikit-learn==0.21.3 statsmodels==0.10.1 python-igraph=",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1636
https://github.com/scverse/scanpy/issues/1636:1961,modifiability,layer,layer,1961,"bar_title, size_title, figsize, dendrogram, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, swap_axes, dot_color_df, show, save, ax, return_fig, **kwds). 930 dot_color_df=dot_color_df,. 931 ax=ax,. --> 932 **kwds,. 933 ). 934 . ~/anaconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/_dotplot.py in __init__(self, adata, var_names, groupby, use_raw, log, num_categories, categories_order, title, figsize, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, expression_cutoff, mean_only_expressed, standard_scale, dot_color_df, dot_size_df, ax, **kwds). 142 layer=layer,. 143 ax=ax,. --> 144 **kwds,. 145 ). 146 . ~/anaconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/_baseplot_class.py in __init__(self, adata, var_names, groupby, use_raw, log, num_categories, categories_order, title, figsize, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, ax, **kwds). 111 num_categories,. 112 layer=layer,. --> 113 gene_symbols=gene_symbols,. 114 ). 115 . ~/anaconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/_anndata.py in _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer, gene_symbols). 1837 # translate the column names to the symbol names. 1838 obs_tidy.rename(. -> 1839 columns={var_names[x]: symbols[x] for x in range(len(var_names))},. 1840 inplace=True,. 1841 ). ~/anaconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/_anndata.py in <dictcomp>(.0). 1837 # translate the column names to the symbol names. 1838 obs_tidy.rename(. -> 1839 columns={var_names[x]: symbols[x] for x in range(len(var_names))},. 1840 inplace=True,. 1841 ). NameError: free variable 'symbols' referenced before assignment in enclosing scope. ```. #### Versions. <details>. scanpy==1.6.0 anndata==0.7.4 umap==0.3.10 numpy==1.18.4 scipy==1.4.1 pandas==1.0.3 scikit-learn==0.21.3 statsmodels==0.10.1 python-igraph==0.8.2 louvain==0.6.1 leidenalg==0.8.0. </det",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1636
https://github.com/scverse/scanpy/issues/1636:1967,modifiability,layer,layer,1967,"tle, size_title, figsize, dendrogram, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, swap_axes, dot_color_df, show, save, ax, return_fig, **kwds). 930 dot_color_df=dot_color_df,. 931 ax=ax,. --> 932 **kwds,. 933 ). 934 . ~/anaconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/_dotplot.py in __init__(self, adata, var_names, groupby, use_raw, log, num_categories, categories_order, title, figsize, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, expression_cutoff, mean_only_expressed, standard_scale, dot_color_df, dot_size_df, ax, **kwds). 142 layer=layer,. 143 ax=ax,. --> 144 **kwds,. 145 ). 146 . ~/anaconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/_baseplot_class.py in __init__(self, adata, var_names, groupby, use_raw, log, num_categories, categories_order, title, figsize, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, ax, **kwds). 111 num_categories,. 112 layer=layer,. --> 113 gene_symbols=gene_symbols,. 114 ). 115 . ~/anaconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/_anndata.py in _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer, gene_symbols). 1837 # translate the column names to the symbol names. 1838 obs_tidy.rename(. -> 1839 columns={var_names[x]: symbols[x] for x in range(len(var_names))},. 1840 inplace=True,. 1841 ). ~/anaconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/_anndata.py in <dictcomp>(.0). 1837 # translate the column names to the symbol names. 1838 obs_tidy.rename(. -> 1839 columns={var_names[x]: symbols[x] for x in range(len(var_names))},. 1840 inplace=True,. 1841 ). NameError: free variable 'symbols' referenced before assignment in enclosing scope. ```. #### Versions. <details>. scanpy==1.6.0 anndata==0.7.4 umap==0.3.10 numpy==1.18.4 scipy==1.4.1 pandas==1.0.3 scikit-learn==0.21.3 statsmodels==0.10.1 python-igraph==0.8.2 louvain==0.6.1 leidenalg==0.8.0. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1636
https://github.com/scverse/scanpy/issues/1636:2066,modifiability,pac,packages,2066,"tle, size_title, figsize, dendrogram, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, swap_axes, dot_color_df, show, save, ax, return_fig, **kwds). 930 dot_color_df=dot_color_df,. 931 ax=ax,. --> 932 **kwds,. 933 ). 934 . ~/anaconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/_dotplot.py in __init__(self, adata, var_names, groupby, use_raw, log, num_categories, categories_order, title, figsize, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, expression_cutoff, mean_only_expressed, standard_scale, dot_color_df, dot_size_df, ax, **kwds). 142 layer=layer,. 143 ax=ax,. --> 144 **kwds,. 145 ). 146 . ~/anaconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/_baseplot_class.py in __init__(self, adata, var_names, groupby, use_raw, log, num_categories, categories_order, title, figsize, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, ax, **kwds). 111 num_categories,. 112 layer=layer,. --> 113 gene_symbols=gene_symbols,. 114 ). 115 . ~/anaconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/_anndata.py in _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer, gene_symbols). 1837 # translate the column names to the symbol names. 1838 obs_tidy.rename(. -> 1839 columns={var_names[x]: symbols[x] for x in range(len(var_names))},. 1840 inplace=True,. 1841 ). ~/anaconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/_anndata.py in <dictcomp>(.0). 1837 # translate the column names to the symbol names. 1838 obs_tidy.rename(. -> 1839 columns={var_names[x]: symbols[x] for x in range(len(var_names))},. 1840 inplace=True,. 1841 ). NameError: free variable 'symbols' referenced before assignment in enclosing scope. ```. #### Versions. <details>. scanpy==1.6.0 anndata==0.7.4 umap==0.3.10 numpy==1.18.4 scipy==1.4.1 pandas==1.0.3 scikit-learn==0.21.3 statsmodels==0.10.1 python-igraph==0.8.2 louvain==0.6.1 leidenalg==0.8.0. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1636
https://github.com/scverse/scanpy/issues/1636:2182,modifiability,layer,layer,2182,"tle, size_title, figsize, dendrogram, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, swap_axes, dot_color_df, show, save, ax, return_fig, **kwds). 930 dot_color_df=dot_color_df,. 931 ax=ax,. --> 932 **kwds,. 933 ). 934 . ~/anaconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/_dotplot.py in __init__(self, adata, var_names, groupby, use_raw, log, num_categories, categories_order, title, figsize, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, expression_cutoff, mean_only_expressed, standard_scale, dot_color_df, dot_size_df, ax, **kwds). 142 layer=layer,. 143 ax=ax,. --> 144 **kwds,. 145 ). 146 . ~/anaconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/_baseplot_class.py in __init__(self, adata, var_names, groupby, use_raw, log, num_categories, categories_order, title, figsize, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, ax, **kwds). 111 num_categories,. 112 layer=layer,. --> 113 gene_symbols=gene_symbols,. 114 ). 115 . ~/anaconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/_anndata.py in _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer, gene_symbols). 1837 # translate the column names to the symbol names. 1838 obs_tidy.rename(. -> 1839 columns={var_names[x]: symbols[x] for x in range(len(var_names))},. 1840 inplace=True,. 1841 ). ~/anaconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/_anndata.py in <dictcomp>(.0). 1837 # translate the column names to the symbol names. 1838 obs_tidy.rename(. -> 1839 columns={var_names[x]: symbols[x] for x in range(len(var_names))},. 1840 inplace=True,. 1841 ). NameError: free variable 'symbols' referenced before assignment in enclosing scope. ```. #### Versions. <details>. scanpy==1.6.0 anndata==0.7.4 umap==0.3.10 numpy==1.18.4 scipy==1.4.1 pandas==1.0.3 scikit-learn==0.21.3 statsmodels==0.10.1 python-igraph==0.8.2 louvain==0.6.1 leidenalg==0.8.0. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1636
https://github.com/scverse/scanpy/issues/1636:2428,modifiability,pac,packages,2428,"tle, size_title, figsize, dendrogram, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, swap_axes, dot_color_df, show, save, ax, return_fig, **kwds). 930 dot_color_df=dot_color_df,. 931 ax=ax,. --> 932 **kwds,. 933 ). 934 . ~/anaconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/_dotplot.py in __init__(self, adata, var_names, groupby, use_raw, log, num_categories, categories_order, title, figsize, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, expression_cutoff, mean_only_expressed, standard_scale, dot_color_df, dot_size_df, ax, **kwds). 142 layer=layer,. 143 ax=ax,. --> 144 **kwds,. 145 ). 146 . ~/anaconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/_baseplot_class.py in __init__(self, adata, var_names, groupby, use_raw, log, num_categories, categories_order, title, figsize, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, ax, **kwds). 111 num_categories,. 112 layer=layer,. --> 113 gene_symbols=gene_symbols,. 114 ). 115 . ~/anaconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/_anndata.py in _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer, gene_symbols). 1837 # translate the column names to the symbol names. 1838 obs_tidy.rename(. -> 1839 columns={var_names[x]: symbols[x] for x in range(len(var_names))},. 1840 inplace=True,. 1841 ). ~/anaconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/_anndata.py in <dictcomp>(.0). 1837 # translate the column names to the symbol names. 1838 obs_tidy.rename(. -> 1839 columns={var_names[x]: symbols[x] for x in range(len(var_names))},. 1840 inplace=True,. 1841 ). NameError: free variable 'symbols' referenced before assignment in enclosing scope. ```. #### Versions. <details>. scanpy==1.6.0 anndata==0.7.4 umap==0.3.10 numpy==1.18.4 scipy==1.4.1 pandas==1.0.3 scikit-learn==0.21.3 statsmodels==0.10.1 python-igraph==0.8.2 louvain==0.6.1 leidenalg==0.8.0. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1636
https://github.com/scverse/scanpy/issues/1636:2682,modifiability,variab,variable,2682,"tle, size_title, figsize, dendrogram, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, swap_axes, dot_color_df, show, save, ax, return_fig, **kwds). 930 dot_color_df=dot_color_df,. 931 ax=ax,. --> 932 **kwds,. 933 ). 934 . ~/anaconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/_dotplot.py in __init__(self, adata, var_names, groupby, use_raw, log, num_categories, categories_order, title, figsize, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, expression_cutoff, mean_only_expressed, standard_scale, dot_color_df, dot_size_df, ax, **kwds). 142 layer=layer,. 143 ax=ax,. --> 144 **kwds,. 145 ). 146 . ~/anaconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/_baseplot_class.py in __init__(self, adata, var_names, groupby, use_raw, log, num_categories, categories_order, title, figsize, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, ax, **kwds). 111 num_categories,. 112 layer=layer,. --> 113 gene_symbols=gene_symbols,. 114 ). 115 . ~/anaconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/_anndata.py in _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer, gene_symbols). 1837 # translate the column names to the symbol names. 1838 obs_tidy.rename(. -> 1839 columns={var_names[x]: symbols[x] for x in range(len(var_names))},. 1840 inplace=True,. 1841 ). ~/anaconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/_anndata.py in <dictcomp>(.0). 1837 # translate the column names to the symbol names. 1838 obs_tidy.rename(. -> 1839 columns={var_names[x]: symbols[x] for x in range(len(var_names))},. 1840 inplace=True,. 1841 ). NameError: free variable 'symbols' referenced before assignment in enclosing scope. ```. #### Versions. <details>. scanpy==1.6.0 anndata==0.7.4 umap==0.3.10 numpy==1.18.4 scipy==1.4.1 pandas==1.0.3 scikit-learn==0.21.3 statsmodels==0.10.1 python-igraph==0.8.2 louvain==0.6.1 leidenalg==0.8.0. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1636
https://github.com/scverse/scanpy/issues/1636:2760,modifiability,Version,Versions,2760,"tle, size_title, figsize, dendrogram, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, swap_axes, dot_color_df, show, save, ax, return_fig, **kwds). 930 dot_color_df=dot_color_df,. 931 ax=ax,. --> 932 **kwds,. 933 ). 934 . ~/anaconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/_dotplot.py in __init__(self, adata, var_names, groupby, use_raw, log, num_categories, categories_order, title, figsize, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, expression_cutoff, mean_only_expressed, standard_scale, dot_color_df, dot_size_df, ax, **kwds). 142 layer=layer,. 143 ax=ax,. --> 144 **kwds,. 145 ). 146 . ~/anaconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/_baseplot_class.py in __init__(self, adata, var_names, groupby, use_raw, log, num_categories, categories_order, title, figsize, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, ax, **kwds). 111 num_categories,. 112 layer=layer,. --> 113 gene_symbols=gene_symbols,. 114 ). 115 . ~/anaconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/_anndata.py in _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer, gene_symbols). 1837 # translate the column names to the symbol names. 1838 obs_tidy.rename(. -> 1839 columns={var_names[x]: symbols[x] for x in range(len(var_names))},. 1840 inplace=True,. 1841 ). ~/anaconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/_anndata.py in <dictcomp>(.0). 1837 # translate the column names to the symbol names. 1838 obs_tidy.rename(. -> 1839 columns={var_names[x]: symbols[x] for x in range(len(var_names))},. 1840 inplace=True,. 1841 ). NameError: free variable 'symbols' referenced before assignment in enclosing scope. ```. #### Versions. <details>. scanpy==1.6.0 anndata==0.7.4 umap==0.3.10 numpy==1.18.4 scipy==1.4.1 pandas==1.0.3 scikit-learn==0.21.3 statsmodels==0.10.1 python-igraph==0.8.2 louvain==0.6.1 leidenalg==0.8.0. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1636
https://github.com/scverse/scanpy/issues/1636:371,safety,TEST,TEST,371,"dotplot cannot set gene_symbols; I would like to change the var name when plotting dotplot by setting the `gene_symbols` to the desired name. But this generates the NameError. Below is an example using the scanpy built-in dataset. ```python. adata = sc.datasets.pbmc68k_reduced(). ax = sc.pl.dotplot(adata, 'C1QA', groupby=['bulk_labels'], swap_axes=False, gene_symbols='TEST'). ```. ```pytb. ---------------------------------------------------------------------------. NameError Traceback (most recent call last). <ipython-input-45-206578ef4fd5> in <module>. 1 adata = sc.datasets.pbmc68k_reduced(). ----> 2 ax = sc.pl.dotplot(adata, 'C1QA', groupby=['bulk_labels'], swap_axes=False, gene_symbols='TEST'). 3 . ~/anaconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/_dotplot.py in dotplot(adata, var_names, groupby, use_raw, log, num_categories, expression_cutoff, mean_only_expressed, cmap, dot_max, dot_min, standard_scale, smallest_dot, title, colorbar_title, size_title, figsize, dendrogram, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, swap_axes, dot_color_df, show, save, ax, return_fig, **kwds). 930 dot_color_df=dot_color_df,. 931 ax=ax,. --> 932 **kwds,. 933 ). 934 . ~/anaconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/_dotplot.py in __init__(self, adata, var_names, groupby, use_raw, log, num_categories, categories_order, title, figsize, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, expression_cutoff, mean_only_expressed, standard_scale, dot_color_df, dot_size_df, ax, **kwds). 142 layer=layer,. 143 ax=ax,. --> 144 **kwds,. 145 ). 146 . ~/anaconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/_baseplot_class.py in __init__(self, adata, var_names, groupby, use_raw, log, num_categories, categories_order, title, figsize, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, ax, **kwds). 111 num_categories,. 112 layer=layer,. --> 113 gene_symbols=gene",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1636
https://github.com/scverse/scanpy/issues/1636:524,safety,input,input-,524,"dotplot cannot set gene_symbols; I would like to change the var name when plotting dotplot by setting the `gene_symbols` to the desired name. But this generates the NameError. Below is an example using the scanpy built-in dataset. ```python. adata = sc.datasets.pbmc68k_reduced(). ax = sc.pl.dotplot(adata, 'C1QA', groupby=['bulk_labels'], swap_axes=False, gene_symbols='TEST'). ```. ```pytb. ---------------------------------------------------------------------------. NameError Traceback (most recent call last). <ipython-input-45-206578ef4fd5> in <module>. 1 adata = sc.datasets.pbmc68k_reduced(). ----> 2 ax = sc.pl.dotplot(adata, 'C1QA', groupby=['bulk_labels'], swap_axes=False, gene_symbols='TEST'). 3 . ~/anaconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/_dotplot.py in dotplot(adata, var_names, groupby, use_raw, log, num_categories, expression_cutoff, mean_only_expressed, cmap, dot_max, dot_min, standard_scale, smallest_dot, title, colorbar_title, size_title, figsize, dendrogram, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, swap_axes, dot_color_df, show, save, ax, return_fig, **kwds). 930 dot_color_df=dot_color_df,. 931 ax=ax,. --> 932 **kwds,. 933 ). 934 . ~/anaconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/_dotplot.py in __init__(self, adata, var_names, groupby, use_raw, log, num_categories, categories_order, title, figsize, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, expression_cutoff, mean_only_expressed, standard_scale, dot_color_df, dot_size_df, ax, **kwds). 142 layer=layer,. 143 ax=ax,. --> 144 **kwds,. 145 ). 146 . ~/anaconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/_baseplot_class.py in __init__(self, adata, var_names, groupby, use_raw, log, num_categories, categories_order, title, figsize, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, ax, **kwds). 111 num_categories,. 112 layer=layer,. --> 113 gene_symbols=gene",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1636
https://github.com/scverse/scanpy/issues/1636:551,safety,modul,module,551,"dotplot cannot set gene_symbols; I would like to change the var name when plotting dotplot by setting the `gene_symbols` to the desired name. But this generates the NameError. Below is an example using the scanpy built-in dataset. ```python. adata = sc.datasets.pbmc68k_reduced(). ax = sc.pl.dotplot(adata, 'C1QA', groupby=['bulk_labels'], swap_axes=False, gene_symbols='TEST'). ```. ```pytb. ---------------------------------------------------------------------------. NameError Traceback (most recent call last). <ipython-input-45-206578ef4fd5> in <module>. 1 adata = sc.datasets.pbmc68k_reduced(). ----> 2 ax = sc.pl.dotplot(adata, 'C1QA', groupby=['bulk_labels'], swap_axes=False, gene_symbols='TEST'). 3 . ~/anaconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/_dotplot.py in dotplot(adata, var_names, groupby, use_raw, log, num_categories, expression_cutoff, mean_only_expressed, cmap, dot_max, dot_min, standard_scale, smallest_dot, title, colorbar_title, size_title, figsize, dendrogram, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, swap_axes, dot_color_df, show, save, ax, return_fig, **kwds). 930 dot_color_df=dot_color_df,. 931 ax=ax,. --> 932 **kwds,. 933 ). 934 . ~/anaconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/_dotplot.py in __init__(self, adata, var_names, groupby, use_raw, log, num_categories, categories_order, title, figsize, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, expression_cutoff, mean_only_expressed, standard_scale, dot_color_df, dot_size_df, ax, **kwds). 142 layer=layer,. 143 ax=ax,. --> 144 **kwds,. 145 ). 146 . ~/anaconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/_baseplot_class.py in __init__(self, adata, var_names, groupby, use_raw, log, num_categories, categories_order, title, figsize, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, ax, **kwds). 111 num_categories,. 112 layer=layer,. --> 113 gene_symbols=gene",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1636
https://github.com/scverse/scanpy/issues/1636:699,safety,TEST,TEST,699,"dotplot cannot set gene_symbols; I would like to change the var name when plotting dotplot by setting the `gene_symbols` to the desired name. But this generates the NameError. Below is an example using the scanpy built-in dataset. ```python. adata = sc.datasets.pbmc68k_reduced(). ax = sc.pl.dotplot(adata, 'C1QA', groupby=['bulk_labels'], swap_axes=False, gene_symbols='TEST'). ```. ```pytb. ---------------------------------------------------------------------------. NameError Traceback (most recent call last). <ipython-input-45-206578ef4fd5> in <module>. 1 adata = sc.datasets.pbmc68k_reduced(). ----> 2 ax = sc.pl.dotplot(adata, 'C1QA', groupby=['bulk_labels'], swap_axes=False, gene_symbols='TEST'). 3 . ~/anaconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/_dotplot.py in dotplot(adata, var_names, groupby, use_raw, log, num_categories, expression_cutoff, mean_only_expressed, cmap, dot_max, dot_min, standard_scale, smallest_dot, title, colorbar_title, size_title, figsize, dendrogram, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, swap_axes, dot_color_df, show, save, ax, return_fig, **kwds). 930 dot_color_df=dot_color_df,. 931 ax=ax,. --> 932 **kwds,. 933 ). 934 . ~/anaconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/_dotplot.py in __init__(self, adata, var_names, groupby, use_raw, log, num_categories, categories_order, title, figsize, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, expression_cutoff, mean_only_expressed, standard_scale, dot_color_df, dot_size_df, ax, **kwds). 142 layer=layer,. 143 ax=ax,. --> 144 **kwds,. 145 ). 146 . ~/anaconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/_baseplot_class.py in __init__(self, adata, var_names, groupby, use_raw, log, num_categories, categories_order, title, figsize, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, ax, **kwds). 111 num_categories,. 112 layer=layer,. --> 113 gene_symbols=gene",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1636
https://github.com/scverse/scanpy/issues/1636:837,safety,log,log,837,"dotplot cannot set gene_symbols; I would like to change the var name when plotting dotplot by setting the `gene_symbols` to the desired name. But this generates the NameError. Below is an example using the scanpy built-in dataset. ```python. adata = sc.datasets.pbmc68k_reduced(). ax = sc.pl.dotplot(adata, 'C1QA', groupby=['bulk_labels'], swap_axes=False, gene_symbols='TEST'). ```. ```pytb. ---------------------------------------------------------------------------. NameError Traceback (most recent call last). <ipython-input-45-206578ef4fd5> in <module>. 1 adata = sc.datasets.pbmc68k_reduced(). ----> 2 ax = sc.pl.dotplot(adata, 'C1QA', groupby=['bulk_labels'], swap_axes=False, gene_symbols='TEST'). 3 . ~/anaconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/_dotplot.py in dotplot(adata, var_names, groupby, use_raw, log, num_categories, expression_cutoff, mean_only_expressed, cmap, dot_max, dot_min, standard_scale, smallest_dot, title, colorbar_title, size_title, figsize, dendrogram, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, swap_axes, dot_color_df, show, save, ax, return_fig, **kwds). 930 dot_color_df=dot_color_df,. 931 ax=ax,. --> 932 **kwds,. 933 ). 934 . ~/anaconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/_dotplot.py in __init__(self, adata, var_names, groupby, use_raw, log, num_categories, categories_order, title, figsize, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, expression_cutoff, mean_only_expressed, standard_scale, dot_color_df, dot_size_df, ax, **kwds). 142 layer=layer,. 143 ax=ax,. --> 144 **kwds,. 145 ). 146 . ~/anaconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/_baseplot_class.py in __init__(self, adata, var_names, groupby, use_raw, log, num_categories, categories_order, title, figsize, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, ax, **kwds). 111 num_categories,. 112 layer=layer,. --> 113 gene_symbols=gene",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1636
https://github.com/scverse/scanpy/issues/1636:1357,safety,log,log,1357,"ne_symbols='TEST'). ```. ```pytb. ---------------------------------------------------------------------------. NameError Traceback (most recent call last). <ipython-input-45-206578ef4fd5> in <module>. 1 adata = sc.datasets.pbmc68k_reduced(). ----> 2 ax = sc.pl.dotplot(adata, 'C1QA', groupby=['bulk_labels'], swap_axes=False, gene_symbols='TEST'). 3 . ~/anaconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/_dotplot.py in dotplot(adata, var_names, groupby, use_raw, log, num_categories, expression_cutoff, mean_only_expressed, cmap, dot_max, dot_min, standard_scale, smallest_dot, title, colorbar_title, size_title, figsize, dendrogram, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, swap_axes, dot_color_df, show, save, ax, return_fig, **kwds). 930 dot_color_df=dot_color_df,. 931 ax=ax,. --> 932 **kwds,. 933 ). 934 . ~/anaconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/_dotplot.py in __init__(self, adata, var_names, groupby, use_raw, log, num_categories, categories_order, title, figsize, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, expression_cutoff, mean_only_expressed, standard_scale, dot_color_df, dot_size_df, ax, **kwds). 142 layer=layer,. 143 ax=ax,. --> 144 **kwds,. 145 ). 146 . ~/anaconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/_baseplot_class.py in __init__(self, adata, var_names, groupby, use_raw, log, num_categories, categories_order, title, figsize, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, ax, **kwds). 111 num_categories,. 112 layer=layer,. --> 113 gene_symbols=gene_symbols,. 114 ). 115 . ~/anaconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/_anndata.py in _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer, gene_symbols). 1837 # translate the column names to the symbol names. 1838 obs_tidy.rename(. -> 1839 columns={var_names[x]: symbols[x] for x in range(len(var_names))},. 1",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1636
https://github.com/scverse/scanpy/issues/1636:1788,safety,log,log,1788,"in dotplot(adata, var_names, groupby, use_raw, log, num_categories, expression_cutoff, mean_only_expressed, cmap, dot_max, dot_min, standard_scale, smallest_dot, title, colorbar_title, size_title, figsize, dendrogram, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, swap_axes, dot_color_df, show, save, ax, return_fig, **kwds). 930 dot_color_df=dot_color_df,. 931 ax=ax,. --> 932 **kwds,. 933 ). 934 . ~/anaconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/_dotplot.py in __init__(self, adata, var_names, groupby, use_raw, log, num_categories, categories_order, title, figsize, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, expression_cutoff, mean_only_expressed, standard_scale, dot_color_df, dot_size_df, ax, **kwds). 142 layer=layer,. 143 ax=ax,. --> 144 **kwds,. 145 ). 146 . ~/anaconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/_baseplot_class.py in __init__(self, adata, var_names, groupby, use_raw, log, num_categories, categories_order, title, figsize, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, ax, **kwds). 111 num_categories,. 112 layer=layer,. --> 113 gene_symbols=gene_symbols,. 114 ). 115 . ~/anaconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/_anndata.py in _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer, gene_symbols). 1837 # translate the column names to the symbol names. 1838 obs_tidy.rename(. -> 1839 columns={var_names[x]: symbols[x] for x in range(len(var_names))},. 1840 inplace=True,. 1841 ). ~/anaconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/_anndata.py in <dictcomp>(.0). 1837 # translate the column names to the symbol names. 1838 obs_tidy.rename(. -> 1839 columns={var_names[x]: symbols[x] for x in range(len(var_names))},. 1840 inplace=True,. 1841 ). NameError: free variable 'symbols' referenced before assignment in enclosing scope. ```. #### Versions. <details>. scanpy==1",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1636
https://github.com/scverse/scanpy/issues/1636:2161,safety,log,log,2161,"tle, size_title, figsize, dendrogram, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, swap_axes, dot_color_df, show, save, ax, return_fig, **kwds). 930 dot_color_df=dot_color_df,. 931 ax=ax,. --> 932 **kwds,. 933 ). 934 . ~/anaconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/_dotplot.py in __init__(self, adata, var_names, groupby, use_raw, log, num_categories, categories_order, title, figsize, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, expression_cutoff, mean_only_expressed, standard_scale, dot_color_df, dot_size_df, ax, **kwds). 142 layer=layer,. 143 ax=ax,. --> 144 **kwds,. 145 ). 146 . ~/anaconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/_baseplot_class.py in __init__(self, adata, var_names, groupby, use_raw, log, num_categories, categories_order, title, figsize, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, ax, **kwds). 111 num_categories,. 112 layer=layer,. --> 113 gene_symbols=gene_symbols,. 114 ). 115 . ~/anaconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/_anndata.py in _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer, gene_symbols). 1837 # translate the column names to the symbol names. 1838 obs_tidy.rename(. -> 1839 columns={var_names[x]: symbols[x] for x in range(len(var_names))},. 1840 inplace=True,. 1841 ). ~/anaconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/_anndata.py in <dictcomp>(.0). 1837 # translate the column names to the symbol names. 1838 obs_tidy.rename(. -> 1839 columns={var_names[x]: symbols[x] for x in range(len(var_names))},. 1840 inplace=True,. 1841 ). NameError: free variable 'symbols' referenced before assignment in enclosing scope. ```. #### Versions. <details>. scanpy==1.6.0 anndata==0.7.4 umap==0.3.10 numpy==1.18.4 scipy==1.4.1 pandas==1.0.3 scikit-learn==0.21.3 statsmodels==0.10.1 python-igraph==0.8.2 louvain==0.6.1 leidenalg==0.8.0. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1636
https://github.com/scverse/scanpy/issues/1636:837,security,log,log,837,"dotplot cannot set gene_symbols; I would like to change the var name when plotting dotplot by setting the `gene_symbols` to the desired name. But this generates the NameError. Below is an example using the scanpy built-in dataset. ```python. adata = sc.datasets.pbmc68k_reduced(). ax = sc.pl.dotplot(adata, 'C1QA', groupby=['bulk_labels'], swap_axes=False, gene_symbols='TEST'). ```. ```pytb. ---------------------------------------------------------------------------. NameError Traceback (most recent call last). <ipython-input-45-206578ef4fd5> in <module>. 1 adata = sc.datasets.pbmc68k_reduced(). ----> 2 ax = sc.pl.dotplot(adata, 'C1QA', groupby=['bulk_labels'], swap_axes=False, gene_symbols='TEST'). 3 . ~/anaconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/_dotplot.py in dotplot(adata, var_names, groupby, use_raw, log, num_categories, expression_cutoff, mean_only_expressed, cmap, dot_max, dot_min, standard_scale, smallest_dot, title, colorbar_title, size_title, figsize, dendrogram, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, swap_axes, dot_color_df, show, save, ax, return_fig, **kwds). 930 dot_color_df=dot_color_df,. 931 ax=ax,. --> 932 **kwds,. 933 ). 934 . ~/anaconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/_dotplot.py in __init__(self, adata, var_names, groupby, use_raw, log, num_categories, categories_order, title, figsize, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, expression_cutoff, mean_only_expressed, standard_scale, dot_color_df, dot_size_df, ax, **kwds). 142 layer=layer,. 143 ax=ax,. --> 144 **kwds,. 145 ). 146 . ~/anaconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/_baseplot_class.py in __init__(self, adata, var_names, groupby, use_raw, log, num_categories, categories_order, title, figsize, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, ax, **kwds). 111 num_categories,. 112 layer=layer,. --> 113 gene_symbols=gene",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1636
https://github.com/scverse/scanpy/issues/1636:1357,security,log,log,1357,"ne_symbols='TEST'). ```. ```pytb. ---------------------------------------------------------------------------. NameError Traceback (most recent call last). <ipython-input-45-206578ef4fd5> in <module>. 1 adata = sc.datasets.pbmc68k_reduced(). ----> 2 ax = sc.pl.dotplot(adata, 'C1QA', groupby=['bulk_labels'], swap_axes=False, gene_symbols='TEST'). 3 . ~/anaconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/_dotplot.py in dotplot(adata, var_names, groupby, use_raw, log, num_categories, expression_cutoff, mean_only_expressed, cmap, dot_max, dot_min, standard_scale, smallest_dot, title, colorbar_title, size_title, figsize, dendrogram, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, swap_axes, dot_color_df, show, save, ax, return_fig, **kwds). 930 dot_color_df=dot_color_df,. 931 ax=ax,. --> 932 **kwds,. 933 ). 934 . ~/anaconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/_dotplot.py in __init__(self, adata, var_names, groupby, use_raw, log, num_categories, categories_order, title, figsize, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, expression_cutoff, mean_only_expressed, standard_scale, dot_color_df, dot_size_df, ax, **kwds). 142 layer=layer,. 143 ax=ax,. --> 144 **kwds,. 145 ). 146 . ~/anaconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/_baseplot_class.py in __init__(self, adata, var_names, groupby, use_raw, log, num_categories, categories_order, title, figsize, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, ax, **kwds). 111 num_categories,. 112 layer=layer,. --> 113 gene_symbols=gene_symbols,. 114 ). 115 . ~/anaconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/_anndata.py in _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer, gene_symbols). 1837 # translate the column names to the symbol names. 1838 obs_tidy.rename(. -> 1839 columns={var_names[x]: symbols[x] for x in range(len(var_names))},. 1",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1636
https://github.com/scverse/scanpy/issues/1636:1788,security,log,log,1788,"in dotplot(adata, var_names, groupby, use_raw, log, num_categories, expression_cutoff, mean_only_expressed, cmap, dot_max, dot_min, standard_scale, smallest_dot, title, colorbar_title, size_title, figsize, dendrogram, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, swap_axes, dot_color_df, show, save, ax, return_fig, **kwds). 930 dot_color_df=dot_color_df,. 931 ax=ax,. --> 932 **kwds,. 933 ). 934 . ~/anaconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/_dotplot.py in __init__(self, adata, var_names, groupby, use_raw, log, num_categories, categories_order, title, figsize, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, expression_cutoff, mean_only_expressed, standard_scale, dot_color_df, dot_size_df, ax, **kwds). 142 layer=layer,. 143 ax=ax,. --> 144 **kwds,. 145 ). 146 . ~/anaconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/_baseplot_class.py in __init__(self, adata, var_names, groupby, use_raw, log, num_categories, categories_order, title, figsize, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, ax, **kwds). 111 num_categories,. 112 layer=layer,. --> 113 gene_symbols=gene_symbols,. 114 ). 115 . ~/anaconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/_anndata.py in _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer, gene_symbols). 1837 # translate the column names to the symbol names. 1838 obs_tidy.rename(. -> 1839 columns={var_names[x]: symbols[x] for x in range(len(var_names))},. 1840 inplace=True,. 1841 ). ~/anaconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/_anndata.py in <dictcomp>(.0). 1837 # translate the column names to the symbol names. 1838 obs_tidy.rename(. -> 1839 columns={var_names[x]: symbols[x] for x in range(len(var_names))},. 1840 inplace=True,. 1841 ). NameError: free variable 'symbols' referenced before assignment in enclosing scope. ```. #### Versions. <details>. scanpy==1",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1636
https://github.com/scverse/scanpy/issues/1636:2161,security,log,log,2161,"tle, size_title, figsize, dendrogram, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, swap_axes, dot_color_df, show, save, ax, return_fig, **kwds). 930 dot_color_df=dot_color_df,. 931 ax=ax,. --> 932 **kwds,. 933 ). 934 . ~/anaconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/_dotplot.py in __init__(self, adata, var_names, groupby, use_raw, log, num_categories, categories_order, title, figsize, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, expression_cutoff, mean_only_expressed, standard_scale, dot_color_df, dot_size_df, ax, **kwds). 142 layer=layer,. 143 ax=ax,. --> 144 **kwds,. 145 ). 146 . ~/anaconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/_baseplot_class.py in __init__(self, adata, var_names, groupby, use_raw, log, num_categories, categories_order, title, figsize, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, ax, **kwds). 111 num_categories,. 112 layer=layer,. --> 113 gene_symbols=gene_symbols,. 114 ). 115 . ~/anaconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/_anndata.py in _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer, gene_symbols). 1837 # translate the column names to the symbol names. 1838 obs_tidy.rename(. -> 1839 columns={var_names[x]: symbols[x] for x in range(len(var_names))},. 1840 inplace=True,. 1841 ). ~/anaconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/_anndata.py in <dictcomp>(.0). 1837 # translate the column names to the symbol names. 1838 obs_tidy.rename(. -> 1839 columns={var_names[x]: symbols[x] for x in range(len(var_names))},. 1840 inplace=True,. 1841 ). NameError: free variable 'symbols' referenced before assignment in enclosing scope. ```. #### Versions. <details>. scanpy==1.6.0 anndata==0.7.4 umap==0.3.10 numpy==1.18.4 scipy==1.4.1 pandas==1.0.3 scikit-learn==0.21.3 statsmodels==0.10.1 python-igraph==0.8.2 louvain==0.6.1 leidenalg==0.8.0. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1636
https://github.com/scverse/scanpy/issues/1636:371,testability,TEST,TEST,371,"dotplot cannot set gene_symbols; I would like to change the var name when plotting dotplot by setting the `gene_symbols` to the desired name. But this generates the NameError. Below is an example using the scanpy built-in dataset. ```python. adata = sc.datasets.pbmc68k_reduced(). ax = sc.pl.dotplot(adata, 'C1QA', groupby=['bulk_labels'], swap_axes=False, gene_symbols='TEST'). ```. ```pytb. ---------------------------------------------------------------------------. NameError Traceback (most recent call last). <ipython-input-45-206578ef4fd5> in <module>. 1 adata = sc.datasets.pbmc68k_reduced(). ----> 2 ax = sc.pl.dotplot(adata, 'C1QA', groupby=['bulk_labels'], swap_axes=False, gene_symbols='TEST'). 3 . ~/anaconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/_dotplot.py in dotplot(adata, var_names, groupby, use_raw, log, num_categories, expression_cutoff, mean_only_expressed, cmap, dot_max, dot_min, standard_scale, smallest_dot, title, colorbar_title, size_title, figsize, dendrogram, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, swap_axes, dot_color_df, show, save, ax, return_fig, **kwds). 930 dot_color_df=dot_color_df,. 931 ax=ax,. --> 932 **kwds,. 933 ). 934 . ~/anaconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/_dotplot.py in __init__(self, adata, var_names, groupby, use_raw, log, num_categories, categories_order, title, figsize, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, expression_cutoff, mean_only_expressed, standard_scale, dot_color_df, dot_size_df, ax, **kwds). 142 layer=layer,. 143 ax=ax,. --> 144 **kwds,. 145 ). 146 . ~/anaconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/_baseplot_class.py in __init__(self, adata, var_names, groupby, use_raw, log, num_categories, categories_order, title, figsize, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, ax, **kwds). 111 num_categories,. 112 layer=layer,. --> 113 gene_symbols=gene",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1636
https://github.com/scverse/scanpy/issues/1636:480,testability,Trace,Traceback,480,"dotplot cannot set gene_symbols; I would like to change the var name when plotting dotplot by setting the `gene_symbols` to the desired name. But this generates the NameError. Below is an example using the scanpy built-in dataset. ```python. adata = sc.datasets.pbmc68k_reduced(). ax = sc.pl.dotplot(adata, 'C1QA', groupby=['bulk_labels'], swap_axes=False, gene_symbols='TEST'). ```. ```pytb. ---------------------------------------------------------------------------. NameError Traceback (most recent call last). <ipython-input-45-206578ef4fd5> in <module>. 1 adata = sc.datasets.pbmc68k_reduced(). ----> 2 ax = sc.pl.dotplot(adata, 'C1QA', groupby=['bulk_labels'], swap_axes=False, gene_symbols='TEST'). 3 . ~/anaconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/_dotplot.py in dotplot(adata, var_names, groupby, use_raw, log, num_categories, expression_cutoff, mean_only_expressed, cmap, dot_max, dot_min, standard_scale, smallest_dot, title, colorbar_title, size_title, figsize, dendrogram, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, swap_axes, dot_color_df, show, save, ax, return_fig, **kwds). 930 dot_color_df=dot_color_df,. 931 ax=ax,. --> 932 **kwds,. 933 ). 934 . ~/anaconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/_dotplot.py in __init__(self, adata, var_names, groupby, use_raw, log, num_categories, categories_order, title, figsize, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, expression_cutoff, mean_only_expressed, standard_scale, dot_color_df, dot_size_df, ax, **kwds). 142 layer=layer,. 143 ax=ax,. --> 144 **kwds,. 145 ). 146 . ~/anaconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/_baseplot_class.py in __init__(self, adata, var_names, groupby, use_raw, log, num_categories, categories_order, title, figsize, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, ax, **kwds). 111 num_categories,. 112 layer=layer,. --> 113 gene_symbols=gene",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1636
https://github.com/scverse/scanpy/issues/1636:699,testability,TEST,TEST,699,"dotplot cannot set gene_symbols; I would like to change the var name when plotting dotplot by setting the `gene_symbols` to the desired name. But this generates the NameError. Below is an example using the scanpy built-in dataset. ```python. adata = sc.datasets.pbmc68k_reduced(). ax = sc.pl.dotplot(adata, 'C1QA', groupby=['bulk_labels'], swap_axes=False, gene_symbols='TEST'). ```. ```pytb. ---------------------------------------------------------------------------. NameError Traceback (most recent call last). <ipython-input-45-206578ef4fd5> in <module>. 1 adata = sc.datasets.pbmc68k_reduced(). ----> 2 ax = sc.pl.dotplot(adata, 'C1QA', groupby=['bulk_labels'], swap_axes=False, gene_symbols='TEST'). 3 . ~/anaconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/_dotplot.py in dotplot(adata, var_names, groupby, use_raw, log, num_categories, expression_cutoff, mean_only_expressed, cmap, dot_max, dot_min, standard_scale, smallest_dot, title, colorbar_title, size_title, figsize, dendrogram, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, swap_axes, dot_color_df, show, save, ax, return_fig, **kwds). 930 dot_color_df=dot_color_df,. 931 ax=ax,. --> 932 **kwds,. 933 ). 934 . ~/anaconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/_dotplot.py in __init__(self, adata, var_names, groupby, use_raw, log, num_categories, categories_order, title, figsize, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, expression_cutoff, mean_only_expressed, standard_scale, dot_color_df, dot_size_df, ax, **kwds). 142 layer=layer,. 143 ax=ax,. --> 144 **kwds,. 145 ). 146 . ~/anaconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/_baseplot_class.py in __init__(self, adata, var_names, groupby, use_raw, log, num_categories, categories_order, title, figsize, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, ax, **kwds). 111 num_categories,. 112 layer=layer,. --> 113 gene_symbols=gene",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1636
https://github.com/scverse/scanpy/issues/1636:837,testability,log,log,837,"dotplot cannot set gene_symbols; I would like to change the var name when plotting dotplot by setting the `gene_symbols` to the desired name. But this generates the NameError. Below is an example using the scanpy built-in dataset. ```python. adata = sc.datasets.pbmc68k_reduced(). ax = sc.pl.dotplot(adata, 'C1QA', groupby=['bulk_labels'], swap_axes=False, gene_symbols='TEST'). ```. ```pytb. ---------------------------------------------------------------------------. NameError Traceback (most recent call last). <ipython-input-45-206578ef4fd5> in <module>. 1 adata = sc.datasets.pbmc68k_reduced(). ----> 2 ax = sc.pl.dotplot(adata, 'C1QA', groupby=['bulk_labels'], swap_axes=False, gene_symbols='TEST'). 3 . ~/anaconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/_dotplot.py in dotplot(adata, var_names, groupby, use_raw, log, num_categories, expression_cutoff, mean_only_expressed, cmap, dot_max, dot_min, standard_scale, smallest_dot, title, colorbar_title, size_title, figsize, dendrogram, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, swap_axes, dot_color_df, show, save, ax, return_fig, **kwds). 930 dot_color_df=dot_color_df,. 931 ax=ax,. --> 932 **kwds,. 933 ). 934 . ~/anaconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/_dotplot.py in __init__(self, adata, var_names, groupby, use_raw, log, num_categories, categories_order, title, figsize, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, expression_cutoff, mean_only_expressed, standard_scale, dot_color_df, dot_size_df, ax, **kwds). 142 layer=layer,. 143 ax=ax,. --> 144 **kwds,. 145 ). 146 . ~/anaconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/_baseplot_class.py in __init__(self, adata, var_names, groupby, use_raw, log, num_categories, categories_order, title, figsize, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, ax, **kwds). 111 num_categories,. 112 layer=layer,. --> 113 gene_symbols=gene",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1636
https://github.com/scverse/scanpy/issues/1636:1357,testability,log,log,1357,"ne_symbols='TEST'). ```. ```pytb. ---------------------------------------------------------------------------. NameError Traceback (most recent call last). <ipython-input-45-206578ef4fd5> in <module>. 1 adata = sc.datasets.pbmc68k_reduced(). ----> 2 ax = sc.pl.dotplot(adata, 'C1QA', groupby=['bulk_labels'], swap_axes=False, gene_symbols='TEST'). 3 . ~/anaconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/_dotplot.py in dotplot(adata, var_names, groupby, use_raw, log, num_categories, expression_cutoff, mean_only_expressed, cmap, dot_max, dot_min, standard_scale, smallest_dot, title, colorbar_title, size_title, figsize, dendrogram, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, swap_axes, dot_color_df, show, save, ax, return_fig, **kwds). 930 dot_color_df=dot_color_df,. 931 ax=ax,. --> 932 **kwds,. 933 ). 934 . ~/anaconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/_dotplot.py in __init__(self, adata, var_names, groupby, use_raw, log, num_categories, categories_order, title, figsize, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, expression_cutoff, mean_only_expressed, standard_scale, dot_color_df, dot_size_df, ax, **kwds). 142 layer=layer,. 143 ax=ax,. --> 144 **kwds,. 145 ). 146 . ~/anaconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/_baseplot_class.py in __init__(self, adata, var_names, groupby, use_raw, log, num_categories, categories_order, title, figsize, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, ax, **kwds). 111 num_categories,. 112 layer=layer,. --> 113 gene_symbols=gene_symbols,. 114 ). 115 . ~/anaconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/_anndata.py in _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer, gene_symbols). 1837 # translate the column names to the symbol names. 1838 obs_tidy.rename(. -> 1839 columns={var_names[x]: symbols[x] for x in range(len(var_names))},. 1",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1636
https://github.com/scverse/scanpy/issues/1636:1788,testability,log,log,1788,"in dotplot(adata, var_names, groupby, use_raw, log, num_categories, expression_cutoff, mean_only_expressed, cmap, dot_max, dot_min, standard_scale, smallest_dot, title, colorbar_title, size_title, figsize, dendrogram, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, swap_axes, dot_color_df, show, save, ax, return_fig, **kwds). 930 dot_color_df=dot_color_df,. 931 ax=ax,. --> 932 **kwds,. 933 ). 934 . ~/anaconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/_dotplot.py in __init__(self, adata, var_names, groupby, use_raw, log, num_categories, categories_order, title, figsize, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, expression_cutoff, mean_only_expressed, standard_scale, dot_color_df, dot_size_df, ax, **kwds). 142 layer=layer,. 143 ax=ax,. --> 144 **kwds,. 145 ). 146 . ~/anaconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/_baseplot_class.py in __init__(self, adata, var_names, groupby, use_raw, log, num_categories, categories_order, title, figsize, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, ax, **kwds). 111 num_categories,. 112 layer=layer,. --> 113 gene_symbols=gene_symbols,. 114 ). 115 . ~/anaconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/_anndata.py in _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer, gene_symbols). 1837 # translate the column names to the symbol names. 1838 obs_tidy.rename(. -> 1839 columns={var_names[x]: symbols[x] for x in range(len(var_names))},. 1840 inplace=True,. 1841 ). ~/anaconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/_anndata.py in <dictcomp>(.0). 1837 # translate the column names to the symbol names. 1838 obs_tidy.rename(. -> 1839 columns={var_names[x]: symbols[x] for x in range(len(var_names))},. 1840 inplace=True,. 1841 ). NameError: free variable 'symbols' referenced before assignment in enclosing scope. ```. #### Versions. <details>. scanpy==1",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1636
https://github.com/scverse/scanpy/issues/1636:2161,testability,log,log,2161,"tle, size_title, figsize, dendrogram, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, swap_axes, dot_color_df, show, save, ax, return_fig, **kwds). 930 dot_color_df=dot_color_df,. 931 ax=ax,. --> 932 **kwds,. 933 ). 934 . ~/anaconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/_dotplot.py in __init__(self, adata, var_names, groupby, use_raw, log, num_categories, categories_order, title, figsize, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, expression_cutoff, mean_only_expressed, standard_scale, dot_color_df, dot_size_df, ax, **kwds). 142 layer=layer,. 143 ax=ax,. --> 144 **kwds,. 145 ). 146 . ~/anaconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/_baseplot_class.py in __init__(self, adata, var_names, groupby, use_raw, log, num_categories, categories_order, title, figsize, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, ax, **kwds). 111 num_categories,. 112 layer=layer,. --> 113 gene_symbols=gene_symbols,. 114 ). 115 . ~/anaconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/_anndata.py in _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer, gene_symbols). 1837 # translate the column names to the symbol names. 1838 obs_tidy.rename(. -> 1839 columns={var_names[x]: symbols[x] for x in range(len(var_names))},. 1840 inplace=True,. 1841 ). ~/anaconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/_anndata.py in <dictcomp>(.0). 1837 # translate the column names to the symbol names. 1838 obs_tidy.rename(. -> 1839 columns={var_names[x]: symbols[x] for x in range(len(var_names))},. 1840 inplace=True,. 1841 ). NameError: free variable 'symbols' referenced before assignment in enclosing scope. ```. #### Versions. <details>. scanpy==1.6.0 anndata==0.7.4 umap==0.3.10 numpy==1.18.4 scipy==1.4.1 pandas==1.0.3 scikit-learn==0.21.3 statsmodels==0.10.1 python-igraph==0.8.2 louvain==0.6.1 leidenalg==0.8.0. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1636
https://github.com/scverse/scanpy/issues/1636:524,usability,input,input-,524,"dotplot cannot set gene_symbols; I would like to change the var name when plotting dotplot by setting the `gene_symbols` to the desired name. But this generates the NameError. Below is an example using the scanpy built-in dataset. ```python. adata = sc.datasets.pbmc68k_reduced(). ax = sc.pl.dotplot(adata, 'C1QA', groupby=['bulk_labels'], swap_axes=False, gene_symbols='TEST'). ```. ```pytb. ---------------------------------------------------------------------------. NameError Traceback (most recent call last). <ipython-input-45-206578ef4fd5> in <module>. 1 adata = sc.datasets.pbmc68k_reduced(). ----> 2 ax = sc.pl.dotplot(adata, 'C1QA', groupby=['bulk_labels'], swap_axes=False, gene_symbols='TEST'). 3 . ~/anaconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/_dotplot.py in dotplot(adata, var_names, groupby, use_raw, log, num_categories, expression_cutoff, mean_only_expressed, cmap, dot_max, dot_min, standard_scale, smallest_dot, title, colorbar_title, size_title, figsize, dendrogram, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, swap_axes, dot_color_df, show, save, ax, return_fig, **kwds). 930 dot_color_df=dot_color_df,. 931 ax=ax,. --> 932 **kwds,. 933 ). 934 . ~/anaconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/_dotplot.py in __init__(self, adata, var_names, groupby, use_raw, log, num_categories, categories_order, title, figsize, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, expression_cutoff, mean_only_expressed, standard_scale, dot_color_df, dot_size_df, ax, **kwds). 142 layer=layer,. 143 ax=ax,. --> 144 **kwds,. 145 ). 146 . ~/anaconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/_baseplot_class.py in __init__(self, adata, var_names, groupby, use_raw, log, num_categories, categories_order, title, figsize, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, ax, **kwds). 111 num_categories,. 112 layer=layer,. --> 113 gene_symbols=gene",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1636
https://github.com/scverse/scanpy/issues/1636:2871,usability,learn,learn,2871,"tle, size_title, figsize, dendrogram, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, swap_axes, dot_color_df, show, save, ax, return_fig, **kwds). 930 dot_color_df=dot_color_df,. 931 ax=ax,. --> 932 **kwds,. 933 ). 934 . ~/anaconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/_dotplot.py in __init__(self, adata, var_names, groupby, use_raw, log, num_categories, categories_order, title, figsize, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, expression_cutoff, mean_only_expressed, standard_scale, dot_color_df, dot_size_df, ax, **kwds). 142 layer=layer,. 143 ax=ax,. --> 144 **kwds,. 145 ). 146 . ~/anaconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/_baseplot_class.py in __init__(self, adata, var_names, groupby, use_raw, log, num_categories, categories_order, title, figsize, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, ax, **kwds). 111 num_categories,. 112 layer=layer,. --> 113 gene_symbols=gene_symbols,. 114 ). 115 . ~/anaconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/_anndata.py in _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer, gene_symbols). 1837 # translate the column names to the symbol names. 1838 obs_tidy.rename(. -> 1839 columns={var_names[x]: symbols[x] for x in range(len(var_names))},. 1840 inplace=True,. 1841 ). ~/anaconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/_anndata.py in <dictcomp>(.0). 1837 # translate the column names to the symbol names. 1838 obs_tidy.rename(. -> 1839 columns={var_names[x]: symbols[x] for x in range(len(var_names))},. 1840 inplace=True,. 1841 ). NameError: free variable 'symbols' referenced before assignment in enclosing scope. ```. #### Versions. <details>. scanpy==1.6.0 anndata==0.7.4 umap==0.3.10 numpy==1.18.4 scipy==1.4.1 pandas==1.0.3 scikit-learn==0.21.3 statsmodels==0.10.1 python-igraph==0.8.2 louvain==0.6.1 leidenalg==0.8.0. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1636
https://github.com/scverse/scanpy/pull/1638:4,deployability,fail,failing,4,Fix failing CI for scanorama and python3.6; Suggested by: https://github.com/brianhie/scanorama/issues/73#issuecomment-775647669. See referenced issue for more details. This should be back ported to the 1.7.x branch.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1638
https://github.com/scverse/scanpy/pull/1638:4,reliability,fail,failing,4,Fix failing CI for scanorama and python3.6; Suggested by: https://github.com/brianhie/scanorama/issues/73#issuecomment-775647669. See referenced issue for more details. This should be back ported to the 1.7.x branch.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1638
https://github.com/scverse/scanpy/pull/1640:14,deployability,api,api,14,Remove scanpy.api; Fixes #1627,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1640
https://github.com/scverse/scanpy/pull/1640:14,integrability,api,api,14,Remove scanpy.api; Fixes #1627,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1640
https://github.com/scverse/scanpy/pull/1640:14,interoperability,api,api,14,Remove scanpy.api; Fixes #1627,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1640
https://github.com/scverse/scanpy/pull/1641:39,deployability,fail,failing,39,Backport PR #1638 on branch 1.7.x (Fix failing CI for scanorama and python3.6); Backport PR #1638: Fix failing CI for scanorama and python3.6,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1641
https://github.com/scverse/scanpy/pull/1641:103,deployability,fail,failing,103,Backport PR #1638 on branch 1.7.x (Fix failing CI for scanorama and python3.6); Backport PR #1638: Fix failing CI for scanorama and python3.6,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1641
https://github.com/scverse/scanpy/pull/1641:39,reliability,fail,failing,39,Backport PR #1638 on branch 1.7.x (Fix failing CI for scanorama and python3.6); Backport PR #1638: Fix failing CI for scanorama and python3.6,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1641
https://github.com/scverse/scanpy/pull/1641:103,reliability,fail,failing,103,Backport PR #1638 on branch 1.7.x (Fix failing CI for scanorama and python3.6); Backport PR #1638: Fix failing CI for scanorama and python3.6,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1641
https://github.com/scverse/scanpy/pull/1642:606,availability,ping,pinging,606,"enable highly_variable_genes_seurat_v3 to work with pseudocounts; Hi,. I'm working with pseudocounts data (kallisto/alevin/salomon output). I think they are called ""pseudocount"": if a read is assigned to two regions (genes) , a probability is assigned (e.g. gene1=0.2, gene2=0.8). Nevertheless, they can still considered counts and so it would be cool to use the `highly_variable_genes flavour=seuratv3` . . I added an additional argument in case users would like to enforce this, as it was similarly done/discussed in https://github.com/theislab/scvelo/issues/190. Would like to hear what you guys think, pinging @adamgayoso (thanks for the great overleaf doc! ).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1642
https://github.com/scverse/scanpy/pull/1642:347,energy efficiency,cool,cool,347,"enable highly_variable_genes_seurat_v3 to work with pseudocounts; Hi,. I'm working with pseudocounts data (kallisto/alevin/salomon output). I think they are called ""pseudocount"": if a read is assigned to two regions (genes) , a probability is assigned (e.g. gene1=0.2, gene2=0.8). Nevertheless, they can still considered counts and so it would be cool to use the `highly_variable_genes flavour=seuratv3` . . I added an additional argument in case users would like to enforce this, as it was similarly done/discussed in https://github.com/theislab/scvelo/issues/190. Would like to hear what you guys think, pinging @adamgayoso (thanks for the great overleaf doc! ).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1642
https://github.com/scverse/scanpy/pull/1642:447,usability,user,users,447,"enable highly_variable_genes_seurat_v3 to work with pseudocounts; Hi,. I'm working with pseudocounts data (kallisto/alevin/salomon output). I think they are called ""pseudocount"": if a read is assigned to two regions (genes) , a probability is assigned (e.g. gene1=0.2, gene2=0.8). Nevertheless, they can still considered counts and so it would be cool to use the `highly_variable_genes flavour=seuratv3` . . I added an additional argument in case users would like to enforce this, as it was similarly done/discussed in https://github.com/theislab/scvelo/issues/190. Would like to hear what you guys think, pinging @adamgayoso (thanks for the great overleaf doc! ).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1642
https://github.com/scverse/scanpy/issues/1643:890,deployability,build,building,890,"Any interest in getting SCTransform up on Scanpy?; <!-- What kind of feature would you like to request? -->. - [ ] Additional function parameters / changed functionality / changed defaults? - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`? - [ ] New plotting function: A kind of plot you would like to seein `sc.pl`? - [ ] External tools: Do you know an existing package that should go into `sc.external.*`? - [x] Other? <!-- Please describe your wishes below: -->. I recently ported [SCTransform](https://github.com/ChristophH/sctransform) from R into python. Any interest in getting it onto Scanpy? The original paper is [here](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-019-1874-1). It's a variance-stabilizing transformation that overcomes some key drawbacks of previous, similar methods (e.g. overfitting caused by building regression models from individual genes as opposed to groups of similar genes). It also eliminates the need for pseudocounts, log transformations, or library size normalization. . My code is [here](https://github.com/atarashansky/SCTransformPy). Implementation notes (from the SCTransformPy README):. - Poisson regression is done using the `statsmodels` package and parallelized with `multiprocessing`. . - Improved Sheather & Jones bandwidth calculation is implemented by the `KDEpy` package. - Estimating the negative binomial dispersion factor, `theta`, using MLE was translated from the `theta.ml` function in R. - Pearson residuals are automatically clipped to be non-negative. This ensures that sparsity structure can be preserved in the data. Practically, the results do not change much when allowing for dense, negative values. Anecdotally, it produces very similar results to the R implementation, though the code itself is still a little rough around the edges. I also have to do more formal quantitative benchmarking to ensure results are similar to those of the original package. I thought",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643
https://github.com/scverse/scanpy/issues/1643:1025,deployability,log,log,1025,"ransform up on Scanpy?; <!-- What kind of feature would you like to request? -->. - [ ] Additional function parameters / changed functionality / changed defaults? - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`? - [ ] New plotting function: A kind of plot you would like to seein `sc.pl`? - [ ] External tools: Do you know an existing package that should go into `sc.external.*`? - [x] Other? <!-- Please describe your wishes below: -->. I recently ported [SCTransform](https://github.com/ChristophH/sctransform) from R into python. Any interest in getting it onto Scanpy? The original paper is [here](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-019-1874-1). It's a variance-stabilizing transformation that overcomes some key drawbacks of previous, similar methods (e.g. overfitting caused by building regression models from individual genes as opposed to groups of similar genes). It also eliminates the need for pseudocounts, log transformations, or library size normalization. . My code is [here](https://github.com/atarashansky/SCTransformPy). Implementation notes (from the SCTransformPy README):. - Poisson regression is done using the `statsmodels` package and parallelized with `multiprocessing`. . - Improved Sheather & Jones bandwidth calculation is implemented by the `KDEpy` package. - Estimating the negative binomial dispersion factor, `theta`, using MLE was translated from the `theta.ml` function in R. - Pearson residuals are automatically clipped to be non-negative. This ensures that sparsity structure can be preserved in the data. Practically, the results do not change much when allowing for dense, negative values. Anecdotally, it produces very similar results to the R implementation, though the code itself is still a little rough around the edges. I also have to do more formal quantitative benchmarking to ensure results are similar to those of the original package. I thought I'd gauge interest here pr",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643
https://github.com/scverse/scanpy/issues/1643:1540,deployability,automat,automatically,1540,"eature would you like to request? -->. - [ ] Additional function parameters / changed functionality / changed defaults? - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`? - [ ] New plotting function: A kind of plot you would like to seein `sc.pl`? - [ ] External tools: Do you know an existing package that should go into `sc.external.*`? - [x] Other? <!-- Please describe your wishes below: -->. I recently ported [SCTransform](https://github.com/ChristophH/sctransform) from R into python. Any interest in getting it onto Scanpy? The original paper is [here](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-019-1874-1). It's a variance-stabilizing transformation that overcomes some key drawbacks of previous, similar methods (e.g. overfitting caused by building regression models from individual genes as opposed to groups of similar genes). It also eliminates the need for pseudocounts, log transformations, or library size normalization. . My code is [here](https://github.com/atarashansky/SCTransformPy). Implementation notes (from the SCTransformPy README):. - Poisson regression is done using the `statsmodels` package and parallelized with `multiprocessing`. . - Improved Sheather & Jones bandwidth calculation is implemented by the `KDEpy` package. - Estimating the negative binomial dispersion factor, `theta`, using MLE was translated from the `theta.ml` function in R. - Pearson residuals are automatically clipped to be non-negative. This ensures that sparsity structure can be preserved in the data. Practically, the results do not change much when allowing for dense, negative values. Anecdotally, it produces very similar results to the R implementation, though the code itself is still a little rough around the edges. I also have to do more formal quantitative benchmarking to ensure results are similar to those of the original package. I thought I'd gauge interest here prior to working on making it `scanpy`-ready.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643
https://github.com/scverse/scanpy/issues/1643:823,energy efficiency,draw,drawbacks,823,"Any interest in getting SCTransform up on Scanpy?; <!-- What kind of feature would you like to request? -->. - [ ] Additional function parameters / changed functionality / changed defaults? - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`? - [ ] New plotting function: A kind of plot you would like to seein `sc.pl`? - [ ] External tools: Do you know an existing package that should go into `sc.external.*`? - [x] Other? <!-- Please describe your wishes below: -->. I recently ported [SCTransform](https://github.com/ChristophH/sctransform) from R into python. Any interest in getting it onto Scanpy? The original paper is [here](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-019-1874-1). It's a variance-stabilizing transformation that overcomes some key drawbacks of previous, similar methods (e.g. overfitting caused by building regression models from individual genes as opposed to groups of similar genes). It also eliminates the need for pseudocounts, log transformations, or library size normalization. . My code is [here](https://github.com/atarashansky/SCTransformPy). Implementation notes (from the SCTransformPy README):. - Poisson regression is done using the `statsmodels` package and parallelized with `multiprocessing`. . - Improved Sheather & Jones bandwidth calculation is implemented by the `KDEpy` package. - Estimating the negative binomial dispersion factor, `theta`, using MLE was translated from the `theta.ml` function in R. - Pearson residuals are automatically clipped to be non-negative. This ensures that sparsity structure can be preserved in the data. Practically, the results do not change much when allowing for dense, negative values. Anecdotally, it produces very similar results to the R implementation, though the code itself is still a little rough around the edges. I also have to do more formal quantitative benchmarking to ensure results are similar to those of the original package. I thought",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643
https://github.com/scverse/scanpy/issues/1643:910,energy efficiency,model,models,910,"Any interest in getting SCTransform up on Scanpy?; <!-- What kind of feature would you like to request? -->. - [ ] Additional function parameters / changed functionality / changed defaults? - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`? - [ ] New plotting function: A kind of plot you would like to seein `sc.pl`? - [ ] External tools: Do you know an existing package that should go into `sc.external.*`? - [x] Other? <!-- Please describe your wishes below: -->. I recently ported [SCTransform](https://github.com/ChristophH/sctransform) from R into python. Any interest in getting it onto Scanpy? The original paper is [here](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-019-1874-1). It's a variance-stabilizing transformation that overcomes some key drawbacks of previous, similar methods (e.g. overfitting caused by building regression models from individual genes as opposed to groups of similar genes). It also eliminates the need for pseudocounts, log transformations, or library size normalization. . My code is [here](https://github.com/atarashansky/SCTransformPy). Implementation notes (from the SCTransformPy README):. - Poisson regression is done using the `statsmodels` package and parallelized with `multiprocessing`. . - Improved Sheather & Jones bandwidth calculation is implemented by the `KDEpy` package. - Estimating the negative binomial dispersion factor, `theta`, using MLE was translated from the `theta.ml` function in R. - Pearson residuals are automatically clipped to be non-negative. This ensures that sparsity structure can be preserved in the data. Practically, the results do not change much when allowing for dense, negative values. Anecdotally, it produces very similar results to the R implementation, though the code itself is still a little rough around the edges. I also have to do more formal quantitative benchmarking to ensure results are similar to those of the original package. I thought",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643
https://github.com/scverse/scanpy/issues/1643:1395,energy efficiency,Estimat,Estimating,1395,"eature would you like to request? -->. - [ ] Additional function parameters / changed functionality / changed defaults? - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`? - [ ] New plotting function: A kind of plot you would like to seein `sc.pl`? - [ ] External tools: Do you know an existing package that should go into `sc.external.*`? - [x] Other? <!-- Please describe your wishes below: -->. I recently ported [SCTransform](https://github.com/ChristophH/sctransform) from R into python. Any interest in getting it onto Scanpy? The original paper is [here](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-019-1874-1). It's a variance-stabilizing transformation that overcomes some key drawbacks of previous, similar methods (e.g. overfitting caused by building regression models from individual genes as opposed to groups of similar genes). It also eliminates the need for pseudocounts, log transformations, or library size normalization. . My code is [here](https://github.com/atarashansky/SCTransformPy). Implementation notes (from the SCTransformPy README):. - Poisson regression is done using the `statsmodels` package and parallelized with `multiprocessing`. . - Improved Sheather & Jones bandwidth calculation is implemented by the `KDEpy` package. - Estimating the negative binomial dispersion factor, `theta`, using MLE was translated from the `theta.ml` function in R. - Pearson residuals are automatically clipped to be non-negative. This ensures that sparsity structure can be preserved in the data. Practically, the results do not change much when allowing for dense, negative values. Anecdotally, it produces very similar results to the R implementation, though the code itself is still a little rough around the edges. I also have to do more formal quantitative benchmarking to ensure results are similar to those of the original package. I thought I'd gauge interest here prior to working on making it `scanpy`-ready.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643
https://github.com/scverse/scanpy/issues/1643:784,integrability,transform,transformation,784,"Any interest in getting SCTransform up on Scanpy?; <!-- What kind of feature would you like to request? -->. - [ ] Additional function parameters / changed functionality / changed defaults? - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`? - [ ] New plotting function: A kind of plot you would like to seein `sc.pl`? - [ ] External tools: Do you know an existing package that should go into `sc.external.*`? - [x] Other? <!-- Please describe your wishes below: -->. I recently ported [SCTransform](https://github.com/ChristophH/sctransform) from R into python. Any interest in getting it onto Scanpy? The original paper is [here](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-019-1874-1). It's a variance-stabilizing transformation that overcomes some key drawbacks of previous, similar methods (e.g. overfitting caused by building regression models from individual genes as opposed to groups of similar genes). It also eliminates the need for pseudocounts, log transformations, or library size normalization. . My code is [here](https://github.com/atarashansky/SCTransformPy). Implementation notes (from the SCTransformPy README):. - Poisson regression is done using the `statsmodels` package and parallelized with `multiprocessing`. . - Improved Sheather & Jones bandwidth calculation is implemented by the `KDEpy` package. - Estimating the negative binomial dispersion factor, `theta`, using MLE was translated from the `theta.ml` function in R. - Pearson residuals are automatically clipped to be non-negative. This ensures that sparsity structure can be preserved in the data. Practically, the results do not change much when allowing for dense, negative values. Anecdotally, it produces very similar results to the R implementation, though the code itself is still a little rough around the edges. I also have to do more formal quantitative benchmarking to ensure results are similar to those of the original package. I thought",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643
https://github.com/scverse/scanpy/issues/1643:1029,integrability,transform,transformations,1029,"p on Scanpy?; <!-- What kind of feature would you like to request? -->. - [ ] Additional function parameters / changed functionality / changed defaults? - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`? - [ ] New plotting function: A kind of plot you would like to seein `sc.pl`? - [ ] External tools: Do you know an existing package that should go into `sc.external.*`? - [x] Other? <!-- Please describe your wishes below: -->. I recently ported [SCTransform](https://github.com/ChristophH/sctransform) from R into python. Any interest in getting it onto Scanpy? The original paper is [here](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-019-1874-1). It's a variance-stabilizing transformation that overcomes some key drawbacks of previous, similar methods (e.g. overfitting caused by building regression models from individual genes as opposed to groups of similar genes). It also eliminates the need for pseudocounts, log transformations, or library size normalization. . My code is [here](https://github.com/atarashansky/SCTransformPy). Implementation notes (from the SCTransformPy README):. - Poisson regression is done using the `statsmodels` package and parallelized with `multiprocessing`. . - Improved Sheather & Jones bandwidth calculation is implemented by the `KDEpy` package. - Estimating the negative binomial dispersion factor, `theta`, using MLE was translated from the `theta.ml` function in R. - Pearson residuals are automatically clipped to be non-negative. This ensures that sparsity structure can be preserved in the data. Practically, the results do not change much when allowing for dense, negative values. Anecdotally, it produces very similar results to the R implementation, though the code itself is still a little rough around the edges. I also have to do more formal quantitative benchmarking to ensure results are similar to those of the original package. I thought I'd gauge interest here prior to wor",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643
https://github.com/scverse/scanpy/issues/1643:1470,integrability,translat,translated,1470,"eature would you like to request? -->. - [ ] Additional function parameters / changed functionality / changed defaults? - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`? - [ ] New plotting function: A kind of plot you would like to seein `sc.pl`? - [ ] External tools: Do you know an existing package that should go into `sc.external.*`? - [x] Other? <!-- Please describe your wishes below: -->. I recently ported [SCTransform](https://github.com/ChristophH/sctransform) from R into python. Any interest in getting it onto Scanpy? The original paper is [here](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-019-1874-1). It's a variance-stabilizing transformation that overcomes some key drawbacks of previous, similar methods (e.g. overfitting caused by building regression models from individual genes as opposed to groups of similar genes). It also eliminates the need for pseudocounts, log transformations, or library size normalization. . My code is [here](https://github.com/atarashansky/SCTransformPy). Implementation notes (from the SCTransformPy README):. - Poisson regression is done using the `statsmodels` package and parallelized with `multiprocessing`. . - Improved Sheather & Jones bandwidth calculation is implemented by the `KDEpy` package. - Estimating the negative binomial dispersion factor, `theta`, using MLE was translated from the `theta.ml` function in R. - Pearson residuals are automatically clipped to be non-negative. This ensures that sparsity structure can be preserved in the data. Practically, the results do not change much when allowing for dense, negative values. Anecdotally, it produces very similar results to the R implementation, though the code itself is still a little rough around the edges. I also have to do more formal quantitative benchmarking to ensure results are similar to those of the original package. I thought I'd gauge interest here prior to working on making it `scanpy`-ready.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643
https://github.com/scverse/scanpy/issues/1643:784,interoperability,transform,transformation,784,"Any interest in getting SCTransform up on Scanpy?; <!-- What kind of feature would you like to request? -->. - [ ] Additional function parameters / changed functionality / changed defaults? - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`? - [ ] New plotting function: A kind of plot you would like to seein `sc.pl`? - [ ] External tools: Do you know an existing package that should go into `sc.external.*`? - [x] Other? <!-- Please describe your wishes below: -->. I recently ported [SCTransform](https://github.com/ChristophH/sctransform) from R into python. Any interest in getting it onto Scanpy? The original paper is [here](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-019-1874-1). It's a variance-stabilizing transformation that overcomes some key drawbacks of previous, similar methods (e.g. overfitting caused by building regression models from individual genes as opposed to groups of similar genes). It also eliminates the need for pseudocounts, log transformations, or library size normalization. . My code is [here](https://github.com/atarashansky/SCTransformPy). Implementation notes (from the SCTransformPy README):. - Poisson regression is done using the `statsmodels` package and parallelized with `multiprocessing`. . - Improved Sheather & Jones bandwidth calculation is implemented by the `KDEpy` package. - Estimating the negative binomial dispersion factor, `theta`, using MLE was translated from the `theta.ml` function in R. - Pearson residuals are automatically clipped to be non-negative. This ensures that sparsity structure can be preserved in the data. Practically, the results do not change much when allowing for dense, negative values. Anecdotally, it produces very similar results to the R implementation, though the code itself is still a little rough around the edges. I also have to do more formal quantitative benchmarking to ensure results are similar to those of the original package. I thought",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643
https://github.com/scverse/scanpy/issues/1643:1029,interoperability,transform,transformations,1029,"p on Scanpy?; <!-- What kind of feature would you like to request? -->. - [ ] Additional function parameters / changed functionality / changed defaults? - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`? - [ ] New plotting function: A kind of plot you would like to seein `sc.pl`? - [ ] External tools: Do you know an existing package that should go into `sc.external.*`? - [x] Other? <!-- Please describe your wishes below: -->. I recently ported [SCTransform](https://github.com/ChristophH/sctransform) from R into python. Any interest in getting it onto Scanpy? The original paper is [here](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-019-1874-1). It's a variance-stabilizing transformation that overcomes some key drawbacks of previous, similar methods (e.g. overfitting caused by building regression models from individual genes as opposed to groups of similar genes). It also eliminates the need for pseudocounts, log transformations, or library size normalization. . My code is [here](https://github.com/atarashansky/SCTransformPy). Implementation notes (from the SCTransformPy README):. - Poisson regression is done using the `statsmodels` package and parallelized with `multiprocessing`. . - Improved Sheather & Jones bandwidth calculation is implemented by the `KDEpy` package. - Estimating the negative binomial dispersion factor, `theta`, using MLE was translated from the `theta.ml` function in R. - Pearson residuals are automatically clipped to be non-negative. This ensures that sparsity structure can be preserved in the data. Practically, the results do not change much when allowing for dense, negative values. Anecdotally, it produces very similar results to the R implementation, though the code itself is still a little rough around the edges. I also have to do more formal quantitative benchmarking to ensure results are similar to those of the original package. I thought I'd gauge interest here prior to wor",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643
https://github.com/scverse/scanpy/issues/1643:1470,interoperability,translat,translated,1470,"eature would you like to request? -->. - [ ] Additional function parameters / changed functionality / changed defaults? - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`? - [ ] New plotting function: A kind of plot you would like to seein `sc.pl`? - [ ] External tools: Do you know an existing package that should go into `sc.external.*`? - [x] Other? <!-- Please describe your wishes below: -->. I recently ported [SCTransform](https://github.com/ChristophH/sctransform) from R into python. Any interest in getting it onto Scanpy? The original paper is [here](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-019-1874-1). It's a variance-stabilizing transformation that overcomes some key drawbacks of previous, similar methods (e.g. overfitting caused by building regression models from individual genes as opposed to groups of similar genes). It also eliminates the need for pseudocounts, log transformations, or library size normalization. . My code is [here](https://github.com/atarashansky/SCTransformPy). Implementation notes (from the SCTransformPy README):. - Poisson regression is done using the `statsmodels` package and parallelized with `multiprocessing`. . - Improved Sheather & Jones bandwidth calculation is implemented by the `KDEpy` package. - Estimating the negative binomial dispersion factor, `theta`, using MLE was translated from the `theta.ml` function in R. - Pearson residuals are automatically clipped to be non-negative. This ensures that sparsity structure can be preserved in the data. Practically, the results do not change much when allowing for dense, negative values. Anecdotally, it produces very similar results to the R implementation, though the code itself is still a little rough around the edges. I also have to do more formal quantitative benchmarking to ensure results are similar to those of the original package. I thought I'd gauge interest here prior to working on making it `scanpy`-ready.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643
https://github.com/scverse/scanpy/issues/1643:135,modifiability,paramet,parameters,135,"Any interest in getting SCTransform up on Scanpy?; <!-- What kind of feature would you like to request? -->. - [ ] Additional function parameters / changed functionality / changed defaults? - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`? - [ ] New plotting function: A kind of plot you would like to seein `sc.pl`? - [ ] External tools: Do you know an existing package that should go into `sc.external.*`? - [x] Other? <!-- Please describe your wishes below: -->. I recently ported [SCTransform](https://github.com/ChristophH/sctransform) from R into python. Any interest in getting it onto Scanpy? The original paper is [here](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-019-1874-1). It's a variance-stabilizing transformation that overcomes some key drawbacks of previous, similar methods (e.g. overfitting caused by building regression models from individual genes as opposed to groups of similar genes). It also eliminates the need for pseudocounts, log transformations, or library size normalization. . My code is [here](https://github.com/atarashansky/SCTransformPy). Implementation notes (from the SCTransformPy README):. - Poisson regression is done using the `statsmodels` package and parallelized with `multiprocessing`. . - Improved Sheather & Jones bandwidth calculation is implemented by the `KDEpy` package. - Estimating the negative binomial dispersion factor, `theta`, using MLE was translated from the `theta.ml` function in R. - Pearson residuals are automatically clipped to be non-negative. This ensures that sparsity structure can be preserved in the data. Practically, the results do not change much when allowing for dense, negative values. Anecdotally, it produces very similar results to the R implementation, though the code itself is still a little rough around the edges. I also have to do more formal quantitative benchmarking to ensure results are similar to those of the original package. I thought",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643
https://github.com/scverse/scanpy/issues/1643:412,modifiability,pac,package,412,"Any interest in getting SCTransform up on Scanpy?; <!-- What kind of feature would you like to request? -->. - [ ] Additional function parameters / changed functionality / changed defaults? - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`? - [ ] New plotting function: A kind of plot you would like to seein `sc.pl`? - [ ] External tools: Do you know an existing package that should go into `sc.external.*`? - [x] Other? <!-- Please describe your wishes below: -->. I recently ported [SCTransform](https://github.com/ChristophH/sctransform) from R into python. Any interest in getting it onto Scanpy? The original paper is [here](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-019-1874-1). It's a variance-stabilizing transformation that overcomes some key drawbacks of previous, similar methods (e.g. overfitting caused by building regression models from individual genes as opposed to groups of similar genes). It also eliminates the need for pseudocounts, log transformations, or library size normalization. . My code is [here](https://github.com/atarashansky/SCTransformPy). Implementation notes (from the SCTransformPy README):. - Poisson regression is done using the `statsmodels` package and parallelized with `multiprocessing`. . - Improved Sheather & Jones bandwidth calculation is implemented by the `KDEpy` package. - Estimating the negative binomial dispersion factor, `theta`, using MLE was translated from the `theta.ml` function in R. - Pearson residuals are automatically clipped to be non-negative. This ensures that sparsity structure can be preserved in the data. Practically, the results do not change much when allowing for dense, negative values. Anecdotally, it produces very similar results to the R implementation, though the code itself is still a little rough around the edges. I also have to do more formal quantitative benchmarking to ensure results are similar to those of the original package. I thought",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643
https://github.com/scverse/scanpy/issues/1643:1253,modifiability,pac,package,1253,"eature would you like to request? -->. - [ ] Additional function parameters / changed functionality / changed defaults? - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`? - [ ] New plotting function: A kind of plot you would like to seein `sc.pl`? - [ ] External tools: Do you know an existing package that should go into `sc.external.*`? - [x] Other? <!-- Please describe your wishes below: -->. I recently ported [SCTransform](https://github.com/ChristophH/sctransform) from R into python. Any interest in getting it onto Scanpy? The original paper is [here](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-019-1874-1). It's a variance-stabilizing transformation that overcomes some key drawbacks of previous, similar methods (e.g. overfitting caused by building regression models from individual genes as opposed to groups of similar genes). It also eliminates the need for pseudocounts, log transformations, or library size normalization. . My code is [here](https://github.com/atarashansky/SCTransformPy). Implementation notes (from the SCTransformPy README):. - Poisson regression is done using the `statsmodels` package and parallelized with `multiprocessing`. . - Improved Sheather & Jones bandwidth calculation is implemented by the `KDEpy` package. - Estimating the negative binomial dispersion factor, `theta`, using MLE was translated from the `theta.ml` function in R. - Pearson residuals are automatically clipped to be non-negative. This ensures that sparsity structure can be preserved in the data. Practically, the results do not change much when allowing for dense, negative values. Anecdotally, it produces very similar results to the R implementation, though the code itself is still a little rough around the edges. I also have to do more formal quantitative benchmarking to ensure results are similar to those of the original package. I thought I'd gauge interest here prior to working on making it `scanpy`-ready.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643
https://github.com/scverse/scanpy/issues/1643:1384,modifiability,pac,package,1384,"eature would you like to request? -->. - [ ] Additional function parameters / changed functionality / changed defaults? - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`? - [ ] New plotting function: A kind of plot you would like to seein `sc.pl`? - [ ] External tools: Do you know an existing package that should go into `sc.external.*`? - [x] Other? <!-- Please describe your wishes below: -->. I recently ported [SCTransform](https://github.com/ChristophH/sctransform) from R into python. Any interest in getting it onto Scanpy? The original paper is [here](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-019-1874-1). It's a variance-stabilizing transformation that overcomes some key drawbacks of previous, similar methods (e.g. overfitting caused by building regression models from individual genes as opposed to groups of similar genes). It also eliminates the need for pseudocounts, log transformations, or library size normalization. . My code is [here](https://github.com/atarashansky/SCTransformPy). Implementation notes (from the SCTransformPy README):. - Poisson regression is done using the `statsmodels` package and parallelized with `multiprocessing`. . - Improved Sheather & Jones bandwidth calculation is implemented by the `KDEpy` package. - Estimating the negative binomial dispersion factor, `theta`, using MLE was translated from the `theta.ml` function in R. - Pearson residuals are automatically clipped to be non-negative. This ensures that sparsity structure can be preserved in the data. Practically, the results do not change much when allowing for dense, negative values. Anecdotally, it produces very similar results to the R implementation, though the code itself is still a little rough around the edges. I also have to do more formal quantitative benchmarking to ensure results are similar to those of the original package. I thought I'd gauge interest here prior to working on making it `scanpy`-ready.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643
https://github.com/scverse/scanpy/issues/1643:1982,modifiability,pac,package,1982,"eature would you like to request? -->. - [ ] Additional function parameters / changed functionality / changed defaults? - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`? - [ ] New plotting function: A kind of plot you would like to seein `sc.pl`? - [ ] External tools: Do you know an existing package that should go into `sc.external.*`? - [x] Other? <!-- Please describe your wishes below: -->. I recently ported [SCTransform](https://github.com/ChristophH/sctransform) from R into python. Any interest in getting it onto Scanpy? The original paper is [here](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-019-1874-1). It's a variance-stabilizing transformation that overcomes some key drawbacks of previous, similar methods (e.g. overfitting caused by building regression models from individual genes as opposed to groups of similar genes). It also eliminates the need for pseudocounts, log transformations, or library size normalization. . My code is [here](https://github.com/atarashansky/SCTransformPy). Implementation notes (from the SCTransformPy README):. - Poisson regression is done using the `statsmodels` package and parallelized with `multiprocessing`. . - Improved Sheather & Jones bandwidth calculation is implemented by the `KDEpy` package. - Estimating the negative binomial dispersion factor, `theta`, using MLE was translated from the `theta.ml` function in R. - Pearson residuals are automatically clipped to be non-negative. This ensures that sparsity structure can be preserved in the data. Practically, the results do not change much when allowing for dense, negative values. Anecdotally, it produces very similar results to the R implementation, though the code itself is still a little rough around the edges. I also have to do more formal quantitative benchmarking to ensure results are similar to those of the original package. I thought I'd gauge interest here prior to working on making it `scanpy`-ready.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643
https://github.com/scverse/scanpy/issues/1643:1265,performance,parallel,parallelized,1265,"eature would you like to request? -->. - [ ] Additional function parameters / changed functionality / changed defaults? - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`? - [ ] New plotting function: A kind of plot you would like to seein `sc.pl`? - [ ] External tools: Do you know an existing package that should go into `sc.external.*`? - [x] Other? <!-- Please describe your wishes below: -->. I recently ported [SCTransform](https://github.com/ChristophH/sctransform) from R into python. Any interest in getting it onto Scanpy? The original paper is [here](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-019-1874-1). It's a variance-stabilizing transformation that overcomes some key drawbacks of previous, similar methods (e.g. overfitting caused by building regression models from individual genes as opposed to groups of similar genes). It also eliminates the need for pseudocounts, log transformations, or library size normalization. . My code is [here](https://github.com/atarashansky/SCTransformPy). Implementation notes (from the SCTransformPy README):. - Poisson regression is done using the `statsmodels` package and parallelized with `multiprocessing`. . - Improved Sheather & Jones bandwidth calculation is implemented by the `KDEpy` package. - Estimating the negative binomial dispersion factor, `theta`, using MLE was translated from the `theta.ml` function in R. - Pearson residuals are automatically clipped to be non-negative. This ensures that sparsity structure can be preserved in the data. Practically, the results do not change much when allowing for dense, negative values. Anecdotally, it produces very similar results to the R implementation, though the code itself is still a little rough around the edges. I also have to do more formal quantitative benchmarking to ensure results are similar to those of the original package. I thought I'd gauge interest here prior to working on making it `scanpy`-ready.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643
https://github.com/scverse/scanpy/issues/1643:772,reliability,stabil,stabilizing,772,"Any interest in getting SCTransform up on Scanpy?; <!-- What kind of feature would you like to request? -->. - [ ] Additional function parameters / changed functionality / changed defaults? - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`? - [ ] New plotting function: A kind of plot you would like to seein `sc.pl`? - [ ] External tools: Do you know an existing package that should go into `sc.external.*`? - [x] Other? <!-- Please describe your wishes below: -->. I recently ported [SCTransform](https://github.com/ChristophH/sctransform) from R into python. Any interest in getting it onto Scanpy? The original paper is [here](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-019-1874-1). It's a variance-stabilizing transformation that overcomes some key drawbacks of previous, similar methods (e.g. overfitting caused by building regression models from individual genes as opposed to groups of similar genes). It also eliminates the need for pseudocounts, log transformations, or library size normalization. . My code is [here](https://github.com/atarashansky/SCTransformPy). Implementation notes (from the SCTransformPy README):. - Poisson regression is done using the `statsmodels` package and parallelized with `multiprocessing`. . - Improved Sheather & Jones bandwidth calculation is implemented by the `KDEpy` package. - Estimating the negative binomial dispersion factor, `theta`, using MLE was translated from the `theta.ml` function in R. - Pearson residuals are automatically clipped to be non-negative. This ensures that sparsity structure can be preserved in the data. Practically, the results do not change much when allowing for dense, negative values. Anecdotally, it produces very similar results to the R implementation, though the code itself is still a little rough around the edges. I also have to do more formal quantitative benchmarking to ensure results are similar to those of the original package. I thought",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643
https://github.com/scverse/scanpy/issues/1643:1202,reliability,Poisson,Poisson,1202,"eature would you like to request? -->. - [ ] Additional function parameters / changed functionality / changed defaults? - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`? - [ ] New plotting function: A kind of plot you would like to seein `sc.pl`? - [ ] External tools: Do you know an existing package that should go into `sc.external.*`? - [x] Other? <!-- Please describe your wishes below: -->. I recently ported [SCTransform](https://github.com/ChristophH/sctransform) from R into python. Any interest in getting it onto Scanpy? The original paper is [here](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-019-1874-1). It's a variance-stabilizing transformation that overcomes some key drawbacks of previous, similar methods (e.g. overfitting caused by building regression models from individual genes as opposed to groups of similar genes). It also eliminates the need for pseudocounts, log transformations, or library size normalization. . My code is [here](https://github.com/atarashansky/SCTransformPy). Implementation notes (from the SCTransformPy README):. - Poisson regression is done using the `statsmodels` package and parallelized with `multiprocessing`. . - Improved Sheather & Jones bandwidth calculation is implemented by the `KDEpy` package. - Estimating the negative binomial dispersion factor, `theta`, using MLE was translated from the `theta.ml` function in R. - Pearson residuals are automatically clipped to be non-negative. This ensures that sparsity structure can be preserved in the data. Practically, the results do not change much when allowing for dense, negative values. Anecdotally, it produces very similar results to the R implementation, though the code itself is still a little rough around the edges. I also have to do more formal quantitative benchmarking to ensure results are similar to those of the original package. I thought I'd gauge interest here prior to working on making it `scanpy`-ready.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643
https://github.com/scverse/scanpy/issues/1643:1649,reliability,Pra,Practically,1649,"eature would you like to request? -->. - [ ] Additional function parameters / changed functionality / changed defaults? - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`? - [ ] New plotting function: A kind of plot you would like to seein `sc.pl`? - [ ] External tools: Do you know an existing package that should go into `sc.external.*`? - [x] Other? <!-- Please describe your wishes below: -->. I recently ported [SCTransform](https://github.com/ChristophH/sctransform) from R into python. Any interest in getting it onto Scanpy? The original paper is [here](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-019-1874-1). It's a variance-stabilizing transformation that overcomes some key drawbacks of previous, similar methods (e.g. overfitting caused by building regression models from individual genes as opposed to groups of similar genes). It also eliminates the need for pseudocounts, log transformations, or library size normalization. . My code is [here](https://github.com/atarashansky/SCTransformPy). Implementation notes (from the SCTransformPy README):. - Poisson regression is done using the `statsmodels` package and parallelized with `multiprocessing`. . - Improved Sheather & Jones bandwidth calculation is implemented by the `KDEpy` package. - Estimating the negative binomial dispersion factor, `theta`, using MLE was translated from the `theta.ml` function in R. - Pearson residuals are automatically clipped to be non-negative. This ensures that sparsity structure can be preserved in the data. Practically, the results do not change much when allowing for dense, negative values. Anecdotally, it produces very similar results to the R implementation, though the code itself is still a little rough around the edges. I also have to do more formal quantitative benchmarking to ensure results are similar to those of the original package. I thought I'd gauge interest here prior to working on making it `scanpy`-ready.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643
https://github.com/scverse/scanpy/issues/1643:1025,safety,log,log,1025,"ransform up on Scanpy?; <!-- What kind of feature would you like to request? -->. - [ ] Additional function parameters / changed functionality / changed defaults? - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`? - [ ] New plotting function: A kind of plot you would like to seein `sc.pl`? - [ ] External tools: Do you know an existing package that should go into `sc.external.*`? - [x] Other? <!-- Please describe your wishes below: -->. I recently ported [SCTransform](https://github.com/ChristophH/sctransform) from R into python. Any interest in getting it onto Scanpy? The original paper is [here](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-019-1874-1). It's a variance-stabilizing transformation that overcomes some key drawbacks of previous, similar methods (e.g. overfitting caused by building regression models from individual genes as opposed to groups of similar genes). It also eliminates the need for pseudocounts, log transformations, or library size normalization. . My code is [here](https://github.com/atarashansky/SCTransformPy). Implementation notes (from the SCTransformPy README):. - Poisson regression is done using the `statsmodels` package and parallelized with `multiprocessing`. . - Improved Sheather & Jones bandwidth calculation is implemented by the `KDEpy` package. - Estimating the negative binomial dispersion factor, `theta`, using MLE was translated from the `theta.ml` function in R. - Pearson residuals are automatically clipped to be non-negative. This ensures that sparsity structure can be preserved in the data. Practically, the results do not change much when allowing for dense, negative values. Anecdotally, it produces very similar results to the R implementation, though the code itself is still a little rough around the edges. I also have to do more formal quantitative benchmarking to ensure results are similar to those of the original package. I thought I'd gauge interest here pr",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643
https://github.com/scverse/scanpy/issues/1643:910,security,model,models,910,"Any interest in getting SCTransform up on Scanpy?; <!-- What kind of feature would you like to request? -->. - [ ] Additional function parameters / changed functionality / changed defaults? - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`? - [ ] New plotting function: A kind of plot you would like to seein `sc.pl`? - [ ] External tools: Do you know an existing package that should go into `sc.external.*`? - [x] Other? <!-- Please describe your wishes below: -->. I recently ported [SCTransform](https://github.com/ChristophH/sctransform) from R into python. Any interest in getting it onto Scanpy? The original paper is [here](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-019-1874-1). It's a variance-stabilizing transformation that overcomes some key drawbacks of previous, similar methods (e.g. overfitting caused by building regression models from individual genes as opposed to groups of similar genes). It also eliminates the need for pseudocounts, log transformations, or library size normalization. . My code is [here](https://github.com/atarashansky/SCTransformPy). Implementation notes (from the SCTransformPy README):. - Poisson regression is done using the `statsmodels` package and parallelized with `multiprocessing`. . - Improved Sheather & Jones bandwidth calculation is implemented by the `KDEpy` package. - Estimating the negative binomial dispersion factor, `theta`, using MLE was translated from the `theta.ml` function in R. - Pearson residuals are automatically clipped to be non-negative. This ensures that sparsity structure can be preserved in the data. Practically, the results do not change much when allowing for dense, negative values. Anecdotally, it produces very similar results to the R implementation, though the code itself is still a little rough around the edges. I also have to do more formal quantitative benchmarking to ensure results are similar to those of the original package. I thought",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643
https://github.com/scverse/scanpy/issues/1643:1025,security,log,log,1025,"ransform up on Scanpy?; <!-- What kind of feature would you like to request? -->. - [ ] Additional function parameters / changed functionality / changed defaults? - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`? - [ ] New plotting function: A kind of plot you would like to seein `sc.pl`? - [ ] External tools: Do you know an existing package that should go into `sc.external.*`? - [x] Other? <!-- Please describe your wishes below: -->. I recently ported [SCTransform](https://github.com/ChristophH/sctransform) from R into python. Any interest in getting it onto Scanpy? The original paper is [here](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-019-1874-1). It's a variance-stabilizing transformation that overcomes some key drawbacks of previous, similar methods (e.g. overfitting caused by building regression models from individual genes as opposed to groups of similar genes). It also eliminates the need for pseudocounts, log transformations, or library size normalization. . My code is [here](https://github.com/atarashansky/SCTransformPy). Implementation notes (from the SCTransformPy README):. - Poisson regression is done using the `statsmodels` package and parallelized with `multiprocessing`. . - Improved Sheather & Jones bandwidth calculation is implemented by the `KDEpy` package. - Estimating the negative binomial dispersion factor, `theta`, using MLE was translated from the `theta.ml` function in R. - Pearson residuals are automatically clipped to be non-negative. This ensures that sparsity structure can be preserved in the data. Practically, the results do not change much when allowing for dense, negative values. Anecdotally, it produces very similar results to the R implementation, though the code itself is still a little rough around the edges. I also have to do more formal quantitative benchmarking to ensure results are similar to those of the original package. I thought I'd gauge interest here pr",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643
https://github.com/scverse/scanpy/issues/1643:217,testability,simpl,simple,217,"Any interest in getting SCTransform up on Scanpy?; <!-- What kind of feature would you like to request? -->. - [ ] Additional function parameters / changed functionality / changed defaults? - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`? - [ ] New plotting function: A kind of plot you would like to seein `sc.pl`? - [ ] External tools: Do you know an existing package that should go into `sc.external.*`? - [x] Other? <!-- Please describe your wishes below: -->. I recently ported [SCTransform](https://github.com/ChristophH/sctransform) from R into python. Any interest in getting it onto Scanpy? The original paper is [here](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-019-1874-1). It's a variance-stabilizing transformation that overcomes some key drawbacks of previous, similar methods (e.g. overfitting caused by building regression models from individual genes as opposed to groups of similar genes). It also eliminates the need for pseudocounts, log transformations, or library size normalization. . My code is [here](https://github.com/atarashansky/SCTransformPy). Implementation notes (from the SCTransformPy README):. - Poisson regression is done using the `statsmodels` package and parallelized with `multiprocessing`. . - Improved Sheather & Jones bandwidth calculation is implemented by the `KDEpy` package. - Estimating the negative binomial dispersion factor, `theta`, using MLE was translated from the `theta.ml` function in R. - Pearson residuals are automatically clipped to be non-negative. This ensures that sparsity structure can be preserved in the data. Practically, the results do not change much when allowing for dense, negative values. Anecdotally, it produces very similar results to the R implementation, though the code itself is still a little rough around the edges. I also have to do more formal quantitative benchmarking to ensure results are similar to those of the original package. I thought",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643
https://github.com/scverse/scanpy/issues/1643:899,testability,regress,regression,899,"Any interest in getting SCTransform up on Scanpy?; <!-- What kind of feature would you like to request? -->. - [ ] Additional function parameters / changed functionality / changed defaults? - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`? - [ ] New plotting function: A kind of plot you would like to seein `sc.pl`? - [ ] External tools: Do you know an existing package that should go into `sc.external.*`? - [x] Other? <!-- Please describe your wishes below: -->. I recently ported [SCTransform](https://github.com/ChristophH/sctransform) from R into python. Any interest in getting it onto Scanpy? The original paper is [here](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-019-1874-1). It's a variance-stabilizing transformation that overcomes some key drawbacks of previous, similar methods (e.g. overfitting caused by building regression models from individual genes as opposed to groups of similar genes). It also eliminates the need for pseudocounts, log transformations, or library size normalization. . My code is [here](https://github.com/atarashansky/SCTransformPy). Implementation notes (from the SCTransformPy README):. - Poisson regression is done using the `statsmodels` package and parallelized with `multiprocessing`. . - Improved Sheather & Jones bandwidth calculation is implemented by the `KDEpy` package. - Estimating the negative binomial dispersion factor, `theta`, using MLE was translated from the `theta.ml` function in R. - Pearson residuals are automatically clipped to be non-negative. This ensures that sparsity structure can be preserved in the data. Practically, the results do not change much when allowing for dense, negative values. Anecdotally, it produces very similar results to the R implementation, though the code itself is still a little rough around the edges. I also have to do more formal quantitative benchmarking to ensure results are similar to those of the original package. I thought",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643
https://github.com/scverse/scanpy/issues/1643:1025,testability,log,log,1025,"ransform up on Scanpy?; <!-- What kind of feature would you like to request? -->. - [ ] Additional function parameters / changed functionality / changed defaults? - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`? - [ ] New plotting function: A kind of plot you would like to seein `sc.pl`? - [ ] External tools: Do you know an existing package that should go into `sc.external.*`? - [x] Other? <!-- Please describe your wishes below: -->. I recently ported [SCTransform](https://github.com/ChristophH/sctransform) from R into python. Any interest in getting it onto Scanpy? The original paper is [here](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-019-1874-1). It's a variance-stabilizing transformation that overcomes some key drawbacks of previous, similar methods (e.g. overfitting caused by building regression models from individual genes as opposed to groups of similar genes). It also eliminates the need for pseudocounts, log transformations, or library size normalization. . My code is [here](https://github.com/atarashansky/SCTransformPy). Implementation notes (from the SCTransformPy README):. - Poisson regression is done using the `statsmodels` package and parallelized with `multiprocessing`. . - Improved Sheather & Jones bandwidth calculation is implemented by the `KDEpy` package. - Estimating the negative binomial dispersion factor, `theta`, using MLE was translated from the `theta.ml` function in R. - Pearson residuals are automatically clipped to be non-negative. This ensures that sparsity structure can be preserved in the data. Practically, the results do not change much when allowing for dense, negative values. Anecdotally, it produces very similar results to the R implementation, though the code itself is still a little rough around the edges. I also have to do more formal quantitative benchmarking to ensure results are similar to those of the original package. I thought I'd gauge interest here pr",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643
https://github.com/scverse/scanpy/issues/1643:1210,testability,regress,regression,1210,"eature would you like to request? -->. - [ ] Additional function parameters / changed functionality / changed defaults? - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`? - [ ] New plotting function: A kind of plot you would like to seein `sc.pl`? - [ ] External tools: Do you know an existing package that should go into `sc.external.*`? - [x] Other? <!-- Please describe your wishes below: -->. I recently ported [SCTransform](https://github.com/ChristophH/sctransform) from R into python. Any interest in getting it onto Scanpy? The original paper is [here](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-019-1874-1). It's a variance-stabilizing transformation that overcomes some key drawbacks of previous, similar methods (e.g. overfitting caused by building regression models from individual genes as opposed to groups of similar genes). It also eliminates the need for pseudocounts, log transformations, or library size normalization. . My code is [here](https://github.com/atarashansky/SCTransformPy). Implementation notes (from the SCTransformPy README):. - Poisson regression is done using the `statsmodels` package and parallelized with `multiprocessing`. . - Improved Sheather & Jones bandwidth calculation is implemented by the `KDEpy` package. - Estimating the negative binomial dispersion factor, `theta`, using MLE was translated from the `theta.ml` function in R. - Pearson residuals are automatically clipped to be non-negative. This ensures that sparsity structure can be preserved in the data. Practically, the results do not change much when allowing for dense, negative values. Anecdotally, it produces very similar results to the R implementation, though the code itself is still a little rough around the edges. I also have to do more formal quantitative benchmarking to ensure results are similar to those of the original package. I thought I'd gauge interest here prior to working on making it `scanpy`-ready.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643
https://github.com/scverse/scanpy/issues/1643:1540,testability,automat,automatically,1540,"eature would you like to request? -->. - [ ] Additional function parameters / changed functionality / changed defaults? - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`? - [ ] New plotting function: A kind of plot you would like to seein `sc.pl`? - [ ] External tools: Do you know an existing package that should go into `sc.external.*`? - [x] Other? <!-- Please describe your wishes below: -->. I recently ported [SCTransform](https://github.com/ChristophH/sctransform) from R into python. Any interest in getting it onto Scanpy? The original paper is [here](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-019-1874-1). It's a variance-stabilizing transformation that overcomes some key drawbacks of previous, similar methods (e.g. overfitting caused by building regression models from individual genes as opposed to groups of similar genes). It also eliminates the need for pseudocounts, log transformations, or library size normalization. . My code is [here](https://github.com/atarashansky/SCTransformPy). Implementation notes (from the SCTransformPy README):. - Poisson regression is done using the `statsmodels` package and parallelized with `multiprocessing`. . - Improved Sheather & Jones bandwidth calculation is implemented by the `KDEpy` package. - Estimating the negative binomial dispersion factor, `theta`, using MLE was translated from the `theta.ml` function in R. - Pearson residuals are automatically clipped to be non-negative. This ensures that sparsity structure can be preserved in the data. Practically, the results do not change much when allowing for dense, negative values. Anecdotally, it produces very similar results to the R implementation, though the code itself is still a little rough around the edges. I also have to do more formal quantitative benchmarking to ensure results are similar to those of the original package. I thought I'd gauge interest here prior to working on making it `scanpy`-ready.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643
https://github.com/scverse/scanpy/issues/1643:209,usability,tool,tool,209,"Any interest in getting SCTransform up on Scanpy?; <!-- What kind of feature would you like to request? -->. - [ ] Additional function parameters / changed functionality / changed defaults? - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`? - [ ] New plotting function: A kind of plot you would like to seein `sc.pl`? - [ ] External tools: Do you know an existing package that should go into `sc.external.*`? - [x] Other? <!-- Please describe your wishes below: -->. I recently ported [SCTransform](https://github.com/ChristophH/sctransform) from R into python. Any interest in getting it onto Scanpy? The original paper is [here](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-019-1874-1). It's a variance-stabilizing transformation that overcomes some key drawbacks of previous, similar methods (e.g. overfitting caused by building regression models from individual genes as opposed to groups of similar genes). It also eliminates the need for pseudocounts, log transformations, or library size normalization. . My code is [here](https://github.com/atarashansky/SCTransformPy). Implementation notes (from the SCTransformPy README):. - Poisson regression is done using the `statsmodels` package and parallelized with `multiprocessing`. . - Improved Sheather & Jones bandwidth calculation is implemented by the `KDEpy` package. - Estimating the negative binomial dispersion factor, `theta`, using MLE was translated from the `theta.ml` function in R. - Pearson residuals are automatically clipped to be non-negative. This ensures that sparsity structure can be preserved in the data. Practically, the results do not change much when allowing for dense, negative values. Anecdotally, it produces very similar results to the R implementation, though the code itself is still a little rough around the edges. I also have to do more formal quantitative benchmarking to ensure results are similar to those of the original package. I thought",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643
https://github.com/scverse/scanpy/issues/1643:217,usability,simpl,simple,217,"Any interest in getting SCTransform up on Scanpy?; <!-- What kind of feature would you like to request? -->. - [ ] Additional function parameters / changed functionality / changed defaults? - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`? - [ ] New plotting function: A kind of plot you would like to seein `sc.pl`? - [ ] External tools: Do you know an existing package that should go into `sc.external.*`? - [x] Other? <!-- Please describe your wishes below: -->. I recently ported [SCTransform](https://github.com/ChristophH/sctransform) from R into python. Any interest in getting it onto Scanpy? The original paper is [here](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-019-1874-1). It's a variance-stabilizing transformation that overcomes some key drawbacks of previous, similar methods (e.g. overfitting caused by building regression models from individual genes as opposed to groups of similar genes). It also eliminates the need for pseudocounts, log transformations, or library size normalization. . My code is [here](https://github.com/atarashansky/SCTransformPy). Implementation notes (from the SCTransformPy README):. - Poisson regression is done using the `statsmodels` package and parallelized with `multiprocessing`. . - Improved Sheather & Jones bandwidth calculation is implemented by the `KDEpy` package. - Estimating the negative binomial dispersion factor, `theta`, using MLE was translated from the `theta.ml` function in R. - Pearson residuals are automatically clipped to be non-negative. This ensures that sparsity structure can be preserved in the data. Practically, the results do not change much when allowing for dense, negative values. Anecdotally, it produces very similar results to the R implementation, though the code itself is still a little rough around the edges. I also have to do more formal quantitative benchmarking to ensure results are similar to those of the original package. I thought",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643
https://github.com/scverse/scanpy/issues/1643:233,usability,tool,tool,233,"Any interest in getting SCTransform up on Scanpy?; <!-- What kind of feature would you like to request? -->. - [ ] Additional function parameters / changed functionality / changed defaults? - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`? - [ ] New plotting function: A kind of plot you would like to seein `sc.pl`? - [ ] External tools: Do you know an existing package that should go into `sc.external.*`? - [x] Other? <!-- Please describe your wishes below: -->. I recently ported [SCTransform](https://github.com/ChristophH/sctransform) from R into python. Any interest in getting it onto Scanpy? The original paper is [here](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-019-1874-1). It's a variance-stabilizing transformation that overcomes some key drawbacks of previous, similar methods (e.g. overfitting caused by building regression models from individual genes as opposed to groups of similar genes). It also eliminates the need for pseudocounts, log transformations, or library size normalization. . My code is [here](https://github.com/atarashansky/SCTransformPy). Implementation notes (from the SCTransformPy README):. - Poisson regression is done using the `statsmodels` package and parallelized with `multiprocessing`. . - Improved Sheather & Jones bandwidth calculation is implemented by the `KDEpy` package. - Estimating the negative binomial dispersion factor, `theta`, using MLE was translated from the `theta.ml` function in R. - Pearson residuals are automatically clipped to be non-negative. This ensures that sparsity structure can be preserved in the data. Practically, the results do not change much when allowing for dense, negative values. Anecdotally, it produces very similar results to the R implementation, though the code itself is still a little rough around the edges. I also have to do more formal quantitative benchmarking to ensure results are similar to those of the original package. I thought",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643
https://github.com/scverse/scanpy/issues/1643:281,usability,tool,tools,281,"Any interest in getting SCTransform up on Scanpy?; <!-- What kind of feature would you like to request? -->. - [ ] Additional function parameters / changed functionality / changed defaults? - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`? - [ ] New plotting function: A kind of plot you would like to seein `sc.pl`? - [ ] External tools: Do you know an existing package that should go into `sc.external.*`? - [x] Other? <!-- Please describe your wishes below: -->. I recently ported [SCTransform](https://github.com/ChristophH/sctransform) from R into python. Any interest in getting it onto Scanpy? The original paper is [here](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-019-1874-1). It's a variance-stabilizing transformation that overcomes some key drawbacks of previous, similar methods (e.g. overfitting caused by building regression models from individual genes as opposed to groups of similar genes). It also eliminates the need for pseudocounts, log transformations, or library size normalization. . My code is [here](https://github.com/atarashansky/SCTransformPy). Implementation notes (from the SCTransformPy README):. - Poisson regression is done using the `statsmodels` package and parallelized with `multiprocessing`. . - Improved Sheather & Jones bandwidth calculation is implemented by the `KDEpy` package. - Estimating the negative binomial dispersion factor, `theta`, using MLE was translated from the `theta.ml` function in R. - Pearson residuals are automatically clipped to be non-negative. This ensures that sparsity structure can be preserved in the data. Practically, the results do not change much when allowing for dense, negative values. Anecdotally, it produces very similar results to the R implementation, though the code itself is still a little rough around the edges. I also have to do more formal quantitative benchmarking to ensure results are similar to those of the original package. I thought",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643
https://github.com/scverse/scanpy/issues/1643:381,usability,tool,tools,381,"Any interest in getting SCTransform up on Scanpy?; <!-- What kind of feature would you like to request? -->. - [ ] Additional function parameters / changed functionality / changed defaults? - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`? - [ ] New plotting function: A kind of plot you would like to seein `sc.pl`? - [ ] External tools: Do you know an existing package that should go into `sc.external.*`? - [x] Other? <!-- Please describe your wishes below: -->. I recently ported [SCTransform](https://github.com/ChristophH/sctransform) from R into python. Any interest in getting it onto Scanpy? The original paper is [here](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-019-1874-1). It's a variance-stabilizing transformation that overcomes some key drawbacks of previous, similar methods (e.g. overfitting caused by building regression models from individual genes as opposed to groups of similar genes). It also eliminates the need for pseudocounts, log transformations, or library size normalization. . My code is [here](https://github.com/atarashansky/SCTransformPy). Implementation notes (from the SCTransformPy README):. - Poisson regression is done using the `statsmodels` package and parallelized with `multiprocessing`. . - Improved Sheather & Jones bandwidth calculation is implemented by the `KDEpy` package. - Estimating the negative binomial dispersion factor, `theta`, using MLE was translated from the `theta.ml` function in R. - Pearson residuals are automatically clipped to be non-negative. This ensures that sparsity structure can be preserved in the data. Practically, the results do not change much when allowing for dense, negative values. Anecdotally, it produces very similar results to the R implementation, though the code itself is still a little rough around the edges. I also have to do more formal quantitative benchmarking to ensure results are similar to those of the original package. I thought",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643
https://github.com/scverse/scanpy/issues/1644:149,deployability,version,version,149,"scrublet ignores threshold; - [x] I have checked that this issue has not already been reported. - [x] I have confirmed this bug exists on the latest version of scanpy. - [] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. ### Minimal code sample (that we can copy&paste without having any data). ```python. import scanpy as sc. adata = sc.datasets.pbmc3k(). sc.external.pp.scrublet(adata, threshold='I am ignored'). ```. ```pytb. /opt/conda/lib/python3.7/site-packages/scanpy/preprocessing/_normalization.py:138: UserWarning: Revieved a view of an AnnData. Making a copy. view_to_actual(adata). Automatically set threshold at doublet score = 0.27. Detected doublet rate = 1.5%. Estimated detectable doublet fraction = 44.3%. Overall doublet rate:. 	Expected = 5.0%. 	Estimated = 3.4%. ```. #### Versions. <details>. scanpy==1.7.0 anndata==0.7.5 umap==0.5.1 numpy==1.20.0 scipy==1.6.0 pandas==1.2.1 scikit-learn==0.24.1 statsmodels==0.12.2 python-igraph==0.8.3 louvain==0.7.0 leidenalg==0.8.3. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1644
https://github.com/scverse/scanpy/issues/1644:627,deployability,Automat,Automatically,627,"scrublet ignores threshold; - [x] I have checked that this issue has not already been reported. - [x] I have confirmed this bug exists on the latest version of scanpy. - [] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. ### Minimal code sample (that we can copy&paste without having any data). ```python. import scanpy as sc. adata = sc.datasets.pbmc3k(). sc.external.pp.scrublet(adata, threshold='I am ignored'). ```. ```pytb. /opt/conda/lib/python3.7/site-packages/scanpy/preprocessing/_normalization.py:138: UserWarning: Revieved a view of an AnnData. Making a copy. view_to_actual(adata). Automatically set threshold at doublet score = 0.27. Detected doublet rate = 1.5%. Estimated detectable doublet fraction = 44.3%. Overall doublet rate:. 	Expected = 5.0%. 	Estimated = 3.4%. ```. #### Versions. <details>. scanpy==1.7.0 anndata==0.7.5 umap==0.5.1 numpy==1.20.0 scipy==1.6.0 pandas==1.2.1 scikit-learn==0.24.1 statsmodels==0.12.2 python-igraph==0.8.3 louvain==0.7.0 leidenalg==0.8.3. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1644
https://github.com/scverse/scanpy/issues/1644:827,deployability,Version,Versions,827,"scrublet ignores threshold; - [x] I have checked that this issue has not already been reported. - [x] I have confirmed this bug exists on the latest version of scanpy. - [] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. ### Minimal code sample (that we can copy&paste without having any data). ```python. import scanpy as sc. adata = sc.datasets.pbmc3k(). sc.external.pp.scrublet(adata, threshold='I am ignored'). ```. ```pytb. /opt/conda/lib/python3.7/site-packages/scanpy/preprocessing/_normalization.py:138: UserWarning: Revieved a view of an AnnData. Making a copy. view_to_actual(adata). Automatically set threshold at doublet score = 0.27. Detected doublet rate = 1.5%. Estimated detectable doublet fraction = 44.3%. Overall doublet rate:. 	Expected = 5.0%. 	Estimated = 3.4%. ```. #### Versions. <details>. scanpy==1.7.0 anndata==0.7.5 umap==0.5.1 numpy==1.20.0 scipy==1.6.0 pandas==1.2.1 scikit-learn==0.24.1 statsmodels==0.12.2 python-igraph==0.8.3 louvain==0.7.0 leidenalg==0.8.3. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1644
https://github.com/scverse/scanpy/issues/1644:710,energy efficiency,Estimat,Estimated,710,"scrublet ignores threshold; - [x] I have checked that this issue has not already been reported. - [x] I have confirmed this bug exists on the latest version of scanpy. - [] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. ### Minimal code sample (that we can copy&paste without having any data). ```python. import scanpy as sc. adata = sc.datasets.pbmc3k(). sc.external.pp.scrublet(adata, threshold='I am ignored'). ```. ```pytb. /opt/conda/lib/python3.7/site-packages/scanpy/preprocessing/_normalization.py:138: UserWarning: Revieved a view of an AnnData. Making a copy. view_to_actual(adata). Automatically set threshold at doublet score = 0.27. Detected doublet rate = 1.5%. Estimated detectable doublet fraction = 44.3%. Overall doublet rate:. 	Expected = 5.0%. 	Estimated = 3.4%. ```. #### Versions. <details>. scanpy==1.7.0 anndata==0.7.5 umap==0.5.1 numpy==1.20.0 scipy==1.6.0 pandas==1.2.1 scikit-learn==0.24.1 statsmodels==0.12.2 python-igraph==0.8.3 louvain==0.7.0 leidenalg==0.8.3. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1644
https://github.com/scverse/scanpy/issues/1644:799,energy efficiency,Estimat,Estimated,799,"scrublet ignores threshold; - [x] I have checked that this issue has not already been reported. - [x] I have confirmed this bug exists on the latest version of scanpy. - [] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. ### Minimal code sample (that we can copy&paste without having any data). ```python. import scanpy as sc. adata = sc.datasets.pbmc3k(). sc.external.pp.scrublet(adata, threshold='I am ignored'). ```. ```pytb. /opt/conda/lib/python3.7/site-packages/scanpy/preprocessing/_normalization.py:138: UserWarning: Revieved a view of an AnnData. Making a copy. view_to_actual(adata). Automatically set threshold at doublet score = 0.27. Detected doublet rate = 1.5%. Estimated detectable doublet fraction = 44.3%. Overall doublet rate:. 	Expected = 5.0%. 	Estimated = 3.4%. ```. #### Versions. <details>. scanpy==1.7.0 anndata==0.7.5 umap==0.5.1 numpy==1.20.0 scipy==1.6.0 pandas==1.2.1 scikit-learn==0.24.1 statsmodels==0.12.2 python-igraph==0.8.3 louvain==0.7.0 leidenalg==0.8.3. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1644
https://github.com/scverse/scanpy/issues/1644:149,integrability,version,version,149,"scrublet ignores threshold; - [x] I have checked that this issue has not already been reported. - [x] I have confirmed this bug exists on the latest version of scanpy. - [] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. ### Minimal code sample (that we can copy&paste without having any data). ```python. import scanpy as sc. adata = sc.datasets.pbmc3k(). sc.external.pp.scrublet(adata, threshold='I am ignored'). ```. ```pytb. /opt/conda/lib/python3.7/site-packages/scanpy/preprocessing/_normalization.py:138: UserWarning: Revieved a view of an AnnData. Making a copy. view_to_actual(adata). Automatically set threshold at doublet score = 0.27. Detected doublet rate = 1.5%. Estimated detectable doublet fraction = 44.3%. Overall doublet rate:. 	Expected = 5.0%. 	Estimated = 3.4%. ```. #### Versions. <details>. scanpy==1.7.0 anndata==0.7.5 umap==0.5.1 numpy==1.20.0 scipy==1.6.0 pandas==1.2.1 scikit-learn==0.24.1 statsmodels==0.12.2 python-igraph==0.8.3 louvain==0.7.0 leidenalg==0.8.3. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1644
https://github.com/scverse/scanpy/issues/1644:827,integrability,Version,Versions,827,"scrublet ignores threshold; - [x] I have checked that this issue has not already been reported. - [x] I have confirmed this bug exists on the latest version of scanpy. - [] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. ### Minimal code sample (that we can copy&paste without having any data). ```python. import scanpy as sc. adata = sc.datasets.pbmc3k(). sc.external.pp.scrublet(adata, threshold='I am ignored'). ```. ```pytb. /opt/conda/lib/python3.7/site-packages/scanpy/preprocessing/_normalization.py:138: UserWarning: Revieved a view of an AnnData. Making a copy. view_to_actual(adata). Automatically set threshold at doublet score = 0.27. Detected doublet rate = 1.5%. Estimated detectable doublet fraction = 44.3%. Overall doublet rate:. 	Expected = 5.0%. 	Estimated = 3.4%. ```. #### Versions. <details>. scanpy==1.7.0 anndata==0.7.5 umap==0.5.1 numpy==1.20.0 scipy==1.6.0 pandas==1.2.1 scikit-learn==0.24.1 statsmodels==0.12.2 python-igraph==0.8.3 louvain==0.7.0 leidenalg==0.8.3. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1644
https://github.com/scverse/scanpy/issues/1644:149,modifiability,version,version,149,"scrublet ignores threshold; - [x] I have checked that this issue has not already been reported. - [x] I have confirmed this bug exists on the latest version of scanpy. - [] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. ### Minimal code sample (that we can copy&paste without having any data). ```python. import scanpy as sc. adata = sc.datasets.pbmc3k(). sc.external.pp.scrublet(adata, threshold='I am ignored'). ```. ```pytb. /opt/conda/lib/python3.7/site-packages/scanpy/preprocessing/_normalization.py:138: UserWarning: Revieved a view of an AnnData. Making a copy. view_to_actual(adata). Automatically set threshold at doublet score = 0.27. Detected doublet rate = 1.5%. Estimated detectable doublet fraction = 44.3%. Overall doublet rate:. 	Expected = 5.0%. 	Estimated = 3.4%. ```. #### Versions. <details>. scanpy==1.7.0 anndata==0.7.5 umap==0.5.1 numpy==1.20.0 scipy==1.6.0 pandas==1.2.1 scikit-learn==0.24.1 statsmodels==0.12.2 python-igraph==0.8.3 louvain==0.7.0 leidenalg==0.8.3. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1644
https://github.com/scverse/scanpy/issues/1644:492,modifiability,pac,packages,492,"scrublet ignores threshold; - [x] I have checked that this issue has not already been reported. - [x] I have confirmed this bug exists on the latest version of scanpy. - [] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. ### Minimal code sample (that we can copy&paste without having any data). ```python. import scanpy as sc. adata = sc.datasets.pbmc3k(). sc.external.pp.scrublet(adata, threshold='I am ignored'). ```. ```pytb. /opt/conda/lib/python3.7/site-packages/scanpy/preprocessing/_normalization.py:138: UserWarning: Revieved a view of an AnnData. Making a copy. view_to_actual(adata). Automatically set threshold at doublet score = 0.27. Detected doublet rate = 1.5%. Estimated detectable doublet fraction = 44.3%. Overall doublet rate:. 	Expected = 5.0%. 	Estimated = 3.4%. ```. #### Versions. <details>. scanpy==1.7.0 anndata==0.7.5 umap==0.5.1 numpy==1.20.0 scipy==1.6.0 pandas==1.2.1 scikit-learn==0.24.1 statsmodels==0.12.2 python-igraph==0.8.3 louvain==0.7.0 leidenalg==0.8.3. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1644
https://github.com/scverse/scanpy/issues/1644:827,modifiability,Version,Versions,827,"scrublet ignores threshold; - [x] I have checked that this issue has not already been reported. - [x] I have confirmed this bug exists on the latest version of scanpy. - [] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. ### Minimal code sample (that we can copy&paste without having any data). ```python. import scanpy as sc. adata = sc.datasets.pbmc3k(). sc.external.pp.scrublet(adata, threshold='I am ignored'). ```. ```pytb. /opt/conda/lib/python3.7/site-packages/scanpy/preprocessing/_normalization.py:138: UserWarning: Revieved a view of an AnnData. Making a copy. view_to_actual(adata). Automatically set threshold at doublet score = 0.27. Detected doublet rate = 1.5%. Estimated detectable doublet fraction = 44.3%. Overall doublet rate:. 	Expected = 5.0%. 	Estimated = 3.4%. ```. #### Versions. <details>. scanpy==1.7.0 anndata==0.7.5 umap==0.5.1 numpy==1.20.0 scipy==1.6.0 pandas==1.2.1 scikit-learn==0.24.1 statsmodels==0.12.2 python-igraph==0.8.3 louvain==0.7.0 leidenalg==0.8.3. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1644
https://github.com/scverse/scanpy/issues/1644:680,safety,Detect,Detected,680,"scrublet ignores threshold; - [x] I have checked that this issue has not already been reported. - [x] I have confirmed this bug exists on the latest version of scanpy. - [] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. ### Minimal code sample (that we can copy&paste without having any data). ```python. import scanpy as sc. adata = sc.datasets.pbmc3k(). sc.external.pp.scrublet(adata, threshold='I am ignored'). ```. ```pytb. /opt/conda/lib/python3.7/site-packages/scanpy/preprocessing/_normalization.py:138: UserWarning: Revieved a view of an AnnData. Making a copy. view_to_actual(adata). Automatically set threshold at doublet score = 0.27. Detected doublet rate = 1.5%. Estimated detectable doublet fraction = 44.3%. Overall doublet rate:. 	Expected = 5.0%. 	Estimated = 3.4%. ```. #### Versions. <details>. scanpy==1.7.0 anndata==0.7.5 umap==0.5.1 numpy==1.20.0 scipy==1.6.0 pandas==1.2.1 scikit-learn==0.24.1 statsmodels==0.12.2 python-igraph==0.8.3 louvain==0.7.0 leidenalg==0.8.3. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1644
https://github.com/scverse/scanpy/issues/1644:720,safety,detect,detectable,720,"scrublet ignores threshold; - [x] I have checked that this issue has not already been reported. - [x] I have confirmed this bug exists on the latest version of scanpy. - [] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. ### Minimal code sample (that we can copy&paste without having any data). ```python. import scanpy as sc. adata = sc.datasets.pbmc3k(). sc.external.pp.scrublet(adata, threshold='I am ignored'). ```. ```pytb. /opt/conda/lib/python3.7/site-packages/scanpy/preprocessing/_normalization.py:138: UserWarning: Revieved a view of an AnnData. Making a copy. view_to_actual(adata). Automatically set threshold at doublet score = 0.27. Detected doublet rate = 1.5%. Estimated detectable doublet fraction = 44.3%. Overall doublet rate:. 	Expected = 5.0%. 	Estimated = 3.4%. ```. #### Versions. <details>. scanpy==1.7.0 anndata==0.7.5 umap==0.5.1 numpy==1.20.0 scipy==1.6.0 pandas==1.2.1 scikit-learn==0.24.1 statsmodels==0.12.2 python-igraph==0.8.3 louvain==0.7.0 leidenalg==0.8.3. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1644
https://github.com/scverse/scanpy/issues/1644:680,security,Detect,Detected,680,"scrublet ignores threshold; - [x] I have checked that this issue has not already been reported. - [x] I have confirmed this bug exists on the latest version of scanpy. - [] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. ### Minimal code sample (that we can copy&paste without having any data). ```python. import scanpy as sc. adata = sc.datasets.pbmc3k(). sc.external.pp.scrublet(adata, threshold='I am ignored'). ```. ```pytb. /opt/conda/lib/python3.7/site-packages/scanpy/preprocessing/_normalization.py:138: UserWarning: Revieved a view of an AnnData. Making a copy. view_to_actual(adata). Automatically set threshold at doublet score = 0.27. Detected doublet rate = 1.5%. Estimated detectable doublet fraction = 44.3%. Overall doublet rate:. 	Expected = 5.0%. 	Estimated = 3.4%. ```. #### Versions. <details>. scanpy==1.7.0 anndata==0.7.5 umap==0.5.1 numpy==1.20.0 scipy==1.6.0 pandas==1.2.1 scikit-learn==0.24.1 statsmodels==0.12.2 python-igraph==0.8.3 louvain==0.7.0 leidenalg==0.8.3. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1644
https://github.com/scverse/scanpy/issues/1644:720,security,detect,detectable,720,"scrublet ignores threshold; - [x] I have checked that this issue has not already been reported. - [x] I have confirmed this bug exists on the latest version of scanpy. - [] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. ### Minimal code sample (that we can copy&paste without having any data). ```python. import scanpy as sc. adata = sc.datasets.pbmc3k(). sc.external.pp.scrublet(adata, threshold='I am ignored'). ```. ```pytb. /opt/conda/lib/python3.7/site-packages/scanpy/preprocessing/_normalization.py:138: UserWarning: Revieved a view of an AnnData. Making a copy. view_to_actual(adata). Automatically set threshold at doublet score = 0.27. Detected doublet rate = 1.5%. Estimated detectable doublet fraction = 44.3%. Overall doublet rate:. 	Expected = 5.0%. 	Estimated = 3.4%. ```. #### Versions. <details>. scanpy==1.7.0 anndata==0.7.5 umap==0.5.1 numpy==1.20.0 scipy==1.6.0 pandas==1.2.1 scikit-learn==0.24.1 statsmodels==0.12.2 python-igraph==0.8.3 louvain==0.7.0 leidenalg==0.8.3. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1644
https://github.com/scverse/scanpy/issues/1644:627,testability,Automat,Automatically,627,"scrublet ignores threshold; - [x] I have checked that this issue has not already been reported. - [x] I have confirmed this bug exists on the latest version of scanpy. - [] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. ### Minimal code sample (that we can copy&paste without having any data). ```python. import scanpy as sc. adata = sc.datasets.pbmc3k(). sc.external.pp.scrublet(adata, threshold='I am ignored'). ```. ```pytb. /opt/conda/lib/python3.7/site-packages/scanpy/preprocessing/_normalization.py:138: UserWarning: Revieved a view of an AnnData. Making a copy. view_to_actual(adata). Automatically set threshold at doublet score = 0.27. Detected doublet rate = 1.5%. Estimated detectable doublet fraction = 44.3%. Overall doublet rate:. 	Expected = 5.0%. 	Estimated = 3.4%. ```. #### Versions. <details>. scanpy==1.7.0 anndata==0.7.5 umap==0.5.1 numpy==1.20.0 scipy==1.6.0 pandas==1.2.1 scikit-learn==0.24.1 statsmodels==0.12.2 python-igraph==0.8.3 louvain==0.7.0 leidenalg==0.8.3. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1644
https://github.com/scverse/scanpy/issues/1644:109,usability,confirm,confirmed,109,"scrublet ignores threshold; - [x] I have checked that this issue has not already been reported. - [x] I have confirmed this bug exists on the latest version of scanpy. - [] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. ### Minimal code sample (that we can copy&paste without having any data). ```python. import scanpy as sc. adata = sc.datasets.pbmc3k(). sc.external.pp.scrublet(adata, threshold='I am ignored'). ```. ```pytb. /opt/conda/lib/python3.7/site-packages/scanpy/preprocessing/_normalization.py:138: UserWarning: Revieved a view of an AnnData. Making a copy. view_to_actual(adata). Automatically set threshold at doublet score = 0.27. Detected doublet rate = 1.5%. Estimated detectable doublet fraction = 44.3%. Overall doublet rate:. 	Expected = 5.0%. 	Estimated = 3.4%. ```. #### Versions. <details>. scanpy==1.7.0 anndata==0.7.5 umap==0.5.1 numpy==1.20.0 scipy==1.6.0 pandas==1.2.1 scikit-learn==0.24.1 statsmodels==0.12.2 python-igraph==0.8.3 louvain==0.7.0 leidenalg==0.8.3. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1644
https://github.com/scverse/scanpy/issues/1644:191,usability,confirm,confirmed,191,"scrublet ignores threshold; - [x] I have checked that this issue has not already been reported. - [x] I have confirmed this bug exists on the latest version of scanpy. - [] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. ### Minimal code sample (that we can copy&paste without having any data). ```python. import scanpy as sc. adata = sc.datasets.pbmc3k(). sc.external.pp.scrublet(adata, threshold='I am ignored'). ```. ```pytb. /opt/conda/lib/python3.7/site-packages/scanpy/preprocessing/_normalization.py:138: UserWarning: Revieved a view of an AnnData. Making a copy. view_to_actual(adata). Automatically set threshold at doublet score = 0.27. Detected doublet rate = 1.5%. Estimated detectable doublet fraction = 44.3%. Overall doublet rate:. 	Expected = 5.0%. 	Estimated = 3.4%. ```. #### Versions. <details>. scanpy==1.7.0 anndata==0.7.5 umap==0.5.1 numpy==1.20.0 scipy==1.6.0 pandas==1.2.1 scikit-learn==0.24.1 statsmodels==0.12.2 python-igraph==0.8.3 louvain==0.7.0 leidenalg==0.8.3. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1644
https://github.com/scverse/scanpy/issues/1644:258,usability,Minim,Minimal,258,"scrublet ignores threshold; - [x] I have checked that this issue has not already been reported. - [x] I have confirmed this bug exists on the latest version of scanpy. - [] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. ### Minimal code sample (that we can copy&paste without having any data). ```python. import scanpy as sc. adata = sc.datasets.pbmc3k(). sc.external.pp.scrublet(adata, threshold='I am ignored'). ```. ```pytb. /opt/conda/lib/python3.7/site-packages/scanpy/preprocessing/_normalization.py:138: UserWarning: Revieved a view of an AnnData. Making a copy. view_to_actual(adata). Automatically set threshold at doublet score = 0.27. Detected doublet rate = 1.5%. Estimated detectable doublet fraction = 44.3%. Overall doublet rate:. 	Expected = 5.0%. 	Estimated = 3.4%. ```. #### Versions. <details>. scanpy==1.7.0 anndata==0.7.5 umap==0.5.1 numpy==1.20.0 scipy==1.6.0 pandas==1.2.1 scikit-learn==0.24.1 statsmodels==0.12.2 python-igraph==0.8.3 louvain==0.7.0 leidenalg==0.8.3. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1644
https://github.com/scverse/scanpy/issues/1644:545,usability,User,UserWarning,545,"scrublet ignores threshold; - [x] I have checked that this issue has not already been reported. - [x] I have confirmed this bug exists on the latest version of scanpy. - [] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. ### Minimal code sample (that we can copy&paste without having any data). ```python. import scanpy as sc. adata = sc.datasets.pbmc3k(). sc.external.pp.scrublet(adata, threshold='I am ignored'). ```. ```pytb. /opt/conda/lib/python3.7/site-packages/scanpy/preprocessing/_normalization.py:138: UserWarning: Revieved a view of an AnnData. Making a copy. view_to_actual(adata). Automatically set threshold at doublet score = 0.27. Detected doublet rate = 1.5%. Estimated detectable doublet fraction = 44.3%. Overall doublet rate:. 	Expected = 5.0%. 	Estimated = 3.4%. ```. #### Versions. <details>. scanpy==1.7.0 anndata==0.7.5 umap==0.5.1 numpy==1.20.0 scipy==1.6.0 pandas==1.2.1 scikit-learn==0.24.1 statsmodels==0.12.2 python-igraph==0.8.3 louvain==0.7.0 leidenalg==0.8.3. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1644
https://github.com/scverse/scanpy/issues/1644:937,usability,learn,learn,937,"scrublet ignores threshold; - [x] I have checked that this issue has not already been reported. - [x] I have confirmed this bug exists on the latest version of scanpy. - [] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. ### Minimal code sample (that we can copy&paste without having any data). ```python. import scanpy as sc. adata = sc.datasets.pbmc3k(). sc.external.pp.scrublet(adata, threshold='I am ignored'). ```. ```pytb. /opt/conda/lib/python3.7/site-packages/scanpy/preprocessing/_normalization.py:138: UserWarning: Revieved a view of an AnnData. Making a copy. view_to_actual(adata). Automatically set threshold at doublet score = 0.27. Detected doublet rate = 1.5%. Estimated detectable doublet fraction = 44.3%. Overall doublet rate:. 	Expected = 5.0%. 	Estimated = 3.4%. ```. #### Versions. <details>. scanpy==1.7.0 anndata==0.7.5 umap==0.5.1 numpy==1.20.0 scipy==1.6.0 pandas==1.2.1 scikit-learn==0.24.1 statsmodels==0.12.2 python-igraph==0.8.3 louvain==0.7.0 leidenalg==0.8.3. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1644
https://github.com/scverse/scanpy/issues/1645:154,deployability,version,version,154,"scrublet crashes on dense input; - [x] I have checked that this issue has not already been reported. - [x] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. ### Minimal code sample (that we can copy&paste without having any data). ```python. import scanpy as sc. adata = sc.datasets.paul15(). sc.external.pp.scrublet(adata, threshold=0.1). ```. ```pytb. WARNING: In Scanpy 0.*, this returned logarithmized data. Now it returns non-logarithmized data. ... storing 'paul15_clusters' as categorical. /opt/conda/lib/python3.7/site-packages/pandas/core/arrays/categorical.py:2487: FutureWarning: The `inplace` parameter in pandas.Categorical.remove_unused_categories is deprecated and will be removed in a future version. res = method(*args, **kwargs). Trying to set attribute `.uns` of view, copying. /opt/conda/lib/python3.7/site-packages/scanpy/preprocessing/_normalization.py:138: UserWarning: Revieved a view of an AnnData. Making a copy. view_to_actual(adata). ---------------------------------------------------------------------------. AttributeError Traceback (most recent call last). <ipython-input-21-3dabe52b6132> in <module>. 1 import scanpy as sc. 2 adata = sc.datasets.paul15(). ----> 3 sc.external.pp.scrublet(adata, threshold=0.1). /opt/conda/lib/python3.7/site-packages/scanpy/external/pp/_scrublet.py in scrublet(adata, adata_sim, sim_doublet_ratio, expected_doublet_rate, stdev_doublet_rate, synthetic_doublet_umi_subsampling, knn_dist_metric, normalize_variance, log_transform, mean_center, n_prin_comps, use_approx_neighbors, get_doublet_neighbor_parents, n_neighbors, threshold, verbose, copy, random_state). 208 expected_doublet_rate=expected_doublet_rate,. 209 stdev_doublet_rate=stdev_doublet_rate,. --> 210 random_state=random_state,. 211 ). 212 . /opt/conda/lib/python3.7/site-packages/scanpy/external/pp/_scrublet.py in _scrublet_call_doublets(adata_obs, adata_sim, n_neigh",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1645
https://github.com/scverse/scanpy/issues/1645:495,deployability,log,logarithmized,495,"scrublet crashes on dense input; - [x] I have checked that this issue has not already been reported. - [x] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. ### Minimal code sample (that we can copy&paste without having any data). ```python. import scanpy as sc. adata = sc.datasets.paul15(). sc.external.pp.scrublet(adata, threshold=0.1). ```. ```pytb. WARNING: In Scanpy 0.*, this returned logarithmized data. Now it returns non-logarithmized data. ... storing 'paul15_clusters' as categorical. /opt/conda/lib/python3.7/site-packages/pandas/core/arrays/categorical.py:2487: FutureWarning: The `inplace` parameter in pandas.Categorical.remove_unused_categories is deprecated and will be removed in a future version. res = method(*args, **kwargs). Trying to set attribute `.uns` of view, copying. /opt/conda/lib/python3.7/site-packages/scanpy/preprocessing/_normalization.py:138: UserWarning: Revieved a view of an AnnData. Making a copy. view_to_actual(adata). ---------------------------------------------------------------------------. AttributeError Traceback (most recent call last). <ipython-input-21-3dabe52b6132> in <module>. 1 import scanpy as sc. 2 adata = sc.datasets.paul15(). ----> 3 sc.external.pp.scrublet(adata, threshold=0.1). /opt/conda/lib/python3.7/site-packages/scanpy/external/pp/_scrublet.py in scrublet(adata, adata_sim, sim_doublet_ratio, expected_doublet_rate, stdev_doublet_rate, synthetic_doublet_umi_subsampling, knn_dist_metric, normalize_variance, log_transform, mean_center, n_prin_comps, use_approx_neighbors, get_doublet_neighbor_parents, n_neighbors, threshold, verbose, copy, random_state). 208 expected_doublet_rate=expected_doublet_rate,. 209 stdev_doublet_rate=stdev_doublet_rate,. --> 210 random_state=random_state,. 211 ). 212 . /opt/conda/lib/python3.7/site-packages/scanpy/external/pp/_scrublet.py in _scrublet_call_doublets(adata_obs, adata_sim, n_neigh",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1645
https://github.com/scverse/scanpy/issues/1645:534,deployability,log,logarithmized,534,"scrublet crashes on dense input; - [x] I have checked that this issue has not already been reported. - [x] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. ### Minimal code sample (that we can copy&paste without having any data). ```python. import scanpy as sc. adata = sc.datasets.paul15(). sc.external.pp.scrublet(adata, threshold=0.1). ```. ```pytb. WARNING: In Scanpy 0.*, this returned logarithmized data. Now it returns non-logarithmized data. ... storing 'paul15_clusters' as categorical. /opt/conda/lib/python3.7/site-packages/pandas/core/arrays/categorical.py:2487: FutureWarning: The `inplace` parameter in pandas.Categorical.remove_unused_categories is deprecated and will be removed in a future version. res = method(*args, **kwargs). Trying to set attribute `.uns` of view, copying. /opt/conda/lib/python3.7/site-packages/scanpy/preprocessing/_normalization.py:138: UserWarning: Revieved a view of an AnnData. Making a copy. view_to_actual(adata). ---------------------------------------------------------------------------. AttributeError Traceback (most recent call last). <ipython-input-21-3dabe52b6132> in <module>. 1 import scanpy as sc. 2 adata = sc.datasets.paul15(). ----> 3 sc.external.pp.scrublet(adata, threshold=0.1). /opt/conda/lib/python3.7/site-packages/scanpy/external/pp/_scrublet.py in scrublet(adata, adata_sim, sim_doublet_ratio, expected_doublet_rate, stdev_doublet_rate, synthetic_doublet_umi_subsampling, knn_dist_metric, normalize_variance, log_transform, mean_center, n_prin_comps, use_approx_neighbors, get_doublet_neighbor_parents, n_neighbors, threshold, verbose, copy, random_state). 208 expected_doublet_rate=expected_doublet_rate,. 209 stdev_doublet_rate=stdev_doublet_rate,. --> 210 random_state=random_state,. 211 ). 212 . /opt/conda/lib/python3.7/site-packages/scanpy/external/pp/_scrublet.py in _scrublet_call_doublets(adata_obs, adata_sim, n_neigh",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1645
https://github.com/scverse/scanpy/issues/1645:811,deployability,version,version,811,"scrublet crashes on dense input; - [x] I have checked that this issue has not already been reported. - [x] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. ### Minimal code sample (that we can copy&paste without having any data). ```python. import scanpy as sc. adata = sc.datasets.paul15(). sc.external.pp.scrublet(adata, threshold=0.1). ```. ```pytb. WARNING: In Scanpy 0.*, this returned logarithmized data. Now it returns non-logarithmized data. ... storing 'paul15_clusters' as categorical. /opt/conda/lib/python3.7/site-packages/pandas/core/arrays/categorical.py:2487: FutureWarning: The `inplace` parameter in pandas.Categorical.remove_unused_categories is deprecated and will be removed in a future version. res = method(*args, **kwargs). Trying to set attribute `.uns` of view, copying. /opt/conda/lib/python3.7/site-packages/scanpy/preprocessing/_normalization.py:138: UserWarning: Revieved a view of an AnnData. Making a copy. view_to_actual(adata). ---------------------------------------------------------------------------. AttributeError Traceback (most recent call last). <ipython-input-21-3dabe52b6132> in <module>. 1 import scanpy as sc. 2 adata = sc.datasets.paul15(). ----> 3 sc.external.pp.scrublet(adata, threshold=0.1). /opt/conda/lib/python3.7/site-packages/scanpy/external/pp/_scrublet.py in scrublet(adata, adata_sim, sim_doublet_ratio, expected_doublet_rate, stdev_doublet_rate, synthetic_doublet_umi_subsampling, knn_dist_metric, normalize_variance, log_transform, mean_center, n_prin_comps, use_approx_neighbors, get_doublet_neighbor_parents, n_neighbors, threshold, verbose, copy, random_state). 208 expected_doublet_rate=expected_doublet_rate,. 209 stdev_doublet_rate=stdev_doublet_rate,. --> 210 random_state=random_state,. 211 ). 212 . /opt/conda/lib/python3.7/site-packages/scanpy/external/pp/_scrublet.py in _scrublet_call_doublets(adata_obs, adata_sim, n_neigh",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1645
https://github.com/scverse/scanpy/issues/1645:1228,deployability,modul,module,1228,"aster branch of scanpy. ---. ### Minimal code sample (that we can copy&paste without having any data). ```python. import scanpy as sc. adata = sc.datasets.paul15(). sc.external.pp.scrublet(adata, threshold=0.1). ```. ```pytb. WARNING: In Scanpy 0.*, this returned logarithmized data. Now it returns non-logarithmized data. ... storing 'paul15_clusters' as categorical. /opt/conda/lib/python3.7/site-packages/pandas/core/arrays/categorical.py:2487: FutureWarning: The `inplace` parameter in pandas.Categorical.remove_unused_categories is deprecated and will be removed in a future version. res = method(*args, **kwargs). Trying to set attribute `.uns` of view, copying. /opt/conda/lib/python3.7/site-packages/scanpy/preprocessing/_normalization.py:138: UserWarning: Revieved a view of an AnnData. Making a copy. view_to_actual(adata). ---------------------------------------------------------------------------. AttributeError Traceback (most recent call last). <ipython-input-21-3dabe52b6132> in <module>. 1 import scanpy as sc. 2 adata = sc.datasets.paul15(). ----> 3 sc.external.pp.scrublet(adata, threshold=0.1). /opt/conda/lib/python3.7/site-packages/scanpy/external/pp/_scrublet.py in scrublet(adata, adata_sim, sim_doublet_ratio, expected_doublet_rate, stdev_doublet_rate, synthetic_doublet_umi_subsampling, knn_dist_metric, normalize_variance, log_transform, mean_center, n_prin_comps, use_approx_neighbors, get_doublet_neighbor_parents, n_neighbors, threshold, verbose, copy, random_state). 208 expected_doublet_rate=expected_doublet_rate,. 209 stdev_doublet_rate=stdev_doublet_rate,. --> 210 random_state=random_state,. 211 ). 212 . /opt/conda/lib/python3.7/site-packages/scanpy/external/pp/_scrublet.py in _scrublet_call_doublets(adata_obs, adata_sim, n_neighbors, expected_doublet_rate, stdev_doublet_rate, mean_center, normalize_variance, n_prin_comps, use_approx_neighbors, knn_dist_metric, get_doublet_neighbor_parents, random_state, verbose). 349 . 350 if mean_center and normalize_var",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1645
https://github.com/scverse/scanpy/issues/1645:2996,deployability,Version,Versions,2996,"t-21-3dabe52b6132> in <module>. 1 import scanpy as sc. 2 adata = sc.datasets.paul15(). ----> 3 sc.external.pp.scrublet(adata, threshold=0.1). /opt/conda/lib/python3.7/site-packages/scanpy/external/pp/_scrublet.py in scrublet(adata, adata_sim, sim_doublet_ratio, expected_doublet_rate, stdev_doublet_rate, synthetic_doublet_umi_subsampling, knn_dist_metric, normalize_variance, log_transform, mean_center, n_prin_comps, use_approx_neighbors, get_doublet_neighbor_parents, n_neighbors, threshold, verbose, copy, random_state). 208 expected_doublet_rate=expected_doublet_rate,. 209 stdev_doublet_rate=stdev_doublet_rate,. --> 210 random_state=random_state,. 211 ). 212 . /opt/conda/lib/python3.7/site-packages/scanpy/external/pp/_scrublet.py in _scrublet_call_doublets(adata_obs, adata_sim, n_neighbors, expected_doublet_rate, stdev_doublet_rate, mean_center, normalize_variance, n_prin_comps, use_approx_neighbors, knn_dist_metric, get_doublet_neighbor_parents, random_state, verbose). 349 . 350 if mean_center and normalize_variance:. --> 351 sl.pipeline_zscore(scrub). 352 elif mean_center:. 353 sl.pipeline_mean_center(scrub). /opt/conda/lib/python3.7/site-packages/scrublet/helper_functions.py in pipeline_zscore(self). 62 def pipeline_zscore(self):. 63 gene_means = self._E_obs_norm.mean(0). ---> 64 gene_stdevs = np.sqrt(sparse_var(self._E_obs_norm)). 65 self._E_obs_norm = np.array(sparse_zscore(self._E_obs_norm, gene_means, gene_stdevs)). 66 if self._E_sim_norm is not None:. /opt/conda/lib/python3.7/site-packages/scrublet/helper_functions.py in sparse_var(E, axis). 153 ''' variance across the specified axis '''. 154 . --> 155 mean_gene = E.mean(axis=axis).A.squeeze(). 156 tmp = E.copy(). 157 tmp.data **= 2. AttributeError: 'numpy.ndarray' object has no attribute 'A'. ```. #### Versions. <details>. scanpy==1.7.0 anndata==0.7.5 umap==0.5.1 numpy==1.20.0 scipy==1.6.0 pandas==1.2.1 scikit-learn==0.24.1 statsmodels==0.12.2 python-igraph==0.8.3 louvain==0.7.0 leidenalg==0.8.3. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1645
https://github.com/scverse/scanpy/issues/1645:646,energy efficiency,core,core,646,"scrublet crashes on dense input; - [x] I have checked that this issue has not already been reported. - [x] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. ### Minimal code sample (that we can copy&paste without having any data). ```python. import scanpy as sc. adata = sc.datasets.paul15(). sc.external.pp.scrublet(adata, threshold=0.1). ```. ```pytb. WARNING: In Scanpy 0.*, this returned logarithmized data. Now it returns non-logarithmized data. ... storing 'paul15_clusters' as categorical. /opt/conda/lib/python3.7/site-packages/pandas/core/arrays/categorical.py:2487: FutureWarning: The `inplace` parameter in pandas.Categorical.remove_unused_categories is deprecated and will be removed in a future version. res = method(*args, **kwargs). Trying to set attribute `.uns` of view, copying. /opt/conda/lib/python3.7/site-packages/scanpy/preprocessing/_normalization.py:138: UserWarning: Revieved a view of an AnnData. Making a copy. view_to_actual(adata). ---------------------------------------------------------------------------. AttributeError Traceback (most recent call last). <ipython-input-21-3dabe52b6132> in <module>. 1 import scanpy as sc. 2 adata = sc.datasets.paul15(). ----> 3 sc.external.pp.scrublet(adata, threshold=0.1). /opt/conda/lib/python3.7/site-packages/scanpy/external/pp/_scrublet.py in scrublet(adata, adata_sim, sim_doublet_ratio, expected_doublet_rate, stdev_doublet_rate, synthetic_doublet_umi_subsampling, knn_dist_metric, normalize_variance, log_transform, mean_center, n_prin_comps, use_approx_neighbors, get_doublet_neighbor_parents, n_neighbors, threshold, verbose, copy, random_state). 208 expected_doublet_rate=expected_doublet_rate,. 209 stdev_doublet_rate=stdev_doublet_rate,. --> 210 random_state=random_state,. 211 ). 212 . /opt/conda/lib/python3.7/site-packages/scanpy/external/pp/_scrublet.py in _scrublet_call_doublets(adata_obs, adata_sim, n_neigh",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1645
https://github.com/scverse/scanpy/issues/1645:154,integrability,version,version,154,"scrublet crashes on dense input; - [x] I have checked that this issue has not already been reported. - [x] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. ### Minimal code sample (that we can copy&paste without having any data). ```python. import scanpy as sc. adata = sc.datasets.paul15(). sc.external.pp.scrublet(adata, threshold=0.1). ```. ```pytb. WARNING: In Scanpy 0.*, this returned logarithmized data. Now it returns non-logarithmized data. ... storing 'paul15_clusters' as categorical. /opt/conda/lib/python3.7/site-packages/pandas/core/arrays/categorical.py:2487: FutureWarning: The `inplace` parameter in pandas.Categorical.remove_unused_categories is deprecated and will be removed in a future version. res = method(*args, **kwargs). Trying to set attribute `.uns` of view, copying. /opt/conda/lib/python3.7/site-packages/scanpy/preprocessing/_normalization.py:138: UserWarning: Revieved a view of an AnnData. Making a copy. view_to_actual(adata). ---------------------------------------------------------------------------. AttributeError Traceback (most recent call last). <ipython-input-21-3dabe52b6132> in <module>. 1 import scanpy as sc. 2 adata = sc.datasets.paul15(). ----> 3 sc.external.pp.scrublet(adata, threshold=0.1). /opt/conda/lib/python3.7/site-packages/scanpy/external/pp/_scrublet.py in scrublet(adata, adata_sim, sim_doublet_ratio, expected_doublet_rate, stdev_doublet_rate, synthetic_doublet_umi_subsampling, knn_dist_metric, normalize_variance, log_transform, mean_center, n_prin_comps, use_approx_neighbors, get_doublet_neighbor_parents, n_neighbors, threshold, verbose, copy, random_state). 208 expected_doublet_rate=expected_doublet_rate,. 209 stdev_doublet_rate=stdev_doublet_rate,. --> 210 random_state=random_state,. 211 ). 212 . /opt/conda/lib/python3.7/site-packages/scanpy/external/pp/_scrublet.py in _scrublet_call_doublets(adata_obs, adata_sim, n_neigh",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1645
https://github.com/scverse/scanpy/issues/1645:811,integrability,version,version,811,"scrublet crashes on dense input; - [x] I have checked that this issue has not already been reported. - [x] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. ### Minimal code sample (that we can copy&paste without having any data). ```python. import scanpy as sc. adata = sc.datasets.paul15(). sc.external.pp.scrublet(adata, threshold=0.1). ```. ```pytb. WARNING: In Scanpy 0.*, this returned logarithmized data. Now it returns non-logarithmized data. ... storing 'paul15_clusters' as categorical. /opt/conda/lib/python3.7/site-packages/pandas/core/arrays/categorical.py:2487: FutureWarning: The `inplace` parameter in pandas.Categorical.remove_unused_categories is deprecated and will be removed in a future version. res = method(*args, **kwargs). Trying to set attribute `.uns` of view, copying. /opt/conda/lib/python3.7/site-packages/scanpy/preprocessing/_normalization.py:138: UserWarning: Revieved a view of an AnnData. Making a copy. view_to_actual(adata). ---------------------------------------------------------------------------. AttributeError Traceback (most recent call last). <ipython-input-21-3dabe52b6132> in <module>. 1 import scanpy as sc. 2 adata = sc.datasets.paul15(). ----> 3 sc.external.pp.scrublet(adata, threshold=0.1). /opt/conda/lib/python3.7/site-packages/scanpy/external/pp/_scrublet.py in scrublet(adata, adata_sim, sim_doublet_ratio, expected_doublet_rate, stdev_doublet_rate, synthetic_doublet_umi_subsampling, knn_dist_metric, normalize_variance, log_transform, mean_center, n_prin_comps, use_approx_neighbors, get_doublet_neighbor_parents, n_neighbors, threshold, verbose, copy, random_state). 208 expected_doublet_rate=expected_doublet_rate,. 209 stdev_doublet_rate=stdev_doublet_rate,. --> 210 random_state=random_state,. 211 ). 212 . /opt/conda/lib/python3.7/site-packages/scanpy/external/pp/_scrublet.py in _scrublet_call_doublets(adata_obs, adata_sim, n_neigh",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1645
https://github.com/scverse/scanpy/issues/1645:2996,integrability,Version,Versions,2996,"t-21-3dabe52b6132> in <module>. 1 import scanpy as sc. 2 adata = sc.datasets.paul15(). ----> 3 sc.external.pp.scrublet(adata, threshold=0.1). /opt/conda/lib/python3.7/site-packages/scanpy/external/pp/_scrublet.py in scrublet(adata, adata_sim, sim_doublet_ratio, expected_doublet_rate, stdev_doublet_rate, synthetic_doublet_umi_subsampling, knn_dist_metric, normalize_variance, log_transform, mean_center, n_prin_comps, use_approx_neighbors, get_doublet_neighbor_parents, n_neighbors, threshold, verbose, copy, random_state). 208 expected_doublet_rate=expected_doublet_rate,. 209 stdev_doublet_rate=stdev_doublet_rate,. --> 210 random_state=random_state,. 211 ). 212 . /opt/conda/lib/python3.7/site-packages/scanpy/external/pp/_scrublet.py in _scrublet_call_doublets(adata_obs, adata_sim, n_neighbors, expected_doublet_rate, stdev_doublet_rate, mean_center, normalize_variance, n_prin_comps, use_approx_neighbors, knn_dist_metric, get_doublet_neighbor_parents, random_state, verbose). 349 . 350 if mean_center and normalize_variance:. --> 351 sl.pipeline_zscore(scrub). 352 elif mean_center:. 353 sl.pipeline_mean_center(scrub). /opt/conda/lib/python3.7/site-packages/scrublet/helper_functions.py in pipeline_zscore(self). 62 def pipeline_zscore(self):. 63 gene_means = self._E_obs_norm.mean(0). ---> 64 gene_stdevs = np.sqrt(sparse_var(self._E_obs_norm)). 65 self._E_obs_norm = np.array(sparse_zscore(self._E_obs_norm, gene_means, gene_stdevs)). 66 if self._E_sim_norm is not None:. /opt/conda/lib/python3.7/site-packages/scrublet/helper_functions.py in sparse_var(E, axis). 153 ''' variance across the specified axis '''. 154 . --> 155 mean_gene = E.mean(axis=axis).A.squeeze(). 156 tmp = E.copy(). 157 tmp.data **= 2. AttributeError: 'numpy.ndarray' object has no attribute 'A'. ```. #### Versions. <details>. scanpy==1.7.0 anndata==0.7.5 umap==0.5.1 numpy==1.20.0 scipy==1.6.0 pandas==1.2.1 scikit-learn==0.24.1 statsmodels==0.12.2 python-igraph==0.8.3 louvain==0.7.0 leidenalg==0.8.3. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1645
https://github.com/scverse/scanpy/issues/1645:2808,interoperability,specif,specified,2808,"t-21-3dabe52b6132> in <module>. 1 import scanpy as sc. 2 adata = sc.datasets.paul15(). ----> 3 sc.external.pp.scrublet(adata, threshold=0.1). /opt/conda/lib/python3.7/site-packages/scanpy/external/pp/_scrublet.py in scrublet(adata, adata_sim, sim_doublet_ratio, expected_doublet_rate, stdev_doublet_rate, synthetic_doublet_umi_subsampling, knn_dist_metric, normalize_variance, log_transform, mean_center, n_prin_comps, use_approx_neighbors, get_doublet_neighbor_parents, n_neighbors, threshold, verbose, copy, random_state). 208 expected_doublet_rate=expected_doublet_rate,. 209 stdev_doublet_rate=stdev_doublet_rate,. --> 210 random_state=random_state,. 211 ). 212 . /opt/conda/lib/python3.7/site-packages/scanpy/external/pp/_scrublet.py in _scrublet_call_doublets(adata_obs, adata_sim, n_neighbors, expected_doublet_rate, stdev_doublet_rate, mean_center, normalize_variance, n_prin_comps, use_approx_neighbors, knn_dist_metric, get_doublet_neighbor_parents, random_state, verbose). 349 . 350 if mean_center and normalize_variance:. --> 351 sl.pipeline_zscore(scrub). 352 elif mean_center:. 353 sl.pipeline_mean_center(scrub). /opt/conda/lib/python3.7/site-packages/scrublet/helper_functions.py in pipeline_zscore(self). 62 def pipeline_zscore(self):. 63 gene_means = self._E_obs_norm.mean(0). ---> 64 gene_stdevs = np.sqrt(sparse_var(self._E_obs_norm)). 65 self._E_obs_norm = np.array(sparse_zscore(self._E_obs_norm, gene_means, gene_stdevs)). 66 if self._E_sim_norm is not None:. /opt/conda/lib/python3.7/site-packages/scrublet/helper_functions.py in sparse_var(E, axis). 153 ''' variance across the specified axis '''. 154 . --> 155 mean_gene = E.mean(axis=axis).A.squeeze(). 156 tmp = E.copy(). 157 tmp.data **= 2. AttributeError: 'numpy.ndarray' object has no attribute 'A'. ```. #### Versions. <details>. scanpy==1.7.0 anndata==0.7.5 umap==0.5.1 numpy==1.20.0 scipy==1.6.0 pandas==1.2.1 scikit-learn==0.24.1 statsmodels==0.12.2 python-igraph==0.8.3 louvain==0.7.0 leidenalg==0.8.3. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1645
https://github.com/scverse/scanpy/issues/1645:154,modifiability,version,version,154,"scrublet crashes on dense input; - [x] I have checked that this issue has not already been reported. - [x] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. ### Minimal code sample (that we can copy&paste without having any data). ```python. import scanpy as sc. adata = sc.datasets.paul15(). sc.external.pp.scrublet(adata, threshold=0.1). ```. ```pytb. WARNING: In Scanpy 0.*, this returned logarithmized data. Now it returns non-logarithmized data. ... storing 'paul15_clusters' as categorical. /opt/conda/lib/python3.7/site-packages/pandas/core/arrays/categorical.py:2487: FutureWarning: The `inplace` parameter in pandas.Categorical.remove_unused_categories is deprecated and will be removed in a future version. res = method(*args, **kwargs). Trying to set attribute `.uns` of view, copying. /opt/conda/lib/python3.7/site-packages/scanpy/preprocessing/_normalization.py:138: UserWarning: Revieved a view of an AnnData. Making a copy. view_to_actual(adata). ---------------------------------------------------------------------------. AttributeError Traceback (most recent call last). <ipython-input-21-3dabe52b6132> in <module>. 1 import scanpy as sc. 2 adata = sc.datasets.paul15(). ----> 3 sc.external.pp.scrublet(adata, threshold=0.1). /opt/conda/lib/python3.7/site-packages/scanpy/external/pp/_scrublet.py in scrublet(adata, adata_sim, sim_doublet_ratio, expected_doublet_rate, stdev_doublet_rate, synthetic_doublet_umi_subsampling, knn_dist_metric, normalize_variance, log_transform, mean_center, n_prin_comps, use_approx_neighbors, get_doublet_neighbor_parents, n_neighbors, threshold, verbose, copy, random_state). 208 expected_doublet_rate=expected_doublet_rate,. 209 stdev_doublet_rate=stdev_doublet_rate,. --> 210 random_state=random_state,. 211 ). 212 . /opt/conda/lib/python3.7/site-packages/scanpy/external/pp/_scrublet.py in _scrublet_call_doublets(adata_obs, adata_sim, n_neigh",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1645
https://github.com/scverse/scanpy/issues/1645:630,modifiability,pac,packages,630,"scrublet crashes on dense input; - [x] I have checked that this issue has not already been reported. - [x] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. ### Minimal code sample (that we can copy&paste without having any data). ```python. import scanpy as sc. adata = sc.datasets.paul15(). sc.external.pp.scrublet(adata, threshold=0.1). ```. ```pytb. WARNING: In Scanpy 0.*, this returned logarithmized data. Now it returns non-logarithmized data. ... storing 'paul15_clusters' as categorical. /opt/conda/lib/python3.7/site-packages/pandas/core/arrays/categorical.py:2487: FutureWarning: The `inplace` parameter in pandas.Categorical.remove_unused_categories is deprecated and will be removed in a future version. res = method(*args, **kwargs). Trying to set attribute `.uns` of view, copying. /opt/conda/lib/python3.7/site-packages/scanpy/preprocessing/_normalization.py:138: UserWarning: Revieved a view of an AnnData. Making a copy. view_to_actual(adata). ---------------------------------------------------------------------------. AttributeError Traceback (most recent call last). <ipython-input-21-3dabe52b6132> in <module>. 1 import scanpy as sc. 2 adata = sc.datasets.paul15(). ----> 3 sc.external.pp.scrublet(adata, threshold=0.1). /opt/conda/lib/python3.7/site-packages/scanpy/external/pp/_scrublet.py in scrublet(adata, adata_sim, sim_doublet_ratio, expected_doublet_rate, stdev_doublet_rate, synthetic_doublet_umi_subsampling, knn_dist_metric, normalize_variance, log_transform, mean_center, n_prin_comps, use_approx_neighbors, get_doublet_neighbor_parents, n_neighbors, threshold, verbose, copy, random_state). 208 expected_doublet_rate=expected_doublet_rate,. 209 stdev_doublet_rate=stdev_doublet_rate,. --> 210 random_state=random_state,. 211 ). 212 . /opt/conda/lib/python3.7/site-packages/scanpy/external/pp/_scrublet.py in _scrublet_call_doublets(adata_obs, adata_sim, n_neigh",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1645
https://github.com/scverse/scanpy/issues/1645:708,modifiability,paramet,parameter,708,"scrublet crashes on dense input; - [x] I have checked that this issue has not already been reported. - [x] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. ### Minimal code sample (that we can copy&paste without having any data). ```python. import scanpy as sc. adata = sc.datasets.paul15(). sc.external.pp.scrublet(adata, threshold=0.1). ```. ```pytb. WARNING: In Scanpy 0.*, this returned logarithmized data. Now it returns non-logarithmized data. ... storing 'paul15_clusters' as categorical. /opt/conda/lib/python3.7/site-packages/pandas/core/arrays/categorical.py:2487: FutureWarning: The `inplace` parameter in pandas.Categorical.remove_unused_categories is deprecated and will be removed in a future version. res = method(*args, **kwargs). Trying to set attribute `.uns` of view, copying. /opt/conda/lib/python3.7/site-packages/scanpy/preprocessing/_normalization.py:138: UserWarning: Revieved a view of an AnnData. Making a copy. view_to_actual(adata). ---------------------------------------------------------------------------. AttributeError Traceback (most recent call last). <ipython-input-21-3dabe52b6132> in <module>. 1 import scanpy as sc. 2 adata = sc.datasets.paul15(). ----> 3 sc.external.pp.scrublet(adata, threshold=0.1). /opt/conda/lib/python3.7/site-packages/scanpy/external/pp/_scrublet.py in scrublet(adata, adata_sim, sim_doublet_ratio, expected_doublet_rate, stdev_doublet_rate, synthetic_doublet_umi_subsampling, knn_dist_metric, normalize_variance, log_transform, mean_center, n_prin_comps, use_approx_neighbors, get_doublet_neighbor_parents, n_neighbors, threshold, verbose, copy, random_state). 208 expected_doublet_rate=expected_doublet_rate,. 209 stdev_doublet_rate=stdev_doublet_rate,. --> 210 random_state=random_state,. 211 ). 212 . /opt/conda/lib/python3.7/site-packages/scanpy/external/pp/_scrublet.py in _scrublet_call_doublets(adata_obs, adata_sim, n_neigh",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1645
https://github.com/scverse/scanpy/issues/1645:811,modifiability,version,version,811,"scrublet crashes on dense input; - [x] I have checked that this issue has not already been reported. - [x] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. ### Minimal code sample (that we can copy&paste without having any data). ```python. import scanpy as sc. adata = sc.datasets.paul15(). sc.external.pp.scrublet(adata, threshold=0.1). ```. ```pytb. WARNING: In Scanpy 0.*, this returned logarithmized data. Now it returns non-logarithmized data. ... storing 'paul15_clusters' as categorical. /opt/conda/lib/python3.7/site-packages/pandas/core/arrays/categorical.py:2487: FutureWarning: The `inplace` parameter in pandas.Categorical.remove_unused_categories is deprecated and will be removed in a future version. res = method(*args, **kwargs). Trying to set attribute `.uns` of view, copying. /opt/conda/lib/python3.7/site-packages/scanpy/preprocessing/_normalization.py:138: UserWarning: Revieved a view of an AnnData. Making a copy. view_to_actual(adata). ---------------------------------------------------------------------------. AttributeError Traceback (most recent call last). <ipython-input-21-3dabe52b6132> in <module>. 1 import scanpy as sc. 2 adata = sc.datasets.paul15(). ----> 3 sc.external.pp.scrublet(adata, threshold=0.1). /opt/conda/lib/python3.7/site-packages/scanpy/external/pp/_scrublet.py in scrublet(adata, adata_sim, sim_doublet_ratio, expected_doublet_rate, stdev_doublet_rate, synthetic_doublet_umi_subsampling, knn_dist_metric, normalize_variance, log_transform, mean_center, n_prin_comps, use_approx_neighbors, get_doublet_neighbor_parents, n_neighbors, threshold, verbose, copy, random_state). 208 expected_doublet_rate=expected_doublet_rate,. 209 stdev_doublet_rate=stdev_doublet_rate,. --> 210 random_state=random_state,. 211 ). 212 . /opt/conda/lib/python3.7/site-packages/scanpy/external/pp/_scrublet.py in _scrublet_call_doublets(adata_obs, adata_sim, n_neigh",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1645
https://github.com/scverse/scanpy/issues/1645:930,modifiability,pac,packages,930,"scrublet crashes on dense input; - [x] I have checked that this issue has not already been reported. - [x] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. ### Minimal code sample (that we can copy&paste without having any data). ```python. import scanpy as sc. adata = sc.datasets.paul15(). sc.external.pp.scrublet(adata, threshold=0.1). ```. ```pytb. WARNING: In Scanpy 0.*, this returned logarithmized data. Now it returns non-logarithmized data. ... storing 'paul15_clusters' as categorical. /opt/conda/lib/python3.7/site-packages/pandas/core/arrays/categorical.py:2487: FutureWarning: The `inplace` parameter in pandas.Categorical.remove_unused_categories is deprecated and will be removed in a future version. res = method(*args, **kwargs). Trying to set attribute `.uns` of view, copying. /opt/conda/lib/python3.7/site-packages/scanpy/preprocessing/_normalization.py:138: UserWarning: Revieved a view of an AnnData. Making a copy. view_to_actual(adata). ---------------------------------------------------------------------------. AttributeError Traceback (most recent call last). <ipython-input-21-3dabe52b6132> in <module>. 1 import scanpy as sc. 2 adata = sc.datasets.paul15(). ----> 3 sc.external.pp.scrublet(adata, threshold=0.1). /opt/conda/lib/python3.7/site-packages/scanpy/external/pp/_scrublet.py in scrublet(adata, adata_sim, sim_doublet_ratio, expected_doublet_rate, stdev_doublet_rate, synthetic_doublet_umi_subsampling, knn_dist_metric, normalize_variance, log_transform, mean_center, n_prin_comps, use_approx_neighbors, get_doublet_neighbor_parents, n_neighbors, threshold, verbose, copy, random_state). 208 expected_doublet_rate=expected_doublet_rate,. 209 stdev_doublet_rate=stdev_doublet_rate,. --> 210 random_state=random_state,. 211 ). 212 . /opt/conda/lib/python3.7/site-packages/scanpy/external/pp/_scrublet.py in _scrublet_call_doublets(adata_obs, adata_sim, n_neigh",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1645
https://github.com/scverse/scanpy/issues/1645:1228,modifiability,modul,module,1228,"aster branch of scanpy. ---. ### Minimal code sample (that we can copy&paste without having any data). ```python. import scanpy as sc. adata = sc.datasets.paul15(). sc.external.pp.scrublet(adata, threshold=0.1). ```. ```pytb. WARNING: In Scanpy 0.*, this returned logarithmized data. Now it returns non-logarithmized data. ... storing 'paul15_clusters' as categorical. /opt/conda/lib/python3.7/site-packages/pandas/core/arrays/categorical.py:2487: FutureWarning: The `inplace` parameter in pandas.Categorical.remove_unused_categories is deprecated and will be removed in a future version. res = method(*args, **kwargs). Trying to set attribute `.uns` of view, copying. /opt/conda/lib/python3.7/site-packages/scanpy/preprocessing/_normalization.py:138: UserWarning: Revieved a view of an AnnData. Making a copy. view_to_actual(adata). ---------------------------------------------------------------------------. AttributeError Traceback (most recent call last). <ipython-input-21-3dabe52b6132> in <module>. 1 import scanpy as sc. 2 adata = sc.datasets.paul15(). ----> 3 sc.external.pp.scrublet(adata, threshold=0.1). /opt/conda/lib/python3.7/site-packages/scanpy/external/pp/_scrublet.py in scrublet(adata, adata_sim, sim_doublet_ratio, expected_doublet_rate, stdev_doublet_rate, synthetic_doublet_umi_subsampling, knn_dist_metric, normalize_variance, log_transform, mean_center, n_prin_comps, use_approx_neighbors, get_doublet_neighbor_parents, n_neighbors, threshold, verbose, copy, random_state). 208 expected_doublet_rate=expected_doublet_rate,. 209 stdev_doublet_rate=stdev_doublet_rate,. --> 210 random_state=random_state,. 211 ). 212 . /opt/conda/lib/python3.7/site-packages/scanpy/external/pp/_scrublet.py in _scrublet_call_doublets(adata_obs, adata_sim, n_neighbors, expected_doublet_rate, stdev_doublet_rate, mean_center, normalize_variance, n_prin_comps, use_approx_neighbors, knn_dist_metric, get_doublet_neighbor_parents, random_state, verbose). 349 . 350 if mean_center and normalize_var",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1645
https://github.com/scverse/scanpy/issues/1645:1377,modifiability,pac,packages,1377,"sets.paul15(). sc.external.pp.scrublet(adata, threshold=0.1). ```. ```pytb. WARNING: In Scanpy 0.*, this returned logarithmized data. Now it returns non-logarithmized data. ... storing 'paul15_clusters' as categorical. /opt/conda/lib/python3.7/site-packages/pandas/core/arrays/categorical.py:2487: FutureWarning: The `inplace` parameter in pandas.Categorical.remove_unused_categories is deprecated and will be removed in a future version. res = method(*args, **kwargs). Trying to set attribute `.uns` of view, copying. /opt/conda/lib/python3.7/site-packages/scanpy/preprocessing/_normalization.py:138: UserWarning: Revieved a view of an AnnData. Making a copy. view_to_actual(adata). ---------------------------------------------------------------------------. AttributeError Traceback (most recent call last). <ipython-input-21-3dabe52b6132> in <module>. 1 import scanpy as sc. 2 adata = sc.datasets.paul15(). ----> 3 sc.external.pp.scrublet(adata, threshold=0.1). /opt/conda/lib/python3.7/site-packages/scanpy/external/pp/_scrublet.py in scrublet(adata, adata_sim, sim_doublet_ratio, expected_doublet_rate, stdev_doublet_rate, synthetic_doublet_umi_subsampling, knn_dist_metric, normalize_variance, log_transform, mean_center, n_prin_comps, use_approx_neighbors, get_doublet_neighbor_parents, n_neighbors, threshold, verbose, copy, random_state). 208 expected_doublet_rate=expected_doublet_rate,. 209 stdev_doublet_rate=stdev_doublet_rate,. --> 210 random_state=random_state,. 211 ). 212 . /opt/conda/lib/python3.7/site-packages/scanpy/external/pp/_scrublet.py in _scrublet_call_doublets(adata_obs, adata_sim, n_neighbors, expected_doublet_rate, stdev_doublet_rate, mean_center, normalize_variance, n_prin_comps, use_approx_neighbors, knn_dist_metric, get_doublet_neighbor_parents, random_state, verbose). 349 . 350 if mean_center and normalize_variance:. --> 351 sl.pipeline_zscore(scrub). 352 elif mean_center:. 353 sl.pipeline_mean_center(scrub). /opt/conda/lib/python3.7/site-packages/scrublet/",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1645
https://github.com/scverse/scanpy/issues/1645:1903,modifiability,pac,packages,1903,"nda/lib/python3.7/site-packages/scanpy/preprocessing/_normalization.py:138: UserWarning: Revieved a view of an AnnData. Making a copy. view_to_actual(adata). ---------------------------------------------------------------------------. AttributeError Traceback (most recent call last). <ipython-input-21-3dabe52b6132> in <module>. 1 import scanpy as sc. 2 adata = sc.datasets.paul15(). ----> 3 sc.external.pp.scrublet(adata, threshold=0.1). /opt/conda/lib/python3.7/site-packages/scanpy/external/pp/_scrublet.py in scrublet(adata, adata_sim, sim_doublet_ratio, expected_doublet_rate, stdev_doublet_rate, synthetic_doublet_umi_subsampling, knn_dist_metric, normalize_variance, log_transform, mean_center, n_prin_comps, use_approx_neighbors, get_doublet_neighbor_parents, n_neighbors, threshold, verbose, copy, random_state). 208 expected_doublet_rate=expected_doublet_rate,. 209 stdev_doublet_rate=stdev_doublet_rate,. --> 210 random_state=random_state,. 211 ). 212 . /opt/conda/lib/python3.7/site-packages/scanpy/external/pp/_scrublet.py in _scrublet_call_doublets(adata_obs, adata_sim, n_neighbors, expected_doublet_rate, stdev_doublet_rate, mean_center, normalize_variance, n_prin_comps, use_approx_neighbors, knn_dist_metric, get_doublet_neighbor_parents, random_state, verbose). 349 . 350 if mean_center and normalize_variance:. --> 351 sl.pipeline_zscore(scrub). 352 elif mean_center:. 353 sl.pipeline_mean_center(scrub). /opt/conda/lib/python3.7/site-packages/scrublet/helper_functions.py in pipeline_zscore(self). 62 def pipeline_zscore(self):. 63 gene_means = self._E_obs_norm.mean(0). ---> 64 gene_stdevs = np.sqrt(sparse_var(self._E_obs_norm)). 65 self._E_obs_norm = np.array(sparse_zscore(self._E_obs_norm, gene_means, gene_stdevs)). 66 if self._E_sim_norm is not None:. /opt/conda/lib/python3.7/site-packages/scrublet/helper_functions.py in sparse_var(E, axis). 153 ''' variance across the specified axis '''. 154 . --> 155 mean_gene = E.mean(axis=axis).A.squeeze(). 156 tmp = E.copy(). 15",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1645
https://github.com/scverse/scanpy/issues/1645:2363,modifiability,pac,packages,2363,"t-21-3dabe52b6132> in <module>. 1 import scanpy as sc. 2 adata = sc.datasets.paul15(). ----> 3 sc.external.pp.scrublet(adata, threshold=0.1). /opt/conda/lib/python3.7/site-packages/scanpy/external/pp/_scrublet.py in scrublet(adata, adata_sim, sim_doublet_ratio, expected_doublet_rate, stdev_doublet_rate, synthetic_doublet_umi_subsampling, knn_dist_metric, normalize_variance, log_transform, mean_center, n_prin_comps, use_approx_neighbors, get_doublet_neighbor_parents, n_neighbors, threshold, verbose, copy, random_state). 208 expected_doublet_rate=expected_doublet_rate,. 209 stdev_doublet_rate=stdev_doublet_rate,. --> 210 random_state=random_state,. 211 ). 212 . /opt/conda/lib/python3.7/site-packages/scanpy/external/pp/_scrublet.py in _scrublet_call_doublets(adata_obs, adata_sim, n_neighbors, expected_doublet_rate, stdev_doublet_rate, mean_center, normalize_variance, n_prin_comps, use_approx_neighbors, knn_dist_metric, get_doublet_neighbor_parents, random_state, verbose). 349 . 350 if mean_center and normalize_variance:. --> 351 sl.pipeline_zscore(scrub). 352 elif mean_center:. 353 sl.pipeline_mean_center(scrub). /opt/conda/lib/python3.7/site-packages/scrublet/helper_functions.py in pipeline_zscore(self). 62 def pipeline_zscore(self):. 63 gene_means = self._E_obs_norm.mean(0). ---> 64 gene_stdevs = np.sqrt(sparse_var(self._E_obs_norm)). 65 self._E_obs_norm = np.array(sparse_zscore(self._E_obs_norm, gene_means, gene_stdevs)). 66 if self._E_sim_norm is not None:. /opt/conda/lib/python3.7/site-packages/scrublet/helper_functions.py in sparse_var(E, axis). 153 ''' variance across the specified axis '''. 154 . --> 155 mean_gene = E.mean(axis=axis).A.squeeze(). 156 tmp = E.copy(). 157 tmp.data **= 2. AttributeError: 'numpy.ndarray' object has no attribute 'A'. ```. #### Versions. <details>. scanpy==1.7.0 anndata==0.7.5 umap==0.5.1 numpy==1.20.0 scipy==1.6.0 pandas==1.2.1 scikit-learn==0.24.1 statsmodels==0.12.2 python-igraph==0.8.3 louvain==0.7.0 leidenalg==0.8.3. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1645
https://github.com/scverse/scanpy/issues/1645:2718,modifiability,pac,packages,2718,"t-21-3dabe52b6132> in <module>. 1 import scanpy as sc. 2 adata = sc.datasets.paul15(). ----> 3 sc.external.pp.scrublet(adata, threshold=0.1). /opt/conda/lib/python3.7/site-packages/scanpy/external/pp/_scrublet.py in scrublet(adata, adata_sim, sim_doublet_ratio, expected_doublet_rate, stdev_doublet_rate, synthetic_doublet_umi_subsampling, knn_dist_metric, normalize_variance, log_transform, mean_center, n_prin_comps, use_approx_neighbors, get_doublet_neighbor_parents, n_neighbors, threshold, verbose, copy, random_state). 208 expected_doublet_rate=expected_doublet_rate,. 209 stdev_doublet_rate=stdev_doublet_rate,. --> 210 random_state=random_state,. 211 ). 212 . /opt/conda/lib/python3.7/site-packages/scanpy/external/pp/_scrublet.py in _scrublet_call_doublets(adata_obs, adata_sim, n_neighbors, expected_doublet_rate, stdev_doublet_rate, mean_center, normalize_variance, n_prin_comps, use_approx_neighbors, knn_dist_metric, get_doublet_neighbor_parents, random_state, verbose). 349 . 350 if mean_center and normalize_variance:. --> 351 sl.pipeline_zscore(scrub). 352 elif mean_center:. 353 sl.pipeline_mean_center(scrub). /opt/conda/lib/python3.7/site-packages/scrublet/helper_functions.py in pipeline_zscore(self). 62 def pipeline_zscore(self):. 63 gene_means = self._E_obs_norm.mean(0). ---> 64 gene_stdevs = np.sqrt(sparse_var(self._E_obs_norm)). 65 self._E_obs_norm = np.array(sparse_zscore(self._E_obs_norm, gene_means, gene_stdevs)). 66 if self._E_sim_norm is not None:. /opt/conda/lib/python3.7/site-packages/scrublet/helper_functions.py in sparse_var(E, axis). 153 ''' variance across the specified axis '''. 154 . --> 155 mean_gene = E.mean(axis=axis).A.squeeze(). 156 tmp = E.copy(). 157 tmp.data **= 2. AttributeError: 'numpy.ndarray' object has no attribute 'A'. ```. #### Versions. <details>. scanpy==1.7.0 anndata==0.7.5 umap==0.5.1 numpy==1.20.0 scipy==1.6.0 pandas==1.2.1 scikit-learn==0.24.1 statsmodels==0.12.2 python-igraph==0.8.3 louvain==0.7.0 leidenalg==0.8.3. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1645
https://github.com/scverse/scanpy/issues/1645:2996,modifiability,Version,Versions,2996,"t-21-3dabe52b6132> in <module>. 1 import scanpy as sc. 2 adata = sc.datasets.paul15(). ----> 3 sc.external.pp.scrublet(adata, threshold=0.1). /opt/conda/lib/python3.7/site-packages/scanpy/external/pp/_scrublet.py in scrublet(adata, adata_sim, sim_doublet_ratio, expected_doublet_rate, stdev_doublet_rate, synthetic_doublet_umi_subsampling, knn_dist_metric, normalize_variance, log_transform, mean_center, n_prin_comps, use_approx_neighbors, get_doublet_neighbor_parents, n_neighbors, threshold, verbose, copy, random_state). 208 expected_doublet_rate=expected_doublet_rate,. 209 stdev_doublet_rate=stdev_doublet_rate,. --> 210 random_state=random_state,. 211 ). 212 . /opt/conda/lib/python3.7/site-packages/scanpy/external/pp/_scrublet.py in _scrublet_call_doublets(adata_obs, adata_sim, n_neighbors, expected_doublet_rate, stdev_doublet_rate, mean_center, normalize_variance, n_prin_comps, use_approx_neighbors, knn_dist_metric, get_doublet_neighbor_parents, random_state, verbose). 349 . 350 if mean_center and normalize_variance:. --> 351 sl.pipeline_zscore(scrub). 352 elif mean_center:. 353 sl.pipeline_mean_center(scrub). /opt/conda/lib/python3.7/site-packages/scrublet/helper_functions.py in pipeline_zscore(self). 62 def pipeline_zscore(self):. 63 gene_means = self._E_obs_norm.mean(0). ---> 64 gene_stdevs = np.sqrt(sparse_var(self._E_obs_norm)). 65 self._E_obs_norm = np.array(sparse_zscore(self._E_obs_norm, gene_means, gene_stdevs)). 66 if self._E_sim_norm is not None:. /opt/conda/lib/python3.7/site-packages/scrublet/helper_functions.py in sparse_var(E, axis). 153 ''' variance across the specified axis '''. 154 . --> 155 mean_gene = E.mean(axis=axis).A.squeeze(). 156 tmp = E.copy(). 157 tmp.data **= 2. AttributeError: 'numpy.ndarray' object has no attribute 'A'. ```. #### Versions. <details>. scanpy==1.7.0 anndata==0.7.5 umap==0.5.1 numpy==1.20.0 scipy==1.6.0 pandas==1.2.1 scikit-learn==0.24.1 statsmodels==0.12.2 python-igraph==0.8.3 louvain==0.7.0 leidenalg==0.8.3. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1645
https://github.com/scverse/scanpy/issues/1645:26,safety,input,input,26,"scrublet crashes on dense input; - [x] I have checked that this issue has not already been reported. - [x] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. ### Minimal code sample (that we can copy&paste without having any data). ```python. import scanpy as sc. adata = sc.datasets.paul15(). sc.external.pp.scrublet(adata, threshold=0.1). ```. ```pytb. WARNING: In Scanpy 0.*, this returned logarithmized data. Now it returns non-logarithmized data. ... storing 'paul15_clusters' as categorical. /opt/conda/lib/python3.7/site-packages/pandas/core/arrays/categorical.py:2487: FutureWarning: The `inplace` parameter in pandas.Categorical.remove_unused_categories is deprecated and will be removed in a future version. res = method(*args, **kwargs). Trying to set attribute `.uns` of view, copying. /opt/conda/lib/python3.7/site-packages/scanpy/preprocessing/_normalization.py:138: UserWarning: Revieved a view of an AnnData. Making a copy. view_to_actual(adata). ---------------------------------------------------------------------------. AttributeError Traceback (most recent call last). <ipython-input-21-3dabe52b6132> in <module>. 1 import scanpy as sc. 2 adata = sc.datasets.paul15(). ----> 3 sc.external.pp.scrublet(adata, threshold=0.1). /opt/conda/lib/python3.7/site-packages/scanpy/external/pp/_scrublet.py in scrublet(adata, adata_sim, sim_doublet_ratio, expected_doublet_rate, stdev_doublet_rate, synthetic_doublet_umi_subsampling, knn_dist_metric, normalize_variance, log_transform, mean_center, n_prin_comps, use_approx_neighbors, get_doublet_neighbor_parents, n_neighbors, threshold, verbose, copy, random_state). 208 expected_doublet_rate=expected_doublet_rate,. 209 stdev_doublet_rate=stdev_doublet_rate,. --> 210 random_state=random_state,. 211 ). 212 . /opt/conda/lib/python3.7/site-packages/scanpy/external/pp/_scrublet.py in _scrublet_call_doublets(adata_obs, adata_sim, n_neigh",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1645
https://github.com/scverse/scanpy/issues/1645:495,safety,log,logarithmized,495,"scrublet crashes on dense input; - [x] I have checked that this issue has not already been reported. - [x] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. ### Minimal code sample (that we can copy&paste without having any data). ```python. import scanpy as sc. adata = sc.datasets.paul15(). sc.external.pp.scrublet(adata, threshold=0.1). ```. ```pytb. WARNING: In Scanpy 0.*, this returned logarithmized data. Now it returns non-logarithmized data. ... storing 'paul15_clusters' as categorical. /opt/conda/lib/python3.7/site-packages/pandas/core/arrays/categorical.py:2487: FutureWarning: The `inplace` parameter in pandas.Categorical.remove_unused_categories is deprecated and will be removed in a future version. res = method(*args, **kwargs). Trying to set attribute `.uns` of view, copying. /opt/conda/lib/python3.7/site-packages/scanpy/preprocessing/_normalization.py:138: UserWarning: Revieved a view of an AnnData. Making a copy. view_to_actual(adata). ---------------------------------------------------------------------------. AttributeError Traceback (most recent call last). <ipython-input-21-3dabe52b6132> in <module>. 1 import scanpy as sc. 2 adata = sc.datasets.paul15(). ----> 3 sc.external.pp.scrublet(adata, threshold=0.1). /opt/conda/lib/python3.7/site-packages/scanpy/external/pp/_scrublet.py in scrublet(adata, adata_sim, sim_doublet_ratio, expected_doublet_rate, stdev_doublet_rate, synthetic_doublet_umi_subsampling, knn_dist_metric, normalize_variance, log_transform, mean_center, n_prin_comps, use_approx_neighbors, get_doublet_neighbor_parents, n_neighbors, threshold, verbose, copy, random_state). 208 expected_doublet_rate=expected_doublet_rate,. 209 stdev_doublet_rate=stdev_doublet_rate,. --> 210 random_state=random_state,. 211 ). 212 . /opt/conda/lib/python3.7/site-packages/scanpy/external/pp/_scrublet.py in _scrublet_call_doublets(adata_obs, adata_sim, n_neigh",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1645
https://github.com/scverse/scanpy/issues/1645:534,safety,log,logarithmized,534,"scrublet crashes on dense input; - [x] I have checked that this issue has not already been reported. - [x] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. ### Minimal code sample (that we can copy&paste without having any data). ```python. import scanpy as sc. adata = sc.datasets.paul15(). sc.external.pp.scrublet(adata, threshold=0.1). ```. ```pytb. WARNING: In Scanpy 0.*, this returned logarithmized data. Now it returns non-logarithmized data. ... storing 'paul15_clusters' as categorical. /opt/conda/lib/python3.7/site-packages/pandas/core/arrays/categorical.py:2487: FutureWarning: The `inplace` parameter in pandas.Categorical.remove_unused_categories is deprecated and will be removed in a future version. res = method(*args, **kwargs). Trying to set attribute `.uns` of view, copying. /opt/conda/lib/python3.7/site-packages/scanpy/preprocessing/_normalization.py:138: UserWarning: Revieved a view of an AnnData. Making a copy. view_to_actual(adata). ---------------------------------------------------------------------------. AttributeError Traceback (most recent call last). <ipython-input-21-3dabe52b6132> in <module>. 1 import scanpy as sc. 2 adata = sc.datasets.paul15(). ----> 3 sc.external.pp.scrublet(adata, threshold=0.1). /opt/conda/lib/python3.7/site-packages/scanpy/external/pp/_scrublet.py in scrublet(adata, adata_sim, sim_doublet_ratio, expected_doublet_rate, stdev_doublet_rate, synthetic_doublet_umi_subsampling, knn_dist_metric, normalize_variance, log_transform, mean_center, n_prin_comps, use_approx_neighbors, get_doublet_neighbor_parents, n_neighbors, threshold, verbose, copy, random_state). 208 expected_doublet_rate=expected_doublet_rate,. 209 stdev_doublet_rate=stdev_doublet_rate,. --> 210 random_state=random_state,. 211 ). 212 . /opt/conda/lib/python3.7/site-packages/scanpy/external/pp/_scrublet.py in _scrublet_call_doublets(adata_obs, adata_sim, n_neigh",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1645
https://github.com/scverse/scanpy/issues/1645:1201,safety,input,input-,1201,"ed this bug exists on the master branch of scanpy. ---. ### Minimal code sample (that we can copy&paste without having any data). ```python. import scanpy as sc. adata = sc.datasets.paul15(). sc.external.pp.scrublet(adata, threshold=0.1). ```. ```pytb. WARNING: In Scanpy 0.*, this returned logarithmized data. Now it returns non-logarithmized data. ... storing 'paul15_clusters' as categorical. /opt/conda/lib/python3.7/site-packages/pandas/core/arrays/categorical.py:2487: FutureWarning: The `inplace` parameter in pandas.Categorical.remove_unused_categories is deprecated and will be removed in a future version. res = method(*args, **kwargs). Trying to set attribute `.uns` of view, copying. /opt/conda/lib/python3.7/site-packages/scanpy/preprocessing/_normalization.py:138: UserWarning: Revieved a view of an AnnData. Making a copy. view_to_actual(adata). ---------------------------------------------------------------------------. AttributeError Traceback (most recent call last). <ipython-input-21-3dabe52b6132> in <module>. 1 import scanpy as sc. 2 adata = sc.datasets.paul15(). ----> 3 sc.external.pp.scrublet(adata, threshold=0.1). /opt/conda/lib/python3.7/site-packages/scanpy/external/pp/_scrublet.py in scrublet(adata, adata_sim, sim_doublet_ratio, expected_doublet_rate, stdev_doublet_rate, synthetic_doublet_umi_subsampling, knn_dist_metric, normalize_variance, log_transform, mean_center, n_prin_comps, use_approx_neighbors, get_doublet_neighbor_parents, n_neighbors, threshold, verbose, copy, random_state). 208 expected_doublet_rate=expected_doublet_rate,. 209 stdev_doublet_rate=stdev_doublet_rate,. --> 210 random_state=random_state,. 211 ). 212 . /opt/conda/lib/python3.7/site-packages/scanpy/external/pp/_scrublet.py in _scrublet_call_doublets(adata_obs, adata_sim, n_neighbors, expected_doublet_rate, stdev_doublet_rate, mean_center, normalize_variance, n_prin_comps, use_approx_neighbors, knn_dist_metric, get_doublet_neighbor_parents, random_state, verbose). 349 . 350 if me",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1645
https://github.com/scverse/scanpy/issues/1645:1228,safety,modul,module,1228,"aster branch of scanpy. ---. ### Minimal code sample (that we can copy&paste without having any data). ```python. import scanpy as sc. adata = sc.datasets.paul15(). sc.external.pp.scrublet(adata, threshold=0.1). ```. ```pytb. WARNING: In Scanpy 0.*, this returned logarithmized data. Now it returns non-logarithmized data. ... storing 'paul15_clusters' as categorical. /opt/conda/lib/python3.7/site-packages/pandas/core/arrays/categorical.py:2487: FutureWarning: The `inplace` parameter in pandas.Categorical.remove_unused_categories is deprecated and will be removed in a future version. res = method(*args, **kwargs). Trying to set attribute `.uns` of view, copying. /opt/conda/lib/python3.7/site-packages/scanpy/preprocessing/_normalization.py:138: UserWarning: Revieved a view of an AnnData. Making a copy. view_to_actual(adata). ---------------------------------------------------------------------------. AttributeError Traceback (most recent call last). <ipython-input-21-3dabe52b6132> in <module>. 1 import scanpy as sc. 2 adata = sc.datasets.paul15(). ----> 3 sc.external.pp.scrublet(adata, threshold=0.1). /opt/conda/lib/python3.7/site-packages/scanpy/external/pp/_scrublet.py in scrublet(adata, adata_sim, sim_doublet_ratio, expected_doublet_rate, stdev_doublet_rate, synthetic_doublet_umi_subsampling, knn_dist_metric, normalize_variance, log_transform, mean_center, n_prin_comps, use_approx_neighbors, get_doublet_neighbor_parents, n_neighbors, threshold, verbose, copy, random_state). 208 expected_doublet_rate=expected_doublet_rate,. 209 stdev_doublet_rate=stdev_doublet_rate,. --> 210 random_state=random_state,. 211 ). 212 . /opt/conda/lib/python3.7/site-packages/scanpy/external/pp/_scrublet.py in _scrublet_call_doublets(adata_obs, adata_sim, n_neighbors, expected_doublet_rate, stdev_doublet_rate, mean_center, normalize_variance, n_prin_comps, use_approx_neighbors, knn_dist_metric, get_doublet_neighbor_parents, random_state, verbose). 349 . 350 if mean_center and normalize_var",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1645
https://github.com/scverse/scanpy/issues/1645:495,security,log,logarithmized,495,"scrublet crashes on dense input; - [x] I have checked that this issue has not already been reported. - [x] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. ### Minimal code sample (that we can copy&paste without having any data). ```python. import scanpy as sc. adata = sc.datasets.paul15(). sc.external.pp.scrublet(adata, threshold=0.1). ```. ```pytb. WARNING: In Scanpy 0.*, this returned logarithmized data. Now it returns non-logarithmized data. ... storing 'paul15_clusters' as categorical. /opt/conda/lib/python3.7/site-packages/pandas/core/arrays/categorical.py:2487: FutureWarning: The `inplace` parameter in pandas.Categorical.remove_unused_categories is deprecated and will be removed in a future version. res = method(*args, **kwargs). Trying to set attribute `.uns` of view, copying. /opt/conda/lib/python3.7/site-packages/scanpy/preprocessing/_normalization.py:138: UserWarning: Revieved a view of an AnnData. Making a copy. view_to_actual(adata). ---------------------------------------------------------------------------. AttributeError Traceback (most recent call last). <ipython-input-21-3dabe52b6132> in <module>. 1 import scanpy as sc. 2 adata = sc.datasets.paul15(). ----> 3 sc.external.pp.scrublet(adata, threshold=0.1). /opt/conda/lib/python3.7/site-packages/scanpy/external/pp/_scrublet.py in scrublet(adata, adata_sim, sim_doublet_ratio, expected_doublet_rate, stdev_doublet_rate, synthetic_doublet_umi_subsampling, knn_dist_metric, normalize_variance, log_transform, mean_center, n_prin_comps, use_approx_neighbors, get_doublet_neighbor_parents, n_neighbors, threshold, verbose, copy, random_state). 208 expected_doublet_rate=expected_doublet_rate,. 209 stdev_doublet_rate=stdev_doublet_rate,. --> 210 random_state=random_state,. 211 ). 212 . /opt/conda/lib/python3.7/site-packages/scanpy/external/pp/_scrublet.py in _scrublet_call_doublets(adata_obs, adata_sim, n_neigh",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1645
https://github.com/scverse/scanpy/issues/1645:534,security,log,logarithmized,534,"scrublet crashes on dense input; - [x] I have checked that this issue has not already been reported. - [x] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. ### Minimal code sample (that we can copy&paste without having any data). ```python. import scanpy as sc. adata = sc.datasets.paul15(). sc.external.pp.scrublet(adata, threshold=0.1). ```. ```pytb. WARNING: In Scanpy 0.*, this returned logarithmized data. Now it returns non-logarithmized data. ... storing 'paul15_clusters' as categorical. /opt/conda/lib/python3.7/site-packages/pandas/core/arrays/categorical.py:2487: FutureWarning: The `inplace` parameter in pandas.Categorical.remove_unused_categories is deprecated and will be removed in a future version. res = method(*args, **kwargs). Trying to set attribute `.uns` of view, copying. /opt/conda/lib/python3.7/site-packages/scanpy/preprocessing/_normalization.py:138: UserWarning: Revieved a view of an AnnData. Making a copy. view_to_actual(adata). ---------------------------------------------------------------------------. AttributeError Traceback (most recent call last). <ipython-input-21-3dabe52b6132> in <module>. 1 import scanpy as sc. 2 adata = sc.datasets.paul15(). ----> 3 sc.external.pp.scrublet(adata, threshold=0.1). /opt/conda/lib/python3.7/site-packages/scanpy/external/pp/_scrublet.py in scrublet(adata, adata_sim, sim_doublet_ratio, expected_doublet_rate, stdev_doublet_rate, synthetic_doublet_umi_subsampling, knn_dist_metric, normalize_variance, log_transform, mean_center, n_prin_comps, use_approx_neighbors, get_doublet_neighbor_parents, n_neighbors, threshold, verbose, copy, random_state). 208 expected_doublet_rate=expected_doublet_rate,. 209 stdev_doublet_rate=stdev_doublet_rate,. --> 210 random_state=random_state,. 211 ). 212 . /opt/conda/lib/python3.7/site-packages/scanpy/external/pp/_scrublet.py in _scrublet_call_doublets(adata_obs, adata_sim, n_neigh",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1645
https://github.com/scverse/scanpy/issues/1645:495,testability,log,logarithmized,495,"scrublet crashes on dense input; - [x] I have checked that this issue has not already been reported. - [x] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. ### Minimal code sample (that we can copy&paste without having any data). ```python. import scanpy as sc. adata = sc.datasets.paul15(). sc.external.pp.scrublet(adata, threshold=0.1). ```. ```pytb. WARNING: In Scanpy 0.*, this returned logarithmized data. Now it returns non-logarithmized data. ... storing 'paul15_clusters' as categorical. /opt/conda/lib/python3.7/site-packages/pandas/core/arrays/categorical.py:2487: FutureWarning: The `inplace` parameter in pandas.Categorical.remove_unused_categories is deprecated and will be removed in a future version. res = method(*args, **kwargs). Trying to set attribute `.uns` of view, copying. /opt/conda/lib/python3.7/site-packages/scanpy/preprocessing/_normalization.py:138: UserWarning: Revieved a view of an AnnData. Making a copy. view_to_actual(adata). ---------------------------------------------------------------------------. AttributeError Traceback (most recent call last). <ipython-input-21-3dabe52b6132> in <module>. 1 import scanpy as sc. 2 adata = sc.datasets.paul15(). ----> 3 sc.external.pp.scrublet(adata, threshold=0.1). /opt/conda/lib/python3.7/site-packages/scanpy/external/pp/_scrublet.py in scrublet(adata, adata_sim, sim_doublet_ratio, expected_doublet_rate, stdev_doublet_rate, synthetic_doublet_umi_subsampling, knn_dist_metric, normalize_variance, log_transform, mean_center, n_prin_comps, use_approx_neighbors, get_doublet_neighbor_parents, n_neighbors, threshold, verbose, copy, random_state). 208 expected_doublet_rate=expected_doublet_rate,. 209 stdev_doublet_rate=stdev_doublet_rate,. --> 210 random_state=random_state,. 211 ). 212 . /opt/conda/lib/python3.7/site-packages/scanpy/external/pp/_scrublet.py in _scrublet_call_doublets(adata_obs, adata_sim, n_neigh",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1645
https://github.com/scverse/scanpy/issues/1645:534,testability,log,logarithmized,534,"scrublet crashes on dense input; - [x] I have checked that this issue has not already been reported. - [x] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. ### Minimal code sample (that we can copy&paste without having any data). ```python. import scanpy as sc. adata = sc.datasets.paul15(). sc.external.pp.scrublet(adata, threshold=0.1). ```. ```pytb. WARNING: In Scanpy 0.*, this returned logarithmized data. Now it returns non-logarithmized data. ... storing 'paul15_clusters' as categorical. /opt/conda/lib/python3.7/site-packages/pandas/core/arrays/categorical.py:2487: FutureWarning: The `inplace` parameter in pandas.Categorical.remove_unused_categories is deprecated and will be removed in a future version. res = method(*args, **kwargs). Trying to set attribute `.uns` of view, copying. /opt/conda/lib/python3.7/site-packages/scanpy/preprocessing/_normalization.py:138: UserWarning: Revieved a view of an AnnData. Making a copy. view_to_actual(adata). ---------------------------------------------------------------------------. AttributeError Traceback (most recent call last). <ipython-input-21-3dabe52b6132> in <module>. 1 import scanpy as sc. 2 adata = sc.datasets.paul15(). ----> 3 sc.external.pp.scrublet(adata, threshold=0.1). /opt/conda/lib/python3.7/site-packages/scanpy/external/pp/_scrublet.py in scrublet(adata, adata_sim, sim_doublet_ratio, expected_doublet_rate, stdev_doublet_rate, synthetic_doublet_umi_subsampling, knn_dist_metric, normalize_variance, log_transform, mean_center, n_prin_comps, use_approx_neighbors, get_doublet_neighbor_parents, n_neighbors, threshold, verbose, copy, random_state). 208 expected_doublet_rate=expected_doublet_rate,. 209 stdev_doublet_rate=stdev_doublet_rate,. --> 210 random_state=random_state,. 211 ). 212 . /opt/conda/lib/python3.7/site-packages/scanpy/external/pp/_scrublet.py in _scrublet_call_doublets(adata_obs, adata_sim, n_neigh",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1645
https://github.com/scverse/scanpy/issues/1645:1157,testability,Trace,Traceback,1157,"of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. ### Minimal code sample (that we can copy&paste without having any data). ```python. import scanpy as sc. adata = sc.datasets.paul15(). sc.external.pp.scrublet(adata, threshold=0.1). ```. ```pytb. WARNING: In Scanpy 0.*, this returned logarithmized data. Now it returns non-logarithmized data. ... storing 'paul15_clusters' as categorical. /opt/conda/lib/python3.7/site-packages/pandas/core/arrays/categorical.py:2487: FutureWarning: The `inplace` parameter in pandas.Categorical.remove_unused_categories is deprecated and will be removed in a future version. res = method(*args, **kwargs). Trying to set attribute `.uns` of view, copying. /opt/conda/lib/python3.7/site-packages/scanpy/preprocessing/_normalization.py:138: UserWarning: Revieved a view of an AnnData. Making a copy. view_to_actual(adata). ---------------------------------------------------------------------------. AttributeError Traceback (most recent call last). <ipython-input-21-3dabe52b6132> in <module>. 1 import scanpy as sc. 2 adata = sc.datasets.paul15(). ----> 3 sc.external.pp.scrublet(adata, threshold=0.1). /opt/conda/lib/python3.7/site-packages/scanpy/external/pp/_scrublet.py in scrublet(adata, adata_sim, sim_doublet_ratio, expected_doublet_rate, stdev_doublet_rate, synthetic_doublet_umi_subsampling, knn_dist_metric, normalize_variance, log_transform, mean_center, n_prin_comps, use_approx_neighbors, get_doublet_neighbor_parents, n_neighbors, threshold, verbose, copy, random_state). 208 expected_doublet_rate=expected_doublet_rate,. 209 stdev_doublet_rate=stdev_doublet_rate,. --> 210 random_state=random_state,. 211 ). 212 . /opt/conda/lib/python3.7/site-packages/scanpy/external/pp/_scrublet.py in _scrublet_call_doublets(adata_obs, adata_sim, n_neighbors, expected_doublet_rate, stdev_doublet_rate, mean_center, normalize_variance, n_prin_comps, use_approx_neighbors, knn_dist_metric, get_doublet_neighbor_parent",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1645
https://github.com/scverse/scanpy/issues/1645:26,usability,input,input,26,"scrublet crashes on dense input; - [x] I have checked that this issue has not already been reported. - [x] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. ### Minimal code sample (that we can copy&paste without having any data). ```python. import scanpy as sc. adata = sc.datasets.paul15(). sc.external.pp.scrublet(adata, threshold=0.1). ```. ```pytb. WARNING: In Scanpy 0.*, this returned logarithmized data. Now it returns non-logarithmized data. ... storing 'paul15_clusters' as categorical. /opt/conda/lib/python3.7/site-packages/pandas/core/arrays/categorical.py:2487: FutureWarning: The `inplace` parameter in pandas.Categorical.remove_unused_categories is deprecated and will be removed in a future version. res = method(*args, **kwargs). Trying to set attribute `.uns` of view, copying. /opt/conda/lib/python3.7/site-packages/scanpy/preprocessing/_normalization.py:138: UserWarning: Revieved a view of an AnnData. Making a copy. view_to_actual(adata). ---------------------------------------------------------------------------. AttributeError Traceback (most recent call last). <ipython-input-21-3dabe52b6132> in <module>. 1 import scanpy as sc. 2 adata = sc.datasets.paul15(). ----> 3 sc.external.pp.scrublet(adata, threshold=0.1). /opt/conda/lib/python3.7/site-packages/scanpy/external/pp/_scrublet.py in scrublet(adata, adata_sim, sim_doublet_ratio, expected_doublet_rate, stdev_doublet_rate, synthetic_doublet_umi_subsampling, knn_dist_metric, normalize_variance, log_transform, mean_center, n_prin_comps, use_approx_neighbors, get_doublet_neighbor_parents, n_neighbors, threshold, verbose, copy, random_state). 208 expected_doublet_rate=expected_doublet_rate,. 209 stdev_doublet_rate=stdev_doublet_rate,. --> 210 random_state=random_state,. 211 ). 212 . /opt/conda/lib/python3.7/site-packages/scanpy/external/pp/_scrublet.py in _scrublet_call_doublets(adata_obs, adata_sim, n_neigh",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1645
https://github.com/scverse/scanpy/issues/1645:114,usability,confirm,confirmed,114,"scrublet crashes on dense input; - [x] I have checked that this issue has not already been reported. - [x] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. ### Minimal code sample (that we can copy&paste without having any data). ```python. import scanpy as sc. adata = sc.datasets.paul15(). sc.external.pp.scrublet(adata, threshold=0.1). ```. ```pytb. WARNING: In Scanpy 0.*, this returned logarithmized data. Now it returns non-logarithmized data. ... storing 'paul15_clusters' as categorical. /opt/conda/lib/python3.7/site-packages/pandas/core/arrays/categorical.py:2487: FutureWarning: The `inplace` parameter in pandas.Categorical.remove_unused_categories is deprecated and will be removed in a future version. res = method(*args, **kwargs). Trying to set attribute `.uns` of view, copying. /opt/conda/lib/python3.7/site-packages/scanpy/preprocessing/_normalization.py:138: UserWarning: Revieved a view of an AnnData. Making a copy. view_to_actual(adata). ---------------------------------------------------------------------------. AttributeError Traceback (most recent call last). <ipython-input-21-3dabe52b6132> in <module>. 1 import scanpy as sc. 2 adata = sc.datasets.paul15(). ----> 3 sc.external.pp.scrublet(adata, threshold=0.1). /opt/conda/lib/python3.7/site-packages/scanpy/external/pp/_scrublet.py in scrublet(adata, adata_sim, sim_doublet_ratio, expected_doublet_rate, stdev_doublet_rate, synthetic_doublet_umi_subsampling, knn_dist_metric, normalize_variance, log_transform, mean_center, n_prin_comps, use_approx_neighbors, get_doublet_neighbor_parents, n_neighbors, threshold, verbose, copy, random_state). 208 expected_doublet_rate=expected_doublet_rate,. 209 stdev_doublet_rate=stdev_doublet_rate,. --> 210 random_state=random_state,. 211 ). 212 . /opt/conda/lib/python3.7/site-packages/scanpy/external/pp/_scrublet.py in _scrublet_call_doublets(adata_obs, adata_sim, n_neigh",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1645
https://github.com/scverse/scanpy/issues/1645:197,usability,confirm,confirmed,197,"scrublet crashes on dense input; - [x] I have checked that this issue has not already been reported. - [x] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. ### Minimal code sample (that we can copy&paste without having any data). ```python. import scanpy as sc. adata = sc.datasets.paul15(). sc.external.pp.scrublet(adata, threshold=0.1). ```. ```pytb. WARNING: In Scanpy 0.*, this returned logarithmized data. Now it returns non-logarithmized data. ... storing 'paul15_clusters' as categorical. /opt/conda/lib/python3.7/site-packages/pandas/core/arrays/categorical.py:2487: FutureWarning: The `inplace` parameter in pandas.Categorical.remove_unused_categories is deprecated and will be removed in a future version. res = method(*args, **kwargs). Trying to set attribute `.uns` of view, copying. /opt/conda/lib/python3.7/site-packages/scanpy/preprocessing/_normalization.py:138: UserWarning: Revieved a view of an AnnData. Making a copy. view_to_actual(adata). ---------------------------------------------------------------------------. AttributeError Traceback (most recent call last). <ipython-input-21-3dabe52b6132> in <module>. 1 import scanpy as sc. 2 adata = sc.datasets.paul15(). ----> 3 sc.external.pp.scrublet(adata, threshold=0.1). /opt/conda/lib/python3.7/site-packages/scanpy/external/pp/_scrublet.py in scrublet(adata, adata_sim, sim_doublet_ratio, expected_doublet_rate, stdev_doublet_rate, synthetic_doublet_umi_subsampling, knn_dist_metric, normalize_variance, log_transform, mean_center, n_prin_comps, use_approx_neighbors, get_doublet_neighbor_parents, n_neighbors, threshold, verbose, copy, random_state). 208 expected_doublet_rate=expected_doublet_rate,. 209 stdev_doublet_rate=stdev_doublet_rate,. --> 210 random_state=random_state,. 211 ). 212 . /opt/conda/lib/python3.7/site-packages/scanpy/external/pp/_scrublet.py in _scrublet_call_doublets(adata_obs, adata_sim, n_neigh",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1645
https://github.com/scverse/scanpy/issues/1645:264,usability,Minim,Minimal,264,"scrublet crashes on dense input; - [x] I have checked that this issue has not already been reported. - [x] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. ### Minimal code sample (that we can copy&paste without having any data). ```python. import scanpy as sc. adata = sc.datasets.paul15(). sc.external.pp.scrublet(adata, threshold=0.1). ```. ```pytb. WARNING: In Scanpy 0.*, this returned logarithmized data. Now it returns non-logarithmized data. ... storing 'paul15_clusters' as categorical. /opt/conda/lib/python3.7/site-packages/pandas/core/arrays/categorical.py:2487: FutureWarning: The `inplace` parameter in pandas.Categorical.remove_unused_categories is deprecated and will be removed in a future version. res = method(*args, **kwargs). Trying to set attribute `.uns` of view, copying. /opt/conda/lib/python3.7/site-packages/scanpy/preprocessing/_normalization.py:138: UserWarning: Revieved a view of an AnnData. Making a copy. view_to_actual(adata). ---------------------------------------------------------------------------. AttributeError Traceback (most recent call last). <ipython-input-21-3dabe52b6132> in <module>. 1 import scanpy as sc. 2 adata = sc.datasets.paul15(). ----> 3 sc.external.pp.scrublet(adata, threshold=0.1). /opt/conda/lib/python3.7/site-packages/scanpy/external/pp/_scrublet.py in scrublet(adata, adata_sim, sim_doublet_ratio, expected_doublet_rate, stdev_doublet_rate, synthetic_doublet_umi_subsampling, knn_dist_metric, normalize_variance, log_transform, mean_center, n_prin_comps, use_approx_neighbors, get_doublet_neighbor_parents, n_neighbors, threshold, verbose, copy, random_state). 208 expected_doublet_rate=expected_doublet_rate,. 209 stdev_doublet_rate=stdev_doublet_rate,. --> 210 random_state=random_state,. 211 ). 212 . /opt/conda/lib/python3.7/site-packages/scanpy/external/pp/_scrublet.py in _scrublet_call_doublets(adata_obs, adata_sim, n_neigh",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1645
https://github.com/scverse/scanpy/issues/1645:983,usability,User,UserWarning,983,"scrublet crashes on dense input; - [x] I have checked that this issue has not already been reported. - [x] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. ### Minimal code sample (that we can copy&paste without having any data). ```python. import scanpy as sc. adata = sc.datasets.paul15(). sc.external.pp.scrublet(adata, threshold=0.1). ```. ```pytb. WARNING: In Scanpy 0.*, this returned logarithmized data. Now it returns non-logarithmized data. ... storing 'paul15_clusters' as categorical. /opt/conda/lib/python3.7/site-packages/pandas/core/arrays/categorical.py:2487: FutureWarning: The `inplace` parameter in pandas.Categorical.remove_unused_categories is deprecated and will be removed in a future version. res = method(*args, **kwargs). Trying to set attribute `.uns` of view, copying. /opt/conda/lib/python3.7/site-packages/scanpy/preprocessing/_normalization.py:138: UserWarning: Revieved a view of an AnnData. Making a copy. view_to_actual(adata). ---------------------------------------------------------------------------. AttributeError Traceback (most recent call last). <ipython-input-21-3dabe52b6132> in <module>. 1 import scanpy as sc. 2 adata = sc.datasets.paul15(). ----> 3 sc.external.pp.scrublet(adata, threshold=0.1). /opt/conda/lib/python3.7/site-packages/scanpy/external/pp/_scrublet.py in scrublet(adata, adata_sim, sim_doublet_ratio, expected_doublet_rate, stdev_doublet_rate, synthetic_doublet_umi_subsampling, knn_dist_metric, normalize_variance, log_transform, mean_center, n_prin_comps, use_approx_neighbors, get_doublet_neighbor_parents, n_neighbors, threshold, verbose, copy, random_state). 208 expected_doublet_rate=expected_doublet_rate,. 209 stdev_doublet_rate=stdev_doublet_rate,. --> 210 random_state=random_state,. 211 ). 212 . /opt/conda/lib/python3.7/site-packages/scanpy/external/pp/_scrublet.py in _scrublet_call_doublets(adata_obs, adata_sim, n_neigh",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1645
https://github.com/scverse/scanpy/issues/1645:1201,usability,input,input-,1201,"ed this bug exists on the master branch of scanpy. ---. ### Minimal code sample (that we can copy&paste without having any data). ```python. import scanpy as sc. adata = sc.datasets.paul15(). sc.external.pp.scrublet(adata, threshold=0.1). ```. ```pytb. WARNING: In Scanpy 0.*, this returned logarithmized data. Now it returns non-logarithmized data. ... storing 'paul15_clusters' as categorical. /opt/conda/lib/python3.7/site-packages/pandas/core/arrays/categorical.py:2487: FutureWarning: The `inplace` parameter in pandas.Categorical.remove_unused_categories is deprecated and will be removed in a future version. res = method(*args, **kwargs). Trying to set attribute `.uns` of view, copying. /opt/conda/lib/python3.7/site-packages/scanpy/preprocessing/_normalization.py:138: UserWarning: Revieved a view of an AnnData. Making a copy. view_to_actual(adata). ---------------------------------------------------------------------------. AttributeError Traceback (most recent call last). <ipython-input-21-3dabe52b6132> in <module>. 1 import scanpy as sc. 2 adata = sc.datasets.paul15(). ----> 3 sc.external.pp.scrublet(adata, threshold=0.1). /opt/conda/lib/python3.7/site-packages/scanpy/external/pp/_scrublet.py in scrublet(adata, adata_sim, sim_doublet_ratio, expected_doublet_rate, stdev_doublet_rate, synthetic_doublet_umi_subsampling, knn_dist_metric, normalize_variance, log_transform, mean_center, n_prin_comps, use_approx_neighbors, get_doublet_neighbor_parents, n_neighbors, threshold, verbose, copy, random_state). 208 expected_doublet_rate=expected_doublet_rate,. 209 stdev_doublet_rate=stdev_doublet_rate,. --> 210 random_state=random_state,. 211 ). 212 . /opt/conda/lib/python3.7/site-packages/scanpy/external/pp/_scrublet.py in _scrublet_call_doublets(adata_obs, adata_sim, n_neighbors, expected_doublet_rate, stdev_doublet_rate, mean_center, normalize_variance, n_prin_comps, use_approx_neighbors, knn_dist_metric, get_doublet_neighbor_parents, random_state, verbose). 349 . 350 if me",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1645
https://github.com/scverse/scanpy/issues/1645:3106,usability,learn,learn,3106,"t-21-3dabe52b6132> in <module>. 1 import scanpy as sc. 2 adata = sc.datasets.paul15(). ----> 3 sc.external.pp.scrublet(adata, threshold=0.1). /opt/conda/lib/python3.7/site-packages/scanpy/external/pp/_scrublet.py in scrublet(adata, adata_sim, sim_doublet_ratio, expected_doublet_rate, stdev_doublet_rate, synthetic_doublet_umi_subsampling, knn_dist_metric, normalize_variance, log_transform, mean_center, n_prin_comps, use_approx_neighbors, get_doublet_neighbor_parents, n_neighbors, threshold, verbose, copy, random_state). 208 expected_doublet_rate=expected_doublet_rate,. 209 stdev_doublet_rate=stdev_doublet_rate,. --> 210 random_state=random_state,. 211 ). 212 . /opt/conda/lib/python3.7/site-packages/scanpy/external/pp/_scrublet.py in _scrublet_call_doublets(adata_obs, adata_sim, n_neighbors, expected_doublet_rate, stdev_doublet_rate, mean_center, normalize_variance, n_prin_comps, use_approx_neighbors, knn_dist_metric, get_doublet_neighbor_parents, random_state, verbose). 349 . 350 if mean_center and normalize_variance:. --> 351 sl.pipeline_zscore(scrub). 352 elif mean_center:. 353 sl.pipeline_mean_center(scrub). /opt/conda/lib/python3.7/site-packages/scrublet/helper_functions.py in pipeline_zscore(self). 62 def pipeline_zscore(self):. 63 gene_means = self._E_obs_norm.mean(0). ---> 64 gene_stdevs = np.sqrt(sparse_var(self._E_obs_norm)). 65 self._E_obs_norm = np.array(sparse_zscore(self._E_obs_norm, gene_means, gene_stdevs)). 66 if self._E_sim_norm is not None:. /opt/conda/lib/python3.7/site-packages/scrublet/helper_functions.py in sparse_var(E, axis). 153 ''' variance across the specified axis '''. 154 . --> 155 mean_gene = E.mean(axis=axis).A.squeeze(). 156 tmp = E.copy(). 157 tmp.data **= 2. AttributeError: 'numpy.ndarray' object has no attribute 'A'. ```. #### Versions. <details>. scanpy==1.7.0 anndata==0.7.5 umap==0.5.1 numpy==1.20.0 scipy==1.6.0 pandas==1.2.1 scikit-learn==0.24.1 statsmodels==0.12.2 python-igraph==0.8.3 louvain==0.7.0 leidenalg==0.8.3. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1645
https://github.com/scverse/scanpy/issues/1646:1932,availability,operat,operator,1932,"aster branch of scanpy. ---. ### Minimal code sample (that we can copy&paste without having any data). ```python. import scanpy as sc. adata = sc.datasets.blobs(). sc.pp.pca(adata). adata.obs['boolean'] = True. sc.pl.pca(adata, color='boolean'). ```. ```pytb. ... storing 'blobs' as categorical. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-11-1415b8dea7b8> in <module>. 5 adata.obs['boolean'] = True. 6 . ----> 7 sc.pl.pca(adata, color='boolean'). /opt/conda/lib/python3.7/site-packages/scanpy/plotting/_tools/scatterplots.py in pca(adata, annotate_var_explained, show, return_fig, save, **kwargs). 727 if not annotate_var_explained:. 728 return embedding(. --> 729 adata, 'pca', show=show, return_fig=return_fig, save=save, **kwargs. 730 ). 731 else:. /opt/conda/lib/python3.7/site-packages/scanpy/plotting/_tools/scatterplots.py in embedding(adata, basis, color, gene_symbols, use_raw, sort_order, edges, edges_width, edges_color, neighbors_key, arrows, arrows_kwds, groups, components, layer, projection, scale_factor, color_map, cmap, palette, na_color, na_in_legend, size, frameon, legend_fontsize, legend_fontweight, legend_loc, legend_fontoutline, vmax, vmin, add_outline, outline_width, outline_color, ncols, hspace, wspace, title, show, save, ax, return_fig, **kwargs). 257 if sort_order is True and value_to_plot is not None and categorical is False:. 258 # Higher values plotted on top, null values on bottom. --> 259 order = np.argsort(-color_vector, kind=""stable"")[::-1]. 260 elif sort_order and categorical:. 261 # Null points go on bottom. TypeError: The numpy boolean negative, the `-` operator, is not supported, use the `~` operator or the logical_not function instead. ```. #### Versions. <details>. scanpy==1.7.0 anndata==0.7.5 umap==0.5.1 numpy==1.20.0 scipy==1.6.0 pandas==1.2.1 scikit-learn==0.24.1 statsmodels==0.12.2 python-igraph==0.8.3 louvain==0.7.0 leidenalg==0.8.3. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1646
https://github.com/scverse/scanpy/issues/1646:1972,availability,operat,operator,1972,"aster branch of scanpy. ---. ### Minimal code sample (that we can copy&paste without having any data). ```python. import scanpy as sc. adata = sc.datasets.blobs(). sc.pp.pca(adata). adata.obs['boolean'] = True. sc.pl.pca(adata, color='boolean'). ```. ```pytb. ... storing 'blobs' as categorical. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-11-1415b8dea7b8> in <module>. 5 adata.obs['boolean'] = True. 6 . ----> 7 sc.pl.pca(adata, color='boolean'). /opt/conda/lib/python3.7/site-packages/scanpy/plotting/_tools/scatterplots.py in pca(adata, annotate_var_explained, show, return_fig, save, **kwargs). 727 if not annotate_var_explained:. 728 return embedding(. --> 729 adata, 'pca', show=show, return_fig=return_fig, save=save, **kwargs. 730 ). 731 else:. /opt/conda/lib/python3.7/site-packages/scanpy/plotting/_tools/scatterplots.py in embedding(adata, basis, color, gene_symbols, use_raw, sort_order, edges, edges_width, edges_color, neighbors_key, arrows, arrows_kwds, groups, components, layer, projection, scale_factor, color_map, cmap, palette, na_color, na_in_legend, size, frameon, legend_fontsize, legend_fontweight, legend_loc, legend_fontoutline, vmax, vmin, add_outline, outline_width, outline_color, ncols, hspace, wspace, title, show, save, ax, return_fig, **kwargs). 257 if sort_order is True and value_to_plot is not None and categorical is False:. 258 # Higher values plotted on top, null values on bottom. --> 259 order = np.argsort(-color_vector, kind=""stable"")[::-1]. 260 elif sort_order and categorical:. 261 # Null points go on bottom. TypeError: The numpy boolean negative, the `-` operator, is not supported, use the `~` operator or the logical_not function instead. ```. #### Versions. <details>. scanpy==1.7.0 anndata==0.7.5 umap==0.5.1 numpy==1.20.0 scipy==1.6.0 pandas==1.2.1 scikit-learn==0.24.1 statsmodels==0.12.2 python-igraph==0.8.3 louvain==0.7.0 leidenalg==0.8.3. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1646
https://github.com/scverse/scanpy/issues/1646:160,deployability,version,version,160,"Support coloring by boolean variables; - [x] I have checked that this issue has not already been reported. - [x] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. ### Minimal code sample (that we can copy&paste without having any data). ```python. import scanpy as sc. adata = sc.datasets.blobs(). sc.pp.pca(adata). adata.obs['boolean'] = True. sc.pl.pca(adata, color='boolean'). ```. ```pytb. ... storing 'blobs' as categorical. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-11-1415b8dea7b8> in <module>. 5 adata.obs['boolean'] = True. 6 . ----> 7 sc.pl.pca(adata, color='boolean'). /opt/conda/lib/python3.7/site-packages/scanpy/plotting/_tools/scatterplots.py in pca(adata, annotate_var_explained, show, return_fig, save, **kwargs). 727 if not annotate_var_explained:. 728 return embedding(. --> 729 adata, 'pca', show=show, return_fig=return_fig, save=save, **kwargs. 730 ). 731 else:. /opt/conda/lib/python3.7/site-packages/scanpy/plotting/_tools/scatterplots.py in embedding(adata, basis, color, gene_symbols, use_raw, sort_order, edges, edges_width, edges_color, neighbors_key, arrows, arrows_kwds, groups, components, layer, projection, scale_factor, color_map, cmap, palette, na_color, na_in_legend, size, frameon, legend_fontsize, legend_fontweight, legend_loc, legend_fontoutline, vmax, vmin, add_outline, outline_width, outline_color, ncols, hspace, wspace, title, show, save, ax, return_fig, **kwargs). 257 if sort_order is True and value_to_plot is not None and categorical is False:. 258 # Higher values plotted on top, null values on bottom. --> 259 order = np.argsort(-color_vector, kind=""stable"")[::-1]. 260 elif sort_order and categorical:. 261 # Null points go on bottom. TypeError: The numpy boolean negative, the `-` operator, is not supported, use the `~` operator or the logical_not ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1646
https://github.com/scverse/scanpy/issues/1646:691,deployability,modul,module,691,"Support coloring by boolean variables; - [x] I have checked that this issue has not already been reported. - [x] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. ### Minimal code sample (that we can copy&paste without having any data). ```python. import scanpy as sc. adata = sc.datasets.blobs(). sc.pp.pca(adata). adata.obs['boolean'] = True. sc.pl.pca(adata, color='boolean'). ```. ```pytb. ... storing 'blobs' as categorical. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-11-1415b8dea7b8> in <module>. 5 adata.obs['boolean'] = True. 6 . ----> 7 sc.pl.pca(adata, color='boolean'). /opt/conda/lib/python3.7/site-packages/scanpy/plotting/_tools/scatterplots.py in pca(adata, annotate_var_explained, show, return_fig, save, **kwargs). 727 if not annotate_var_explained:. 728 return embedding(. --> 729 adata, 'pca', show=show, return_fig=return_fig, save=save, **kwargs. 730 ). 731 else:. /opt/conda/lib/python3.7/site-packages/scanpy/plotting/_tools/scatterplots.py in embedding(adata, basis, color, gene_symbols, use_raw, sort_order, edges, edges_width, edges_color, neighbors_key, arrows, arrows_kwds, groups, components, layer, projection, scale_factor, color_map, cmap, palette, na_color, na_in_legend, size, frameon, legend_fontsize, legend_fontweight, legend_loc, legend_fontoutline, vmax, vmin, add_outline, outline_width, outline_color, ncols, hspace, wspace, title, show, save, ax, return_fig, **kwargs). 257 if sort_order is True and value_to_plot is not None and categorical is False:. 258 # Higher values plotted on top, null values on bottom. --> 259 order = np.argsort(-color_vector, kind=""stable"")[::-1]. 260 elif sort_order and categorical:. 261 # Null points go on bottom. TypeError: The numpy boolean negative, the `-` operator, is not supported, use the `~` operator or the logical_not ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1646
https://github.com/scverse/scanpy/issues/1646:2028,deployability,Version,Versions,2028,"aster branch of scanpy. ---. ### Minimal code sample (that we can copy&paste without having any data). ```python. import scanpy as sc. adata = sc.datasets.blobs(). sc.pp.pca(adata). adata.obs['boolean'] = True. sc.pl.pca(adata, color='boolean'). ```. ```pytb. ... storing 'blobs' as categorical. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-11-1415b8dea7b8> in <module>. 5 adata.obs['boolean'] = True. 6 . ----> 7 sc.pl.pca(adata, color='boolean'). /opt/conda/lib/python3.7/site-packages/scanpy/plotting/_tools/scatterplots.py in pca(adata, annotate_var_explained, show, return_fig, save, **kwargs). 727 if not annotate_var_explained:. 728 return embedding(. --> 729 adata, 'pca', show=show, return_fig=return_fig, save=save, **kwargs. 730 ). 731 else:. /opt/conda/lib/python3.7/site-packages/scanpy/plotting/_tools/scatterplots.py in embedding(adata, basis, color, gene_symbols, use_raw, sort_order, edges, edges_width, edges_color, neighbors_key, arrows, arrows_kwds, groups, components, layer, projection, scale_factor, color_map, cmap, palette, na_color, na_in_legend, size, frameon, legend_fontsize, legend_fontweight, legend_loc, legend_fontoutline, vmax, vmin, add_outline, outline_width, outline_color, ncols, hspace, wspace, title, show, save, ax, return_fig, **kwargs). 257 if sort_order is True and value_to_plot is not None and categorical is False:. 258 # Higher values plotted on top, null values on bottom. --> 259 order = np.argsort(-color_vector, kind=""stable"")[::-1]. 260 elif sort_order and categorical:. 261 # Null points go on bottom. TypeError: The numpy boolean negative, the `-` operator, is not supported, use the `~` operator or the logical_not function instead. ```. #### Versions. <details>. scanpy==1.7.0 anndata==0.7.5 umap==0.5.1 numpy==1.20.0 scipy==1.6.0 pandas==1.2.1 scikit-learn==0.24.1 statsmodels==0.12.2 python-igraph==0.8.3 louvain==0.7.0 leidenalg==0.8.3. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1646
https://github.com/scverse/scanpy/issues/1646:160,integrability,version,version,160,"Support coloring by boolean variables; - [x] I have checked that this issue has not already been reported. - [x] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. ### Minimal code sample (that we can copy&paste without having any data). ```python. import scanpy as sc. adata = sc.datasets.blobs(). sc.pp.pca(adata). adata.obs['boolean'] = True. sc.pl.pca(adata, color='boolean'). ```. ```pytb. ... storing 'blobs' as categorical. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-11-1415b8dea7b8> in <module>. 5 adata.obs['boolean'] = True. 6 . ----> 7 sc.pl.pca(adata, color='boolean'). /opt/conda/lib/python3.7/site-packages/scanpy/plotting/_tools/scatterplots.py in pca(adata, annotate_var_explained, show, return_fig, save, **kwargs). 727 if not annotate_var_explained:. 728 return embedding(. --> 729 adata, 'pca', show=show, return_fig=return_fig, save=save, **kwargs. 730 ). 731 else:. /opt/conda/lib/python3.7/site-packages/scanpy/plotting/_tools/scatterplots.py in embedding(adata, basis, color, gene_symbols, use_raw, sort_order, edges, edges_width, edges_color, neighbors_key, arrows, arrows_kwds, groups, components, layer, projection, scale_factor, color_map, cmap, palette, na_color, na_in_legend, size, frameon, legend_fontsize, legend_fontweight, legend_loc, legend_fontoutline, vmax, vmin, add_outline, outline_width, outline_color, ncols, hspace, wspace, title, show, save, ax, return_fig, **kwargs). 257 if sort_order is True and value_to_plot is not None and categorical is False:. 258 # Higher values plotted on top, null values on bottom. --> 259 order = np.argsort(-color_vector, kind=""stable"")[::-1]. 260 elif sort_order and categorical:. 261 # Null points go on bottom. TypeError: The numpy boolean negative, the `-` operator, is not supported, use the `~` operator or the logical_not ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1646
https://github.com/scverse/scanpy/issues/1646:1307,integrability,compon,components,1307,"aster branch of scanpy. ---. ### Minimal code sample (that we can copy&paste without having any data). ```python. import scanpy as sc. adata = sc.datasets.blobs(). sc.pp.pca(adata). adata.obs['boolean'] = True. sc.pl.pca(adata, color='boolean'). ```. ```pytb. ... storing 'blobs' as categorical. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-11-1415b8dea7b8> in <module>. 5 adata.obs['boolean'] = True. 6 . ----> 7 sc.pl.pca(adata, color='boolean'). /opt/conda/lib/python3.7/site-packages/scanpy/plotting/_tools/scatterplots.py in pca(adata, annotate_var_explained, show, return_fig, save, **kwargs). 727 if not annotate_var_explained:. 728 return embedding(. --> 729 adata, 'pca', show=show, return_fig=return_fig, save=save, **kwargs. 730 ). 731 else:. /opt/conda/lib/python3.7/site-packages/scanpy/plotting/_tools/scatterplots.py in embedding(adata, basis, color, gene_symbols, use_raw, sort_order, edges, edges_width, edges_color, neighbors_key, arrows, arrows_kwds, groups, components, layer, projection, scale_factor, color_map, cmap, palette, na_color, na_in_legend, size, frameon, legend_fontsize, legend_fontweight, legend_loc, legend_fontoutline, vmax, vmin, add_outline, outline_width, outline_color, ncols, hspace, wspace, title, show, save, ax, return_fig, **kwargs). 257 if sort_order is True and value_to_plot is not None and categorical is False:. 258 # Higher values plotted on top, null values on bottom. --> 259 order = np.argsort(-color_vector, kind=""stable"")[::-1]. 260 elif sort_order and categorical:. 261 # Null points go on bottom. TypeError: The numpy boolean negative, the `-` operator, is not supported, use the `~` operator or the logical_not function instead. ```. #### Versions. <details>. scanpy==1.7.0 anndata==0.7.5 umap==0.5.1 numpy==1.20.0 scipy==1.6.0 pandas==1.2.1 scikit-learn==0.24.1 statsmodels==0.12.2 python-igraph==0.8.3 louvain==0.7.0 leidenalg==0.8.3. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1646
https://github.com/scverse/scanpy/issues/1646:2028,integrability,Version,Versions,2028,"aster branch of scanpy. ---. ### Minimal code sample (that we can copy&paste without having any data). ```python. import scanpy as sc. adata = sc.datasets.blobs(). sc.pp.pca(adata). adata.obs['boolean'] = True. sc.pl.pca(adata, color='boolean'). ```. ```pytb. ... storing 'blobs' as categorical. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-11-1415b8dea7b8> in <module>. 5 adata.obs['boolean'] = True. 6 . ----> 7 sc.pl.pca(adata, color='boolean'). /opt/conda/lib/python3.7/site-packages/scanpy/plotting/_tools/scatterplots.py in pca(adata, annotate_var_explained, show, return_fig, save, **kwargs). 727 if not annotate_var_explained:. 728 return embedding(. --> 729 adata, 'pca', show=show, return_fig=return_fig, save=save, **kwargs. 730 ). 731 else:. /opt/conda/lib/python3.7/site-packages/scanpy/plotting/_tools/scatterplots.py in embedding(adata, basis, color, gene_symbols, use_raw, sort_order, edges, edges_width, edges_color, neighbors_key, arrows, arrows_kwds, groups, components, layer, projection, scale_factor, color_map, cmap, palette, na_color, na_in_legend, size, frameon, legend_fontsize, legend_fontweight, legend_loc, legend_fontoutline, vmax, vmin, add_outline, outline_width, outline_color, ncols, hspace, wspace, title, show, save, ax, return_fig, **kwargs). 257 if sort_order is True and value_to_plot is not None and categorical is False:. 258 # Higher values plotted on top, null values on bottom. --> 259 order = np.argsort(-color_vector, kind=""stable"")[::-1]. 260 elif sort_order and categorical:. 261 # Null points go on bottom. TypeError: The numpy boolean negative, the `-` operator, is not supported, use the `~` operator or the logical_not function instead. ```. #### Versions. <details>. scanpy==1.7.0 anndata==0.7.5 umap==0.5.1 numpy==1.20.0 scipy==1.6.0 pandas==1.2.1 scikit-learn==0.24.1 statsmodels==0.12.2 python-igraph==0.8.3 louvain==0.7.0 leidenalg==0.8.3. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1646
https://github.com/scverse/scanpy/issues/1646:1307,interoperability,compon,components,1307,"aster branch of scanpy. ---. ### Minimal code sample (that we can copy&paste without having any data). ```python. import scanpy as sc. adata = sc.datasets.blobs(). sc.pp.pca(adata). adata.obs['boolean'] = True. sc.pl.pca(adata, color='boolean'). ```. ```pytb. ... storing 'blobs' as categorical. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-11-1415b8dea7b8> in <module>. 5 adata.obs['boolean'] = True. 6 . ----> 7 sc.pl.pca(adata, color='boolean'). /opt/conda/lib/python3.7/site-packages/scanpy/plotting/_tools/scatterplots.py in pca(adata, annotate_var_explained, show, return_fig, save, **kwargs). 727 if not annotate_var_explained:. 728 return embedding(. --> 729 adata, 'pca', show=show, return_fig=return_fig, save=save, **kwargs. 730 ). 731 else:. /opt/conda/lib/python3.7/site-packages/scanpy/plotting/_tools/scatterplots.py in embedding(adata, basis, color, gene_symbols, use_raw, sort_order, edges, edges_width, edges_color, neighbors_key, arrows, arrows_kwds, groups, components, layer, projection, scale_factor, color_map, cmap, palette, na_color, na_in_legend, size, frameon, legend_fontsize, legend_fontweight, legend_loc, legend_fontoutline, vmax, vmin, add_outline, outline_width, outline_color, ncols, hspace, wspace, title, show, save, ax, return_fig, **kwargs). 257 if sort_order is True and value_to_plot is not None and categorical is False:. 258 # Higher values plotted on top, null values on bottom. --> 259 order = np.argsort(-color_vector, kind=""stable"")[::-1]. 260 elif sort_order and categorical:. 261 # Null points go on bottom. TypeError: The numpy boolean negative, the `-` operator, is not supported, use the `~` operator or the logical_not function instead. ```. #### Versions. <details>. scanpy==1.7.0 anndata==0.7.5 umap==0.5.1 numpy==1.20.0 scipy==1.6.0 pandas==1.2.1 scikit-learn==0.24.1 statsmodels==0.12.2 python-igraph==0.8.3 louvain==0.7.0 leidenalg==0.8.3. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1646
https://github.com/scverse/scanpy/issues/1646:28,modifiability,variab,variables,28,"Support coloring by boolean variables; - [x] I have checked that this issue has not already been reported. - [x] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. ### Minimal code sample (that we can copy&paste without having any data). ```python. import scanpy as sc. adata = sc.datasets.blobs(). sc.pp.pca(adata). adata.obs['boolean'] = True. sc.pl.pca(adata, color='boolean'). ```. ```pytb. ... storing 'blobs' as categorical. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-11-1415b8dea7b8> in <module>. 5 adata.obs['boolean'] = True. 6 . ----> 7 sc.pl.pca(adata, color='boolean'). /opt/conda/lib/python3.7/site-packages/scanpy/plotting/_tools/scatterplots.py in pca(adata, annotate_var_explained, show, return_fig, save, **kwargs). 727 if not annotate_var_explained:. 728 return embedding(. --> 729 adata, 'pca', show=show, return_fig=return_fig, save=save, **kwargs. 730 ). 731 else:. /opt/conda/lib/python3.7/site-packages/scanpy/plotting/_tools/scatterplots.py in embedding(adata, basis, color, gene_symbols, use_raw, sort_order, edges, edges_width, edges_color, neighbors_key, arrows, arrows_kwds, groups, components, layer, projection, scale_factor, color_map, cmap, palette, na_color, na_in_legend, size, frameon, legend_fontsize, legend_fontweight, legend_loc, legend_fontoutline, vmax, vmin, add_outline, outline_width, outline_color, ncols, hspace, wspace, title, show, save, ax, return_fig, **kwargs). 257 if sort_order is True and value_to_plot is not None and categorical is False:. 258 # Higher values plotted on top, null values on bottom. --> 259 order = np.argsort(-color_vector, kind=""stable"")[::-1]. 260 elif sort_order and categorical:. 261 # Null points go on bottom. TypeError: The numpy boolean negative, the `-` operator, is not supported, use the `~` operator or the logical_not ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1646
https://github.com/scverse/scanpy/issues/1646:160,modifiability,version,version,160,"Support coloring by boolean variables; - [x] I have checked that this issue has not already been reported. - [x] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. ### Minimal code sample (that we can copy&paste without having any data). ```python. import scanpy as sc. adata = sc.datasets.blobs(). sc.pp.pca(adata). adata.obs['boolean'] = True. sc.pl.pca(adata, color='boolean'). ```. ```pytb. ... storing 'blobs' as categorical. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-11-1415b8dea7b8> in <module>. 5 adata.obs['boolean'] = True. 6 . ----> 7 sc.pl.pca(adata, color='boolean'). /opt/conda/lib/python3.7/site-packages/scanpy/plotting/_tools/scatterplots.py in pca(adata, annotate_var_explained, show, return_fig, save, **kwargs). 727 if not annotate_var_explained:. 728 return embedding(. --> 729 adata, 'pca', show=show, return_fig=return_fig, save=save, **kwargs. 730 ). 731 else:. /opt/conda/lib/python3.7/site-packages/scanpy/plotting/_tools/scatterplots.py in embedding(adata, basis, color, gene_symbols, use_raw, sort_order, edges, edges_width, edges_color, neighbors_key, arrows, arrows_kwds, groups, components, layer, projection, scale_factor, color_map, cmap, palette, na_color, na_in_legend, size, frameon, legend_fontsize, legend_fontweight, legend_loc, legend_fontoutline, vmax, vmin, add_outline, outline_width, outline_color, ncols, hspace, wspace, title, show, save, ax, return_fig, **kwargs). 257 if sort_order is True and value_to_plot is not None and categorical is False:. 258 # Higher values plotted on top, null values on bottom. --> 259 order = np.argsort(-color_vector, kind=""stable"")[::-1]. 260 elif sort_order and categorical:. 261 # Null points go on bottom. TypeError: The numpy boolean negative, the `-` operator, is not supported, use the `~` operator or the logical_not ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1646
https://github.com/scverse/scanpy/issues/1646:691,modifiability,modul,module,691,"Support coloring by boolean variables; - [x] I have checked that this issue has not already been reported. - [x] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. ### Minimal code sample (that we can copy&paste without having any data). ```python. import scanpy as sc. adata = sc.datasets.blobs(). sc.pp.pca(adata). adata.obs['boolean'] = True. sc.pl.pca(adata, color='boolean'). ```. ```pytb. ... storing 'blobs' as categorical. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-11-1415b8dea7b8> in <module>. 5 adata.obs['boolean'] = True. 6 . ----> 7 sc.pl.pca(adata, color='boolean'). /opt/conda/lib/python3.7/site-packages/scanpy/plotting/_tools/scatterplots.py in pca(adata, annotate_var_explained, show, return_fig, save, **kwargs). 727 if not annotate_var_explained:. 728 return embedding(. --> 729 adata, 'pca', show=show, return_fig=return_fig, save=save, **kwargs. 730 ). 731 else:. /opt/conda/lib/python3.7/site-packages/scanpy/plotting/_tools/scatterplots.py in embedding(adata, basis, color, gene_symbols, use_raw, sort_order, edges, edges_width, edges_color, neighbors_key, arrows, arrows_kwds, groups, components, layer, projection, scale_factor, color_map, cmap, palette, na_color, na_in_legend, size, frameon, legend_fontsize, legend_fontweight, legend_loc, legend_fontoutline, vmax, vmin, add_outline, outline_width, outline_color, ncols, hspace, wspace, title, show, save, ax, return_fig, **kwargs). 257 if sort_order is True and value_to_plot is not None and categorical is False:. 258 # Higher values plotted on top, null values on bottom. --> 259 order = np.argsort(-color_vector, kind=""stable"")[::-1]. 260 elif sort_order and categorical:. 261 # Null points go on bottom. TypeError: The numpy boolean negative, the `-` operator, is not supported, use the `~` operator or the logical_not ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1646
https://github.com/scverse/scanpy/issues/1646:808,modifiability,pac,packages,808,"Support coloring by boolean variables; - [x] I have checked that this issue has not already been reported. - [x] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. ### Minimal code sample (that we can copy&paste without having any data). ```python. import scanpy as sc. adata = sc.datasets.blobs(). sc.pp.pca(adata). adata.obs['boolean'] = True. sc.pl.pca(adata, color='boolean'). ```. ```pytb. ... storing 'blobs' as categorical. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-11-1415b8dea7b8> in <module>. 5 adata.obs['boolean'] = True. 6 . ----> 7 sc.pl.pca(adata, color='boolean'). /opt/conda/lib/python3.7/site-packages/scanpy/plotting/_tools/scatterplots.py in pca(adata, annotate_var_explained, show, return_fig, save, **kwargs). 727 if not annotate_var_explained:. 728 return embedding(. --> 729 adata, 'pca', show=show, return_fig=return_fig, save=save, **kwargs. 730 ). 731 else:. /opt/conda/lib/python3.7/site-packages/scanpy/plotting/_tools/scatterplots.py in embedding(adata, basis, color, gene_symbols, use_raw, sort_order, edges, edges_width, edges_color, neighbors_key, arrows, arrows_kwds, groups, components, layer, projection, scale_factor, color_map, cmap, palette, na_color, na_in_legend, size, frameon, legend_fontsize, legend_fontweight, legend_loc, legend_fontoutline, vmax, vmin, add_outline, outline_width, outline_color, ncols, hspace, wspace, title, show, save, ax, return_fig, **kwargs). 257 if sort_order is True and value_to_plot is not None and categorical is False:. 258 # Higher values plotted on top, null values on bottom. --> 259 order = np.argsort(-color_vector, kind=""stable"")[::-1]. 260 elif sort_order and categorical:. 261 # Null points go on bottom. TypeError: The numpy boolean negative, the `-` operator, is not supported, use the `~` operator or the logical_not ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1646
https://github.com/scverse/scanpy/issues/1646:1113,modifiability,pac,packages,1113,"ve confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. ### Minimal code sample (that we can copy&paste without having any data). ```python. import scanpy as sc. adata = sc.datasets.blobs(). sc.pp.pca(adata). adata.obs['boolean'] = True. sc.pl.pca(adata, color='boolean'). ```. ```pytb. ... storing 'blobs' as categorical. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-11-1415b8dea7b8> in <module>. 5 adata.obs['boolean'] = True. 6 . ----> 7 sc.pl.pca(adata, color='boolean'). /opt/conda/lib/python3.7/site-packages/scanpy/plotting/_tools/scatterplots.py in pca(adata, annotate_var_explained, show, return_fig, save, **kwargs). 727 if not annotate_var_explained:. 728 return embedding(. --> 729 adata, 'pca', show=show, return_fig=return_fig, save=save, **kwargs. 730 ). 731 else:. /opt/conda/lib/python3.7/site-packages/scanpy/plotting/_tools/scatterplots.py in embedding(adata, basis, color, gene_symbols, use_raw, sort_order, edges, edges_width, edges_color, neighbors_key, arrows, arrows_kwds, groups, components, layer, projection, scale_factor, color_map, cmap, palette, na_color, na_in_legend, size, frameon, legend_fontsize, legend_fontweight, legend_loc, legend_fontoutline, vmax, vmin, add_outline, outline_width, outline_color, ncols, hspace, wspace, title, show, save, ax, return_fig, **kwargs). 257 if sort_order is True and value_to_plot is not None and categorical is False:. 258 # Higher values plotted on top, null values on bottom. --> 259 order = np.argsort(-color_vector, kind=""stable"")[::-1]. 260 elif sort_order and categorical:. 261 # Null points go on bottom. TypeError: The numpy boolean negative, the `-` operator, is not supported, use the `~` operator or the logical_not function instead. ```. #### Versions. <details>. scanpy==1.7.0 anndata==0.7.5 umap==0.5.1 numpy==1.20.0 scipy==1.6.0 ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1646
https://github.com/scverse/scanpy/issues/1646:1307,modifiability,compon,components,1307,"aster branch of scanpy. ---. ### Minimal code sample (that we can copy&paste without having any data). ```python. import scanpy as sc. adata = sc.datasets.blobs(). sc.pp.pca(adata). adata.obs['boolean'] = True. sc.pl.pca(adata, color='boolean'). ```. ```pytb. ... storing 'blobs' as categorical. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-11-1415b8dea7b8> in <module>. 5 adata.obs['boolean'] = True. 6 . ----> 7 sc.pl.pca(adata, color='boolean'). /opt/conda/lib/python3.7/site-packages/scanpy/plotting/_tools/scatterplots.py in pca(adata, annotate_var_explained, show, return_fig, save, **kwargs). 727 if not annotate_var_explained:. 728 return embedding(. --> 729 adata, 'pca', show=show, return_fig=return_fig, save=save, **kwargs. 730 ). 731 else:. /opt/conda/lib/python3.7/site-packages/scanpy/plotting/_tools/scatterplots.py in embedding(adata, basis, color, gene_symbols, use_raw, sort_order, edges, edges_width, edges_color, neighbors_key, arrows, arrows_kwds, groups, components, layer, projection, scale_factor, color_map, cmap, palette, na_color, na_in_legend, size, frameon, legend_fontsize, legend_fontweight, legend_loc, legend_fontoutline, vmax, vmin, add_outline, outline_width, outline_color, ncols, hspace, wspace, title, show, save, ax, return_fig, **kwargs). 257 if sort_order is True and value_to_plot is not None and categorical is False:. 258 # Higher values plotted on top, null values on bottom. --> 259 order = np.argsort(-color_vector, kind=""stable"")[::-1]. 260 elif sort_order and categorical:. 261 # Null points go on bottom. TypeError: The numpy boolean negative, the `-` operator, is not supported, use the `~` operator or the logical_not function instead. ```. #### Versions. <details>. scanpy==1.7.0 anndata==0.7.5 umap==0.5.1 numpy==1.20.0 scipy==1.6.0 pandas==1.2.1 scikit-learn==0.24.1 statsmodels==0.12.2 python-igraph==0.8.3 louvain==0.7.0 leidenalg==0.8.3. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1646
https://github.com/scverse/scanpy/issues/1646:1319,modifiability,layer,layer,1319,"aster branch of scanpy. ---. ### Minimal code sample (that we can copy&paste without having any data). ```python. import scanpy as sc. adata = sc.datasets.blobs(). sc.pp.pca(adata). adata.obs['boolean'] = True. sc.pl.pca(adata, color='boolean'). ```. ```pytb. ... storing 'blobs' as categorical. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-11-1415b8dea7b8> in <module>. 5 adata.obs['boolean'] = True. 6 . ----> 7 sc.pl.pca(adata, color='boolean'). /opt/conda/lib/python3.7/site-packages/scanpy/plotting/_tools/scatterplots.py in pca(adata, annotate_var_explained, show, return_fig, save, **kwargs). 727 if not annotate_var_explained:. 728 return embedding(. --> 729 adata, 'pca', show=show, return_fig=return_fig, save=save, **kwargs. 730 ). 731 else:. /opt/conda/lib/python3.7/site-packages/scanpy/plotting/_tools/scatterplots.py in embedding(adata, basis, color, gene_symbols, use_raw, sort_order, edges, edges_width, edges_color, neighbors_key, arrows, arrows_kwds, groups, components, layer, projection, scale_factor, color_map, cmap, palette, na_color, na_in_legend, size, frameon, legend_fontsize, legend_fontweight, legend_loc, legend_fontoutline, vmax, vmin, add_outline, outline_width, outline_color, ncols, hspace, wspace, title, show, save, ax, return_fig, **kwargs). 257 if sort_order is True and value_to_plot is not None and categorical is False:. 258 # Higher values plotted on top, null values on bottom. --> 259 order = np.argsort(-color_vector, kind=""stable"")[::-1]. 260 elif sort_order and categorical:. 261 # Null points go on bottom. TypeError: The numpy boolean negative, the `-` operator, is not supported, use the `~` operator or the logical_not function instead. ```. #### Versions. <details>. scanpy==1.7.0 anndata==0.7.5 umap==0.5.1 numpy==1.20.0 scipy==1.6.0 pandas==1.2.1 scikit-learn==0.24.1 statsmodels==0.12.2 python-igraph==0.8.3 louvain==0.7.0 leidenalg==0.8.3. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1646
https://github.com/scverse/scanpy/issues/1646:2028,modifiability,Version,Versions,2028,"aster branch of scanpy. ---. ### Minimal code sample (that we can copy&paste without having any data). ```python. import scanpy as sc. adata = sc.datasets.blobs(). sc.pp.pca(adata). adata.obs['boolean'] = True. sc.pl.pca(adata, color='boolean'). ```. ```pytb. ... storing 'blobs' as categorical. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-11-1415b8dea7b8> in <module>. 5 adata.obs['boolean'] = True. 6 . ----> 7 sc.pl.pca(adata, color='boolean'). /opt/conda/lib/python3.7/site-packages/scanpy/plotting/_tools/scatterplots.py in pca(adata, annotate_var_explained, show, return_fig, save, **kwargs). 727 if not annotate_var_explained:. 728 return embedding(. --> 729 adata, 'pca', show=show, return_fig=return_fig, save=save, **kwargs. 730 ). 731 else:. /opt/conda/lib/python3.7/site-packages/scanpy/plotting/_tools/scatterplots.py in embedding(adata, basis, color, gene_symbols, use_raw, sort_order, edges, edges_width, edges_color, neighbors_key, arrows, arrows_kwds, groups, components, layer, projection, scale_factor, color_map, cmap, palette, na_color, na_in_legend, size, frameon, legend_fontsize, legend_fontweight, legend_loc, legend_fontoutline, vmax, vmin, add_outline, outline_width, outline_color, ncols, hspace, wspace, title, show, save, ax, return_fig, **kwargs). 257 if sort_order is True and value_to_plot is not None and categorical is False:. 258 # Higher values plotted on top, null values on bottom. --> 259 order = np.argsort(-color_vector, kind=""stable"")[::-1]. 260 elif sort_order and categorical:. 261 # Null points go on bottom. TypeError: The numpy boolean negative, the `-` operator, is not supported, use the `~` operator or the logical_not function instead. ```. #### Versions. <details>. scanpy==1.7.0 anndata==0.7.5 umap==0.5.1 numpy==1.20.0 scipy==1.6.0 pandas==1.2.1 scikit-learn==0.24.1 statsmodels==0.12.2 python-igraph==0.8.3 louvain==0.7.0 leidenalg==0.8.3. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1646
https://github.com/scverse/scanpy/issues/1646:664,safety,input,input-,664,"Support coloring by boolean variables; - [x] I have checked that this issue has not already been reported. - [x] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. ### Minimal code sample (that we can copy&paste without having any data). ```python. import scanpy as sc. adata = sc.datasets.blobs(). sc.pp.pca(adata). adata.obs['boolean'] = True. sc.pl.pca(adata, color='boolean'). ```. ```pytb. ... storing 'blobs' as categorical. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-11-1415b8dea7b8> in <module>. 5 adata.obs['boolean'] = True. 6 . ----> 7 sc.pl.pca(adata, color='boolean'). /opt/conda/lib/python3.7/site-packages/scanpy/plotting/_tools/scatterplots.py in pca(adata, annotate_var_explained, show, return_fig, save, **kwargs). 727 if not annotate_var_explained:. 728 return embedding(. --> 729 adata, 'pca', show=show, return_fig=return_fig, save=save, **kwargs. 730 ). 731 else:. /opt/conda/lib/python3.7/site-packages/scanpy/plotting/_tools/scatterplots.py in embedding(adata, basis, color, gene_symbols, use_raw, sort_order, edges, edges_width, edges_color, neighbors_key, arrows, arrows_kwds, groups, components, layer, projection, scale_factor, color_map, cmap, palette, na_color, na_in_legend, size, frameon, legend_fontsize, legend_fontweight, legend_loc, legend_fontoutline, vmax, vmin, add_outline, outline_width, outline_color, ncols, hspace, wspace, title, show, save, ax, return_fig, **kwargs). 257 if sort_order is True and value_to_plot is not None and categorical is False:. 258 # Higher values plotted on top, null values on bottom. --> 259 order = np.argsort(-color_vector, kind=""stable"")[::-1]. 260 elif sort_order and categorical:. 261 # Null points go on bottom. TypeError: The numpy boolean negative, the `-` operator, is not supported, use the `~` operator or the logical_not ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1646
https://github.com/scverse/scanpy/issues/1646:691,safety,modul,module,691,"Support coloring by boolean variables; - [x] I have checked that this issue has not already been reported. - [x] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. ### Minimal code sample (that we can copy&paste without having any data). ```python. import scanpy as sc. adata = sc.datasets.blobs(). sc.pp.pca(adata). adata.obs['boolean'] = True. sc.pl.pca(adata, color='boolean'). ```. ```pytb. ... storing 'blobs' as categorical. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-11-1415b8dea7b8> in <module>. 5 adata.obs['boolean'] = True. 6 . ----> 7 sc.pl.pca(adata, color='boolean'). /opt/conda/lib/python3.7/site-packages/scanpy/plotting/_tools/scatterplots.py in pca(adata, annotate_var_explained, show, return_fig, save, **kwargs). 727 if not annotate_var_explained:. 728 return embedding(. --> 729 adata, 'pca', show=show, return_fig=return_fig, save=save, **kwargs. 730 ). 731 else:. /opt/conda/lib/python3.7/site-packages/scanpy/plotting/_tools/scatterplots.py in embedding(adata, basis, color, gene_symbols, use_raw, sort_order, edges, edges_width, edges_color, neighbors_key, arrows, arrows_kwds, groups, components, layer, projection, scale_factor, color_map, cmap, palette, na_color, na_in_legend, size, frameon, legend_fontsize, legend_fontweight, legend_loc, legend_fontoutline, vmax, vmin, add_outline, outline_width, outline_color, ncols, hspace, wspace, title, show, save, ax, return_fig, **kwargs). 257 if sort_order is True and value_to_plot is not None and categorical is False:. 258 # Higher values plotted on top, null values on bottom. --> 259 order = np.argsort(-color_vector, kind=""stable"")[::-1]. 260 elif sort_order and categorical:. 261 # Null points go on bottom. TypeError: The numpy boolean negative, the `-` operator, is not supported, use the `~` operator or the logical_not ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1646
https://github.com/scverse/scanpy/issues/1646:620,testability,Trace,Traceback,620,"Support coloring by boolean variables; - [x] I have checked that this issue has not already been reported. - [x] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. ### Minimal code sample (that we can copy&paste without having any data). ```python. import scanpy as sc. adata = sc.datasets.blobs(). sc.pp.pca(adata). adata.obs['boolean'] = True. sc.pl.pca(adata, color='boolean'). ```. ```pytb. ... storing 'blobs' as categorical. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-11-1415b8dea7b8> in <module>. 5 adata.obs['boolean'] = True. 6 . ----> 7 sc.pl.pca(adata, color='boolean'). /opt/conda/lib/python3.7/site-packages/scanpy/plotting/_tools/scatterplots.py in pca(adata, annotate_var_explained, show, return_fig, save, **kwargs). 727 if not annotate_var_explained:. 728 return embedding(. --> 729 adata, 'pca', show=show, return_fig=return_fig, save=save, **kwargs. 730 ). 731 else:. /opt/conda/lib/python3.7/site-packages/scanpy/plotting/_tools/scatterplots.py in embedding(adata, basis, color, gene_symbols, use_raw, sort_order, edges, edges_width, edges_color, neighbors_key, arrows, arrows_kwds, groups, components, layer, projection, scale_factor, color_map, cmap, palette, na_color, na_in_legend, size, frameon, legend_fontsize, legend_fontweight, legend_loc, legend_fontoutline, vmax, vmin, add_outline, outline_width, outline_color, ncols, hspace, wspace, title, show, save, ax, return_fig, **kwargs). 257 if sort_order is True and value_to_plot is not None and categorical is False:. 258 # Higher values plotted on top, null values on bottom. --> 259 order = np.argsort(-color_vector, kind=""stable"")[::-1]. 260 elif sort_order and categorical:. 261 # Null points go on bottom. TypeError: The numpy boolean negative, the `-` operator, is not supported, use the `~` operator or the logical_not ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1646
https://github.com/scverse/scanpy/issues/1646:0,usability,Support,Support,0,"Support coloring by boolean variables; - [x] I have checked that this issue has not already been reported. - [x] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. ### Minimal code sample (that we can copy&paste without having any data). ```python. import scanpy as sc. adata = sc.datasets.blobs(). sc.pp.pca(adata). adata.obs['boolean'] = True. sc.pl.pca(adata, color='boolean'). ```. ```pytb. ... storing 'blobs' as categorical. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-11-1415b8dea7b8> in <module>. 5 adata.obs['boolean'] = True. 6 . ----> 7 sc.pl.pca(adata, color='boolean'). /opt/conda/lib/python3.7/site-packages/scanpy/plotting/_tools/scatterplots.py in pca(adata, annotate_var_explained, show, return_fig, save, **kwargs). 727 if not annotate_var_explained:. 728 return embedding(. --> 729 adata, 'pca', show=show, return_fig=return_fig, save=save, **kwargs. 730 ). 731 else:. /opt/conda/lib/python3.7/site-packages/scanpy/plotting/_tools/scatterplots.py in embedding(adata, basis, color, gene_symbols, use_raw, sort_order, edges, edges_width, edges_color, neighbors_key, arrows, arrows_kwds, groups, components, layer, projection, scale_factor, color_map, cmap, palette, na_color, na_in_legend, size, frameon, legend_fontsize, legend_fontweight, legend_loc, legend_fontoutline, vmax, vmin, add_outline, outline_width, outline_color, ncols, hspace, wspace, title, show, save, ax, return_fig, **kwargs). 257 if sort_order is True and value_to_plot is not None and categorical is False:. 258 # Higher values plotted on top, null values on bottom. --> 259 order = np.argsort(-color_vector, kind=""stable"")[::-1]. 260 elif sort_order and categorical:. 261 # Null points go on bottom. TypeError: The numpy boolean negative, the `-` operator, is not supported, use the `~` operator or the logical_not ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1646
https://github.com/scverse/scanpy/issues/1646:120,usability,confirm,confirmed,120,"Support coloring by boolean variables; - [x] I have checked that this issue has not already been reported. - [x] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. ### Minimal code sample (that we can copy&paste without having any data). ```python. import scanpy as sc. adata = sc.datasets.blobs(). sc.pp.pca(adata). adata.obs['boolean'] = True. sc.pl.pca(adata, color='boolean'). ```. ```pytb. ... storing 'blobs' as categorical. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-11-1415b8dea7b8> in <module>. 5 adata.obs['boolean'] = True. 6 . ----> 7 sc.pl.pca(adata, color='boolean'). /opt/conda/lib/python3.7/site-packages/scanpy/plotting/_tools/scatterplots.py in pca(adata, annotate_var_explained, show, return_fig, save, **kwargs). 727 if not annotate_var_explained:. 728 return embedding(. --> 729 adata, 'pca', show=show, return_fig=return_fig, save=save, **kwargs. 730 ). 731 else:. /opt/conda/lib/python3.7/site-packages/scanpy/plotting/_tools/scatterplots.py in embedding(adata, basis, color, gene_symbols, use_raw, sort_order, edges, edges_width, edges_color, neighbors_key, arrows, arrows_kwds, groups, components, layer, projection, scale_factor, color_map, cmap, palette, na_color, na_in_legend, size, frameon, legend_fontsize, legend_fontweight, legend_loc, legend_fontoutline, vmax, vmin, add_outline, outline_width, outline_color, ncols, hspace, wspace, title, show, save, ax, return_fig, **kwargs). 257 if sort_order is True and value_to_plot is not None and categorical is False:. 258 # Higher values plotted on top, null values on bottom. --> 259 order = np.argsort(-color_vector, kind=""stable"")[::-1]. 260 elif sort_order and categorical:. 261 # Null points go on bottom. TypeError: The numpy boolean negative, the `-` operator, is not supported, use the `~` operator or the logical_not ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1646
https://github.com/scverse/scanpy/issues/1646:203,usability,confirm,confirmed,203,"Support coloring by boolean variables; - [x] I have checked that this issue has not already been reported. - [x] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. ### Minimal code sample (that we can copy&paste without having any data). ```python. import scanpy as sc. adata = sc.datasets.blobs(). sc.pp.pca(adata). adata.obs['boolean'] = True. sc.pl.pca(adata, color='boolean'). ```. ```pytb. ... storing 'blobs' as categorical. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-11-1415b8dea7b8> in <module>. 5 adata.obs['boolean'] = True. 6 . ----> 7 sc.pl.pca(adata, color='boolean'). /opt/conda/lib/python3.7/site-packages/scanpy/plotting/_tools/scatterplots.py in pca(adata, annotate_var_explained, show, return_fig, save, **kwargs). 727 if not annotate_var_explained:. 728 return embedding(. --> 729 adata, 'pca', show=show, return_fig=return_fig, save=save, **kwargs. 730 ). 731 else:. /opt/conda/lib/python3.7/site-packages/scanpy/plotting/_tools/scatterplots.py in embedding(adata, basis, color, gene_symbols, use_raw, sort_order, edges, edges_width, edges_color, neighbors_key, arrows, arrows_kwds, groups, components, layer, projection, scale_factor, color_map, cmap, palette, na_color, na_in_legend, size, frameon, legend_fontsize, legend_fontweight, legend_loc, legend_fontoutline, vmax, vmin, add_outline, outline_width, outline_color, ncols, hspace, wspace, title, show, save, ax, return_fig, **kwargs). 257 if sort_order is True and value_to_plot is not None and categorical is False:. 258 # Higher values plotted on top, null values on bottom. --> 259 order = np.argsort(-color_vector, kind=""stable"")[::-1]. 260 elif sort_order and categorical:. 261 # Null points go on bottom. TypeError: The numpy boolean negative, the `-` operator, is not supported, use the `~` operator or the logical_not ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1646
https://github.com/scverse/scanpy/issues/1646:270,usability,Minim,Minimal,270,"Support coloring by boolean variables; - [x] I have checked that this issue has not already been reported. - [x] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. ### Minimal code sample (that we can copy&paste without having any data). ```python. import scanpy as sc. adata = sc.datasets.blobs(). sc.pp.pca(adata). adata.obs['boolean'] = True. sc.pl.pca(adata, color='boolean'). ```. ```pytb. ... storing 'blobs' as categorical. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-11-1415b8dea7b8> in <module>. 5 adata.obs['boolean'] = True. 6 . ----> 7 sc.pl.pca(adata, color='boolean'). /opt/conda/lib/python3.7/site-packages/scanpy/plotting/_tools/scatterplots.py in pca(adata, annotate_var_explained, show, return_fig, save, **kwargs). 727 if not annotate_var_explained:. 728 return embedding(. --> 729 adata, 'pca', show=show, return_fig=return_fig, save=save, **kwargs. 730 ). 731 else:. /opt/conda/lib/python3.7/site-packages/scanpy/plotting/_tools/scatterplots.py in embedding(adata, basis, color, gene_symbols, use_raw, sort_order, edges, edges_width, edges_color, neighbors_key, arrows, arrows_kwds, groups, components, layer, projection, scale_factor, color_map, cmap, palette, na_color, na_in_legend, size, frameon, legend_fontsize, legend_fontweight, legend_loc, legend_fontoutline, vmax, vmin, add_outline, outline_width, outline_color, ncols, hspace, wspace, title, show, save, ax, return_fig, **kwargs). 257 if sort_order is True and value_to_plot is not None and categorical is False:. 258 # Higher values plotted on top, null values on bottom. --> 259 order = np.argsort(-color_vector, kind=""stable"")[::-1]. 260 elif sort_order and categorical:. 261 # Null points go on bottom. TypeError: The numpy boolean negative, the `-` operator, is not supported, use the `~` operator or the logical_not ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1646
https://github.com/scverse/scanpy/issues/1646:664,usability,input,input-,664,"Support coloring by boolean variables; - [x] I have checked that this issue has not already been reported. - [x] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. ### Minimal code sample (that we can copy&paste without having any data). ```python. import scanpy as sc. adata = sc.datasets.blobs(). sc.pp.pca(adata). adata.obs['boolean'] = True. sc.pl.pca(adata, color='boolean'). ```. ```pytb. ... storing 'blobs' as categorical. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-11-1415b8dea7b8> in <module>. 5 adata.obs['boolean'] = True. 6 . ----> 7 sc.pl.pca(adata, color='boolean'). /opt/conda/lib/python3.7/site-packages/scanpy/plotting/_tools/scatterplots.py in pca(adata, annotate_var_explained, show, return_fig, save, **kwargs). 727 if not annotate_var_explained:. 728 return embedding(. --> 729 adata, 'pca', show=show, return_fig=return_fig, save=save, **kwargs. 730 ). 731 else:. /opt/conda/lib/python3.7/site-packages/scanpy/plotting/_tools/scatterplots.py in embedding(adata, basis, color, gene_symbols, use_raw, sort_order, edges, edges_width, edges_color, neighbors_key, arrows, arrows_kwds, groups, components, layer, projection, scale_factor, color_map, cmap, palette, na_color, na_in_legend, size, frameon, legend_fontsize, legend_fontweight, legend_loc, legend_fontoutline, vmax, vmin, add_outline, outline_width, outline_color, ncols, hspace, wspace, title, show, save, ax, return_fig, **kwargs). 257 if sort_order is True and value_to_plot is not None and categorical is False:. 258 # Higher values plotted on top, null values on bottom. --> 259 order = np.argsort(-color_vector, kind=""stable"")[::-1]. 260 elif sort_order and categorical:. 261 # Null points go on bottom. TypeError: The numpy boolean negative, the `-` operator, is not supported, use the `~` operator or the logical_not ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1646
https://github.com/scverse/scanpy/issues/1646:1949,usability,support,supported,1949,"aster branch of scanpy. ---. ### Minimal code sample (that we can copy&paste without having any data). ```python. import scanpy as sc. adata = sc.datasets.blobs(). sc.pp.pca(adata). adata.obs['boolean'] = True. sc.pl.pca(adata, color='boolean'). ```. ```pytb. ... storing 'blobs' as categorical. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-11-1415b8dea7b8> in <module>. 5 adata.obs['boolean'] = True. 6 . ----> 7 sc.pl.pca(adata, color='boolean'). /opt/conda/lib/python3.7/site-packages/scanpy/plotting/_tools/scatterplots.py in pca(adata, annotate_var_explained, show, return_fig, save, **kwargs). 727 if not annotate_var_explained:. 728 return embedding(. --> 729 adata, 'pca', show=show, return_fig=return_fig, save=save, **kwargs. 730 ). 731 else:. /opt/conda/lib/python3.7/site-packages/scanpy/plotting/_tools/scatterplots.py in embedding(adata, basis, color, gene_symbols, use_raw, sort_order, edges, edges_width, edges_color, neighbors_key, arrows, arrows_kwds, groups, components, layer, projection, scale_factor, color_map, cmap, palette, na_color, na_in_legend, size, frameon, legend_fontsize, legend_fontweight, legend_loc, legend_fontoutline, vmax, vmin, add_outline, outline_width, outline_color, ncols, hspace, wspace, title, show, save, ax, return_fig, **kwargs). 257 if sort_order is True and value_to_plot is not None and categorical is False:. 258 # Higher values plotted on top, null values on bottom. --> 259 order = np.argsort(-color_vector, kind=""stable"")[::-1]. 260 elif sort_order and categorical:. 261 # Null points go on bottom. TypeError: The numpy boolean negative, the `-` operator, is not supported, use the `~` operator or the logical_not function instead. ```. #### Versions. <details>. scanpy==1.7.0 anndata==0.7.5 umap==0.5.1 numpy==1.20.0 scipy==1.6.0 pandas==1.2.1 scikit-learn==0.24.1 statsmodels==0.12.2 python-igraph==0.8.3 louvain==0.7.0 leidenalg==0.8.3. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1646
https://github.com/scverse/scanpy/issues/1646:2138,usability,learn,learn,2138,"aster branch of scanpy. ---. ### Minimal code sample (that we can copy&paste without having any data). ```python. import scanpy as sc. adata = sc.datasets.blobs(). sc.pp.pca(adata). adata.obs['boolean'] = True. sc.pl.pca(adata, color='boolean'). ```. ```pytb. ... storing 'blobs' as categorical. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-11-1415b8dea7b8> in <module>. 5 adata.obs['boolean'] = True. 6 . ----> 7 sc.pl.pca(adata, color='boolean'). /opt/conda/lib/python3.7/site-packages/scanpy/plotting/_tools/scatterplots.py in pca(adata, annotate_var_explained, show, return_fig, save, **kwargs). 727 if not annotate_var_explained:. 728 return embedding(. --> 729 adata, 'pca', show=show, return_fig=return_fig, save=save, **kwargs. 730 ). 731 else:. /opt/conda/lib/python3.7/site-packages/scanpy/plotting/_tools/scatterplots.py in embedding(adata, basis, color, gene_symbols, use_raw, sort_order, edges, edges_width, edges_color, neighbors_key, arrows, arrows_kwds, groups, components, layer, projection, scale_factor, color_map, cmap, palette, na_color, na_in_legend, size, frameon, legend_fontsize, legend_fontweight, legend_loc, legend_fontoutline, vmax, vmin, add_outline, outline_width, outline_color, ncols, hspace, wspace, title, show, save, ax, return_fig, **kwargs). 257 if sort_order is True and value_to_plot is not None and categorical is False:. 258 # Higher values plotted on top, null values on bottom. --> 259 order = np.argsort(-color_vector, kind=""stable"")[::-1]. 260 elif sort_order and categorical:. 261 # Null points go on bottom. TypeError: The numpy boolean negative, the `-` operator, is not supported, use the `~` operator or the logical_not function instead. ```. #### Versions. <details>. scanpy==1.7.0 anndata==0.7.5 umap==0.5.1 numpy==1.20.0 scipy==1.6.0 pandas==1.2.1 scikit-learn==0.24.1 statsmodels==0.12.2 python-igraph==0.8.3 louvain==0.7.0 leidenalg==0.8.3. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1646
https://github.com/scverse/scanpy/issues/1647:388,availability,cluster,clustermap,388,"cell type coloring in sc.pl.matrixplot; <!-- What kind of feature would you like to request? -->. Is there a way to select the coloring for the cell types in the ""groupby"" argument in sc.pl.matrixplot? . <!-- Please describe your wishes below: -->. I have a custom color palette based on continuity from my other plots in my other subfigures. equivalent to the row_colors argument in sns.clustermap, is there a way to specify the right side colors for the different clusters? thank you! ...",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1647
https://github.com/scverse/scanpy/issues/1647:466,availability,cluster,clusters,466,"cell type coloring in sc.pl.matrixplot; <!-- What kind of feature would you like to request? -->. Is there a way to select the coloring for the cell types in the ""groupby"" argument in sc.pl.matrixplot? . <!-- Please describe your wishes below: -->. I have a custom color palette based on continuity from my other plots in my other subfigures. equivalent to the row_colors argument in sns.clustermap, is there a way to specify the right side colors for the different clusters? thank you! ...",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1647
https://github.com/scverse/scanpy/issues/1647:288,deployability,continu,continuity,288,"cell type coloring in sc.pl.matrixplot; <!-- What kind of feature would you like to request? -->. Is there a way to select the coloring for the cell types in the ""groupby"" argument in sc.pl.matrixplot? . <!-- Please describe your wishes below: -->. I have a custom color palette based on continuity from my other plots in my other subfigures. equivalent to the row_colors argument in sns.clustermap, is there a way to specify the right side colors for the different clusters? thank you! ...",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1647
https://github.com/scverse/scanpy/issues/1647:388,deployability,cluster,clustermap,388,"cell type coloring in sc.pl.matrixplot; <!-- What kind of feature would you like to request? -->. Is there a way to select the coloring for the cell types in the ""groupby"" argument in sc.pl.matrixplot? . <!-- Please describe your wishes below: -->. I have a custom color palette based on continuity from my other plots in my other subfigures. equivalent to the row_colors argument in sns.clustermap, is there a way to specify the right side colors for the different clusters? thank you! ...",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1647
https://github.com/scverse/scanpy/issues/1647:466,deployability,cluster,clusters,466,"cell type coloring in sc.pl.matrixplot; <!-- What kind of feature would you like to request? -->. Is there a way to select the coloring for the cell types in the ""groupby"" argument in sc.pl.matrixplot? . <!-- Please describe your wishes below: -->. I have a custom color palette based on continuity from my other plots in my other subfigures. equivalent to the row_colors argument in sns.clustermap, is there a way to specify the right side colors for the different clusters? thank you! ...",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1647
https://github.com/scverse/scanpy/issues/1647:331,integrability,sub,subfigures,331,"cell type coloring in sc.pl.matrixplot; <!-- What kind of feature would you like to request? -->. Is there a way to select the coloring for the cell types in the ""groupby"" argument in sc.pl.matrixplot? . <!-- Please describe your wishes below: -->. I have a custom color palette based on continuity from my other plots in my other subfigures. equivalent to the row_colors argument in sns.clustermap, is there a way to specify the right side colors for the different clusters? thank you! ...",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1647
https://github.com/scverse/scanpy/issues/1647:418,interoperability,specif,specify,418,"cell type coloring in sc.pl.matrixplot; <!-- What kind of feature would you like to request? -->. Is there a way to select the coloring for the cell types in the ""groupby"" argument in sc.pl.matrixplot? . <!-- Please describe your wishes below: -->. I have a custom color palette based on continuity from my other plots in my other subfigures. equivalent to the row_colors argument in sns.clustermap, is there a way to specify the right side colors for the different clusters? thank you! ...",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1647
https://github.com/scverse/scanpy/issues/1647:258,usability,custom,custom,258,"cell type coloring in sc.pl.matrixplot; <!-- What kind of feature would you like to request? -->. Is there a way to select the coloring for the cell types in the ""groupby"" argument in sc.pl.matrixplot? . <!-- Please describe your wishes below: -->. I have a custom color palette based on continuity from my other plots in my other subfigures. equivalent to the row_colors argument in sns.clustermap, is there a way to specify the right side colors for the different clusters? thank you! ...",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1647
https://github.com/scverse/scanpy/issues/1648:851,deployability,manag,manager,851,"Intended way to set_figure_params for a single figure; <!--.  If you need help using Scanpy, please ask in https://scanpy.discourse.group/ instead . If you want to know about design decisions and the like, please ask below:. -->. I was wondering, what is the intended way to temporarily set plotting parameters, e.g. figsize? Say I want to increase the figsize just for a single UMAP plot. . I could either temporarily `set_figure_params` and reset it manually afterwards. ```python. sc.set_figure_params(figsize=(8, 8)). sc.pl.umap(adata, color=""cell_type""). sc.set_figure_params(figsize=(4, 4)) # or whatever it was before... ```. Or create the figure separately:. ```python. _, ax = plt.subplots(figsize=(8, 8)). sc.pl.umap(adata, color=""cell_type"", ax=ax). ```. <p><br></p>. Would it make sense to extend `set_figure_params` to act as a context manager? ```python. with sc.set_figure_params(figsize=(8, 8), frameon=False):. sc.pl.umap(adata, color=""cell_type""). ```. But maybe there's a comparable way already? .",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1648
https://github.com/scverse/scanpy/issues/1648:851,energy efficiency,manag,manager,851,"Intended way to set_figure_params for a single figure; <!--.  If you need help using Scanpy, please ask in https://scanpy.discourse.group/ instead . If you want to know about design decisions and the like, please ask below:. -->. I was wondering, what is the intended way to temporarily set plotting parameters, e.g. figsize? Say I want to increase the figsize just for a single UMAP plot. . I could either temporarily `set_figure_params` and reset it manually afterwards. ```python. sc.set_figure_params(figsize=(8, 8)). sc.pl.umap(adata, color=""cell_type""). sc.set_figure_params(figsize=(4, 4)) # or whatever it was before... ```. Or create the figure separately:. ```python. _, ax = plt.subplots(figsize=(8, 8)). sc.pl.umap(adata, color=""cell_type"", ax=ax). ```. <p><br></p>. Would it make sense to extend `set_figure_params` to act as a context manager? ```python. with sc.set_figure_params(figsize=(8, 8), frameon=False):. sc.pl.umap(adata, color=""cell_type""). ```. But maybe there's a comparable way already? .",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1648
https://github.com/scverse/scanpy/issues/1648:692,integrability,sub,subplots,692,"Intended way to set_figure_params for a single figure; <!--.  If you need help using Scanpy, please ask in https://scanpy.discourse.group/ instead . If you want to know about design decisions and the like, please ask below:. -->. I was wondering, what is the intended way to temporarily set plotting parameters, e.g. figsize? Say I want to increase the figsize just for a single UMAP plot. . I could either temporarily `set_figure_params` and reset it manually afterwards. ```python. sc.set_figure_params(figsize=(8, 8)). sc.pl.umap(adata, color=""cell_type""). sc.set_figure_params(figsize=(4, 4)) # or whatever it was before... ```. Or create the figure separately:. ```python. _, ax = plt.subplots(figsize=(8, 8)). sc.pl.umap(adata, color=""cell_type"", ax=ax). ```. <p><br></p>. Would it make sense to extend `set_figure_params` to act as a context manager? ```python. with sc.set_figure_params(figsize=(8, 8), frameon=False):. sc.pl.umap(adata, color=""cell_type""). ```. But maybe there's a comparable way already? .",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1648
https://github.com/scverse/scanpy/issues/1648:177,modifiability,design decis,design decisions,177,"Intended way to set_figure_params for a single figure; <!--.  If you need help using Scanpy, please ask in https://scanpy.discourse.group/ instead . If you want to know about design decisions and the like, please ask below:. -->. I was wondering, what is the intended way to temporarily set plotting parameters, e.g. figsize? Say I want to increase the figsize just for a single UMAP plot. . I could either temporarily `set_figure_params` and reset it manually afterwards. ```python. sc.set_figure_params(figsize=(8, 8)). sc.pl.umap(adata, color=""cell_type""). sc.set_figure_params(figsize=(4, 4)) # or whatever it was before... ```. Or create the figure separately:. ```python. _, ax = plt.subplots(figsize=(8, 8)). sc.pl.umap(adata, color=""cell_type"", ax=ax). ```. <p><br></p>. Would it make sense to extend `set_figure_params` to act as a context manager? ```python. with sc.set_figure_params(figsize=(8, 8), frameon=False):. sc.pl.umap(adata, color=""cell_type""). ```. But maybe there's a comparable way already? .",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1648
https://github.com/scverse/scanpy/issues/1648:302,modifiability,paramet,parameters,302,"Intended way to set_figure_params for a single figure; <!--.  If you need help using Scanpy, please ask in https://scanpy.discourse.group/ instead . If you want to know about design decisions and the like, please ask below:. -->. I was wondering, what is the intended way to temporarily set plotting parameters, e.g. figsize? Say I want to increase the figsize just for a single UMAP plot. . I could either temporarily `set_figure_params` and reset it manually afterwards. ```python. sc.set_figure_params(figsize=(8, 8)). sc.pl.umap(adata, color=""cell_type""). sc.set_figure_params(figsize=(4, 4)) # or whatever it was before... ```. Or create the figure separately:. ```python. _, ax = plt.subplots(figsize=(8, 8)). sc.pl.umap(adata, color=""cell_type"", ax=ax). ```. <p><br></p>. Would it make sense to extend `set_figure_params` to act as a context manager? ```python. with sc.set_figure_params(figsize=(8, 8), frameon=False):. sc.pl.umap(adata, color=""cell_type""). ```. But maybe there's a comparable way already? .",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1648
https://github.com/scverse/scanpy/issues/1648:804,modifiability,exten,extend,804,"Intended way to set_figure_params for a single figure; <!--.  If you need help using Scanpy, please ask in https://scanpy.discourse.group/ instead . If you want to know about design decisions and the like, please ask below:. -->. I was wondering, what is the intended way to temporarily set plotting parameters, e.g. figsize? Say I want to increase the figsize just for a single UMAP plot. . I could either temporarily `set_figure_params` and reset it manually afterwards. ```python. sc.set_figure_params(figsize=(8, 8)). sc.pl.umap(adata, color=""cell_type""). sc.set_figure_params(figsize=(4, 4)) # or whatever it was before... ```. Or create the figure separately:. ```python. _, ax = plt.subplots(figsize=(8, 8)). sc.pl.umap(adata, color=""cell_type"", ax=ax). ```. <p><br></p>. Would it make sense to extend `set_figure_params` to act as a context manager? ```python. with sc.set_figure_params(figsize=(8, 8), frameon=False):. sc.pl.umap(adata, color=""cell_type""). ```. But maybe there's a comparable way already? .",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1648
https://github.com/scverse/scanpy/issues/1648:851,safety,manag,manager,851,"Intended way to set_figure_params for a single figure; <!--.  If you need help using Scanpy, please ask in https://scanpy.discourse.group/ instead . If you want to know about design decisions and the like, please ask below:. -->. I was wondering, what is the intended way to temporarily set plotting parameters, e.g. figsize? Say I want to increase the figsize just for a single UMAP plot. . I could either temporarily `set_figure_params` and reset it manually afterwards. ```python. sc.set_figure_params(figsize=(8, 8)). sc.pl.umap(adata, color=""cell_type""). sc.set_figure_params(figsize=(4, 4)) # or whatever it was before... ```. Or create the figure separately:. ```python. _, ax = plt.subplots(figsize=(8, 8)). sc.pl.umap(adata, color=""cell_type"", ax=ax). ```. <p><br></p>. Would it make sense to extend `set_figure_params` to act as a context manager? ```python. with sc.set_figure_params(figsize=(8, 8), frameon=False):. sc.pl.umap(adata, color=""cell_type""). ```. But maybe there's a comparable way already? .",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1648
https://github.com/scverse/scanpy/issues/1648:843,testability,context,context,843,"Intended way to set_figure_params for a single figure; <!--.  If you need help using Scanpy, please ask in https://scanpy.discourse.group/ instead . If you want to know about design decisions and the like, please ask below:. -->. I was wondering, what is the intended way to temporarily set plotting parameters, e.g. figsize? Say I want to increase the figsize just for a single UMAP plot. . I could either temporarily `set_figure_params` and reset it manually afterwards. ```python. sc.set_figure_params(figsize=(8, 8)). sc.pl.umap(adata, color=""cell_type""). sc.set_figure_params(figsize=(4, 4)) # or whatever it was before... ```. Or create the figure separately:. ```python. _, ax = plt.subplots(figsize=(8, 8)). sc.pl.umap(adata, color=""cell_type"", ax=ax). ```. <p><br></p>. Would it make sense to extend `set_figure_params` to act as a context manager? ```python. with sc.set_figure_params(figsize=(8, 8), frameon=False):. sc.pl.umap(adata, color=""cell_type""). ```. But maybe there's a comparable way already? .",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1648
https://github.com/scverse/scanpy/issues/1648:75,usability,help,help,75,"Intended way to set_figure_params for a single figure; <!--.  If you need help using Scanpy, please ask in https://scanpy.discourse.group/ instead . If you want to know about design decisions and the like, please ask below:. -->. I was wondering, what is the intended way to temporarily set plotting parameters, e.g. figsize? Say I want to increase the figsize just for a single UMAP plot. . I could either temporarily `set_figure_params` and reset it manually afterwards. ```python. sc.set_figure_params(figsize=(8, 8)). sc.pl.umap(adata, color=""cell_type""). sc.set_figure_params(figsize=(4, 4)) # or whatever it was before... ```. Or create the figure separately:. ```python. _, ax = plt.subplots(figsize=(8, 8)). sc.pl.umap(adata, color=""cell_type"", ax=ax). ```. <p><br></p>. Would it make sense to extend `set_figure_params` to act as a context manager? ```python. with sc.set_figure_params(figsize=(8, 8), frameon=False):. sc.pl.umap(adata, color=""cell_type""). ```. But maybe there's a comparable way already? .",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1648
https://github.com/scverse/scanpy/pull/1649:89,deployability,log,log,89,compare_abs argument in filter_rank_genes_groups; Argument to compare absolute values of log fold change with `min_fold_change`. https://github.com/theislab/scanpy/issues/1325. I think `compare_abs` is a better name than `rankby_abs` for this.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1649
https://github.com/scverse/scanpy/pull/1649:89,safety,log,log,89,compare_abs argument in filter_rank_genes_groups; Argument to compare absolute values of log fold change with `min_fold_change`. https://github.com/theislab/scanpy/issues/1325. I think `compare_abs` is a better name than `rankby_abs` for this.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1649
https://github.com/scverse/scanpy/pull/1649:89,security,log,log,89,compare_abs argument in filter_rank_genes_groups; Argument to compare absolute values of log fold change with `min_fold_change`. https://github.com/theislab/scanpy/issues/1325. I think `compare_abs` is a better name than `rankby_abs` for this.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1649
https://github.com/scverse/scanpy/pull/1649:89,testability,log,log,89,compare_abs argument in filter_rank_genes_groups; Argument to compare absolute values of log fold change with `min_fold_change`. https://github.com/theislab/scanpy/issues/1325. I think `compare_abs` is a better name than `rankby_abs` for this.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1649
https://github.com/scverse/scanpy/issues/1650:41,deployability,scale,scale,41,"how to export the normlized data not the scale data on h5ad anndata object?; Hi @ALL,. I want that the object of annData to save the normalized expression matrix that exclude the scaling matrix to perform the pyscenic regulon analysis.but the code adata.to_df().to_csv(EXP_MTX_QC_FNAME) just save the scaling matrix that had the negative number in but just normalized d matrix.so i merely want to export the normlized matrix data and then imported to the pyscenic to analyize. So how can i do for this request? Any advice would be appreciated. Best,. hanhuihong",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1650
https://github.com/scverse/scanpy/issues/1650:41,energy efficiency,scale,scale,41,"how to export the normlized data not the scale data on h5ad anndata object?; Hi @ALL,. I want that the object of annData to save the normalized expression matrix that exclude the scaling matrix to perform the pyscenic regulon analysis.but the code adata.to_df().to_csv(EXP_MTX_QC_FNAME) just save the scaling matrix that had the negative number in but just normalized d matrix.so i merely want to export the normlized matrix data and then imported to the pyscenic to analyize. So how can i do for this request? Any advice would be appreciated. Best,. hanhuihong",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1650
https://github.com/scverse/scanpy/issues/1650:41,modifiability,scal,scale,41,"how to export the normlized data not the scale data on h5ad anndata object?; Hi @ALL,. I want that the object of annData to save the normalized expression matrix that exclude the scaling matrix to perform the pyscenic regulon analysis.but the code adata.to_df().to_csv(EXP_MTX_QC_FNAME) just save the scaling matrix that had the negative number in but just normalized d matrix.so i merely want to export the normlized matrix data and then imported to the pyscenic to analyize. So how can i do for this request? Any advice would be appreciated. Best,. hanhuihong",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1650
https://github.com/scverse/scanpy/issues/1650:179,modifiability,scal,scaling,179,"how to export the normlized data not the scale data on h5ad anndata object?; Hi @ALL,. I want that the object of annData to save the normalized expression matrix that exclude the scaling matrix to perform the pyscenic regulon analysis.but the code adata.to_df().to_csv(EXP_MTX_QC_FNAME) just save the scaling matrix that had the negative number in but just normalized d matrix.so i merely want to export the normlized matrix data and then imported to the pyscenic to analyize. So how can i do for this request? Any advice would be appreciated. Best,. hanhuihong",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1650
https://github.com/scverse/scanpy/issues/1650:301,modifiability,scal,scaling,301,"how to export the normlized data not the scale data on h5ad anndata object?; Hi @ALL,. I want that the object of annData to save the normalized expression matrix that exclude the scaling matrix to perform the pyscenic regulon analysis.but the code adata.to_df().to_csv(EXP_MTX_QC_FNAME) just save the scaling matrix that had the negative number in but just normalized d matrix.so i merely want to export the normlized matrix data and then imported to the pyscenic to analyize. So how can i do for this request? Any advice would be appreciated. Best,. hanhuihong",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1650
https://github.com/scverse/scanpy/issues/1650:41,performance,scale,scale,41,"how to export the normlized data not the scale data on h5ad anndata object?; Hi @ALL,. I want that the object of annData to save the normalized expression matrix that exclude the scaling matrix to perform the pyscenic regulon analysis.but the code adata.to_df().to_csv(EXP_MTX_QC_FNAME) just save the scaling matrix that had the negative number in but just normalized d matrix.so i merely want to export the normlized matrix data and then imported to the pyscenic to analyize. So how can i do for this request? Any advice would be appreciated. Best,. hanhuihong",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1650
https://github.com/scverse/scanpy/issues/1650:197,performance,perform,perform,197,"how to export the normlized data not the scale data on h5ad anndata object?; Hi @ALL,. I want that the object of annData to save the normalized expression matrix that exclude the scaling matrix to perform the pyscenic regulon analysis.but the code adata.to_df().to_csv(EXP_MTX_QC_FNAME) just save the scaling matrix that had the negative number in but just normalized d matrix.so i merely want to export the normlized matrix data and then imported to the pyscenic to analyize. So how can i do for this request? Any advice would be appreciated. Best,. hanhuihong",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1650
https://github.com/scverse/scanpy/issues/1650:197,usability,perform,perform,197,"how to export the normlized data not the scale data on h5ad anndata object?; Hi @ALL,. I want that the object of annData to save the normalized expression matrix that exclude the scaling matrix to perform the pyscenic regulon analysis.but the code adata.to_df().to_csv(EXP_MTX_QC_FNAME) just save the scaling matrix that had the negative number in but just normalized d matrix.so i merely want to export the normlized matrix data and then imported to the pyscenic to analyize. So how can i do for this request? Any advice would be appreciated. Best,. hanhuihong",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1650
https://github.com/scverse/scanpy/issues/1651:58,availability,error,error,58,"Option to ignore ""nan"" with sc.pl.rank_genes_groups() and error while writing data to .h5ad; - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [X] (optional) I have confirmed this bug exists on the master branch of scanpy. `sc.tl.filter_rank_genes_groups()` replaces gene names with ""nan"" values, would be nice to be able to ignore these with `sc.pl.rank_genes_groups()` and instead show the top n actual non-filtered genes. ### Minimal code sample. ```python. adata = sc.datasets.pbmc68k_reduced(). sc.tl.rank_genes_groups(adata, 'bulk_labels', method='wilcoxon'). sc.tl.filter_rank_genes_groups(adata, min_fold_change=3). sc.pl.rank_genes_groups(adata, key=""rank_genes_groups_filtered""). ```. ![image](https://user-images.githubusercontent.com/20436557/107974043-ea8bfc00-6fad-11eb-9368-1ba78336a3b2.png). #### Versions. ```. scanpy==1.7.0 anndata==0.7.5 umap==0.5.1 numpy==1.19.5 scipy==1.5.4 pandas==1.1.5 scikit-learn==0.24.1 statsmodels==0.12.2 python-igraph==0.8.3 leidenalg==0.8.3. ````.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1651
https://github.com/scverse/scanpy/issues/1651:214,deployability,version,version,214,"Option to ignore ""nan"" with sc.pl.rank_genes_groups() and error while writing data to .h5ad; - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [X] (optional) I have confirmed this bug exists on the master branch of scanpy. `sc.tl.filter_rank_genes_groups()` replaces gene names with ""nan"" values, would be nice to be able to ignore these with `sc.pl.rank_genes_groups()` and instead show the top n actual non-filtered genes. ### Minimal code sample. ```python. adata = sc.datasets.pbmc68k_reduced(). sc.tl.rank_genes_groups(adata, 'bulk_labels', method='wilcoxon'). sc.tl.filter_rank_genes_groups(adata, min_fold_change=3). sc.pl.rank_genes_groups(adata, key=""rank_genes_groups_filtered""). ```. ![image](https://user-images.githubusercontent.com/20436557/107974043-ea8bfc00-6fad-11eb-9368-1ba78336a3b2.png). #### Versions. ```. scanpy==1.7.0 anndata==0.7.5 umap==0.5.1 numpy==1.19.5 scipy==1.5.4 pandas==1.1.5 scikit-learn==0.24.1 statsmodels==0.12.2 python-igraph==0.8.3 leidenalg==0.8.3. ````.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1651
https://github.com/scverse/scanpy/issues/1651:905,deployability,Version,Versions,905,"Option to ignore ""nan"" with sc.pl.rank_genes_groups() and error while writing data to .h5ad; - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [X] (optional) I have confirmed this bug exists on the master branch of scanpy. `sc.tl.filter_rank_genes_groups()` replaces gene names with ""nan"" values, would be nice to be able to ignore these with `sc.pl.rank_genes_groups()` and instead show the top n actual non-filtered genes. ### Minimal code sample. ```python. adata = sc.datasets.pbmc68k_reduced(). sc.tl.rank_genes_groups(adata, 'bulk_labels', method='wilcoxon'). sc.tl.filter_rank_genes_groups(adata, min_fold_change=3). sc.pl.rank_genes_groups(adata, key=""rank_genes_groups_filtered""). ```. ![image](https://user-images.githubusercontent.com/20436557/107974043-ea8bfc00-6fad-11eb-9368-1ba78336a3b2.png). #### Versions. ```. scanpy==1.7.0 anndata==0.7.5 umap==0.5.1 numpy==1.19.5 scipy==1.5.4 pandas==1.1.5 scikit-learn==0.24.1 statsmodels==0.12.2 python-igraph==0.8.3 leidenalg==0.8.3. ````.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1651
https://github.com/scverse/scanpy/issues/1651:214,integrability,version,version,214,"Option to ignore ""nan"" with sc.pl.rank_genes_groups() and error while writing data to .h5ad; - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [X] (optional) I have confirmed this bug exists on the master branch of scanpy. `sc.tl.filter_rank_genes_groups()` replaces gene names with ""nan"" values, would be nice to be able to ignore these with `sc.pl.rank_genes_groups()` and instead show the top n actual non-filtered genes. ### Minimal code sample. ```python. adata = sc.datasets.pbmc68k_reduced(). sc.tl.rank_genes_groups(adata, 'bulk_labels', method='wilcoxon'). sc.tl.filter_rank_genes_groups(adata, min_fold_change=3). sc.pl.rank_genes_groups(adata, key=""rank_genes_groups_filtered""). ```. ![image](https://user-images.githubusercontent.com/20436557/107974043-ea8bfc00-6fad-11eb-9368-1ba78336a3b2.png). #### Versions. ```. scanpy==1.7.0 anndata==0.7.5 umap==0.5.1 numpy==1.19.5 scipy==1.5.4 pandas==1.1.5 scikit-learn==0.24.1 statsmodels==0.12.2 python-igraph==0.8.3 leidenalg==0.8.3. ````.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1651
https://github.com/scverse/scanpy/issues/1651:501,integrability,filter,filtered,501,"Option to ignore ""nan"" with sc.pl.rank_genes_groups() and error while writing data to .h5ad; - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [X] (optional) I have confirmed this bug exists on the master branch of scanpy. `sc.tl.filter_rank_genes_groups()` replaces gene names with ""nan"" values, would be nice to be able to ignore these with `sc.pl.rank_genes_groups()` and instead show the top n actual non-filtered genes. ### Minimal code sample. ```python. adata = sc.datasets.pbmc68k_reduced(). sc.tl.rank_genes_groups(adata, 'bulk_labels', method='wilcoxon'). sc.tl.filter_rank_genes_groups(adata, min_fold_change=3). sc.pl.rank_genes_groups(adata, key=""rank_genes_groups_filtered""). ```. ![image](https://user-images.githubusercontent.com/20436557/107974043-ea8bfc00-6fad-11eb-9368-1ba78336a3b2.png). #### Versions. ```. scanpy==1.7.0 anndata==0.7.5 umap==0.5.1 numpy==1.19.5 scipy==1.5.4 pandas==1.1.5 scikit-learn==0.24.1 statsmodels==0.12.2 python-igraph==0.8.3 leidenalg==0.8.3. ````.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1651
https://github.com/scverse/scanpy/issues/1651:905,integrability,Version,Versions,905,"Option to ignore ""nan"" with sc.pl.rank_genes_groups() and error while writing data to .h5ad; - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [X] (optional) I have confirmed this bug exists on the master branch of scanpy. `sc.tl.filter_rank_genes_groups()` replaces gene names with ""nan"" values, would be nice to be able to ignore these with `sc.pl.rank_genes_groups()` and instead show the top n actual non-filtered genes. ### Minimal code sample. ```python. adata = sc.datasets.pbmc68k_reduced(). sc.tl.rank_genes_groups(adata, 'bulk_labels', method='wilcoxon'). sc.tl.filter_rank_genes_groups(adata, min_fold_change=3). sc.pl.rank_genes_groups(adata, key=""rank_genes_groups_filtered""). ```. ![image](https://user-images.githubusercontent.com/20436557/107974043-ea8bfc00-6fad-11eb-9368-1ba78336a3b2.png). #### Versions. ```. scanpy==1.7.0 anndata==0.7.5 umap==0.5.1 numpy==1.19.5 scipy==1.5.4 pandas==1.1.5 scikit-learn==0.24.1 statsmodels==0.12.2 python-igraph==0.8.3 leidenalg==0.8.3. ````.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1651
https://github.com/scverse/scanpy/issues/1651:214,modifiability,version,version,214,"Option to ignore ""nan"" with sc.pl.rank_genes_groups() and error while writing data to .h5ad; - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [X] (optional) I have confirmed this bug exists on the master branch of scanpy. `sc.tl.filter_rank_genes_groups()` replaces gene names with ""nan"" values, would be nice to be able to ignore these with `sc.pl.rank_genes_groups()` and instead show the top n actual non-filtered genes. ### Minimal code sample. ```python. adata = sc.datasets.pbmc68k_reduced(). sc.tl.rank_genes_groups(adata, 'bulk_labels', method='wilcoxon'). sc.tl.filter_rank_genes_groups(adata, min_fold_change=3). sc.pl.rank_genes_groups(adata, key=""rank_genes_groups_filtered""). ```. ![image](https://user-images.githubusercontent.com/20436557/107974043-ea8bfc00-6fad-11eb-9368-1ba78336a3b2.png). #### Versions. ```. scanpy==1.7.0 anndata==0.7.5 umap==0.5.1 numpy==1.19.5 scipy==1.5.4 pandas==1.1.5 scikit-learn==0.24.1 statsmodels==0.12.2 python-igraph==0.8.3 leidenalg==0.8.3. ````.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1651
https://github.com/scverse/scanpy/issues/1651:905,modifiability,Version,Versions,905,"Option to ignore ""nan"" with sc.pl.rank_genes_groups() and error while writing data to .h5ad; - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [X] (optional) I have confirmed this bug exists on the master branch of scanpy. `sc.tl.filter_rank_genes_groups()` replaces gene names with ""nan"" values, would be nice to be able to ignore these with `sc.pl.rank_genes_groups()` and instead show the top n actual non-filtered genes. ### Minimal code sample. ```python. adata = sc.datasets.pbmc68k_reduced(). sc.tl.rank_genes_groups(adata, 'bulk_labels', method='wilcoxon'). sc.tl.filter_rank_genes_groups(adata, min_fold_change=3). sc.pl.rank_genes_groups(adata, key=""rank_genes_groups_filtered""). ```. ![image](https://user-images.githubusercontent.com/20436557/107974043-ea8bfc00-6fad-11eb-9368-1ba78336a3b2.png). #### Versions. ```. scanpy==1.7.0 anndata==0.7.5 umap==0.5.1 numpy==1.19.5 scipy==1.5.4 pandas==1.1.5 scikit-learn==0.24.1 statsmodels==0.12.2 python-igraph==0.8.3 leidenalg==0.8.3. ````.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1651
https://github.com/scverse/scanpy/issues/1651:58,performance,error,error,58,"Option to ignore ""nan"" with sc.pl.rank_genes_groups() and error while writing data to .h5ad; - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [X] (optional) I have confirmed this bug exists on the master branch of scanpy. `sc.tl.filter_rank_genes_groups()` replaces gene names with ""nan"" values, would be nice to be able to ignore these with `sc.pl.rank_genes_groups()` and instead show the top n actual non-filtered genes. ### Minimal code sample. ```python. adata = sc.datasets.pbmc68k_reduced(). sc.tl.rank_genes_groups(adata, 'bulk_labels', method='wilcoxon'). sc.tl.filter_rank_genes_groups(adata, min_fold_change=3). sc.pl.rank_genes_groups(adata, key=""rank_genes_groups_filtered""). ```. ![image](https://user-images.githubusercontent.com/20436557/107974043-ea8bfc00-6fad-11eb-9368-1ba78336a3b2.png). #### Versions. ```. scanpy==1.7.0 anndata==0.7.5 umap==0.5.1 numpy==1.19.5 scipy==1.5.4 pandas==1.1.5 scikit-learn==0.24.1 statsmodels==0.12.2 python-igraph==0.8.3 leidenalg==0.8.3. ````.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1651
https://github.com/scverse/scanpy/issues/1651:58,safety,error,error,58,"Option to ignore ""nan"" with sc.pl.rank_genes_groups() and error while writing data to .h5ad; - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [X] (optional) I have confirmed this bug exists on the master branch of scanpy. `sc.tl.filter_rank_genes_groups()` replaces gene names with ""nan"" values, would be nice to be able to ignore these with `sc.pl.rank_genes_groups()` and instead show the top n actual non-filtered genes. ### Minimal code sample. ```python. adata = sc.datasets.pbmc68k_reduced(). sc.tl.rank_genes_groups(adata, 'bulk_labels', method='wilcoxon'). sc.tl.filter_rank_genes_groups(adata, min_fold_change=3). sc.pl.rank_genes_groups(adata, key=""rank_genes_groups_filtered""). ```. ![image](https://user-images.githubusercontent.com/20436557/107974043-ea8bfc00-6fad-11eb-9368-1ba78336a3b2.png). #### Versions. ```. scanpy==1.7.0 anndata==0.7.5 umap==0.5.1 numpy==1.19.5 scipy==1.5.4 pandas==1.1.5 scikit-learn==0.24.1 statsmodels==0.12.2 python-igraph==0.8.3 leidenalg==0.8.3. ````.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1651
https://github.com/scverse/scanpy/issues/1651:58,usability,error,error,58,"Option to ignore ""nan"" with sc.pl.rank_genes_groups() and error while writing data to .h5ad; - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [X] (optional) I have confirmed this bug exists on the master branch of scanpy. `sc.tl.filter_rank_genes_groups()` replaces gene names with ""nan"" values, would be nice to be able to ignore these with `sc.pl.rank_genes_groups()` and instead show the top n actual non-filtered genes. ### Minimal code sample. ```python. adata = sc.datasets.pbmc68k_reduced(). sc.tl.rank_genes_groups(adata, 'bulk_labels', method='wilcoxon'). sc.tl.filter_rank_genes_groups(adata, min_fold_change=3). sc.pl.rank_genes_groups(adata, key=""rank_genes_groups_filtered""). ```. ![image](https://user-images.githubusercontent.com/20436557/107974043-ea8bfc00-6fad-11eb-9368-1ba78336a3b2.png). #### Versions. ```. scanpy==1.7.0 anndata==0.7.5 umap==0.5.1 numpy==1.19.5 scipy==1.5.4 pandas==1.1.5 scikit-learn==0.24.1 statsmodels==0.12.2 python-igraph==0.8.3 leidenalg==0.8.3. ````.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1651
https://github.com/scverse/scanpy/issues/1651:174,usability,confirm,confirmed,174,"Option to ignore ""nan"" with sc.pl.rank_genes_groups() and error while writing data to .h5ad; - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [X] (optional) I have confirmed this bug exists on the master branch of scanpy. `sc.tl.filter_rank_genes_groups()` replaces gene names with ""nan"" values, would be nice to be able to ignore these with `sc.pl.rank_genes_groups()` and instead show the top n actual non-filtered genes. ### Minimal code sample. ```python. adata = sc.datasets.pbmc68k_reduced(). sc.tl.rank_genes_groups(adata, 'bulk_labels', method='wilcoxon'). sc.tl.filter_rank_genes_groups(adata, min_fold_change=3). sc.pl.rank_genes_groups(adata, key=""rank_genes_groups_filtered""). ```. ![image](https://user-images.githubusercontent.com/20436557/107974043-ea8bfc00-6fad-11eb-9368-1ba78336a3b2.png). #### Versions. ```. scanpy==1.7.0 anndata==0.7.5 umap==0.5.1 numpy==1.19.5 scipy==1.5.4 pandas==1.1.5 scikit-learn==0.24.1 statsmodels==0.12.2 python-igraph==0.8.3 leidenalg==0.8.3. ````.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1651
https://github.com/scverse/scanpy/issues/1651:257,usability,confirm,confirmed,257,"Option to ignore ""nan"" with sc.pl.rank_genes_groups() and error while writing data to .h5ad; - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [X] (optional) I have confirmed this bug exists on the master branch of scanpy. `sc.tl.filter_rank_genes_groups()` replaces gene names with ""nan"" values, would be nice to be able to ignore these with `sc.pl.rank_genes_groups()` and instead show the top n actual non-filtered genes. ### Minimal code sample. ```python. adata = sc.datasets.pbmc68k_reduced(). sc.tl.rank_genes_groups(adata, 'bulk_labels', method='wilcoxon'). sc.tl.filter_rank_genes_groups(adata, min_fold_change=3). sc.pl.rank_genes_groups(adata, key=""rank_genes_groups_filtered""). ```. ![image](https://user-images.githubusercontent.com/20436557/107974043-ea8bfc00-6fad-11eb-9368-1ba78336a3b2.png). #### Versions. ```. scanpy==1.7.0 anndata==0.7.5 umap==0.5.1 numpy==1.19.5 scipy==1.5.4 pandas==1.1.5 scikit-learn==0.24.1 statsmodels==0.12.2 python-igraph==0.8.3 leidenalg==0.8.3. ````.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1651
https://github.com/scverse/scanpy/issues/1651:521,usability,Minim,Minimal,521,"Option to ignore ""nan"" with sc.pl.rank_genes_groups() and error while writing data to .h5ad; - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [X] (optional) I have confirmed this bug exists on the master branch of scanpy. `sc.tl.filter_rank_genes_groups()` replaces gene names with ""nan"" values, would be nice to be able to ignore these with `sc.pl.rank_genes_groups()` and instead show the top n actual non-filtered genes. ### Minimal code sample. ```python. adata = sc.datasets.pbmc68k_reduced(). sc.tl.rank_genes_groups(adata, 'bulk_labels', method='wilcoxon'). sc.tl.filter_rank_genes_groups(adata, min_fold_change=3). sc.pl.rank_genes_groups(adata, key=""rank_genes_groups_filtered""). ```. ![image](https://user-images.githubusercontent.com/20436557/107974043-ea8bfc00-6fad-11eb-9368-1ba78336a3b2.png). #### Versions. ```. scanpy==1.7.0 anndata==0.7.5 umap==0.5.1 numpy==1.19.5 scipy==1.5.4 pandas==1.1.5 scikit-learn==0.24.1 statsmodels==0.12.2 python-igraph==0.8.3 leidenalg==0.8.3. ````.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1651
https://github.com/scverse/scanpy/issues/1651:804,usability,user,user-images,804,"Option to ignore ""nan"" with sc.pl.rank_genes_groups() and error while writing data to .h5ad; - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [X] (optional) I have confirmed this bug exists on the master branch of scanpy. `sc.tl.filter_rank_genes_groups()` replaces gene names with ""nan"" values, would be nice to be able to ignore these with `sc.pl.rank_genes_groups()` and instead show the top n actual non-filtered genes. ### Minimal code sample. ```python. adata = sc.datasets.pbmc68k_reduced(). sc.tl.rank_genes_groups(adata, 'bulk_labels', method='wilcoxon'). sc.tl.filter_rank_genes_groups(adata, min_fold_change=3). sc.pl.rank_genes_groups(adata, key=""rank_genes_groups_filtered""). ```. ![image](https://user-images.githubusercontent.com/20436557/107974043-ea8bfc00-6fad-11eb-9368-1ba78336a3b2.png). #### Versions. ```. scanpy==1.7.0 anndata==0.7.5 umap==0.5.1 numpy==1.19.5 scipy==1.5.4 pandas==1.1.5 scikit-learn==0.24.1 statsmodels==0.12.2 python-igraph==0.8.3 leidenalg==0.8.3. ````.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1651
https://github.com/scverse/scanpy/issues/1651:1009,usability,learn,learn,1009,"Option to ignore ""nan"" with sc.pl.rank_genes_groups() and error while writing data to .h5ad; - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [X] (optional) I have confirmed this bug exists on the master branch of scanpy. `sc.tl.filter_rank_genes_groups()` replaces gene names with ""nan"" values, would be nice to be able to ignore these with `sc.pl.rank_genes_groups()` and instead show the top n actual non-filtered genes. ### Minimal code sample. ```python. adata = sc.datasets.pbmc68k_reduced(). sc.tl.rank_genes_groups(adata, 'bulk_labels', method='wilcoxon'). sc.tl.filter_rank_genes_groups(adata, min_fold_change=3). sc.pl.rank_genes_groups(adata, key=""rank_genes_groups_filtered""). ```. ![image](https://user-images.githubusercontent.com/20436557/107974043-ea8bfc00-6fad-11eb-9368-1ba78336a3b2.png). #### Versions. ```. scanpy==1.7.0 anndata==0.7.5 umap==0.5.1 numpy==1.19.5 scipy==1.5.4 pandas==1.1.5 scikit-learn==0.24.1 statsmodels==0.12.2 python-igraph==0.8.3 leidenalg==0.8.3. ````.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1651
https://github.com/scverse/scanpy/issues/1652:3935,availability,error,errors,3935,"e_nndescent as sparse_nnd. 23 import pynndescent.distances as pynnd_dist. ~/.local/lib/python3.9/site-packages/pynndescent/sparse.py in <module>. 341 },. 342 ). --> 343 def sparse_alternative_jaccard(ind1, data1, ind2, data2):. 344 num_non_zero = arr_union(ind1, ind2).shape[0]. 345 num_equal = arr_intersect(ind1, ind2).shape[0]. ~/.local/lib/python3.9/site-packages/numba/core/decorators.py in wrapper(func). 216 with typeinfer.register_dispatcher(disp):. 217 for sig in sigs:. --> 218 disp.compile(sig). 219 disp.disable_compile(). 220 return disp. ~/.local/lib/python3.9/site-packages/numba/core/compiler_lock.py in _acquire_compile_lock(*args, **kwargs). 30 def _acquire_compile_lock(*args, **kwargs):. 31 with self:. ---> 32 return func(*args, **kwargs). 33 return _acquire_compile_lock. 34 . ~/.local/lib/python3.9/site-packages/numba/core/dispatcher.py in compile(self, sig). 817 self._cache_misses[sig] += 1. 818 try:. --> 819 cres = self._compiler.compile(args, return_type). 820 except errors.ForceLiteralArg as e:. 821 def folded(args, kws):. ~/.local/lib/python3.9/site-packages/numba/core/dispatcher.py in compile(self, args, return_type). 80 return retval. 81 else:. ---> 82 raise retval. 83 . 84 def _compile_cached(self, args, return_type):. ~/.local/lib/python3.9/site-packages/numba/core/dispatcher.py in _compile_cached(self, args, return_type). 90 . 91 try:. ---> 92 retval = self._compile_core(args, return_type). 93 except errors.TypingError as e:. 94 self._failed_cache[key] = e. ~/.local/lib/python3.9/site-packages/numba/core/dispatcher.py in _compile_core(self, args, return_type). 103 . 104 impl = self._get_implementation(args, {}). --> 105 cres = compiler.compile_extra(self.targetdescr.typing_context,. 106 self.targetdescr.target_context,. 107 impl,. ~/.local/lib/python3.9/site-packages/numba/core/compiler.py in compile_extra(typingctx, targetctx, func, args, return_type, flags, locals, library, pipeline_class). 625 pipeline = pipeline_class(typingctx, targetctx, ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:4384,availability,error,errors,4384,"(disp):. 217 for sig in sigs:. --> 218 disp.compile(sig). 219 disp.disable_compile(). 220 return disp. ~/.local/lib/python3.9/site-packages/numba/core/compiler_lock.py in _acquire_compile_lock(*args, **kwargs). 30 def _acquire_compile_lock(*args, **kwargs):. 31 with self:. ---> 32 return func(*args, **kwargs). 33 return _acquire_compile_lock. 34 . ~/.local/lib/python3.9/site-packages/numba/core/dispatcher.py in compile(self, sig). 817 self._cache_misses[sig] += 1. 818 try:. --> 819 cres = self._compiler.compile(args, return_type). 820 except errors.ForceLiteralArg as e:. 821 def folded(args, kws):. ~/.local/lib/python3.9/site-packages/numba/core/dispatcher.py in compile(self, args, return_type). 80 return retval. 81 else:. ---> 82 raise retval. 83 . 84 def _compile_cached(self, args, return_type):. ~/.local/lib/python3.9/site-packages/numba/core/dispatcher.py in _compile_cached(self, args, return_type). 90 . 91 try:. ---> 92 retval = self._compile_core(args, return_type). 93 except errors.TypingError as e:. 94 self._failed_cache[key] = e. ~/.local/lib/python3.9/site-packages/numba/core/dispatcher.py in _compile_core(self, args, return_type). 103 . 104 impl = self._get_implementation(args, {}). --> 105 cres = compiler.compile_extra(self.targetdescr.typing_context,. 106 self.targetdescr.target_context,. 107 impl,. ~/.local/lib/python3.9/site-packages/numba/core/compiler.py in compile_extra(typingctx, targetctx, func, args, return_type, flags, locals, library, pipeline_class). 625 pipeline = pipeline_class(typingctx, targetctx, library,. 626 args, return_type, flags, locals). --> 627 return pipeline.compile_extra(func). 628 . 629 . ~/.local/lib/python3.9/site-packages/numba/core/compiler.py in compile_extra(self, func). 361 self.state.lifted = (). 362 self.state.lifted_from = None. --> 363 return self._compile_bytecode(). 364 . 365 def compile_ir(self, func_ir, lifted=(), lifted_from=None):. ~/.local/lib/python3.9/site-packages/numba/core/compiler.py in _compile_byteco",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:5143,availability,state,state,5143," 84 def _compile_cached(self, args, return_type):. ~/.local/lib/python3.9/site-packages/numba/core/dispatcher.py in _compile_cached(self, args, return_type). 90 . 91 try:. ---> 92 retval = self._compile_core(args, return_type). 93 except errors.TypingError as e:. 94 self._failed_cache[key] = e. ~/.local/lib/python3.9/site-packages/numba/core/dispatcher.py in _compile_core(self, args, return_type). 103 . 104 impl = self._get_implementation(args, {}). --> 105 cres = compiler.compile_extra(self.targetdescr.typing_context,. 106 self.targetdescr.target_context,. 107 impl,. ~/.local/lib/python3.9/site-packages/numba/core/compiler.py in compile_extra(typingctx, targetctx, func, args, return_type, flags, locals, library, pipeline_class). 625 pipeline = pipeline_class(typingctx, targetctx, library,. 626 args, return_type, flags, locals). --> 627 return pipeline.compile_extra(func). 628 . 629 . ~/.local/lib/python3.9/site-packages/numba/core/compiler.py in compile_extra(self, func). 361 self.state.lifted = (). 362 self.state.lifted_from = None. --> 363 return self._compile_bytecode(). 364 . 365 def compile_ir(self, func_ir, lifted=(), lifted_from=None):. ~/.local/lib/python3.9/site-packages/numba/core/compiler.py in _compile_bytecode(self). 423 """""". 424 assert self.state.func_ir is None. --> 425 return self._compile_core(). 426 . 427 def _compile_ir(self):. ~/.local/lib/python3.9/site-packages/numba/core/compiler.py in _compile_core(self). 403 self.state.status.fail_reason = e. 404 if is_final_pipeline:. --> 405 raise e. 406 else:. 407 raise CompilerError(""All available pipelines exhausted""). ~/.local/lib/python3.9/site-packages/numba/core/compiler.py in _compile_core(self). 394 res = None. 395 try:. --> 396 pm.run(self.state). 397 if self.state.cr is not None:. 398 break. ~/.local/lib/python3.9/site-packages/numba/core/compiler_machinery.py in run(self, state). 339 (self.pipeline_name, pass_desc). 340 patched_exception = self._patch_error(msg, e). --> 341 raise patched_excep",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:5171,availability,state,state,5171,", args, return_type):. ~/.local/lib/python3.9/site-packages/numba/core/dispatcher.py in _compile_cached(self, args, return_type). 90 . 91 try:. ---> 92 retval = self._compile_core(args, return_type). 93 except errors.TypingError as e:. 94 self._failed_cache[key] = e. ~/.local/lib/python3.9/site-packages/numba/core/dispatcher.py in _compile_core(self, args, return_type). 103 . 104 impl = self._get_implementation(args, {}). --> 105 cres = compiler.compile_extra(self.targetdescr.typing_context,. 106 self.targetdescr.target_context,. 107 impl,. ~/.local/lib/python3.9/site-packages/numba/core/compiler.py in compile_extra(typingctx, targetctx, func, args, return_type, flags, locals, library, pipeline_class). 625 pipeline = pipeline_class(typingctx, targetctx, library,. 626 args, return_type, flags, locals). --> 627 return pipeline.compile_extra(func). 628 . 629 . ~/.local/lib/python3.9/site-packages/numba/core/compiler.py in compile_extra(self, func). 361 self.state.lifted = (). 362 self.state.lifted_from = None. --> 363 return self._compile_bytecode(). 364 . 365 def compile_ir(self, func_ir, lifted=(), lifted_from=None):. ~/.local/lib/python3.9/site-packages/numba/core/compiler.py in _compile_bytecode(self). 423 """""". 424 assert self.state.func_ir is None. --> 425 return self._compile_core(). 426 . 427 def _compile_ir(self):. ~/.local/lib/python3.9/site-packages/numba/core/compiler.py in _compile_core(self). 403 self.state.status.fail_reason = e. 404 if is_final_pipeline:. --> 405 raise e. 406 else:. 407 raise CompilerError(""All available pipelines exhausted""). ~/.local/lib/python3.9/site-packages/numba/core/compiler.py in _compile_core(self). 394 res = None. 395 try:. --> 396 pm.run(self.state). 397 if self.state.cr is not None:. 398 break. ~/.local/lib/python3.9/site-packages/numba/core/compiler_machinery.py in run(self, state). 339 (self.pipeline_name, pass_desc). 340 patched_exception = self._patch_error(msg, e). --> 341 raise patched_exception. 342 . 343 def dependen",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:5422,availability,state,state,5422,"_cache[key] = e. ~/.local/lib/python3.9/site-packages/numba/core/dispatcher.py in _compile_core(self, args, return_type). 103 . 104 impl = self._get_implementation(args, {}). --> 105 cres = compiler.compile_extra(self.targetdescr.typing_context,. 106 self.targetdescr.target_context,. 107 impl,. ~/.local/lib/python3.9/site-packages/numba/core/compiler.py in compile_extra(typingctx, targetctx, func, args, return_type, flags, locals, library, pipeline_class). 625 pipeline = pipeline_class(typingctx, targetctx, library,. 626 args, return_type, flags, locals). --> 627 return pipeline.compile_extra(func). 628 . 629 . ~/.local/lib/python3.9/site-packages/numba/core/compiler.py in compile_extra(self, func). 361 self.state.lifted = (). 362 self.state.lifted_from = None. --> 363 return self._compile_bytecode(). 364 . 365 def compile_ir(self, func_ir, lifted=(), lifted_from=None):. ~/.local/lib/python3.9/site-packages/numba/core/compiler.py in _compile_bytecode(self). 423 """""". 424 assert self.state.func_ir is None. --> 425 return self._compile_core(). 426 . 427 def _compile_ir(self):. ~/.local/lib/python3.9/site-packages/numba/core/compiler.py in _compile_core(self). 403 self.state.status.fail_reason = e. 404 if is_final_pipeline:. --> 405 raise e. 406 else:. 407 raise CompilerError(""All available pipelines exhausted""). ~/.local/lib/python3.9/site-packages/numba/core/compiler.py in _compile_core(self). 394 res = None. 395 try:. --> 396 pm.run(self.state). 397 if self.state.cr is not None:. 398 break. ~/.local/lib/python3.9/site-packages/numba/core/compiler_machinery.py in run(self, state). 339 (self.pipeline_name, pass_desc). 340 patched_exception = self._patch_error(msg, e). --> 341 raise patched_exception. 342 . 343 def dependency_analysis(self):. ~/.local/lib/python3.9/site-packages/numba/core/compiler_machinery.py in run(self, state). 330 pass_inst = _pass_registry.get(pss).pass_inst. 331 if isinstance(pass_inst, CompilerPass):. --> 332 self._runPass(idx, pass_inst, state)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:5609,availability,state,state,5609," = compiler.compile_extra(self.targetdescr.typing_context,. 106 self.targetdescr.target_context,. 107 impl,. ~/.local/lib/python3.9/site-packages/numba/core/compiler.py in compile_extra(typingctx, targetctx, func, args, return_type, flags, locals, library, pipeline_class). 625 pipeline = pipeline_class(typingctx, targetctx, library,. 626 args, return_type, flags, locals). --> 627 return pipeline.compile_extra(func). 628 . 629 . ~/.local/lib/python3.9/site-packages/numba/core/compiler.py in compile_extra(self, func). 361 self.state.lifted = (). 362 self.state.lifted_from = None. --> 363 return self._compile_bytecode(). 364 . 365 def compile_ir(self, func_ir, lifted=(), lifted_from=None):. ~/.local/lib/python3.9/site-packages/numba/core/compiler.py in _compile_bytecode(self). 423 """""". 424 assert self.state.func_ir is None. --> 425 return self._compile_core(). 426 . 427 def _compile_ir(self):. ~/.local/lib/python3.9/site-packages/numba/core/compiler.py in _compile_core(self). 403 self.state.status.fail_reason = e. 404 if is_final_pipeline:. --> 405 raise e. 406 else:. 407 raise CompilerError(""All available pipelines exhausted""). ~/.local/lib/python3.9/site-packages/numba/core/compiler.py in _compile_core(self). 394 res = None. 395 try:. --> 396 pm.run(self.state). 397 if self.state.cr is not None:. 398 break. ~/.local/lib/python3.9/site-packages/numba/core/compiler_machinery.py in run(self, state). 339 (self.pipeline_name, pass_desc). 340 patched_exception = self._patch_error(msg, e). --> 341 raise patched_exception. 342 . 343 def dependency_analysis(self):. ~/.local/lib/python3.9/site-packages/numba/core/compiler_machinery.py in run(self, state). 330 pass_inst = _pass_registry.get(pss).pass_inst. 331 if isinstance(pass_inst, CompilerPass):. --> 332 self._runPass(idx, pass_inst, state). 333 else:. 334 raise BaseException(""Legacy pass in use""). ~/.local/lib/python3.9/site-packages/numba/core/compiler_lock.py in _acquire_compile_lock(*args, **kwargs). 30 def _acquire_com",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:5723,availability,avail,available,5723,"l/lib/python3.9/site-packages/numba/core/compiler.py in compile_extra(typingctx, targetctx, func, args, return_type, flags, locals, library, pipeline_class). 625 pipeline = pipeline_class(typingctx, targetctx, library,. 626 args, return_type, flags, locals). --> 627 return pipeline.compile_extra(func). 628 . 629 . ~/.local/lib/python3.9/site-packages/numba/core/compiler.py in compile_extra(self, func). 361 self.state.lifted = (). 362 self.state.lifted_from = None. --> 363 return self._compile_bytecode(). 364 . 365 def compile_ir(self, func_ir, lifted=(), lifted_from=None):. ~/.local/lib/python3.9/site-packages/numba/core/compiler.py in _compile_bytecode(self). 423 """""". 424 assert self.state.func_ir is None. --> 425 return self._compile_core(). 426 . 427 def _compile_ir(self):. ~/.local/lib/python3.9/site-packages/numba/core/compiler.py in _compile_core(self). 403 self.state.status.fail_reason = e. 404 if is_final_pipeline:. --> 405 raise e. 406 else:. 407 raise CompilerError(""All available pipelines exhausted""). ~/.local/lib/python3.9/site-packages/numba/core/compiler.py in _compile_core(self). 394 res = None. 395 try:. --> 396 pm.run(self.state). 397 if self.state.cr is not None:. 398 break. ~/.local/lib/python3.9/site-packages/numba/core/compiler_machinery.py in run(self, state). 339 (self.pipeline_name, pass_desc). 340 patched_exception = self._patch_error(msg, e). --> 341 raise patched_exception. 342 . 343 def dependency_analysis(self):. ~/.local/lib/python3.9/site-packages/numba/core/compiler_machinery.py in run(self, state). 330 pass_inst = _pass_registry.get(pss).pass_inst. 331 if isinstance(pass_inst, CompilerPass):. --> 332 self._runPass(idx, pass_inst, state). 333 else:. 334 raise BaseException(""Legacy pass in use""). ~/.local/lib/python3.9/site-packages/numba/core/compiler_lock.py in _acquire_compile_lock(*args, **kwargs). 30 def _acquire_compile_lock(*args, **kwargs):. 31 with self:. ---> 32 return func(*args, **kwargs). 33 return _acquire_compile_lock. 3",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:5886,availability,state,state,5886," pipeline = pipeline_class(typingctx, targetctx, library,. 626 args, return_type, flags, locals). --> 627 return pipeline.compile_extra(func). 628 . 629 . ~/.local/lib/python3.9/site-packages/numba/core/compiler.py in compile_extra(self, func). 361 self.state.lifted = (). 362 self.state.lifted_from = None. --> 363 return self._compile_bytecode(). 364 . 365 def compile_ir(self, func_ir, lifted=(), lifted_from=None):. ~/.local/lib/python3.9/site-packages/numba/core/compiler.py in _compile_bytecode(self). 423 """""". 424 assert self.state.func_ir is None. --> 425 return self._compile_core(). 426 . 427 def _compile_ir(self):. ~/.local/lib/python3.9/site-packages/numba/core/compiler.py in _compile_core(self). 403 self.state.status.fail_reason = e. 404 if is_final_pipeline:. --> 405 raise e. 406 else:. 407 raise CompilerError(""All available pipelines exhausted""). ~/.local/lib/python3.9/site-packages/numba/core/compiler.py in _compile_core(self). 394 res = None. 395 try:. --> 396 pm.run(self.state). 397 if self.state.cr is not None:. 398 break. ~/.local/lib/python3.9/site-packages/numba/core/compiler_machinery.py in run(self, state). 339 (self.pipeline_name, pass_desc). 340 patched_exception = self._patch_error(msg, e). --> 341 raise patched_exception. 342 . 343 def dependency_analysis(self):. ~/.local/lib/python3.9/site-packages/numba/core/compiler_machinery.py in run(self, state). 330 pass_inst = _pass_registry.get(pss).pass_inst. 331 if isinstance(pass_inst, CompilerPass):. --> 332 self._runPass(idx, pass_inst, state). 333 else:. 334 raise BaseException(""Legacy pass in use""). ~/.local/lib/python3.9/site-packages/numba/core/compiler_lock.py in _acquire_compile_lock(*args, **kwargs). 30 def _acquire_compile_lock(*args, **kwargs):. 31 with self:. ---> 32 return func(*args, **kwargs). 33 return _acquire_compile_lock. 34 . ~/.local/lib/python3.9/site-packages/numba/core/compiler_machinery.py in _runPass(self, index, pss, internal_state). 289 mutated |= check(pss.run_initializat",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:5906,availability,state,state,5906,"_class(typingctx, targetctx, library,. 626 args, return_type, flags, locals). --> 627 return pipeline.compile_extra(func). 628 . 629 . ~/.local/lib/python3.9/site-packages/numba/core/compiler.py in compile_extra(self, func). 361 self.state.lifted = (). 362 self.state.lifted_from = None. --> 363 return self._compile_bytecode(). 364 . 365 def compile_ir(self, func_ir, lifted=(), lifted_from=None):. ~/.local/lib/python3.9/site-packages/numba/core/compiler.py in _compile_bytecode(self). 423 """""". 424 assert self.state.func_ir is None. --> 425 return self._compile_core(). 426 . 427 def _compile_ir(self):. ~/.local/lib/python3.9/site-packages/numba/core/compiler.py in _compile_core(self). 403 self.state.status.fail_reason = e. 404 if is_final_pipeline:. --> 405 raise e. 406 else:. 407 raise CompilerError(""All available pipelines exhausted""). ~/.local/lib/python3.9/site-packages/numba/core/compiler.py in _compile_core(self). 394 res = None. 395 try:. --> 396 pm.run(self.state). 397 if self.state.cr is not None:. 398 break. ~/.local/lib/python3.9/site-packages/numba/core/compiler_machinery.py in run(self, state). 339 (self.pipeline_name, pass_desc). 340 patched_exception = self._patch_error(msg, e). --> 341 raise patched_exception. 342 . 343 def dependency_analysis(self):. ~/.local/lib/python3.9/site-packages/numba/core/compiler_machinery.py in run(self, state). 330 pass_inst = _pass_registry.get(pss).pass_inst. 331 if isinstance(pass_inst, CompilerPass):. --> 332 self._runPass(idx, pass_inst, state). 333 else:. 334 raise BaseException(""Legacy pass in use""). ~/.local/lib/python3.9/site-packages/numba/core/compiler_lock.py in _acquire_compile_lock(*args, **kwargs). 30 def _acquire_compile_lock(*args, **kwargs):. 31 with self:. ---> 32 return func(*args, **kwargs). 33 return _acquire_compile_lock. 34 . ~/.local/lib/python3.9/site-packages/numba/core/compiler_machinery.py in _runPass(self, index, pss, internal_state). 289 mutated |= check(pss.run_initialization, internal_state)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:6023,availability,state,state,6023,"unc). 628 . 629 . ~/.local/lib/python3.9/site-packages/numba/core/compiler.py in compile_extra(self, func). 361 self.state.lifted = (). 362 self.state.lifted_from = None. --> 363 return self._compile_bytecode(). 364 . 365 def compile_ir(self, func_ir, lifted=(), lifted_from=None):. ~/.local/lib/python3.9/site-packages/numba/core/compiler.py in _compile_bytecode(self). 423 """""". 424 assert self.state.func_ir is None. --> 425 return self._compile_core(). 426 . 427 def _compile_ir(self):. ~/.local/lib/python3.9/site-packages/numba/core/compiler.py in _compile_core(self). 403 self.state.status.fail_reason = e. 404 if is_final_pipeline:. --> 405 raise e. 406 else:. 407 raise CompilerError(""All available pipelines exhausted""). ~/.local/lib/python3.9/site-packages/numba/core/compiler.py in _compile_core(self). 394 res = None. 395 try:. --> 396 pm.run(self.state). 397 if self.state.cr is not None:. 398 break. ~/.local/lib/python3.9/site-packages/numba/core/compiler_machinery.py in run(self, state). 339 (self.pipeline_name, pass_desc). 340 patched_exception = self._patch_error(msg, e). --> 341 raise patched_exception. 342 . 343 def dependency_analysis(self):. ~/.local/lib/python3.9/site-packages/numba/core/compiler_machinery.py in run(self, state). 330 pass_inst = _pass_registry.get(pss).pass_inst. 331 if isinstance(pass_inst, CompilerPass):. --> 332 self._runPass(idx, pass_inst, state). 333 else:. 334 raise BaseException(""Legacy pass in use""). ~/.local/lib/python3.9/site-packages/numba/core/compiler_lock.py in _acquire_compile_lock(*args, **kwargs). 30 def _acquire_compile_lock(*args, **kwargs):. 31 with self:. ---> 32 return func(*args, **kwargs). 33 return _acquire_compile_lock. 34 . ~/.local/lib/python3.9/site-packages/numba/core/compiler_machinery.py in _runPass(self, index, pss, internal_state). 289 mutated |= check(pss.run_initialization, internal_state). 290 with SimpleTimer() as pass_time:. --> 291 mutated |= check(pss.run_pass, internal_state). 292 with SimpleTimer(",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:6277,availability,state,state,6277,"fted=(), lifted_from=None):. ~/.local/lib/python3.9/site-packages/numba/core/compiler.py in _compile_bytecode(self). 423 """""". 424 assert self.state.func_ir is None. --> 425 return self._compile_core(). 426 . 427 def _compile_ir(self):. ~/.local/lib/python3.9/site-packages/numba/core/compiler.py in _compile_core(self). 403 self.state.status.fail_reason = e. 404 if is_final_pipeline:. --> 405 raise e. 406 else:. 407 raise CompilerError(""All available pipelines exhausted""). ~/.local/lib/python3.9/site-packages/numba/core/compiler.py in _compile_core(self). 394 res = None. 395 try:. --> 396 pm.run(self.state). 397 if self.state.cr is not None:. 398 break. ~/.local/lib/python3.9/site-packages/numba/core/compiler_machinery.py in run(self, state). 339 (self.pipeline_name, pass_desc). 340 patched_exception = self._patch_error(msg, e). --> 341 raise patched_exception. 342 . 343 def dependency_analysis(self):. ~/.local/lib/python3.9/site-packages/numba/core/compiler_machinery.py in run(self, state). 330 pass_inst = _pass_registry.get(pss).pass_inst. 331 if isinstance(pass_inst, CompilerPass):. --> 332 self._runPass(idx, pass_inst, state). 333 else:. 334 raise BaseException(""Legacy pass in use""). ~/.local/lib/python3.9/site-packages/numba/core/compiler_lock.py in _acquire_compile_lock(*args, **kwargs). 30 def _acquire_compile_lock(*args, **kwargs):. 31 with self:. ---> 32 return func(*args, **kwargs). 33 return _acquire_compile_lock. 34 . ~/.local/lib/python3.9/site-packages/numba/core/compiler_machinery.py in _runPass(self, index, pss, internal_state). 289 mutated |= check(pss.run_initialization, internal_state). 290 with SimpleTimer() as pass_time:. --> 291 mutated |= check(pss.run_pass, internal_state). 292 with SimpleTimer() as finalize_time:. 293 mutated |= check(pss.run_finalizer, internal_state). ~/.local/lib/python3.9/site-packages/numba/core/compiler_machinery.py in check(func, compiler_state). 262 . 263 def check(func, compiler_state):. --> 264 mangled = func(compile",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:6419,availability,state,state,6419,"state.func_ir is None. --> 425 return self._compile_core(). 426 . 427 def _compile_ir(self):. ~/.local/lib/python3.9/site-packages/numba/core/compiler.py in _compile_core(self). 403 self.state.status.fail_reason = e. 404 if is_final_pipeline:. --> 405 raise e. 406 else:. 407 raise CompilerError(""All available pipelines exhausted""). ~/.local/lib/python3.9/site-packages/numba/core/compiler.py in _compile_core(self). 394 res = None. 395 try:. --> 396 pm.run(self.state). 397 if self.state.cr is not None:. 398 break. ~/.local/lib/python3.9/site-packages/numba/core/compiler_machinery.py in run(self, state). 339 (self.pipeline_name, pass_desc). 340 patched_exception = self._patch_error(msg, e). --> 341 raise patched_exception. 342 . 343 def dependency_analysis(self):. ~/.local/lib/python3.9/site-packages/numba/core/compiler_machinery.py in run(self, state). 330 pass_inst = _pass_registry.get(pss).pass_inst. 331 if isinstance(pass_inst, CompilerPass):. --> 332 self._runPass(idx, pass_inst, state). 333 else:. 334 raise BaseException(""Legacy pass in use""). ~/.local/lib/python3.9/site-packages/numba/core/compiler_lock.py in _acquire_compile_lock(*args, **kwargs). 30 def _acquire_compile_lock(*args, **kwargs):. 31 with self:. ---> 32 return func(*args, **kwargs). 33 return _acquire_compile_lock. 34 . ~/.local/lib/python3.9/site-packages/numba/core/compiler_machinery.py in _runPass(self, index, pss, internal_state). 289 mutated |= check(pss.run_initialization, internal_state). 290 with SimpleTimer() as pass_time:. --> 291 mutated |= check(pss.run_pass, internal_state). 292 with SimpleTimer() as finalize_time:. 293 mutated |= check(pss.run_finalizer, internal_state). ~/.local/lib/python3.9/site-packages/numba/core/compiler_machinery.py in check(func, compiler_state). 262 . 263 def check(func, compiler_state):. --> 264 mangled = func(compiler_state). 265 if mangled not in (True, False):. 266 msg = (""CompilerPass implementations should return True/False. "". ~/.local/lib/python3.9/s",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:7480,availability,state,state,7480,". ~/.local/lib/python3.9/site-packages/numba/core/compiler_lock.py in _acquire_compile_lock(*args, **kwargs). 30 def _acquire_compile_lock(*args, **kwargs):. 31 with self:. ---> 32 return func(*args, **kwargs). 33 return _acquire_compile_lock. 34 . ~/.local/lib/python3.9/site-packages/numba/core/compiler_machinery.py in _runPass(self, index, pss, internal_state). 289 mutated |= check(pss.run_initialization, internal_state). 290 with SimpleTimer() as pass_time:. --> 291 mutated |= check(pss.run_pass, internal_state). 292 with SimpleTimer() as finalize_time:. 293 mutated |= check(pss.run_finalizer, internal_state). ~/.local/lib/python3.9/site-packages/numba/core/compiler_machinery.py in check(func, compiler_state). 262 . 263 def check(func, compiler_state):. --> 264 mangled = func(compiler_state). 265 if mangled not in (True, False):. 266 msg = (""CompilerPass implementations should return True/False. "". ~/.local/lib/python3.9/site-packages/numba/core/typed_passes.py in run_pass(self, state). 90 % (state.func_id.func_name,)):. 91 # Type inference. ---> 92 typemap, return_type, calltypes = type_inference_stage(. 93 state.typingctx,. 94 state.func_ir,. ~/.local/lib/python3.9/site-packages/numba/core/typed_passes.py in type_inference_stage(typingctx, interp, args, return_type, locals, raise_errors). 68 . 69 infer.build_constraint(). ---> 70 infer.propagate(raise_errors=raise_errors). 71 typemap, restype, calltypes = infer.unify(raise_errors=raise_errors). 72 . ~/.local/lib/python3.9/site-packages/numba/core/typeinfer.py in propagate(self, raise_errors). 1069 if isinstance(e, ForceLiteralArg)]. 1070 if not force_lit_args:. -> 1071 raise errors[0]. 1072 else:. 1073 raise reduce(operator.or_, force_lit_args). TypingError: Failed in nopython mode pipeline (step: nopython frontend). Failed in nopython mode pipeline (step: nopython frontend). Failed in nopython mode pipeline (step: nopython mode backend). Failed in nopython mode pipeline (step: nopython mode backend). Failed in",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:7494,availability,state,state,7494,"/python3.9/site-packages/numba/core/compiler_lock.py in _acquire_compile_lock(*args, **kwargs). 30 def _acquire_compile_lock(*args, **kwargs):. 31 with self:. ---> 32 return func(*args, **kwargs). 33 return _acquire_compile_lock. 34 . ~/.local/lib/python3.9/site-packages/numba/core/compiler_machinery.py in _runPass(self, index, pss, internal_state). 289 mutated |= check(pss.run_initialization, internal_state). 290 with SimpleTimer() as pass_time:. --> 291 mutated |= check(pss.run_pass, internal_state). 292 with SimpleTimer() as finalize_time:. 293 mutated |= check(pss.run_finalizer, internal_state). ~/.local/lib/python3.9/site-packages/numba/core/compiler_machinery.py in check(func, compiler_state). 262 . 263 def check(func, compiler_state):. --> 264 mangled = func(compiler_state). 265 if mangled not in (True, False):. 266 msg = (""CompilerPass implementations should return True/False. "". ~/.local/lib/python3.9/site-packages/numba/core/typed_passes.py in run_pass(self, state). 90 % (state.func_id.func_name,)):. 91 # Type inference. ---> 92 typemap, return_type, calltypes = type_inference_stage(. 93 state.typingctx,. 94 state.func_ir,. ~/.local/lib/python3.9/site-packages/numba/core/typed_passes.py in type_inference_stage(typingctx, interp, args, return_type, locals, raise_errors). 68 . 69 infer.build_constraint(). ---> 70 infer.propagate(raise_errors=raise_errors). 71 typemap, restype, calltypes = infer.unify(raise_errors=raise_errors). 72 . ~/.local/lib/python3.9/site-packages/numba/core/typeinfer.py in propagate(self, raise_errors). 1069 if isinstance(e, ForceLiteralArg)]. 1070 if not force_lit_args:. -> 1071 raise errors[0]. 1072 else:. 1073 raise reduce(operator.or_, force_lit_args). TypingError: Failed in nopython mode pipeline (step: nopython frontend). Failed in nopython mode pipeline (step: nopython frontend). Failed in nopython mode pipeline (step: nopython mode backend). Failed in nopython mode pipeline (step: nopython mode backend). Failed in nopython mode",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:7612,availability,state,state,7612,"e_lock(*args, **kwargs):. 31 with self:. ---> 32 return func(*args, **kwargs). 33 return _acquire_compile_lock. 34 . ~/.local/lib/python3.9/site-packages/numba/core/compiler_machinery.py in _runPass(self, index, pss, internal_state). 289 mutated |= check(pss.run_initialization, internal_state). 290 with SimpleTimer() as pass_time:. --> 291 mutated |= check(pss.run_pass, internal_state). 292 with SimpleTimer() as finalize_time:. 293 mutated |= check(pss.run_finalizer, internal_state). ~/.local/lib/python3.9/site-packages/numba/core/compiler_machinery.py in check(func, compiler_state). 262 . 263 def check(func, compiler_state):. --> 264 mangled = func(compiler_state). 265 if mangled not in (True, False):. 266 msg = (""CompilerPass implementations should return True/False. "". ~/.local/lib/python3.9/site-packages/numba/core/typed_passes.py in run_pass(self, state). 90 % (state.func_id.func_name,)):. 91 # Type inference. ---> 92 typemap, return_type, calltypes = type_inference_stage(. 93 state.typingctx,. 94 state.func_ir,. ~/.local/lib/python3.9/site-packages/numba/core/typed_passes.py in type_inference_stage(typingctx, interp, args, return_type, locals, raise_errors). 68 . 69 infer.build_constraint(). ---> 70 infer.propagate(raise_errors=raise_errors). 71 typemap, restype, calltypes = infer.unify(raise_errors=raise_errors). 72 . ~/.local/lib/python3.9/site-packages/numba/core/typeinfer.py in propagate(self, raise_errors). 1069 if isinstance(e, ForceLiteralArg)]. 1070 if not force_lit_args:. -> 1071 raise errors[0]. 1072 else:. 1073 raise reduce(operator.or_, force_lit_args). TypingError: Failed in nopython mode pipeline (step: nopython frontend). Failed in nopython mode pipeline (step: nopython frontend). Failed in nopython mode pipeline (step: nopython mode backend). Failed in nopython mode pipeline (step: nopython mode backend). Failed in nopython mode pipeline (step: nopython frontend). No implementation of function Function(<function make_quicksort_impl.<locals>.run",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:7633,availability,state,state,7633,"s):. 31 with self:. ---> 32 return func(*args, **kwargs). 33 return _acquire_compile_lock. 34 . ~/.local/lib/python3.9/site-packages/numba/core/compiler_machinery.py in _runPass(self, index, pss, internal_state). 289 mutated |= check(pss.run_initialization, internal_state). 290 with SimpleTimer() as pass_time:. --> 291 mutated |= check(pss.run_pass, internal_state). 292 with SimpleTimer() as finalize_time:. 293 mutated |= check(pss.run_finalizer, internal_state). ~/.local/lib/python3.9/site-packages/numba/core/compiler_machinery.py in check(func, compiler_state). 262 . 263 def check(func, compiler_state):. --> 264 mangled = func(compiler_state). 265 if mangled not in (True, False):. 266 msg = (""CompilerPass implementations should return True/False. "". ~/.local/lib/python3.9/site-packages/numba/core/typed_passes.py in run_pass(self, state). 90 % (state.func_id.func_name,)):. 91 # Type inference. ---> 92 typemap, return_type, calltypes = type_inference_stage(. 93 state.typingctx,. 94 state.func_ir,. ~/.local/lib/python3.9/site-packages/numba/core/typed_passes.py in type_inference_stage(typingctx, interp, args, return_type, locals, raise_errors). 68 . 69 infer.build_constraint(). ---> 70 infer.propagate(raise_errors=raise_errors). 71 typemap, restype, calltypes = infer.unify(raise_errors=raise_errors). 72 . ~/.local/lib/python3.9/site-packages/numba/core/typeinfer.py in propagate(self, raise_errors). 1069 if isinstance(e, ForceLiteralArg)]. 1070 if not force_lit_args:. -> 1071 raise errors[0]. 1072 else:. 1073 raise reduce(operator.or_, force_lit_args). TypingError: Failed in nopython mode pipeline (step: nopython frontend). Failed in nopython mode pipeline (step: nopython frontend). Failed in nopython mode pipeline (step: nopython mode backend). Failed in nopython mode pipeline (step: nopython mode backend). Failed in nopython mode pipeline (step: nopython frontend). No implementation of function Function(<function make_quicksort_impl.<locals>.run_quicksort at 0x7f455",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:8141,availability,error,errors,8141,"ba/core/compiler_machinery.py in check(func, compiler_state). 262 . 263 def check(func, compiler_state):. --> 264 mangled = func(compiler_state). 265 if mangled not in (True, False):. 266 msg = (""CompilerPass implementations should return True/False. "". ~/.local/lib/python3.9/site-packages/numba/core/typed_passes.py in run_pass(self, state). 90 % (state.func_id.func_name,)):. 91 # Type inference. ---> 92 typemap, return_type, calltypes = type_inference_stage(. 93 state.typingctx,. 94 state.func_ir,. ~/.local/lib/python3.9/site-packages/numba/core/typed_passes.py in type_inference_stage(typingctx, interp, args, return_type, locals, raise_errors). 68 . 69 infer.build_constraint(). ---> 70 infer.propagate(raise_errors=raise_errors). 71 typemap, restype, calltypes = infer.unify(raise_errors=raise_errors). 72 . ~/.local/lib/python3.9/site-packages/numba/core/typeinfer.py in propagate(self, raise_errors). 1069 if isinstance(e, ForceLiteralArg)]. 1070 if not force_lit_args:. -> 1071 raise errors[0]. 1072 else:. 1073 raise reduce(operator.or_, force_lit_args). TypingError: Failed in nopython mode pipeline (step: nopython frontend). Failed in nopython mode pipeline (step: nopython frontend). Failed in nopython mode pipeline (step: nopython mode backend). Failed in nopython mode pipeline (step: nopython mode backend). Failed in nopython mode pipeline (step: nopython frontend). No implementation of function Function(<function make_quicksort_impl.<locals>.run_quicksort at 0x7f455d24c280>) found for signature:. . >>> run_quicksort(array(int32, 1d, C)). . There are 2 candidate implementations:. - Of which 2 did not match due to:. Overload in function 'register_jitable.<locals>.wrap.<locals>.ov_wrap': File: numba/core/extending.py: Line 150. With argument(s): '(array(int32, 1d, C))':. Rejected as the implementation raised a specific error:. UnsupportedError: Failed in nopython mode pipeline (step: analyzing bytecode). Use of unsupported opcode (LOAD_ASSERTION_ERROR) found. . File ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:8182,availability,operat,operator,8182,"c, compiler_state). 262 . 263 def check(func, compiler_state):. --> 264 mangled = func(compiler_state). 265 if mangled not in (True, False):. 266 msg = (""CompilerPass implementations should return True/False. "". ~/.local/lib/python3.9/site-packages/numba/core/typed_passes.py in run_pass(self, state). 90 % (state.func_id.func_name,)):. 91 # Type inference. ---> 92 typemap, return_type, calltypes = type_inference_stage(. 93 state.typingctx,. 94 state.func_ir,. ~/.local/lib/python3.9/site-packages/numba/core/typed_passes.py in type_inference_stage(typingctx, interp, args, return_type, locals, raise_errors). 68 . 69 infer.build_constraint(). ---> 70 infer.propagate(raise_errors=raise_errors). 71 typemap, restype, calltypes = infer.unify(raise_errors=raise_errors). 72 . ~/.local/lib/python3.9/site-packages/numba/core/typeinfer.py in propagate(self, raise_errors). 1069 if isinstance(e, ForceLiteralArg)]. 1070 if not force_lit_args:. -> 1071 raise errors[0]. 1072 else:. 1073 raise reduce(operator.or_, force_lit_args). TypingError: Failed in nopython mode pipeline (step: nopython frontend). Failed in nopython mode pipeline (step: nopython frontend). Failed in nopython mode pipeline (step: nopython mode backend). Failed in nopython mode pipeline (step: nopython mode backend). Failed in nopython mode pipeline (step: nopython frontend). No implementation of function Function(<function make_quicksort_impl.<locals>.run_quicksort at 0x7f455d24c280>) found for signature:. . >>> run_quicksort(array(int32, 1d, C)). . There are 2 candidate implementations:. - Of which 2 did not match due to:. Overload in function 'register_jitable.<locals>.wrap.<locals>.ov_wrap': File: numba/core/extending.py: Line 150. With argument(s): '(array(int32, 1d, C))':. Rejected as the implementation raised a specific error:. UnsupportedError: Failed in nopython mode pipeline (step: analyzing bytecode). Use of unsupported opcode (LOAD_ASSERTION_ERROR) found. . File ""../../../../.local/lib/python3.9/site-pac",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:8994,availability,error,error,8994,"s/numba/core/typeinfer.py in propagate(self, raise_errors). 1069 if isinstance(e, ForceLiteralArg)]. 1070 if not force_lit_args:. -> 1071 raise errors[0]. 1072 else:. 1073 raise reduce(operator.or_, force_lit_args). TypingError: Failed in nopython mode pipeline (step: nopython frontend). Failed in nopython mode pipeline (step: nopython frontend). Failed in nopython mode pipeline (step: nopython mode backend). Failed in nopython mode pipeline (step: nopython mode backend). Failed in nopython mode pipeline (step: nopython frontend). No implementation of function Function(<function make_quicksort_impl.<locals>.run_quicksort at 0x7f455d24c280>) found for signature:. . >>> run_quicksort(array(int32, 1d, C)). . There are 2 candidate implementations:. - Of which 2 did not match due to:. Overload in function 'register_jitable.<locals>.wrap.<locals>.ov_wrap': File: numba/core/extending.py: Line 150. With argument(s): '(array(int32, 1d, C))':. Rejected as the implementation raised a specific error:. UnsupportedError: Failed in nopython mode pipeline (step: analyzing bytecode). Use of unsupported opcode (LOAD_ASSERTION_ERROR) found. . File ""../../../../.local/lib/python3.9/site-packages/numba/misc/quicksort.py"", line 180:. def run_quicksort(A):. <source elided>. while high - low >= SMALL_QUICKSORT:. assert n < MAX_STACK. ^. . raised from /home/gabriel/.local/lib/python3.9/site-packages/numba/core/byteflow.py:269. During: resolving callee type: Function(<function make_quicksort_impl.<locals>.run_quicksort at 0x7f455d24c280>). During: typing of call at /home/gabriel/.local/lib/python3.9/site-packages/numba/np/arrayobj.py (5007). File ""../../../../.local/lib/python3.9/site-packages/numba/np/arrayobj.py"", line 5007:. def array_sort_impl(arr):. <source elided>. # Note we clobber the return value. sort_func(arr). ^. During: lowering ""$14call_method.5 = call $12load_method.4(func=$12load_method.4, args=[], kws=(), vararg=None)"" at /home/gabriel/.local/lib/python3.9/site-packages/numb",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:13,deployability,Fail,Failed,13,"TypingError: Failed in nopython mode pipeline (step: nopython frontend); Hi, I am working with a big dataset over 100000 cells and I run into a problem when computing the neigbours. Find below an small example using a random matrix. . Observations,. when I run N_cells = 8000 it works perfectly for gauss metric. when running N_cells = 10000, the neighbors computation crashes. The problem is even more dramatic for method=""umap"". . ### Minimal code sample (that we can copy&paste without having any data). ```python. import scanpy. import numpy. matrix = numpy.random.uniform(size=[10000,1000]). adata = scanpy.AnnData(matrix). scanpy.pp.pca(adata,n_comps=10). scanpy.pp.neighbors(adata,method=""gauss"",n_pcs=10). ```. ```pytb. ---------------------------------------------------------------------------. TypingError Traceback (most recent call last). <ipython-input-3-692f9d70cb0c> in <module>. 9 . 10 scanpy.pp.pca(adata,n_comps=10). ---> 11 scanpy.pp.neighbors(adata,method=""gauss"",n_pcs=10). ~/.local/lib/python3.9/site-packages/scanpy/neighbors/__init__.py in neighbors(adata, n_neighbors, n_pcs, use_rep, knn, random_state, method, metric, metric_kwds, key_added, copy). 118 adata._init_as_actual(adata.copy()). 119 neighbors = Neighbors(adata). --> 120 neighbors.compute_neighbors(. 121 n_neighbors=n_neighbors, knn=knn, n_pcs=n_pcs, use_rep=use_rep,. 122 method=method, metric=metric, metric_kwds=metric_kwds,. ~/.local/lib/python3.9/site-packages/scanpy/neighbors/__init__.py in compute_neighbors(self, n_neighbors, knn, n_pcs, use_rep, method, random_state, write_knn_indices, metric, metric_kwds). 732 X = pairwise_distances(X, metric=metric, **metric_kwds). 733 metric = 'precomputed'. --> 734 knn_indices, knn_distances, forest = compute_neighbors_umap(. 735 X, n_neighbors, random_state, metric=metric, metric_kwds=metric_kwds). 736 # very cautious here. ~/.local/lib/python3.9/site-packages/scanpy/neighbors/__init__.py in compute_neighbors_umap(X, n_neighbors, random_state, metric, m",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:37,deployability,pipelin,pipeline,37,"TypingError: Failed in nopython mode pipeline (step: nopython frontend); Hi, I am working with a big dataset over 100000 cells and I run into a problem when computing the neigbours. Find below an small example using a random matrix. . Observations,. when I run N_cells = 8000 it works perfectly for gauss metric. when running N_cells = 10000, the neighbors computation crashes. The problem is even more dramatic for method=""umap"". . ### Minimal code sample (that we can copy&paste without having any data). ```python. import scanpy. import numpy. matrix = numpy.random.uniform(size=[10000,1000]). adata = scanpy.AnnData(matrix). scanpy.pp.pca(adata,n_comps=10). scanpy.pp.neighbors(adata,method=""gauss"",n_pcs=10). ```. ```pytb. ---------------------------------------------------------------------------. TypingError Traceback (most recent call last). <ipython-input-3-692f9d70cb0c> in <module>. 9 . 10 scanpy.pp.pca(adata,n_comps=10). ---> 11 scanpy.pp.neighbors(adata,method=""gauss"",n_pcs=10). ~/.local/lib/python3.9/site-packages/scanpy/neighbors/__init__.py in neighbors(adata, n_neighbors, n_pcs, use_rep, knn, random_state, method, metric, metric_kwds, key_added, copy). 118 adata._init_as_actual(adata.copy()). 119 neighbors = Neighbors(adata). --> 120 neighbors.compute_neighbors(. 121 n_neighbors=n_neighbors, knn=knn, n_pcs=n_pcs, use_rep=use_rep,. 122 method=method, metric=metric, metric_kwds=metric_kwds,. ~/.local/lib/python3.9/site-packages/scanpy/neighbors/__init__.py in compute_neighbors(self, n_neighbors, knn, n_pcs, use_rep, method, random_state, write_knn_indices, metric, metric_kwds). 732 X = pairwise_distances(X, metric=metric, **metric_kwds). 733 metric = 'precomputed'. --> 734 knn_indices, knn_distances, forest = compute_neighbors_umap(. 735 X, n_neighbors, random_state, metric=metric, metric_kwds=metric_kwds). 736 # very cautious here. ~/.local/lib/python3.9/site-packages/scanpy/neighbors/__init__.py in compute_neighbors_umap(X, n_neighbors, random_state, metric, m",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:235,deployability,Observ,Observations,235,"TypingError: Failed in nopython mode pipeline (step: nopython frontend); Hi, I am working with a big dataset over 100000 cells and I run into a problem when computing the neigbours. Find below an small example using a random matrix. . Observations,. when I run N_cells = 8000 it works perfectly for gauss metric. when running N_cells = 10000, the neighbors computation crashes. The problem is even more dramatic for method=""umap"". . ### Minimal code sample (that we can copy&paste without having any data). ```python. import scanpy. import numpy. matrix = numpy.random.uniform(size=[10000,1000]). adata = scanpy.AnnData(matrix). scanpy.pp.pca(adata,n_comps=10). scanpy.pp.neighbors(adata,method=""gauss"",n_pcs=10). ```. ```pytb. ---------------------------------------------------------------------------. TypingError Traceback (most recent call last). <ipython-input-3-692f9d70cb0c> in <module>. 9 . 10 scanpy.pp.pca(adata,n_comps=10). ---> 11 scanpy.pp.neighbors(adata,method=""gauss"",n_pcs=10). ~/.local/lib/python3.9/site-packages/scanpy/neighbors/__init__.py in neighbors(adata, n_neighbors, n_pcs, use_rep, knn, random_state, method, metric, metric_kwds, key_added, copy). 118 adata._init_as_actual(adata.copy()). 119 neighbors = Neighbors(adata). --> 120 neighbors.compute_neighbors(. 121 n_neighbors=n_neighbors, knn=knn, n_pcs=n_pcs, use_rep=use_rep,. 122 method=method, metric=metric, metric_kwds=metric_kwds,. ~/.local/lib/python3.9/site-packages/scanpy/neighbors/__init__.py in compute_neighbors(self, n_neighbors, knn, n_pcs, use_rep, method, random_state, write_knn_indices, metric, metric_kwds). 732 X = pairwise_distances(X, metric=metric, **metric_kwds). 733 metric = 'precomputed'. --> 734 knn_indices, knn_distances, forest = compute_neighbors_umap(. 735 X, n_neighbors, random_state, metric=metric, metric_kwds=metric_kwds). 736 # very cautious here. ~/.local/lib/python3.9/site-packages/scanpy/neighbors/__init__.py in compute_neighbors_umap(X, n_neighbors, random_state, metric, m",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:887,deployability,modul,module,887,"TypingError: Failed in nopython mode pipeline (step: nopython frontend); Hi, I am working with a big dataset over 100000 cells and I run into a problem when computing the neigbours. Find below an small example using a random matrix. . Observations,. when I run N_cells = 8000 it works perfectly for gauss metric. when running N_cells = 10000, the neighbors computation crashes. The problem is even more dramatic for method=""umap"". . ### Minimal code sample (that we can copy&paste without having any data). ```python. import scanpy. import numpy. matrix = numpy.random.uniform(size=[10000,1000]). adata = scanpy.AnnData(matrix). scanpy.pp.pca(adata,n_comps=10). scanpy.pp.neighbors(adata,method=""gauss"",n_pcs=10). ```. ```pytb. ---------------------------------------------------------------------------. TypingError Traceback (most recent call last). <ipython-input-3-692f9d70cb0c> in <module>. 9 . 10 scanpy.pp.pca(adata,n_comps=10). ---> 11 scanpy.pp.neighbors(adata,method=""gauss"",n_pcs=10). ~/.local/lib/python3.9/site-packages/scanpy/neighbors/__init__.py in neighbors(adata, n_neighbors, n_pcs, use_rep, knn, random_state, method, metric, metric_kwds, key_added, copy). 118 adata._init_as_actual(adata.copy()). 119 neighbors = Neighbors(adata). --> 120 neighbors.compute_neighbors(. 121 n_neighbors=n_neighbors, knn=knn, n_pcs=n_pcs, use_rep=use_rep,. 122 method=method, metric=metric, metric_kwds=metric_kwds,. ~/.local/lib/python3.9/site-packages/scanpy/neighbors/__init__.py in compute_neighbors(self, n_neighbors, knn, n_pcs, use_rep, method, random_state, write_knn_indices, metric, metric_kwds). 732 X = pairwise_distances(X, metric=metric, **metric_kwds). 733 metric = 'precomputed'. --> 734 knn_indices, knn_distances, forest = compute_neighbors_umap(. 735 X, n_neighbors, random_state, metric=metric, metric_kwds=metric_kwds). 736 # very cautious here. ~/.local/lib/python3.9/site-packages/scanpy/neighbors/__init__.py in compute_neighbors_umap(X, n_neighbors, random_state, metric, m",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:2112,deployability,instal,installed,2112,"andom_state, method, metric, metric_kwds, key_added, copy). 118 adata._init_as_actual(adata.copy()). 119 neighbors = Neighbors(adata). --> 120 neighbors.compute_neighbors(. 121 n_neighbors=n_neighbors, knn=knn, n_pcs=n_pcs, use_rep=use_rep,. 122 method=method, metric=metric, metric_kwds=metric_kwds,. ~/.local/lib/python3.9/site-packages/scanpy/neighbors/__init__.py in compute_neighbors(self, n_neighbors, knn, n_pcs, use_rep, method, random_state, write_knn_indices, metric, metric_kwds). 732 X = pairwise_distances(X, metric=metric, **metric_kwds). 733 metric = 'precomputed'. --> 734 knn_indices, knn_distances, forest = compute_neighbors_umap(. 735 X, n_neighbors, random_state, metric=metric, metric_kwds=metric_kwds). 736 # very cautious here. ~/.local/lib/python3.9/site-packages/scanpy/neighbors/__init__.py in compute_neighbors_umap(X, n_neighbors, random_state, metric, metric_kwds, angular, verbose). 273 # umap 0.5.0. 274 warnings.filterwarnings(""ignore"", message=r""Tensorflow not installed""). --> 275 from umap.umap_ import nearest_neighbors. 276 . 277 random_state = check_random_state(random_state). ~/.local/lib/python3.9/site-packages/umap/umap_.py in <module>. 45 ). 46 . ---> 47 from pynndescent import NNDescent. 48 from pynndescent.distances import named_distances as pynn_named_distances. 49 from pynndescent.sparse import sparse_named_distances as pynn_sparse_named_distances. ~/.local/lib/python3.9/site-packages/pynndescent/__init__.py in <module>. 1 import pkg_resources. 2 import numba. ----> 3 from .pynndescent_ import NNDescent, PyNNDescentTransformer. 4 . 5 # Workaround: https://github.com/numba/numba/issues/3341. ~/.local/lib/python3.9/site-packages/pynndescent/pynndescent_.py in <module>. 19 import heapq. 20 . ---> 21 import pynndescent.sparse as sparse. 22 import pynndescent.sparse_nndescent as sparse_nnd. 23 import pynndescent.distances as pynnd_dist. ~/.local/lib/python3.9/site-packages/pynndescent/sparse.py in <module>. 341 },. 342 ). --> 343 def sparse",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:2289,deployability,modul,module,2289,"1 n_neighbors=n_neighbors, knn=knn, n_pcs=n_pcs, use_rep=use_rep,. 122 method=method, metric=metric, metric_kwds=metric_kwds,. ~/.local/lib/python3.9/site-packages/scanpy/neighbors/__init__.py in compute_neighbors(self, n_neighbors, knn, n_pcs, use_rep, method, random_state, write_knn_indices, metric, metric_kwds). 732 X = pairwise_distances(X, metric=metric, **metric_kwds). 733 metric = 'precomputed'. --> 734 knn_indices, knn_distances, forest = compute_neighbors_umap(. 735 X, n_neighbors, random_state, metric=metric, metric_kwds=metric_kwds). 736 # very cautious here. ~/.local/lib/python3.9/site-packages/scanpy/neighbors/__init__.py in compute_neighbors_umap(X, n_neighbors, random_state, metric, metric_kwds, angular, verbose). 273 # umap 0.5.0. 274 warnings.filterwarnings(""ignore"", message=r""Tensorflow not installed""). --> 275 from umap.umap_ import nearest_neighbors. 276 . 277 random_state = check_random_state(random_state). ~/.local/lib/python3.9/site-packages/umap/umap_.py in <module>. 45 ). 46 . ---> 47 from pynndescent import NNDescent. 48 from pynndescent.distances import named_distances as pynn_named_distances. 49 from pynndescent.sparse import sparse_named_distances as pynn_sparse_named_distances. ~/.local/lib/python3.9/site-packages/pynndescent/__init__.py in <module>. 1 import pkg_resources. 2 import numba. ----> 3 from .pynndescent_ import NNDescent, PyNNDescentTransformer. 4 . 5 # Workaround: https://github.com/numba/numba/issues/3341. ~/.local/lib/python3.9/site-packages/pynndescent/pynndescent_.py in <module>. 19 import heapq. 20 . ---> 21 import pynndescent.sparse as sparse. 22 import pynndescent.sparse_nndescent as sparse_nnd. 23 import pynndescent.distances as pynnd_dist. ~/.local/lib/python3.9/site-packages/pynndescent/sparse.py in <module>. 341 },. 342 ). --> 343 def sparse_alternative_jaccard(ind1, data1, ind2, data2):. 344 num_non_zero = arr_union(ind1, ind2).shape[0]. 345 num_equal = arr_intersect(ind1, ind2).shape[0]. ~/.local/lib/python3.9/",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:2584,deployability,modul,module,2584,"metric, metric_kwds). 732 X = pairwise_distances(X, metric=metric, **metric_kwds). 733 metric = 'precomputed'. --> 734 knn_indices, knn_distances, forest = compute_neighbors_umap(. 735 X, n_neighbors, random_state, metric=metric, metric_kwds=metric_kwds). 736 # very cautious here. ~/.local/lib/python3.9/site-packages/scanpy/neighbors/__init__.py in compute_neighbors_umap(X, n_neighbors, random_state, metric, metric_kwds, angular, verbose). 273 # umap 0.5.0. 274 warnings.filterwarnings(""ignore"", message=r""Tensorflow not installed""). --> 275 from umap.umap_ import nearest_neighbors. 276 . 277 random_state = check_random_state(random_state). ~/.local/lib/python3.9/site-packages/umap/umap_.py in <module>. 45 ). 46 . ---> 47 from pynndescent import NNDescent. 48 from pynndescent.distances import named_distances as pynn_named_distances. 49 from pynndescent.sparse import sparse_named_distances as pynn_sparse_named_distances. ~/.local/lib/python3.9/site-packages/pynndescent/__init__.py in <module>. 1 import pkg_resources. 2 import numba. ----> 3 from .pynndescent_ import NNDescent, PyNNDescentTransformer. 4 . 5 # Workaround: https://github.com/numba/numba/issues/3341. ~/.local/lib/python3.9/site-packages/pynndescent/pynndescent_.py in <module>. 19 import heapq. 20 . ---> 21 import pynndescent.sparse as sparse. 22 import pynndescent.sparse_nndescent as sparse_nnd. 23 import pynndescent.distances as pynnd_dist. ~/.local/lib/python3.9/site-packages/pynndescent/sparse.py in <module>. 341 },. 342 ). --> 343 def sparse_alternative_jaccard(ind1, data1, ind2, data2):. 344 num_non_zero = arr_union(ind1, ind2).shape[0]. 345 num_equal = arr_intersect(ind1, ind2).shape[0]. ~/.local/lib/python3.9/site-packages/numba/core/decorators.py in wrapper(func). 216 with typeinfer.register_dispatcher(disp):. 217 for sig in sigs:. --> 218 disp.compile(sig). 219 disp.disable_compile(). 220 return disp. ~/.local/lib/python3.9/site-packages/numba/core/compiler_lock.py in _acquire_compile_lock(*args, ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:2835,deployability,modul,module,2835,"ds). 736 # very cautious here. ~/.local/lib/python3.9/site-packages/scanpy/neighbors/__init__.py in compute_neighbors_umap(X, n_neighbors, random_state, metric, metric_kwds, angular, verbose). 273 # umap 0.5.0. 274 warnings.filterwarnings(""ignore"", message=r""Tensorflow not installed""). --> 275 from umap.umap_ import nearest_neighbors. 276 . 277 random_state = check_random_state(random_state). ~/.local/lib/python3.9/site-packages/umap/umap_.py in <module>. 45 ). 46 . ---> 47 from pynndescent import NNDescent. 48 from pynndescent.distances import named_distances as pynn_named_distances. 49 from pynndescent.sparse import sparse_named_distances as pynn_sparse_named_distances. ~/.local/lib/python3.9/site-packages/pynndescent/__init__.py in <module>. 1 import pkg_resources. 2 import numba. ----> 3 from .pynndescent_ import NNDescent, PyNNDescentTransformer. 4 . 5 # Workaround: https://github.com/numba/numba/issues/3341. ~/.local/lib/python3.9/site-packages/pynndescent/pynndescent_.py in <module>. 19 import heapq. 20 . ---> 21 import pynndescent.sparse as sparse. 22 import pynndescent.sparse_nndescent as sparse_nnd. 23 import pynndescent.distances as pynnd_dist. ~/.local/lib/python3.9/site-packages/pynndescent/sparse.py in <module>. 341 },. 342 ). --> 343 def sparse_alternative_jaccard(ind1, data1, ind2, data2):. 344 num_non_zero = arr_union(ind1, ind2).shape[0]. 345 num_equal = arr_intersect(ind1, ind2).shape[0]. ~/.local/lib/python3.9/site-packages/numba/core/decorators.py in wrapper(func). 216 with typeinfer.register_dispatcher(disp):. 217 for sig in sigs:. --> 218 disp.compile(sig). 219 disp.disable_compile(). 220 return disp. ~/.local/lib/python3.9/site-packages/numba/core/compiler_lock.py in _acquire_compile_lock(*args, **kwargs). 30 def _acquire_compile_lock(*args, **kwargs):. 31 with self:. ---> 32 return func(*args, **kwargs). 33 return _acquire_compile_lock. 34 . ~/.local/lib/python3.9/site-packages/numba/core/dispatcher.py in compile(self, sig). 817 self._cache_",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:3075,deployability,modul,module,3075,"ignore"", message=r""Tensorflow not installed""). --> 275 from umap.umap_ import nearest_neighbors. 276 . 277 random_state = check_random_state(random_state). ~/.local/lib/python3.9/site-packages/umap/umap_.py in <module>. 45 ). 46 . ---> 47 from pynndescent import NNDescent. 48 from pynndescent.distances import named_distances as pynn_named_distances. 49 from pynndescent.sparse import sparse_named_distances as pynn_sparse_named_distances. ~/.local/lib/python3.9/site-packages/pynndescent/__init__.py in <module>. 1 import pkg_resources. 2 import numba. ----> 3 from .pynndescent_ import NNDescent, PyNNDescentTransformer. 4 . 5 # Workaround: https://github.com/numba/numba/issues/3341. ~/.local/lib/python3.9/site-packages/pynndescent/pynndescent_.py in <module>. 19 import heapq. 20 . ---> 21 import pynndescent.sparse as sparse. 22 import pynndescent.sparse_nndescent as sparse_nnd. 23 import pynndescent.distances as pynnd_dist. ~/.local/lib/python3.9/site-packages/pynndescent/sparse.py in <module>. 341 },. 342 ). --> 343 def sparse_alternative_jaccard(ind1, data1, ind2, data2):. 344 num_non_zero = arr_union(ind1, ind2).shape[0]. 345 num_equal = arr_intersect(ind1, ind2).shape[0]. ~/.local/lib/python3.9/site-packages/numba/core/decorators.py in wrapper(func). 216 with typeinfer.register_dispatcher(disp):. 217 for sig in sigs:. --> 218 disp.compile(sig). 219 disp.disable_compile(). 220 return disp. ~/.local/lib/python3.9/site-packages/numba/core/compiler_lock.py in _acquire_compile_lock(*args, **kwargs). 30 def _acquire_compile_lock(*args, **kwargs):. 31 with self:. ---> 32 return func(*args, **kwargs). 33 return _acquire_compile_lock. 34 . ~/.local/lib/python3.9/site-packages/numba/core/dispatcher.py in compile(self, sig). 817 self._cache_misses[sig] += 1. 818 try:. --> 819 cres = self._compiler.compile(args, return_type). 820 except errors.ForceLiteralArg as e:. 821 def folded(args, kws):. ~/.local/lib/python3.9/site-packages/numba/core/dispatcher.py in compile(self, args, ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:4890,deployability,pipelin,pipeline,4890,"r.compile(args, return_type). 820 except errors.ForceLiteralArg as e:. 821 def folded(args, kws):. ~/.local/lib/python3.9/site-packages/numba/core/dispatcher.py in compile(self, args, return_type). 80 return retval. 81 else:. ---> 82 raise retval. 83 . 84 def _compile_cached(self, args, return_type):. ~/.local/lib/python3.9/site-packages/numba/core/dispatcher.py in _compile_cached(self, args, return_type). 90 . 91 try:. ---> 92 retval = self._compile_core(args, return_type). 93 except errors.TypingError as e:. 94 self._failed_cache[key] = e. ~/.local/lib/python3.9/site-packages/numba/core/dispatcher.py in _compile_core(self, args, return_type). 103 . 104 impl = self._get_implementation(args, {}). --> 105 cres = compiler.compile_extra(self.targetdescr.typing_context,. 106 self.targetdescr.target_context,. 107 impl,. ~/.local/lib/python3.9/site-packages/numba/core/compiler.py in compile_extra(typingctx, targetctx, func, args, return_type, flags, locals, library, pipeline_class). 625 pipeline = pipeline_class(typingctx, targetctx, library,. 626 args, return_type, flags, locals). --> 627 return pipeline.compile_extra(func). 628 . 629 . ~/.local/lib/python3.9/site-packages/numba/core/compiler.py in compile_extra(self, func). 361 self.state.lifted = (). 362 self.state.lifted_from = None. --> 363 return self._compile_bytecode(). 364 . 365 def compile_ir(self, func_ir, lifted=(), lifted_from=None):. ~/.local/lib/python3.9/site-packages/numba/core/compiler.py in _compile_bytecode(self). 423 """""". 424 assert self.state.func_ir is None. --> 425 return self._compile_core(). 426 . 427 def _compile_ir(self):. ~/.local/lib/python3.9/site-packages/numba/core/compiler.py in _compile_core(self). 403 self.state.status.fail_reason = e. 404 if is_final_pipeline:. --> 405 raise e. 406 else:. 407 raise CompilerError(""All available pipelines exhausted""). ~/.local/lib/python3.9/site-packages/numba/core/compiler.py in _compile_core(self). 394 res = None. 395 try:. --> 396 pm.run(self.state). ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:5002,deployability,pipelin,pipeline,5002,"python3.9/site-packages/numba/core/dispatcher.py in compile(self, args, return_type). 80 return retval. 81 else:. ---> 82 raise retval. 83 . 84 def _compile_cached(self, args, return_type):. ~/.local/lib/python3.9/site-packages/numba/core/dispatcher.py in _compile_cached(self, args, return_type). 90 . 91 try:. ---> 92 retval = self._compile_core(args, return_type). 93 except errors.TypingError as e:. 94 self._failed_cache[key] = e. ~/.local/lib/python3.9/site-packages/numba/core/dispatcher.py in _compile_core(self, args, return_type). 103 . 104 impl = self._get_implementation(args, {}). --> 105 cres = compiler.compile_extra(self.targetdescr.typing_context,. 106 self.targetdescr.target_context,. 107 impl,. ~/.local/lib/python3.9/site-packages/numba/core/compiler.py in compile_extra(typingctx, targetctx, func, args, return_type, flags, locals, library, pipeline_class). 625 pipeline = pipeline_class(typingctx, targetctx, library,. 626 args, return_type, flags, locals). --> 627 return pipeline.compile_extra(func). 628 . 629 . ~/.local/lib/python3.9/site-packages/numba/core/compiler.py in compile_extra(self, func). 361 self.state.lifted = (). 362 self.state.lifted_from = None. --> 363 return self._compile_bytecode(). 364 . 365 def compile_ir(self, func_ir, lifted=(), lifted_from=None):. ~/.local/lib/python3.9/site-packages/numba/core/compiler.py in _compile_bytecode(self). 423 """""". 424 assert self.state.func_ir is None. --> 425 return self._compile_core(). 426 . 427 def _compile_ir(self):. ~/.local/lib/python3.9/site-packages/numba/core/compiler.py in _compile_core(self). 403 self.state.status.fail_reason = e. 404 if is_final_pipeline:. --> 405 raise e. 406 else:. 407 raise CompilerError(""All available pipelines exhausted""). ~/.local/lib/python3.9/site-packages/numba/core/compiler.py in _compile_core(self). 394 res = None. 395 try:. --> 396 pm.run(self.state). 397 if self.state.cr is not None:. 398 break. ~/.local/lib/python3.9/site-packages/numba/core/compiler_machinery",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:5733,deployability,pipelin,pipelines,5733,"on3.9/site-packages/numba/core/compiler.py in compile_extra(typingctx, targetctx, func, args, return_type, flags, locals, library, pipeline_class). 625 pipeline = pipeline_class(typingctx, targetctx, library,. 626 args, return_type, flags, locals). --> 627 return pipeline.compile_extra(func). 628 . 629 . ~/.local/lib/python3.9/site-packages/numba/core/compiler.py in compile_extra(self, func). 361 self.state.lifted = (). 362 self.state.lifted_from = None. --> 363 return self._compile_bytecode(). 364 . 365 def compile_ir(self, func_ir, lifted=(), lifted_from=None):. ~/.local/lib/python3.9/site-packages/numba/core/compiler.py in _compile_bytecode(self). 423 """""". 424 assert self.state.func_ir is None. --> 425 return self._compile_core(). 426 . 427 def _compile_ir(self):. ~/.local/lib/python3.9/site-packages/numba/core/compiler.py in _compile_core(self). 403 self.state.status.fail_reason = e. 404 if is_final_pipeline:. --> 405 raise e. 406 else:. 407 raise CompilerError(""All available pipelines exhausted""). ~/.local/lib/python3.9/site-packages/numba/core/compiler.py in _compile_core(self). 394 res = None. 395 try:. --> 396 pm.run(self.state). 397 if self.state.cr is not None:. 398 break. ~/.local/lib/python3.9/site-packages/numba/core/compiler_machinery.py in run(self, state). 339 (self.pipeline_name, pass_desc). 340 patched_exception = self._patch_error(msg, e). --> 341 raise patched_exception. 342 . 343 def dependency_analysis(self):. ~/.local/lib/python3.9/site-packages/numba/core/compiler_machinery.py in run(self, state). 330 pass_inst = _pass_registry.get(pss).pass_inst. 331 if isinstance(pass_inst, CompilerPass):. --> 332 self._runPass(idx, pass_inst, state). 333 else:. 334 raise BaseException(""Legacy pass in use""). ~/.local/lib/python3.9/site-packages/numba/core/compiler_lock.py in _acquire_compile_lock(*args, **kwargs). 30 def _acquire_compile_lock(*args, **kwargs):. 31 with self:. ---> 32 return func(*args, **kwargs). 33 return _acquire_compile_lock. 34 . ~/.loc",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:8226,deployability,Fail,Failed,8226,"c, compiler_state):. --> 264 mangled = func(compiler_state). 265 if mangled not in (True, False):. 266 msg = (""CompilerPass implementations should return True/False. "". ~/.local/lib/python3.9/site-packages/numba/core/typed_passes.py in run_pass(self, state). 90 % (state.func_id.func_name,)):. 91 # Type inference. ---> 92 typemap, return_type, calltypes = type_inference_stage(. 93 state.typingctx,. 94 state.func_ir,. ~/.local/lib/python3.9/site-packages/numba/core/typed_passes.py in type_inference_stage(typingctx, interp, args, return_type, locals, raise_errors). 68 . 69 infer.build_constraint(). ---> 70 infer.propagate(raise_errors=raise_errors). 71 typemap, restype, calltypes = infer.unify(raise_errors=raise_errors). 72 . ~/.local/lib/python3.9/site-packages/numba/core/typeinfer.py in propagate(self, raise_errors). 1069 if isinstance(e, ForceLiteralArg)]. 1070 if not force_lit_args:. -> 1071 raise errors[0]. 1072 else:. 1073 raise reduce(operator.or_, force_lit_args). TypingError: Failed in nopython mode pipeline (step: nopython frontend). Failed in nopython mode pipeline (step: nopython frontend). Failed in nopython mode pipeline (step: nopython mode backend). Failed in nopython mode pipeline (step: nopython mode backend). Failed in nopython mode pipeline (step: nopython frontend). No implementation of function Function(<function make_quicksort_impl.<locals>.run_quicksort at 0x7f455d24c280>) found for signature:. . >>> run_quicksort(array(int32, 1d, C)). . There are 2 candidate implementations:. - Of which 2 did not match due to:. Overload in function 'register_jitable.<locals>.wrap.<locals>.ov_wrap': File: numba/core/extending.py: Line 150. With argument(s): '(array(int32, 1d, C))':. Rejected as the implementation raised a specific error:. UnsupportedError: Failed in nopython mode pipeline (step: analyzing bytecode). Use of unsupported opcode (LOAD_ASSERTION_ERROR) found. . File ""../../../../.local/lib/python3.9/site-packages/numba/misc/quicksort.py"", line 180:. ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:8250,deployability,pipelin,pipeline,8250,"264 mangled = func(compiler_state). 265 if mangled not in (True, False):. 266 msg = (""CompilerPass implementations should return True/False. "". ~/.local/lib/python3.9/site-packages/numba/core/typed_passes.py in run_pass(self, state). 90 % (state.func_id.func_name,)):. 91 # Type inference. ---> 92 typemap, return_type, calltypes = type_inference_stage(. 93 state.typingctx,. 94 state.func_ir,. ~/.local/lib/python3.9/site-packages/numba/core/typed_passes.py in type_inference_stage(typingctx, interp, args, return_type, locals, raise_errors). 68 . 69 infer.build_constraint(). ---> 70 infer.propagate(raise_errors=raise_errors). 71 typemap, restype, calltypes = infer.unify(raise_errors=raise_errors). 72 . ~/.local/lib/python3.9/site-packages/numba/core/typeinfer.py in propagate(self, raise_errors). 1069 if isinstance(e, ForceLiteralArg)]. 1070 if not force_lit_args:. -> 1071 raise errors[0]. 1072 else:. 1073 raise reduce(operator.or_, force_lit_args). TypingError: Failed in nopython mode pipeline (step: nopython frontend). Failed in nopython mode pipeline (step: nopython frontend). Failed in nopython mode pipeline (step: nopython mode backend). Failed in nopython mode pipeline (step: nopython mode backend). Failed in nopython mode pipeline (step: nopython frontend). No implementation of function Function(<function make_quicksort_impl.<locals>.run_quicksort at 0x7f455d24c280>) found for signature:. . >>> run_quicksort(array(int32, 1d, C)). . There are 2 candidate implementations:. - Of which 2 did not match due to:. Overload in function 'register_jitable.<locals>.wrap.<locals>.ov_wrap': File: numba/core/extending.py: Line 150. With argument(s): '(array(int32, 1d, C))':. Rejected as the implementation raised a specific error:. UnsupportedError: Failed in nopython mode pipeline (step: analyzing bytecode). Use of unsupported opcode (LOAD_ASSERTION_ERROR) found. . File ""../../../../.local/lib/python3.9/site-packages/numba/misc/quicksort.py"", line 180:. def run_quicksort(A):. <s",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:8286,deployability,Fail,Failed,8286," 265 if mangled not in (True, False):. 266 msg = (""CompilerPass implementations should return True/False. "". ~/.local/lib/python3.9/site-packages/numba/core/typed_passes.py in run_pass(self, state). 90 % (state.func_id.func_name,)):. 91 # Type inference. ---> 92 typemap, return_type, calltypes = type_inference_stage(. 93 state.typingctx,. 94 state.func_ir,. ~/.local/lib/python3.9/site-packages/numba/core/typed_passes.py in type_inference_stage(typingctx, interp, args, return_type, locals, raise_errors). 68 . 69 infer.build_constraint(). ---> 70 infer.propagate(raise_errors=raise_errors). 71 typemap, restype, calltypes = infer.unify(raise_errors=raise_errors). 72 . ~/.local/lib/python3.9/site-packages/numba/core/typeinfer.py in propagate(self, raise_errors). 1069 if isinstance(e, ForceLiteralArg)]. 1070 if not force_lit_args:. -> 1071 raise errors[0]. 1072 else:. 1073 raise reduce(operator.or_, force_lit_args). TypingError: Failed in nopython mode pipeline (step: nopython frontend). Failed in nopython mode pipeline (step: nopython frontend). Failed in nopython mode pipeline (step: nopython mode backend). Failed in nopython mode pipeline (step: nopython mode backend). Failed in nopython mode pipeline (step: nopython frontend). No implementation of function Function(<function make_quicksort_impl.<locals>.run_quicksort at 0x7f455d24c280>) found for signature:. . >>> run_quicksort(array(int32, 1d, C)). . There are 2 candidate implementations:. - Of which 2 did not match due to:. Overload in function 'register_jitable.<locals>.wrap.<locals>.ov_wrap': File: numba/core/extending.py: Line 150. With argument(s): '(array(int32, 1d, C))':. Rejected as the implementation raised a specific error:. UnsupportedError: Failed in nopython mode pipeline (step: analyzing bytecode). Use of unsupported opcode (LOAD_ASSERTION_ERROR) found. . File ""../../../../.local/lib/python3.9/site-packages/numba/misc/quicksort.py"", line 180:. def run_quicksort(A):. <source elided>. while high - low >= ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:8310,deployability,pipelin,pipeline,8310,"rue, False):. 266 msg = (""CompilerPass implementations should return True/False. "". ~/.local/lib/python3.9/site-packages/numba/core/typed_passes.py in run_pass(self, state). 90 % (state.func_id.func_name,)):. 91 # Type inference. ---> 92 typemap, return_type, calltypes = type_inference_stage(. 93 state.typingctx,. 94 state.func_ir,. ~/.local/lib/python3.9/site-packages/numba/core/typed_passes.py in type_inference_stage(typingctx, interp, args, return_type, locals, raise_errors). 68 . 69 infer.build_constraint(). ---> 70 infer.propagate(raise_errors=raise_errors). 71 typemap, restype, calltypes = infer.unify(raise_errors=raise_errors). 72 . ~/.local/lib/python3.9/site-packages/numba/core/typeinfer.py in propagate(self, raise_errors). 1069 if isinstance(e, ForceLiteralArg)]. 1070 if not force_lit_args:. -> 1071 raise errors[0]. 1072 else:. 1073 raise reduce(operator.or_, force_lit_args). TypingError: Failed in nopython mode pipeline (step: nopython frontend). Failed in nopython mode pipeline (step: nopython frontend). Failed in nopython mode pipeline (step: nopython mode backend). Failed in nopython mode pipeline (step: nopython mode backend). Failed in nopython mode pipeline (step: nopython frontend). No implementation of function Function(<function make_quicksort_impl.<locals>.run_quicksort at 0x7f455d24c280>) found for signature:. . >>> run_quicksort(array(int32, 1d, C)). . There are 2 candidate implementations:. - Of which 2 did not match due to:. Overload in function 'register_jitable.<locals>.wrap.<locals>.ov_wrap': File: numba/core/extending.py: Line 150. With argument(s): '(array(int32, 1d, C))':. Rejected as the implementation raised a specific error:. UnsupportedError: Failed in nopython mode pipeline (step: analyzing bytecode). Use of unsupported opcode (LOAD_ASSERTION_ERROR) found. . File ""../../../../.local/lib/python3.9/site-packages/numba/misc/quicksort.py"", line 180:. def run_quicksort(A):. <source elided>. while high - low >= SMALL_QUICKSORT:. assert ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:8346,deployability,Fail,Failed,8346,"ass implementations should return True/False. "". ~/.local/lib/python3.9/site-packages/numba/core/typed_passes.py in run_pass(self, state). 90 % (state.func_id.func_name,)):. 91 # Type inference. ---> 92 typemap, return_type, calltypes = type_inference_stage(. 93 state.typingctx,. 94 state.func_ir,. ~/.local/lib/python3.9/site-packages/numba/core/typed_passes.py in type_inference_stage(typingctx, interp, args, return_type, locals, raise_errors). 68 . 69 infer.build_constraint(). ---> 70 infer.propagate(raise_errors=raise_errors). 71 typemap, restype, calltypes = infer.unify(raise_errors=raise_errors). 72 . ~/.local/lib/python3.9/site-packages/numba/core/typeinfer.py in propagate(self, raise_errors). 1069 if isinstance(e, ForceLiteralArg)]. 1070 if not force_lit_args:. -> 1071 raise errors[0]. 1072 else:. 1073 raise reduce(operator.or_, force_lit_args). TypingError: Failed in nopython mode pipeline (step: nopython frontend). Failed in nopython mode pipeline (step: nopython frontend). Failed in nopython mode pipeline (step: nopython mode backend). Failed in nopython mode pipeline (step: nopython mode backend). Failed in nopython mode pipeline (step: nopython frontend). No implementation of function Function(<function make_quicksort_impl.<locals>.run_quicksort at 0x7f455d24c280>) found for signature:. . >>> run_quicksort(array(int32, 1d, C)). . There are 2 candidate implementations:. - Of which 2 did not match due to:. Overload in function 'register_jitable.<locals>.wrap.<locals>.ov_wrap': File: numba/core/extending.py: Line 150. With argument(s): '(array(int32, 1d, C))':. Rejected as the implementation raised a specific error:. UnsupportedError: Failed in nopython mode pipeline (step: analyzing bytecode). Use of unsupported opcode (LOAD_ASSERTION_ERROR) found. . File ""../../../../.local/lib/python3.9/site-packages/numba/misc/quicksort.py"", line 180:. def run_quicksort(A):. <source elided>. while high - low >= SMALL_QUICKSORT:. assert n < MAX_STACK. ^. . raised from /ho",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:8370,deployability,pipelin,pipeline,8370,"d return True/False. "". ~/.local/lib/python3.9/site-packages/numba/core/typed_passes.py in run_pass(self, state). 90 % (state.func_id.func_name,)):. 91 # Type inference. ---> 92 typemap, return_type, calltypes = type_inference_stage(. 93 state.typingctx,. 94 state.func_ir,. ~/.local/lib/python3.9/site-packages/numba/core/typed_passes.py in type_inference_stage(typingctx, interp, args, return_type, locals, raise_errors). 68 . 69 infer.build_constraint(). ---> 70 infer.propagate(raise_errors=raise_errors). 71 typemap, restype, calltypes = infer.unify(raise_errors=raise_errors). 72 . ~/.local/lib/python3.9/site-packages/numba/core/typeinfer.py in propagate(self, raise_errors). 1069 if isinstance(e, ForceLiteralArg)]. 1070 if not force_lit_args:. -> 1071 raise errors[0]. 1072 else:. 1073 raise reduce(operator.or_, force_lit_args). TypingError: Failed in nopython mode pipeline (step: nopython frontend). Failed in nopython mode pipeline (step: nopython frontend). Failed in nopython mode pipeline (step: nopython mode backend). Failed in nopython mode pipeline (step: nopython mode backend). Failed in nopython mode pipeline (step: nopython frontend). No implementation of function Function(<function make_quicksort_impl.<locals>.run_quicksort at 0x7f455d24c280>) found for signature:. . >>> run_quicksort(array(int32, 1d, C)). . There are 2 candidate implementations:. - Of which 2 did not match due to:. Overload in function 'register_jitable.<locals>.wrap.<locals>.ov_wrap': File: numba/core/extending.py: Line 150. With argument(s): '(array(int32, 1d, C))':. Rejected as the implementation raised a specific error:. UnsupportedError: Failed in nopython mode pipeline (step: analyzing bytecode). Use of unsupported opcode (LOAD_ASSERTION_ERROR) found. . File ""../../../../.local/lib/python3.9/site-packages/numba/misc/quicksort.py"", line 180:. def run_quicksort(A):. <source elided>. while high - low >= SMALL_QUICKSORT:. assert n < MAX_STACK. ^. . raised from /home/gabriel/.local/lib/pyt",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:8410,deployability,Fail,Failed,8410,"thon3.9/site-packages/numba/core/typed_passes.py in run_pass(self, state). 90 % (state.func_id.func_name,)):. 91 # Type inference. ---> 92 typemap, return_type, calltypes = type_inference_stage(. 93 state.typingctx,. 94 state.func_ir,. ~/.local/lib/python3.9/site-packages/numba/core/typed_passes.py in type_inference_stage(typingctx, interp, args, return_type, locals, raise_errors). 68 . 69 infer.build_constraint(). ---> 70 infer.propagate(raise_errors=raise_errors). 71 typemap, restype, calltypes = infer.unify(raise_errors=raise_errors). 72 . ~/.local/lib/python3.9/site-packages/numba/core/typeinfer.py in propagate(self, raise_errors). 1069 if isinstance(e, ForceLiteralArg)]. 1070 if not force_lit_args:. -> 1071 raise errors[0]. 1072 else:. 1073 raise reduce(operator.or_, force_lit_args). TypingError: Failed in nopython mode pipeline (step: nopython frontend). Failed in nopython mode pipeline (step: nopython frontend). Failed in nopython mode pipeline (step: nopython mode backend). Failed in nopython mode pipeline (step: nopython mode backend). Failed in nopython mode pipeline (step: nopython frontend). No implementation of function Function(<function make_quicksort_impl.<locals>.run_quicksort at 0x7f455d24c280>) found for signature:. . >>> run_quicksort(array(int32, 1d, C)). . There are 2 candidate implementations:. - Of which 2 did not match due to:. Overload in function 'register_jitable.<locals>.wrap.<locals>.ov_wrap': File: numba/core/extending.py: Line 150. With argument(s): '(array(int32, 1d, C))':. Rejected as the implementation raised a specific error:. UnsupportedError: Failed in nopython mode pipeline (step: analyzing bytecode). Use of unsupported opcode (LOAD_ASSERTION_ERROR) found. . File ""../../../../.local/lib/python3.9/site-packages/numba/misc/quicksort.py"", line 180:. def run_quicksort(A):. <source elided>. while high - low >= SMALL_QUICKSORT:. assert n < MAX_STACK. ^. . raised from /home/gabriel/.local/lib/python3.9/site-packages/numba/core/byteflo",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:8434,deployability,pipelin,pipeline,8434,"ba/core/typed_passes.py in run_pass(self, state). 90 % (state.func_id.func_name,)):. 91 # Type inference. ---> 92 typemap, return_type, calltypes = type_inference_stage(. 93 state.typingctx,. 94 state.func_ir,. ~/.local/lib/python3.9/site-packages/numba/core/typed_passes.py in type_inference_stage(typingctx, interp, args, return_type, locals, raise_errors). 68 . 69 infer.build_constraint(). ---> 70 infer.propagate(raise_errors=raise_errors). 71 typemap, restype, calltypes = infer.unify(raise_errors=raise_errors). 72 . ~/.local/lib/python3.9/site-packages/numba/core/typeinfer.py in propagate(self, raise_errors). 1069 if isinstance(e, ForceLiteralArg)]. 1070 if not force_lit_args:. -> 1071 raise errors[0]. 1072 else:. 1073 raise reduce(operator.or_, force_lit_args). TypingError: Failed in nopython mode pipeline (step: nopython frontend). Failed in nopython mode pipeline (step: nopython frontend). Failed in nopython mode pipeline (step: nopython mode backend). Failed in nopython mode pipeline (step: nopython mode backend). Failed in nopython mode pipeline (step: nopython frontend). No implementation of function Function(<function make_quicksort_impl.<locals>.run_quicksort at 0x7f455d24c280>) found for signature:. . >>> run_quicksort(array(int32, 1d, C)). . There are 2 candidate implementations:. - Of which 2 did not match due to:. Overload in function 'register_jitable.<locals>.wrap.<locals>.ov_wrap': File: numba/core/extending.py: Line 150. With argument(s): '(array(int32, 1d, C))':. Rejected as the implementation raised a specific error:. UnsupportedError: Failed in nopython mode pipeline (step: analyzing bytecode). Use of unsupported opcode (LOAD_ASSERTION_ERROR) found. . File ""../../../../.local/lib/python3.9/site-packages/numba/misc/quicksort.py"", line 180:. def run_quicksort(A):. <source elided>. while high - low >= SMALL_QUICKSORT:. assert n < MAX_STACK. ^. . raised from /home/gabriel/.local/lib/python3.9/site-packages/numba/core/byteflow.py:269. During: resolvi",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:8474,deployability,Fail,Failed,8474,"f, state). 90 % (state.func_id.func_name,)):. 91 # Type inference. ---> 92 typemap, return_type, calltypes = type_inference_stage(. 93 state.typingctx,. 94 state.func_ir,. ~/.local/lib/python3.9/site-packages/numba/core/typed_passes.py in type_inference_stage(typingctx, interp, args, return_type, locals, raise_errors). 68 . 69 infer.build_constraint(). ---> 70 infer.propagate(raise_errors=raise_errors). 71 typemap, restype, calltypes = infer.unify(raise_errors=raise_errors). 72 . ~/.local/lib/python3.9/site-packages/numba/core/typeinfer.py in propagate(self, raise_errors). 1069 if isinstance(e, ForceLiteralArg)]. 1070 if not force_lit_args:. -> 1071 raise errors[0]. 1072 else:. 1073 raise reduce(operator.or_, force_lit_args). TypingError: Failed in nopython mode pipeline (step: nopython frontend). Failed in nopython mode pipeline (step: nopython frontend). Failed in nopython mode pipeline (step: nopython mode backend). Failed in nopython mode pipeline (step: nopython mode backend). Failed in nopython mode pipeline (step: nopython frontend). No implementation of function Function(<function make_quicksort_impl.<locals>.run_quicksort at 0x7f455d24c280>) found for signature:. . >>> run_quicksort(array(int32, 1d, C)). . There are 2 candidate implementations:. - Of which 2 did not match due to:. Overload in function 'register_jitable.<locals>.wrap.<locals>.ov_wrap': File: numba/core/extending.py: Line 150. With argument(s): '(array(int32, 1d, C))':. Rejected as the implementation raised a specific error:. UnsupportedError: Failed in nopython mode pipeline (step: analyzing bytecode). Use of unsupported opcode (LOAD_ASSERTION_ERROR) found. . File ""../../../../.local/lib/python3.9/site-packages/numba/misc/quicksort.py"", line 180:. def run_quicksort(A):. <source elided>. while high - low >= SMALL_QUICKSORT:. assert n < MAX_STACK. ^. . raised from /home/gabriel/.local/lib/python3.9/site-packages/numba/core/byteflow.py:269. During: resolving callee type: Function(<function make",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:8498,deployability,pipelin,pipeline,8498,"nc_id.func_name,)):. 91 # Type inference. ---> 92 typemap, return_type, calltypes = type_inference_stage(. 93 state.typingctx,. 94 state.func_ir,. ~/.local/lib/python3.9/site-packages/numba/core/typed_passes.py in type_inference_stage(typingctx, interp, args, return_type, locals, raise_errors). 68 . 69 infer.build_constraint(). ---> 70 infer.propagate(raise_errors=raise_errors). 71 typemap, restype, calltypes = infer.unify(raise_errors=raise_errors). 72 . ~/.local/lib/python3.9/site-packages/numba/core/typeinfer.py in propagate(self, raise_errors). 1069 if isinstance(e, ForceLiteralArg)]. 1070 if not force_lit_args:. -> 1071 raise errors[0]. 1072 else:. 1073 raise reduce(operator.or_, force_lit_args). TypingError: Failed in nopython mode pipeline (step: nopython frontend). Failed in nopython mode pipeline (step: nopython frontend). Failed in nopython mode pipeline (step: nopython mode backend). Failed in nopython mode pipeline (step: nopython mode backend). Failed in nopython mode pipeline (step: nopython frontend). No implementation of function Function(<function make_quicksort_impl.<locals>.run_quicksort at 0x7f455d24c280>) found for signature:. . >>> run_quicksort(array(int32, 1d, C)). . There are 2 candidate implementations:. - Of which 2 did not match due to:. Overload in function 'register_jitable.<locals>.wrap.<locals>.ov_wrap': File: numba/core/extending.py: Line 150. With argument(s): '(array(int32, 1d, C))':. Rejected as the implementation raised a specific error:. UnsupportedError: Failed in nopython mode pipeline (step: analyzing bytecode). Use of unsupported opcode (LOAD_ASSERTION_ERROR) found. . File ""../../../../.local/lib/python3.9/site-packages/numba/misc/quicksort.py"", line 180:. def run_quicksort(A):. <source elided>. while high - low >= SMALL_QUICKSORT:. assert n < MAX_STACK. ^. . raised from /home/gabriel/.local/lib/python3.9/site-packages/numba/core/byteflow.py:269. During: resolving callee type: Function(<function make_quicksort_impl.<locals>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:9020,deployability,Fail,Failed,9020,"in propagate(self, raise_errors). 1069 if isinstance(e, ForceLiteralArg)]. 1070 if not force_lit_args:. -> 1071 raise errors[0]. 1072 else:. 1073 raise reduce(operator.or_, force_lit_args). TypingError: Failed in nopython mode pipeline (step: nopython frontend). Failed in nopython mode pipeline (step: nopython frontend). Failed in nopython mode pipeline (step: nopython mode backend). Failed in nopython mode pipeline (step: nopython mode backend). Failed in nopython mode pipeline (step: nopython frontend). No implementation of function Function(<function make_quicksort_impl.<locals>.run_quicksort at 0x7f455d24c280>) found for signature:. . >>> run_quicksort(array(int32, 1d, C)). . There are 2 candidate implementations:. - Of which 2 did not match due to:. Overload in function 'register_jitable.<locals>.wrap.<locals>.ov_wrap': File: numba/core/extending.py: Line 150. With argument(s): '(array(int32, 1d, C))':. Rejected as the implementation raised a specific error:. UnsupportedError: Failed in nopython mode pipeline (step: analyzing bytecode). Use of unsupported opcode (LOAD_ASSERTION_ERROR) found. . File ""../../../../.local/lib/python3.9/site-packages/numba/misc/quicksort.py"", line 180:. def run_quicksort(A):. <source elided>. while high - low >= SMALL_QUICKSORT:. assert n < MAX_STACK. ^. . raised from /home/gabriel/.local/lib/python3.9/site-packages/numba/core/byteflow.py:269. During: resolving callee type: Function(<function make_quicksort_impl.<locals>.run_quicksort at 0x7f455d24c280>). During: typing of call at /home/gabriel/.local/lib/python3.9/site-packages/numba/np/arrayobj.py (5007). File ""../../../../.local/lib/python3.9/site-packages/numba/np/arrayobj.py"", line 5007:. def array_sort_impl(arr):. <source elided>. # Note we clobber the return value. sort_func(arr). ^. During: lowering ""$14call_method.5 = call $12load_method.4(func=$12load_method.4, args=[], kws=(), vararg=None)"" at /home/gabriel/.local/lib/python3.9/site-packages/numba/np/arrayobj.py (5017). D",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:9044,deployability,pipelin,pipeline,9044,"errors). 1069 if isinstance(e, ForceLiteralArg)]. 1070 if not force_lit_args:. -> 1071 raise errors[0]. 1072 else:. 1073 raise reduce(operator.or_, force_lit_args). TypingError: Failed in nopython mode pipeline (step: nopython frontend). Failed in nopython mode pipeline (step: nopython frontend). Failed in nopython mode pipeline (step: nopython mode backend). Failed in nopython mode pipeline (step: nopython mode backend). Failed in nopython mode pipeline (step: nopython frontend). No implementation of function Function(<function make_quicksort_impl.<locals>.run_quicksort at 0x7f455d24c280>) found for signature:. . >>> run_quicksort(array(int32, 1d, C)). . There are 2 candidate implementations:. - Of which 2 did not match due to:. Overload in function 'register_jitable.<locals>.wrap.<locals>.ov_wrap': File: numba/core/extending.py: Line 150. With argument(s): '(array(int32, 1d, C))':. Rejected as the implementation raised a specific error:. UnsupportedError: Failed in nopython mode pipeline (step: analyzing bytecode). Use of unsupported opcode (LOAD_ASSERTION_ERROR) found. . File ""../../../../.local/lib/python3.9/site-packages/numba/misc/quicksort.py"", line 180:. def run_quicksort(A):. <source elided>. while high - low >= SMALL_QUICKSORT:. assert n < MAX_STACK. ^. . raised from /home/gabriel/.local/lib/python3.9/site-packages/numba/core/byteflow.py:269. During: resolving callee type: Function(<function make_quicksort_impl.<locals>.run_quicksort at 0x7f455d24c280>). During: typing of call at /home/gabriel/.local/lib/python3.9/site-packages/numba/np/arrayobj.py (5007). File ""../../../../.local/lib/python3.9/site-packages/numba/np/arrayobj.py"", line 5007:. def array_sort_impl(arr):. <source elided>. # Note we clobber the return value. sort_func(arr). ^. During: lowering ""$14call_method.5 = call $12load_method.4(func=$12load_method.4, args=[], kws=(), vararg=None)"" at /home/gabriel/.local/lib/python3.9/site-packages/numba/np/arrayobj.py (5017). During: lowering ""$8call_m",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:11417,deployability,Version,Versions,11417,"rr, sparse.py:28)], kws=(), vararg=None)"" at /home/gabriel/.local/lib/python3.9/site-packages/pynndescent/sparse.py (28). During: resolving callee type: type(CPUDispatcher(<function arr_unique at 0x7f4574e5aa60>)). During: typing of call at /home/gabriel/.local/lib/python3.9/site-packages/pynndescent/sparse.py (41). During: resolving callee type: type(CPUDispatcher(<function arr_unique at 0x7f4574e5aa60>)). During: typing of call at /home/gabriel/.local/lib/python3.9/site-packages/pynndescent/sparse.py (41). File ""../../../../.local/lib/python3.9/site-packages/pynndescent/sparse.py"", line 41:. def arr_union(ar1, ar2):. <source elided>. else:. return arr_unique(np.concatenate((ar1, ar2))). ^. During: resolving callee type: type(CPUDispatcher(<function arr_union at 0x7f4574e5ac10>)). During: typing of call at /home/gabriel/.local/lib/python3.9/site-packages/pynndescent/sparse.py (344). During: resolving callee type: type(CPUDispatcher(<function arr_union at 0x7f4574e5ac10>)). During: typing of call at /home/gabriel/.local/lib/python3.9/site-packages/pynndescent/sparse.py (344). File ""../../../../.local/lib/python3.9/site-packages/pynndescent/sparse.py"", line 344:. def sparse_alternative_jaccard(ind1, data1, ind2, data2):. num_non_zero = arr_union(ind1, ind2).shape[0]. ^. ```. #### Versions. <details>. anndata 0.7.5. scanpy 1.7.0. sinfo 0.3.1. -----. PIL 8.1.0. PyQt5 NA. anndata 0.7.5. appdirs 1.4.4. cffi 1.14.4. colorama 0.4.4. constants NA. cycler 0.10.0. cython_runtime NA. dateutil 2.8.1. get_version 2.1. google NA. h5py 3.1.0. highs_wrapper NA. joblib 1.0.1. kiwisolver 1.3.1. legacy_api_wrap 1.2. llvmlite 0.34.0. matplotlib 3.3.4. mpl_toolkits NA. natsort 7.1.1. numba 0.51.2. numexpr 2.7.2. numpy 1.20.0. packaging 20.8. pandas 1.2.2. pkg_resources NA. psutil 5.8.0. pyexpat NA. pyparsing 2.4.7. pytoml NA. pytz 2021.1. scanpy 1.7.0. scipy 1.6.0. setuptools_scm NA. sinfo 0.3.1. sip NA. six 1.15.0. sklearn 0.24.1. sphinxcontrib NA. tables 3.6.1. yaml 5.3.1. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:3312,energy efficiency,core,core,3312,"47 from pynndescent import NNDescent. 48 from pynndescent.distances import named_distances as pynn_named_distances. 49 from pynndescent.sparse import sparse_named_distances as pynn_sparse_named_distances. ~/.local/lib/python3.9/site-packages/pynndescent/__init__.py in <module>. 1 import pkg_resources. 2 import numba. ----> 3 from .pynndescent_ import NNDescent, PyNNDescentTransformer. 4 . 5 # Workaround: https://github.com/numba/numba/issues/3341. ~/.local/lib/python3.9/site-packages/pynndescent/pynndescent_.py in <module>. 19 import heapq. 20 . ---> 21 import pynndescent.sparse as sparse. 22 import pynndescent.sparse_nndescent as sparse_nnd. 23 import pynndescent.distances as pynnd_dist. ~/.local/lib/python3.9/site-packages/pynndescent/sparse.py in <module>. 341 },. 342 ). --> 343 def sparse_alternative_jaccard(ind1, data1, ind2, data2):. 344 num_non_zero = arr_union(ind1, ind2).shape[0]. 345 num_equal = arr_intersect(ind1, ind2).shape[0]. ~/.local/lib/python3.9/site-packages/numba/core/decorators.py in wrapper(func). 216 with typeinfer.register_dispatcher(disp):. 217 for sig in sigs:. --> 218 disp.compile(sig). 219 disp.disable_compile(). 220 return disp. ~/.local/lib/python3.9/site-packages/numba/core/compiler_lock.py in _acquire_compile_lock(*args, **kwargs). 30 def _acquire_compile_lock(*args, **kwargs):. 31 with self:. ---> 32 return func(*args, **kwargs). 33 return _acquire_compile_lock. 34 . ~/.local/lib/python3.9/site-packages/numba/core/dispatcher.py in compile(self, sig). 817 self._cache_misses[sig] += 1. 818 try:. --> 819 cres = self._compiler.compile(args, return_type). 820 except errors.ForceLiteralArg as e:. 821 def folded(args, kws):. ~/.local/lib/python3.9/site-packages/numba/core/dispatcher.py in compile(self, args, return_type). 80 return retval. 81 else:. ---> 82 raise retval. 83 . 84 def _compile_cached(self, args, return_type):. ~/.local/lib/python3.9/site-packages/numba/core/dispatcher.py in _compile_cached(self, args, return_type). 90 . 91 tr",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:3533,energy efficiency,core,core,3533,"hon3.9/site-packages/pynndescent/__init__.py in <module>. 1 import pkg_resources. 2 import numba. ----> 3 from .pynndescent_ import NNDescent, PyNNDescentTransformer. 4 . 5 # Workaround: https://github.com/numba/numba/issues/3341. ~/.local/lib/python3.9/site-packages/pynndescent/pynndescent_.py in <module>. 19 import heapq. 20 . ---> 21 import pynndescent.sparse as sparse. 22 import pynndescent.sparse_nndescent as sparse_nnd. 23 import pynndescent.distances as pynnd_dist. ~/.local/lib/python3.9/site-packages/pynndescent/sparse.py in <module>. 341 },. 342 ). --> 343 def sparse_alternative_jaccard(ind1, data1, ind2, data2):. 344 num_non_zero = arr_union(ind1, ind2).shape[0]. 345 num_equal = arr_intersect(ind1, ind2).shape[0]. ~/.local/lib/python3.9/site-packages/numba/core/decorators.py in wrapper(func). 216 with typeinfer.register_dispatcher(disp):. 217 for sig in sigs:. --> 218 disp.compile(sig). 219 disp.disable_compile(). 220 return disp. ~/.local/lib/python3.9/site-packages/numba/core/compiler_lock.py in _acquire_compile_lock(*args, **kwargs). 30 def _acquire_compile_lock(*args, **kwargs):. 31 with self:. ---> 32 return func(*args, **kwargs). 33 return _acquire_compile_lock. 34 . ~/.local/lib/python3.9/site-packages/numba/core/dispatcher.py in compile(self, sig). 817 self._cache_misses[sig] += 1. 818 try:. --> 819 cres = self._compiler.compile(args, return_type). 820 except errors.ForceLiteralArg as e:. 821 def folded(args, kws):. ~/.local/lib/python3.9/site-packages/numba/core/dispatcher.py in compile(self, args, return_type). 80 return retval. 81 else:. ---> 82 raise retval. 83 . 84 def _compile_cached(self, args, return_type):. ~/.local/lib/python3.9/site-packages/numba/core/dispatcher.py in _compile_cached(self, args, return_type). 90 . 91 try:. ---> 92 retval = self._compile_core(args, return_type). 93 except errors.TypingError as e:. 94 self._failed_cache[key] = e. ~/.local/lib/python3.9/site-packages/numba/core/dispatcher.py in _compile_core(self, args, re",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:3780,energy efficiency,core,core,3780,"hon3.9/site-packages/pynndescent/pynndescent_.py in <module>. 19 import heapq. 20 . ---> 21 import pynndescent.sparse as sparse. 22 import pynndescent.sparse_nndescent as sparse_nnd. 23 import pynndescent.distances as pynnd_dist. ~/.local/lib/python3.9/site-packages/pynndescent/sparse.py in <module>. 341 },. 342 ). --> 343 def sparse_alternative_jaccard(ind1, data1, ind2, data2):. 344 num_non_zero = arr_union(ind1, ind2).shape[0]. 345 num_equal = arr_intersect(ind1, ind2).shape[0]. ~/.local/lib/python3.9/site-packages/numba/core/decorators.py in wrapper(func). 216 with typeinfer.register_dispatcher(disp):. 217 for sig in sigs:. --> 218 disp.compile(sig). 219 disp.disable_compile(). 220 return disp. ~/.local/lib/python3.9/site-packages/numba/core/compiler_lock.py in _acquire_compile_lock(*args, **kwargs). 30 def _acquire_compile_lock(*args, **kwargs):. 31 with self:. ---> 32 return func(*args, **kwargs). 33 return _acquire_compile_lock. 34 . ~/.local/lib/python3.9/site-packages/numba/core/dispatcher.py in compile(self, sig). 817 self._cache_misses[sig] += 1. 818 try:. --> 819 cres = self._compiler.compile(args, return_type). 820 except errors.ForceLiteralArg as e:. 821 def folded(args, kws):. ~/.local/lib/python3.9/site-packages/numba/core/dispatcher.py in compile(self, args, return_type). 80 return retval. 81 else:. ---> 82 raise retval. 83 . 84 def _compile_cached(self, args, return_type):. ~/.local/lib/python3.9/site-packages/numba/core/dispatcher.py in _compile_cached(self, args, return_type). 90 . 91 try:. ---> 92 retval = self._compile_core(args, return_type). 93 except errors.TypingError as e:. 94 self._failed_cache[key] = e. ~/.local/lib/python3.9/site-packages/numba/core/dispatcher.py in _compile_core(self, args, return_type). 103 . 104 impl = self._get_implementation(args, {}). --> 105 cres = compiler.compile_extra(self.targetdescr.typing_context,. 106 self.targetdescr.target_context,. 107 impl,. ~/.local/lib/python3.9/site-packages/numba/core/compiler.py i",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:4036,energy efficiency,core,core,4036,"e-packages/pynndescent/sparse.py in <module>. 341 },. 342 ). --> 343 def sparse_alternative_jaccard(ind1, data1, ind2, data2):. 344 num_non_zero = arr_union(ind1, ind2).shape[0]. 345 num_equal = arr_intersect(ind1, ind2).shape[0]. ~/.local/lib/python3.9/site-packages/numba/core/decorators.py in wrapper(func). 216 with typeinfer.register_dispatcher(disp):. 217 for sig in sigs:. --> 218 disp.compile(sig). 219 disp.disable_compile(). 220 return disp. ~/.local/lib/python3.9/site-packages/numba/core/compiler_lock.py in _acquire_compile_lock(*args, **kwargs). 30 def _acquire_compile_lock(*args, **kwargs):. 31 with self:. ---> 32 return func(*args, **kwargs). 33 return _acquire_compile_lock. 34 . ~/.local/lib/python3.9/site-packages/numba/core/dispatcher.py in compile(self, sig). 817 self._cache_misses[sig] += 1. 818 try:. --> 819 cres = self._compiler.compile(args, return_type). 820 except errors.ForceLiteralArg as e:. 821 def folded(args, kws):. ~/.local/lib/python3.9/site-packages/numba/core/dispatcher.py in compile(self, args, return_type). 80 return retval. 81 else:. ---> 82 raise retval. 83 . 84 def _compile_cached(self, args, return_type):. ~/.local/lib/python3.9/site-packages/numba/core/dispatcher.py in _compile_cached(self, args, return_type). 90 . 91 try:. ---> 92 retval = self._compile_core(args, return_type). 93 except errors.TypingError as e:. 94 self._failed_cache[key] = e. ~/.local/lib/python3.9/site-packages/numba/core/dispatcher.py in _compile_core(self, args, return_type). 103 . 104 impl = self._get_implementation(args, {}). --> 105 cres = compiler.compile_extra(self.targetdescr.typing_context,. 106 self.targetdescr.target_context,. 107 impl,. ~/.local/lib/python3.9/site-packages/numba/core/compiler.py in compile_extra(typingctx, targetctx, func, args, return_type, flags, locals, library, pipeline_class). 625 pipeline = pipeline_class(typingctx, targetctx, library,. 626 args, return_type, flags, locals). --> 627 return pipeline.compile_extra(func). 628 . ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:4240,energy efficiency,core,core,4240,"sect(ind1, ind2).shape[0]. ~/.local/lib/python3.9/site-packages/numba/core/decorators.py in wrapper(func). 216 with typeinfer.register_dispatcher(disp):. 217 for sig in sigs:. --> 218 disp.compile(sig). 219 disp.disable_compile(). 220 return disp. ~/.local/lib/python3.9/site-packages/numba/core/compiler_lock.py in _acquire_compile_lock(*args, **kwargs). 30 def _acquire_compile_lock(*args, **kwargs):. 31 with self:. ---> 32 return func(*args, **kwargs). 33 return _acquire_compile_lock. 34 . ~/.local/lib/python3.9/site-packages/numba/core/dispatcher.py in compile(self, sig). 817 self._cache_misses[sig] += 1. 818 try:. --> 819 cres = self._compiler.compile(args, return_type). 820 except errors.ForceLiteralArg as e:. 821 def folded(args, kws):. ~/.local/lib/python3.9/site-packages/numba/core/dispatcher.py in compile(self, args, return_type). 80 return retval. 81 else:. ---> 82 raise retval. 83 . 84 def _compile_cached(self, args, return_type):. ~/.local/lib/python3.9/site-packages/numba/core/dispatcher.py in _compile_cached(self, args, return_type). 90 . 91 try:. ---> 92 retval = self._compile_core(args, return_type). 93 except errors.TypingError as e:. 94 self._failed_cache[key] = e. ~/.local/lib/python3.9/site-packages/numba/core/dispatcher.py in _compile_core(self, args, return_type). 103 . 104 impl = self._get_implementation(args, {}). --> 105 cres = compiler.compile_extra(self.targetdescr.typing_context,. 106 self.targetdescr.target_context,. 107 impl,. ~/.local/lib/python3.9/site-packages/numba/core/compiler.py in compile_extra(typingctx, targetctx, func, args, return_type, flags, locals, library, pipeline_class). 625 pipeline = pipeline_class(typingctx, targetctx, library,. 626 args, return_type, flags, locals). --> 627 return pipeline.compile_extra(func). 628 . 629 . ~/.local/lib/python3.9/site-packages/numba/core/compiler.py in compile_extra(self, func). 361 self.state.lifted = (). 362 self.state.lifted_from = None. --> 363 return self._compile_bytecode(). 364 ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:4485,energy efficiency,core,core,4485,"p. ~/.local/lib/python3.9/site-packages/numba/core/compiler_lock.py in _acquire_compile_lock(*args, **kwargs). 30 def _acquire_compile_lock(*args, **kwargs):. 31 with self:. ---> 32 return func(*args, **kwargs). 33 return _acquire_compile_lock. 34 . ~/.local/lib/python3.9/site-packages/numba/core/dispatcher.py in compile(self, sig). 817 self._cache_misses[sig] += 1. 818 try:. --> 819 cres = self._compiler.compile(args, return_type). 820 except errors.ForceLiteralArg as e:. 821 def folded(args, kws):. ~/.local/lib/python3.9/site-packages/numba/core/dispatcher.py in compile(self, args, return_type). 80 return retval. 81 else:. ---> 82 raise retval. 83 . 84 def _compile_cached(self, args, return_type):. ~/.local/lib/python3.9/site-packages/numba/core/dispatcher.py in _compile_cached(self, args, return_type). 90 . 91 try:. ---> 92 retval = self._compile_core(args, return_type). 93 except errors.TypingError as e:. 94 self._failed_cache[key] = e. ~/.local/lib/python3.9/site-packages/numba/core/dispatcher.py in _compile_core(self, args, return_type). 103 . 104 impl = self._get_implementation(args, {}). --> 105 cres = compiler.compile_extra(self.targetdescr.typing_context,. 106 self.targetdescr.target_context,. 107 impl,. ~/.local/lib/python3.9/site-packages/numba/core/compiler.py in compile_extra(typingctx, targetctx, func, args, return_type, flags, locals, library, pipeline_class). 625 pipeline = pipeline_class(typingctx, targetctx, library,. 626 args, return_type, flags, locals). --> 627 return pipeline.compile_extra(func). 628 . 629 . ~/.local/lib/python3.9/site-packages/numba/core/compiler.py in compile_extra(self, func). 361 self.state.lifted = (). 362 self.state.lifted_from = None. --> 363 return self._compile_bytecode(). 364 . 365 def compile_ir(self, func_ir, lifted=(), lifted_from=None):. ~/.local/lib/python3.9/site-packages/numba/core/compiler.py in _compile_bytecode(self). 423 """""". 424 assert self.state.func_ir is None. --> 425 return self._compile_core(). 426 .",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:4764,energy efficiency,core,core,4764,"ackages/numba/core/dispatcher.py in compile(self, sig). 817 self._cache_misses[sig] += 1. 818 try:. --> 819 cres = self._compiler.compile(args, return_type). 820 except errors.ForceLiteralArg as e:. 821 def folded(args, kws):. ~/.local/lib/python3.9/site-packages/numba/core/dispatcher.py in compile(self, args, return_type). 80 return retval. 81 else:. ---> 82 raise retval. 83 . 84 def _compile_cached(self, args, return_type):. ~/.local/lib/python3.9/site-packages/numba/core/dispatcher.py in _compile_cached(self, args, return_type). 90 . 91 try:. ---> 92 retval = self._compile_core(args, return_type). 93 except errors.TypingError as e:. 94 self._failed_cache[key] = e. ~/.local/lib/python3.9/site-packages/numba/core/dispatcher.py in _compile_core(self, args, return_type). 103 . 104 impl = self._get_implementation(args, {}). --> 105 cres = compiler.compile_extra(self.targetdescr.typing_context,. 106 self.targetdescr.target_context,. 107 impl,. ~/.local/lib/python3.9/site-packages/numba/core/compiler.py in compile_extra(typingctx, targetctx, func, args, return_type, flags, locals, library, pipeline_class). 625 pipeline = pipeline_class(typingctx, targetctx, library,. 626 args, return_type, flags, locals). --> 627 return pipeline.compile_extra(func). 628 . 629 . ~/.local/lib/python3.9/site-packages/numba/core/compiler.py in compile_extra(self, func). 361 self.state.lifted = (). 362 self.state.lifted_from = None. --> 363 return self._compile_bytecode(). 364 . 365 def compile_ir(self, func_ir, lifted=(), lifted_from=None):. ~/.local/lib/python3.9/site-packages/numba/core/compiler.py in _compile_bytecode(self). 423 """""". 424 assert self.state.func_ir is None. --> 425 return self._compile_core(). 426 . 427 def _compile_ir(self):. ~/.local/lib/python3.9/site-packages/numba/core/compiler.py in _compile_core(self). 403 self.state.status.fail_reason = e. 404 if is_final_pipeline:. --> 405 raise e. 406 else:. 407 raise CompilerError(""All available pipelines exhausted""). ~/.local/l",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:5087,energy efficiency,core,core,5087,"). 80 return retval. 81 else:. ---> 82 raise retval. 83 . 84 def _compile_cached(self, args, return_type):. ~/.local/lib/python3.9/site-packages/numba/core/dispatcher.py in _compile_cached(self, args, return_type). 90 . 91 try:. ---> 92 retval = self._compile_core(args, return_type). 93 except errors.TypingError as e:. 94 self._failed_cache[key] = e. ~/.local/lib/python3.9/site-packages/numba/core/dispatcher.py in _compile_core(self, args, return_type). 103 . 104 impl = self._get_implementation(args, {}). --> 105 cres = compiler.compile_extra(self.targetdescr.typing_context,. 106 self.targetdescr.target_context,. 107 impl,. ~/.local/lib/python3.9/site-packages/numba/core/compiler.py in compile_extra(typingctx, targetctx, func, args, return_type, flags, locals, library, pipeline_class). 625 pipeline = pipeline_class(typingctx, targetctx, library,. 626 args, return_type, flags, locals). --> 627 return pipeline.compile_extra(func). 628 . 629 . ~/.local/lib/python3.9/site-packages/numba/core/compiler.py in compile_extra(self, func). 361 self.state.lifted = (). 362 self.state.lifted_from = None. --> 363 return self._compile_bytecode(). 364 . 365 def compile_ir(self, func_ir, lifted=(), lifted_from=None):. ~/.local/lib/python3.9/site-packages/numba/core/compiler.py in _compile_bytecode(self). 423 """""". 424 assert self.state.func_ir is None. --> 425 return self._compile_core(). 426 . 427 def _compile_ir(self):. ~/.local/lib/python3.9/site-packages/numba/core/compiler.py in _compile_core(self). 403 self.state.status.fail_reason = e. 404 if is_final_pipeline:. --> 405 raise e. 406 else:. 407 raise CompilerError(""All available pipelines exhausted""). ~/.local/lib/python3.9/site-packages/numba/core/compiler.py in _compile_core(self). 394 res = None. 395 try:. --> 396 pm.run(self.state). 397 if self.state.cr is not None:. 398 break. ~/.local/lib/python3.9/site-packages/numba/core/compiler_machinery.py in run(self, state). 339 (self.pipeline_name, pass_desc). 340 patched_exception",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:5352,energy efficiency,core,core,5352,"args, return_type). 93 except errors.TypingError as e:. 94 self._failed_cache[key] = e. ~/.local/lib/python3.9/site-packages/numba/core/dispatcher.py in _compile_core(self, args, return_type). 103 . 104 impl = self._get_implementation(args, {}). --> 105 cres = compiler.compile_extra(self.targetdescr.typing_context,. 106 self.targetdescr.target_context,. 107 impl,. ~/.local/lib/python3.9/site-packages/numba/core/compiler.py in compile_extra(typingctx, targetctx, func, args, return_type, flags, locals, library, pipeline_class). 625 pipeline = pipeline_class(typingctx, targetctx, library,. 626 args, return_type, flags, locals). --> 627 return pipeline.compile_extra(func). 628 . 629 . ~/.local/lib/python3.9/site-packages/numba/core/compiler.py in compile_extra(self, func). 361 self.state.lifted = (). 362 self.state.lifted_from = None. --> 363 return self._compile_bytecode(). 364 . 365 def compile_ir(self, func_ir, lifted=(), lifted_from=None):. ~/.local/lib/python3.9/site-packages/numba/core/compiler.py in _compile_bytecode(self). 423 """""". 424 assert self.state.func_ir is None. --> 425 return self._compile_core(). 426 . 427 def _compile_ir(self):. ~/.local/lib/python3.9/site-packages/numba/core/compiler.py in _compile_core(self). 403 self.state.status.fail_reason = e. 404 if is_final_pipeline:. --> 405 raise e. 406 else:. 407 raise CompilerError(""All available pipelines exhausted""). ~/.local/lib/python3.9/site-packages/numba/core/compiler.py in _compile_core(self). 394 res = None. 395 try:. --> 396 pm.run(self.state). 397 if self.state.cr is not None:. 398 break. ~/.local/lib/python3.9/site-packages/numba/core/compiler_machinery.py in run(self, state). 339 (self.pipeline_name, pass_desc). 340 patched_exception = self._patch_error(msg, e). --> 341 raise patched_exception. 342 . 343 def dependency_analysis(self):. ~/.local/lib/python3.9/site-packages/numba/core/compiler_machinery.py in run(self, state). 330 pass_inst = _pass_registry.get(pss).pass_inst. 331 if isinstance(",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:5559,energy efficiency,core,core,5559," = self._get_implementation(args, {}). --> 105 cres = compiler.compile_extra(self.targetdescr.typing_context,. 106 self.targetdescr.target_context,. 107 impl,. ~/.local/lib/python3.9/site-packages/numba/core/compiler.py in compile_extra(typingctx, targetctx, func, args, return_type, flags, locals, library, pipeline_class). 625 pipeline = pipeline_class(typingctx, targetctx, library,. 626 args, return_type, flags, locals). --> 627 return pipeline.compile_extra(func). 628 . 629 . ~/.local/lib/python3.9/site-packages/numba/core/compiler.py in compile_extra(self, func). 361 self.state.lifted = (). 362 self.state.lifted_from = None. --> 363 return self._compile_bytecode(). 364 . 365 def compile_ir(self, func_ir, lifted=(), lifted_from=None):. ~/.local/lib/python3.9/site-packages/numba/core/compiler.py in _compile_bytecode(self). 423 """""". 424 assert self.state.func_ir is None. --> 425 return self._compile_core(). 426 . 427 def _compile_ir(self):. ~/.local/lib/python3.9/site-packages/numba/core/compiler.py in _compile_core(self). 403 self.state.status.fail_reason = e. 404 if is_final_pipeline:. --> 405 raise e. 406 else:. 407 raise CompilerError(""All available pipelines exhausted""). ~/.local/lib/python3.9/site-packages/numba/core/compiler.py in _compile_core(self). 394 res = None. 395 try:. --> 396 pm.run(self.state). 397 if self.state.cr is not None:. 398 break. ~/.local/lib/python3.9/site-packages/numba/core/compiler_machinery.py in run(self, state). 339 (self.pipeline_name, pass_desc). 340 patched_exception = self._patch_error(msg, e). --> 341 raise patched_exception. 342 . 343 def dependency_analysis(self):. ~/.local/lib/python3.9/site-packages/numba/core/compiler_machinery.py in run(self, state). 330 pass_inst = _pass_registry.get(pss).pass_inst. 331 if isinstance(pass_inst, CompilerPass):. --> 332 self._runPass(idx, pass_inst, state). 333 else:. 334 raise BaseException(""Legacy pass in use""). ~/.local/lib/python3.9/site-packages/numba/core/compiler_lock.py in _acquire",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:5799,energy efficiency,core,core,5799,"ingctx, targetctx, func, args, return_type, flags, locals, library, pipeline_class). 625 pipeline = pipeline_class(typingctx, targetctx, library,. 626 args, return_type, flags, locals). --> 627 return pipeline.compile_extra(func). 628 . 629 . ~/.local/lib/python3.9/site-packages/numba/core/compiler.py in compile_extra(self, func). 361 self.state.lifted = (). 362 self.state.lifted_from = None. --> 363 return self._compile_bytecode(). 364 . 365 def compile_ir(self, func_ir, lifted=(), lifted_from=None):. ~/.local/lib/python3.9/site-packages/numba/core/compiler.py in _compile_bytecode(self). 423 """""". 424 assert self.state.func_ir is None. --> 425 return self._compile_core(). 426 . 427 def _compile_ir(self):. ~/.local/lib/python3.9/site-packages/numba/core/compiler.py in _compile_core(self). 403 self.state.status.fail_reason = e. 404 if is_final_pipeline:. --> 405 raise e. 406 else:. 407 raise CompilerError(""All available pipelines exhausted""). ~/.local/lib/python3.9/site-packages/numba/core/compiler.py in _compile_core(self). 394 res = None. 395 try:. --> 396 pm.run(self.state). 397 if self.state.cr is not None:. 398 break. ~/.local/lib/python3.9/site-packages/numba/core/compiler_machinery.py in run(self, state). 339 (self.pipeline_name, pass_desc). 340 patched_exception = self._patch_error(msg, e). --> 341 raise patched_exception. 342 . 343 def dependency_analysis(self):. ~/.local/lib/python3.9/site-packages/numba/core/compiler_machinery.py in run(self, state). 330 pass_inst = _pass_registry.get(pss).pass_inst. 331 if isinstance(pass_inst, CompilerPass):. --> 332 self._runPass(idx, pass_inst, state). 333 else:. 334 raise BaseException(""Legacy pass in use""). ~/.local/lib/python3.9/site-packages/numba/core/compiler_lock.py in _acquire_compile_lock(*args, **kwargs). 30 def _acquire_compile_lock(*args, **kwargs):. 31 with self:. ---> 32 return func(*args, **kwargs). 33 return _acquire_compile_lock. 34 . ~/.local/lib/python3.9/site-packages/numba/core/compiler_machinery.py",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:5983,energy efficiency,core,core,5983,". --> 627 return pipeline.compile_extra(func). 628 . 629 . ~/.local/lib/python3.9/site-packages/numba/core/compiler.py in compile_extra(self, func). 361 self.state.lifted = (). 362 self.state.lifted_from = None. --> 363 return self._compile_bytecode(). 364 . 365 def compile_ir(self, func_ir, lifted=(), lifted_from=None):. ~/.local/lib/python3.9/site-packages/numba/core/compiler.py in _compile_bytecode(self). 423 """""". 424 assert self.state.func_ir is None. --> 425 return self._compile_core(). 426 . 427 def _compile_ir(self):. ~/.local/lib/python3.9/site-packages/numba/core/compiler.py in _compile_core(self). 403 self.state.status.fail_reason = e. 404 if is_final_pipeline:. --> 405 raise e. 406 else:. 407 raise CompilerError(""All available pipelines exhausted""). ~/.local/lib/python3.9/site-packages/numba/core/compiler.py in _compile_core(self). 394 res = None. 395 try:. --> 396 pm.run(self.state). 397 if self.state.cr is not None:. 398 break. ~/.local/lib/python3.9/site-packages/numba/core/compiler_machinery.py in run(self, state). 339 (self.pipeline_name, pass_desc). 340 patched_exception = self._patch_error(msg, e). --> 341 raise patched_exception. 342 . 343 def dependency_analysis(self):. ~/.local/lib/python3.9/site-packages/numba/core/compiler_machinery.py in run(self, state). 330 pass_inst = _pass_registry.get(pss).pass_inst. 331 if isinstance(pass_inst, CompilerPass):. --> 332 self._runPass(idx, pass_inst, state). 333 else:. 334 raise BaseException(""Legacy pass in use""). ~/.local/lib/python3.9/site-packages/numba/core/compiler_lock.py in _acquire_compile_lock(*args, **kwargs). 30 def _acquire_compile_lock(*args, **kwargs):. 31 with self:. ---> 32 return func(*args, **kwargs). 33 return _acquire_compile_lock. 34 . ~/.local/lib/python3.9/site-packages/numba/core/compiler_machinery.py in _runPass(self, index, pss, internal_state). 289 mutated |= check(pss.run_initialization, internal_state). 290 with SimpleTimer() as pass_time:. --> 291 mutated |= check(pss.run_pas",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:6237,energy efficiency,core,core,6237,"64 . 365 def compile_ir(self, func_ir, lifted=(), lifted_from=None):. ~/.local/lib/python3.9/site-packages/numba/core/compiler.py in _compile_bytecode(self). 423 """""". 424 assert self.state.func_ir is None. --> 425 return self._compile_core(). 426 . 427 def _compile_ir(self):. ~/.local/lib/python3.9/site-packages/numba/core/compiler.py in _compile_core(self). 403 self.state.status.fail_reason = e. 404 if is_final_pipeline:. --> 405 raise e. 406 else:. 407 raise CompilerError(""All available pipelines exhausted""). ~/.local/lib/python3.9/site-packages/numba/core/compiler.py in _compile_core(self). 394 res = None. 395 try:. --> 396 pm.run(self.state). 397 if self.state.cr is not None:. 398 break. ~/.local/lib/python3.9/site-packages/numba/core/compiler_machinery.py in run(self, state). 339 (self.pipeline_name, pass_desc). 340 patched_exception = self._patch_error(msg, e). --> 341 raise patched_exception. 342 . 343 def dependency_analysis(self):. ~/.local/lib/python3.9/site-packages/numba/core/compiler_machinery.py in run(self, state). 330 pass_inst = _pass_registry.get(pss).pass_inst. 331 if isinstance(pass_inst, CompilerPass):. --> 332 self._runPass(idx, pass_inst, state). 333 else:. 334 raise BaseException(""Legacy pass in use""). ~/.local/lib/python3.9/site-packages/numba/core/compiler_lock.py in _acquire_compile_lock(*args, **kwargs). 30 def _acquire_compile_lock(*args, **kwargs):. 31 with self:. ---> 32 return func(*args, **kwargs). 33 return _acquire_compile_lock. 34 . ~/.local/lib/python3.9/site-packages/numba/core/compiler_machinery.py in _runPass(self, index, pss, internal_state). 289 mutated |= check(pss.run_initialization, internal_state). 290 with SimpleTimer() as pass_time:. --> 291 mutated |= check(pss.run_pass, internal_state). 292 with SimpleTimer() as finalize_time:. 293 mutated |= check(pss.run_finalizer, internal_state). ~/.local/lib/python3.9/site-packages/numba/core/compiler_machinery.py in check(func, compiler_state). 262 . 263 def check(func, compile",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:6528,energy efficiency,core,core,6528,"ython3.9/site-packages/numba/core/compiler.py in _compile_core(self). 403 self.state.status.fail_reason = e. 404 if is_final_pipeline:. --> 405 raise e. 406 else:. 407 raise CompilerError(""All available pipelines exhausted""). ~/.local/lib/python3.9/site-packages/numba/core/compiler.py in _compile_core(self). 394 res = None. 395 try:. --> 396 pm.run(self.state). 397 if self.state.cr is not None:. 398 break. ~/.local/lib/python3.9/site-packages/numba/core/compiler_machinery.py in run(self, state). 339 (self.pipeline_name, pass_desc). 340 patched_exception = self._patch_error(msg, e). --> 341 raise patched_exception. 342 . 343 def dependency_analysis(self):. ~/.local/lib/python3.9/site-packages/numba/core/compiler_machinery.py in run(self, state). 330 pass_inst = _pass_registry.get(pss).pass_inst. 331 if isinstance(pass_inst, CompilerPass):. --> 332 self._runPass(idx, pass_inst, state). 333 else:. 334 raise BaseException(""Legacy pass in use""). ~/.local/lib/python3.9/site-packages/numba/core/compiler_lock.py in _acquire_compile_lock(*args, **kwargs). 30 def _acquire_compile_lock(*args, **kwargs):. 31 with self:. ---> 32 return func(*args, **kwargs). 33 return _acquire_compile_lock. 34 . ~/.local/lib/python3.9/site-packages/numba/core/compiler_machinery.py in _runPass(self, index, pss, internal_state). 289 mutated |= check(pss.run_initialization, internal_state). 290 with SimpleTimer() as pass_time:. --> 291 mutated |= check(pss.run_pass, internal_state). 292 with SimpleTimer() as finalize_time:. 293 mutated |= check(pss.run_finalizer, internal_state). ~/.local/lib/python3.9/site-packages/numba/core/compiler_machinery.py in check(func, compiler_state). 262 . 263 def check(func, compiler_state):. --> 264 mangled = func(compiler_state). 265 if mangled not in (True, False):. 266 msg = (""CompilerPass implementations should return True/False. "". ~/.local/lib/python3.9/site-packages/numba/core/typed_passes.py in run_pass(self, state). 90 % (state.func_id.func_name,)):. 91 # Ty",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:6775,energy efficiency,core,core,6775,"9/site-packages/numba/core/compiler.py in _compile_core(self). 394 res = None. 395 try:. --> 396 pm.run(self.state). 397 if self.state.cr is not None:. 398 break. ~/.local/lib/python3.9/site-packages/numba/core/compiler_machinery.py in run(self, state). 339 (self.pipeline_name, pass_desc). 340 patched_exception = self._patch_error(msg, e). --> 341 raise patched_exception. 342 . 343 def dependency_analysis(self):. ~/.local/lib/python3.9/site-packages/numba/core/compiler_machinery.py in run(self, state). 330 pass_inst = _pass_registry.get(pss).pass_inst. 331 if isinstance(pass_inst, CompilerPass):. --> 332 self._runPass(idx, pass_inst, state). 333 else:. 334 raise BaseException(""Legacy pass in use""). ~/.local/lib/python3.9/site-packages/numba/core/compiler_lock.py in _acquire_compile_lock(*args, **kwargs). 30 def _acquire_compile_lock(*args, **kwargs):. 31 with self:. ---> 32 return func(*args, **kwargs). 33 return _acquire_compile_lock. 34 . ~/.local/lib/python3.9/site-packages/numba/core/compiler_machinery.py in _runPass(self, index, pss, internal_state). 289 mutated |= check(pss.run_initialization, internal_state). 290 with SimpleTimer() as pass_time:. --> 291 mutated |= check(pss.run_pass, internal_state). 292 with SimpleTimer() as finalize_time:. 293 mutated |= check(pss.run_finalizer, internal_state). ~/.local/lib/python3.9/site-packages/numba/core/compiler_machinery.py in check(func, compiler_state). 262 . 263 def check(func, compiler_state):. --> 264 mangled = func(compiler_state). 265 if mangled not in (True, False):. 266 msg = (""CompilerPass implementations should return True/False. "". ~/.local/lib/python3.9/site-packages/numba/core/typed_passes.py in run_pass(self, state). 90 % (state.func_id.func_name,)):. 91 # Type inference. ---> 92 typemap, return_type, calltypes = type_inference_stage(. 93 state.typingctx,. 94 state.func_ir,. ~/.local/lib/python3.9/site-packages/numba/core/typed_passes.py in type_inference_stage(typingctx, interp, args, return_type, lo",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:7147,energy efficiency,core,core,7147,"n. 342 . 343 def dependency_analysis(self):. ~/.local/lib/python3.9/site-packages/numba/core/compiler_machinery.py in run(self, state). 330 pass_inst = _pass_registry.get(pss).pass_inst. 331 if isinstance(pass_inst, CompilerPass):. --> 332 self._runPass(idx, pass_inst, state). 333 else:. 334 raise BaseException(""Legacy pass in use""). ~/.local/lib/python3.9/site-packages/numba/core/compiler_lock.py in _acquire_compile_lock(*args, **kwargs). 30 def _acquire_compile_lock(*args, **kwargs):. 31 with self:. ---> 32 return func(*args, **kwargs). 33 return _acquire_compile_lock. 34 . ~/.local/lib/python3.9/site-packages/numba/core/compiler_machinery.py in _runPass(self, index, pss, internal_state). 289 mutated |= check(pss.run_initialization, internal_state). 290 with SimpleTimer() as pass_time:. --> 291 mutated |= check(pss.run_pass, internal_state). 292 with SimpleTimer() as finalize_time:. 293 mutated |= check(pss.run_finalizer, internal_state). ~/.local/lib/python3.9/site-packages/numba/core/compiler_machinery.py in check(func, compiler_state). 262 . 263 def check(func, compiler_state):. --> 264 mangled = func(compiler_state). 265 if mangled not in (True, False):. 266 msg = (""CompilerPass implementations should return True/False. "". ~/.local/lib/python3.9/site-packages/numba/core/typed_passes.py in run_pass(self, state). 90 % (state.func_id.func_name,)):. 91 # Type inference. ---> 92 typemap, return_type, calltypes = type_inference_stage(. 93 state.typingctx,. 94 state.func_ir,. ~/.local/lib/python3.9/site-packages/numba/core/typed_passes.py in type_inference_stage(typingctx, interp, args, return_type, locals, raise_errors). 68 . 69 infer.build_constraint(). ---> 70 infer.propagate(raise_errors=raise_errors). 71 typemap, restype, calltypes = infer.unify(raise_errors=raise_errors). 72 . ~/.local/lib/python3.9/site-packages/numba/core/typeinfer.py in propagate(self, raise_errors). 1069 if isinstance(e, ForceLiteralArg)]. 1070 if not force_lit_args:. -> 1071 raise errors[0",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:7441,energy efficiency,core,core,7441,"aise BaseException(""Legacy pass in use""). ~/.local/lib/python3.9/site-packages/numba/core/compiler_lock.py in _acquire_compile_lock(*args, **kwargs). 30 def _acquire_compile_lock(*args, **kwargs):. 31 with self:. ---> 32 return func(*args, **kwargs). 33 return _acquire_compile_lock. 34 . ~/.local/lib/python3.9/site-packages/numba/core/compiler_machinery.py in _runPass(self, index, pss, internal_state). 289 mutated |= check(pss.run_initialization, internal_state). 290 with SimpleTimer() as pass_time:. --> 291 mutated |= check(pss.run_pass, internal_state). 292 with SimpleTimer() as finalize_time:. 293 mutated |= check(pss.run_finalizer, internal_state). ~/.local/lib/python3.9/site-packages/numba/core/compiler_machinery.py in check(func, compiler_state). 262 . 263 def check(func, compiler_state):. --> 264 mangled = func(compiler_state). 265 if mangled not in (True, False):. 266 msg = (""CompilerPass implementations should return True/False. "". ~/.local/lib/python3.9/site-packages/numba/core/typed_passes.py in run_pass(self, state). 90 % (state.func_id.func_name,)):. 91 # Type inference. ---> 92 typemap, return_type, calltypes = type_inference_stage(. 93 state.typingctx,. 94 state.func_ir,. ~/.local/lib/python3.9/site-packages/numba/core/typed_passes.py in type_inference_stage(typingctx, interp, args, return_type, locals, raise_errors). 68 . 69 infer.build_constraint(). ---> 70 infer.propagate(raise_errors=raise_errors). 71 typemap, restype, calltypes = infer.unify(raise_errors=raise_errors). 72 . ~/.local/lib/python3.9/site-packages/numba/core/typeinfer.py in propagate(self, raise_errors). 1069 if isinstance(e, ForceLiteralArg)]. 1070 if not force_lit_args:. -> 1071 raise errors[0]. 1072 else:. 1073 raise reduce(operator.or_, force_lit_args). TypingError: Failed in nopython mode pipeline (step: nopython frontend). Failed in nopython mode pipeline (step: nopython frontend). Failed in nopython mode pipeline (step: nopython mode backend). Failed in nopython mode pipeline ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:7692,energy efficiency,core,core,7692,"33 return _acquire_compile_lock. 34 . ~/.local/lib/python3.9/site-packages/numba/core/compiler_machinery.py in _runPass(self, index, pss, internal_state). 289 mutated |= check(pss.run_initialization, internal_state). 290 with SimpleTimer() as pass_time:. --> 291 mutated |= check(pss.run_pass, internal_state). 292 with SimpleTimer() as finalize_time:. 293 mutated |= check(pss.run_finalizer, internal_state). ~/.local/lib/python3.9/site-packages/numba/core/compiler_machinery.py in check(func, compiler_state). 262 . 263 def check(func, compiler_state):. --> 264 mangled = func(compiler_state). 265 if mangled not in (True, False):. 266 msg = (""CompilerPass implementations should return True/False. "". ~/.local/lib/python3.9/site-packages/numba/core/typed_passes.py in run_pass(self, state). 90 % (state.func_id.func_name,)):. 91 # Type inference. ---> 92 typemap, return_type, calltypes = type_inference_stage(. 93 state.typingctx,. 94 state.func_ir,. ~/.local/lib/python3.9/site-packages/numba/core/typed_passes.py in type_inference_stage(typingctx, interp, args, return_type, locals, raise_errors). 68 . 69 infer.build_constraint(). ---> 70 infer.propagate(raise_errors=raise_errors). 71 typemap, restype, calltypes = infer.unify(raise_errors=raise_errors). 72 . ~/.local/lib/python3.9/site-packages/numba/core/typeinfer.py in propagate(self, raise_errors). 1069 if isinstance(e, ForceLiteralArg)]. 1070 if not force_lit_args:. -> 1071 raise errors[0]. 1072 else:. 1073 raise reduce(operator.or_, force_lit_args). TypingError: Failed in nopython mode pipeline (step: nopython frontend). Failed in nopython mode pipeline (step: nopython frontend). Failed in nopython mode pipeline (step: nopython mode backend). Failed in nopython mode pipeline (step: nopython mode backend). Failed in nopython mode pipeline (step: nopython frontend). No implementation of function Function(<function make_quicksort_impl.<locals>.run_quicksort at 0x7f455d24c280>) found for signature:. . >>> run_quicksort(array(",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:8005,energy efficiency,core,core,8005,"2 with SimpleTimer() as finalize_time:. 293 mutated |= check(pss.run_finalizer, internal_state). ~/.local/lib/python3.9/site-packages/numba/core/compiler_machinery.py in check(func, compiler_state). 262 . 263 def check(func, compiler_state):. --> 264 mangled = func(compiler_state). 265 if mangled not in (True, False):. 266 msg = (""CompilerPass implementations should return True/False. "". ~/.local/lib/python3.9/site-packages/numba/core/typed_passes.py in run_pass(self, state). 90 % (state.func_id.func_name,)):. 91 # Type inference. ---> 92 typemap, return_type, calltypes = type_inference_stage(. 93 state.typingctx,. 94 state.func_ir,. ~/.local/lib/python3.9/site-packages/numba/core/typed_passes.py in type_inference_stage(typingctx, interp, args, return_type, locals, raise_errors). 68 . 69 infer.build_constraint(). ---> 70 infer.propagate(raise_errors=raise_errors). 71 typemap, restype, calltypes = infer.unify(raise_errors=raise_errors). 72 . ~/.local/lib/python3.9/site-packages/numba/core/typeinfer.py in propagate(self, raise_errors). 1069 if isinstance(e, ForceLiteralArg)]. 1070 if not force_lit_args:. -> 1071 raise errors[0]. 1072 else:. 1073 raise reduce(operator.or_, force_lit_args). TypingError: Failed in nopython mode pipeline (step: nopython frontend). Failed in nopython mode pipeline (step: nopython frontend). Failed in nopython mode pipeline (step: nopython mode backend). Failed in nopython mode pipeline (step: nopython mode backend). Failed in nopython mode pipeline (step: nopython frontend). No implementation of function Function(<function make_quicksort_impl.<locals>.run_quicksort at 0x7f455d24c280>) found for signature:. . >>> run_quicksort(array(int32, 1d, C)). . There are 2 candidate implementations:. - Of which 2 did not match due to:. Overload in function 'register_jitable.<locals>.wrap.<locals>.ov_wrap': File: numba/core/extending.py: Line 150. With argument(s): '(array(int32, 1d, C))':. Rejected as the implementation raised a specific error:. Unsup",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:8175,energy efficiency,reduc,reduce,8175,"heck(func, compiler_state). 262 . 263 def check(func, compiler_state):. --> 264 mangled = func(compiler_state). 265 if mangled not in (True, False):. 266 msg = (""CompilerPass implementations should return True/False. "". ~/.local/lib/python3.9/site-packages/numba/core/typed_passes.py in run_pass(self, state). 90 % (state.func_id.func_name,)):. 91 # Type inference. ---> 92 typemap, return_type, calltypes = type_inference_stage(. 93 state.typingctx,. 94 state.func_ir,. ~/.local/lib/python3.9/site-packages/numba/core/typed_passes.py in type_inference_stage(typingctx, interp, args, return_type, locals, raise_errors). 68 . 69 infer.build_constraint(). ---> 70 infer.propagate(raise_errors=raise_errors). 71 typemap, restype, calltypes = infer.unify(raise_errors=raise_errors). 72 . ~/.local/lib/python3.9/site-packages/numba/core/typeinfer.py in propagate(self, raise_errors). 1069 if isinstance(e, ForceLiteralArg)]. 1070 if not force_lit_args:. -> 1071 raise errors[0]. 1072 else:. 1073 raise reduce(operator.or_, force_lit_args). TypingError: Failed in nopython mode pipeline (step: nopython frontend). Failed in nopython mode pipeline (step: nopython frontend). Failed in nopython mode pipeline (step: nopython mode backend). Failed in nopython mode pipeline (step: nopython mode backend). Failed in nopython mode pipeline (step: nopython frontend). No implementation of function Function(<function make_quicksort_impl.<locals>.run_quicksort at 0x7f455d24c280>) found for signature:. . >>> run_quicksort(array(int32, 1d, C)). . There are 2 candidate implementations:. - Of which 2 did not match due to:. Overload in function 'register_jitable.<locals>.wrap.<locals>.ov_wrap': File: numba/core/extending.py: Line 150. With argument(s): '(array(int32, 1d, C))':. Rejected as the implementation raised a specific error:. UnsupportedError: Failed in nopython mode pipeline (step: analyzing bytecode). Use of unsupported opcode (LOAD_ASSERTION_ERROR) found. . File ""../../../../.local/lib/python3.9/",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:8872,energy efficiency,core,core,8872,"_errors). 71 typemap, restype, calltypes = infer.unify(raise_errors=raise_errors). 72 . ~/.local/lib/python3.9/site-packages/numba/core/typeinfer.py in propagate(self, raise_errors). 1069 if isinstance(e, ForceLiteralArg)]. 1070 if not force_lit_args:. -> 1071 raise errors[0]. 1072 else:. 1073 raise reduce(operator.or_, force_lit_args). TypingError: Failed in nopython mode pipeline (step: nopython frontend). Failed in nopython mode pipeline (step: nopython frontend). Failed in nopython mode pipeline (step: nopython mode backend). Failed in nopython mode pipeline (step: nopython mode backend). Failed in nopython mode pipeline (step: nopython frontend). No implementation of function Function(<function make_quicksort_impl.<locals>.run_quicksort at 0x7f455d24c280>) found for signature:. . >>> run_quicksort(array(int32, 1d, C)). . There are 2 candidate implementations:. - Of which 2 did not match due to:. Overload in function 'register_jitable.<locals>.wrap.<locals>.ov_wrap': File: numba/core/extending.py: Line 150. With argument(s): '(array(int32, 1d, C))':. Rejected as the implementation raised a specific error:. UnsupportedError: Failed in nopython mode pipeline (step: analyzing bytecode). Use of unsupported opcode (LOAD_ASSERTION_ERROR) found. . File ""../../../../.local/lib/python3.9/site-packages/numba/misc/quicksort.py"", line 180:. def run_quicksort(A):. <source elided>. while high - low >= SMALL_QUICKSORT:. assert n < MAX_STACK. ^. . raised from /home/gabriel/.local/lib/python3.9/site-packages/numba/core/byteflow.py:269. During: resolving callee type: Function(<function make_quicksort_impl.<locals>.run_quicksort at 0x7f455d24c280>). During: typing of call at /home/gabriel/.local/lib/python3.9/site-packages/numba/np/arrayobj.py (5007). File ""../../../../.local/lib/python3.9/site-packages/numba/np/arrayobj.py"", line 5007:. def array_sort_impl(arr):. <source elided>. # Note we clobber the return value. sort_func(arr). ^. During: lowering ""$14call_method.5 = call $12l",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:9401,energy efficiency,core,core,9401,"kend). Failed in nopython mode pipeline (step: nopython mode backend). Failed in nopython mode pipeline (step: nopython frontend). No implementation of function Function(<function make_quicksort_impl.<locals>.run_quicksort at 0x7f455d24c280>) found for signature:. . >>> run_quicksort(array(int32, 1d, C)). . There are 2 candidate implementations:. - Of which 2 did not match due to:. Overload in function 'register_jitable.<locals>.wrap.<locals>.ov_wrap': File: numba/core/extending.py: Line 150. With argument(s): '(array(int32, 1d, C))':. Rejected as the implementation raised a specific error:. UnsupportedError: Failed in nopython mode pipeline (step: analyzing bytecode). Use of unsupported opcode (LOAD_ASSERTION_ERROR) found. . File ""../../../../.local/lib/python3.9/site-packages/numba/misc/quicksort.py"", line 180:. def run_quicksort(A):. <source elided>. while high - low >= SMALL_QUICKSORT:. assert n < MAX_STACK. ^. . raised from /home/gabriel/.local/lib/python3.9/site-packages/numba/core/byteflow.py:269. During: resolving callee type: Function(<function make_quicksort_impl.<locals>.run_quicksort at 0x7f455d24c280>). During: typing of call at /home/gabriel/.local/lib/python3.9/site-packages/numba/np/arrayobj.py (5007). File ""../../../../.local/lib/python3.9/site-packages/numba/np/arrayobj.py"", line 5007:. def array_sort_impl(arr):. <source elided>. # Note we clobber the return value. sort_func(arr). ^. During: lowering ""$14call_method.5 = call $12load_method.4(func=$12load_method.4, args=[], kws=(), vararg=None)"" at /home/gabriel/.local/lib/python3.9/site-packages/numba/np/arrayobj.py (5017). During: lowering ""$8call_method.3 = call $4load_method.1(arr, func=$4load_method.1, args=[Var(arr, sparse.py:28)], kws=(), vararg=None)"" at /home/gabriel/.local/lib/python3.9/site-packages/pynndescent/sparse.py (28). During: resolving callee type: type(CPUDispatcher(<function arr_unique at 0x7f4574e5aa60>)). During: typing of call at /home/gabriel/.local/lib/python3.9/site-packa",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:10275,energy efficiency,CPU,CPUDispatcher,10275,"low >= SMALL_QUICKSORT:. assert n < MAX_STACK. ^. . raised from /home/gabriel/.local/lib/python3.9/site-packages/numba/core/byteflow.py:269. During: resolving callee type: Function(<function make_quicksort_impl.<locals>.run_quicksort at 0x7f455d24c280>). During: typing of call at /home/gabriel/.local/lib/python3.9/site-packages/numba/np/arrayobj.py (5007). File ""../../../../.local/lib/python3.9/site-packages/numba/np/arrayobj.py"", line 5007:. def array_sort_impl(arr):. <source elided>. # Note we clobber the return value. sort_func(arr). ^. During: lowering ""$14call_method.5 = call $12load_method.4(func=$12load_method.4, args=[], kws=(), vararg=None)"" at /home/gabriel/.local/lib/python3.9/site-packages/numba/np/arrayobj.py (5017). During: lowering ""$8call_method.3 = call $4load_method.1(arr, func=$4load_method.1, args=[Var(arr, sparse.py:28)], kws=(), vararg=None)"" at /home/gabriel/.local/lib/python3.9/site-packages/pynndescent/sparse.py (28). During: resolving callee type: type(CPUDispatcher(<function arr_unique at 0x7f4574e5aa60>)). During: typing of call at /home/gabriel/.local/lib/python3.9/site-packages/pynndescent/sparse.py (41). During: resolving callee type: type(CPUDispatcher(<function arr_unique at 0x7f4574e5aa60>)). During: typing of call at /home/gabriel/.local/lib/python3.9/site-packages/pynndescent/sparse.py (41). File ""../../../../.local/lib/python3.9/site-packages/pynndescent/sparse.py"", line 41:. def arr_union(ar1, ar2):. <source elided>. else:. return arr_unique(np.concatenate((ar1, ar2))). ^. During: resolving callee type: type(CPUDispatcher(<function arr_union at 0x7f4574e5ac10>)). During: typing of call at /home/gabriel/.local/lib/python3.9/site-packages/pynndescent/sparse.py (344). During: resolving callee type: type(CPUDispatcher(<function arr_union at 0x7f4574e5ac10>)). During: typing of call at /home/gabriel/.local/lib/python3.9/site-packages/pynndescent/sparse.py (344). File ""../../../../.local/lib/python3.9/site-packages/pynndescent/sparse.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:10471,energy efficiency,CPU,CPUDispatcher,10471,"quicksort_impl.<locals>.run_quicksort at 0x7f455d24c280>). During: typing of call at /home/gabriel/.local/lib/python3.9/site-packages/numba/np/arrayobj.py (5007). File ""../../../../.local/lib/python3.9/site-packages/numba/np/arrayobj.py"", line 5007:. def array_sort_impl(arr):. <source elided>. # Note we clobber the return value. sort_func(arr). ^. During: lowering ""$14call_method.5 = call $12load_method.4(func=$12load_method.4, args=[], kws=(), vararg=None)"" at /home/gabriel/.local/lib/python3.9/site-packages/numba/np/arrayobj.py (5017). During: lowering ""$8call_method.3 = call $4load_method.1(arr, func=$4load_method.1, args=[Var(arr, sparse.py:28)], kws=(), vararg=None)"" at /home/gabriel/.local/lib/python3.9/site-packages/pynndescent/sparse.py (28). During: resolving callee type: type(CPUDispatcher(<function arr_unique at 0x7f4574e5aa60>)). During: typing of call at /home/gabriel/.local/lib/python3.9/site-packages/pynndescent/sparse.py (41). During: resolving callee type: type(CPUDispatcher(<function arr_unique at 0x7f4574e5aa60>)). During: typing of call at /home/gabriel/.local/lib/python3.9/site-packages/pynndescent/sparse.py (41). File ""../../../../.local/lib/python3.9/site-packages/pynndescent/sparse.py"", line 41:. def arr_union(ar1, ar2):. <source elided>. else:. return arr_unique(np.concatenate((ar1, ar2))). ^. During: resolving callee type: type(CPUDispatcher(<function arr_union at 0x7f4574e5ac10>)). During: typing of call at /home/gabriel/.local/lib/python3.9/site-packages/pynndescent/sparse.py (344). During: resolving callee type: type(CPUDispatcher(<function arr_union at 0x7f4574e5ac10>)). During: typing of call at /home/gabriel/.local/lib/python3.9/site-packages/pynndescent/sparse.py (344). File ""../../../../.local/lib/python3.9/site-packages/pynndescent/sparse.py"", line 344:. def sparse_alternative_jaccard(ind1, data1, ind2, data2):. num_non_zero = arr_union(ind1, ind2).shape[0]. ^. ```. #### Versions. <details>. anndata 0.7.5. scanpy 1.7.0. sinfo 0.3.1",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:10854,energy efficiency,CPU,CPUDispatcher,10854,"5 = call $12load_method.4(func=$12load_method.4, args=[], kws=(), vararg=None)"" at /home/gabriel/.local/lib/python3.9/site-packages/numba/np/arrayobj.py (5017). During: lowering ""$8call_method.3 = call $4load_method.1(arr, func=$4load_method.1, args=[Var(arr, sparse.py:28)], kws=(), vararg=None)"" at /home/gabriel/.local/lib/python3.9/site-packages/pynndescent/sparse.py (28). During: resolving callee type: type(CPUDispatcher(<function arr_unique at 0x7f4574e5aa60>)). During: typing of call at /home/gabriel/.local/lib/python3.9/site-packages/pynndescent/sparse.py (41). During: resolving callee type: type(CPUDispatcher(<function arr_unique at 0x7f4574e5aa60>)). During: typing of call at /home/gabriel/.local/lib/python3.9/site-packages/pynndescent/sparse.py (41). File ""../../../../.local/lib/python3.9/site-packages/pynndescent/sparse.py"", line 41:. def arr_union(ar1, ar2):. <source elided>. else:. return arr_unique(np.concatenate((ar1, ar2))). ^. During: resolving callee type: type(CPUDispatcher(<function arr_union at 0x7f4574e5ac10>)). During: typing of call at /home/gabriel/.local/lib/python3.9/site-packages/pynndescent/sparse.py (344). During: resolving callee type: type(CPUDispatcher(<function arr_union at 0x7f4574e5ac10>)). During: typing of call at /home/gabriel/.local/lib/python3.9/site-packages/pynndescent/sparse.py (344). File ""../../../../.local/lib/python3.9/site-packages/pynndescent/sparse.py"", line 344:. def sparse_alternative_jaccard(ind1, data1, ind2, data2):. num_non_zero = arr_union(ind1, ind2).shape[0]. ^. ```. #### Versions. <details>. anndata 0.7.5. scanpy 1.7.0. sinfo 0.3.1. -----. PIL 8.1.0. PyQt5 NA. anndata 0.7.5. appdirs 1.4.4. cffi 1.14.4. colorama 0.4.4. constants NA. cycler 0.10.0. cython_runtime NA. dateutil 2.8.1. get_version 2.1. google NA. h5py 3.1.0. highs_wrapper NA. joblib 1.0.1. kiwisolver 1.3.1. legacy_api_wrap 1.2. llvmlite 0.34.0. matplotlib 3.3.4. mpl_toolkits NA. natsort 7.1.1. numba 0.51.2. numexpr 2.7.2. numpy 1.20.0. packaging",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:11050,energy efficiency,CPU,CPUDispatcher,11050," call $4load_method.1(arr, func=$4load_method.1, args=[Var(arr, sparse.py:28)], kws=(), vararg=None)"" at /home/gabriel/.local/lib/python3.9/site-packages/pynndescent/sparse.py (28). During: resolving callee type: type(CPUDispatcher(<function arr_unique at 0x7f4574e5aa60>)). During: typing of call at /home/gabriel/.local/lib/python3.9/site-packages/pynndescent/sparse.py (41). During: resolving callee type: type(CPUDispatcher(<function arr_unique at 0x7f4574e5aa60>)). During: typing of call at /home/gabriel/.local/lib/python3.9/site-packages/pynndescent/sparse.py (41). File ""../../../../.local/lib/python3.9/site-packages/pynndescent/sparse.py"", line 41:. def arr_union(ar1, ar2):. <source elided>. else:. return arr_unique(np.concatenate((ar1, ar2))). ^. During: resolving callee type: type(CPUDispatcher(<function arr_union at 0x7f4574e5ac10>)). During: typing of call at /home/gabriel/.local/lib/python3.9/site-packages/pynndescent/sparse.py (344). During: resolving callee type: type(CPUDispatcher(<function arr_union at 0x7f4574e5ac10>)). During: typing of call at /home/gabriel/.local/lib/python3.9/site-packages/pynndescent/sparse.py (344). File ""../../../../.local/lib/python3.9/site-packages/pynndescent/sparse.py"", line 344:. def sparse_alternative_jaccard(ind1, data1, ind2, data2):. num_non_zero = arr_union(ind1, ind2).shape[0]. ^. ```. #### Versions. <details>. anndata 0.7.5. scanpy 1.7.0. sinfo 0.3.1. -----. PIL 8.1.0. PyQt5 NA. anndata 0.7.5. appdirs 1.4.4. cffi 1.14.4. colorama 0.4.4. constants NA. cycler 0.10.0. cython_runtime NA. dateutil 2.8.1. get_version 2.1. google NA. h5py 3.1.0. highs_wrapper NA. joblib 1.0.1. kiwisolver 1.3.1. legacy_api_wrap 1.2. llvmlite 0.34.0. matplotlib 3.3.4. mpl_toolkits NA. natsort 7.1.1. numba 0.51.2. numexpr 2.7.2. numpy 1.20.0. packaging 20.8. pandas 1.2.2. pkg_resources NA. psutil 5.8.0. pyexpat NA. pyparsing 2.4.7. pytoml NA. pytz 2021.1. scanpy 1.7.0. scipy 1.6.0. setuptools_scm NA. sinfo 0.3.1. sip NA. six 1.15.0. sklearn 0.2",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:37,integrability,pipelin,pipeline,37,"TypingError: Failed in nopython mode pipeline (step: nopython frontend); Hi, I am working with a big dataset over 100000 cells and I run into a problem when computing the neigbours. Find below an small example using a random matrix. . Observations,. when I run N_cells = 8000 it works perfectly for gauss metric. when running N_cells = 10000, the neighbors computation crashes. The problem is even more dramatic for method=""umap"". . ### Minimal code sample (that we can copy&paste without having any data). ```python. import scanpy. import numpy. matrix = numpy.random.uniform(size=[10000,1000]). adata = scanpy.AnnData(matrix). scanpy.pp.pca(adata,n_comps=10). scanpy.pp.neighbors(adata,method=""gauss"",n_pcs=10). ```. ```pytb. ---------------------------------------------------------------------------. TypingError Traceback (most recent call last). <ipython-input-3-692f9d70cb0c> in <module>. 9 . 10 scanpy.pp.pca(adata,n_comps=10). ---> 11 scanpy.pp.neighbors(adata,method=""gauss"",n_pcs=10). ~/.local/lib/python3.9/site-packages/scanpy/neighbors/__init__.py in neighbors(adata, n_neighbors, n_pcs, use_rep, knn, random_state, method, metric, metric_kwds, key_added, copy). 118 adata._init_as_actual(adata.copy()). 119 neighbors = Neighbors(adata). --> 120 neighbors.compute_neighbors(. 121 n_neighbors=n_neighbors, knn=knn, n_pcs=n_pcs, use_rep=use_rep,. 122 method=method, metric=metric, metric_kwds=metric_kwds,. ~/.local/lib/python3.9/site-packages/scanpy/neighbors/__init__.py in compute_neighbors(self, n_neighbors, knn, n_pcs, use_rep, method, random_state, write_knn_indices, metric, metric_kwds). 732 X = pairwise_distances(X, metric=metric, **metric_kwds). 733 metric = 'precomputed'. --> 734 knn_indices, knn_distances, forest = compute_neighbors_umap(. 735 X, n_neighbors, random_state, metric=metric, metric_kwds=metric_kwds). 736 # very cautious here. ~/.local/lib/python3.9/site-packages/scanpy/neighbors/__init__.py in compute_neighbors_umap(X, n_neighbors, random_state, metric, m",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:2062,integrability,filter,filterwarnings,2062,"hbors(adata, n_neighbors, n_pcs, use_rep, knn, random_state, method, metric, metric_kwds, key_added, copy). 118 adata._init_as_actual(adata.copy()). 119 neighbors = Neighbors(adata). --> 120 neighbors.compute_neighbors(. 121 n_neighbors=n_neighbors, knn=knn, n_pcs=n_pcs, use_rep=use_rep,. 122 method=method, metric=metric, metric_kwds=metric_kwds,. ~/.local/lib/python3.9/site-packages/scanpy/neighbors/__init__.py in compute_neighbors(self, n_neighbors, knn, n_pcs, use_rep, method, random_state, write_knn_indices, metric, metric_kwds). 732 X = pairwise_distances(X, metric=metric, **metric_kwds). 733 metric = 'precomputed'. --> 734 knn_indices, knn_distances, forest = compute_neighbors_umap(. 735 X, n_neighbors, random_state, metric=metric, metric_kwds=metric_kwds). 736 # very cautious here. ~/.local/lib/python3.9/site-packages/scanpy/neighbors/__init__.py in compute_neighbors_umap(X, n_neighbors, random_state, metric, metric_kwds, angular, verbose). 273 # umap 0.5.0. 274 warnings.filterwarnings(""ignore"", message=r""Tensorflow not installed""). --> 275 from umap.umap_ import nearest_neighbors. 276 . 277 random_state = check_random_state(random_state). ~/.local/lib/python3.9/site-packages/umap/umap_.py in <module>. 45 ). 46 . ---> 47 from pynndescent import NNDescent. 48 from pynndescent.distances import named_distances as pynn_named_distances. 49 from pynndescent.sparse import sparse_named_distances as pynn_sparse_named_distances. ~/.local/lib/python3.9/site-packages/pynndescent/__init__.py in <module>. 1 import pkg_resources. 2 import numba. ----> 3 from .pynndescent_ import NNDescent, PyNNDescentTransformer. 4 . 5 # Workaround: https://github.com/numba/numba/issues/3341. ~/.local/lib/python3.9/site-packages/pynndescent/pynndescent_.py in <module>. 19 import heapq. 20 . ---> 21 import pynndescent.sparse as sparse. 22 import pynndescent.sparse_nndescent as sparse_nnd. 23 import pynndescent.distances as pynnd_dist. ~/.local/lib/python3.9/site-packages/pynndescent/sparse.p",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:2087,integrability,messag,message,2087,"rs, n_pcs, use_rep, knn, random_state, method, metric, metric_kwds, key_added, copy). 118 adata._init_as_actual(adata.copy()). 119 neighbors = Neighbors(adata). --> 120 neighbors.compute_neighbors(. 121 n_neighbors=n_neighbors, knn=knn, n_pcs=n_pcs, use_rep=use_rep,. 122 method=method, metric=metric, metric_kwds=metric_kwds,. ~/.local/lib/python3.9/site-packages/scanpy/neighbors/__init__.py in compute_neighbors(self, n_neighbors, knn, n_pcs, use_rep, method, random_state, write_knn_indices, metric, metric_kwds). 732 X = pairwise_distances(X, metric=metric, **metric_kwds). 733 metric = 'precomputed'. --> 734 knn_indices, knn_distances, forest = compute_neighbors_umap(. 735 X, n_neighbors, random_state, metric=metric, metric_kwds=metric_kwds). 736 # very cautious here. ~/.local/lib/python3.9/site-packages/scanpy/neighbors/__init__.py in compute_neighbors_umap(X, n_neighbors, random_state, metric, metric_kwds, angular, verbose). 273 # umap 0.5.0. 274 warnings.filterwarnings(""ignore"", message=r""Tensorflow not installed""). --> 275 from umap.umap_ import nearest_neighbors. 276 . 277 random_state = check_random_state(random_state). ~/.local/lib/python3.9/site-packages/umap/umap_.py in <module>. 45 ). 46 . ---> 47 from pynndescent import NNDescent. 48 from pynndescent.distances import named_distances as pynn_named_distances. 49 from pynndescent.sparse import sparse_named_distances as pynn_sparse_named_distances. ~/.local/lib/python3.9/site-packages/pynndescent/__init__.py in <module>. 1 import pkg_resources. 2 import numba. ----> 3 from .pynndescent_ import NNDescent, PyNNDescentTransformer. 4 . 5 # Workaround: https://github.com/numba/numba/issues/3341. ~/.local/lib/python3.9/site-packages/pynndescent/pynndescent_.py in <module>. 19 import heapq. 20 . ---> 21 import pynndescent.sparse as sparse. 22 import pynndescent.sparse_nndescent as sparse_nnd. 23 import pynndescent.distances as pynnd_dist. ~/.local/lib/python3.9/site-packages/pynndescent/sparse.py in <module>. 341 },.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:3334,integrability,wrap,wrapper,3334,"rt NNDescent. 48 from pynndescent.distances import named_distances as pynn_named_distances. 49 from pynndescent.sparse import sparse_named_distances as pynn_sparse_named_distances. ~/.local/lib/python3.9/site-packages/pynndescent/__init__.py in <module>. 1 import pkg_resources. 2 import numba. ----> 3 from .pynndescent_ import NNDescent, PyNNDescentTransformer. 4 . 5 # Workaround: https://github.com/numba/numba/issues/3341. ~/.local/lib/python3.9/site-packages/pynndescent/pynndescent_.py in <module>. 19 import heapq. 20 . ---> 21 import pynndescent.sparse as sparse. 22 import pynndescent.sparse_nndescent as sparse_nnd. 23 import pynndescent.distances as pynnd_dist. ~/.local/lib/python3.9/site-packages/pynndescent/sparse.py in <module>. 341 },. 342 ). --> 343 def sparse_alternative_jaccard(ind1, data1, ind2, data2):. 344 num_non_zero = arr_union(ind1, ind2).shape[0]. 345 num_equal = arr_intersect(ind1, ind2).shape[0]. ~/.local/lib/python3.9/site-packages/numba/core/decorators.py in wrapper(func). 216 with typeinfer.register_dispatcher(disp):. 217 for sig in sigs:. --> 218 disp.compile(sig). 219 disp.disable_compile(). 220 return disp. ~/.local/lib/python3.9/site-packages/numba/core/compiler_lock.py in _acquire_compile_lock(*args, **kwargs). 30 def _acquire_compile_lock(*args, **kwargs):. 31 with self:. ---> 32 return func(*args, **kwargs). 33 return _acquire_compile_lock. 34 . ~/.local/lib/python3.9/site-packages/numba/core/dispatcher.py in compile(self, sig). 817 self._cache_misses[sig] += 1. 818 try:. --> 819 cres = self._compiler.compile(args, return_type). 820 except errors.ForceLiteralArg as e:. 821 def folded(args, kws):. ~/.local/lib/python3.9/site-packages/numba/core/dispatcher.py in compile(self, args, return_type). 80 return retval. 81 else:. ---> 82 raise retval. 83 . 84 def _compile_cached(self, args, return_type):. ~/.local/lib/python3.9/site-packages/numba/core/dispatcher.py in _compile_cached(self, args, return_type). 90 . 91 try:. ---> 92 retval = sel",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:4890,integrability,pipelin,pipeline,4890,"r.compile(args, return_type). 820 except errors.ForceLiteralArg as e:. 821 def folded(args, kws):. ~/.local/lib/python3.9/site-packages/numba/core/dispatcher.py in compile(self, args, return_type). 80 return retval. 81 else:. ---> 82 raise retval. 83 . 84 def _compile_cached(self, args, return_type):. ~/.local/lib/python3.9/site-packages/numba/core/dispatcher.py in _compile_cached(self, args, return_type). 90 . 91 try:. ---> 92 retval = self._compile_core(args, return_type). 93 except errors.TypingError as e:. 94 self._failed_cache[key] = e. ~/.local/lib/python3.9/site-packages/numba/core/dispatcher.py in _compile_core(self, args, return_type). 103 . 104 impl = self._get_implementation(args, {}). --> 105 cres = compiler.compile_extra(self.targetdescr.typing_context,. 106 self.targetdescr.target_context,. 107 impl,. ~/.local/lib/python3.9/site-packages/numba/core/compiler.py in compile_extra(typingctx, targetctx, func, args, return_type, flags, locals, library, pipeline_class). 625 pipeline = pipeline_class(typingctx, targetctx, library,. 626 args, return_type, flags, locals). --> 627 return pipeline.compile_extra(func). 628 . 629 . ~/.local/lib/python3.9/site-packages/numba/core/compiler.py in compile_extra(self, func). 361 self.state.lifted = (). 362 self.state.lifted_from = None. --> 363 return self._compile_bytecode(). 364 . 365 def compile_ir(self, func_ir, lifted=(), lifted_from=None):. ~/.local/lib/python3.9/site-packages/numba/core/compiler.py in _compile_bytecode(self). 423 """""". 424 assert self.state.func_ir is None. --> 425 return self._compile_core(). 426 . 427 def _compile_ir(self):. ~/.local/lib/python3.9/site-packages/numba/core/compiler.py in _compile_core(self). 403 self.state.status.fail_reason = e. 404 if is_final_pipeline:. --> 405 raise e. 406 else:. 407 raise CompilerError(""All available pipelines exhausted""). ~/.local/lib/python3.9/site-packages/numba/core/compiler.py in _compile_core(self). 394 res = None. 395 try:. --> 396 pm.run(self.state). ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:5002,integrability,pipelin,pipeline,5002,"python3.9/site-packages/numba/core/dispatcher.py in compile(self, args, return_type). 80 return retval. 81 else:. ---> 82 raise retval. 83 . 84 def _compile_cached(self, args, return_type):. ~/.local/lib/python3.9/site-packages/numba/core/dispatcher.py in _compile_cached(self, args, return_type). 90 . 91 try:. ---> 92 retval = self._compile_core(args, return_type). 93 except errors.TypingError as e:. 94 self._failed_cache[key] = e. ~/.local/lib/python3.9/site-packages/numba/core/dispatcher.py in _compile_core(self, args, return_type). 103 . 104 impl = self._get_implementation(args, {}). --> 105 cres = compiler.compile_extra(self.targetdescr.typing_context,. 106 self.targetdescr.target_context,. 107 impl,. ~/.local/lib/python3.9/site-packages/numba/core/compiler.py in compile_extra(typingctx, targetctx, func, args, return_type, flags, locals, library, pipeline_class). 625 pipeline = pipeline_class(typingctx, targetctx, library,. 626 args, return_type, flags, locals). --> 627 return pipeline.compile_extra(func). 628 . 629 . ~/.local/lib/python3.9/site-packages/numba/core/compiler.py in compile_extra(self, func). 361 self.state.lifted = (). 362 self.state.lifted_from = None. --> 363 return self._compile_bytecode(). 364 . 365 def compile_ir(self, func_ir, lifted=(), lifted_from=None):. ~/.local/lib/python3.9/site-packages/numba/core/compiler.py in _compile_bytecode(self). 423 """""". 424 assert self.state.func_ir is None. --> 425 return self._compile_core(). 426 . 427 def _compile_ir(self):. ~/.local/lib/python3.9/site-packages/numba/core/compiler.py in _compile_core(self). 403 self.state.status.fail_reason = e. 404 if is_final_pipeline:. --> 405 raise e. 406 else:. 407 raise CompilerError(""All available pipelines exhausted""). ~/.local/lib/python3.9/site-packages/numba/core/compiler.py in _compile_core(self). 394 res = None. 395 try:. --> 396 pm.run(self.state). 397 if self.state.cr is not None:. 398 break. ~/.local/lib/python3.9/site-packages/numba/core/compiler_machinery",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:5143,integrability,state,state,5143," 84 def _compile_cached(self, args, return_type):. ~/.local/lib/python3.9/site-packages/numba/core/dispatcher.py in _compile_cached(self, args, return_type). 90 . 91 try:. ---> 92 retval = self._compile_core(args, return_type). 93 except errors.TypingError as e:. 94 self._failed_cache[key] = e. ~/.local/lib/python3.9/site-packages/numba/core/dispatcher.py in _compile_core(self, args, return_type). 103 . 104 impl = self._get_implementation(args, {}). --> 105 cres = compiler.compile_extra(self.targetdescr.typing_context,. 106 self.targetdescr.target_context,. 107 impl,. ~/.local/lib/python3.9/site-packages/numba/core/compiler.py in compile_extra(typingctx, targetctx, func, args, return_type, flags, locals, library, pipeline_class). 625 pipeline = pipeline_class(typingctx, targetctx, library,. 626 args, return_type, flags, locals). --> 627 return pipeline.compile_extra(func). 628 . 629 . ~/.local/lib/python3.9/site-packages/numba/core/compiler.py in compile_extra(self, func). 361 self.state.lifted = (). 362 self.state.lifted_from = None. --> 363 return self._compile_bytecode(). 364 . 365 def compile_ir(self, func_ir, lifted=(), lifted_from=None):. ~/.local/lib/python3.9/site-packages/numba/core/compiler.py in _compile_bytecode(self). 423 """""". 424 assert self.state.func_ir is None. --> 425 return self._compile_core(). 426 . 427 def _compile_ir(self):. ~/.local/lib/python3.9/site-packages/numba/core/compiler.py in _compile_core(self). 403 self.state.status.fail_reason = e. 404 if is_final_pipeline:. --> 405 raise e. 406 else:. 407 raise CompilerError(""All available pipelines exhausted""). ~/.local/lib/python3.9/site-packages/numba/core/compiler.py in _compile_core(self). 394 res = None. 395 try:. --> 396 pm.run(self.state). 397 if self.state.cr is not None:. 398 break. ~/.local/lib/python3.9/site-packages/numba/core/compiler_machinery.py in run(self, state). 339 (self.pipeline_name, pass_desc). 340 patched_exception = self._patch_error(msg, e). --> 341 raise patched_excep",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:5171,integrability,state,state,5171,", args, return_type):. ~/.local/lib/python3.9/site-packages/numba/core/dispatcher.py in _compile_cached(self, args, return_type). 90 . 91 try:. ---> 92 retval = self._compile_core(args, return_type). 93 except errors.TypingError as e:. 94 self._failed_cache[key] = e. ~/.local/lib/python3.9/site-packages/numba/core/dispatcher.py in _compile_core(self, args, return_type). 103 . 104 impl = self._get_implementation(args, {}). --> 105 cres = compiler.compile_extra(self.targetdescr.typing_context,. 106 self.targetdescr.target_context,. 107 impl,. ~/.local/lib/python3.9/site-packages/numba/core/compiler.py in compile_extra(typingctx, targetctx, func, args, return_type, flags, locals, library, pipeline_class). 625 pipeline = pipeline_class(typingctx, targetctx, library,. 626 args, return_type, flags, locals). --> 627 return pipeline.compile_extra(func). 628 . 629 . ~/.local/lib/python3.9/site-packages/numba/core/compiler.py in compile_extra(self, func). 361 self.state.lifted = (). 362 self.state.lifted_from = None. --> 363 return self._compile_bytecode(). 364 . 365 def compile_ir(self, func_ir, lifted=(), lifted_from=None):. ~/.local/lib/python3.9/site-packages/numba/core/compiler.py in _compile_bytecode(self). 423 """""". 424 assert self.state.func_ir is None. --> 425 return self._compile_core(). 426 . 427 def _compile_ir(self):. ~/.local/lib/python3.9/site-packages/numba/core/compiler.py in _compile_core(self). 403 self.state.status.fail_reason = e. 404 if is_final_pipeline:. --> 405 raise e. 406 else:. 407 raise CompilerError(""All available pipelines exhausted""). ~/.local/lib/python3.9/site-packages/numba/core/compiler.py in _compile_core(self). 394 res = None. 395 try:. --> 396 pm.run(self.state). 397 if self.state.cr is not None:. 398 break. ~/.local/lib/python3.9/site-packages/numba/core/compiler_machinery.py in run(self, state). 339 (self.pipeline_name, pass_desc). 340 patched_exception = self._patch_error(msg, e). --> 341 raise patched_exception. 342 . 343 def dependen",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:5422,integrability,state,state,5422,"_cache[key] = e. ~/.local/lib/python3.9/site-packages/numba/core/dispatcher.py in _compile_core(self, args, return_type). 103 . 104 impl = self._get_implementation(args, {}). --> 105 cres = compiler.compile_extra(self.targetdescr.typing_context,. 106 self.targetdescr.target_context,. 107 impl,. ~/.local/lib/python3.9/site-packages/numba/core/compiler.py in compile_extra(typingctx, targetctx, func, args, return_type, flags, locals, library, pipeline_class). 625 pipeline = pipeline_class(typingctx, targetctx, library,. 626 args, return_type, flags, locals). --> 627 return pipeline.compile_extra(func). 628 . 629 . ~/.local/lib/python3.9/site-packages/numba/core/compiler.py in compile_extra(self, func). 361 self.state.lifted = (). 362 self.state.lifted_from = None. --> 363 return self._compile_bytecode(). 364 . 365 def compile_ir(self, func_ir, lifted=(), lifted_from=None):. ~/.local/lib/python3.9/site-packages/numba/core/compiler.py in _compile_bytecode(self). 423 """""". 424 assert self.state.func_ir is None. --> 425 return self._compile_core(). 426 . 427 def _compile_ir(self):. ~/.local/lib/python3.9/site-packages/numba/core/compiler.py in _compile_core(self). 403 self.state.status.fail_reason = e. 404 if is_final_pipeline:. --> 405 raise e. 406 else:. 407 raise CompilerError(""All available pipelines exhausted""). ~/.local/lib/python3.9/site-packages/numba/core/compiler.py in _compile_core(self). 394 res = None. 395 try:. --> 396 pm.run(self.state). 397 if self.state.cr is not None:. 398 break. ~/.local/lib/python3.9/site-packages/numba/core/compiler_machinery.py in run(self, state). 339 (self.pipeline_name, pass_desc). 340 patched_exception = self._patch_error(msg, e). --> 341 raise patched_exception. 342 . 343 def dependency_analysis(self):. ~/.local/lib/python3.9/site-packages/numba/core/compiler_machinery.py in run(self, state). 330 pass_inst = _pass_registry.get(pss).pass_inst. 331 if isinstance(pass_inst, CompilerPass):. --> 332 self._runPass(idx, pass_inst, state)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:5609,integrability,state,state,5609," = compiler.compile_extra(self.targetdescr.typing_context,. 106 self.targetdescr.target_context,. 107 impl,. ~/.local/lib/python3.9/site-packages/numba/core/compiler.py in compile_extra(typingctx, targetctx, func, args, return_type, flags, locals, library, pipeline_class). 625 pipeline = pipeline_class(typingctx, targetctx, library,. 626 args, return_type, flags, locals). --> 627 return pipeline.compile_extra(func). 628 . 629 . ~/.local/lib/python3.9/site-packages/numba/core/compiler.py in compile_extra(self, func). 361 self.state.lifted = (). 362 self.state.lifted_from = None. --> 363 return self._compile_bytecode(). 364 . 365 def compile_ir(self, func_ir, lifted=(), lifted_from=None):. ~/.local/lib/python3.9/site-packages/numba/core/compiler.py in _compile_bytecode(self). 423 """""". 424 assert self.state.func_ir is None. --> 425 return self._compile_core(). 426 . 427 def _compile_ir(self):. ~/.local/lib/python3.9/site-packages/numba/core/compiler.py in _compile_core(self). 403 self.state.status.fail_reason = e. 404 if is_final_pipeline:. --> 405 raise e. 406 else:. 407 raise CompilerError(""All available pipelines exhausted""). ~/.local/lib/python3.9/site-packages/numba/core/compiler.py in _compile_core(self). 394 res = None. 395 try:. --> 396 pm.run(self.state). 397 if self.state.cr is not None:. 398 break. ~/.local/lib/python3.9/site-packages/numba/core/compiler_machinery.py in run(self, state). 339 (self.pipeline_name, pass_desc). 340 patched_exception = self._patch_error(msg, e). --> 341 raise patched_exception. 342 . 343 def dependency_analysis(self):. ~/.local/lib/python3.9/site-packages/numba/core/compiler_machinery.py in run(self, state). 330 pass_inst = _pass_registry.get(pss).pass_inst. 331 if isinstance(pass_inst, CompilerPass):. --> 332 self._runPass(idx, pass_inst, state). 333 else:. 334 raise BaseException(""Legacy pass in use""). ~/.local/lib/python3.9/site-packages/numba/core/compiler_lock.py in _acquire_compile_lock(*args, **kwargs). 30 def _acquire_com",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:5733,integrability,pipelin,pipelines,5733,"on3.9/site-packages/numba/core/compiler.py in compile_extra(typingctx, targetctx, func, args, return_type, flags, locals, library, pipeline_class). 625 pipeline = pipeline_class(typingctx, targetctx, library,. 626 args, return_type, flags, locals). --> 627 return pipeline.compile_extra(func). 628 . 629 . ~/.local/lib/python3.9/site-packages/numba/core/compiler.py in compile_extra(self, func). 361 self.state.lifted = (). 362 self.state.lifted_from = None. --> 363 return self._compile_bytecode(). 364 . 365 def compile_ir(self, func_ir, lifted=(), lifted_from=None):. ~/.local/lib/python3.9/site-packages/numba/core/compiler.py in _compile_bytecode(self). 423 """""". 424 assert self.state.func_ir is None. --> 425 return self._compile_core(). 426 . 427 def _compile_ir(self):. ~/.local/lib/python3.9/site-packages/numba/core/compiler.py in _compile_core(self). 403 self.state.status.fail_reason = e. 404 if is_final_pipeline:. --> 405 raise e. 406 else:. 407 raise CompilerError(""All available pipelines exhausted""). ~/.local/lib/python3.9/site-packages/numba/core/compiler.py in _compile_core(self). 394 res = None. 395 try:. --> 396 pm.run(self.state). 397 if self.state.cr is not None:. 398 break. ~/.local/lib/python3.9/site-packages/numba/core/compiler_machinery.py in run(self, state). 339 (self.pipeline_name, pass_desc). 340 patched_exception = self._patch_error(msg, e). --> 341 raise patched_exception. 342 . 343 def dependency_analysis(self):. ~/.local/lib/python3.9/site-packages/numba/core/compiler_machinery.py in run(self, state). 330 pass_inst = _pass_registry.get(pss).pass_inst. 331 if isinstance(pass_inst, CompilerPass):. --> 332 self._runPass(idx, pass_inst, state). 333 else:. 334 raise BaseException(""Legacy pass in use""). ~/.local/lib/python3.9/site-packages/numba/core/compiler_lock.py in _acquire_compile_lock(*args, **kwargs). 30 def _acquire_compile_lock(*args, **kwargs):. 31 with self:. ---> 32 return func(*args, **kwargs). 33 return _acquire_compile_lock. 34 . ~/.loc",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:5886,integrability,state,state,5886," pipeline = pipeline_class(typingctx, targetctx, library,. 626 args, return_type, flags, locals). --> 627 return pipeline.compile_extra(func). 628 . 629 . ~/.local/lib/python3.9/site-packages/numba/core/compiler.py in compile_extra(self, func). 361 self.state.lifted = (). 362 self.state.lifted_from = None. --> 363 return self._compile_bytecode(). 364 . 365 def compile_ir(self, func_ir, lifted=(), lifted_from=None):. ~/.local/lib/python3.9/site-packages/numba/core/compiler.py in _compile_bytecode(self). 423 """""". 424 assert self.state.func_ir is None. --> 425 return self._compile_core(). 426 . 427 def _compile_ir(self):. ~/.local/lib/python3.9/site-packages/numba/core/compiler.py in _compile_core(self). 403 self.state.status.fail_reason = e. 404 if is_final_pipeline:. --> 405 raise e. 406 else:. 407 raise CompilerError(""All available pipelines exhausted""). ~/.local/lib/python3.9/site-packages/numba/core/compiler.py in _compile_core(self). 394 res = None. 395 try:. --> 396 pm.run(self.state). 397 if self.state.cr is not None:. 398 break. ~/.local/lib/python3.9/site-packages/numba/core/compiler_machinery.py in run(self, state). 339 (self.pipeline_name, pass_desc). 340 patched_exception = self._patch_error(msg, e). --> 341 raise patched_exception. 342 . 343 def dependency_analysis(self):. ~/.local/lib/python3.9/site-packages/numba/core/compiler_machinery.py in run(self, state). 330 pass_inst = _pass_registry.get(pss).pass_inst. 331 if isinstance(pass_inst, CompilerPass):. --> 332 self._runPass(idx, pass_inst, state). 333 else:. 334 raise BaseException(""Legacy pass in use""). ~/.local/lib/python3.9/site-packages/numba/core/compiler_lock.py in _acquire_compile_lock(*args, **kwargs). 30 def _acquire_compile_lock(*args, **kwargs):. 31 with self:. ---> 32 return func(*args, **kwargs). 33 return _acquire_compile_lock. 34 . ~/.local/lib/python3.9/site-packages/numba/core/compiler_machinery.py in _runPass(self, index, pss, internal_state). 289 mutated |= check(pss.run_initializat",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:5906,integrability,state,state,5906,"_class(typingctx, targetctx, library,. 626 args, return_type, flags, locals). --> 627 return pipeline.compile_extra(func). 628 . 629 . ~/.local/lib/python3.9/site-packages/numba/core/compiler.py in compile_extra(self, func). 361 self.state.lifted = (). 362 self.state.lifted_from = None. --> 363 return self._compile_bytecode(). 364 . 365 def compile_ir(self, func_ir, lifted=(), lifted_from=None):. ~/.local/lib/python3.9/site-packages/numba/core/compiler.py in _compile_bytecode(self). 423 """""". 424 assert self.state.func_ir is None. --> 425 return self._compile_core(). 426 . 427 def _compile_ir(self):. ~/.local/lib/python3.9/site-packages/numba/core/compiler.py in _compile_core(self). 403 self.state.status.fail_reason = e. 404 if is_final_pipeline:. --> 405 raise e. 406 else:. 407 raise CompilerError(""All available pipelines exhausted""). ~/.local/lib/python3.9/site-packages/numba/core/compiler.py in _compile_core(self). 394 res = None. 395 try:. --> 396 pm.run(self.state). 397 if self.state.cr is not None:. 398 break. ~/.local/lib/python3.9/site-packages/numba/core/compiler_machinery.py in run(self, state). 339 (self.pipeline_name, pass_desc). 340 patched_exception = self._patch_error(msg, e). --> 341 raise patched_exception. 342 . 343 def dependency_analysis(self):. ~/.local/lib/python3.9/site-packages/numba/core/compiler_machinery.py in run(self, state). 330 pass_inst = _pass_registry.get(pss).pass_inst. 331 if isinstance(pass_inst, CompilerPass):. --> 332 self._runPass(idx, pass_inst, state). 333 else:. 334 raise BaseException(""Legacy pass in use""). ~/.local/lib/python3.9/site-packages/numba/core/compiler_lock.py in _acquire_compile_lock(*args, **kwargs). 30 def _acquire_compile_lock(*args, **kwargs):. 31 with self:. ---> 32 return func(*args, **kwargs). 33 return _acquire_compile_lock. 34 . ~/.local/lib/python3.9/site-packages/numba/core/compiler_machinery.py in _runPass(self, index, pss, internal_state). 289 mutated |= check(pss.run_initialization, internal_state)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:6023,integrability,state,state,6023,"unc). 628 . 629 . ~/.local/lib/python3.9/site-packages/numba/core/compiler.py in compile_extra(self, func). 361 self.state.lifted = (). 362 self.state.lifted_from = None. --> 363 return self._compile_bytecode(). 364 . 365 def compile_ir(self, func_ir, lifted=(), lifted_from=None):. ~/.local/lib/python3.9/site-packages/numba/core/compiler.py in _compile_bytecode(self). 423 """""". 424 assert self.state.func_ir is None. --> 425 return self._compile_core(). 426 . 427 def _compile_ir(self):. ~/.local/lib/python3.9/site-packages/numba/core/compiler.py in _compile_core(self). 403 self.state.status.fail_reason = e. 404 if is_final_pipeline:. --> 405 raise e. 406 else:. 407 raise CompilerError(""All available pipelines exhausted""). ~/.local/lib/python3.9/site-packages/numba/core/compiler.py in _compile_core(self). 394 res = None. 395 try:. --> 396 pm.run(self.state). 397 if self.state.cr is not None:. 398 break. ~/.local/lib/python3.9/site-packages/numba/core/compiler_machinery.py in run(self, state). 339 (self.pipeline_name, pass_desc). 340 patched_exception = self._patch_error(msg, e). --> 341 raise patched_exception. 342 . 343 def dependency_analysis(self):. ~/.local/lib/python3.9/site-packages/numba/core/compiler_machinery.py in run(self, state). 330 pass_inst = _pass_registry.get(pss).pass_inst. 331 if isinstance(pass_inst, CompilerPass):. --> 332 self._runPass(idx, pass_inst, state). 333 else:. 334 raise BaseException(""Legacy pass in use""). ~/.local/lib/python3.9/site-packages/numba/core/compiler_lock.py in _acquire_compile_lock(*args, **kwargs). 30 def _acquire_compile_lock(*args, **kwargs):. 31 with self:. ---> 32 return func(*args, **kwargs). 33 return _acquire_compile_lock. 34 . ~/.local/lib/python3.9/site-packages/numba/core/compiler_machinery.py in _runPass(self, index, pss, internal_state). 289 mutated |= check(pss.run_initialization, internal_state). 290 with SimpleTimer() as pass_time:. --> 291 mutated |= check(pss.run_pass, internal_state). 292 with SimpleTimer(",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:6277,integrability,state,state,6277,"fted=(), lifted_from=None):. ~/.local/lib/python3.9/site-packages/numba/core/compiler.py in _compile_bytecode(self). 423 """""". 424 assert self.state.func_ir is None. --> 425 return self._compile_core(). 426 . 427 def _compile_ir(self):. ~/.local/lib/python3.9/site-packages/numba/core/compiler.py in _compile_core(self). 403 self.state.status.fail_reason = e. 404 if is_final_pipeline:. --> 405 raise e. 406 else:. 407 raise CompilerError(""All available pipelines exhausted""). ~/.local/lib/python3.9/site-packages/numba/core/compiler.py in _compile_core(self). 394 res = None. 395 try:. --> 396 pm.run(self.state). 397 if self.state.cr is not None:. 398 break. ~/.local/lib/python3.9/site-packages/numba/core/compiler_machinery.py in run(self, state). 339 (self.pipeline_name, pass_desc). 340 patched_exception = self._patch_error(msg, e). --> 341 raise patched_exception. 342 . 343 def dependency_analysis(self):. ~/.local/lib/python3.9/site-packages/numba/core/compiler_machinery.py in run(self, state). 330 pass_inst = _pass_registry.get(pss).pass_inst. 331 if isinstance(pass_inst, CompilerPass):. --> 332 self._runPass(idx, pass_inst, state). 333 else:. 334 raise BaseException(""Legacy pass in use""). ~/.local/lib/python3.9/site-packages/numba/core/compiler_lock.py in _acquire_compile_lock(*args, **kwargs). 30 def _acquire_compile_lock(*args, **kwargs):. 31 with self:. ---> 32 return func(*args, **kwargs). 33 return _acquire_compile_lock. 34 . ~/.local/lib/python3.9/site-packages/numba/core/compiler_machinery.py in _runPass(self, index, pss, internal_state). 289 mutated |= check(pss.run_initialization, internal_state). 290 with SimpleTimer() as pass_time:. --> 291 mutated |= check(pss.run_pass, internal_state). 292 with SimpleTimer() as finalize_time:. 293 mutated |= check(pss.run_finalizer, internal_state). ~/.local/lib/python3.9/site-packages/numba/core/compiler_machinery.py in check(func, compiler_state). 262 . 263 def check(func, compiler_state):. --> 264 mangled = func(compile",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:6419,integrability,state,state,6419,"state.func_ir is None. --> 425 return self._compile_core(). 426 . 427 def _compile_ir(self):. ~/.local/lib/python3.9/site-packages/numba/core/compiler.py in _compile_core(self). 403 self.state.status.fail_reason = e. 404 if is_final_pipeline:. --> 405 raise e. 406 else:. 407 raise CompilerError(""All available pipelines exhausted""). ~/.local/lib/python3.9/site-packages/numba/core/compiler.py in _compile_core(self). 394 res = None. 395 try:. --> 396 pm.run(self.state). 397 if self.state.cr is not None:. 398 break. ~/.local/lib/python3.9/site-packages/numba/core/compiler_machinery.py in run(self, state). 339 (self.pipeline_name, pass_desc). 340 patched_exception = self._patch_error(msg, e). --> 341 raise patched_exception. 342 . 343 def dependency_analysis(self):. ~/.local/lib/python3.9/site-packages/numba/core/compiler_machinery.py in run(self, state). 330 pass_inst = _pass_registry.get(pss).pass_inst. 331 if isinstance(pass_inst, CompilerPass):. --> 332 self._runPass(idx, pass_inst, state). 333 else:. 334 raise BaseException(""Legacy pass in use""). ~/.local/lib/python3.9/site-packages/numba/core/compiler_lock.py in _acquire_compile_lock(*args, **kwargs). 30 def _acquire_compile_lock(*args, **kwargs):. 31 with self:. ---> 32 return func(*args, **kwargs). 33 return _acquire_compile_lock. 34 . ~/.local/lib/python3.9/site-packages/numba/core/compiler_machinery.py in _runPass(self, index, pss, internal_state). 289 mutated |= check(pss.run_initialization, internal_state). 290 with SimpleTimer() as pass_time:. --> 291 mutated |= check(pss.run_pass, internal_state). 292 with SimpleTimer() as finalize_time:. 293 mutated |= check(pss.run_finalizer, internal_state). ~/.local/lib/python3.9/site-packages/numba/core/compiler_machinery.py in check(func, compiler_state). 262 . 263 def check(func, compiler_state):. --> 264 mangled = func(compiler_state). 265 if mangled not in (True, False):. 266 msg = (""CompilerPass implementations should return True/False. "". ~/.local/lib/python3.9/s",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:7480,integrability,state,state,7480,". ~/.local/lib/python3.9/site-packages/numba/core/compiler_lock.py in _acquire_compile_lock(*args, **kwargs). 30 def _acquire_compile_lock(*args, **kwargs):. 31 with self:. ---> 32 return func(*args, **kwargs). 33 return _acquire_compile_lock. 34 . ~/.local/lib/python3.9/site-packages/numba/core/compiler_machinery.py in _runPass(self, index, pss, internal_state). 289 mutated |= check(pss.run_initialization, internal_state). 290 with SimpleTimer() as pass_time:. --> 291 mutated |= check(pss.run_pass, internal_state). 292 with SimpleTimer() as finalize_time:. 293 mutated |= check(pss.run_finalizer, internal_state). ~/.local/lib/python3.9/site-packages/numba/core/compiler_machinery.py in check(func, compiler_state). 262 . 263 def check(func, compiler_state):. --> 264 mangled = func(compiler_state). 265 if mangled not in (True, False):. 266 msg = (""CompilerPass implementations should return True/False. "". ~/.local/lib/python3.9/site-packages/numba/core/typed_passes.py in run_pass(self, state). 90 % (state.func_id.func_name,)):. 91 # Type inference. ---> 92 typemap, return_type, calltypes = type_inference_stage(. 93 state.typingctx,. 94 state.func_ir,. ~/.local/lib/python3.9/site-packages/numba/core/typed_passes.py in type_inference_stage(typingctx, interp, args, return_type, locals, raise_errors). 68 . 69 infer.build_constraint(). ---> 70 infer.propagate(raise_errors=raise_errors). 71 typemap, restype, calltypes = infer.unify(raise_errors=raise_errors). 72 . ~/.local/lib/python3.9/site-packages/numba/core/typeinfer.py in propagate(self, raise_errors). 1069 if isinstance(e, ForceLiteralArg)]. 1070 if not force_lit_args:. -> 1071 raise errors[0]. 1072 else:. 1073 raise reduce(operator.or_, force_lit_args). TypingError: Failed in nopython mode pipeline (step: nopython frontend). Failed in nopython mode pipeline (step: nopython frontend). Failed in nopython mode pipeline (step: nopython mode backend). Failed in nopython mode pipeline (step: nopython mode backend). Failed in",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:7494,integrability,state,state,7494,"/python3.9/site-packages/numba/core/compiler_lock.py in _acquire_compile_lock(*args, **kwargs). 30 def _acquire_compile_lock(*args, **kwargs):. 31 with self:. ---> 32 return func(*args, **kwargs). 33 return _acquire_compile_lock. 34 . ~/.local/lib/python3.9/site-packages/numba/core/compiler_machinery.py in _runPass(self, index, pss, internal_state). 289 mutated |= check(pss.run_initialization, internal_state). 290 with SimpleTimer() as pass_time:. --> 291 mutated |= check(pss.run_pass, internal_state). 292 with SimpleTimer() as finalize_time:. 293 mutated |= check(pss.run_finalizer, internal_state). ~/.local/lib/python3.9/site-packages/numba/core/compiler_machinery.py in check(func, compiler_state). 262 . 263 def check(func, compiler_state):. --> 264 mangled = func(compiler_state). 265 if mangled not in (True, False):. 266 msg = (""CompilerPass implementations should return True/False. "". ~/.local/lib/python3.9/site-packages/numba/core/typed_passes.py in run_pass(self, state). 90 % (state.func_id.func_name,)):. 91 # Type inference. ---> 92 typemap, return_type, calltypes = type_inference_stage(. 93 state.typingctx,. 94 state.func_ir,. ~/.local/lib/python3.9/site-packages/numba/core/typed_passes.py in type_inference_stage(typingctx, interp, args, return_type, locals, raise_errors). 68 . 69 infer.build_constraint(). ---> 70 infer.propagate(raise_errors=raise_errors). 71 typemap, restype, calltypes = infer.unify(raise_errors=raise_errors). 72 . ~/.local/lib/python3.9/site-packages/numba/core/typeinfer.py in propagate(self, raise_errors). 1069 if isinstance(e, ForceLiteralArg)]. 1070 if not force_lit_args:. -> 1071 raise errors[0]. 1072 else:. 1073 raise reduce(operator.or_, force_lit_args). TypingError: Failed in nopython mode pipeline (step: nopython frontend). Failed in nopython mode pipeline (step: nopython frontend). Failed in nopython mode pipeline (step: nopython mode backend). Failed in nopython mode pipeline (step: nopython mode backend). Failed in nopython mode",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:7612,integrability,state,state,7612,"e_lock(*args, **kwargs):. 31 with self:. ---> 32 return func(*args, **kwargs). 33 return _acquire_compile_lock. 34 . ~/.local/lib/python3.9/site-packages/numba/core/compiler_machinery.py in _runPass(self, index, pss, internal_state). 289 mutated |= check(pss.run_initialization, internal_state). 290 with SimpleTimer() as pass_time:. --> 291 mutated |= check(pss.run_pass, internal_state). 292 with SimpleTimer() as finalize_time:. 293 mutated |= check(pss.run_finalizer, internal_state). ~/.local/lib/python3.9/site-packages/numba/core/compiler_machinery.py in check(func, compiler_state). 262 . 263 def check(func, compiler_state):. --> 264 mangled = func(compiler_state). 265 if mangled not in (True, False):. 266 msg = (""CompilerPass implementations should return True/False. "". ~/.local/lib/python3.9/site-packages/numba/core/typed_passes.py in run_pass(self, state). 90 % (state.func_id.func_name,)):. 91 # Type inference. ---> 92 typemap, return_type, calltypes = type_inference_stage(. 93 state.typingctx,. 94 state.func_ir,. ~/.local/lib/python3.9/site-packages/numba/core/typed_passes.py in type_inference_stage(typingctx, interp, args, return_type, locals, raise_errors). 68 . 69 infer.build_constraint(). ---> 70 infer.propagate(raise_errors=raise_errors). 71 typemap, restype, calltypes = infer.unify(raise_errors=raise_errors). 72 . ~/.local/lib/python3.9/site-packages/numba/core/typeinfer.py in propagate(self, raise_errors). 1069 if isinstance(e, ForceLiteralArg)]. 1070 if not force_lit_args:. -> 1071 raise errors[0]. 1072 else:. 1073 raise reduce(operator.or_, force_lit_args). TypingError: Failed in nopython mode pipeline (step: nopython frontend). Failed in nopython mode pipeline (step: nopython frontend). Failed in nopython mode pipeline (step: nopython mode backend). Failed in nopython mode pipeline (step: nopython mode backend). Failed in nopython mode pipeline (step: nopython frontend). No implementation of function Function(<function make_quicksort_impl.<locals>.run",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:7633,integrability,state,state,7633,"s):. 31 with self:. ---> 32 return func(*args, **kwargs). 33 return _acquire_compile_lock. 34 . ~/.local/lib/python3.9/site-packages/numba/core/compiler_machinery.py in _runPass(self, index, pss, internal_state). 289 mutated |= check(pss.run_initialization, internal_state). 290 with SimpleTimer() as pass_time:. --> 291 mutated |= check(pss.run_pass, internal_state). 292 with SimpleTimer() as finalize_time:. 293 mutated |= check(pss.run_finalizer, internal_state). ~/.local/lib/python3.9/site-packages/numba/core/compiler_machinery.py in check(func, compiler_state). 262 . 263 def check(func, compiler_state):. --> 264 mangled = func(compiler_state). 265 if mangled not in (True, False):. 266 msg = (""CompilerPass implementations should return True/False. "". ~/.local/lib/python3.9/site-packages/numba/core/typed_passes.py in run_pass(self, state). 90 % (state.func_id.func_name,)):. 91 # Type inference. ---> 92 typemap, return_type, calltypes = type_inference_stage(. 93 state.typingctx,. 94 state.func_ir,. ~/.local/lib/python3.9/site-packages/numba/core/typed_passes.py in type_inference_stage(typingctx, interp, args, return_type, locals, raise_errors). 68 . 69 infer.build_constraint(). ---> 70 infer.propagate(raise_errors=raise_errors). 71 typemap, restype, calltypes = infer.unify(raise_errors=raise_errors). 72 . ~/.local/lib/python3.9/site-packages/numba/core/typeinfer.py in propagate(self, raise_errors). 1069 if isinstance(e, ForceLiteralArg)]. 1070 if not force_lit_args:. -> 1071 raise errors[0]. 1072 else:. 1073 raise reduce(operator.or_, force_lit_args). TypingError: Failed in nopython mode pipeline (step: nopython frontend). Failed in nopython mode pipeline (step: nopython frontend). Failed in nopython mode pipeline (step: nopython mode backend). Failed in nopython mode pipeline (step: nopython mode backend). Failed in nopython mode pipeline (step: nopython frontend). No implementation of function Function(<function make_quicksort_impl.<locals>.run_quicksort at 0x7f455",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:8250,integrability,pipelin,pipeline,8250,"264 mangled = func(compiler_state). 265 if mangled not in (True, False):. 266 msg = (""CompilerPass implementations should return True/False. "". ~/.local/lib/python3.9/site-packages/numba/core/typed_passes.py in run_pass(self, state). 90 % (state.func_id.func_name,)):. 91 # Type inference. ---> 92 typemap, return_type, calltypes = type_inference_stage(. 93 state.typingctx,. 94 state.func_ir,. ~/.local/lib/python3.9/site-packages/numba/core/typed_passes.py in type_inference_stage(typingctx, interp, args, return_type, locals, raise_errors). 68 . 69 infer.build_constraint(). ---> 70 infer.propagate(raise_errors=raise_errors). 71 typemap, restype, calltypes = infer.unify(raise_errors=raise_errors). 72 . ~/.local/lib/python3.9/site-packages/numba/core/typeinfer.py in propagate(self, raise_errors). 1069 if isinstance(e, ForceLiteralArg)]. 1070 if not force_lit_args:. -> 1071 raise errors[0]. 1072 else:. 1073 raise reduce(operator.or_, force_lit_args). TypingError: Failed in nopython mode pipeline (step: nopython frontend). Failed in nopython mode pipeline (step: nopython frontend). Failed in nopython mode pipeline (step: nopython mode backend). Failed in nopython mode pipeline (step: nopython mode backend). Failed in nopython mode pipeline (step: nopython frontend). No implementation of function Function(<function make_quicksort_impl.<locals>.run_quicksort at 0x7f455d24c280>) found for signature:. . >>> run_quicksort(array(int32, 1d, C)). . There are 2 candidate implementations:. - Of which 2 did not match due to:. Overload in function 'register_jitable.<locals>.wrap.<locals>.ov_wrap': File: numba/core/extending.py: Line 150. With argument(s): '(array(int32, 1d, C))':. Rejected as the implementation raised a specific error:. UnsupportedError: Failed in nopython mode pipeline (step: analyzing bytecode). Use of unsupported opcode (LOAD_ASSERTION_ERROR) found. . File ""../../../../.local/lib/python3.9/site-packages/numba/misc/quicksort.py"", line 180:. def run_quicksort(A):. <s",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:8310,integrability,pipelin,pipeline,8310,"rue, False):. 266 msg = (""CompilerPass implementations should return True/False. "". ~/.local/lib/python3.9/site-packages/numba/core/typed_passes.py in run_pass(self, state). 90 % (state.func_id.func_name,)):. 91 # Type inference. ---> 92 typemap, return_type, calltypes = type_inference_stage(. 93 state.typingctx,. 94 state.func_ir,. ~/.local/lib/python3.9/site-packages/numba/core/typed_passes.py in type_inference_stage(typingctx, interp, args, return_type, locals, raise_errors). 68 . 69 infer.build_constraint(). ---> 70 infer.propagate(raise_errors=raise_errors). 71 typemap, restype, calltypes = infer.unify(raise_errors=raise_errors). 72 . ~/.local/lib/python3.9/site-packages/numba/core/typeinfer.py in propagate(self, raise_errors). 1069 if isinstance(e, ForceLiteralArg)]. 1070 if not force_lit_args:. -> 1071 raise errors[0]. 1072 else:. 1073 raise reduce(operator.or_, force_lit_args). TypingError: Failed in nopython mode pipeline (step: nopython frontend). Failed in nopython mode pipeline (step: nopython frontend). Failed in nopython mode pipeline (step: nopython mode backend). Failed in nopython mode pipeline (step: nopython mode backend). Failed in nopython mode pipeline (step: nopython frontend). No implementation of function Function(<function make_quicksort_impl.<locals>.run_quicksort at 0x7f455d24c280>) found for signature:. . >>> run_quicksort(array(int32, 1d, C)). . There are 2 candidate implementations:. - Of which 2 did not match due to:. Overload in function 'register_jitable.<locals>.wrap.<locals>.ov_wrap': File: numba/core/extending.py: Line 150. With argument(s): '(array(int32, 1d, C))':. Rejected as the implementation raised a specific error:. UnsupportedError: Failed in nopython mode pipeline (step: analyzing bytecode). Use of unsupported opcode (LOAD_ASSERTION_ERROR) found. . File ""../../../../.local/lib/python3.9/site-packages/numba/misc/quicksort.py"", line 180:. def run_quicksort(A):. <source elided>. while high - low >= SMALL_QUICKSORT:. assert ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:8370,integrability,pipelin,pipeline,8370,"d return True/False. "". ~/.local/lib/python3.9/site-packages/numba/core/typed_passes.py in run_pass(self, state). 90 % (state.func_id.func_name,)):. 91 # Type inference. ---> 92 typemap, return_type, calltypes = type_inference_stage(. 93 state.typingctx,. 94 state.func_ir,. ~/.local/lib/python3.9/site-packages/numba/core/typed_passes.py in type_inference_stage(typingctx, interp, args, return_type, locals, raise_errors). 68 . 69 infer.build_constraint(). ---> 70 infer.propagate(raise_errors=raise_errors). 71 typemap, restype, calltypes = infer.unify(raise_errors=raise_errors). 72 . ~/.local/lib/python3.9/site-packages/numba/core/typeinfer.py in propagate(self, raise_errors). 1069 if isinstance(e, ForceLiteralArg)]. 1070 if not force_lit_args:. -> 1071 raise errors[0]. 1072 else:. 1073 raise reduce(operator.or_, force_lit_args). TypingError: Failed in nopython mode pipeline (step: nopython frontend). Failed in nopython mode pipeline (step: nopython frontend). Failed in nopython mode pipeline (step: nopython mode backend). Failed in nopython mode pipeline (step: nopython mode backend). Failed in nopython mode pipeline (step: nopython frontend). No implementation of function Function(<function make_quicksort_impl.<locals>.run_quicksort at 0x7f455d24c280>) found for signature:. . >>> run_quicksort(array(int32, 1d, C)). . There are 2 candidate implementations:. - Of which 2 did not match due to:. Overload in function 'register_jitable.<locals>.wrap.<locals>.ov_wrap': File: numba/core/extending.py: Line 150. With argument(s): '(array(int32, 1d, C))':. Rejected as the implementation raised a specific error:. UnsupportedError: Failed in nopython mode pipeline (step: analyzing bytecode). Use of unsupported opcode (LOAD_ASSERTION_ERROR) found. . File ""../../../../.local/lib/python3.9/site-packages/numba/misc/quicksort.py"", line 180:. def run_quicksort(A):. <source elided>. while high - low >= SMALL_QUICKSORT:. assert n < MAX_STACK. ^. . raised from /home/gabriel/.local/lib/pyt",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:8434,integrability,pipelin,pipeline,8434,"ba/core/typed_passes.py in run_pass(self, state). 90 % (state.func_id.func_name,)):. 91 # Type inference. ---> 92 typemap, return_type, calltypes = type_inference_stage(. 93 state.typingctx,. 94 state.func_ir,. ~/.local/lib/python3.9/site-packages/numba/core/typed_passes.py in type_inference_stage(typingctx, interp, args, return_type, locals, raise_errors). 68 . 69 infer.build_constraint(). ---> 70 infer.propagate(raise_errors=raise_errors). 71 typemap, restype, calltypes = infer.unify(raise_errors=raise_errors). 72 . ~/.local/lib/python3.9/site-packages/numba/core/typeinfer.py in propagate(self, raise_errors). 1069 if isinstance(e, ForceLiteralArg)]. 1070 if not force_lit_args:. -> 1071 raise errors[0]. 1072 else:. 1073 raise reduce(operator.or_, force_lit_args). TypingError: Failed in nopython mode pipeline (step: nopython frontend). Failed in nopython mode pipeline (step: nopython frontend). Failed in nopython mode pipeline (step: nopython mode backend). Failed in nopython mode pipeline (step: nopython mode backend). Failed in nopython mode pipeline (step: nopython frontend). No implementation of function Function(<function make_quicksort_impl.<locals>.run_quicksort at 0x7f455d24c280>) found for signature:. . >>> run_quicksort(array(int32, 1d, C)). . There are 2 candidate implementations:. - Of which 2 did not match due to:. Overload in function 'register_jitable.<locals>.wrap.<locals>.ov_wrap': File: numba/core/extending.py: Line 150. With argument(s): '(array(int32, 1d, C))':. Rejected as the implementation raised a specific error:. UnsupportedError: Failed in nopython mode pipeline (step: analyzing bytecode). Use of unsupported opcode (LOAD_ASSERTION_ERROR) found. . File ""../../../../.local/lib/python3.9/site-packages/numba/misc/quicksort.py"", line 180:. def run_quicksort(A):. <source elided>. while high - low >= SMALL_QUICKSORT:. assert n < MAX_STACK. ^. . raised from /home/gabriel/.local/lib/python3.9/site-packages/numba/core/byteflow.py:269. During: resolvi",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:8498,integrability,pipelin,pipeline,8498,"nc_id.func_name,)):. 91 # Type inference. ---> 92 typemap, return_type, calltypes = type_inference_stage(. 93 state.typingctx,. 94 state.func_ir,. ~/.local/lib/python3.9/site-packages/numba/core/typed_passes.py in type_inference_stage(typingctx, interp, args, return_type, locals, raise_errors). 68 . 69 infer.build_constraint(). ---> 70 infer.propagate(raise_errors=raise_errors). 71 typemap, restype, calltypes = infer.unify(raise_errors=raise_errors). 72 . ~/.local/lib/python3.9/site-packages/numba/core/typeinfer.py in propagate(self, raise_errors). 1069 if isinstance(e, ForceLiteralArg)]. 1070 if not force_lit_args:. -> 1071 raise errors[0]. 1072 else:. 1073 raise reduce(operator.or_, force_lit_args). TypingError: Failed in nopython mode pipeline (step: nopython frontend). Failed in nopython mode pipeline (step: nopython frontend). Failed in nopython mode pipeline (step: nopython mode backend). Failed in nopython mode pipeline (step: nopython mode backend). Failed in nopython mode pipeline (step: nopython frontend). No implementation of function Function(<function make_quicksort_impl.<locals>.run_quicksort at 0x7f455d24c280>) found for signature:. . >>> run_quicksort(array(int32, 1d, C)). . There are 2 candidate implementations:. - Of which 2 did not match due to:. Overload in function 'register_jitable.<locals>.wrap.<locals>.ov_wrap': File: numba/core/extending.py: Line 150. With argument(s): '(array(int32, 1d, C))':. Rejected as the implementation raised a specific error:. UnsupportedError: Failed in nopython mode pipeline (step: analyzing bytecode). Use of unsupported opcode (LOAD_ASSERTION_ERROR) found. . File ""../../../../.local/lib/python3.9/site-packages/numba/misc/quicksort.py"", line 180:. def run_quicksort(A):. <source elided>. while high - low >= SMALL_QUICKSORT:. assert n < MAX_STACK. ^. . raised from /home/gabriel/.local/lib/python3.9/site-packages/numba/core/byteflow.py:269. During: resolving callee type: Function(<function make_quicksort_impl.<locals>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:8836,integrability,wrap,wrap,8836,"0 infer.propagate(raise_errors=raise_errors). 71 typemap, restype, calltypes = infer.unify(raise_errors=raise_errors). 72 . ~/.local/lib/python3.9/site-packages/numba/core/typeinfer.py in propagate(self, raise_errors). 1069 if isinstance(e, ForceLiteralArg)]. 1070 if not force_lit_args:. -> 1071 raise errors[0]. 1072 else:. 1073 raise reduce(operator.or_, force_lit_args). TypingError: Failed in nopython mode pipeline (step: nopython frontend). Failed in nopython mode pipeline (step: nopython frontend). Failed in nopython mode pipeline (step: nopython mode backend). Failed in nopython mode pipeline (step: nopython mode backend). Failed in nopython mode pipeline (step: nopython frontend). No implementation of function Function(<function make_quicksort_impl.<locals>.run_quicksort at 0x7f455d24c280>) found for signature:. . >>> run_quicksort(array(int32, 1d, C)). . There are 2 candidate implementations:. - Of which 2 did not match due to:. Overload in function 'register_jitable.<locals>.wrap.<locals>.ov_wrap': File: numba/core/extending.py: Line 150. With argument(s): '(array(int32, 1d, C))':. Rejected as the implementation raised a specific error:. UnsupportedError: Failed in nopython mode pipeline (step: analyzing bytecode). Use of unsupported opcode (LOAD_ASSERTION_ERROR) found. . File ""../../../../.local/lib/python3.9/site-packages/numba/misc/quicksort.py"", line 180:. def run_quicksort(A):. <source elided>. while high - low >= SMALL_QUICKSORT:. assert n < MAX_STACK. ^. . raised from /home/gabriel/.local/lib/python3.9/site-packages/numba/core/byteflow.py:269. During: resolving callee type: Function(<function make_quicksort_impl.<locals>.run_quicksort at 0x7f455d24c280>). During: typing of call at /home/gabriel/.local/lib/python3.9/site-packages/numba/np/arrayobj.py (5007). File ""../../../../.local/lib/python3.9/site-packages/numba/np/arrayobj.py"", line 5007:. def array_sort_impl(arr):. <source elided>. # Note we clobber the return value. sort_func(arr). ^. During: lo",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:9044,integrability,pipelin,pipeline,9044,"errors). 1069 if isinstance(e, ForceLiteralArg)]. 1070 if not force_lit_args:. -> 1071 raise errors[0]. 1072 else:. 1073 raise reduce(operator.or_, force_lit_args). TypingError: Failed in nopython mode pipeline (step: nopython frontend). Failed in nopython mode pipeline (step: nopython frontend). Failed in nopython mode pipeline (step: nopython mode backend). Failed in nopython mode pipeline (step: nopython mode backend). Failed in nopython mode pipeline (step: nopython frontend). No implementation of function Function(<function make_quicksort_impl.<locals>.run_quicksort at 0x7f455d24c280>) found for signature:. . >>> run_quicksort(array(int32, 1d, C)). . There are 2 candidate implementations:. - Of which 2 did not match due to:. Overload in function 'register_jitable.<locals>.wrap.<locals>.ov_wrap': File: numba/core/extending.py: Line 150. With argument(s): '(array(int32, 1d, C))':. Rejected as the implementation raised a specific error:. UnsupportedError: Failed in nopython mode pipeline (step: analyzing bytecode). Use of unsupported opcode (LOAD_ASSERTION_ERROR) found. . File ""../../../../.local/lib/python3.9/site-packages/numba/misc/quicksort.py"", line 180:. def run_quicksort(A):. <source elided>. while high - low >= SMALL_QUICKSORT:. assert n < MAX_STACK. ^. . raised from /home/gabriel/.local/lib/python3.9/site-packages/numba/core/byteflow.py:269. During: resolving callee type: Function(<function make_quicksort_impl.<locals>.run_quicksort at 0x7f455d24c280>). During: typing of call at /home/gabriel/.local/lib/python3.9/site-packages/numba/np/arrayobj.py (5007). File ""../../../../.local/lib/python3.9/site-packages/numba/np/arrayobj.py"", line 5007:. def array_sort_impl(arr):. <source elided>. # Note we clobber the return value. sort_func(arr). ^. During: lowering ""$14call_method.5 = call $12load_method.4(func=$12load_method.4, args=[], kws=(), vararg=None)"" at /home/gabriel/.local/lib/python3.9/site-packages/numba/np/arrayobj.py (5017). During: lowering ""$8call_m",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:11417,integrability,Version,Versions,11417,"rr, sparse.py:28)], kws=(), vararg=None)"" at /home/gabriel/.local/lib/python3.9/site-packages/pynndescent/sparse.py (28). During: resolving callee type: type(CPUDispatcher(<function arr_unique at 0x7f4574e5aa60>)). During: typing of call at /home/gabriel/.local/lib/python3.9/site-packages/pynndescent/sparse.py (41). During: resolving callee type: type(CPUDispatcher(<function arr_unique at 0x7f4574e5aa60>)). During: typing of call at /home/gabriel/.local/lib/python3.9/site-packages/pynndescent/sparse.py (41). File ""../../../../.local/lib/python3.9/site-packages/pynndescent/sparse.py"", line 41:. def arr_union(ar1, ar2):. <source elided>. else:. return arr_unique(np.concatenate((ar1, ar2))). ^. During: resolving callee type: type(CPUDispatcher(<function arr_union at 0x7f4574e5ac10>)). During: typing of call at /home/gabriel/.local/lib/python3.9/site-packages/pynndescent/sparse.py (344). During: resolving callee type: type(CPUDispatcher(<function arr_union at 0x7f4574e5ac10>)). During: typing of call at /home/gabriel/.local/lib/python3.9/site-packages/pynndescent/sparse.py (344). File ""../../../../.local/lib/python3.9/site-packages/pynndescent/sparse.py"", line 344:. def sparse_alternative_jaccard(ind1, data1, ind2, data2):. num_non_zero = arr_union(ind1, ind2).shape[0]. ^. ```. #### Versions. <details>. anndata 0.7.5. scanpy 1.7.0. sinfo 0.3.1. -----. PIL 8.1.0. PyQt5 NA. anndata 0.7.5. appdirs 1.4.4. cffi 1.14.4. colorama 0.4.4. constants NA. cycler 0.10.0. cython_runtime NA. dateutil 2.8.1. get_version 2.1. google NA. h5py 3.1.0. highs_wrapper NA. joblib 1.0.1. kiwisolver 1.3.1. legacy_api_wrap 1.2. llvmlite 0.34.0. matplotlib 3.3.4. mpl_toolkits NA. natsort 7.1.1. numba 0.51.2. numexpr 2.7.2. numpy 1.20.0. packaging 20.8. pandas 1.2.2. pkg_resources NA. psutil 5.8.0. pyexpat NA. pyparsing 2.4.7. pytoml NA. pytz 2021.1. scanpy 1.7.0. scipy 1.6.0. setuptools_scm NA. sinfo 0.3.1. sip NA. six 1.15.0. sklearn 0.24.1. sphinxcontrib NA. tables 3.6.1. yaml 5.3.1. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:2087,interoperability,messag,message,2087,"rs, n_pcs, use_rep, knn, random_state, method, metric, metric_kwds, key_added, copy). 118 adata._init_as_actual(adata.copy()). 119 neighbors = Neighbors(adata). --> 120 neighbors.compute_neighbors(. 121 n_neighbors=n_neighbors, knn=knn, n_pcs=n_pcs, use_rep=use_rep,. 122 method=method, metric=metric, metric_kwds=metric_kwds,. ~/.local/lib/python3.9/site-packages/scanpy/neighbors/__init__.py in compute_neighbors(self, n_neighbors, knn, n_pcs, use_rep, method, random_state, write_knn_indices, metric, metric_kwds). 732 X = pairwise_distances(X, metric=metric, **metric_kwds). 733 metric = 'precomputed'. --> 734 knn_indices, knn_distances, forest = compute_neighbors_umap(. 735 X, n_neighbors, random_state, metric=metric, metric_kwds=metric_kwds). 736 # very cautious here. ~/.local/lib/python3.9/site-packages/scanpy/neighbors/__init__.py in compute_neighbors_umap(X, n_neighbors, random_state, metric, metric_kwds, angular, verbose). 273 # umap 0.5.0. 274 warnings.filterwarnings(""ignore"", message=r""Tensorflow not installed""). --> 275 from umap.umap_ import nearest_neighbors. 276 . 277 random_state = check_random_state(random_state). ~/.local/lib/python3.9/site-packages/umap/umap_.py in <module>. 45 ). 46 . ---> 47 from pynndescent import NNDescent. 48 from pynndescent.distances import named_distances as pynn_named_distances. 49 from pynndescent.sparse import sparse_named_distances as pynn_sparse_named_distances. ~/.local/lib/python3.9/site-packages/pynndescent/__init__.py in <module>. 1 import pkg_resources. 2 import numba. ----> 3 from .pynndescent_ import NNDescent, PyNNDescentTransformer. 4 . 5 # Workaround: https://github.com/numba/numba/issues/3341. ~/.local/lib/python3.9/site-packages/pynndescent/pynndescent_.py in <module>. 19 import heapq. 20 . ---> 21 import pynndescent.sparse as sparse. 22 import pynndescent.sparse_nndescent as sparse_nnd. 23 import pynndescent.distances as pynnd_dist. ~/.local/lib/python3.9/site-packages/pynndescent/sparse.py in <module>. 341 },.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:3334,interoperability,wrapper,wrapper,3334,"rt NNDescent. 48 from pynndescent.distances import named_distances as pynn_named_distances. 49 from pynndescent.sparse import sparse_named_distances as pynn_sparse_named_distances. ~/.local/lib/python3.9/site-packages/pynndescent/__init__.py in <module>. 1 import pkg_resources. 2 import numba. ----> 3 from .pynndescent_ import NNDescent, PyNNDescentTransformer. 4 . 5 # Workaround: https://github.com/numba/numba/issues/3341. ~/.local/lib/python3.9/site-packages/pynndescent/pynndescent_.py in <module>. 19 import heapq. 20 . ---> 21 import pynndescent.sparse as sparse. 22 import pynndescent.sparse_nndescent as sparse_nnd. 23 import pynndescent.distances as pynnd_dist. ~/.local/lib/python3.9/site-packages/pynndescent/sparse.py in <module>. 341 },. 342 ). --> 343 def sparse_alternative_jaccard(ind1, data1, ind2, data2):. 344 num_non_zero = arr_union(ind1, ind2).shape[0]. 345 num_equal = arr_intersect(ind1, ind2).shape[0]. ~/.local/lib/python3.9/site-packages/numba/core/decorators.py in wrapper(func). 216 with typeinfer.register_dispatcher(disp):. 217 for sig in sigs:. --> 218 disp.compile(sig). 219 disp.disable_compile(). 220 return disp. ~/.local/lib/python3.9/site-packages/numba/core/compiler_lock.py in _acquire_compile_lock(*args, **kwargs). 30 def _acquire_compile_lock(*args, **kwargs):. 31 with self:. ---> 32 return func(*args, **kwargs). 33 return _acquire_compile_lock. 34 . ~/.local/lib/python3.9/site-packages/numba/core/dispatcher.py in compile(self, sig). 817 self._cache_misses[sig] += 1. 818 try:. --> 819 cres = self._compiler.compile(args, return_type). 820 except errors.ForceLiteralArg as e:. 821 def folded(args, kws):. ~/.local/lib/python3.9/site-packages/numba/core/dispatcher.py in compile(self, args, return_type). 80 return retval. 81 else:. ---> 82 raise retval. 83 . 84 def _compile_cached(self, args, return_type):. ~/.local/lib/python3.9/site-packages/numba/core/dispatcher.py in _compile_cached(self, args, return_type). 90 . 91 try:. ---> 92 retval = sel",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:8985,interoperability,specif,specific,8985,"-packages/numba/core/typeinfer.py in propagate(self, raise_errors). 1069 if isinstance(e, ForceLiteralArg)]. 1070 if not force_lit_args:. -> 1071 raise errors[0]. 1072 else:. 1073 raise reduce(operator.or_, force_lit_args). TypingError: Failed in nopython mode pipeline (step: nopython frontend). Failed in nopython mode pipeline (step: nopython frontend). Failed in nopython mode pipeline (step: nopython mode backend). Failed in nopython mode pipeline (step: nopython mode backend). Failed in nopython mode pipeline (step: nopython frontend). No implementation of function Function(<function make_quicksort_impl.<locals>.run_quicksort at 0x7f455d24c280>) found for signature:. . >>> run_quicksort(array(int32, 1d, C)). . There are 2 candidate implementations:. - Of which 2 did not match due to:. Overload in function 'register_jitable.<locals>.wrap.<locals>.ov_wrap': File: numba/core/extending.py: Line 150. With argument(s): '(array(int32, 1d, C))':. Rejected as the implementation raised a specific error:. UnsupportedError: Failed in nopython mode pipeline (step: analyzing bytecode). Use of unsupported opcode (LOAD_ASSERTION_ERROR) found. . File ""../../../../.local/lib/python3.9/site-packages/numba/misc/quicksort.py"", line 180:. def run_quicksort(A):. <source elided>. while high - low >= SMALL_QUICKSORT:. assert n < MAX_STACK. ^. . raised from /home/gabriel/.local/lib/python3.9/site-packages/numba/core/byteflow.py:269. During: resolving callee type: Function(<function make_quicksort_impl.<locals>.run_quicksort at 0x7f455d24c280>). During: typing of call at /home/gabriel/.local/lib/python3.9/site-packages/numba/np/arrayobj.py (5007). File ""../../../../.local/lib/python3.9/site-packages/numba/np/arrayobj.py"", line 5007:. def array_sort_impl(arr):. <source elided>. # Note we clobber the return value. sort_func(arr). ^. During: lowering ""$14call_method.5 = call $12load_method.4(func=$12load_method.4, args=[], kws=(), vararg=None)"" at /home/gabriel/.local/lib/python3.9/site-packa",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:887,modifiability,modul,module,887,"TypingError: Failed in nopython mode pipeline (step: nopython frontend); Hi, I am working with a big dataset over 100000 cells and I run into a problem when computing the neigbours. Find below an small example using a random matrix. . Observations,. when I run N_cells = 8000 it works perfectly for gauss metric. when running N_cells = 10000, the neighbors computation crashes. The problem is even more dramatic for method=""umap"". . ### Minimal code sample (that we can copy&paste without having any data). ```python. import scanpy. import numpy. matrix = numpy.random.uniform(size=[10000,1000]). adata = scanpy.AnnData(matrix). scanpy.pp.pca(adata,n_comps=10). scanpy.pp.neighbors(adata,method=""gauss"",n_pcs=10). ```. ```pytb. ---------------------------------------------------------------------------. TypingError Traceback (most recent call last). <ipython-input-3-692f9d70cb0c> in <module>. 9 . 10 scanpy.pp.pca(adata,n_comps=10). ---> 11 scanpy.pp.neighbors(adata,method=""gauss"",n_pcs=10). ~/.local/lib/python3.9/site-packages/scanpy/neighbors/__init__.py in neighbors(adata, n_neighbors, n_pcs, use_rep, knn, random_state, method, metric, metric_kwds, key_added, copy). 118 adata._init_as_actual(adata.copy()). 119 neighbors = Neighbors(adata). --> 120 neighbors.compute_neighbors(. 121 n_neighbors=n_neighbors, knn=knn, n_pcs=n_pcs, use_rep=use_rep,. 122 method=method, metric=metric, metric_kwds=metric_kwds,. ~/.local/lib/python3.9/site-packages/scanpy/neighbors/__init__.py in compute_neighbors(self, n_neighbors, knn, n_pcs, use_rep, method, random_state, write_knn_indices, metric, metric_kwds). 732 X = pairwise_distances(X, metric=metric, **metric_kwds). 733 metric = 'precomputed'. --> 734 knn_indices, knn_distances, forest = compute_neighbors_umap(. 735 X, n_neighbors, random_state, metric=metric, metric_kwds=metric_kwds). 736 # very cautious here. ~/.local/lib/python3.9/site-packages/scanpy/neighbors/__init__.py in compute_neighbors_umap(X, n_neighbors, random_state, metric, m",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:1024,modifiability,pac,packages,1024,"hon mode pipeline (step: nopython frontend); Hi, I am working with a big dataset over 100000 cells and I run into a problem when computing the neigbours. Find below an small example using a random matrix. . Observations,. when I run N_cells = 8000 it works perfectly for gauss metric. when running N_cells = 10000, the neighbors computation crashes. The problem is even more dramatic for method=""umap"". . ### Minimal code sample (that we can copy&paste without having any data). ```python. import scanpy. import numpy. matrix = numpy.random.uniform(size=[10000,1000]). adata = scanpy.AnnData(matrix). scanpy.pp.pca(adata,n_comps=10). scanpy.pp.neighbors(adata,method=""gauss"",n_pcs=10). ```. ```pytb. ---------------------------------------------------------------------------. TypingError Traceback (most recent call last). <ipython-input-3-692f9d70cb0c> in <module>. 9 . 10 scanpy.pp.pca(adata,n_comps=10). ---> 11 scanpy.pp.neighbors(adata,method=""gauss"",n_pcs=10). ~/.local/lib/python3.9/site-packages/scanpy/neighbors/__init__.py in neighbors(adata, n_neighbors, n_pcs, use_rep, knn, random_state, method, metric, metric_kwds, key_added, copy). 118 adata._init_as_actual(adata.copy()). 119 neighbors = Neighbors(adata). --> 120 neighbors.compute_neighbors(. 121 n_neighbors=n_neighbors, knn=knn, n_pcs=n_pcs, use_rep=use_rep,. 122 method=method, metric=metric, metric_kwds=metric_kwds,. ~/.local/lib/python3.9/site-packages/scanpy/neighbors/__init__.py in compute_neighbors(self, n_neighbors, knn, n_pcs, use_rep, method, random_state, write_knn_indices, metric, metric_kwds). 732 X = pairwise_distances(X, metric=metric, **metric_kwds). 733 metric = 'precomputed'. --> 734 knn_indices, knn_distances, forest = compute_neighbors_umap(. 735 X, n_neighbors, random_state, metric=metric, metric_kwds=metric_kwds). 736 # very cautious here. ~/.local/lib/python3.9/site-packages/scanpy/neighbors/__init__.py in compute_neighbors_umap(X, n_neighbors, random_state, metric, metric_kwds, angular, verbose",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:1447,modifiability,pac,packages,1447,"ample (that we can copy&paste without having any data). ```python. import scanpy. import numpy. matrix = numpy.random.uniform(size=[10000,1000]). adata = scanpy.AnnData(matrix). scanpy.pp.pca(adata,n_comps=10). scanpy.pp.neighbors(adata,method=""gauss"",n_pcs=10). ```. ```pytb. ---------------------------------------------------------------------------. TypingError Traceback (most recent call last). <ipython-input-3-692f9d70cb0c> in <module>. 9 . 10 scanpy.pp.pca(adata,n_comps=10). ---> 11 scanpy.pp.neighbors(adata,method=""gauss"",n_pcs=10). ~/.local/lib/python3.9/site-packages/scanpy/neighbors/__init__.py in neighbors(adata, n_neighbors, n_pcs, use_rep, knn, random_state, method, metric, metric_kwds, key_added, copy). 118 adata._init_as_actual(adata.copy()). 119 neighbors = Neighbors(adata). --> 120 neighbors.compute_neighbors(. 121 n_neighbors=n_neighbors, knn=knn, n_pcs=n_pcs, use_rep=use_rep,. 122 method=method, metric=metric, metric_kwds=metric_kwds,. ~/.local/lib/python3.9/site-packages/scanpy/neighbors/__init__.py in compute_neighbors(self, n_neighbors, knn, n_pcs, use_rep, method, random_state, write_knn_indices, metric, metric_kwds). 732 X = pairwise_distances(X, metric=metric, **metric_kwds). 733 metric = 'precomputed'. --> 734 knn_indices, knn_distances, forest = compute_neighbors_umap(. 735 X, n_neighbors, random_state, metric=metric, metric_kwds=metric_kwds). 736 # very cautious here. ~/.local/lib/python3.9/site-packages/scanpy/neighbors/__init__.py in compute_neighbors_umap(X, n_neighbors, random_state, metric, metric_kwds, angular, verbose). 273 # umap 0.5.0. 274 warnings.filterwarnings(""ignore"", message=r""Tensorflow not installed""). --> 275 from umap.umap_ import nearest_neighbors. 276 . 277 random_state = check_random_state(random_state). ~/.local/lib/python3.9/site-packages/umap/umap_.py in <module>. 45 ). 46 . ---> 47 from pynndescent import NNDescent. 48 from pynndescent.distances import named_distances as pynn_named_distances. 49 from pynndescent.s",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:1897,modifiability,pac,packages,1897,"0 scanpy.pp.pca(adata,n_comps=10). ---> 11 scanpy.pp.neighbors(adata,method=""gauss"",n_pcs=10). ~/.local/lib/python3.9/site-packages/scanpy/neighbors/__init__.py in neighbors(adata, n_neighbors, n_pcs, use_rep, knn, random_state, method, metric, metric_kwds, key_added, copy). 118 adata._init_as_actual(adata.copy()). 119 neighbors = Neighbors(adata). --> 120 neighbors.compute_neighbors(. 121 n_neighbors=n_neighbors, knn=knn, n_pcs=n_pcs, use_rep=use_rep,. 122 method=method, metric=metric, metric_kwds=metric_kwds,. ~/.local/lib/python3.9/site-packages/scanpy/neighbors/__init__.py in compute_neighbors(self, n_neighbors, knn, n_pcs, use_rep, method, random_state, write_knn_indices, metric, metric_kwds). 732 X = pairwise_distances(X, metric=metric, **metric_kwds). 733 metric = 'precomputed'. --> 734 knn_indices, knn_distances, forest = compute_neighbors_umap(. 735 X, n_neighbors, random_state, metric=metric, metric_kwds=metric_kwds). 736 # very cautious here. ~/.local/lib/python3.9/site-packages/scanpy/neighbors/__init__.py in compute_neighbors_umap(X, n_neighbors, random_state, metric, metric_kwds, angular, verbose). 273 # umap 0.5.0. 274 warnings.filterwarnings(""ignore"", message=r""Tensorflow not installed""). --> 275 from umap.umap_ import nearest_neighbors. 276 . 277 random_state = check_random_state(random_state). ~/.local/lib/python3.9/site-packages/umap/umap_.py in <module>. 45 ). 46 . ---> 47 from pynndescent import NNDescent. 48 from pynndescent.distances import named_distances as pynn_named_distances. 49 from pynndescent.sparse import sparse_named_distances as pynn_sparse_named_distances. ~/.local/lib/python3.9/site-packages/pynndescent/__init__.py in <module>. 1 import pkg_resources. 2 import numba. ----> 3 from .pynndescent_ import NNDescent, PyNNDescentTransformer. 4 . 5 # Workaround: https://github.com/numba/numba/issues/3341. ~/.local/lib/python3.9/site-packages/pynndescent/pynndescent_.py in <module>. 19 import heapq. 20 . ---> 21 import pynndescent.sparse a",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:2262,modifiability,pac,packages,2262,"ors.compute_neighbors(. 121 n_neighbors=n_neighbors, knn=knn, n_pcs=n_pcs, use_rep=use_rep,. 122 method=method, metric=metric, metric_kwds=metric_kwds,. ~/.local/lib/python3.9/site-packages/scanpy/neighbors/__init__.py in compute_neighbors(self, n_neighbors, knn, n_pcs, use_rep, method, random_state, write_knn_indices, metric, metric_kwds). 732 X = pairwise_distances(X, metric=metric, **metric_kwds). 733 metric = 'precomputed'. --> 734 knn_indices, knn_distances, forest = compute_neighbors_umap(. 735 X, n_neighbors, random_state, metric=metric, metric_kwds=metric_kwds). 736 # very cautious here. ~/.local/lib/python3.9/site-packages/scanpy/neighbors/__init__.py in compute_neighbors_umap(X, n_neighbors, random_state, metric, metric_kwds, angular, verbose). 273 # umap 0.5.0. 274 warnings.filterwarnings(""ignore"", message=r""Tensorflow not installed""). --> 275 from umap.umap_ import nearest_neighbors. 276 . 277 random_state = check_random_state(random_state). ~/.local/lib/python3.9/site-packages/umap/umap_.py in <module>. 45 ). 46 . ---> 47 from pynndescent import NNDescent. 48 from pynndescent.distances import named_distances as pynn_named_distances. 49 from pynndescent.sparse import sparse_named_distances as pynn_sparse_named_distances. ~/.local/lib/python3.9/site-packages/pynndescent/__init__.py in <module>. 1 import pkg_resources. 2 import numba. ----> 3 from .pynndescent_ import NNDescent, PyNNDescentTransformer. 4 . 5 # Workaround: https://github.com/numba/numba/issues/3341. ~/.local/lib/python3.9/site-packages/pynndescent/pynndescent_.py in <module>. 19 import heapq. 20 . ---> 21 import pynndescent.sparse as sparse. 22 import pynndescent.sparse_nndescent as sparse_nnd. 23 import pynndescent.distances as pynnd_dist. ~/.local/lib/python3.9/site-packages/pynndescent/sparse.py in <module>. 341 },. 342 ). --> 343 def sparse_alternative_jaccard(ind1, data1, ind2, data2):. 344 num_non_zero = arr_union(ind1, ind2).shape[0]. 345 num_equal = arr_intersect(ind1, ind2).shape[0",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:2289,modifiability,modul,module,2289,"1 n_neighbors=n_neighbors, knn=knn, n_pcs=n_pcs, use_rep=use_rep,. 122 method=method, metric=metric, metric_kwds=metric_kwds,. ~/.local/lib/python3.9/site-packages/scanpy/neighbors/__init__.py in compute_neighbors(self, n_neighbors, knn, n_pcs, use_rep, method, random_state, write_knn_indices, metric, metric_kwds). 732 X = pairwise_distances(X, metric=metric, **metric_kwds). 733 metric = 'precomputed'. --> 734 knn_indices, knn_distances, forest = compute_neighbors_umap(. 735 X, n_neighbors, random_state, metric=metric, metric_kwds=metric_kwds). 736 # very cautious here. ~/.local/lib/python3.9/site-packages/scanpy/neighbors/__init__.py in compute_neighbors_umap(X, n_neighbors, random_state, metric, metric_kwds, angular, verbose). 273 # umap 0.5.0. 274 warnings.filterwarnings(""ignore"", message=r""Tensorflow not installed""). --> 275 from umap.umap_ import nearest_neighbors. 276 . 277 random_state = check_random_state(random_state). ~/.local/lib/python3.9/site-packages/umap/umap_.py in <module>. 45 ). 46 . ---> 47 from pynndescent import NNDescent. 48 from pynndescent.distances import named_distances as pynn_named_distances. 49 from pynndescent.sparse import sparse_named_distances as pynn_sparse_named_distances. ~/.local/lib/python3.9/site-packages/pynndescent/__init__.py in <module>. 1 import pkg_resources. 2 import numba. ----> 3 from .pynndescent_ import NNDescent, PyNNDescentTransformer. 4 . 5 # Workaround: https://github.com/numba/numba/issues/3341. ~/.local/lib/python3.9/site-packages/pynndescent/pynndescent_.py in <module>. 19 import heapq. 20 . ---> 21 import pynndescent.sparse as sparse. 22 import pynndescent.sparse_nndescent as sparse_nnd. 23 import pynndescent.distances as pynnd_dist. ~/.local/lib/python3.9/site-packages/pynndescent/sparse.py in <module>. 341 },. 342 ). --> 343 def sparse_alternative_jaccard(ind1, data1, ind2, data2):. 344 num_non_zero = arr_union(ind1, ind2).shape[0]. 345 num_equal = arr_intersect(ind1, ind2).shape[0]. ~/.local/lib/python3.9/",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:2547,modifiability,pac,packages,2547,"d, random_state, write_knn_indices, metric, metric_kwds). 732 X = pairwise_distances(X, metric=metric, **metric_kwds). 733 metric = 'precomputed'. --> 734 knn_indices, knn_distances, forest = compute_neighbors_umap(. 735 X, n_neighbors, random_state, metric=metric, metric_kwds=metric_kwds). 736 # very cautious here. ~/.local/lib/python3.9/site-packages/scanpy/neighbors/__init__.py in compute_neighbors_umap(X, n_neighbors, random_state, metric, metric_kwds, angular, verbose). 273 # umap 0.5.0. 274 warnings.filterwarnings(""ignore"", message=r""Tensorflow not installed""). --> 275 from umap.umap_ import nearest_neighbors. 276 . 277 random_state = check_random_state(random_state). ~/.local/lib/python3.9/site-packages/umap/umap_.py in <module>. 45 ). 46 . ---> 47 from pynndescent import NNDescent. 48 from pynndescent.distances import named_distances as pynn_named_distances. 49 from pynndescent.sparse import sparse_named_distances as pynn_sparse_named_distances. ~/.local/lib/python3.9/site-packages/pynndescent/__init__.py in <module>. 1 import pkg_resources. 2 import numba. ----> 3 from .pynndescent_ import NNDescent, PyNNDescentTransformer. 4 . 5 # Workaround: https://github.com/numba/numba/issues/3341. ~/.local/lib/python3.9/site-packages/pynndescent/pynndescent_.py in <module>. 19 import heapq. 20 . ---> 21 import pynndescent.sparse as sparse. 22 import pynndescent.sparse_nndescent as sparse_nnd. 23 import pynndescent.distances as pynnd_dist. ~/.local/lib/python3.9/site-packages/pynndescent/sparse.py in <module>. 341 },. 342 ). --> 343 def sparse_alternative_jaccard(ind1, data1, ind2, data2):. 344 num_non_zero = arr_union(ind1, ind2).shape[0]. 345 num_equal = arr_intersect(ind1, ind2).shape[0]. ~/.local/lib/python3.9/site-packages/numba/core/decorators.py in wrapper(func). 216 with typeinfer.register_dispatcher(disp):. 217 for sig in sigs:. --> 218 disp.compile(sig). 219 disp.disable_compile(). 220 return disp. ~/.local/lib/python3.9/site-packages/numba/core/compiler_lock",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:2584,modifiability,modul,module,2584,"metric, metric_kwds). 732 X = pairwise_distances(X, metric=metric, **metric_kwds). 733 metric = 'precomputed'. --> 734 knn_indices, knn_distances, forest = compute_neighbors_umap(. 735 X, n_neighbors, random_state, metric=metric, metric_kwds=metric_kwds). 736 # very cautious here. ~/.local/lib/python3.9/site-packages/scanpy/neighbors/__init__.py in compute_neighbors_umap(X, n_neighbors, random_state, metric, metric_kwds, angular, verbose). 273 # umap 0.5.0. 274 warnings.filterwarnings(""ignore"", message=r""Tensorflow not installed""). --> 275 from umap.umap_ import nearest_neighbors. 276 . 277 random_state = check_random_state(random_state). ~/.local/lib/python3.9/site-packages/umap/umap_.py in <module>. 45 ). 46 . ---> 47 from pynndescent import NNDescent. 48 from pynndescent.distances import named_distances as pynn_named_distances. 49 from pynndescent.sparse import sparse_named_distances as pynn_sparse_named_distances. ~/.local/lib/python3.9/site-packages/pynndescent/__init__.py in <module>. 1 import pkg_resources. 2 import numba. ----> 3 from .pynndescent_ import NNDescent, PyNNDescentTransformer. 4 . 5 # Workaround: https://github.com/numba/numba/issues/3341. ~/.local/lib/python3.9/site-packages/pynndescent/pynndescent_.py in <module>. 19 import heapq. 20 . ---> 21 import pynndescent.sparse as sparse. 22 import pynndescent.sparse_nndescent as sparse_nnd. 23 import pynndescent.distances as pynnd_dist. ~/.local/lib/python3.9/site-packages/pynndescent/sparse.py in <module>. 341 },. 342 ). --> 343 def sparse_alternative_jaccard(ind1, data1, ind2, data2):. 344 num_non_zero = arr_union(ind1, ind2).shape[0]. 345 num_equal = arr_intersect(ind1, ind2).shape[0]. ~/.local/lib/python3.9/site-packages/numba/core/decorators.py in wrapper(func). 216 with typeinfer.register_dispatcher(disp):. 217 for sig in sigs:. --> 218 disp.compile(sig). 219 disp.disable_compile(). 220 return disp. ~/.local/lib/python3.9/site-packages/numba/core/compiler_lock.py in _acquire_compile_lock(*args, ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:2794,modifiability,pac,packages,2794,"te, metric=metric, metric_kwds=metric_kwds). 736 # very cautious here. ~/.local/lib/python3.9/site-packages/scanpy/neighbors/__init__.py in compute_neighbors_umap(X, n_neighbors, random_state, metric, metric_kwds, angular, verbose). 273 # umap 0.5.0. 274 warnings.filterwarnings(""ignore"", message=r""Tensorflow not installed""). --> 275 from umap.umap_ import nearest_neighbors. 276 . 277 random_state = check_random_state(random_state). ~/.local/lib/python3.9/site-packages/umap/umap_.py in <module>. 45 ). 46 . ---> 47 from pynndescent import NNDescent. 48 from pynndescent.distances import named_distances as pynn_named_distances. 49 from pynndescent.sparse import sparse_named_distances as pynn_sparse_named_distances. ~/.local/lib/python3.9/site-packages/pynndescent/__init__.py in <module>. 1 import pkg_resources. 2 import numba. ----> 3 from .pynndescent_ import NNDescent, PyNNDescentTransformer. 4 . 5 # Workaround: https://github.com/numba/numba/issues/3341. ~/.local/lib/python3.9/site-packages/pynndescent/pynndescent_.py in <module>. 19 import heapq. 20 . ---> 21 import pynndescent.sparse as sparse. 22 import pynndescent.sparse_nndescent as sparse_nnd. 23 import pynndescent.distances as pynnd_dist. ~/.local/lib/python3.9/site-packages/pynndescent/sparse.py in <module>. 341 },. 342 ). --> 343 def sparse_alternative_jaccard(ind1, data1, ind2, data2):. 344 num_non_zero = arr_union(ind1, ind2).shape[0]. 345 num_equal = arr_intersect(ind1, ind2).shape[0]. ~/.local/lib/python3.9/site-packages/numba/core/decorators.py in wrapper(func). 216 with typeinfer.register_dispatcher(disp):. 217 for sig in sigs:. --> 218 disp.compile(sig). 219 disp.disable_compile(). 220 return disp. ~/.local/lib/python3.9/site-packages/numba/core/compiler_lock.py in _acquire_compile_lock(*args, **kwargs). 30 def _acquire_compile_lock(*args, **kwargs):. 31 with self:. ---> 32 return func(*args, **kwargs). 33 return _acquire_compile_lock. 34 . ~/.local/lib/python3.9/site-packages/numba/core/dispatcher.py",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:2835,modifiability,modul,module,2835,"ds). 736 # very cautious here. ~/.local/lib/python3.9/site-packages/scanpy/neighbors/__init__.py in compute_neighbors_umap(X, n_neighbors, random_state, metric, metric_kwds, angular, verbose). 273 # umap 0.5.0. 274 warnings.filterwarnings(""ignore"", message=r""Tensorflow not installed""). --> 275 from umap.umap_ import nearest_neighbors. 276 . 277 random_state = check_random_state(random_state). ~/.local/lib/python3.9/site-packages/umap/umap_.py in <module>. 45 ). 46 . ---> 47 from pynndescent import NNDescent. 48 from pynndescent.distances import named_distances as pynn_named_distances. 49 from pynndescent.sparse import sparse_named_distances as pynn_sparse_named_distances. ~/.local/lib/python3.9/site-packages/pynndescent/__init__.py in <module>. 1 import pkg_resources. 2 import numba. ----> 3 from .pynndescent_ import NNDescent, PyNNDescentTransformer. 4 . 5 # Workaround: https://github.com/numba/numba/issues/3341. ~/.local/lib/python3.9/site-packages/pynndescent/pynndescent_.py in <module>. 19 import heapq. 20 . ---> 21 import pynndescent.sparse as sparse. 22 import pynndescent.sparse_nndescent as sparse_nnd. 23 import pynndescent.distances as pynnd_dist. ~/.local/lib/python3.9/site-packages/pynndescent/sparse.py in <module>. 341 },. 342 ). --> 343 def sparse_alternative_jaccard(ind1, data1, ind2, data2):. 344 num_non_zero = arr_union(ind1, ind2).shape[0]. 345 num_equal = arr_intersect(ind1, ind2).shape[0]. ~/.local/lib/python3.9/site-packages/numba/core/decorators.py in wrapper(func). 216 with typeinfer.register_dispatcher(disp):. 217 for sig in sigs:. --> 218 disp.compile(sig). 219 disp.disable_compile(). 220 return disp. ~/.local/lib/python3.9/site-packages/numba/core/compiler_lock.py in _acquire_compile_lock(*args, **kwargs). 30 def _acquire_compile_lock(*args, **kwargs):. 31 with self:. ---> 32 return func(*args, **kwargs). 33 return _acquire_compile_lock. 34 . ~/.local/lib/python3.9/site-packages/numba/core/dispatcher.py in compile(self, sig). 817 self._cache_",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:3040,modifiability,pac,packages,3040,"5.0. 274 warnings.filterwarnings(""ignore"", message=r""Tensorflow not installed""). --> 275 from umap.umap_ import nearest_neighbors. 276 . 277 random_state = check_random_state(random_state). ~/.local/lib/python3.9/site-packages/umap/umap_.py in <module>. 45 ). 46 . ---> 47 from pynndescent import NNDescent. 48 from pynndescent.distances import named_distances as pynn_named_distances. 49 from pynndescent.sparse import sparse_named_distances as pynn_sparse_named_distances. ~/.local/lib/python3.9/site-packages/pynndescent/__init__.py in <module>. 1 import pkg_resources. 2 import numba. ----> 3 from .pynndescent_ import NNDescent, PyNNDescentTransformer. 4 . 5 # Workaround: https://github.com/numba/numba/issues/3341. ~/.local/lib/python3.9/site-packages/pynndescent/pynndescent_.py in <module>. 19 import heapq. 20 . ---> 21 import pynndescent.sparse as sparse. 22 import pynndescent.sparse_nndescent as sparse_nnd. 23 import pynndescent.distances as pynnd_dist. ~/.local/lib/python3.9/site-packages/pynndescent/sparse.py in <module>. 341 },. 342 ). --> 343 def sparse_alternative_jaccard(ind1, data1, ind2, data2):. 344 num_non_zero = arr_union(ind1, ind2).shape[0]. 345 num_equal = arr_intersect(ind1, ind2).shape[0]. ~/.local/lib/python3.9/site-packages/numba/core/decorators.py in wrapper(func). 216 with typeinfer.register_dispatcher(disp):. 217 for sig in sigs:. --> 218 disp.compile(sig). 219 disp.disable_compile(). 220 return disp. ~/.local/lib/python3.9/site-packages/numba/core/compiler_lock.py in _acquire_compile_lock(*args, **kwargs). 30 def _acquire_compile_lock(*args, **kwargs):. 31 with self:. ---> 32 return func(*args, **kwargs). 33 return _acquire_compile_lock. 34 . ~/.local/lib/python3.9/site-packages/numba/core/dispatcher.py in compile(self, sig). 817 self._cache_misses[sig] += 1. 818 try:. --> 819 cres = self._compiler.compile(args, return_type). 820 except errors.ForceLiteralArg as e:. 821 def folded(args, kws):. ~/.local/lib/python3.9/site-packages/numba/core/dis",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:3075,modifiability,modul,module,3075,"ignore"", message=r""Tensorflow not installed""). --> 275 from umap.umap_ import nearest_neighbors. 276 . 277 random_state = check_random_state(random_state). ~/.local/lib/python3.9/site-packages/umap/umap_.py in <module>. 45 ). 46 . ---> 47 from pynndescent import NNDescent. 48 from pynndescent.distances import named_distances as pynn_named_distances. 49 from pynndescent.sparse import sparse_named_distances as pynn_sparse_named_distances. ~/.local/lib/python3.9/site-packages/pynndescent/__init__.py in <module>. 1 import pkg_resources. 2 import numba. ----> 3 from .pynndescent_ import NNDescent, PyNNDescentTransformer. 4 . 5 # Workaround: https://github.com/numba/numba/issues/3341. ~/.local/lib/python3.9/site-packages/pynndescent/pynndescent_.py in <module>. 19 import heapq. 20 . ---> 21 import pynndescent.sparse as sparse. 22 import pynndescent.sparse_nndescent as sparse_nnd. 23 import pynndescent.distances as pynnd_dist. ~/.local/lib/python3.9/site-packages/pynndescent/sparse.py in <module>. 341 },. 342 ). --> 343 def sparse_alternative_jaccard(ind1, data1, ind2, data2):. 344 num_non_zero = arr_union(ind1, ind2).shape[0]. 345 num_equal = arr_intersect(ind1, ind2).shape[0]. ~/.local/lib/python3.9/site-packages/numba/core/decorators.py in wrapper(func). 216 with typeinfer.register_dispatcher(disp):. 217 for sig in sigs:. --> 218 disp.compile(sig). 219 disp.disable_compile(). 220 return disp. ~/.local/lib/python3.9/site-packages/numba/core/compiler_lock.py in _acquire_compile_lock(*args, **kwargs). 30 def _acquire_compile_lock(*args, **kwargs):. 31 with self:. ---> 32 return func(*args, **kwargs). 33 return _acquire_compile_lock. 34 . ~/.local/lib/python3.9/site-packages/numba/core/dispatcher.py in compile(self, sig). 817 self._cache_misses[sig] += 1. 818 try:. --> 819 cres = self._compiler.compile(args, return_type). 820 except errors.ForceLiteralArg as e:. 821 def folded(args, kws):. ~/.local/lib/python3.9/site-packages/numba/core/dispatcher.py in compile(self, args, ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:3297,modifiability,pac,packages,3297,"). 46 . ---> 47 from pynndescent import NNDescent. 48 from pynndescent.distances import named_distances as pynn_named_distances. 49 from pynndescent.sparse import sparse_named_distances as pynn_sparse_named_distances. ~/.local/lib/python3.9/site-packages/pynndescent/__init__.py in <module>. 1 import pkg_resources. 2 import numba. ----> 3 from .pynndescent_ import NNDescent, PyNNDescentTransformer. 4 . 5 # Workaround: https://github.com/numba/numba/issues/3341. ~/.local/lib/python3.9/site-packages/pynndescent/pynndescent_.py in <module>. 19 import heapq. 20 . ---> 21 import pynndescent.sparse as sparse. 22 import pynndescent.sparse_nndescent as sparse_nnd. 23 import pynndescent.distances as pynnd_dist. ~/.local/lib/python3.9/site-packages/pynndescent/sparse.py in <module>. 341 },. 342 ). --> 343 def sparse_alternative_jaccard(ind1, data1, ind2, data2):. 344 num_non_zero = arr_union(ind1, ind2).shape[0]. 345 num_equal = arr_intersect(ind1, ind2).shape[0]. ~/.local/lib/python3.9/site-packages/numba/core/decorators.py in wrapper(func). 216 with typeinfer.register_dispatcher(disp):. 217 for sig in sigs:. --> 218 disp.compile(sig). 219 disp.disable_compile(). 220 return disp. ~/.local/lib/python3.9/site-packages/numba/core/compiler_lock.py in _acquire_compile_lock(*args, **kwargs). 30 def _acquire_compile_lock(*args, **kwargs):. 31 with self:. ---> 32 return func(*args, **kwargs). 33 return _acquire_compile_lock. 34 . ~/.local/lib/python3.9/site-packages/numba/core/dispatcher.py in compile(self, sig). 817 self._cache_misses[sig] += 1. 818 try:. --> 819 cres = self._compiler.compile(args, return_type). 820 except errors.ForceLiteralArg as e:. 821 def folded(args, kws):. ~/.local/lib/python3.9/site-packages/numba/core/dispatcher.py in compile(self, args, return_type). 80 return retval. 81 else:. ---> 82 raise retval. 83 . 84 def _compile_cached(self, args, return_type):. ~/.local/lib/python3.9/site-packages/numba/core/dispatcher.py in _compile_cached(self, args, return_type",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:3317,modifiability,deco,decorators,3317,"pynndescent import NNDescent. 48 from pynndescent.distances import named_distances as pynn_named_distances. 49 from pynndescent.sparse import sparse_named_distances as pynn_sparse_named_distances. ~/.local/lib/python3.9/site-packages/pynndescent/__init__.py in <module>. 1 import pkg_resources. 2 import numba. ----> 3 from .pynndescent_ import NNDescent, PyNNDescentTransformer. 4 . 5 # Workaround: https://github.com/numba/numba/issues/3341. ~/.local/lib/python3.9/site-packages/pynndescent/pynndescent_.py in <module>. 19 import heapq. 20 . ---> 21 import pynndescent.sparse as sparse. 22 import pynndescent.sparse_nndescent as sparse_nnd. 23 import pynndescent.distances as pynnd_dist. ~/.local/lib/python3.9/site-packages/pynndescent/sparse.py in <module>. 341 },. 342 ). --> 343 def sparse_alternative_jaccard(ind1, data1, ind2, data2):. 344 num_non_zero = arr_union(ind1, ind2).shape[0]. 345 num_equal = arr_intersect(ind1, ind2).shape[0]. ~/.local/lib/python3.9/site-packages/numba/core/decorators.py in wrapper(func). 216 with typeinfer.register_dispatcher(disp):. 217 for sig in sigs:. --> 218 disp.compile(sig). 219 disp.disable_compile(). 220 return disp. ~/.local/lib/python3.9/site-packages/numba/core/compiler_lock.py in _acquire_compile_lock(*args, **kwargs). 30 def _acquire_compile_lock(*args, **kwargs):. 31 with self:. ---> 32 return func(*args, **kwargs). 33 return _acquire_compile_lock. 34 . ~/.local/lib/python3.9/site-packages/numba/core/dispatcher.py in compile(self, sig). 817 self._cache_misses[sig] += 1. 818 try:. --> 819 cres = self._compiler.compile(args, return_type). 820 except errors.ForceLiteralArg as e:. 821 def folded(args, kws):. ~/.local/lib/python3.9/site-packages/numba/core/dispatcher.py in compile(self, args, return_type). 80 return retval. 81 else:. ---> 82 raise retval. 83 . 84 def _compile_cached(self, args, return_type):. ~/.local/lib/python3.9/site-packages/numba/core/dispatcher.py in _compile_cached(self, args, return_type). 90 . 91 try:. --->",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:3518,modifiability,pac,packages,3518,"local/lib/python3.9/site-packages/pynndescent/__init__.py in <module>. 1 import pkg_resources. 2 import numba. ----> 3 from .pynndescent_ import NNDescent, PyNNDescentTransformer. 4 . 5 # Workaround: https://github.com/numba/numba/issues/3341. ~/.local/lib/python3.9/site-packages/pynndescent/pynndescent_.py in <module>. 19 import heapq. 20 . ---> 21 import pynndescent.sparse as sparse. 22 import pynndescent.sparse_nndescent as sparse_nnd. 23 import pynndescent.distances as pynnd_dist. ~/.local/lib/python3.9/site-packages/pynndescent/sparse.py in <module>. 341 },. 342 ). --> 343 def sparse_alternative_jaccard(ind1, data1, ind2, data2):. 344 num_non_zero = arr_union(ind1, ind2).shape[0]. 345 num_equal = arr_intersect(ind1, ind2).shape[0]. ~/.local/lib/python3.9/site-packages/numba/core/decorators.py in wrapper(func). 216 with typeinfer.register_dispatcher(disp):. 217 for sig in sigs:. --> 218 disp.compile(sig). 219 disp.disable_compile(). 220 return disp. ~/.local/lib/python3.9/site-packages/numba/core/compiler_lock.py in _acquire_compile_lock(*args, **kwargs). 30 def _acquire_compile_lock(*args, **kwargs):. 31 with self:. ---> 32 return func(*args, **kwargs). 33 return _acquire_compile_lock. 34 . ~/.local/lib/python3.9/site-packages/numba/core/dispatcher.py in compile(self, sig). 817 self._cache_misses[sig] += 1. 818 try:. --> 819 cres = self._compiler.compile(args, return_type). 820 except errors.ForceLiteralArg as e:. 821 def folded(args, kws):. ~/.local/lib/python3.9/site-packages/numba/core/dispatcher.py in compile(self, args, return_type). 80 return retval. 81 else:. ---> 82 raise retval. 83 . 84 def _compile_cached(self, args, return_type):. ~/.local/lib/python3.9/site-packages/numba/core/dispatcher.py in _compile_cached(self, args, return_type). 90 . 91 try:. ---> 92 retval = self._compile_core(args, return_type). 93 except errors.TypingError as e:. 94 self._failed_cache[key] = e. ~/.local/lib/python3.9/site-packages/numba/core/dispatcher.py in _compile_core(s",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:3765,modifiability,pac,packages,3765,"local/lib/python3.9/site-packages/pynndescent/pynndescent_.py in <module>. 19 import heapq. 20 . ---> 21 import pynndescent.sparse as sparse. 22 import pynndescent.sparse_nndescent as sparse_nnd. 23 import pynndescent.distances as pynnd_dist. ~/.local/lib/python3.9/site-packages/pynndescent/sparse.py in <module>. 341 },. 342 ). --> 343 def sparse_alternative_jaccard(ind1, data1, ind2, data2):. 344 num_non_zero = arr_union(ind1, ind2).shape[0]. 345 num_equal = arr_intersect(ind1, ind2).shape[0]. ~/.local/lib/python3.9/site-packages/numba/core/decorators.py in wrapper(func). 216 with typeinfer.register_dispatcher(disp):. 217 for sig in sigs:. --> 218 disp.compile(sig). 219 disp.disable_compile(). 220 return disp. ~/.local/lib/python3.9/site-packages/numba/core/compiler_lock.py in _acquire_compile_lock(*args, **kwargs). 30 def _acquire_compile_lock(*args, **kwargs):. 31 with self:. ---> 32 return func(*args, **kwargs). 33 return _acquire_compile_lock. 34 . ~/.local/lib/python3.9/site-packages/numba/core/dispatcher.py in compile(self, sig). 817 self._cache_misses[sig] += 1. 818 try:. --> 819 cres = self._compiler.compile(args, return_type). 820 except errors.ForceLiteralArg as e:. 821 def folded(args, kws):. ~/.local/lib/python3.9/site-packages/numba/core/dispatcher.py in compile(self, args, return_type). 80 return retval. 81 else:. ---> 82 raise retval. 83 . 84 def _compile_cached(self, args, return_type):. ~/.local/lib/python3.9/site-packages/numba/core/dispatcher.py in _compile_cached(self, args, return_type). 90 . 91 try:. ---> 92 retval = self._compile_core(args, return_type). 93 except errors.TypingError as e:. 94 self._failed_cache[key] = e. ~/.local/lib/python3.9/site-packages/numba/core/dispatcher.py in _compile_core(self, args, return_type). 103 . 104 impl = self._get_implementation(args, {}). --> 105 cres = compiler.compile_extra(self.targetdescr.typing_context,. 106 self.targetdescr.target_context,. 107 impl,. ~/.local/lib/python3.9/site-packages/numba/core/",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:4021,modifiability,pac,packages,4021,"python3.9/site-packages/pynndescent/sparse.py in <module>. 341 },. 342 ). --> 343 def sparse_alternative_jaccard(ind1, data1, ind2, data2):. 344 num_non_zero = arr_union(ind1, ind2).shape[0]. 345 num_equal = arr_intersect(ind1, ind2).shape[0]. ~/.local/lib/python3.9/site-packages/numba/core/decorators.py in wrapper(func). 216 with typeinfer.register_dispatcher(disp):. 217 for sig in sigs:. --> 218 disp.compile(sig). 219 disp.disable_compile(). 220 return disp. ~/.local/lib/python3.9/site-packages/numba/core/compiler_lock.py in _acquire_compile_lock(*args, **kwargs). 30 def _acquire_compile_lock(*args, **kwargs):. 31 with self:. ---> 32 return func(*args, **kwargs). 33 return _acquire_compile_lock. 34 . ~/.local/lib/python3.9/site-packages/numba/core/dispatcher.py in compile(self, sig). 817 self._cache_misses[sig] += 1. 818 try:. --> 819 cres = self._compiler.compile(args, return_type). 820 except errors.ForceLiteralArg as e:. 821 def folded(args, kws):. ~/.local/lib/python3.9/site-packages/numba/core/dispatcher.py in compile(self, args, return_type). 80 return retval. 81 else:. ---> 82 raise retval. 83 . 84 def _compile_cached(self, args, return_type):. ~/.local/lib/python3.9/site-packages/numba/core/dispatcher.py in _compile_cached(self, args, return_type). 90 . 91 try:. ---> 92 retval = self._compile_core(args, return_type). 93 except errors.TypingError as e:. 94 self._failed_cache[key] = e. ~/.local/lib/python3.9/site-packages/numba/core/dispatcher.py in _compile_core(self, args, return_type). 103 . 104 impl = self._get_implementation(args, {}). --> 105 cres = compiler.compile_extra(self.targetdescr.typing_context,. 106 self.targetdescr.target_context,. 107 impl,. ~/.local/lib/python3.9/site-packages/numba/core/compiler.py in compile_extra(typingctx, targetctx, func, args, return_type, flags, locals, library, pipeline_class). 625 pipeline = pipeline_class(typingctx, targetctx, library,. 626 args, return_type, flags, locals). --> 627 return pipeline.compile_extra(",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:4225,modifiability,pac,packages,4225,"l = arr_intersect(ind1, ind2).shape[0]. ~/.local/lib/python3.9/site-packages/numba/core/decorators.py in wrapper(func). 216 with typeinfer.register_dispatcher(disp):. 217 for sig in sigs:. --> 218 disp.compile(sig). 219 disp.disable_compile(). 220 return disp. ~/.local/lib/python3.9/site-packages/numba/core/compiler_lock.py in _acquire_compile_lock(*args, **kwargs). 30 def _acquire_compile_lock(*args, **kwargs):. 31 with self:. ---> 32 return func(*args, **kwargs). 33 return _acquire_compile_lock. 34 . ~/.local/lib/python3.9/site-packages/numba/core/dispatcher.py in compile(self, sig). 817 self._cache_misses[sig] += 1. 818 try:. --> 819 cres = self._compiler.compile(args, return_type). 820 except errors.ForceLiteralArg as e:. 821 def folded(args, kws):. ~/.local/lib/python3.9/site-packages/numba/core/dispatcher.py in compile(self, args, return_type). 80 return retval. 81 else:. ---> 82 raise retval. 83 . 84 def _compile_cached(self, args, return_type):. ~/.local/lib/python3.9/site-packages/numba/core/dispatcher.py in _compile_cached(self, args, return_type). 90 . 91 try:. ---> 92 retval = self._compile_core(args, return_type). 93 except errors.TypingError as e:. 94 self._failed_cache[key] = e. ~/.local/lib/python3.9/site-packages/numba/core/dispatcher.py in _compile_core(self, args, return_type). 103 . 104 impl = self._get_implementation(args, {}). --> 105 cres = compiler.compile_extra(self.targetdescr.typing_context,. 106 self.targetdescr.target_context,. 107 impl,. ~/.local/lib/python3.9/site-packages/numba/core/compiler.py in compile_extra(typingctx, targetctx, func, args, return_type, flags, locals, library, pipeline_class). 625 pipeline = pipeline_class(typingctx, targetctx, library,. 626 args, return_type, flags, locals). --> 627 return pipeline.compile_extra(func). 628 . 629 . ~/.local/lib/python3.9/site-packages/numba/core/compiler.py in compile_extra(self, func). 361 self.state.lifted = (). 362 self.state.lifted_from = None. --> 363 return self._compile_byt",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:4470,modifiability,pac,packages,4470,"20 return disp. ~/.local/lib/python3.9/site-packages/numba/core/compiler_lock.py in _acquire_compile_lock(*args, **kwargs). 30 def _acquire_compile_lock(*args, **kwargs):. 31 with self:. ---> 32 return func(*args, **kwargs). 33 return _acquire_compile_lock. 34 . ~/.local/lib/python3.9/site-packages/numba/core/dispatcher.py in compile(self, sig). 817 self._cache_misses[sig] += 1. 818 try:. --> 819 cres = self._compiler.compile(args, return_type). 820 except errors.ForceLiteralArg as e:. 821 def folded(args, kws):. ~/.local/lib/python3.9/site-packages/numba/core/dispatcher.py in compile(self, args, return_type). 80 return retval. 81 else:. ---> 82 raise retval. 83 . 84 def _compile_cached(self, args, return_type):. ~/.local/lib/python3.9/site-packages/numba/core/dispatcher.py in _compile_cached(self, args, return_type). 90 . 91 try:. ---> 92 retval = self._compile_core(args, return_type). 93 except errors.TypingError as e:. 94 self._failed_cache[key] = e. ~/.local/lib/python3.9/site-packages/numba/core/dispatcher.py in _compile_core(self, args, return_type). 103 . 104 impl = self._get_implementation(args, {}). --> 105 cres = compiler.compile_extra(self.targetdescr.typing_context,. 106 self.targetdescr.target_context,. 107 impl,. ~/.local/lib/python3.9/site-packages/numba/core/compiler.py in compile_extra(typingctx, targetctx, func, args, return_type, flags, locals, library, pipeline_class). 625 pipeline = pipeline_class(typingctx, targetctx, library,. 626 args, return_type, flags, locals). --> 627 return pipeline.compile_extra(func). 628 . 629 . ~/.local/lib/python3.9/site-packages/numba/core/compiler.py in compile_extra(self, func). 361 self.state.lifted = (). 362 self.state.lifted_from = None. --> 363 return self._compile_bytecode(). 364 . 365 def compile_ir(self, func_ir, lifted=(), lifted_from=None):. ~/.local/lib/python3.9/site-packages/numba/core/compiler.py in _compile_bytecode(self). 423 """""". 424 assert self.state.func_ir is None. --> 425 return self._compile_",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:4749,modifiability,pac,packages,4749,"hon3.9/site-packages/numba/core/dispatcher.py in compile(self, sig). 817 self._cache_misses[sig] += 1. 818 try:. --> 819 cres = self._compiler.compile(args, return_type). 820 except errors.ForceLiteralArg as e:. 821 def folded(args, kws):. ~/.local/lib/python3.9/site-packages/numba/core/dispatcher.py in compile(self, args, return_type). 80 return retval. 81 else:. ---> 82 raise retval. 83 . 84 def _compile_cached(self, args, return_type):. ~/.local/lib/python3.9/site-packages/numba/core/dispatcher.py in _compile_cached(self, args, return_type). 90 . 91 try:. ---> 92 retval = self._compile_core(args, return_type). 93 except errors.TypingError as e:. 94 self._failed_cache[key] = e. ~/.local/lib/python3.9/site-packages/numba/core/dispatcher.py in _compile_core(self, args, return_type). 103 . 104 impl = self._get_implementation(args, {}). --> 105 cres = compiler.compile_extra(self.targetdescr.typing_context,. 106 self.targetdescr.target_context,. 107 impl,. ~/.local/lib/python3.9/site-packages/numba/core/compiler.py in compile_extra(typingctx, targetctx, func, args, return_type, flags, locals, library, pipeline_class). 625 pipeline = pipeline_class(typingctx, targetctx, library,. 626 args, return_type, flags, locals). --> 627 return pipeline.compile_extra(func). 628 . 629 . ~/.local/lib/python3.9/site-packages/numba/core/compiler.py in compile_extra(self, func). 361 self.state.lifted = (). 362 self.state.lifted_from = None. --> 363 return self._compile_bytecode(). 364 . 365 def compile_ir(self, func_ir, lifted=(), lifted_from=None):. ~/.local/lib/python3.9/site-packages/numba/core/compiler.py in _compile_bytecode(self). 423 """""". 424 assert self.state.func_ir is None. --> 425 return self._compile_core(). 426 . 427 def _compile_ir(self):. ~/.local/lib/python3.9/site-packages/numba/core/compiler.py in _compile_core(self). 403 self.state.status.fail_reason = e. 404 if is_final_pipeline:. --> 405 raise e. 406 else:. 407 raise CompilerError(""All available pipelines exhausted""",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:5072,modifiability,pac,packages,5072,", return_type). 80 return retval. 81 else:. ---> 82 raise retval. 83 . 84 def _compile_cached(self, args, return_type):. ~/.local/lib/python3.9/site-packages/numba/core/dispatcher.py in _compile_cached(self, args, return_type). 90 . 91 try:. ---> 92 retval = self._compile_core(args, return_type). 93 except errors.TypingError as e:. 94 self._failed_cache[key] = e. ~/.local/lib/python3.9/site-packages/numba/core/dispatcher.py in _compile_core(self, args, return_type). 103 . 104 impl = self._get_implementation(args, {}). --> 105 cres = compiler.compile_extra(self.targetdescr.typing_context,. 106 self.targetdescr.target_context,. 107 impl,. ~/.local/lib/python3.9/site-packages/numba/core/compiler.py in compile_extra(typingctx, targetctx, func, args, return_type, flags, locals, library, pipeline_class). 625 pipeline = pipeline_class(typingctx, targetctx, library,. 626 args, return_type, flags, locals). --> 627 return pipeline.compile_extra(func). 628 . 629 . ~/.local/lib/python3.9/site-packages/numba/core/compiler.py in compile_extra(self, func). 361 self.state.lifted = (). 362 self.state.lifted_from = None. --> 363 return self._compile_bytecode(). 364 . 365 def compile_ir(self, func_ir, lifted=(), lifted_from=None):. ~/.local/lib/python3.9/site-packages/numba/core/compiler.py in _compile_bytecode(self). 423 """""". 424 assert self.state.func_ir is None. --> 425 return self._compile_core(). 426 . 427 def _compile_ir(self):. ~/.local/lib/python3.9/site-packages/numba/core/compiler.py in _compile_core(self). 403 self.state.status.fail_reason = e. 404 if is_final_pipeline:. --> 405 raise e. 406 else:. 407 raise CompilerError(""All available pipelines exhausted""). ~/.local/lib/python3.9/site-packages/numba/core/compiler.py in _compile_core(self). 394 res = None. 395 try:. --> 396 pm.run(self.state). 397 if self.state.cr is not None:. 398 break. ~/.local/lib/python3.9/site-packages/numba/core/compiler_machinery.py in run(self, state). 339 (self.pipeline_name, pass_desc). 340 patc",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:5337,modifiability,pac,packages,5337,"compile_core(args, return_type). 93 except errors.TypingError as e:. 94 self._failed_cache[key] = e. ~/.local/lib/python3.9/site-packages/numba/core/dispatcher.py in _compile_core(self, args, return_type). 103 . 104 impl = self._get_implementation(args, {}). --> 105 cres = compiler.compile_extra(self.targetdescr.typing_context,. 106 self.targetdescr.target_context,. 107 impl,. ~/.local/lib/python3.9/site-packages/numba/core/compiler.py in compile_extra(typingctx, targetctx, func, args, return_type, flags, locals, library, pipeline_class). 625 pipeline = pipeline_class(typingctx, targetctx, library,. 626 args, return_type, flags, locals). --> 627 return pipeline.compile_extra(func). 628 . 629 . ~/.local/lib/python3.9/site-packages/numba/core/compiler.py in compile_extra(self, func). 361 self.state.lifted = (). 362 self.state.lifted_from = None. --> 363 return self._compile_bytecode(). 364 . 365 def compile_ir(self, func_ir, lifted=(), lifted_from=None):. ~/.local/lib/python3.9/site-packages/numba/core/compiler.py in _compile_bytecode(self). 423 """""". 424 assert self.state.func_ir is None. --> 425 return self._compile_core(). 426 . 427 def _compile_ir(self):. ~/.local/lib/python3.9/site-packages/numba/core/compiler.py in _compile_core(self). 403 self.state.status.fail_reason = e. 404 if is_final_pipeline:. --> 405 raise e. 406 else:. 407 raise CompilerError(""All available pipelines exhausted""). ~/.local/lib/python3.9/site-packages/numba/core/compiler.py in _compile_core(self). 394 res = None. 395 try:. --> 396 pm.run(self.state). 397 if self.state.cr is not None:. 398 break. ~/.local/lib/python3.9/site-packages/numba/core/compiler_machinery.py in run(self, state). 339 (self.pipeline_name, pass_desc). 340 patched_exception = self._patch_error(msg, e). --> 341 raise patched_exception. 342 . 343 def dependency_analysis(self):. ~/.local/lib/python3.9/site-packages/numba/core/compiler_machinery.py in run(self, state). 330 pass_inst = _pass_registry.get(pss).pass_inst. 331 i",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:5544,modifiability,pac,packages,5544,"03 . 104 impl = self._get_implementation(args, {}). --> 105 cres = compiler.compile_extra(self.targetdescr.typing_context,. 106 self.targetdescr.target_context,. 107 impl,. ~/.local/lib/python3.9/site-packages/numba/core/compiler.py in compile_extra(typingctx, targetctx, func, args, return_type, flags, locals, library, pipeline_class). 625 pipeline = pipeline_class(typingctx, targetctx, library,. 626 args, return_type, flags, locals). --> 627 return pipeline.compile_extra(func). 628 . 629 . ~/.local/lib/python3.9/site-packages/numba/core/compiler.py in compile_extra(self, func). 361 self.state.lifted = (). 362 self.state.lifted_from = None. --> 363 return self._compile_bytecode(). 364 . 365 def compile_ir(self, func_ir, lifted=(), lifted_from=None):. ~/.local/lib/python3.9/site-packages/numba/core/compiler.py in _compile_bytecode(self). 423 """""". 424 assert self.state.func_ir is None. --> 425 return self._compile_core(). 426 . 427 def _compile_ir(self):. ~/.local/lib/python3.9/site-packages/numba/core/compiler.py in _compile_core(self). 403 self.state.status.fail_reason = e. 404 if is_final_pipeline:. --> 405 raise e. 406 else:. 407 raise CompilerError(""All available pipelines exhausted""). ~/.local/lib/python3.9/site-packages/numba/core/compiler.py in _compile_core(self). 394 res = None. 395 try:. --> 396 pm.run(self.state). 397 if self.state.cr is not None:. 398 break. ~/.local/lib/python3.9/site-packages/numba/core/compiler_machinery.py in run(self, state). 339 (self.pipeline_name, pass_desc). 340 patched_exception = self._patch_error(msg, e). --> 341 raise patched_exception. 342 . 343 def dependency_analysis(self):. ~/.local/lib/python3.9/site-packages/numba/core/compiler_machinery.py in run(self, state). 330 pass_inst = _pass_registry.get(pss).pass_inst. 331 if isinstance(pass_inst, CompilerPass):. --> 332 self._runPass(idx, pass_inst, state). 333 else:. 334 raise BaseException(""Legacy pass in use""). ~/.local/lib/python3.9/site-packages/numba/core/compiler_lock.p",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:5784,modifiability,pac,packages,5784,"ile_extra(typingctx, targetctx, func, args, return_type, flags, locals, library, pipeline_class). 625 pipeline = pipeline_class(typingctx, targetctx, library,. 626 args, return_type, flags, locals). --> 627 return pipeline.compile_extra(func). 628 . 629 . ~/.local/lib/python3.9/site-packages/numba/core/compiler.py in compile_extra(self, func). 361 self.state.lifted = (). 362 self.state.lifted_from = None. --> 363 return self._compile_bytecode(). 364 . 365 def compile_ir(self, func_ir, lifted=(), lifted_from=None):. ~/.local/lib/python3.9/site-packages/numba/core/compiler.py in _compile_bytecode(self). 423 """""". 424 assert self.state.func_ir is None. --> 425 return self._compile_core(). 426 . 427 def _compile_ir(self):. ~/.local/lib/python3.9/site-packages/numba/core/compiler.py in _compile_core(self). 403 self.state.status.fail_reason = e. 404 if is_final_pipeline:. --> 405 raise e. 406 else:. 407 raise CompilerError(""All available pipelines exhausted""). ~/.local/lib/python3.9/site-packages/numba/core/compiler.py in _compile_core(self). 394 res = None. 395 try:. --> 396 pm.run(self.state). 397 if self.state.cr is not None:. 398 break. ~/.local/lib/python3.9/site-packages/numba/core/compiler_machinery.py in run(self, state). 339 (self.pipeline_name, pass_desc). 340 patched_exception = self._patch_error(msg, e). --> 341 raise patched_exception. 342 . 343 def dependency_analysis(self):. ~/.local/lib/python3.9/site-packages/numba/core/compiler_machinery.py in run(self, state). 330 pass_inst = _pass_registry.get(pss).pass_inst. 331 if isinstance(pass_inst, CompilerPass):. --> 332 self._runPass(idx, pass_inst, state). 333 else:. 334 raise BaseException(""Legacy pass in use""). ~/.local/lib/python3.9/site-packages/numba/core/compiler_lock.py in _acquire_compile_lock(*args, **kwargs). 30 def _acquire_compile_lock(*args, **kwargs):. 31 with self:. ---> 32 return func(*args, **kwargs). 33 return _acquire_compile_lock. 34 . ~/.local/lib/python3.9/site-packages/numba/core/compiler",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:5968,modifiability,pac,packages,5968,"lags, locals). --> 627 return pipeline.compile_extra(func). 628 . 629 . ~/.local/lib/python3.9/site-packages/numba/core/compiler.py in compile_extra(self, func). 361 self.state.lifted = (). 362 self.state.lifted_from = None. --> 363 return self._compile_bytecode(). 364 . 365 def compile_ir(self, func_ir, lifted=(), lifted_from=None):. ~/.local/lib/python3.9/site-packages/numba/core/compiler.py in _compile_bytecode(self). 423 """""". 424 assert self.state.func_ir is None. --> 425 return self._compile_core(). 426 . 427 def _compile_ir(self):. ~/.local/lib/python3.9/site-packages/numba/core/compiler.py in _compile_core(self). 403 self.state.status.fail_reason = e. 404 if is_final_pipeline:. --> 405 raise e. 406 else:. 407 raise CompilerError(""All available pipelines exhausted""). ~/.local/lib/python3.9/site-packages/numba/core/compiler.py in _compile_core(self). 394 res = None. 395 try:. --> 396 pm.run(self.state). 397 if self.state.cr is not None:. 398 break. ~/.local/lib/python3.9/site-packages/numba/core/compiler_machinery.py in run(self, state). 339 (self.pipeline_name, pass_desc). 340 patched_exception = self._patch_error(msg, e). --> 341 raise patched_exception. 342 . 343 def dependency_analysis(self):. ~/.local/lib/python3.9/site-packages/numba/core/compiler_machinery.py in run(self, state). 330 pass_inst = _pass_registry.get(pss).pass_inst. 331 if isinstance(pass_inst, CompilerPass):. --> 332 self._runPass(idx, pass_inst, state). 333 else:. 334 raise BaseException(""Legacy pass in use""). ~/.local/lib/python3.9/site-packages/numba/core/compiler_lock.py in _acquire_compile_lock(*args, **kwargs). 30 def _acquire_compile_lock(*args, **kwargs):. 31 with self:. ---> 32 return func(*args, **kwargs). 33 return _acquire_compile_lock. 34 . ~/.local/lib/python3.9/site-packages/numba/core/compiler_machinery.py in _runPass(self, index, pss, internal_state). 289 mutated |= check(pss.run_initialization, internal_state). 290 with SimpleTimer() as pass_time:. --> 291 mutated |= chec",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:6222,modifiability,pac,packages,6222,"bytecode(). 364 . 365 def compile_ir(self, func_ir, lifted=(), lifted_from=None):. ~/.local/lib/python3.9/site-packages/numba/core/compiler.py in _compile_bytecode(self). 423 """""". 424 assert self.state.func_ir is None. --> 425 return self._compile_core(). 426 . 427 def _compile_ir(self):. ~/.local/lib/python3.9/site-packages/numba/core/compiler.py in _compile_core(self). 403 self.state.status.fail_reason = e. 404 if is_final_pipeline:. --> 405 raise e. 406 else:. 407 raise CompilerError(""All available pipelines exhausted""). ~/.local/lib/python3.9/site-packages/numba/core/compiler.py in _compile_core(self). 394 res = None. 395 try:. --> 396 pm.run(self.state). 397 if self.state.cr is not None:. 398 break. ~/.local/lib/python3.9/site-packages/numba/core/compiler_machinery.py in run(self, state). 339 (self.pipeline_name, pass_desc). 340 patched_exception = self._patch_error(msg, e). --> 341 raise patched_exception. 342 . 343 def dependency_analysis(self):. ~/.local/lib/python3.9/site-packages/numba/core/compiler_machinery.py in run(self, state). 330 pass_inst = _pass_registry.get(pss).pass_inst. 331 if isinstance(pass_inst, CompilerPass):. --> 332 self._runPass(idx, pass_inst, state). 333 else:. 334 raise BaseException(""Legacy pass in use""). ~/.local/lib/python3.9/site-packages/numba/core/compiler_lock.py in _acquire_compile_lock(*args, **kwargs). 30 def _acquire_compile_lock(*args, **kwargs):. 31 with self:. ---> 32 return func(*args, **kwargs). 33 return _acquire_compile_lock. 34 . ~/.local/lib/python3.9/site-packages/numba/core/compiler_machinery.py in _runPass(self, index, pss, internal_state). 289 mutated |= check(pss.run_initialization, internal_state). 290 with SimpleTimer() as pass_time:. --> 291 mutated |= check(pss.run_pass, internal_state). 292 with SimpleTimer() as finalize_time:. 293 mutated |= check(pss.run_finalizer, internal_state). ~/.local/lib/python3.9/site-packages/numba/core/compiler_machinery.py in check(func, compiler_state). 262 . 263 def check(",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:6513,modifiability,pac,packages,6513,"/.local/lib/python3.9/site-packages/numba/core/compiler.py in _compile_core(self). 403 self.state.status.fail_reason = e. 404 if is_final_pipeline:. --> 405 raise e. 406 else:. 407 raise CompilerError(""All available pipelines exhausted""). ~/.local/lib/python3.9/site-packages/numba/core/compiler.py in _compile_core(self). 394 res = None. 395 try:. --> 396 pm.run(self.state). 397 if self.state.cr is not None:. 398 break. ~/.local/lib/python3.9/site-packages/numba/core/compiler_machinery.py in run(self, state). 339 (self.pipeline_name, pass_desc). 340 patched_exception = self._patch_error(msg, e). --> 341 raise patched_exception. 342 . 343 def dependency_analysis(self):. ~/.local/lib/python3.9/site-packages/numba/core/compiler_machinery.py in run(self, state). 330 pass_inst = _pass_registry.get(pss).pass_inst. 331 if isinstance(pass_inst, CompilerPass):. --> 332 self._runPass(idx, pass_inst, state). 333 else:. 334 raise BaseException(""Legacy pass in use""). ~/.local/lib/python3.9/site-packages/numba/core/compiler_lock.py in _acquire_compile_lock(*args, **kwargs). 30 def _acquire_compile_lock(*args, **kwargs):. 31 with self:. ---> 32 return func(*args, **kwargs). 33 return _acquire_compile_lock. 34 . ~/.local/lib/python3.9/site-packages/numba/core/compiler_machinery.py in _runPass(self, index, pss, internal_state). 289 mutated |= check(pss.run_initialization, internal_state). 290 with SimpleTimer() as pass_time:. --> 291 mutated |= check(pss.run_pass, internal_state). 292 with SimpleTimer() as finalize_time:. 293 mutated |= check(pss.run_finalizer, internal_state). ~/.local/lib/python3.9/site-packages/numba/core/compiler_machinery.py in check(func, compiler_state). 262 . 263 def check(func, compiler_state):. --> 264 mangled = func(compiler_state). 265 if mangled not in (True, False):. 266 msg = (""CompilerPass implementations should return True/False. "". ~/.local/lib/python3.9/site-packages/numba/core/typed_passes.py in run_pass(self, state). 90 % (state.func_id.func_name",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:6760,modifiability,pac,packages,6760,"/lib/python3.9/site-packages/numba/core/compiler.py in _compile_core(self). 394 res = None. 395 try:. --> 396 pm.run(self.state). 397 if self.state.cr is not None:. 398 break. ~/.local/lib/python3.9/site-packages/numba/core/compiler_machinery.py in run(self, state). 339 (self.pipeline_name, pass_desc). 340 patched_exception = self._patch_error(msg, e). --> 341 raise patched_exception. 342 . 343 def dependency_analysis(self):. ~/.local/lib/python3.9/site-packages/numba/core/compiler_machinery.py in run(self, state). 330 pass_inst = _pass_registry.get(pss).pass_inst. 331 if isinstance(pass_inst, CompilerPass):. --> 332 self._runPass(idx, pass_inst, state). 333 else:. 334 raise BaseException(""Legacy pass in use""). ~/.local/lib/python3.9/site-packages/numba/core/compiler_lock.py in _acquire_compile_lock(*args, **kwargs). 30 def _acquire_compile_lock(*args, **kwargs):. 31 with self:. ---> 32 return func(*args, **kwargs). 33 return _acquire_compile_lock. 34 . ~/.local/lib/python3.9/site-packages/numba/core/compiler_machinery.py in _runPass(self, index, pss, internal_state). 289 mutated |= check(pss.run_initialization, internal_state). 290 with SimpleTimer() as pass_time:. --> 291 mutated |= check(pss.run_pass, internal_state). 292 with SimpleTimer() as finalize_time:. 293 mutated |= check(pss.run_finalizer, internal_state). ~/.local/lib/python3.9/site-packages/numba/core/compiler_machinery.py in check(func, compiler_state). 262 . 263 def check(func, compiler_state):. --> 264 mangled = func(compiler_state). 265 if mangled not in (True, False):. 266 msg = (""CompilerPass implementations should return True/False. "". ~/.local/lib/python3.9/site-packages/numba/core/typed_passes.py in run_pass(self, state). 90 % (state.func_id.func_name,)):. 91 # Type inference. ---> 92 typemap, return_type, calltypes = type_inference_stage(. 93 state.typingctx,. 94 state.func_ir,. ~/.local/lib/python3.9/site-packages/numba/core/typed_passes.py in type_inference_stage(typingctx, interp, args, re",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:7132,modifiability,pac,packages,7132,"ched_exception. 342 . 343 def dependency_analysis(self):. ~/.local/lib/python3.9/site-packages/numba/core/compiler_machinery.py in run(self, state). 330 pass_inst = _pass_registry.get(pss).pass_inst. 331 if isinstance(pass_inst, CompilerPass):. --> 332 self._runPass(idx, pass_inst, state). 333 else:. 334 raise BaseException(""Legacy pass in use""). ~/.local/lib/python3.9/site-packages/numba/core/compiler_lock.py in _acquire_compile_lock(*args, **kwargs). 30 def _acquire_compile_lock(*args, **kwargs):. 31 with self:. ---> 32 return func(*args, **kwargs). 33 return _acquire_compile_lock. 34 . ~/.local/lib/python3.9/site-packages/numba/core/compiler_machinery.py in _runPass(self, index, pss, internal_state). 289 mutated |= check(pss.run_initialization, internal_state). 290 with SimpleTimer() as pass_time:. --> 291 mutated |= check(pss.run_pass, internal_state). 292 with SimpleTimer() as finalize_time:. 293 mutated |= check(pss.run_finalizer, internal_state). ~/.local/lib/python3.9/site-packages/numba/core/compiler_machinery.py in check(func, compiler_state). 262 . 263 def check(func, compiler_state):. --> 264 mangled = func(compiler_state). 265 if mangled not in (True, False):. 266 msg = (""CompilerPass implementations should return True/False. "". ~/.local/lib/python3.9/site-packages/numba/core/typed_passes.py in run_pass(self, state). 90 % (state.func_id.func_name,)):. 91 # Type inference. ---> 92 typemap, return_type, calltypes = type_inference_stage(. 93 state.typingctx,. 94 state.func_ir,. ~/.local/lib/python3.9/site-packages/numba/core/typed_passes.py in type_inference_stage(typingctx, interp, args, return_type, locals, raise_errors). 68 . 69 infer.build_constraint(). ---> 70 infer.propagate(raise_errors=raise_errors). 71 typemap, restype, calltypes = infer.unify(raise_errors=raise_errors). 72 . ~/.local/lib/python3.9/site-packages/numba/core/typeinfer.py in propagate(self, raise_errors). 1069 if isinstance(e, ForceLiteralArg)]. 1070 if not force_lit_args:. -> 1071 r",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:7426,modifiability,pac,packages,7426," else:. 334 raise BaseException(""Legacy pass in use""). ~/.local/lib/python3.9/site-packages/numba/core/compiler_lock.py in _acquire_compile_lock(*args, **kwargs). 30 def _acquire_compile_lock(*args, **kwargs):. 31 with self:. ---> 32 return func(*args, **kwargs). 33 return _acquire_compile_lock. 34 . ~/.local/lib/python3.9/site-packages/numba/core/compiler_machinery.py in _runPass(self, index, pss, internal_state). 289 mutated |= check(pss.run_initialization, internal_state). 290 with SimpleTimer() as pass_time:. --> 291 mutated |= check(pss.run_pass, internal_state). 292 with SimpleTimer() as finalize_time:. 293 mutated |= check(pss.run_finalizer, internal_state). ~/.local/lib/python3.9/site-packages/numba/core/compiler_machinery.py in check(func, compiler_state). 262 . 263 def check(func, compiler_state):. --> 264 mangled = func(compiler_state). 265 if mangled not in (True, False):. 266 msg = (""CompilerPass implementations should return True/False. "". ~/.local/lib/python3.9/site-packages/numba/core/typed_passes.py in run_pass(self, state). 90 % (state.func_id.func_name,)):. 91 # Type inference. ---> 92 typemap, return_type, calltypes = type_inference_stage(. 93 state.typingctx,. 94 state.func_ir,. ~/.local/lib/python3.9/site-packages/numba/core/typed_passes.py in type_inference_stage(typingctx, interp, args, return_type, locals, raise_errors). 68 . 69 infer.build_constraint(). ---> 70 infer.propagate(raise_errors=raise_errors). 71 typemap, restype, calltypes = infer.unify(raise_errors=raise_errors). 72 . ~/.local/lib/python3.9/site-packages/numba/core/typeinfer.py in propagate(self, raise_errors). 1069 if isinstance(e, ForceLiteralArg)]. 1070 if not force_lit_args:. -> 1071 raise errors[0]. 1072 else:. 1073 raise reduce(operator.or_, force_lit_args). TypingError: Failed in nopython mode pipeline (step: nopython frontend). Failed in nopython mode pipeline (step: nopython frontend). Failed in nopython mode pipeline (step: nopython mode backend). Failed in nopython m",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:7677,modifiability,pac,packages,7677,", **kwargs). 33 return _acquire_compile_lock. 34 . ~/.local/lib/python3.9/site-packages/numba/core/compiler_machinery.py in _runPass(self, index, pss, internal_state). 289 mutated |= check(pss.run_initialization, internal_state). 290 with SimpleTimer() as pass_time:. --> 291 mutated |= check(pss.run_pass, internal_state). 292 with SimpleTimer() as finalize_time:. 293 mutated |= check(pss.run_finalizer, internal_state). ~/.local/lib/python3.9/site-packages/numba/core/compiler_machinery.py in check(func, compiler_state). 262 . 263 def check(func, compiler_state):. --> 264 mangled = func(compiler_state). 265 if mangled not in (True, False):. 266 msg = (""CompilerPass implementations should return True/False. "". ~/.local/lib/python3.9/site-packages/numba/core/typed_passes.py in run_pass(self, state). 90 % (state.func_id.func_name,)):. 91 # Type inference. ---> 92 typemap, return_type, calltypes = type_inference_stage(. 93 state.typingctx,. 94 state.func_ir,. ~/.local/lib/python3.9/site-packages/numba/core/typed_passes.py in type_inference_stage(typingctx, interp, args, return_type, locals, raise_errors). 68 . 69 infer.build_constraint(). ---> 70 infer.propagate(raise_errors=raise_errors). 71 typemap, restype, calltypes = infer.unify(raise_errors=raise_errors). 72 . ~/.local/lib/python3.9/site-packages/numba/core/typeinfer.py in propagate(self, raise_errors). 1069 if isinstance(e, ForceLiteralArg)]. 1070 if not force_lit_args:. -> 1071 raise errors[0]. 1072 else:. 1073 raise reduce(operator.or_, force_lit_args). TypingError: Failed in nopython mode pipeline (step: nopython frontend). Failed in nopython mode pipeline (step: nopython frontend). Failed in nopython mode pipeline (step: nopython mode backend). Failed in nopython mode pipeline (step: nopython mode backend). Failed in nopython mode pipeline (step: nopython frontend). No implementation of function Function(<function make_quicksort_impl.<locals>.run_quicksort at 0x7f455d24c280>) found for signature:. . >>> run_qui",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:7990,modifiability,pac,packages,7990,"al_state). 292 with SimpleTimer() as finalize_time:. 293 mutated |= check(pss.run_finalizer, internal_state). ~/.local/lib/python3.9/site-packages/numba/core/compiler_machinery.py in check(func, compiler_state). 262 . 263 def check(func, compiler_state):. --> 264 mangled = func(compiler_state). 265 if mangled not in (True, False):. 266 msg = (""CompilerPass implementations should return True/False. "". ~/.local/lib/python3.9/site-packages/numba/core/typed_passes.py in run_pass(self, state). 90 % (state.func_id.func_name,)):. 91 # Type inference. ---> 92 typemap, return_type, calltypes = type_inference_stage(. 93 state.typingctx,. 94 state.func_ir,. ~/.local/lib/python3.9/site-packages/numba/core/typed_passes.py in type_inference_stage(typingctx, interp, args, return_type, locals, raise_errors). 68 . 69 infer.build_constraint(). ---> 70 infer.propagate(raise_errors=raise_errors). 71 typemap, restype, calltypes = infer.unify(raise_errors=raise_errors). 72 . ~/.local/lib/python3.9/site-packages/numba/core/typeinfer.py in propagate(self, raise_errors). 1069 if isinstance(e, ForceLiteralArg)]. 1070 if not force_lit_args:. -> 1071 raise errors[0]. 1072 else:. 1073 raise reduce(operator.or_, force_lit_args). TypingError: Failed in nopython mode pipeline (step: nopython frontend). Failed in nopython mode pipeline (step: nopython frontend). Failed in nopython mode pipeline (step: nopython mode backend). Failed in nopython mode pipeline (step: nopython mode backend). Failed in nopython mode pipeline (step: nopython frontend). No implementation of function Function(<function make_quicksort_impl.<locals>.run_quicksort at 0x7f455d24c280>) found for signature:. . >>> run_quicksort(array(int32, 1d, C)). . There are 2 candidate implementations:. - Of which 2 did not match due to:. Overload in function 'register_jitable.<locals>.wrap.<locals>.ov_wrap': File: numba/core/extending.py: Line 150. With argument(s): '(array(int32, 1d, C))':. Rejected as the implementation raised a specific ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:8877,modifiability,exten,extending,8877,". 71 typemap, restype, calltypes = infer.unify(raise_errors=raise_errors). 72 . ~/.local/lib/python3.9/site-packages/numba/core/typeinfer.py in propagate(self, raise_errors). 1069 if isinstance(e, ForceLiteralArg)]. 1070 if not force_lit_args:. -> 1071 raise errors[0]. 1072 else:. 1073 raise reduce(operator.or_, force_lit_args). TypingError: Failed in nopython mode pipeline (step: nopython frontend). Failed in nopython mode pipeline (step: nopython frontend). Failed in nopython mode pipeline (step: nopython mode backend). Failed in nopython mode pipeline (step: nopython mode backend). Failed in nopython mode pipeline (step: nopython frontend). No implementation of function Function(<function make_quicksort_impl.<locals>.run_quicksort at 0x7f455d24c280>) found for signature:. . >>> run_quicksort(array(int32, 1d, C)). . There are 2 candidate implementations:. - Of which 2 did not match due to:. Overload in function 'register_jitable.<locals>.wrap.<locals>.ov_wrap': File: numba/core/extending.py: Line 150. With argument(s): '(array(int32, 1d, C))':. Rejected as the implementation raised a specific error:. UnsupportedError: Failed in nopython mode pipeline (step: analyzing bytecode). Use of unsupported opcode (LOAD_ASSERTION_ERROR) found. . File ""../../../../.local/lib/python3.9/site-packages/numba/misc/quicksort.py"", line 180:. def run_quicksort(A):. <source elided>. while high - low >= SMALL_QUICKSORT:. assert n < MAX_STACK. ^. . raised from /home/gabriel/.local/lib/python3.9/site-packages/numba/core/byteflow.py:269. During: resolving callee type: Function(<function make_quicksort_impl.<locals>.run_quicksort at 0x7f455d24c280>). During: typing of call at /home/gabriel/.local/lib/python3.9/site-packages/numba/np/arrayobj.py (5007). File ""../../../../.local/lib/python3.9/site-packages/numba/np/arrayobj.py"", line 5007:. def array_sort_impl(arr):. <source elided>. # Note we clobber the return value. sort_func(arr). ^. During: lowering ""$14call_method.5 = call $12load_meth",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:9183,modifiability,pac,packages,9183,"tor.or_, force_lit_args). TypingError: Failed in nopython mode pipeline (step: nopython frontend). Failed in nopython mode pipeline (step: nopython frontend). Failed in nopython mode pipeline (step: nopython mode backend). Failed in nopython mode pipeline (step: nopython mode backend). Failed in nopython mode pipeline (step: nopython frontend). No implementation of function Function(<function make_quicksort_impl.<locals>.run_quicksort at 0x7f455d24c280>) found for signature:. . >>> run_quicksort(array(int32, 1d, C)). . There are 2 candidate implementations:. - Of which 2 did not match due to:. Overload in function 'register_jitable.<locals>.wrap.<locals>.ov_wrap': File: numba/core/extending.py: Line 150. With argument(s): '(array(int32, 1d, C))':. Rejected as the implementation raised a specific error:. UnsupportedError: Failed in nopython mode pipeline (step: analyzing bytecode). Use of unsupported opcode (LOAD_ASSERTION_ERROR) found. . File ""../../../../.local/lib/python3.9/site-packages/numba/misc/quicksort.py"", line 180:. def run_quicksort(A):. <source elided>. while high - low >= SMALL_QUICKSORT:. assert n < MAX_STACK. ^. . raised from /home/gabriel/.local/lib/python3.9/site-packages/numba/core/byteflow.py:269. During: resolving callee type: Function(<function make_quicksort_impl.<locals>.run_quicksort at 0x7f455d24c280>). During: typing of call at /home/gabriel/.local/lib/python3.9/site-packages/numba/np/arrayobj.py (5007). File ""../../../../.local/lib/python3.9/site-packages/numba/np/arrayobj.py"", line 5007:. def array_sort_impl(arr):. <source elided>. # Note we clobber the return value. sort_func(arr). ^. During: lowering ""$14call_method.5 = call $12load_method.4(func=$12load_method.4, args=[], kws=(), vararg=None)"" at /home/gabriel/.local/lib/python3.9/site-packages/numba/np/arrayobj.py (5017). During: lowering ""$8call_method.3 = call $4load_method.1(arr, func=$4load_method.1, args=[Var(arr, sparse.py:28)], kws=(), vararg=None)"" at /home/gabriel/.local/lib/",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:9386,modifiability,pac,packages,9386,"thon mode backend). Failed in nopython mode pipeline (step: nopython mode backend). Failed in nopython mode pipeline (step: nopython frontend). No implementation of function Function(<function make_quicksort_impl.<locals>.run_quicksort at 0x7f455d24c280>) found for signature:. . >>> run_quicksort(array(int32, 1d, C)). . There are 2 candidate implementations:. - Of which 2 did not match due to:. Overload in function 'register_jitable.<locals>.wrap.<locals>.ov_wrap': File: numba/core/extending.py: Line 150. With argument(s): '(array(int32, 1d, C))':. Rejected as the implementation raised a specific error:. UnsupportedError: Failed in nopython mode pipeline (step: analyzing bytecode). Use of unsupported opcode (LOAD_ASSERTION_ERROR) found. . File ""../../../../.local/lib/python3.9/site-packages/numba/misc/quicksort.py"", line 180:. def run_quicksort(A):. <source elided>. while high - low >= SMALL_QUICKSORT:. assert n < MAX_STACK. ^. . raised from /home/gabriel/.local/lib/python3.9/site-packages/numba/core/byteflow.py:269. During: resolving callee type: Function(<function make_quicksort_impl.<locals>.run_quicksort at 0x7f455d24c280>). During: typing of call at /home/gabriel/.local/lib/python3.9/site-packages/numba/np/arrayobj.py (5007). File ""../../../../.local/lib/python3.9/site-packages/numba/np/arrayobj.py"", line 5007:. def array_sort_impl(arr):. <source elided>. # Note we clobber the return value. sort_func(arr). ^. During: lowering ""$14call_method.5 = call $12load_method.4(func=$12load_method.4, args=[], kws=(), vararg=None)"" at /home/gabriel/.local/lib/python3.9/site-packages/numba/np/arrayobj.py (5017). During: lowering ""$8call_method.3 = call $4load_method.1(arr, func=$4load_method.1, args=[Var(arr, sparse.py:28)], kws=(), vararg=None)"" at /home/gabriel/.local/lib/python3.9/site-packages/pynndescent/sparse.py (28). During: resolving callee type: type(CPUDispatcher(<function arr_unique at 0x7f4574e5aa60>)). During: typing of call at /home/gabriel/.local/lib/python3",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:9603,modifiability,pac,packages,9603,"als>.run_quicksort at 0x7f455d24c280>) found for signature:. . >>> run_quicksort(array(int32, 1d, C)). . There are 2 candidate implementations:. - Of which 2 did not match due to:. Overload in function 'register_jitable.<locals>.wrap.<locals>.ov_wrap': File: numba/core/extending.py: Line 150. With argument(s): '(array(int32, 1d, C))':. Rejected as the implementation raised a specific error:. UnsupportedError: Failed in nopython mode pipeline (step: analyzing bytecode). Use of unsupported opcode (LOAD_ASSERTION_ERROR) found. . File ""../../../../.local/lib/python3.9/site-packages/numba/misc/quicksort.py"", line 180:. def run_quicksort(A):. <source elided>. while high - low >= SMALL_QUICKSORT:. assert n < MAX_STACK. ^. . raised from /home/gabriel/.local/lib/python3.9/site-packages/numba/core/byteflow.py:269. During: resolving callee type: Function(<function make_quicksort_impl.<locals>.run_quicksort at 0x7f455d24c280>). During: typing of call at /home/gabriel/.local/lib/python3.9/site-packages/numba/np/arrayobj.py (5007). File ""../../../../.local/lib/python3.9/site-packages/numba/np/arrayobj.py"", line 5007:. def array_sort_impl(arr):. <source elided>. # Note we clobber the return value. sort_func(arr). ^. During: lowering ""$14call_method.5 = call $12load_method.4(func=$12load_method.4, args=[], kws=(), vararg=None)"" at /home/gabriel/.local/lib/python3.9/site-packages/numba/np/arrayobj.py (5017). During: lowering ""$8call_method.3 = call $4load_method.1(arr, func=$4load_method.1, args=[Var(arr, sparse.py:28)], kws=(), vararg=None)"" at /home/gabriel/.local/lib/python3.9/site-packages/pynndescent/sparse.py (28). During: resolving callee type: type(CPUDispatcher(<function arr_unique at 0x7f4574e5aa60>)). During: typing of call at /home/gabriel/.local/lib/python3.9/site-packages/pynndescent/sparse.py (41). During: resolving callee type: type(CPUDispatcher(<function arr_unique at 0x7f4574e5aa60>)). During: typing of call at /home/gabriel/.local/lib/python3.9/site-packages/pynn",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:9685,modifiability,pac,packages,9685,"rray(int32, 1d, C)). . There are 2 candidate implementations:. - Of which 2 did not match due to:. Overload in function 'register_jitable.<locals>.wrap.<locals>.ov_wrap': File: numba/core/extending.py: Line 150. With argument(s): '(array(int32, 1d, C))':. Rejected as the implementation raised a specific error:. UnsupportedError: Failed in nopython mode pipeline (step: analyzing bytecode). Use of unsupported opcode (LOAD_ASSERTION_ERROR) found. . File ""../../../../.local/lib/python3.9/site-packages/numba/misc/quicksort.py"", line 180:. def run_quicksort(A):. <source elided>. while high - low >= SMALL_QUICKSORT:. assert n < MAX_STACK. ^. . raised from /home/gabriel/.local/lib/python3.9/site-packages/numba/core/byteflow.py:269. During: resolving callee type: Function(<function make_quicksort_impl.<locals>.run_quicksort at 0x7f455d24c280>). During: typing of call at /home/gabriel/.local/lib/python3.9/site-packages/numba/np/arrayobj.py (5007). File ""../../../../.local/lib/python3.9/site-packages/numba/np/arrayobj.py"", line 5007:. def array_sort_impl(arr):. <source elided>. # Note we clobber the return value. sort_func(arr). ^. During: lowering ""$14call_method.5 = call $12load_method.4(func=$12load_method.4, args=[], kws=(), vararg=None)"" at /home/gabriel/.local/lib/python3.9/site-packages/numba/np/arrayobj.py (5017). During: lowering ""$8call_method.3 = call $4load_method.1(arr, func=$4load_method.1, args=[Var(arr, sparse.py:28)], kws=(), vararg=None)"" at /home/gabriel/.local/lib/python3.9/site-packages/pynndescent/sparse.py (28). During: resolving callee type: type(CPUDispatcher(<function arr_unique at 0x7f4574e5aa60>)). During: typing of call at /home/gabriel/.local/lib/python3.9/site-packages/pynndescent/sparse.py (41). During: resolving callee type: type(CPUDispatcher(<function arr_unique at 0x7f4574e5aa60>)). During: typing of call at /home/gabriel/.local/lib/python3.9/site-packages/pynndescent/sparse.py (41). File ""../../../../.local/lib/python3.9/site-packages/pynnd",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:9984,modifiability,pac,packages,9984,"cific error:. UnsupportedError: Failed in nopython mode pipeline (step: analyzing bytecode). Use of unsupported opcode (LOAD_ASSERTION_ERROR) found. . File ""../../../../.local/lib/python3.9/site-packages/numba/misc/quicksort.py"", line 180:. def run_quicksort(A):. <source elided>. while high - low >= SMALL_QUICKSORT:. assert n < MAX_STACK. ^. . raised from /home/gabriel/.local/lib/python3.9/site-packages/numba/core/byteflow.py:269. During: resolving callee type: Function(<function make_quicksort_impl.<locals>.run_quicksort at 0x7f455d24c280>). During: typing of call at /home/gabriel/.local/lib/python3.9/site-packages/numba/np/arrayobj.py (5007). File ""../../../../.local/lib/python3.9/site-packages/numba/np/arrayobj.py"", line 5007:. def array_sort_impl(arr):. <source elided>. # Note we clobber the return value. sort_func(arr). ^. During: lowering ""$14call_method.5 = call $12load_method.4(func=$12load_method.4, args=[], kws=(), vararg=None)"" at /home/gabriel/.local/lib/python3.9/site-packages/numba/np/arrayobj.py (5017). During: lowering ""$8call_method.3 = call $4load_method.1(arr, func=$4load_method.1, args=[Var(arr, sparse.py:28)], kws=(), vararg=None)"" at /home/gabriel/.local/lib/python3.9/site-packages/pynndescent/sparse.py (28). During: resolving callee type: type(CPUDispatcher(<function arr_unique at 0x7f4574e5aa60>)). During: typing of call at /home/gabriel/.local/lib/python3.9/site-packages/pynndescent/sparse.py (41). During: resolving callee type: type(CPUDispatcher(<function arr_unique at 0x7f4574e5aa60>)). During: typing of call at /home/gabriel/.local/lib/python3.9/site-packages/pynndescent/sparse.py (41). File ""../../../../.local/lib/python3.9/site-packages/pynndescent/sparse.py"", line 41:. def arr_union(ar1, ar2):. <source elided>. else:. return arr_unique(np.concatenate((ar1, ar2))). ^. During: resolving callee type: type(CPUDispatcher(<function arr_union at 0x7f4574e5ac10>)). During: typing of call at /home/gabriel/.local/lib/python3.9/site-packages/pyn",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:10202,modifiability,pac,packages,10202,"cksort.py"", line 180:. def run_quicksort(A):. <source elided>. while high - low >= SMALL_QUICKSORT:. assert n < MAX_STACK. ^. . raised from /home/gabriel/.local/lib/python3.9/site-packages/numba/core/byteflow.py:269. During: resolving callee type: Function(<function make_quicksort_impl.<locals>.run_quicksort at 0x7f455d24c280>). During: typing of call at /home/gabriel/.local/lib/python3.9/site-packages/numba/np/arrayobj.py (5007). File ""../../../../.local/lib/python3.9/site-packages/numba/np/arrayobj.py"", line 5007:. def array_sort_impl(arr):. <source elided>. # Note we clobber the return value. sort_func(arr). ^. During: lowering ""$14call_method.5 = call $12load_method.4(func=$12load_method.4, args=[], kws=(), vararg=None)"" at /home/gabriel/.local/lib/python3.9/site-packages/numba/np/arrayobj.py (5017). During: lowering ""$8call_method.3 = call $4load_method.1(arr, func=$4load_method.1, args=[Var(arr, sparse.py:28)], kws=(), vararg=None)"" at /home/gabriel/.local/lib/python3.9/site-packages/pynndescent/sparse.py (28). During: resolving callee type: type(CPUDispatcher(<function arr_unique at 0x7f4574e5aa60>)). During: typing of call at /home/gabriel/.local/lib/python3.9/site-packages/pynndescent/sparse.py (41). During: resolving callee type: type(CPUDispatcher(<function arr_unique at 0x7f4574e5aa60>)). During: typing of call at /home/gabriel/.local/lib/python3.9/site-packages/pynndescent/sparse.py (41). File ""../../../../.local/lib/python3.9/site-packages/pynndescent/sparse.py"", line 41:. def arr_union(ar1, ar2):. <source elided>. else:. return arr_unique(np.concatenate((ar1, ar2))). ^. During: resolving callee type: type(CPUDispatcher(<function arr_union at 0x7f4574e5ac10>)). During: typing of call at /home/gabriel/.local/lib/python3.9/site-packages/pynndescent/sparse.py (344). During: resolving callee type: type(CPUDispatcher(<function arr_union at 0x7f4574e5ac10>)). During: typing of call at /home/gabriel/.local/lib/python3.9/site-packages/pynndescent/sparse.py (34",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:10398,modifiability,pac,packages,10398,"ore/byteflow.py:269. During: resolving callee type: Function(<function make_quicksort_impl.<locals>.run_quicksort at 0x7f455d24c280>). During: typing of call at /home/gabriel/.local/lib/python3.9/site-packages/numba/np/arrayobj.py (5007). File ""../../../../.local/lib/python3.9/site-packages/numba/np/arrayobj.py"", line 5007:. def array_sort_impl(arr):. <source elided>. # Note we clobber the return value. sort_func(arr). ^. During: lowering ""$14call_method.5 = call $12load_method.4(func=$12load_method.4, args=[], kws=(), vararg=None)"" at /home/gabriel/.local/lib/python3.9/site-packages/numba/np/arrayobj.py (5017). During: lowering ""$8call_method.3 = call $4load_method.1(arr, func=$4load_method.1, args=[Var(arr, sparse.py:28)], kws=(), vararg=None)"" at /home/gabriel/.local/lib/python3.9/site-packages/pynndescent/sparse.py (28). During: resolving callee type: type(CPUDispatcher(<function arr_unique at 0x7f4574e5aa60>)). During: typing of call at /home/gabriel/.local/lib/python3.9/site-packages/pynndescent/sparse.py (41). During: resolving callee type: type(CPUDispatcher(<function arr_unique at 0x7f4574e5aa60>)). During: typing of call at /home/gabriel/.local/lib/python3.9/site-packages/pynndescent/sparse.py (41). File ""../../../../.local/lib/python3.9/site-packages/pynndescent/sparse.py"", line 41:. def arr_union(ar1, ar2):. <source elided>. else:. return arr_unique(np.concatenate((ar1, ar2))). ^. During: resolving callee type: type(CPUDispatcher(<function arr_union at 0x7f4574e5ac10>)). During: typing of call at /home/gabriel/.local/lib/python3.9/site-packages/pynndescent/sparse.py (344). During: resolving callee type: type(CPUDispatcher(<function arr_union at 0x7f4574e5ac10>)). During: typing of call at /home/gabriel/.local/lib/python3.9/site-packages/pynndescent/sparse.py (344). File ""../../../../.local/lib/python3.9/site-packages/pynndescent/sparse.py"", line 344:. def sparse_alternative_jaccard(ind1, data1, ind2, data2):. num_non_zero = arr_union(ind1, ind2).shape[0]",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:10594,modifiability,pac,packages,10594,"site-packages/numba/np/arrayobj.py (5007). File ""../../../../.local/lib/python3.9/site-packages/numba/np/arrayobj.py"", line 5007:. def array_sort_impl(arr):. <source elided>. # Note we clobber the return value. sort_func(arr). ^. During: lowering ""$14call_method.5 = call $12load_method.4(func=$12load_method.4, args=[], kws=(), vararg=None)"" at /home/gabriel/.local/lib/python3.9/site-packages/numba/np/arrayobj.py (5017). During: lowering ""$8call_method.3 = call $4load_method.1(arr, func=$4load_method.1, args=[Var(arr, sparse.py:28)], kws=(), vararg=None)"" at /home/gabriel/.local/lib/python3.9/site-packages/pynndescent/sparse.py (28). During: resolving callee type: type(CPUDispatcher(<function arr_unique at 0x7f4574e5aa60>)). During: typing of call at /home/gabriel/.local/lib/python3.9/site-packages/pynndescent/sparse.py (41). During: resolving callee type: type(CPUDispatcher(<function arr_unique at 0x7f4574e5aa60>)). During: typing of call at /home/gabriel/.local/lib/python3.9/site-packages/pynndescent/sparse.py (41). File ""../../../../.local/lib/python3.9/site-packages/pynndescent/sparse.py"", line 41:. def arr_union(ar1, ar2):. <source elided>. else:. return arr_unique(np.concatenate((ar1, ar2))). ^. During: resolving callee type: type(CPUDispatcher(<function arr_union at 0x7f4574e5ac10>)). During: typing of call at /home/gabriel/.local/lib/python3.9/site-packages/pynndescent/sparse.py (344). During: resolving callee type: type(CPUDispatcher(<function arr_union at 0x7f4574e5ac10>)). During: typing of call at /home/gabriel/.local/lib/python3.9/site-packages/pynndescent/sparse.py (344). File ""../../../../.local/lib/python3.9/site-packages/pynndescent/sparse.py"", line 344:. def sparse_alternative_jaccard(ind1, data1, ind2, data2):. num_non_zero = arr_union(ind1, ind2).shape[0]. ^. ```. #### Versions. <details>. anndata 0.7.5. scanpy 1.7.0. sinfo 0.3.1. -----. PIL 8.1.0. PyQt5 NA. anndata 0.7.5. appdirs 1.4.4. cffi 1.14.4. colorama 0.4.4. constants NA. cycler 0.10.0. cy",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:10675,modifiability,pac,packages,10675,"/site-packages/numba/np/arrayobj.py"", line 5007:. def array_sort_impl(arr):. <source elided>. # Note we clobber the return value. sort_func(arr). ^. During: lowering ""$14call_method.5 = call $12load_method.4(func=$12load_method.4, args=[], kws=(), vararg=None)"" at /home/gabriel/.local/lib/python3.9/site-packages/numba/np/arrayobj.py (5017). During: lowering ""$8call_method.3 = call $4load_method.1(arr, func=$4load_method.1, args=[Var(arr, sparse.py:28)], kws=(), vararg=None)"" at /home/gabriel/.local/lib/python3.9/site-packages/pynndescent/sparse.py (28). During: resolving callee type: type(CPUDispatcher(<function arr_unique at 0x7f4574e5aa60>)). During: typing of call at /home/gabriel/.local/lib/python3.9/site-packages/pynndescent/sparse.py (41). During: resolving callee type: type(CPUDispatcher(<function arr_unique at 0x7f4574e5aa60>)). During: typing of call at /home/gabriel/.local/lib/python3.9/site-packages/pynndescent/sparse.py (41). File ""../../../../.local/lib/python3.9/site-packages/pynndescent/sparse.py"", line 41:. def arr_union(ar1, ar2):. <source elided>. else:. return arr_unique(np.concatenate((ar1, ar2))). ^. During: resolving callee type: type(CPUDispatcher(<function arr_union at 0x7f4574e5ac10>)). During: typing of call at /home/gabriel/.local/lib/python3.9/site-packages/pynndescent/sparse.py (344). During: resolving callee type: type(CPUDispatcher(<function arr_union at 0x7f4574e5ac10>)). During: typing of call at /home/gabriel/.local/lib/python3.9/site-packages/pynndescent/sparse.py (344). File ""../../../../.local/lib/python3.9/site-packages/pynndescent/sparse.py"", line 344:. def sparse_alternative_jaccard(ind1, data1, ind2, data2):. num_non_zero = arr_union(ind1, ind2).shape[0]. ^. ```. #### Versions. <details>. anndata 0.7.5. scanpy 1.7.0. sinfo 0.3.1. -----. PIL 8.1.0. PyQt5 NA. anndata 0.7.5. appdirs 1.4.4. cffi 1.14.4. colorama 0.4.4. constants NA. cycler 0.10.0. cython_runtime NA. dateutil 2.8.1. get_version 2.1. google NA. h5py 3.1.0. highs_wr",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:10976,modifiability,pac,packages,10976,"ite-packages/numba/np/arrayobj.py (5017). During: lowering ""$8call_method.3 = call $4load_method.1(arr, func=$4load_method.1, args=[Var(arr, sparse.py:28)], kws=(), vararg=None)"" at /home/gabriel/.local/lib/python3.9/site-packages/pynndescent/sparse.py (28). During: resolving callee type: type(CPUDispatcher(<function arr_unique at 0x7f4574e5aa60>)). During: typing of call at /home/gabriel/.local/lib/python3.9/site-packages/pynndescent/sparse.py (41). During: resolving callee type: type(CPUDispatcher(<function arr_unique at 0x7f4574e5aa60>)). During: typing of call at /home/gabriel/.local/lib/python3.9/site-packages/pynndescent/sparse.py (41). File ""../../../../.local/lib/python3.9/site-packages/pynndescent/sparse.py"", line 41:. def arr_union(ar1, ar2):. <source elided>. else:. return arr_unique(np.concatenate((ar1, ar2))). ^. During: resolving callee type: type(CPUDispatcher(<function arr_union at 0x7f4574e5ac10>)). During: typing of call at /home/gabriel/.local/lib/python3.9/site-packages/pynndescent/sparse.py (344). During: resolving callee type: type(CPUDispatcher(<function arr_union at 0x7f4574e5ac10>)). During: typing of call at /home/gabriel/.local/lib/python3.9/site-packages/pynndescent/sparse.py (344). File ""../../../../.local/lib/python3.9/site-packages/pynndescent/sparse.py"", line 344:. def sparse_alternative_jaccard(ind1, data1, ind2, data2):. num_non_zero = arr_union(ind1, ind2).shape[0]. ^. ```. #### Versions. <details>. anndata 0.7.5. scanpy 1.7.0. sinfo 0.3.1. -----. PIL 8.1.0. PyQt5 NA. anndata 0.7.5. appdirs 1.4.4. cffi 1.14.4. colorama 0.4.4. constants NA. cycler 0.10.0. cython_runtime NA. dateutil 2.8.1. get_version 2.1. google NA. h5py 3.1.0. highs_wrapper NA. joblib 1.0.1. kiwisolver 1.3.1. legacy_api_wrap 1.2. llvmlite 0.34.0. matplotlib 3.3.4. mpl_toolkits NA. natsort 7.1.1. numba 0.51.2. numexpr 2.7.2. numpy 1.20.0. packaging 20.8. pandas 1.2.2. pkg_resources NA. psutil 5.8.0. pyexpat NA. pyparsing 2.4.7. pytoml NA. pytz 2021.1. scanpy 1.7.0.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:11172,modifiability,pac,packages,11172,"rr, sparse.py:28)], kws=(), vararg=None)"" at /home/gabriel/.local/lib/python3.9/site-packages/pynndescent/sparse.py (28). During: resolving callee type: type(CPUDispatcher(<function arr_unique at 0x7f4574e5aa60>)). During: typing of call at /home/gabriel/.local/lib/python3.9/site-packages/pynndescent/sparse.py (41). During: resolving callee type: type(CPUDispatcher(<function arr_unique at 0x7f4574e5aa60>)). During: typing of call at /home/gabriel/.local/lib/python3.9/site-packages/pynndescent/sparse.py (41). File ""../../../../.local/lib/python3.9/site-packages/pynndescent/sparse.py"", line 41:. def arr_union(ar1, ar2):. <source elided>. else:. return arr_unique(np.concatenate((ar1, ar2))). ^. During: resolving callee type: type(CPUDispatcher(<function arr_union at 0x7f4574e5ac10>)). During: typing of call at /home/gabriel/.local/lib/python3.9/site-packages/pynndescent/sparse.py (344). During: resolving callee type: type(CPUDispatcher(<function arr_union at 0x7f4574e5ac10>)). During: typing of call at /home/gabriel/.local/lib/python3.9/site-packages/pynndescent/sparse.py (344). File ""../../../../.local/lib/python3.9/site-packages/pynndescent/sparse.py"", line 344:. def sparse_alternative_jaccard(ind1, data1, ind2, data2):. num_non_zero = arr_union(ind1, ind2).shape[0]. ^. ```. #### Versions. <details>. anndata 0.7.5. scanpy 1.7.0. sinfo 0.3.1. -----. PIL 8.1.0. PyQt5 NA. anndata 0.7.5. appdirs 1.4.4. cffi 1.14.4. colorama 0.4.4. constants NA. cycler 0.10.0. cython_runtime NA. dateutil 2.8.1. get_version 2.1. google NA. h5py 3.1.0. highs_wrapper NA. joblib 1.0.1. kiwisolver 1.3.1. legacy_api_wrap 1.2. llvmlite 0.34.0. matplotlib 3.3.4. mpl_toolkits NA. natsort 7.1.1. numba 0.51.2. numexpr 2.7.2. numpy 1.20.0. packaging 20.8. pandas 1.2.2. pkg_resources NA. psutil 5.8.0. pyexpat NA. pyparsing 2.4.7. pytoml NA. pytz 2021.1. scanpy 1.7.0. scipy 1.6.0. setuptools_scm NA. sinfo 0.3.1. sip NA. six 1.15.0. sklearn 0.24.1. sphinxcontrib NA. tables 3.6.1. yaml 5.3.1. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:11254,modifiability,pac,packages,11254,"rr, sparse.py:28)], kws=(), vararg=None)"" at /home/gabriel/.local/lib/python3.9/site-packages/pynndescent/sparse.py (28). During: resolving callee type: type(CPUDispatcher(<function arr_unique at 0x7f4574e5aa60>)). During: typing of call at /home/gabriel/.local/lib/python3.9/site-packages/pynndescent/sparse.py (41). During: resolving callee type: type(CPUDispatcher(<function arr_unique at 0x7f4574e5aa60>)). During: typing of call at /home/gabriel/.local/lib/python3.9/site-packages/pynndescent/sparse.py (41). File ""../../../../.local/lib/python3.9/site-packages/pynndescent/sparse.py"", line 41:. def arr_union(ar1, ar2):. <source elided>. else:. return arr_unique(np.concatenate((ar1, ar2))). ^. During: resolving callee type: type(CPUDispatcher(<function arr_union at 0x7f4574e5ac10>)). During: typing of call at /home/gabriel/.local/lib/python3.9/site-packages/pynndescent/sparse.py (344). During: resolving callee type: type(CPUDispatcher(<function arr_union at 0x7f4574e5ac10>)). During: typing of call at /home/gabriel/.local/lib/python3.9/site-packages/pynndescent/sparse.py (344). File ""../../../../.local/lib/python3.9/site-packages/pynndescent/sparse.py"", line 344:. def sparse_alternative_jaccard(ind1, data1, ind2, data2):. num_non_zero = arr_union(ind1, ind2).shape[0]. ^. ```. #### Versions. <details>. anndata 0.7.5. scanpy 1.7.0. sinfo 0.3.1. -----. PIL 8.1.0. PyQt5 NA. anndata 0.7.5. appdirs 1.4.4. cffi 1.14.4. colorama 0.4.4. constants NA. cycler 0.10.0. cython_runtime NA. dateutil 2.8.1. get_version 2.1. google NA. h5py 3.1.0. highs_wrapper NA. joblib 1.0.1. kiwisolver 1.3.1. legacy_api_wrap 1.2. llvmlite 0.34.0. matplotlib 3.3.4. mpl_toolkits NA. natsort 7.1.1. numba 0.51.2. numexpr 2.7.2. numpy 1.20.0. packaging 20.8. pandas 1.2.2. pkg_resources NA. psutil 5.8.0. pyexpat NA. pyparsing 2.4.7. pytoml NA. pytz 2021.1. scanpy 1.7.0. scipy 1.6.0. setuptools_scm NA. sinfo 0.3.1. sip NA. six 1.15.0. sklearn 0.24.1. sphinxcontrib NA. tables 3.6.1. yaml 5.3.1. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:11417,modifiability,Version,Versions,11417,"rr, sparse.py:28)], kws=(), vararg=None)"" at /home/gabriel/.local/lib/python3.9/site-packages/pynndescent/sparse.py (28). During: resolving callee type: type(CPUDispatcher(<function arr_unique at 0x7f4574e5aa60>)). During: typing of call at /home/gabriel/.local/lib/python3.9/site-packages/pynndescent/sparse.py (41). During: resolving callee type: type(CPUDispatcher(<function arr_unique at 0x7f4574e5aa60>)). During: typing of call at /home/gabriel/.local/lib/python3.9/site-packages/pynndescent/sparse.py (41). File ""../../../../.local/lib/python3.9/site-packages/pynndescent/sparse.py"", line 41:. def arr_union(ar1, ar2):. <source elided>. else:. return arr_unique(np.concatenate((ar1, ar2))). ^. During: resolving callee type: type(CPUDispatcher(<function arr_union at 0x7f4574e5ac10>)). During: typing of call at /home/gabriel/.local/lib/python3.9/site-packages/pynndescent/sparse.py (344). During: resolving callee type: type(CPUDispatcher(<function arr_union at 0x7f4574e5ac10>)). During: typing of call at /home/gabriel/.local/lib/python3.9/site-packages/pynndescent/sparse.py (344). File ""../../../../.local/lib/python3.9/site-packages/pynndescent/sparse.py"", line 344:. def sparse_alternative_jaccard(ind1, data1, ind2, data2):. num_non_zero = arr_union(ind1, ind2).shape[0]. ^. ```. #### Versions. <details>. anndata 0.7.5. scanpy 1.7.0. sinfo 0.3.1. -----. PIL 8.1.0. PyQt5 NA. anndata 0.7.5. appdirs 1.4.4. cffi 1.14.4. colorama 0.4.4. constants NA. cycler 0.10.0. cython_runtime NA. dateutil 2.8.1. get_version 2.1. google NA. h5py 3.1.0. highs_wrapper NA. joblib 1.0.1. kiwisolver 1.3.1. legacy_api_wrap 1.2. llvmlite 0.34.0. matplotlib 3.3.4. mpl_toolkits NA. natsort 7.1.1. numba 0.51.2. numexpr 2.7.2. numpy 1.20.0. packaging 20.8. pandas 1.2.2. pkg_resources NA. psutil 5.8.0. pyexpat NA. pyparsing 2.4.7. pytoml NA. pytz 2021.1. scanpy 1.7.0. scipy 1.6.0. setuptools_scm NA. sinfo 0.3.1. sip NA. six 1.15.0. sklearn 0.24.1. sphinxcontrib NA. tables 3.6.1. yaml 5.3.1. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:11852,modifiability,pac,packaging,11852,"rr, sparse.py:28)], kws=(), vararg=None)"" at /home/gabriel/.local/lib/python3.9/site-packages/pynndescent/sparse.py (28). During: resolving callee type: type(CPUDispatcher(<function arr_unique at 0x7f4574e5aa60>)). During: typing of call at /home/gabriel/.local/lib/python3.9/site-packages/pynndescent/sparse.py (41). During: resolving callee type: type(CPUDispatcher(<function arr_unique at 0x7f4574e5aa60>)). During: typing of call at /home/gabriel/.local/lib/python3.9/site-packages/pynndescent/sparse.py (41). File ""../../../../.local/lib/python3.9/site-packages/pynndescent/sparse.py"", line 41:. def arr_union(ar1, ar2):. <source elided>. else:. return arr_unique(np.concatenate((ar1, ar2))). ^. During: resolving callee type: type(CPUDispatcher(<function arr_union at 0x7f4574e5ac10>)). During: typing of call at /home/gabriel/.local/lib/python3.9/site-packages/pynndescent/sparse.py (344). During: resolving callee type: type(CPUDispatcher(<function arr_union at 0x7f4574e5ac10>)). During: typing of call at /home/gabriel/.local/lib/python3.9/site-packages/pynndescent/sparse.py (344). File ""../../../../.local/lib/python3.9/site-packages/pynndescent/sparse.py"", line 344:. def sparse_alternative_jaccard(ind1, data1, ind2, data2):. num_non_zero = arr_union(ind1, ind2).shape[0]. ^. ```. #### Versions. <details>. anndata 0.7.5. scanpy 1.7.0. sinfo 0.3.1. -----. PIL 8.1.0. PyQt5 NA. anndata 0.7.5. appdirs 1.4.4. cffi 1.14.4. colorama 0.4.4. constants NA. cycler 0.10.0. cython_runtime NA. dateutil 2.8.1. get_version 2.1. google NA. h5py 3.1.0. highs_wrapper NA. joblib 1.0.1. kiwisolver 1.3.1. legacy_api_wrap 1.2. llvmlite 0.34.0. matplotlib 3.3.4. mpl_toolkits NA. natsort 7.1.1. numba 0.51.2. numexpr 2.7.2. numpy 1.20.0. packaging 20.8. pandas 1.2.2. pkg_resources NA. psutil 5.8.0. pyexpat NA. pyparsing 2.4.7. pytoml NA. pytz 2021.1. scanpy 1.7.0. scipy 1.6.0. setuptools_scm NA. sinfo 0.3.1. sip NA. six 1.15.0. sklearn 0.24.1. sphinxcontrib NA. tables 3.6.1. yaml 5.3.1. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:3935,performance,error,errors,3935,"e_nndescent as sparse_nnd. 23 import pynndescent.distances as pynnd_dist. ~/.local/lib/python3.9/site-packages/pynndescent/sparse.py in <module>. 341 },. 342 ). --> 343 def sparse_alternative_jaccard(ind1, data1, ind2, data2):. 344 num_non_zero = arr_union(ind1, ind2).shape[0]. 345 num_equal = arr_intersect(ind1, ind2).shape[0]. ~/.local/lib/python3.9/site-packages/numba/core/decorators.py in wrapper(func). 216 with typeinfer.register_dispatcher(disp):. 217 for sig in sigs:. --> 218 disp.compile(sig). 219 disp.disable_compile(). 220 return disp. ~/.local/lib/python3.9/site-packages/numba/core/compiler_lock.py in _acquire_compile_lock(*args, **kwargs). 30 def _acquire_compile_lock(*args, **kwargs):. 31 with self:. ---> 32 return func(*args, **kwargs). 33 return _acquire_compile_lock. 34 . ~/.local/lib/python3.9/site-packages/numba/core/dispatcher.py in compile(self, sig). 817 self._cache_misses[sig] += 1. 818 try:. --> 819 cres = self._compiler.compile(args, return_type). 820 except errors.ForceLiteralArg as e:. 821 def folded(args, kws):. ~/.local/lib/python3.9/site-packages/numba/core/dispatcher.py in compile(self, args, return_type). 80 return retval. 81 else:. ---> 82 raise retval. 83 . 84 def _compile_cached(self, args, return_type):. ~/.local/lib/python3.9/site-packages/numba/core/dispatcher.py in _compile_cached(self, args, return_type). 90 . 91 try:. ---> 92 retval = self._compile_core(args, return_type). 93 except errors.TypingError as e:. 94 self._failed_cache[key] = e. ~/.local/lib/python3.9/site-packages/numba/core/dispatcher.py in _compile_core(self, args, return_type). 103 . 104 impl = self._get_implementation(args, {}). --> 105 cres = compiler.compile_extra(self.targetdescr.typing_context,. 106 self.targetdescr.target_context,. 107 impl,. ~/.local/lib/python3.9/site-packages/numba/core/compiler.py in compile_extra(typingctx, targetctx, func, args, return_type, flags, locals, library, pipeline_class). 625 pipeline = pipeline_class(typingctx, targetctx, ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:4384,performance,error,errors,4384,"(disp):. 217 for sig in sigs:. --> 218 disp.compile(sig). 219 disp.disable_compile(). 220 return disp. ~/.local/lib/python3.9/site-packages/numba/core/compiler_lock.py in _acquire_compile_lock(*args, **kwargs). 30 def _acquire_compile_lock(*args, **kwargs):. 31 with self:. ---> 32 return func(*args, **kwargs). 33 return _acquire_compile_lock. 34 . ~/.local/lib/python3.9/site-packages/numba/core/dispatcher.py in compile(self, sig). 817 self._cache_misses[sig] += 1. 818 try:. --> 819 cres = self._compiler.compile(args, return_type). 820 except errors.ForceLiteralArg as e:. 821 def folded(args, kws):. ~/.local/lib/python3.9/site-packages/numba/core/dispatcher.py in compile(self, args, return_type). 80 return retval. 81 else:. ---> 82 raise retval. 83 . 84 def _compile_cached(self, args, return_type):. ~/.local/lib/python3.9/site-packages/numba/core/dispatcher.py in _compile_cached(self, args, return_type). 90 . 91 try:. ---> 92 retval = self._compile_core(args, return_type). 93 except errors.TypingError as e:. 94 self._failed_cache[key] = e. ~/.local/lib/python3.9/site-packages/numba/core/dispatcher.py in _compile_core(self, args, return_type). 103 . 104 impl = self._get_implementation(args, {}). --> 105 cres = compiler.compile_extra(self.targetdescr.typing_context,. 106 self.targetdescr.target_context,. 107 impl,. ~/.local/lib/python3.9/site-packages/numba/core/compiler.py in compile_extra(typingctx, targetctx, func, args, return_type, flags, locals, library, pipeline_class). 625 pipeline = pipeline_class(typingctx, targetctx, library,. 626 args, return_type, flags, locals). --> 627 return pipeline.compile_extra(func). 628 . 629 . ~/.local/lib/python3.9/site-packages/numba/core/compiler.py in compile_extra(self, func). 361 self.state.lifted = (). 362 self.state.lifted_from = None. --> 363 return self._compile_bytecode(). 364 . 365 def compile_ir(self, func_ir, lifted=(), lifted_from=None):. ~/.local/lib/python3.9/site-packages/numba/core/compiler.py in _compile_byteco",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:8141,performance,error,errors,8141,"ba/core/compiler_machinery.py in check(func, compiler_state). 262 . 263 def check(func, compiler_state):. --> 264 mangled = func(compiler_state). 265 if mangled not in (True, False):. 266 msg = (""CompilerPass implementations should return True/False. "". ~/.local/lib/python3.9/site-packages/numba/core/typed_passes.py in run_pass(self, state). 90 % (state.func_id.func_name,)):. 91 # Type inference. ---> 92 typemap, return_type, calltypes = type_inference_stage(. 93 state.typingctx,. 94 state.func_ir,. ~/.local/lib/python3.9/site-packages/numba/core/typed_passes.py in type_inference_stage(typingctx, interp, args, return_type, locals, raise_errors). 68 . 69 infer.build_constraint(). ---> 70 infer.propagate(raise_errors=raise_errors). 71 typemap, restype, calltypes = infer.unify(raise_errors=raise_errors). 72 . ~/.local/lib/python3.9/site-packages/numba/core/typeinfer.py in propagate(self, raise_errors). 1069 if isinstance(e, ForceLiteralArg)]. 1070 if not force_lit_args:. -> 1071 raise errors[0]. 1072 else:. 1073 raise reduce(operator.or_, force_lit_args). TypingError: Failed in nopython mode pipeline (step: nopython frontend). Failed in nopython mode pipeline (step: nopython frontend). Failed in nopython mode pipeline (step: nopython mode backend). Failed in nopython mode pipeline (step: nopython mode backend). Failed in nopython mode pipeline (step: nopython frontend). No implementation of function Function(<function make_quicksort_impl.<locals>.run_quicksort at 0x7f455d24c280>) found for signature:. . >>> run_quicksort(array(int32, 1d, C)). . There are 2 candidate implementations:. - Of which 2 did not match due to:. Overload in function 'register_jitable.<locals>.wrap.<locals>.ov_wrap': File: numba/core/extending.py: Line 150. With argument(s): '(array(int32, 1d, C))':. Rejected as the implementation raised a specific error:. UnsupportedError: Failed in nopython mode pipeline (step: analyzing bytecode). Use of unsupported opcode (LOAD_ASSERTION_ERROR) found. . File ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:8994,performance,error,error,8994,"s/numba/core/typeinfer.py in propagate(self, raise_errors). 1069 if isinstance(e, ForceLiteralArg)]. 1070 if not force_lit_args:. -> 1071 raise errors[0]. 1072 else:. 1073 raise reduce(operator.or_, force_lit_args). TypingError: Failed in nopython mode pipeline (step: nopython frontend). Failed in nopython mode pipeline (step: nopython frontend). Failed in nopython mode pipeline (step: nopython mode backend). Failed in nopython mode pipeline (step: nopython mode backend). Failed in nopython mode pipeline (step: nopython frontend). No implementation of function Function(<function make_quicksort_impl.<locals>.run_quicksort at 0x7f455d24c280>) found for signature:. . >>> run_quicksort(array(int32, 1d, C)). . There are 2 candidate implementations:. - Of which 2 did not match due to:. Overload in function 'register_jitable.<locals>.wrap.<locals>.ov_wrap': File: numba/core/extending.py: Line 150. With argument(s): '(array(int32, 1d, C))':. Rejected as the implementation raised a specific error:. UnsupportedError: Failed in nopython mode pipeline (step: analyzing bytecode). Use of unsupported opcode (LOAD_ASSERTION_ERROR) found. . File ""../../../../.local/lib/python3.9/site-packages/numba/misc/quicksort.py"", line 180:. def run_quicksort(A):. <source elided>. while high - low >= SMALL_QUICKSORT:. assert n < MAX_STACK. ^. . raised from /home/gabriel/.local/lib/python3.9/site-packages/numba/core/byteflow.py:269. During: resolving callee type: Function(<function make_quicksort_impl.<locals>.run_quicksort at 0x7f455d24c280>). During: typing of call at /home/gabriel/.local/lib/python3.9/site-packages/numba/np/arrayobj.py (5007). File ""../../../../.local/lib/python3.9/site-packages/numba/np/arrayobj.py"", line 5007:. def array_sort_impl(arr):. <source elided>. # Note we clobber the return value. sort_func(arr). ^. During: lowering ""$14call_method.5 = call $12load_method.4(func=$12load_method.4, args=[], kws=(), vararg=None)"" at /home/gabriel/.local/lib/python3.9/site-packages/numb",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:10275,performance,CPU,CPUDispatcher,10275,"low >= SMALL_QUICKSORT:. assert n < MAX_STACK. ^. . raised from /home/gabriel/.local/lib/python3.9/site-packages/numba/core/byteflow.py:269. During: resolving callee type: Function(<function make_quicksort_impl.<locals>.run_quicksort at 0x7f455d24c280>). During: typing of call at /home/gabriel/.local/lib/python3.9/site-packages/numba/np/arrayobj.py (5007). File ""../../../../.local/lib/python3.9/site-packages/numba/np/arrayobj.py"", line 5007:. def array_sort_impl(arr):. <source elided>. # Note we clobber the return value. sort_func(arr). ^. During: lowering ""$14call_method.5 = call $12load_method.4(func=$12load_method.4, args=[], kws=(), vararg=None)"" at /home/gabriel/.local/lib/python3.9/site-packages/numba/np/arrayobj.py (5017). During: lowering ""$8call_method.3 = call $4load_method.1(arr, func=$4load_method.1, args=[Var(arr, sparse.py:28)], kws=(), vararg=None)"" at /home/gabriel/.local/lib/python3.9/site-packages/pynndescent/sparse.py (28). During: resolving callee type: type(CPUDispatcher(<function arr_unique at 0x7f4574e5aa60>)). During: typing of call at /home/gabriel/.local/lib/python3.9/site-packages/pynndescent/sparse.py (41). During: resolving callee type: type(CPUDispatcher(<function arr_unique at 0x7f4574e5aa60>)). During: typing of call at /home/gabriel/.local/lib/python3.9/site-packages/pynndescent/sparse.py (41). File ""../../../../.local/lib/python3.9/site-packages/pynndescent/sparse.py"", line 41:. def arr_union(ar1, ar2):. <source elided>. else:. return arr_unique(np.concatenate((ar1, ar2))). ^. During: resolving callee type: type(CPUDispatcher(<function arr_union at 0x7f4574e5ac10>)). During: typing of call at /home/gabriel/.local/lib/python3.9/site-packages/pynndescent/sparse.py (344). During: resolving callee type: type(CPUDispatcher(<function arr_union at 0x7f4574e5ac10>)). During: typing of call at /home/gabriel/.local/lib/python3.9/site-packages/pynndescent/sparse.py (344). File ""../../../../.local/lib/python3.9/site-packages/pynndescent/sparse.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:10471,performance,CPU,CPUDispatcher,10471,"quicksort_impl.<locals>.run_quicksort at 0x7f455d24c280>). During: typing of call at /home/gabriel/.local/lib/python3.9/site-packages/numba/np/arrayobj.py (5007). File ""../../../../.local/lib/python3.9/site-packages/numba/np/arrayobj.py"", line 5007:. def array_sort_impl(arr):. <source elided>. # Note we clobber the return value. sort_func(arr). ^. During: lowering ""$14call_method.5 = call $12load_method.4(func=$12load_method.4, args=[], kws=(), vararg=None)"" at /home/gabriel/.local/lib/python3.9/site-packages/numba/np/arrayobj.py (5017). During: lowering ""$8call_method.3 = call $4load_method.1(arr, func=$4load_method.1, args=[Var(arr, sparse.py:28)], kws=(), vararg=None)"" at /home/gabriel/.local/lib/python3.9/site-packages/pynndescent/sparse.py (28). During: resolving callee type: type(CPUDispatcher(<function arr_unique at 0x7f4574e5aa60>)). During: typing of call at /home/gabriel/.local/lib/python3.9/site-packages/pynndescent/sparse.py (41). During: resolving callee type: type(CPUDispatcher(<function arr_unique at 0x7f4574e5aa60>)). During: typing of call at /home/gabriel/.local/lib/python3.9/site-packages/pynndescent/sparse.py (41). File ""../../../../.local/lib/python3.9/site-packages/pynndescent/sparse.py"", line 41:. def arr_union(ar1, ar2):. <source elided>. else:. return arr_unique(np.concatenate((ar1, ar2))). ^. During: resolving callee type: type(CPUDispatcher(<function arr_union at 0x7f4574e5ac10>)). During: typing of call at /home/gabriel/.local/lib/python3.9/site-packages/pynndescent/sparse.py (344). During: resolving callee type: type(CPUDispatcher(<function arr_union at 0x7f4574e5ac10>)). During: typing of call at /home/gabriel/.local/lib/python3.9/site-packages/pynndescent/sparse.py (344). File ""../../../../.local/lib/python3.9/site-packages/pynndescent/sparse.py"", line 344:. def sparse_alternative_jaccard(ind1, data1, ind2, data2):. num_non_zero = arr_union(ind1, ind2).shape[0]. ^. ```. #### Versions. <details>. anndata 0.7.5. scanpy 1.7.0. sinfo 0.3.1",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:10854,performance,CPU,CPUDispatcher,10854,"5 = call $12load_method.4(func=$12load_method.4, args=[], kws=(), vararg=None)"" at /home/gabriel/.local/lib/python3.9/site-packages/numba/np/arrayobj.py (5017). During: lowering ""$8call_method.3 = call $4load_method.1(arr, func=$4load_method.1, args=[Var(arr, sparse.py:28)], kws=(), vararg=None)"" at /home/gabriel/.local/lib/python3.9/site-packages/pynndescent/sparse.py (28). During: resolving callee type: type(CPUDispatcher(<function arr_unique at 0x7f4574e5aa60>)). During: typing of call at /home/gabriel/.local/lib/python3.9/site-packages/pynndescent/sparse.py (41). During: resolving callee type: type(CPUDispatcher(<function arr_unique at 0x7f4574e5aa60>)). During: typing of call at /home/gabriel/.local/lib/python3.9/site-packages/pynndescent/sparse.py (41). File ""../../../../.local/lib/python3.9/site-packages/pynndescent/sparse.py"", line 41:. def arr_union(ar1, ar2):. <source elided>. else:. return arr_unique(np.concatenate((ar1, ar2))). ^. During: resolving callee type: type(CPUDispatcher(<function arr_union at 0x7f4574e5ac10>)). During: typing of call at /home/gabriel/.local/lib/python3.9/site-packages/pynndescent/sparse.py (344). During: resolving callee type: type(CPUDispatcher(<function arr_union at 0x7f4574e5ac10>)). During: typing of call at /home/gabriel/.local/lib/python3.9/site-packages/pynndescent/sparse.py (344). File ""../../../../.local/lib/python3.9/site-packages/pynndescent/sparse.py"", line 344:. def sparse_alternative_jaccard(ind1, data1, ind2, data2):. num_non_zero = arr_union(ind1, ind2).shape[0]. ^. ```. #### Versions. <details>. anndata 0.7.5. scanpy 1.7.0. sinfo 0.3.1. -----. PIL 8.1.0. PyQt5 NA. anndata 0.7.5. appdirs 1.4.4. cffi 1.14.4. colorama 0.4.4. constants NA. cycler 0.10.0. cython_runtime NA. dateutil 2.8.1. get_version 2.1. google NA. h5py 3.1.0. highs_wrapper NA. joblib 1.0.1. kiwisolver 1.3.1. legacy_api_wrap 1.2. llvmlite 0.34.0. matplotlib 3.3.4. mpl_toolkits NA. natsort 7.1.1. numba 0.51.2. numexpr 2.7.2. numpy 1.20.0. packaging",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:11050,performance,CPU,CPUDispatcher,11050," call $4load_method.1(arr, func=$4load_method.1, args=[Var(arr, sparse.py:28)], kws=(), vararg=None)"" at /home/gabriel/.local/lib/python3.9/site-packages/pynndescent/sparse.py (28). During: resolving callee type: type(CPUDispatcher(<function arr_unique at 0x7f4574e5aa60>)). During: typing of call at /home/gabriel/.local/lib/python3.9/site-packages/pynndescent/sparse.py (41). During: resolving callee type: type(CPUDispatcher(<function arr_unique at 0x7f4574e5aa60>)). During: typing of call at /home/gabriel/.local/lib/python3.9/site-packages/pynndescent/sparse.py (41). File ""../../../../.local/lib/python3.9/site-packages/pynndescent/sparse.py"", line 41:. def arr_union(ar1, ar2):. <source elided>. else:. return arr_unique(np.concatenate((ar1, ar2))). ^. During: resolving callee type: type(CPUDispatcher(<function arr_union at 0x7f4574e5ac10>)). During: typing of call at /home/gabriel/.local/lib/python3.9/site-packages/pynndescent/sparse.py (344). During: resolving callee type: type(CPUDispatcher(<function arr_union at 0x7f4574e5ac10>)). During: typing of call at /home/gabriel/.local/lib/python3.9/site-packages/pynndescent/sparse.py (344). File ""../../../../.local/lib/python3.9/site-packages/pynndescent/sparse.py"", line 344:. def sparse_alternative_jaccard(ind1, data1, ind2, data2):. num_non_zero = arr_union(ind1, ind2).shape[0]. ^. ```. #### Versions. <details>. anndata 0.7.5. scanpy 1.7.0. sinfo 0.3.1. -----. PIL 8.1.0. PyQt5 NA. anndata 0.7.5. appdirs 1.4.4. cffi 1.14.4. colorama 0.4.4. constants NA. cycler 0.10.0. cython_runtime NA. dateutil 2.8.1. get_version 2.1. google NA. h5py 3.1.0. highs_wrapper NA. joblib 1.0.1. kiwisolver 1.3.1. legacy_api_wrap 1.2. llvmlite 0.34.0. matplotlib 3.3.4. mpl_toolkits NA. natsort 7.1.1. numba 0.51.2. numexpr 2.7.2. numpy 1.20.0. packaging 20.8. pandas 1.2.2. pkg_resources NA. psutil 5.8.0. pyexpat NA. pyparsing 2.4.7. pytoml NA. pytz 2021.1. scanpy 1.7.0. scipy 1.6.0. setuptools_scm NA. sinfo 0.3.1. sip NA. six 1.15.0. sklearn 0.2",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:13,reliability,Fail,Failed,13,"TypingError: Failed in nopython mode pipeline (step: nopython frontend); Hi, I am working with a big dataset over 100000 cells and I run into a problem when computing the neigbours. Find below an small example using a random matrix. . Observations,. when I run N_cells = 8000 it works perfectly for gauss metric. when running N_cells = 10000, the neighbors computation crashes. The problem is even more dramatic for method=""umap"". . ### Minimal code sample (that we can copy&paste without having any data). ```python. import scanpy. import numpy. matrix = numpy.random.uniform(size=[10000,1000]). adata = scanpy.AnnData(matrix). scanpy.pp.pca(adata,n_comps=10). scanpy.pp.neighbors(adata,method=""gauss"",n_pcs=10). ```. ```pytb. ---------------------------------------------------------------------------. TypingError Traceback (most recent call last). <ipython-input-3-692f9d70cb0c> in <module>. 9 . 10 scanpy.pp.pca(adata,n_comps=10). ---> 11 scanpy.pp.neighbors(adata,method=""gauss"",n_pcs=10). ~/.local/lib/python3.9/site-packages/scanpy/neighbors/__init__.py in neighbors(adata, n_neighbors, n_pcs, use_rep, knn, random_state, method, metric, metric_kwds, key_added, copy). 118 adata._init_as_actual(adata.copy()). 119 neighbors = Neighbors(adata). --> 120 neighbors.compute_neighbors(. 121 n_neighbors=n_neighbors, knn=knn, n_pcs=n_pcs, use_rep=use_rep,. 122 method=method, metric=metric, metric_kwds=metric_kwds,. ~/.local/lib/python3.9/site-packages/scanpy/neighbors/__init__.py in compute_neighbors(self, n_neighbors, knn, n_pcs, use_rep, method, random_state, write_knn_indices, metric, metric_kwds). 732 X = pairwise_distances(X, metric=metric, **metric_kwds). 733 metric = 'precomputed'. --> 734 knn_indices, knn_distances, forest = compute_neighbors_umap(. 735 X, n_neighbors, random_state, metric=metric, metric_kwds=metric_kwds). 736 # very cautious here. ~/.local/lib/python3.9/site-packages/scanpy/neighbors/__init__.py in compute_neighbors_umap(X, n_neighbors, random_state, metric, m",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:5723,reliability,availab,available,5723,"l/lib/python3.9/site-packages/numba/core/compiler.py in compile_extra(typingctx, targetctx, func, args, return_type, flags, locals, library, pipeline_class). 625 pipeline = pipeline_class(typingctx, targetctx, library,. 626 args, return_type, flags, locals). --> 627 return pipeline.compile_extra(func). 628 . 629 . ~/.local/lib/python3.9/site-packages/numba/core/compiler.py in compile_extra(self, func). 361 self.state.lifted = (). 362 self.state.lifted_from = None. --> 363 return self._compile_bytecode(). 364 . 365 def compile_ir(self, func_ir, lifted=(), lifted_from=None):. ~/.local/lib/python3.9/site-packages/numba/core/compiler.py in _compile_bytecode(self). 423 """""". 424 assert self.state.func_ir is None. --> 425 return self._compile_core(). 426 . 427 def _compile_ir(self):. ~/.local/lib/python3.9/site-packages/numba/core/compiler.py in _compile_core(self). 403 self.state.status.fail_reason = e. 404 if is_final_pipeline:. --> 405 raise e. 406 else:. 407 raise CompilerError(""All available pipelines exhausted""). ~/.local/lib/python3.9/site-packages/numba/core/compiler.py in _compile_core(self). 394 res = None. 395 try:. --> 396 pm.run(self.state). 397 if self.state.cr is not None:. 398 break. ~/.local/lib/python3.9/site-packages/numba/core/compiler_machinery.py in run(self, state). 339 (self.pipeline_name, pass_desc). 340 patched_exception = self._patch_error(msg, e). --> 341 raise patched_exception. 342 . 343 def dependency_analysis(self):. ~/.local/lib/python3.9/site-packages/numba/core/compiler_machinery.py in run(self, state). 330 pass_inst = _pass_registry.get(pss).pass_inst. 331 if isinstance(pass_inst, CompilerPass):. --> 332 self._runPass(idx, pass_inst, state). 333 else:. 334 raise BaseException(""Legacy pass in use""). ~/.local/lib/python3.9/site-packages/numba/core/compiler_lock.py in _acquire_compile_lock(*args, **kwargs). 30 def _acquire_compile_lock(*args, **kwargs):. 31 with self:. ---> 32 return func(*args, **kwargs). 33 return _acquire_compile_lock. 3",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:8226,reliability,Fail,Failed,8226,"c, compiler_state):. --> 264 mangled = func(compiler_state). 265 if mangled not in (True, False):. 266 msg = (""CompilerPass implementations should return True/False. "". ~/.local/lib/python3.9/site-packages/numba/core/typed_passes.py in run_pass(self, state). 90 % (state.func_id.func_name,)):. 91 # Type inference. ---> 92 typemap, return_type, calltypes = type_inference_stage(. 93 state.typingctx,. 94 state.func_ir,. ~/.local/lib/python3.9/site-packages/numba/core/typed_passes.py in type_inference_stage(typingctx, interp, args, return_type, locals, raise_errors). 68 . 69 infer.build_constraint(). ---> 70 infer.propagate(raise_errors=raise_errors). 71 typemap, restype, calltypes = infer.unify(raise_errors=raise_errors). 72 . ~/.local/lib/python3.9/site-packages/numba/core/typeinfer.py in propagate(self, raise_errors). 1069 if isinstance(e, ForceLiteralArg)]. 1070 if not force_lit_args:. -> 1071 raise errors[0]. 1072 else:. 1073 raise reduce(operator.or_, force_lit_args). TypingError: Failed in nopython mode pipeline (step: nopython frontend). Failed in nopython mode pipeline (step: nopython frontend). Failed in nopython mode pipeline (step: nopython mode backend). Failed in nopython mode pipeline (step: nopython mode backend). Failed in nopython mode pipeline (step: nopython frontend). No implementation of function Function(<function make_quicksort_impl.<locals>.run_quicksort at 0x7f455d24c280>) found for signature:. . >>> run_quicksort(array(int32, 1d, C)). . There are 2 candidate implementations:. - Of which 2 did not match due to:. Overload in function 'register_jitable.<locals>.wrap.<locals>.ov_wrap': File: numba/core/extending.py: Line 150. With argument(s): '(array(int32, 1d, C))':. Rejected as the implementation raised a specific error:. UnsupportedError: Failed in nopython mode pipeline (step: analyzing bytecode). Use of unsupported opcode (LOAD_ASSERTION_ERROR) found. . File ""../../../../.local/lib/python3.9/site-packages/numba/misc/quicksort.py"", line 180:. ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:8286,reliability,Fail,Failed,8286," 265 if mangled not in (True, False):. 266 msg = (""CompilerPass implementations should return True/False. "". ~/.local/lib/python3.9/site-packages/numba/core/typed_passes.py in run_pass(self, state). 90 % (state.func_id.func_name,)):. 91 # Type inference. ---> 92 typemap, return_type, calltypes = type_inference_stage(. 93 state.typingctx,. 94 state.func_ir,. ~/.local/lib/python3.9/site-packages/numba/core/typed_passes.py in type_inference_stage(typingctx, interp, args, return_type, locals, raise_errors). 68 . 69 infer.build_constraint(). ---> 70 infer.propagate(raise_errors=raise_errors). 71 typemap, restype, calltypes = infer.unify(raise_errors=raise_errors). 72 . ~/.local/lib/python3.9/site-packages/numba/core/typeinfer.py in propagate(self, raise_errors). 1069 if isinstance(e, ForceLiteralArg)]. 1070 if not force_lit_args:. -> 1071 raise errors[0]. 1072 else:. 1073 raise reduce(operator.or_, force_lit_args). TypingError: Failed in nopython mode pipeline (step: nopython frontend). Failed in nopython mode pipeline (step: nopython frontend). Failed in nopython mode pipeline (step: nopython mode backend). Failed in nopython mode pipeline (step: nopython mode backend). Failed in nopython mode pipeline (step: nopython frontend). No implementation of function Function(<function make_quicksort_impl.<locals>.run_quicksort at 0x7f455d24c280>) found for signature:. . >>> run_quicksort(array(int32, 1d, C)). . There are 2 candidate implementations:. - Of which 2 did not match due to:. Overload in function 'register_jitable.<locals>.wrap.<locals>.ov_wrap': File: numba/core/extending.py: Line 150. With argument(s): '(array(int32, 1d, C))':. Rejected as the implementation raised a specific error:. UnsupportedError: Failed in nopython mode pipeline (step: analyzing bytecode). Use of unsupported opcode (LOAD_ASSERTION_ERROR) found. . File ""../../../../.local/lib/python3.9/site-packages/numba/misc/quicksort.py"", line 180:. def run_quicksort(A):. <source elided>. while high - low >= ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:8346,reliability,Fail,Failed,8346,"ass implementations should return True/False. "". ~/.local/lib/python3.9/site-packages/numba/core/typed_passes.py in run_pass(self, state). 90 % (state.func_id.func_name,)):. 91 # Type inference. ---> 92 typemap, return_type, calltypes = type_inference_stage(. 93 state.typingctx,. 94 state.func_ir,. ~/.local/lib/python3.9/site-packages/numba/core/typed_passes.py in type_inference_stage(typingctx, interp, args, return_type, locals, raise_errors). 68 . 69 infer.build_constraint(). ---> 70 infer.propagate(raise_errors=raise_errors). 71 typemap, restype, calltypes = infer.unify(raise_errors=raise_errors). 72 . ~/.local/lib/python3.9/site-packages/numba/core/typeinfer.py in propagate(self, raise_errors). 1069 if isinstance(e, ForceLiteralArg)]. 1070 if not force_lit_args:. -> 1071 raise errors[0]. 1072 else:. 1073 raise reduce(operator.or_, force_lit_args). TypingError: Failed in nopython mode pipeline (step: nopython frontend). Failed in nopython mode pipeline (step: nopython frontend). Failed in nopython mode pipeline (step: nopython mode backend). Failed in nopython mode pipeline (step: nopython mode backend). Failed in nopython mode pipeline (step: nopython frontend). No implementation of function Function(<function make_quicksort_impl.<locals>.run_quicksort at 0x7f455d24c280>) found for signature:. . >>> run_quicksort(array(int32, 1d, C)). . There are 2 candidate implementations:. - Of which 2 did not match due to:. Overload in function 'register_jitable.<locals>.wrap.<locals>.ov_wrap': File: numba/core/extending.py: Line 150. With argument(s): '(array(int32, 1d, C))':. Rejected as the implementation raised a specific error:. UnsupportedError: Failed in nopython mode pipeline (step: analyzing bytecode). Use of unsupported opcode (LOAD_ASSERTION_ERROR) found. . File ""../../../../.local/lib/python3.9/site-packages/numba/misc/quicksort.py"", line 180:. def run_quicksort(A):. <source elided>. while high - low >= SMALL_QUICKSORT:. assert n < MAX_STACK. ^. . raised from /ho",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:8410,reliability,Fail,Failed,8410,"thon3.9/site-packages/numba/core/typed_passes.py in run_pass(self, state). 90 % (state.func_id.func_name,)):. 91 # Type inference. ---> 92 typemap, return_type, calltypes = type_inference_stage(. 93 state.typingctx,. 94 state.func_ir,. ~/.local/lib/python3.9/site-packages/numba/core/typed_passes.py in type_inference_stage(typingctx, interp, args, return_type, locals, raise_errors). 68 . 69 infer.build_constraint(). ---> 70 infer.propagate(raise_errors=raise_errors). 71 typemap, restype, calltypes = infer.unify(raise_errors=raise_errors). 72 . ~/.local/lib/python3.9/site-packages/numba/core/typeinfer.py in propagate(self, raise_errors). 1069 if isinstance(e, ForceLiteralArg)]. 1070 if not force_lit_args:. -> 1071 raise errors[0]. 1072 else:. 1073 raise reduce(operator.or_, force_lit_args). TypingError: Failed in nopython mode pipeline (step: nopython frontend). Failed in nopython mode pipeline (step: nopython frontend). Failed in nopython mode pipeline (step: nopython mode backend). Failed in nopython mode pipeline (step: nopython mode backend). Failed in nopython mode pipeline (step: nopython frontend). No implementation of function Function(<function make_quicksort_impl.<locals>.run_quicksort at 0x7f455d24c280>) found for signature:. . >>> run_quicksort(array(int32, 1d, C)). . There are 2 candidate implementations:. - Of which 2 did not match due to:. Overload in function 'register_jitable.<locals>.wrap.<locals>.ov_wrap': File: numba/core/extending.py: Line 150. With argument(s): '(array(int32, 1d, C))':. Rejected as the implementation raised a specific error:. UnsupportedError: Failed in nopython mode pipeline (step: analyzing bytecode). Use of unsupported opcode (LOAD_ASSERTION_ERROR) found. . File ""../../../../.local/lib/python3.9/site-packages/numba/misc/quicksort.py"", line 180:. def run_quicksort(A):. <source elided>. while high - low >= SMALL_QUICKSORT:. assert n < MAX_STACK. ^. . raised from /home/gabriel/.local/lib/python3.9/site-packages/numba/core/byteflo",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:8474,reliability,Fail,Failed,8474,"f, state). 90 % (state.func_id.func_name,)):. 91 # Type inference. ---> 92 typemap, return_type, calltypes = type_inference_stage(. 93 state.typingctx,. 94 state.func_ir,. ~/.local/lib/python3.9/site-packages/numba/core/typed_passes.py in type_inference_stage(typingctx, interp, args, return_type, locals, raise_errors). 68 . 69 infer.build_constraint(). ---> 70 infer.propagate(raise_errors=raise_errors). 71 typemap, restype, calltypes = infer.unify(raise_errors=raise_errors). 72 . ~/.local/lib/python3.9/site-packages/numba/core/typeinfer.py in propagate(self, raise_errors). 1069 if isinstance(e, ForceLiteralArg)]. 1070 if not force_lit_args:. -> 1071 raise errors[0]. 1072 else:. 1073 raise reduce(operator.or_, force_lit_args). TypingError: Failed in nopython mode pipeline (step: nopython frontend). Failed in nopython mode pipeline (step: nopython frontend). Failed in nopython mode pipeline (step: nopython mode backend). Failed in nopython mode pipeline (step: nopython mode backend). Failed in nopython mode pipeline (step: nopython frontend). No implementation of function Function(<function make_quicksort_impl.<locals>.run_quicksort at 0x7f455d24c280>) found for signature:. . >>> run_quicksort(array(int32, 1d, C)). . There are 2 candidate implementations:. - Of which 2 did not match due to:. Overload in function 'register_jitable.<locals>.wrap.<locals>.ov_wrap': File: numba/core/extending.py: Line 150. With argument(s): '(array(int32, 1d, C))':. Rejected as the implementation raised a specific error:. UnsupportedError: Failed in nopython mode pipeline (step: analyzing bytecode). Use of unsupported opcode (LOAD_ASSERTION_ERROR) found. . File ""../../../../.local/lib/python3.9/site-packages/numba/misc/quicksort.py"", line 180:. def run_quicksort(A):. <source elided>. while high - low >= SMALL_QUICKSORT:. assert n < MAX_STACK. ^. . raised from /home/gabriel/.local/lib/python3.9/site-packages/numba/core/byteflow.py:269. During: resolving callee type: Function(<function make",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:9020,reliability,Fail,Failed,9020,"in propagate(self, raise_errors). 1069 if isinstance(e, ForceLiteralArg)]. 1070 if not force_lit_args:. -> 1071 raise errors[0]. 1072 else:. 1073 raise reduce(operator.or_, force_lit_args). TypingError: Failed in nopython mode pipeline (step: nopython frontend). Failed in nopython mode pipeline (step: nopython frontend). Failed in nopython mode pipeline (step: nopython mode backend). Failed in nopython mode pipeline (step: nopython mode backend). Failed in nopython mode pipeline (step: nopython frontend). No implementation of function Function(<function make_quicksort_impl.<locals>.run_quicksort at 0x7f455d24c280>) found for signature:. . >>> run_quicksort(array(int32, 1d, C)). . There are 2 candidate implementations:. - Of which 2 did not match due to:. Overload in function 'register_jitable.<locals>.wrap.<locals>.ov_wrap': File: numba/core/extending.py: Line 150. With argument(s): '(array(int32, 1d, C))':. Rejected as the implementation raised a specific error:. UnsupportedError: Failed in nopython mode pipeline (step: analyzing bytecode). Use of unsupported opcode (LOAD_ASSERTION_ERROR) found. . File ""../../../../.local/lib/python3.9/site-packages/numba/misc/quicksort.py"", line 180:. def run_quicksort(A):. <source elided>. while high - low >= SMALL_QUICKSORT:. assert n < MAX_STACK. ^. . raised from /home/gabriel/.local/lib/python3.9/site-packages/numba/core/byteflow.py:269. During: resolving callee type: Function(<function make_quicksort_impl.<locals>.run_quicksort at 0x7f455d24c280>). During: typing of call at /home/gabriel/.local/lib/python3.9/site-packages/numba/np/arrayobj.py (5007). File ""../../../../.local/lib/python3.9/site-packages/numba/np/arrayobj.py"", line 5007:. def array_sort_impl(arr):. <source elided>. # Note we clobber the return value. sort_func(arr). ^. During: lowering ""$14call_method.5 = call $12load_method.4(func=$12load_method.4, args=[], kws=(), vararg=None)"" at /home/gabriel/.local/lib/python3.9/site-packages/numba/np/arrayobj.py (5017). D",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:861,safety,input,input-,861,"TypingError: Failed in nopython mode pipeline (step: nopython frontend); Hi, I am working with a big dataset over 100000 cells and I run into a problem when computing the neigbours. Find below an small example using a random matrix. . Observations,. when I run N_cells = 8000 it works perfectly for gauss metric. when running N_cells = 10000, the neighbors computation crashes. The problem is even more dramatic for method=""umap"". . ### Minimal code sample (that we can copy&paste without having any data). ```python. import scanpy. import numpy. matrix = numpy.random.uniform(size=[10000,1000]). adata = scanpy.AnnData(matrix). scanpy.pp.pca(adata,n_comps=10). scanpy.pp.neighbors(adata,method=""gauss"",n_pcs=10). ```. ```pytb. ---------------------------------------------------------------------------. TypingError Traceback (most recent call last). <ipython-input-3-692f9d70cb0c> in <module>. 9 . 10 scanpy.pp.pca(adata,n_comps=10). ---> 11 scanpy.pp.neighbors(adata,method=""gauss"",n_pcs=10). ~/.local/lib/python3.9/site-packages/scanpy/neighbors/__init__.py in neighbors(adata, n_neighbors, n_pcs, use_rep, knn, random_state, method, metric, metric_kwds, key_added, copy). 118 adata._init_as_actual(adata.copy()). 119 neighbors = Neighbors(adata). --> 120 neighbors.compute_neighbors(. 121 n_neighbors=n_neighbors, knn=knn, n_pcs=n_pcs, use_rep=use_rep,. 122 method=method, metric=metric, metric_kwds=metric_kwds,. ~/.local/lib/python3.9/site-packages/scanpy/neighbors/__init__.py in compute_neighbors(self, n_neighbors, knn, n_pcs, use_rep, method, random_state, write_knn_indices, metric, metric_kwds). 732 X = pairwise_distances(X, metric=metric, **metric_kwds). 733 metric = 'precomputed'. --> 734 knn_indices, knn_distances, forest = compute_neighbors_umap(. 735 X, n_neighbors, random_state, metric=metric, metric_kwds=metric_kwds). 736 # very cautious here. ~/.local/lib/python3.9/site-packages/scanpy/neighbors/__init__.py in compute_neighbors_umap(X, n_neighbors, random_state, metric, m",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:887,safety,modul,module,887,"TypingError: Failed in nopython mode pipeline (step: nopython frontend); Hi, I am working with a big dataset over 100000 cells and I run into a problem when computing the neigbours. Find below an small example using a random matrix. . Observations,. when I run N_cells = 8000 it works perfectly for gauss metric. when running N_cells = 10000, the neighbors computation crashes. The problem is even more dramatic for method=""umap"". . ### Minimal code sample (that we can copy&paste without having any data). ```python. import scanpy. import numpy. matrix = numpy.random.uniform(size=[10000,1000]). adata = scanpy.AnnData(matrix). scanpy.pp.pca(adata,n_comps=10). scanpy.pp.neighbors(adata,method=""gauss"",n_pcs=10). ```. ```pytb. ---------------------------------------------------------------------------. TypingError Traceback (most recent call last). <ipython-input-3-692f9d70cb0c> in <module>. 9 . 10 scanpy.pp.pca(adata,n_comps=10). ---> 11 scanpy.pp.neighbors(adata,method=""gauss"",n_pcs=10). ~/.local/lib/python3.9/site-packages/scanpy/neighbors/__init__.py in neighbors(adata, n_neighbors, n_pcs, use_rep, knn, random_state, method, metric, metric_kwds, key_added, copy). 118 adata._init_as_actual(adata.copy()). 119 neighbors = Neighbors(adata). --> 120 neighbors.compute_neighbors(. 121 n_neighbors=n_neighbors, knn=knn, n_pcs=n_pcs, use_rep=use_rep,. 122 method=method, metric=metric, metric_kwds=metric_kwds,. ~/.local/lib/python3.9/site-packages/scanpy/neighbors/__init__.py in compute_neighbors(self, n_neighbors, knn, n_pcs, use_rep, method, random_state, write_knn_indices, metric, metric_kwds). 732 X = pairwise_distances(X, metric=metric, **metric_kwds). 733 metric = 'precomputed'. --> 734 knn_indices, knn_distances, forest = compute_neighbors_umap(. 735 X, n_neighbors, random_state, metric=metric, metric_kwds=metric_kwds). 736 # very cautious here. ~/.local/lib/python3.9/site-packages/scanpy/neighbors/__init__.py in compute_neighbors_umap(X, n_neighbors, random_state, metric, m",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:2289,safety,modul,module,2289,"1 n_neighbors=n_neighbors, knn=knn, n_pcs=n_pcs, use_rep=use_rep,. 122 method=method, metric=metric, metric_kwds=metric_kwds,. ~/.local/lib/python3.9/site-packages/scanpy/neighbors/__init__.py in compute_neighbors(self, n_neighbors, knn, n_pcs, use_rep, method, random_state, write_knn_indices, metric, metric_kwds). 732 X = pairwise_distances(X, metric=metric, **metric_kwds). 733 metric = 'precomputed'. --> 734 knn_indices, knn_distances, forest = compute_neighbors_umap(. 735 X, n_neighbors, random_state, metric=metric, metric_kwds=metric_kwds). 736 # very cautious here. ~/.local/lib/python3.9/site-packages/scanpy/neighbors/__init__.py in compute_neighbors_umap(X, n_neighbors, random_state, metric, metric_kwds, angular, verbose). 273 # umap 0.5.0. 274 warnings.filterwarnings(""ignore"", message=r""Tensorflow not installed""). --> 275 from umap.umap_ import nearest_neighbors. 276 . 277 random_state = check_random_state(random_state). ~/.local/lib/python3.9/site-packages/umap/umap_.py in <module>. 45 ). 46 . ---> 47 from pynndescent import NNDescent. 48 from pynndescent.distances import named_distances as pynn_named_distances. 49 from pynndescent.sparse import sparse_named_distances as pynn_sparse_named_distances. ~/.local/lib/python3.9/site-packages/pynndescent/__init__.py in <module>. 1 import pkg_resources. 2 import numba. ----> 3 from .pynndescent_ import NNDescent, PyNNDescentTransformer. 4 . 5 # Workaround: https://github.com/numba/numba/issues/3341. ~/.local/lib/python3.9/site-packages/pynndescent/pynndescent_.py in <module>. 19 import heapq. 20 . ---> 21 import pynndescent.sparse as sparse. 22 import pynndescent.sparse_nndescent as sparse_nnd. 23 import pynndescent.distances as pynnd_dist. ~/.local/lib/python3.9/site-packages/pynndescent/sparse.py in <module>. 341 },. 342 ). --> 343 def sparse_alternative_jaccard(ind1, data1, ind2, data2):. 344 num_non_zero = arr_union(ind1, ind2).shape[0]. 345 num_equal = arr_intersect(ind1, ind2).shape[0]. ~/.local/lib/python3.9/",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:2584,safety,modul,module,2584,"metric, metric_kwds). 732 X = pairwise_distances(X, metric=metric, **metric_kwds). 733 metric = 'precomputed'. --> 734 knn_indices, knn_distances, forest = compute_neighbors_umap(. 735 X, n_neighbors, random_state, metric=metric, metric_kwds=metric_kwds). 736 # very cautious here. ~/.local/lib/python3.9/site-packages/scanpy/neighbors/__init__.py in compute_neighbors_umap(X, n_neighbors, random_state, metric, metric_kwds, angular, verbose). 273 # umap 0.5.0. 274 warnings.filterwarnings(""ignore"", message=r""Tensorflow not installed""). --> 275 from umap.umap_ import nearest_neighbors. 276 . 277 random_state = check_random_state(random_state). ~/.local/lib/python3.9/site-packages/umap/umap_.py in <module>. 45 ). 46 . ---> 47 from pynndescent import NNDescent. 48 from pynndescent.distances import named_distances as pynn_named_distances. 49 from pynndescent.sparse import sparse_named_distances as pynn_sparse_named_distances. ~/.local/lib/python3.9/site-packages/pynndescent/__init__.py in <module>. 1 import pkg_resources. 2 import numba. ----> 3 from .pynndescent_ import NNDescent, PyNNDescentTransformer. 4 . 5 # Workaround: https://github.com/numba/numba/issues/3341. ~/.local/lib/python3.9/site-packages/pynndescent/pynndescent_.py in <module>. 19 import heapq. 20 . ---> 21 import pynndescent.sparse as sparse. 22 import pynndescent.sparse_nndescent as sparse_nnd. 23 import pynndescent.distances as pynnd_dist. ~/.local/lib/python3.9/site-packages/pynndescent/sparse.py in <module>. 341 },. 342 ). --> 343 def sparse_alternative_jaccard(ind1, data1, ind2, data2):. 344 num_non_zero = arr_union(ind1, ind2).shape[0]. 345 num_equal = arr_intersect(ind1, ind2).shape[0]. ~/.local/lib/python3.9/site-packages/numba/core/decorators.py in wrapper(func). 216 with typeinfer.register_dispatcher(disp):. 217 for sig in sigs:. --> 218 disp.compile(sig). 219 disp.disable_compile(). 220 return disp. ~/.local/lib/python3.9/site-packages/numba/core/compiler_lock.py in _acquire_compile_lock(*args, ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:2835,safety,modul,module,2835,"ds). 736 # very cautious here. ~/.local/lib/python3.9/site-packages/scanpy/neighbors/__init__.py in compute_neighbors_umap(X, n_neighbors, random_state, metric, metric_kwds, angular, verbose). 273 # umap 0.5.0. 274 warnings.filterwarnings(""ignore"", message=r""Tensorflow not installed""). --> 275 from umap.umap_ import nearest_neighbors. 276 . 277 random_state = check_random_state(random_state). ~/.local/lib/python3.9/site-packages/umap/umap_.py in <module>. 45 ). 46 . ---> 47 from pynndescent import NNDescent. 48 from pynndescent.distances import named_distances as pynn_named_distances. 49 from pynndescent.sparse import sparse_named_distances as pynn_sparse_named_distances. ~/.local/lib/python3.9/site-packages/pynndescent/__init__.py in <module>. 1 import pkg_resources. 2 import numba. ----> 3 from .pynndescent_ import NNDescent, PyNNDescentTransformer. 4 . 5 # Workaround: https://github.com/numba/numba/issues/3341. ~/.local/lib/python3.9/site-packages/pynndescent/pynndescent_.py in <module>. 19 import heapq. 20 . ---> 21 import pynndescent.sparse as sparse. 22 import pynndescent.sparse_nndescent as sparse_nnd. 23 import pynndescent.distances as pynnd_dist. ~/.local/lib/python3.9/site-packages/pynndescent/sparse.py in <module>. 341 },. 342 ). --> 343 def sparse_alternative_jaccard(ind1, data1, ind2, data2):. 344 num_non_zero = arr_union(ind1, ind2).shape[0]. 345 num_equal = arr_intersect(ind1, ind2).shape[0]. ~/.local/lib/python3.9/site-packages/numba/core/decorators.py in wrapper(func). 216 with typeinfer.register_dispatcher(disp):. 217 for sig in sigs:. --> 218 disp.compile(sig). 219 disp.disable_compile(). 220 return disp. ~/.local/lib/python3.9/site-packages/numba/core/compiler_lock.py in _acquire_compile_lock(*args, **kwargs). 30 def _acquire_compile_lock(*args, **kwargs):. 31 with self:. ---> 32 return func(*args, **kwargs). 33 return _acquire_compile_lock. 34 . ~/.local/lib/python3.9/site-packages/numba/core/dispatcher.py in compile(self, sig). 817 self._cache_",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:3075,safety,modul,module,3075,"ignore"", message=r""Tensorflow not installed""). --> 275 from umap.umap_ import nearest_neighbors. 276 . 277 random_state = check_random_state(random_state). ~/.local/lib/python3.9/site-packages/umap/umap_.py in <module>. 45 ). 46 . ---> 47 from pynndescent import NNDescent. 48 from pynndescent.distances import named_distances as pynn_named_distances. 49 from pynndescent.sparse import sparse_named_distances as pynn_sparse_named_distances. ~/.local/lib/python3.9/site-packages/pynndescent/__init__.py in <module>. 1 import pkg_resources. 2 import numba. ----> 3 from .pynndescent_ import NNDescent, PyNNDescentTransformer. 4 . 5 # Workaround: https://github.com/numba/numba/issues/3341. ~/.local/lib/python3.9/site-packages/pynndescent/pynndescent_.py in <module>. 19 import heapq. 20 . ---> 21 import pynndescent.sparse as sparse. 22 import pynndescent.sparse_nndescent as sparse_nnd. 23 import pynndescent.distances as pynnd_dist. ~/.local/lib/python3.9/site-packages/pynndescent/sparse.py in <module>. 341 },. 342 ). --> 343 def sparse_alternative_jaccard(ind1, data1, ind2, data2):. 344 num_non_zero = arr_union(ind1, ind2).shape[0]. 345 num_equal = arr_intersect(ind1, ind2).shape[0]. ~/.local/lib/python3.9/site-packages/numba/core/decorators.py in wrapper(func). 216 with typeinfer.register_dispatcher(disp):. 217 for sig in sigs:. --> 218 disp.compile(sig). 219 disp.disable_compile(). 220 return disp. ~/.local/lib/python3.9/site-packages/numba/core/compiler_lock.py in _acquire_compile_lock(*args, **kwargs). 30 def _acquire_compile_lock(*args, **kwargs):. 31 with self:. ---> 32 return func(*args, **kwargs). 33 return _acquire_compile_lock. 34 . ~/.local/lib/python3.9/site-packages/numba/core/dispatcher.py in compile(self, sig). 817 self._cache_misses[sig] += 1. 818 try:. --> 819 cres = self._compiler.compile(args, return_type). 820 except errors.ForceLiteralArg as e:. 821 def folded(args, kws):. ~/.local/lib/python3.9/site-packages/numba/core/dispatcher.py in compile(self, args, ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:3928,safety,except,except,3928,"t.sparse_nndescent as sparse_nnd. 23 import pynndescent.distances as pynnd_dist. ~/.local/lib/python3.9/site-packages/pynndescent/sparse.py in <module>. 341 },. 342 ). --> 343 def sparse_alternative_jaccard(ind1, data1, ind2, data2):. 344 num_non_zero = arr_union(ind1, ind2).shape[0]. 345 num_equal = arr_intersect(ind1, ind2).shape[0]. ~/.local/lib/python3.9/site-packages/numba/core/decorators.py in wrapper(func). 216 with typeinfer.register_dispatcher(disp):. 217 for sig in sigs:. --> 218 disp.compile(sig). 219 disp.disable_compile(). 220 return disp. ~/.local/lib/python3.9/site-packages/numba/core/compiler_lock.py in _acquire_compile_lock(*args, **kwargs). 30 def _acquire_compile_lock(*args, **kwargs):. 31 with self:. ---> 32 return func(*args, **kwargs). 33 return _acquire_compile_lock. 34 . ~/.local/lib/python3.9/site-packages/numba/core/dispatcher.py in compile(self, sig). 817 self._cache_misses[sig] += 1. 818 try:. --> 819 cres = self._compiler.compile(args, return_type). 820 except errors.ForceLiteralArg as e:. 821 def folded(args, kws):. ~/.local/lib/python3.9/site-packages/numba/core/dispatcher.py in compile(self, args, return_type). 80 return retval. 81 else:. ---> 82 raise retval. 83 . 84 def _compile_cached(self, args, return_type):. ~/.local/lib/python3.9/site-packages/numba/core/dispatcher.py in _compile_cached(self, args, return_type). 90 . 91 try:. ---> 92 retval = self._compile_core(args, return_type). 93 except errors.TypingError as e:. 94 self._failed_cache[key] = e. ~/.local/lib/python3.9/site-packages/numba/core/dispatcher.py in _compile_core(self, args, return_type). 103 . 104 impl = self._get_implementation(args, {}). --> 105 cres = compiler.compile_extra(self.targetdescr.typing_context,. 106 self.targetdescr.target_context,. 107 impl,. ~/.local/lib/python3.9/site-packages/numba/core/compiler.py in compile_extra(typingctx, targetctx, func, args, return_type, flags, locals, library, pipeline_class). 625 pipeline = pipeline_class(typingctx, targ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:3935,safety,error,errors,3935,"e_nndescent as sparse_nnd. 23 import pynndescent.distances as pynnd_dist. ~/.local/lib/python3.9/site-packages/pynndescent/sparse.py in <module>. 341 },. 342 ). --> 343 def sparse_alternative_jaccard(ind1, data1, ind2, data2):. 344 num_non_zero = arr_union(ind1, ind2).shape[0]. 345 num_equal = arr_intersect(ind1, ind2).shape[0]. ~/.local/lib/python3.9/site-packages/numba/core/decorators.py in wrapper(func). 216 with typeinfer.register_dispatcher(disp):. 217 for sig in sigs:. --> 218 disp.compile(sig). 219 disp.disable_compile(). 220 return disp. ~/.local/lib/python3.9/site-packages/numba/core/compiler_lock.py in _acquire_compile_lock(*args, **kwargs). 30 def _acquire_compile_lock(*args, **kwargs):. 31 with self:. ---> 32 return func(*args, **kwargs). 33 return _acquire_compile_lock. 34 . ~/.local/lib/python3.9/site-packages/numba/core/dispatcher.py in compile(self, sig). 817 self._cache_misses[sig] += 1. 818 try:. --> 819 cres = self._compiler.compile(args, return_type). 820 except errors.ForceLiteralArg as e:. 821 def folded(args, kws):. ~/.local/lib/python3.9/site-packages/numba/core/dispatcher.py in compile(self, args, return_type). 80 return retval. 81 else:. ---> 82 raise retval. 83 . 84 def _compile_cached(self, args, return_type):. ~/.local/lib/python3.9/site-packages/numba/core/dispatcher.py in _compile_cached(self, args, return_type). 90 . 91 try:. ---> 92 retval = self._compile_core(args, return_type). 93 except errors.TypingError as e:. 94 self._failed_cache[key] = e. ~/.local/lib/python3.9/site-packages/numba/core/dispatcher.py in _compile_core(self, args, return_type). 103 . 104 impl = self._get_implementation(args, {}). --> 105 cres = compiler.compile_extra(self.targetdescr.typing_context,. 106 self.targetdescr.target_context,. 107 impl,. ~/.local/lib/python3.9/site-packages/numba/core/compiler.py in compile_extra(typingctx, targetctx, func, args, return_type, flags, locals, library, pipeline_class). 625 pipeline = pipeline_class(typingctx, targetctx, ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:4377,safety,except,except,4377,"patcher(disp):. 217 for sig in sigs:. --> 218 disp.compile(sig). 219 disp.disable_compile(). 220 return disp. ~/.local/lib/python3.9/site-packages/numba/core/compiler_lock.py in _acquire_compile_lock(*args, **kwargs). 30 def _acquire_compile_lock(*args, **kwargs):. 31 with self:. ---> 32 return func(*args, **kwargs). 33 return _acquire_compile_lock. 34 . ~/.local/lib/python3.9/site-packages/numba/core/dispatcher.py in compile(self, sig). 817 self._cache_misses[sig] += 1. 818 try:. --> 819 cres = self._compiler.compile(args, return_type). 820 except errors.ForceLiteralArg as e:. 821 def folded(args, kws):. ~/.local/lib/python3.9/site-packages/numba/core/dispatcher.py in compile(self, args, return_type). 80 return retval. 81 else:. ---> 82 raise retval. 83 . 84 def _compile_cached(self, args, return_type):. ~/.local/lib/python3.9/site-packages/numba/core/dispatcher.py in _compile_cached(self, args, return_type). 90 . 91 try:. ---> 92 retval = self._compile_core(args, return_type). 93 except errors.TypingError as e:. 94 self._failed_cache[key] = e. ~/.local/lib/python3.9/site-packages/numba/core/dispatcher.py in _compile_core(self, args, return_type). 103 . 104 impl = self._get_implementation(args, {}). --> 105 cres = compiler.compile_extra(self.targetdescr.typing_context,. 106 self.targetdescr.target_context,. 107 impl,. ~/.local/lib/python3.9/site-packages/numba/core/compiler.py in compile_extra(typingctx, targetctx, func, args, return_type, flags, locals, library, pipeline_class). 625 pipeline = pipeline_class(typingctx, targetctx, library,. 626 args, return_type, flags, locals). --> 627 return pipeline.compile_extra(func). 628 . 629 . ~/.local/lib/python3.9/site-packages/numba/core/compiler.py in compile_extra(self, func). 361 self.state.lifted = (). 362 self.state.lifted_from = None. --> 363 return self._compile_bytecode(). 364 . 365 def compile_ir(self, func_ir, lifted=(), lifted_from=None):. ~/.local/lib/python3.9/site-packages/numba/core/compiler.py in _compile",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:4384,safety,error,errors,4384,"(disp):. 217 for sig in sigs:. --> 218 disp.compile(sig). 219 disp.disable_compile(). 220 return disp. ~/.local/lib/python3.9/site-packages/numba/core/compiler_lock.py in _acquire_compile_lock(*args, **kwargs). 30 def _acquire_compile_lock(*args, **kwargs):. 31 with self:. ---> 32 return func(*args, **kwargs). 33 return _acquire_compile_lock. 34 . ~/.local/lib/python3.9/site-packages/numba/core/dispatcher.py in compile(self, sig). 817 self._cache_misses[sig] += 1. 818 try:. --> 819 cres = self._compiler.compile(args, return_type). 820 except errors.ForceLiteralArg as e:. 821 def folded(args, kws):. ~/.local/lib/python3.9/site-packages/numba/core/dispatcher.py in compile(self, args, return_type). 80 return retval. 81 else:. ---> 82 raise retval. 83 . 84 def _compile_cached(self, args, return_type):. ~/.local/lib/python3.9/site-packages/numba/core/dispatcher.py in _compile_cached(self, args, return_type). 90 . 91 try:. ---> 92 retval = self._compile_core(args, return_type). 93 except errors.TypingError as e:. 94 self._failed_cache[key] = e. ~/.local/lib/python3.9/site-packages/numba/core/dispatcher.py in _compile_core(self, args, return_type). 103 . 104 impl = self._get_implementation(args, {}). --> 105 cres = compiler.compile_extra(self.targetdescr.typing_context,. 106 self.targetdescr.target_context,. 107 impl,. ~/.local/lib/python3.9/site-packages/numba/core/compiler.py in compile_extra(typingctx, targetctx, func, args, return_type, flags, locals, library, pipeline_class). 625 pipeline = pipeline_class(typingctx, targetctx, library,. 626 args, return_type, flags, locals). --> 627 return pipeline.compile_extra(func). 628 . 629 . ~/.local/lib/python3.9/site-packages/numba/core/compiler.py in compile_extra(self, func). 361 self.state.lifted = (). 362 self.state.lifted_from = None. --> 363 return self._compile_bytecode(). 364 . 365 def compile_ir(self, func_ir, lifted=(), lifted_from=None):. ~/.local/lib/python3.9/site-packages/numba/core/compiler.py in _compile_byteco",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:5723,safety,avail,available,5723,"l/lib/python3.9/site-packages/numba/core/compiler.py in compile_extra(typingctx, targetctx, func, args, return_type, flags, locals, library, pipeline_class). 625 pipeline = pipeline_class(typingctx, targetctx, library,. 626 args, return_type, flags, locals). --> 627 return pipeline.compile_extra(func). 628 . 629 . ~/.local/lib/python3.9/site-packages/numba/core/compiler.py in compile_extra(self, func). 361 self.state.lifted = (). 362 self.state.lifted_from = None. --> 363 return self._compile_bytecode(). 364 . 365 def compile_ir(self, func_ir, lifted=(), lifted_from=None):. ~/.local/lib/python3.9/site-packages/numba/core/compiler.py in _compile_bytecode(self). 423 """""". 424 assert self.state.func_ir is None. --> 425 return self._compile_core(). 426 . 427 def _compile_ir(self):. ~/.local/lib/python3.9/site-packages/numba/core/compiler.py in _compile_core(self). 403 self.state.status.fail_reason = e. 404 if is_final_pipeline:. --> 405 raise e. 406 else:. 407 raise CompilerError(""All available pipelines exhausted""). ~/.local/lib/python3.9/site-packages/numba/core/compiler.py in _compile_core(self). 394 res = None. 395 try:. --> 396 pm.run(self.state). 397 if self.state.cr is not None:. 398 break. ~/.local/lib/python3.9/site-packages/numba/core/compiler_machinery.py in run(self, state). 339 (self.pipeline_name, pass_desc). 340 patched_exception = self._patch_error(msg, e). --> 341 raise patched_exception. 342 . 343 def dependency_analysis(self):. ~/.local/lib/python3.9/site-packages/numba/core/compiler_machinery.py in run(self, state). 330 pass_inst = _pass_registry.get(pss).pass_inst. 331 if isinstance(pass_inst, CompilerPass):. --> 332 self._runPass(idx, pass_inst, state). 333 else:. 334 raise BaseException(""Legacy pass in use""). ~/.local/lib/python3.9/site-packages/numba/core/compiler_lock.py in _acquire_compile_lock(*args, **kwargs). 30 def _acquire_compile_lock(*args, **kwargs):. 31 with self:. ---> 32 return func(*args, **kwargs). 33 return _acquire_compile_lock. 3",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:8141,safety,error,errors,8141,"ba/core/compiler_machinery.py in check(func, compiler_state). 262 . 263 def check(func, compiler_state):. --> 264 mangled = func(compiler_state). 265 if mangled not in (True, False):. 266 msg = (""CompilerPass implementations should return True/False. "". ~/.local/lib/python3.9/site-packages/numba/core/typed_passes.py in run_pass(self, state). 90 % (state.func_id.func_name,)):. 91 # Type inference. ---> 92 typemap, return_type, calltypes = type_inference_stage(. 93 state.typingctx,. 94 state.func_ir,. ~/.local/lib/python3.9/site-packages/numba/core/typed_passes.py in type_inference_stage(typingctx, interp, args, return_type, locals, raise_errors). 68 . 69 infer.build_constraint(). ---> 70 infer.propagate(raise_errors=raise_errors). 71 typemap, restype, calltypes = infer.unify(raise_errors=raise_errors). 72 . ~/.local/lib/python3.9/site-packages/numba/core/typeinfer.py in propagate(self, raise_errors). 1069 if isinstance(e, ForceLiteralArg)]. 1070 if not force_lit_args:. -> 1071 raise errors[0]. 1072 else:. 1073 raise reduce(operator.or_, force_lit_args). TypingError: Failed in nopython mode pipeline (step: nopython frontend). Failed in nopython mode pipeline (step: nopython frontend). Failed in nopython mode pipeline (step: nopython mode backend). Failed in nopython mode pipeline (step: nopython mode backend). Failed in nopython mode pipeline (step: nopython frontend). No implementation of function Function(<function make_quicksort_impl.<locals>.run_quicksort at 0x7f455d24c280>) found for signature:. . >>> run_quicksort(array(int32, 1d, C)). . There are 2 candidate implementations:. - Of which 2 did not match due to:. Overload in function 'register_jitable.<locals>.wrap.<locals>.ov_wrap': File: numba/core/extending.py: Line 150. With argument(s): '(array(int32, 1d, C))':. Rejected as the implementation raised a specific error:. UnsupportedError: Failed in nopython mode pipeline (step: analyzing bytecode). Use of unsupported opcode (LOAD_ASSERTION_ERROR) found. . File ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:8994,safety,error,error,8994,"s/numba/core/typeinfer.py in propagate(self, raise_errors). 1069 if isinstance(e, ForceLiteralArg)]. 1070 if not force_lit_args:. -> 1071 raise errors[0]. 1072 else:. 1073 raise reduce(operator.or_, force_lit_args). TypingError: Failed in nopython mode pipeline (step: nopython frontend). Failed in nopython mode pipeline (step: nopython frontend). Failed in nopython mode pipeline (step: nopython mode backend). Failed in nopython mode pipeline (step: nopython mode backend). Failed in nopython mode pipeline (step: nopython frontend). No implementation of function Function(<function make_quicksort_impl.<locals>.run_quicksort at 0x7f455d24c280>) found for signature:. . >>> run_quicksort(array(int32, 1d, C)). . There are 2 candidate implementations:. - Of which 2 did not match due to:. Overload in function 'register_jitable.<locals>.wrap.<locals>.ov_wrap': File: numba/core/extending.py: Line 150. With argument(s): '(array(int32, 1d, C))':. Rejected as the implementation raised a specific error:. UnsupportedError: Failed in nopython mode pipeline (step: analyzing bytecode). Use of unsupported opcode (LOAD_ASSERTION_ERROR) found. . File ""../../../../.local/lib/python3.9/site-packages/numba/misc/quicksort.py"", line 180:. def run_quicksort(A):. <source elided>. while high - low >= SMALL_QUICKSORT:. assert n < MAX_STACK. ^. . raised from /home/gabriel/.local/lib/python3.9/site-packages/numba/core/byteflow.py:269. During: resolving callee type: Function(<function make_quicksort_impl.<locals>.run_quicksort at 0x7f455d24c280>). During: typing of call at /home/gabriel/.local/lib/python3.9/site-packages/numba/np/arrayobj.py (5007). File ""../../../../.local/lib/python3.9/site-packages/numba/np/arrayobj.py"", line 5007:. def array_sort_impl(arr):. <source elided>. # Note we clobber the return value. sort_func(arr). ^. During: lowering ""$14call_method.5 = call $12load_method.4(func=$12load_method.4, args=[], kws=(), vararg=None)"" at /home/gabriel/.local/lib/python3.9/site-packages/numb",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:5723,security,availab,available,5723,"l/lib/python3.9/site-packages/numba/core/compiler.py in compile_extra(typingctx, targetctx, func, args, return_type, flags, locals, library, pipeline_class). 625 pipeline = pipeline_class(typingctx, targetctx, library,. 626 args, return_type, flags, locals). --> 627 return pipeline.compile_extra(func). 628 . 629 . ~/.local/lib/python3.9/site-packages/numba/core/compiler.py in compile_extra(self, func). 361 self.state.lifted = (). 362 self.state.lifted_from = None. --> 363 return self._compile_bytecode(). 364 . 365 def compile_ir(self, func_ir, lifted=(), lifted_from=None):. ~/.local/lib/python3.9/site-packages/numba/core/compiler.py in _compile_bytecode(self). 423 """""". 424 assert self.state.func_ir is None. --> 425 return self._compile_core(). 426 . 427 def _compile_ir(self):. ~/.local/lib/python3.9/site-packages/numba/core/compiler.py in _compile_core(self). 403 self.state.status.fail_reason = e. 404 if is_final_pipeline:. --> 405 raise e. 406 else:. 407 raise CompilerError(""All available pipelines exhausted""). ~/.local/lib/python3.9/site-packages/numba/core/compiler.py in _compile_core(self). 394 res = None. 395 try:. --> 396 pm.run(self.state). 397 if self.state.cr is not None:. 398 break. ~/.local/lib/python3.9/site-packages/numba/core/compiler_machinery.py in run(self, state). 339 (self.pipeline_name, pass_desc). 340 patched_exception = self._patch_error(msg, e). --> 341 raise patched_exception. 342 . 343 def dependency_analysis(self):. ~/.local/lib/python3.9/site-packages/numba/core/compiler_machinery.py in run(self, state). 330 pass_inst = _pass_registry.get(pss).pass_inst. 331 if isinstance(pass_inst, CompilerPass):. --> 332 self._runPass(idx, pass_inst, state). 333 else:. 334 raise BaseException(""Legacy pass in use""). ~/.local/lib/python3.9/site-packages/numba/core/compiler_lock.py in _acquire_compile_lock(*args, **kwargs). 30 def _acquire_compile_lock(*args, **kwargs):. 31 with self:. ---> 32 return func(*args, **kwargs). 33 return _acquire_compile_lock. 3",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:8656,security,sign,signature,8656,"/python3.9/site-packages/numba/core/typed_passes.py in type_inference_stage(typingctx, interp, args, return_type, locals, raise_errors). 68 . 69 infer.build_constraint(). ---> 70 infer.propagate(raise_errors=raise_errors). 71 typemap, restype, calltypes = infer.unify(raise_errors=raise_errors). 72 . ~/.local/lib/python3.9/site-packages/numba/core/typeinfer.py in propagate(self, raise_errors). 1069 if isinstance(e, ForceLiteralArg)]. 1070 if not force_lit_args:. -> 1071 raise errors[0]. 1072 else:. 1073 raise reduce(operator.or_, force_lit_args). TypingError: Failed in nopython mode pipeline (step: nopython frontend). Failed in nopython mode pipeline (step: nopython frontend). Failed in nopython mode pipeline (step: nopython mode backend). Failed in nopython mode pipeline (step: nopython mode backend). Failed in nopython mode pipeline (step: nopython frontend). No implementation of function Function(<function make_quicksort_impl.<locals>.run_quicksort at 0x7f455d24c280>) found for signature:. . >>> run_quicksort(array(int32, 1d, C)). . There are 2 candidate implementations:. - Of which 2 did not match due to:. Overload in function 'register_jitable.<locals>.wrap.<locals>.ov_wrap': File: numba/core/extending.py: Line 150. With argument(s): '(array(int32, 1d, C))':. Rejected as the implementation raised a specific error:. UnsupportedError: Failed in nopython mode pipeline (step: analyzing bytecode). Use of unsupported opcode (LOAD_ASSERTION_ERROR) found. . File ""../../../../.local/lib/python3.9/site-packages/numba/misc/quicksort.py"", line 180:. def run_quicksort(A):. <source elided>. while high - low >= SMALL_QUICKSORT:. assert n < MAX_STACK. ^. . raised from /home/gabriel/.local/lib/python3.9/site-packages/numba/core/byteflow.py:269. During: resolving callee type: Function(<function make_quicksort_impl.<locals>.run_quicksort at 0x7f455d24c280>). During: typing of call at /home/gabriel/.local/lib/python3.9/site-packages/numba/np/arrayobj.py (5007). File ""../../../../.l",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:235,testability,Observ,Observations,235,"TypingError: Failed in nopython mode pipeline (step: nopython frontend); Hi, I am working with a big dataset over 100000 cells and I run into a problem when computing the neigbours. Find below an small example using a random matrix. . Observations,. when I run N_cells = 8000 it works perfectly for gauss metric. when running N_cells = 10000, the neighbors computation crashes. The problem is even more dramatic for method=""umap"". . ### Minimal code sample (that we can copy&paste without having any data). ```python. import scanpy. import numpy. matrix = numpy.random.uniform(size=[10000,1000]). adata = scanpy.AnnData(matrix). scanpy.pp.pca(adata,n_comps=10). scanpy.pp.neighbors(adata,method=""gauss"",n_pcs=10). ```. ```pytb. ---------------------------------------------------------------------------. TypingError Traceback (most recent call last). <ipython-input-3-692f9d70cb0c> in <module>. 9 . 10 scanpy.pp.pca(adata,n_comps=10). ---> 11 scanpy.pp.neighbors(adata,method=""gauss"",n_pcs=10). ~/.local/lib/python3.9/site-packages/scanpy/neighbors/__init__.py in neighbors(adata, n_neighbors, n_pcs, use_rep, knn, random_state, method, metric, metric_kwds, key_added, copy). 118 adata._init_as_actual(adata.copy()). 119 neighbors = Neighbors(adata). --> 120 neighbors.compute_neighbors(. 121 n_neighbors=n_neighbors, knn=knn, n_pcs=n_pcs, use_rep=use_rep,. 122 method=method, metric=metric, metric_kwds=metric_kwds,. ~/.local/lib/python3.9/site-packages/scanpy/neighbors/__init__.py in compute_neighbors(self, n_neighbors, knn, n_pcs, use_rep, method, random_state, write_knn_indices, metric, metric_kwds). 732 X = pairwise_distances(X, metric=metric, **metric_kwds). 733 metric = 'precomputed'. --> 734 knn_indices, knn_distances, forest = compute_neighbors_umap(. 735 X, n_neighbors, random_state, metric=metric, metric_kwds=metric_kwds). 736 # very cautious here. ~/.local/lib/python3.9/site-packages/scanpy/neighbors/__init__.py in compute_neighbors_umap(X, n_neighbors, random_state, metric, m",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:817,testability,Trace,Traceback,817,"TypingError: Failed in nopython mode pipeline (step: nopython frontend); Hi, I am working with a big dataset over 100000 cells and I run into a problem when computing the neigbours. Find below an small example using a random matrix. . Observations,. when I run N_cells = 8000 it works perfectly for gauss metric. when running N_cells = 10000, the neighbors computation crashes. The problem is even more dramatic for method=""umap"". . ### Minimal code sample (that we can copy&paste without having any data). ```python. import scanpy. import numpy. matrix = numpy.random.uniform(size=[10000,1000]). adata = scanpy.AnnData(matrix). scanpy.pp.pca(adata,n_comps=10). scanpy.pp.neighbors(adata,method=""gauss"",n_pcs=10). ```. ```pytb. ---------------------------------------------------------------------------. TypingError Traceback (most recent call last). <ipython-input-3-692f9d70cb0c> in <module>. 9 . 10 scanpy.pp.pca(adata,n_comps=10). ---> 11 scanpy.pp.neighbors(adata,method=""gauss"",n_pcs=10). ~/.local/lib/python3.9/site-packages/scanpy/neighbors/__init__.py in neighbors(adata, n_neighbors, n_pcs, use_rep, knn, random_state, method, metric, metric_kwds, key_added, copy). 118 adata._init_as_actual(adata.copy()). 119 neighbors = Neighbors(adata). --> 120 neighbors.compute_neighbors(. 121 n_neighbors=n_neighbors, knn=knn, n_pcs=n_pcs, use_rep=use_rep,. 122 method=method, metric=metric, metric_kwds=metric_kwds,. ~/.local/lib/python3.9/site-packages/scanpy/neighbors/__init__.py in compute_neighbors(self, n_neighbors, knn, n_pcs, use_rep, method, random_state, write_knn_indices, metric, metric_kwds). 732 X = pairwise_distances(X, metric=metric, **metric_kwds). 733 metric = 'precomputed'. --> 734 knn_indices, knn_distances, forest = compute_neighbors_umap(. 735 X, n_neighbors, random_state, metric=metric, metric_kwds=metric_kwds). 736 # very cautious here. ~/.local/lib/python3.9/site-packages/scanpy/neighbors/__init__.py in compute_neighbors_umap(X, n_neighbors, random_state, metric, m",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:5410,testability,assert,assert,5410,"self._failed_cache[key] = e. ~/.local/lib/python3.9/site-packages/numba/core/dispatcher.py in _compile_core(self, args, return_type). 103 . 104 impl = self._get_implementation(args, {}). --> 105 cres = compiler.compile_extra(self.targetdescr.typing_context,. 106 self.targetdescr.target_context,. 107 impl,. ~/.local/lib/python3.9/site-packages/numba/core/compiler.py in compile_extra(typingctx, targetctx, func, args, return_type, flags, locals, library, pipeline_class). 625 pipeline = pipeline_class(typingctx, targetctx, library,. 626 args, return_type, flags, locals). --> 627 return pipeline.compile_extra(func). 628 . 629 . ~/.local/lib/python3.9/site-packages/numba/core/compiler.py in compile_extra(self, func). 361 self.state.lifted = (). 362 self.state.lifted_from = None. --> 363 return self._compile_bytecode(). 364 . 365 def compile_ir(self, func_ir, lifted=(), lifted_from=None):. ~/.local/lib/python3.9/site-packages/numba/core/compiler.py in _compile_bytecode(self). 423 """""". 424 assert self.state.func_ir is None. --> 425 return self._compile_core(). 426 . 427 def _compile_ir(self):. ~/.local/lib/python3.9/site-packages/numba/core/compiler.py in _compile_core(self). 403 self.state.status.fail_reason = e. 404 if is_final_pipeline:. --> 405 raise e. 406 else:. 407 raise CompilerError(""All available pipelines exhausted""). ~/.local/lib/python3.9/site-packages/numba/core/compiler.py in _compile_core(self). 394 res = None. 395 try:. --> 396 pm.run(self.state). 397 if self.state.cr is not None:. 398 break. ~/.local/lib/python3.9/site-packages/numba/core/compiler_machinery.py in run(self, state). 339 (self.pipeline_name, pass_desc). 340 patched_exception = self._patch_error(msg, e). --> 341 raise patched_exception. 342 . 343 def dependency_analysis(self):. ~/.local/lib/python3.9/site-packages/numba/core/compiler_machinery.py in run(self, state). 330 pass_inst = _pass_registry.get(pss).pass_inst. 331 if isinstance(pass_inst, CompilerPass):. --> 332 self._runPass(idx, pass_",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:6920,testability,Simpl,SimpleTimer,6920,":. 398 break. ~/.local/lib/python3.9/site-packages/numba/core/compiler_machinery.py in run(self, state). 339 (self.pipeline_name, pass_desc). 340 patched_exception = self._patch_error(msg, e). --> 341 raise patched_exception. 342 . 343 def dependency_analysis(self):. ~/.local/lib/python3.9/site-packages/numba/core/compiler_machinery.py in run(self, state). 330 pass_inst = _pass_registry.get(pss).pass_inst. 331 if isinstance(pass_inst, CompilerPass):. --> 332 self._runPass(idx, pass_inst, state). 333 else:. 334 raise BaseException(""Legacy pass in use""). ~/.local/lib/python3.9/site-packages/numba/core/compiler_lock.py in _acquire_compile_lock(*args, **kwargs). 30 def _acquire_compile_lock(*args, **kwargs):. 31 with self:. ---> 32 return func(*args, **kwargs). 33 return _acquire_compile_lock. 34 . ~/.local/lib/python3.9/site-packages/numba/core/compiler_machinery.py in _runPass(self, index, pss, internal_state). 289 mutated |= check(pss.run_initialization, internal_state). 290 with SimpleTimer() as pass_time:. --> 291 mutated |= check(pss.run_pass, internal_state). 292 with SimpleTimer() as finalize_time:. 293 mutated |= check(pss.run_finalizer, internal_state). ~/.local/lib/python3.9/site-packages/numba/core/compiler_machinery.py in check(func, compiler_state). 262 . 263 def check(func, compiler_state):. --> 264 mangled = func(compiler_state). 265 if mangled not in (True, False):. 266 msg = (""CompilerPass implementations should return True/False. "". ~/.local/lib/python3.9/site-packages/numba/core/typed_passes.py in run_pass(self, state). 90 % (state.func_id.func_name,)):. 91 # Type inference. ---> 92 typemap, return_type, calltypes = type_inference_stage(. 93 state.typingctx,. 94 state.func_ir,. ~/.local/lib/python3.9/site-packages/numba/core/typed_passes.py in type_inference_stage(typingctx, interp, args, return_type, locals, raise_errors). 68 . 69 infer.build_constraint(). ---> 70 infer.propagate(raise_errors=raise_errors). 71 typemap, restype, calltypes = infer.uni",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:7014,testability,Simpl,SimpleTimer,7014,"f, state). 339 (self.pipeline_name, pass_desc). 340 patched_exception = self._patch_error(msg, e). --> 341 raise patched_exception. 342 . 343 def dependency_analysis(self):. ~/.local/lib/python3.9/site-packages/numba/core/compiler_machinery.py in run(self, state). 330 pass_inst = _pass_registry.get(pss).pass_inst. 331 if isinstance(pass_inst, CompilerPass):. --> 332 self._runPass(idx, pass_inst, state). 333 else:. 334 raise BaseException(""Legacy pass in use""). ~/.local/lib/python3.9/site-packages/numba/core/compiler_lock.py in _acquire_compile_lock(*args, **kwargs). 30 def _acquire_compile_lock(*args, **kwargs):. 31 with self:. ---> 32 return func(*args, **kwargs). 33 return _acquire_compile_lock. 34 . ~/.local/lib/python3.9/site-packages/numba/core/compiler_machinery.py in _runPass(self, index, pss, internal_state). 289 mutated |= check(pss.run_initialization, internal_state). 290 with SimpleTimer() as pass_time:. --> 291 mutated |= check(pss.run_pass, internal_state). 292 with SimpleTimer() as finalize_time:. 293 mutated |= check(pss.run_finalizer, internal_state). ~/.local/lib/python3.9/site-packages/numba/core/compiler_machinery.py in check(func, compiler_state). 262 . 263 def check(func, compiler_state):. --> 264 mangled = func(compiler_state). 265 if mangled not in (True, False):. 266 msg = (""CompilerPass implementations should return True/False. "". ~/.local/lib/python3.9/site-packages/numba/core/typed_passes.py in run_pass(self, state). 90 % (state.func_id.func_name,)):. 91 # Type inference. ---> 92 typemap, return_type, calltypes = type_inference_stage(. 93 state.typingctx,. 94 state.func_ir,. ~/.local/lib/python3.9/site-packages/numba/core/typed_passes.py in type_inference_stage(typingctx, interp, args, return_type, locals, raise_errors). 68 . 69 infer.build_constraint(). ---> 70 infer.propagate(raise_errors=raise_errors). 71 typemap, restype, calltypes = infer.unify(raise_errors=raise_errors). 72 . ~/.local/lib/python3.9/site-packages/numba/core/typeinfer.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:9307,testability,assert,assert,9307,"pipeline (step: nopython frontend). Failed in nopython mode pipeline (step: nopython mode backend). Failed in nopython mode pipeline (step: nopython mode backend). Failed in nopython mode pipeline (step: nopython frontend). No implementation of function Function(<function make_quicksort_impl.<locals>.run_quicksort at 0x7f455d24c280>) found for signature:. . >>> run_quicksort(array(int32, 1d, C)). . There are 2 candidate implementations:. - Of which 2 did not match due to:. Overload in function 'register_jitable.<locals>.wrap.<locals>.ov_wrap': File: numba/core/extending.py: Line 150. With argument(s): '(array(int32, 1d, C))':. Rejected as the implementation raised a specific error:. UnsupportedError: Failed in nopython mode pipeline (step: analyzing bytecode). Use of unsupported opcode (LOAD_ASSERTION_ERROR) found. . File ""../../../../.local/lib/python3.9/site-packages/numba/misc/quicksort.py"", line 180:. def run_quicksort(A):. <source elided>. while high - low >= SMALL_QUICKSORT:. assert n < MAX_STACK. ^. . raised from /home/gabriel/.local/lib/python3.9/site-packages/numba/core/byteflow.py:269. During: resolving callee type: Function(<function make_quicksort_impl.<locals>.run_quicksort at 0x7f455d24c280>). During: typing of call at /home/gabriel/.local/lib/python3.9/site-packages/numba/np/arrayobj.py (5007). File ""../../../../.local/lib/python3.9/site-packages/numba/np/arrayobj.py"", line 5007:. def array_sort_impl(arr):. <source elided>. # Note we clobber the return value. sort_func(arr). ^. During: lowering ""$14call_method.5 = call $12load_method.4(func=$12load_method.4, args=[], kws=(), vararg=None)"" at /home/gabriel/.local/lib/python3.9/site-packages/numba/np/arrayobj.py (5017). During: lowering ""$8call_method.3 = call $4load_method.1(arr, func=$4load_method.1, args=[Var(arr, sparse.py:28)], kws=(), vararg=None)"" at /home/gabriel/.local/lib/python3.9/site-packages/pynndescent/sparse.py (28). During: resolving callee type: type(CPUDispatcher(<function arr_unique ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:437,usability,Minim,Minimal,437,"TypingError: Failed in nopython mode pipeline (step: nopython frontend); Hi, I am working with a big dataset over 100000 cells and I run into a problem when computing the neigbours. Find below an small example using a random matrix. . Observations,. when I run N_cells = 8000 it works perfectly for gauss metric. when running N_cells = 10000, the neighbors computation crashes. The problem is even more dramatic for method=""umap"". . ### Minimal code sample (that we can copy&paste without having any data). ```python. import scanpy. import numpy. matrix = numpy.random.uniform(size=[10000,1000]). adata = scanpy.AnnData(matrix). scanpy.pp.pca(adata,n_comps=10). scanpy.pp.neighbors(adata,method=""gauss"",n_pcs=10). ```. ```pytb. ---------------------------------------------------------------------------. TypingError Traceback (most recent call last). <ipython-input-3-692f9d70cb0c> in <module>. 9 . 10 scanpy.pp.pca(adata,n_comps=10). ---> 11 scanpy.pp.neighbors(adata,method=""gauss"",n_pcs=10). ~/.local/lib/python3.9/site-packages/scanpy/neighbors/__init__.py in neighbors(adata, n_neighbors, n_pcs, use_rep, knn, random_state, method, metric, metric_kwds, key_added, copy). 118 adata._init_as_actual(adata.copy()). 119 neighbors = Neighbors(adata). --> 120 neighbors.compute_neighbors(. 121 n_neighbors=n_neighbors, knn=knn, n_pcs=n_pcs, use_rep=use_rep,. 122 method=method, metric=metric, metric_kwds=metric_kwds,. ~/.local/lib/python3.9/site-packages/scanpy/neighbors/__init__.py in compute_neighbors(self, n_neighbors, knn, n_pcs, use_rep, method, random_state, write_knn_indices, metric, metric_kwds). 732 X = pairwise_distances(X, metric=metric, **metric_kwds). 733 metric = 'precomputed'. --> 734 knn_indices, knn_distances, forest = compute_neighbors_umap(. 735 X, n_neighbors, random_state, metric=metric, metric_kwds=metric_kwds). 736 # very cautious here. ~/.local/lib/python3.9/site-packages/scanpy/neighbors/__init__.py in compute_neighbors_umap(X, n_neighbors, random_state, metric, m",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:861,usability,input,input-,861,"TypingError: Failed in nopython mode pipeline (step: nopython frontend); Hi, I am working with a big dataset over 100000 cells and I run into a problem when computing the neigbours. Find below an small example using a random matrix. . Observations,. when I run N_cells = 8000 it works perfectly for gauss metric. when running N_cells = 10000, the neighbors computation crashes. The problem is even more dramatic for method=""umap"". . ### Minimal code sample (that we can copy&paste without having any data). ```python. import scanpy. import numpy. matrix = numpy.random.uniform(size=[10000,1000]). adata = scanpy.AnnData(matrix). scanpy.pp.pca(adata,n_comps=10). scanpy.pp.neighbors(adata,method=""gauss"",n_pcs=10). ```. ```pytb. ---------------------------------------------------------------------------. TypingError Traceback (most recent call last). <ipython-input-3-692f9d70cb0c> in <module>. 9 . 10 scanpy.pp.pca(adata,n_comps=10). ---> 11 scanpy.pp.neighbors(adata,method=""gauss"",n_pcs=10). ~/.local/lib/python3.9/site-packages/scanpy/neighbors/__init__.py in neighbors(adata, n_neighbors, n_pcs, use_rep, knn, random_state, method, metric, metric_kwds, key_added, copy). 118 adata._init_as_actual(adata.copy()). 119 neighbors = Neighbors(adata). --> 120 neighbors.compute_neighbors(. 121 n_neighbors=n_neighbors, knn=knn, n_pcs=n_pcs, use_rep=use_rep,. 122 method=method, metric=metric, metric_kwds=metric_kwds,. ~/.local/lib/python3.9/site-packages/scanpy/neighbors/__init__.py in compute_neighbors(self, n_neighbors, knn, n_pcs, use_rep, method, random_state, write_knn_indices, metric, metric_kwds). 732 X = pairwise_distances(X, metric=metric, **metric_kwds). 733 metric = 'precomputed'. --> 734 knn_indices, knn_distances, forest = compute_neighbors_umap(. 735 X, n_neighbors, random_state, metric=metric, metric_kwds=metric_kwds). 736 # very cautious here. ~/.local/lib/python3.9/site-packages/scanpy/neighbors/__init__.py in compute_neighbors_umap(X, n_neighbors, random_state, metric, m",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:3935,usability,error,errors,3935,"e_nndescent as sparse_nnd. 23 import pynndescent.distances as pynnd_dist. ~/.local/lib/python3.9/site-packages/pynndescent/sparse.py in <module>. 341 },. 342 ). --> 343 def sparse_alternative_jaccard(ind1, data1, ind2, data2):. 344 num_non_zero = arr_union(ind1, ind2).shape[0]. 345 num_equal = arr_intersect(ind1, ind2).shape[0]. ~/.local/lib/python3.9/site-packages/numba/core/decorators.py in wrapper(func). 216 with typeinfer.register_dispatcher(disp):. 217 for sig in sigs:. --> 218 disp.compile(sig). 219 disp.disable_compile(). 220 return disp. ~/.local/lib/python3.9/site-packages/numba/core/compiler_lock.py in _acquire_compile_lock(*args, **kwargs). 30 def _acquire_compile_lock(*args, **kwargs):. 31 with self:. ---> 32 return func(*args, **kwargs). 33 return _acquire_compile_lock. 34 . ~/.local/lib/python3.9/site-packages/numba/core/dispatcher.py in compile(self, sig). 817 self._cache_misses[sig] += 1. 818 try:. --> 819 cres = self._compiler.compile(args, return_type). 820 except errors.ForceLiteralArg as e:. 821 def folded(args, kws):. ~/.local/lib/python3.9/site-packages/numba/core/dispatcher.py in compile(self, args, return_type). 80 return retval. 81 else:. ---> 82 raise retval. 83 . 84 def _compile_cached(self, args, return_type):. ~/.local/lib/python3.9/site-packages/numba/core/dispatcher.py in _compile_cached(self, args, return_type). 90 . 91 try:. ---> 92 retval = self._compile_core(args, return_type). 93 except errors.TypingError as e:. 94 self._failed_cache[key] = e. ~/.local/lib/python3.9/site-packages/numba/core/dispatcher.py in _compile_core(self, args, return_type). 103 . 104 impl = self._get_implementation(args, {}). --> 105 cres = compiler.compile_extra(self.targetdescr.typing_context,. 106 self.targetdescr.target_context,. 107 impl,. ~/.local/lib/python3.9/site-packages/numba/core/compiler.py in compile_extra(typingctx, targetctx, func, args, return_type, flags, locals, library, pipeline_class). 625 pipeline = pipeline_class(typingctx, targetctx, ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:4384,usability,error,errors,4384,"(disp):. 217 for sig in sigs:. --> 218 disp.compile(sig). 219 disp.disable_compile(). 220 return disp. ~/.local/lib/python3.9/site-packages/numba/core/compiler_lock.py in _acquire_compile_lock(*args, **kwargs). 30 def _acquire_compile_lock(*args, **kwargs):. 31 with self:. ---> 32 return func(*args, **kwargs). 33 return _acquire_compile_lock. 34 . ~/.local/lib/python3.9/site-packages/numba/core/dispatcher.py in compile(self, sig). 817 self._cache_misses[sig] += 1. 818 try:. --> 819 cres = self._compiler.compile(args, return_type). 820 except errors.ForceLiteralArg as e:. 821 def folded(args, kws):. ~/.local/lib/python3.9/site-packages/numba/core/dispatcher.py in compile(self, args, return_type). 80 return retval. 81 else:. ---> 82 raise retval. 83 . 84 def _compile_cached(self, args, return_type):. ~/.local/lib/python3.9/site-packages/numba/core/dispatcher.py in _compile_cached(self, args, return_type). 90 . 91 try:. ---> 92 retval = self._compile_core(args, return_type). 93 except errors.TypingError as e:. 94 self._failed_cache[key] = e. ~/.local/lib/python3.9/site-packages/numba/core/dispatcher.py in _compile_core(self, args, return_type). 103 . 104 impl = self._get_implementation(args, {}). --> 105 cres = compiler.compile_extra(self.targetdescr.typing_context,. 106 self.targetdescr.target_context,. 107 impl,. ~/.local/lib/python3.9/site-packages/numba/core/compiler.py in compile_extra(typingctx, targetctx, func, args, return_type, flags, locals, library, pipeline_class). 625 pipeline = pipeline_class(typingctx, targetctx, library,. 626 args, return_type, flags, locals). --> 627 return pipeline.compile_extra(func). 628 . 629 . ~/.local/lib/python3.9/site-packages/numba/core/compiler.py in compile_extra(self, func). 361 self.state.lifted = (). 362 self.state.lifted_from = None. --> 363 return self._compile_bytecode(). 364 . 365 def compile_ir(self, func_ir, lifted=(), lifted_from=None):. ~/.local/lib/python3.9/site-packages/numba/core/compiler.py in _compile_byteco",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:5615,usability,statu,status,5615,"piler.compile_extra(self.targetdescr.typing_context,. 106 self.targetdescr.target_context,. 107 impl,. ~/.local/lib/python3.9/site-packages/numba/core/compiler.py in compile_extra(typingctx, targetctx, func, args, return_type, flags, locals, library, pipeline_class). 625 pipeline = pipeline_class(typingctx, targetctx, library,. 626 args, return_type, flags, locals). --> 627 return pipeline.compile_extra(func). 628 . 629 . ~/.local/lib/python3.9/site-packages/numba/core/compiler.py in compile_extra(self, func). 361 self.state.lifted = (). 362 self.state.lifted_from = None. --> 363 return self._compile_bytecode(). 364 . 365 def compile_ir(self, func_ir, lifted=(), lifted_from=None):. ~/.local/lib/python3.9/site-packages/numba/core/compiler.py in _compile_bytecode(self). 423 """""". 424 assert self.state.func_ir is None. --> 425 return self._compile_core(). 426 . 427 def _compile_ir(self):. ~/.local/lib/python3.9/site-packages/numba/core/compiler.py in _compile_core(self). 403 self.state.status.fail_reason = e. 404 if is_final_pipeline:. --> 405 raise e. 406 else:. 407 raise CompilerError(""All available pipelines exhausted""). ~/.local/lib/python3.9/site-packages/numba/core/compiler.py in _compile_core(self). 394 res = None. 395 try:. --> 396 pm.run(self.state). 397 if self.state.cr is not None:. 398 break. ~/.local/lib/python3.9/site-packages/numba/core/compiler_machinery.py in run(self, state). 339 (self.pipeline_name, pass_desc). 340 patched_exception = self._patch_error(msg, e). --> 341 raise patched_exception. 342 . 343 def dependency_analysis(self):. ~/.local/lib/python3.9/site-packages/numba/core/compiler_machinery.py in run(self, state). 330 pass_inst = _pass_registry.get(pss).pass_inst. 331 if isinstance(pass_inst, CompilerPass):. --> 332 self._runPass(idx, pass_inst, state). 333 else:. 334 raise BaseException(""Legacy pass in use""). ~/.local/lib/python3.9/site-packages/numba/core/compiler_lock.py in _acquire_compile_lock(*args, **kwargs). 30 def _acquire_compile_l",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:6920,usability,Simpl,SimpleTimer,6920,":. 398 break. ~/.local/lib/python3.9/site-packages/numba/core/compiler_machinery.py in run(self, state). 339 (self.pipeline_name, pass_desc). 340 patched_exception = self._patch_error(msg, e). --> 341 raise patched_exception. 342 . 343 def dependency_analysis(self):. ~/.local/lib/python3.9/site-packages/numba/core/compiler_machinery.py in run(self, state). 330 pass_inst = _pass_registry.get(pss).pass_inst. 331 if isinstance(pass_inst, CompilerPass):. --> 332 self._runPass(idx, pass_inst, state). 333 else:. 334 raise BaseException(""Legacy pass in use""). ~/.local/lib/python3.9/site-packages/numba/core/compiler_lock.py in _acquire_compile_lock(*args, **kwargs). 30 def _acquire_compile_lock(*args, **kwargs):. 31 with self:. ---> 32 return func(*args, **kwargs). 33 return _acquire_compile_lock. 34 . ~/.local/lib/python3.9/site-packages/numba/core/compiler_machinery.py in _runPass(self, index, pss, internal_state). 289 mutated |= check(pss.run_initialization, internal_state). 290 with SimpleTimer() as pass_time:. --> 291 mutated |= check(pss.run_pass, internal_state). 292 with SimpleTimer() as finalize_time:. 293 mutated |= check(pss.run_finalizer, internal_state). ~/.local/lib/python3.9/site-packages/numba/core/compiler_machinery.py in check(func, compiler_state). 262 . 263 def check(func, compiler_state):. --> 264 mangled = func(compiler_state). 265 if mangled not in (True, False):. 266 msg = (""CompilerPass implementations should return True/False. "". ~/.local/lib/python3.9/site-packages/numba/core/typed_passes.py in run_pass(self, state). 90 % (state.func_id.func_name,)):. 91 # Type inference. ---> 92 typemap, return_type, calltypes = type_inference_stage(. 93 state.typingctx,. 94 state.func_ir,. ~/.local/lib/python3.9/site-packages/numba/core/typed_passes.py in type_inference_stage(typingctx, interp, args, return_type, locals, raise_errors). 68 . 69 infer.build_constraint(). ---> 70 infer.propagate(raise_errors=raise_errors). 71 typemap, restype, calltypes = infer.uni",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:7014,usability,Simpl,SimpleTimer,7014,"f, state). 339 (self.pipeline_name, pass_desc). 340 patched_exception = self._patch_error(msg, e). --> 341 raise patched_exception. 342 . 343 def dependency_analysis(self):. ~/.local/lib/python3.9/site-packages/numba/core/compiler_machinery.py in run(self, state). 330 pass_inst = _pass_registry.get(pss).pass_inst. 331 if isinstance(pass_inst, CompilerPass):. --> 332 self._runPass(idx, pass_inst, state). 333 else:. 334 raise BaseException(""Legacy pass in use""). ~/.local/lib/python3.9/site-packages/numba/core/compiler_lock.py in _acquire_compile_lock(*args, **kwargs). 30 def _acquire_compile_lock(*args, **kwargs):. 31 with self:. ---> 32 return func(*args, **kwargs). 33 return _acquire_compile_lock. 34 . ~/.local/lib/python3.9/site-packages/numba/core/compiler_machinery.py in _runPass(self, index, pss, internal_state). 289 mutated |= check(pss.run_initialization, internal_state). 290 with SimpleTimer() as pass_time:. --> 291 mutated |= check(pss.run_pass, internal_state). 292 with SimpleTimer() as finalize_time:. 293 mutated |= check(pss.run_finalizer, internal_state). ~/.local/lib/python3.9/site-packages/numba/core/compiler_machinery.py in check(func, compiler_state). 262 . 263 def check(func, compiler_state):. --> 264 mangled = func(compiler_state). 265 if mangled not in (True, False):. 266 msg = (""CompilerPass implementations should return True/False. "". ~/.local/lib/python3.9/site-packages/numba/core/typed_passes.py in run_pass(self, state). 90 % (state.func_id.func_name,)):. 91 # Type inference. ---> 92 typemap, return_type, calltypes = type_inference_stage(. 93 state.typingctx,. 94 state.func_ir,. ~/.local/lib/python3.9/site-packages/numba/core/typed_passes.py in type_inference_stage(typingctx, interp, args, return_type, locals, raise_errors). 68 . 69 infer.build_constraint(). ---> 70 infer.propagate(raise_errors=raise_errors). 71 typemap, restype, calltypes = infer.unify(raise_errors=raise_errors). 72 . ~/.local/lib/python3.9/site-packages/numba/core/typeinfer.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:8141,usability,error,errors,8141,"ba/core/compiler_machinery.py in check(func, compiler_state). 262 . 263 def check(func, compiler_state):. --> 264 mangled = func(compiler_state). 265 if mangled not in (True, False):. 266 msg = (""CompilerPass implementations should return True/False. "". ~/.local/lib/python3.9/site-packages/numba/core/typed_passes.py in run_pass(self, state). 90 % (state.func_id.func_name,)):. 91 # Type inference. ---> 92 typemap, return_type, calltypes = type_inference_stage(. 93 state.typingctx,. 94 state.func_ir,. ~/.local/lib/python3.9/site-packages/numba/core/typed_passes.py in type_inference_stage(typingctx, interp, args, return_type, locals, raise_errors). 68 . 69 infer.build_constraint(). ---> 70 infer.propagate(raise_errors=raise_errors). 71 typemap, restype, calltypes = infer.unify(raise_errors=raise_errors). 72 . ~/.local/lib/python3.9/site-packages/numba/core/typeinfer.py in propagate(self, raise_errors). 1069 if isinstance(e, ForceLiteralArg)]. 1070 if not force_lit_args:. -> 1071 raise errors[0]. 1072 else:. 1073 raise reduce(operator.or_, force_lit_args). TypingError: Failed in nopython mode pipeline (step: nopython frontend). Failed in nopython mode pipeline (step: nopython frontend). Failed in nopython mode pipeline (step: nopython mode backend). Failed in nopython mode pipeline (step: nopython mode backend). Failed in nopython mode pipeline (step: nopython frontend). No implementation of function Function(<function make_quicksort_impl.<locals>.run_quicksort at 0x7f455d24c280>) found for signature:. . >>> run_quicksort(array(int32, 1d, C)). . There are 2 candidate implementations:. - Of which 2 did not match due to:. Overload in function 'register_jitable.<locals>.wrap.<locals>.ov_wrap': File: numba/core/extending.py: Line 150. With argument(s): '(array(int32, 1d, C))':. Rejected as the implementation raised a specific error:. UnsupportedError: Failed in nopython mode pipeline (step: analyzing bytecode). Use of unsupported opcode (LOAD_ASSERTION_ERROR) found. . File ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:8994,usability,error,error,8994,"s/numba/core/typeinfer.py in propagate(self, raise_errors). 1069 if isinstance(e, ForceLiteralArg)]. 1070 if not force_lit_args:. -> 1071 raise errors[0]. 1072 else:. 1073 raise reduce(operator.or_, force_lit_args). TypingError: Failed in nopython mode pipeline (step: nopython frontend). Failed in nopython mode pipeline (step: nopython frontend). Failed in nopython mode pipeline (step: nopython mode backend). Failed in nopython mode pipeline (step: nopython mode backend). Failed in nopython mode pipeline (step: nopython frontend). No implementation of function Function(<function make_quicksort_impl.<locals>.run_quicksort at 0x7f455d24c280>) found for signature:. . >>> run_quicksort(array(int32, 1d, C)). . There are 2 candidate implementations:. - Of which 2 did not match due to:. Overload in function 'register_jitable.<locals>.wrap.<locals>.ov_wrap': File: numba/core/extending.py: Line 150. With argument(s): '(array(int32, 1d, C))':. Rejected as the implementation raised a specific error:. UnsupportedError: Failed in nopython mode pipeline (step: analyzing bytecode). Use of unsupported opcode (LOAD_ASSERTION_ERROR) found. . File ""../../../../.local/lib/python3.9/site-packages/numba/misc/quicksort.py"", line 180:. def run_quicksort(A):. <source elided>. while high - low >= SMALL_QUICKSORT:. assert n < MAX_STACK. ^. . raised from /home/gabriel/.local/lib/python3.9/site-packages/numba/core/byteflow.py:269. During: resolving callee type: Function(<function make_quicksort_impl.<locals>.run_quicksort at 0x7f455d24c280>). During: typing of call at /home/gabriel/.local/lib/python3.9/site-packages/numba/np/arrayobj.py (5007). File ""../../../../.local/lib/python3.9/site-packages/numba/np/arrayobj.py"", line 5007:. def array_sort_impl(arr):. <source elided>. # Note we clobber the return value. sort_func(arr). ^. During: lowering ""$14call_method.5 = call $12load_method.4(func=$12load_method.4, args=[], kws=(), vararg=None)"" at /home/gabriel/.local/lib/python3.9/site-packages/numb",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1653:129,modifiability,paramet,parameters,129,"allow different polygons in `sc.pl.spatial`; <!-- What kind of feature would you like to request? -->. - [ ] Additional function parameters / changed functionality / changed defaults? - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`? - [ ] New plotting function: A kind of plot you would like to seein `sc.pl`? - [ ] External tools: Do you know an existing package that should go into `sc.external.*`? - [x] Other? <!-- Please describe your wishes below: -->. e.g. squares, hexagons cc @jwrth.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1653
https://github.com/scverse/scanpy/issues/1653:406,modifiability,pac,package,406,"allow different polygons in `sc.pl.spatial`; <!-- What kind of feature would you like to request? -->. - [ ] Additional function parameters / changed functionality / changed defaults? - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`? - [ ] New plotting function: A kind of plot you would like to seein `sc.pl`? - [ ] External tools: Do you know an existing package that should go into `sc.external.*`? - [x] Other? <!-- Please describe your wishes below: -->. e.g. squares, hexagons cc @jwrth.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1653
https://github.com/scverse/scanpy/issues/1653:211,testability,simpl,simple,211,"allow different polygons in `sc.pl.spatial`; <!-- What kind of feature would you like to request? -->. - [ ] Additional function parameters / changed functionality / changed defaults? - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`? - [ ] New plotting function: A kind of plot you would like to seein `sc.pl`? - [ ] External tools: Do you know an existing package that should go into `sc.external.*`? - [x] Other? <!-- Please describe your wishes below: -->. e.g. squares, hexagons cc @jwrth.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1653
https://github.com/scverse/scanpy/issues/1653:203,usability,tool,tool,203,"allow different polygons in `sc.pl.spatial`; <!-- What kind of feature would you like to request? -->. - [ ] Additional function parameters / changed functionality / changed defaults? - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`? - [ ] New plotting function: A kind of plot you would like to seein `sc.pl`? - [ ] External tools: Do you know an existing package that should go into `sc.external.*`? - [x] Other? <!-- Please describe your wishes below: -->. e.g. squares, hexagons cc @jwrth.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1653
https://github.com/scverse/scanpy/issues/1653:211,usability,simpl,simple,211,"allow different polygons in `sc.pl.spatial`; <!-- What kind of feature would you like to request? -->. - [ ] Additional function parameters / changed functionality / changed defaults? - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`? - [ ] New plotting function: A kind of plot you would like to seein `sc.pl`? - [ ] External tools: Do you know an existing package that should go into `sc.external.*`? - [x] Other? <!-- Please describe your wishes below: -->. e.g. squares, hexagons cc @jwrth.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1653
https://github.com/scverse/scanpy/issues/1653:227,usability,tool,tool,227,"allow different polygons in `sc.pl.spatial`; <!-- What kind of feature would you like to request? -->. - [ ] Additional function parameters / changed functionality / changed defaults? - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`? - [ ] New plotting function: A kind of plot you would like to seein `sc.pl`? - [ ] External tools: Do you know an existing package that should go into `sc.external.*`? - [x] Other? <!-- Please describe your wishes below: -->. e.g. squares, hexagons cc @jwrth.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1653
https://github.com/scverse/scanpy/issues/1653:275,usability,tool,tools,275,"allow different polygons in `sc.pl.spatial`; <!-- What kind of feature would you like to request? -->. - [ ] Additional function parameters / changed functionality / changed defaults? - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`? - [ ] New plotting function: A kind of plot you would like to seein `sc.pl`? - [ ] External tools: Do you know an existing package that should go into `sc.external.*`? - [x] Other? <!-- Please describe your wishes below: -->. e.g. squares, hexagons cc @jwrth.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1653
https://github.com/scverse/scanpy/issues/1653:375,usability,tool,tools,375,"allow different polygons in `sc.pl.spatial`; <!-- What kind of feature would you like to request? -->. - [ ] Additional function parameters / changed functionality / changed defaults? - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`? - [ ] New plotting function: A kind of plot you would like to seein `sc.pl`? - [ ] External tools: Do you know an existing package that should go into `sc.external.*`? - [x] Other? <!-- Please describe your wishes below: -->. e.g. squares, hexagons cc @jwrth.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1653
https://github.com/scverse/scanpy/pull/1654:233,deployability,modul,module,233,"Fix timeseries_as_heatmap plot function; This fixes:. ```. Traceback (most recent call last):. File ""/private/var/folders/df/6xqpqpcd7h73b6jpx9t6cwhw0000gn/T/tmpn9tl9wf0/job_working_directory/000/2/configs/tmp_r9i0cvx"", line 15, in <module>. sc.pl.dpt_timeseries(. File ""/Users/mvandenb/miniconda3/envs/anndata/lib/python3.8/site-packages/scanpy/plotting/_tools/__init__.py"", line 171, in dpt_timeseries. timeseries_as_heatmap(. File ""/Users/mvandenb/miniconda3/envs/anndata/lib/python3.8/site-packages/scanpy/plotting/_utils.py"", line 206, in timeseries_as_heatmap. pl.colorbar(shrink=0.5). File ""/Users/mvandenb/miniconda3/envs/anndata/lib/python3.8/site-packages/matplotlib/pyplot.py"", line 2188, in colorbar. raise RuntimeError('No mappable was found to use for colorbar '. RuntimeError: No mappable was found to use for colorbar creation. First define a mappable such as an image (with imshow) or a contour set (with contourf). ```. I see that in other places plt.colobar is used in this module you're. doing the same thing. I believe this broke in. 64f04d8.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1654
