id,quality_attribute,keyword,matched_word,match_idx,sentence,source,author,repo,version,wiki,url
https://github.com/google/deepvariant/issues/598:989,performance,Parallel,Parallel,989,"deepvariant software installed using mamba does not run correctly ;lchmod error;No such file or directory: '/usr/bin/python3' error; Hi,. I want to call variations on the Pacbio bam data to get gvcf files. But something went wrong with the program, and I spent two days wondering what went wrong. How do you solve this problem? . Please help! (No root permission;Centos7 x86;Only the user directory has read and write permission). ```. dv_make_examples.py --cores 3 --sample QJ --ref QJref.fa --reads F0F_sorted.merged.addg.uniq.rmdup.bam --logdir . --examples EXAMPLES --gvcf GVCF. ```. ```. Academic tradition requires you to cite works you base your article on. When using programs that use GNU Parallel to process data for publication. please cite:. O. Tange (2011): GNU Parallel - The Command-Line Power Tool,. ;login: The USENIX Magazine, February 2011:42-47. This helps funding further development; AND IT WON'T COST YOU A CENT. If you pay 10000 EUR you should feel free to use GNU Parallel without citing. To silence this citation notice: run 'parallel --citation'. Computers / CPU cores / Max jobs to run. 1:local / 48 / 3. Computer:jobs running/jobs completed/%of started jobs/Average seconds to complete. ETA: 0s Left: 3 AVG: 0.00s local:3/0/100%/0.0s lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/598
https://github.com/google/deepvariant/issues/598:1052,performance,parallel,parallel,1052,"correctly ;lchmod error;No such file or directory: '/usr/bin/python3' error; Hi,. I want to call variations on the Pacbio bam data to get gvcf files. But something went wrong with the program, and I spent two days wondering what went wrong. How do you solve this problem? . Please help! (No root permission;Centos7 x86;Only the user directory has read and write permission). ```. dv_make_examples.py --cores 3 --sample QJ --ref QJref.fa --reads F0F_sorted.merged.addg.uniq.rmdup.bam --logdir . --examples EXAMPLES --gvcf GVCF. ```. ```. Academic tradition requires you to cite works you base your article on. When using programs that use GNU Parallel to process data for publication. please cite:. O. Tange (2011): GNU Parallel - The Command-Line Power Tool,. ;login: The USENIX Magazine, February 2011:42-47. This helps funding further development; AND IT WON'T COST YOU A CENT. If you pay 10000 EUR you should feel free to use GNU Parallel without citing. To silence this citation notice: run 'parallel --citation'. Computers / CPU cores / Max jobs to run. 1:local / 48 / 3. Computer:jobs running/jobs completed/%of started jobs/Average seconds to complete. ETA: 0s Left: 3 AVG: 0.00s local:3/0/100%/0.0s lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Functio",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/598
https://github.com/google/deepvariant/issues/598:1086,performance,CPU,CPU,1086,"file or directory: '/usr/bin/python3' error; Hi,. I want to call variations on the Pacbio bam data to get gvcf files. But something went wrong with the program, and I spent two days wondering what went wrong. How do you solve this problem? . Please help! (No root permission;Centos7 x86;Only the user directory has read and write permission). ```. dv_make_examples.py --cores 3 --sample QJ --ref QJref.fa --reads F0F_sorted.merged.addg.uniq.rmdup.bam --logdir . --examples EXAMPLES --gvcf GVCF. ```. ```. Academic tradition requires you to cite works you base your article on. When using programs that use GNU Parallel to process data for publication. please cite:. O. Tange (2011): GNU Parallel - The Command-Line Power Tool,. ;login: The USENIX Magazine, February 2011:42-47. This helps funding further development; AND IT WON'T COST YOU A CENT. If you pay 10000 EUR you should feel free to use GNU Parallel without citing. To silence this citation notice: run 'parallel --citation'. Computers / CPU cores / Max jobs to run. 1:local / 48 / 3. Computer:jobs running/jobs completed/%of started jobs/Average seconds to complete. ETA: 0s Left: 3 AVG: 0.00s local:3/0/100%/0.0s lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/598
https://github.com/google/deepvariant/issues/598:1288,performance,error,error,1288,"rong. How do you solve this problem? . Please help! (No root permission;Centos7 x86;Only the user directory has read and write permission). ```. dv_make_examples.py --cores 3 --sample QJ --ref QJref.fa --reads F0F_sorted.merged.addg.uniq.rmdup.bam --logdir . --examples EXAMPLES --gvcf GVCF. ```. ```. Academic tradition requires you to cite works you base your article on. When using programs that use GNU Parallel to process data for publication. please cite:. O. Tange (2011): GNU Parallel - The Command-Line Power Tool,. ;login: The USENIX Magazine, February 2011:42-47. This helps funding further development; AND IT WON'T COST YOU A CENT. If you pay 10000 EUR you should feel free to use GNU Parallel without citing. To silence this citation notice: run 'parallel --citation'. Computers / CPU cores / Max jobs to run. 1:local / 48 / 3. Computer:jobs running/jobs completed/%of started jobs/Average seconds to complete. ETA: 0s Left: 3 AVG: 0.00s local:3/0/100%/0.0s lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function n",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/598
https://github.com/google/deepvariant/issues/598:1346,performance,error,error,1346,"ot permission;Centos7 x86;Only the user directory has read and write permission). ```. dv_make_examples.py --cores 3 --sample QJ --ref QJref.fa --reads F0F_sorted.merged.addg.uniq.rmdup.bam --logdir . --examples EXAMPLES --gvcf GVCF. ```. ```. Academic tradition requires you to cite works you base your article on. When using programs that use GNU Parallel to process data for publication. please cite:. O. Tange (2011): GNU Parallel - The Command-Line Power Tool,. ;login: The USENIX Magazine, February 2011:42-47. This helps funding further development; AND IT WON'T COST YOU A CENT. If you pay 10000 EUR you should feel free to use GNU Parallel without citing. To silence this citation notice: run 'parallel --citation'. Computers / CPU cores / Max jobs to run. 1:local / 48 / 3. Computer:jobs running/jobs completed/%of started jobs/Average seconds to complete. ETA: 0s Left: 3 AVG: 0.00s local:3/0/100%/0.0s lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function n",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/598
https://github.com/google/deepvariant/issues/598:1404,performance,error,error,1404," and write permission). ```. dv_make_examples.py --cores 3 --sample QJ --ref QJref.fa --reads F0F_sorted.merged.addg.uniq.rmdup.bam --logdir . --examples EXAMPLES --gvcf GVCF. ```. ```. Academic tradition requires you to cite works you base your article on. When using programs that use GNU Parallel to process data for publication. please cite:. O. Tange (2011): GNU Parallel - The Command-Line Power Tool,. ;login: The USENIX Magazine, February 2011:42-47. This helps funding further development; AND IT WON'T COST YOU A CENT. If you pay 10000 EUR you should feel free to use GNU Parallel without citing. To silence this citation notice: run 'parallel --citation'. Computers / CPU cores / Max jobs to run. 1:local / 48 / 3. Computer:jobs running/jobs completed/%of started jobs/Average seconds to complete. ETA: 0s Left: 3 AVG: 0.00s local:3/0/100%/0.0s lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function n",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/598
https://github.com/google/deepvariant/issues/598:1462,performance,error,error,1462," --sample QJ --ref QJref.fa --reads F0F_sorted.merged.addg.uniq.rmdup.bam --logdir . --examples EXAMPLES --gvcf GVCF. ```. ```. Academic tradition requires you to cite works you base your article on. When using programs that use GNU Parallel to process data for publication. please cite:. O. Tange (2011): GNU Parallel - The Command-Line Power Tool,. ;login: The USENIX Magazine, February 2011:42-47. This helps funding further development; AND IT WON'T COST YOU A CENT. If you pay 10000 EUR you should feel free to use GNU Parallel without citing. To silence this citation notice: run 'parallel --citation'. Computers / CPU cores / Max jobs to run. 1:local / 48 / 3. Computer:jobs running/jobs completed/%of started jobs/Average seconds to complete. ETA: 0s Left: 3 AVG: 0.00s local:3/0/100%/0.0s lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. Traceback (most recent call last):. File """,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/598
https://github.com/google/deepvariant/issues/598:1520,performance,error,error,1520,".uniq.rmdup.bam --logdir . --examples EXAMPLES --gvcf GVCF. ```. ```. Academic tradition requires you to cite works you base your article on. When using programs that use GNU Parallel to process data for publication. please cite:. O. Tange (2011): GNU Parallel - The Command-Line Power Tool,. ;login: The USENIX Magazine, February 2011:42-47. This helps funding further development; AND IT WON'T COST YOU A CENT. If you pay 10000 EUR you should feel free to use GNU Parallel without citing. To silence this citation notice: run 'parallel --citation'. Computers / CPU cores / Max jobs to run. 1:local / 48 / 3. Computer:jobs running/jobs completed/%of started jobs/Average seconds to complete. ETA: 0s Left: 3 AVG: 0.00s local:3/0/100%/0.0s lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. Traceback (most recent call last):. File ""/path/Mambaforge-4.14.0-1/envs/dpv/lib/python3.6/runpy.py""",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/598
https://github.com/google/deepvariant/issues/598:1578,performance,error,error,1578,". ```. ```. Academic tradition requires you to cite works you base your article on. When using programs that use GNU Parallel to process data for publication. please cite:. O. Tange (2011): GNU Parallel - The Command-Line Power Tool,. ;login: The USENIX Magazine, February 2011:42-47. This helps funding further development; AND IT WON'T COST YOU A CENT. If you pay 10000 EUR you should feel free to use GNU Parallel without citing. To silence this citation notice: run 'parallel --citation'. Computers / CPU cores / Max jobs to run. 1:local / 48 / 3. Computer:jobs running/jobs completed/%of started jobs/Average seconds to complete. ETA: 0s Left: 3 AVG: 0.00s local:3/0/100%/0.0s lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. Traceback (most recent call last):. File ""/path/Mambaforge-4.14.0-1/envs/dpv/lib/python3.6/runpy.py"", line 193, in _run_module_as_main. ""__main__"", mod_spec).",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/598
https://github.com/google/deepvariant/issues/598:1636,performance,error,error,1636,"you base your article on. When using programs that use GNU Parallel to process data for publication. please cite:. O. Tange (2011): GNU Parallel - The Command-Line Power Tool,. ;login: The USENIX Magazine, February 2011:42-47. This helps funding further development; AND IT WON'T COST YOU A CENT. If you pay 10000 EUR you should feel free to use GNU Parallel without citing. To silence this citation notice: run 'parallel --citation'. Computers / CPU cores / Max jobs to run. 1:local / 48 / 3. Computer:jobs running/jobs completed/%of started jobs/Average seconds to complete. ETA: 0s Left: 3 AVG: 0.00s local:3/0/100%/0.0s lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. Traceback (most recent call last):. File ""/path/Mambaforge-4.14.0-1/envs/dpv/lib/python3.6/runpy.py"", line 193, in _run_module_as_main. ""__main__"", mod_spec). File ""/path/Mambaforge-4.14.0-1/envs/dpv/lib/python3.6/ru",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/598
https://github.com/google/deepvariant/issues/598:1694,performance,error,error,1694," Parallel to process data for publication. please cite:. O. Tange (2011): GNU Parallel - The Command-Line Power Tool,. ;login: The USENIX Magazine, February 2011:42-47. This helps funding further development; AND IT WON'T COST YOU A CENT. If you pay 10000 EUR you should feel free to use GNU Parallel without citing. To silence this citation notice: run 'parallel --citation'. Computers / CPU cores / Max jobs to run. 1:local / 48 / 3. Computer:jobs running/jobs completed/%of started jobs/Average seconds to complete. ETA: 0s Left: 3 AVG: 0.00s local:3/0/100%/0.0s lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. Traceback (most recent call last):. File ""/path/Mambaforge-4.14.0-1/envs/dpv/lib/python3.6/runpy.py"", line 193, in _run_module_as_main. ""__main__"", mod_spec). File ""/path/Mambaforge-4.14.0-1/envs/dpv/lib/python3.6/runpy.py"", line 85, in _run_code. exec(code, run_globals). F",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/598
https://github.com/google/deepvariant/issues/598:1752,performance,error,error,1752,". Tange (2011): GNU Parallel - The Command-Line Power Tool,. ;login: The USENIX Magazine, February 2011:42-47. This helps funding further development; AND IT WON'T COST YOU A CENT. If you pay 10000 EUR you should feel free to use GNU Parallel without citing. To silence this citation notice: run 'parallel --citation'. Computers / CPU cores / Max jobs to run. 1:local / 48 / 3. Computer:jobs running/jobs completed/%of started jobs/Average seconds to complete. ETA: 0s Left: 3 AVG: 0.00s local:3/0/100%/0.0s lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. Traceback (most recent call last):. File ""/path/Mambaforge-4.14.0-1/envs/dpv/lib/python3.6/runpy.py"", line 193, in _run_module_as_main. ""__main__"", mod_spec). File ""/path/Mambaforge-4.14.0-1/envs/dpv/lib/python3.6/runpy.py"", line 85, in _run_code. exec(code, run_globals). File ""/path/Mambaforge-4.14.0-1/envs/dpv/share/deepvariant-",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/598
https://github.com/google/deepvariant/issues/598:1810,performance,error,error,1810,",. ;login: The USENIX Magazine, February 2011:42-47. This helps funding further development; AND IT WON'T COST YOU A CENT. If you pay 10000 EUR you should feel free to use GNU Parallel without citing. To silence this citation notice: run 'parallel --citation'. Computers / CPU cores / Max jobs to run. 1:local / 48 / 3. Computer:jobs running/jobs completed/%of started jobs/Average seconds to complete. ETA: 0s Left: 3 AVG: 0.00s local:3/0/100%/0.0s lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. Traceback (most recent call last):. File ""/path/Mambaforge-4.14.0-1/envs/dpv/lib/python3.6/runpy.py"", line 193, in _run_module_as_main. ""__main__"", mod_spec). File ""/path/Mambaforge-4.14.0-1/envs/dpv/lib/python3.6/runpy.py"", line 85, in _run_code. exec(code, run_globals). File ""/path/Mambaforge-4.14.0-1/envs/dpv/share/deepvariant-1.4.0-0/binaries/DeepVariant/1.4.0/DeepVariant-1.4.0/make_",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/598
https://github.com/google/deepvariant/issues/598:1868,performance,error,error,1868,"helps funding further development; AND IT WON'T COST YOU A CENT. If you pay 10000 EUR you should feel free to use GNU Parallel without citing. To silence this citation notice: run 'parallel --citation'. Computers / CPU cores / Max jobs to run. 1:local / 48 / 3. Computer:jobs running/jobs completed/%of started jobs/Average seconds to complete. ETA: 0s Left: 3 AVG: 0.00s local:3/0/100%/0.0s lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. Traceback (most recent call last):. File ""/path/Mambaforge-4.14.0-1/envs/dpv/lib/python3.6/runpy.py"", line 193, in _run_module_as_main. ""__main__"", mod_spec). File ""/path/Mambaforge-4.14.0-1/envs/dpv/lib/python3.6/runpy.py"", line 85, in _run_code. exec(code, run_globals). File ""/path/Mambaforge-4.14.0-1/envs/dpv/share/deepvariant-1.4.0-0/binaries/DeepVariant/1.4.0/DeepVariant-1.4.0/make_examples.zip/__main__.py"", line 387, in <module>. File ""/p",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/598
https://github.com/google/deepvariant/issues/598:1926,performance,error,error,1926," CENT. If you pay 10000 EUR you should feel free to use GNU Parallel without citing. To silence this citation notice: run 'parallel --citation'. Computers / CPU cores / Max jobs to run. 1:local / 48 / 3. Computer:jobs running/jobs completed/%of started jobs/Average seconds to complete. ETA: 0s Left: 3 AVG: 0.00s local:3/0/100%/0.0s lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. Traceback (most recent call last):. File ""/path/Mambaforge-4.14.0-1/envs/dpv/lib/python3.6/runpy.py"", line 193, in _run_module_as_main. ""__main__"", mod_spec). File ""/path/Mambaforge-4.14.0-1/envs/dpv/lib/python3.6/runpy.py"", line 85, in _run_code. exec(code, run_globals). File ""/path/Mambaforge-4.14.0-1/envs/dpv/share/deepvariant-1.4.0-0/binaries/DeepVariant/1.4.0/DeepVariant-1.4.0/make_examples.zip/__main__.py"", line 387, in <module>. File ""/path/Mambaforge-4.14.0-1/envs/dpv/share/deepvariant-1.4.0-0",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/598
https://github.com/google/deepvariant/issues/598:1984,performance,error,error,1984,"U Parallel without citing. To silence this citation notice: run 'parallel --citation'. Computers / CPU cores / Max jobs to run. 1:local / 48 / 3. Computer:jobs running/jobs completed/%of started jobs/Average seconds to complete. ETA: 0s Left: 3 AVG: 0.00s local:3/0/100%/0.0s lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. Traceback (most recent call last):. File ""/path/Mambaforge-4.14.0-1/envs/dpv/lib/python3.6/runpy.py"", line 193, in _run_module_as_main. ""__main__"", mod_spec). File ""/path/Mambaforge-4.14.0-1/envs/dpv/lib/python3.6/runpy.py"", line 85, in _run_code. exec(code, run_globals). File ""/path/Mambaforge-4.14.0-1/envs/dpv/share/deepvariant-1.4.0-0/binaries/DeepVariant/1.4.0/DeepVariant-1.4.0/make_examples.zip/__main__.py"", line 387, in <module>. File ""/path/Mambaforge-4.14.0-1/envs/dpv/share/deepvariant-1.4.0-0/binaries/DeepVariant/1.4.0/DeepVariant-1.4.0/make_example",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/598
https://github.com/google/deepvariant/issues/598:2042,performance,error,error,2042,": run 'parallel --citation'. Computers / CPU cores / Max jobs to run. 1:local / 48 / 3. Computer:jobs running/jobs completed/%of started jobs/Average seconds to complete. ETA: 0s Left: 3 AVG: 0.00s local:3/0/100%/0.0s lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. Traceback (most recent call last):. File ""/path/Mambaforge-4.14.0-1/envs/dpv/lib/python3.6/runpy.py"", line 193, in _run_module_as_main. ""__main__"", mod_spec). File ""/path/Mambaforge-4.14.0-1/envs/dpv/lib/python3.6/runpy.py"", line 85, in _run_code. exec(code, run_globals). File ""/path/Mambaforge-4.14.0-1/envs/dpv/share/deepvariant-1.4.0-0/binaries/DeepVariant/1.4.0/DeepVariant-1.4.0/make_examples.zip/__main__.py"", line 387, in <module>. File ""/path/Mambaforge-4.14.0-1/envs/dpv/share/deepvariant-1.4.0-0/binaries/DeepVariant/1.4.0/DeepVariant-1.4.0/make_examples.zip/__main__.py"", line 360, in Main. File ""/path/Mambafo",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/598
https://github.com/google/deepvariant/issues/598:2100,performance,error,error,2100,"obs to run. 1:local / 48 / 3. Computer:jobs running/jobs completed/%of started jobs/Average seconds to complete. ETA: 0s Left: 3 AVG: 0.00s local:3/0/100%/0.0s lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. Traceback (most recent call last):. File ""/path/Mambaforge-4.14.0-1/envs/dpv/lib/python3.6/runpy.py"", line 193, in _run_module_as_main. ""__main__"", mod_spec). File ""/path/Mambaforge-4.14.0-1/envs/dpv/lib/python3.6/runpy.py"", line 85, in _run_code. exec(code, run_globals). File ""/path/Mambaforge-4.14.0-1/envs/dpv/share/deepvariant-1.4.0-0/binaries/DeepVariant/1.4.0/DeepVariant-1.4.0/make_examples.zip/__main__.py"", line 387, in <module>. File ""/path/Mambaforge-4.14.0-1/envs/dpv/share/deepvariant-1.4.0-0/binaries/DeepVariant/1.4.0/DeepVariant-1.4.0/make_examples.zip/__main__.py"", line 360, in Main. File ""/path/Mambaforge-4.14.0-1/envs/dpv/lib/python3.6/subprocess.py"", line 2",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/598
https://github.com/google/deepvariant/issues/598:2158,performance,error,error,2158,"ompleted/%of started jobs/Average seconds to complete. ETA: 0s Left: 3 AVG: 0.00s local:3/0/100%/0.0s lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. Traceback (most recent call last):. File ""/path/Mambaforge-4.14.0-1/envs/dpv/lib/python3.6/runpy.py"", line 193, in _run_module_as_main. ""__main__"", mod_spec). File ""/path/Mambaforge-4.14.0-1/envs/dpv/lib/python3.6/runpy.py"", line 85, in _run_code. exec(code, run_globals). File ""/path/Mambaforge-4.14.0-1/envs/dpv/share/deepvariant-1.4.0-0/binaries/DeepVariant/1.4.0/DeepVariant-1.4.0/make_examples.zip/__main__.py"", line 387, in <module>. File ""/path/Mambaforge-4.14.0-1/envs/dpv/share/deepvariant-1.4.0-0/binaries/DeepVariant/1.4.0/DeepVariant-1.4.0/make_examples.zip/__main__.py"", line 360, in Main. File ""/path/Mambaforge-4.14.0-1/envs/dpv/lib/python3.6/subprocess.py"", line 287, in call. with Popen(*popenargs, **kwargs) as p:. File ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/598
https://github.com/google/deepvariant/issues/598:2216,performance,error,error,2216,": 0s Left: 3 AVG: 0.00s local:3/0/100%/0.0s lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. Traceback (most recent call last):. File ""/path/Mambaforge-4.14.0-1/envs/dpv/lib/python3.6/runpy.py"", line 193, in _run_module_as_main. ""__main__"", mod_spec). File ""/path/Mambaforge-4.14.0-1/envs/dpv/lib/python3.6/runpy.py"", line 85, in _run_code. exec(code, run_globals). File ""/path/Mambaforge-4.14.0-1/envs/dpv/share/deepvariant-1.4.0-0/binaries/DeepVariant/1.4.0/DeepVariant-1.4.0/make_examples.zip/__main__.py"", line 387, in <module>. File ""/path/Mambaforge-4.14.0-1/envs/dpv/share/deepvariant-1.4.0-0/binaries/DeepVariant/1.4.0/DeepVariant-1.4.0/make_examples.zip/__main__.py"", line 360, in Main. File ""/path/Mambaforge-4.14.0-1/envs/dpv/lib/python3.6/subprocess.py"", line 287, in call. with Popen(*popenargs, **kwargs) as p:. File ""/path/Mambaforge-4.14.0-1/envs/dpv/lib/python3.6/subproce",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/598
https://github.com/google/deepvariant/issues/598:2274,performance,error,error,2274,"ttributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. Traceback (most recent call last):. File ""/path/Mambaforge-4.14.0-1/envs/dpv/lib/python3.6/runpy.py"", line 193, in _run_module_as_main. ""__main__"", mod_spec). File ""/path/Mambaforge-4.14.0-1/envs/dpv/lib/python3.6/runpy.py"", line 85, in _run_code. exec(code, run_globals). File ""/path/Mambaforge-4.14.0-1/envs/dpv/share/deepvariant-1.4.0-0/binaries/DeepVariant/1.4.0/DeepVariant-1.4.0/make_examples.zip/__main__.py"", line 387, in <module>. File ""/path/Mambaforge-4.14.0-1/envs/dpv/share/deepvariant-1.4.0-0/binaries/DeepVariant/1.4.0/DeepVariant-1.4.0/make_examples.zip/__main__.py"", line 360, in Main. File ""/path/Mambaforge-4.14.0-1/envs/dpv/lib/python3.6/subprocess.py"", line 287, in call. with Popen(*popenargs, **kwargs) as p:. File ""/path/Mambaforge-4.14.0-1/envs/dpv/lib/python3.6/subprocess.py"", line 729, in __init__. restore_signals, start_new_",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/598
https://github.com/google/deepvariant/issues/598:2332,performance,error,error,2332,"ttributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. Traceback (most recent call last):. File ""/path/Mambaforge-4.14.0-1/envs/dpv/lib/python3.6/runpy.py"", line 193, in _run_module_as_main. ""__main__"", mod_spec). File ""/path/Mambaforge-4.14.0-1/envs/dpv/lib/python3.6/runpy.py"", line 85, in _run_code. exec(code, run_globals). File ""/path/Mambaforge-4.14.0-1/envs/dpv/share/deepvariant-1.4.0-0/binaries/DeepVariant/1.4.0/DeepVariant-1.4.0/make_examples.zip/__main__.py"", line 387, in <module>. File ""/path/Mambaforge-4.14.0-1/envs/dpv/share/deepvariant-1.4.0-0/binaries/DeepVariant/1.4.0/DeepVariant-1.4.0/make_examples.zip/__main__.py"", line 360, in Main. File ""/path/Mambaforge-4.14.0-1/envs/dpv/lib/python3.6/subprocess.py"", line 287, in call. with Popen(*popenargs, **kwargs) as p:. File ""/path/Mambaforge-4.14.0-1/envs/dpv/lib/python3.6/subprocess.py"", line 729, in __init__. restore_signals, start_new_session). File ""/path/Mambaforge-4.14.0-1/envs/dpv/lib/pyt",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/598
https://github.com/google/deepvariant/issues/598:2390,performance,error,error,2390,"ttributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. Traceback (most recent call last):. File ""/path/Mambaforge-4.14.0-1/envs/dpv/lib/python3.6/runpy.py"", line 193, in _run_module_as_main. ""__main__"", mod_spec). File ""/path/Mambaforge-4.14.0-1/envs/dpv/lib/python3.6/runpy.py"", line 85, in _run_code. exec(code, run_globals). File ""/path/Mambaforge-4.14.0-1/envs/dpv/share/deepvariant-1.4.0-0/binaries/DeepVariant/1.4.0/DeepVariant-1.4.0/make_examples.zip/__main__.py"", line 387, in <module>. File ""/path/Mambaforge-4.14.0-1/envs/dpv/share/deepvariant-1.4.0-0/binaries/DeepVariant/1.4.0/DeepVariant-1.4.0/make_examples.zip/__main__.py"", line 360, in Main. File ""/path/Mambaforge-4.14.0-1/envs/dpv/lib/python3.6/subprocess.py"", line 287, in call. with Popen(*popenargs, **kwargs) as p:. File ""/path/Mambaforge-4.14.0-1/envs/dpv/lib/python3.6/subprocess.py"", line 729, in __init__. restore_signals, start_new_session). File ""/path/Mambaforge-4.14.0-1/envs/dpv/lib/python3.6/subprocess.py"", line 1364, in _execute_child. raise",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/598
https://github.com/google/deepvariant/issues/598:3546,performance,parallel,parallel,3546,"tes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. Traceback (most recent call last):. File ""/path/Mambaforge-4.14.0-1/envs/dpv/lib/python3.6/runpy.py"", line 193, in _run_module_as_main. ""__main__"", mod_spec). File ""/path/Mambaforge-4.14.0-1/envs/dpv/lib/python3.6/runpy.py"", line 85, in _run_code. exec(code, run_globals). File ""/path/Mambaforge-4.14.0-1/envs/dpv/share/deepvariant-1.4.0-0/binaries/DeepVariant/1.4.0/DeepVariant-1.4.0/make_examples.zip/__main__.py"", line 387, in <module>. File ""/path/Mambaforge-4.14.0-1/envs/dpv/share/deepvariant-1.4.0-0/binaries/DeepVariant/1.4.0/DeepVariant-1.4.0/make_examples.zip/__main__.py"", line 360, in Main. File ""/path/Mambaforge-4.14.0-1/envs/dpv/lib/python3.6/subprocess.py"", line 287, in call. with Popen(*popenargs, **kwargs) as p:. File ""/path/Mambaforge-4.14.0-1/envs/dpv/lib/python3.6/subprocess.py"", line 729, in __init__. restore_signals, start_new_session). File ""/path/Mambaforge-4.14.0-1/envs/dpv/lib/python3.6/subprocess.py"", line 1364, in _execute_child. raise child_exception_type(errno_num, err_msg, err_filename). FileNotFoundError: [Errno 2] No such file or directory: '/usr/bin/python3': '/usr/bin/python3'. parallel: This job failed:. /path/Mambaforge-4.14.0-1/envs/dpv/bin/python /path/Mambaforge-4.14.0-1/envs/dpv/share/deepvariant-1.4.0-0/binaries/DeepVariant/1.4.0/DeepVariant-1.4.0/make_examples.zip --mode calling --ref QJref.fa --reads F0F_sorted.merged.addg.uniq.rmdup.bam --gvcf GVCF/QJ.gvcf.tfrecord@3.gz --sample_name QJ --examples EXAMPLES/QJ.tfrecord@3.gz --task 2. ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/598
https://github.com/google/deepvariant/issues/598:43,reliability,doe,does,43,"deepvariant software installed using mamba does not run correctly ;lchmod error;No such file or directory: '/usr/bin/python3' error; Hi,. I want to call variations on the Pacbio bam data to get gvcf files. But something went wrong with the program, and I spent two days wondering what went wrong. How do you solve this problem? . Please help! (No root permission;Centos7 x86;Only the user directory has read and write permission). ```. dv_make_examples.py --cores 3 --sample QJ --ref QJref.fa --reads F0F_sorted.merged.addg.uniq.rmdup.bam --logdir . --examples EXAMPLES --gvcf GVCF. ```. ```. Academic tradition requires you to cite works you base your article on. When using programs that use GNU Parallel to process data for publication. please cite:. O. Tange (2011): GNU Parallel - The Command-Line Power Tool,. ;login: The USENIX Magazine, February 2011:42-47. This helps funding further development; AND IT WON'T COST YOU A CENT. If you pay 10000 EUR you should feel free to use GNU Parallel without citing. To silence this citation notice: run 'parallel --citation'. Computers / CPU cores / Max jobs to run. 1:local / 48 / 3. Computer:jobs running/jobs completed/%of started jobs/Average seconds to complete. ETA: 0s Left: 3 AVG: 0.00s local:3/0/100%/0.0s lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/598
https://github.com/google/deepvariant/issues/598:3565,reliability,fail,failed,3565,"tes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. Traceback (most recent call last):. File ""/path/Mambaforge-4.14.0-1/envs/dpv/lib/python3.6/runpy.py"", line 193, in _run_module_as_main. ""__main__"", mod_spec). File ""/path/Mambaforge-4.14.0-1/envs/dpv/lib/python3.6/runpy.py"", line 85, in _run_code. exec(code, run_globals). File ""/path/Mambaforge-4.14.0-1/envs/dpv/share/deepvariant-1.4.0-0/binaries/DeepVariant/1.4.0/DeepVariant-1.4.0/make_examples.zip/__main__.py"", line 387, in <module>. File ""/path/Mambaforge-4.14.0-1/envs/dpv/share/deepvariant-1.4.0-0/binaries/DeepVariant/1.4.0/DeepVariant-1.4.0/make_examples.zip/__main__.py"", line 360, in Main. File ""/path/Mambaforge-4.14.0-1/envs/dpv/lib/python3.6/subprocess.py"", line 287, in call. with Popen(*popenargs, **kwargs) as p:. File ""/path/Mambaforge-4.14.0-1/envs/dpv/lib/python3.6/subprocess.py"", line 729, in __init__. restore_signals, start_new_session). File ""/path/Mambaforge-4.14.0-1/envs/dpv/lib/python3.6/subprocess.py"", line 1364, in _execute_child. raise child_exception_type(errno_num, err_msg, err_filename). FileNotFoundError: [Errno 2] No such file or directory: '/usr/bin/python3': '/usr/bin/python3'. parallel: This job failed:. /path/Mambaforge-4.14.0-1/envs/dpv/bin/python /path/Mambaforge-4.14.0-1/envs/dpv/share/deepvariant-1.4.0-0/binaries/DeepVariant/1.4.0/DeepVariant-1.4.0/make_examples.zip --mode calling --ref QJref.fa --reads F0F_sorted.merged.addg.uniq.rmdup.bam --gvcf GVCF/QJ.gvcf.tfrecord@3.gz --sample_name QJ --examples EXAMPLES/QJ.tfrecord@3.gz --task 2. ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/598
https://github.com/google/deepvariant/issues/598:74,safety,error,error,74,"deepvariant software installed using mamba does not run correctly ;lchmod error;No such file or directory: '/usr/bin/python3' error; Hi,. I want to call variations on the Pacbio bam data to get gvcf files. But something went wrong with the program, and I spent two days wondering what went wrong. How do you solve this problem? . Please help! (No root permission;Centos7 x86;Only the user directory has read and write permission). ```. dv_make_examples.py --cores 3 --sample QJ --ref QJref.fa --reads F0F_sorted.merged.addg.uniq.rmdup.bam --logdir . --examples EXAMPLES --gvcf GVCF. ```. ```. Academic tradition requires you to cite works you base your article on. When using programs that use GNU Parallel to process data for publication. please cite:. O. Tange (2011): GNU Parallel - The Command-Line Power Tool,. ;login: The USENIX Magazine, February 2011:42-47. This helps funding further development; AND IT WON'T COST YOU A CENT. If you pay 10000 EUR you should feel free to use GNU Parallel without citing. To silence this citation notice: run 'parallel --citation'. Computers / CPU cores / Max jobs to run. 1:local / 48 / 3. Computer:jobs running/jobs completed/%of started jobs/Average seconds to complete. ETA: 0s Left: 3 AVG: 0.00s local:3/0/100%/0.0s lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/598
https://github.com/google/deepvariant/issues/598:126,safety,error,error,126,"deepvariant software installed using mamba does not run correctly ;lchmod error;No such file or directory: '/usr/bin/python3' error; Hi,. I want to call variations on the Pacbio bam data to get gvcf files. But something went wrong with the program, and I spent two days wondering what went wrong. How do you solve this problem? . Please help! (No root permission;Centos7 x86;Only the user directory has read and write permission). ```. dv_make_examples.py --cores 3 --sample QJ --ref QJref.fa --reads F0F_sorted.merged.addg.uniq.rmdup.bam --logdir . --examples EXAMPLES --gvcf GVCF. ```. ```. Academic tradition requires you to cite works you base your article on. When using programs that use GNU Parallel to process data for publication. please cite:. O. Tange (2011): GNU Parallel - The Command-Line Power Tool,. ;login: The USENIX Magazine, February 2011:42-47. This helps funding further development; AND IT WON'T COST YOU A CENT. If you pay 10000 EUR you should feel free to use GNU Parallel without citing. To silence this citation notice: run 'parallel --citation'. Computers / CPU cores / Max jobs to run. 1:local / 48 / 3. Computer:jobs running/jobs completed/%of started jobs/Average seconds to complete. ETA: 0s Left: 3 AVG: 0.00s local:3/0/100%/0.0s lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/598
https://github.com/google/deepvariant/issues/598:352,safety,permiss,permission,352,"deepvariant software installed using mamba does not run correctly ;lchmod error;No such file or directory: '/usr/bin/python3' error; Hi,. I want to call variations on the Pacbio bam data to get gvcf files. But something went wrong with the program, and I spent two days wondering what went wrong. How do you solve this problem? . Please help! (No root permission;Centos7 x86;Only the user directory has read and write permission). ```. dv_make_examples.py --cores 3 --sample QJ --ref QJref.fa --reads F0F_sorted.merged.addg.uniq.rmdup.bam --logdir . --examples EXAMPLES --gvcf GVCF. ```. ```. Academic tradition requires you to cite works you base your article on. When using programs that use GNU Parallel to process data for publication. please cite:. O. Tange (2011): GNU Parallel - The Command-Line Power Tool,. ;login: The USENIX Magazine, February 2011:42-47. This helps funding further development; AND IT WON'T COST YOU A CENT. If you pay 10000 EUR you should feel free to use GNU Parallel without citing. To silence this citation notice: run 'parallel --citation'. Computers / CPU cores / Max jobs to run. 1:local / 48 / 3. Computer:jobs running/jobs completed/%of started jobs/Average seconds to complete. ETA: 0s Left: 3 AVG: 0.00s local:3/0/100%/0.0s lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/598
https://github.com/google/deepvariant/issues/598:418,safety,permiss,permission,418,"deepvariant software installed using mamba does not run correctly ;lchmod error;No such file or directory: '/usr/bin/python3' error; Hi,. I want to call variations on the Pacbio bam data to get gvcf files. But something went wrong with the program, and I spent two days wondering what went wrong. How do you solve this problem? . Please help! (No root permission;Centos7 x86;Only the user directory has read and write permission). ```. dv_make_examples.py --cores 3 --sample QJ --ref QJref.fa --reads F0F_sorted.merged.addg.uniq.rmdup.bam --logdir . --examples EXAMPLES --gvcf GVCF. ```. ```. Academic tradition requires you to cite works you base your article on. When using programs that use GNU Parallel to process data for publication. please cite:. O. Tange (2011): GNU Parallel - The Command-Line Power Tool,. ;login: The USENIX Magazine, February 2011:42-47. This helps funding further development; AND IT WON'T COST YOU A CENT. If you pay 10000 EUR you should feel free to use GNU Parallel without citing. To silence this citation notice: run 'parallel --citation'. Computers / CPU cores / Max jobs to run. 1:local / 48 / 3. Computer:jobs running/jobs completed/%of started jobs/Average seconds to complete. ETA: 0s Left: 3 AVG: 0.00s local:3/0/100%/0.0s lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/598
https://github.com/google/deepvariant/issues/598:541,safety,log,logdir,541,"deepvariant software installed using mamba does not run correctly ;lchmod error;No such file or directory: '/usr/bin/python3' error; Hi,. I want to call variations on the Pacbio bam data to get gvcf files. But something went wrong with the program, and I spent two days wondering what went wrong. How do you solve this problem? . Please help! (No root permission;Centos7 x86;Only the user directory has read and write permission). ```. dv_make_examples.py --cores 3 --sample QJ --ref QJref.fa --reads F0F_sorted.merged.addg.uniq.rmdup.bam --logdir . --examples EXAMPLES --gvcf GVCF. ```. ```. Academic tradition requires you to cite works you base your article on. When using programs that use GNU Parallel to process data for publication. please cite:. O. Tange (2011): GNU Parallel - The Command-Line Power Tool,. ;login: The USENIX Magazine, February 2011:42-47. This helps funding further development; AND IT WON'T COST YOU A CENT. If you pay 10000 EUR you should feel free to use GNU Parallel without citing. To silence this citation notice: run 'parallel --citation'. Computers / CPU cores / Max jobs to run. 1:local / 48 / 3. Computer:jobs running/jobs completed/%of started jobs/Average seconds to complete. ETA: 0s Left: 3 AVG: 0.00s local:3/0/100%/0.0s lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/598
https://github.com/google/deepvariant/issues/598:817,safety,log,login,817,"deepvariant software installed using mamba does not run correctly ;lchmod error;No such file or directory: '/usr/bin/python3' error; Hi,. I want to call variations on the Pacbio bam data to get gvcf files. But something went wrong with the program, and I spent two days wondering what went wrong. How do you solve this problem? . Please help! (No root permission;Centos7 x86;Only the user directory has read and write permission). ```. dv_make_examples.py --cores 3 --sample QJ --ref QJref.fa --reads F0F_sorted.merged.addg.uniq.rmdup.bam --logdir . --examples EXAMPLES --gvcf GVCF. ```. ```. Academic tradition requires you to cite works you base your article on. When using programs that use GNU Parallel to process data for publication. please cite:. O. Tange (2011): GNU Parallel - The Command-Line Power Tool,. ;login: The USENIX Magazine, February 2011:42-47. This helps funding further development; AND IT WON'T COST YOU A CENT. If you pay 10000 EUR you should feel free to use GNU Parallel without citing. To silence this citation notice: run 'parallel --citation'. Computers / CPU cores / Max jobs to run. 1:local / 48 / 3. Computer:jobs running/jobs completed/%of started jobs/Average seconds to complete. ETA: 0s Left: 3 AVG: 0.00s local:3/0/100%/0.0s lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/598
https://github.com/google/deepvariant/issues/598:1160,safety,compl,completed,1160,"n the Pacbio bam data to get gvcf files. But something went wrong with the program, and I spent two days wondering what went wrong. How do you solve this problem? . Please help! (No root permission;Centos7 x86;Only the user directory has read and write permission). ```. dv_make_examples.py --cores 3 --sample QJ --ref QJref.fa --reads F0F_sorted.merged.addg.uniq.rmdup.bam --logdir . --examples EXAMPLES --gvcf GVCF. ```. ```. Academic tradition requires you to cite works you base your article on. When using programs that use GNU Parallel to process data for publication. please cite:. O. Tange (2011): GNU Parallel - The Command-Line Power Tool,. ;login: The USENIX Magazine, February 2011:42-47. This helps funding further development; AND IT WON'T COST YOU A CENT. If you pay 10000 EUR you should feel free to use GNU Parallel without citing. To silence this citation notice: run 'parallel --citation'. Computers / CPU cores / Max jobs to run. 1:local / 48 / 3. Computer:jobs running/jobs completed/%of started jobs/Average seconds to complete. ETA: 0s Left: 3 AVG: 0.00s local:3/0/100%/0.0s lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/598
https://github.com/google/deepvariant/issues/598:1206,safety,compl,complete,1206,"something went wrong with the program, and I spent two days wondering what went wrong. How do you solve this problem? . Please help! (No root permission;Centos7 x86;Only the user directory has read and write permission). ```. dv_make_examples.py --cores 3 --sample QJ --ref QJref.fa --reads F0F_sorted.merged.addg.uniq.rmdup.bam --logdir . --examples EXAMPLES --gvcf GVCF. ```. ```. Academic tradition requires you to cite works you base your article on. When using programs that use GNU Parallel to process data for publication. please cite:. O. Tange (2011): GNU Parallel - The Command-Line Power Tool,. ;login: The USENIX Magazine, February 2011:42-47. This helps funding further development; AND IT WON'T COST YOU A CENT. If you pay 10000 EUR you should feel free to use GNU Parallel without citing. To silence this citation notice: run 'parallel --citation'. Computers / CPU cores / Max jobs to run. 1:local / 48 / 3. Computer:jobs running/jobs completed/%of started jobs/Average seconds to complete. ETA: 0s Left: 3 AVG: 0.00s local:3/0/100%/0.0s lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attrib",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/598
https://github.com/google/deepvariant/issues/598:1288,safety,error,error,1288,"rong. How do you solve this problem? . Please help! (No root permission;Centos7 x86;Only the user directory has read and write permission). ```. dv_make_examples.py --cores 3 --sample QJ --ref QJref.fa --reads F0F_sorted.merged.addg.uniq.rmdup.bam --logdir . --examples EXAMPLES --gvcf GVCF. ```. ```. Academic tradition requires you to cite works you base your article on. When using programs that use GNU Parallel to process data for publication. please cite:. O. Tange (2011): GNU Parallel - The Command-Line Power Tool,. ;login: The USENIX Magazine, February 2011:42-47. This helps funding further development; AND IT WON'T COST YOU A CENT. If you pay 10000 EUR you should feel free to use GNU Parallel without citing. To silence this citation notice: run 'parallel --citation'. Computers / CPU cores / Max jobs to run. 1:local / 48 / 3. Computer:jobs running/jobs completed/%of started jobs/Average seconds to complete. ETA: 0s Left: 3 AVG: 0.00s local:3/0/100%/0.0s lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function n",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/598
https://github.com/google/deepvariant/issues/598:1346,safety,error,error,1346,"ot permission;Centos7 x86;Only the user directory has read and write permission). ```. dv_make_examples.py --cores 3 --sample QJ --ref QJref.fa --reads F0F_sorted.merged.addg.uniq.rmdup.bam --logdir . --examples EXAMPLES --gvcf GVCF. ```. ```. Academic tradition requires you to cite works you base your article on. When using programs that use GNU Parallel to process data for publication. please cite:. O. Tange (2011): GNU Parallel - The Command-Line Power Tool,. ;login: The USENIX Magazine, February 2011:42-47. This helps funding further development; AND IT WON'T COST YOU A CENT. If you pay 10000 EUR you should feel free to use GNU Parallel without citing. To silence this citation notice: run 'parallel --citation'. Computers / CPU cores / Max jobs to run. 1:local / 48 / 3. Computer:jobs running/jobs completed/%of started jobs/Average seconds to complete. ETA: 0s Left: 3 AVG: 0.00s local:3/0/100%/0.0s lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function n",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/598
https://github.com/google/deepvariant/issues/598:1404,safety,error,error,1404," and write permission). ```. dv_make_examples.py --cores 3 --sample QJ --ref QJref.fa --reads F0F_sorted.merged.addg.uniq.rmdup.bam --logdir . --examples EXAMPLES --gvcf GVCF. ```. ```. Academic tradition requires you to cite works you base your article on. When using programs that use GNU Parallel to process data for publication. please cite:. O. Tange (2011): GNU Parallel - The Command-Line Power Tool,. ;login: The USENIX Magazine, February 2011:42-47. This helps funding further development; AND IT WON'T COST YOU A CENT. If you pay 10000 EUR you should feel free to use GNU Parallel without citing. To silence this citation notice: run 'parallel --citation'. Computers / CPU cores / Max jobs to run. 1:local / 48 / 3. Computer:jobs running/jobs completed/%of started jobs/Average seconds to complete. ETA: 0s Left: 3 AVG: 0.00s local:3/0/100%/0.0s lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function n",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/598
https://github.com/google/deepvariant/issues/598:1462,safety,error,error,1462," --sample QJ --ref QJref.fa --reads F0F_sorted.merged.addg.uniq.rmdup.bam --logdir . --examples EXAMPLES --gvcf GVCF. ```. ```. Academic tradition requires you to cite works you base your article on. When using programs that use GNU Parallel to process data for publication. please cite:. O. Tange (2011): GNU Parallel - The Command-Line Power Tool,. ;login: The USENIX Magazine, February 2011:42-47. This helps funding further development; AND IT WON'T COST YOU A CENT. If you pay 10000 EUR you should feel free to use GNU Parallel without citing. To silence this citation notice: run 'parallel --citation'. Computers / CPU cores / Max jobs to run. 1:local / 48 / 3. Computer:jobs running/jobs completed/%of started jobs/Average seconds to complete. ETA: 0s Left: 3 AVG: 0.00s local:3/0/100%/0.0s lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. Traceback (most recent call last):. File """,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/598
https://github.com/google/deepvariant/issues/598:1520,safety,error,error,1520,".uniq.rmdup.bam --logdir . --examples EXAMPLES --gvcf GVCF. ```. ```. Academic tradition requires you to cite works you base your article on. When using programs that use GNU Parallel to process data for publication. please cite:. O. Tange (2011): GNU Parallel - The Command-Line Power Tool,. ;login: The USENIX Magazine, February 2011:42-47. This helps funding further development; AND IT WON'T COST YOU A CENT. If you pay 10000 EUR you should feel free to use GNU Parallel without citing. To silence this citation notice: run 'parallel --citation'. Computers / CPU cores / Max jobs to run. 1:local / 48 / 3. Computer:jobs running/jobs completed/%of started jobs/Average seconds to complete. ETA: 0s Left: 3 AVG: 0.00s local:3/0/100%/0.0s lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. Traceback (most recent call last):. File ""/path/Mambaforge-4.14.0-1/envs/dpv/lib/python3.6/runpy.py""",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/598
https://github.com/google/deepvariant/issues/598:1578,safety,error,error,1578,". ```. ```. Academic tradition requires you to cite works you base your article on. When using programs that use GNU Parallel to process data for publication. please cite:. O. Tange (2011): GNU Parallel - The Command-Line Power Tool,. ;login: The USENIX Magazine, February 2011:42-47. This helps funding further development; AND IT WON'T COST YOU A CENT. If you pay 10000 EUR you should feel free to use GNU Parallel without citing. To silence this citation notice: run 'parallel --citation'. Computers / CPU cores / Max jobs to run. 1:local / 48 / 3. Computer:jobs running/jobs completed/%of started jobs/Average seconds to complete. ETA: 0s Left: 3 AVG: 0.00s local:3/0/100%/0.0s lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. Traceback (most recent call last):. File ""/path/Mambaforge-4.14.0-1/envs/dpv/lib/python3.6/runpy.py"", line 193, in _run_module_as_main. ""__main__"", mod_spec).",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/598
https://github.com/google/deepvariant/issues/598:1636,safety,error,error,1636,"you base your article on. When using programs that use GNU Parallel to process data for publication. please cite:. O. Tange (2011): GNU Parallel - The Command-Line Power Tool,. ;login: The USENIX Magazine, February 2011:42-47. This helps funding further development; AND IT WON'T COST YOU A CENT. If you pay 10000 EUR you should feel free to use GNU Parallel without citing. To silence this citation notice: run 'parallel --citation'. Computers / CPU cores / Max jobs to run. 1:local / 48 / 3. Computer:jobs running/jobs completed/%of started jobs/Average seconds to complete. ETA: 0s Left: 3 AVG: 0.00s local:3/0/100%/0.0s lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. Traceback (most recent call last):. File ""/path/Mambaforge-4.14.0-1/envs/dpv/lib/python3.6/runpy.py"", line 193, in _run_module_as_main. ""__main__"", mod_spec). File ""/path/Mambaforge-4.14.0-1/envs/dpv/lib/python3.6/ru",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/598
https://github.com/google/deepvariant/issues/598:1694,safety,error,error,1694," Parallel to process data for publication. please cite:. O. Tange (2011): GNU Parallel - The Command-Line Power Tool,. ;login: The USENIX Magazine, February 2011:42-47. This helps funding further development; AND IT WON'T COST YOU A CENT. If you pay 10000 EUR you should feel free to use GNU Parallel without citing. To silence this citation notice: run 'parallel --citation'. Computers / CPU cores / Max jobs to run. 1:local / 48 / 3. Computer:jobs running/jobs completed/%of started jobs/Average seconds to complete. ETA: 0s Left: 3 AVG: 0.00s local:3/0/100%/0.0s lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. Traceback (most recent call last):. File ""/path/Mambaforge-4.14.0-1/envs/dpv/lib/python3.6/runpy.py"", line 193, in _run_module_as_main. ""__main__"", mod_spec). File ""/path/Mambaforge-4.14.0-1/envs/dpv/lib/python3.6/runpy.py"", line 85, in _run_code. exec(code, run_globals). F",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/598
https://github.com/google/deepvariant/issues/598:1752,safety,error,error,1752,". Tange (2011): GNU Parallel - The Command-Line Power Tool,. ;login: The USENIX Magazine, February 2011:42-47. This helps funding further development; AND IT WON'T COST YOU A CENT. If you pay 10000 EUR you should feel free to use GNU Parallel without citing. To silence this citation notice: run 'parallel --citation'. Computers / CPU cores / Max jobs to run. 1:local / 48 / 3. Computer:jobs running/jobs completed/%of started jobs/Average seconds to complete. ETA: 0s Left: 3 AVG: 0.00s local:3/0/100%/0.0s lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. Traceback (most recent call last):. File ""/path/Mambaforge-4.14.0-1/envs/dpv/lib/python3.6/runpy.py"", line 193, in _run_module_as_main. ""__main__"", mod_spec). File ""/path/Mambaforge-4.14.0-1/envs/dpv/lib/python3.6/runpy.py"", line 85, in _run_code. exec(code, run_globals). File ""/path/Mambaforge-4.14.0-1/envs/dpv/share/deepvariant-",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/598
https://github.com/google/deepvariant/issues/598:1810,safety,error,error,1810,",. ;login: The USENIX Magazine, February 2011:42-47. This helps funding further development; AND IT WON'T COST YOU A CENT. If you pay 10000 EUR you should feel free to use GNU Parallel without citing. To silence this citation notice: run 'parallel --citation'. Computers / CPU cores / Max jobs to run. 1:local / 48 / 3. Computer:jobs running/jobs completed/%of started jobs/Average seconds to complete. ETA: 0s Left: 3 AVG: 0.00s local:3/0/100%/0.0s lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. Traceback (most recent call last):. File ""/path/Mambaforge-4.14.0-1/envs/dpv/lib/python3.6/runpy.py"", line 193, in _run_module_as_main. ""__main__"", mod_spec). File ""/path/Mambaforge-4.14.0-1/envs/dpv/lib/python3.6/runpy.py"", line 85, in _run_code. exec(code, run_globals). File ""/path/Mambaforge-4.14.0-1/envs/dpv/share/deepvariant-1.4.0-0/binaries/DeepVariant/1.4.0/DeepVariant-1.4.0/make_",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/598
https://github.com/google/deepvariant/issues/598:1868,safety,error,error,1868,"helps funding further development; AND IT WON'T COST YOU A CENT. If you pay 10000 EUR you should feel free to use GNU Parallel without citing. To silence this citation notice: run 'parallel --citation'. Computers / CPU cores / Max jobs to run. 1:local / 48 / 3. Computer:jobs running/jobs completed/%of started jobs/Average seconds to complete. ETA: 0s Left: 3 AVG: 0.00s local:3/0/100%/0.0s lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. Traceback (most recent call last):. File ""/path/Mambaforge-4.14.0-1/envs/dpv/lib/python3.6/runpy.py"", line 193, in _run_module_as_main. ""__main__"", mod_spec). File ""/path/Mambaforge-4.14.0-1/envs/dpv/lib/python3.6/runpy.py"", line 85, in _run_code. exec(code, run_globals). File ""/path/Mambaforge-4.14.0-1/envs/dpv/share/deepvariant-1.4.0-0/binaries/DeepVariant/1.4.0/DeepVariant-1.4.0/make_examples.zip/__main__.py"", line 387, in <module>. File ""/p",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/598
https://github.com/google/deepvariant/issues/598:1926,safety,error,error,1926," CENT. If you pay 10000 EUR you should feel free to use GNU Parallel without citing. To silence this citation notice: run 'parallel --citation'. Computers / CPU cores / Max jobs to run. 1:local / 48 / 3. Computer:jobs running/jobs completed/%of started jobs/Average seconds to complete. ETA: 0s Left: 3 AVG: 0.00s local:3/0/100%/0.0s lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. Traceback (most recent call last):. File ""/path/Mambaforge-4.14.0-1/envs/dpv/lib/python3.6/runpy.py"", line 193, in _run_module_as_main. ""__main__"", mod_spec). File ""/path/Mambaforge-4.14.0-1/envs/dpv/lib/python3.6/runpy.py"", line 85, in _run_code. exec(code, run_globals). File ""/path/Mambaforge-4.14.0-1/envs/dpv/share/deepvariant-1.4.0-0/binaries/DeepVariant/1.4.0/DeepVariant-1.4.0/make_examples.zip/__main__.py"", line 387, in <module>. File ""/path/Mambaforge-4.14.0-1/envs/dpv/share/deepvariant-1.4.0-0",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/598
https://github.com/google/deepvariant/issues/598:1984,safety,error,error,1984,"U Parallel without citing. To silence this citation notice: run 'parallel --citation'. Computers / CPU cores / Max jobs to run. 1:local / 48 / 3. Computer:jobs running/jobs completed/%of started jobs/Average seconds to complete. ETA: 0s Left: 3 AVG: 0.00s local:3/0/100%/0.0s lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. Traceback (most recent call last):. File ""/path/Mambaforge-4.14.0-1/envs/dpv/lib/python3.6/runpy.py"", line 193, in _run_module_as_main. ""__main__"", mod_spec). File ""/path/Mambaforge-4.14.0-1/envs/dpv/lib/python3.6/runpy.py"", line 85, in _run_code. exec(code, run_globals). File ""/path/Mambaforge-4.14.0-1/envs/dpv/share/deepvariant-1.4.0-0/binaries/DeepVariant/1.4.0/DeepVariant-1.4.0/make_examples.zip/__main__.py"", line 387, in <module>. File ""/path/Mambaforge-4.14.0-1/envs/dpv/share/deepvariant-1.4.0-0/binaries/DeepVariant/1.4.0/DeepVariant-1.4.0/make_example",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/598
https://github.com/google/deepvariant/issues/598:2042,safety,error,error,2042,": run 'parallel --citation'. Computers / CPU cores / Max jobs to run. 1:local / 48 / 3. Computer:jobs running/jobs completed/%of started jobs/Average seconds to complete. ETA: 0s Left: 3 AVG: 0.00s local:3/0/100%/0.0s lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. Traceback (most recent call last):. File ""/path/Mambaforge-4.14.0-1/envs/dpv/lib/python3.6/runpy.py"", line 193, in _run_module_as_main. ""__main__"", mod_spec). File ""/path/Mambaforge-4.14.0-1/envs/dpv/lib/python3.6/runpy.py"", line 85, in _run_code. exec(code, run_globals). File ""/path/Mambaforge-4.14.0-1/envs/dpv/share/deepvariant-1.4.0-0/binaries/DeepVariant/1.4.0/DeepVariant-1.4.0/make_examples.zip/__main__.py"", line 387, in <module>. File ""/path/Mambaforge-4.14.0-1/envs/dpv/share/deepvariant-1.4.0-0/binaries/DeepVariant/1.4.0/DeepVariant-1.4.0/make_examples.zip/__main__.py"", line 360, in Main. File ""/path/Mambafo",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/598
https://github.com/google/deepvariant/issues/598:2100,safety,error,error,2100,"obs to run. 1:local / 48 / 3. Computer:jobs running/jobs completed/%of started jobs/Average seconds to complete. ETA: 0s Left: 3 AVG: 0.00s local:3/0/100%/0.0s lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. Traceback (most recent call last):. File ""/path/Mambaforge-4.14.0-1/envs/dpv/lib/python3.6/runpy.py"", line 193, in _run_module_as_main. ""__main__"", mod_spec). File ""/path/Mambaforge-4.14.0-1/envs/dpv/lib/python3.6/runpy.py"", line 85, in _run_code. exec(code, run_globals). File ""/path/Mambaforge-4.14.0-1/envs/dpv/share/deepvariant-1.4.0-0/binaries/DeepVariant/1.4.0/DeepVariant-1.4.0/make_examples.zip/__main__.py"", line 387, in <module>. File ""/path/Mambaforge-4.14.0-1/envs/dpv/share/deepvariant-1.4.0-0/binaries/DeepVariant/1.4.0/DeepVariant-1.4.0/make_examples.zip/__main__.py"", line 360, in Main. File ""/path/Mambaforge-4.14.0-1/envs/dpv/lib/python3.6/subprocess.py"", line 2",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/598
https://github.com/google/deepvariant/issues/598:2158,safety,error,error,2158,"ompleted/%of started jobs/Average seconds to complete. ETA: 0s Left: 3 AVG: 0.00s local:3/0/100%/0.0s lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. Traceback (most recent call last):. File ""/path/Mambaforge-4.14.0-1/envs/dpv/lib/python3.6/runpy.py"", line 193, in _run_module_as_main. ""__main__"", mod_spec). File ""/path/Mambaforge-4.14.0-1/envs/dpv/lib/python3.6/runpy.py"", line 85, in _run_code. exec(code, run_globals). File ""/path/Mambaforge-4.14.0-1/envs/dpv/share/deepvariant-1.4.0-0/binaries/DeepVariant/1.4.0/DeepVariant-1.4.0/make_examples.zip/__main__.py"", line 387, in <module>. File ""/path/Mambaforge-4.14.0-1/envs/dpv/share/deepvariant-1.4.0-0/binaries/DeepVariant/1.4.0/DeepVariant-1.4.0/make_examples.zip/__main__.py"", line 360, in Main. File ""/path/Mambaforge-4.14.0-1/envs/dpv/lib/python3.6/subprocess.py"", line 287, in call. with Popen(*popenargs, **kwargs) as p:. File ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/598
https://github.com/google/deepvariant/issues/598:2216,safety,error,error,2216,": 0s Left: 3 AVG: 0.00s local:3/0/100%/0.0s lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. Traceback (most recent call last):. File ""/path/Mambaforge-4.14.0-1/envs/dpv/lib/python3.6/runpy.py"", line 193, in _run_module_as_main. ""__main__"", mod_spec). File ""/path/Mambaforge-4.14.0-1/envs/dpv/lib/python3.6/runpy.py"", line 85, in _run_code. exec(code, run_globals). File ""/path/Mambaforge-4.14.0-1/envs/dpv/share/deepvariant-1.4.0-0/binaries/DeepVariant/1.4.0/DeepVariant-1.4.0/make_examples.zip/__main__.py"", line 387, in <module>. File ""/path/Mambaforge-4.14.0-1/envs/dpv/share/deepvariant-1.4.0-0/binaries/DeepVariant/1.4.0/DeepVariant-1.4.0/make_examples.zip/__main__.py"", line 360, in Main. File ""/path/Mambaforge-4.14.0-1/envs/dpv/lib/python3.6/subprocess.py"", line 287, in call. with Popen(*popenargs, **kwargs) as p:. File ""/path/Mambaforge-4.14.0-1/envs/dpv/lib/python3.6/subproce",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/598
https://github.com/google/deepvariant/issues/598:2274,safety,error,error,2274,"ttributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. Traceback (most recent call last):. File ""/path/Mambaforge-4.14.0-1/envs/dpv/lib/python3.6/runpy.py"", line 193, in _run_module_as_main. ""__main__"", mod_spec). File ""/path/Mambaforge-4.14.0-1/envs/dpv/lib/python3.6/runpy.py"", line 85, in _run_code. exec(code, run_globals). File ""/path/Mambaforge-4.14.0-1/envs/dpv/share/deepvariant-1.4.0-0/binaries/DeepVariant/1.4.0/DeepVariant-1.4.0/make_examples.zip/__main__.py"", line 387, in <module>. File ""/path/Mambaforge-4.14.0-1/envs/dpv/share/deepvariant-1.4.0-0/binaries/DeepVariant/1.4.0/DeepVariant-1.4.0/make_examples.zip/__main__.py"", line 360, in Main. File ""/path/Mambaforge-4.14.0-1/envs/dpv/lib/python3.6/subprocess.py"", line 287, in call. with Popen(*popenargs, **kwargs) as p:. File ""/path/Mambaforge-4.14.0-1/envs/dpv/lib/python3.6/subprocess.py"", line 729, in __init__. restore_signals, start_new_",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/598
https://github.com/google/deepvariant/issues/598:2332,safety,error,error,2332,"ttributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. Traceback (most recent call last):. File ""/path/Mambaforge-4.14.0-1/envs/dpv/lib/python3.6/runpy.py"", line 193, in _run_module_as_main. ""__main__"", mod_spec). File ""/path/Mambaforge-4.14.0-1/envs/dpv/lib/python3.6/runpy.py"", line 85, in _run_code. exec(code, run_globals). File ""/path/Mambaforge-4.14.0-1/envs/dpv/share/deepvariant-1.4.0-0/binaries/DeepVariant/1.4.0/DeepVariant-1.4.0/make_examples.zip/__main__.py"", line 387, in <module>. File ""/path/Mambaforge-4.14.0-1/envs/dpv/share/deepvariant-1.4.0-0/binaries/DeepVariant/1.4.0/DeepVariant-1.4.0/make_examples.zip/__main__.py"", line 360, in Main. File ""/path/Mambaforge-4.14.0-1/envs/dpv/lib/python3.6/subprocess.py"", line 287, in call. with Popen(*popenargs, **kwargs) as p:. File ""/path/Mambaforge-4.14.0-1/envs/dpv/lib/python3.6/subprocess.py"", line 729, in __init__. restore_signals, start_new_session). File ""/path/Mambaforge-4.14.0-1/envs/dpv/lib/pyt",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/598
https://github.com/google/deepvariant/issues/598:2390,safety,error,error,2390,"ttributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. Traceback (most recent call last):. File ""/path/Mambaforge-4.14.0-1/envs/dpv/lib/python3.6/runpy.py"", line 193, in _run_module_as_main. ""__main__"", mod_spec). File ""/path/Mambaforge-4.14.0-1/envs/dpv/lib/python3.6/runpy.py"", line 85, in _run_code. exec(code, run_globals). File ""/path/Mambaforge-4.14.0-1/envs/dpv/share/deepvariant-1.4.0-0/binaries/DeepVariant/1.4.0/DeepVariant-1.4.0/make_examples.zip/__main__.py"", line 387, in <module>. File ""/path/Mambaforge-4.14.0-1/envs/dpv/share/deepvariant-1.4.0-0/binaries/DeepVariant/1.4.0/DeepVariant-1.4.0/make_examples.zip/__main__.py"", line 360, in Main. File ""/path/Mambaforge-4.14.0-1/envs/dpv/lib/python3.6/subprocess.py"", line 287, in call. with Popen(*popenargs, **kwargs) as p:. File ""/path/Mambaforge-4.14.0-1/envs/dpv/lib/python3.6/subprocess.py"", line 729, in __init__. restore_signals, start_new_session). File ""/path/Mambaforge-4.14.0-1/envs/dpv/lib/python3.6/subprocess.py"", line 1364, in _execute_child. raise",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/598
https://github.com/google/deepvariant/issues/598:2854,safety,modul,module,2854,"ttributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. Traceback (most recent call last):. File ""/path/Mambaforge-4.14.0-1/envs/dpv/lib/python3.6/runpy.py"", line 193, in _run_module_as_main. ""__main__"", mod_spec). File ""/path/Mambaforge-4.14.0-1/envs/dpv/lib/python3.6/runpy.py"", line 85, in _run_code. exec(code, run_globals). File ""/path/Mambaforge-4.14.0-1/envs/dpv/share/deepvariant-1.4.0-0/binaries/DeepVariant/1.4.0/DeepVariant-1.4.0/make_examples.zip/__main__.py"", line 387, in <module>. File ""/path/Mambaforge-4.14.0-1/envs/dpv/share/deepvariant-1.4.0-0/binaries/DeepVariant/1.4.0/DeepVariant-1.4.0/make_examples.zip/__main__.py"", line 360, in Main. File ""/path/Mambaforge-4.14.0-1/envs/dpv/lib/python3.6/subprocess.py"", line 287, in call. with Popen(*popenargs, **kwargs) as p:. File ""/path/Mambaforge-4.14.0-1/envs/dpv/lib/python3.6/subprocess.py"", line 729, in __init__. restore_signals, start_new_session). File ""/path/Mambaforge-4.14.0-1/envs/dpv/lib/python3.6/subprocess.py"", line 1364, in _execute_child. raise child_exception_type(errno_num, err_msg, err_filename). FileNotFoundError: [Errno 2] No such file or directory: '/usr/bin/python3': '/usr/bin/python3'. parallel: This job failed:. /path/Mambaforge-4.14.0-1/envs/dpv/bin/python /path/Mambaforge-4.14.0-1/envs/dpv/share/deepvariant-1.4.0-0/binaries/DeepVariant/1.4.0/DeepVariant-1.4.0/make_examples.zip --mode calling --ref QJref.fa --reads F0F_sorted.merged.addg.uniq.rmdup.bam --gvcf GVCF/QJ.gvcf.tfrecord@3.gz --s",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/598
https://github.com/google/deepvariant/issues/598:541,security,log,logdir,541,"deepvariant software installed using mamba does not run correctly ;lchmod error;No such file or directory: '/usr/bin/python3' error; Hi,. I want to call variations on the Pacbio bam data to get gvcf files. But something went wrong with the program, and I spent two days wondering what went wrong. How do you solve this problem? . Please help! (No root permission;Centos7 x86;Only the user directory has read and write permission). ```. dv_make_examples.py --cores 3 --sample QJ --ref QJref.fa --reads F0F_sorted.merged.addg.uniq.rmdup.bam --logdir . --examples EXAMPLES --gvcf GVCF. ```. ```. Academic tradition requires you to cite works you base your article on. When using programs that use GNU Parallel to process data for publication. please cite:. O. Tange (2011): GNU Parallel - The Command-Line Power Tool,. ;login: The USENIX Magazine, February 2011:42-47. This helps funding further development; AND IT WON'T COST YOU A CENT. If you pay 10000 EUR you should feel free to use GNU Parallel without citing. To silence this citation notice: run 'parallel --citation'. Computers / CPU cores / Max jobs to run. 1:local / 48 / 3. Computer:jobs running/jobs completed/%of started jobs/Average seconds to complete. ETA: 0s Left: 3 AVG: 0.00s local:3/0/100%/0.0s lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/598
https://github.com/google/deepvariant/issues/598:790,security,Command-Lin,Command-Line,790,"deepvariant software installed using mamba does not run correctly ;lchmod error;No such file or directory: '/usr/bin/python3' error; Hi,. I want to call variations on the Pacbio bam data to get gvcf files. But something went wrong with the program, and I spent two days wondering what went wrong. How do you solve this problem? . Please help! (No root permission;Centos7 x86;Only the user directory has read and write permission). ```. dv_make_examples.py --cores 3 --sample QJ --ref QJref.fa --reads F0F_sorted.merged.addg.uniq.rmdup.bam --logdir . --examples EXAMPLES --gvcf GVCF. ```. ```. Academic tradition requires you to cite works you base your article on. When using programs that use GNU Parallel to process data for publication. please cite:. O. Tange (2011): GNU Parallel - The Command-Line Power Tool,. ;login: The USENIX Magazine, February 2011:42-47. This helps funding further development; AND IT WON'T COST YOU A CENT. If you pay 10000 EUR you should feel free to use GNU Parallel without citing. To silence this citation notice: run 'parallel --citation'. Computers / CPU cores / Max jobs to run. 1:local / 48 / 3. Computer:jobs running/jobs completed/%of started jobs/Average seconds to complete. ETA: 0s Left: 3 AVG: 0.00s local:3/0/100%/0.0s lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/598
https://github.com/google/deepvariant/issues/598:817,security,log,login,817,"deepvariant software installed using mamba does not run correctly ;lchmod error;No such file or directory: '/usr/bin/python3' error; Hi,. I want to call variations on the Pacbio bam data to get gvcf files. But something went wrong with the program, and I spent two days wondering what went wrong. How do you solve this problem? . Please help! (No root permission;Centos7 x86;Only the user directory has read and write permission). ```. dv_make_examples.py --cores 3 --sample QJ --ref QJref.fa --reads F0F_sorted.merged.addg.uniq.rmdup.bam --logdir . --examples EXAMPLES --gvcf GVCF. ```. ```. Academic tradition requires you to cite works you base your article on. When using programs that use GNU Parallel to process data for publication. please cite:. O. Tange (2011): GNU Parallel - The Command-Line Power Tool,. ;login: The USENIX Magazine, February 2011:42-47. This helps funding further development; AND IT WON'T COST YOU A CENT. If you pay 10000 EUR you should feel free to use GNU Parallel without citing. To silence this citation notice: run 'parallel --citation'. Computers / CPU cores / Max jobs to run. 1:local / 48 / 3. Computer:jobs running/jobs completed/%of started jobs/Average seconds to complete. ETA: 0s Left: 3 AVG: 0.00s local:3/0/100%/0.0s lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/598
https://github.com/google/deepvariant/issues/598:1160,security,compl,completed,1160,"n the Pacbio bam data to get gvcf files. But something went wrong with the program, and I spent two days wondering what went wrong. How do you solve this problem? . Please help! (No root permission;Centos7 x86;Only the user directory has read and write permission). ```. dv_make_examples.py --cores 3 --sample QJ --ref QJref.fa --reads F0F_sorted.merged.addg.uniq.rmdup.bam --logdir . --examples EXAMPLES --gvcf GVCF. ```. ```. Academic tradition requires you to cite works you base your article on. When using programs that use GNU Parallel to process data for publication. please cite:. O. Tange (2011): GNU Parallel - The Command-Line Power Tool,. ;login: The USENIX Magazine, February 2011:42-47. This helps funding further development; AND IT WON'T COST YOU A CENT. If you pay 10000 EUR you should feel free to use GNU Parallel without citing. To silence this citation notice: run 'parallel --citation'. Computers / CPU cores / Max jobs to run. 1:local / 48 / 3. Computer:jobs running/jobs completed/%of started jobs/Average seconds to complete. ETA: 0s Left: 3 AVG: 0.00s local:3/0/100%/0.0s lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/598
https://github.com/google/deepvariant/issues/598:1206,security,compl,complete,1206,"something went wrong with the program, and I spent two days wondering what went wrong. How do you solve this problem? . Please help! (No root permission;Centos7 x86;Only the user directory has read and write permission). ```. dv_make_examples.py --cores 3 --sample QJ --ref QJref.fa --reads F0F_sorted.merged.addg.uniq.rmdup.bam --logdir . --examples EXAMPLES --gvcf GVCF. ```. ```. Academic tradition requires you to cite works you base your article on. When using programs that use GNU Parallel to process data for publication. please cite:. O. Tange (2011): GNU Parallel - The Command-Line Power Tool,. ;login: The USENIX Magazine, February 2011:42-47. This helps funding further development; AND IT WON'T COST YOU A CENT. If you pay 10000 EUR you should feel free to use GNU Parallel without citing. To silence this citation notice: run 'parallel --citation'. Computers / CPU cores / Max jobs to run. 1:local / 48 / 3. Computer:jobs running/jobs completed/%of started jobs/Average seconds to complete. ETA: 0s Left: 3 AVG: 0.00s local:3/0/100%/0.0s lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attrib",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/598
https://github.com/google/deepvariant/issues/598:541,testability,log,logdir,541,"deepvariant software installed using mamba does not run correctly ;lchmod error;No such file or directory: '/usr/bin/python3' error; Hi,. I want to call variations on the Pacbio bam data to get gvcf files. But something went wrong with the program, and I spent two days wondering what went wrong. How do you solve this problem? . Please help! (No root permission;Centos7 x86;Only the user directory has read and write permission). ```. dv_make_examples.py --cores 3 --sample QJ --ref QJref.fa --reads F0F_sorted.merged.addg.uniq.rmdup.bam --logdir . --examples EXAMPLES --gvcf GVCF. ```. ```. Academic tradition requires you to cite works you base your article on. When using programs that use GNU Parallel to process data for publication. please cite:. O. Tange (2011): GNU Parallel - The Command-Line Power Tool,. ;login: The USENIX Magazine, February 2011:42-47. This helps funding further development; AND IT WON'T COST YOU A CENT. If you pay 10000 EUR you should feel free to use GNU Parallel without citing. To silence this citation notice: run 'parallel --citation'. Computers / CPU cores / Max jobs to run. 1:local / 48 / 3. Computer:jobs running/jobs completed/%of started jobs/Average seconds to complete. ETA: 0s Left: 3 AVG: 0.00s local:3/0/100%/0.0s lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/598
https://github.com/google/deepvariant/issues/598:817,testability,log,login,817,"deepvariant software installed using mamba does not run correctly ;lchmod error;No such file or directory: '/usr/bin/python3' error; Hi,. I want to call variations on the Pacbio bam data to get gvcf files. But something went wrong with the program, and I spent two days wondering what went wrong. How do you solve this problem? . Please help! (No root permission;Centos7 x86;Only the user directory has read and write permission). ```. dv_make_examples.py --cores 3 --sample QJ --ref QJref.fa --reads F0F_sorted.merged.addg.uniq.rmdup.bam --logdir . --examples EXAMPLES --gvcf GVCF. ```. ```. Academic tradition requires you to cite works you base your article on. When using programs that use GNU Parallel to process data for publication. please cite:. O. Tange (2011): GNU Parallel - The Command-Line Power Tool,. ;login: The USENIX Magazine, February 2011:42-47. This helps funding further development; AND IT WON'T COST YOU A CENT. If you pay 10000 EUR you should feel free to use GNU Parallel without citing. To silence this citation notice: run 'parallel --citation'. Computers / CPU cores / Max jobs to run. 1:local / 48 / 3. Computer:jobs running/jobs completed/%of started jobs/Average seconds to complete. ETA: 0s Left: 3 AVG: 0.00s local:3/0/100%/0.0s lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/598
https://github.com/google/deepvariant/issues/598:2423,testability,Trace,Traceback,2423,"emented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. Traceback (most recent call last):. File ""/path/Mambaforge-4.14.0-1/envs/dpv/lib/python3.6/runpy.py"", line 193, in _run_module_as_main. ""__main__"", mod_spec). File ""/path/Mambaforge-4.14.0-1/envs/dpv/lib/python3.6/runpy.py"", line 85, in _run_code. exec(code, run_globals). File ""/path/Mambaforge-4.14.0-1/envs/dpv/share/deepvariant-1.4.0-0/binaries/DeepVariant/1.4.0/DeepVariant-1.4.0/make_examples.zip/__main__.py"", line 387, in <module>. File ""/path/Mambaforge-4.14.0-1/envs/dpv/share/deepvariant-1.4.0-0/binaries/DeepVariant/1.4.0/DeepVariant-1.4.0/make_examples.zip/__main__.py"", line 360, in Main. File ""/path/Mambaforge-4.14.0-1/envs/dpv/lib/python3.6/subprocess.py"", line 287, in call. with Popen(*popenargs, **kwargs) as p:. File ""/path/Mambaforge-4.14.0-1/envs/dpv/lib/python3.6/subprocess.py"", line 729, in __init__. restore_signals, start_new_session). File ""/path/Mambaforge-4.14.0-1/envs/dpv/lib/python3.6/subprocess.py"", line 1364, in _execute_child. raise child_exception_type(errno_num, er",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/598
https://github.com/google/deepvariant/issues/598:74,usability,error,error,74,"deepvariant software installed using mamba does not run correctly ;lchmod error;No such file or directory: '/usr/bin/python3' error; Hi,. I want to call variations on the Pacbio bam data to get gvcf files. But something went wrong with the program, and I spent two days wondering what went wrong. How do you solve this problem? . Please help! (No root permission;Centos7 x86;Only the user directory has read and write permission). ```. dv_make_examples.py --cores 3 --sample QJ --ref QJref.fa --reads F0F_sorted.merged.addg.uniq.rmdup.bam --logdir . --examples EXAMPLES --gvcf GVCF. ```. ```. Academic tradition requires you to cite works you base your article on. When using programs that use GNU Parallel to process data for publication. please cite:. O. Tange (2011): GNU Parallel - The Command-Line Power Tool,. ;login: The USENIX Magazine, February 2011:42-47. This helps funding further development; AND IT WON'T COST YOU A CENT. If you pay 10000 EUR you should feel free to use GNU Parallel without citing. To silence this citation notice: run 'parallel --citation'. Computers / CPU cores / Max jobs to run. 1:local / 48 / 3. Computer:jobs running/jobs completed/%of started jobs/Average seconds to complete. ETA: 0s Left: 3 AVG: 0.00s local:3/0/100%/0.0s lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/598
https://github.com/google/deepvariant/issues/598:126,usability,error,error,126,"deepvariant software installed using mamba does not run correctly ;lchmod error;No such file or directory: '/usr/bin/python3' error; Hi,. I want to call variations on the Pacbio bam data to get gvcf files. But something went wrong with the program, and I spent two days wondering what went wrong. How do you solve this problem? . Please help! (No root permission;Centos7 x86;Only the user directory has read and write permission). ```. dv_make_examples.py --cores 3 --sample QJ --ref QJref.fa --reads F0F_sorted.merged.addg.uniq.rmdup.bam --logdir . --examples EXAMPLES --gvcf GVCF. ```. ```. Academic tradition requires you to cite works you base your article on. When using programs that use GNU Parallel to process data for publication. please cite:. O. Tange (2011): GNU Parallel - The Command-Line Power Tool,. ;login: The USENIX Magazine, February 2011:42-47. This helps funding further development; AND IT WON'T COST YOU A CENT. If you pay 10000 EUR you should feel free to use GNU Parallel without citing. To silence this citation notice: run 'parallel --citation'. Computers / CPU cores / Max jobs to run. 1:local / 48 / 3. Computer:jobs running/jobs completed/%of started jobs/Average seconds to complete. ETA: 0s Left: 3 AVG: 0.00s local:3/0/100%/0.0s lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/598
https://github.com/google/deepvariant/issues/598:337,usability,help,help,337,"deepvariant software installed using mamba does not run correctly ;lchmod error;No such file or directory: '/usr/bin/python3' error; Hi,. I want to call variations on the Pacbio bam data to get gvcf files. But something went wrong with the program, and I spent two days wondering what went wrong. How do you solve this problem? . Please help! (No root permission;Centos7 x86;Only the user directory has read and write permission). ```. dv_make_examples.py --cores 3 --sample QJ --ref QJref.fa --reads F0F_sorted.merged.addg.uniq.rmdup.bam --logdir . --examples EXAMPLES --gvcf GVCF. ```. ```. Academic tradition requires you to cite works you base your article on. When using programs that use GNU Parallel to process data for publication. please cite:. O. Tange (2011): GNU Parallel - The Command-Line Power Tool,. ;login: The USENIX Magazine, February 2011:42-47. This helps funding further development; AND IT WON'T COST YOU A CENT. If you pay 10000 EUR you should feel free to use GNU Parallel without citing. To silence this citation notice: run 'parallel --citation'. Computers / CPU cores / Max jobs to run. 1:local / 48 / 3. Computer:jobs running/jobs completed/%of started jobs/Average seconds to complete. ETA: 0s Left: 3 AVG: 0.00s local:3/0/100%/0.0s lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/598
https://github.com/google/deepvariant/issues/598:384,usability,user,user,384,"deepvariant software installed using mamba does not run correctly ;lchmod error;No such file or directory: '/usr/bin/python3' error; Hi,. I want to call variations on the Pacbio bam data to get gvcf files. But something went wrong with the program, and I spent two days wondering what went wrong. How do you solve this problem? . Please help! (No root permission;Centos7 x86;Only the user directory has read and write permission). ```. dv_make_examples.py --cores 3 --sample QJ --ref QJref.fa --reads F0F_sorted.merged.addg.uniq.rmdup.bam --logdir . --examples EXAMPLES --gvcf GVCF. ```. ```. Academic tradition requires you to cite works you base your article on. When using programs that use GNU Parallel to process data for publication. please cite:. O. Tange (2011): GNU Parallel - The Command-Line Power Tool,. ;login: The USENIX Magazine, February 2011:42-47. This helps funding further development; AND IT WON'T COST YOU A CENT. If you pay 10000 EUR you should feel free to use GNU Parallel without citing. To silence this citation notice: run 'parallel --citation'. Computers / CPU cores / Max jobs to run. 1:local / 48 / 3. Computer:jobs running/jobs completed/%of started jobs/Average seconds to complete. ETA: 0s Left: 3 AVG: 0.00s local:3/0/100%/0.0s lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/598
https://github.com/google/deepvariant/issues/598:790,usability,Command,Command-Line,790,"deepvariant software installed using mamba does not run correctly ;lchmod error;No such file or directory: '/usr/bin/python3' error; Hi,. I want to call variations on the Pacbio bam data to get gvcf files. But something went wrong with the program, and I spent two days wondering what went wrong. How do you solve this problem? . Please help! (No root permission;Centos7 x86;Only the user directory has read and write permission). ```. dv_make_examples.py --cores 3 --sample QJ --ref QJref.fa --reads F0F_sorted.merged.addg.uniq.rmdup.bam --logdir . --examples EXAMPLES --gvcf GVCF. ```. ```. Academic tradition requires you to cite works you base your article on. When using programs that use GNU Parallel to process data for publication. please cite:. O. Tange (2011): GNU Parallel - The Command-Line Power Tool,. ;login: The USENIX Magazine, February 2011:42-47. This helps funding further development; AND IT WON'T COST YOU A CENT. If you pay 10000 EUR you should feel free to use GNU Parallel without citing. To silence this citation notice: run 'parallel --citation'. Computers / CPU cores / Max jobs to run. 1:local / 48 / 3. Computer:jobs running/jobs completed/%of started jobs/Average seconds to complete. ETA: 0s Left: 3 AVG: 0.00s local:3/0/100%/0.0s lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/598
https://github.com/google/deepvariant/issues/598:809,usability,Tool,Tool,809,"deepvariant software installed using mamba does not run correctly ;lchmod error;No such file or directory: '/usr/bin/python3' error; Hi,. I want to call variations on the Pacbio bam data to get gvcf files. But something went wrong with the program, and I spent two days wondering what went wrong. How do you solve this problem? . Please help! (No root permission;Centos7 x86;Only the user directory has read and write permission). ```. dv_make_examples.py --cores 3 --sample QJ --ref QJref.fa --reads F0F_sorted.merged.addg.uniq.rmdup.bam --logdir . --examples EXAMPLES --gvcf GVCF. ```. ```. Academic tradition requires you to cite works you base your article on. When using programs that use GNU Parallel to process data for publication. please cite:. O. Tange (2011): GNU Parallel - The Command-Line Power Tool,. ;login: The USENIX Magazine, February 2011:42-47. This helps funding further development; AND IT WON'T COST YOU A CENT. If you pay 10000 EUR you should feel free to use GNU Parallel without citing. To silence this citation notice: run 'parallel --citation'. Computers / CPU cores / Max jobs to run. 1:local / 48 / 3. Computer:jobs running/jobs completed/%of started jobs/Average seconds to complete. ETA: 0s Left: 3 AVG: 0.00s local:3/0/100%/0.0s lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/598
https://github.com/google/deepvariant/issues/598:871,usability,help,helps,871,"deepvariant software installed using mamba does not run correctly ;lchmod error;No such file or directory: '/usr/bin/python3' error; Hi,. I want to call variations on the Pacbio bam data to get gvcf files. But something went wrong with the program, and I spent two days wondering what went wrong. How do you solve this problem? . Please help! (No root permission;Centos7 x86;Only the user directory has read and write permission). ```. dv_make_examples.py --cores 3 --sample QJ --ref QJref.fa --reads F0F_sorted.merged.addg.uniq.rmdup.bam --logdir . --examples EXAMPLES --gvcf GVCF. ```. ```. Academic tradition requires you to cite works you base your article on. When using programs that use GNU Parallel to process data for publication. please cite:. O. Tange (2011): GNU Parallel - The Command-Line Power Tool,. ;login: The USENIX Magazine, February 2011:42-47. This helps funding further development; AND IT WON'T COST YOU A CENT. If you pay 10000 EUR you should feel free to use GNU Parallel without citing. To silence this citation notice: run 'parallel --citation'. Computers / CPU cores / Max jobs to run. 1:local / 48 / 3. Computer:jobs running/jobs completed/%of started jobs/Average seconds to complete. ETA: 0s Left: 3 AVG: 0.00s local:3/0/100%/0.0s lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/598
https://github.com/google/deepvariant/issues/598:1288,usability,error,error,1288,"rong. How do you solve this problem? . Please help! (No root permission;Centos7 x86;Only the user directory has read and write permission). ```. dv_make_examples.py --cores 3 --sample QJ --ref QJref.fa --reads F0F_sorted.merged.addg.uniq.rmdup.bam --logdir . --examples EXAMPLES --gvcf GVCF. ```. ```. Academic tradition requires you to cite works you base your article on. When using programs that use GNU Parallel to process data for publication. please cite:. O. Tange (2011): GNU Parallel - The Command-Line Power Tool,. ;login: The USENIX Magazine, February 2011:42-47. This helps funding further development; AND IT WON'T COST YOU A CENT. If you pay 10000 EUR you should feel free to use GNU Parallel without citing. To silence this citation notice: run 'parallel --citation'. Computers / CPU cores / Max jobs to run. 1:local / 48 / 3. Computer:jobs running/jobs completed/%of started jobs/Average seconds to complete. ETA: 0s Left: 3 AVG: 0.00s local:3/0/100%/0.0s lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function n",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/598
https://github.com/google/deepvariant/issues/598:1346,usability,error,error,1346,"ot permission;Centos7 x86;Only the user directory has read and write permission). ```. dv_make_examples.py --cores 3 --sample QJ --ref QJref.fa --reads F0F_sorted.merged.addg.uniq.rmdup.bam --logdir . --examples EXAMPLES --gvcf GVCF. ```. ```. Academic tradition requires you to cite works you base your article on. When using programs that use GNU Parallel to process data for publication. please cite:. O. Tange (2011): GNU Parallel - The Command-Line Power Tool,. ;login: The USENIX Magazine, February 2011:42-47. This helps funding further development; AND IT WON'T COST YOU A CENT. If you pay 10000 EUR you should feel free to use GNU Parallel without citing. To silence this citation notice: run 'parallel --citation'. Computers / CPU cores / Max jobs to run. 1:local / 48 / 3. Computer:jobs running/jobs completed/%of started jobs/Average seconds to complete. ETA: 0s Left: 3 AVG: 0.00s local:3/0/100%/0.0s lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function n",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/598
https://github.com/google/deepvariant/issues/598:1404,usability,error,error,1404," and write permission). ```. dv_make_examples.py --cores 3 --sample QJ --ref QJref.fa --reads F0F_sorted.merged.addg.uniq.rmdup.bam --logdir . --examples EXAMPLES --gvcf GVCF. ```. ```. Academic tradition requires you to cite works you base your article on. When using programs that use GNU Parallel to process data for publication. please cite:. O. Tange (2011): GNU Parallel - The Command-Line Power Tool,. ;login: The USENIX Magazine, February 2011:42-47. This helps funding further development; AND IT WON'T COST YOU A CENT. If you pay 10000 EUR you should feel free to use GNU Parallel without citing. To silence this citation notice: run 'parallel --citation'. Computers / CPU cores / Max jobs to run. 1:local / 48 / 3. Computer:jobs running/jobs completed/%of started jobs/Average seconds to complete. ETA: 0s Left: 3 AVG: 0.00s local:3/0/100%/0.0s lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function n",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/598
https://github.com/google/deepvariant/issues/598:1462,usability,error,error,1462," --sample QJ --ref QJref.fa --reads F0F_sorted.merged.addg.uniq.rmdup.bam --logdir . --examples EXAMPLES --gvcf GVCF. ```. ```. Academic tradition requires you to cite works you base your article on. When using programs that use GNU Parallel to process data for publication. please cite:. O. Tange (2011): GNU Parallel - The Command-Line Power Tool,. ;login: The USENIX Magazine, February 2011:42-47. This helps funding further development; AND IT WON'T COST YOU A CENT. If you pay 10000 EUR you should feel free to use GNU Parallel without citing. To silence this citation notice: run 'parallel --citation'. Computers / CPU cores / Max jobs to run. 1:local / 48 / 3. Computer:jobs running/jobs completed/%of started jobs/Average seconds to complete. ETA: 0s Left: 3 AVG: 0.00s local:3/0/100%/0.0s lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. Traceback (most recent call last):. File """,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/598
https://github.com/google/deepvariant/issues/598:1520,usability,error,error,1520,".uniq.rmdup.bam --logdir . --examples EXAMPLES --gvcf GVCF. ```. ```. Academic tradition requires you to cite works you base your article on. When using programs that use GNU Parallel to process data for publication. please cite:. O. Tange (2011): GNU Parallel - The Command-Line Power Tool,. ;login: The USENIX Magazine, February 2011:42-47. This helps funding further development; AND IT WON'T COST YOU A CENT. If you pay 10000 EUR you should feel free to use GNU Parallel without citing. To silence this citation notice: run 'parallel --citation'. Computers / CPU cores / Max jobs to run. 1:local / 48 / 3. Computer:jobs running/jobs completed/%of started jobs/Average seconds to complete. ETA: 0s Left: 3 AVG: 0.00s local:3/0/100%/0.0s lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. Traceback (most recent call last):. File ""/path/Mambaforge-4.14.0-1/envs/dpv/lib/python3.6/runpy.py""",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/598
https://github.com/google/deepvariant/issues/598:1578,usability,error,error,1578,". ```. ```. Academic tradition requires you to cite works you base your article on. When using programs that use GNU Parallel to process data for publication. please cite:. O. Tange (2011): GNU Parallel - The Command-Line Power Tool,. ;login: The USENIX Magazine, February 2011:42-47. This helps funding further development; AND IT WON'T COST YOU A CENT. If you pay 10000 EUR you should feel free to use GNU Parallel without citing. To silence this citation notice: run 'parallel --citation'. Computers / CPU cores / Max jobs to run. 1:local / 48 / 3. Computer:jobs running/jobs completed/%of started jobs/Average seconds to complete. ETA: 0s Left: 3 AVG: 0.00s local:3/0/100%/0.0s lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. Traceback (most recent call last):. File ""/path/Mambaforge-4.14.0-1/envs/dpv/lib/python3.6/runpy.py"", line 193, in _run_module_as_main. ""__main__"", mod_spec).",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/598
https://github.com/google/deepvariant/issues/598:1636,usability,error,error,1636,"you base your article on. When using programs that use GNU Parallel to process data for publication. please cite:. O. Tange (2011): GNU Parallel - The Command-Line Power Tool,. ;login: The USENIX Magazine, February 2011:42-47. This helps funding further development; AND IT WON'T COST YOU A CENT. If you pay 10000 EUR you should feel free to use GNU Parallel without citing. To silence this citation notice: run 'parallel --citation'. Computers / CPU cores / Max jobs to run. 1:local / 48 / 3. Computer:jobs running/jobs completed/%of started jobs/Average seconds to complete. ETA: 0s Left: 3 AVG: 0.00s local:3/0/100%/0.0s lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. Traceback (most recent call last):. File ""/path/Mambaforge-4.14.0-1/envs/dpv/lib/python3.6/runpy.py"", line 193, in _run_module_as_main. ""__main__"", mod_spec). File ""/path/Mambaforge-4.14.0-1/envs/dpv/lib/python3.6/ru",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/598
https://github.com/google/deepvariant/issues/598:1694,usability,error,error,1694," Parallel to process data for publication. please cite:. O. Tange (2011): GNU Parallel - The Command-Line Power Tool,. ;login: The USENIX Magazine, February 2011:42-47. This helps funding further development; AND IT WON'T COST YOU A CENT. If you pay 10000 EUR you should feel free to use GNU Parallel without citing. To silence this citation notice: run 'parallel --citation'. Computers / CPU cores / Max jobs to run. 1:local / 48 / 3. Computer:jobs running/jobs completed/%of started jobs/Average seconds to complete. ETA: 0s Left: 3 AVG: 0.00s local:3/0/100%/0.0s lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. Traceback (most recent call last):. File ""/path/Mambaforge-4.14.0-1/envs/dpv/lib/python3.6/runpy.py"", line 193, in _run_module_as_main. ""__main__"", mod_spec). File ""/path/Mambaforge-4.14.0-1/envs/dpv/lib/python3.6/runpy.py"", line 85, in _run_code. exec(code, run_globals). F",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/598
https://github.com/google/deepvariant/issues/598:1752,usability,error,error,1752,". Tange (2011): GNU Parallel - The Command-Line Power Tool,. ;login: The USENIX Magazine, February 2011:42-47. This helps funding further development; AND IT WON'T COST YOU A CENT. If you pay 10000 EUR you should feel free to use GNU Parallel without citing. To silence this citation notice: run 'parallel --citation'. Computers / CPU cores / Max jobs to run. 1:local / 48 / 3. Computer:jobs running/jobs completed/%of started jobs/Average seconds to complete. ETA: 0s Left: 3 AVG: 0.00s local:3/0/100%/0.0s lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. Traceback (most recent call last):. File ""/path/Mambaforge-4.14.0-1/envs/dpv/lib/python3.6/runpy.py"", line 193, in _run_module_as_main. ""__main__"", mod_spec). File ""/path/Mambaforge-4.14.0-1/envs/dpv/lib/python3.6/runpy.py"", line 85, in _run_code. exec(code, run_globals). File ""/path/Mambaforge-4.14.0-1/envs/dpv/share/deepvariant-",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/598
https://github.com/google/deepvariant/issues/598:1810,usability,error,error,1810,",. ;login: The USENIX Magazine, February 2011:42-47. This helps funding further development; AND IT WON'T COST YOU A CENT. If you pay 10000 EUR you should feel free to use GNU Parallel without citing. To silence this citation notice: run 'parallel --citation'. Computers / CPU cores / Max jobs to run. 1:local / 48 / 3. Computer:jobs running/jobs completed/%of started jobs/Average seconds to complete. ETA: 0s Left: 3 AVG: 0.00s local:3/0/100%/0.0s lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. Traceback (most recent call last):. File ""/path/Mambaforge-4.14.0-1/envs/dpv/lib/python3.6/runpy.py"", line 193, in _run_module_as_main. ""__main__"", mod_spec). File ""/path/Mambaforge-4.14.0-1/envs/dpv/lib/python3.6/runpy.py"", line 85, in _run_code. exec(code, run_globals). File ""/path/Mambaforge-4.14.0-1/envs/dpv/share/deepvariant-1.4.0-0/binaries/DeepVariant/1.4.0/DeepVariant-1.4.0/make_",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/598
https://github.com/google/deepvariant/issues/598:1868,usability,error,error,1868,"helps funding further development; AND IT WON'T COST YOU A CENT. If you pay 10000 EUR you should feel free to use GNU Parallel without citing. To silence this citation notice: run 'parallel --citation'. Computers / CPU cores / Max jobs to run. 1:local / 48 / 3. Computer:jobs running/jobs completed/%of started jobs/Average seconds to complete. ETA: 0s Left: 3 AVG: 0.00s local:3/0/100%/0.0s lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. Traceback (most recent call last):. File ""/path/Mambaforge-4.14.0-1/envs/dpv/lib/python3.6/runpy.py"", line 193, in _run_module_as_main. ""__main__"", mod_spec). File ""/path/Mambaforge-4.14.0-1/envs/dpv/lib/python3.6/runpy.py"", line 85, in _run_code. exec(code, run_globals). File ""/path/Mambaforge-4.14.0-1/envs/dpv/share/deepvariant-1.4.0-0/binaries/DeepVariant/1.4.0/DeepVariant-1.4.0/make_examples.zip/__main__.py"", line 387, in <module>. File ""/p",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/598
https://github.com/google/deepvariant/issues/598:1926,usability,error,error,1926," CENT. If you pay 10000 EUR you should feel free to use GNU Parallel without citing. To silence this citation notice: run 'parallel --citation'. Computers / CPU cores / Max jobs to run. 1:local / 48 / 3. Computer:jobs running/jobs completed/%of started jobs/Average seconds to complete. ETA: 0s Left: 3 AVG: 0.00s local:3/0/100%/0.0s lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. Traceback (most recent call last):. File ""/path/Mambaforge-4.14.0-1/envs/dpv/lib/python3.6/runpy.py"", line 193, in _run_module_as_main. ""__main__"", mod_spec). File ""/path/Mambaforge-4.14.0-1/envs/dpv/lib/python3.6/runpy.py"", line 85, in _run_code. exec(code, run_globals). File ""/path/Mambaforge-4.14.0-1/envs/dpv/share/deepvariant-1.4.0-0/binaries/DeepVariant/1.4.0/DeepVariant-1.4.0/make_examples.zip/__main__.py"", line 387, in <module>. File ""/path/Mambaforge-4.14.0-1/envs/dpv/share/deepvariant-1.4.0-0",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/598
https://github.com/google/deepvariant/issues/598:1984,usability,error,error,1984,"U Parallel without citing. To silence this citation notice: run 'parallel --citation'. Computers / CPU cores / Max jobs to run. 1:local / 48 / 3. Computer:jobs running/jobs completed/%of started jobs/Average seconds to complete. ETA: 0s Left: 3 AVG: 0.00s local:3/0/100%/0.0s lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. Traceback (most recent call last):. File ""/path/Mambaforge-4.14.0-1/envs/dpv/lib/python3.6/runpy.py"", line 193, in _run_module_as_main. ""__main__"", mod_spec). File ""/path/Mambaforge-4.14.0-1/envs/dpv/lib/python3.6/runpy.py"", line 85, in _run_code. exec(code, run_globals). File ""/path/Mambaforge-4.14.0-1/envs/dpv/share/deepvariant-1.4.0-0/binaries/DeepVariant/1.4.0/DeepVariant-1.4.0/make_examples.zip/__main__.py"", line 387, in <module>. File ""/path/Mambaforge-4.14.0-1/envs/dpv/share/deepvariant-1.4.0-0/binaries/DeepVariant/1.4.0/DeepVariant-1.4.0/make_example",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/598
https://github.com/google/deepvariant/issues/598:2042,usability,error,error,2042,": run 'parallel --citation'. Computers / CPU cores / Max jobs to run. 1:local / 48 / 3. Computer:jobs running/jobs completed/%of started jobs/Average seconds to complete. ETA: 0s Left: 3 AVG: 0.00s local:3/0/100%/0.0s lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. Traceback (most recent call last):. File ""/path/Mambaforge-4.14.0-1/envs/dpv/lib/python3.6/runpy.py"", line 193, in _run_module_as_main. ""__main__"", mod_spec). File ""/path/Mambaforge-4.14.0-1/envs/dpv/lib/python3.6/runpy.py"", line 85, in _run_code. exec(code, run_globals). File ""/path/Mambaforge-4.14.0-1/envs/dpv/share/deepvariant-1.4.0-0/binaries/DeepVariant/1.4.0/DeepVariant-1.4.0/make_examples.zip/__main__.py"", line 387, in <module>. File ""/path/Mambaforge-4.14.0-1/envs/dpv/share/deepvariant-1.4.0-0/binaries/DeepVariant/1.4.0/DeepVariant-1.4.0/make_examples.zip/__main__.py"", line 360, in Main. File ""/path/Mambafo",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/598
https://github.com/google/deepvariant/issues/598:2100,usability,error,error,2100,"obs to run. 1:local / 48 / 3. Computer:jobs running/jobs completed/%of started jobs/Average seconds to complete. ETA: 0s Left: 3 AVG: 0.00s local:3/0/100%/0.0s lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. Traceback (most recent call last):. File ""/path/Mambaforge-4.14.0-1/envs/dpv/lib/python3.6/runpy.py"", line 193, in _run_module_as_main. ""__main__"", mod_spec). File ""/path/Mambaforge-4.14.0-1/envs/dpv/lib/python3.6/runpy.py"", line 85, in _run_code. exec(code, run_globals). File ""/path/Mambaforge-4.14.0-1/envs/dpv/share/deepvariant-1.4.0-0/binaries/DeepVariant/1.4.0/DeepVariant-1.4.0/make_examples.zip/__main__.py"", line 387, in <module>. File ""/path/Mambaforge-4.14.0-1/envs/dpv/share/deepvariant-1.4.0-0/binaries/DeepVariant/1.4.0/DeepVariant-1.4.0/make_examples.zip/__main__.py"", line 360, in Main. File ""/path/Mambaforge-4.14.0-1/envs/dpv/lib/python3.6/subprocess.py"", line 2",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/598
https://github.com/google/deepvariant/issues/598:2158,usability,error,error,2158,"ompleted/%of started jobs/Average seconds to complete. ETA: 0s Left: 3 AVG: 0.00s local:3/0/100%/0.0s lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. Traceback (most recent call last):. File ""/path/Mambaforge-4.14.0-1/envs/dpv/lib/python3.6/runpy.py"", line 193, in _run_module_as_main. ""__main__"", mod_spec). File ""/path/Mambaforge-4.14.0-1/envs/dpv/lib/python3.6/runpy.py"", line 85, in _run_code. exec(code, run_globals). File ""/path/Mambaforge-4.14.0-1/envs/dpv/share/deepvariant-1.4.0-0/binaries/DeepVariant/1.4.0/DeepVariant-1.4.0/make_examples.zip/__main__.py"", line 387, in <module>. File ""/path/Mambaforge-4.14.0-1/envs/dpv/share/deepvariant-1.4.0-0/binaries/DeepVariant/1.4.0/DeepVariant-1.4.0/make_examples.zip/__main__.py"", line 360, in Main. File ""/path/Mambaforge-4.14.0-1/envs/dpv/lib/python3.6/subprocess.py"", line 287, in call. with Popen(*popenargs, **kwargs) as p:. File ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/598
https://github.com/google/deepvariant/issues/598:2216,usability,error,error,2216,": 0s Left: 3 AVG: 0.00s local:3/0/100%/0.0s lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. Traceback (most recent call last):. File ""/path/Mambaforge-4.14.0-1/envs/dpv/lib/python3.6/runpy.py"", line 193, in _run_module_as_main. ""__main__"", mod_spec). File ""/path/Mambaforge-4.14.0-1/envs/dpv/lib/python3.6/runpy.py"", line 85, in _run_code. exec(code, run_globals). File ""/path/Mambaforge-4.14.0-1/envs/dpv/share/deepvariant-1.4.0-0/binaries/DeepVariant/1.4.0/DeepVariant-1.4.0/make_examples.zip/__main__.py"", line 387, in <module>. File ""/path/Mambaforge-4.14.0-1/envs/dpv/share/deepvariant-1.4.0-0/binaries/DeepVariant/1.4.0/DeepVariant-1.4.0/make_examples.zip/__main__.py"", line 360, in Main. File ""/path/Mambaforge-4.14.0-1/envs/dpv/lib/python3.6/subprocess.py"", line 287, in call. with Popen(*popenargs, **kwargs) as p:. File ""/path/Mambaforge-4.14.0-1/envs/dpv/lib/python3.6/subproce",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/598
https://github.com/google/deepvariant/issues/598:2274,usability,error,error,2274,"ttributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. Traceback (most recent call last):. File ""/path/Mambaforge-4.14.0-1/envs/dpv/lib/python3.6/runpy.py"", line 193, in _run_module_as_main. ""__main__"", mod_spec). File ""/path/Mambaforge-4.14.0-1/envs/dpv/lib/python3.6/runpy.py"", line 85, in _run_code. exec(code, run_globals). File ""/path/Mambaforge-4.14.0-1/envs/dpv/share/deepvariant-1.4.0-0/binaries/DeepVariant/1.4.0/DeepVariant-1.4.0/make_examples.zip/__main__.py"", line 387, in <module>. File ""/path/Mambaforge-4.14.0-1/envs/dpv/share/deepvariant-1.4.0-0/binaries/DeepVariant/1.4.0/DeepVariant-1.4.0/make_examples.zip/__main__.py"", line 360, in Main. File ""/path/Mambaforge-4.14.0-1/envs/dpv/lib/python3.6/subprocess.py"", line 287, in call. with Popen(*popenargs, **kwargs) as p:. File ""/path/Mambaforge-4.14.0-1/envs/dpv/lib/python3.6/subprocess.py"", line 729, in __init__. restore_signals, start_new_",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/598
https://github.com/google/deepvariant/issues/598:2332,usability,error,error,2332,"ttributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. Traceback (most recent call last):. File ""/path/Mambaforge-4.14.0-1/envs/dpv/lib/python3.6/runpy.py"", line 193, in _run_module_as_main. ""__main__"", mod_spec). File ""/path/Mambaforge-4.14.0-1/envs/dpv/lib/python3.6/runpy.py"", line 85, in _run_code. exec(code, run_globals). File ""/path/Mambaforge-4.14.0-1/envs/dpv/share/deepvariant-1.4.0-0/binaries/DeepVariant/1.4.0/DeepVariant-1.4.0/make_examples.zip/__main__.py"", line 387, in <module>. File ""/path/Mambaforge-4.14.0-1/envs/dpv/share/deepvariant-1.4.0-0/binaries/DeepVariant/1.4.0/DeepVariant-1.4.0/make_examples.zip/__main__.py"", line 360, in Main. File ""/path/Mambaforge-4.14.0-1/envs/dpv/lib/python3.6/subprocess.py"", line 287, in call. with Popen(*popenargs, **kwargs) as p:. File ""/path/Mambaforge-4.14.0-1/envs/dpv/lib/python3.6/subprocess.py"", line 729, in __init__. restore_signals, start_new_session). File ""/path/Mambaforge-4.14.0-1/envs/dpv/lib/pyt",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/598
https://github.com/google/deepvariant/issues/598:2390,usability,error,error,2390,"ttributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. lchmod (file attributes) error: Function not implemented. Traceback (most recent call last):. File ""/path/Mambaforge-4.14.0-1/envs/dpv/lib/python3.6/runpy.py"", line 193, in _run_module_as_main. ""__main__"", mod_spec). File ""/path/Mambaforge-4.14.0-1/envs/dpv/lib/python3.6/runpy.py"", line 85, in _run_code. exec(code, run_globals). File ""/path/Mambaforge-4.14.0-1/envs/dpv/share/deepvariant-1.4.0-0/binaries/DeepVariant/1.4.0/DeepVariant-1.4.0/make_examples.zip/__main__.py"", line 387, in <module>. File ""/path/Mambaforge-4.14.0-1/envs/dpv/share/deepvariant-1.4.0-0/binaries/DeepVariant/1.4.0/DeepVariant-1.4.0/make_examples.zip/__main__.py"", line 360, in Main. File ""/path/Mambaforge-4.14.0-1/envs/dpv/lib/python3.6/subprocess.py"", line 287, in call. with Popen(*popenargs, **kwargs) as p:. File ""/path/Mambaforge-4.14.0-1/envs/dpv/lib/python3.6/subprocess.py"", line 729, in __init__. restore_signals, start_new_session). File ""/path/Mambaforge-4.14.0-1/envs/dpv/lib/python3.6/subprocess.py"", line 1364, in _execute_child. raise",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/598
https://github.com/google/deepvariant/issues/599:343,availability,Operat,Operating,343,"DeepVariant does not complete postprocess_variants; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**:. **Describe the issue:** DeepVariant does not complete postprocess_variants (feels like stuck; already waiting for completion for two days; other SAMPLES have already finished runninig). **Setup**. - Operating system: HPC. - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): Docker --> Singularity. - Type of data: WGS data. **Steps to reproduce:**. - Command: `run_deepvariant --model_type=WGS --ref=${FASTA} --reads=${BAMFILE} --output_vcf=${OUT_SAMPLE}.vcf.gz --output_gvcf=${OUT_SAMPLE}.g.vcf.gz`. - Error trace: (if applicable) . No error, but DeepVariant does not seem to continue processing:. ```. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""/home/rzei0002/xm41_scratch/hg38_resources/resources_broad_hg38_v0_Homo_sapiens_assembly38.fasta"" --infile ""/tmp/tmp8vfbqj_y/call_variants_output.tfrecord.gz"" --outfile ""/scratch/xm41. /ct/bamsDown/30x/DEEPV/GRALF001.vcf.gz"" --nonvariant_site_tfrecord_path ""/tmp/tmp8vfbqj_y/gvcf.tfrecord@1.gz"" --gvcf_outfile ""/scratch/xm41/ct/bamsDown/30x/DEEPV/GRALF001.g.vcf.gz"". ```. ![image](https://user-images.githubusercontent.com/73748542/208531940-a0cbd86a-acb7-4ff4-9168-57c3b865625c.png). Based on other processes, the final process should not take that long:. ![image](https://user-images.githubusercontent.com/73748542/208532607-d0ae4fd2-38ac-48f9-889d-2d4343e60fe5.png). Is there a way to only re-initiate the postprocess_variants process? .",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/599
https://github.com/google/deepvariant/issues/599:682,availability,Error,Error,682,"DeepVariant does not complete postprocess_variants; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**:. **Describe the issue:** DeepVariant does not complete postprocess_variants (feels like stuck; already waiting for completion for two days; other SAMPLES have already finished runninig). **Setup**. - Operating system: HPC. - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): Docker --> Singularity. - Type of data: WGS data. **Steps to reproduce:**. - Command: `run_deepvariant --model_type=WGS --ref=${FASTA} --reads=${BAMFILE} --output_vcf=${OUT_SAMPLE}.vcf.gz --output_gvcf=${OUT_SAMPLE}.g.vcf.gz`. - Error trace: (if applicable) . No error, but DeepVariant does not seem to continue processing:. ```. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""/home/rzei0002/xm41_scratch/hg38_resources/resources_broad_hg38_v0_Homo_sapiens_assembly38.fasta"" --infile ""/tmp/tmp8vfbqj_y/call_variants_output.tfrecord.gz"" --outfile ""/scratch/xm41. /ct/bamsDown/30x/DEEPV/GRALF001.vcf.gz"" --nonvariant_site_tfrecord_path ""/tmp/tmp8vfbqj_y/gvcf.tfrecord@1.gz"" --gvcf_outfile ""/scratch/xm41/ct/bamsDown/30x/DEEPV/GRALF001.g.vcf.gz"". ```. ![image](https://user-images.githubusercontent.com/73748542/208531940-a0cbd86a-acb7-4ff4-9168-57c3b865625c.png). Based on other processes, the final process should not take that long:. ![image](https://user-images.githubusercontent.com/73748542/208532607-d0ae4fd2-38ac-48f9-889d-2d4343e60fe5.png). Is there a way to only re-initiate the postprocess_variants process? .",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/599
https://github.com/google/deepvariant/issues/599:716,availability,error,error,716,"DeepVariant does not complete postprocess_variants; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**:. **Describe the issue:** DeepVariant does not complete postprocess_variants (feels like stuck; already waiting for completion for two days; other SAMPLES have already finished runninig). **Setup**. - Operating system: HPC. - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): Docker --> Singularity. - Type of data: WGS data. **Steps to reproduce:**. - Command: `run_deepvariant --model_type=WGS --ref=${FASTA} --reads=${BAMFILE} --output_vcf=${OUT_SAMPLE}.vcf.gz --output_gvcf=${OUT_SAMPLE}.g.vcf.gz`. - Error trace: (if applicable) . No error, but DeepVariant does not seem to continue processing:. ```. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""/home/rzei0002/xm41_scratch/hg38_resources/resources_broad_hg38_v0_Homo_sapiens_assembly38.fasta"" --infile ""/tmp/tmp8vfbqj_y/call_variants_output.tfrecord.gz"" --outfile ""/scratch/xm41. /ct/bamsDown/30x/DEEPV/GRALF001.vcf.gz"" --nonvariant_site_tfrecord_path ""/tmp/tmp8vfbqj_y/gvcf.tfrecord@1.gz"" --gvcf_outfile ""/scratch/xm41/ct/bamsDown/30x/DEEPV/GRALF001.g.vcf.gz"". ```. ![image](https://user-images.githubusercontent.com/73748542/208531940-a0cbd86a-acb7-4ff4-9168-57c3b865625c.png). Based on other processes, the final process should not take that long:. ![image](https://user-images.githubusercontent.com/73748542/208532607-d0ae4fd2-38ac-48f9-889d-2d4343e60fe5.png). Is there a way to only re-initiate the postprocess_variants process? .",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/599
https://github.com/google/deepvariant/issues/599:380,deployability,version,version,380,"DeepVariant does not complete postprocess_variants; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**:. **Describe the issue:** DeepVariant does not complete postprocess_variants (feels like stuck; already waiting for completion for two days; other SAMPLES have already finished runninig). **Setup**. - Operating system: HPC. - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): Docker --> Singularity. - Type of data: WGS data. **Steps to reproduce:**. - Command: `run_deepvariant --model_type=WGS --ref=${FASTA} --reads=${BAMFILE} --output_vcf=${OUT_SAMPLE}.vcf.gz --output_gvcf=${OUT_SAMPLE}.g.vcf.gz`. - Error trace: (if applicable) . No error, but DeepVariant does not seem to continue processing:. ```. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""/home/rzei0002/xm41_scratch/hg38_resources/resources_broad_hg38_v0_Homo_sapiens_assembly38.fasta"" --infile ""/tmp/tmp8vfbqj_y/call_variants_output.tfrecord.gz"" --outfile ""/scratch/xm41. /ct/bamsDown/30x/DEEPV/GRALF001.vcf.gz"" --nonvariant_site_tfrecord_path ""/tmp/tmp8vfbqj_y/gvcf.tfrecord@1.gz"" --gvcf_outfile ""/scratch/xm41/ct/bamsDown/30x/DEEPV/GRALF001.g.vcf.gz"". ```. ![image](https://user-images.githubusercontent.com/73748542/208531940-a0cbd86a-acb7-4ff4-9168-57c3b865625c.png). Based on other processes, the final process should not take that long:. ![image](https://user-images.githubusercontent.com/73748542/208532607-d0ae4fd2-38ac-48f9-889d-2d4343e60fe5.png). Is there a way to only re-initiate the postprocess_variants process? .",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/599
https://github.com/google/deepvariant/issues/599:398,deployability,Instal,Installation,398,"DeepVariant does not complete postprocess_variants; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**:. **Describe the issue:** DeepVariant does not complete postprocess_variants (feels like stuck; already waiting for completion for two days; other SAMPLES have already finished runninig). **Setup**. - Operating system: HPC. - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): Docker --> Singularity. - Type of data: WGS data. **Steps to reproduce:**. - Command: `run_deepvariant --model_type=WGS --ref=${FASTA} --reads=${BAMFILE} --output_vcf=${OUT_SAMPLE}.vcf.gz --output_gvcf=${OUT_SAMPLE}.g.vcf.gz`. - Error trace: (if applicable) . No error, but DeepVariant does not seem to continue processing:. ```. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""/home/rzei0002/xm41_scratch/hg38_resources/resources_broad_hg38_v0_Homo_sapiens_assembly38.fasta"" --infile ""/tmp/tmp8vfbqj_y/call_variants_output.tfrecord.gz"" --outfile ""/scratch/xm41. /ct/bamsDown/30x/DEEPV/GRALF001.vcf.gz"" --nonvariant_site_tfrecord_path ""/tmp/tmp8vfbqj_y/gvcf.tfrecord@1.gz"" --gvcf_outfile ""/scratch/xm41/ct/bamsDown/30x/DEEPV/GRALF001.g.vcf.gz"". ```. ![image](https://user-images.githubusercontent.com/73748542/208531940-a0cbd86a-acb7-4ff4-9168-57c3b865625c.png). Based on other processes, the final process should not take that long:. ![image](https://user-images.githubusercontent.com/73748542/208532607-d0ae4fd2-38ac-48f9-889d-2d4343e60fe5.png). Is there a way to only re-initiate the postprocess_variants process? .",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/599
https://github.com/google/deepvariant/issues/599:756,deployability,continu,continue,756,"DeepVariant does not complete postprocess_variants; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**:. **Describe the issue:** DeepVariant does not complete postprocess_variants (feels like stuck; already waiting for completion for two days; other SAMPLES have already finished runninig). **Setup**. - Operating system: HPC. - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): Docker --> Singularity. - Type of data: WGS data. **Steps to reproduce:**. - Command: `run_deepvariant --model_type=WGS --ref=${FASTA} --reads=${BAMFILE} --output_vcf=${OUT_SAMPLE}.vcf.gz --output_gvcf=${OUT_SAMPLE}.g.vcf.gz`. - Error trace: (if applicable) . No error, but DeepVariant does not seem to continue processing:. ```. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""/home/rzei0002/xm41_scratch/hg38_resources/resources_broad_hg38_v0_Homo_sapiens_assembly38.fasta"" --infile ""/tmp/tmp8vfbqj_y/call_variants_output.tfrecord.gz"" --outfile ""/scratch/xm41. /ct/bamsDown/30x/DEEPV/GRALF001.vcf.gz"" --nonvariant_site_tfrecord_path ""/tmp/tmp8vfbqj_y/gvcf.tfrecord@1.gz"" --gvcf_outfile ""/scratch/xm41/ct/bamsDown/30x/DEEPV/GRALF001.g.vcf.gz"". ```. ![image](https://user-images.githubusercontent.com/73748542/208531940-a0cbd86a-acb7-4ff4-9168-57c3b865625c.png). Based on other processes, the final process should not take that long:. ![image](https://user-images.githubusercontent.com/73748542/208532607-d0ae4fd2-38ac-48f9-889d-2d4343e60fe5.png). Is there a way to only re-initiate the postprocess_variants process? .",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/599
https://github.com/google/deepvariant/issues/599:380,integrability,version,version,380,"DeepVariant does not complete postprocess_variants; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**:. **Describe the issue:** DeepVariant does not complete postprocess_variants (feels like stuck; already waiting for completion for two days; other SAMPLES have already finished runninig). **Setup**. - Operating system: HPC. - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): Docker --> Singularity. - Type of data: WGS data. **Steps to reproduce:**. - Command: `run_deepvariant --model_type=WGS --ref=${FASTA} --reads=${BAMFILE} --output_vcf=${OUT_SAMPLE}.vcf.gz --output_gvcf=${OUT_SAMPLE}.g.vcf.gz`. - Error trace: (if applicable) . No error, but DeepVariant does not seem to continue processing:. ```. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""/home/rzei0002/xm41_scratch/hg38_resources/resources_broad_hg38_v0_Homo_sapiens_assembly38.fasta"" --infile ""/tmp/tmp8vfbqj_y/call_variants_output.tfrecord.gz"" --outfile ""/scratch/xm41. /ct/bamsDown/30x/DEEPV/GRALF001.vcf.gz"" --nonvariant_site_tfrecord_path ""/tmp/tmp8vfbqj_y/gvcf.tfrecord@1.gz"" --gvcf_outfile ""/scratch/xm41/ct/bamsDown/30x/DEEPV/GRALF001.g.vcf.gz"". ```. ![image](https://user-images.githubusercontent.com/73748542/208531940-a0cbd86a-acb7-4ff4-9168-57c3b865625c.png). Based on other processes, the final process should not take that long:. ![image](https://user-images.githubusercontent.com/73748542/208532607-d0ae4fd2-38ac-48f9-889d-2d4343e60fe5.png). Is there a way to only re-initiate the postprocess_variants process? .",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/599
https://github.com/google/deepvariant/issues/599:380,modifiability,version,version,380,"DeepVariant does not complete postprocess_variants; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**:. **Describe the issue:** DeepVariant does not complete postprocess_variants (feels like stuck; already waiting for completion for two days; other SAMPLES have already finished runninig). **Setup**. - Operating system: HPC. - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): Docker --> Singularity. - Type of data: WGS data. **Steps to reproduce:**. - Command: `run_deepvariant --model_type=WGS --ref=${FASTA} --reads=${BAMFILE} --output_vcf=${OUT_SAMPLE}.vcf.gz --output_gvcf=${OUT_SAMPLE}.g.vcf.gz`. - Error trace: (if applicable) . No error, but DeepVariant does not seem to continue processing:. ```. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""/home/rzei0002/xm41_scratch/hg38_resources/resources_broad_hg38_v0_Homo_sapiens_assembly38.fasta"" --infile ""/tmp/tmp8vfbqj_y/call_variants_output.tfrecord.gz"" --outfile ""/scratch/xm41. /ct/bamsDown/30x/DEEPV/GRALF001.vcf.gz"" --nonvariant_site_tfrecord_path ""/tmp/tmp8vfbqj_y/gvcf.tfrecord@1.gz"" --gvcf_outfile ""/scratch/xm41/ct/bamsDown/30x/DEEPV/GRALF001.g.vcf.gz"". ```. ![image](https://user-images.githubusercontent.com/73748542/208531940-a0cbd86a-acb7-4ff4-9168-57c3b865625c.png). Based on other processes, the final process should not take that long:. ![image](https://user-images.githubusercontent.com/73748542/208532607-d0ae4fd2-38ac-48f9-889d-2d4343e60fe5.png). Is there a way to only re-initiate the postprocess_variants process? .",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/599
https://github.com/google/deepvariant/issues/599:682,performance,Error,Error,682,"DeepVariant does not complete postprocess_variants; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**:. **Describe the issue:** DeepVariant does not complete postprocess_variants (feels like stuck; already waiting for completion for two days; other SAMPLES have already finished runninig). **Setup**. - Operating system: HPC. - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): Docker --> Singularity. - Type of data: WGS data. **Steps to reproduce:**. - Command: `run_deepvariant --model_type=WGS --ref=${FASTA} --reads=${BAMFILE} --output_vcf=${OUT_SAMPLE}.vcf.gz --output_gvcf=${OUT_SAMPLE}.g.vcf.gz`. - Error trace: (if applicable) . No error, but DeepVariant does not seem to continue processing:. ```. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""/home/rzei0002/xm41_scratch/hg38_resources/resources_broad_hg38_v0_Homo_sapiens_assembly38.fasta"" --infile ""/tmp/tmp8vfbqj_y/call_variants_output.tfrecord.gz"" --outfile ""/scratch/xm41. /ct/bamsDown/30x/DEEPV/GRALF001.vcf.gz"" --nonvariant_site_tfrecord_path ""/tmp/tmp8vfbqj_y/gvcf.tfrecord@1.gz"" --gvcf_outfile ""/scratch/xm41/ct/bamsDown/30x/DEEPV/GRALF001.g.vcf.gz"". ```. ![image](https://user-images.githubusercontent.com/73748542/208531940-a0cbd86a-acb7-4ff4-9168-57c3b865625c.png). Based on other processes, the final process should not take that long:. ![image](https://user-images.githubusercontent.com/73748542/208532607-d0ae4fd2-38ac-48f9-889d-2d4343e60fe5.png). Is there a way to only re-initiate the postprocess_variants process? .",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/599
https://github.com/google/deepvariant/issues/599:716,performance,error,error,716,"DeepVariant does not complete postprocess_variants; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**:. **Describe the issue:** DeepVariant does not complete postprocess_variants (feels like stuck; already waiting for completion for two days; other SAMPLES have already finished runninig). **Setup**. - Operating system: HPC. - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): Docker --> Singularity. - Type of data: WGS data. **Steps to reproduce:**. - Command: `run_deepvariant --model_type=WGS --ref=${FASTA} --reads=${BAMFILE} --output_vcf=${OUT_SAMPLE}.vcf.gz --output_gvcf=${OUT_SAMPLE}.g.vcf.gz`. - Error trace: (if applicable) . No error, but DeepVariant does not seem to continue processing:. ```. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""/home/rzei0002/xm41_scratch/hg38_resources/resources_broad_hg38_v0_Homo_sapiens_assembly38.fasta"" --infile ""/tmp/tmp8vfbqj_y/call_variants_output.tfrecord.gz"" --outfile ""/scratch/xm41. /ct/bamsDown/30x/DEEPV/GRALF001.vcf.gz"" --nonvariant_site_tfrecord_path ""/tmp/tmp8vfbqj_y/gvcf.tfrecord@1.gz"" --gvcf_outfile ""/scratch/xm41/ct/bamsDown/30x/DEEPV/GRALF001.g.vcf.gz"". ```. ![image](https://user-images.githubusercontent.com/73748542/208531940-a0cbd86a-acb7-4ff4-9168-57c3b865625c.png). Based on other processes, the final process should not take that long:. ![image](https://user-images.githubusercontent.com/73748542/208532607-d0ae4fd2-38ac-48f9-889d-2d4343e60fe5.png). Is there a way to only re-initiate the postprocess_variants process? .",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/599
https://github.com/google/deepvariant/issues/599:816,performance,time,time,816,"DeepVariant does not complete postprocess_variants; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**:. **Describe the issue:** DeepVariant does not complete postprocess_variants (feels like stuck; already waiting for completion for two days; other SAMPLES have already finished runninig). **Setup**. - Operating system: HPC. - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): Docker --> Singularity. - Type of data: WGS data. **Steps to reproduce:**. - Command: `run_deepvariant --model_type=WGS --ref=${FASTA} --reads=${BAMFILE} --output_vcf=${OUT_SAMPLE}.vcf.gz --output_gvcf=${OUT_SAMPLE}.g.vcf.gz`. - Error trace: (if applicable) . No error, but DeepVariant does not seem to continue processing:. ```. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""/home/rzei0002/xm41_scratch/hg38_resources/resources_broad_hg38_v0_Homo_sapiens_assembly38.fasta"" --infile ""/tmp/tmp8vfbqj_y/call_variants_output.tfrecord.gz"" --outfile ""/scratch/xm41. /ct/bamsDown/30x/DEEPV/GRALF001.vcf.gz"" --nonvariant_site_tfrecord_path ""/tmp/tmp8vfbqj_y/gvcf.tfrecord@1.gz"" --gvcf_outfile ""/scratch/xm41/ct/bamsDown/30x/DEEPV/GRALF001.g.vcf.gz"". ```. ![image](https://user-images.githubusercontent.com/73748542/208531940-a0cbd86a-acb7-4ff4-9168-57c3b865625c.png). Based on other processes, the final process should not take that long:. ![image](https://user-images.githubusercontent.com/73748542/208532607-d0ae4fd2-38ac-48f9-889d-2d4343e60fe5.png). Is there a way to only re-initiate the postprocess_variants process? .",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/599
https://github.com/google/deepvariant/issues/599:12,reliability,doe,does,12,"DeepVariant does not complete postprocess_variants; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**:. **Describe the issue:** DeepVariant does not complete postprocess_variants (feels like stuck; already waiting for completion for two days; other SAMPLES have already finished runninig). **Setup**. - Operating system: HPC. - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): Docker --> Singularity. - Type of data: WGS data. **Steps to reproduce:**. - Command: `run_deepvariant --model_type=WGS --ref=${FASTA} --reads=${BAMFILE} --output_vcf=${OUT_SAMPLE}.vcf.gz --output_gvcf=${OUT_SAMPLE}.g.vcf.gz`. - Error trace: (if applicable) . No error, but DeepVariant does not seem to continue processing:. ```. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""/home/rzei0002/xm41_scratch/hg38_resources/resources_broad_hg38_v0_Homo_sapiens_assembly38.fasta"" --infile ""/tmp/tmp8vfbqj_y/call_variants_output.tfrecord.gz"" --outfile ""/scratch/xm41. /ct/bamsDown/30x/DEEPV/GRALF001.vcf.gz"" --nonvariant_site_tfrecord_path ""/tmp/tmp8vfbqj_y/gvcf.tfrecord@1.gz"" --gvcf_outfile ""/scratch/xm41/ct/bamsDown/30x/DEEPV/GRALF001.g.vcf.gz"". ```. ![image](https://user-images.githubusercontent.com/73748542/208531940-a0cbd86a-acb7-4ff4-9168-57c3b865625c.png). Based on other processes, the final process should not take that long:. ![image](https://user-images.githubusercontent.com/73748542/208532607-d0ae4fd2-38ac-48f9-889d-2d4343e60fe5.png). Is there a way to only re-initiate the postprocess_variants process? .",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/599
https://github.com/google/deepvariant/issues/599:180,reliability,doe,does,180,"DeepVariant does not complete postprocess_variants; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**:. **Describe the issue:** DeepVariant does not complete postprocess_variants (feels like stuck; already waiting for completion for two days; other SAMPLES have already finished runninig). **Setup**. - Operating system: HPC. - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): Docker --> Singularity. - Type of data: WGS data. **Steps to reproduce:**. - Command: `run_deepvariant --model_type=WGS --ref=${FASTA} --reads=${BAMFILE} --output_vcf=${OUT_SAMPLE}.vcf.gz --output_gvcf=${OUT_SAMPLE}.g.vcf.gz`. - Error trace: (if applicable) . No error, but DeepVariant does not seem to continue processing:. ```. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""/home/rzei0002/xm41_scratch/hg38_resources/resources_broad_hg38_v0_Homo_sapiens_assembly38.fasta"" --infile ""/tmp/tmp8vfbqj_y/call_variants_output.tfrecord.gz"" --outfile ""/scratch/xm41. /ct/bamsDown/30x/DEEPV/GRALF001.vcf.gz"" --nonvariant_site_tfrecord_path ""/tmp/tmp8vfbqj_y/gvcf.tfrecord@1.gz"" --gvcf_outfile ""/scratch/xm41/ct/bamsDown/30x/DEEPV/GRALF001.g.vcf.gz"". ```. ![image](https://user-images.githubusercontent.com/73748542/208531940-a0cbd86a-acb7-4ff4-9168-57c3b865625c.png). Based on other processes, the final process should not take that long:. ![image](https://user-images.githubusercontent.com/73748542/208532607-d0ae4fd2-38ac-48f9-889d-2d4343e60fe5.png). Is there a way to only re-initiate the postprocess_variants process? .",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/599
https://github.com/google/deepvariant/issues/599:739,reliability,doe,does,739,"DeepVariant does not complete postprocess_variants; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**:. **Describe the issue:** DeepVariant does not complete postprocess_variants (feels like stuck; already waiting for completion for two days; other SAMPLES have already finished runninig). **Setup**. - Operating system: HPC. - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): Docker --> Singularity. - Type of data: WGS data. **Steps to reproduce:**. - Command: `run_deepvariant --model_type=WGS --ref=${FASTA} --reads=${BAMFILE} --output_vcf=${OUT_SAMPLE}.vcf.gz --output_gvcf=${OUT_SAMPLE}.g.vcf.gz`. - Error trace: (if applicable) . No error, but DeepVariant does not seem to continue processing:. ```. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""/home/rzei0002/xm41_scratch/hg38_resources/resources_broad_hg38_v0_Homo_sapiens_assembly38.fasta"" --infile ""/tmp/tmp8vfbqj_y/call_variants_output.tfrecord.gz"" --outfile ""/scratch/xm41. /ct/bamsDown/30x/DEEPV/GRALF001.vcf.gz"" --nonvariant_site_tfrecord_path ""/tmp/tmp8vfbqj_y/gvcf.tfrecord@1.gz"" --gvcf_outfile ""/scratch/xm41/ct/bamsDown/30x/DEEPV/GRALF001.g.vcf.gz"". ```. ![image](https://user-images.githubusercontent.com/73748542/208531940-a0cbd86a-acb7-4ff4-9168-57c3b865625c.png). Based on other processes, the final process should not take that long:. ![image](https://user-images.githubusercontent.com/73748542/208532607-d0ae4fd2-38ac-48f9-889d-2d4343e60fe5.png). Is there a way to only re-initiate the postprocess_variants process? .",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/599
https://github.com/google/deepvariant/issues/599:21,safety,compl,complete,21,"DeepVariant does not complete postprocess_variants; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**:. **Describe the issue:** DeepVariant does not complete postprocess_variants (feels like stuck; already waiting for completion for two days; other SAMPLES have already finished runninig). **Setup**. - Operating system: HPC. - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): Docker --> Singularity. - Type of data: WGS data. **Steps to reproduce:**. - Command: `run_deepvariant --model_type=WGS --ref=${FASTA} --reads=${BAMFILE} --output_vcf=${OUT_SAMPLE}.vcf.gz --output_gvcf=${OUT_SAMPLE}.g.vcf.gz`. - Error trace: (if applicable) . No error, but DeepVariant does not seem to continue processing:. ```. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""/home/rzei0002/xm41_scratch/hg38_resources/resources_broad_hg38_v0_Homo_sapiens_assembly38.fasta"" --infile ""/tmp/tmp8vfbqj_y/call_variants_output.tfrecord.gz"" --outfile ""/scratch/xm41. /ct/bamsDown/30x/DEEPV/GRALF001.vcf.gz"" --nonvariant_site_tfrecord_path ""/tmp/tmp8vfbqj_y/gvcf.tfrecord@1.gz"" --gvcf_outfile ""/scratch/xm41/ct/bamsDown/30x/DEEPV/GRALF001.g.vcf.gz"". ```. ![image](https://user-images.githubusercontent.com/73748542/208531940-a0cbd86a-acb7-4ff4-9168-57c3b865625c.png). Based on other processes, the final process should not take that long:. ![image](https://user-images.githubusercontent.com/73748542/208532607-d0ae4fd2-38ac-48f9-889d-2d4343e60fe5.png). Is there a way to only re-initiate the postprocess_variants process? .",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/599
https://github.com/google/deepvariant/issues/599:189,safety,compl,complete,189,"DeepVariant does not complete postprocess_variants; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**:. **Describe the issue:** DeepVariant does not complete postprocess_variants (feels like stuck; already waiting for completion for two days; other SAMPLES have already finished runninig). **Setup**. - Operating system: HPC. - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): Docker --> Singularity. - Type of data: WGS data. **Steps to reproduce:**. - Command: `run_deepvariant --model_type=WGS --ref=${FASTA} --reads=${BAMFILE} --output_vcf=${OUT_SAMPLE}.vcf.gz --output_gvcf=${OUT_SAMPLE}.g.vcf.gz`. - Error trace: (if applicable) . No error, but DeepVariant does not seem to continue processing:. ```. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""/home/rzei0002/xm41_scratch/hg38_resources/resources_broad_hg38_v0_Homo_sapiens_assembly38.fasta"" --infile ""/tmp/tmp8vfbqj_y/call_variants_output.tfrecord.gz"" --outfile ""/scratch/xm41. /ct/bamsDown/30x/DEEPV/GRALF001.vcf.gz"" --nonvariant_site_tfrecord_path ""/tmp/tmp8vfbqj_y/gvcf.tfrecord@1.gz"" --gvcf_outfile ""/scratch/xm41/ct/bamsDown/30x/DEEPV/GRALF001.g.vcf.gz"". ```. ![image](https://user-images.githubusercontent.com/73748542/208531940-a0cbd86a-acb7-4ff4-9168-57c3b865625c.png). Based on other processes, the final process should not take that long:. ![image](https://user-images.githubusercontent.com/73748542/208532607-d0ae4fd2-38ac-48f9-889d-2d4343e60fe5.png). Is there a way to only re-initiate the postprocess_variants process? .",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/599
https://github.com/google/deepvariant/issues/599:258,safety,compl,completion,258,"DeepVariant does not complete postprocess_variants; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**:. **Describe the issue:** DeepVariant does not complete postprocess_variants (feels like stuck; already waiting for completion for two days; other SAMPLES have already finished runninig). **Setup**. - Operating system: HPC. - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): Docker --> Singularity. - Type of data: WGS data. **Steps to reproduce:**. - Command: `run_deepvariant --model_type=WGS --ref=${FASTA} --reads=${BAMFILE} --output_vcf=${OUT_SAMPLE}.vcf.gz --output_gvcf=${OUT_SAMPLE}.g.vcf.gz`. - Error trace: (if applicable) . No error, but DeepVariant does not seem to continue processing:. ```. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""/home/rzei0002/xm41_scratch/hg38_resources/resources_broad_hg38_v0_Homo_sapiens_assembly38.fasta"" --infile ""/tmp/tmp8vfbqj_y/call_variants_output.tfrecord.gz"" --outfile ""/scratch/xm41. /ct/bamsDown/30x/DEEPV/GRALF001.vcf.gz"" --nonvariant_site_tfrecord_path ""/tmp/tmp8vfbqj_y/gvcf.tfrecord@1.gz"" --gvcf_outfile ""/scratch/xm41/ct/bamsDown/30x/DEEPV/GRALF001.g.vcf.gz"". ```. ![image](https://user-images.githubusercontent.com/73748542/208531940-a0cbd86a-acb7-4ff4-9168-57c3b865625c.png). Based on other processes, the final process should not take that long:. ![image](https://user-images.githubusercontent.com/73748542/208532607-d0ae4fd2-38ac-48f9-889d-2d4343e60fe5.png). Is there a way to only re-initiate the postprocess_variants process? .",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/599
https://github.com/google/deepvariant/issues/599:682,safety,Error,Error,682,"DeepVariant does not complete postprocess_variants; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**:. **Describe the issue:** DeepVariant does not complete postprocess_variants (feels like stuck; already waiting for completion for two days; other SAMPLES have already finished runninig). **Setup**. - Operating system: HPC. - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): Docker --> Singularity. - Type of data: WGS data. **Steps to reproduce:**. - Command: `run_deepvariant --model_type=WGS --ref=${FASTA} --reads=${BAMFILE} --output_vcf=${OUT_SAMPLE}.vcf.gz --output_gvcf=${OUT_SAMPLE}.g.vcf.gz`. - Error trace: (if applicable) . No error, but DeepVariant does not seem to continue processing:. ```. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""/home/rzei0002/xm41_scratch/hg38_resources/resources_broad_hg38_v0_Homo_sapiens_assembly38.fasta"" --infile ""/tmp/tmp8vfbqj_y/call_variants_output.tfrecord.gz"" --outfile ""/scratch/xm41. /ct/bamsDown/30x/DEEPV/GRALF001.vcf.gz"" --nonvariant_site_tfrecord_path ""/tmp/tmp8vfbqj_y/gvcf.tfrecord@1.gz"" --gvcf_outfile ""/scratch/xm41/ct/bamsDown/30x/DEEPV/GRALF001.g.vcf.gz"". ```. ![image](https://user-images.githubusercontent.com/73748542/208531940-a0cbd86a-acb7-4ff4-9168-57c3b865625c.png). Based on other processes, the final process should not take that long:. ![image](https://user-images.githubusercontent.com/73748542/208532607-d0ae4fd2-38ac-48f9-889d-2d4343e60fe5.png). Is there a way to only re-initiate the postprocess_variants process? .",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/599
https://github.com/google/deepvariant/issues/599:716,safety,error,error,716,"DeepVariant does not complete postprocess_variants; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**:. **Describe the issue:** DeepVariant does not complete postprocess_variants (feels like stuck; already waiting for completion for two days; other SAMPLES have already finished runninig). **Setup**. - Operating system: HPC. - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): Docker --> Singularity. - Type of data: WGS data. **Steps to reproduce:**. - Command: `run_deepvariant --model_type=WGS --ref=${FASTA} --reads=${BAMFILE} --output_vcf=${OUT_SAMPLE}.vcf.gz --output_gvcf=${OUT_SAMPLE}.g.vcf.gz`. - Error trace: (if applicable) . No error, but DeepVariant does not seem to continue processing:. ```. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""/home/rzei0002/xm41_scratch/hg38_resources/resources_broad_hg38_v0_Homo_sapiens_assembly38.fasta"" --infile ""/tmp/tmp8vfbqj_y/call_variants_output.tfrecord.gz"" --outfile ""/scratch/xm41. /ct/bamsDown/30x/DEEPV/GRALF001.vcf.gz"" --nonvariant_site_tfrecord_path ""/tmp/tmp8vfbqj_y/gvcf.tfrecord@1.gz"" --gvcf_outfile ""/scratch/xm41/ct/bamsDown/30x/DEEPV/GRALF001.g.vcf.gz"". ```. ![image](https://user-images.githubusercontent.com/73748542/208531940-a0cbd86a-acb7-4ff4-9168-57c3b865625c.png). Based on other processes, the final process should not take that long:. ![image](https://user-images.githubusercontent.com/73748542/208532607-d0ae4fd2-38ac-48f9-889d-2d4343e60fe5.png). Is there a way to only re-initiate the postprocess_variants process? .",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/599
https://github.com/google/deepvariant/issues/599:21,security,compl,complete,21,"DeepVariant does not complete postprocess_variants; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**:. **Describe the issue:** DeepVariant does not complete postprocess_variants (feels like stuck; already waiting for completion for two days; other SAMPLES have already finished runninig). **Setup**. - Operating system: HPC. - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): Docker --> Singularity. - Type of data: WGS data. **Steps to reproduce:**. - Command: `run_deepvariant --model_type=WGS --ref=${FASTA} --reads=${BAMFILE} --output_vcf=${OUT_SAMPLE}.vcf.gz --output_gvcf=${OUT_SAMPLE}.g.vcf.gz`. - Error trace: (if applicable) . No error, but DeepVariant does not seem to continue processing:. ```. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""/home/rzei0002/xm41_scratch/hg38_resources/resources_broad_hg38_v0_Homo_sapiens_assembly38.fasta"" --infile ""/tmp/tmp8vfbqj_y/call_variants_output.tfrecord.gz"" --outfile ""/scratch/xm41. /ct/bamsDown/30x/DEEPV/GRALF001.vcf.gz"" --nonvariant_site_tfrecord_path ""/tmp/tmp8vfbqj_y/gvcf.tfrecord@1.gz"" --gvcf_outfile ""/scratch/xm41/ct/bamsDown/30x/DEEPV/GRALF001.g.vcf.gz"". ```. ![image](https://user-images.githubusercontent.com/73748542/208531940-a0cbd86a-acb7-4ff4-9168-57c3b865625c.png). Based on other processes, the final process should not take that long:. ![image](https://user-images.githubusercontent.com/73748542/208532607-d0ae4fd2-38ac-48f9-889d-2d4343e60fe5.png). Is there a way to only re-initiate the postprocess_variants process? .",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/599
https://github.com/google/deepvariant/issues/599:189,security,compl,complete,189,"DeepVariant does not complete postprocess_variants; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**:. **Describe the issue:** DeepVariant does not complete postprocess_variants (feels like stuck; already waiting for completion for two days; other SAMPLES have already finished runninig). **Setup**. - Operating system: HPC. - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): Docker --> Singularity. - Type of data: WGS data. **Steps to reproduce:**. - Command: `run_deepvariant --model_type=WGS --ref=${FASTA} --reads=${BAMFILE} --output_vcf=${OUT_SAMPLE}.vcf.gz --output_gvcf=${OUT_SAMPLE}.g.vcf.gz`. - Error trace: (if applicable) . No error, but DeepVariant does not seem to continue processing:. ```. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""/home/rzei0002/xm41_scratch/hg38_resources/resources_broad_hg38_v0_Homo_sapiens_assembly38.fasta"" --infile ""/tmp/tmp8vfbqj_y/call_variants_output.tfrecord.gz"" --outfile ""/scratch/xm41. /ct/bamsDown/30x/DEEPV/GRALF001.vcf.gz"" --nonvariant_site_tfrecord_path ""/tmp/tmp8vfbqj_y/gvcf.tfrecord@1.gz"" --gvcf_outfile ""/scratch/xm41/ct/bamsDown/30x/DEEPV/GRALF001.g.vcf.gz"". ```. ![image](https://user-images.githubusercontent.com/73748542/208531940-a0cbd86a-acb7-4ff4-9168-57c3b865625c.png). Based on other processes, the final process should not take that long:. ![image](https://user-images.githubusercontent.com/73748542/208532607-d0ae4fd2-38ac-48f9-889d-2d4343e60fe5.png). Is there a way to only re-initiate the postprocess_variants process? .",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/599
https://github.com/google/deepvariant/issues/599:258,security,compl,completion,258,"DeepVariant does not complete postprocess_variants; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**:. **Describe the issue:** DeepVariant does not complete postprocess_variants (feels like stuck; already waiting for completion for two days; other SAMPLES have already finished runninig). **Setup**. - Operating system: HPC. - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): Docker --> Singularity. - Type of data: WGS data. **Steps to reproduce:**. - Command: `run_deepvariant --model_type=WGS --ref=${FASTA} --reads=${BAMFILE} --output_vcf=${OUT_SAMPLE}.vcf.gz --output_gvcf=${OUT_SAMPLE}.g.vcf.gz`. - Error trace: (if applicable) . No error, but DeepVariant does not seem to continue processing:. ```. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""/home/rzei0002/xm41_scratch/hg38_resources/resources_broad_hg38_v0_Homo_sapiens_assembly38.fasta"" --infile ""/tmp/tmp8vfbqj_y/call_variants_output.tfrecord.gz"" --outfile ""/scratch/xm41. /ct/bamsDown/30x/DEEPV/GRALF001.vcf.gz"" --nonvariant_site_tfrecord_path ""/tmp/tmp8vfbqj_y/gvcf.tfrecord@1.gz"" --gvcf_outfile ""/scratch/xm41/ct/bamsDown/30x/DEEPV/GRALF001.g.vcf.gz"". ```. ![image](https://user-images.githubusercontent.com/73748542/208531940-a0cbd86a-acb7-4ff4-9168-57c3b865625c.png). Based on other processes, the final process should not take that long:. ![image](https://user-images.githubusercontent.com/73748542/208532607-d0ae4fd2-38ac-48f9-889d-2d4343e60fe5.png). Is there a way to only re-initiate the postprocess_variants process? .",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/599
https://github.com/google/deepvariant/issues/599:688,testability,trace,trace,688,"DeepVariant does not complete postprocess_variants; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**:. **Describe the issue:** DeepVariant does not complete postprocess_variants (feels like stuck; already waiting for completion for two days; other SAMPLES have already finished runninig). **Setup**. - Operating system: HPC. - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): Docker --> Singularity. - Type of data: WGS data. **Steps to reproduce:**. - Command: `run_deepvariant --model_type=WGS --ref=${FASTA} --reads=${BAMFILE} --output_vcf=${OUT_SAMPLE}.vcf.gz --output_gvcf=${OUT_SAMPLE}.g.vcf.gz`. - Error trace: (if applicable) . No error, but DeepVariant does not seem to continue processing:. ```. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""/home/rzei0002/xm41_scratch/hg38_resources/resources_broad_hg38_v0_Homo_sapiens_assembly38.fasta"" --infile ""/tmp/tmp8vfbqj_y/call_variants_output.tfrecord.gz"" --outfile ""/scratch/xm41. /ct/bamsDown/30x/DEEPV/GRALF001.vcf.gz"" --nonvariant_site_tfrecord_path ""/tmp/tmp8vfbqj_y/gvcf.tfrecord@1.gz"" --gvcf_outfile ""/scratch/xm41/ct/bamsDown/30x/DEEPV/GRALF001.g.vcf.gz"". ```. ![image](https://user-images.githubusercontent.com/73748542/208531940-a0cbd86a-acb7-4ff4-9168-57c3b865625c.png). Based on other processes, the final process should not take that long:. ![image](https://user-images.githubusercontent.com/73748542/208532607-d0ae4fd2-38ac-48f9-889d-2d4343e60fe5.png). Is there a way to only re-initiate the postprocess_variants process? .",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/599
https://github.com/google/deepvariant/issues/599:530,usability,Command,Command,530,"DeepVariant does not complete postprocess_variants; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**:. **Describe the issue:** DeepVariant does not complete postprocess_variants (feels like stuck; already waiting for completion for two days; other SAMPLES have already finished runninig). **Setup**. - Operating system: HPC. - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): Docker --> Singularity. - Type of data: WGS data. **Steps to reproduce:**. - Command: `run_deepvariant --model_type=WGS --ref=${FASTA} --reads=${BAMFILE} --output_vcf=${OUT_SAMPLE}.vcf.gz --output_gvcf=${OUT_SAMPLE}.g.vcf.gz`. - Error trace: (if applicable) . No error, but DeepVariant does not seem to continue processing:. ```. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""/home/rzei0002/xm41_scratch/hg38_resources/resources_broad_hg38_v0_Homo_sapiens_assembly38.fasta"" --infile ""/tmp/tmp8vfbqj_y/call_variants_output.tfrecord.gz"" --outfile ""/scratch/xm41. /ct/bamsDown/30x/DEEPV/GRALF001.vcf.gz"" --nonvariant_site_tfrecord_path ""/tmp/tmp8vfbqj_y/gvcf.tfrecord@1.gz"" --gvcf_outfile ""/scratch/xm41/ct/bamsDown/30x/DEEPV/GRALF001.g.vcf.gz"". ```. ![image](https://user-images.githubusercontent.com/73748542/208531940-a0cbd86a-acb7-4ff4-9168-57c3b865625c.png). Based on other processes, the final process should not take that long:. ![image](https://user-images.githubusercontent.com/73748542/208532607-d0ae4fd2-38ac-48f9-889d-2d4343e60fe5.png). Is there a way to only re-initiate the postprocess_variants process? .",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/599
https://github.com/google/deepvariant/issues/599:682,usability,Error,Error,682,"DeepVariant does not complete postprocess_variants; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**:. **Describe the issue:** DeepVariant does not complete postprocess_variants (feels like stuck; already waiting for completion for two days; other SAMPLES have already finished runninig). **Setup**. - Operating system: HPC. - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): Docker --> Singularity. - Type of data: WGS data. **Steps to reproduce:**. - Command: `run_deepvariant --model_type=WGS --ref=${FASTA} --reads=${BAMFILE} --output_vcf=${OUT_SAMPLE}.vcf.gz --output_gvcf=${OUT_SAMPLE}.g.vcf.gz`. - Error trace: (if applicable) . No error, but DeepVariant does not seem to continue processing:. ```. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""/home/rzei0002/xm41_scratch/hg38_resources/resources_broad_hg38_v0_Homo_sapiens_assembly38.fasta"" --infile ""/tmp/tmp8vfbqj_y/call_variants_output.tfrecord.gz"" --outfile ""/scratch/xm41. /ct/bamsDown/30x/DEEPV/GRALF001.vcf.gz"" --nonvariant_site_tfrecord_path ""/tmp/tmp8vfbqj_y/gvcf.tfrecord@1.gz"" --gvcf_outfile ""/scratch/xm41/ct/bamsDown/30x/DEEPV/GRALF001.g.vcf.gz"". ```. ![image](https://user-images.githubusercontent.com/73748542/208531940-a0cbd86a-acb7-4ff4-9168-57c3b865625c.png). Based on other processes, the final process should not take that long:. ![image](https://user-images.githubusercontent.com/73748542/208532607-d0ae4fd2-38ac-48f9-889d-2d4343e60fe5.png). Is there a way to only re-initiate the postprocess_variants process? .",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/599
https://github.com/google/deepvariant/issues/599:716,usability,error,error,716,"DeepVariant does not complete postprocess_variants; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**:. **Describe the issue:** DeepVariant does not complete postprocess_variants (feels like stuck; already waiting for completion for two days; other SAMPLES have already finished runninig). **Setup**. - Operating system: HPC. - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): Docker --> Singularity. - Type of data: WGS data. **Steps to reproduce:**. - Command: `run_deepvariant --model_type=WGS --ref=${FASTA} --reads=${BAMFILE} --output_vcf=${OUT_SAMPLE}.vcf.gz --output_gvcf=${OUT_SAMPLE}.g.vcf.gz`. - Error trace: (if applicable) . No error, but DeepVariant does not seem to continue processing:. ```. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""/home/rzei0002/xm41_scratch/hg38_resources/resources_broad_hg38_v0_Homo_sapiens_assembly38.fasta"" --infile ""/tmp/tmp8vfbqj_y/call_variants_output.tfrecord.gz"" --outfile ""/scratch/xm41. /ct/bamsDown/30x/DEEPV/GRALF001.vcf.gz"" --nonvariant_site_tfrecord_path ""/tmp/tmp8vfbqj_y/gvcf.tfrecord@1.gz"" --gvcf_outfile ""/scratch/xm41/ct/bamsDown/30x/DEEPV/GRALF001.g.vcf.gz"". ```. ![image](https://user-images.githubusercontent.com/73748542/208531940-a0cbd86a-acb7-4ff4-9168-57c3b865625c.png). Based on other processes, the final process should not take that long:. ![image](https://user-images.githubusercontent.com/73748542/208532607-d0ae4fd2-38ac-48f9-889d-2d4343e60fe5.png). Is there a way to only re-initiate the postprocess_variants process? .",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/599
https://github.com/google/deepvariant/issues/599:801,usability,command,command,801,"DeepVariant does not complete postprocess_variants; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**:. **Describe the issue:** DeepVariant does not complete postprocess_variants (feels like stuck; already waiting for completion for two days; other SAMPLES have already finished runninig). **Setup**. - Operating system: HPC. - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): Docker --> Singularity. - Type of data: WGS data. **Steps to reproduce:**. - Command: `run_deepvariant --model_type=WGS --ref=${FASTA} --reads=${BAMFILE} --output_vcf=${OUT_SAMPLE}.vcf.gz --output_gvcf=${OUT_SAMPLE}.g.vcf.gz`. - Error trace: (if applicable) . No error, but DeepVariant does not seem to continue processing:. ```. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""/home/rzei0002/xm41_scratch/hg38_resources/resources_broad_hg38_v0_Homo_sapiens_assembly38.fasta"" --infile ""/tmp/tmp8vfbqj_y/call_variants_output.tfrecord.gz"" --outfile ""/scratch/xm41. /ct/bamsDown/30x/DEEPV/GRALF001.vcf.gz"" --nonvariant_site_tfrecord_path ""/tmp/tmp8vfbqj_y/gvcf.tfrecord@1.gz"" --gvcf_outfile ""/scratch/xm41/ct/bamsDown/30x/DEEPV/GRALF001.g.vcf.gz"". ```. ![image](https://user-images.githubusercontent.com/73748542/208531940-a0cbd86a-acb7-4ff4-9168-57c3b865625c.png). Based on other processes, the final process should not take that long:. ![image](https://user-images.githubusercontent.com/73748542/208532607-d0ae4fd2-38ac-48f9-889d-2d4343e60fe5.png). Is there a way to only re-initiate the postprocess_variants process? .",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/599
https://github.com/google/deepvariant/issues/599:1259,usability,user,user-images,1259,"DeepVariant does not complete postprocess_variants; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**:. **Describe the issue:** DeepVariant does not complete postprocess_variants (feels like stuck; already waiting for completion for two days; other SAMPLES have already finished runninig). **Setup**. - Operating system: HPC. - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): Docker --> Singularity. - Type of data: WGS data. **Steps to reproduce:**. - Command: `run_deepvariant --model_type=WGS --ref=${FASTA} --reads=${BAMFILE} --output_vcf=${OUT_SAMPLE}.vcf.gz --output_gvcf=${OUT_SAMPLE}.g.vcf.gz`. - Error trace: (if applicable) . No error, but DeepVariant does not seem to continue processing:. ```. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""/home/rzei0002/xm41_scratch/hg38_resources/resources_broad_hg38_v0_Homo_sapiens_assembly38.fasta"" --infile ""/tmp/tmp8vfbqj_y/call_variants_output.tfrecord.gz"" --outfile ""/scratch/xm41. /ct/bamsDown/30x/DEEPV/GRALF001.vcf.gz"" --nonvariant_site_tfrecord_path ""/tmp/tmp8vfbqj_y/gvcf.tfrecord@1.gz"" --gvcf_outfile ""/scratch/xm41/ct/bamsDown/30x/DEEPV/GRALF001.g.vcf.gz"". ```. ![image](https://user-images.githubusercontent.com/73748542/208531940-a0cbd86a-acb7-4ff4-9168-57c3b865625c.png). Based on other processes, the final process should not take that long:. ![image](https://user-images.githubusercontent.com/73748542/208532607-d0ae4fd2-38ac-48f9-889d-2d4343e60fe5.png). Is there a way to only re-initiate the postprocess_variants process? .",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/599
https://github.com/google/deepvariant/issues/599:1444,usability,user,user-images,1444,"DeepVariant does not complete postprocess_variants; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**:. **Describe the issue:** DeepVariant does not complete postprocess_variants (feels like stuck; already waiting for completion for two days; other SAMPLES have already finished runninig). **Setup**. - Operating system: HPC. - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): Docker --> Singularity. - Type of data: WGS data. **Steps to reproduce:**. - Command: `run_deepvariant --model_type=WGS --ref=${FASTA} --reads=${BAMFILE} --output_vcf=${OUT_SAMPLE}.vcf.gz --output_gvcf=${OUT_SAMPLE}.g.vcf.gz`. - Error trace: (if applicable) . No error, but DeepVariant does not seem to continue processing:. ```. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""/home/rzei0002/xm41_scratch/hg38_resources/resources_broad_hg38_v0_Homo_sapiens_assembly38.fasta"" --infile ""/tmp/tmp8vfbqj_y/call_variants_output.tfrecord.gz"" --outfile ""/scratch/xm41. /ct/bamsDown/30x/DEEPV/GRALF001.vcf.gz"" --nonvariant_site_tfrecord_path ""/tmp/tmp8vfbqj_y/gvcf.tfrecord@1.gz"" --gvcf_outfile ""/scratch/xm41/ct/bamsDown/30x/DEEPV/GRALF001.g.vcf.gz"". ```. ![image](https://user-images.githubusercontent.com/73748542/208531940-a0cbd86a-acb7-4ff4-9168-57c3b865625c.png). Based on other processes, the final process should not take that long:. ![image](https://user-images.githubusercontent.com/73748542/208532607-d0ae4fd2-38ac-48f9-889d-2d4343e60fe5.png). Is there a way to only re-initiate the postprocess_variants process? .",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/599
https://github.com/google/deepvariant/issues/600:212,integrability,buffer,buffer,212,"RNA seq file take too much time !; Until now it takes 60 hours !! . That is the output on terminal, until now it runs , why it is like that? . * Running the command:* . time seq 0 7 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""Homo_sapiens_assembly38.fasta"" --reads ""SRR10720227.RGG.marked.sorted.bam"" --examples ""/tmp/tmpe2z79_52/make_examples.tfrecord@8.gz"" --split_skip_reads --task {} . . I1219 19:34:53.123960 140628390905664 genomics_reader.py:222] Reading SRR10720227.RGG.marked.sorted.bam with NativeSamReader . I1219 19:34:53.243814 140628390905664 make_examples_core.py:236] Task 7/8: Preparing inputs . I1219 19:34:53.094258 139794990966592 genomics_reader.py:222] Reading SRR10720227.RGG.marked.sorted.bam with NativeSamReader . I1219 19:34:53.115872 139794990966592 make_examples_core.py:236] Task 1/8: Preparing inputs . I1219 19:34:53.235279 139794990966592 genomics_reader.py:222] Reading SRR10720227.RGG.marked.sorted.bam with NativeSamReader . I1219 19:34:53.134214 140309844559680 genomics_reader.py:222] Reading SRR10720227.RGG.marked.sorted.bam with NativeSamReader . I1219 19:34:53.192481 140309844559680 make_examples_core.py:236] Task 6/8: Preparing inputs . I1219 19:34:53.107017 140536106870592 genomics_reader.py:222] Reading SRR10720227.RGG.marked.sorted.bam with NativeSamReader . I1219 19:34:53.122070 140536106870592 make_examples_core.py:236] Task 4/8: Preparing inputs . I1219 19:34:53.226204 140536106870592 genomics_reader.py:222] Reading SRR10720227.RGG.marked.sorted.bam with NativeSamReader . I1219 19:34:53.129359 140434427647808 genomics_reader.py:222] Reading SRR10720227.RGG.marked.sorted.bam with NativeSamReader . I1219 19:34:53.162234 140434427647808 make_examples_core.py:236] Task 5/8: Preparing inputs . I1219 19:34:53.209864 140434427647808 genomics_reader.py:222] Reading SRR10720227.RGG.marked.sorted.bam with NativeSamReader . I1219 19:34:53.105211 139854300972864 genomics_reader.py:222] Reading SR",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/600
https://github.com/google/deepvariant/issues/600:27,performance,time,time,27,"RNA seq file take too much time !; Until now it takes 60 hours !! . That is the output on terminal, until now it runs , why it is like that? . * Running the command:* . time seq 0 7 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""Homo_sapiens_assembly38.fasta"" --reads ""SRR10720227.RGG.marked.sorted.bam"" --examples ""/tmp/tmpe2z79_52/make_examples.tfrecord@8.gz"" --split_skip_reads --task {} . . I1219 19:34:53.123960 140628390905664 genomics_reader.py:222] Reading SRR10720227.RGG.marked.sorted.bam with NativeSamReader . I1219 19:34:53.243814 140628390905664 make_examples_core.py:236] Task 7/8: Preparing inputs . I1219 19:34:53.094258 139794990966592 genomics_reader.py:222] Reading SRR10720227.RGG.marked.sorted.bam with NativeSamReader . I1219 19:34:53.115872 139794990966592 make_examples_core.py:236] Task 1/8: Preparing inputs . I1219 19:34:53.235279 139794990966592 genomics_reader.py:222] Reading SRR10720227.RGG.marked.sorted.bam with NativeSamReader . I1219 19:34:53.134214 140309844559680 genomics_reader.py:222] Reading SRR10720227.RGG.marked.sorted.bam with NativeSamReader . I1219 19:34:53.192481 140309844559680 make_examples_core.py:236] Task 6/8: Preparing inputs . I1219 19:34:53.107017 140536106870592 genomics_reader.py:222] Reading SRR10720227.RGG.marked.sorted.bam with NativeSamReader . I1219 19:34:53.122070 140536106870592 make_examples_core.py:236] Task 4/8: Preparing inputs . I1219 19:34:53.226204 140536106870592 genomics_reader.py:222] Reading SRR10720227.RGG.marked.sorted.bam with NativeSamReader . I1219 19:34:53.129359 140434427647808 genomics_reader.py:222] Reading SRR10720227.RGG.marked.sorted.bam with NativeSamReader . I1219 19:34:53.162234 140434427647808 make_examples_core.py:236] Task 5/8: Preparing inputs . I1219 19:34:53.209864 140434427647808 genomics_reader.py:222] Reading SRR10720227.RGG.marked.sorted.bam with NativeSamReader . I1219 19:34:53.105211 139854300972864 genomics_reader.py:222] Reading SR",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/600
https://github.com/google/deepvariant/issues/600:169,performance,time,time,169,"RNA seq file take too much time !; Until now it takes 60 hours !! . That is the output on terminal, until now it runs , why it is like that? . * Running the command:* . time seq 0 7 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""Homo_sapiens_assembly38.fasta"" --reads ""SRR10720227.RGG.marked.sorted.bam"" --examples ""/tmp/tmpe2z79_52/make_examples.tfrecord@8.gz"" --split_skip_reads --task {} . . I1219 19:34:53.123960 140628390905664 genomics_reader.py:222] Reading SRR10720227.RGG.marked.sorted.bam with NativeSamReader . I1219 19:34:53.243814 140628390905664 make_examples_core.py:236] Task 7/8: Preparing inputs . I1219 19:34:53.094258 139794990966592 genomics_reader.py:222] Reading SRR10720227.RGG.marked.sorted.bam with NativeSamReader . I1219 19:34:53.115872 139794990966592 make_examples_core.py:236] Task 1/8: Preparing inputs . I1219 19:34:53.235279 139794990966592 genomics_reader.py:222] Reading SRR10720227.RGG.marked.sorted.bam with NativeSamReader . I1219 19:34:53.134214 140309844559680 genomics_reader.py:222] Reading SRR10720227.RGG.marked.sorted.bam with NativeSamReader . I1219 19:34:53.192481 140309844559680 make_examples_core.py:236] Task 6/8: Preparing inputs . I1219 19:34:53.107017 140536106870592 genomics_reader.py:222] Reading SRR10720227.RGG.marked.sorted.bam with NativeSamReader . I1219 19:34:53.122070 140536106870592 make_examples_core.py:236] Task 4/8: Preparing inputs . I1219 19:34:53.226204 140536106870592 genomics_reader.py:222] Reading SRR10720227.RGG.marked.sorted.bam with NativeSamReader . I1219 19:34:53.129359 140434427647808 genomics_reader.py:222] Reading SRR10720227.RGG.marked.sorted.bam with NativeSamReader . I1219 19:34:53.162234 140434427647808 make_examples_core.py:236] Task 5/8: Preparing inputs . I1219 19:34:53.209864 140434427647808 genomics_reader.py:222] Reading SRR10720227.RGG.marked.sorted.bam with NativeSamReader . I1219 19:34:53.105211 139854300972864 genomics_reader.py:222] Reading SR",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/600
https://github.com/google/deepvariant/issues/600:184,performance,parallel,parallel,184,"RNA seq file take too much time !; Until now it takes 60 hours !! . That is the output on terminal, until now it runs , why it is like that? . * Running the command:* . time seq 0 7 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""Homo_sapiens_assembly38.fasta"" --reads ""SRR10720227.RGG.marked.sorted.bam"" --examples ""/tmp/tmpe2z79_52/make_examples.tfrecord@8.gz"" --split_skip_reads --task {} . . I1219 19:34:53.123960 140628390905664 genomics_reader.py:222] Reading SRR10720227.RGG.marked.sorted.bam with NativeSamReader . I1219 19:34:53.243814 140628390905664 make_examples_core.py:236] Task 7/8: Preparing inputs . I1219 19:34:53.094258 139794990966592 genomics_reader.py:222] Reading SRR10720227.RGG.marked.sorted.bam with NativeSamReader . I1219 19:34:53.115872 139794990966592 make_examples_core.py:236] Task 1/8: Preparing inputs . I1219 19:34:53.235279 139794990966592 genomics_reader.py:222] Reading SRR10720227.RGG.marked.sorted.bam with NativeSamReader . I1219 19:34:53.134214 140309844559680 genomics_reader.py:222] Reading SRR10720227.RGG.marked.sorted.bam with NativeSamReader . I1219 19:34:53.192481 140309844559680 make_examples_core.py:236] Task 6/8: Preparing inputs . I1219 19:34:53.107017 140536106870592 genomics_reader.py:222] Reading SRR10720227.RGG.marked.sorted.bam with NativeSamReader . I1219 19:34:53.122070 140536106870592 make_examples_core.py:236] Task 4/8: Preparing inputs . I1219 19:34:53.226204 140536106870592 genomics_reader.py:222] Reading SRR10720227.RGG.marked.sorted.bam with NativeSamReader . I1219 19:34:53.129359 140434427647808 genomics_reader.py:222] Reading SRR10720227.RGG.marked.sorted.bam with NativeSamReader . I1219 19:34:53.162234 140434427647808 make_examples_core.py:236] Task 5/8: Preparing inputs . I1219 19:34:53.209864 140434427647808 genomics_reader.py:222] Reading SRR10720227.RGG.marked.sorted.bam with NativeSamReader . I1219 19:34:53.105211 139854300972864 genomics_reader.py:222] Reading SR",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/600
https://github.com/google/deepvariant/issues/600:654,safety,input,inputs,654,"RNA seq file take too much time !; Until now it takes 60 hours !! . That is the output on terminal, until now it runs , why it is like that? . * Running the command:* . time seq 0 7 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""Homo_sapiens_assembly38.fasta"" --reads ""SRR10720227.RGG.marked.sorted.bam"" --examples ""/tmp/tmpe2z79_52/make_examples.tfrecord@8.gz"" --split_skip_reads --task {} . . I1219 19:34:53.123960 140628390905664 genomics_reader.py:222] Reading SRR10720227.RGG.marked.sorted.bam with NativeSamReader . I1219 19:34:53.243814 140628390905664 make_examples_core.py:236] Task 7/8: Preparing inputs . I1219 19:34:53.094258 139794990966592 genomics_reader.py:222] Reading SRR10720227.RGG.marked.sorted.bam with NativeSamReader . I1219 19:34:53.115872 139794990966592 make_examples_core.py:236] Task 1/8: Preparing inputs . I1219 19:34:53.235279 139794990966592 genomics_reader.py:222] Reading SRR10720227.RGG.marked.sorted.bam with NativeSamReader . I1219 19:34:53.134214 140309844559680 genomics_reader.py:222] Reading SRR10720227.RGG.marked.sorted.bam with NativeSamReader . I1219 19:34:53.192481 140309844559680 make_examples_core.py:236] Task 6/8: Preparing inputs . I1219 19:34:53.107017 140536106870592 genomics_reader.py:222] Reading SRR10720227.RGG.marked.sorted.bam with NativeSamReader . I1219 19:34:53.122070 140536106870592 make_examples_core.py:236] Task 4/8: Preparing inputs . I1219 19:34:53.226204 140536106870592 genomics_reader.py:222] Reading SRR10720227.RGG.marked.sorted.bam with NativeSamReader . I1219 19:34:53.129359 140434427647808 genomics_reader.py:222] Reading SRR10720227.RGG.marked.sorted.bam with NativeSamReader . I1219 19:34:53.162234 140434427647808 make_examples_core.py:236] Task 5/8: Preparing inputs . I1219 19:34:53.209864 140434427647808 genomics_reader.py:222] Reading SRR10720227.RGG.marked.sorted.bam with NativeSamReader . I1219 19:34:53.105211 139854300972864 genomics_reader.py:222] Reading SR",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/600
https://github.com/google/deepvariant/issues/600:875,safety,input,inputs,875,"RNA seq file take too much time !; Until now it takes 60 hours !! . That is the output on terminal, until now it runs , why it is like that? . * Running the command:* . time seq 0 7 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""Homo_sapiens_assembly38.fasta"" --reads ""SRR10720227.RGG.marked.sorted.bam"" --examples ""/tmp/tmpe2z79_52/make_examples.tfrecord@8.gz"" --split_skip_reads --task {} . . I1219 19:34:53.123960 140628390905664 genomics_reader.py:222] Reading SRR10720227.RGG.marked.sorted.bam with NativeSamReader . I1219 19:34:53.243814 140628390905664 make_examples_core.py:236] Task 7/8: Preparing inputs . I1219 19:34:53.094258 139794990966592 genomics_reader.py:222] Reading SRR10720227.RGG.marked.sorted.bam with NativeSamReader . I1219 19:34:53.115872 139794990966592 make_examples_core.py:236] Task 1/8: Preparing inputs . I1219 19:34:53.235279 139794990966592 genomics_reader.py:222] Reading SRR10720227.RGG.marked.sorted.bam with NativeSamReader . I1219 19:34:53.134214 140309844559680 genomics_reader.py:222] Reading SRR10720227.RGG.marked.sorted.bam with NativeSamReader . I1219 19:34:53.192481 140309844559680 make_examples_core.py:236] Task 6/8: Preparing inputs . I1219 19:34:53.107017 140536106870592 genomics_reader.py:222] Reading SRR10720227.RGG.marked.sorted.bam with NativeSamReader . I1219 19:34:53.122070 140536106870592 make_examples_core.py:236] Task 4/8: Preparing inputs . I1219 19:34:53.226204 140536106870592 genomics_reader.py:222] Reading SRR10720227.RGG.marked.sorted.bam with NativeSamReader . I1219 19:34:53.129359 140434427647808 genomics_reader.py:222] Reading SRR10720227.RGG.marked.sorted.bam with NativeSamReader . I1219 19:34:53.162234 140434427647808 make_examples_core.py:236] Task 5/8: Preparing inputs . I1219 19:34:53.209864 140434427647808 genomics_reader.py:222] Reading SRR10720227.RGG.marked.sorted.bam with NativeSamReader . I1219 19:34:53.105211 139854300972864 genomics_reader.py:222] Reading SR",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/600
https://github.com/google/deepvariant/issues/600:1223,safety,input,inputs,1223,"epvariant/bin/make_examples --mode calling --ref ""Homo_sapiens_assembly38.fasta"" --reads ""SRR10720227.RGG.marked.sorted.bam"" --examples ""/tmp/tmpe2z79_52/make_examples.tfrecord@8.gz"" --split_skip_reads --task {} . . I1219 19:34:53.123960 140628390905664 genomics_reader.py:222] Reading SRR10720227.RGG.marked.sorted.bam with NativeSamReader . I1219 19:34:53.243814 140628390905664 make_examples_core.py:236] Task 7/8: Preparing inputs . I1219 19:34:53.094258 139794990966592 genomics_reader.py:222] Reading SRR10720227.RGG.marked.sorted.bam with NativeSamReader . I1219 19:34:53.115872 139794990966592 make_examples_core.py:236] Task 1/8: Preparing inputs . I1219 19:34:53.235279 139794990966592 genomics_reader.py:222] Reading SRR10720227.RGG.marked.sorted.bam with NativeSamReader . I1219 19:34:53.134214 140309844559680 genomics_reader.py:222] Reading SRR10720227.RGG.marked.sorted.bam with NativeSamReader . I1219 19:34:53.192481 140309844559680 make_examples_core.py:236] Task 6/8: Preparing inputs . I1219 19:34:53.107017 140536106870592 genomics_reader.py:222] Reading SRR10720227.RGG.marked.sorted.bam with NativeSamReader . I1219 19:34:53.122070 140536106870592 make_examples_core.py:236] Task 4/8: Preparing inputs . I1219 19:34:53.226204 140536106870592 genomics_reader.py:222] Reading SRR10720227.RGG.marked.sorted.bam with NativeSamReader . I1219 19:34:53.129359 140434427647808 genomics_reader.py:222] Reading SRR10720227.RGG.marked.sorted.bam with NativeSamReader . I1219 19:34:53.162234 140434427647808 make_examples_core.py:236] Task 5/8: Preparing inputs . I1219 19:34:53.209864 140434427647808 genomics_reader.py:222] Reading SRR10720227.RGG.marked.sorted.bam with NativeSamReader . I1219 19:34:53.105211 139854300972864 genomics_reader.py:222] Reading SRR10720227.RGG.marked.sorted.bam with NativeSamReader . I1219 19:34:53.131687 139854300972864 make_examples_core.py:236] Task 3/8: Preparing inputs . I1219 19:34:53.201329 139691329734464 genomics_reader.py:222] Reading SRR1072",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/600
https://github.com/google/deepvariant/issues/600:1444,safety,input,inputs,1444, 19:34:53.123960 140628390905664 genomics_reader.py:222] Reading SRR10720227.RGG.marked.sorted.bam with NativeSamReader . I1219 19:34:53.243814 140628390905664 make_examples_core.py:236] Task 7/8: Preparing inputs . I1219 19:34:53.094258 139794990966592 genomics_reader.py:222] Reading SRR10720227.RGG.marked.sorted.bam with NativeSamReader . I1219 19:34:53.115872 139794990966592 make_examples_core.py:236] Task 1/8: Preparing inputs . I1219 19:34:53.235279 139794990966592 genomics_reader.py:222] Reading SRR10720227.RGG.marked.sorted.bam with NativeSamReader . I1219 19:34:53.134214 140309844559680 genomics_reader.py:222] Reading SRR10720227.RGG.marked.sorted.bam with NativeSamReader . I1219 19:34:53.192481 140309844559680 make_examples_core.py:236] Task 6/8: Preparing inputs . I1219 19:34:53.107017 140536106870592 genomics_reader.py:222] Reading SRR10720227.RGG.marked.sorted.bam with NativeSamReader . I1219 19:34:53.122070 140536106870592 make_examples_core.py:236] Task 4/8: Preparing inputs . I1219 19:34:53.226204 140536106870592 genomics_reader.py:222] Reading SRR10720227.RGG.marked.sorted.bam with NativeSamReader . I1219 19:34:53.129359 140434427647808 genomics_reader.py:222] Reading SRR10720227.RGG.marked.sorted.bam with NativeSamReader . I1219 19:34:53.162234 140434427647808 make_examples_core.py:236] Task 5/8: Preparing inputs . I1219 19:34:53.209864 140434427647808 genomics_reader.py:222] Reading SRR10720227.RGG.marked.sorted.bam with NativeSamReader . I1219 19:34:53.105211 139854300972864 genomics_reader.py:222] Reading SRR10720227.RGG.marked.sorted.bam with NativeSamReader . I1219 19:34:53.131687 139854300972864 make_examples_core.py:236] Task 3/8: Preparing inputs . I1219 19:34:53.201329 139691329734464 genomics_reader.py:222] Reading SRR10720227.RGG.marked.sorted.bam with NativeSamReader . I1219 19:34:53.210919 139691329734464 make_examples_core.py:236] Task 2/8: Preparing inputs . I1219 19:34:53.269506 139691329734464 genomics_reader.py:222] Reading SRR1072,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/600
https://github.com/google/deepvariant/issues/600:1792,safety,input,inputs,1792, 19:34:53.115872 139794990966592 make_examples_core.py:236] Task 1/8: Preparing inputs . I1219 19:34:53.235279 139794990966592 genomics_reader.py:222] Reading SRR10720227.RGG.marked.sorted.bam with NativeSamReader . I1219 19:34:53.134214 140309844559680 genomics_reader.py:222] Reading SRR10720227.RGG.marked.sorted.bam with NativeSamReader . I1219 19:34:53.192481 140309844559680 make_examples_core.py:236] Task 6/8: Preparing inputs . I1219 19:34:53.107017 140536106870592 genomics_reader.py:222] Reading SRR10720227.RGG.marked.sorted.bam with NativeSamReader . I1219 19:34:53.122070 140536106870592 make_examples_core.py:236] Task 4/8: Preparing inputs . I1219 19:34:53.226204 140536106870592 genomics_reader.py:222] Reading SRR10720227.RGG.marked.sorted.bam with NativeSamReader . I1219 19:34:53.129359 140434427647808 genomics_reader.py:222] Reading SRR10720227.RGG.marked.sorted.bam with NativeSamReader . I1219 19:34:53.162234 140434427647808 make_examples_core.py:236] Task 5/8: Preparing inputs . I1219 19:34:53.209864 140434427647808 genomics_reader.py:222] Reading SRR10720227.RGG.marked.sorted.bam with NativeSamReader . I1219 19:34:53.105211 139854300972864 genomics_reader.py:222] Reading SRR10720227.RGG.marked.sorted.bam with NativeSamReader . I1219 19:34:53.131687 139854300972864 make_examples_core.py:236] Task 3/8: Preparing inputs . I1219 19:34:53.201329 139691329734464 genomics_reader.py:222] Reading SRR10720227.RGG.marked.sorted.bam with NativeSamReader . I1219 19:34:53.210919 139691329734464 make_examples_core.py:236] Task 2/8: Preparing inputs . I1219 19:34:53.269506 139691329734464 genomics_reader.py:222] Reading SRR10720227.RGG.marked.sorted.bam with NativeSamReader . I1219 19:34:53.152369 140549443307328 genomics_reader.py:222] Reading SRR10720227.RGG.marked.sorted.bam with NativeSamReader . I1219 19:34:53.174761 140549443307328 make_examples_core.py:236] Task 0/8: Preparing inputs . I1219 19:34:53.282668 140549443307328 genomics_reader.py:222] Reading SRR1072,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/600
https://github.com/google/deepvariant/issues/600:2140,safety,input,inputs,2140, 19:34:53.192481 140309844559680 make_examples_core.py:236] Task 6/8: Preparing inputs . I1219 19:34:53.107017 140536106870592 genomics_reader.py:222] Reading SRR10720227.RGG.marked.sorted.bam with NativeSamReader . I1219 19:34:53.122070 140536106870592 make_examples_core.py:236] Task 4/8: Preparing inputs . I1219 19:34:53.226204 140536106870592 genomics_reader.py:222] Reading SRR10720227.RGG.marked.sorted.bam with NativeSamReader . I1219 19:34:53.129359 140434427647808 genomics_reader.py:222] Reading SRR10720227.RGG.marked.sorted.bam with NativeSamReader . I1219 19:34:53.162234 140434427647808 make_examples_core.py:236] Task 5/8: Preparing inputs . I1219 19:34:53.209864 140434427647808 genomics_reader.py:222] Reading SRR10720227.RGG.marked.sorted.bam with NativeSamReader . I1219 19:34:53.105211 139854300972864 genomics_reader.py:222] Reading SRR10720227.RGG.marked.sorted.bam with NativeSamReader . I1219 19:34:53.131687 139854300972864 make_examples_core.py:236] Task 3/8: Preparing inputs . I1219 19:34:53.201329 139691329734464 genomics_reader.py:222] Reading SRR10720227.RGG.marked.sorted.bam with NativeSamReader . I1219 19:34:53.210919 139691329734464 make_examples_core.py:236] Task 2/8: Preparing inputs . I1219 19:34:53.269506 139691329734464 genomics_reader.py:222] Reading SRR10720227.RGG.marked.sorted.bam with NativeSamReader . I1219 19:34:53.152369 140549443307328 genomics_reader.py:222] Reading SRR10720227.RGG.marked.sorted.bam with NativeSamReader . I1219 19:34:53.174761 140549443307328 make_examples_core.py:236] Task 0/8: Preparing inputs . I1219 19:34:53.282668 140549443307328 genomics_reader.py:222] Reading SRR10720227.RGG.marked.sorted.bam with NativeSamReader . I1219 19:34:53.454261 140628390905664 genomics_reader.py:222] Reading SRR10720227.RGG.marked.sorted.bam with NativeSamReader . I1219 19:34:53.366882 140309844559680 genomics_reader.py:222] Reading SRR10720227.RGG.marked.sorted.bam with NativeSamReader . I1219 19:34:53.342017 139854300972864 genomi,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/600
https://github.com/google/deepvariant/issues/600:2361,safety,input,inputs,2361," 19:34:53.122070 140536106870592 make_examples_core.py:236] Task 4/8: Preparing inputs . I1219 19:34:53.226204 140536106870592 genomics_reader.py:222] Reading SRR10720227.RGG.marked.sorted.bam with NativeSamReader . I1219 19:34:53.129359 140434427647808 genomics_reader.py:222] Reading SRR10720227.RGG.marked.sorted.bam with NativeSamReader . I1219 19:34:53.162234 140434427647808 make_examples_core.py:236] Task 5/8: Preparing inputs . I1219 19:34:53.209864 140434427647808 genomics_reader.py:222] Reading SRR10720227.RGG.marked.sorted.bam with NativeSamReader . I1219 19:34:53.105211 139854300972864 genomics_reader.py:222] Reading SRR10720227.RGG.marked.sorted.bam with NativeSamReader . I1219 19:34:53.131687 139854300972864 make_examples_core.py:236] Task 3/8: Preparing inputs . I1219 19:34:53.201329 139691329734464 genomics_reader.py:222] Reading SRR10720227.RGG.marked.sorted.bam with NativeSamReader . I1219 19:34:53.210919 139691329734464 make_examples_core.py:236] Task 2/8: Preparing inputs . I1219 19:34:53.269506 139691329734464 genomics_reader.py:222] Reading SRR10720227.RGG.marked.sorted.bam with NativeSamReader . I1219 19:34:53.152369 140549443307328 genomics_reader.py:222] Reading SRR10720227.RGG.marked.sorted.bam with NativeSamReader . I1219 19:34:53.174761 140549443307328 make_examples_core.py:236] Task 0/8: Preparing inputs . I1219 19:34:53.282668 140549443307328 genomics_reader.py:222] Reading SRR10720227.RGG.marked.sorted.bam with NativeSamReader . I1219 19:34:53.454261 140628390905664 genomics_reader.py:222] Reading SRR10720227.RGG.marked.sorted.bam with NativeSamReader . I1219 19:34:53.366882 140309844559680 genomics_reader.py:222] Reading SRR10720227.RGG.marked.sorted.bam with NativeSamReader . I1219 19:34:53.342017 139854300972864 genomics_reader.py:222] Reading SRR10720227.RGG.marked.sorted.bam with NativeSamReader . I1219 19:34:54.719705 140628390905664 make_examples_core.py:236] Task 7/8: Common contigs are ['chr1', 'chr2', 'chr3', 'chr4', 'chr5', 'ch",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/600
https://github.com/google/deepvariant/issues/600:2709,safety,input,inputs,2709," 19:34:53.162234 140434427647808 make_examples_core.py:236] Task 5/8: Preparing inputs . I1219 19:34:53.209864 140434427647808 genomics_reader.py:222] Reading SRR10720227.RGG.marked.sorted.bam with NativeSamReader . I1219 19:34:53.105211 139854300972864 genomics_reader.py:222] Reading SRR10720227.RGG.marked.sorted.bam with NativeSamReader . I1219 19:34:53.131687 139854300972864 make_examples_core.py:236] Task 3/8: Preparing inputs . I1219 19:34:53.201329 139691329734464 genomics_reader.py:222] Reading SRR10720227.RGG.marked.sorted.bam with NativeSamReader . I1219 19:34:53.210919 139691329734464 make_examples_core.py:236] Task 2/8: Preparing inputs . I1219 19:34:53.269506 139691329734464 genomics_reader.py:222] Reading SRR10720227.RGG.marked.sorted.bam with NativeSamReader . I1219 19:34:53.152369 140549443307328 genomics_reader.py:222] Reading SRR10720227.RGG.marked.sorted.bam with NativeSamReader . I1219 19:34:53.174761 140549443307328 make_examples_core.py:236] Task 0/8: Preparing inputs . I1219 19:34:53.282668 140549443307328 genomics_reader.py:222] Reading SRR10720227.RGG.marked.sorted.bam with NativeSamReader . I1219 19:34:53.454261 140628390905664 genomics_reader.py:222] Reading SRR10720227.RGG.marked.sorted.bam with NativeSamReader . I1219 19:34:53.366882 140309844559680 genomics_reader.py:222] Reading SRR10720227.RGG.marked.sorted.bam with NativeSamReader . I1219 19:34:53.342017 139854300972864 genomics_reader.py:222] Reading SRR10720227.RGG.marked.sorted.bam with NativeSamReader . I1219 19:34:54.719705 140628390905664 make_examples_core.py:236] Task 7/8: Common contigs are ['chr1', 'chr2', 'chr3', 'chr4', 'chr5', 'chr6', 'chr7', 'chr8', 'chr9', 'chr10', 'chr11', 'chr12', 'chr13', 'chr14', 'chr15', 'chr16', 'chr17', 'chr18', 'chr19', 'chr20', 'chr21', 'chr22', 'chrX', 'chrY', 'chrM'] . I1222 17:05:35.016938 140674909861696 make_examples_core.py:236] 115600 candidates (123081 examples) [152.69s elapsed] . I1222 17:08:13.613651 140674909861696 make_examples_cor",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/600
https://github.com/google/deepvariant/issues/600:157,usability,command,command,157,"RNA seq file take too much time !; Until now it takes 60 hours !! . That is the output on terminal, until now it runs , why it is like that? . * Running the command:* . time seq 0 7 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""Homo_sapiens_assembly38.fasta"" --reads ""SRR10720227.RGG.marked.sorted.bam"" --examples ""/tmp/tmpe2z79_52/make_examples.tfrecord@8.gz"" --split_skip_reads --task {} . . I1219 19:34:53.123960 140628390905664 genomics_reader.py:222] Reading SRR10720227.RGG.marked.sorted.bam with NativeSamReader . I1219 19:34:53.243814 140628390905664 make_examples_core.py:236] Task 7/8: Preparing inputs . I1219 19:34:53.094258 139794990966592 genomics_reader.py:222] Reading SRR10720227.RGG.marked.sorted.bam with NativeSamReader . I1219 19:34:53.115872 139794990966592 make_examples_core.py:236] Task 1/8: Preparing inputs . I1219 19:34:53.235279 139794990966592 genomics_reader.py:222] Reading SRR10720227.RGG.marked.sorted.bam with NativeSamReader . I1219 19:34:53.134214 140309844559680 genomics_reader.py:222] Reading SRR10720227.RGG.marked.sorted.bam with NativeSamReader . I1219 19:34:53.192481 140309844559680 make_examples_core.py:236] Task 6/8: Preparing inputs . I1219 19:34:53.107017 140536106870592 genomics_reader.py:222] Reading SRR10720227.RGG.marked.sorted.bam with NativeSamReader . I1219 19:34:53.122070 140536106870592 make_examples_core.py:236] Task 4/8: Preparing inputs . I1219 19:34:53.226204 140536106870592 genomics_reader.py:222] Reading SRR10720227.RGG.marked.sorted.bam with NativeSamReader . I1219 19:34:53.129359 140434427647808 genomics_reader.py:222] Reading SRR10720227.RGG.marked.sorted.bam with NativeSamReader . I1219 19:34:53.162234 140434427647808 make_examples_core.py:236] Task 5/8: Preparing inputs . I1219 19:34:53.209864 140434427647808 genomics_reader.py:222] Reading SRR10720227.RGG.marked.sorted.bam with NativeSamReader . I1219 19:34:53.105211 139854300972864 genomics_reader.py:222] Reading SR",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/600
https://github.com/google/deepvariant/issues/600:654,usability,input,inputs,654,"RNA seq file take too much time !; Until now it takes 60 hours !! . That is the output on terminal, until now it runs , why it is like that? . * Running the command:* . time seq 0 7 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""Homo_sapiens_assembly38.fasta"" --reads ""SRR10720227.RGG.marked.sorted.bam"" --examples ""/tmp/tmpe2z79_52/make_examples.tfrecord@8.gz"" --split_skip_reads --task {} . . I1219 19:34:53.123960 140628390905664 genomics_reader.py:222] Reading SRR10720227.RGG.marked.sorted.bam with NativeSamReader . I1219 19:34:53.243814 140628390905664 make_examples_core.py:236] Task 7/8: Preparing inputs . I1219 19:34:53.094258 139794990966592 genomics_reader.py:222] Reading SRR10720227.RGG.marked.sorted.bam with NativeSamReader . I1219 19:34:53.115872 139794990966592 make_examples_core.py:236] Task 1/8: Preparing inputs . I1219 19:34:53.235279 139794990966592 genomics_reader.py:222] Reading SRR10720227.RGG.marked.sorted.bam with NativeSamReader . I1219 19:34:53.134214 140309844559680 genomics_reader.py:222] Reading SRR10720227.RGG.marked.sorted.bam with NativeSamReader . I1219 19:34:53.192481 140309844559680 make_examples_core.py:236] Task 6/8: Preparing inputs . I1219 19:34:53.107017 140536106870592 genomics_reader.py:222] Reading SRR10720227.RGG.marked.sorted.bam with NativeSamReader . I1219 19:34:53.122070 140536106870592 make_examples_core.py:236] Task 4/8: Preparing inputs . I1219 19:34:53.226204 140536106870592 genomics_reader.py:222] Reading SRR10720227.RGG.marked.sorted.bam with NativeSamReader . I1219 19:34:53.129359 140434427647808 genomics_reader.py:222] Reading SRR10720227.RGG.marked.sorted.bam with NativeSamReader . I1219 19:34:53.162234 140434427647808 make_examples_core.py:236] Task 5/8: Preparing inputs . I1219 19:34:53.209864 140434427647808 genomics_reader.py:222] Reading SRR10720227.RGG.marked.sorted.bam with NativeSamReader . I1219 19:34:53.105211 139854300972864 genomics_reader.py:222] Reading SR",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/600
https://github.com/google/deepvariant/issues/600:875,usability,input,inputs,875,"RNA seq file take too much time !; Until now it takes 60 hours !! . That is the output on terminal, until now it runs , why it is like that? . * Running the command:* . time seq 0 7 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""Homo_sapiens_assembly38.fasta"" --reads ""SRR10720227.RGG.marked.sorted.bam"" --examples ""/tmp/tmpe2z79_52/make_examples.tfrecord@8.gz"" --split_skip_reads --task {} . . I1219 19:34:53.123960 140628390905664 genomics_reader.py:222] Reading SRR10720227.RGG.marked.sorted.bam with NativeSamReader . I1219 19:34:53.243814 140628390905664 make_examples_core.py:236] Task 7/8: Preparing inputs . I1219 19:34:53.094258 139794990966592 genomics_reader.py:222] Reading SRR10720227.RGG.marked.sorted.bam with NativeSamReader . I1219 19:34:53.115872 139794990966592 make_examples_core.py:236] Task 1/8: Preparing inputs . I1219 19:34:53.235279 139794990966592 genomics_reader.py:222] Reading SRR10720227.RGG.marked.sorted.bam with NativeSamReader . I1219 19:34:53.134214 140309844559680 genomics_reader.py:222] Reading SRR10720227.RGG.marked.sorted.bam with NativeSamReader . I1219 19:34:53.192481 140309844559680 make_examples_core.py:236] Task 6/8: Preparing inputs . I1219 19:34:53.107017 140536106870592 genomics_reader.py:222] Reading SRR10720227.RGG.marked.sorted.bam with NativeSamReader . I1219 19:34:53.122070 140536106870592 make_examples_core.py:236] Task 4/8: Preparing inputs . I1219 19:34:53.226204 140536106870592 genomics_reader.py:222] Reading SRR10720227.RGG.marked.sorted.bam with NativeSamReader . I1219 19:34:53.129359 140434427647808 genomics_reader.py:222] Reading SRR10720227.RGG.marked.sorted.bam with NativeSamReader . I1219 19:34:53.162234 140434427647808 make_examples_core.py:236] Task 5/8: Preparing inputs . I1219 19:34:53.209864 140434427647808 genomics_reader.py:222] Reading SRR10720227.RGG.marked.sorted.bam with NativeSamReader . I1219 19:34:53.105211 139854300972864 genomics_reader.py:222] Reading SR",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/600
https://github.com/google/deepvariant/issues/600:1223,usability,input,inputs,1223,"epvariant/bin/make_examples --mode calling --ref ""Homo_sapiens_assembly38.fasta"" --reads ""SRR10720227.RGG.marked.sorted.bam"" --examples ""/tmp/tmpe2z79_52/make_examples.tfrecord@8.gz"" --split_skip_reads --task {} . . I1219 19:34:53.123960 140628390905664 genomics_reader.py:222] Reading SRR10720227.RGG.marked.sorted.bam with NativeSamReader . I1219 19:34:53.243814 140628390905664 make_examples_core.py:236] Task 7/8: Preparing inputs . I1219 19:34:53.094258 139794990966592 genomics_reader.py:222] Reading SRR10720227.RGG.marked.sorted.bam with NativeSamReader . I1219 19:34:53.115872 139794990966592 make_examples_core.py:236] Task 1/8: Preparing inputs . I1219 19:34:53.235279 139794990966592 genomics_reader.py:222] Reading SRR10720227.RGG.marked.sorted.bam with NativeSamReader . I1219 19:34:53.134214 140309844559680 genomics_reader.py:222] Reading SRR10720227.RGG.marked.sorted.bam with NativeSamReader . I1219 19:34:53.192481 140309844559680 make_examples_core.py:236] Task 6/8: Preparing inputs . I1219 19:34:53.107017 140536106870592 genomics_reader.py:222] Reading SRR10720227.RGG.marked.sorted.bam with NativeSamReader . I1219 19:34:53.122070 140536106870592 make_examples_core.py:236] Task 4/8: Preparing inputs . I1219 19:34:53.226204 140536106870592 genomics_reader.py:222] Reading SRR10720227.RGG.marked.sorted.bam with NativeSamReader . I1219 19:34:53.129359 140434427647808 genomics_reader.py:222] Reading SRR10720227.RGG.marked.sorted.bam with NativeSamReader . I1219 19:34:53.162234 140434427647808 make_examples_core.py:236] Task 5/8: Preparing inputs . I1219 19:34:53.209864 140434427647808 genomics_reader.py:222] Reading SRR10720227.RGG.marked.sorted.bam with NativeSamReader . I1219 19:34:53.105211 139854300972864 genomics_reader.py:222] Reading SRR10720227.RGG.marked.sorted.bam with NativeSamReader . I1219 19:34:53.131687 139854300972864 make_examples_core.py:236] Task 3/8: Preparing inputs . I1219 19:34:53.201329 139691329734464 genomics_reader.py:222] Reading SRR1072",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/600
https://github.com/google/deepvariant/issues/600:1444,usability,input,inputs,1444, 19:34:53.123960 140628390905664 genomics_reader.py:222] Reading SRR10720227.RGG.marked.sorted.bam with NativeSamReader . I1219 19:34:53.243814 140628390905664 make_examples_core.py:236] Task 7/8: Preparing inputs . I1219 19:34:53.094258 139794990966592 genomics_reader.py:222] Reading SRR10720227.RGG.marked.sorted.bam with NativeSamReader . I1219 19:34:53.115872 139794990966592 make_examples_core.py:236] Task 1/8: Preparing inputs . I1219 19:34:53.235279 139794990966592 genomics_reader.py:222] Reading SRR10720227.RGG.marked.sorted.bam with NativeSamReader . I1219 19:34:53.134214 140309844559680 genomics_reader.py:222] Reading SRR10720227.RGG.marked.sorted.bam with NativeSamReader . I1219 19:34:53.192481 140309844559680 make_examples_core.py:236] Task 6/8: Preparing inputs . I1219 19:34:53.107017 140536106870592 genomics_reader.py:222] Reading SRR10720227.RGG.marked.sorted.bam with NativeSamReader . I1219 19:34:53.122070 140536106870592 make_examples_core.py:236] Task 4/8: Preparing inputs . I1219 19:34:53.226204 140536106870592 genomics_reader.py:222] Reading SRR10720227.RGG.marked.sorted.bam with NativeSamReader . I1219 19:34:53.129359 140434427647808 genomics_reader.py:222] Reading SRR10720227.RGG.marked.sorted.bam with NativeSamReader . I1219 19:34:53.162234 140434427647808 make_examples_core.py:236] Task 5/8: Preparing inputs . I1219 19:34:53.209864 140434427647808 genomics_reader.py:222] Reading SRR10720227.RGG.marked.sorted.bam with NativeSamReader . I1219 19:34:53.105211 139854300972864 genomics_reader.py:222] Reading SRR10720227.RGG.marked.sorted.bam with NativeSamReader . I1219 19:34:53.131687 139854300972864 make_examples_core.py:236] Task 3/8: Preparing inputs . I1219 19:34:53.201329 139691329734464 genomics_reader.py:222] Reading SRR10720227.RGG.marked.sorted.bam with NativeSamReader . I1219 19:34:53.210919 139691329734464 make_examples_core.py:236] Task 2/8: Preparing inputs . I1219 19:34:53.269506 139691329734464 genomics_reader.py:222] Reading SRR1072,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/600
https://github.com/google/deepvariant/issues/600:1792,usability,input,inputs,1792, 19:34:53.115872 139794990966592 make_examples_core.py:236] Task 1/8: Preparing inputs . I1219 19:34:53.235279 139794990966592 genomics_reader.py:222] Reading SRR10720227.RGG.marked.sorted.bam with NativeSamReader . I1219 19:34:53.134214 140309844559680 genomics_reader.py:222] Reading SRR10720227.RGG.marked.sorted.bam with NativeSamReader . I1219 19:34:53.192481 140309844559680 make_examples_core.py:236] Task 6/8: Preparing inputs . I1219 19:34:53.107017 140536106870592 genomics_reader.py:222] Reading SRR10720227.RGG.marked.sorted.bam with NativeSamReader . I1219 19:34:53.122070 140536106870592 make_examples_core.py:236] Task 4/8: Preparing inputs . I1219 19:34:53.226204 140536106870592 genomics_reader.py:222] Reading SRR10720227.RGG.marked.sorted.bam with NativeSamReader . I1219 19:34:53.129359 140434427647808 genomics_reader.py:222] Reading SRR10720227.RGG.marked.sorted.bam with NativeSamReader . I1219 19:34:53.162234 140434427647808 make_examples_core.py:236] Task 5/8: Preparing inputs . I1219 19:34:53.209864 140434427647808 genomics_reader.py:222] Reading SRR10720227.RGG.marked.sorted.bam with NativeSamReader . I1219 19:34:53.105211 139854300972864 genomics_reader.py:222] Reading SRR10720227.RGG.marked.sorted.bam with NativeSamReader . I1219 19:34:53.131687 139854300972864 make_examples_core.py:236] Task 3/8: Preparing inputs . I1219 19:34:53.201329 139691329734464 genomics_reader.py:222] Reading SRR10720227.RGG.marked.sorted.bam with NativeSamReader . I1219 19:34:53.210919 139691329734464 make_examples_core.py:236] Task 2/8: Preparing inputs . I1219 19:34:53.269506 139691329734464 genomics_reader.py:222] Reading SRR10720227.RGG.marked.sorted.bam with NativeSamReader . I1219 19:34:53.152369 140549443307328 genomics_reader.py:222] Reading SRR10720227.RGG.marked.sorted.bam with NativeSamReader . I1219 19:34:53.174761 140549443307328 make_examples_core.py:236] Task 0/8: Preparing inputs . I1219 19:34:53.282668 140549443307328 genomics_reader.py:222] Reading SRR1072,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/600
https://github.com/google/deepvariant/issues/600:2140,usability,input,inputs,2140, 19:34:53.192481 140309844559680 make_examples_core.py:236] Task 6/8: Preparing inputs . I1219 19:34:53.107017 140536106870592 genomics_reader.py:222] Reading SRR10720227.RGG.marked.sorted.bam with NativeSamReader . I1219 19:34:53.122070 140536106870592 make_examples_core.py:236] Task 4/8: Preparing inputs . I1219 19:34:53.226204 140536106870592 genomics_reader.py:222] Reading SRR10720227.RGG.marked.sorted.bam with NativeSamReader . I1219 19:34:53.129359 140434427647808 genomics_reader.py:222] Reading SRR10720227.RGG.marked.sorted.bam with NativeSamReader . I1219 19:34:53.162234 140434427647808 make_examples_core.py:236] Task 5/8: Preparing inputs . I1219 19:34:53.209864 140434427647808 genomics_reader.py:222] Reading SRR10720227.RGG.marked.sorted.bam with NativeSamReader . I1219 19:34:53.105211 139854300972864 genomics_reader.py:222] Reading SRR10720227.RGG.marked.sorted.bam with NativeSamReader . I1219 19:34:53.131687 139854300972864 make_examples_core.py:236] Task 3/8: Preparing inputs . I1219 19:34:53.201329 139691329734464 genomics_reader.py:222] Reading SRR10720227.RGG.marked.sorted.bam with NativeSamReader . I1219 19:34:53.210919 139691329734464 make_examples_core.py:236] Task 2/8: Preparing inputs . I1219 19:34:53.269506 139691329734464 genomics_reader.py:222] Reading SRR10720227.RGG.marked.sorted.bam with NativeSamReader . I1219 19:34:53.152369 140549443307328 genomics_reader.py:222] Reading SRR10720227.RGG.marked.sorted.bam with NativeSamReader . I1219 19:34:53.174761 140549443307328 make_examples_core.py:236] Task 0/8: Preparing inputs . I1219 19:34:53.282668 140549443307328 genomics_reader.py:222] Reading SRR10720227.RGG.marked.sorted.bam with NativeSamReader . I1219 19:34:53.454261 140628390905664 genomics_reader.py:222] Reading SRR10720227.RGG.marked.sorted.bam with NativeSamReader . I1219 19:34:53.366882 140309844559680 genomics_reader.py:222] Reading SRR10720227.RGG.marked.sorted.bam with NativeSamReader . I1219 19:34:53.342017 139854300972864 genomi,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/600
https://github.com/google/deepvariant/issues/600:2361,usability,input,inputs,2361," 19:34:53.122070 140536106870592 make_examples_core.py:236] Task 4/8: Preparing inputs . I1219 19:34:53.226204 140536106870592 genomics_reader.py:222] Reading SRR10720227.RGG.marked.sorted.bam with NativeSamReader . I1219 19:34:53.129359 140434427647808 genomics_reader.py:222] Reading SRR10720227.RGG.marked.sorted.bam with NativeSamReader . I1219 19:34:53.162234 140434427647808 make_examples_core.py:236] Task 5/8: Preparing inputs . I1219 19:34:53.209864 140434427647808 genomics_reader.py:222] Reading SRR10720227.RGG.marked.sorted.bam with NativeSamReader . I1219 19:34:53.105211 139854300972864 genomics_reader.py:222] Reading SRR10720227.RGG.marked.sorted.bam with NativeSamReader . I1219 19:34:53.131687 139854300972864 make_examples_core.py:236] Task 3/8: Preparing inputs . I1219 19:34:53.201329 139691329734464 genomics_reader.py:222] Reading SRR10720227.RGG.marked.sorted.bam with NativeSamReader . I1219 19:34:53.210919 139691329734464 make_examples_core.py:236] Task 2/8: Preparing inputs . I1219 19:34:53.269506 139691329734464 genomics_reader.py:222] Reading SRR10720227.RGG.marked.sorted.bam with NativeSamReader . I1219 19:34:53.152369 140549443307328 genomics_reader.py:222] Reading SRR10720227.RGG.marked.sorted.bam with NativeSamReader . I1219 19:34:53.174761 140549443307328 make_examples_core.py:236] Task 0/8: Preparing inputs . I1219 19:34:53.282668 140549443307328 genomics_reader.py:222] Reading SRR10720227.RGG.marked.sorted.bam with NativeSamReader . I1219 19:34:53.454261 140628390905664 genomics_reader.py:222] Reading SRR10720227.RGG.marked.sorted.bam with NativeSamReader . I1219 19:34:53.366882 140309844559680 genomics_reader.py:222] Reading SRR10720227.RGG.marked.sorted.bam with NativeSamReader . I1219 19:34:53.342017 139854300972864 genomics_reader.py:222] Reading SRR10720227.RGG.marked.sorted.bam with NativeSamReader . I1219 19:34:54.719705 140628390905664 make_examples_core.py:236] Task 7/8: Common contigs are ['chr1', 'chr2', 'chr3', 'chr4', 'chr5', 'ch",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/600
https://github.com/google/deepvariant/issues/600:2709,usability,input,inputs,2709," 19:34:53.162234 140434427647808 make_examples_core.py:236] Task 5/8: Preparing inputs . I1219 19:34:53.209864 140434427647808 genomics_reader.py:222] Reading SRR10720227.RGG.marked.sorted.bam with NativeSamReader . I1219 19:34:53.105211 139854300972864 genomics_reader.py:222] Reading SRR10720227.RGG.marked.sorted.bam with NativeSamReader . I1219 19:34:53.131687 139854300972864 make_examples_core.py:236] Task 3/8: Preparing inputs . I1219 19:34:53.201329 139691329734464 genomics_reader.py:222] Reading SRR10720227.RGG.marked.sorted.bam with NativeSamReader . I1219 19:34:53.210919 139691329734464 make_examples_core.py:236] Task 2/8: Preparing inputs . I1219 19:34:53.269506 139691329734464 genomics_reader.py:222] Reading SRR10720227.RGG.marked.sorted.bam with NativeSamReader . I1219 19:34:53.152369 140549443307328 genomics_reader.py:222] Reading SRR10720227.RGG.marked.sorted.bam with NativeSamReader . I1219 19:34:53.174761 140549443307328 make_examples_core.py:236] Task 0/8: Preparing inputs . I1219 19:34:53.282668 140549443307328 genomics_reader.py:222] Reading SRR10720227.RGG.marked.sorted.bam with NativeSamReader . I1219 19:34:53.454261 140628390905664 genomics_reader.py:222] Reading SRR10720227.RGG.marked.sorted.bam with NativeSamReader . I1219 19:34:53.366882 140309844559680 genomics_reader.py:222] Reading SRR10720227.RGG.marked.sorted.bam with NativeSamReader . I1219 19:34:53.342017 139854300972864 genomics_reader.py:222] Reading SRR10720227.RGG.marked.sorted.bam with NativeSamReader . I1219 19:34:54.719705 140628390905664 make_examples_core.py:236] Task 7/8: Common contigs are ['chr1', 'chr2', 'chr3', 'chr4', 'chr5', 'chr6', 'chr7', 'chr8', 'chr9', 'chr10', 'chr11', 'chr12', 'chr13', 'chr14', 'chr15', 'chr16', 'chr17', 'chr18', 'chr19', 'chr20', 'chr21', 'chr22', 'chrX', 'chrY', 'chrM'] . I1222 17:05:35.016938 140674909861696 make_examples_core.py:236] 115600 candidates (123081 examples) [152.69s elapsed] . I1222 17:08:13.613651 140674909861696 make_examples_cor",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/600
https://github.com/google/deepvariant/issues/601:342,availability,error,error,342,"Problem encountered while running DeepVariant using singularity on Torque HPC; **Describe the issue:**. Hi, I am following the [deepvariant-quick-start](https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-quick-start.md) tutorial on singularity to try out DeepVariant on our study. When I do `singularity run` command, I get the error about `temple()` please see the error message below. I'm wondering if anyone can help with this. **Setup**. - Operating system: CentOS Linux 7 (Core). - Singularity version: 3.5-8.el7. - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): singularity. - Type of data: WGS. **Steps to reproduce:**. - Command:. > singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. > docker://google/deepvariant:""1.4.0"" \. > /opt/deepvariant/bin/run_deepvariant \. > --model_type=WGS \. > --ref=${reference_genome} \. > --reads=${bam} \. > --regions=""chr1"" \. > --output_vcf=${vcf_dir}/${sample}.vcf.gz \. > --output_gvcf=${gvcf_dir}/${sample}.g.vcf.gz \. > --intermediate_results_dir ${tmp_dir} \. > --num_shards=${ncpu}. - Error trace: (if applicable). > Error in tempfile() using template /XXX/parXXXXX.par: Parent directory (/XXX/) does not exist at /usr/bin/parallel line 3889. **Additional comments:**. I also tried with `--no-home` flag which did not work at all. . I don't have the root access since I am running this on a HPC Torque system managed by others.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/601
https://github.com/google/deepvariant/issues/601:380,availability,error,error,380,"Problem encountered while running DeepVariant using singularity on Torque HPC; **Describe the issue:**. Hi, I am following the [deepvariant-quick-start](https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-quick-start.md) tutorial on singularity to try out DeepVariant on our study. When I do `singularity run` command, I get the error about `temple()` please see the error message below. I'm wondering if anyone can help with this. **Setup**. - Operating system: CentOS Linux 7 (Core). - Singularity version: 3.5-8.el7. - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): singularity. - Type of data: WGS. **Steps to reproduce:**. - Command:. > singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. > docker://google/deepvariant:""1.4.0"" \. > /opt/deepvariant/bin/run_deepvariant \. > --model_type=WGS \. > --ref=${reference_genome} \. > --reads=${bam} \. > --regions=""chr1"" \. > --output_vcf=${vcf_dir}/${sample}.vcf.gz \. > --output_gvcf=${gvcf_dir}/${sample}.g.vcf.gz \. > --intermediate_results_dir ${tmp_dir} \. > --num_shards=${ncpu}. - Error trace: (if applicable). > Error in tempfile() using template /XXX/parXXXXX.par: Parent directory (/XXX/) does not exist at /usr/bin/parallel line 3889. **Additional comments:**. I also tried with `--no-home` flag which did not work at all. . I don't have the root access since I am running this on a HPC Torque system managed by others.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/601
https://github.com/google/deepvariant/issues/601:458,availability,Operat,Operating,458,"Problem encountered while running DeepVariant using singularity on Torque HPC; **Describe the issue:**. Hi, I am following the [deepvariant-quick-start](https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-quick-start.md) tutorial on singularity to try out DeepVariant on our study. When I do `singularity run` command, I get the error about `temple()` please see the error message below. I'm wondering if anyone can help with this. **Setup**. - Operating system: CentOS Linux 7 (Core). - Singularity version: 3.5-8.el7. - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): singularity. - Type of data: WGS. **Steps to reproduce:**. - Command:. > singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. > docker://google/deepvariant:""1.4.0"" \. > /opt/deepvariant/bin/run_deepvariant \. > --model_type=WGS \. > --ref=${reference_genome} \. > --reads=${bam} \. > --regions=""chr1"" \. > --output_vcf=${vcf_dir}/${sample}.vcf.gz \. > --output_gvcf=${gvcf_dir}/${sample}.g.vcf.gz \. > --intermediate_results_dir ${tmp_dir} \. > --num_shards=${ncpu}. - Error trace: (if applicable). > Error in tempfile() using template /XXX/parXXXXX.par: Parent directory (/XXX/) does not exist at /usr/bin/parallel line 3889. **Additional comments:**. I also tried with `--no-home` flag which did not work at all. . I don't have the root access since I am running this on a HPC Torque system managed by others.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/601
https://github.com/google/deepvariant/issues/601:1092,availability,Error,Error,1092,"Problem encountered while running DeepVariant using singularity on Torque HPC; **Describe the issue:**. Hi, I am following the [deepvariant-quick-start](https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-quick-start.md) tutorial on singularity to try out DeepVariant on our study. When I do `singularity run` command, I get the error about `temple()` please see the error message below. I'm wondering if anyone can help with this. **Setup**. - Operating system: CentOS Linux 7 (Core). - Singularity version: 3.5-8.el7. - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): singularity. - Type of data: WGS. **Steps to reproduce:**. - Command:. > singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. > docker://google/deepvariant:""1.4.0"" \. > /opt/deepvariant/bin/run_deepvariant \. > --model_type=WGS \. > --ref=${reference_genome} \. > --reads=${bam} \. > --regions=""chr1"" \. > --output_vcf=${vcf_dir}/${sample}.vcf.gz \. > --output_gvcf=${gvcf_dir}/${sample}.g.vcf.gz \. > --intermediate_results_dir ${tmp_dir} \. > --num_shards=${ncpu}. - Error trace: (if applicable). > Error in tempfile() using template /XXX/parXXXXX.par: Parent directory (/XXX/) does not exist at /usr/bin/parallel line 3889. **Additional comments:**. I also tried with `--no-home` flag which did not work at all. . I don't have the root access since I am running this on a HPC Torque system managed by others.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/601
https://github.com/google/deepvariant/issues/601:1124,availability,Error,Error,1124,"Problem encountered while running DeepVariant using singularity on Torque HPC; **Describe the issue:**. Hi, I am following the [deepvariant-quick-start](https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-quick-start.md) tutorial on singularity to try out DeepVariant on our study. When I do `singularity run` command, I get the error about `temple()` please see the error message below. I'm wondering if anyone can help with this. **Setup**. - Operating system: CentOS Linux 7 (Core). - Singularity version: 3.5-8.el7. - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): singularity. - Type of data: WGS. **Steps to reproduce:**. - Command:. > singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. > docker://google/deepvariant:""1.4.0"" \. > /opt/deepvariant/bin/run_deepvariant \. > --model_type=WGS \. > --ref=${reference_genome} \. > --reads=${bam} \. > --regions=""chr1"" \. > --output_vcf=${vcf_dir}/${sample}.vcf.gz \. > --output_gvcf=${gvcf_dir}/${sample}.g.vcf.gz \. > --intermediate_results_dir ${tmp_dir} \. > --num_shards=${ncpu}. - Error trace: (if applicable). > Error in tempfile() using template /XXX/parXXXXX.par: Parent directory (/XXX/) does not exist at /usr/bin/parallel line 3889. **Additional comments:**. I also tried with `--no-home` flag which did not work at all. . I don't have the root access since I am running this on a HPC Torque system managed by others.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/601
https://github.com/google/deepvariant/issues/601:513,deployability,version,version,513,"Problem encountered while running DeepVariant using singularity on Torque HPC; **Describe the issue:**. Hi, I am following the [deepvariant-quick-start](https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-quick-start.md) tutorial on singularity to try out DeepVariant on our study. When I do `singularity run` command, I get the error about `temple()` please see the error message below. I'm wondering if anyone can help with this. **Setup**. - Operating system: CentOS Linux 7 (Core). - Singularity version: 3.5-8.el7. - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): singularity. - Type of data: WGS. **Steps to reproduce:**. - Command:. > singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. > docker://google/deepvariant:""1.4.0"" \. > /opt/deepvariant/bin/run_deepvariant \. > --model_type=WGS \. > --ref=${reference_genome} \. > --reads=${bam} \. > --regions=""chr1"" \. > --output_vcf=${vcf_dir}/${sample}.vcf.gz \. > --output_gvcf=${gvcf_dir}/${sample}.g.vcf.gz \. > --intermediate_results_dir ${tmp_dir} \. > --num_shards=${ncpu}. - Error trace: (if applicable). > Error in tempfile() using template /XXX/parXXXXX.par: Parent directory (/XXX/) does not exist at /usr/bin/parallel line 3889. **Additional comments:**. I also tried with `--no-home` flag which did not work at all. . I don't have the root access since I am running this on a HPC Torque system managed by others.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/601
https://github.com/google/deepvariant/issues/601:547,deployability,version,version,547,"Problem encountered while running DeepVariant using singularity on Torque HPC; **Describe the issue:**. Hi, I am following the [deepvariant-quick-start](https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-quick-start.md) tutorial on singularity to try out DeepVariant on our study. When I do `singularity run` command, I get the error about `temple()` please see the error message below. I'm wondering if anyone can help with this. **Setup**. - Operating system: CentOS Linux 7 (Core). - Singularity version: 3.5-8.el7. - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): singularity. - Type of data: WGS. **Steps to reproduce:**. - Command:. > singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. > docker://google/deepvariant:""1.4.0"" \. > /opt/deepvariant/bin/run_deepvariant \. > --model_type=WGS \. > --ref=${reference_genome} \. > --reads=${bam} \. > --regions=""chr1"" \. > --output_vcf=${vcf_dir}/${sample}.vcf.gz \. > --output_gvcf=${gvcf_dir}/${sample}.g.vcf.gz \. > --intermediate_results_dir ${tmp_dir} \. > --num_shards=${ncpu}. - Error trace: (if applicable). > Error in tempfile() using template /XXX/parXXXXX.par: Parent directory (/XXX/) does not exist at /usr/bin/parallel line 3889. **Additional comments:**. I also tried with `--no-home` flag which did not work at all. . I don't have the root access since I am running this on a HPC Torque system managed by others.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/601
https://github.com/google/deepvariant/issues/601:565,deployability,Instal,Installation,565,"Problem encountered while running DeepVariant using singularity on Torque HPC; **Describe the issue:**. Hi, I am following the [deepvariant-quick-start](https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-quick-start.md) tutorial on singularity to try out DeepVariant on our study. When I do `singularity run` command, I get the error about `temple()` please see the error message below. I'm wondering if anyone can help with this. **Setup**. - Operating system: CentOS Linux 7 (Core). - Singularity version: 3.5-8.el7. - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): singularity. - Type of data: WGS. **Steps to reproduce:**. - Command:. > singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. > docker://google/deepvariant:""1.4.0"" \. > /opt/deepvariant/bin/run_deepvariant \. > --model_type=WGS \. > --ref=${reference_genome} \. > --reads=${bam} \. > --regions=""chr1"" \. > --output_vcf=${vcf_dir}/${sample}.vcf.gz \. > --output_gvcf=${gvcf_dir}/${sample}.g.vcf.gz \. > --intermediate_results_dir ${tmp_dir} \. > --num_shards=${ncpu}. - Error trace: (if applicable). > Error in tempfile() using template /XXX/parXXXXX.par: Parent directory (/XXX/) does not exist at /usr/bin/parallel line 3889. **Additional comments:**. I also tried with `--no-home` flag which did not work at all. . I don't have the root access since I am running this on a HPC Torque system managed by others.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/601
https://github.com/google/deepvariant/issues/601:1416,deployability,manag,managed,1416,"Problem encountered while running DeepVariant using singularity on Torque HPC; **Describe the issue:**. Hi, I am following the [deepvariant-quick-start](https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-quick-start.md) tutorial on singularity to try out DeepVariant on our study. When I do `singularity run` command, I get the error about `temple()` please see the error message below. I'm wondering if anyone can help with this. **Setup**. - Operating system: CentOS Linux 7 (Core). - Singularity version: 3.5-8.el7. - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): singularity. - Type of data: WGS. **Steps to reproduce:**. - Command:. > singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. > docker://google/deepvariant:""1.4.0"" \. > /opt/deepvariant/bin/run_deepvariant \. > --model_type=WGS \. > --ref=${reference_genome} \. > --reads=${bam} \. > --regions=""chr1"" \. > --output_vcf=${vcf_dir}/${sample}.vcf.gz \. > --output_gvcf=${gvcf_dir}/${sample}.g.vcf.gz \. > --intermediate_results_dir ${tmp_dir} \. > --num_shards=${ncpu}. - Error trace: (if applicable). > Error in tempfile() using template /XXX/parXXXXX.par: Parent directory (/XXX/) does not exist at /usr/bin/parallel line 3889. **Additional comments:**. I also tried with `--no-home` flag which did not work at all. . I don't have the root access since I am running this on a HPC Torque system managed by others.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/601
https://github.com/google/deepvariant/issues/601:492,energy efficiency,Core,Core,492,"Problem encountered while running DeepVariant using singularity on Torque HPC; **Describe the issue:**. Hi, I am following the [deepvariant-quick-start](https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-quick-start.md) tutorial on singularity to try out DeepVariant on our study. When I do `singularity run` command, I get the error about `temple()` please see the error message below. I'm wondering if anyone can help with this. **Setup**. - Operating system: CentOS Linux 7 (Core). - Singularity version: 3.5-8.el7. - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): singularity. - Type of data: WGS. **Steps to reproduce:**. - Command:. > singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. > docker://google/deepvariant:""1.4.0"" \. > /opt/deepvariant/bin/run_deepvariant \. > --model_type=WGS \. > --ref=${reference_genome} \. > --reads=${bam} \. > --regions=""chr1"" \. > --output_vcf=${vcf_dir}/${sample}.vcf.gz \. > --output_gvcf=${gvcf_dir}/${sample}.g.vcf.gz \. > --intermediate_results_dir ${tmp_dir} \. > --num_shards=${ncpu}. - Error trace: (if applicable). > Error in tempfile() using template /XXX/parXXXXX.par: Parent directory (/XXX/) does not exist at /usr/bin/parallel line 3889. **Additional comments:**. I also tried with `--no-home` flag which did not work at all. . I don't have the root access since I am running this on a HPC Torque system managed by others.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/601
https://github.com/google/deepvariant/issues/601:1416,energy efficiency,manag,managed,1416,"Problem encountered while running DeepVariant using singularity on Torque HPC; **Describe the issue:**. Hi, I am following the [deepvariant-quick-start](https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-quick-start.md) tutorial on singularity to try out DeepVariant on our study. When I do `singularity run` command, I get the error about `temple()` please see the error message below. I'm wondering if anyone can help with this. **Setup**. - Operating system: CentOS Linux 7 (Core). - Singularity version: 3.5-8.el7. - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): singularity. - Type of data: WGS. **Steps to reproduce:**. - Command:. > singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. > docker://google/deepvariant:""1.4.0"" \. > /opt/deepvariant/bin/run_deepvariant \. > --model_type=WGS \. > --ref=${reference_genome} \. > --reads=${bam} \. > --regions=""chr1"" \. > --output_vcf=${vcf_dir}/${sample}.vcf.gz \. > --output_gvcf=${gvcf_dir}/${sample}.g.vcf.gz \. > --intermediate_results_dir ${tmp_dir} \. > --num_shards=${ncpu}. - Error trace: (if applicable). > Error in tempfile() using template /XXX/parXXXXX.par: Parent directory (/XXX/) does not exist at /usr/bin/parallel line 3889. **Additional comments:**. I also tried with `--no-home` flag which did not work at all. . I don't have the root access since I am running this on a HPC Torque system managed by others.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/601
https://github.com/google/deepvariant/issues/601:386,integrability,messag,message,386,"Problem encountered while running DeepVariant using singularity on Torque HPC; **Describe the issue:**. Hi, I am following the [deepvariant-quick-start](https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-quick-start.md) tutorial on singularity to try out DeepVariant on our study. When I do `singularity run` command, I get the error about `temple()` please see the error message below. I'm wondering if anyone can help with this. **Setup**. - Operating system: CentOS Linux 7 (Core). - Singularity version: 3.5-8.el7. - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): singularity. - Type of data: WGS. **Steps to reproduce:**. - Command:. > singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. > docker://google/deepvariant:""1.4.0"" \. > /opt/deepvariant/bin/run_deepvariant \. > --model_type=WGS \. > --ref=${reference_genome} \. > --reads=${bam} \. > --regions=""chr1"" \. > --output_vcf=${vcf_dir}/${sample}.vcf.gz \. > --output_gvcf=${gvcf_dir}/${sample}.g.vcf.gz \. > --intermediate_results_dir ${tmp_dir} \. > --num_shards=${ncpu}. - Error trace: (if applicable). > Error in tempfile() using template /XXX/parXXXXX.par: Parent directory (/XXX/) does not exist at /usr/bin/parallel line 3889. **Additional comments:**. I also tried with `--no-home` flag which did not work at all. . I don't have the root access since I am running this on a HPC Torque system managed by others.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/601
https://github.com/google/deepvariant/issues/601:513,integrability,version,version,513,"Problem encountered while running DeepVariant using singularity on Torque HPC; **Describe the issue:**. Hi, I am following the [deepvariant-quick-start](https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-quick-start.md) tutorial on singularity to try out DeepVariant on our study. When I do `singularity run` command, I get the error about `temple()` please see the error message below. I'm wondering if anyone can help with this. **Setup**. - Operating system: CentOS Linux 7 (Core). - Singularity version: 3.5-8.el7. - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): singularity. - Type of data: WGS. **Steps to reproduce:**. - Command:. > singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. > docker://google/deepvariant:""1.4.0"" \. > /opt/deepvariant/bin/run_deepvariant \. > --model_type=WGS \. > --ref=${reference_genome} \. > --reads=${bam} \. > --regions=""chr1"" \. > --output_vcf=${vcf_dir}/${sample}.vcf.gz \. > --output_gvcf=${gvcf_dir}/${sample}.g.vcf.gz \. > --intermediate_results_dir ${tmp_dir} \. > --num_shards=${ncpu}. - Error trace: (if applicable). > Error in tempfile() using template /XXX/parXXXXX.par: Parent directory (/XXX/) does not exist at /usr/bin/parallel line 3889. **Additional comments:**. I also tried with `--no-home` flag which did not work at all. . I don't have the root access since I am running this on a HPC Torque system managed by others.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/601
https://github.com/google/deepvariant/issues/601:547,integrability,version,version,547,"Problem encountered while running DeepVariant using singularity on Torque HPC; **Describe the issue:**. Hi, I am following the [deepvariant-quick-start](https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-quick-start.md) tutorial on singularity to try out DeepVariant on our study. When I do `singularity run` command, I get the error about `temple()` please see the error message below. I'm wondering if anyone can help with this. **Setup**. - Operating system: CentOS Linux 7 (Core). - Singularity version: 3.5-8.el7. - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): singularity. - Type of data: WGS. **Steps to reproduce:**. - Command:. > singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. > docker://google/deepvariant:""1.4.0"" \. > /opt/deepvariant/bin/run_deepvariant \. > --model_type=WGS \. > --ref=${reference_genome} \. > --reads=${bam} \. > --regions=""chr1"" \. > --output_vcf=${vcf_dir}/${sample}.vcf.gz \. > --output_gvcf=${gvcf_dir}/${sample}.g.vcf.gz \. > --intermediate_results_dir ${tmp_dir} \. > --num_shards=${ncpu}. - Error trace: (if applicable). > Error in tempfile() using template /XXX/parXXXXX.par: Parent directory (/XXX/) does not exist at /usr/bin/parallel line 3889. **Additional comments:**. I also tried with `--no-home` flag which did not work at all. . I don't have the root access since I am running this on a HPC Torque system managed by others.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/601
https://github.com/google/deepvariant/issues/601:386,interoperability,messag,message,386,"Problem encountered while running DeepVariant using singularity on Torque HPC; **Describe the issue:**. Hi, I am following the [deepvariant-quick-start](https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-quick-start.md) tutorial on singularity to try out DeepVariant on our study. When I do `singularity run` command, I get the error about `temple()` please see the error message below. I'm wondering if anyone can help with this. **Setup**. - Operating system: CentOS Linux 7 (Core). - Singularity version: 3.5-8.el7. - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): singularity. - Type of data: WGS. **Steps to reproduce:**. - Command:. > singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. > docker://google/deepvariant:""1.4.0"" \. > /opt/deepvariant/bin/run_deepvariant \. > --model_type=WGS \. > --ref=${reference_genome} \. > --reads=${bam} \. > --regions=""chr1"" \. > --output_vcf=${vcf_dir}/${sample}.vcf.gz \. > --output_gvcf=${gvcf_dir}/${sample}.g.vcf.gz \. > --intermediate_results_dir ${tmp_dir} \. > --num_shards=${ncpu}. - Error trace: (if applicable). > Error in tempfile() using template /XXX/parXXXXX.par: Parent directory (/XXX/) does not exist at /usr/bin/parallel line 3889. **Additional comments:**. I also tried with `--no-home` flag which did not work at all. . I don't have the root access since I am running this on a HPC Torque system managed by others.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/601
https://github.com/google/deepvariant/issues/601:513,modifiability,version,version,513,"Problem encountered while running DeepVariant using singularity on Torque HPC; **Describe the issue:**. Hi, I am following the [deepvariant-quick-start](https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-quick-start.md) tutorial on singularity to try out DeepVariant on our study. When I do `singularity run` command, I get the error about `temple()` please see the error message below. I'm wondering if anyone can help with this. **Setup**. - Operating system: CentOS Linux 7 (Core). - Singularity version: 3.5-8.el7. - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): singularity. - Type of data: WGS. **Steps to reproduce:**. - Command:. > singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. > docker://google/deepvariant:""1.4.0"" \. > /opt/deepvariant/bin/run_deepvariant \. > --model_type=WGS \. > --ref=${reference_genome} \. > --reads=${bam} \. > --regions=""chr1"" \. > --output_vcf=${vcf_dir}/${sample}.vcf.gz \. > --output_gvcf=${gvcf_dir}/${sample}.g.vcf.gz \. > --intermediate_results_dir ${tmp_dir} \. > --num_shards=${ncpu}. - Error trace: (if applicable). > Error in tempfile() using template /XXX/parXXXXX.par: Parent directory (/XXX/) does not exist at /usr/bin/parallel line 3889. **Additional comments:**. I also tried with `--no-home` flag which did not work at all. . I don't have the root access since I am running this on a HPC Torque system managed by others.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/601
https://github.com/google/deepvariant/issues/601:547,modifiability,version,version,547,"Problem encountered while running DeepVariant using singularity on Torque HPC; **Describe the issue:**. Hi, I am following the [deepvariant-quick-start](https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-quick-start.md) tutorial on singularity to try out DeepVariant on our study. When I do `singularity run` command, I get the error about `temple()` please see the error message below. I'm wondering if anyone can help with this. **Setup**. - Operating system: CentOS Linux 7 (Core). - Singularity version: 3.5-8.el7. - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): singularity. - Type of data: WGS. **Steps to reproduce:**. - Command:. > singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. > docker://google/deepvariant:""1.4.0"" \. > /opt/deepvariant/bin/run_deepvariant \. > --model_type=WGS \. > --ref=${reference_genome} \. > --reads=${bam} \. > --regions=""chr1"" \. > --output_vcf=${vcf_dir}/${sample}.vcf.gz \. > --output_gvcf=${gvcf_dir}/${sample}.g.vcf.gz \. > --intermediate_results_dir ${tmp_dir} \. > --num_shards=${ncpu}. - Error trace: (if applicable). > Error in tempfile() using template /XXX/parXXXXX.par: Parent directory (/XXX/) does not exist at /usr/bin/parallel line 3889. **Additional comments:**. I also tried with `--no-home` flag which did not work at all. . I don't have the root access since I am running this on a HPC Torque system managed by others.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/601
https://github.com/google/deepvariant/issues/601:342,performance,error,error,342,"Problem encountered while running DeepVariant using singularity on Torque HPC; **Describe the issue:**. Hi, I am following the [deepvariant-quick-start](https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-quick-start.md) tutorial on singularity to try out DeepVariant on our study. When I do `singularity run` command, I get the error about `temple()` please see the error message below. I'm wondering if anyone can help with this. **Setup**. - Operating system: CentOS Linux 7 (Core). - Singularity version: 3.5-8.el7. - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): singularity. - Type of data: WGS. **Steps to reproduce:**. - Command:. > singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. > docker://google/deepvariant:""1.4.0"" \. > /opt/deepvariant/bin/run_deepvariant \. > --model_type=WGS \. > --ref=${reference_genome} \. > --reads=${bam} \. > --regions=""chr1"" \. > --output_vcf=${vcf_dir}/${sample}.vcf.gz \. > --output_gvcf=${gvcf_dir}/${sample}.g.vcf.gz \. > --intermediate_results_dir ${tmp_dir} \. > --num_shards=${ncpu}. - Error trace: (if applicable). > Error in tempfile() using template /XXX/parXXXXX.par: Parent directory (/XXX/) does not exist at /usr/bin/parallel line 3889. **Additional comments:**. I also tried with `--no-home` flag which did not work at all. . I don't have the root access since I am running this on a HPC Torque system managed by others.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/601
https://github.com/google/deepvariant/issues/601:380,performance,error,error,380,"Problem encountered while running DeepVariant using singularity on Torque HPC; **Describe the issue:**. Hi, I am following the [deepvariant-quick-start](https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-quick-start.md) tutorial on singularity to try out DeepVariant on our study. When I do `singularity run` command, I get the error about `temple()` please see the error message below. I'm wondering if anyone can help with this. **Setup**. - Operating system: CentOS Linux 7 (Core). - Singularity version: 3.5-8.el7. - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): singularity. - Type of data: WGS. **Steps to reproduce:**. - Command:. > singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. > docker://google/deepvariant:""1.4.0"" \. > /opt/deepvariant/bin/run_deepvariant \. > --model_type=WGS \. > --ref=${reference_genome} \. > --reads=${bam} \. > --regions=""chr1"" \. > --output_vcf=${vcf_dir}/${sample}.vcf.gz \. > --output_gvcf=${gvcf_dir}/${sample}.g.vcf.gz \. > --intermediate_results_dir ${tmp_dir} \. > --num_shards=${ncpu}. - Error trace: (if applicable). > Error in tempfile() using template /XXX/parXXXXX.par: Parent directory (/XXX/) does not exist at /usr/bin/parallel line 3889. **Additional comments:**. I also tried with `--no-home` flag which did not work at all. . I don't have the root access since I am running this on a HPC Torque system managed by others.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/601
https://github.com/google/deepvariant/issues/601:1092,performance,Error,Error,1092,"Problem encountered while running DeepVariant using singularity on Torque HPC; **Describe the issue:**. Hi, I am following the [deepvariant-quick-start](https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-quick-start.md) tutorial on singularity to try out DeepVariant on our study. When I do `singularity run` command, I get the error about `temple()` please see the error message below. I'm wondering if anyone can help with this. **Setup**. - Operating system: CentOS Linux 7 (Core). - Singularity version: 3.5-8.el7. - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): singularity. - Type of data: WGS. **Steps to reproduce:**. - Command:. > singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. > docker://google/deepvariant:""1.4.0"" \. > /opt/deepvariant/bin/run_deepvariant \. > --model_type=WGS \. > --ref=${reference_genome} \. > --reads=${bam} \. > --regions=""chr1"" \. > --output_vcf=${vcf_dir}/${sample}.vcf.gz \. > --output_gvcf=${gvcf_dir}/${sample}.g.vcf.gz \. > --intermediate_results_dir ${tmp_dir} \. > --num_shards=${ncpu}. - Error trace: (if applicable). > Error in tempfile() using template /XXX/parXXXXX.par: Parent directory (/XXX/) does not exist at /usr/bin/parallel line 3889. **Additional comments:**. I also tried with `--no-home` flag which did not work at all. . I don't have the root access since I am running this on a HPC Torque system managed by others.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/601
https://github.com/google/deepvariant/issues/601:1124,performance,Error,Error,1124,"Problem encountered while running DeepVariant using singularity on Torque HPC; **Describe the issue:**. Hi, I am following the [deepvariant-quick-start](https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-quick-start.md) tutorial on singularity to try out DeepVariant on our study. When I do `singularity run` command, I get the error about `temple()` please see the error message below. I'm wondering if anyone can help with this. **Setup**. - Operating system: CentOS Linux 7 (Core). - Singularity version: 3.5-8.el7. - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): singularity. - Type of data: WGS. **Steps to reproduce:**. - Command:. > singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. > docker://google/deepvariant:""1.4.0"" \. > /opt/deepvariant/bin/run_deepvariant \. > --model_type=WGS \. > --ref=${reference_genome} \. > --reads=${bam} \. > --regions=""chr1"" \. > --output_vcf=${vcf_dir}/${sample}.vcf.gz \. > --output_gvcf=${gvcf_dir}/${sample}.g.vcf.gz \. > --intermediate_results_dir ${tmp_dir} \. > --num_shards=${ncpu}. - Error trace: (if applicable). > Error in tempfile() using template /XXX/parXXXXX.par: Parent directory (/XXX/) does not exist at /usr/bin/parallel line 3889. **Additional comments:**. I also tried with `--no-home` flag which did not work at all. . I don't have the root access since I am running this on a HPC Torque system managed by others.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/601
https://github.com/google/deepvariant/issues/601:1230,performance,parallel,parallel,1230,"Problem encountered while running DeepVariant using singularity on Torque HPC; **Describe the issue:**. Hi, I am following the [deepvariant-quick-start](https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-quick-start.md) tutorial on singularity to try out DeepVariant on our study. When I do `singularity run` command, I get the error about `temple()` please see the error message below. I'm wondering if anyone can help with this. **Setup**. - Operating system: CentOS Linux 7 (Core). - Singularity version: 3.5-8.el7. - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): singularity. - Type of data: WGS. **Steps to reproduce:**. - Command:. > singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. > docker://google/deepvariant:""1.4.0"" \. > /opt/deepvariant/bin/run_deepvariant \. > --model_type=WGS \. > --ref=${reference_genome} \. > --reads=${bam} \. > --regions=""chr1"" \. > --output_vcf=${vcf_dir}/${sample}.vcf.gz \. > --output_gvcf=${gvcf_dir}/${sample}.g.vcf.gz \. > --intermediate_results_dir ${tmp_dir} \. > --num_shards=${ncpu}. - Error trace: (if applicable). > Error in tempfile() using template /XXX/parXXXXX.par: Parent directory (/XXX/) does not exist at /usr/bin/parallel line 3889. **Additional comments:**. I also tried with `--no-home` flag which did not work at all. . I don't have the root access since I am running this on a HPC Torque system managed by others.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/601
https://github.com/google/deepvariant/issues/601:1203,reliability,doe,does,1203,"Problem encountered while running DeepVariant using singularity on Torque HPC; **Describe the issue:**. Hi, I am following the [deepvariant-quick-start](https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-quick-start.md) tutorial on singularity to try out DeepVariant on our study. When I do `singularity run` command, I get the error about `temple()` please see the error message below. I'm wondering if anyone can help with this. **Setup**. - Operating system: CentOS Linux 7 (Core). - Singularity version: 3.5-8.el7. - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): singularity. - Type of data: WGS. **Steps to reproduce:**. - Command:. > singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. > docker://google/deepvariant:""1.4.0"" \. > /opt/deepvariant/bin/run_deepvariant \. > --model_type=WGS \. > --ref=${reference_genome} \. > --reads=${bam} \. > --regions=""chr1"" \. > --output_vcf=${vcf_dir}/${sample}.vcf.gz \. > --output_gvcf=${gvcf_dir}/${sample}.g.vcf.gz \. > --intermediate_results_dir ${tmp_dir} \. > --num_shards=${ncpu}. - Error trace: (if applicable). > Error in tempfile() using template /XXX/parXXXXX.par: Parent directory (/XXX/) does not exist at /usr/bin/parallel line 3889. **Additional comments:**. I also tried with `--no-home` flag which did not work at all. . I don't have the root access since I am running this on a HPC Torque system managed by others.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/601
https://github.com/google/deepvariant/issues/601:342,safety,error,error,342,"Problem encountered while running DeepVariant using singularity on Torque HPC; **Describe the issue:**. Hi, I am following the [deepvariant-quick-start](https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-quick-start.md) tutorial on singularity to try out DeepVariant on our study. When I do `singularity run` command, I get the error about `temple()` please see the error message below. I'm wondering if anyone can help with this. **Setup**. - Operating system: CentOS Linux 7 (Core). - Singularity version: 3.5-8.el7. - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): singularity. - Type of data: WGS. **Steps to reproduce:**. - Command:. > singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. > docker://google/deepvariant:""1.4.0"" \. > /opt/deepvariant/bin/run_deepvariant \. > --model_type=WGS \. > --ref=${reference_genome} \. > --reads=${bam} \. > --regions=""chr1"" \. > --output_vcf=${vcf_dir}/${sample}.vcf.gz \. > --output_gvcf=${gvcf_dir}/${sample}.g.vcf.gz \. > --intermediate_results_dir ${tmp_dir} \. > --num_shards=${ncpu}. - Error trace: (if applicable). > Error in tempfile() using template /XXX/parXXXXX.par: Parent directory (/XXX/) does not exist at /usr/bin/parallel line 3889. **Additional comments:**. I also tried with `--no-home` flag which did not work at all. . I don't have the root access since I am running this on a HPC Torque system managed by others.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/601
https://github.com/google/deepvariant/issues/601:380,safety,error,error,380,"Problem encountered while running DeepVariant using singularity on Torque HPC; **Describe the issue:**. Hi, I am following the [deepvariant-quick-start](https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-quick-start.md) tutorial on singularity to try out DeepVariant on our study. When I do `singularity run` command, I get the error about `temple()` please see the error message below. I'm wondering if anyone can help with this. **Setup**. - Operating system: CentOS Linux 7 (Core). - Singularity version: 3.5-8.el7. - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): singularity. - Type of data: WGS. **Steps to reproduce:**. - Command:. > singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. > docker://google/deepvariant:""1.4.0"" \. > /opt/deepvariant/bin/run_deepvariant \. > --model_type=WGS \. > --ref=${reference_genome} \. > --reads=${bam} \. > --regions=""chr1"" \. > --output_vcf=${vcf_dir}/${sample}.vcf.gz \. > --output_gvcf=${gvcf_dir}/${sample}.g.vcf.gz \. > --intermediate_results_dir ${tmp_dir} \. > --num_shards=${ncpu}. - Error trace: (if applicable). > Error in tempfile() using template /XXX/parXXXXX.par: Parent directory (/XXX/) does not exist at /usr/bin/parallel line 3889. **Additional comments:**. I also tried with `--no-home` flag which did not work at all. . I don't have the root access since I am running this on a HPC Torque system managed by others.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/601
https://github.com/google/deepvariant/issues/601:1092,safety,Error,Error,1092,"Problem encountered while running DeepVariant using singularity on Torque HPC; **Describe the issue:**. Hi, I am following the [deepvariant-quick-start](https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-quick-start.md) tutorial on singularity to try out DeepVariant on our study. When I do `singularity run` command, I get the error about `temple()` please see the error message below. I'm wondering if anyone can help with this. **Setup**. - Operating system: CentOS Linux 7 (Core). - Singularity version: 3.5-8.el7. - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): singularity. - Type of data: WGS. **Steps to reproduce:**. - Command:. > singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. > docker://google/deepvariant:""1.4.0"" \. > /opt/deepvariant/bin/run_deepvariant \. > --model_type=WGS \. > --ref=${reference_genome} \. > --reads=${bam} \. > --regions=""chr1"" \. > --output_vcf=${vcf_dir}/${sample}.vcf.gz \. > --output_gvcf=${gvcf_dir}/${sample}.g.vcf.gz \. > --intermediate_results_dir ${tmp_dir} \. > --num_shards=${ncpu}. - Error trace: (if applicable). > Error in tempfile() using template /XXX/parXXXXX.par: Parent directory (/XXX/) does not exist at /usr/bin/parallel line 3889. **Additional comments:**. I also tried with `--no-home` flag which did not work at all. . I don't have the root access since I am running this on a HPC Torque system managed by others.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/601
https://github.com/google/deepvariant/issues/601:1124,safety,Error,Error,1124,"Problem encountered while running DeepVariant using singularity on Torque HPC; **Describe the issue:**. Hi, I am following the [deepvariant-quick-start](https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-quick-start.md) tutorial on singularity to try out DeepVariant on our study. When I do `singularity run` command, I get the error about `temple()` please see the error message below. I'm wondering if anyone can help with this. **Setup**. - Operating system: CentOS Linux 7 (Core). - Singularity version: 3.5-8.el7. - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): singularity. - Type of data: WGS. **Steps to reproduce:**. - Command:. > singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. > docker://google/deepvariant:""1.4.0"" \. > /opt/deepvariant/bin/run_deepvariant \. > --model_type=WGS \. > --ref=${reference_genome} \. > --reads=${bam} \. > --regions=""chr1"" \. > --output_vcf=${vcf_dir}/${sample}.vcf.gz \. > --output_gvcf=${gvcf_dir}/${sample}.g.vcf.gz \. > --intermediate_results_dir ${tmp_dir} \. > --num_shards=${ncpu}. - Error trace: (if applicable). > Error in tempfile() using template /XXX/parXXXXX.par: Parent directory (/XXX/) does not exist at /usr/bin/parallel line 3889. **Additional comments:**. I also tried with `--no-home` flag which did not work at all. . I don't have the root access since I am running this on a HPC Torque system managed by others.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/601
https://github.com/google/deepvariant/issues/601:1416,safety,manag,managed,1416,"Problem encountered while running DeepVariant using singularity on Torque HPC; **Describe the issue:**. Hi, I am following the [deepvariant-quick-start](https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-quick-start.md) tutorial on singularity to try out DeepVariant on our study. When I do `singularity run` command, I get the error about `temple()` please see the error message below. I'm wondering if anyone can help with this. **Setup**. - Operating system: CentOS Linux 7 (Core). - Singularity version: 3.5-8.el7. - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): singularity. - Type of data: WGS. **Steps to reproduce:**. - Command:. > singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. > docker://google/deepvariant:""1.4.0"" \. > /opt/deepvariant/bin/run_deepvariant \. > --model_type=WGS \. > --ref=${reference_genome} \. > --reads=${bam} \. > --regions=""chr1"" \. > --output_vcf=${vcf_dir}/${sample}.vcf.gz \. > --output_gvcf=${gvcf_dir}/${sample}.g.vcf.gz \. > --intermediate_results_dir ${tmp_dir} \. > --num_shards=${ncpu}. - Error trace: (if applicable). > Error in tempfile() using template /XXX/parXXXXX.par: Parent directory (/XXX/) does not exist at /usr/bin/parallel line 3889. **Additional comments:**. I also tried with `--no-home` flag which did not work at all. . I don't have the root access since I am running this on a HPC Torque system managed by others.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/601
https://github.com/google/deepvariant/issues/601:1362,security,access,access,1362,"Problem encountered while running DeepVariant using singularity on Torque HPC; **Describe the issue:**. Hi, I am following the [deepvariant-quick-start](https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-quick-start.md) tutorial on singularity to try out DeepVariant on our study. When I do `singularity run` command, I get the error about `temple()` please see the error message below. I'm wondering if anyone can help with this. **Setup**. - Operating system: CentOS Linux 7 (Core). - Singularity version: 3.5-8.el7. - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): singularity. - Type of data: WGS. **Steps to reproduce:**. - Command:. > singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. > docker://google/deepvariant:""1.4.0"" \. > /opt/deepvariant/bin/run_deepvariant \. > --model_type=WGS \. > --ref=${reference_genome} \. > --reads=${bam} \. > --regions=""chr1"" \. > --output_vcf=${vcf_dir}/${sample}.vcf.gz \. > --output_gvcf=${gvcf_dir}/${sample}.g.vcf.gz \. > --intermediate_results_dir ${tmp_dir} \. > --num_shards=${ncpu}. - Error trace: (if applicable). > Error in tempfile() using template /XXX/parXXXXX.par: Parent directory (/XXX/) does not exist at /usr/bin/parallel line 3889. **Additional comments:**. I also tried with `--no-home` flag which did not work at all. . I don't have the root access since I am running this on a HPC Torque system managed by others.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/601
https://github.com/google/deepvariant/issues/601:1098,testability,trace,trace,1098,"Problem encountered while running DeepVariant using singularity on Torque HPC; **Describe the issue:**. Hi, I am following the [deepvariant-quick-start](https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-quick-start.md) tutorial on singularity to try out DeepVariant on our study. When I do `singularity run` command, I get the error about `temple()` please see the error message below. I'm wondering if anyone can help with this. **Setup**. - Operating system: CentOS Linux 7 (Core). - Singularity version: 3.5-8.el7. - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): singularity. - Type of data: WGS. **Steps to reproduce:**. - Command:. > singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. > docker://google/deepvariant:""1.4.0"" \. > /opt/deepvariant/bin/run_deepvariant \. > --model_type=WGS \. > --ref=${reference_genome} \. > --reads=${bam} \. > --regions=""chr1"" \. > --output_vcf=${vcf_dir}/${sample}.vcf.gz \. > --output_gvcf=${gvcf_dir}/${sample}.g.vcf.gz \. > --intermediate_results_dir ${tmp_dir} \. > --num_shards=${ncpu}. - Error trace: (if applicable). > Error in tempfile() using template /XXX/parXXXXX.par: Parent directory (/XXX/) does not exist at /usr/bin/parallel line 3889. **Additional comments:**. I also tried with `--no-home` flag which did not work at all. . I don't have the root access since I am running this on a HPC Torque system managed by others.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/601
https://github.com/google/deepvariant/issues/601:323,usability,command,command,323,"Problem encountered while running DeepVariant using singularity on Torque HPC; **Describe the issue:**. Hi, I am following the [deepvariant-quick-start](https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-quick-start.md) tutorial on singularity to try out DeepVariant on our study. When I do `singularity run` command, I get the error about `temple()` please see the error message below. I'm wondering if anyone can help with this. **Setup**. - Operating system: CentOS Linux 7 (Core). - Singularity version: 3.5-8.el7. - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): singularity. - Type of data: WGS. **Steps to reproduce:**. - Command:. > singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. > docker://google/deepvariant:""1.4.0"" \. > /opt/deepvariant/bin/run_deepvariant \. > --model_type=WGS \. > --ref=${reference_genome} \. > --reads=${bam} \. > --regions=""chr1"" \. > --output_vcf=${vcf_dir}/${sample}.vcf.gz \. > --output_gvcf=${gvcf_dir}/${sample}.g.vcf.gz \. > --intermediate_results_dir ${tmp_dir} \. > --num_shards=${ncpu}. - Error trace: (if applicable). > Error in tempfile() using template /XXX/parXXXXX.par: Parent directory (/XXX/) does not exist at /usr/bin/parallel line 3889. **Additional comments:**. I also tried with `--no-home` flag which did not work at all. . I don't have the root access since I am running this on a HPC Torque system managed by others.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/601
https://github.com/google/deepvariant/issues/601:342,usability,error,error,342,"Problem encountered while running DeepVariant using singularity on Torque HPC; **Describe the issue:**. Hi, I am following the [deepvariant-quick-start](https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-quick-start.md) tutorial on singularity to try out DeepVariant on our study. When I do `singularity run` command, I get the error about `temple()` please see the error message below. I'm wondering if anyone can help with this. **Setup**. - Operating system: CentOS Linux 7 (Core). - Singularity version: 3.5-8.el7. - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): singularity. - Type of data: WGS. **Steps to reproduce:**. - Command:. > singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. > docker://google/deepvariant:""1.4.0"" \. > /opt/deepvariant/bin/run_deepvariant \. > --model_type=WGS \. > --ref=${reference_genome} \. > --reads=${bam} \. > --regions=""chr1"" \. > --output_vcf=${vcf_dir}/${sample}.vcf.gz \. > --output_gvcf=${gvcf_dir}/${sample}.g.vcf.gz \. > --intermediate_results_dir ${tmp_dir} \. > --num_shards=${ncpu}. - Error trace: (if applicable). > Error in tempfile() using template /XXX/parXXXXX.par: Parent directory (/XXX/) does not exist at /usr/bin/parallel line 3889. **Additional comments:**. I also tried with `--no-home` flag which did not work at all. . I don't have the root access since I am running this on a HPC Torque system managed by others.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/601
https://github.com/google/deepvariant/issues/601:380,usability,error,error,380,"Problem encountered while running DeepVariant using singularity on Torque HPC; **Describe the issue:**. Hi, I am following the [deepvariant-quick-start](https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-quick-start.md) tutorial on singularity to try out DeepVariant on our study. When I do `singularity run` command, I get the error about `temple()` please see the error message below. I'm wondering if anyone can help with this. **Setup**. - Operating system: CentOS Linux 7 (Core). - Singularity version: 3.5-8.el7. - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): singularity. - Type of data: WGS. **Steps to reproduce:**. - Command:. > singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. > docker://google/deepvariant:""1.4.0"" \. > /opt/deepvariant/bin/run_deepvariant \. > --model_type=WGS \. > --ref=${reference_genome} \. > --reads=${bam} \. > --regions=""chr1"" \. > --output_vcf=${vcf_dir}/${sample}.vcf.gz \. > --output_gvcf=${gvcf_dir}/${sample}.g.vcf.gz \. > --intermediate_results_dir ${tmp_dir} \. > --num_shards=${ncpu}. - Error trace: (if applicable). > Error in tempfile() using template /XXX/parXXXXX.par: Parent directory (/XXX/) does not exist at /usr/bin/parallel line 3889. **Additional comments:**. I also tried with `--no-home` flag which did not work at all. . I don't have the root access since I am running this on a HPC Torque system managed by others.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/601
https://github.com/google/deepvariant/issues/601:429,usability,help,help,429,"Problem encountered while running DeepVariant using singularity on Torque HPC; **Describe the issue:**. Hi, I am following the [deepvariant-quick-start](https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-quick-start.md) tutorial on singularity to try out DeepVariant on our study. When I do `singularity run` command, I get the error about `temple()` please see the error message below. I'm wondering if anyone can help with this. **Setup**. - Operating system: CentOS Linux 7 (Core). - Singularity version: 3.5-8.el7. - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): singularity. - Type of data: WGS. **Steps to reproduce:**. - Command:. > singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. > docker://google/deepvariant:""1.4.0"" \. > /opt/deepvariant/bin/run_deepvariant \. > --model_type=WGS \. > --ref=${reference_genome} \. > --reads=${bam} \. > --regions=""chr1"" \. > --output_vcf=${vcf_dir}/${sample}.vcf.gz \. > --output_gvcf=${gvcf_dir}/${sample}.g.vcf.gz \. > --intermediate_results_dir ${tmp_dir} \. > --num_shards=${ncpu}. - Error trace: (if applicable). > Error in tempfile() using template /XXX/parXXXXX.par: Parent directory (/XXX/) does not exist at /usr/bin/parallel line 3889. **Additional comments:**. I also tried with `--no-home` flag which did not work at all. . I don't have the root access since I am running this on a HPC Torque system managed by others.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/601
https://github.com/google/deepvariant/issues/601:681,usability,Command,Command,681,"Problem encountered while running DeepVariant using singularity on Torque HPC; **Describe the issue:**. Hi, I am following the [deepvariant-quick-start](https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-quick-start.md) tutorial on singularity to try out DeepVariant on our study. When I do `singularity run` command, I get the error about `temple()` please see the error message below. I'm wondering if anyone can help with this. **Setup**. - Operating system: CentOS Linux 7 (Core). - Singularity version: 3.5-8.el7. - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): singularity. - Type of data: WGS. **Steps to reproduce:**. - Command:. > singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. > docker://google/deepvariant:""1.4.0"" \. > /opt/deepvariant/bin/run_deepvariant \. > --model_type=WGS \. > --ref=${reference_genome} \. > --reads=${bam} \. > --regions=""chr1"" \. > --output_vcf=${vcf_dir}/${sample}.vcf.gz \. > --output_gvcf=${gvcf_dir}/${sample}.g.vcf.gz \. > --intermediate_results_dir ${tmp_dir} \. > --num_shards=${ncpu}. - Error trace: (if applicable). > Error in tempfile() using template /XXX/parXXXXX.par: Parent directory (/XXX/) does not exist at /usr/bin/parallel line 3889. **Additional comments:**. I also tried with `--no-home` flag which did not work at all. . I don't have the root access since I am running this on a HPC Torque system managed by others.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/601
https://github.com/google/deepvariant/issues/601:1092,usability,Error,Error,1092,"Problem encountered while running DeepVariant using singularity on Torque HPC; **Describe the issue:**. Hi, I am following the [deepvariant-quick-start](https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-quick-start.md) tutorial on singularity to try out DeepVariant on our study. When I do `singularity run` command, I get the error about `temple()` please see the error message below. I'm wondering if anyone can help with this. **Setup**. - Operating system: CentOS Linux 7 (Core). - Singularity version: 3.5-8.el7. - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): singularity. - Type of data: WGS. **Steps to reproduce:**. - Command:. > singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. > docker://google/deepvariant:""1.4.0"" \. > /opt/deepvariant/bin/run_deepvariant \. > --model_type=WGS \. > --ref=${reference_genome} \. > --reads=${bam} \. > --regions=""chr1"" \. > --output_vcf=${vcf_dir}/${sample}.vcf.gz \. > --output_gvcf=${gvcf_dir}/${sample}.g.vcf.gz \. > --intermediate_results_dir ${tmp_dir} \. > --num_shards=${ncpu}. - Error trace: (if applicable). > Error in tempfile() using template /XXX/parXXXXX.par: Parent directory (/XXX/) does not exist at /usr/bin/parallel line 3889. **Additional comments:**. I also tried with `--no-home` flag which did not work at all. . I don't have the root access since I am running this on a HPC Torque system managed by others.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/601
https://github.com/google/deepvariant/issues/601:1124,usability,Error,Error,1124,"Problem encountered while running DeepVariant using singularity on Torque HPC; **Describe the issue:**. Hi, I am following the [deepvariant-quick-start](https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-quick-start.md) tutorial on singularity to try out DeepVariant on our study. When I do `singularity run` command, I get the error about `temple()` please see the error message below. I'm wondering if anyone can help with this. **Setup**. - Operating system: CentOS Linux 7 (Core). - Singularity version: 3.5-8.el7. - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): singularity. - Type of data: WGS. **Steps to reproduce:**. - Command:. > singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. > docker://google/deepvariant:""1.4.0"" \. > /opt/deepvariant/bin/run_deepvariant \. > --model_type=WGS \. > --ref=${reference_genome} \. > --reads=${bam} \. > --regions=""chr1"" \. > --output_vcf=${vcf_dir}/${sample}.vcf.gz \. > --output_gvcf=${gvcf_dir}/${sample}.g.vcf.gz \. > --intermediate_results_dir ${tmp_dir} \. > --num_shards=${ncpu}. - Error trace: (if applicable). > Error in tempfile() using template /XXX/parXXXXX.par: Parent directory (/XXX/) does not exist at /usr/bin/parallel line 3889. **Additional comments:**. I also tried with `--no-home` flag which did not work at all. . I don't have the root access since I am running this on a HPC Torque system managed by others.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/601
https://github.com/google/deepvariant/issues/602:110,availability,cluster,cluster,110,"AttributeError: deleter; Hi DeepVariant team,. I'm running DeepVariant via Singularity via Snakemake on a HPC cluster and overall, if I create an interactive session, my entire WES pipeline runs fine until an odd DeepVariant job fails (by that I mean that let's say 30 deepvariant jobs finish ok and the 31st fails). I can then restart the Snakemake run and the same job will run fine, followed by more jobs that will also run fine until another odd jobs fails. This is also particularly true when I use the --cluster command in Snakemake (i.e., send it to a SLURM job scheduler from the master node) - in this event every single job fails. I could of course run all my samples on a single interactive session, keep checking the log file and restart the run every time it fails but I guess that's less than optimal plus this way I can really only run one sample at the time. For the interactive sessions I request 180G and 64cpus (in my case it's: ```srsh --mem=180G --cpus-per-task=64 --partition=long```). . I would request same parameters when using --cluster so:. ```snakemake --cluster ""sbatch --mem=180G cpus-per-task=64"" --jobs 64 --profie profile/ ```(where profile holds singularity args etc.). Singularity image is deepvariant_1.4.0.sif. my Snakemake rule:. ```. rule deepvariant:. input:. bam=rules.apply_bqsr.output.bam,. ref='/mnt/shared/scratch/kmarians/private/dyslexia_gatk/workflow/resources/genome.fasta'. output:. vcf=""results/deepvariant/{sample}.vcf.gz"". params:. model=""WES"". threads: . 64. resources:. mem_mb=163840. log:. ""logs/deepvariant/{sample}/stdout.log"". singularity:. ""singularity/deepvariant_1.4.0.sif"". # ""singularity/deepvariant_1.4.0-gpu.sif"" # for GPU. shell:. """""". /opt/deepvariant/bin/run_deepvariant --model_type {params.model} --ref {input.ref} --reads {input.bam} --output_vcf {output.vcf} --num_shards {threads} --make_examples_extra_args='vsc_min_count_snps=3,vsc_min_fraction_snps=0.2,vsc_min_count_indels=3,vsc_min_fraction_indels=0.10'. """""". ```. Below ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/602
https://github.com/google/deepvariant/issues/602:510,availability,cluster,cluster,510,"AttributeError: deleter; Hi DeepVariant team,. I'm running DeepVariant via Singularity via Snakemake on a HPC cluster and overall, if I create an interactive session, my entire WES pipeline runs fine until an odd DeepVariant job fails (by that I mean that let's say 30 deepvariant jobs finish ok and the 31st fails). I can then restart the Snakemake run and the same job will run fine, followed by more jobs that will also run fine until another odd jobs fails. This is also particularly true when I use the --cluster command in Snakemake (i.e., send it to a SLURM job scheduler from the master node) - in this event every single job fails. I could of course run all my samples on a single interactive session, keep checking the log file and restart the run every time it fails but I guess that's less than optimal plus this way I can really only run one sample at the time. For the interactive sessions I request 180G and 64cpus (in my case it's: ```srsh --mem=180G --cpus-per-task=64 --partition=long```). . I would request same parameters when using --cluster so:. ```snakemake --cluster ""sbatch --mem=180G cpus-per-task=64"" --jobs 64 --profie profile/ ```(where profile holds singularity args etc.). Singularity image is deepvariant_1.4.0.sif. my Snakemake rule:. ```. rule deepvariant:. input:. bam=rules.apply_bqsr.output.bam,. ref='/mnt/shared/scratch/kmarians/private/dyslexia_gatk/workflow/resources/genome.fasta'. output:. vcf=""results/deepvariant/{sample}.vcf.gz"". params:. model=""WES"". threads: . 64. resources:. mem_mb=163840. log:. ""logs/deepvariant/{sample}/stdout.log"". singularity:. ""singularity/deepvariant_1.4.0.sif"". # ""singularity/deepvariant_1.4.0-gpu.sif"" # for GPU. shell:. """""". /opt/deepvariant/bin/run_deepvariant --model_type {params.model} --ref {input.ref} --reads {input.bam} --output_vcf {output.vcf} --num_shards {threads} --make_examples_extra_args='vsc_min_count_snps=3,vsc_min_fraction_snps=0.2,vsc_min_count_indels=3,vsc_min_fraction_indels=0.10'. """""". ```. Below ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/602
https://github.com/google/deepvariant/issues/602:1055,availability,cluster,cluster,1055,"DeepVariant via Singularity via Snakemake on a HPC cluster and overall, if I create an interactive session, my entire WES pipeline runs fine until an odd DeepVariant job fails (by that I mean that let's say 30 deepvariant jobs finish ok and the 31st fails). I can then restart the Snakemake run and the same job will run fine, followed by more jobs that will also run fine until another odd jobs fails. This is also particularly true when I use the --cluster command in Snakemake (i.e., send it to a SLURM job scheduler from the master node) - in this event every single job fails. I could of course run all my samples on a single interactive session, keep checking the log file and restart the run every time it fails but I guess that's less than optimal plus this way I can really only run one sample at the time. For the interactive sessions I request 180G and 64cpus (in my case it's: ```srsh --mem=180G --cpus-per-task=64 --partition=long```). . I would request same parameters when using --cluster so:. ```snakemake --cluster ""sbatch --mem=180G cpus-per-task=64"" --jobs 64 --profie profile/ ```(where profile holds singularity args etc.). Singularity image is deepvariant_1.4.0.sif. my Snakemake rule:. ```. rule deepvariant:. input:. bam=rules.apply_bqsr.output.bam,. ref='/mnt/shared/scratch/kmarians/private/dyslexia_gatk/workflow/resources/genome.fasta'. output:. vcf=""results/deepvariant/{sample}.vcf.gz"". params:. model=""WES"". threads: . 64. resources:. mem_mb=163840. log:. ""logs/deepvariant/{sample}/stdout.log"". singularity:. ""singularity/deepvariant_1.4.0.sif"". # ""singularity/deepvariant_1.4.0-gpu.sif"" # for GPU. shell:. """""". /opt/deepvariant/bin/run_deepvariant --model_type {params.model} --ref {input.ref} --reads {input.bam} --output_vcf {output.vcf} --num_shards {threads} --make_examples_extra_args='vsc_min_count_snps=3,vsc_min_fraction_snps=0.2,vsc_min_count_indels=3,vsc_min_fraction_indels=0.10'. """""". ```. Below is the begening and end of the log file. I am happy to incl",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/602
https://github.com/google/deepvariant/issues/602:1083,availability,cluster,cluster,1083,"via Snakemake on a HPC cluster and overall, if I create an interactive session, my entire WES pipeline runs fine until an odd DeepVariant job fails (by that I mean that let's say 30 deepvariant jobs finish ok and the 31st fails). I can then restart the Snakemake run and the same job will run fine, followed by more jobs that will also run fine until another odd jobs fails. This is also particularly true when I use the --cluster command in Snakemake (i.e., send it to a SLURM job scheduler from the master node) - in this event every single job fails. I could of course run all my samples on a single interactive session, keep checking the log file and restart the run every time it fails but I guess that's less than optimal plus this way I can really only run one sample at the time. For the interactive sessions I request 180G and 64cpus (in my case it's: ```srsh --mem=180G --cpus-per-task=64 --partition=long```). . I would request same parameters when using --cluster so:. ```snakemake --cluster ""sbatch --mem=180G cpus-per-task=64"" --jobs 64 --profie profile/ ```(where profile holds singularity args etc.). Singularity image is deepvariant_1.4.0.sif. my Snakemake rule:. ```. rule deepvariant:. input:. bam=rules.apply_bqsr.output.bam,. ref='/mnt/shared/scratch/kmarians/private/dyslexia_gatk/workflow/resources/genome.fasta'. output:. vcf=""results/deepvariant/{sample}.vcf.gz"". params:. model=""WES"". threads: . 64. resources:. mem_mb=163840. log:. ""logs/deepvariant/{sample}/stdout.log"". singularity:. ""singularity/deepvariant_1.4.0.sif"". # ""singularity/deepvariant_1.4.0-gpu.sif"" # for GPU. shell:. """""". /opt/deepvariant/bin/run_deepvariant --model_type {params.model} --ref {input.ref} --reads {input.bam} --output_vcf {output.vcf} --num_shards {threads} --make_examples_extra_args='vsc_min_count_snps=3,vsc_min_fraction_snps=0.2,vsc_min_count_indels=3,vsc_min_fraction_indels=0.10'. """""". ```. Below is the begening and end of the log file. I am happy to include the entire log file but ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/602
https://github.com/google/deepvariant/issues/602:2655,availability,down,down,2655,"ariant_1.4.0-gpu.sif"" # for GPU. shell:. """""". /opt/deepvariant/bin/run_deepvariant --model_type {params.model} --ref {input.ref} --reads {input.bam} --output_vcf {output.vcf} --num_shards {threads} --make_examples_extra_args='vsc_min_count_snps=3,vsc_min_fraction_snps=0.2,vsc_min_count_indels=3,vsc_min_fraction_indels=0.10'. """""". ```. Below is the begening and end of the log file. I am happy to include the entire log file but there is nothing out of the ordinary between those lines below (same output as for jobs that finished successfully). Could you please advise on what parameters to change to successfully run DeepVariant by submitting it to the SLURM scheduler? . I0104 18:49:24.340415 140179943589696 make_examples_core.py:243] Task 13/64: Found 2793 candidate variants. I0104 18:49:24.340478 140179943589696 make_examples_core.py:243] Task 13/64: Created 2879 examples. Building DAG of jobs... Using shell: /usr/bin/bash. Provided cores: 64. Rules claiming more threads will be scaled down. Select jobs to execute... > [Wed Jan 4 18:30:51 2023]. > rule deepvariant:. > input: results/recal/s534_EKDN210017195-1A_HTTJ3DSX2_L2.bam, /mnt/shared/scratch/kmarians/private/dyslexia_gatk/workflow/resources/genome.fasta. > output: results/deepvariant/s534_EKDN210017195-1A_HTTJ3DSX2_L2.vcf.gz. > log: logs/deepvariant/s534_EKDN210017195-1A_HTTJ3DSX2_L2/stdout.log. > jobid: 0. > wildcards: sample=s534_EKDN210017195-1A_HTTJ3DSX2_L2. > threads: 64. > resources: mem_mb=163840, disk_mb=16401, tmpdir=/tmp/kmarians_4189323. > . > Activating singularity image singularity/deepvariant_1.4.0.sif. > INFO: Convert SIF file to sandbox... > I0104 18:31:03.183642 139718628308800 run_deepvariant.py:342] Re-using the directory for intermediate results in /tmp/kmarians_4189323/tmpxrz5rqbp. > . > ***** Intermediate results will be written to /tmp/kmarians_4189323/tmpxrz5rqbp in docker. ****. > . > . > ***** Running the command:*****. > time seq 0 63 | parallel -q --halt 2 --line-buffer /opt/deepvarian",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/602
https://github.com/google/deepvariant/issues/602:4595,availability,checkpoint,checkpoint,4595,"0 63 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/mnt/shared/scratch/kmarians/private/dyslexia_gatk/workflow/resources/genome.fasta"" --reads ""results/recal/s534_EKDN210017195-1A_HTTJ3DSX2_L2.bam"" --examples ""/tmp/kmarians_4189323/tmpxrz5rqbp/make_examples.tfrecord@64.gz"" --channels ""insert_size"" --vsc_min_count_indels ""3"" --vsc_min_count_snps ""3"" --vsc_min_fraction_indels ""0.10"" --vsc_min_fraction_snps ""0.2"" --task {}. > *. > *. > *. > I0104 18:49:24.340415 140179943589696 make_examples_core.py:243] Task 13/64: Found 2793 candidate variants. > I0104 18:49:24.340478 140179943589696 make_examples_core.py:243] Task 13/64: Created 2879 examples. > real	18m21.465s. > user	893m16.250s. > sys	15m57.561s. > . > ***** Running the command:*****. > time /opt/deepvariant/bin/call_variants --outfile ""/tmp/kmarians_4189323/tmpxrz5rqbp/call_variants_output.tfrecord.gz"" --examples ""/tmp/kmarians_4189323/tmpxrz5rqbp/make_examples.tfrecord@64.gz"" --checkpoint ""/opt/models/wes/model.ckpt"" --openvino_model_dir ""/tmp/kmarians_4189323/tmpxrz5rqbp"". > . > I0104 18:49:26.695084 139646404110144 call_variants.py:317] From /tmp/kmarians_4189323/tmpxrz5rqbp/make_examples.tfrecord-00000-of-00064.gz.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 19]. > I0104 18:49:26.697470 139646404110144 call_variants.py:317] From /opt/models/wes/model.ckpt.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 19]. > Traceback (most recent call last):. > File ""/tmp/kmarians_4189323/Bazel.runfiles_zims94rl/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 513, in <module>. > tf.compat.v1.app.run(). > File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. > _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). > File ""/tmp/kmarians_4189323/Bazel.runfiles_zims94rl/runfiles/absl_p",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/602
https://github.com/google/deepvariant/issues/602:7393,availability,avail,available,7393,"s). > File ""/tmp/kmarians_4189323/Bazel.runfiles_zims94rl/runfiles/absl_py/absl/app.py"", line 251, in _run_main. > sys.exit(main(argv)). > File ""/tmp/kmarians_4189323/Bazel.runfiles_zims94rl/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 494, in main. > call_variants(. > File ""/tmp/kmarians_4189323/Bazel.runfiles_zims94rl/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 389, in call_variants. > init_op = tf.group(tf.compat.v1.global_variables_initializer(),. > File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/ops/variables.py"", line 3319, in global_variables_initializer. > return variables_initializer(global_variables()). > File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/ops/variables.py"", line 3139, in global_variables. > return ops.get_collection(ops.GraphKeys.GLOBAL_VARIABLES, scope). > File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py"", line 6605, in get_collection. > return get_default_graph().get_collection(key, scope). > File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py"", line 6231, in get_default_graph. > return _default_graph_stack.get_default(). > File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py"", line 5742, in get_default. > self._global_default_graph = Graph(). > File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py"", line 3113, in __init__. > self._scoped_c_graph = c_api_util.ScopedTFGraph(). > File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/c_api_util.py"", line 50, in __init__. > self.graph = c_api.TF_NewGraph(). > RuntimeError: random_device::random_device(const std::string&): device not available. > Exception ignored in: <function ScopedTFGraph.__del__ at 0x7f01d64378b0>. > Traceback (most recent call last):. > File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/c_api_util.py"", line 58, in __del__. > AttributeError: deleter",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/602
https://github.com/google/deepvariant/issues/602:110,deployability,cluster,cluster,110,"AttributeError: deleter; Hi DeepVariant team,. I'm running DeepVariant via Singularity via Snakemake on a HPC cluster and overall, if I create an interactive session, my entire WES pipeline runs fine until an odd DeepVariant job fails (by that I mean that let's say 30 deepvariant jobs finish ok and the 31st fails). I can then restart the Snakemake run and the same job will run fine, followed by more jobs that will also run fine until another odd jobs fails. This is also particularly true when I use the --cluster command in Snakemake (i.e., send it to a SLURM job scheduler from the master node) - in this event every single job fails. I could of course run all my samples on a single interactive session, keep checking the log file and restart the run every time it fails but I guess that's less than optimal plus this way I can really only run one sample at the time. For the interactive sessions I request 180G and 64cpus (in my case it's: ```srsh --mem=180G --cpus-per-task=64 --partition=long```). . I would request same parameters when using --cluster so:. ```snakemake --cluster ""sbatch --mem=180G cpus-per-task=64"" --jobs 64 --profie profile/ ```(where profile holds singularity args etc.). Singularity image is deepvariant_1.4.0.sif. my Snakemake rule:. ```. rule deepvariant:. input:. bam=rules.apply_bqsr.output.bam,. ref='/mnt/shared/scratch/kmarians/private/dyslexia_gatk/workflow/resources/genome.fasta'. output:. vcf=""results/deepvariant/{sample}.vcf.gz"". params:. model=""WES"". threads: . 64. resources:. mem_mb=163840. log:. ""logs/deepvariant/{sample}/stdout.log"". singularity:. ""singularity/deepvariant_1.4.0.sif"". # ""singularity/deepvariant_1.4.0-gpu.sif"" # for GPU. shell:. """""". /opt/deepvariant/bin/run_deepvariant --model_type {params.model} --ref {input.ref} --reads {input.bam} --output_vcf {output.vcf} --num_shards {threads} --make_examples_extra_args='vsc_min_count_snps=3,vsc_min_fraction_snps=0.2,vsc_min_count_indels=3,vsc_min_fraction_indels=0.10'. """""". ```. Below ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/602
https://github.com/google/deepvariant/issues/602:181,deployability,pipelin,pipeline,181,"AttributeError: deleter; Hi DeepVariant team,. I'm running DeepVariant via Singularity via Snakemake on a HPC cluster and overall, if I create an interactive session, my entire WES pipeline runs fine until an odd DeepVariant job fails (by that I mean that let's say 30 deepvariant jobs finish ok and the 31st fails). I can then restart the Snakemake run and the same job will run fine, followed by more jobs that will also run fine until another odd jobs fails. This is also particularly true when I use the --cluster command in Snakemake (i.e., send it to a SLURM job scheduler from the master node) - in this event every single job fails. I could of course run all my samples on a single interactive session, keep checking the log file and restart the run every time it fails but I guess that's less than optimal plus this way I can really only run one sample at the time. For the interactive sessions I request 180G and 64cpus (in my case it's: ```srsh --mem=180G --cpus-per-task=64 --partition=long```). . I would request same parameters when using --cluster so:. ```snakemake --cluster ""sbatch --mem=180G cpus-per-task=64"" --jobs 64 --profie profile/ ```(where profile holds singularity args etc.). Singularity image is deepvariant_1.4.0.sif. my Snakemake rule:. ```. rule deepvariant:. input:. bam=rules.apply_bqsr.output.bam,. ref='/mnt/shared/scratch/kmarians/private/dyslexia_gatk/workflow/resources/genome.fasta'. output:. vcf=""results/deepvariant/{sample}.vcf.gz"". params:. model=""WES"". threads: . 64. resources:. mem_mb=163840. log:. ""logs/deepvariant/{sample}/stdout.log"". singularity:. ""singularity/deepvariant_1.4.0.sif"". # ""singularity/deepvariant_1.4.0-gpu.sif"" # for GPU. shell:. """""". /opt/deepvariant/bin/run_deepvariant --model_type {params.model} --ref {input.ref} --reads {input.bam} --output_vcf {output.vcf} --num_shards {threads} --make_examples_extra_args='vsc_min_count_snps=3,vsc_min_fraction_snps=0.2,vsc_min_count_indels=3,vsc_min_fraction_indels=0.10'. """""". ```. Below ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/602
https://github.com/google/deepvariant/issues/602:229,deployability,fail,fails,229,"AttributeError: deleter; Hi DeepVariant team,. I'm running DeepVariant via Singularity via Snakemake on a HPC cluster and overall, if I create an interactive session, my entire WES pipeline runs fine until an odd DeepVariant job fails (by that I mean that let's say 30 deepvariant jobs finish ok and the 31st fails). I can then restart the Snakemake run and the same job will run fine, followed by more jobs that will also run fine until another odd jobs fails. This is also particularly true when I use the --cluster command in Snakemake (i.e., send it to a SLURM job scheduler from the master node) - in this event every single job fails. I could of course run all my samples on a single interactive session, keep checking the log file and restart the run every time it fails but I guess that's less than optimal plus this way I can really only run one sample at the time. For the interactive sessions I request 180G and 64cpus (in my case it's: ```srsh --mem=180G --cpus-per-task=64 --partition=long```). . I would request same parameters when using --cluster so:. ```snakemake --cluster ""sbatch --mem=180G cpus-per-task=64"" --jobs 64 --profie profile/ ```(where profile holds singularity args etc.). Singularity image is deepvariant_1.4.0.sif. my Snakemake rule:. ```. rule deepvariant:. input:. bam=rules.apply_bqsr.output.bam,. ref='/mnt/shared/scratch/kmarians/private/dyslexia_gatk/workflow/resources/genome.fasta'. output:. vcf=""results/deepvariant/{sample}.vcf.gz"". params:. model=""WES"". threads: . 64. resources:. mem_mb=163840. log:. ""logs/deepvariant/{sample}/stdout.log"". singularity:. ""singularity/deepvariant_1.4.0.sif"". # ""singularity/deepvariant_1.4.0-gpu.sif"" # for GPU. shell:. """""". /opt/deepvariant/bin/run_deepvariant --model_type {params.model} --ref {input.ref} --reads {input.bam} --output_vcf {output.vcf} --num_shards {threads} --make_examples_extra_args='vsc_min_count_snps=3,vsc_min_fraction_snps=0.2,vsc_min_count_indels=3,vsc_min_fraction_indels=0.10'. """""". ```. Below ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/602
https://github.com/google/deepvariant/issues/602:309,deployability,fail,fails,309,"AttributeError: deleter; Hi DeepVariant team,. I'm running DeepVariant via Singularity via Snakemake on a HPC cluster and overall, if I create an interactive session, my entire WES pipeline runs fine until an odd DeepVariant job fails (by that I mean that let's say 30 deepvariant jobs finish ok and the 31st fails). I can then restart the Snakemake run and the same job will run fine, followed by more jobs that will also run fine until another odd jobs fails. This is also particularly true when I use the --cluster command in Snakemake (i.e., send it to a SLURM job scheduler from the master node) - in this event every single job fails. I could of course run all my samples on a single interactive session, keep checking the log file and restart the run every time it fails but I guess that's less than optimal plus this way I can really only run one sample at the time. For the interactive sessions I request 180G and 64cpus (in my case it's: ```srsh --mem=180G --cpus-per-task=64 --partition=long```). . I would request same parameters when using --cluster so:. ```snakemake --cluster ""sbatch --mem=180G cpus-per-task=64"" --jobs 64 --profie profile/ ```(where profile holds singularity args etc.). Singularity image is deepvariant_1.4.0.sif. my Snakemake rule:. ```. rule deepvariant:. input:. bam=rules.apply_bqsr.output.bam,. ref='/mnt/shared/scratch/kmarians/private/dyslexia_gatk/workflow/resources/genome.fasta'. output:. vcf=""results/deepvariant/{sample}.vcf.gz"". params:. model=""WES"". threads: . 64. resources:. mem_mb=163840. log:. ""logs/deepvariant/{sample}/stdout.log"". singularity:. ""singularity/deepvariant_1.4.0.sif"". # ""singularity/deepvariant_1.4.0-gpu.sif"" # for GPU. shell:. """""". /opt/deepvariant/bin/run_deepvariant --model_type {params.model} --ref {input.ref} --reads {input.bam} --output_vcf {output.vcf} --num_shards {threads} --make_examples_extra_args='vsc_min_count_snps=3,vsc_min_fraction_snps=0.2,vsc_min_count_indels=3,vsc_min_fraction_indels=0.10'. """""". ```. Below ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/602
https://github.com/google/deepvariant/issues/602:455,deployability,fail,fails,455,"AttributeError: deleter; Hi DeepVariant team,. I'm running DeepVariant via Singularity via Snakemake on a HPC cluster and overall, if I create an interactive session, my entire WES pipeline runs fine until an odd DeepVariant job fails (by that I mean that let's say 30 deepvariant jobs finish ok and the 31st fails). I can then restart the Snakemake run and the same job will run fine, followed by more jobs that will also run fine until another odd jobs fails. This is also particularly true when I use the --cluster command in Snakemake (i.e., send it to a SLURM job scheduler from the master node) - in this event every single job fails. I could of course run all my samples on a single interactive session, keep checking the log file and restart the run every time it fails but I guess that's less than optimal plus this way I can really only run one sample at the time. For the interactive sessions I request 180G and 64cpus (in my case it's: ```srsh --mem=180G --cpus-per-task=64 --partition=long```). . I would request same parameters when using --cluster so:. ```snakemake --cluster ""sbatch --mem=180G cpus-per-task=64"" --jobs 64 --profie profile/ ```(where profile holds singularity args etc.). Singularity image is deepvariant_1.4.0.sif. my Snakemake rule:. ```. rule deepvariant:. input:. bam=rules.apply_bqsr.output.bam,. ref='/mnt/shared/scratch/kmarians/private/dyslexia_gatk/workflow/resources/genome.fasta'. output:. vcf=""results/deepvariant/{sample}.vcf.gz"". params:. model=""WES"". threads: . 64. resources:. mem_mb=163840. log:. ""logs/deepvariant/{sample}/stdout.log"". singularity:. ""singularity/deepvariant_1.4.0.sif"". # ""singularity/deepvariant_1.4.0-gpu.sif"" # for GPU. shell:. """""". /opt/deepvariant/bin/run_deepvariant --model_type {params.model} --ref {input.ref} --reads {input.bam} --output_vcf {output.vcf} --num_shards {threads} --make_examples_extra_args='vsc_min_count_snps=3,vsc_min_fraction_snps=0.2,vsc_min_count_indels=3,vsc_min_fraction_indels=0.10'. """""". ```. Below ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/602
https://github.com/google/deepvariant/issues/602:510,deployability,cluster,cluster,510,"AttributeError: deleter; Hi DeepVariant team,. I'm running DeepVariant via Singularity via Snakemake on a HPC cluster and overall, if I create an interactive session, my entire WES pipeline runs fine until an odd DeepVariant job fails (by that I mean that let's say 30 deepvariant jobs finish ok and the 31st fails). I can then restart the Snakemake run and the same job will run fine, followed by more jobs that will also run fine until another odd jobs fails. This is also particularly true when I use the --cluster command in Snakemake (i.e., send it to a SLURM job scheduler from the master node) - in this event every single job fails. I could of course run all my samples on a single interactive session, keep checking the log file and restart the run every time it fails but I guess that's less than optimal plus this way I can really only run one sample at the time. For the interactive sessions I request 180G and 64cpus (in my case it's: ```srsh --mem=180G --cpus-per-task=64 --partition=long```). . I would request same parameters when using --cluster so:. ```snakemake --cluster ""sbatch --mem=180G cpus-per-task=64"" --jobs 64 --profie profile/ ```(where profile holds singularity args etc.). Singularity image is deepvariant_1.4.0.sif. my Snakemake rule:. ```. rule deepvariant:. input:. bam=rules.apply_bqsr.output.bam,. ref='/mnt/shared/scratch/kmarians/private/dyslexia_gatk/workflow/resources/genome.fasta'. output:. vcf=""results/deepvariant/{sample}.vcf.gz"". params:. model=""WES"". threads: . 64. resources:. mem_mb=163840. log:. ""logs/deepvariant/{sample}/stdout.log"". singularity:. ""singularity/deepvariant_1.4.0.sif"". # ""singularity/deepvariant_1.4.0-gpu.sif"" # for GPU. shell:. """""". /opt/deepvariant/bin/run_deepvariant --model_type {params.model} --ref {input.ref} --reads {input.bam} --output_vcf {output.vcf} --num_shards {threads} --make_examples_extra_args='vsc_min_count_snps=3,vsc_min_fraction_snps=0.2,vsc_min_count_indels=3,vsc_min_fraction_indels=0.10'. """""". ```. Below ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/602
https://github.com/google/deepvariant/issues/602:634,deployability,fail,fails,634,"AttributeError: deleter; Hi DeepVariant team,. I'm running DeepVariant via Singularity via Snakemake on a HPC cluster and overall, if I create an interactive session, my entire WES pipeline runs fine until an odd DeepVariant job fails (by that I mean that let's say 30 deepvariant jobs finish ok and the 31st fails). I can then restart the Snakemake run and the same job will run fine, followed by more jobs that will also run fine until another odd jobs fails. This is also particularly true when I use the --cluster command in Snakemake (i.e., send it to a SLURM job scheduler from the master node) - in this event every single job fails. I could of course run all my samples on a single interactive session, keep checking the log file and restart the run every time it fails but I guess that's less than optimal plus this way I can really only run one sample at the time. For the interactive sessions I request 180G and 64cpus (in my case it's: ```srsh --mem=180G --cpus-per-task=64 --partition=long```). . I would request same parameters when using --cluster so:. ```snakemake --cluster ""sbatch --mem=180G cpus-per-task=64"" --jobs 64 --profie profile/ ```(where profile holds singularity args etc.). Singularity image is deepvariant_1.4.0.sif. my Snakemake rule:. ```. rule deepvariant:. input:. bam=rules.apply_bqsr.output.bam,. ref='/mnt/shared/scratch/kmarians/private/dyslexia_gatk/workflow/resources/genome.fasta'. output:. vcf=""results/deepvariant/{sample}.vcf.gz"". params:. model=""WES"". threads: . 64. resources:. mem_mb=163840. log:. ""logs/deepvariant/{sample}/stdout.log"". singularity:. ""singularity/deepvariant_1.4.0.sif"". # ""singularity/deepvariant_1.4.0-gpu.sif"" # for GPU. shell:. """""". /opt/deepvariant/bin/run_deepvariant --model_type {params.model} --ref {input.ref} --reads {input.bam} --output_vcf {output.vcf} --num_shards {threads} --make_examples_extra_args='vsc_min_count_snps=3,vsc_min_fraction_snps=0.2,vsc_min_count_indels=3,vsc_min_fraction_indels=0.10'. """""". ```. Below ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/602
https://github.com/google/deepvariant/issues/602:729,deployability,log,log,729,"AttributeError: deleter; Hi DeepVariant team,. I'm running DeepVariant via Singularity via Snakemake on a HPC cluster and overall, if I create an interactive session, my entire WES pipeline runs fine until an odd DeepVariant job fails (by that I mean that let's say 30 deepvariant jobs finish ok and the 31st fails). I can then restart the Snakemake run and the same job will run fine, followed by more jobs that will also run fine until another odd jobs fails. This is also particularly true when I use the --cluster command in Snakemake (i.e., send it to a SLURM job scheduler from the master node) - in this event every single job fails. I could of course run all my samples on a single interactive session, keep checking the log file and restart the run every time it fails but I guess that's less than optimal plus this way I can really only run one sample at the time. For the interactive sessions I request 180G and 64cpus (in my case it's: ```srsh --mem=180G --cpus-per-task=64 --partition=long```). . I would request same parameters when using --cluster so:. ```snakemake --cluster ""sbatch --mem=180G cpus-per-task=64"" --jobs 64 --profie profile/ ```(where profile holds singularity args etc.). Singularity image is deepvariant_1.4.0.sif. my Snakemake rule:. ```. rule deepvariant:. input:. bam=rules.apply_bqsr.output.bam,. ref='/mnt/shared/scratch/kmarians/private/dyslexia_gatk/workflow/resources/genome.fasta'. output:. vcf=""results/deepvariant/{sample}.vcf.gz"". params:. model=""WES"". threads: . 64. resources:. mem_mb=163840. log:. ""logs/deepvariant/{sample}/stdout.log"". singularity:. ""singularity/deepvariant_1.4.0.sif"". # ""singularity/deepvariant_1.4.0-gpu.sif"" # for GPU. shell:. """""". /opt/deepvariant/bin/run_deepvariant --model_type {params.model} --ref {input.ref} --reads {input.bam} --output_vcf {output.vcf} --num_shards {threads} --make_examples_extra_args='vsc_min_count_snps=3,vsc_min_fraction_snps=0.2,vsc_min_count_indels=3,vsc_min_fraction_indels=0.10'. """""". ```. Below ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/602
https://github.com/google/deepvariant/issues/602:772,deployability,fail,fails,772,"AttributeError: deleter; Hi DeepVariant team,. I'm running DeepVariant via Singularity via Snakemake on a HPC cluster and overall, if I create an interactive session, my entire WES pipeline runs fine until an odd DeepVariant job fails (by that I mean that let's say 30 deepvariant jobs finish ok and the 31st fails). I can then restart the Snakemake run and the same job will run fine, followed by more jobs that will also run fine until another odd jobs fails. This is also particularly true when I use the --cluster command in Snakemake (i.e., send it to a SLURM job scheduler from the master node) - in this event every single job fails. I could of course run all my samples on a single interactive session, keep checking the log file and restart the run every time it fails but I guess that's less than optimal plus this way I can really only run one sample at the time. For the interactive sessions I request 180G and 64cpus (in my case it's: ```srsh --mem=180G --cpus-per-task=64 --partition=long```). . I would request same parameters when using --cluster so:. ```snakemake --cluster ""sbatch --mem=180G cpus-per-task=64"" --jobs 64 --profie profile/ ```(where profile holds singularity args etc.). Singularity image is deepvariant_1.4.0.sif. my Snakemake rule:. ```. rule deepvariant:. input:. bam=rules.apply_bqsr.output.bam,. ref='/mnt/shared/scratch/kmarians/private/dyslexia_gatk/workflow/resources/genome.fasta'. output:. vcf=""results/deepvariant/{sample}.vcf.gz"". params:. model=""WES"". threads: . 64. resources:. mem_mb=163840. log:. ""logs/deepvariant/{sample}/stdout.log"". singularity:. ""singularity/deepvariant_1.4.0.sif"". # ""singularity/deepvariant_1.4.0-gpu.sif"" # for GPU. shell:. """""". /opt/deepvariant/bin/run_deepvariant --model_type {params.model} --ref {input.ref} --reads {input.bam} --output_vcf {output.vcf} --num_shards {threads} --make_examples_extra_args='vsc_min_count_snps=3,vsc_min_fraction_snps=0.2,vsc_min_count_indels=3,vsc_min_fraction_indels=0.10'. """""". ```. Below ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/602
https://github.com/google/deepvariant/issues/602:1055,deployability,cluster,cluster,1055,"DeepVariant via Singularity via Snakemake on a HPC cluster and overall, if I create an interactive session, my entire WES pipeline runs fine until an odd DeepVariant job fails (by that I mean that let's say 30 deepvariant jobs finish ok and the 31st fails). I can then restart the Snakemake run and the same job will run fine, followed by more jobs that will also run fine until another odd jobs fails. This is also particularly true when I use the --cluster command in Snakemake (i.e., send it to a SLURM job scheduler from the master node) - in this event every single job fails. I could of course run all my samples on a single interactive session, keep checking the log file and restart the run every time it fails but I guess that's less than optimal plus this way I can really only run one sample at the time. For the interactive sessions I request 180G and 64cpus (in my case it's: ```srsh --mem=180G --cpus-per-task=64 --partition=long```). . I would request same parameters when using --cluster so:. ```snakemake --cluster ""sbatch --mem=180G cpus-per-task=64"" --jobs 64 --profie profile/ ```(where profile holds singularity args etc.). Singularity image is deepvariant_1.4.0.sif. my Snakemake rule:. ```. rule deepvariant:. input:. bam=rules.apply_bqsr.output.bam,. ref='/mnt/shared/scratch/kmarians/private/dyslexia_gatk/workflow/resources/genome.fasta'. output:. vcf=""results/deepvariant/{sample}.vcf.gz"". params:. model=""WES"". threads: . 64. resources:. mem_mb=163840. log:. ""logs/deepvariant/{sample}/stdout.log"". singularity:. ""singularity/deepvariant_1.4.0.sif"". # ""singularity/deepvariant_1.4.0-gpu.sif"" # for GPU. shell:. """""". /opt/deepvariant/bin/run_deepvariant --model_type {params.model} --ref {input.ref} --reads {input.bam} --output_vcf {output.vcf} --num_shards {threads} --make_examples_extra_args='vsc_min_count_snps=3,vsc_min_fraction_snps=0.2,vsc_min_count_indels=3,vsc_min_fraction_indels=0.10'. """""". ```. Below is the begening and end of the log file. I am happy to incl",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/602
https://github.com/google/deepvariant/issues/602:1083,deployability,cluster,cluster,1083,"via Snakemake on a HPC cluster and overall, if I create an interactive session, my entire WES pipeline runs fine until an odd DeepVariant job fails (by that I mean that let's say 30 deepvariant jobs finish ok and the 31st fails). I can then restart the Snakemake run and the same job will run fine, followed by more jobs that will also run fine until another odd jobs fails. This is also particularly true when I use the --cluster command in Snakemake (i.e., send it to a SLURM job scheduler from the master node) - in this event every single job fails. I could of course run all my samples on a single interactive session, keep checking the log file and restart the run every time it fails but I guess that's less than optimal plus this way I can really only run one sample at the time. For the interactive sessions I request 180G and 64cpus (in my case it's: ```srsh --mem=180G --cpus-per-task=64 --partition=long```). . I would request same parameters when using --cluster so:. ```snakemake --cluster ""sbatch --mem=180G cpus-per-task=64"" --jobs 64 --profie profile/ ```(where profile holds singularity args etc.). Singularity image is deepvariant_1.4.0.sif. my Snakemake rule:. ```. rule deepvariant:. input:. bam=rules.apply_bqsr.output.bam,. ref='/mnt/shared/scratch/kmarians/private/dyslexia_gatk/workflow/resources/genome.fasta'. output:. vcf=""results/deepvariant/{sample}.vcf.gz"". params:. model=""WES"". threads: . 64. resources:. mem_mb=163840. log:. ""logs/deepvariant/{sample}/stdout.log"". singularity:. ""singularity/deepvariant_1.4.0.sif"". # ""singularity/deepvariant_1.4.0-gpu.sif"" # for GPU. shell:. """""". /opt/deepvariant/bin/run_deepvariant --model_type {params.model} --ref {input.ref} --reads {input.bam} --output_vcf {output.vcf} --num_shards {threads} --make_examples_extra_args='vsc_min_count_snps=3,vsc_min_fraction_snps=0.2,vsc_min_count_indels=3,vsc_min_fraction_indels=0.10'. """""". ```. Below is the begening and end of the log file. I am happy to include the entire log file but ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/602
https://github.com/google/deepvariant/issues/602:1399,deployability,resourc,resources,1399,"obs that will also run fine until another odd jobs fails. This is also particularly true when I use the --cluster command in Snakemake (i.e., send it to a SLURM job scheduler from the master node) - in this event every single job fails. I could of course run all my samples on a single interactive session, keep checking the log file and restart the run every time it fails but I guess that's less than optimal plus this way I can really only run one sample at the time. For the interactive sessions I request 180G and 64cpus (in my case it's: ```srsh --mem=180G --cpus-per-task=64 --partition=long```). . I would request same parameters when using --cluster so:. ```snakemake --cluster ""sbatch --mem=180G cpus-per-task=64"" --jobs 64 --profie profile/ ```(where profile holds singularity args etc.). Singularity image is deepvariant_1.4.0.sif. my Snakemake rule:. ```. rule deepvariant:. input:. bam=rules.apply_bqsr.output.bam,. ref='/mnt/shared/scratch/kmarians/private/dyslexia_gatk/workflow/resources/genome.fasta'. output:. vcf=""results/deepvariant/{sample}.vcf.gz"". params:. model=""WES"". threads: . 64. resources:. mem_mb=163840. log:. ""logs/deepvariant/{sample}/stdout.log"". singularity:. ""singularity/deepvariant_1.4.0.sif"". # ""singularity/deepvariant_1.4.0-gpu.sif"" # for GPU. shell:. """""". /opt/deepvariant/bin/run_deepvariant --model_type {params.model} --ref {input.ref} --reads {input.bam} --output_vcf {output.vcf} --num_shards {threads} --make_examples_extra_args='vsc_min_count_snps=3,vsc_min_fraction_snps=0.2,vsc_min_count_indels=3,vsc_min_fraction_indels=0.10'. """""". ```. Below is the begening and end of the log file. I am happy to include the entire log file but there is nothing out of the ordinary between those lines below (same output as for jobs that finished successfully). Could you please advise on what parameters to change to successfully run DeepVariant by submitting it to the SLURM scheduler? . I0104 18:49:24.340415 140179943589696 make_examples_core.py:243] Task 13",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/602
https://github.com/google/deepvariant/issues/602:1513,deployability,resourc,resources,1513,"command in Snakemake (i.e., send it to a SLURM job scheduler from the master node) - in this event every single job fails. I could of course run all my samples on a single interactive session, keep checking the log file and restart the run every time it fails but I guess that's less than optimal plus this way I can really only run one sample at the time. For the interactive sessions I request 180G and 64cpus (in my case it's: ```srsh --mem=180G --cpus-per-task=64 --partition=long```). . I would request same parameters when using --cluster so:. ```snakemake --cluster ""sbatch --mem=180G cpus-per-task=64"" --jobs 64 --profie profile/ ```(where profile holds singularity args etc.). Singularity image is deepvariant_1.4.0.sif. my Snakemake rule:. ```. rule deepvariant:. input:. bam=rules.apply_bqsr.output.bam,. ref='/mnt/shared/scratch/kmarians/private/dyslexia_gatk/workflow/resources/genome.fasta'. output:. vcf=""results/deepvariant/{sample}.vcf.gz"". params:. model=""WES"". threads: . 64. resources:. mem_mb=163840. log:. ""logs/deepvariant/{sample}/stdout.log"". singularity:. ""singularity/deepvariant_1.4.0.sif"". # ""singularity/deepvariant_1.4.0-gpu.sif"" # for GPU. shell:. """""". /opt/deepvariant/bin/run_deepvariant --model_type {params.model} --ref {input.ref} --reads {input.bam} --output_vcf {output.vcf} --num_shards {threads} --make_examples_extra_args='vsc_min_count_snps=3,vsc_min_fraction_snps=0.2,vsc_min_count_indels=3,vsc_min_fraction_indels=0.10'. """""". ```. Below is the begening and end of the log file. I am happy to include the entire log file but there is nothing out of the ordinary between those lines below (same output as for jobs that finished successfully). Could you please advise on what parameters to change to successfully run DeepVariant by submitting it to the SLURM scheduler? . I0104 18:49:24.340415 140179943589696 make_examples_core.py:243] Task 13/64: Found 2793 candidate variants. I0104 18:49:24.340478 140179943589696 make_examples_core.py:243] Task 13/64: C",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/602
https://github.com/google/deepvariant/issues/602:1540,deployability,log,log,1540,"e., send it to a SLURM job scheduler from the master node) - in this event every single job fails. I could of course run all my samples on a single interactive session, keep checking the log file and restart the run every time it fails but I guess that's less than optimal plus this way I can really only run one sample at the time. For the interactive sessions I request 180G and 64cpus (in my case it's: ```srsh --mem=180G --cpus-per-task=64 --partition=long```). . I would request same parameters when using --cluster so:. ```snakemake --cluster ""sbatch --mem=180G cpus-per-task=64"" --jobs 64 --profie profile/ ```(where profile holds singularity args etc.). Singularity image is deepvariant_1.4.0.sif. my Snakemake rule:. ```. rule deepvariant:. input:. bam=rules.apply_bqsr.output.bam,. ref='/mnt/shared/scratch/kmarians/private/dyslexia_gatk/workflow/resources/genome.fasta'. output:. vcf=""results/deepvariant/{sample}.vcf.gz"". params:. model=""WES"". threads: . 64. resources:. mem_mb=163840. log:. ""logs/deepvariant/{sample}/stdout.log"". singularity:. ""singularity/deepvariant_1.4.0.sif"". # ""singularity/deepvariant_1.4.0-gpu.sif"" # for GPU. shell:. """""". /opt/deepvariant/bin/run_deepvariant --model_type {params.model} --ref {input.ref} --reads {input.bam} --output_vcf {output.vcf} --num_shards {threads} --make_examples_extra_args='vsc_min_count_snps=3,vsc_min_fraction_snps=0.2,vsc_min_count_indels=3,vsc_min_fraction_indels=0.10'. """""". ```. Below is the begening and end of the log file. I am happy to include the entire log file but there is nothing out of the ordinary between those lines below (same output as for jobs that finished successfully). Could you please advise on what parameters to change to successfully run DeepVariant by submitting it to the SLURM scheduler? . I0104 18:49:24.340415 140179943589696 make_examples_core.py:243] Task 13/64: Found 2793 candidate variants. I0104 18:49:24.340478 140179943589696 make_examples_core.py:243] Task 13/64: Created 2879 examples. Bu",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/602
https://github.com/google/deepvariant/issues/602:1547,deployability,log,logs,1547,"d it to a SLURM job scheduler from the master node) - in this event every single job fails. I could of course run all my samples on a single interactive session, keep checking the log file and restart the run every time it fails but I guess that's less than optimal plus this way I can really only run one sample at the time. For the interactive sessions I request 180G and 64cpus (in my case it's: ```srsh --mem=180G --cpus-per-task=64 --partition=long```). . I would request same parameters when using --cluster so:. ```snakemake --cluster ""sbatch --mem=180G cpus-per-task=64"" --jobs 64 --profie profile/ ```(where profile holds singularity args etc.). Singularity image is deepvariant_1.4.0.sif. my Snakemake rule:. ```. rule deepvariant:. input:. bam=rules.apply_bqsr.output.bam,. ref='/mnt/shared/scratch/kmarians/private/dyslexia_gatk/workflow/resources/genome.fasta'. output:. vcf=""results/deepvariant/{sample}.vcf.gz"". params:. model=""WES"". threads: . 64. resources:. mem_mb=163840. log:. ""logs/deepvariant/{sample}/stdout.log"". singularity:. ""singularity/deepvariant_1.4.0.sif"". # ""singularity/deepvariant_1.4.0-gpu.sif"" # for GPU. shell:. """""". /opt/deepvariant/bin/run_deepvariant --model_type {params.model} --ref {input.ref} --reads {input.bam} --output_vcf {output.vcf} --num_shards {threads} --make_examples_extra_args='vsc_min_count_snps=3,vsc_min_fraction_snps=0.2,vsc_min_count_indels=3,vsc_min_fraction_indels=0.10'. """""". ```. Below is the begening and end of the log file. I am happy to include the entire log file but there is nothing out of the ordinary between those lines below (same output as for jobs that finished successfully). Could you please advise on what parameters to change to successfully run DeepVariant by submitting it to the SLURM scheduler? . I0104 18:49:24.340415 140179943589696 make_examples_core.py:243] Task 13/64: Found 2793 candidate variants. I0104 18:49:24.340478 140179943589696 make_examples_core.py:243] Task 13/64: Created 2879 examples. Building ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/602
https://github.com/google/deepvariant/issues/602:1580,deployability,log,log,1580,"m the master node) - in this event every single job fails. I could of course run all my samples on a single interactive session, keep checking the log file and restart the run every time it fails but I guess that's less than optimal plus this way I can really only run one sample at the time. For the interactive sessions I request 180G and 64cpus (in my case it's: ```srsh --mem=180G --cpus-per-task=64 --partition=long```). . I would request same parameters when using --cluster so:. ```snakemake --cluster ""sbatch --mem=180G cpus-per-task=64"" --jobs 64 --profie profile/ ```(where profile holds singularity args etc.). Singularity image is deepvariant_1.4.0.sif. my Snakemake rule:. ```. rule deepvariant:. input:. bam=rules.apply_bqsr.output.bam,. ref='/mnt/shared/scratch/kmarians/private/dyslexia_gatk/workflow/resources/genome.fasta'. output:. vcf=""results/deepvariant/{sample}.vcf.gz"". params:. model=""WES"". threads: . 64. resources:. mem_mb=163840. log:. ""logs/deepvariant/{sample}/stdout.log"". singularity:. ""singularity/deepvariant_1.4.0.sif"". # ""singularity/deepvariant_1.4.0-gpu.sif"" # for GPU. shell:. """""". /opt/deepvariant/bin/run_deepvariant --model_type {params.model} --ref {input.ref} --reads {input.bam} --output_vcf {output.vcf} --num_shards {threads} --make_examples_extra_args='vsc_min_count_snps=3,vsc_min_fraction_snps=0.2,vsc_min_count_indels=3,vsc_min_fraction_indels=0.10'. """""". ```. Below is the begening and end of the log file. I am happy to include the entire log file but there is nothing out of the ordinary between those lines below (same output as for jobs that finished successfully). Could you please advise on what parameters to change to successfully run DeepVariant by submitting it to the SLURM scheduler? . I0104 18:49:24.340415 140179943589696 make_examples_core.py:243] Task 13/64: Found 2793 candidate variants. I0104 18:49:24.340478 140179943589696 make_examples_core.py:243] Task 13/64: Created 2879 examples. Building DAG of jobs... Using shell: /usr/",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/602
https://github.com/google/deepvariant/issues/602:2031,deployability,log,log,2031,"rameters when using --cluster so:. ```snakemake --cluster ""sbatch --mem=180G cpus-per-task=64"" --jobs 64 --profie profile/ ```(where profile holds singularity args etc.). Singularity image is deepvariant_1.4.0.sif. my Snakemake rule:. ```. rule deepvariant:. input:. bam=rules.apply_bqsr.output.bam,. ref='/mnt/shared/scratch/kmarians/private/dyslexia_gatk/workflow/resources/genome.fasta'. output:. vcf=""results/deepvariant/{sample}.vcf.gz"". params:. model=""WES"". threads: . 64. resources:. mem_mb=163840. log:. ""logs/deepvariant/{sample}/stdout.log"". singularity:. ""singularity/deepvariant_1.4.0.sif"". # ""singularity/deepvariant_1.4.0-gpu.sif"" # for GPU. shell:. """""". /opt/deepvariant/bin/run_deepvariant --model_type {params.model} --ref {input.ref} --reads {input.bam} --output_vcf {output.vcf} --num_shards {threads} --make_examples_extra_args='vsc_min_count_snps=3,vsc_min_fraction_snps=0.2,vsc_min_count_indels=3,vsc_min_fraction_indels=0.10'. """""". ```. Below is the begening and end of the log file. I am happy to include the entire log file but there is nothing out of the ordinary between those lines below (same output as for jobs that finished successfully). Could you please advise on what parameters to change to successfully run DeepVariant by submitting it to the SLURM scheduler? . I0104 18:49:24.340415 140179943589696 make_examples_core.py:243] Task 13/64: Found 2793 candidate variants. I0104 18:49:24.340478 140179943589696 make_examples_core.py:243] Task 13/64: Created 2879 examples. Building DAG of jobs... Using shell: /usr/bin/bash. Provided cores: 64. Rules claiming more threads will be scaled down. Select jobs to execute... > [Wed Jan 4 18:30:51 2023]. > rule deepvariant:. > input: results/recal/s534_EKDN210017195-1A_HTTJ3DSX2_L2.bam, /mnt/shared/scratch/kmarians/private/dyslexia_gatk/workflow/resources/genome.fasta. > output: results/deepvariant/s534_EKDN210017195-1A_HTTJ3DSX2_L2.vcf.gz. > log: logs/deepvariant/s534_EKDN210017195-1A_HTTJ3DSX2_L2/stdout.log. > job",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/602
https://github.com/google/deepvariant/issues/602:2074,deployability,log,log,2074,"make --cluster ""sbatch --mem=180G cpus-per-task=64"" --jobs 64 --profie profile/ ```(where profile holds singularity args etc.). Singularity image is deepvariant_1.4.0.sif. my Snakemake rule:. ```. rule deepvariant:. input:. bam=rules.apply_bqsr.output.bam,. ref='/mnt/shared/scratch/kmarians/private/dyslexia_gatk/workflow/resources/genome.fasta'. output:. vcf=""results/deepvariant/{sample}.vcf.gz"". params:. model=""WES"". threads: . 64. resources:. mem_mb=163840. log:. ""logs/deepvariant/{sample}/stdout.log"". singularity:. ""singularity/deepvariant_1.4.0.sif"". # ""singularity/deepvariant_1.4.0-gpu.sif"" # for GPU. shell:. """""". /opt/deepvariant/bin/run_deepvariant --model_type {params.model} --ref {input.ref} --reads {input.bam} --output_vcf {output.vcf} --num_shards {threads} --make_examples_extra_args='vsc_min_count_snps=3,vsc_min_fraction_snps=0.2,vsc_min_count_indels=3,vsc_min_fraction_indels=0.10'. """""". ```. Below is the begening and end of the log file. I am happy to include the entire log file but there is nothing out of the ordinary between those lines below (same output as for jobs that finished successfully). Could you please advise on what parameters to change to successfully run DeepVariant by submitting it to the SLURM scheduler? . I0104 18:49:24.340415 140179943589696 make_examples_core.py:243] Task 13/64: Found 2793 candidate variants. I0104 18:49:24.340478 140179943589696 make_examples_core.py:243] Task 13/64: Created 2879 examples. Building DAG of jobs... Using shell: /usr/bin/bash. Provided cores: 64. Rules claiming more threads will be scaled down. Select jobs to execute... > [Wed Jan 4 18:30:51 2023]. > rule deepvariant:. > input: results/recal/s534_EKDN210017195-1A_HTTJ3DSX2_L2.bam, /mnt/shared/scratch/kmarians/private/dyslexia_gatk/workflow/resources/genome.fasta. > output: results/deepvariant/s534_EKDN210017195-1A_HTTJ3DSX2_L2.vcf.gz. > log: logs/deepvariant/s534_EKDN210017195-1A_HTTJ3DSX2_L2/stdout.log. > jobid: 0. > wildcards: sample=s534_EKDN2100171",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/602
https://github.com/google/deepvariant/issues/602:2540,deployability,Build,Building,2540,". ""logs/deepvariant/{sample}/stdout.log"". singularity:. ""singularity/deepvariant_1.4.0.sif"". # ""singularity/deepvariant_1.4.0-gpu.sif"" # for GPU. shell:. """""". /opt/deepvariant/bin/run_deepvariant --model_type {params.model} --ref {input.ref} --reads {input.bam} --output_vcf {output.vcf} --num_shards {threads} --make_examples_extra_args='vsc_min_count_snps=3,vsc_min_fraction_snps=0.2,vsc_min_count_indels=3,vsc_min_fraction_indels=0.10'. """""". ```. Below is the begening and end of the log file. I am happy to include the entire log file but there is nothing out of the ordinary between those lines below (same output as for jobs that finished successfully). Could you please advise on what parameters to change to successfully run DeepVariant by submitting it to the SLURM scheduler? . I0104 18:49:24.340415 140179943589696 make_examples_core.py:243] Task 13/64: Found 2793 candidate variants. I0104 18:49:24.340478 140179943589696 make_examples_core.py:243] Task 13/64: Created 2879 examples. Building DAG of jobs... Using shell: /usr/bin/bash. Provided cores: 64. Rules claiming more threads will be scaled down. Select jobs to execute... > [Wed Jan 4 18:30:51 2023]. > rule deepvariant:. > input: results/recal/s534_EKDN210017195-1A_HTTJ3DSX2_L2.bam, /mnt/shared/scratch/kmarians/private/dyslexia_gatk/workflow/resources/genome.fasta. > output: results/deepvariant/s534_EKDN210017195-1A_HTTJ3DSX2_L2.vcf.gz. > log: logs/deepvariant/s534_EKDN210017195-1A_HTTJ3DSX2_L2/stdout.log. > jobid: 0. > wildcards: sample=s534_EKDN210017195-1A_HTTJ3DSX2_L2. > threads: 64. > resources: mem_mb=163840, disk_mb=16401, tmpdir=/tmp/kmarians_4189323. > . > Activating singularity image singularity/deepvariant_1.4.0.sif. > INFO: Convert SIF file to sandbox... > I0104 18:31:03.183642 139718628308800 run_deepvariant.py:342] Re-using the directory for intermediate results in /tmp/kmarians_4189323/tmpxrz5rqbp. > . > ***** Intermediate results will be written to /tmp/kmarians_4189323/tmpxrz5rqbp in docker. ****",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/602
https://github.com/google/deepvariant/issues/602:2648,deployability,scale,scaled,2648,"/deepvariant_1.4.0-gpu.sif"" # for GPU. shell:. """""". /opt/deepvariant/bin/run_deepvariant --model_type {params.model} --ref {input.ref} --reads {input.bam} --output_vcf {output.vcf} --num_shards {threads} --make_examples_extra_args='vsc_min_count_snps=3,vsc_min_fraction_snps=0.2,vsc_min_count_indels=3,vsc_min_fraction_indels=0.10'. """""". ```. Below is the begening and end of the log file. I am happy to include the entire log file but there is nothing out of the ordinary between those lines below (same output as for jobs that finished successfully). Could you please advise on what parameters to change to successfully run DeepVariant by submitting it to the SLURM scheduler? . I0104 18:49:24.340415 140179943589696 make_examples_core.py:243] Task 13/64: Found 2793 candidate variants. I0104 18:49:24.340478 140179943589696 make_examples_core.py:243] Task 13/64: Created 2879 examples. Building DAG of jobs... Using shell: /usr/bin/bash. Provided cores: 64. Rules claiming more threads will be scaled down. Select jobs to execute... > [Wed Jan 4 18:30:51 2023]. > rule deepvariant:. > input: results/recal/s534_EKDN210017195-1A_HTTJ3DSX2_L2.bam, /mnt/shared/scratch/kmarians/private/dyslexia_gatk/workflow/resources/genome.fasta. > output: results/deepvariant/s534_EKDN210017195-1A_HTTJ3DSX2_L2.vcf.gz. > log: logs/deepvariant/s534_EKDN210017195-1A_HTTJ3DSX2_L2/stdout.log. > jobid: 0. > wildcards: sample=s534_EKDN210017195-1A_HTTJ3DSX2_L2. > threads: 64. > resources: mem_mb=163840, disk_mb=16401, tmpdir=/tmp/kmarians_4189323. > . > Activating singularity image singularity/deepvariant_1.4.0.sif. > INFO: Convert SIF file to sandbox... > I0104 18:31:03.183642 139718628308800 run_deepvariant.py:342] Re-using the directory for intermediate results in /tmp/kmarians_4189323/tmpxrz5rqbp. > . > ***** Intermediate results will be written to /tmp/kmarians_4189323/tmpxrz5rqbp in docker. ****. > . > . > ***** Running the command:*****. > time seq 0 63 | parallel -q --halt 2 --line-buffer /opt/deep",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/602
https://github.com/google/deepvariant/issues/602:2860,deployability,resourc,resources,2860,"mples_extra_args='vsc_min_count_snps=3,vsc_min_fraction_snps=0.2,vsc_min_count_indels=3,vsc_min_fraction_indels=0.10'. """""". ```. Below is the begening and end of the log file. I am happy to include the entire log file but there is nothing out of the ordinary between those lines below (same output as for jobs that finished successfully). Could you please advise on what parameters to change to successfully run DeepVariant by submitting it to the SLURM scheduler? . I0104 18:49:24.340415 140179943589696 make_examples_core.py:243] Task 13/64: Found 2793 candidate variants. I0104 18:49:24.340478 140179943589696 make_examples_core.py:243] Task 13/64: Created 2879 examples. Building DAG of jobs... Using shell: /usr/bin/bash. Provided cores: 64. Rules claiming more threads will be scaled down. Select jobs to execute... > [Wed Jan 4 18:30:51 2023]. > rule deepvariant:. > input: results/recal/s534_EKDN210017195-1A_HTTJ3DSX2_L2.bam, /mnt/shared/scratch/kmarians/private/dyslexia_gatk/workflow/resources/genome.fasta. > output: results/deepvariant/s534_EKDN210017195-1A_HTTJ3DSX2_L2.vcf.gz. > log: logs/deepvariant/s534_EKDN210017195-1A_HTTJ3DSX2_L2/stdout.log. > jobid: 0. > wildcards: sample=s534_EKDN210017195-1A_HTTJ3DSX2_L2. > threads: 64. > resources: mem_mb=163840, disk_mb=16401, tmpdir=/tmp/kmarians_4189323. > . > Activating singularity image singularity/deepvariant_1.4.0.sif. > INFO: Convert SIF file to sandbox... > I0104 18:31:03.183642 139718628308800 run_deepvariant.py:342] Re-using the directory for intermediate results in /tmp/kmarians_4189323/tmpxrz5rqbp. > . > ***** Intermediate results will be written to /tmp/kmarians_4189323/tmpxrz5rqbp in docker. ****. > . > . > ***** Running the command:*****. > time seq 0 63 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/mnt/shared/scratch/kmarians/private/dyslexia_gatk/workflow/resources/genome.fasta"" --reads ""results/recal/s534_EKDN210017195-1A_HTTJ3DSX2_L2.bam"" --examples ""/tmp/km",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/602
https://github.com/google/deepvariant/issues/602:2959,deployability,log,log,2959,"fraction_indels=0.10'. """""". ```. Below is the begening and end of the log file. I am happy to include the entire log file but there is nothing out of the ordinary between those lines below (same output as for jobs that finished successfully). Could you please advise on what parameters to change to successfully run DeepVariant by submitting it to the SLURM scheduler? . I0104 18:49:24.340415 140179943589696 make_examples_core.py:243] Task 13/64: Found 2793 candidate variants. I0104 18:49:24.340478 140179943589696 make_examples_core.py:243] Task 13/64: Created 2879 examples. Building DAG of jobs... Using shell: /usr/bin/bash. Provided cores: 64. Rules claiming more threads will be scaled down. Select jobs to execute... > [Wed Jan 4 18:30:51 2023]. > rule deepvariant:. > input: results/recal/s534_EKDN210017195-1A_HTTJ3DSX2_L2.bam, /mnt/shared/scratch/kmarians/private/dyslexia_gatk/workflow/resources/genome.fasta. > output: results/deepvariant/s534_EKDN210017195-1A_HTTJ3DSX2_L2.vcf.gz. > log: logs/deepvariant/s534_EKDN210017195-1A_HTTJ3DSX2_L2/stdout.log. > jobid: 0. > wildcards: sample=s534_EKDN210017195-1A_HTTJ3DSX2_L2. > threads: 64. > resources: mem_mb=163840, disk_mb=16401, tmpdir=/tmp/kmarians_4189323. > . > Activating singularity image singularity/deepvariant_1.4.0.sif. > INFO: Convert SIF file to sandbox... > I0104 18:31:03.183642 139718628308800 run_deepvariant.py:342] Re-using the directory for intermediate results in /tmp/kmarians_4189323/tmpxrz5rqbp. > . > ***** Intermediate results will be written to /tmp/kmarians_4189323/tmpxrz5rqbp in docker. ****. > . > . > ***** Running the command:*****. > time seq 0 63 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/mnt/shared/scratch/kmarians/private/dyslexia_gatk/workflow/resources/genome.fasta"" --reads ""results/recal/s534_EKDN210017195-1A_HTTJ3DSX2_L2.bam"" --examples ""/tmp/kmarians_4189323/tmpxrz5rqbp/make_examples.tfrecord@64.gz"" --channels ""insert_size"" --vsc_min_coun",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/602
https://github.com/google/deepvariant/issues/602:2964,deployability,log,logs,2964,"ion_indels=0.10'. """""". ```. Below is the begening and end of the log file. I am happy to include the entire log file but there is nothing out of the ordinary between those lines below (same output as for jobs that finished successfully). Could you please advise on what parameters to change to successfully run DeepVariant by submitting it to the SLURM scheduler? . I0104 18:49:24.340415 140179943589696 make_examples_core.py:243] Task 13/64: Found 2793 candidate variants. I0104 18:49:24.340478 140179943589696 make_examples_core.py:243] Task 13/64: Created 2879 examples. Building DAG of jobs... Using shell: /usr/bin/bash. Provided cores: 64. Rules claiming more threads will be scaled down. Select jobs to execute... > [Wed Jan 4 18:30:51 2023]. > rule deepvariant:. > input: results/recal/s534_EKDN210017195-1A_HTTJ3DSX2_L2.bam, /mnt/shared/scratch/kmarians/private/dyslexia_gatk/workflow/resources/genome.fasta. > output: results/deepvariant/s534_EKDN210017195-1A_HTTJ3DSX2_L2.vcf.gz. > log: logs/deepvariant/s534_EKDN210017195-1A_HTTJ3DSX2_L2/stdout.log. > jobid: 0. > wildcards: sample=s534_EKDN210017195-1A_HTTJ3DSX2_L2. > threads: 64. > resources: mem_mb=163840, disk_mb=16401, tmpdir=/tmp/kmarians_4189323. > . > Activating singularity image singularity/deepvariant_1.4.0.sif. > INFO: Convert SIF file to sandbox... > I0104 18:31:03.183642 139718628308800 run_deepvariant.py:342] Re-using the directory for intermediate results in /tmp/kmarians_4189323/tmpxrz5rqbp. > . > ***** Intermediate results will be written to /tmp/kmarians_4189323/tmpxrz5rqbp in docker. ****. > . > . > ***** Running the command:*****. > time seq 0 63 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/mnt/shared/scratch/kmarians/private/dyslexia_gatk/workflow/resources/genome.fasta"" --reads ""results/recal/s534_EKDN210017195-1A_HTTJ3DSX2_L2.bam"" --examples ""/tmp/kmarians_4189323/tmpxrz5rqbp/make_examples.tfrecord@64.gz"" --channels ""insert_size"" --vsc_min_count_ind",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/602
https://github.com/google/deepvariant/issues/602:3023,deployability,log,log,3023,"f the log file. I am happy to include the entire log file but there is nothing out of the ordinary between those lines below (same output as for jobs that finished successfully). Could you please advise on what parameters to change to successfully run DeepVariant by submitting it to the SLURM scheduler? . I0104 18:49:24.340415 140179943589696 make_examples_core.py:243] Task 13/64: Found 2793 candidate variants. I0104 18:49:24.340478 140179943589696 make_examples_core.py:243] Task 13/64: Created 2879 examples. Building DAG of jobs... Using shell: /usr/bin/bash. Provided cores: 64. Rules claiming more threads will be scaled down. Select jobs to execute... > [Wed Jan 4 18:30:51 2023]. > rule deepvariant:. > input: results/recal/s534_EKDN210017195-1A_HTTJ3DSX2_L2.bam, /mnt/shared/scratch/kmarians/private/dyslexia_gatk/workflow/resources/genome.fasta. > output: results/deepvariant/s534_EKDN210017195-1A_HTTJ3DSX2_L2.vcf.gz. > log: logs/deepvariant/s534_EKDN210017195-1A_HTTJ3DSX2_L2/stdout.log. > jobid: 0. > wildcards: sample=s534_EKDN210017195-1A_HTTJ3DSX2_L2. > threads: 64. > resources: mem_mb=163840, disk_mb=16401, tmpdir=/tmp/kmarians_4189323. > . > Activating singularity image singularity/deepvariant_1.4.0.sif. > INFO: Convert SIF file to sandbox... > I0104 18:31:03.183642 139718628308800 run_deepvariant.py:342] Re-using the directory for intermediate results in /tmp/kmarians_4189323/tmpxrz5rqbp. > . > ***** Intermediate results will be written to /tmp/kmarians_4189323/tmpxrz5rqbp in docker. ****. > . > . > ***** Running the command:*****. > time seq 0 63 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/mnt/shared/scratch/kmarians/private/dyslexia_gatk/workflow/resources/genome.fasta"" --reads ""results/recal/s534_EKDN210017195-1A_HTTJ3DSX2_L2.bam"" --examples ""/tmp/kmarians_4189323/tmpxrz5rqbp/make_examples.tfrecord@64.gz"" --channels ""insert_size"" --vsc_min_count_indels ""3"" --vsc_min_count_snps ""3"" --vsc_min_fraction_indels ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/602
https://github.com/google/deepvariant/issues/602:3113,deployability,resourc,resources,3113,"inary between those lines below (same output as for jobs that finished successfully). Could you please advise on what parameters to change to successfully run DeepVariant by submitting it to the SLURM scheduler? . I0104 18:49:24.340415 140179943589696 make_examples_core.py:243] Task 13/64: Found 2793 candidate variants. I0104 18:49:24.340478 140179943589696 make_examples_core.py:243] Task 13/64: Created 2879 examples. Building DAG of jobs... Using shell: /usr/bin/bash. Provided cores: 64. Rules claiming more threads will be scaled down. Select jobs to execute... > [Wed Jan 4 18:30:51 2023]. > rule deepvariant:. > input: results/recal/s534_EKDN210017195-1A_HTTJ3DSX2_L2.bam, /mnt/shared/scratch/kmarians/private/dyslexia_gatk/workflow/resources/genome.fasta. > output: results/deepvariant/s534_EKDN210017195-1A_HTTJ3DSX2_L2.vcf.gz. > log: logs/deepvariant/s534_EKDN210017195-1A_HTTJ3DSX2_L2/stdout.log. > jobid: 0. > wildcards: sample=s534_EKDN210017195-1A_HTTJ3DSX2_L2. > threads: 64. > resources: mem_mb=163840, disk_mb=16401, tmpdir=/tmp/kmarians_4189323. > . > Activating singularity image singularity/deepvariant_1.4.0.sif. > INFO: Convert SIF file to sandbox... > I0104 18:31:03.183642 139718628308800 run_deepvariant.py:342] Re-using the directory for intermediate results in /tmp/kmarians_4189323/tmpxrz5rqbp. > . > ***** Intermediate results will be written to /tmp/kmarians_4189323/tmpxrz5rqbp in docker. ****. > . > . > ***** Running the command:*****. > time seq 0 63 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/mnt/shared/scratch/kmarians/private/dyslexia_gatk/workflow/resources/genome.fasta"" --reads ""results/recal/s534_EKDN210017195-1A_HTTJ3DSX2_L2.bam"" --examples ""/tmp/kmarians_4189323/tmpxrz5rqbp/make_examples.tfrecord@64.gz"" --channels ""insert_size"" --vsc_min_count_indels ""3"" --vsc_min_count_snps ""3"" --vsc_min_fraction_indels ""0.10"" --vsc_min_fraction_snps ""0.2"" --task {}. > *. > *. > *. > I0104 18:49:24.340415 140179",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/602
https://github.com/google/deepvariant/issues/602:3759,deployability,resourc,resources,3759,"_EKDN210017195-1A_HTTJ3DSX2_L2.bam, /mnt/shared/scratch/kmarians/private/dyslexia_gatk/workflow/resources/genome.fasta. > output: results/deepvariant/s534_EKDN210017195-1A_HTTJ3DSX2_L2.vcf.gz. > log: logs/deepvariant/s534_EKDN210017195-1A_HTTJ3DSX2_L2/stdout.log. > jobid: 0. > wildcards: sample=s534_EKDN210017195-1A_HTTJ3DSX2_L2. > threads: 64. > resources: mem_mb=163840, disk_mb=16401, tmpdir=/tmp/kmarians_4189323. > . > Activating singularity image singularity/deepvariant_1.4.0.sif. > INFO: Convert SIF file to sandbox... > I0104 18:31:03.183642 139718628308800 run_deepvariant.py:342] Re-using the directory for intermediate results in /tmp/kmarians_4189323/tmpxrz5rqbp. > . > ***** Intermediate results will be written to /tmp/kmarians_4189323/tmpxrz5rqbp in docker. ****. > . > . > ***** Running the command:*****. > time seq 0 63 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/mnt/shared/scratch/kmarians/private/dyslexia_gatk/workflow/resources/genome.fasta"" --reads ""results/recal/s534_EKDN210017195-1A_HTTJ3DSX2_L2.bam"" --examples ""/tmp/kmarians_4189323/tmpxrz5rqbp/make_examples.tfrecord@64.gz"" --channels ""insert_size"" --vsc_min_count_indels ""3"" --vsc_min_count_snps ""3"" --vsc_min_fraction_indels ""0.10"" --vsc_min_fraction_snps ""0.2"" --task {}. > *. > *. > *. > I0104 18:49:24.340415 140179943589696 make_examples_core.py:243] Task 13/64: Found 2793 candidate variants. > I0104 18:49:24.340478 140179943589696 make_examples_core.py:243] Task 13/64: Created 2879 examples. > real	18m21.465s. > user	893m16.250s. > sys	15m57.561s. > . > ***** Running the command:*****. > time /opt/deepvariant/bin/call_variants --outfile ""/tmp/kmarians_4189323/tmpxrz5rqbp/call_variants_output.tfrecord.gz"" --examples ""/tmp/kmarians_4189323/tmpxrz5rqbp/make_examples.tfrecord@64.gz"" --checkpoint ""/opt/models/wes/model.ckpt"" --openvino_model_dir ""/tmp/kmarians_4189323/tmpxrz5rqbp"". > . > I0104 18:49:26.695084 139646404110144 call_variants.py:317] From ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/602
https://github.com/google/deepvariant/issues/602:5324,deployability,modul,module,5324,"93m16.250s. > sys	15m57.561s. > . > ***** Running the command:*****. > time /opt/deepvariant/bin/call_variants --outfile ""/tmp/kmarians_4189323/tmpxrz5rqbp/call_variants_output.tfrecord.gz"" --examples ""/tmp/kmarians_4189323/tmpxrz5rqbp/make_examples.tfrecord@64.gz"" --checkpoint ""/opt/models/wes/model.ckpt"" --openvino_model_dir ""/tmp/kmarians_4189323/tmpxrz5rqbp"". > . > I0104 18:49:26.695084 139646404110144 call_variants.py:317] From /tmp/kmarians_4189323/tmpxrz5rqbp/make_examples.tfrecord-00000-of-00064.gz.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 19]. > I0104 18:49:26.697470 139646404110144 call_variants.py:317] From /opt/models/wes/model.ckpt.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 19]. > Traceback (most recent call last):. > File ""/tmp/kmarians_4189323/Bazel.runfiles_zims94rl/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 513, in <module>. > tf.compat.v1.app.run(). > File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. > _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). > File ""/tmp/kmarians_4189323/Bazel.runfiles_zims94rl/runfiles/absl_py/absl/app.py"", line 300, in run. > _run_main(main, args). > File ""/tmp/kmarians_4189323/Bazel.runfiles_zims94rl/runfiles/absl_py/absl/app.py"", line 251, in _run_main. > sys.exit(main(argv)). > File ""/tmp/kmarians_4189323/Bazel.runfiles_zims94rl/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 494, in main. > call_variants(. > File ""/tmp/kmarians_4189323/Bazel.runfiles_zims94rl/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 389, in call_variants. > init_op = tf.group(tf.compat.v1.global_variables_initializer(),. > File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/ops/variables.py"", line 3319, in global_variables_initializer. > return variables_initializer(global_var",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/602
https://github.com/google/deepvariant/issues/602:569,energy efficiency,schedul,scheduler,569,"AttributeError: deleter; Hi DeepVariant team,. I'm running DeepVariant via Singularity via Snakemake on a HPC cluster and overall, if I create an interactive session, my entire WES pipeline runs fine until an odd DeepVariant job fails (by that I mean that let's say 30 deepvariant jobs finish ok and the 31st fails). I can then restart the Snakemake run and the same job will run fine, followed by more jobs that will also run fine until another odd jobs fails. This is also particularly true when I use the --cluster command in Snakemake (i.e., send it to a SLURM job scheduler from the master node) - in this event every single job fails. I could of course run all my samples on a single interactive session, keep checking the log file and restart the run every time it fails but I guess that's less than optimal plus this way I can really only run one sample at the time. For the interactive sessions I request 180G and 64cpus (in my case it's: ```srsh --mem=180G --cpus-per-task=64 --partition=long```). . I would request same parameters when using --cluster so:. ```snakemake --cluster ""sbatch --mem=180G cpus-per-task=64"" --jobs 64 --profie profile/ ```(where profile holds singularity args etc.). Singularity image is deepvariant_1.4.0.sif. my Snakemake rule:. ```. rule deepvariant:. input:. bam=rules.apply_bqsr.output.bam,. ref='/mnt/shared/scratch/kmarians/private/dyslexia_gatk/workflow/resources/genome.fasta'. output:. vcf=""results/deepvariant/{sample}.vcf.gz"". params:. model=""WES"". threads: . 64. resources:. mem_mb=163840. log:. ""logs/deepvariant/{sample}/stdout.log"". singularity:. ""singularity/deepvariant_1.4.0.sif"". # ""singularity/deepvariant_1.4.0-gpu.sif"" # for GPU. shell:. """""". /opt/deepvariant/bin/run_deepvariant --model_type {params.model} --ref {input.ref} --reads {input.bam} --output_vcf {output.vcf} --num_shards {threads} --make_examples_extra_args='vsc_min_count_snps=3,vsc_min_fraction_snps=0.2,vsc_min_count_indels=3,vsc_min_fraction_indels=0.10'. """""". ```. Below ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/602
https://github.com/google/deepvariant/issues/602:807,energy efficiency,optim,optimal,807,"AttributeError: deleter; Hi DeepVariant team,. I'm running DeepVariant via Singularity via Snakemake on a HPC cluster and overall, if I create an interactive session, my entire WES pipeline runs fine until an odd DeepVariant job fails (by that I mean that let's say 30 deepvariant jobs finish ok and the 31st fails). I can then restart the Snakemake run and the same job will run fine, followed by more jobs that will also run fine until another odd jobs fails. This is also particularly true when I use the --cluster command in Snakemake (i.e., send it to a SLURM job scheduler from the master node) - in this event every single job fails. I could of course run all my samples on a single interactive session, keep checking the log file and restart the run every time it fails but I guess that's less than optimal plus this way I can really only run one sample at the time. For the interactive sessions I request 180G and 64cpus (in my case it's: ```srsh --mem=180G --cpus-per-task=64 --partition=long```). . I would request same parameters when using --cluster so:. ```snakemake --cluster ""sbatch --mem=180G cpus-per-task=64"" --jobs 64 --profie profile/ ```(where profile holds singularity args etc.). Singularity image is deepvariant_1.4.0.sif. my Snakemake rule:. ```. rule deepvariant:. input:. bam=rules.apply_bqsr.output.bam,. ref='/mnt/shared/scratch/kmarians/private/dyslexia_gatk/workflow/resources/genome.fasta'. output:. vcf=""results/deepvariant/{sample}.vcf.gz"". params:. model=""WES"". threads: . 64. resources:. mem_mb=163840. log:. ""logs/deepvariant/{sample}/stdout.log"". singularity:. ""singularity/deepvariant_1.4.0.sif"". # ""singularity/deepvariant_1.4.0-gpu.sif"" # for GPU. shell:. """""". /opt/deepvariant/bin/run_deepvariant --model_type {params.model} --ref {input.ref} --reads {input.bam} --output_vcf {output.vcf} --num_shards {threads} --make_examples_extra_args='vsc_min_count_snps=3,vsc_min_fraction_snps=0.2,vsc_min_count_indels=3,vsc_min_fraction_indels=0.10'. """""". ```. Below ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/602
https://github.com/google/deepvariant/issues/602:969,energy efficiency,cpu,cpus-per-task,969,"AttributeError: deleter; Hi DeepVariant team,. I'm running DeepVariant via Singularity via Snakemake on a HPC cluster and overall, if I create an interactive session, my entire WES pipeline runs fine until an odd DeepVariant job fails (by that I mean that let's say 30 deepvariant jobs finish ok and the 31st fails). I can then restart the Snakemake run and the same job will run fine, followed by more jobs that will also run fine until another odd jobs fails. This is also particularly true when I use the --cluster command in Snakemake (i.e., send it to a SLURM job scheduler from the master node) - in this event every single job fails. I could of course run all my samples on a single interactive session, keep checking the log file and restart the run every time it fails but I guess that's less than optimal plus this way I can really only run one sample at the time. For the interactive sessions I request 180G and 64cpus (in my case it's: ```srsh --mem=180G --cpus-per-task=64 --partition=long```). . I would request same parameters when using --cluster so:. ```snakemake --cluster ""sbatch --mem=180G cpus-per-task=64"" --jobs 64 --profie profile/ ```(where profile holds singularity args etc.). Singularity image is deepvariant_1.4.0.sif. my Snakemake rule:. ```. rule deepvariant:. input:. bam=rules.apply_bqsr.output.bam,. ref='/mnt/shared/scratch/kmarians/private/dyslexia_gatk/workflow/resources/genome.fasta'. output:. vcf=""results/deepvariant/{sample}.vcf.gz"". params:. model=""WES"". threads: . 64. resources:. mem_mb=163840. log:. ""logs/deepvariant/{sample}/stdout.log"". singularity:. ""singularity/deepvariant_1.4.0.sif"". # ""singularity/deepvariant_1.4.0-gpu.sif"" # for GPU. shell:. """""". /opt/deepvariant/bin/run_deepvariant --model_type {params.model} --ref {input.ref} --reads {input.bam} --output_vcf {output.vcf} --num_shards {threads} --make_examples_extra_args='vsc_min_count_snps=3,vsc_min_fraction_snps=0.2,vsc_min_count_indels=3,vsc_min_fraction_indels=0.10'. """""". ```. Below ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/602
https://github.com/google/deepvariant/issues/602:1110,energy efficiency,cpu,cpus-per-task,1110," and overall, if I create an interactive session, my entire WES pipeline runs fine until an odd DeepVariant job fails (by that I mean that let's say 30 deepvariant jobs finish ok and the 31st fails). I can then restart the Snakemake run and the same job will run fine, followed by more jobs that will also run fine until another odd jobs fails. This is also particularly true when I use the --cluster command in Snakemake (i.e., send it to a SLURM job scheduler from the master node) - in this event every single job fails. I could of course run all my samples on a single interactive session, keep checking the log file and restart the run every time it fails but I guess that's less than optimal plus this way I can really only run one sample at the time. For the interactive sessions I request 180G and 64cpus (in my case it's: ```srsh --mem=180G --cpus-per-task=64 --partition=long```). . I would request same parameters when using --cluster so:. ```snakemake --cluster ""sbatch --mem=180G cpus-per-task=64"" --jobs 64 --profie profile/ ```(where profile holds singularity args etc.). Singularity image is deepvariant_1.4.0.sif. my Snakemake rule:. ```. rule deepvariant:. input:. bam=rules.apply_bqsr.output.bam,. ref='/mnt/shared/scratch/kmarians/private/dyslexia_gatk/workflow/resources/genome.fasta'. output:. vcf=""results/deepvariant/{sample}.vcf.gz"". params:. model=""WES"". threads: . 64. resources:. mem_mb=163840. log:. ""logs/deepvariant/{sample}/stdout.log"". singularity:. ""singularity/deepvariant_1.4.0.sif"". # ""singularity/deepvariant_1.4.0-gpu.sif"" # for GPU. shell:. """""". /opt/deepvariant/bin/run_deepvariant --model_type {params.model} --ref {input.ref} --reads {input.bam} --output_vcf {output.vcf} --num_shards {threads} --make_examples_extra_args='vsc_min_count_snps=3,vsc_min_fraction_snps=0.2,vsc_min_count_indels=3,vsc_min_fraction_indels=0.10'. """""". ```. Below is the begening and end of the log file. I am happy to include the entire log file but there is nothing out of the or",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/602
https://github.com/google/deepvariant/issues/602:1147,energy efficiency,profil,profile,1147,"active session, my entire WES pipeline runs fine until an odd DeepVariant job fails (by that I mean that let's say 30 deepvariant jobs finish ok and the 31st fails). I can then restart the Snakemake run and the same job will run fine, followed by more jobs that will also run fine until another odd jobs fails. This is also particularly true when I use the --cluster command in Snakemake (i.e., send it to a SLURM job scheduler from the master node) - in this event every single job fails. I could of course run all my samples on a single interactive session, keep checking the log file and restart the run every time it fails but I guess that's less than optimal plus this way I can really only run one sample at the time. For the interactive sessions I request 180G and 64cpus (in my case it's: ```srsh --mem=180G --cpus-per-task=64 --partition=long```). . I would request same parameters when using --cluster so:. ```snakemake --cluster ""sbatch --mem=180G cpus-per-task=64"" --jobs 64 --profie profile/ ```(where profile holds singularity args etc.). Singularity image is deepvariant_1.4.0.sif. my Snakemake rule:. ```. rule deepvariant:. input:. bam=rules.apply_bqsr.output.bam,. ref='/mnt/shared/scratch/kmarians/private/dyslexia_gatk/workflow/resources/genome.fasta'. output:. vcf=""results/deepvariant/{sample}.vcf.gz"". params:. model=""WES"". threads: . 64. resources:. mem_mb=163840. log:. ""logs/deepvariant/{sample}/stdout.log"". singularity:. ""singularity/deepvariant_1.4.0.sif"". # ""singularity/deepvariant_1.4.0-gpu.sif"" # for GPU. shell:. """""". /opt/deepvariant/bin/run_deepvariant --model_type {params.model} --ref {input.ref} --reads {input.bam} --output_vcf {output.vcf} --num_shards {threads} --make_examples_extra_args='vsc_min_count_snps=3,vsc_min_fraction_snps=0.2,vsc_min_count_indels=3,vsc_min_fraction_indels=0.10'. """""". ```. Below is the begening and end of the log file. I am happy to include the entire log file but there is nothing out of the ordinary between those lines below (",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/602
https://github.com/google/deepvariant/issues/602:1166,energy efficiency,profil,profile,1166,"entire WES pipeline runs fine until an odd DeepVariant job fails (by that I mean that let's say 30 deepvariant jobs finish ok and the 31st fails). I can then restart the Snakemake run and the same job will run fine, followed by more jobs that will also run fine until another odd jobs fails. This is also particularly true when I use the --cluster command in Snakemake (i.e., send it to a SLURM job scheduler from the master node) - in this event every single job fails. I could of course run all my samples on a single interactive session, keep checking the log file and restart the run every time it fails but I guess that's less than optimal plus this way I can really only run one sample at the time. For the interactive sessions I request 180G and 64cpus (in my case it's: ```srsh --mem=180G --cpus-per-task=64 --partition=long```). . I would request same parameters when using --cluster so:. ```snakemake --cluster ""sbatch --mem=180G cpus-per-task=64"" --jobs 64 --profie profile/ ```(where profile holds singularity args etc.). Singularity image is deepvariant_1.4.0.sif. my Snakemake rule:. ```. rule deepvariant:. input:. bam=rules.apply_bqsr.output.bam,. ref='/mnt/shared/scratch/kmarians/private/dyslexia_gatk/workflow/resources/genome.fasta'. output:. vcf=""results/deepvariant/{sample}.vcf.gz"". params:. model=""WES"". threads: . 64. resources:. mem_mb=163840. log:. ""logs/deepvariant/{sample}/stdout.log"". singularity:. ""singularity/deepvariant_1.4.0.sif"". # ""singularity/deepvariant_1.4.0-gpu.sif"" # for GPU. shell:. """""". /opt/deepvariant/bin/run_deepvariant --model_type {params.model} --ref {input.ref} --reads {input.bam} --output_vcf {output.vcf} --num_shards {threads} --make_examples_extra_args='vsc_min_count_snps=3,vsc_min_fraction_snps=0.2,vsc_min_count_indels=3,vsc_min_fraction_indels=0.10'. """""". ```. Below is the begening and end of the log file. I am happy to include the entire log file but there is nothing out of the ordinary between those lines below (same output as for ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/602
https://github.com/google/deepvariant/issues/602:1399,energy efficiency,resourc,resources,1399,"obs that will also run fine until another odd jobs fails. This is also particularly true when I use the --cluster command in Snakemake (i.e., send it to a SLURM job scheduler from the master node) - in this event every single job fails. I could of course run all my samples on a single interactive session, keep checking the log file and restart the run every time it fails but I guess that's less than optimal plus this way I can really only run one sample at the time. For the interactive sessions I request 180G and 64cpus (in my case it's: ```srsh --mem=180G --cpus-per-task=64 --partition=long```). . I would request same parameters when using --cluster so:. ```snakemake --cluster ""sbatch --mem=180G cpus-per-task=64"" --jobs 64 --profie profile/ ```(where profile holds singularity args etc.). Singularity image is deepvariant_1.4.0.sif. my Snakemake rule:. ```. rule deepvariant:. input:. bam=rules.apply_bqsr.output.bam,. ref='/mnt/shared/scratch/kmarians/private/dyslexia_gatk/workflow/resources/genome.fasta'. output:. vcf=""results/deepvariant/{sample}.vcf.gz"". params:. model=""WES"". threads: . 64. resources:. mem_mb=163840. log:. ""logs/deepvariant/{sample}/stdout.log"". singularity:. ""singularity/deepvariant_1.4.0.sif"". # ""singularity/deepvariant_1.4.0-gpu.sif"" # for GPU. shell:. """""". /opt/deepvariant/bin/run_deepvariant --model_type {params.model} --ref {input.ref} --reads {input.bam} --output_vcf {output.vcf} --num_shards {threads} --make_examples_extra_args='vsc_min_count_snps=3,vsc_min_fraction_snps=0.2,vsc_min_count_indels=3,vsc_min_fraction_indels=0.10'. """""". ```. Below is the begening and end of the log file. I am happy to include the entire log file but there is nothing out of the ordinary between those lines below (same output as for jobs that finished successfully). Could you please advise on what parameters to change to successfully run DeepVariant by submitting it to the SLURM scheduler? . I0104 18:49:24.340415 140179943589696 make_examples_core.py:243] Task 13",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/602
https://github.com/google/deepvariant/issues/602:1485,energy efficiency,model,model,1485,"true when I use the --cluster command in Snakemake (i.e., send it to a SLURM job scheduler from the master node) - in this event every single job fails. I could of course run all my samples on a single interactive session, keep checking the log file and restart the run every time it fails but I guess that's less than optimal plus this way I can really only run one sample at the time. For the interactive sessions I request 180G and 64cpus (in my case it's: ```srsh --mem=180G --cpus-per-task=64 --partition=long```). . I would request same parameters when using --cluster so:. ```snakemake --cluster ""sbatch --mem=180G cpus-per-task=64"" --jobs 64 --profie profile/ ```(where profile holds singularity args etc.). Singularity image is deepvariant_1.4.0.sif. my Snakemake rule:. ```. rule deepvariant:. input:. bam=rules.apply_bqsr.output.bam,. ref='/mnt/shared/scratch/kmarians/private/dyslexia_gatk/workflow/resources/genome.fasta'. output:. vcf=""results/deepvariant/{sample}.vcf.gz"". params:. model=""WES"". threads: . 64. resources:. mem_mb=163840. log:. ""logs/deepvariant/{sample}/stdout.log"". singularity:. ""singularity/deepvariant_1.4.0.sif"". # ""singularity/deepvariant_1.4.0-gpu.sif"" # for GPU. shell:. """""". /opt/deepvariant/bin/run_deepvariant --model_type {params.model} --ref {input.ref} --reads {input.bam} --output_vcf {output.vcf} --num_shards {threads} --make_examples_extra_args='vsc_min_count_snps=3,vsc_min_fraction_snps=0.2,vsc_min_count_indels=3,vsc_min_fraction_indels=0.10'. """""". ```. Below is the begening and end of the log file. I am happy to include the entire log file but there is nothing out of the ordinary between those lines below (same output as for jobs that finished successfully). Could you please advise on what parameters to change to successfully run DeepVariant by submitting it to the SLURM scheduler? . I0104 18:49:24.340415 140179943589696 make_examples_core.py:243] Task 13/64: Found 2793 candidate variants. I0104 18:49:24.340478 140179943589696 make_examp",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/602
https://github.com/google/deepvariant/issues/602:1513,energy efficiency,resourc,resources,1513,"command in Snakemake (i.e., send it to a SLURM job scheduler from the master node) - in this event every single job fails. I could of course run all my samples on a single interactive session, keep checking the log file and restart the run every time it fails but I guess that's less than optimal plus this way I can really only run one sample at the time. For the interactive sessions I request 180G and 64cpus (in my case it's: ```srsh --mem=180G --cpus-per-task=64 --partition=long```). . I would request same parameters when using --cluster so:. ```snakemake --cluster ""sbatch --mem=180G cpus-per-task=64"" --jobs 64 --profie profile/ ```(where profile holds singularity args etc.). Singularity image is deepvariant_1.4.0.sif. my Snakemake rule:. ```. rule deepvariant:. input:. bam=rules.apply_bqsr.output.bam,. ref='/mnt/shared/scratch/kmarians/private/dyslexia_gatk/workflow/resources/genome.fasta'. output:. vcf=""results/deepvariant/{sample}.vcf.gz"". params:. model=""WES"". threads: . 64. resources:. mem_mb=163840. log:. ""logs/deepvariant/{sample}/stdout.log"". singularity:. ""singularity/deepvariant_1.4.0.sif"". # ""singularity/deepvariant_1.4.0-gpu.sif"" # for GPU. shell:. """""". /opt/deepvariant/bin/run_deepvariant --model_type {params.model} --ref {input.ref} --reads {input.bam} --output_vcf {output.vcf} --num_shards {threads} --make_examples_extra_args='vsc_min_count_snps=3,vsc_min_fraction_snps=0.2,vsc_min_count_indels=3,vsc_min_fraction_indels=0.10'. """""". ```. Below is the begening and end of the log file. I am happy to include the entire log file but there is nothing out of the ordinary between those lines below (same output as for jobs that finished successfully). Could you please advise on what parameters to change to successfully run DeepVariant by submitting it to the SLURM scheduler? . I0104 18:49:24.340415 140179943589696 make_examples_core.py:243] Task 13/64: Found 2793 candidate variants. I0104 18:49:24.340478 140179943589696 make_examples_core.py:243] Task 13/64: C",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/602
https://github.com/google/deepvariant/issues/602:1670,energy efficiency,gpu,gpu,1670,"mples on a single interactive session, keep checking the log file and restart the run every time it fails but I guess that's less than optimal plus this way I can really only run one sample at the time. For the interactive sessions I request 180G and 64cpus (in my case it's: ```srsh --mem=180G --cpus-per-task=64 --partition=long```). . I would request same parameters when using --cluster so:. ```snakemake --cluster ""sbatch --mem=180G cpus-per-task=64"" --jobs 64 --profie profile/ ```(where profile holds singularity args etc.). Singularity image is deepvariant_1.4.0.sif. my Snakemake rule:. ```. rule deepvariant:. input:. bam=rules.apply_bqsr.output.bam,. ref='/mnt/shared/scratch/kmarians/private/dyslexia_gatk/workflow/resources/genome.fasta'. output:. vcf=""results/deepvariant/{sample}.vcf.gz"". params:. model=""WES"". threads: . 64. resources:. mem_mb=163840. log:. ""logs/deepvariant/{sample}/stdout.log"". singularity:. ""singularity/deepvariant_1.4.0.sif"". # ""singularity/deepvariant_1.4.0-gpu.sif"" # for GPU. shell:. """""". /opt/deepvariant/bin/run_deepvariant --model_type {params.model} --ref {input.ref} --reads {input.bam} --output_vcf {output.vcf} --num_shards {threads} --make_examples_extra_args='vsc_min_count_snps=3,vsc_min_fraction_snps=0.2,vsc_min_count_indels=3,vsc_min_fraction_indels=0.10'. """""". ```. Below is the begening and end of the log file. I am happy to include the entire log file but there is nothing out of the ordinary between those lines below (same output as for jobs that finished successfully). Could you please advise on what parameters to change to successfully run DeepVariant by submitting it to the SLURM scheduler? . I0104 18:49:24.340415 140179943589696 make_examples_core.py:243] Task 13/64: Found 2793 candidate variants. I0104 18:49:24.340478 140179943589696 make_examples_core.py:243] Task 13/64: Created 2879 examples. Building DAG of jobs... Using shell: /usr/bin/bash. Provided cores: 64. Rules claiming more threads will be scaled down. Select jobs",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/602
https://github.com/google/deepvariant/issues/602:1685,energy efficiency,GPU,GPU,1685,"le interactive session, keep checking the log file and restart the run every time it fails but I guess that's less than optimal plus this way I can really only run one sample at the time. For the interactive sessions I request 180G and 64cpus (in my case it's: ```srsh --mem=180G --cpus-per-task=64 --partition=long```). . I would request same parameters when using --cluster so:. ```snakemake --cluster ""sbatch --mem=180G cpus-per-task=64"" --jobs 64 --profie profile/ ```(where profile holds singularity args etc.). Singularity image is deepvariant_1.4.0.sif. my Snakemake rule:. ```. rule deepvariant:. input:. bam=rules.apply_bqsr.output.bam,. ref='/mnt/shared/scratch/kmarians/private/dyslexia_gatk/workflow/resources/genome.fasta'. output:. vcf=""results/deepvariant/{sample}.vcf.gz"". params:. model=""WES"". threads: . 64. resources:. mem_mb=163840. log:. ""logs/deepvariant/{sample}/stdout.log"". singularity:. ""singularity/deepvariant_1.4.0.sif"". # ""singularity/deepvariant_1.4.0-gpu.sif"" # for GPU. shell:. """""". /opt/deepvariant/bin/run_deepvariant --model_type {params.model} --ref {input.ref} --reads {input.bam} --output_vcf {output.vcf} --num_shards {threads} --make_examples_extra_args='vsc_min_count_snps=3,vsc_min_fraction_snps=0.2,vsc_min_count_indels=3,vsc_min_fraction_indels=0.10'. """""". ```. Below is the begening and end of the log file. I am happy to include the entire log file but there is nothing out of the ordinary between those lines below (same output as for jobs that finished successfully). Could you please advise on what parameters to change to successfully run DeepVariant by submitting it to the SLURM scheduler? . I0104 18:49:24.340415 140179943589696 make_examples_core.py:243] Task 13/64: Found 2793 candidate variants. I0104 18:49:24.340478 140179943589696 make_examples_core.py:243] Task 13/64: Created 2879 examples. Building DAG of jobs... Using shell: /usr/bin/bash. Provided cores: 64. Rules claiming more threads will be scaled down. Select jobs to execute... ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/602
https://github.com/google/deepvariant/issues/602:1761,energy efficiency,model,model,1761,"time it fails but I guess that's less than optimal plus this way I can really only run one sample at the time. For the interactive sessions I request 180G and 64cpus (in my case it's: ```srsh --mem=180G --cpus-per-task=64 --partition=long```). . I would request same parameters when using --cluster so:. ```snakemake --cluster ""sbatch --mem=180G cpus-per-task=64"" --jobs 64 --profie profile/ ```(where profile holds singularity args etc.). Singularity image is deepvariant_1.4.0.sif. my Snakemake rule:. ```. rule deepvariant:. input:. bam=rules.apply_bqsr.output.bam,. ref='/mnt/shared/scratch/kmarians/private/dyslexia_gatk/workflow/resources/genome.fasta'. output:. vcf=""results/deepvariant/{sample}.vcf.gz"". params:. model=""WES"". threads: . 64. resources:. mem_mb=163840. log:. ""logs/deepvariant/{sample}/stdout.log"". singularity:. ""singularity/deepvariant_1.4.0.sif"". # ""singularity/deepvariant_1.4.0-gpu.sif"" # for GPU. shell:. """""". /opt/deepvariant/bin/run_deepvariant --model_type {params.model} --ref {input.ref} --reads {input.bam} --output_vcf {output.vcf} --num_shards {threads} --make_examples_extra_args='vsc_min_count_snps=3,vsc_min_fraction_snps=0.2,vsc_min_count_indels=3,vsc_min_fraction_indels=0.10'. """""". ```. Below is the begening and end of the log file. I am happy to include the entire log file but there is nothing out of the ordinary between those lines below (same output as for jobs that finished successfully). Could you please advise on what parameters to change to successfully run DeepVariant by submitting it to the SLURM scheduler? . I0104 18:49:24.340415 140179943589696 make_examples_core.py:243] Task 13/64: Found 2793 candidate variants. I0104 18:49:24.340478 140179943589696 make_examples_core.py:243] Task 13/64: Created 2879 examples. Building DAG of jobs... Using shell: /usr/bin/bash. Provided cores: 64. Rules claiming more threads will be scaled down. Select jobs to execute... > [Wed Jan 4 18:30:51 2023]. > rule deepvariant:. > input: results/recal/s534",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/602
https://github.com/google/deepvariant/issues/602:2319,energy efficiency,schedul,scheduler,2319,"put.bam,. ref='/mnt/shared/scratch/kmarians/private/dyslexia_gatk/workflow/resources/genome.fasta'. output:. vcf=""results/deepvariant/{sample}.vcf.gz"". params:. model=""WES"". threads: . 64. resources:. mem_mb=163840. log:. ""logs/deepvariant/{sample}/stdout.log"". singularity:. ""singularity/deepvariant_1.4.0.sif"". # ""singularity/deepvariant_1.4.0-gpu.sif"" # for GPU. shell:. """""". /opt/deepvariant/bin/run_deepvariant --model_type {params.model} --ref {input.ref} --reads {input.bam} --output_vcf {output.vcf} --num_shards {threads} --make_examples_extra_args='vsc_min_count_snps=3,vsc_min_fraction_snps=0.2,vsc_min_count_indels=3,vsc_min_fraction_indels=0.10'. """""". ```. Below is the begening and end of the log file. I am happy to include the entire log file but there is nothing out of the ordinary between those lines below (same output as for jobs that finished successfully). Could you please advise on what parameters to change to successfully run DeepVariant by submitting it to the SLURM scheduler? . I0104 18:49:24.340415 140179943589696 make_examples_core.py:243] Task 13/64: Found 2793 candidate variants. I0104 18:49:24.340478 140179943589696 make_examples_core.py:243] Task 13/64: Created 2879 examples. Building DAG of jobs... Using shell: /usr/bin/bash. Provided cores: 64. Rules claiming more threads will be scaled down. Select jobs to execute... > [Wed Jan 4 18:30:51 2023]. > rule deepvariant:. > input: results/recal/s534_EKDN210017195-1A_HTTJ3DSX2_L2.bam, /mnt/shared/scratch/kmarians/private/dyslexia_gatk/workflow/resources/genome.fasta. > output: results/deepvariant/s534_EKDN210017195-1A_HTTJ3DSX2_L2.vcf.gz. > log: logs/deepvariant/s534_EKDN210017195-1A_HTTJ3DSX2_L2/stdout.log. > jobid: 0. > wildcards: sample=s534_EKDN210017195-1A_HTTJ3DSX2_L2. > threads: 64. > resources: mem_mb=163840, disk_mb=16401, tmpdir=/tmp/kmarians_4189323. > . > Activating singularity image singularity/deepvariant_1.4.0.sif. > INFO: Convert SIF file to sandbox... > I0104 18:31:03.183642 1397186",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/602
https://github.com/google/deepvariant/issues/602:2601,energy efficiency,core,cores,2601,"gularity/deepvariant_1.4.0.sif"". # ""singularity/deepvariant_1.4.0-gpu.sif"" # for GPU. shell:. """""". /opt/deepvariant/bin/run_deepvariant --model_type {params.model} --ref {input.ref} --reads {input.bam} --output_vcf {output.vcf} --num_shards {threads} --make_examples_extra_args='vsc_min_count_snps=3,vsc_min_fraction_snps=0.2,vsc_min_count_indels=3,vsc_min_fraction_indels=0.10'. """""". ```. Below is the begening and end of the log file. I am happy to include the entire log file but there is nothing out of the ordinary between those lines below (same output as for jobs that finished successfully). Could you please advise on what parameters to change to successfully run DeepVariant by submitting it to the SLURM scheduler? . I0104 18:49:24.340415 140179943589696 make_examples_core.py:243] Task 13/64: Found 2793 candidate variants. I0104 18:49:24.340478 140179943589696 make_examples_core.py:243] Task 13/64: Created 2879 examples. Building DAG of jobs... Using shell: /usr/bin/bash. Provided cores: 64. Rules claiming more threads will be scaled down. Select jobs to execute... > [Wed Jan 4 18:30:51 2023]. > rule deepvariant:. > input: results/recal/s534_EKDN210017195-1A_HTTJ3DSX2_L2.bam, /mnt/shared/scratch/kmarians/private/dyslexia_gatk/workflow/resources/genome.fasta. > output: results/deepvariant/s534_EKDN210017195-1A_HTTJ3DSX2_L2.vcf.gz. > log: logs/deepvariant/s534_EKDN210017195-1A_HTTJ3DSX2_L2/stdout.log. > jobid: 0. > wildcards: sample=s534_EKDN210017195-1A_HTTJ3DSX2_L2. > threads: 64. > resources: mem_mb=163840, disk_mb=16401, tmpdir=/tmp/kmarians_4189323. > . > Activating singularity image singularity/deepvariant_1.4.0.sif. > INFO: Convert SIF file to sandbox... > I0104 18:31:03.183642 139718628308800 run_deepvariant.py:342] Re-using the directory for intermediate results in /tmp/kmarians_4189323/tmpxrz5rqbp. > . > ***** Intermediate results will be written to /tmp/kmarians_4189323/tmpxrz5rqbp in docker. ****. > . > . > ***** Running the command:*****. > time seq 0 63",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/602
https://github.com/google/deepvariant/issues/602:2648,energy efficiency,scale,scaled,2648,"/deepvariant_1.4.0-gpu.sif"" # for GPU. shell:. """""". /opt/deepvariant/bin/run_deepvariant --model_type {params.model} --ref {input.ref} --reads {input.bam} --output_vcf {output.vcf} --num_shards {threads} --make_examples_extra_args='vsc_min_count_snps=3,vsc_min_fraction_snps=0.2,vsc_min_count_indels=3,vsc_min_fraction_indels=0.10'. """""". ```. Below is the begening and end of the log file. I am happy to include the entire log file but there is nothing out of the ordinary between those lines below (same output as for jobs that finished successfully). Could you please advise on what parameters to change to successfully run DeepVariant by submitting it to the SLURM scheduler? . I0104 18:49:24.340415 140179943589696 make_examples_core.py:243] Task 13/64: Found 2793 candidate variants. I0104 18:49:24.340478 140179943589696 make_examples_core.py:243] Task 13/64: Created 2879 examples. Building DAG of jobs... Using shell: /usr/bin/bash. Provided cores: 64. Rules claiming more threads will be scaled down. Select jobs to execute... > [Wed Jan 4 18:30:51 2023]. > rule deepvariant:. > input: results/recal/s534_EKDN210017195-1A_HTTJ3DSX2_L2.bam, /mnt/shared/scratch/kmarians/private/dyslexia_gatk/workflow/resources/genome.fasta. > output: results/deepvariant/s534_EKDN210017195-1A_HTTJ3DSX2_L2.vcf.gz. > log: logs/deepvariant/s534_EKDN210017195-1A_HTTJ3DSX2_L2/stdout.log. > jobid: 0. > wildcards: sample=s534_EKDN210017195-1A_HTTJ3DSX2_L2. > threads: 64. > resources: mem_mb=163840, disk_mb=16401, tmpdir=/tmp/kmarians_4189323. > . > Activating singularity image singularity/deepvariant_1.4.0.sif. > INFO: Convert SIF file to sandbox... > I0104 18:31:03.183642 139718628308800 run_deepvariant.py:342] Re-using the directory for intermediate results in /tmp/kmarians_4189323/tmpxrz5rqbp. > . > ***** Intermediate results will be written to /tmp/kmarians_4189323/tmpxrz5rqbp in docker. ****. > . > . > ***** Running the command:*****. > time seq 0 63 | parallel -q --halt 2 --line-buffer /opt/deep",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/602
https://github.com/google/deepvariant/issues/602:2860,energy efficiency,resourc,resources,2860,"mples_extra_args='vsc_min_count_snps=3,vsc_min_fraction_snps=0.2,vsc_min_count_indels=3,vsc_min_fraction_indels=0.10'. """""". ```. Below is the begening and end of the log file. I am happy to include the entire log file but there is nothing out of the ordinary between those lines below (same output as for jobs that finished successfully). Could you please advise on what parameters to change to successfully run DeepVariant by submitting it to the SLURM scheduler? . I0104 18:49:24.340415 140179943589696 make_examples_core.py:243] Task 13/64: Found 2793 candidate variants. I0104 18:49:24.340478 140179943589696 make_examples_core.py:243] Task 13/64: Created 2879 examples. Building DAG of jobs... Using shell: /usr/bin/bash. Provided cores: 64. Rules claiming more threads will be scaled down. Select jobs to execute... > [Wed Jan 4 18:30:51 2023]. > rule deepvariant:. > input: results/recal/s534_EKDN210017195-1A_HTTJ3DSX2_L2.bam, /mnt/shared/scratch/kmarians/private/dyslexia_gatk/workflow/resources/genome.fasta. > output: results/deepvariant/s534_EKDN210017195-1A_HTTJ3DSX2_L2.vcf.gz. > log: logs/deepvariant/s534_EKDN210017195-1A_HTTJ3DSX2_L2/stdout.log. > jobid: 0. > wildcards: sample=s534_EKDN210017195-1A_HTTJ3DSX2_L2. > threads: 64. > resources: mem_mb=163840, disk_mb=16401, tmpdir=/tmp/kmarians_4189323. > . > Activating singularity image singularity/deepvariant_1.4.0.sif. > INFO: Convert SIF file to sandbox... > I0104 18:31:03.183642 139718628308800 run_deepvariant.py:342] Re-using the directory for intermediate results in /tmp/kmarians_4189323/tmpxrz5rqbp. > . > ***** Intermediate results will be written to /tmp/kmarians_4189323/tmpxrz5rqbp in docker. ****. > . > . > ***** Running the command:*****. > time seq 0 63 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/mnt/shared/scratch/kmarians/private/dyslexia_gatk/workflow/resources/genome.fasta"" --reads ""results/recal/s534_EKDN210017195-1A_HTTJ3DSX2_L2.bam"" --examples ""/tmp/km",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/602
https://github.com/google/deepvariant/issues/602:3113,energy efficiency,resourc,resources,3113,"inary between those lines below (same output as for jobs that finished successfully). Could you please advise on what parameters to change to successfully run DeepVariant by submitting it to the SLURM scheduler? . I0104 18:49:24.340415 140179943589696 make_examples_core.py:243] Task 13/64: Found 2793 candidate variants. I0104 18:49:24.340478 140179943589696 make_examples_core.py:243] Task 13/64: Created 2879 examples. Building DAG of jobs... Using shell: /usr/bin/bash. Provided cores: 64. Rules claiming more threads will be scaled down. Select jobs to execute... > [Wed Jan 4 18:30:51 2023]. > rule deepvariant:. > input: results/recal/s534_EKDN210017195-1A_HTTJ3DSX2_L2.bam, /mnt/shared/scratch/kmarians/private/dyslexia_gatk/workflow/resources/genome.fasta. > output: results/deepvariant/s534_EKDN210017195-1A_HTTJ3DSX2_L2.vcf.gz. > log: logs/deepvariant/s534_EKDN210017195-1A_HTTJ3DSX2_L2/stdout.log. > jobid: 0. > wildcards: sample=s534_EKDN210017195-1A_HTTJ3DSX2_L2. > threads: 64. > resources: mem_mb=163840, disk_mb=16401, tmpdir=/tmp/kmarians_4189323. > . > Activating singularity image singularity/deepvariant_1.4.0.sif. > INFO: Convert SIF file to sandbox... > I0104 18:31:03.183642 139718628308800 run_deepvariant.py:342] Re-using the directory for intermediate results in /tmp/kmarians_4189323/tmpxrz5rqbp. > . > ***** Intermediate results will be written to /tmp/kmarians_4189323/tmpxrz5rqbp in docker. ****. > . > . > ***** Running the command:*****. > time seq 0 63 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/mnt/shared/scratch/kmarians/private/dyslexia_gatk/workflow/resources/genome.fasta"" --reads ""results/recal/s534_EKDN210017195-1A_HTTJ3DSX2_L2.bam"" --examples ""/tmp/kmarians_4189323/tmpxrz5rqbp/make_examples.tfrecord@64.gz"" --channels ""insert_size"" --vsc_min_count_indels ""3"" --vsc_min_count_snps ""3"" --vsc_min_fraction_indels ""0.10"" --vsc_min_fraction_snps ""0.2"" --task {}. > *. > *. > *. > I0104 18:49:24.340415 140179",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/602
https://github.com/google/deepvariant/issues/602:3759,energy efficiency,resourc,resources,3759,"_EKDN210017195-1A_HTTJ3DSX2_L2.bam, /mnt/shared/scratch/kmarians/private/dyslexia_gatk/workflow/resources/genome.fasta. > output: results/deepvariant/s534_EKDN210017195-1A_HTTJ3DSX2_L2.vcf.gz. > log: logs/deepvariant/s534_EKDN210017195-1A_HTTJ3DSX2_L2/stdout.log. > jobid: 0. > wildcards: sample=s534_EKDN210017195-1A_HTTJ3DSX2_L2. > threads: 64. > resources: mem_mb=163840, disk_mb=16401, tmpdir=/tmp/kmarians_4189323. > . > Activating singularity image singularity/deepvariant_1.4.0.sif. > INFO: Convert SIF file to sandbox... > I0104 18:31:03.183642 139718628308800 run_deepvariant.py:342] Re-using the directory for intermediate results in /tmp/kmarians_4189323/tmpxrz5rqbp. > . > ***** Intermediate results will be written to /tmp/kmarians_4189323/tmpxrz5rqbp in docker. ****. > . > . > ***** Running the command:*****. > time seq 0 63 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/mnt/shared/scratch/kmarians/private/dyslexia_gatk/workflow/resources/genome.fasta"" --reads ""results/recal/s534_EKDN210017195-1A_HTTJ3DSX2_L2.bam"" --examples ""/tmp/kmarians_4189323/tmpxrz5rqbp/make_examples.tfrecord@64.gz"" --channels ""insert_size"" --vsc_min_count_indels ""3"" --vsc_min_count_snps ""3"" --vsc_min_fraction_indels ""0.10"" --vsc_min_fraction_snps ""0.2"" --task {}. > *. > *. > *. > I0104 18:49:24.340415 140179943589696 make_examples_core.py:243] Task 13/64: Found 2793 candidate variants. > I0104 18:49:24.340478 140179943589696 make_examples_core.py:243] Task 13/64: Created 2879 examples. > real	18m21.465s. > user	893m16.250s. > sys	15m57.561s. > . > ***** Running the command:*****. > time /opt/deepvariant/bin/call_variants --outfile ""/tmp/kmarians_4189323/tmpxrz5rqbp/call_variants_output.tfrecord.gz"" --examples ""/tmp/kmarians_4189323/tmpxrz5rqbp/make_examples.tfrecord@64.gz"" --checkpoint ""/opt/models/wes/model.ckpt"" --openvino_model_dir ""/tmp/kmarians_4189323/tmpxrz5rqbp"". > . > I0104 18:49:26.695084 139646404110144 call_variants.py:317] From ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/602
https://github.com/google/deepvariant/issues/602:4612,energy efficiency,model,models,4612," -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/mnt/shared/scratch/kmarians/private/dyslexia_gatk/workflow/resources/genome.fasta"" --reads ""results/recal/s534_EKDN210017195-1A_HTTJ3DSX2_L2.bam"" --examples ""/tmp/kmarians_4189323/tmpxrz5rqbp/make_examples.tfrecord@64.gz"" --channels ""insert_size"" --vsc_min_count_indels ""3"" --vsc_min_count_snps ""3"" --vsc_min_fraction_indels ""0.10"" --vsc_min_fraction_snps ""0.2"" --task {}. > *. > *. > *. > I0104 18:49:24.340415 140179943589696 make_examples_core.py:243] Task 13/64: Found 2793 candidate variants. > I0104 18:49:24.340478 140179943589696 make_examples_core.py:243] Task 13/64: Created 2879 examples. > real	18m21.465s. > user	893m16.250s. > sys	15m57.561s. > . > ***** Running the command:*****. > time /opt/deepvariant/bin/call_variants --outfile ""/tmp/kmarians_4189323/tmpxrz5rqbp/call_variants_output.tfrecord.gz"" --examples ""/tmp/kmarians_4189323/tmpxrz5rqbp/make_examples.tfrecord@64.gz"" --checkpoint ""/opt/models/wes/model.ckpt"" --openvino_model_dir ""/tmp/kmarians_4189323/tmpxrz5rqbp"". > . > I0104 18:49:26.695084 139646404110144 call_variants.py:317] From /tmp/kmarians_4189323/tmpxrz5rqbp/make_examples.tfrecord-00000-of-00064.gz.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 19]. > I0104 18:49:26.697470 139646404110144 call_variants.py:317] From /opt/models/wes/model.ckpt.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 19]. > Traceback (most recent call last):. > File ""/tmp/kmarians_4189323/Bazel.runfiles_zims94rl/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 513, in <module>. > tf.compat.v1.app.run(). > File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. > _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). > File ""/tmp/kmarians_4189323/Bazel.runfiles_zims94rl/runfiles/absl_py/absl/app.py"",",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/602
https://github.com/google/deepvariant/issues/602:4623,energy efficiency,model,model,4623,"2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/mnt/shared/scratch/kmarians/private/dyslexia_gatk/workflow/resources/genome.fasta"" --reads ""results/recal/s534_EKDN210017195-1A_HTTJ3DSX2_L2.bam"" --examples ""/tmp/kmarians_4189323/tmpxrz5rqbp/make_examples.tfrecord@64.gz"" --channels ""insert_size"" --vsc_min_count_indels ""3"" --vsc_min_count_snps ""3"" --vsc_min_fraction_indels ""0.10"" --vsc_min_fraction_snps ""0.2"" --task {}. > *. > *. > *. > I0104 18:49:24.340415 140179943589696 make_examples_core.py:243] Task 13/64: Found 2793 candidate variants. > I0104 18:49:24.340478 140179943589696 make_examples_core.py:243] Task 13/64: Created 2879 examples. > real	18m21.465s. > user	893m16.250s. > sys	15m57.561s. > . > ***** Running the command:*****. > time /opt/deepvariant/bin/call_variants --outfile ""/tmp/kmarians_4189323/tmpxrz5rqbp/call_variants_output.tfrecord.gz"" --examples ""/tmp/kmarians_4189323/tmpxrz5rqbp/make_examples.tfrecord@64.gz"" --checkpoint ""/opt/models/wes/model.ckpt"" --openvino_model_dir ""/tmp/kmarians_4189323/tmpxrz5rqbp"". > . > I0104 18:49:26.695084 139646404110144 call_variants.py:317] From /tmp/kmarians_4189323/tmpxrz5rqbp/make_examples.tfrecord-00000-of-00064.gz.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 19]. > I0104 18:49:26.697470 139646404110144 call_variants.py:317] From /opt/models/wes/model.ckpt.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 19]. > Traceback (most recent call last):. > File ""/tmp/kmarians_4189323/Bazel.runfiles_zims94rl/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 513, in <module>. > tf.compat.v1.app.run(). > File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. > _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). > File ""/tmp/kmarians_4189323/Bazel.runfiles_zims94rl/runfiles/absl_py/absl/app.py"", line 300, ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/602
https://github.com/google/deepvariant/issues/602:5022,energy efficiency,model,models,5022,"""0.10"" --vsc_min_fraction_snps ""0.2"" --task {}. > *. > *. > *. > I0104 18:49:24.340415 140179943589696 make_examples_core.py:243] Task 13/64: Found 2793 candidate variants. > I0104 18:49:24.340478 140179943589696 make_examples_core.py:243] Task 13/64: Created 2879 examples. > real	18m21.465s. > user	893m16.250s. > sys	15m57.561s. > . > ***** Running the command:*****. > time /opt/deepvariant/bin/call_variants --outfile ""/tmp/kmarians_4189323/tmpxrz5rqbp/call_variants_output.tfrecord.gz"" --examples ""/tmp/kmarians_4189323/tmpxrz5rqbp/make_examples.tfrecord@64.gz"" --checkpoint ""/opt/models/wes/model.ckpt"" --openvino_model_dir ""/tmp/kmarians_4189323/tmpxrz5rqbp"". > . > I0104 18:49:26.695084 139646404110144 call_variants.py:317] From /tmp/kmarians_4189323/tmpxrz5rqbp/make_examples.tfrecord-00000-of-00064.gz.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 19]. > I0104 18:49:26.697470 139646404110144 call_variants.py:317] From /opt/models/wes/model.ckpt.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 19]. > Traceback (most recent call last):. > File ""/tmp/kmarians_4189323/Bazel.runfiles_zims94rl/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 513, in <module>. > tf.compat.v1.app.run(). > File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. > _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). > File ""/tmp/kmarians_4189323/Bazel.runfiles_zims94rl/runfiles/absl_py/absl/app.py"", line 300, in run. > _run_main(main, args). > File ""/tmp/kmarians_4189323/Bazel.runfiles_zims94rl/runfiles/absl_py/absl/app.py"", line 251, in _run_main. > sys.exit(main(argv)). > File ""/tmp/kmarians_4189323/Bazel.runfiles_zims94rl/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 494, in main. > call_variants(. > File ""/tmp/kmarians_4189323/Bazel.runfiles_zims94rl/runfiles/com_google_deepv",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/602
https://github.com/google/deepvariant/issues/602:5033,energy efficiency,model,model,5033,"c_min_fraction_snps ""0.2"" --task {}. > *. > *. > *. > I0104 18:49:24.340415 140179943589696 make_examples_core.py:243] Task 13/64: Found 2793 candidate variants. > I0104 18:49:24.340478 140179943589696 make_examples_core.py:243] Task 13/64: Created 2879 examples. > real	18m21.465s. > user	893m16.250s. > sys	15m57.561s. > . > ***** Running the command:*****. > time /opt/deepvariant/bin/call_variants --outfile ""/tmp/kmarians_4189323/tmpxrz5rqbp/call_variants_output.tfrecord.gz"" --examples ""/tmp/kmarians_4189323/tmpxrz5rqbp/make_examples.tfrecord@64.gz"" --checkpoint ""/opt/models/wes/model.ckpt"" --openvino_model_dir ""/tmp/kmarians_4189323/tmpxrz5rqbp"". > . > I0104 18:49:26.695084 139646404110144 call_variants.py:317] From /tmp/kmarians_4189323/tmpxrz5rqbp/make_examples.tfrecord-00000-of-00064.gz.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 19]. > I0104 18:49:26.697470 139646404110144 call_variants.py:317] From /opt/models/wes/model.ckpt.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 19]. > Traceback (most recent call last):. > File ""/tmp/kmarians_4189323/Bazel.runfiles_zims94rl/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 513, in <module>. > tf.compat.v1.app.run(). > File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. > _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). > File ""/tmp/kmarians_4189323/Bazel.runfiles_zims94rl/runfiles/absl_py/absl/app.py"", line 300, in run. > _run_main(main, args). > File ""/tmp/kmarians_4189323/Bazel.runfiles_zims94rl/runfiles/absl_py/absl/app.py"", line 251, in _run_main. > sys.exit(main(argv)). > File ""/tmp/kmarians_4189323/Bazel.runfiles_zims94rl/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 494, in main. > call_variants(. > File ""/tmp/kmarians_4189323/Bazel.runfiles_zims94rl/runfiles/com_google_deepvariant/deep",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/602
https://github.com/google/deepvariant/issues/602:181,integrability,pipelin,pipeline,181,"AttributeError: deleter; Hi DeepVariant team,. I'm running DeepVariant via Singularity via Snakemake on a HPC cluster and overall, if I create an interactive session, my entire WES pipeline runs fine until an odd DeepVariant job fails (by that I mean that let's say 30 deepvariant jobs finish ok and the 31st fails). I can then restart the Snakemake run and the same job will run fine, followed by more jobs that will also run fine until another odd jobs fails. This is also particularly true when I use the --cluster command in Snakemake (i.e., send it to a SLURM job scheduler from the master node) - in this event every single job fails. I could of course run all my samples on a single interactive session, keep checking the log file and restart the run every time it fails but I guess that's less than optimal plus this way I can really only run one sample at the time. For the interactive sessions I request 180G and 64cpus (in my case it's: ```srsh --mem=180G --cpus-per-task=64 --partition=long```). . I would request same parameters when using --cluster so:. ```snakemake --cluster ""sbatch --mem=180G cpus-per-task=64"" --jobs 64 --profie profile/ ```(where profile holds singularity args etc.). Singularity image is deepvariant_1.4.0.sif. my Snakemake rule:. ```. rule deepvariant:. input:. bam=rules.apply_bqsr.output.bam,. ref='/mnt/shared/scratch/kmarians/private/dyslexia_gatk/workflow/resources/genome.fasta'. output:. vcf=""results/deepvariant/{sample}.vcf.gz"". params:. model=""WES"". threads: . 64. resources:. mem_mb=163840. log:. ""logs/deepvariant/{sample}/stdout.log"". singularity:. ""singularity/deepvariant_1.4.0.sif"". # ""singularity/deepvariant_1.4.0-gpu.sif"" # for GPU. shell:. """""". /opt/deepvariant/bin/run_deepvariant --model_type {params.model} --ref {input.ref} --reads {input.bam} --output_vcf {output.vcf} --num_shards {threads} --make_examples_extra_args='vsc_min_count_snps=3,vsc_min_fraction_snps=0.2,vsc_min_count_indels=3,vsc_min_fraction_indels=0.10'. """""". ```. Below ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/602
https://github.com/google/deepvariant/issues/602:611,integrability,event,event,611,"AttributeError: deleter; Hi DeepVariant team,. I'm running DeepVariant via Singularity via Snakemake on a HPC cluster and overall, if I create an interactive session, my entire WES pipeline runs fine until an odd DeepVariant job fails (by that I mean that let's say 30 deepvariant jobs finish ok and the 31st fails). I can then restart the Snakemake run and the same job will run fine, followed by more jobs that will also run fine until another odd jobs fails. This is also particularly true when I use the --cluster command in Snakemake (i.e., send it to a SLURM job scheduler from the master node) - in this event every single job fails. I could of course run all my samples on a single interactive session, keep checking the log file and restart the run every time it fails but I guess that's less than optimal plus this way I can really only run one sample at the time. For the interactive sessions I request 180G and 64cpus (in my case it's: ```srsh --mem=180G --cpus-per-task=64 --partition=long```). . I would request same parameters when using --cluster so:. ```snakemake --cluster ""sbatch --mem=180G cpus-per-task=64"" --jobs 64 --profie profile/ ```(where profile holds singularity args etc.). Singularity image is deepvariant_1.4.0.sif. my Snakemake rule:. ```. rule deepvariant:. input:. bam=rules.apply_bqsr.output.bam,. ref='/mnt/shared/scratch/kmarians/private/dyslexia_gatk/workflow/resources/genome.fasta'. output:. vcf=""results/deepvariant/{sample}.vcf.gz"". params:. model=""WES"". threads: . 64. resources:. mem_mb=163840. log:. ""logs/deepvariant/{sample}/stdout.log"". singularity:. ""singularity/deepvariant_1.4.0.sif"". # ""singularity/deepvariant_1.4.0-gpu.sif"" # for GPU. shell:. """""". /opt/deepvariant/bin/run_deepvariant --model_type {params.model} --ref {input.ref} --reads {input.bam} --output_vcf {output.vcf} --num_shards {threads} --make_examples_extra_args='vsc_min_count_snps=3,vsc_min_fraction_snps=0.2,vsc_min_count_indels=3,vsc_min_fraction_indels=0.10'. """""". ```. Below ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/602
https://github.com/google/deepvariant/issues/602:2292,integrability,sub,submitting,2292,":. bam=rules.apply_bqsr.output.bam,. ref='/mnt/shared/scratch/kmarians/private/dyslexia_gatk/workflow/resources/genome.fasta'. output:. vcf=""results/deepvariant/{sample}.vcf.gz"". params:. model=""WES"". threads: . 64. resources:. mem_mb=163840. log:. ""logs/deepvariant/{sample}/stdout.log"". singularity:. ""singularity/deepvariant_1.4.0.sif"". # ""singularity/deepvariant_1.4.0-gpu.sif"" # for GPU. shell:. """""". /opt/deepvariant/bin/run_deepvariant --model_type {params.model} --ref {input.ref} --reads {input.bam} --output_vcf {output.vcf} --num_shards {threads} --make_examples_extra_args='vsc_min_count_snps=3,vsc_min_fraction_snps=0.2,vsc_min_count_indels=3,vsc_min_fraction_indels=0.10'. """""". ```. Below is the begening and end of the log file. I am happy to include the entire log file but there is nothing out of the ordinary between those lines below (same output as for jobs that finished successfully). Could you please advise on what parameters to change to successfully run DeepVariant by submitting it to the SLURM scheduler? . I0104 18:49:24.340415 140179943589696 make_examples_core.py:243] Task 13/64: Found 2793 candidate variants. I0104 18:49:24.340478 140179943589696 make_examples_core.py:243] Task 13/64: Created 2879 examples. Building DAG of jobs... Using shell: /usr/bin/bash. Provided cores: 64. Rules claiming more threads will be scaled down. Select jobs to execute... > [Wed Jan 4 18:30:51 2023]. > rule deepvariant:. > input: results/recal/s534_EKDN210017195-1A_HTTJ3DSX2_L2.bam, /mnt/shared/scratch/kmarians/private/dyslexia_gatk/workflow/resources/genome.fasta. > output: results/deepvariant/s534_EKDN210017195-1A_HTTJ3DSX2_L2.vcf.gz. > log: logs/deepvariant/s534_EKDN210017195-1A_HTTJ3DSX2_L2/stdout.log. > jobid: 0. > wildcards: sample=s534_EKDN210017195-1A_HTTJ3DSX2_L2. > threads: 64. > resources: mem_mb=163840, disk_mb=16401, tmpdir=/tmp/kmarians_4189323. > . > Activating singularity image singularity/deepvariant_1.4.0.sif. > INFO: Convert SIF file to sandbox... > I0",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/602
https://github.com/google/deepvariant/issues/602:3635,integrability,buffer,buffer,3635,"s will be scaled down. Select jobs to execute... > [Wed Jan 4 18:30:51 2023]. > rule deepvariant:. > input: results/recal/s534_EKDN210017195-1A_HTTJ3DSX2_L2.bam, /mnt/shared/scratch/kmarians/private/dyslexia_gatk/workflow/resources/genome.fasta. > output: results/deepvariant/s534_EKDN210017195-1A_HTTJ3DSX2_L2.vcf.gz. > log: logs/deepvariant/s534_EKDN210017195-1A_HTTJ3DSX2_L2/stdout.log. > jobid: 0. > wildcards: sample=s534_EKDN210017195-1A_HTTJ3DSX2_L2. > threads: 64. > resources: mem_mb=163840, disk_mb=16401, tmpdir=/tmp/kmarians_4189323. > . > Activating singularity image singularity/deepvariant_1.4.0.sif. > INFO: Convert SIF file to sandbox... > I0104 18:31:03.183642 139718628308800 run_deepvariant.py:342] Re-using the directory for intermediate results in /tmp/kmarians_4189323/tmpxrz5rqbp. > . > ***** Intermediate results will be written to /tmp/kmarians_4189323/tmpxrz5rqbp in docker. ****. > . > . > ***** Running the command:*****. > time seq 0 63 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/mnt/shared/scratch/kmarians/private/dyslexia_gatk/workflow/resources/genome.fasta"" --reads ""results/recal/s534_EKDN210017195-1A_HTTJ3DSX2_L2.bam"" --examples ""/tmp/kmarians_4189323/tmpxrz5rqbp/make_examples.tfrecord@64.gz"" --channels ""insert_size"" --vsc_min_count_indels ""3"" --vsc_min_count_snps ""3"" --vsc_min_fraction_indels ""0.10"" --vsc_min_fraction_snps ""0.2"" --task {}. > *. > *. > *. > I0104 18:49:24.340415 140179943589696 make_examples_core.py:243] Task 13/64: Found 2793 candidate variants. > I0104 18:49:24.340478 140179943589696 make_examples_core.py:243] Task 13/64: Created 2879 examples. > real	18m21.465s. > user	893m16.250s. > sys	15m57.561s. > . > ***** Running the command:*****. > time /opt/deepvariant/bin/call_variants --outfile ""/tmp/kmarians_4189323/tmpxrz5rqbp/call_variants_output.tfrecord.gz"" --examples ""/tmp/kmarians_4189323/tmpxrz5rqbp/make_examples.tfrecord@64.gz"" --checkpoint ""/opt/models/wes/model.ckpt"" --o",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/602
https://github.com/google/deepvariant/issues/602:1344,interoperability,share,shared,1344,"ke run and the same job will run fine, followed by more jobs that will also run fine until another odd jobs fails. This is also particularly true when I use the --cluster command in Snakemake (i.e., send it to a SLURM job scheduler from the master node) - in this event every single job fails. I could of course run all my samples on a single interactive session, keep checking the log file and restart the run every time it fails but I guess that's less than optimal plus this way I can really only run one sample at the time. For the interactive sessions I request 180G and 64cpus (in my case it's: ```srsh --mem=180G --cpus-per-task=64 --partition=long```). . I would request same parameters when using --cluster so:. ```snakemake --cluster ""sbatch --mem=180G cpus-per-task=64"" --jobs 64 --profie profile/ ```(where profile holds singularity args etc.). Singularity image is deepvariant_1.4.0.sif. my Snakemake rule:. ```. rule deepvariant:. input:. bam=rules.apply_bqsr.output.bam,. ref='/mnt/shared/scratch/kmarians/private/dyslexia_gatk/workflow/resources/genome.fasta'. output:. vcf=""results/deepvariant/{sample}.vcf.gz"". params:. model=""WES"". threads: . 64. resources:. mem_mb=163840. log:. ""logs/deepvariant/{sample}/stdout.log"". singularity:. ""singularity/deepvariant_1.4.0.sif"". # ""singularity/deepvariant_1.4.0-gpu.sif"" # for GPU. shell:. """""". /opt/deepvariant/bin/run_deepvariant --model_type {params.model} --ref {input.ref} --reads {input.bam} --output_vcf {output.vcf} --num_shards {threads} --make_examples_extra_args='vsc_min_count_snps=3,vsc_min_fraction_snps=0.2,vsc_min_count_indels=3,vsc_min_fraction_indels=0.10'. """""". ```. Below is the begening and end of the log file. I am happy to include the entire log file but there is nothing out of the ordinary between those lines below (same output as for jobs that finished successfully). Could you please advise on what parameters to change to successfully run DeepVariant by submitting it to the SLURM scheduler? . I0104 18:49:24.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/602
https://github.com/google/deepvariant/issues/602:2805,interoperability,share,shared,2805,"output_vcf {output.vcf} --num_shards {threads} --make_examples_extra_args='vsc_min_count_snps=3,vsc_min_fraction_snps=0.2,vsc_min_count_indels=3,vsc_min_fraction_indels=0.10'. """""". ```. Below is the begening and end of the log file. I am happy to include the entire log file but there is nothing out of the ordinary between those lines below (same output as for jobs that finished successfully). Could you please advise on what parameters to change to successfully run DeepVariant by submitting it to the SLURM scheduler? . I0104 18:49:24.340415 140179943589696 make_examples_core.py:243] Task 13/64: Found 2793 candidate variants. I0104 18:49:24.340478 140179943589696 make_examples_core.py:243] Task 13/64: Created 2879 examples. Building DAG of jobs... Using shell: /usr/bin/bash. Provided cores: 64. Rules claiming more threads will be scaled down. Select jobs to execute... > [Wed Jan 4 18:30:51 2023]. > rule deepvariant:. > input: results/recal/s534_EKDN210017195-1A_HTTJ3DSX2_L2.bam, /mnt/shared/scratch/kmarians/private/dyslexia_gatk/workflow/resources/genome.fasta. > output: results/deepvariant/s534_EKDN210017195-1A_HTTJ3DSX2_L2.vcf.gz. > log: logs/deepvariant/s534_EKDN210017195-1A_HTTJ3DSX2_L2/stdout.log. > jobid: 0. > wildcards: sample=s534_EKDN210017195-1A_HTTJ3DSX2_L2. > threads: 64. > resources: mem_mb=163840, disk_mb=16401, tmpdir=/tmp/kmarians_4189323. > . > Activating singularity image singularity/deepvariant_1.4.0.sif. > INFO: Convert SIF file to sandbox... > I0104 18:31:03.183642 139718628308800 run_deepvariant.py:342] Re-using the directory for intermediate results in /tmp/kmarians_4189323/tmpxrz5rqbp. > . > ***** Intermediate results will be written to /tmp/kmarians_4189323/tmpxrz5rqbp in docker. ****. > . > . > ***** Running the command:*****. > time seq 0 63 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/mnt/shared/scratch/kmarians/private/dyslexia_gatk/workflow/resources/genome.fasta"" --reads ""results/recal/s5",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/602
https://github.com/google/deepvariant/issues/602:3704,interoperability,share,shared,3704,"1 2023]. > rule deepvariant:. > input: results/recal/s534_EKDN210017195-1A_HTTJ3DSX2_L2.bam, /mnt/shared/scratch/kmarians/private/dyslexia_gatk/workflow/resources/genome.fasta. > output: results/deepvariant/s534_EKDN210017195-1A_HTTJ3DSX2_L2.vcf.gz. > log: logs/deepvariant/s534_EKDN210017195-1A_HTTJ3DSX2_L2/stdout.log. > jobid: 0. > wildcards: sample=s534_EKDN210017195-1A_HTTJ3DSX2_L2. > threads: 64. > resources: mem_mb=163840, disk_mb=16401, tmpdir=/tmp/kmarians_4189323. > . > Activating singularity image singularity/deepvariant_1.4.0.sif. > INFO: Convert SIF file to sandbox... > I0104 18:31:03.183642 139718628308800 run_deepvariant.py:342] Re-using the directory for intermediate results in /tmp/kmarians_4189323/tmpxrz5rqbp. > . > ***** Intermediate results will be written to /tmp/kmarians_4189323/tmpxrz5rqbp in docker. ****. > . > . > ***** Running the command:*****. > time seq 0 63 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/mnt/shared/scratch/kmarians/private/dyslexia_gatk/workflow/resources/genome.fasta"" --reads ""results/recal/s534_EKDN210017195-1A_HTTJ3DSX2_L2.bam"" --examples ""/tmp/kmarians_4189323/tmpxrz5rqbp/make_examples.tfrecord@64.gz"" --channels ""insert_size"" --vsc_min_count_indels ""3"" --vsc_min_count_snps ""3"" --vsc_min_fraction_indels ""0.10"" --vsc_min_fraction_snps ""0.2"" --task {}. > *. > *. > *. > I0104 18:49:24.340415 140179943589696 make_examples_core.py:243] Task 13/64: Found 2793 candidate variants. > I0104 18:49:24.340478 140179943589696 make_examples_core.py:243] Task 13/64: Created 2879 examples. > real	18m21.465s. > user	893m16.250s. > sys	15m57.561s. > . > ***** Running the command:*****. > time /opt/deepvariant/bin/call_variants --outfile ""/tmp/kmarians_4189323/tmpxrz5rqbp/call_variants_output.tfrecord.gz"" --examples ""/tmp/kmarians_4189323/tmpxrz5rqbp/make_examples.tfrecord@64.gz"" --checkpoint ""/opt/models/wes/model.ckpt"" --openvino_model_dir ""/tmp/kmarians_4189323/tmpxrz5rqbp"". > . > I0104 18",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/602
https://github.com/google/deepvariant/issues/602:5424,interoperability,platform,platform,5424,"_variants --outfile ""/tmp/kmarians_4189323/tmpxrz5rqbp/call_variants_output.tfrecord.gz"" --examples ""/tmp/kmarians_4189323/tmpxrz5rqbp/make_examples.tfrecord@64.gz"" --checkpoint ""/opt/models/wes/model.ckpt"" --openvino_model_dir ""/tmp/kmarians_4189323/tmpxrz5rqbp"". > . > I0104 18:49:26.695084 139646404110144 call_variants.py:317] From /tmp/kmarians_4189323/tmpxrz5rqbp/make_examples.tfrecord-00000-of-00064.gz.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 19]. > I0104 18:49:26.697470 139646404110144 call_variants.py:317] From /opt/models/wes/model.ckpt.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 19]. > Traceback (most recent call last):. > File ""/tmp/kmarians_4189323/Bazel.runfiles_zims94rl/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 513, in <module>. > tf.compat.v1.app.run(). > File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. > _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). > File ""/tmp/kmarians_4189323/Bazel.runfiles_zims94rl/runfiles/absl_py/absl/app.py"", line 300, in run. > _run_main(main, args). > File ""/tmp/kmarians_4189323/Bazel.runfiles_zims94rl/runfiles/absl_py/absl/app.py"", line 251, in _run_main. > sys.exit(main(argv)). > File ""/tmp/kmarians_4189323/Bazel.runfiles_zims94rl/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 494, in main. > call_variants(. > File ""/tmp/kmarians_4189323/Bazel.runfiles_zims94rl/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 389, in call_variants. > init_op = tf.group(tf.compat.v1.global_variables_initializer(),. > File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/ops/variables.py"", line 3319, in global_variables_initializer. > return variables_initializer(global_variables()). > File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/ops/variables.py"", line 3",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/602
https://github.com/google/deepvariant/issues/602:1031,modifiability,paramet,parameters,1031,"ant team,. I'm running DeepVariant via Singularity via Snakemake on a HPC cluster and overall, if I create an interactive session, my entire WES pipeline runs fine until an odd DeepVariant job fails (by that I mean that let's say 30 deepvariant jobs finish ok and the 31st fails). I can then restart the Snakemake run and the same job will run fine, followed by more jobs that will also run fine until another odd jobs fails. This is also particularly true when I use the --cluster command in Snakemake (i.e., send it to a SLURM job scheduler from the master node) - in this event every single job fails. I could of course run all my samples on a single interactive session, keep checking the log file and restart the run every time it fails but I guess that's less than optimal plus this way I can really only run one sample at the time. For the interactive sessions I request 180G and 64cpus (in my case it's: ```srsh --mem=180G --cpus-per-task=64 --partition=long```). . I would request same parameters when using --cluster so:. ```snakemake --cluster ""sbatch --mem=180G cpus-per-task=64"" --jobs 64 --profie profile/ ```(where profile holds singularity args etc.). Singularity image is deepvariant_1.4.0.sif. my Snakemake rule:. ```. rule deepvariant:. input:. bam=rules.apply_bqsr.output.bam,. ref='/mnt/shared/scratch/kmarians/private/dyslexia_gatk/workflow/resources/genome.fasta'. output:. vcf=""results/deepvariant/{sample}.vcf.gz"". params:. model=""WES"". threads: . 64. resources:. mem_mb=163840. log:. ""logs/deepvariant/{sample}/stdout.log"". singularity:. ""singularity/deepvariant_1.4.0.sif"". # ""singularity/deepvariant_1.4.0-gpu.sif"" # for GPU. shell:. """""". /opt/deepvariant/bin/run_deepvariant --model_type {params.model} --ref {input.ref} --reads {input.bam} --output_vcf {output.vcf} --num_shards {threads} --make_examples_extra_args='vsc_min_count_snps=3,vsc_min_fraction_snps=0.2,vsc_min_count_indels=3,vsc_min_fraction_indels=0.10'. """""". ```. Below is the begening and end of the log f",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/602
https://github.com/google/deepvariant/issues/602:2236,modifiability,paramet,parameters,2236,"0.sif. my Snakemake rule:. ```. rule deepvariant:. input:. bam=rules.apply_bqsr.output.bam,. ref='/mnt/shared/scratch/kmarians/private/dyslexia_gatk/workflow/resources/genome.fasta'. output:. vcf=""results/deepvariant/{sample}.vcf.gz"". params:. model=""WES"". threads: . 64. resources:. mem_mb=163840. log:. ""logs/deepvariant/{sample}/stdout.log"". singularity:. ""singularity/deepvariant_1.4.0.sif"". # ""singularity/deepvariant_1.4.0-gpu.sif"" # for GPU. shell:. """""". /opt/deepvariant/bin/run_deepvariant --model_type {params.model} --ref {input.ref} --reads {input.bam} --output_vcf {output.vcf} --num_shards {threads} --make_examples_extra_args='vsc_min_count_snps=3,vsc_min_fraction_snps=0.2,vsc_min_count_indels=3,vsc_min_fraction_indels=0.10'. """""". ```. Below is the begening and end of the log file. I am happy to include the entire log file but there is nothing out of the ordinary between those lines below (same output as for jobs that finished successfully). Could you please advise on what parameters to change to successfully run DeepVariant by submitting it to the SLURM scheduler? . I0104 18:49:24.340415 140179943589696 make_examples_core.py:243] Task 13/64: Found 2793 candidate variants. I0104 18:49:24.340478 140179943589696 make_examples_core.py:243] Task 13/64: Created 2879 examples. Building DAG of jobs... Using shell: /usr/bin/bash. Provided cores: 64. Rules claiming more threads will be scaled down. Select jobs to execute... > [Wed Jan 4 18:30:51 2023]. > rule deepvariant:. > input: results/recal/s534_EKDN210017195-1A_HTTJ3DSX2_L2.bam, /mnt/shared/scratch/kmarians/private/dyslexia_gatk/workflow/resources/genome.fasta. > output: results/deepvariant/s534_EKDN210017195-1A_HTTJ3DSX2_L2.vcf.gz. > log: logs/deepvariant/s534_EKDN210017195-1A_HTTJ3DSX2_L2/stdout.log. > jobid: 0. > wildcards: sample=s534_EKDN210017195-1A_HTTJ3DSX2_L2. > threads: 64. > resources: mem_mb=163840, disk_mb=16401, tmpdir=/tmp/kmarians_4189323. > . > Activating singularity image singularity/deepvarian",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/602
https://github.com/google/deepvariant/issues/602:2648,modifiability,scal,scaled,2648,"/deepvariant_1.4.0-gpu.sif"" # for GPU. shell:. """""". /opt/deepvariant/bin/run_deepvariant --model_type {params.model} --ref {input.ref} --reads {input.bam} --output_vcf {output.vcf} --num_shards {threads} --make_examples_extra_args='vsc_min_count_snps=3,vsc_min_fraction_snps=0.2,vsc_min_count_indels=3,vsc_min_fraction_indels=0.10'. """""". ```. Below is the begening and end of the log file. I am happy to include the entire log file but there is nothing out of the ordinary between those lines below (same output as for jobs that finished successfully). Could you please advise on what parameters to change to successfully run DeepVariant by submitting it to the SLURM scheduler? . I0104 18:49:24.340415 140179943589696 make_examples_core.py:243] Task 13/64: Found 2793 candidate variants. I0104 18:49:24.340478 140179943589696 make_examples_core.py:243] Task 13/64: Created 2879 examples. Building DAG of jobs... Using shell: /usr/bin/bash. Provided cores: 64. Rules claiming more threads will be scaled down. Select jobs to execute... > [Wed Jan 4 18:30:51 2023]. > rule deepvariant:. > input: results/recal/s534_EKDN210017195-1A_HTTJ3DSX2_L2.bam, /mnt/shared/scratch/kmarians/private/dyslexia_gatk/workflow/resources/genome.fasta. > output: results/deepvariant/s534_EKDN210017195-1A_HTTJ3DSX2_L2.vcf.gz. > log: logs/deepvariant/s534_EKDN210017195-1A_HTTJ3DSX2_L2/stdout.log. > jobid: 0. > wildcards: sample=s534_EKDN210017195-1A_HTTJ3DSX2_L2. > threads: 64. > resources: mem_mb=163840, disk_mb=16401, tmpdir=/tmp/kmarians_4189323. > . > Activating singularity image singularity/deepvariant_1.4.0.sif. > INFO: Convert SIF file to sandbox... > I0104 18:31:03.183642 139718628308800 run_deepvariant.py:342] Re-using the directory for intermediate results in /tmp/kmarians_4189323/tmpxrz5rqbp. > . > ***** Intermediate results will be written to /tmp/kmarians_4189323/tmpxrz5rqbp in docker. ****. > . > . > ***** Running the command:*****. > time seq 0 63 | parallel -q --halt 2 --line-buffer /opt/deep",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/602
https://github.com/google/deepvariant/issues/602:3384,modifiability,interm,intermediate,3384,"y:243] Task 13/64: Found 2793 candidate variants. I0104 18:49:24.340478 140179943589696 make_examples_core.py:243] Task 13/64: Created 2879 examples. Building DAG of jobs... Using shell: /usr/bin/bash. Provided cores: 64. Rules claiming more threads will be scaled down. Select jobs to execute... > [Wed Jan 4 18:30:51 2023]. > rule deepvariant:. > input: results/recal/s534_EKDN210017195-1A_HTTJ3DSX2_L2.bam, /mnt/shared/scratch/kmarians/private/dyslexia_gatk/workflow/resources/genome.fasta. > output: results/deepvariant/s534_EKDN210017195-1A_HTTJ3DSX2_L2.vcf.gz. > log: logs/deepvariant/s534_EKDN210017195-1A_HTTJ3DSX2_L2/stdout.log. > jobid: 0. > wildcards: sample=s534_EKDN210017195-1A_HTTJ3DSX2_L2. > threads: 64. > resources: mem_mb=163840, disk_mb=16401, tmpdir=/tmp/kmarians_4189323. > . > Activating singularity image singularity/deepvariant_1.4.0.sif. > INFO: Convert SIF file to sandbox... > I0104 18:31:03.183642 139718628308800 run_deepvariant.py:342] Re-using the directory for intermediate results in /tmp/kmarians_4189323/tmpxrz5rqbp. > . > ***** Intermediate results will be written to /tmp/kmarians_4189323/tmpxrz5rqbp in docker. ****. > . > . > ***** Running the command:*****. > time seq 0 63 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/mnt/shared/scratch/kmarians/private/dyslexia_gatk/workflow/resources/genome.fasta"" --reads ""results/recal/s534_EKDN210017195-1A_HTTJ3DSX2_L2.bam"" --examples ""/tmp/kmarians_4189323/tmpxrz5rqbp/make_examples.tfrecord@64.gz"" --channels ""insert_size"" --vsc_min_count_indels ""3"" --vsc_min_count_snps ""3"" --vsc_min_fraction_indels ""0.10"" --vsc_min_fraction_snps ""0.2"" --task {}. > *. > *. > *. > I0104 18:49:24.340415 140179943589696 make_examples_core.py:243] Task 13/64: Found 2793 candidate variants. > I0104 18:49:24.340478 140179943589696 make_examples_core.py:243] Task 13/64: Created 2879 examples. > real	18m21.465s. > user	893m16.250s. > sys	15m57.561s. > . > ***** Running the command:*",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/602
https://github.com/google/deepvariant/issues/602:3455,modifiability,Interm,Intermediate,3455," 140179943589696 make_examples_core.py:243] Task 13/64: Created 2879 examples. Building DAG of jobs... Using shell: /usr/bin/bash. Provided cores: 64. Rules claiming more threads will be scaled down. Select jobs to execute... > [Wed Jan 4 18:30:51 2023]. > rule deepvariant:. > input: results/recal/s534_EKDN210017195-1A_HTTJ3DSX2_L2.bam, /mnt/shared/scratch/kmarians/private/dyslexia_gatk/workflow/resources/genome.fasta. > output: results/deepvariant/s534_EKDN210017195-1A_HTTJ3DSX2_L2.vcf.gz. > log: logs/deepvariant/s534_EKDN210017195-1A_HTTJ3DSX2_L2/stdout.log. > jobid: 0. > wildcards: sample=s534_EKDN210017195-1A_HTTJ3DSX2_L2. > threads: 64. > resources: mem_mb=163840, disk_mb=16401, tmpdir=/tmp/kmarians_4189323. > . > Activating singularity image singularity/deepvariant_1.4.0.sif. > INFO: Convert SIF file to sandbox... > I0104 18:31:03.183642 139718628308800 run_deepvariant.py:342] Re-using the directory for intermediate results in /tmp/kmarians_4189323/tmpxrz5rqbp. > . > ***** Intermediate results will be written to /tmp/kmarians_4189323/tmpxrz5rqbp in docker. ****. > . > . > ***** Running the command:*****. > time seq 0 63 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/mnt/shared/scratch/kmarians/private/dyslexia_gatk/workflow/resources/genome.fasta"" --reads ""results/recal/s534_EKDN210017195-1A_HTTJ3DSX2_L2.bam"" --examples ""/tmp/kmarians_4189323/tmpxrz5rqbp/make_examples.tfrecord@64.gz"" --channels ""insert_size"" --vsc_min_count_indels ""3"" --vsc_min_count_snps ""3"" --vsc_min_fraction_indels ""0.10"" --vsc_min_fraction_snps ""0.2"" --task {}. > *. > *. > *. > I0104 18:49:24.340415 140179943589696 make_examples_core.py:243] Task 13/64: Found 2793 candidate variants. > I0104 18:49:24.340478 140179943589696 make_examples_core.py:243] Task 13/64: Created 2879 examples. > real	18m21.465s. > user	893m16.250s. > sys	15m57.561s. > . > ***** Running the command:*****. > time /opt/deepvariant/bin/call_variants --outfile ""/tmp/kmarian",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/602
https://github.com/google/deepvariant/issues/602:5324,modifiability,modul,module,5324,"93m16.250s. > sys	15m57.561s. > . > ***** Running the command:*****. > time /opt/deepvariant/bin/call_variants --outfile ""/tmp/kmarians_4189323/tmpxrz5rqbp/call_variants_output.tfrecord.gz"" --examples ""/tmp/kmarians_4189323/tmpxrz5rqbp/make_examples.tfrecord@64.gz"" --checkpoint ""/opt/models/wes/model.ckpt"" --openvino_model_dir ""/tmp/kmarians_4189323/tmpxrz5rqbp"". > . > I0104 18:49:26.695084 139646404110144 call_variants.py:317] From /tmp/kmarians_4189323/tmpxrz5rqbp/make_examples.tfrecord-00000-of-00064.gz.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 19]. > I0104 18:49:26.697470 139646404110144 call_variants.py:317] From /opt/models/wes/model.ckpt.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 19]. > Traceback (most recent call last):. > File ""/tmp/kmarians_4189323/Bazel.runfiles_zims94rl/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 513, in <module>. > tf.compat.v1.app.run(). > File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. > _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). > File ""/tmp/kmarians_4189323/Bazel.runfiles_zims94rl/runfiles/absl_py/absl/app.py"", line 300, in run. > _run_main(main, args). > File ""/tmp/kmarians_4189323/Bazel.runfiles_zims94rl/runfiles/absl_py/absl/app.py"", line 251, in _run_main. > sys.exit(main(argv)). > File ""/tmp/kmarians_4189323/Bazel.runfiles_zims94rl/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 494, in main. > call_variants(. > File ""/tmp/kmarians_4189323/Bazel.runfiles_zims94rl/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 389, in call_variants. > init_op = tf.group(tf.compat.v1.global_variables_initializer(),. > File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/ops/variables.py"", line 3319, in global_variables_initializer. > return variables_initializer(global_var",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/602
https://github.com/google/deepvariant/issues/602:5397,modifiability,pac,packages,5397,"e /opt/deepvariant/bin/call_variants --outfile ""/tmp/kmarians_4189323/tmpxrz5rqbp/call_variants_output.tfrecord.gz"" --examples ""/tmp/kmarians_4189323/tmpxrz5rqbp/make_examples.tfrecord@64.gz"" --checkpoint ""/opt/models/wes/model.ckpt"" --openvino_model_dir ""/tmp/kmarians_4189323/tmpxrz5rqbp"". > . > I0104 18:49:26.695084 139646404110144 call_variants.py:317] From /tmp/kmarians_4189323/tmpxrz5rqbp/make_examples.tfrecord-00000-of-00064.gz.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 19]. > I0104 18:49:26.697470 139646404110144 call_variants.py:317] From /opt/models/wes/model.ckpt.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 19]. > Traceback (most recent call last):. > File ""/tmp/kmarians_4189323/Bazel.runfiles_zims94rl/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 513, in <module>. > tf.compat.v1.app.run(). > File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. > _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). > File ""/tmp/kmarians_4189323/Bazel.runfiles_zims94rl/runfiles/absl_py/absl/app.py"", line 300, in run. > _run_main(main, args). > File ""/tmp/kmarians_4189323/Bazel.runfiles_zims94rl/runfiles/absl_py/absl/app.py"", line 251, in _run_main. > sys.exit(main(argv)). > File ""/tmp/kmarians_4189323/Bazel.runfiles_zims94rl/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 494, in main. > call_variants(. > File ""/tmp/kmarians_4189323/Bazel.runfiles_zims94rl/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 389, in call_variants. > init_op = tf.group(tf.compat.v1.global_variables_initializer(),. > File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/ops/variables.py"", line 3319, in global_variables_initializer. > return variables_initializer(global_variables()). > File ""/usr/local/lib/python3.8/dist-packages/tensorflow/pytho",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/602
https://github.com/google/deepvariant/issues/602:6196,modifiability,pac,packages,6196,"""/tmp/kmarians_4189323/Bazel.runfiles_zims94rl/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 513, in <module>. > tf.compat.v1.app.run(). > File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. > _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). > File ""/tmp/kmarians_4189323/Bazel.runfiles_zims94rl/runfiles/absl_py/absl/app.py"", line 300, in run. > _run_main(main, args). > File ""/tmp/kmarians_4189323/Bazel.runfiles_zims94rl/runfiles/absl_py/absl/app.py"", line 251, in _run_main. > sys.exit(main(argv)). > File ""/tmp/kmarians_4189323/Bazel.runfiles_zims94rl/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 494, in main. > call_variants(. > File ""/tmp/kmarians_4189323/Bazel.runfiles_zims94rl/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 389, in call_variants. > init_op = tf.group(tf.compat.v1.global_variables_initializer(),. > File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/ops/variables.py"", line 3319, in global_variables_initializer. > return variables_initializer(global_variables()). > File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/ops/variables.py"", line 3139, in global_variables. > return ops.get_collection(ops.GraphKeys.GLOBAL_VARIABLES, scope). > File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py"", line 6605, in get_collection. > return get_default_graph().get_collection(key, scope). > File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py"", line 6231, in get_default_graph. > return _default_graph_stack.get_default(). > File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py"", line 5742, in get_default. > self._global_default_graph = Graph(). > File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py"", line 3113, in __init__. > self._scoped_c_graph = c_api_util.ScopedTFGraph(). > File ""/usr/local/lib/python3.8/",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/602
https://github.com/google/deepvariant/issues/602:6227,modifiability,variab,variables,6227,"files_zims94rl/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 513, in <module>. > tf.compat.v1.app.run(). > File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. > _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). > File ""/tmp/kmarians_4189323/Bazel.runfiles_zims94rl/runfiles/absl_py/absl/app.py"", line 300, in run. > _run_main(main, args). > File ""/tmp/kmarians_4189323/Bazel.runfiles_zims94rl/runfiles/absl_py/absl/app.py"", line 251, in _run_main. > sys.exit(main(argv)). > File ""/tmp/kmarians_4189323/Bazel.runfiles_zims94rl/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 494, in main. > call_variants(. > File ""/tmp/kmarians_4189323/Bazel.runfiles_zims94rl/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 389, in call_variants. > init_op = tf.group(tf.compat.v1.global_variables_initializer(),. > File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/ops/variables.py"", line 3319, in global_variables_initializer. > return variables_initializer(global_variables()). > File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/ops/variables.py"", line 3139, in global_variables. > return ops.get_collection(ops.GraphKeys.GLOBAL_VARIABLES, scope). > File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py"", line 6605, in get_collection. > return get_default_graph().get_collection(key, scope). > File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py"", line 6231, in get_default_graph. > return _default_graph_stack.get_default(). > File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py"", line 5742, in get_default. > self._global_default_graph = Graph(). > File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py"", line 3113, in __init__. > self._scoped_c_graph = c_api_util.ScopedTFGraph(). > File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/602
https://github.com/google/deepvariant/issues/602:6376,modifiability,pac,packages,6376,"b/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. > _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). > File ""/tmp/kmarians_4189323/Bazel.runfiles_zims94rl/runfiles/absl_py/absl/app.py"", line 300, in run. > _run_main(main, args). > File ""/tmp/kmarians_4189323/Bazel.runfiles_zims94rl/runfiles/absl_py/absl/app.py"", line 251, in _run_main. > sys.exit(main(argv)). > File ""/tmp/kmarians_4189323/Bazel.runfiles_zims94rl/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 494, in main. > call_variants(. > File ""/tmp/kmarians_4189323/Bazel.runfiles_zims94rl/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 389, in call_variants. > init_op = tf.group(tf.compat.v1.global_variables_initializer(),. > File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/ops/variables.py"", line 3319, in global_variables_initializer. > return variables_initializer(global_variables()). > File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/ops/variables.py"", line 3139, in global_variables. > return ops.get_collection(ops.GraphKeys.GLOBAL_VARIABLES, scope). > File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py"", line 6605, in get_collection. > return get_default_graph().get_collection(key, scope). > File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py"", line 6231, in get_default_graph. > return _default_graph_stack.get_default(). > File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py"", line 5742, in get_default. > self._global_default_graph = Graph(). > File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py"", line 3113, in __init__. > self._scoped_c_graph = c_api_util.ScopedTFGraph(). > File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/c_api_util.py"", line 50, in __init__. > self.graph = c_api.TF_NewGraph(). > RuntimeError: random_device::random_device(const std::string&)",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/602
https://github.com/google/deepvariant/issues/602:6407,modifiability,variab,variables,6407,"flow/python/platform/app.py"", line 40, in run. > _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). > File ""/tmp/kmarians_4189323/Bazel.runfiles_zims94rl/runfiles/absl_py/absl/app.py"", line 300, in run. > _run_main(main, args). > File ""/tmp/kmarians_4189323/Bazel.runfiles_zims94rl/runfiles/absl_py/absl/app.py"", line 251, in _run_main. > sys.exit(main(argv)). > File ""/tmp/kmarians_4189323/Bazel.runfiles_zims94rl/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 494, in main. > call_variants(. > File ""/tmp/kmarians_4189323/Bazel.runfiles_zims94rl/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 389, in call_variants. > init_op = tf.group(tf.compat.v1.global_variables_initializer(),. > File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/ops/variables.py"", line 3319, in global_variables_initializer. > return variables_initializer(global_variables()). > File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/ops/variables.py"", line 3139, in global_variables. > return ops.get_collection(ops.GraphKeys.GLOBAL_VARIABLES, scope). > File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py"", line 6605, in get_collection. > return get_default_graph().get_collection(key, scope). > File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py"", line 6231, in get_default_graph. > return _default_graph_stack.get_default(). > File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py"", line 5742, in get_default. > self._global_default_graph = Graph(). > File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py"", line 3113, in __init__. > self._scoped_c_graph = c_api_util.ScopedTFGraph(). > File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/c_api_util.py"", line 50, in __init__. > self.graph = c_api.TF_NewGraph(). > RuntimeError: random_device::random_device(const std::string&): device not available. > Except",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/602
https://github.com/google/deepvariant/issues/602:6560,modifiability,pac,packages,6560,"el.runfiles_zims94rl/runfiles/absl_py/absl/app.py"", line 300, in run. > _run_main(main, args). > File ""/tmp/kmarians_4189323/Bazel.runfiles_zims94rl/runfiles/absl_py/absl/app.py"", line 251, in _run_main. > sys.exit(main(argv)). > File ""/tmp/kmarians_4189323/Bazel.runfiles_zims94rl/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 494, in main. > call_variants(. > File ""/tmp/kmarians_4189323/Bazel.runfiles_zims94rl/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 389, in call_variants. > init_op = tf.group(tf.compat.v1.global_variables_initializer(),. > File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/ops/variables.py"", line 3319, in global_variables_initializer. > return variables_initializer(global_variables()). > File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/ops/variables.py"", line 3139, in global_variables. > return ops.get_collection(ops.GraphKeys.GLOBAL_VARIABLES, scope). > File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py"", line 6605, in get_collection. > return get_default_graph().get_collection(key, scope). > File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py"", line 6231, in get_default_graph. > return _default_graph_stack.get_default(). > File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py"", line 5742, in get_default. > self._global_default_graph = Graph(). > File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py"", line 3113, in __init__. > self._scoped_c_graph = c_api_util.ScopedTFGraph(). > File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/c_api_util.py"", line 50, in __init__. > self.graph = c_api.TF_NewGraph(). > RuntimeError: random_device::random_device(const std::string&): device not available. > Exception ignored in: <function ScopedTFGraph.__del__ at 0x7f01d64378b0>. > Traceback (most recent call last):. > File ""/usr/local/lib/python3.8/dist-packages",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/602
https://github.com/google/deepvariant/issues/602:6731,modifiability,pac,packages,6731,"s). > File ""/tmp/kmarians_4189323/Bazel.runfiles_zims94rl/runfiles/absl_py/absl/app.py"", line 251, in _run_main. > sys.exit(main(argv)). > File ""/tmp/kmarians_4189323/Bazel.runfiles_zims94rl/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 494, in main. > call_variants(. > File ""/tmp/kmarians_4189323/Bazel.runfiles_zims94rl/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 389, in call_variants. > init_op = tf.group(tf.compat.v1.global_variables_initializer(),. > File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/ops/variables.py"", line 3319, in global_variables_initializer. > return variables_initializer(global_variables()). > File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/ops/variables.py"", line 3139, in global_variables. > return ops.get_collection(ops.GraphKeys.GLOBAL_VARIABLES, scope). > File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py"", line 6605, in get_collection. > return get_default_graph().get_collection(key, scope). > File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py"", line 6231, in get_default_graph. > return _default_graph_stack.get_default(). > File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py"", line 5742, in get_default. > self._global_default_graph = Graph(). > File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py"", line 3113, in __init__. > self._scoped_c_graph = c_api_util.ScopedTFGraph(). > File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/c_api_util.py"", line 50, in __init__. > self.graph = c_api.TF_NewGraph(). > RuntimeError: random_device::random_device(const std::string&): device not available. > Exception ignored in: <function ScopedTFGraph.__del__ at 0x7f01d64378b0>. > Traceback (most recent call last):. > File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/c_api_util.py"", line 58, in __del__. > AttributeError: deleter",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/602
https://github.com/google/deepvariant/issues/602:6893,modifiability,pac,packages,6893,"s). > File ""/tmp/kmarians_4189323/Bazel.runfiles_zims94rl/runfiles/absl_py/absl/app.py"", line 251, in _run_main. > sys.exit(main(argv)). > File ""/tmp/kmarians_4189323/Bazel.runfiles_zims94rl/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 494, in main. > call_variants(. > File ""/tmp/kmarians_4189323/Bazel.runfiles_zims94rl/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 389, in call_variants. > init_op = tf.group(tf.compat.v1.global_variables_initializer(),. > File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/ops/variables.py"", line 3319, in global_variables_initializer. > return variables_initializer(global_variables()). > File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/ops/variables.py"", line 3139, in global_variables. > return ops.get_collection(ops.GraphKeys.GLOBAL_VARIABLES, scope). > File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py"", line 6605, in get_collection. > return get_default_graph().get_collection(key, scope). > File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py"", line 6231, in get_default_graph. > return _default_graph_stack.get_default(). > File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py"", line 5742, in get_default. > self._global_default_graph = Graph(). > File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py"", line 3113, in __init__. > self._scoped_c_graph = c_api_util.ScopedTFGraph(). > File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/c_api_util.py"", line 50, in __init__. > self.graph = c_api.TF_NewGraph(). > RuntimeError: random_device::random_device(const std::string&): device not available. > Exception ignored in: <function ScopedTFGraph.__del__ at 0x7f01d64378b0>. > Traceback (most recent call last):. > File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/c_api_util.py"", line 58, in __del__. > AttributeError: deleter",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/602
https://github.com/google/deepvariant/issues/602:7044,modifiability,pac,packages,7044,"s). > File ""/tmp/kmarians_4189323/Bazel.runfiles_zims94rl/runfiles/absl_py/absl/app.py"", line 251, in _run_main. > sys.exit(main(argv)). > File ""/tmp/kmarians_4189323/Bazel.runfiles_zims94rl/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 494, in main. > call_variants(. > File ""/tmp/kmarians_4189323/Bazel.runfiles_zims94rl/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 389, in call_variants. > init_op = tf.group(tf.compat.v1.global_variables_initializer(),. > File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/ops/variables.py"", line 3319, in global_variables_initializer. > return variables_initializer(global_variables()). > File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/ops/variables.py"", line 3139, in global_variables. > return ops.get_collection(ops.GraphKeys.GLOBAL_VARIABLES, scope). > File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py"", line 6605, in get_collection. > return get_default_graph().get_collection(key, scope). > File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py"", line 6231, in get_default_graph. > return _default_graph_stack.get_default(). > File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py"", line 5742, in get_default. > self._global_default_graph = Graph(). > File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py"", line 3113, in __init__. > self._scoped_c_graph = c_api_util.ScopedTFGraph(). > File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/c_api_util.py"", line 50, in __init__. > self.graph = c_api.TF_NewGraph(). > RuntimeError: random_device::random_device(const std::string&): device not available. > Exception ignored in: <function ScopedTFGraph.__del__ at 0x7f01d64378b0>. > Traceback (most recent call last):. > File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/c_api_util.py"", line 58, in __del__. > AttributeError: deleter",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/602
https://github.com/google/deepvariant/issues/602:7205,modifiability,pac,packages,7205,"s). > File ""/tmp/kmarians_4189323/Bazel.runfiles_zims94rl/runfiles/absl_py/absl/app.py"", line 251, in _run_main. > sys.exit(main(argv)). > File ""/tmp/kmarians_4189323/Bazel.runfiles_zims94rl/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 494, in main. > call_variants(. > File ""/tmp/kmarians_4189323/Bazel.runfiles_zims94rl/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 389, in call_variants. > init_op = tf.group(tf.compat.v1.global_variables_initializer(),. > File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/ops/variables.py"", line 3319, in global_variables_initializer. > return variables_initializer(global_variables()). > File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/ops/variables.py"", line 3139, in global_variables. > return ops.get_collection(ops.GraphKeys.GLOBAL_VARIABLES, scope). > File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py"", line 6605, in get_collection. > return get_default_graph().get_collection(key, scope). > File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py"", line 6231, in get_default_graph. > return _default_graph_stack.get_default(). > File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py"", line 5742, in get_default. > self._global_default_graph = Graph(). > File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py"", line 3113, in __init__. > self._scoped_c_graph = c_api_util.ScopedTFGraph(). > File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/c_api_util.py"", line 50, in __init__. > self.graph = c_api.TF_NewGraph(). > RuntimeError: random_device::random_device(const std::string&): device not available. > Exception ignored in: <function ScopedTFGraph.__del__ at 0x7f01d64378b0>. > Traceback (most recent call last):. > File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/c_api_util.py"", line 58, in __del__. > AttributeError: deleter",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/602
https://github.com/google/deepvariant/issues/602:7556,modifiability,pac,packages,7556,"s). > File ""/tmp/kmarians_4189323/Bazel.runfiles_zims94rl/runfiles/absl_py/absl/app.py"", line 251, in _run_main. > sys.exit(main(argv)). > File ""/tmp/kmarians_4189323/Bazel.runfiles_zims94rl/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 494, in main. > call_variants(. > File ""/tmp/kmarians_4189323/Bazel.runfiles_zims94rl/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 389, in call_variants. > init_op = tf.group(tf.compat.v1.global_variables_initializer(),. > File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/ops/variables.py"", line 3319, in global_variables_initializer. > return variables_initializer(global_variables()). > File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/ops/variables.py"", line 3139, in global_variables. > return ops.get_collection(ops.GraphKeys.GLOBAL_VARIABLES, scope). > File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py"", line 6605, in get_collection. > return get_default_graph().get_collection(key, scope). > File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py"", line 6231, in get_default_graph. > return _default_graph_stack.get_default(). > File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py"", line 5742, in get_default. > self._global_default_graph = Graph(). > File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py"", line 3113, in __init__. > self._scoped_c_graph = c_api_util.ScopedTFGraph(). > File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/c_api_util.py"", line 50, in __init__. > self.graph = c_api.TF_NewGraph(). > RuntimeError: random_device::random_device(const std::string&): device not available. > Exception ignored in: <function ScopedTFGraph.__del__ at 0x7f01d64378b0>. > Traceback (most recent call last):. > File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/c_api_util.py"", line 58, in __del__. > AttributeError: deleter",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/602
https://github.com/google/deepvariant/issues/602:569,performance,schedul,scheduler,569,"AttributeError: deleter; Hi DeepVariant team,. I'm running DeepVariant via Singularity via Snakemake on a HPC cluster and overall, if I create an interactive session, my entire WES pipeline runs fine until an odd DeepVariant job fails (by that I mean that let's say 30 deepvariant jobs finish ok and the 31st fails). I can then restart the Snakemake run and the same job will run fine, followed by more jobs that will also run fine until another odd jobs fails. This is also particularly true when I use the --cluster command in Snakemake (i.e., send it to a SLURM job scheduler from the master node) - in this event every single job fails. I could of course run all my samples on a single interactive session, keep checking the log file and restart the run every time it fails but I guess that's less than optimal plus this way I can really only run one sample at the time. For the interactive sessions I request 180G and 64cpus (in my case it's: ```srsh --mem=180G --cpus-per-task=64 --partition=long```). . I would request same parameters when using --cluster so:. ```snakemake --cluster ""sbatch --mem=180G cpus-per-task=64"" --jobs 64 --profie profile/ ```(where profile holds singularity args etc.). Singularity image is deepvariant_1.4.0.sif. my Snakemake rule:. ```. rule deepvariant:. input:. bam=rules.apply_bqsr.output.bam,. ref='/mnt/shared/scratch/kmarians/private/dyslexia_gatk/workflow/resources/genome.fasta'. output:. vcf=""results/deepvariant/{sample}.vcf.gz"". params:. model=""WES"". threads: . 64. resources:. mem_mb=163840. log:. ""logs/deepvariant/{sample}/stdout.log"". singularity:. ""singularity/deepvariant_1.4.0.sif"". # ""singularity/deepvariant_1.4.0-gpu.sif"" # for GPU. shell:. """""". /opt/deepvariant/bin/run_deepvariant --model_type {params.model} --ref {input.ref} --reads {input.bam} --output_vcf {output.vcf} --num_shards {threads} --make_examples_extra_args='vsc_min_count_snps=3,vsc_min_fraction_snps=0.2,vsc_min_count_indels=3,vsc_min_fraction_indels=0.10'. """""". ```. Below ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/602
https://github.com/google/deepvariant/issues/602:764,performance,time,time,764,"AttributeError: deleter; Hi DeepVariant team,. I'm running DeepVariant via Singularity via Snakemake on a HPC cluster and overall, if I create an interactive session, my entire WES pipeline runs fine until an odd DeepVariant job fails (by that I mean that let's say 30 deepvariant jobs finish ok and the 31st fails). I can then restart the Snakemake run and the same job will run fine, followed by more jobs that will also run fine until another odd jobs fails. This is also particularly true when I use the --cluster command in Snakemake (i.e., send it to a SLURM job scheduler from the master node) - in this event every single job fails. I could of course run all my samples on a single interactive session, keep checking the log file and restart the run every time it fails but I guess that's less than optimal plus this way I can really only run one sample at the time. For the interactive sessions I request 180G and 64cpus (in my case it's: ```srsh --mem=180G --cpus-per-task=64 --partition=long```). . I would request same parameters when using --cluster so:. ```snakemake --cluster ""sbatch --mem=180G cpus-per-task=64"" --jobs 64 --profie profile/ ```(where profile holds singularity args etc.). Singularity image is deepvariant_1.4.0.sif. my Snakemake rule:. ```. rule deepvariant:. input:. bam=rules.apply_bqsr.output.bam,. ref='/mnt/shared/scratch/kmarians/private/dyslexia_gatk/workflow/resources/genome.fasta'. output:. vcf=""results/deepvariant/{sample}.vcf.gz"". params:. model=""WES"". threads: . 64. resources:. mem_mb=163840. log:. ""logs/deepvariant/{sample}/stdout.log"". singularity:. ""singularity/deepvariant_1.4.0.sif"". # ""singularity/deepvariant_1.4.0-gpu.sif"" # for GPU. shell:. """""". /opt/deepvariant/bin/run_deepvariant --model_type {params.model} --ref {input.ref} --reads {input.bam} --output_vcf {output.vcf} --num_shards {threads} --make_examples_extra_args='vsc_min_count_snps=3,vsc_min_fraction_snps=0.2,vsc_min_count_indels=3,vsc_min_fraction_indels=0.10'. """""". ```. Below ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/602
https://github.com/google/deepvariant/issues/602:869,performance,time,time,869,"AttributeError: deleter; Hi DeepVariant team,. I'm running DeepVariant via Singularity via Snakemake on a HPC cluster and overall, if I create an interactive session, my entire WES pipeline runs fine until an odd DeepVariant job fails (by that I mean that let's say 30 deepvariant jobs finish ok and the 31st fails). I can then restart the Snakemake run and the same job will run fine, followed by more jobs that will also run fine until another odd jobs fails. This is also particularly true when I use the --cluster command in Snakemake (i.e., send it to a SLURM job scheduler from the master node) - in this event every single job fails. I could of course run all my samples on a single interactive session, keep checking the log file and restart the run every time it fails but I guess that's less than optimal plus this way I can really only run one sample at the time. For the interactive sessions I request 180G and 64cpus (in my case it's: ```srsh --mem=180G --cpus-per-task=64 --partition=long```). . I would request same parameters when using --cluster so:. ```snakemake --cluster ""sbatch --mem=180G cpus-per-task=64"" --jobs 64 --profie profile/ ```(where profile holds singularity args etc.). Singularity image is deepvariant_1.4.0.sif. my Snakemake rule:. ```. rule deepvariant:. input:. bam=rules.apply_bqsr.output.bam,. ref='/mnt/shared/scratch/kmarians/private/dyslexia_gatk/workflow/resources/genome.fasta'. output:. vcf=""results/deepvariant/{sample}.vcf.gz"". params:. model=""WES"". threads: . 64. resources:. mem_mb=163840. log:. ""logs/deepvariant/{sample}/stdout.log"". singularity:. ""singularity/deepvariant_1.4.0.sif"". # ""singularity/deepvariant_1.4.0-gpu.sif"" # for GPU. shell:. """""". /opt/deepvariant/bin/run_deepvariant --model_type {params.model} --ref {input.ref} --reads {input.bam} --output_vcf {output.vcf} --num_shards {threads} --make_examples_extra_args='vsc_min_count_snps=3,vsc_min_fraction_snps=0.2,vsc_min_count_indels=3,vsc_min_fraction_indels=0.10'. """""". ```. Below ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/602
https://github.com/google/deepvariant/issues/602:969,performance,cpu,cpus-per-task,969,"AttributeError: deleter; Hi DeepVariant team,. I'm running DeepVariant via Singularity via Snakemake on a HPC cluster and overall, if I create an interactive session, my entire WES pipeline runs fine until an odd DeepVariant job fails (by that I mean that let's say 30 deepvariant jobs finish ok and the 31st fails). I can then restart the Snakemake run and the same job will run fine, followed by more jobs that will also run fine until another odd jobs fails. This is also particularly true when I use the --cluster command in Snakemake (i.e., send it to a SLURM job scheduler from the master node) - in this event every single job fails. I could of course run all my samples on a single interactive session, keep checking the log file and restart the run every time it fails but I guess that's less than optimal plus this way I can really only run one sample at the time. For the interactive sessions I request 180G and 64cpus (in my case it's: ```srsh --mem=180G --cpus-per-task=64 --partition=long```). . I would request same parameters when using --cluster so:. ```snakemake --cluster ""sbatch --mem=180G cpus-per-task=64"" --jobs 64 --profie profile/ ```(where profile holds singularity args etc.). Singularity image is deepvariant_1.4.0.sif. my Snakemake rule:. ```. rule deepvariant:. input:. bam=rules.apply_bqsr.output.bam,. ref='/mnt/shared/scratch/kmarians/private/dyslexia_gatk/workflow/resources/genome.fasta'. output:. vcf=""results/deepvariant/{sample}.vcf.gz"". params:. model=""WES"". threads: . 64. resources:. mem_mb=163840. log:. ""logs/deepvariant/{sample}/stdout.log"". singularity:. ""singularity/deepvariant_1.4.0.sif"". # ""singularity/deepvariant_1.4.0-gpu.sif"" # for GPU. shell:. """""". /opt/deepvariant/bin/run_deepvariant --model_type {params.model} --ref {input.ref} --reads {input.bam} --output_vcf {output.vcf} --num_shards {threads} --make_examples_extra_args='vsc_min_count_snps=3,vsc_min_fraction_snps=0.2,vsc_min_count_indels=3,vsc_min_fraction_indels=0.10'. """""". ```. Below ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/602
https://github.com/google/deepvariant/issues/602:1110,performance,cpu,cpus-per-task,1110," and overall, if I create an interactive session, my entire WES pipeline runs fine until an odd DeepVariant job fails (by that I mean that let's say 30 deepvariant jobs finish ok and the 31st fails). I can then restart the Snakemake run and the same job will run fine, followed by more jobs that will also run fine until another odd jobs fails. This is also particularly true when I use the --cluster command in Snakemake (i.e., send it to a SLURM job scheduler from the master node) - in this event every single job fails. I could of course run all my samples on a single interactive session, keep checking the log file and restart the run every time it fails but I guess that's less than optimal plus this way I can really only run one sample at the time. For the interactive sessions I request 180G and 64cpus (in my case it's: ```srsh --mem=180G --cpus-per-task=64 --partition=long```). . I would request same parameters when using --cluster so:. ```snakemake --cluster ""sbatch --mem=180G cpus-per-task=64"" --jobs 64 --profie profile/ ```(where profile holds singularity args etc.). Singularity image is deepvariant_1.4.0.sif. my Snakemake rule:. ```. rule deepvariant:. input:. bam=rules.apply_bqsr.output.bam,. ref='/mnt/shared/scratch/kmarians/private/dyslexia_gatk/workflow/resources/genome.fasta'. output:. vcf=""results/deepvariant/{sample}.vcf.gz"". params:. model=""WES"". threads: . 64. resources:. mem_mb=163840. log:. ""logs/deepvariant/{sample}/stdout.log"". singularity:. ""singularity/deepvariant_1.4.0.sif"". # ""singularity/deepvariant_1.4.0-gpu.sif"" # for GPU. shell:. """""". /opt/deepvariant/bin/run_deepvariant --model_type {params.model} --ref {input.ref} --reads {input.bam} --output_vcf {output.vcf} --num_shards {threads} --make_examples_extra_args='vsc_min_count_snps=3,vsc_min_fraction_snps=0.2,vsc_min_count_indels=3,vsc_min_fraction_indels=0.10'. """""". ```. Below is the begening and end of the log file. I am happy to include the entire log file but there is nothing out of the or",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/602
https://github.com/google/deepvariant/issues/602:1147,performance,profil,profile,1147,"active session, my entire WES pipeline runs fine until an odd DeepVariant job fails (by that I mean that let's say 30 deepvariant jobs finish ok and the 31st fails). I can then restart the Snakemake run and the same job will run fine, followed by more jobs that will also run fine until another odd jobs fails. This is also particularly true when I use the --cluster command in Snakemake (i.e., send it to a SLURM job scheduler from the master node) - in this event every single job fails. I could of course run all my samples on a single interactive session, keep checking the log file and restart the run every time it fails but I guess that's less than optimal plus this way I can really only run one sample at the time. For the interactive sessions I request 180G and 64cpus (in my case it's: ```srsh --mem=180G --cpus-per-task=64 --partition=long```). . I would request same parameters when using --cluster so:. ```snakemake --cluster ""sbatch --mem=180G cpus-per-task=64"" --jobs 64 --profie profile/ ```(where profile holds singularity args etc.). Singularity image is deepvariant_1.4.0.sif. my Snakemake rule:. ```. rule deepvariant:. input:. bam=rules.apply_bqsr.output.bam,. ref='/mnt/shared/scratch/kmarians/private/dyslexia_gatk/workflow/resources/genome.fasta'. output:. vcf=""results/deepvariant/{sample}.vcf.gz"". params:. model=""WES"". threads: . 64. resources:. mem_mb=163840. log:. ""logs/deepvariant/{sample}/stdout.log"". singularity:. ""singularity/deepvariant_1.4.0.sif"". # ""singularity/deepvariant_1.4.0-gpu.sif"" # for GPU. shell:. """""". /opt/deepvariant/bin/run_deepvariant --model_type {params.model} --ref {input.ref} --reads {input.bam} --output_vcf {output.vcf} --num_shards {threads} --make_examples_extra_args='vsc_min_count_snps=3,vsc_min_fraction_snps=0.2,vsc_min_count_indels=3,vsc_min_fraction_indels=0.10'. """""". ```. Below is the begening and end of the log file. I am happy to include the entire log file but there is nothing out of the ordinary between those lines below (",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/602
https://github.com/google/deepvariant/issues/602:1166,performance,profil,profile,1166,"entire WES pipeline runs fine until an odd DeepVariant job fails (by that I mean that let's say 30 deepvariant jobs finish ok and the 31st fails). I can then restart the Snakemake run and the same job will run fine, followed by more jobs that will also run fine until another odd jobs fails. This is also particularly true when I use the --cluster command in Snakemake (i.e., send it to a SLURM job scheduler from the master node) - in this event every single job fails. I could of course run all my samples on a single interactive session, keep checking the log file and restart the run every time it fails but I guess that's less than optimal plus this way I can really only run one sample at the time. For the interactive sessions I request 180G and 64cpus (in my case it's: ```srsh --mem=180G --cpus-per-task=64 --partition=long```). . I would request same parameters when using --cluster so:. ```snakemake --cluster ""sbatch --mem=180G cpus-per-task=64"" --jobs 64 --profie profile/ ```(where profile holds singularity args etc.). Singularity image is deepvariant_1.4.0.sif. my Snakemake rule:. ```. rule deepvariant:. input:. bam=rules.apply_bqsr.output.bam,. ref='/mnt/shared/scratch/kmarians/private/dyslexia_gatk/workflow/resources/genome.fasta'. output:. vcf=""results/deepvariant/{sample}.vcf.gz"". params:. model=""WES"". threads: . 64. resources:. mem_mb=163840. log:. ""logs/deepvariant/{sample}/stdout.log"". singularity:. ""singularity/deepvariant_1.4.0.sif"". # ""singularity/deepvariant_1.4.0-gpu.sif"" # for GPU. shell:. """""". /opt/deepvariant/bin/run_deepvariant --model_type {params.model} --ref {input.ref} --reads {input.bam} --output_vcf {output.vcf} --num_shards {threads} --make_examples_extra_args='vsc_min_count_snps=3,vsc_min_fraction_snps=0.2,vsc_min_count_indels=3,vsc_min_fraction_indels=0.10'. """""". ```. Below is the begening and end of the log file. I am happy to include the entire log file but there is nothing out of the ordinary between those lines below (same output as for ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/602
https://github.com/google/deepvariant/issues/602:1399,performance,resourc,resources,1399,"obs that will also run fine until another odd jobs fails. This is also particularly true when I use the --cluster command in Snakemake (i.e., send it to a SLURM job scheduler from the master node) - in this event every single job fails. I could of course run all my samples on a single interactive session, keep checking the log file and restart the run every time it fails but I guess that's less than optimal plus this way I can really only run one sample at the time. For the interactive sessions I request 180G and 64cpus (in my case it's: ```srsh --mem=180G --cpus-per-task=64 --partition=long```). . I would request same parameters when using --cluster so:. ```snakemake --cluster ""sbatch --mem=180G cpus-per-task=64"" --jobs 64 --profie profile/ ```(where profile holds singularity args etc.). Singularity image is deepvariant_1.4.0.sif. my Snakemake rule:. ```. rule deepvariant:. input:. bam=rules.apply_bqsr.output.bam,. ref='/mnt/shared/scratch/kmarians/private/dyslexia_gatk/workflow/resources/genome.fasta'. output:. vcf=""results/deepvariant/{sample}.vcf.gz"". params:. model=""WES"". threads: . 64. resources:. mem_mb=163840. log:. ""logs/deepvariant/{sample}/stdout.log"". singularity:. ""singularity/deepvariant_1.4.0.sif"". # ""singularity/deepvariant_1.4.0-gpu.sif"" # for GPU. shell:. """""". /opt/deepvariant/bin/run_deepvariant --model_type {params.model} --ref {input.ref} --reads {input.bam} --output_vcf {output.vcf} --num_shards {threads} --make_examples_extra_args='vsc_min_count_snps=3,vsc_min_fraction_snps=0.2,vsc_min_count_indels=3,vsc_min_fraction_indels=0.10'. """""". ```. Below is the begening and end of the log file. I am happy to include the entire log file but there is nothing out of the ordinary between those lines below (same output as for jobs that finished successfully). Could you please advise on what parameters to change to successfully run DeepVariant by submitting it to the SLURM scheduler? . I0104 18:49:24.340415 140179943589696 make_examples_core.py:243] Task 13",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/602
https://github.com/google/deepvariant/issues/602:1513,performance,resourc,resources,1513,"command in Snakemake (i.e., send it to a SLURM job scheduler from the master node) - in this event every single job fails. I could of course run all my samples on a single interactive session, keep checking the log file and restart the run every time it fails but I guess that's less than optimal plus this way I can really only run one sample at the time. For the interactive sessions I request 180G and 64cpus (in my case it's: ```srsh --mem=180G --cpus-per-task=64 --partition=long```). . I would request same parameters when using --cluster so:. ```snakemake --cluster ""sbatch --mem=180G cpus-per-task=64"" --jobs 64 --profie profile/ ```(where profile holds singularity args etc.). Singularity image is deepvariant_1.4.0.sif. my Snakemake rule:. ```. rule deepvariant:. input:. bam=rules.apply_bqsr.output.bam,. ref='/mnt/shared/scratch/kmarians/private/dyslexia_gatk/workflow/resources/genome.fasta'. output:. vcf=""results/deepvariant/{sample}.vcf.gz"". params:. model=""WES"". threads: . 64. resources:. mem_mb=163840. log:. ""logs/deepvariant/{sample}/stdout.log"". singularity:. ""singularity/deepvariant_1.4.0.sif"". # ""singularity/deepvariant_1.4.0-gpu.sif"" # for GPU. shell:. """""". /opt/deepvariant/bin/run_deepvariant --model_type {params.model} --ref {input.ref} --reads {input.bam} --output_vcf {output.vcf} --num_shards {threads} --make_examples_extra_args='vsc_min_count_snps=3,vsc_min_fraction_snps=0.2,vsc_min_count_indels=3,vsc_min_fraction_indels=0.10'. """""". ```. Below is the begening and end of the log file. I am happy to include the entire log file but there is nothing out of the ordinary between those lines below (same output as for jobs that finished successfully). Could you please advise on what parameters to change to successfully run DeepVariant by submitting it to the SLURM scheduler? . I0104 18:49:24.340415 140179943589696 make_examples_core.py:243] Task 13/64: Found 2793 candidate variants. I0104 18:49:24.340478 140179943589696 make_examples_core.py:243] Task 13/64: C",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/602
https://github.com/google/deepvariant/issues/602:1670,performance,gpu,gpu,1670,"mples on a single interactive session, keep checking the log file and restart the run every time it fails but I guess that's less than optimal plus this way I can really only run one sample at the time. For the interactive sessions I request 180G and 64cpus (in my case it's: ```srsh --mem=180G --cpus-per-task=64 --partition=long```). . I would request same parameters when using --cluster so:. ```snakemake --cluster ""sbatch --mem=180G cpus-per-task=64"" --jobs 64 --profie profile/ ```(where profile holds singularity args etc.). Singularity image is deepvariant_1.4.0.sif. my Snakemake rule:. ```. rule deepvariant:. input:. bam=rules.apply_bqsr.output.bam,. ref='/mnt/shared/scratch/kmarians/private/dyslexia_gatk/workflow/resources/genome.fasta'. output:. vcf=""results/deepvariant/{sample}.vcf.gz"". params:. model=""WES"". threads: . 64. resources:. mem_mb=163840. log:. ""logs/deepvariant/{sample}/stdout.log"". singularity:. ""singularity/deepvariant_1.4.0.sif"". # ""singularity/deepvariant_1.4.0-gpu.sif"" # for GPU. shell:. """""". /opt/deepvariant/bin/run_deepvariant --model_type {params.model} --ref {input.ref} --reads {input.bam} --output_vcf {output.vcf} --num_shards {threads} --make_examples_extra_args='vsc_min_count_snps=3,vsc_min_fraction_snps=0.2,vsc_min_count_indels=3,vsc_min_fraction_indels=0.10'. """""". ```. Below is the begening and end of the log file. I am happy to include the entire log file but there is nothing out of the ordinary between those lines below (same output as for jobs that finished successfully). Could you please advise on what parameters to change to successfully run DeepVariant by submitting it to the SLURM scheduler? . I0104 18:49:24.340415 140179943589696 make_examples_core.py:243] Task 13/64: Found 2793 candidate variants. I0104 18:49:24.340478 140179943589696 make_examples_core.py:243] Task 13/64: Created 2879 examples. Building DAG of jobs... Using shell: /usr/bin/bash. Provided cores: 64. Rules claiming more threads will be scaled down. Select jobs",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/602
https://github.com/google/deepvariant/issues/602:1685,performance,GPU,GPU,1685,"le interactive session, keep checking the log file and restart the run every time it fails but I guess that's less than optimal plus this way I can really only run one sample at the time. For the interactive sessions I request 180G and 64cpus (in my case it's: ```srsh --mem=180G --cpus-per-task=64 --partition=long```). . I would request same parameters when using --cluster so:. ```snakemake --cluster ""sbatch --mem=180G cpus-per-task=64"" --jobs 64 --profie profile/ ```(where profile holds singularity args etc.). Singularity image is deepvariant_1.4.0.sif. my Snakemake rule:. ```. rule deepvariant:. input:. bam=rules.apply_bqsr.output.bam,. ref='/mnt/shared/scratch/kmarians/private/dyslexia_gatk/workflow/resources/genome.fasta'. output:. vcf=""results/deepvariant/{sample}.vcf.gz"". params:. model=""WES"". threads: . 64. resources:. mem_mb=163840. log:. ""logs/deepvariant/{sample}/stdout.log"". singularity:. ""singularity/deepvariant_1.4.0.sif"". # ""singularity/deepvariant_1.4.0-gpu.sif"" # for GPU. shell:. """""". /opt/deepvariant/bin/run_deepvariant --model_type {params.model} --ref {input.ref} --reads {input.bam} --output_vcf {output.vcf} --num_shards {threads} --make_examples_extra_args='vsc_min_count_snps=3,vsc_min_fraction_snps=0.2,vsc_min_count_indels=3,vsc_min_fraction_indels=0.10'. """""". ```. Below is the begening and end of the log file. I am happy to include the entire log file but there is nothing out of the ordinary between those lines below (same output as for jobs that finished successfully). Could you please advise on what parameters to change to successfully run DeepVariant by submitting it to the SLURM scheduler? . I0104 18:49:24.340415 140179943589696 make_examples_core.py:243] Task 13/64: Found 2793 candidate variants. I0104 18:49:24.340478 140179943589696 make_examples_core.py:243] Task 13/64: Created 2879 examples. Building DAG of jobs... Using shell: /usr/bin/bash. Provided cores: 64. Rules claiming more threads will be scaled down. Select jobs to execute... ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/602
https://github.com/google/deepvariant/issues/602:2319,performance,schedul,scheduler,2319,"put.bam,. ref='/mnt/shared/scratch/kmarians/private/dyslexia_gatk/workflow/resources/genome.fasta'. output:. vcf=""results/deepvariant/{sample}.vcf.gz"". params:. model=""WES"". threads: . 64. resources:. mem_mb=163840. log:. ""logs/deepvariant/{sample}/stdout.log"". singularity:. ""singularity/deepvariant_1.4.0.sif"". # ""singularity/deepvariant_1.4.0-gpu.sif"" # for GPU. shell:. """""". /opt/deepvariant/bin/run_deepvariant --model_type {params.model} --ref {input.ref} --reads {input.bam} --output_vcf {output.vcf} --num_shards {threads} --make_examples_extra_args='vsc_min_count_snps=3,vsc_min_fraction_snps=0.2,vsc_min_count_indels=3,vsc_min_fraction_indels=0.10'. """""". ```. Below is the begening and end of the log file. I am happy to include the entire log file but there is nothing out of the ordinary between those lines below (same output as for jobs that finished successfully). Could you please advise on what parameters to change to successfully run DeepVariant by submitting it to the SLURM scheduler? . I0104 18:49:24.340415 140179943589696 make_examples_core.py:243] Task 13/64: Found 2793 candidate variants. I0104 18:49:24.340478 140179943589696 make_examples_core.py:243] Task 13/64: Created 2879 examples. Building DAG of jobs... Using shell: /usr/bin/bash. Provided cores: 64. Rules claiming more threads will be scaled down. Select jobs to execute... > [Wed Jan 4 18:30:51 2023]. > rule deepvariant:. > input: results/recal/s534_EKDN210017195-1A_HTTJ3DSX2_L2.bam, /mnt/shared/scratch/kmarians/private/dyslexia_gatk/workflow/resources/genome.fasta. > output: results/deepvariant/s534_EKDN210017195-1A_HTTJ3DSX2_L2.vcf.gz. > log: logs/deepvariant/s534_EKDN210017195-1A_HTTJ3DSX2_L2/stdout.log. > jobid: 0. > wildcards: sample=s534_EKDN210017195-1A_HTTJ3DSX2_L2. > threads: 64. > resources: mem_mb=163840, disk_mb=16401, tmpdir=/tmp/kmarians_4189323. > . > Activating singularity image singularity/deepvariant_1.4.0.sif. > INFO: Convert SIF file to sandbox... > I0104 18:31:03.183642 1397186",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/602
https://github.com/google/deepvariant/issues/602:2648,performance,scale,scaled,2648,"/deepvariant_1.4.0-gpu.sif"" # for GPU. shell:. """""". /opt/deepvariant/bin/run_deepvariant --model_type {params.model} --ref {input.ref} --reads {input.bam} --output_vcf {output.vcf} --num_shards {threads} --make_examples_extra_args='vsc_min_count_snps=3,vsc_min_fraction_snps=0.2,vsc_min_count_indels=3,vsc_min_fraction_indels=0.10'. """""". ```. Below is the begening and end of the log file. I am happy to include the entire log file but there is nothing out of the ordinary between those lines below (same output as for jobs that finished successfully). Could you please advise on what parameters to change to successfully run DeepVariant by submitting it to the SLURM scheduler? . I0104 18:49:24.340415 140179943589696 make_examples_core.py:243] Task 13/64: Found 2793 candidate variants. I0104 18:49:24.340478 140179943589696 make_examples_core.py:243] Task 13/64: Created 2879 examples. Building DAG of jobs... Using shell: /usr/bin/bash. Provided cores: 64. Rules claiming more threads will be scaled down. Select jobs to execute... > [Wed Jan 4 18:30:51 2023]. > rule deepvariant:. > input: results/recal/s534_EKDN210017195-1A_HTTJ3DSX2_L2.bam, /mnt/shared/scratch/kmarians/private/dyslexia_gatk/workflow/resources/genome.fasta. > output: results/deepvariant/s534_EKDN210017195-1A_HTTJ3DSX2_L2.vcf.gz. > log: logs/deepvariant/s534_EKDN210017195-1A_HTTJ3DSX2_L2/stdout.log. > jobid: 0. > wildcards: sample=s534_EKDN210017195-1A_HTTJ3DSX2_L2. > threads: 64. > resources: mem_mb=163840, disk_mb=16401, tmpdir=/tmp/kmarians_4189323. > . > Activating singularity image singularity/deepvariant_1.4.0.sif. > INFO: Convert SIF file to sandbox... > I0104 18:31:03.183642 139718628308800 run_deepvariant.py:342] Re-using the directory for intermediate results in /tmp/kmarians_4189323/tmpxrz5rqbp. > . > ***** Intermediate results will be written to /tmp/kmarians_4189323/tmpxrz5rqbp in docker. ****. > . > . > ***** Running the command:*****. > time seq 0 63 | parallel -q --halt 2 --line-buffer /opt/deep",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/602
https://github.com/google/deepvariant/issues/602:2860,performance,resourc,resources,2860,"mples_extra_args='vsc_min_count_snps=3,vsc_min_fraction_snps=0.2,vsc_min_count_indels=3,vsc_min_fraction_indels=0.10'. """""". ```. Below is the begening and end of the log file. I am happy to include the entire log file but there is nothing out of the ordinary between those lines below (same output as for jobs that finished successfully). Could you please advise on what parameters to change to successfully run DeepVariant by submitting it to the SLURM scheduler? . I0104 18:49:24.340415 140179943589696 make_examples_core.py:243] Task 13/64: Found 2793 candidate variants. I0104 18:49:24.340478 140179943589696 make_examples_core.py:243] Task 13/64: Created 2879 examples. Building DAG of jobs... Using shell: /usr/bin/bash. Provided cores: 64. Rules claiming more threads will be scaled down. Select jobs to execute... > [Wed Jan 4 18:30:51 2023]. > rule deepvariant:. > input: results/recal/s534_EKDN210017195-1A_HTTJ3DSX2_L2.bam, /mnt/shared/scratch/kmarians/private/dyslexia_gatk/workflow/resources/genome.fasta. > output: results/deepvariant/s534_EKDN210017195-1A_HTTJ3DSX2_L2.vcf.gz. > log: logs/deepvariant/s534_EKDN210017195-1A_HTTJ3DSX2_L2/stdout.log. > jobid: 0. > wildcards: sample=s534_EKDN210017195-1A_HTTJ3DSX2_L2. > threads: 64. > resources: mem_mb=163840, disk_mb=16401, tmpdir=/tmp/kmarians_4189323. > . > Activating singularity image singularity/deepvariant_1.4.0.sif. > INFO: Convert SIF file to sandbox... > I0104 18:31:03.183642 139718628308800 run_deepvariant.py:342] Re-using the directory for intermediate results in /tmp/kmarians_4189323/tmpxrz5rqbp. > . > ***** Intermediate results will be written to /tmp/kmarians_4189323/tmpxrz5rqbp in docker. ****. > . > . > ***** Running the command:*****. > time seq 0 63 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/mnt/shared/scratch/kmarians/private/dyslexia_gatk/workflow/resources/genome.fasta"" --reads ""results/recal/s534_EKDN210017195-1A_HTTJ3DSX2_L2.bam"" --examples ""/tmp/km",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/602
https://github.com/google/deepvariant/issues/602:3113,performance,resourc,resources,3113,"inary between those lines below (same output as for jobs that finished successfully). Could you please advise on what parameters to change to successfully run DeepVariant by submitting it to the SLURM scheduler? . I0104 18:49:24.340415 140179943589696 make_examples_core.py:243] Task 13/64: Found 2793 candidate variants. I0104 18:49:24.340478 140179943589696 make_examples_core.py:243] Task 13/64: Created 2879 examples. Building DAG of jobs... Using shell: /usr/bin/bash. Provided cores: 64. Rules claiming more threads will be scaled down. Select jobs to execute... > [Wed Jan 4 18:30:51 2023]. > rule deepvariant:. > input: results/recal/s534_EKDN210017195-1A_HTTJ3DSX2_L2.bam, /mnt/shared/scratch/kmarians/private/dyslexia_gatk/workflow/resources/genome.fasta. > output: results/deepvariant/s534_EKDN210017195-1A_HTTJ3DSX2_L2.vcf.gz. > log: logs/deepvariant/s534_EKDN210017195-1A_HTTJ3DSX2_L2/stdout.log. > jobid: 0. > wildcards: sample=s534_EKDN210017195-1A_HTTJ3DSX2_L2. > threads: 64. > resources: mem_mb=163840, disk_mb=16401, tmpdir=/tmp/kmarians_4189323. > . > Activating singularity image singularity/deepvariant_1.4.0.sif. > INFO: Convert SIF file to sandbox... > I0104 18:31:03.183642 139718628308800 run_deepvariant.py:342] Re-using the directory for intermediate results in /tmp/kmarians_4189323/tmpxrz5rqbp. > . > ***** Intermediate results will be written to /tmp/kmarians_4189323/tmpxrz5rqbp in docker. ****. > . > . > ***** Running the command:*****. > time seq 0 63 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/mnt/shared/scratch/kmarians/private/dyslexia_gatk/workflow/resources/genome.fasta"" --reads ""results/recal/s534_EKDN210017195-1A_HTTJ3DSX2_L2.bam"" --examples ""/tmp/kmarians_4189323/tmpxrz5rqbp/make_examples.tfrecord@64.gz"" --channels ""insert_size"" --vsc_min_count_indels ""3"" --vsc_min_count_snps ""3"" --vsc_min_fraction_indels ""0.10"" --vsc_min_fraction_snps ""0.2"" --task {}. > *. > *. > *. > I0104 18:49:24.340415 140179",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/602
https://github.com/google/deepvariant/issues/602:3591,performance,time,time,3591,"rovided cores: 64. Rules claiming more threads will be scaled down. Select jobs to execute... > [Wed Jan 4 18:30:51 2023]. > rule deepvariant:. > input: results/recal/s534_EKDN210017195-1A_HTTJ3DSX2_L2.bam, /mnt/shared/scratch/kmarians/private/dyslexia_gatk/workflow/resources/genome.fasta. > output: results/deepvariant/s534_EKDN210017195-1A_HTTJ3DSX2_L2.vcf.gz. > log: logs/deepvariant/s534_EKDN210017195-1A_HTTJ3DSX2_L2/stdout.log. > jobid: 0. > wildcards: sample=s534_EKDN210017195-1A_HTTJ3DSX2_L2. > threads: 64. > resources: mem_mb=163840, disk_mb=16401, tmpdir=/tmp/kmarians_4189323. > . > Activating singularity image singularity/deepvariant_1.4.0.sif. > INFO: Convert SIF file to sandbox... > I0104 18:31:03.183642 139718628308800 run_deepvariant.py:342] Re-using the directory for intermediate results in /tmp/kmarians_4189323/tmpxrz5rqbp. > . > ***** Intermediate results will be written to /tmp/kmarians_4189323/tmpxrz5rqbp in docker. ****. > . > . > ***** Running the command:*****. > time seq 0 63 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/mnt/shared/scratch/kmarians/private/dyslexia_gatk/workflow/resources/genome.fasta"" --reads ""results/recal/s534_EKDN210017195-1A_HTTJ3DSX2_L2.bam"" --examples ""/tmp/kmarians_4189323/tmpxrz5rqbp/make_examples.tfrecord@64.gz"" --channels ""insert_size"" --vsc_min_count_indels ""3"" --vsc_min_count_snps ""3"" --vsc_min_fraction_indels ""0.10"" --vsc_min_fraction_snps ""0.2"" --task {}. > *. > *. > *. > I0104 18:49:24.340415 140179943589696 make_examples_core.py:243] Task 13/64: Found 2793 candidate variants. > I0104 18:49:24.340478 140179943589696 make_examples_core.py:243] Task 13/64: Created 2879 examples. > real	18m21.465s. > user	893m16.250s. > sys	15m57.561s. > . > ***** Running the command:*****. > time /opt/deepvariant/bin/call_variants --outfile ""/tmp/kmarians_4189323/tmpxrz5rqbp/call_variants_output.tfrecord.gz"" --examples ""/tmp/kmarians_4189323/tmpxrz5rqbp/make_examples.tfrecord@64.gz"" ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/602
https://github.com/google/deepvariant/issues/602:3607,performance,parallel,parallel,3607," Rules claiming more threads will be scaled down. Select jobs to execute... > [Wed Jan 4 18:30:51 2023]. > rule deepvariant:. > input: results/recal/s534_EKDN210017195-1A_HTTJ3DSX2_L2.bam, /mnt/shared/scratch/kmarians/private/dyslexia_gatk/workflow/resources/genome.fasta. > output: results/deepvariant/s534_EKDN210017195-1A_HTTJ3DSX2_L2.vcf.gz. > log: logs/deepvariant/s534_EKDN210017195-1A_HTTJ3DSX2_L2/stdout.log. > jobid: 0. > wildcards: sample=s534_EKDN210017195-1A_HTTJ3DSX2_L2. > threads: 64. > resources: mem_mb=163840, disk_mb=16401, tmpdir=/tmp/kmarians_4189323. > . > Activating singularity image singularity/deepvariant_1.4.0.sif. > INFO: Convert SIF file to sandbox... > I0104 18:31:03.183642 139718628308800 run_deepvariant.py:342] Re-using the directory for intermediate results in /tmp/kmarians_4189323/tmpxrz5rqbp. > . > ***** Intermediate results will be written to /tmp/kmarians_4189323/tmpxrz5rqbp in docker. ****. > . > . > ***** Running the command:*****. > time seq 0 63 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/mnt/shared/scratch/kmarians/private/dyslexia_gatk/workflow/resources/genome.fasta"" --reads ""results/recal/s534_EKDN210017195-1A_HTTJ3DSX2_L2.bam"" --examples ""/tmp/kmarians_4189323/tmpxrz5rqbp/make_examples.tfrecord@64.gz"" --channels ""insert_size"" --vsc_min_count_indels ""3"" --vsc_min_count_snps ""3"" --vsc_min_fraction_indels ""0.10"" --vsc_min_fraction_snps ""0.2"" --task {}. > *. > *. > *. > I0104 18:49:24.340415 140179943589696 make_examples_core.py:243] Task 13/64: Found 2793 candidate variants. > I0104 18:49:24.340478 140179943589696 make_examples_core.py:243] Task 13/64: Created 2879 examples. > real	18m21.465s. > user	893m16.250s. > sys	15m57.561s. > . > ***** Running the command:*****. > time /opt/deepvariant/bin/call_variants --outfile ""/tmp/kmarians_4189323/tmpxrz5rqbp/call_variants_output.tfrecord.gz"" --examples ""/tmp/kmarians_4189323/tmpxrz5rqbp/make_examples.tfrecord@64.gz"" --checkpoint ""/opt",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/602
https://github.com/google/deepvariant/issues/602:3759,performance,resourc,resources,3759,"_EKDN210017195-1A_HTTJ3DSX2_L2.bam, /mnt/shared/scratch/kmarians/private/dyslexia_gatk/workflow/resources/genome.fasta. > output: results/deepvariant/s534_EKDN210017195-1A_HTTJ3DSX2_L2.vcf.gz. > log: logs/deepvariant/s534_EKDN210017195-1A_HTTJ3DSX2_L2/stdout.log. > jobid: 0. > wildcards: sample=s534_EKDN210017195-1A_HTTJ3DSX2_L2. > threads: 64. > resources: mem_mb=163840, disk_mb=16401, tmpdir=/tmp/kmarians_4189323. > . > Activating singularity image singularity/deepvariant_1.4.0.sif. > INFO: Convert SIF file to sandbox... > I0104 18:31:03.183642 139718628308800 run_deepvariant.py:342] Re-using the directory for intermediate results in /tmp/kmarians_4189323/tmpxrz5rqbp. > . > ***** Intermediate results will be written to /tmp/kmarians_4189323/tmpxrz5rqbp in docker. ****. > . > . > ***** Running the command:*****. > time seq 0 63 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/mnt/shared/scratch/kmarians/private/dyslexia_gatk/workflow/resources/genome.fasta"" --reads ""results/recal/s534_EKDN210017195-1A_HTTJ3DSX2_L2.bam"" --examples ""/tmp/kmarians_4189323/tmpxrz5rqbp/make_examples.tfrecord@64.gz"" --channels ""insert_size"" --vsc_min_count_indels ""3"" --vsc_min_count_snps ""3"" --vsc_min_fraction_indels ""0.10"" --vsc_min_fraction_snps ""0.2"" --task {}. > *. > *. > *. > I0104 18:49:24.340415 140179943589696 make_examples_core.py:243] Task 13/64: Found 2793 candidate variants. > I0104 18:49:24.340478 140179943589696 make_examples_core.py:243] Task 13/64: Created 2879 examples. > real	18m21.465s. > user	893m16.250s. > sys	15m57.561s. > . > ***** Running the command:*****. > time /opt/deepvariant/bin/call_variants --outfile ""/tmp/kmarians_4189323/tmpxrz5rqbp/call_variants_output.tfrecord.gz"" --examples ""/tmp/kmarians_4189323/tmpxrz5rqbp/make_examples.tfrecord@64.gz"" --checkpoint ""/opt/models/wes/model.ckpt"" --openvino_model_dir ""/tmp/kmarians_4189323/tmpxrz5rqbp"". > . > I0104 18:49:26.695084 139646404110144 call_variants.py:317] From ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/602
https://github.com/google/deepvariant/issues/602:4398,performance,time,time,4398,"ults in /tmp/kmarians_4189323/tmpxrz5rqbp. > . > ***** Intermediate results will be written to /tmp/kmarians_4189323/tmpxrz5rqbp in docker. ****. > . > . > ***** Running the command:*****. > time seq 0 63 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/mnt/shared/scratch/kmarians/private/dyslexia_gatk/workflow/resources/genome.fasta"" --reads ""results/recal/s534_EKDN210017195-1A_HTTJ3DSX2_L2.bam"" --examples ""/tmp/kmarians_4189323/tmpxrz5rqbp/make_examples.tfrecord@64.gz"" --channels ""insert_size"" --vsc_min_count_indels ""3"" --vsc_min_count_snps ""3"" --vsc_min_fraction_indels ""0.10"" --vsc_min_fraction_snps ""0.2"" --task {}. > *. > *. > *. > I0104 18:49:24.340415 140179943589696 make_examples_core.py:243] Task 13/64: Found 2793 candidate variants. > I0104 18:49:24.340478 140179943589696 make_examples_core.py:243] Task 13/64: Created 2879 examples. > real	18m21.465s. > user	893m16.250s. > sys	15m57.561s. > . > ***** Running the command:*****. > time /opt/deepvariant/bin/call_variants --outfile ""/tmp/kmarians_4189323/tmpxrz5rqbp/call_variants_output.tfrecord.gz"" --examples ""/tmp/kmarians_4189323/tmpxrz5rqbp/make_examples.tfrecord@64.gz"" --checkpoint ""/opt/models/wes/model.ckpt"" --openvino_model_dir ""/tmp/kmarians_4189323/tmpxrz5rqbp"". > . > I0104 18:49:26.695084 139646404110144 call_variants.py:317] From /tmp/kmarians_4189323/tmpxrz5rqbp/make_examples.tfrecord-00000-of-00064.gz.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 19]. > I0104 18:49:26.697470 139646404110144 call_variants.py:317] From /opt/models/wes/model.ckpt.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 19]. > Traceback (most recent call last):. > File ""/tmp/kmarians_4189323/Bazel.runfiles_zims94rl/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 513, in <module>. > tf.compat.v1.app.run(). > File ""/usr/local/lib/python3.8/dist-pac",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/602
https://github.com/google/deepvariant/issues/602:229,reliability,fail,fails,229,"AttributeError: deleter; Hi DeepVariant team,. I'm running DeepVariant via Singularity via Snakemake on a HPC cluster and overall, if I create an interactive session, my entire WES pipeline runs fine until an odd DeepVariant job fails (by that I mean that let's say 30 deepvariant jobs finish ok and the 31st fails). I can then restart the Snakemake run and the same job will run fine, followed by more jobs that will also run fine until another odd jobs fails. This is also particularly true when I use the --cluster command in Snakemake (i.e., send it to a SLURM job scheduler from the master node) - in this event every single job fails. I could of course run all my samples on a single interactive session, keep checking the log file and restart the run every time it fails but I guess that's less than optimal plus this way I can really only run one sample at the time. For the interactive sessions I request 180G and 64cpus (in my case it's: ```srsh --mem=180G --cpus-per-task=64 --partition=long```). . I would request same parameters when using --cluster so:. ```snakemake --cluster ""sbatch --mem=180G cpus-per-task=64"" --jobs 64 --profie profile/ ```(where profile holds singularity args etc.). Singularity image is deepvariant_1.4.0.sif. my Snakemake rule:. ```. rule deepvariant:. input:. bam=rules.apply_bqsr.output.bam,. ref='/mnt/shared/scratch/kmarians/private/dyslexia_gatk/workflow/resources/genome.fasta'. output:. vcf=""results/deepvariant/{sample}.vcf.gz"". params:. model=""WES"". threads: . 64. resources:. mem_mb=163840. log:. ""logs/deepvariant/{sample}/stdout.log"". singularity:. ""singularity/deepvariant_1.4.0.sif"". # ""singularity/deepvariant_1.4.0-gpu.sif"" # for GPU. shell:. """""". /opt/deepvariant/bin/run_deepvariant --model_type {params.model} --ref {input.ref} --reads {input.bam} --output_vcf {output.vcf} --num_shards {threads} --make_examples_extra_args='vsc_min_count_snps=3,vsc_min_fraction_snps=0.2,vsc_min_count_indels=3,vsc_min_fraction_indels=0.10'. """""". ```. Below ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/602
https://github.com/google/deepvariant/issues/602:309,reliability,fail,fails,309,"AttributeError: deleter; Hi DeepVariant team,. I'm running DeepVariant via Singularity via Snakemake on a HPC cluster and overall, if I create an interactive session, my entire WES pipeline runs fine until an odd DeepVariant job fails (by that I mean that let's say 30 deepvariant jobs finish ok and the 31st fails). I can then restart the Snakemake run and the same job will run fine, followed by more jobs that will also run fine until another odd jobs fails. This is also particularly true when I use the --cluster command in Snakemake (i.e., send it to a SLURM job scheduler from the master node) - in this event every single job fails. I could of course run all my samples on a single interactive session, keep checking the log file and restart the run every time it fails but I guess that's less than optimal plus this way I can really only run one sample at the time. For the interactive sessions I request 180G and 64cpus (in my case it's: ```srsh --mem=180G --cpus-per-task=64 --partition=long```). . I would request same parameters when using --cluster so:. ```snakemake --cluster ""sbatch --mem=180G cpus-per-task=64"" --jobs 64 --profie profile/ ```(where profile holds singularity args etc.). Singularity image is deepvariant_1.4.0.sif. my Snakemake rule:. ```. rule deepvariant:. input:. bam=rules.apply_bqsr.output.bam,. ref='/mnt/shared/scratch/kmarians/private/dyslexia_gatk/workflow/resources/genome.fasta'. output:. vcf=""results/deepvariant/{sample}.vcf.gz"". params:. model=""WES"". threads: . 64. resources:. mem_mb=163840. log:. ""logs/deepvariant/{sample}/stdout.log"". singularity:. ""singularity/deepvariant_1.4.0.sif"". # ""singularity/deepvariant_1.4.0-gpu.sif"" # for GPU. shell:. """""". /opt/deepvariant/bin/run_deepvariant --model_type {params.model} --ref {input.ref} --reads {input.bam} --output_vcf {output.vcf} --num_shards {threads} --make_examples_extra_args='vsc_min_count_snps=3,vsc_min_fraction_snps=0.2,vsc_min_count_indels=3,vsc_min_fraction_indels=0.10'. """""". ```. Below ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/602
https://github.com/google/deepvariant/issues/602:455,reliability,fail,fails,455,"AttributeError: deleter; Hi DeepVariant team,. I'm running DeepVariant via Singularity via Snakemake on a HPC cluster and overall, if I create an interactive session, my entire WES pipeline runs fine until an odd DeepVariant job fails (by that I mean that let's say 30 deepvariant jobs finish ok and the 31st fails). I can then restart the Snakemake run and the same job will run fine, followed by more jobs that will also run fine until another odd jobs fails. This is also particularly true when I use the --cluster command in Snakemake (i.e., send it to a SLURM job scheduler from the master node) - in this event every single job fails. I could of course run all my samples on a single interactive session, keep checking the log file and restart the run every time it fails but I guess that's less than optimal plus this way I can really only run one sample at the time. For the interactive sessions I request 180G and 64cpus (in my case it's: ```srsh --mem=180G --cpus-per-task=64 --partition=long```). . I would request same parameters when using --cluster so:. ```snakemake --cluster ""sbatch --mem=180G cpus-per-task=64"" --jobs 64 --profie profile/ ```(where profile holds singularity args etc.). Singularity image is deepvariant_1.4.0.sif. my Snakemake rule:. ```. rule deepvariant:. input:. bam=rules.apply_bqsr.output.bam,. ref='/mnt/shared/scratch/kmarians/private/dyslexia_gatk/workflow/resources/genome.fasta'. output:. vcf=""results/deepvariant/{sample}.vcf.gz"". params:. model=""WES"". threads: . 64. resources:. mem_mb=163840. log:. ""logs/deepvariant/{sample}/stdout.log"". singularity:. ""singularity/deepvariant_1.4.0.sif"". # ""singularity/deepvariant_1.4.0-gpu.sif"" # for GPU. shell:. """""". /opt/deepvariant/bin/run_deepvariant --model_type {params.model} --ref {input.ref} --reads {input.bam} --output_vcf {output.vcf} --num_shards {threads} --make_examples_extra_args='vsc_min_count_snps=3,vsc_min_fraction_snps=0.2,vsc_min_count_indels=3,vsc_min_fraction_indels=0.10'. """""". ```. Below ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/602
https://github.com/google/deepvariant/issues/602:634,reliability,fail,fails,634,"AttributeError: deleter; Hi DeepVariant team,. I'm running DeepVariant via Singularity via Snakemake on a HPC cluster and overall, if I create an interactive session, my entire WES pipeline runs fine until an odd DeepVariant job fails (by that I mean that let's say 30 deepvariant jobs finish ok and the 31st fails). I can then restart the Snakemake run and the same job will run fine, followed by more jobs that will also run fine until another odd jobs fails. This is also particularly true when I use the --cluster command in Snakemake (i.e., send it to a SLURM job scheduler from the master node) - in this event every single job fails. I could of course run all my samples on a single interactive session, keep checking the log file and restart the run every time it fails but I guess that's less than optimal plus this way I can really only run one sample at the time. For the interactive sessions I request 180G and 64cpus (in my case it's: ```srsh --mem=180G --cpus-per-task=64 --partition=long```). . I would request same parameters when using --cluster so:. ```snakemake --cluster ""sbatch --mem=180G cpus-per-task=64"" --jobs 64 --profie profile/ ```(where profile holds singularity args etc.). Singularity image is deepvariant_1.4.0.sif. my Snakemake rule:. ```. rule deepvariant:. input:. bam=rules.apply_bqsr.output.bam,. ref='/mnt/shared/scratch/kmarians/private/dyslexia_gatk/workflow/resources/genome.fasta'. output:. vcf=""results/deepvariant/{sample}.vcf.gz"". params:. model=""WES"". threads: . 64. resources:. mem_mb=163840. log:. ""logs/deepvariant/{sample}/stdout.log"". singularity:. ""singularity/deepvariant_1.4.0.sif"". # ""singularity/deepvariant_1.4.0-gpu.sif"" # for GPU. shell:. """""". /opt/deepvariant/bin/run_deepvariant --model_type {params.model} --ref {input.ref} --reads {input.bam} --output_vcf {output.vcf} --num_shards {threads} --make_examples_extra_args='vsc_min_count_snps=3,vsc_min_fraction_snps=0.2,vsc_min_count_indels=3,vsc_min_fraction_indels=0.10'. """""". ```. Below ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/602
https://github.com/google/deepvariant/issues/602:772,reliability,fail,fails,772,"AttributeError: deleter; Hi DeepVariant team,. I'm running DeepVariant via Singularity via Snakemake on a HPC cluster and overall, if I create an interactive session, my entire WES pipeline runs fine until an odd DeepVariant job fails (by that I mean that let's say 30 deepvariant jobs finish ok and the 31st fails). I can then restart the Snakemake run and the same job will run fine, followed by more jobs that will also run fine until another odd jobs fails. This is also particularly true when I use the --cluster command in Snakemake (i.e., send it to a SLURM job scheduler from the master node) - in this event every single job fails. I could of course run all my samples on a single interactive session, keep checking the log file and restart the run every time it fails but I guess that's less than optimal plus this way I can really only run one sample at the time. For the interactive sessions I request 180G and 64cpus (in my case it's: ```srsh --mem=180G --cpus-per-task=64 --partition=long```). . I would request same parameters when using --cluster so:. ```snakemake --cluster ""sbatch --mem=180G cpus-per-task=64"" --jobs 64 --profie profile/ ```(where profile holds singularity args etc.). Singularity image is deepvariant_1.4.0.sif. my Snakemake rule:. ```. rule deepvariant:. input:. bam=rules.apply_bqsr.output.bam,. ref='/mnt/shared/scratch/kmarians/private/dyslexia_gatk/workflow/resources/genome.fasta'. output:. vcf=""results/deepvariant/{sample}.vcf.gz"". params:. model=""WES"". threads: . 64. resources:. mem_mb=163840. log:. ""logs/deepvariant/{sample}/stdout.log"". singularity:. ""singularity/deepvariant_1.4.0.sif"". # ""singularity/deepvariant_1.4.0-gpu.sif"" # for GPU. shell:. """""". /opt/deepvariant/bin/run_deepvariant --model_type {params.model} --ref {input.ref} --reads {input.bam} --output_vcf {output.vcf} --num_shards {threads} --make_examples_extra_args='vsc_min_count_snps=3,vsc_min_fraction_snps=0.2,vsc_min_count_indels=3,vsc_min_fraction_indels=0.10'. """""". ```. Below ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/602
https://github.com/google/deepvariant/issues/602:4595,reliability,checkpoint,checkpoint,4595,"0 63 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/mnt/shared/scratch/kmarians/private/dyslexia_gatk/workflow/resources/genome.fasta"" --reads ""results/recal/s534_EKDN210017195-1A_HTTJ3DSX2_L2.bam"" --examples ""/tmp/kmarians_4189323/tmpxrz5rqbp/make_examples.tfrecord@64.gz"" --channels ""insert_size"" --vsc_min_count_indels ""3"" --vsc_min_count_snps ""3"" --vsc_min_fraction_indels ""0.10"" --vsc_min_fraction_snps ""0.2"" --task {}. > *. > *. > *. > I0104 18:49:24.340415 140179943589696 make_examples_core.py:243] Task 13/64: Found 2793 candidate variants. > I0104 18:49:24.340478 140179943589696 make_examples_core.py:243] Task 13/64: Created 2879 examples. > real	18m21.465s. > user	893m16.250s. > sys	15m57.561s. > . > ***** Running the command:*****. > time /opt/deepvariant/bin/call_variants --outfile ""/tmp/kmarians_4189323/tmpxrz5rqbp/call_variants_output.tfrecord.gz"" --examples ""/tmp/kmarians_4189323/tmpxrz5rqbp/make_examples.tfrecord@64.gz"" --checkpoint ""/opt/models/wes/model.ckpt"" --openvino_model_dir ""/tmp/kmarians_4189323/tmpxrz5rqbp"". > . > I0104 18:49:26.695084 139646404110144 call_variants.py:317] From /tmp/kmarians_4189323/tmpxrz5rqbp/make_examples.tfrecord-00000-of-00064.gz.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 19]. > I0104 18:49:26.697470 139646404110144 call_variants.py:317] From /opt/models/wes/model.ckpt.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 19]. > Traceback (most recent call last):. > File ""/tmp/kmarians_4189323/Bazel.runfiles_zims94rl/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 513, in <module>. > tf.compat.v1.app.run(). > File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. > _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). > File ""/tmp/kmarians_4189323/Bazel.runfiles_zims94rl/runfiles/absl_p",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/602
https://github.com/google/deepvariant/issues/602:7393,reliability,availab,available,7393,"s). > File ""/tmp/kmarians_4189323/Bazel.runfiles_zims94rl/runfiles/absl_py/absl/app.py"", line 251, in _run_main. > sys.exit(main(argv)). > File ""/tmp/kmarians_4189323/Bazel.runfiles_zims94rl/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 494, in main. > call_variants(. > File ""/tmp/kmarians_4189323/Bazel.runfiles_zims94rl/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 389, in call_variants. > init_op = tf.group(tf.compat.v1.global_variables_initializer(),. > File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/ops/variables.py"", line 3319, in global_variables_initializer. > return variables_initializer(global_variables()). > File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/ops/variables.py"", line 3139, in global_variables. > return ops.get_collection(ops.GraphKeys.GLOBAL_VARIABLES, scope). > File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py"", line 6605, in get_collection. > return get_default_graph().get_collection(key, scope). > File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py"", line 6231, in get_default_graph. > return _default_graph_stack.get_default(). > File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py"", line 5742, in get_default. > self._global_default_graph = Graph(). > File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py"", line 3113, in __init__. > self._scoped_c_graph = c_api_util.ScopedTFGraph(). > File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/c_api_util.py"", line 50, in __init__. > self.graph = c_api.TF_NewGraph(). > RuntimeError: random_device::random_device(const std::string&): device not available. > Exception ignored in: <function ScopedTFGraph.__del__ at 0x7f01d64378b0>. > Traceback (most recent call last):. > File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/c_api_util.py"", line 58, in __del__. > AttributeError: deleter",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/602
https://github.com/google/deepvariant/issues/602:729,safety,log,log,729,"AttributeError: deleter; Hi DeepVariant team,. I'm running DeepVariant via Singularity via Snakemake on a HPC cluster and overall, if I create an interactive session, my entire WES pipeline runs fine until an odd DeepVariant job fails (by that I mean that let's say 30 deepvariant jobs finish ok and the 31st fails). I can then restart the Snakemake run and the same job will run fine, followed by more jobs that will also run fine until another odd jobs fails. This is also particularly true when I use the --cluster command in Snakemake (i.e., send it to a SLURM job scheduler from the master node) - in this event every single job fails. I could of course run all my samples on a single interactive session, keep checking the log file and restart the run every time it fails but I guess that's less than optimal plus this way I can really only run one sample at the time. For the interactive sessions I request 180G and 64cpus (in my case it's: ```srsh --mem=180G --cpus-per-task=64 --partition=long```). . I would request same parameters when using --cluster so:. ```snakemake --cluster ""sbatch --mem=180G cpus-per-task=64"" --jobs 64 --profie profile/ ```(where profile holds singularity args etc.). Singularity image is deepvariant_1.4.0.sif. my Snakemake rule:. ```. rule deepvariant:. input:. bam=rules.apply_bqsr.output.bam,. ref='/mnt/shared/scratch/kmarians/private/dyslexia_gatk/workflow/resources/genome.fasta'. output:. vcf=""results/deepvariant/{sample}.vcf.gz"". params:. model=""WES"". threads: . 64. resources:. mem_mb=163840. log:. ""logs/deepvariant/{sample}/stdout.log"". singularity:. ""singularity/deepvariant_1.4.0.sif"". # ""singularity/deepvariant_1.4.0-gpu.sif"" # for GPU. shell:. """""". /opt/deepvariant/bin/run_deepvariant --model_type {params.model} --ref {input.ref} --reads {input.bam} --output_vcf {output.vcf} --num_shards {threads} --make_examples_extra_args='vsc_min_count_snps=3,vsc_min_fraction_snps=0.2,vsc_min_count_indels=3,vsc_min_fraction_indels=0.10'. """""". ```. Below ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/602
https://github.com/google/deepvariant/issues/602:1292,safety,input,input,1292," and the 31st fails). I can then restart the Snakemake run and the same job will run fine, followed by more jobs that will also run fine until another odd jobs fails. This is also particularly true when I use the --cluster command in Snakemake (i.e., send it to a SLURM job scheduler from the master node) - in this event every single job fails. I could of course run all my samples on a single interactive session, keep checking the log file and restart the run every time it fails but I guess that's less than optimal plus this way I can really only run one sample at the time. For the interactive sessions I request 180G and 64cpus (in my case it's: ```srsh --mem=180G --cpus-per-task=64 --partition=long```). . I would request same parameters when using --cluster so:. ```snakemake --cluster ""sbatch --mem=180G cpus-per-task=64"" --jobs 64 --profie profile/ ```(where profile holds singularity args etc.). Singularity image is deepvariant_1.4.0.sif. my Snakemake rule:. ```. rule deepvariant:. input:. bam=rules.apply_bqsr.output.bam,. ref='/mnt/shared/scratch/kmarians/private/dyslexia_gatk/workflow/resources/genome.fasta'. output:. vcf=""results/deepvariant/{sample}.vcf.gz"". params:. model=""WES"". threads: . 64. resources:. mem_mb=163840. log:. ""logs/deepvariant/{sample}/stdout.log"". singularity:. ""singularity/deepvariant_1.4.0.sif"". # ""singularity/deepvariant_1.4.0-gpu.sif"" # for GPU. shell:. """""". /opt/deepvariant/bin/run_deepvariant --model_type {params.model} --ref {input.ref} --reads {input.bam} --output_vcf {output.vcf} --num_shards {threads} --make_examples_extra_args='vsc_min_count_snps=3,vsc_min_fraction_snps=0.2,vsc_min_count_indels=3,vsc_min_fraction_indels=0.10'. """""". ```. Below is the begening and end of the log file. I am happy to include the entire log file but there is nothing out of the ordinary between those lines below (same output as for jobs that finished successfully). Could you please advise on what parameters to change to successfully run DeepVariant by sub",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/602
https://github.com/google/deepvariant/issues/602:1399,safety,resourc,resources,1399,"obs that will also run fine until another odd jobs fails. This is also particularly true when I use the --cluster command in Snakemake (i.e., send it to a SLURM job scheduler from the master node) - in this event every single job fails. I could of course run all my samples on a single interactive session, keep checking the log file and restart the run every time it fails but I guess that's less than optimal plus this way I can really only run one sample at the time. For the interactive sessions I request 180G and 64cpus (in my case it's: ```srsh --mem=180G --cpus-per-task=64 --partition=long```). . I would request same parameters when using --cluster so:. ```snakemake --cluster ""sbatch --mem=180G cpus-per-task=64"" --jobs 64 --profie profile/ ```(where profile holds singularity args etc.). Singularity image is deepvariant_1.4.0.sif. my Snakemake rule:. ```. rule deepvariant:. input:. bam=rules.apply_bqsr.output.bam,. ref='/mnt/shared/scratch/kmarians/private/dyslexia_gatk/workflow/resources/genome.fasta'. output:. vcf=""results/deepvariant/{sample}.vcf.gz"". params:. model=""WES"". threads: . 64. resources:. mem_mb=163840. log:. ""logs/deepvariant/{sample}/stdout.log"". singularity:. ""singularity/deepvariant_1.4.0.sif"". # ""singularity/deepvariant_1.4.0-gpu.sif"" # for GPU. shell:. """""". /opt/deepvariant/bin/run_deepvariant --model_type {params.model} --ref {input.ref} --reads {input.bam} --output_vcf {output.vcf} --num_shards {threads} --make_examples_extra_args='vsc_min_count_snps=3,vsc_min_fraction_snps=0.2,vsc_min_count_indels=3,vsc_min_fraction_indels=0.10'. """""". ```. Below is the begening and end of the log file. I am happy to include the entire log file but there is nothing out of the ordinary between those lines below (same output as for jobs that finished successfully). Could you please advise on what parameters to change to successfully run DeepVariant by submitting it to the SLURM scheduler? . I0104 18:49:24.340415 140179943589696 make_examples_core.py:243] Task 13",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/602
https://github.com/google/deepvariant/issues/602:1513,safety,resourc,resources,1513,"command in Snakemake (i.e., send it to a SLURM job scheduler from the master node) - in this event every single job fails. I could of course run all my samples on a single interactive session, keep checking the log file and restart the run every time it fails but I guess that's less than optimal plus this way I can really only run one sample at the time. For the interactive sessions I request 180G and 64cpus (in my case it's: ```srsh --mem=180G --cpus-per-task=64 --partition=long```). . I would request same parameters when using --cluster so:. ```snakemake --cluster ""sbatch --mem=180G cpus-per-task=64"" --jobs 64 --profie profile/ ```(where profile holds singularity args etc.). Singularity image is deepvariant_1.4.0.sif. my Snakemake rule:. ```. rule deepvariant:. input:. bam=rules.apply_bqsr.output.bam,. ref='/mnt/shared/scratch/kmarians/private/dyslexia_gatk/workflow/resources/genome.fasta'. output:. vcf=""results/deepvariant/{sample}.vcf.gz"". params:. model=""WES"". threads: . 64. resources:. mem_mb=163840. log:. ""logs/deepvariant/{sample}/stdout.log"". singularity:. ""singularity/deepvariant_1.4.0.sif"". # ""singularity/deepvariant_1.4.0-gpu.sif"" # for GPU. shell:. """""". /opt/deepvariant/bin/run_deepvariant --model_type {params.model} --ref {input.ref} --reads {input.bam} --output_vcf {output.vcf} --num_shards {threads} --make_examples_extra_args='vsc_min_count_snps=3,vsc_min_fraction_snps=0.2,vsc_min_count_indels=3,vsc_min_fraction_indels=0.10'. """""". ```. Below is the begening and end of the log file. I am happy to include the entire log file but there is nothing out of the ordinary between those lines below (same output as for jobs that finished successfully). Could you please advise on what parameters to change to successfully run DeepVariant by submitting it to the SLURM scheduler? . I0104 18:49:24.340415 140179943589696 make_examples_core.py:243] Task 13/64: Found 2793 candidate variants. I0104 18:49:24.340478 140179943589696 make_examples_core.py:243] Task 13/64: C",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/602
https://github.com/google/deepvariant/issues/602:1540,safety,log,log,1540,"e., send it to a SLURM job scheduler from the master node) - in this event every single job fails. I could of course run all my samples on a single interactive session, keep checking the log file and restart the run every time it fails but I guess that's less than optimal plus this way I can really only run one sample at the time. For the interactive sessions I request 180G and 64cpus (in my case it's: ```srsh --mem=180G --cpus-per-task=64 --partition=long```). . I would request same parameters when using --cluster so:. ```snakemake --cluster ""sbatch --mem=180G cpus-per-task=64"" --jobs 64 --profie profile/ ```(where profile holds singularity args etc.). Singularity image is deepvariant_1.4.0.sif. my Snakemake rule:. ```. rule deepvariant:. input:. bam=rules.apply_bqsr.output.bam,. ref='/mnt/shared/scratch/kmarians/private/dyslexia_gatk/workflow/resources/genome.fasta'. output:. vcf=""results/deepvariant/{sample}.vcf.gz"". params:. model=""WES"". threads: . 64. resources:. mem_mb=163840. log:. ""logs/deepvariant/{sample}/stdout.log"". singularity:. ""singularity/deepvariant_1.4.0.sif"". # ""singularity/deepvariant_1.4.0-gpu.sif"" # for GPU. shell:. """""". /opt/deepvariant/bin/run_deepvariant --model_type {params.model} --ref {input.ref} --reads {input.bam} --output_vcf {output.vcf} --num_shards {threads} --make_examples_extra_args='vsc_min_count_snps=3,vsc_min_fraction_snps=0.2,vsc_min_count_indels=3,vsc_min_fraction_indels=0.10'. """""". ```. Below is the begening and end of the log file. I am happy to include the entire log file but there is nothing out of the ordinary between those lines below (same output as for jobs that finished successfully). Could you please advise on what parameters to change to successfully run DeepVariant by submitting it to the SLURM scheduler? . I0104 18:49:24.340415 140179943589696 make_examples_core.py:243] Task 13/64: Found 2793 candidate variants. I0104 18:49:24.340478 140179943589696 make_examples_core.py:243] Task 13/64: Created 2879 examples. Bu",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/602
https://github.com/google/deepvariant/issues/602:1547,safety,log,logs,1547,"d it to a SLURM job scheduler from the master node) - in this event every single job fails. I could of course run all my samples on a single interactive session, keep checking the log file and restart the run every time it fails but I guess that's less than optimal plus this way I can really only run one sample at the time. For the interactive sessions I request 180G and 64cpus (in my case it's: ```srsh --mem=180G --cpus-per-task=64 --partition=long```). . I would request same parameters when using --cluster so:. ```snakemake --cluster ""sbatch --mem=180G cpus-per-task=64"" --jobs 64 --profie profile/ ```(where profile holds singularity args etc.). Singularity image is deepvariant_1.4.0.sif. my Snakemake rule:. ```. rule deepvariant:. input:. bam=rules.apply_bqsr.output.bam,. ref='/mnt/shared/scratch/kmarians/private/dyslexia_gatk/workflow/resources/genome.fasta'. output:. vcf=""results/deepvariant/{sample}.vcf.gz"". params:. model=""WES"". threads: . 64. resources:. mem_mb=163840. log:. ""logs/deepvariant/{sample}/stdout.log"". singularity:. ""singularity/deepvariant_1.4.0.sif"". # ""singularity/deepvariant_1.4.0-gpu.sif"" # for GPU. shell:. """""". /opt/deepvariant/bin/run_deepvariant --model_type {params.model} --ref {input.ref} --reads {input.bam} --output_vcf {output.vcf} --num_shards {threads} --make_examples_extra_args='vsc_min_count_snps=3,vsc_min_fraction_snps=0.2,vsc_min_count_indels=3,vsc_min_fraction_indels=0.10'. """""". ```. Below is the begening and end of the log file. I am happy to include the entire log file but there is nothing out of the ordinary between those lines below (same output as for jobs that finished successfully). Could you please advise on what parameters to change to successfully run DeepVariant by submitting it to the SLURM scheduler? . I0104 18:49:24.340415 140179943589696 make_examples_core.py:243] Task 13/64: Found 2793 candidate variants. I0104 18:49:24.340478 140179943589696 make_examples_core.py:243] Task 13/64: Created 2879 examples. Building ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/602
https://github.com/google/deepvariant/issues/602:1580,safety,log,log,1580,"m the master node) - in this event every single job fails. I could of course run all my samples on a single interactive session, keep checking the log file and restart the run every time it fails but I guess that's less than optimal plus this way I can really only run one sample at the time. For the interactive sessions I request 180G and 64cpus (in my case it's: ```srsh --mem=180G --cpus-per-task=64 --partition=long```). . I would request same parameters when using --cluster so:. ```snakemake --cluster ""sbatch --mem=180G cpus-per-task=64"" --jobs 64 --profie profile/ ```(where profile holds singularity args etc.). Singularity image is deepvariant_1.4.0.sif. my Snakemake rule:. ```. rule deepvariant:. input:. bam=rules.apply_bqsr.output.bam,. ref='/mnt/shared/scratch/kmarians/private/dyslexia_gatk/workflow/resources/genome.fasta'. output:. vcf=""results/deepvariant/{sample}.vcf.gz"". params:. model=""WES"". threads: . 64. resources:. mem_mb=163840. log:. ""logs/deepvariant/{sample}/stdout.log"". singularity:. ""singularity/deepvariant_1.4.0.sif"". # ""singularity/deepvariant_1.4.0-gpu.sif"" # for GPU. shell:. """""". /opt/deepvariant/bin/run_deepvariant --model_type {params.model} --ref {input.ref} --reads {input.bam} --output_vcf {output.vcf} --num_shards {threads} --make_examples_extra_args='vsc_min_count_snps=3,vsc_min_fraction_snps=0.2,vsc_min_count_indels=3,vsc_min_fraction_indels=0.10'. """""". ```. Below is the begening and end of the log file. I am happy to include the entire log file but there is nothing out of the ordinary between those lines below (same output as for jobs that finished successfully). Could you please advise on what parameters to change to successfully run DeepVariant by submitting it to the SLURM scheduler? . I0104 18:49:24.340415 140179943589696 make_examples_core.py:243] Task 13/64: Found 2793 candidate variants. I0104 18:49:24.340478 140179943589696 make_examples_core.py:243] Task 13/64: Created 2879 examples. Building DAG of jobs... Using shell: /usr/",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/602
https://github.com/google/deepvariant/issues/602:1775,safety,input,input,1775,"but I guess that's less than optimal plus this way I can really only run one sample at the time. For the interactive sessions I request 180G and 64cpus (in my case it's: ```srsh --mem=180G --cpus-per-task=64 --partition=long```). . I would request same parameters when using --cluster so:. ```snakemake --cluster ""sbatch --mem=180G cpus-per-task=64"" --jobs 64 --profie profile/ ```(where profile holds singularity args etc.). Singularity image is deepvariant_1.4.0.sif. my Snakemake rule:. ```. rule deepvariant:. input:. bam=rules.apply_bqsr.output.bam,. ref='/mnt/shared/scratch/kmarians/private/dyslexia_gatk/workflow/resources/genome.fasta'. output:. vcf=""results/deepvariant/{sample}.vcf.gz"". params:. model=""WES"". threads: . 64. resources:. mem_mb=163840. log:. ""logs/deepvariant/{sample}/stdout.log"". singularity:. ""singularity/deepvariant_1.4.0.sif"". # ""singularity/deepvariant_1.4.0-gpu.sif"" # for GPU. shell:. """""". /opt/deepvariant/bin/run_deepvariant --model_type {params.model} --ref {input.ref} --reads {input.bam} --output_vcf {output.vcf} --num_shards {threads} --make_examples_extra_args='vsc_min_count_snps=3,vsc_min_fraction_snps=0.2,vsc_min_count_indels=3,vsc_min_fraction_indels=0.10'. """""". ```. Below is the begening and end of the log file. I am happy to include the entire log file but there is nothing out of the ordinary between those lines below (same output as for jobs that finished successfully). Could you please advise on what parameters to change to successfully run DeepVariant by submitting it to the SLURM scheduler? . I0104 18:49:24.340415 140179943589696 make_examples_core.py:243] Task 13/64: Found 2793 candidate variants. I0104 18:49:24.340478 140179943589696 make_examples_core.py:243] Task 13/64: Created 2879 examples. Building DAG of jobs... Using shell: /usr/bin/bash. Provided cores: 64. Rules claiming more threads will be scaled down. Select jobs to execute... > [Wed Jan 4 18:30:51 2023]. > rule deepvariant:. > input: results/recal/s534_EKDN210017195",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/602
https://github.com/google/deepvariant/issues/602:1795,safety,input,input,1795,"ess than optimal plus this way I can really only run one sample at the time. For the interactive sessions I request 180G and 64cpus (in my case it's: ```srsh --mem=180G --cpus-per-task=64 --partition=long```). . I would request same parameters when using --cluster so:. ```snakemake --cluster ""sbatch --mem=180G cpus-per-task=64"" --jobs 64 --profie profile/ ```(where profile holds singularity args etc.). Singularity image is deepvariant_1.4.0.sif. my Snakemake rule:. ```. rule deepvariant:. input:. bam=rules.apply_bqsr.output.bam,. ref='/mnt/shared/scratch/kmarians/private/dyslexia_gatk/workflow/resources/genome.fasta'. output:. vcf=""results/deepvariant/{sample}.vcf.gz"". params:. model=""WES"". threads: . 64. resources:. mem_mb=163840. log:. ""logs/deepvariant/{sample}/stdout.log"". singularity:. ""singularity/deepvariant_1.4.0.sif"". # ""singularity/deepvariant_1.4.0-gpu.sif"" # for GPU. shell:. """""". /opt/deepvariant/bin/run_deepvariant --model_type {params.model} --ref {input.ref} --reads {input.bam} --output_vcf {output.vcf} --num_shards {threads} --make_examples_extra_args='vsc_min_count_snps=3,vsc_min_fraction_snps=0.2,vsc_min_count_indels=3,vsc_min_fraction_indels=0.10'. """""". ```. Below is the begening and end of the log file. I am happy to include the entire log file but there is nothing out of the ordinary between those lines below (same output as for jobs that finished successfully). Could you please advise on what parameters to change to successfully run DeepVariant by submitting it to the SLURM scheduler? . I0104 18:49:24.340415 140179943589696 make_examples_core.py:243] Task 13/64: Found 2793 candidate variants. I0104 18:49:24.340478 140179943589696 make_examples_core.py:243] Task 13/64: Created 2879 examples. Building DAG of jobs... Using shell: /usr/bin/bash. Provided cores: 64. Rules claiming more threads will be scaled down. Select jobs to execute... > [Wed Jan 4 18:30:51 2023]. > rule deepvariant:. > input: results/recal/s534_EKDN210017195-1A_HTTJ3DSX2_L2.bam",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/602
https://github.com/google/deepvariant/issues/602:2031,safety,log,log,2031,"rameters when using --cluster so:. ```snakemake --cluster ""sbatch --mem=180G cpus-per-task=64"" --jobs 64 --profie profile/ ```(where profile holds singularity args etc.). Singularity image is deepvariant_1.4.0.sif. my Snakemake rule:. ```. rule deepvariant:. input:. bam=rules.apply_bqsr.output.bam,. ref='/mnt/shared/scratch/kmarians/private/dyslexia_gatk/workflow/resources/genome.fasta'. output:. vcf=""results/deepvariant/{sample}.vcf.gz"". params:. model=""WES"". threads: . 64. resources:. mem_mb=163840. log:. ""logs/deepvariant/{sample}/stdout.log"". singularity:. ""singularity/deepvariant_1.4.0.sif"". # ""singularity/deepvariant_1.4.0-gpu.sif"" # for GPU. shell:. """""". /opt/deepvariant/bin/run_deepvariant --model_type {params.model} --ref {input.ref} --reads {input.bam} --output_vcf {output.vcf} --num_shards {threads} --make_examples_extra_args='vsc_min_count_snps=3,vsc_min_fraction_snps=0.2,vsc_min_count_indels=3,vsc_min_fraction_indels=0.10'. """""". ```. Below is the begening and end of the log file. I am happy to include the entire log file but there is nothing out of the ordinary between those lines below (same output as for jobs that finished successfully). Could you please advise on what parameters to change to successfully run DeepVariant by submitting it to the SLURM scheduler? . I0104 18:49:24.340415 140179943589696 make_examples_core.py:243] Task 13/64: Found 2793 candidate variants. I0104 18:49:24.340478 140179943589696 make_examples_core.py:243] Task 13/64: Created 2879 examples. Building DAG of jobs... Using shell: /usr/bin/bash. Provided cores: 64. Rules claiming more threads will be scaled down. Select jobs to execute... > [Wed Jan 4 18:30:51 2023]. > rule deepvariant:. > input: results/recal/s534_EKDN210017195-1A_HTTJ3DSX2_L2.bam, /mnt/shared/scratch/kmarians/private/dyslexia_gatk/workflow/resources/genome.fasta. > output: results/deepvariant/s534_EKDN210017195-1A_HTTJ3DSX2_L2.vcf.gz. > log: logs/deepvariant/s534_EKDN210017195-1A_HTTJ3DSX2_L2/stdout.log. > job",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/602
https://github.com/google/deepvariant/issues/602:2074,safety,log,log,2074,"make --cluster ""sbatch --mem=180G cpus-per-task=64"" --jobs 64 --profie profile/ ```(where profile holds singularity args etc.). Singularity image is deepvariant_1.4.0.sif. my Snakemake rule:. ```. rule deepvariant:. input:. bam=rules.apply_bqsr.output.bam,. ref='/mnt/shared/scratch/kmarians/private/dyslexia_gatk/workflow/resources/genome.fasta'. output:. vcf=""results/deepvariant/{sample}.vcf.gz"". params:. model=""WES"". threads: . 64. resources:. mem_mb=163840. log:. ""logs/deepvariant/{sample}/stdout.log"". singularity:. ""singularity/deepvariant_1.4.0.sif"". # ""singularity/deepvariant_1.4.0-gpu.sif"" # for GPU. shell:. """""". /opt/deepvariant/bin/run_deepvariant --model_type {params.model} --ref {input.ref} --reads {input.bam} --output_vcf {output.vcf} --num_shards {threads} --make_examples_extra_args='vsc_min_count_snps=3,vsc_min_fraction_snps=0.2,vsc_min_count_indels=3,vsc_min_fraction_indels=0.10'. """""". ```. Below is the begening and end of the log file. I am happy to include the entire log file but there is nothing out of the ordinary between those lines below (same output as for jobs that finished successfully). Could you please advise on what parameters to change to successfully run DeepVariant by submitting it to the SLURM scheduler? . I0104 18:49:24.340415 140179943589696 make_examples_core.py:243] Task 13/64: Found 2793 candidate variants. I0104 18:49:24.340478 140179943589696 make_examples_core.py:243] Task 13/64: Created 2879 examples. Building DAG of jobs... Using shell: /usr/bin/bash. Provided cores: 64. Rules claiming more threads will be scaled down. Select jobs to execute... > [Wed Jan 4 18:30:51 2023]. > rule deepvariant:. > input: results/recal/s534_EKDN210017195-1A_HTTJ3DSX2_L2.bam, /mnt/shared/scratch/kmarians/private/dyslexia_gatk/workflow/resources/genome.fasta. > output: results/deepvariant/s534_EKDN210017195-1A_HTTJ3DSX2_L2.vcf.gz. > log: logs/deepvariant/s534_EKDN210017195-1A_HTTJ3DSX2_L2/stdout.log. > jobid: 0. > wildcards: sample=s534_EKDN2100171",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/602
https://github.com/google/deepvariant/issues/602:2739,safety,input,input,2739,"model_type {params.model} --ref {input.ref} --reads {input.bam} --output_vcf {output.vcf} --num_shards {threads} --make_examples_extra_args='vsc_min_count_snps=3,vsc_min_fraction_snps=0.2,vsc_min_count_indels=3,vsc_min_fraction_indels=0.10'. """""". ```. Below is the begening and end of the log file. I am happy to include the entire log file but there is nothing out of the ordinary between those lines below (same output as for jobs that finished successfully). Could you please advise on what parameters to change to successfully run DeepVariant by submitting it to the SLURM scheduler? . I0104 18:49:24.340415 140179943589696 make_examples_core.py:243] Task 13/64: Found 2793 candidate variants. I0104 18:49:24.340478 140179943589696 make_examples_core.py:243] Task 13/64: Created 2879 examples. Building DAG of jobs... Using shell: /usr/bin/bash. Provided cores: 64. Rules claiming more threads will be scaled down. Select jobs to execute... > [Wed Jan 4 18:30:51 2023]. > rule deepvariant:. > input: results/recal/s534_EKDN210017195-1A_HTTJ3DSX2_L2.bam, /mnt/shared/scratch/kmarians/private/dyslexia_gatk/workflow/resources/genome.fasta. > output: results/deepvariant/s534_EKDN210017195-1A_HTTJ3DSX2_L2.vcf.gz. > log: logs/deepvariant/s534_EKDN210017195-1A_HTTJ3DSX2_L2/stdout.log. > jobid: 0. > wildcards: sample=s534_EKDN210017195-1A_HTTJ3DSX2_L2. > threads: 64. > resources: mem_mb=163840, disk_mb=16401, tmpdir=/tmp/kmarians_4189323. > . > Activating singularity image singularity/deepvariant_1.4.0.sif. > INFO: Convert SIF file to sandbox... > I0104 18:31:03.183642 139718628308800 run_deepvariant.py:342] Re-using the directory for intermediate results in /tmp/kmarians_4189323/tmpxrz5rqbp. > . > ***** Intermediate results will be written to /tmp/kmarians_4189323/tmpxrz5rqbp in docker. ****. > . > . > ***** Running the command:*****. > time seq 0 63 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/mnt/shared/scratch/kmarians/private/dyslex",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/602
https://github.com/google/deepvariant/issues/602:2860,safety,resourc,resources,2860,"mples_extra_args='vsc_min_count_snps=3,vsc_min_fraction_snps=0.2,vsc_min_count_indels=3,vsc_min_fraction_indels=0.10'. """""". ```. Below is the begening and end of the log file. I am happy to include the entire log file but there is nothing out of the ordinary between those lines below (same output as for jobs that finished successfully). Could you please advise on what parameters to change to successfully run DeepVariant by submitting it to the SLURM scheduler? . I0104 18:49:24.340415 140179943589696 make_examples_core.py:243] Task 13/64: Found 2793 candidate variants. I0104 18:49:24.340478 140179943589696 make_examples_core.py:243] Task 13/64: Created 2879 examples. Building DAG of jobs... Using shell: /usr/bin/bash. Provided cores: 64. Rules claiming more threads will be scaled down. Select jobs to execute... > [Wed Jan 4 18:30:51 2023]. > rule deepvariant:. > input: results/recal/s534_EKDN210017195-1A_HTTJ3DSX2_L2.bam, /mnt/shared/scratch/kmarians/private/dyslexia_gatk/workflow/resources/genome.fasta. > output: results/deepvariant/s534_EKDN210017195-1A_HTTJ3DSX2_L2.vcf.gz. > log: logs/deepvariant/s534_EKDN210017195-1A_HTTJ3DSX2_L2/stdout.log. > jobid: 0. > wildcards: sample=s534_EKDN210017195-1A_HTTJ3DSX2_L2. > threads: 64. > resources: mem_mb=163840, disk_mb=16401, tmpdir=/tmp/kmarians_4189323. > . > Activating singularity image singularity/deepvariant_1.4.0.sif. > INFO: Convert SIF file to sandbox... > I0104 18:31:03.183642 139718628308800 run_deepvariant.py:342] Re-using the directory for intermediate results in /tmp/kmarians_4189323/tmpxrz5rqbp. > . > ***** Intermediate results will be written to /tmp/kmarians_4189323/tmpxrz5rqbp in docker. ****. > . > . > ***** Running the command:*****. > time seq 0 63 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/mnt/shared/scratch/kmarians/private/dyslexia_gatk/workflow/resources/genome.fasta"" --reads ""results/recal/s534_EKDN210017195-1A_HTTJ3DSX2_L2.bam"" --examples ""/tmp/km",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/602
https://github.com/google/deepvariant/issues/602:2959,safety,log,log,2959,"fraction_indels=0.10'. """""". ```. Below is the begening and end of the log file. I am happy to include the entire log file but there is nothing out of the ordinary between those lines below (same output as for jobs that finished successfully). Could you please advise on what parameters to change to successfully run DeepVariant by submitting it to the SLURM scheduler? . I0104 18:49:24.340415 140179943589696 make_examples_core.py:243] Task 13/64: Found 2793 candidate variants. I0104 18:49:24.340478 140179943589696 make_examples_core.py:243] Task 13/64: Created 2879 examples. Building DAG of jobs... Using shell: /usr/bin/bash. Provided cores: 64. Rules claiming more threads will be scaled down. Select jobs to execute... > [Wed Jan 4 18:30:51 2023]. > rule deepvariant:. > input: results/recal/s534_EKDN210017195-1A_HTTJ3DSX2_L2.bam, /mnt/shared/scratch/kmarians/private/dyslexia_gatk/workflow/resources/genome.fasta. > output: results/deepvariant/s534_EKDN210017195-1A_HTTJ3DSX2_L2.vcf.gz. > log: logs/deepvariant/s534_EKDN210017195-1A_HTTJ3DSX2_L2/stdout.log. > jobid: 0. > wildcards: sample=s534_EKDN210017195-1A_HTTJ3DSX2_L2. > threads: 64. > resources: mem_mb=163840, disk_mb=16401, tmpdir=/tmp/kmarians_4189323. > . > Activating singularity image singularity/deepvariant_1.4.0.sif. > INFO: Convert SIF file to sandbox... > I0104 18:31:03.183642 139718628308800 run_deepvariant.py:342] Re-using the directory for intermediate results in /tmp/kmarians_4189323/tmpxrz5rqbp. > . > ***** Intermediate results will be written to /tmp/kmarians_4189323/tmpxrz5rqbp in docker. ****. > . > . > ***** Running the command:*****. > time seq 0 63 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/mnt/shared/scratch/kmarians/private/dyslexia_gatk/workflow/resources/genome.fasta"" --reads ""results/recal/s534_EKDN210017195-1A_HTTJ3DSX2_L2.bam"" --examples ""/tmp/kmarians_4189323/tmpxrz5rqbp/make_examples.tfrecord@64.gz"" --channels ""insert_size"" --vsc_min_coun",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/602
https://github.com/google/deepvariant/issues/602:2964,safety,log,logs,2964,"ion_indels=0.10'. """""". ```. Below is the begening and end of the log file. I am happy to include the entire log file but there is nothing out of the ordinary between those lines below (same output as for jobs that finished successfully). Could you please advise on what parameters to change to successfully run DeepVariant by submitting it to the SLURM scheduler? . I0104 18:49:24.340415 140179943589696 make_examples_core.py:243] Task 13/64: Found 2793 candidate variants. I0104 18:49:24.340478 140179943589696 make_examples_core.py:243] Task 13/64: Created 2879 examples. Building DAG of jobs... Using shell: /usr/bin/bash. Provided cores: 64. Rules claiming more threads will be scaled down. Select jobs to execute... > [Wed Jan 4 18:30:51 2023]. > rule deepvariant:. > input: results/recal/s534_EKDN210017195-1A_HTTJ3DSX2_L2.bam, /mnt/shared/scratch/kmarians/private/dyslexia_gatk/workflow/resources/genome.fasta. > output: results/deepvariant/s534_EKDN210017195-1A_HTTJ3DSX2_L2.vcf.gz. > log: logs/deepvariant/s534_EKDN210017195-1A_HTTJ3DSX2_L2/stdout.log. > jobid: 0. > wildcards: sample=s534_EKDN210017195-1A_HTTJ3DSX2_L2. > threads: 64. > resources: mem_mb=163840, disk_mb=16401, tmpdir=/tmp/kmarians_4189323. > . > Activating singularity image singularity/deepvariant_1.4.0.sif. > INFO: Convert SIF file to sandbox... > I0104 18:31:03.183642 139718628308800 run_deepvariant.py:342] Re-using the directory for intermediate results in /tmp/kmarians_4189323/tmpxrz5rqbp. > . > ***** Intermediate results will be written to /tmp/kmarians_4189323/tmpxrz5rqbp in docker. ****. > . > . > ***** Running the command:*****. > time seq 0 63 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/mnt/shared/scratch/kmarians/private/dyslexia_gatk/workflow/resources/genome.fasta"" --reads ""results/recal/s534_EKDN210017195-1A_HTTJ3DSX2_L2.bam"" --examples ""/tmp/kmarians_4189323/tmpxrz5rqbp/make_examples.tfrecord@64.gz"" --channels ""insert_size"" --vsc_min_count_ind",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/602
https://github.com/google/deepvariant/issues/602:3023,safety,log,log,3023,"f the log file. I am happy to include the entire log file but there is nothing out of the ordinary between those lines below (same output as for jobs that finished successfully). Could you please advise on what parameters to change to successfully run DeepVariant by submitting it to the SLURM scheduler? . I0104 18:49:24.340415 140179943589696 make_examples_core.py:243] Task 13/64: Found 2793 candidate variants. I0104 18:49:24.340478 140179943589696 make_examples_core.py:243] Task 13/64: Created 2879 examples. Building DAG of jobs... Using shell: /usr/bin/bash. Provided cores: 64. Rules claiming more threads will be scaled down. Select jobs to execute... > [Wed Jan 4 18:30:51 2023]. > rule deepvariant:. > input: results/recal/s534_EKDN210017195-1A_HTTJ3DSX2_L2.bam, /mnt/shared/scratch/kmarians/private/dyslexia_gatk/workflow/resources/genome.fasta. > output: results/deepvariant/s534_EKDN210017195-1A_HTTJ3DSX2_L2.vcf.gz. > log: logs/deepvariant/s534_EKDN210017195-1A_HTTJ3DSX2_L2/stdout.log. > jobid: 0. > wildcards: sample=s534_EKDN210017195-1A_HTTJ3DSX2_L2. > threads: 64. > resources: mem_mb=163840, disk_mb=16401, tmpdir=/tmp/kmarians_4189323. > . > Activating singularity image singularity/deepvariant_1.4.0.sif. > INFO: Convert SIF file to sandbox... > I0104 18:31:03.183642 139718628308800 run_deepvariant.py:342] Re-using the directory for intermediate results in /tmp/kmarians_4189323/tmpxrz5rqbp. > . > ***** Intermediate results will be written to /tmp/kmarians_4189323/tmpxrz5rqbp in docker. ****. > . > . > ***** Running the command:*****. > time seq 0 63 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/mnt/shared/scratch/kmarians/private/dyslexia_gatk/workflow/resources/genome.fasta"" --reads ""results/recal/s534_EKDN210017195-1A_HTTJ3DSX2_L2.bam"" --examples ""/tmp/kmarians_4189323/tmpxrz5rqbp/make_examples.tfrecord@64.gz"" --channels ""insert_size"" --vsc_min_count_indels ""3"" --vsc_min_count_snps ""3"" --vsc_min_fraction_indels ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/602
https://github.com/google/deepvariant/issues/602:3113,safety,resourc,resources,3113,"inary between those lines below (same output as for jobs that finished successfully). Could you please advise on what parameters to change to successfully run DeepVariant by submitting it to the SLURM scheduler? . I0104 18:49:24.340415 140179943589696 make_examples_core.py:243] Task 13/64: Found 2793 candidate variants. I0104 18:49:24.340478 140179943589696 make_examples_core.py:243] Task 13/64: Created 2879 examples. Building DAG of jobs... Using shell: /usr/bin/bash. Provided cores: 64. Rules claiming more threads will be scaled down. Select jobs to execute... > [Wed Jan 4 18:30:51 2023]. > rule deepvariant:. > input: results/recal/s534_EKDN210017195-1A_HTTJ3DSX2_L2.bam, /mnt/shared/scratch/kmarians/private/dyslexia_gatk/workflow/resources/genome.fasta. > output: results/deepvariant/s534_EKDN210017195-1A_HTTJ3DSX2_L2.vcf.gz. > log: logs/deepvariant/s534_EKDN210017195-1A_HTTJ3DSX2_L2/stdout.log. > jobid: 0. > wildcards: sample=s534_EKDN210017195-1A_HTTJ3DSX2_L2. > threads: 64. > resources: mem_mb=163840, disk_mb=16401, tmpdir=/tmp/kmarians_4189323. > . > Activating singularity image singularity/deepvariant_1.4.0.sif. > INFO: Convert SIF file to sandbox... > I0104 18:31:03.183642 139718628308800 run_deepvariant.py:342] Re-using the directory for intermediate results in /tmp/kmarians_4189323/tmpxrz5rqbp. > . > ***** Intermediate results will be written to /tmp/kmarians_4189323/tmpxrz5rqbp in docker. ****. > . > . > ***** Running the command:*****. > time seq 0 63 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/mnt/shared/scratch/kmarians/private/dyslexia_gatk/workflow/resources/genome.fasta"" --reads ""results/recal/s534_EKDN210017195-1A_HTTJ3DSX2_L2.bam"" --examples ""/tmp/kmarians_4189323/tmpxrz5rqbp/make_examples.tfrecord@64.gz"" --channels ""insert_size"" --vsc_min_count_indels ""3"" --vsc_min_count_snps ""3"" --vsc_min_fraction_indels ""0.10"" --vsc_min_fraction_snps ""0.2"" --task {}. > *. > *. > *. > I0104 18:49:24.340415 140179",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/602
https://github.com/google/deepvariant/issues/602:3759,safety,resourc,resources,3759,"_EKDN210017195-1A_HTTJ3DSX2_L2.bam, /mnt/shared/scratch/kmarians/private/dyslexia_gatk/workflow/resources/genome.fasta. > output: results/deepvariant/s534_EKDN210017195-1A_HTTJ3DSX2_L2.vcf.gz. > log: logs/deepvariant/s534_EKDN210017195-1A_HTTJ3DSX2_L2/stdout.log. > jobid: 0. > wildcards: sample=s534_EKDN210017195-1A_HTTJ3DSX2_L2. > threads: 64. > resources: mem_mb=163840, disk_mb=16401, tmpdir=/tmp/kmarians_4189323. > . > Activating singularity image singularity/deepvariant_1.4.0.sif. > INFO: Convert SIF file to sandbox... > I0104 18:31:03.183642 139718628308800 run_deepvariant.py:342] Re-using the directory for intermediate results in /tmp/kmarians_4189323/tmpxrz5rqbp. > . > ***** Intermediate results will be written to /tmp/kmarians_4189323/tmpxrz5rqbp in docker. ****. > . > . > ***** Running the command:*****. > time seq 0 63 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/mnt/shared/scratch/kmarians/private/dyslexia_gatk/workflow/resources/genome.fasta"" --reads ""results/recal/s534_EKDN210017195-1A_HTTJ3DSX2_L2.bam"" --examples ""/tmp/kmarians_4189323/tmpxrz5rqbp/make_examples.tfrecord@64.gz"" --channels ""insert_size"" --vsc_min_count_indels ""3"" --vsc_min_count_snps ""3"" --vsc_min_fraction_indels ""0.10"" --vsc_min_fraction_snps ""0.2"" --task {}. > *. > *. > *. > I0104 18:49:24.340415 140179943589696 make_examples_core.py:243] Task 13/64: Found 2793 candidate variants. > I0104 18:49:24.340478 140179943589696 make_examples_core.py:243] Task 13/64: Created 2879 examples. > real	18m21.465s. > user	893m16.250s. > sys	15m57.561s. > . > ***** Running the command:*****. > time /opt/deepvariant/bin/call_variants --outfile ""/tmp/kmarians_4189323/tmpxrz5rqbp/call_variants_output.tfrecord.gz"" --examples ""/tmp/kmarians_4189323/tmpxrz5rqbp/make_examples.tfrecord@64.gz"" --checkpoint ""/opt/models/wes/model.ckpt"" --openvino_model_dir ""/tmp/kmarians_4189323/tmpxrz5rqbp"". > . > I0104 18:49:26.695084 139646404110144 call_variants.py:317] From ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/602
https://github.com/google/deepvariant/issues/602:4867,safety,input,input,4867,"s_4189323/tmpxrz5rqbp/make_examples.tfrecord@64.gz"" --channels ""insert_size"" --vsc_min_count_indels ""3"" --vsc_min_count_snps ""3"" --vsc_min_fraction_indels ""0.10"" --vsc_min_fraction_snps ""0.2"" --task {}. > *. > *. > *. > I0104 18:49:24.340415 140179943589696 make_examples_core.py:243] Task 13/64: Found 2793 candidate variants. > I0104 18:49:24.340478 140179943589696 make_examples_core.py:243] Task 13/64: Created 2879 examples. > real	18m21.465s. > user	893m16.250s. > sys	15m57.561s. > . > ***** Running the command:*****. > time /opt/deepvariant/bin/call_variants --outfile ""/tmp/kmarians_4189323/tmpxrz5rqbp/call_variants_output.tfrecord.gz"" --examples ""/tmp/kmarians_4189323/tmpxrz5rqbp/make_examples.tfrecord@64.gz"" --checkpoint ""/opt/models/wes/model.ckpt"" --openvino_model_dir ""/tmp/kmarians_4189323/tmpxrz5rqbp"". > . > I0104 18:49:26.695084 139646404110144 call_variants.py:317] From /tmp/kmarians_4189323/tmpxrz5rqbp/make_examples.tfrecord-00000-of-00064.gz.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 19]. > I0104 18:49:26.697470 139646404110144 call_variants.py:317] From /opt/models/wes/model.ckpt.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 19]. > Traceback (most recent call last):. > File ""/tmp/kmarians_4189323/Bazel.runfiles_zims94rl/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 513, in <module>. > tf.compat.v1.app.run(). > File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. > _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). > File ""/tmp/kmarians_4189323/Bazel.runfiles_zims94rl/runfiles/absl_py/absl/app.py"", line 300, in run. > _run_main(main, args). > File ""/tmp/kmarians_4189323/Bazel.runfiles_zims94rl/runfiles/absl_py/absl/app.py"", line 251, in _run_main. > sys.exit(main(argv)). > File ""/tmp/kmarians_4189323/Bazel.runfiles_zims94rl/runfiles/com_google_deep",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/602
https://github.com/google/deepvariant/issues/602:4910,safety,input,input,4910,"d@64.gz"" --channels ""insert_size"" --vsc_min_count_indels ""3"" --vsc_min_count_snps ""3"" --vsc_min_fraction_indels ""0.10"" --vsc_min_fraction_snps ""0.2"" --task {}. > *. > *. > *. > I0104 18:49:24.340415 140179943589696 make_examples_core.py:243] Task 13/64: Found 2793 candidate variants. > I0104 18:49:24.340478 140179943589696 make_examples_core.py:243] Task 13/64: Created 2879 examples. > real	18m21.465s. > user	893m16.250s. > sys	15m57.561s. > . > ***** Running the command:*****. > time /opt/deepvariant/bin/call_variants --outfile ""/tmp/kmarians_4189323/tmpxrz5rqbp/call_variants_output.tfrecord.gz"" --examples ""/tmp/kmarians_4189323/tmpxrz5rqbp/make_examples.tfrecord@64.gz"" --checkpoint ""/opt/models/wes/model.ckpt"" --openvino_model_dir ""/tmp/kmarians_4189323/tmpxrz5rqbp"". > . > I0104 18:49:26.695084 139646404110144 call_variants.py:317] From /tmp/kmarians_4189323/tmpxrz5rqbp/make_examples.tfrecord-00000-of-00064.gz.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 19]. > I0104 18:49:26.697470 139646404110144 call_variants.py:317] From /opt/models/wes/model.ckpt.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 19]. > Traceback (most recent call last):. > File ""/tmp/kmarians_4189323/Bazel.runfiles_zims94rl/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 513, in <module>. > tf.compat.v1.app.run(). > File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. > _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). > File ""/tmp/kmarians_4189323/Bazel.runfiles_zims94rl/runfiles/absl_py/absl/app.py"", line 300, in run. > _run_main(main, args). > File ""/tmp/kmarians_4189323/Bazel.runfiles_zims94rl/runfiles/absl_py/absl/app.py"", line 251, in _run_main. > sys.exit(main(argv)). > File ""/tmp/kmarians_4189323/Bazel.runfiles_zims94rl/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/602
https://github.com/google/deepvariant/issues/602:5072,safety,input,input,5072,"*. > *. > *. > I0104 18:49:24.340415 140179943589696 make_examples_core.py:243] Task 13/64: Found 2793 candidate variants. > I0104 18:49:24.340478 140179943589696 make_examples_core.py:243] Task 13/64: Created 2879 examples. > real	18m21.465s. > user	893m16.250s. > sys	15m57.561s. > . > ***** Running the command:*****. > time /opt/deepvariant/bin/call_variants --outfile ""/tmp/kmarians_4189323/tmpxrz5rqbp/call_variants_output.tfrecord.gz"" --examples ""/tmp/kmarians_4189323/tmpxrz5rqbp/make_examples.tfrecord@64.gz"" --checkpoint ""/opt/models/wes/model.ckpt"" --openvino_model_dir ""/tmp/kmarians_4189323/tmpxrz5rqbp"". > . > I0104 18:49:26.695084 139646404110144 call_variants.py:317] From /tmp/kmarians_4189323/tmpxrz5rqbp/make_examples.tfrecord-00000-of-00064.gz.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 19]. > I0104 18:49:26.697470 139646404110144 call_variants.py:317] From /opt/models/wes/model.ckpt.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 19]. > Traceback (most recent call last):. > File ""/tmp/kmarians_4189323/Bazel.runfiles_zims94rl/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 513, in <module>. > tf.compat.v1.app.run(). > File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. > _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). > File ""/tmp/kmarians_4189323/Bazel.runfiles_zims94rl/runfiles/absl_py/absl/app.py"", line 300, in run. > _run_main(main, args). > File ""/tmp/kmarians_4189323/Bazel.runfiles_zims94rl/runfiles/absl_py/absl/app.py"", line 251, in _run_main. > sys.exit(main(argv)). > File ""/tmp/kmarians_4189323/Bazel.runfiles_zims94rl/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 494, in main. > call_variants(. > File ""/tmp/kmarians_4189323/Bazel.runfiles_zims94rl/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 389, in",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/602
https://github.com/google/deepvariant/issues/602:5115,safety,input,input,5115,"943589696 make_examples_core.py:243] Task 13/64: Found 2793 candidate variants. > I0104 18:49:24.340478 140179943589696 make_examples_core.py:243] Task 13/64: Created 2879 examples. > real	18m21.465s. > user	893m16.250s. > sys	15m57.561s. > . > ***** Running the command:*****. > time /opt/deepvariant/bin/call_variants --outfile ""/tmp/kmarians_4189323/tmpxrz5rqbp/call_variants_output.tfrecord.gz"" --examples ""/tmp/kmarians_4189323/tmpxrz5rqbp/make_examples.tfrecord@64.gz"" --checkpoint ""/opt/models/wes/model.ckpt"" --openvino_model_dir ""/tmp/kmarians_4189323/tmpxrz5rqbp"". > . > I0104 18:49:26.695084 139646404110144 call_variants.py:317] From /tmp/kmarians_4189323/tmpxrz5rqbp/make_examples.tfrecord-00000-of-00064.gz.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 19]. > I0104 18:49:26.697470 139646404110144 call_variants.py:317] From /opt/models/wes/model.ckpt.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 19]. > Traceback (most recent call last):. > File ""/tmp/kmarians_4189323/Bazel.runfiles_zims94rl/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 513, in <module>. > tf.compat.v1.app.run(). > File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. > _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). > File ""/tmp/kmarians_4189323/Bazel.runfiles_zims94rl/runfiles/absl_py/absl/app.py"", line 300, in run. > _run_main(main, args). > File ""/tmp/kmarians_4189323/Bazel.runfiles_zims94rl/runfiles/absl_py/absl/app.py"", line 251, in _run_main. > sys.exit(main(argv)). > File ""/tmp/kmarians_4189323/Bazel.runfiles_zims94rl/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 494, in main. > call_variants(. > File ""/tmp/kmarians_4189323/Bazel.runfiles_zims94rl/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 389, in call_variants. > init_op = tf.group(tf.com",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/602
https://github.com/google/deepvariant/issues/602:5324,safety,modul,module,5324,"93m16.250s. > sys	15m57.561s. > . > ***** Running the command:*****. > time /opt/deepvariant/bin/call_variants --outfile ""/tmp/kmarians_4189323/tmpxrz5rqbp/call_variants_output.tfrecord.gz"" --examples ""/tmp/kmarians_4189323/tmpxrz5rqbp/make_examples.tfrecord@64.gz"" --checkpoint ""/opt/models/wes/model.ckpt"" --openvino_model_dir ""/tmp/kmarians_4189323/tmpxrz5rqbp"". > . > I0104 18:49:26.695084 139646404110144 call_variants.py:317] From /tmp/kmarians_4189323/tmpxrz5rqbp/make_examples.tfrecord-00000-of-00064.gz.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 19]. > I0104 18:49:26.697470 139646404110144 call_variants.py:317] From /opt/models/wes/model.ckpt.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 19]. > Traceback (most recent call last):. > File ""/tmp/kmarians_4189323/Bazel.runfiles_zims94rl/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 513, in <module>. > tf.compat.v1.app.run(). > File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. > _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). > File ""/tmp/kmarians_4189323/Bazel.runfiles_zims94rl/runfiles/absl_py/absl/app.py"", line 300, in run. > _run_main(main, args). > File ""/tmp/kmarians_4189323/Bazel.runfiles_zims94rl/runfiles/absl_py/absl/app.py"", line 251, in _run_main. > sys.exit(main(argv)). > File ""/tmp/kmarians_4189323/Bazel.runfiles_zims94rl/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 494, in main. > call_variants(. > File ""/tmp/kmarians_4189323/Bazel.runfiles_zims94rl/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 389, in call_variants. > init_op = tf.group(tf.compat.v1.global_variables_initializer(),. > File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/ops/variables.py"", line 3319, in global_variables_initializer. > return variables_initializer(global_var",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/602
https://github.com/google/deepvariant/issues/602:7393,safety,avail,available,7393,"s). > File ""/tmp/kmarians_4189323/Bazel.runfiles_zims94rl/runfiles/absl_py/absl/app.py"", line 251, in _run_main. > sys.exit(main(argv)). > File ""/tmp/kmarians_4189323/Bazel.runfiles_zims94rl/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 494, in main. > call_variants(. > File ""/tmp/kmarians_4189323/Bazel.runfiles_zims94rl/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 389, in call_variants. > init_op = tf.group(tf.compat.v1.global_variables_initializer(),. > File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/ops/variables.py"", line 3319, in global_variables_initializer. > return variables_initializer(global_variables()). > File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/ops/variables.py"", line 3139, in global_variables. > return ops.get_collection(ops.GraphKeys.GLOBAL_VARIABLES, scope). > File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py"", line 6605, in get_collection. > return get_default_graph().get_collection(key, scope). > File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py"", line 6231, in get_default_graph. > return _default_graph_stack.get_default(). > File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py"", line 5742, in get_default. > self._global_default_graph = Graph(). > File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py"", line 3113, in __init__. > self._scoped_c_graph = c_api_util.ScopedTFGraph(). > File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/c_api_util.py"", line 50, in __init__. > self.graph = c_api.TF_NewGraph(). > RuntimeError: random_device::random_device(const std::string&): device not available. > Exception ignored in: <function ScopedTFGraph.__del__ at 0x7f01d64378b0>. > Traceback (most recent call last):. > File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/c_api_util.py"", line 58, in __del__. > AttributeError: deleter",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/602
https://github.com/google/deepvariant/issues/602:7406,safety,Except,Exception,7406,"s). > File ""/tmp/kmarians_4189323/Bazel.runfiles_zims94rl/runfiles/absl_py/absl/app.py"", line 251, in _run_main. > sys.exit(main(argv)). > File ""/tmp/kmarians_4189323/Bazel.runfiles_zims94rl/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 494, in main. > call_variants(. > File ""/tmp/kmarians_4189323/Bazel.runfiles_zims94rl/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 389, in call_variants. > init_op = tf.group(tf.compat.v1.global_variables_initializer(),. > File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/ops/variables.py"", line 3319, in global_variables_initializer. > return variables_initializer(global_variables()). > File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/ops/variables.py"", line 3139, in global_variables. > return ops.get_collection(ops.GraphKeys.GLOBAL_VARIABLES, scope). > File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py"", line 6605, in get_collection. > return get_default_graph().get_collection(key, scope). > File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py"", line 6231, in get_default_graph. > return _default_graph_stack.get_default(). > File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py"", line 5742, in get_default. > self._global_default_graph = Graph(). > File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py"", line 3113, in __init__. > self._scoped_c_graph = c_api_util.ScopedTFGraph(). > File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/c_api_util.py"", line 50, in __init__. > self.graph = c_api.TF_NewGraph(). > RuntimeError: random_device::random_device(const std::string&): device not available. > Exception ignored in: <function ScopedTFGraph.__del__ at 0x7f01d64378b0>. > Traceback (most recent call last):. > File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/c_api_util.py"", line 58, in __del__. > AttributeError: deleter",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/602
https://github.com/google/deepvariant/issues/602:40,security,team,team,40,"AttributeError: deleter; Hi DeepVariant team,. I'm running DeepVariant via Singularity via Snakemake on a HPC cluster and overall, if I create an interactive session, my entire WES pipeline runs fine until an odd DeepVariant job fails (by that I mean that let's say 30 deepvariant jobs finish ok and the 31st fails). I can then restart the Snakemake run and the same job will run fine, followed by more jobs that will also run fine until another odd jobs fails. This is also particularly true when I use the --cluster command in Snakemake (i.e., send it to a SLURM job scheduler from the master node) - in this event every single job fails. I could of course run all my samples on a single interactive session, keep checking the log file and restart the run every time it fails but I guess that's less than optimal plus this way I can really only run one sample at the time. For the interactive sessions I request 180G and 64cpus (in my case it's: ```srsh --mem=180G --cpus-per-task=64 --partition=long```). . I would request same parameters when using --cluster so:. ```snakemake --cluster ""sbatch --mem=180G cpus-per-task=64"" --jobs 64 --profie profile/ ```(where profile holds singularity args etc.). Singularity image is deepvariant_1.4.0.sif. my Snakemake rule:. ```. rule deepvariant:. input:. bam=rules.apply_bqsr.output.bam,. ref='/mnt/shared/scratch/kmarians/private/dyslexia_gatk/workflow/resources/genome.fasta'. output:. vcf=""results/deepvariant/{sample}.vcf.gz"". params:. model=""WES"". threads: . 64. resources:. mem_mb=163840. log:. ""logs/deepvariant/{sample}/stdout.log"". singularity:. ""singularity/deepvariant_1.4.0.sif"". # ""singularity/deepvariant_1.4.0-gpu.sif"" # for GPU. shell:. """""". /opt/deepvariant/bin/run_deepvariant --model_type {params.model} --ref {input.ref} --reads {input.bam} --output_vcf {output.vcf} --num_shards {threads} --make_examples_extra_args='vsc_min_count_snps=3,vsc_min_fraction_snps=0.2,vsc_min_count_indels=3,vsc_min_fraction_indels=0.10'. """""". ```. Below ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/602
https://github.com/google/deepvariant/issues/602:158,security,session,session,158,"AttributeError: deleter; Hi DeepVariant team,. I'm running DeepVariant via Singularity via Snakemake on a HPC cluster and overall, if I create an interactive session, my entire WES pipeline runs fine until an odd DeepVariant job fails (by that I mean that let's say 30 deepvariant jobs finish ok and the 31st fails). I can then restart the Snakemake run and the same job will run fine, followed by more jobs that will also run fine until another odd jobs fails. This is also particularly true when I use the --cluster command in Snakemake (i.e., send it to a SLURM job scheduler from the master node) - in this event every single job fails. I could of course run all my samples on a single interactive session, keep checking the log file and restart the run every time it fails but I guess that's less than optimal plus this way I can really only run one sample at the time. For the interactive sessions I request 180G and 64cpus (in my case it's: ```srsh --mem=180G --cpus-per-task=64 --partition=long```). . I would request same parameters when using --cluster so:. ```snakemake --cluster ""sbatch --mem=180G cpus-per-task=64"" --jobs 64 --profie profile/ ```(where profile holds singularity args etc.). Singularity image is deepvariant_1.4.0.sif. my Snakemake rule:. ```. rule deepvariant:. input:. bam=rules.apply_bqsr.output.bam,. ref='/mnt/shared/scratch/kmarians/private/dyslexia_gatk/workflow/resources/genome.fasta'. output:. vcf=""results/deepvariant/{sample}.vcf.gz"". params:. model=""WES"". threads: . 64. resources:. mem_mb=163840. log:. ""logs/deepvariant/{sample}/stdout.log"". singularity:. ""singularity/deepvariant_1.4.0.sif"". # ""singularity/deepvariant_1.4.0-gpu.sif"" # for GPU. shell:. """""". /opt/deepvariant/bin/run_deepvariant --model_type {params.model} --ref {input.ref} --reads {input.bam} --output_vcf {output.vcf} --num_shards {threads} --make_examples_extra_args='vsc_min_count_snps=3,vsc_min_fraction_snps=0.2,vsc_min_count_indels=3,vsc_min_fraction_indels=0.10'. """""". ```. Below ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/602
https://github.com/google/deepvariant/issues/602:702,security,session,session,702,"AttributeError: deleter; Hi DeepVariant team,. I'm running DeepVariant via Singularity via Snakemake on a HPC cluster and overall, if I create an interactive session, my entire WES pipeline runs fine until an odd DeepVariant job fails (by that I mean that let's say 30 deepvariant jobs finish ok and the 31st fails). I can then restart the Snakemake run and the same job will run fine, followed by more jobs that will also run fine until another odd jobs fails. This is also particularly true when I use the --cluster command in Snakemake (i.e., send it to a SLURM job scheduler from the master node) - in this event every single job fails. I could of course run all my samples on a single interactive session, keep checking the log file and restart the run every time it fails but I guess that's less than optimal plus this way I can really only run one sample at the time. For the interactive sessions I request 180G and 64cpus (in my case it's: ```srsh --mem=180G --cpus-per-task=64 --partition=long```). . I would request same parameters when using --cluster so:. ```snakemake --cluster ""sbatch --mem=180G cpus-per-task=64"" --jobs 64 --profie profile/ ```(where profile holds singularity args etc.). Singularity image is deepvariant_1.4.0.sif. my Snakemake rule:. ```. rule deepvariant:. input:. bam=rules.apply_bqsr.output.bam,. ref='/mnt/shared/scratch/kmarians/private/dyslexia_gatk/workflow/resources/genome.fasta'. output:. vcf=""results/deepvariant/{sample}.vcf.gz"". params:. model=""WES"". threads: . 64. resources:. mem_mb=163840. log:. ""logs/deepvariant/{sample}/stdout.log"". singularity:. ""singularity/deepvariant_1.4.0.sif"". # ""singularity/deepvariant_1.4.0-gpu.sif"" # for GPU. shell:. """""". /opt/deepvariant/bin/run_deepvariant --model_type {params.model} --ref {input.ref} --reads {input.bam} --output_vcf {output.vcf} --num_shards {threads} --make_examples_extra_args='vsc_min_count_snps=3,vsc_min_fraction_snps=0.2,vsc_min_count_indels=3,vsc_min_fraction_indels=0.10'. """""". ```. Below ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/602
https://github.com/google/deepvariant/issues/602:729,security,log,log,729,"AttributeError: deleter; Hi DeepVariant team,. I'm running DeepVariant via Singularity via Snakemake on a HPC cluster and overall, if I create an interactive session, my entire WES pipeline runs fine until an odd DeepVariant job fails (by that I mean that let's say 30 deepvariant jobs finish ok and the 31st fails). I can then restart the Snakemake run and the same job will run fine, followed by more jobs that will also run fine until another odd jobs fails. This is also particularly true when I use the --cluster command in Snakemake (i.e., send it to a SLURM job scheduler from the master node) - in this event every single job fails. I could of course run all my samples on a single interactive session, keep checking the log file and restart the run every time it fails but I guess that's less than optimal plus this way I can really only run one sample at the time. For the interactive sessions I request 180G and 64cpus (in my case it's: ```srsh --mem=180G --cpus-per-task=64 --partition=long```). . I would request same parameters when using --cluster so:. ```snakemake --cluster ""sbatch --mem=180G cpus-per-task=64"" --jobs 64 --profie profile/ ```(where profile holds singularity args etc.). Singularity image is deepvariant_1.4.0.sif. my Snakemake rule:. ```. rule deepvariant:. input:. bam=rules.apply_bqsr.output.bam,. ref='/mnt/shared/scratch/kmarians/private/dyslexia_gatk/workflow/resources/genome.fasta'. output:. vcf=""results/deepvariant/{sample}.vcf.gz"". params:. model=""WES"". threads: . 64. resources:. mem_mb=163840. log:. ""logs/deepvariant/{sample}/stdout.log"". singularity:. ""singularity/deepvariant_1.4.0.sif"". # ""singularity/deepvariant_1.4.0-gpu.sif"" # for GPU. shell:. """""". /opt/deepvariant/bin/run_deepvariant --model_type {params.model} --ref {input.ref} --reads {input.bam} --output_vcf {output.vcf} --num_shards {threads} --make_examples_extra_args='vsc_min_count_snps=3,vsc_min_fraction_snps=0.2,vsc_min_count_indels=3,vsc_min_fraction_indels=0.10'. """""". ```. Below ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/602
https://github.com/google/deepvariant/issues/602:895,security,session,sessions,895,"AttributeError: deleter; Hi DeepVariant team,. I'm running DeepVariant via Singularity via Snakemake on a HPC cluster and overall, if I create an interactive session, my entire WES pipeline runs fine until an odd DeepVariant job fails (by that I mean that let's say 30 deepvariant jobs finish ok and the 31st fails). I can then restart the Snakemake run and the same job will run fine, followed by more jobs that will also run fine until another odd jobs fails. This is also particularly true when I use the --cluster command in Snakemake (i.e., send it to a SLURM job scheduler from the master node) - in this event every single job fails. I could of course run all my samples on a single interactive session, keep checking the log file and restart the run every time it fails but I guess that's less than optimal plus this way I can really only run one sample at the time. For the interactive sessions I request 180G and 64cpus (in my case it's: ```srsh --mem=180G --cpus-per-task=64 --partition=long```). . I would request same parameters when using --cluster so:. ```snakemake --cluster ""sbatch --mem=180G cpus-per-task=64"" --jobs 64 --profie profile/ ```(where profile holds singularity args etc.). Singularity image is deepvariant_1.4.0.sif. my Snakemake rule:. ```. rule deepvariant:. input:. bam=rules.apply_bqsr.output.bam,. ref='/mnt/shared/scratch/kmarians/private/dyslexia_gatk/workflow/resources/genome.fasta'. output:. vcf=""results/deepvariant/{sample}.vcf.gz"". params:. model=""WES"". threads: . 64. resources:. mem_mb=163840. log:. ""logs/deepvariant/{sample}/stdout.log"". singularity:. ""singularity/deepvariant_1.4.0.sif"". # ""singularity/deepvariant_1.4.0-gpu.sif"" # for GPU. shell:. """""". /opt/deepvariant/bin/run_deepvariant --model_type {params.model} --ref {input.ref} --reads {input.bam} --output_vcf {output.vcf} --num_shards {threads} --make_examples_extra_args='vsc_min_count_snps=3,vsc_min_fraction_snps=0.2,vsc_min_count_indels=3,vsc_min_fraction_indels=0.10'. """""". ```. Below ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/602
https://github.com/google/deepvariant/issues/602:1485,security,model,model,1485,"true when I use the --cluster command in Snakemake (i.e., send it to a SLURM job scheduler from the master node) - in this event every single job fails. I could of course run all my samples on a single interactive session, keep checking the log file and restart the run every time it fails but I guess that's less than optimal plus this way I can really only run one sample at the time. For the interactive sessions I request 180G and 64cpus (in my case it's: ```srsh --mem=180G --cpus-per-task=64 --partition=long```). . I would request same parameters when using --cluster so:. ```snakemake --cluster ""sbatch --mem=180G cpus-per-task=64"" --jobs 64 --profie profile/ ```(where profile holds singularity args etc.). Singularity image is deepvariant_1.4.0.sif. my Snakemake rule:. ```. rule deepvariant:. input:. bam=rules.apply_bqsr.output.bam,. ref='/mnt/shared/scratch/kmarians/private/dyslexia_gatk/workflow/resources/genome.fasta'. output:. vcf=""results/deepvariant/{sample}.vcf.gz"". params:. model=""WES"". threads: . 64. resources:. mem_mb=163840. log:. ""logs/deepvariant/{sample}/stdout.log"". singularity:. ""singularity/deepvariant_1.4.0.sif"". # ""singularity/deepvariant_1.4.0-gpu.sif"" # for GPU. shell:. """""". /opt/deepvariant/bin/run_deepvariant --model_type {params.model} --ref {input.ref} --reads {input.bam} --output_vcf {output.vcf} --num_shards {threads} --make_examples_extra_args='vsc_min_count_snps=3,vsc_min_fraction_snps=0.2,vsc_min_count_indels=3,vsc_min_fraction_indels=0.10'. """""". ```. Below is the begening and end of the log file. I am happy to include the entire log file but there is nothing out of the ordinary between those lines below (same output as for jobs that finished successfully). Could you please advise on what parameters to change to successfully run DeepVariant by submitting it to the SLURM scheduler? . I0104 18:49:24.340415 140179943589696 make_examples_core.py:243] Task 13/64: Found 2793 candidate variants. I0104 18:49:24.340478 140179943589696 make_examp",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/602
https://github.com/google/deepvariant/issues/602:1540,security,log,log,1540,"e., send it to a SLURM job scheduler from the master node) - in this event every single job fails. I could of course run all my samples on a single interactive session, keep checking the log file and restart the run every time it fails but I guess that's less than optimal plus this way I can really only run one sample at the time. For the interactive sessions I request 180G and 64cpus (in my case it's: ```srsh --mem=180G --cpus-per-task=64 --partition=long```). . I would request same parameters when using --cluster so:. ```snakemake --cluster ""sbatch --mem=180G cpus-per-task=64"" --jobs 64 --profie profile/ ```(where profile holds singularity args etc.). Singularity image is deepvariant_1.4.0.sif. my Snakemake rule:. ```. rule deepvariant:. input:. bam=rules.apply_bqsr.output.bam,. ref='/mnt/shared/scratch/kmarians/private/dyslexia_gatk/workflow/resources/genome.fasta'. output:. vcf=""results/deepvariant/{sample}.vcf.gz"". params:. model=""WES"". threads: . 64. resources:. mem_mb=163840. log:. ""logs/deepvariant/{sample}/stdout.log"". singularity:. ""singularity/deepvariant_1.4.0.sif"". # ""singularity/deepvariant_1.4.0-gpu.sif"" # for GPU. shell:. """""". /opt/deepvariant/bin/run_deepvariant --model_type {params.model} --ref {input.ref} --reads {input.bam} --output_vcf {output.vcf} --num_shards {threads} --make_examples_extra_args='vsc_min_count_snps=3,vsc_min_fraction_snps=0.2,vsc_min_count_indels=3,vsc_min_fraction_indels=0.10'. """""". ```. Below is the begening and end of the log file. I am happy to include the entire log file but there is nothing out of the ordinary between those lines below (same output as for jobs that finished successfully). Could you please advise on what parameters to change to successfully run DeepVariant by submitting it to the SLURM scheduler? . I0104 18:49:24.340415 140179943589696 make_examples_core.py:243] Task 13/64: Found 2793 candidate variants. I0104 18:49:24.340478 140179943589696 make_examples_core.py:243] Task 13/64: Created 2879 examples. Bu",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/602
https://github.com/google/deepvariant/issues/602:1547,security,log,logs,1547,"d it to a SLURM job scheduler from the master node) - in this event every single job fails. I could of course run all my samples on a single interactive session, keep checking the log file and restart the run every time it fails but I guess that's less than optimal plus this way I can really only run one sample at the time. For the interactive sessions I request 180G and 64cpus (in my case it's: ```srsh --mem=180G --cpus-per-task=64 --partition=long```). . I would request same parameters when using --cluster so:. ```snakemake --cluster ""sbatch --mem=180G cpus-per-task=64"" --jobs 64 --profie profile/ ```(where profile holds singularity args etc.). Singularity image is deepvariant_1.4.0.sif. my Snakemake rule:. ```. rule deepvariant:. input:. bam=rules.apply_bqsr.output.bam,. ref='/mnt/shared/scratch/kmarians/private/dyslexia_gatk/workflow/resources/genome.fasta'. output:. vcf=""results/deepvariant/{sample}.vcf.gz"". params:. model=""WES"". threads: . 64. resources:. mem_mb=163840. log:. ""logs/deepvariant/{sample}/stdout.log"". singularity:. ""singularity/deepvariant_1.4.0.sif"". # ""singularity/deepvariant_1.4.0-gpu.sif"" # for GPU. shell:. """""". /opt/deepvariant/bin/run_deepvariant --model_type {params.model} --ref {input.ref} --reads {input.bam} --output_vcf {output.vcf} --num_shards {threads} --make_examples_extra_args='vsc_min_count_snps=3,vsc_min_fraction_snps=0.2,vsc_min_count_indels=3,vsc_min_fraction_indels=0.10'. """""". ```. Below is the begening and end of the log file. I am happy to include the entire log file but there is nothing out of the ordinary between those lines below (same output as for jobs that finished successfully). Could you please advise on what parameters to change to successfully run DeepVariant by submitting it to the SLURM scheduler? . I0104 18:49:24.340415 140179943589696 make_examples_core.py:243] Task 13/64: Found 2793 candidate variants. I0104 18:49:24.340478 140179943589696 make_examples_core.py:243] Task 13/64: Created 2879 examples. Building ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/602
https://github.com/google/deepvariant/issues/602:1580,security,log,log,1580,"m the master node) - in this event every single job fails. I could of course run all my samples on a single interactive session, keep checking the log file and restart the run every time it fails but I guess that's less than optimal plus this way I can really only run one sample at the time. For the interactive sessions I request 180G and 64cpus (in my case it's: ```srsh --mem=180G --cpus-per-task=64 --partition=long```). . I would request same parameters when using --cluster so:. ```snakemake --cluster ""sbatch --mem=180G cpus-per-task=64"" --jobs 64 --profie profile/ ```(where profile holds singularity args etc.). Singularity image is deepvariant_1.4.0.sif. my Snakemake rule:. ```. rule deepvariant:. input:. bam=rules.apply_bqsr.output.bam,. ref='/mnt/shared/scratch/kmarians/private/dyslexia_gatk/workflow/resources/genome.fasta'. output:. vcf=""results/deepvariant/{sample}.vcf.gz"". params:. model=""WES"". threads: . 64. resources:. mem_mb=163840. log:. ""logs/deepvariant/{sample}/stdout.log"". singularity:. ""singularity/deepvariant_1.4.0.sif"". # ""singularity/deepvariant_1.4.0-gpu.sif"" # for GPU. shell:. """""". /opt/deepvariant/bin/run_deepvariant --model_type {params.model} --ref {input.ref} --reads {input.bam} --output_vcf {output.vcf} --num_shards {threads} --make_examples_extra_args='vsc_min_count_snps=3,vsc_min_fraction_snps=0.2,vsc_min_count_indels=3,vsc_min_fraction_indels=0.10'. """""". ```. Below is the begening and end of the log file. I am happy to include the entire log file but there is nothing out of the ordinary between those lines below (same output as for jobs that finished successfully). Could you please advise on what parameters to change to successfully run DeepVariant by submitting it to the SLURM scheduler? . I0104 18:49:24.340415 140179943589696 make_examples_core.py:243] Task 13/64: Found 2793 candidate variants. I0104 18:49:24.340478 140179943589696 make_examples_core.py:243] Task 13/64: Created 2879 examples. Building DAG of jobs... Using shell: /usr/",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/602
https://github.com/google/deepvariant/issues/602:1761,security,model,model,1761,"time it fails but I guess that's less than optimal plus this way I can really only run one sample at the time. For the interactive sessions I request 180G and 64cpus (in my case it's: ```srsh --mem=180G --cpus-per-task=64 --partition=long```). . I would request same parameters when using --cluster so:. ```snakemake --cluster ""sbatch --mem=180G cpus-per-task=64"" --jobs 64 --profie profile/ ```(where profile holds singularity args etc.). Singularity image is deepvariant_1.4.0.sif. my Snakemake rule:. ```. rule deepvariant:. input:. bam=rules.apply_bqsr.output.bam,. ref='/mnt/shared/scratch/kmarians/private/dyslexia_gatk/workflow/resources/genome.fasta'. output:. vcf=""results/deepvariant/{sample}.vcf.gz"". params:. model=""WES"". threads: . 64. resources:. mem_mb=163840. log:. ""logs/deepvariant/{sample}/stdout.log"". singularity:. ""singularity/deepvariant_1.4.0.sif"". # ""singularity/deepvariant_1.4.0-gpu.sif"" # for GPU. shell:. """""". /opt/deepvariant/bin/run_deepvariant --model_type {params.model} --ref {input.ref} --reads {input.bam} --output_vcf {output.vcf} --num_shards {threads} --make_examples_extra_args='vsc_min_count_snps=3,vsc_min_fraction_snps=0.2,vsc_min_count_indels=3,vsc_min_fraction_indels=0.10'. """""". ```. Below is the begening and end of the log file. I am happy to include the entire log file but there is nothing out of the ordinary between those lines below (same output as for jobs that finished successfully). Could you please advise on what parameters to change to successfully run DeepVariant by submitting it to the SLURM scheduler? . I0104 18:49:24.340415 140179943589696 make_examples_core.py:243] Task 13/64: Found 2793 candidate variants. I0104 18:49:24.340478 140179943589696 make_examples_core.py:243] Task 13/64: Created 2879 examples. Building DAG of jobs... Using shell: /usr/bin/bash. Provided cores: 64. Rules claiming more threads will be scaled down. Select jobs to execute... > [Wed Jan 4 18:30:51 2023]. > rule deepvariant:. > input: results/recal/s534",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/602
https://github.com/google/deepvariant/issues/602:2031,security,log,log,2031,"rameters when using --cluster so:. ```snakemake --cluster ""sbatch --mem=180G cpus-per-task=64"" --jobs 64 --profie profile/ ```(where profile holds singularity args etc.). Singularity image is deepvariant_1.4.0.sif. my Snakemake rule:. ```. rule deepvariant:. input:. bam=rules.apply_bqsr.output.bam,. ref='/mnt/shared/scratch/kmarians/private/dyslexia_gatk/workflow/resources/genome.fasta'. output:. vcf=""results/deepvariant/{sample}.vcf.gz"". params:. model=""WES"". threads: . 64. resources:. mem_mb=163840. log:. ""logs/deepvariant/{sample}/stdout.log"". singularity:. ""singularity/deepvariant_1.4.0.sif"". # ""singularity/deepvariant_1.4.0-gpu.sif"" # for GPU. shell:. """""". /opt/deepvariant/bin/run_deepvariant --model_type {params.model} --ref {input.ref} --reads {input.bam} --output_vcf {output.vcf} --num_shards {threads} --make_examples_extra_args='vsc_min_count_snps=3,vsc_min_fraction_snps=0.2,vsc_min_count_indels=3,vsc_min_fraction_indels=0.10'. """""". ```. Below is the begening and end of the log file. I am happy to include the entire log file but there is nothing out of the ordinary between those lines below (same output as for jobs that finished successfully). Could you please advise on what parameters to change to successfully run DeepVariant by submitting it to the SLURM scheduler? . I0104 18:49:24.340415 140179943589696 make_examples_core.py:243] Task 13/64: Found 2793 candidate variants. I0104 18:49:24.340478 140179943589696 make_examples_core.py:243] Task 13/64: Created 2879 examples. Building DAG of jobs... Using shell: /usr/bin/bash. Provided cores: 64. Rules claiming more threads will be scaled down. Select jobs to execute... > [Wed Jan 4 18:30:51 2023]. > rule deepvariant:. > input: results/recal/s534_EKDN210017195-1A_HTTJ3DSX2_L2.bam, /mnt/shared/scratch/kmarians/private/dyslexia_gatk/workflow/resources/genome.fasta. > output: results/deepvariant/s534_EKDN210017195-1A_HTTJ3DSX2_L2.vcf.gz. > log: logs/deepvariant/s534_EKDN210017195-1A_HTTJ3DSX2_L2/stdout.log. > job",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/602
https://github.com/google/deepvariant/issues/602:2074,security,log,log,2074,"make --cluster ""sbatch --mem=180G cpus-per-task=64"" --jobs 64 --profie profile/ ```(where profile holds singularity args etc.). Singularity image is deepvariant_1.4.0.sif. my Snakemake rule:. ```. rule deepvariant:. input:. bam=rules.apply_bqsr.output.bam,. ref='/mnt/shared/scratch/kmarians/private/dyslexia_gatk/workflow/resources/genome.fasta'. output:. vcf=""results/deepvariant/{sample}.vcf.gz"". params:. model=""WES"". threads: . 64. resources:. mem_mb=163840. log:. ""logs/deepvariant/{sample}/stdout.log"". singularity:. ""singularity/deepvariant_1.4.0.sif"". # ""singularity/deepvariant_1.4.0-gpu.sif"" # for GPU. shell:. """""". /opt/deepvariant/bin/run_deepvariant --model_type {params.model} --ref {input.ref} --reads {input.bam} --output_vcf {output.vcf} --num_shards {threads} --make_examples_extra_args='vsc_min_count_snps=3,vsc_min_fraction_snps=0.2,vsc_min_count_indels=3,vsc_min_fraction_indels=0.10'. """""". ```. Below is the begening and end of the log file. I am happy to include the entire log file but there is nothing out of the ordinary between those lines below (same output as for jobs that finished successfully). Could you please advise on what parameters to change to successfully run DeepVariant by submitting it to the SLURM scheduler? . I0104 18:49:24.340415 140179943589696 make_examples_core.py:243] Task 13/64: Found 2793 candidate variants. I0104 18:49:24.340478 140179943589696 make_examples_core.py:243] Task 13/64: Created 2879 examples. Building DAG of jobs... Using shell: /usr/bin/bash. Provided cores: 64. Rules claiming more threads will be scaled down. Select jobs to execute... > [Wed Jan 4 18:30:51 2023]. > rule deepvariant:. > input: results/recal/s534_EKDN210017195-1A_HTTJ3DSX2_L2.bam, /mnt/shared/scratch/kmarians/private/dyslexia_gatk/workflow/resources/genome.fasta. > output: results/deepvariant/s534_EKDN210017195-1A_HTTJ3DSX2_L2.vcf.gz. > log: logs/deepvariant/s534_EKDN210017195-1A_HTTJ3DSX2_L2/stdout.log. > jobid: 0. > wildcards: sample=s534_EKDN2100171",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/602
https://github.com/google/deepvariant/issues/602:2959,security,log,log,2959,"fraction_indels=0.10'. """""". ```. Below is the begening and end of the log file. I am happy to include the entire log file but there is nothing out of the ordinary between those lines below (same output as for jobs that finished successfully). Could you please advise on what parameters to change to successfully run DeepVariant by submitting it to the SLURM scheduler? . I0104 18:49:24.340415 140179943589696 make_examples_core.py:243] Task 13/64: Found 2793 candidate variants. I0104 18:49:24.340478 140179943589696 make_examples_core.py:243] Task 13/64: Created 2879 examples. Building DAG of jobs... Using shell: /usr/bin/bash. Provided cores: 64. Rules claiming more threads will be scaled down. Select jobs to execute... > [Wed Jan 4 18:30:51 2023]. > rule deepvariant:. > input: results/recal/s534_EKDN210017195-1A_HTTJ3DSX2_L2.bam, /mnt/shared/scratch/kmarians/private/dyslexia_gatk/workflow/resources/genome.fasta. > output: results/deepvariant/s534_EKDN210017195-1A_HTTJ3DSX2_L2.vcf.gz. > log: logs/deepvariant/s534_EKDN210017195-1A_HTTJ3DSX2_L2/stdout.log. > jobid: 0. > wildcards: sample=s534_EKDN210017195-1A_HTTJ3DSX2_L2. > threads: 64. > resources: mem_mb=163840, disk_mb=16401, tmpdir=/tmp/kmarians_4189323. > . > Activating singularity image singularity/deepvariant_1.4.0.sif. > INFO: Convert SIF file to sandbox... > I0104 18:31:03.183642 139718628308800 run_deepvariant.py:342] Re-using the directory for intermediate results in /tmp/kmarians_4189323/tmpxrz5rqbp. > . > ***** Intermediate results will be written to /tmp/kmarians_4189323/tmpxrz5rqbp in docker. ****. > . > . > ***** Running the command:*****. > time seq 0 63 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/mnt/shared/scratch/kmarians/private/dyslexia_gatk/workflow/resources/genome.fasta"" --reads ""results/recal/s534_EKDN210017195-1A_HTTJ3DSX2_L2.bam"" --examples ""/tmp/kmarians_4189323/tmpxrz5rqbp/make_examples.tfrecord@64.gz"" --channels ""insert_size"" --vsc_min_coun",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/602
https://github.com/google/deepvariant/issues/602:2964,security,log,logs,2964,"ion_indels=0.10'. """""". ```. Below is the begening and end of the log file. I am happy to include the entire log file but there is nothing out of the ordinary between those lines below (same output as for jobs that finished successfully). Could you please advise on what parameters to change to successfully run DeepVariant by submitting it to the SLURM scheduler? . I0104 18:49:24.340415 140179943589696 make_examples_core.py:243] Task 13/64: Found 2793 candidate variants. I0104 18:49:24.340478 140179943589696 make_examples_core.py:243] Task 13/64: Created 2879 examples. Building DAG of jobs... Using shell: /usr/bin/bash. Provided cores: 64. Rules claiming more threads will be scaled down. Select jobs to execute... > [Wed Jan 4 18:30:51 2023]. > rule deepvariant:. > input: results/recal/s534_EKDN210017195-1A_HTTJ3DSX2_L2.bam, /mnt/shared/scratch/kmarians/private/dyslexia_gatk/workflow/resources/genome.fasta. > output: results/deepvariant/s534_EKDN210017195-1A_HTTJ3DSX2_L2.vcf.gz. > log: logs/deepvariant/s534_EKDN210017195-1A_HTTJ3DSX2_L2/stdout.log. > jobid: 0. > wildcards: sample=s534_EKDN210017195-1A_HTTJ3DSX2_L2. > threads: 64. > resources: mem_mb=163840, disk_mb=16401, tmpdir=/tmp/kmarians_4189323. > . > Activating singularity image singularity/deepvariant_1.4.0.sif. > INFO: Convert SIF file to sandbox... > I0104 18:31:03.183642 139718628308800 run_deepvariant.py:342] Re-using the directory for intermediate results in /tmp/kmarians_4189323/tmpxrz5rqbp. > . > ***** Intermediate results will be written to /tmp/kmarians_4189323/tmpxrz5rqbp in docker. ****. > . > . > ***** Running the command:*****. > time seq 0 63 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/mnt/shared/scratch/kmarians/private/dyslexia_gatk/workflow/resources/genome.fasta"" --reads ""results/recal/s534_EKDN210017195-1A_HTTJ3DSX2_L2.bam"" --examples ""/tmp/kmarians_4189323/tmpxrz5rqbp/make_examples.tfrecord@64.gz"" --channels ""insert_size"" --vsc_min_count_ind",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/602
https://github.com/google/deepvariant/issues/602:3023,security,log,log,3023,"f the log file. I am happy to include the entire log file but there is nothing out of the ordinary between those lines below (same output as for jobs that finished successfully). Could you please advise on what parameters to change to successfully run DeepVariant by submitting it to the SLURM scheduler? . I0104 18:49:24.340415 140179943589696 make_examples_core.py:243] Task 13/64: Found 2793 candidate variants. I0104 18:49:24.340478 140179943589696 make_examples_core.py:243] Task 13/64: Created 2879 examples. Building DAG of jobs... Using shell: /usr/bin/bash. Provided cores: 64. Rules claiming more threads will be scaled down. Select jobs to execute... > [Wed Jan 4 18:30:51 2023]. > rule deepvariant:. > input: results/recal/s534_EKDN210017195-1A_HTTJ3DSX2_L2.bam, /mnt/shared/scratch/kmarians/private/dyslexia_gatk/workflow/resources/genome.fasta. > output: results/deepvariant/s534_EKDN210017195-1A_HTTJ3DSX2_L2.vcf.gz. > log: logs/deepvariant/s534_EKDN210017195-1A_HTTJ3DSX2_L2/stdout.log. > jobid: 0. > wildcards: sample=s534_EKDN210017195-1A_HTTJ3DSX2_L2. > threads: 64. > resources: mem_mb=163840, disk_mb=16401, tmpdir=/tmp/kmarians_4189323. > . > Activating singularity image singularity/deepvariant_1.4.0.sif. > INFO: Convert SIF file to sandbox... > I0104 18:31:03.183642 139718628308800 run_deepvariant.py:342] Re-using the directory for intermediate results in /tmp/kmarians_4189323/tmpxrz5rqbp. > . > ***** Intermediate results will be written to /tmp/kmarians_4189323/tmpxrz5rqbp in docker. ****. > . > . > ***** Running the command:*****. > time seq 0 63 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/mnt/shared/scratch/kmarians/private/dyslexia_gatk/workflow/resources/genome.fasta"" --reads ""results/recal/s534_EKDN210017195-1A_HTTJ3DSX2_L2.bam"" --examples ""/tmp/kmarians_4189323/tmpxrz5rqbp/make_examples.tfrecord@64.gz"" --channels ""insert_size"" --vsc_min_count_indels ""3"" --vsc_min_count_snps ""3"" --vsc_min_fraction_indels ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/602
https://github.com/google/deepvariant/issues/602:3282,security,sandbox,sandbox,3282,"nt by submitting it to the SLURM scheduler? . I0104 18:49:24.340415 140179943589696 make_examples_core.py:243] Task 13/64: Found 2793 candidate variants. I0104 18:49:24.340478 140179943589696 make_examples_core.py:243] Task 13/64: Created 2879 examples. Building DAG of jobs... Using shell: /usr/bin/bash. Provided cores: 64. Rules claiming more threads will be scaled down. Select jobs to execute... > [Wed Jan 4 18:30:51 2023]. > rule deepvariant:. > input: results/recal/s534_EKDN210017195-1A_HTTJ3DSX2_L2.bam, /mnt/shared/scratch/kmarians/private/dyslexia_gatk/workflow/resources/genome.fasta. > output: results/deepvariant/s534_EKDN210017195-1A_HTTJ3DSX2_L2.vcf.gz. > log: logs/deepvariant/s534_EKDN210017195-1A_HTTJ3DSX2_L2/stdout.log. > jobid: 0. > wildcards: sample=s534_EKDN210017195-1A_HTTJ3DSX2_L2. > threads: 64. > resources: mem_mb=163840, disk_mb=16401, tmpdir=/tmp/kmarians_4189323. > . > Activating singularity image singularity/deepvariant_1.4.0.sif. > INFO: Convert SIF file to sandbox... > I0104 18:31:03.183642 139718628308800 run_deepvariant.py:342] Re-using the directory for intermediate results in /tmp/kmarians_4189323/tmpxrz5rqbp. > . > ***** Intermediate results will be written to /tmp/kmarians_4189323/tmpxrz5rqbp in docker. ****. > . > . > ***** Running the command:*****. > time seq 0 63 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/mnt/shared/scratch/kmarians/private/dyslexia_gatk/workflow/resources/genome.fasta"" --reads ""results/recal/s534_EKDN210017195-1A_HTTJ3DSX2_L2.bam"" --examples ""/tmp/kmarians_4189323/tmpxrz5rqbp/make_examples.tfrecord@64.gz"" --channels ""insert_size"" --vsc_min_count_indels ""3"" --vsc_min_count_snps ""3"" --vsc_min_fraction_indels ""0.10"" --vsc_min_fraction_snps ""0.2"" --task {}. > *. > *. > *. > I0104 18:49:24.340415 140179943589696 make_examples_core.py:243] Task 13/64: Found 2793 candidate variants. > I0104 18:49:24.340478 140179943589696 make_examples_core.py:243] Task 13/64: Created 2",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/602
https://github.com/google/deepvariant/issues/602:4612,security,model,models,4612," -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/mnt/shared/scratch/kmarians/private/dyslexia_gatk/workflow/resources/genome.fasta"" --reads ""results/recal/s534_EKDN210017195-1A_HTTJ3DSX2_L2.bam"" --examples ""/tmp/kmarians_4189323/tmpxrz5rqbp/make_examples.tfrecord@64.gz"" --channels ""insert_size"" --vsc_min_count_indels ""3"" --vsc_min_count_snps ""3"" --vsc_min_fraction_indels ""0.10"" --vsc_min_fraction_snps ""0.2"" --task {}. > *. > *. > *. > I0104 18:49:24.340415 140179943589696 make_examples_core.py:243] Task 13/64: Found 2793 candidate variants. > I0104 18:49:24.340478 140179943589696 make_examples_core.py:243] Task 13/64: Created 2879 examples. > real	18m21.465s. > user	893m16.250s. > sys	15m57.561s. > . > ***** Running the command:*****. > time /opt/deepvariant/bin/call_variants --outfile ""/tmp/kmarians_4189323/tmpxrz5rqbp/call_variants_output.tfrecord.gz"" --examples ""/tmp/kmarians_4189323/tmpxrz5rqbp/make_examples.tfrecord@64.gz"" --checkpoint ""/opt/models/wes/model.ckpt"" --openvino_model_dir ""/tmp/kmarians_4189323/tmpxrz5rqbp"". > . > I0104 18:49:26.695084 139646404110144 call_variants.py:317] From /tmp/kmarians_4189323/tmpxrz5rqbp/make_examples.tfrecord-00000-of-00064.gz.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 19]. > I0104 18:49:26.697470 139646404110144 call_variants.py:317] From /opt/models/wes/model.ckpt.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 19]. > Traceback (most recent call last):. > File ""/tmp/kmarians_4189323/Bazel.runfiles_zims94rl/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 513, in <module>. > tf.compat.v1.app.run(). > File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. > _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). > File ""/tmp/kmarians_4189323/Bazel.runfiles_zims94rl/runfiles/absl_py/absl/app.py"",",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/602
https://github.com/google/deepvariant/issues/602:4623,security,model,model,4623,"2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/mnt/shared/scratch/kmarians/private/dyslexia_gatk/workflow/resources/genome.fasta"" --reads ""results/recal/s534_EKDN210017195-1A_HTTJ3DSX2_L2.bam"" --examples ""/tmp/kmarians_4189323/tmpxrz5rqbp/make_examples.tfrecord@64.gz"" --channels ""insert_size"" --vsc_min_count_indels ""3"" --vsc_min_count_snps ""3"" --vsc_min_fraction_indels ""0.10"" --vsc_min_fraction_snps ""0.2"" --task {}. > *. > *. > *. > I0104 18:49:24.340415 140179943589696 make_examples_core.py:243] Task 13/64: Found 2793 candidate variants. > I0104 18:49:24.340478 140179943589696 make_examples_core.py:243] Task 13/64: Created 2879 examples. > real	18m21.465s. > user	893m16.250s. > sys	15m57.561s. > . > ***** Running the command:*****. > time /opt/deepvariant/bin/call_variants --outfile ""/tmp/kmarians_4189323/tmpxrz5rqbp/call_variants_output.tfrecord.gz"" --examples ""/tmp/kmarians_4189323/tmpxrz5rqbp/make_examples.tfrecord@64.gz"" --checkpoint ""/opt/models/wes/model.ckpt"" --openvino_model_dir ""/tmp/kmarians_4189323/tmpxrz5rqbp"". > . > I0104 18:49:26.695084 139646404110144 call_variants.py:317] From /tmp/kmarians_4189323/tmpxrz5rqbp/make_examples.tfrecord-00000-of-00064.gz.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 19]. > I0104 18:49:26.697470 139646404110144 call_variants.py:317] From /opt/models/wes/model.ckpt.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 19]. > Traceback (most recent call last):. > File ""/tmp/kmarians_4189323/Bazel.runfiles_zims94rl/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 513, in <module>. > tf.compat.v1.app.run(). > File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. > _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). > File ""/tmp/kmarians_4189323/Bazel.runfiles_zims94rl/runfiles/absl_py/absl/app.py"", line 300, ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/602
https://github.com/google/deepvariant/issues/602:5022,security,model,models,5022,"""0.10"" --vsc_min_fraction_snps ""0.2"" --task {}. > *. > *. > *. > I0104 18:49:24.340415 140179943589696 make_examples_core.py:243] Task 13/64: Found 2793 candidate variants. > I0104 18:49:24.340478 140179943589696 make_examples_core.py:243] Task 13/64: Created 2879 examples. > real	18m21.465s. > user	893m16.250s. > sys	15m57.561s. > . > ***** Running the command:*****. > time /opt/deepvariant/bin/call_variants --outfile ""/tmp/kmarians_4189323/tmpxrz5rqbp/call_variants_output.tfrecord.gz"" --examples ""/tmp/kmarians_4189323/tmpxrz5rqbp/make_examples.tfrecord@64.gz"" --checkpoint ""/opt/models/wes/model.ckpt"" --openvino_model_dir ""/tmp/kmarians_4189323/tmpxrz5rqbp"". > . > I0104 18:49:26.695084 139646404110144 call_variants.py:317] From /tmp/kmarians_4189323/tmpxrz5rqbp/make_examples.tfrecord-00000-of-00064.gz.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 19]. > I0104 18:49:26.697470 139646404110144 call_variants.py:317] From /opt/models/wes/model.ckpt.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 19]. > Traceback (most recent call last):. > File ""/tmp/kmarians_4189323/Bazel.runfiles_zims94rl/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 513, in <module>. > tf.compat.v1.app.run(). > File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. > _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). > File ""/tmp/kmarians_4189323/Bazel.runfiles_zims94rl/runfiles/absl_py/absl/app.py"", line 300, in run. > _run_main(main, args). > File ""/tmp/kmarians_4189323/Bazel.runfiles_zims94rl/runfiles/absl_py/absl/app.py"", line 251, in _run_main. > sys.exit(main(argv)). > File ""/tmp/kmarians_4189323/Bazel.runfiles_zims94rl/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 494, in main. > call_variants(. > File ""/tmp/kmarians_4189323/Bazel.runfiles_zims94rl/runfiles/com_google_deepv",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/602
https://github.com/google/deepvariant/issues/602:5033,security,model,model,5033,"c_min_fraction_snps ""0.2"" --task {}. > *. > *. > *. > I0104 18:49:24.340415 140179943589696 make_examples_core.py:243] Task 13/64: Found 2793 candidate variants. > I0104 18:49:24.340478 140179943589696 make_examples_core.py:243] Task 13/64: Created 2879 examples. > real	18m21.465s. > user	893m16.250s. > sys	15m57.561s. > . > ***** Running the command:*****. > time /opt/deepvariant/bin/call_variants --outfile ""/tmp/kmarians_4189323/tmpxrz5rqbp/call_variants_output.tfrecord.gz"" --examples ""/tmp/kmarians_4189323/tmpxrz5rqbp/make_examples.tfrecord@64.gz"" --checkpoint ""/opt/models/wes/model.ckpt"" --openvino_model_dir ""/tmp/kmarians_4189323/tmpxrz5rqbp"". > . > I0104 18:49:26.695084 139646404110144 call_variants.py:317] From /tmp/kmarians_4189323/tmpxrz5rqbp/make_examples.tfrecord-00000-of-00064.gz.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 19]. > I0104 18:49:26.697470 139646404110144 call_variants.py:317] From /opt/models/wes/model.ckpt.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 19]. > Traceback (most recent call last):. > File ""/tmp/kmarians_4189323/Bazel.runfiles_zims94rl/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 513, in <module>. > tf.compat.v1.app.run(). > File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. > _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). > File ""/tmp/kmarians_4189323/Bazel.runfiles_zims94rl/runfiles/absl_py/absl/app.py"", line 300, in run. > _run_main(main, args). > File ""/tmp/kmarians_4189323/Bazel.runfiles_zims94rl/runfiles/absl_py/absl/app.py"", line 251, in _run_main. > sys.exit(main(argv)). > File ""/tmp/kmarians_4189323/Bazel.runfiles_zims94rl/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 494, in main. > call_variants(. > File ""/tmp/kmarians_4189323/Bazel.runfiles_zims94rl/runfiles/com_google_deepvariant/deep",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/602
https://github.com/google/deepvariant/issues/602:7393,security,availab,available,7393,"s). > File ""/tmp/kmarians_4189323/Bazel.runfiles_zims94rl/runfiles/absl_py/absl/app.py"", line 251, in _run_main. > sys.exit(main(argv)). > File ""/tmp/kmarians_4189323/Bazel.runfiles_zims94rl/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 494, in main. > call_variants(. > File ""/tmp/kmarians_4189323/Bazel.runfiles_zims94rl/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 389, in call_variants. > init_op = tf.group(tf.compat.v1.global_variables_initializer(),. > File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/ops/variables.py"", line 3319, in global_variables_initializer. > return variables_initializer(global_variables()). > File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/ops/variables.py"", line 3139, in global_variables. > return ops.get_collection(ops.GraphKeys.GLOBAL_VARIABLES, scope). > File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py"", line 6605, in get_collection. > return get_default_graph().get_collection(key, scope). > File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py"", line 6231, in get_default_graph. > return _default_graph_stack.get_default(). > File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py"", line 5742, in get_default. > self._global_default_graph = Graph(). > File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py"", line 3113, in __init__. > self._scoped_c_graph = c_api_util.ScopedTFGraph(). > File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/c_api_util.py"", line 50, in __init__. > self.graph = c_api.TF_NewGraph(). > RuntimeError: random_device::random_device(const std::string&): device not available. > Exception ignored in: <function ScopedTFGraph.__del__ at 0x7f01d64378b0>. > Traceback (most recent call last):. > File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/c_api_util.py"", line 58, in __del__. > AttributeError: deleter",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/602
https://github.com/google/deepvariant/issues/602:729,testability,log,log,729,"AttributeError: deleter; Hi DeepVariant team,. I'm running DeepVariant via Singularity via Snakemake on a HPC cluster and overall, if I create an interactive session, my entire WES pipeline runs fine until an odd DeepVariant job fails (by that I mean that let's say 30 deepvariant jobs finish ok and the 31st fails). I can then restart the Snakemake run and the same job will run fine, followed by more jobs that will also run fine until another odd jobs fails. This is also particularly true when I use the --cluster command in Snakemake (i.e., send it to a SLURM job scheduler from the master node) - in this event every single job fails. I could of course run all my samples on a single interactive session, keep checking the log file and restart the run every time it fails but I guess that's less than optimal plus this way I can really only run one sample at the time. For the interactive sessions I request 180G and 64cpus (in my case it's: ```srsh --mem=180G --cpus-per-task=64 --partition=long```). . I would request same parameters when using --cluster so:. ```snakemake --cluster ""sbatch --mem=180G cpus-per-task=64"" --jobs 64 --profie profile/ ```(where profile holds singularity args etc.). Singularity image is deepvariant_1.4.0.sif. my Snakemake rule:. ```. rule deepvariant:. input:. bam=rules.apply_bqsr.output.bam,. ref='/mnt/shared/scratch/kmarians/private/dyslexia_gatk/workflow/resources/genome.fasta'. output:. vcf=""results/deepvariant/{sample}.vcf.gz"". params:. model=""WES"". threads: . 64. resources:. mem_mb=163840. log:. ""logs/deepvariant/{sample}/stdout.log"". singularity:. ""singularity/deepvariant_1.4.0.sif"". # ""singularity/deepvariant_1.4.0-gpu.sif"" # for GPU. shell:. """""". /opt/deepvariant/bin/run_deepvariant --model_type {params.model} --ref {input.ref} --reads {input.bam} --output_vcf {output.vcf} --num_shards {threads} --make_examples_extra_args='vsc_min_count_snps=3,vsc_min_fraction_snps=0.2,vsc_min_count_indels=3,vsc_min_fraction_indels=0.10'. """""". ```. Below ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/602
https://github.com/google/deepvariant/issues/602:1399,testability,resourc,resources,1399,"obs that will also run fine until another odd jobs fails. This is also particularly true when I use the --cluster command in Snakemake (i.e., send it to a SLURM job scheduler from the master node) - in this event every single job fails. I could of course run all my samples on a single interactive session, keep checking the log file and restart the run every time it fails but I guess that's less than optimal plus this way I can really only run one sample at the time. For the interactive sessions I request 180G and 64cpus (in my case it's: ```srsh --mem=180G --cpus-per-task=64 --partition=long```). . I would request same parameters when using --cluster so:. ```snakemake --cluster ""sbatch --mem=180G cpus-per-task=64"" --jobs 64 --profie profile/ ```(where profile holds singularity args etc.). Singularity image is deepvariant_1.4.0.sif. my Snakemake rule:. ```. rule deepvariant:. input:. bam=rules.apply_bqsr.output.bam,. ref='/mnt/shared/scratch/kmarians/private/dyslexia_gatk/workflow/resources/genome.fasta'. output:. vcf=""results/deepvariant/{sample}.vcf.gz"". params:. model=""WES"". threads: . 64. resources:. mem_mb=163840. log:. ""logs/deepvariant/{sample}/stdout.log"". singularity:. ""singularity/deepvariant_1.4.0.sif"". # ""singularity/deepvariant_1.4.0-gpu.sif"" # for GPU. shell:. """""". /opt/deepvariant/bin/run_deepvariant --model_type {params.model} --ref {input.ref} --reads {input.bam} --output_vcf {output.vcf} --num_shards {threads} --make_examples_extra_args='vsc_min_count_snps=3,vsc_min_fraction_snps=0.2,vsc_min_count_indels=3,vsc_min_fraction_indels=0.10'. """""". ```. Below is the begening and end of the log file. I am happy to include the entire log file but there is nothing out of the ordinary between those lines below (same output as for jobs that finished successfully). Could you please advise on what parameters to change to successfully run DeepVariant by submitting it to the SLURM scheduler? . I0104 18:49:24.340415 140179943589696 make_examples_core.py:243] Task 13",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/602
https://github.com/google/deepvariant/issues/602:1513,testability,resourc,resources,1513,"command in Snakemake (i.e., send it to a SLURM job scheduler from the master node) - in this event every single job fails. I could of course run all my samples on a single interactive session, keep checking the log file and restart the run every time it fails but I guess that's less than optimal plus this way I can really only run one sample at the time. For the interactive sessions I request 180G and 64cpus (in my case it's: ```srsh --mem=180G --cpus-per-task=64 --partition=long```). . I would request same parameters when using --cluster so:. ```snakemake --cluster ""sbatch --mem=180G cpus-per-task=64"" --jobs 64 --profie profile/ ```(where profile holds singularity args etc.). Singularity image is deepvariant_1.4.0.sif. my Snakemake rule:. ```. rule deepvariant:. input:. bam=rules.apply_bqsr.output.bam,. ref='/mnt/shared/scratch/kmarians/private/dyslexia_gatk/workflow/resources/genome.fasta'. output:. vcf=""results/deepvariant/{sample}.vcf.gz"". params:. model=""WES"". threads: . 64. resources:. mem_mb=163840. log:. ""logs/deepvariant/{sample}/stdout.log"". singularity:. ""singularity/deepvariant_1.4.0.sif"". # ""singularity/deepvariant_1.4.0-gpu.sif"" # for GPU. shell:. """""". /opt/deepvariant/bin/run_deepvariant --model_type {params.model} --ref {input.ref} --reads {input.bam} --output_vcf {output.vcf} --num_shards {threads} --make_examples_extra_args='vsc_min_count_snps=3,vsc_min_fraction_snps=0.2,vsc_min_count_indels=3,vsc_min_fraction_indels=0.10'. """""". ```. Below is the begening and end of the log file. I am happy to include the entire log file but there is nothing out of the ordinary between those lines below (same output as for jobs that finished successfully). Could you please advise on what parameters to change to successfully run DeepVariant by submitting it to the SLURM scheduler? . I0104 18:49:24.340415 140179943589696 make_examples_core.py:243] Task 13/64: Found 2793 candidate variants. I0104 18:49:24.340478 140179943589696 make_examples_core.py:243] Task 13/64: C",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/602
https://github.com/google/deepvariant/issues/602:1540,testability,log,log,1540,"e., send it to a SLURM job scheduler from the master node) - in this event every single job fails. I could of course run all my samples on a single interactive session, keep checking the log file and restart the run every time it fails but I guess that's less than optimal plus this way I can really only run one sample at the time. For the interactive sessions I request 180G and 64cpus (in my case it's: ```srsh --mem=180G --cpus-per-task=64 --partition=long```). . I would request same parameters when using --cluster so:. ```snakemake --cluster ""sbatch --mem=180G cpus-per-task=64"" --jobs 64 --profie profile/ ```(where profile holds singularity args etc.). Singularity image is deepvariant_1.4.0.sif. my Snakemake rule:. ```. rule deepvariant:. input:. bam=rules.apply_bqsr.output.bam,. ref='/mnt/shared/scratch/kmarians/private/dyslexia_gatk/workflow/resources/genome.fasta'. output:. vcf=""results/deepvariant/{sample}.vcf.gz"". params:. model=""WES"". threads: . 64. resources:. mem_mb=163840. log:. ""logs/deepvariant/{sample}/stdout.log"". singularity:. ""singularity/deepvariant_1.4.0.sif"". # ""singularity/deepvariant_1.4.0-gpu.sif"" # for GPU. shell:. """""". /opt/deepvariant/bin/run_deepvariant --model_type {params.model} --ref {input.ref} --reads {input.bam} --output_vcf {output.vcf} --num_shards {threads} --make_examples_extra_args='vsc_min_count_snps=3,vsc_min_fraction_snps=0.2,vsc_min_count_indels=3,vsc_min_fraction_indels=0.10'. """""". ```. Below is the begening and end of the log file. I am happy to include the entire log file but there is nothing out of the ordinary between those lines below (same output as for jobs that finished successfully). Could you please advise on what parameters to change to successfully run DeepVariant by submitting it to the SLURM scheduler? . I0104 18:49:24.340415 140179943589696 make_examples_core.py:243] Task 13/64: Found 2793 candidate variants. I0104 18:49:24.340478 140179943589696 make_examples_core.py:243] Task 13/64: Created 2879 examples. Bu",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/602
https://github.com/google/deepvariant/issues/602:1547,testability,log,logs,1547,"d it to a SLURM job scheduler from the master node) - in this event every single job fails. I could of course run all my samples on a single interactive session, keep checking the log file and restart the run every time it fails but I guess that's less than optimal plus this way I can really only run one sample at the time. For the interactive sessions I request 180G and 64cpus (in my case it's: ```srsh --mem=180G --cpus-per-task=64 --partition=long```). . I would request same parameters when using --cluster so:. ```snakemake --cluster ""sbatch --mem=180G cpus-per-task=64"" --jobs 64 --profie profile/ ```(where profile holds singularity args etc.). Singularity image is deepvariant_1.4.0.sif. my Snakemake rule:. ```. rule deepvariant:. input:. bam=rules.apply_bqsr.output.bam,. ref='/mnt/shared/scratch/kmarians/private/dyslexia_gatk/workflow/resources/genome.fasta'. output:. vcf=""results/deepvariant/{sample}.vcf.gz"". params:. model=""WES"". threads: . 64. resources:. mem_mb=163840. log:. ""logs/deepvariant/{sample}/stdout.log"". singularity:. ""singularity/deepvariant_1.4.0.sif"". # ""singularity/deepvariant_1.4.0-gpu.sif"" # for GPU. shell:. """""". /opt/deepvariant/bin/run_deepvariant --model_type {params.model} --ref {input.ref} --reads {input.bam} --output_vcf {output.vcf} --num_shards {threads} --make_examples_extra_args='vsc_min_count_snps=3,vsc_min_fraction_snps=0.2,vsc_min_count_indels=3,vsc_min_fraction_indels=0.10'. """""". ```. Below is the begening and end of the log file. I am happy to include the entire log file but there is nothing out of the ordinary between those lines below (same output as for jobs that finished successfully). Could you please advise on what parameters to change to successfully run DeepVariant by submitting it to the SLURM scheduler? . I0104 18:49:24.340415 140179943589696 make_examples_core.py:243] Task 13/64: Found 2793 candidate variants. I0104 18:49:24.340478 140179943589696 make_examples_core.py:243] Task 13/64: Created 2879 examples. Building ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/602
https://github.com/google/deepvariant/issues/602:1580,testability,log,log,1580,"m the master node) - in this event every single job fails. I could of course run all my samples on a single interactive session, keep checking the log file and restart the run every time it fails but I guess that's less than optimal plus this way I can really only run one sample at the time. For the interactive sessions I request 180G and 64cpus (in my case it's: ```srsh --mem=180G --cpus-per-task=64 --partition=long```). . I would request same parameters when using --cluster so:. ```snakemake --cluster ""sbatch --mem=180G cpus-per-task=64"" --jobs 64 --profie profile/ ```(where profile holds singularity args etc.). Singularity image is deepvariant_1.4.0.sif. my Snakemake rule:. ```. rule deepvariant:. input:. bam=rules.apply_bqsr.output.bam,. ref='/mnt/shared/scratch/kmarians/private/dyslexia_gatk/workflow/resources/genome.fasta'. output:. vcf=""results/deepvariant/{sample}.vcf.gz"". params:. model=""WES"". threads: . 64. resources:. mem_mb=163840. log:. ""logs/deepvariant/{sample}/stdout.log"". singularity:. ""singularity/deepvariant_1.4.0.sif"". # ""singularity/deepvariant_1.4.0-gpu.sif"" # for GPU. shell:. """""". /opt/deepvariant/bin/run_deepvariant --model_type {params.model} --ref {input.ref} --reads {input.bam} --output_vcf {output.vcf} --num_shards {threads} --make_examples_extra_args='vsc_min_count_snps=3,vsc_min_fraction_snps=0.2,vsc_min_count_indels=3,vsc_min_fraction_indels=0.10'. """""". ```. Below is the begening and end of the log file. I am happy to include the entire log file but there is nothing out of the ordinary between those lines below (same output as for jobs that finished successfully). Could you please advise on what parameters to change to successfully run DeepVariant by submitting it to the SLURM scheduler? . I0104 18:49:24.340415 140179943589696 make_examples_core.py:243] Task 13/64: Found 2793 candidate variants. I0104 18:49:24.340478 140179943589696 make_examples_core.py:243] Task 13/64: Created 2879 examples. Building DAG of jobs... Using shell: /usr/",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/602
https://github.com/google/deepvariant/issues/602:2031,testability,log,log,2031,"rameters when using --cluster so:. ```snakemake --cluster ""sbatch --mem=180G cpus-per-task=64"" --jobs 64 --profie profile/ ```(where profile holds singularity args etc.). Singularity image is deepvariant_1.4.0.sif. my Snakemake rule:. ```. rule deepvariant:. input:. bam=rules.apply_bqsr.output.bam,. ref='/mnt/shared/scratch/kmarians/private/dyslexia_gatk/workflow/resources/genome.fasta'. output:. vcf=""results/deepvariant/{sample}.vcf.gz"". params:. model=""WES"". threads: . 64. resources:. mem_mb=163840. log:. ""logs/deepvariant/{sample}/stdout.log"". singularity:. ""singularity/deepvariant_1.4.0.sif"". # ""singularity/deepvariant_1.4.0-gpu.sif"" # for GPU. shell:. """""". /opt/deepvariant/bin/run_deepvariant --model_type {params.model} --ref {input.ref} --reads {input.bam} --output_vcf {output.vcf} --num_shards {threads} --make_examples_extra_args='vsc_min_count_snps=3,vsc_min_fraction_snps=0.2,vsc_min_count_indels=3,vsc_min_fraction_indels=0.10'. """""". ```. Below is the begening and end of the log file. I am happy to include the entire log file but there is nothing out of the ordinary between those lines below (same output as for jobs that finished successfully). Could you please advise on what parameters to change to successfully run DeepVariant by submitting it to the SLURM scheduler? . I0104 18:49:24.340415 140179943589696 make_examples_core.py:243] Task 13/64: Found 2793 candidate variants. I0104 18:49:24.340478 140179943589696 make_examples_core.py:243] Task 13/64: Created 2879 examples. Building DAG of jobs... Using shell: /usr/bin/bash. Provided cores: 64. Rules claiming more threads will be scaled down. Select jobs to execute... > [Wed Jan 4 18:30:51 2023]. > rule deepvariant:. > input: results/recal/s534_EKDN210017195-1A_HTTJ3DSX2_L2.bam, /mnt/shared/scratch/kmarians/private/dyslexia_gatk/workflow/resources/genome.fasta. > output: results/deepvariant/s534_EKDN210017195-1A_HTTJ3DSX2_L2.vcf.gz. > log: logs/deepvariant/s534_EKDN210017195-1A_HTTJ3DSX2_L2/stdout.log. > job",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/602
https://github.com/google/deepvariant/issues/602:2074,testability,log,log,2074,"make --cluster ""sbatch --mem=180G cpus-per-task=64"" --jobs 64 --profie profile/ ```(where profile holds singularity args etc.). Singularity image is deepvariant_1.4.0.sif. my Snakemake rule:. ```. rule deepvariant:. input:. bam=rules.apply_bqsr.output.bam,. ref='/mnt/shared/scratch/kmarians/private/dyslexia_gatk/workflow/resources/genome.fasta'. output:. vcf=""results/deepvariant/{sample}.vcf.gz"". params:. model=""WES"". threads: . 64. resources:. mem_mb=163840. log:. ""logs/deepvariant/{sample}/stdout.log"". singularity:. ""singularity/deepvariant_1.4.0.sif"". # ""singularity/deepvariant_1.4.0-gpu.sif"" # for GPU. shell:. """""". /opt/deepvariant/bin/run_deepvariant --model_type {params.model} --ref {input.ref} --reads {input.bam} --output_vcf {output.vcf} --num_shards {threads} --make_examples_extra_args='vsc_min_count_snps=3,vsc_min_fraction_snps=0.2,vsc_min_count_indels=3,vsc_min_fraction_indels=0.10'. """""". ```. Below is the begening and end of the log file. I am happy to include the entire log file but there is nothing out of the ordinary between those lines below (same output as for jobs that finished successfully). Could you please advise on what parameters to change to successfully run DeepVariant by submitting it to the SLURM scheduler? . I0104 18:49:24.340415 140179943589696 make_examples_core.py:243] Task 13/64: Found 2793 candidate variants. I0104 18:49:24.340478 140179943589696 make_examples_core.py:243] Task 13/64: Created 2879 examples. Building DAG of jobs... Using shell: /usr/bin/bash. Provided cores: 64. Rules claiming more threads will be scaled down. Select jobs to execute... > [Wed Jan 4 18:30:51 2023]. > rule deepvariant:. > input: results/recal/s534_EKDN210017195-1A_HTTJ3DSX2_L2.bam, /mnt/shared/scratch/kmarians/private/dyslexia_gatk/workflow/resources/genome.fasta. > output: results/deepvariant/s534_EKDN210017195-1A_HTTJ3DSX2_L2.vcf.gz. > log: logs/deepvariant/s534_EKDN210017195-1A_HTTJ3DSX2_L2/stdout.log. > jobid: 0. > wildcards: sample=s534_EKDN2100171",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/602
https://github.com/google/deepvariant/issues/602:2860,testability,resourc,resources,2860,"mples_extra_args='vsc_min_count_snps=3,vsc_min_fraction_snps=0.2,vsc_min_count_indels=3,vsc_min_fraction_indels=0.10'. """""". ```. Below is the begening and end of the log file. I am happy to include the entire log file but there is nothing out of the ordinary between those lines below (same output as for jobs that finished successfully). Could you please advise on what parameters to change to successfully run DeepVariant by submitting it to the SLURM scheduler? . I0104 18:49:24.340415 140179943589696 make_examples_core.py:243] Task 13/64: Found 2793 candidate variants. I0104 18:49:24.340478 140179943589696 make_examples_core.py:243] Task 13/64: Created 2879 examples. Building DAG of jobs... Using shell: /usr/bin/bash. Provided cores: 64. Rules claiming more threads will be scaled down. Select jobs to execute... > [Wed Jan 4 18:30:51 2023]. > rule deepvariant:. > input: results/recal/s534_EKDN210017195-1A_HTTJ3DSX2_L2.bam, /mnt/shared/scratch/kmarians/private/dyslexia_gatk/workflow/resources/genome.fasta. > output: results/deepvariant/s534_EKDN210017195-1A_HTTJ3DSX2_L2.vcf.gz. > log: logs/deepvariant/s534_EKDN210017195-1A_HTTJ3DSX2_L2/stdout.log. > jobid: 0. > wildcards: sample=s534_EKDN210017195-1A_HTTJ3DSX2_L2. > threads: 64. > resources: mem_mb=163840, disk_mb=16401, tmpdir=/tmp/kmarians_4189323. > . > Activating singularity image singularity/deepvariant_1.4.0.sif. > INFO: Convert SIF file to sandbox... > I0104 18:31:03.183642 139718628308800 run_deepvariant.py:342] Re-using the directory for intermediate results in /tmp/kmarians_4189323/tmpxrz5rqbp. > . > ***** Intermediate results will be written to /tmp/kmarians_4189323/tmpxrz5rqbp in docker. ****. > . > . > ***** Running the command:*****. > time seq 0 63 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/mnt/shared/scratch/kmarians/private/dyslexia_gatk/workflow/resources/genome.fasta"" --reads ""results/recal/s534_EKDN210017195-1A_HTTJ3DSX2_L2.bam"" --examples ""/tmp/km",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/602
https://github.com/google/deepvariant/issues/602:2959,testability,log,log,2959,"fraction_indels=0.10'. """""". ```. Below is the begening and end of the log file. I am happy to include the entire log file but there is nothing out of the ordinary between those lines below (same output as for jobs that finished successfully). Could you please advise on what parameters to change to successfully run DeepVariant by submitting it to the SLURM scheduler? . I0104 18:49:24.340415 140179943589696 make_examples_core.py:243] Task 13/64: Found 2793 candidate variants. I0104 18:49:24.340478 140179943589696 make_examples_core.py:243] Task 13/64: Created 2879 examples. Building DAG of jobs... Using shell: /usr/bin/bash. Provided cores: 64. Rules claiming more threads will be scaled down. Select jobs to execute... > [Wed Jan 4 18:30:51 2023]. > rule deepvariant:. > input: results/recal/s534_EKDN210017195-1A_HTTJ3DSX2_L2.bam, /mnt/shared/scratch/kmarians/private/dyslexia_gatk/workflow/resources/genome.fasta. > output: results/deepvariant/s534_EKDN210017195-1A_HTTJ3DSX2_L2.vcf.gz. > log: logs/deepvariant/s534_EKDN210017195-1A_HTTJ3DSX2_L2/stdout.log. > jobid: 0. > wildcards: sample=s534_EKDN210017195-1A_HTTJ3DSX2_L2. > threads: 64. > resources: mem_mb=163840, disk_mb=16401, tmpdir=/tmp/kmarians_4189323. > . > Activating singularity image singularity/deepvariant_1.4.0.sif. > INFO: Convert SIF file to sandbox... > I0104 18:31:03.183642 139718628308800 run_deepvariant.py:342] Re-using the directory for intermediate results in /tmp/kmarians_4189323/tmpxrz5rqbp. > . > ***** Intermediate results will be written to /tmp/kmarians_4189323/tmpxrz5rqbp in docker. ****. > . > . > ***** Running the command:*****. > time seq 0 63 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/mnt/shared/scratch/kmarians/private/dyslexia_gatk/workflow/resources/genome.fasta"" --reads ""results/recal/s534_EKDN210017195-1A_HTTJ3DSX2_L2.bam"" --examples ""/tmp/kmarians_4189323/tmpxrz5rqbp/make_examples.tfrecord@64.gz"" --channels ""insert_size"" --vsc_min_coun",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/602
https://github.com/google/deepvariant/issues/602:2964,testability,log,logs,2964,"ion_indels=0.10'. """""". ```. Below is the begening and end of the log file. I am happy to include the entire log file but there is nothing out of the ordinary between those lines below (same output as for jobs that finished successfully). Could you please advise on what parameters to change to successfully run DeepVariant by submitting it to the SLURM scheduler? . I0104 18:49:24.340415 140179943589696 make_examples_core.py:243] Task 13/64: Found 2793 candidate variants. I0104 18:49:24.340478 140179943589696 make_examples_core.py:243] Task 13/64: Created 2879 examples. Building DAG of jobs... Using shell: /usr/bin/bash. Provided cores: 64. Rules claiming more threads will be scaled down. Select jobs to execute... > [Wed Jan 4 18:30:51 2023]. > rule deepvariant:. > input: results/recal/s534_EKDN210017195-1A_HTTJ3DSX2_L2.bam, /mnt/shared/scratch/kmarians/private/dyslexia_gatk/workflow/resources/genome.fasta. > output: results/deepvariant/s534_EKDN210017195-1A_HTTJ3DSX2_L2.vcf.gz. > log: logs/deepvariant/s534_EKDN210017195-1A_HTTJ3DSX2_L2/stdout.log. > jobid: 0. > wildcards: sample=s534_EKDN210017195-1A_HTTJ3DSX2_L2. > threads: 64. > resources: mem_mb=163840, disk_mb=16401, tmpdir=/tmp/kmarians_4189323. > . > Activating singularity image singularity/deepvariant_1.4.0.sif. > INFO: Convert SIF file to sandbox... > I0104 18:31:03.183642 139718628308800 run_deepvariant.py:342] Re-using the directory for intermediate results in /tmp/kmarians_4189323/tmpxrz5rqbp. > . > ***** Intermediate results will be written to /tmp/kmarians_4189323/tmpxrz5rqbp in docker. ****. > . > . > ***** Running the command:*****. > time seq 0 63 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/mnt/shared/scratch/kmarians/private/dyslexia_gatk/workflow/resources/genome.fasta"" --reads ""results/recal/s534_EKDN210017195-1A_HTTJ3DSX2_L2.bam"" --examples ""/tmp/kmarians_4189323/tmpxrz5rqbp/make_examples.tfrecord@64.gz"" --channels ""insert_size"" --vsc_min_count_ind",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/602
https://github.com/google/deepvariant/issues/602:3023,testability,log,log,3023,"f the log file. I am happy to include the entire log file but there is nothing out of the ordinary between those lines below (same output as for jobs that finished successfully). Could you please advise on what parameters to change to successfully run DeepVariant by submitting it to the SLURM scheduler? . I0104 18:49:24.340415 140179943589696 make_examples_core.py:243] Task 13/64: Found 2793 candidate variants. I0104 18:49:24.340478 140179943589696 make_examples_core.py:243] Task 13/64: Created 2879 examples. Building DAG of jobs... Using shell: /usr/bin/bash. Provided cores: 64. Rules claiming more threads will be scaled down. Select jobs to execute... > [Wed Jan 4 18:30:51 2023]. > rule deepvariant:. > input: results/recal/s534_EKDN210017195-1A_HTTJ3DSX2_L2.bam, /mnt/shared/scratch/kmarians/private/dyslexia_gatk/workflow/resources/genome.fasta. > output: results/deepvariant/s534_EKDN210017195-1A_HTTJ3DSX2_L2.vcf.gz. > log: logs/deepvariant/s534_EKDN210017195-1A_HTTJ3DSX2_L2/stdout.log. > jobid: 0. > wildcards: sample=s534_EKDN210017195-1A_HTTJ3DSX2_L2. > threads: 64. > resources: mem_mb=163840, disk_mb=16401, tmpdir=/tmp/kmarians_4189323. > . > Activating singularity image singularity/deepvariant_1.4.0.sif. > INFO: Convert SIF file to sandbox... > I0104 18:31:03.183642 139718628308800 run_deepvariant.py:342] Re-using the directory for intermediate results in /tmp/kmarians_4189323/tmpxrz5rqbp. > . > ***** Intermediate results will be written to /tmp/kmarians_4189323/tmpxrz5rqbp in docker. ****. > . > . > ***** Running the command:*****. > time seq 0 63 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/mnt/shared/scratch/kmarians/private/dyslexia_gatk/workflow/resources/genome.fasta"" --reads ""results/recal/s534_EKDN210017195-1A_HTTJ3DSX2_L2.bam"" --examples ""/tmp/kmarians_4189323/tmpxrz5rqbp/make_examples.tfrecord@64.gz"" --channels ""insert_size"" --vsc_min_count_indels ""3"" --vsc_min_count_snps ""3"" --vsc_min_fraction_indels ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/602
https://github.com/google/deepvariant/issues/602:3113,testability,resourc,resources,3113,"inary between those lines below (same output as for jobs that finished successfully). Could you please advise on what parameters to change to successfully run DeepVariant by submitting it to the SLURM scheduler? . I0104 18:49:24.340415 140179943589696 make_examples_core.py:243] Task 13/64: Found 2793 candidate variants. I0104 18:49:24.340478 140179943589696 make_examples_core.py:243] Task 13/64: Created 2879 examples. Building DAG of jobs... Using shell: /usr/bin/bash. Provided cores: 64. Rules claiming more threads will be scaled down. Select jobs to execute... > [Wed Jan 4 18:30:51 2023]. > rule deepvariant:. > input: results/recal/s534_EKDN210017195-1A_HTTJ3DSX2_L2.bam, /mnt/shared/scratch/kmarians/private/dyslexia_gatk/workflow/resources/genome.fasta. > output: results/deepvariant/s534_EKDN210017195-1A_HTTJ3DSX2_L2.vcf.gz. > log: logs/deepvariant/s534_EKDN210017195-1A_HTTJ3DSX2_L2/stdout.log. > jobid: 0. > wildcards: sample=s534_EKDN210017195-1A_HTTJ3DSX2_L2. > threads: 64. > resources: mem_mb=163840, disk_mb=16401, tmpdir=/tmp/kmarians_4189323. > . > Activating singularity image singularity/deepvariant_1.4.0.sif. > INFO: Convert SIF file to sandbox... > I0104 18:31:03.183642 139718628308800 run_deepvariant.py:342] Re-using the directory for intermediate results in /tmp/kmarians_4189323/tmpxrz5rqbp. > . > ***** Intermediate results will be written to /tmp/kmarians_4189323/tmpxrz5rqbp in docker. ****. > . > . > ***** Running the command:*****. > time seq 0 63 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/mnt/shared/scratch/kmarians/private/dyslexia_gatk/workflow/resources/genome.fasta"" --reads ""results/recal/s534_EKDN210017195-1A_HTTJ3DSX2_L2.bam"" --examples ""/tmp/kmarians_4189323/tmpxrz5rqbp/make_examples.tfrecord@64.gz"" --channels ""insert_size"" --vsc_min_count_indels ""3"" --vsc_min_count_snps ""3"" --vsc_min_fraction_indels ""0.10"" --vsc_min_fraction_snps ""0.2"" --task {}. > *. > *. > *. > I0104 18:49:24.340415 140179",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/602
https://github.com/google/deepvariant/issues/602:3759,testability,resourc,resources,3759,"_EKDN210017195-1A_HTTJ3DSX2_L2.bam, /mnt/shared/scratch/kmarians/private/dyslexia_gatk/workflow/resources/genome.fasta. > output: results/deepvariant/s534_EKDN210017195-1A_HTTJ3DSX2_L2.vcf.gz. > log: logs/deepvariant/s534_EKDN210017195-1A_HTTJ3DSX2_L2/stdout.log. > jobid: 0. > wildcards: sample=s534_EKDN210017195-1A_HTTJ3DSX2_L2. > threads: 64. > resources: mem_mb=163840, disk_mb=16401, tmpdir=/tmp/kmarians_4189323. > . > Activating singularity image singularity/deepvariant_1.4.0.sif. > INFO: Convert SIF file to sandbox... > I0104 18:31:03.183642 139718628308800 run_deepvariant.py:342] Re-using the directory for intermediate results in /tmp/kmarians_4189323/tmpxrz5rqbp. > . > ***** Intermediate results will be written to /tmp/kmarians_4189323/tmpxrz5rqbp in docker. ****. > . > . > ***** Running the command:*****. > time seq 0 63 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/mnt/shared/scratch/kmarians/private/dyslexia_gatk/workflow/resources/genome.fasta"" --reads ""results/recal/s534_EKDN210017195-1A_HTTJ3DSX2_L2.bam"" --examples ""/tmp/kmarians_4189323/tmpxrz5rqbp/make_examples.tfrecord@64.gz"" --channels ""insert_size"" --vsc_min_count_indels ""3"" --vsc_min_count_snps ""3"" --vsc_min_fraction_indels ""0.10"" --vsc_min_fraction_snps ""0.2"" --task {}. > *. > *. > *. > I0104 18:49:24.340415 140179943589696 make_examples_core.py:243] Task 13/64: Found 2793 candidate variants. > I0104 18:49:24.340478 140179943589696 make_examples_core.py:243] Task 13/64: Created 2879 examples. > real	18m21.465s. > user	893m16.250s. > sys	15m57.561s. > . > ***** Running the command:*****. > time /opt/deepvariant/bin/call_variants --outfile ""/tmp/kmarians_4189323/tmpxrz5rqbp/call_variants_output.tfrecord.gz"" --examples ""/tmp/kmarians_4189323/tmpxrz5rqbp/make_examples.tfrecord@64.gz"" --checkpoint ""/opt/models/wes/model.ckpt"" --openvino_model_dir ""/tmp/kmarians_4189323/tmpxrz5rqbp"". > . > I0104 18:49:26.695084 139646404110144 call_variants.py:317] From ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/602
https://github.com/google/deepvariant/issues/602:5157,testability,Trace,Traceback,5157,"/64: Found 2793 candidate variants. > I0104 18:49:24.340478 140179943589696 make_examples_core.py:243] Task 13/64: Created 2879 examples. > real	18m21.465s. > user	893m16.250s. > sys	15m57.561s. > . > ***** Running the command:*****. > time /opt/deepvariant/bin/call_variants --outfile ""/tmp/kmarians_4189323/tmpxrz5rqbp/call_variants_output.tfrecord.gz"" --examples ""/tmp/kmarians_4189323/tmpxrz5rqbp/make_examples.tfrecord@64.gz"" --checkpoint ""/opt/models/wes/model.ckpt"" --openvino_model_dir ""/tmp/kmarians_4189323/tmpxrz5rqbp"". > . > I0104 18:49:26.695084 139646404110144 call_variants.py:317] From /tmp/kmarians_4189323/tmpxrz5rqbp/make_examples.tfrecord-00000-of-00064.gz.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 19]. > I0104 18:49:26.697470 139646404110144 call_variants.py:317] From /opt/models/wes/model.ckpt.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 19]. > Traceback (most recent call last):. > File ""/tmp/kmarians_4189323/Bazel.runfiles_zims94rl/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 513, in <module>. > tf.compat.v1.app.run(). > File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. > _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). > File ""/tmp/kmarians_4189323/Bazel.runfiles_zims94rl/runfiles/absl_py/absl/app.py"", line 300, in run. > _run_main(main, args). > File ""/tmp/kmarians_4189323/Bazel.runfiles_zims94rl/runfiles/absl_py/absl/app.py"", line 251, in _run_main. > sys.exit(main(argv)). > File ""/tmp/kmarians_4189323/Bazel.runfiles_zims94rl/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 494, in main. > call_variants(. > File ""/tmp/kmarians_4189323/Bazel.runfiles_zims94rl/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 389, in call_variants. > init_op = tf.group(tf.compat.v1.global_variables_initializer(),. > Fi",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/602
https://github.com/google/deepvariant/issues/602:7482,testability,Trace,Traceback,7482,"s). > File ""/tmp/kmarians_4189323/Bazel.runfiles_zims94rl/runfiles/absl_py/absl/app.py"", line 251, in _run_main. > sys.exit(main(argv)). > File ""/tmp/kmarians_4189323/Bazel.runfiles_zims94rl/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 494, in main. > call_variants(. > File ""/tmp/kmarians_4189323/Bazel.runfiles_zims94rl/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 389, in call_variants. > init_op = tf.group(tf.compat.v1.global_variables_initializer(),. > File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/ops/variables.py"", line 3319, in global_variables_initializer. > return variables_initializer(global_variables()). > File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/ops/variables.py"", line 3139, in global_variables. > return ops.get_collection(ops.GraphKeys.GLOBAL_VARIABLES, scope). > File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py"", line 6605, in get_collection. > return get_default_graph().get_collection(key, scope). > File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py"", line 6231, in get_default_graph. > return _default_graph_stack.get_default(). > File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py"", line 5742, in get_default. > self._global_default_graph = Graph(). > File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py"", line 3113, in __init__. > self._scoped_c_graph = c_api_util.ScopedTFGraph(). > File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/c_api_util.py"", line 50, in __init__. > self.graph = c_api.TF_NewGraph(). > RuntimeError: random_device::random_device(const std::string&): device not available. > Exception ignored in: <function ScopedTFGraph.__del__ at 0x7f01d64378b0>. > Traceback (most recent call last):. > File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/c_api_util.py"", line 58, in __del__. > AttributeError: deleter",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/602
https://github.com/google/deepvariant/issues/602:146,usability,interact,interactive,146,"AttributeError: deleter; Hi DeepVariant team,. I'm running DeepVariant via Singularity via Snakemake on a HPC cluster and overall, if I create an interactive session, my entire WES pipeline runs fine until an odd DeepVariant job fails (by that I mean that let's say 30 deepvariant jobs finish ok and the 31st fails). I can then restart the Snakemake run and the same job will run fine, followed by more jobs that will also run fine until another odd jobs fails. This is also particularly true when I use the --cluster command in Snakemake (i.e., send it to a SLURM job scheduler from the master node) - in this event every single job fails. I could of course run all my samples on a single interactive session, keep checking the log file and restart the run every time it fails but I guess that's less than optimal plus this way I can really only run one sample at the time. For the interactive sessions I request 180G and 64cpus (in my case it's: ```srsh --mem=180G --cpus-per-task=64 --partition=long```). . I would request same parameters when using --cluster so:. ```snakemake --cluster ""sbatch --mem=180G cpus-per-task=64"" --jobs 64 --profie profile/ ```(where profile holds singularity args etc.). Singularity image is deepvariant_1.4.0.sif. my Snakemake rule:. ```. rule deepvariant:. input:. bam=rules.apply_bqsr.output.bam,. ref='/mnt/shared/scratch/kmarians/private/dyslexia_gatk/workflow/resources/genome.fasta'. output:. vcf=""results/deepvariant/{sample}.vcf.gz"". params:. model=""WES"". threads: . 64. resources:. mem_mb=163840. log:. ""logs/deepvariant/{sample}/stdout.log"". singularity:. ""singularity/deepvariant_1.4.0.sif"". # ""singularity/deepvariant_1.4.0-gpu.sif"" # for GPU. shell:. """""". /opt/deepvariant/bin/run_deepvariant --model_type {params.model} --ref {input.ref} --reads {input.bam} --output_vcf {output.vcf} --num_shards {threads} --make_examples_extra_args='vsc_min_count_snps=3,vsc_min_fraction_snps=0.2,vsc_min_count_indels=3,vsc_min_fraction_indels=0.10'. """""". ```. Below ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/602
https://github.com/google/deepvariant/issues/602:518,usability,command,command,518,"AttributeError: deleter; Hi DeepVariant team,. I'm running DeepVariant via Singularity via Snakemake on a HPC cluster and overall, if I create an interactive session, my entire WES pipeline runs fine until an odd DeepVariant job fails (by that I mean that let's say 30 deepvariant jobs finish ok and the 31st fails). I can then restart the Snakemake run and the same job will run fine, followed by more jobs that will also run fine until another odd jobs fails. This is also particularly true when I use the --cluster command in Snakemake (i.e., send it to a SLURM job scheduler from the master node) - in this event every single job fails. I could of course run all my samples on a single interactive session, keep checking the log file and restart the run every time it fails but I guess that's less than optimal plus this way I can really only run one sample at the time. For the interactive sessions I request 180G and 64cpus (in my case it's: ```srsh --mem=180G --cpus-per-task=64 --partition=long```). . I would request same parameters when using --cluster so:. ```snakemake --cluster ""sbatch --mem=180G cpus-per-task=64"" --jobs 64 --profie profile/ ```(where profile holds singularity args etc.). Singularity image is deepvariant_1.4.0.sif. my Snakemake rule:. ```. rule deepvariant:. input:. bam=rules.apply_bqsr.output.bam,. ref='/mnt/shared/scratch/kmarians/private/dyslexia_gatk/workflow/resources/genome.fasta'. output:. vcf=""results/deepvariant/{sample}.vcf.gz"". params:. model=""WES"". threads: . 64. resources:. mem_mb=163840. log:. ""logs/deepvariant/{sample}/stdout.log"". singularity:. ""singularity/deepvariant_1.4.0.sif"". # ""singularity/deepvariant_1.4.0-gpu.sif"" # for GPU. shell:. """""". /opt/deepvariant/bin/run_deepvariant --model_type {params.model} --ref {input.ref} --reads {input.bam} --output_vcf {output.vcf} --num_shards {threads} --make_examples_extra_args='vsc_min_count_snps=3,vsc_min_fraction_snps=0.2,vsc_min_count_indels=3,vsc_min_fraction_indels=0.10'. """""". ```. Below ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/602
https://github.com/google/deepvariant/issues/602:690,usability,interact,interactive,690,"AttributeError: deleter; Hi DeepVariant team,. I'm running DeepVariant via Singularity via Snakemake on a HPC cluster and overall, if I create an interactive session, my entire WES pipeline runs fine until an odd DeepVariant job fails (by that I mean that let's say 30 deepvariant jobs finish ok and the 31st fails). I can then restart the Snakemake run and the same job will run fine, followed by more jobs that will also run fine until another odd jobs fails. This is also particularly true when I use the --cluster command in Snakemake (i.e., send it to a SLURM job scheduler from the master node) - in this event every single job fails. I could of course run all my samples on a single interactive session, keep checking the log file and restart the run every time it fails but I guess that's less than optimal plus this way I can really only run one sample at the time. For the interactive sessions I request 180G and 64cpus (in my case it's: ```srsh --mem=180G --cpus-per-task=64 --partition=long```). . I would request same parameters when using --cluster so:. ```snakemake --cluster ""sbatch --mem=180G cpus-per-task=64"" --jobs 64 --profie profile/ ```(where profile holds singularity args etc.). Singularity image is deepvariant_1.4.0.sif. my Snakemake rule:. ```. rule deepvariant:. input:. bam=rules.apply_bqsr.output.bam,. ref='/mnt/shared/scratch/kmarians/private/dyslexia_gatk/workflow/resources/genome.fasta'. output:. vcf=""results/deepvariant/{sample}.vcf.gz"". params:. model=""WES"". threads: . 64. resources:. mem_mb=163840. log:. ""logs/deepvariant/{sample}/stdout.log"". singularity:. ""singularity/deepvariant_1.4.0.sif"". # ""singularity/deepvariant_1.4.0-gpu.sif"" # for GPU. shell:. """""". /opt/deepvariant/bin/run_deepvariant --model_type {params.model} --ref {input.ref} --reads {input.bam} --output_vcf {output.vcf} --num_shards {threads} --make_examples_extra_args='vsc_min_count_snps=3,vsc_min_fraction_snps=0.2,vsc_min_count_indels=3,vsc_min_fraction_indels=0.10'. """""". ```. Below ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/602
https://github.com/google/deepvariant/issues/602:883,usability,interact,interactive,883,"AttributeError: deleter; Hi DeepVariant team,. I'm running DeepVariant via Singularity via Snakemake on a HPC cluster and overall, if I create an interactive session, my entire WES pipeline runs fine until an odd DeepVariant job fails (by that I mean that let's say 30 deepvariant jobs finish ok and the 31st fails). I can then restart the Snakemake run and the same job will run fine, followed by more jobs that will also run fine until another odd jobs fails. This is also particularly true when I use the --cluster command in Snakemake (i.e., send it to a SLURM job scheduler from the master node) - in this event every single job fails. I could of course run all my samples on a single interactive session, keep checking the log file and restart the run every time it fails but I guess that's less than optimal plus this way I can really only run one sample at the time. For the interactive sessions I request 180G and 64cpus (in my case it's: ```srsh --mem=180G --cpus-per-task=64 --partition=long```). . I would request same parameters when using --cluster so:. ```snakemake --cluster ""sbatch --mem=180G cpus-per-task=64"" --jobs 64 --profie profile/ ```(where profile holds singularity args etc.). Singularity image is deepvariant_1.4.0.sif. my Snakemake rule:. ```. rule deepvariant:. input:. bam=rules.apply_bqsr.output.bam,. ref='/mnt/shared/scratch/kmarians/private/dyslexia_gatk/workflow/resources/genome.fasta'. output:. vcf=""results/deepvariant/{sample}.vcf.gz"". params:. model=""WES"". threads: . 64. resources:. mem_mb=163840. log:. ""logs/deepvariant/{sample}/stdout.log"". singularity:. ""singularity/deepvariant_1.4.0.sif"". # ""singularity/deepvariant_1.4.0-gpu.sif"" # for GPU. shell:. """""". /opt/deepvariant/bin/run_deepvariant --model_type {params.model} --ref {input.ref} --reads {input.bam} --output_vcf {output.vcf} --num_shards {threads} --make_examples_extra_args='vsc_min_count_snps=3,vsc_min_fraction_snps=0.2,vsc_min_count_indels=3,vsc_min_fraction_indels=0.10'. """""". ```. Below ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/602
https://github.com/google/deepvariant/issues/602:1292,usability,input,input,1292," and the 31st fails). I can then restart the Snakemake run and the same job will run fine, followed by more jobs that will also run fine until another odd jobs fails. This is also particularly true when I use the --cluster command in Snakemake (i.e., send it to a SLURM job scheduler from the master node) - in this event every single job fails. I could of course run all my samples on a single interactive session, keep checking the log file and restart the run every time it fails but I guess that's less than optimal plus this way I can really only run one sample at the time. For the interactive sessions I request 180G and 64cpus (in my case it's: ```srsh --mem=180G --cpus-per-task=64 --partition=long```). . I would request same parameters when using --cluster so:. ```snakemake --cluster ""sbatch --mem=180G cpus-per-task=64"" --jobs 64 --profie profile/ ```(where profile holds singularity args etc.). Singularity image is deepvariant_1.4.0.sif. my Snakemake rule:. ```. rule deepvariant:. input:. bam=rules.apply_bqsr.output.bam,. ref='/mnt/shared/scratch/kmarians/private/dyslexia_gatk/workflow/resources/genome.fasta'. output:. vcf=""results/deepvariant/{sample}.vcf.gz"". params:. model=""WES"". threads: . 64. resources:. mem_mb=163840. log:. ""logs/deepvariant/{sample}/stdout.log"". singularity:. ""singularity/deepvariant_1.4.0.sif"". # ""singularity/deepvariant_1.4.0-gpu.sif"" # for GPU. shell:. """""". /opt/deepvariant/bin/run_deepvariant --model_type {params.model} --ref {input.ref} --reads {input.bam} --output_vcf {output.vcf} --num_shards {threads} --make_examples_extra_args='vsc_min_count_snps=3,vsc_min_fraction_snps=0.2,vsc_min_count_indels=3,vsc_min_fraction_indels=0.10'. """""". ```. Below is the begening and end of the log file. I am happy to include the entire log file but there is nothing out of the ordinary between those lines below (same output as for jobs that finished successfully). Could you please advise on what parameters to change to successfully run DeepVariant by sub",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/602
https://github.com/google/deepvariant/issues/602:1390,usability,workflow,workflow,1390," by more jobs that will also run fine until another odd jobs fails. This is also particularly true when I use the --cluster command in Snakemake (i.e., send it to a SLURM job scheduler from the master node) - in this event every single job fails. I could of course run all my samples on a single interactive session, keep checking the log file and restart the run every time it fails but I guess that's less than optimal plus this way I can really only run one sample at the time. For the interactive sessions I request 180G and 64cpus (in my case it's: ```srsh --mem=180G --cpus-per-task=64 --partition=long```). . I would request same parameters when using --cluster so:. ```snakemake --cluster ""sbatch --mem=180G cpus-per-task=64"" --jobs 64 --profie profile/ ```(where profile holds singularity args etc.). Singularity image is deepvariant_1.4.0.sif. my Snakemake rule:. ```. rule deepvariant:. input:. bam=rules.apply_bqsr.output.bam,. ref='/mnt/shared/scratch/kmarians/private/dyslexia_gatk/workflow/resources/genome.fasta'. output:. vcf=""results/deepvariant/{sample}.vcf.gz"". params:. model=""WES"". threads: . 64. resources:. mem_mb=163840. log:. ""logs/deepvariant/{sample}/stdout.log"". singularity:. ""singularity/deepvariant_1.4.0.sif"". # ""singularity/deepvariant_1.4.0-gpu.sif"" # for GPU. shell:. """""". /opt/deepvariant/bin/run_deepvariant --model_type {params.model} --ref {input.ref} --reads {input.bam} --output_vcf {output.vcf} --num_shards {threads} --make_examples_extra_args='vsc_min_count_snps=3,vsc_min_fraction_snps=0.2,vsc_min_count_indels=3,vsc_min_fraction_indels=0.10'. """""". ```. Below is the begening and end of the log file. I am happy to include the entire log file but there is nothing out of the ordinary between those lines below (same output as for jobs that finished successfully). Could you please advise on what parameters to change to successfully run DeepVariant by submitting it to the SLURM scheduler? . I0104 18:49:24.340415 140179943589696 make_examples_core.py:24",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/602
https://github.com/google/deepvariant/issues/602:1775,usability,input,input,1775,"but I guess that's less than optimal plus this way I can really only run one sample at the time. For the interactive sessions I request 180G and 64cpus (in my case it's: ```srsh --mem=180G --cpus-per-task=64 --partition=long```). . I would request same parameters when using --cluster so:. ```snakemake --cluster ""sbatch --mem=180G cpus-per-task=64"" --jobs 64 --profie profile/ ```(where profile holds singularity args etc.). Singularity image is deepvariant_1.4.0.sif. my Snakemake rule:. ```. rule deepvariant:. input:. bam=rules.apply_bqsr.output.bam,. ref='/mnt/shared/scratch/kmarians/private/dyslexia_gatk/workflow/resources/genome.fasta'. output:. vcf=""results/deepvariant/{sample}.vcf.gz"". params:. model=""WES"". threads: . 64. resources:. mem_mb=163840. log:. ""logs/deepvariant/{sample}/stdout.log"". singularity:. ""singularity/deepvariant_1.4.0.sif"". # ""singularity/deepvariant_1.4.0-gpu.sif"" # for GPU. shell:. """""". /opt/deepvariant/bin/run_deepvariant --model_type {params.model} --ref {input.ref} --reads {input.bam} --output_vcf {output.vcf} --num_shards {threads} --make_examples_extra_args='vsc_min_count_snps=3,vsc_min_fraction_snps=0.2,vsc_min_count_indels=3,vsc_min_fraction_indels=0.10'. """""". ```. Below is the begening and end of the log file. I am happy to include the entire log file but there is nothing out of the ordinary between those lines below (same output as for jobs that finished successfully). Could you please advise on what parameters to change to successfully run DeepVariant by submitting it to the SLURM scheduler? . I0104 18:49:24.340415 140179943589696 make_examples_core.py:243] Task 13/64: Found 2793 candidate variants. I0104 18:49:24.340478 140179943589696 make_examples_core.py:243] Task 13/64: Created 2879 examples. Building DAG of jobs... Using shell: /usr/bin/bash. Provided cores: 64. Rules claiming more threads will be scaled down. Select jobs to execute... > [Wed Jan 4 18:30:51 2023]. > rule deepvariant:. > input: results/recal/s534_EKDN210017195",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/602
https://github.com/google/deepvariant/issues/602:1795,usability,input,input,1795,"ess than optimal plus this way I can really only run one sample at the time. For the interactive sessions I request 180G and 64cpus (in my case it's: ```srsh --mem=180G --cpus-per-task=64 --partition=long```). . I would request same parameters when using --cluster so:. ```snakemake --cluster ""sbatch --mem=180G cpus-per-task=64"" --jobs 64 --profie profile/ ```(where profile holds singularity args etc.). Singularity image is deepvariant_1.4.0.sif. my Snakemake rule:. ```. rule deepvariant:. input:. bam=rules.apply_bqsr.output.bam,. ref='/mnt/shared/scratch/kmarians/private/dyslexia_gatk/workflow/resources/genome.fasta'. output:. vcf=""results/deepvariant/{sample}.vcf.gz"". params:. model=""WES"". threads: . 64. resources:. mem_mb=163840. log:. ""logs/deepvariant/{sample}/stdout.log"". singularity:. ""singularity/deepvariant_1.4.0.sif"". # ""singularity/deepvariant_1.4.0-gpu.sif"" # for GPU. shell:. """""". /opt/deepvariant/bin/run_deepvariant --model_type {params.model} --ref {input.ref} --reads {input.bam} --output_vcf {output.vcf} --num_shards {threads} --make_examples_extra_args='vsc_min_count_snps=3,vsc_min_fraction_snps=0.2,vsc_min_count_indels=3,vsc_min_fraction_indels=0.10'. """""". ```. Below is the begening and end of the log file. I am happy to include the entire log file but there is nothing out of the ordinary between those lines below (same output as for jobs that finished successfully). Could you please advise on what parameters to change to successfully run DeepVariant by submitting it to the SLURM scheduler? . I0104 18:49:24.340415 140179943589696 make_examples_core.py:243] Task 13/64: Found 2793 candidate variants. I0104 18:49:24.340478 140179943589696 make_examples_core.py:243] Task 13/64: Created 2879 examples. Building DAG of jobs... Using shell: /usr/bin/bash. Provided cores: 64. Rules claiming more threads will be scaled down. Select jobs to execute... > [Wed Jan 4 18:30:51 2023]. > rule deepvariant:. > input: results/recal/s534_EKDN210017195-1A_HTTJ3DSX2_L2.bam",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/602
https://github.com/google/deepvariant/issues/602:2739,usability,input,input,2739,"model_type {params.model} --ref {input.ref} --reads {input.bam} --output_vcf {output.vcf} --num_shards {threads} --make_examples_extra_args='vsc_min_count_snps=3,vsc_min_fraction_snps=0.2,vsc_min_count_indels=3,vsc_min_fraction_indels=0.10'. """""". ```. Below is the begening and end of the log file. I am happy to include the entire log file but there is nothing out of the ordinary between those lines below (same output as for jobs that finished successfully). Could you please advise on what parameters to change to successfully run DeepVariant by submitting it to the SLURM scheduler? . I0104 18:49:24.340415 140179943589696 make_examples_core.py:243] Task 13/64: Found 2793 candidate variants. I0104 18:49:24.340478 140179943589696 make_examples_core.py:243] Task 13/64: Created 2879 examples. Building DAG of jobs... Using shell: /usr/bin/bash. Provided cores: 64. Rules claiming more threads will be scaled down. Select jobs to execute... > [Wed Jan 4 18:30:51 2023]. > rule deepvariant:. > input: results/recal/s534_EKDN210017195-1A_HTTJ3DSX2_L2.bam, /mnt/shared/scratch/kmarians/private/dyslexia_gatk/workflow/resources/genome.fasta. > output: results/deepvariant/s534_EKDN210017195-1A_HTTJ3DSX2_L2.vcf.gz. > log: logs/deepvariant/s534_EKDN210017195-1A_HTTJ3DSX2_L2/stdout.log. > jobid: 0. > wildcards: sample=s534_EKDN210017195-1A_HTTJ3DSX2_L2. > threads: 64. > resources: mem_mb=163840, disk_mb=16401, tmpdir=/tmp/kmarians_4189323. > . > Activating singularity image singularity/deepvariant_1.4.0.sif. > INFO: Convert SIF file to sandbox... > I0104 18:31:03.183642 139718628308800 run_deepvariant.py:342] Re-using the directory for intermediate results in /tmp/kmarians_4189323/tmpxrz5rqbp. > . > ***** Intermediate results will be written to /tmp/kmarians_4189323/tmpxrz5rqbp in docker. ****. > . > . > ***** Running the command:*****. > time seq 0 63 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/mnt/shared/scratch/kmarians/private/dyslex",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/602
https://github.com/google/deepvariant/issues/602:2851,usability,workflow,workflow,2851,"--make_examples_extra_args='vsc_min_count_snps=3,vsc_min_fraction_snps=0.2,vsc_min_count_indels=3,vsc_min_fraction_indels=0.10'. """""". ```. Below is the begening and end of the log file. I am happy to include the entire log file but there is nothing out of the ordinary between those lines below (same output as for jobs that finished successfully). Could you please advise on what parameters to change to successfully run DeepVariant by submitting it to the SLURM scheduler? . I0104 18:49:24.340415 140179943589696 make_examples_core.py:243] Task 13/64: Found 2793 candidate variants. I0104 18:49:24.340478 140179943589696 make_examples_core.py:243] Task 13/64: Created 2879 examples. Building DAG of jobs... Using shell: /usr/bin/bash. Provided cores: 64. Rules claiming more threads will be scaled down. Select jobs to execute... > [Wed Jan 4 18:30:51 2023]. > rule deepvariant:. > input: results/recal/s534_EKDN210017195-1A_HTTJ3DSX2_L2.bam, /mnt/shared/scratch/kmarians/private/dyslexia_gatk/workflow/resources/genome.fasta. > output: results/deepvariant/s534_EKDN210017195-1A_HTTJ3DSX2_L2.vcf.gz. > log: logs/deepvariant/s534_EKDN210017195-1A_HTTJ3DSX2_L2/stdout.log. > jobid: 0. > wildcards: sample=s534_EKDN210017195-1A_HTTJ3DSX2_L2. > threads: 64. > resources: mem_mb=163840, disk_mb=16401, tmpdir=/tmp/kmarians_4189323. > . > Activating singularity image singularity/deepvariant_1.4.0.sif. > INFO: Convert SIF file to sandbox... > I0104 18:31:03.183642 139718628308800 run_deepvariant.py:342] Re-using the directory for intermediate results in /tmp/kmarians_4189323/tmpxrz5rqbp. > . > ***** Intermediate results will be written to /tmp/kmarians_4189323/tmpxrz5rqbp in docker. ****. > . > . > ***** Running the command:*****. > time seq 0 63 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/mnt/shared/scratch/kmarians/private/dyslexia_gatk/workflow/resources/genome.fasta"" --reads ""results/recal/s534_EKDN210017195-1A_HTTJ3DSX2_L2.bam"" --example",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/602
https://github.com/google/deepvariant/issues/602:3574,usability,command,command,3574,"usr/bin/bash. Provided cores: 64. Rules claiming more threads will be scaled down. Select jobs to execute... > [Wed Jan 4 18:30:51 2023]. > rule deepvariant:. > input: results/recal/s534_EKDN210017195-1A_HTTJ3DSX2_L2.bam, /mnt/shared/scratch/kmarians/private/dyslexia_gatk/workflow/resources/genome.fasta. > output: results/deepvariant/s534_EKDN210017195-1A_HTTJ3DSX2_L2.vcf.gz. > log: logs/deepvariant/s534_EKDN210017195-1A_HTTJ3DSX2_L2/stdout.log. > jobid: 0. > wildcards: sample=s534_EKDN210017195-1A_HTTJ3DSX2_L2. > threads: 64. > resources: mem_mb=163840, disk_mb=16401, tmpdir=/tmp/kmarians_4189323. > . > Activating singularity image singularity/deepvariant_1.4.0.sif. > INFO: Convert SIF file to sandbox... > I0104 18:31:03.183642 139718628308800 run_deepvariant.py:342] Re-using the directory for intermediate results in /tmp/kmarians_4189323/tmpxrz5rqbp. > . > ***** Intermediate results will be written to /tmp/kmarians_4189323/tmpxrz5rqbp in docker. ****. > . > . > ***** Running the command:*****. > time seq 0 63 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/mnt/shared/scratch/kmarians/private/dyslexia_gatk/workflow/resources/genome.fasta"" --reads ""results/recal/s534_EKDN210017195-1A_HTTJ3DSX2_L2.bam"" --examples ""/tmp/kmarians_4189323/tmpxrz5rqbp/make_examples.tfrecord@64.gz"" --channels ""insert_size"" --vsc_min_count_indels ""3"" --vsc_min_count_snps ""3"" --vsc_min_fraction_indels ""0.10"" --vsc_min_fraction_snps ""0.2"" --task {}. > *. > *. > *. > I0104 18:49:24.340415 140179943589696 make_examples_core.py:243] Task 13/64: Found 2793 candidate variants. > I0104 18:49:24.340478 140179943589696 make_examples_core.py:243] Task 13/64: Created 2879 examples. > real	18m21.465s. > user	893m16.250s. > sys	15m57.561s. > . > ***** Running the command:*****. > time /opt/deepvariant/bin/call_variants --outfile ""/tmp/kmarians_4189323/tmpxrz5rqbp/call_variants_output.tfrecord.gz"" --examples ""/tmp/kmarians_4189323/tmpxrz5rqbp/make_examples.t",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/602
https://github.com/google/deepvariant/issues/602:3750,usability,workflow,workflow,3750,"recal/s534_EKDN210017195-1A_HTTJ3DSX2_L2.bam, /mnt/shared/scratch/kmarians/private/dyslexia_gatk/workflow/resources/genome.fasta. > output: results/deepvariant/s534_EKDN210017195-1A_HTTJ3DSX2_L2.vcf.gz. > log: logs/deepvariant/s534_EKDN210017195-1A_HTTJ3DSX2_L2/stdout.log. > jobid: 0. > wildcards: sample=s534_EKDN210017195-1A_HTTJ3DSX2_L2. > threads: 64. > resources: mem_mb=163840, disk_mb=16401, tmpdir=/tmp/kmarians_4189323. > . > Activating singularity image singularity/deepvariant_1.4.0.sif. > INFO: Convert SIF file to sandbox... > I0104 18:31:03.183642 139718628308800 run_deepvariant.py:342] Re-using the directory for intermediate results in /tmp/kmarians_4189323/tmpxrz5rqbp. > . > ***** Intermediate results will be written to /tmp/kmarians_4189323/tmpxrz5rqbp in docker. ****. > . > . > ***** Running the command:*****. > time seq 0 63 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/mnt/shared/scratch/kmarians/private/dyslexia_gatk/workflow/resources/genome.fasta"" --reads ""results/recal/s534_EKDN210017195-1A_HTTJ3DSX2_L2.bam"" --examples ""/tmp/kmarians_4189323/tmpxrz5rqbp/make_examples.tfrecord@64.gz"" --channels ""insert_size"" --vsc_min_count_indels ""3"" --vsc_min_count_snps ""3"" --vsc_min_fraction_indels ""0.10"" --vsc_min_fraction_snps ""0.2"" --task {}. > *. > *. > *. > I0104 18:49:24.340415 140179943589696 make_examples_core.py:243] Task 13/64: Found 2793 candidate variants. > I0104 18:49:24.340478 140179943589696 make_examples_core.py:243] Task 13/64: Created 2879 examples. > real	18m21.465s. > user	893m16.250s. > sys	15m57.561s. > . > ***** Running the command:*****. > time /opt/deepvariant/bin/call_variants --outfile ""/tmp/kmarians_4189323/tmpxrz5rqbp/call_variants_output.tfrecord.gz"" --examples ""/tmp/kmarians_4189323/tmpxrz5rqbp/make_examples.tfrecord@64.gz"" --checkpoint ""/opt/models/wes/model.ckpt"" --openvino_model_dir ""/tmp/kmarians_4189323/tmpxrz5rqbp"". > . > I0104 18:49:26.695084 139646404110144 call_variants.py:",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/602
https://github.com/google/deepvariant/issues/602:4321,usability,user,user,4321,"628308800 run_deepvariant.py:342] Re-using the directory for intermediate results in /tmp/kmarians_4189323/tmpxrz5rqbp. > . > ***** Intermediate results will be written to /tmp/kmarians_4189323/tmpxrz5rqbp in docker. ****. > . > . > ***** Running the command:*****. > time seq 0 63 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/mnt/shared/scratch/kmarians/private/dyslexia_gatk/workflow/resources/genome.fasta"" --reads ""results/recal/s534_EKDN210017195-1A_HTTJ3DSX2_L2.bam"" --examples ""/tmp/kmarians_4189323/tmpxrz5rqbp/make_examples.tfrecord@64.gz"" --channels ""insert_size"" --vsc_min_count_indels ""3"" --vsc_min_count_snps ""3"" --vsc_min_fraction_indels ""0.10"" --vsc_min_fraction_snps ""0.2"" --task {}. > *. > *. > *. > I0104 18:49:24.340415 140179943589696 make_examples_core.py:243] Task 13/64: Found 2793 candidate variants. > I0104 18:49:24.340478 140179943589696 make_examples_core.py:243] Task 13/64: Created 2879 examples. > real	18m21.465s. > user	893m16.250s. > sys	15m57.561s. > . > ***** Running the command:*****. > time /opt/deepvariant/bin/call_variants --outfile ""/tmp/kmarians_4189323/tmpxrz5rqbp/call_variants_output.tfrecord.gz"" --examples ""/tmp/kmarians_4189323/tmpxrz5rqbp/make_examples.tfrecord@64.gz"" --checkpoint ""/opt/models/wes/model.ckpt"" --openvino_model_dir ""/tmp/kmarians_4189323/tmpxrz5rqbp"". > . > I0104 18:49:26.695084 139646404110144 call_variants.py:317] From /tmp/kmarians_4189323/tmpxrz5rqbp/make_examples.tfrecord-00000-of-00064.gz.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 19]. > I0104 18:49:26.697470 139646404110144 call_variants.py:317] From /opt/models/wes/model.ckpt.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 19]. > Traceback (most recent call last):. > File ""/tmp/kmarians_4189323/Bazel.runfiles_zims94rl/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 513, in ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/602
https://github.com/google/deepvariant/issues/602:4381,usability,command,command,4381,"ntermediate results in /tmp/kmarians_4189323/tmpxrz5rqbp. > . > ***** Intermediate results will be written to /tmp/kmarians_4189323/tmpxrz5rqbp in docker. ****. > . > . > ***** Running the command:*****. > time seq 0 63 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/mnt/shared/scratch/kmarians/private/dyslexia_gatk/workflow/resources/genome.fasta"" --reads ""results/recal/s534_EKDN210017195-1A_HTTJ3DSX2_L2.bam"" --examples ""/tmp/kmarians_4189323/tmpxrz5rqbp/make_examples.tfrecord@64.gz"" --channels ""insert_size"" --vsc_min_count_indels ""3"" --vsc_min_count_snps ""3"" --vsc_min_fraction_indels ""0.10"" --vsc_min_fraction_snps ""0.2"" --task {}. > *. > *. > *. > I0104 18:49:24.340415 140179943589696 make_examples_core.py:243] Task 13/64: Found 2793 candidate variants. > I0104 18:49:24.340478 140179943589696 make_examples_core.py:243] Task 13/64: Created 2879 examples. > real	18m21.465s. > user	893m16.250s. > sys	15m57.561s. > . > ***** Running the command:*****. > time /opt/deepvariant/bin/call_variants --outfile ""/tmp/kmarians_4189323/tmpxrz5rqbp/call_variants_output.tfrecord.gz"" --examples ""/tmp/kmarians_4189323/tmpxrz5rqbp/make_examples.tfrecord@64.gz"" --checkpoint ""/opt/models/wes/model.ckpt"" --openvino_model_dir ""/tmp/kmarians_4189323/tmpxrz5rqbp"". > . > I0104 18:49:26.695084 139646404110144 call_variants.py:317] From /tmp/kmarians_4189323/tmpxrz5rqbp/make_examples.tfrecord-00000-of-00064.gz.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 19]. > I0104 18:49:26.697470 139646404110144 call_variants.py:317] From /opt/models/wes/model.ckpt.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 19]. > Traceback (most recent call last):. > File ""/tmp/kmarians_4189323/Bazel.runfiles_zims94rl/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 513, in <module>. > tf.compat.v1.app.run(). > File ""/usr/local/lib/pyt",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/602
https://github.com/google/deepvariant/issues/602:4867,usability,input,input,4867,"s_4189323/tmpxrz5rqbp/make_examples.tfrecord@64.gz"" --channels ""insert_size"" --vsc_min_count_indels ""3"" --vsc_min_count_snps ""3"" --vsc_min_fraction_indels ""0.10"" --vsc_min_fraction_snps ""0.2"" --task {}. > *. > *. > *. > I0104 18:49:24.340415 140179943589696 make_examples_core.py:243] Task 13/64: Found 2793 candidate variants. > I0104 18:49:24.340478 140179943589696 make_examples_core.py:243] Task 13/64: Created 2879 examples. > real	18m21.465s. > user	893m16.250s. > sys	15m57.561s. > . > ***** Running the command:*****. > time /opt/deepvariant/bin/call_variants --outfile ""/tmp/kmarians_4189323/tmpxrz5rqbp/call_variants_output.tfrecord.gz"" --examples ""/tmp/kmarians_4189323/tmpxrz5rqbp/make_examples.tfrecord@64.gz"" --checkpoint ""/opt/models/wes/model.ckpt"" --openvino_model_dir ""/tmp/kmarians_4189323/tmpxrz5rqbp"". > . > I0104 18:49:26.695084 139646404110144 call_variants.py:317] From /tmp/kmarians_4189323/tmpxrz5rqbp/make_examples.tfrecord-00000-of-00064.gz.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 19]. > I0104 18:49:26.697470 139646404110144 call_variants.py:317] From /opt/models/wes/model.ckpt.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 19]. > Traceback (most recent call last):. > File ""/tmp/kmarians_4189323/Bazel.runfiles_zims94rl/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 513, in <module>. > tf.compat.v1.app.run(). > File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. > _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). > File ""/tmp/kmarians_4189323/Bazel.runfiles_zims94rl/runfiles/absl_py/absl/app.py"", line 300, in run. > _run_main(main, args). > File ""/tmp/kmarians_4189323/Bazel.runfiles_zims94rl/runfiles/absl_py/absl/app.py"", line 251, in _run_main. > sys.exit(main(argv)). > File ""/tmp/kmarians_4189323/Bazel.runfiles_zims94rl/runfiles/com_google_deep",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/602
https://github.com/google/deepvariant/issues/602:4910,usability,input,input,4910,"d@64.gz"" --channels ""insert_size"" --vsc_min_count_indels ""3"" --vsc_min_count_snps ""3"" --vsc_min_fraction_indels ""0.10"" --vsc_min_fraction_snps ""0.2"" --task {}. > *. > *. > *. > I0104 18:49:24.340415 140179943589696 make_examples_core.py:243] Task 13/64: Found 2793 candidate variants. > I0104 18:49:24.340478 140179943589696 make_examples_core.py:243] Task 13/64: Created 2879 examples. > real	18m21.465s. > user	893m16.250s. > sys	15m57.561s. > . > ***** Running the command:*****. > time /opt/deepvariant/bin/call_variants --outfile ""/tmp/kmarians_4189323/tmpxrz5rqbp/call_variants_output.tfrecord.gz"" --examples ""/tmp/kmarians_4189323/tmpxrz5rqbp/make_examples.tfrecord@64.gz"" --checkpoint ""/opt/models/wes/model.ckpt"" --openvino_model_dir ""/tmp/kmarians_4189323/tmpxrz5rqbp"". > . > I0104 18:49:26.695084 139646404110144 call_variants.py:317] From /tmp/kmarians_4189323/tmpxrz5rqbp/make_examples.tfrecord-00000-of-00064.gz.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 19]. > I0104 18:49:26.697470 139646404110144 call_variants.py:317] From /opt/models/wes/model.ckpt.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 19]. > Traceback (most recent call last):. > File ""/tmp/kmarians_4189323/Bazel.runfiles_zims94rl/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 513, in <module>. > tf.compat.v1.app.run(). > File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. > _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). > File ""/tmp/kmarians_4189323/Bazel.runfiles_zims94rl/runfiles/absl_py/absl/app.py"", line 300, in run. > _run_main(main, args). > File ""/tmp/kmarians_4189323/Bazel.runfiles_zims94rl/runfiles/absl_py/absl/app.py"", line 251, in _run_main. > sys.exit(main(argv)). > File ""/tmp/kmarians_4189323/Bazel.runfiles_zims94rl/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/602
https://github.com/google/deepvariant/issues/602:5072,usability,input,input,5072,"*. > *. > *. > I0104 18:49:24.340415 140179943589696 make_examples_core.py:243] Task 13/64: Found 2793 candidate variants. > I0104 18:49:24.340478 140179943589696 make_examples_core.py:243] Task 13/64: Created 2879 examples. > real	18m21.465s. > user	893m16.250s. > sys	15m57.561s. > . > ***** Running the command:*****. > time /opt/deepvariant/bin/call_variants --outfile ""/tmp/kmarians_4189323/tmpxrz5rqbp/call_variants_output.tfrecord.gz"" --examples ""/tmp/kmarians_4189323/tmpxrz5rqbp/make_examples.tfrecord@64.gz"" --checkpoint ""/opt/models/wes/model.ckpt"" --openvino_model_dir ""/tmp/kmarians_4189323/tmpxrz5rqbp"". > . > I0104 18:49:26.695084 139646404110144 call_variants.py:317] From /tmp/kmarians_4189323/tmpxrz5rqbp/make_examples.tfrecord-00000-of-00064.gz.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 19]. > I0104 18:49:26.697470 139646404110144 call_variants.py:317] From /opt/models/wes/model.ckpt.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 19]. > Traceback (most recent call last):. > File ""/tmp/kmarians_4189323/Bazel.runfiles_zims94rl/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 513, in <module>. > tf.compat.v1.app.run(). > File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. > _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). > File ""/tmp/kmarians_4189323/Bazel.runfiles_zims94rl/runfiles/absl_py/absl/app.py"", line 300, in run. > _run_main(main, args). > File ""/tmp/kmarians_4189323/Bazel.runfiles_zims94rl/runfiles/absl_py/absl/app.py"", line 251, in _run_main. > sys.exit(main(argv)). > File ""/tmp/kmarians_4189323/Bazel.runfiles_zims94rl/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 494, in main. > call_variants(. > File ""/tmp/kmarians_4189323/Bazel.runfiles_zims94rl/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 389, in",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/602
https://github.com/google/deepvariant/issues/602:5115,usability,input,input,5115,"943589696 make_examples_core.py:243] Task 13/64: Found 2793 candidate variants. > I0104 18:49:24.340478 140179943589696 make_examples_core.py:243] Task 13/64: Created 2879 examples. > real	18m21.465s. > user	893m16.250s. > sys	15m57.561s. > . > ***** Running the command:*****. > time /opt/deepvariant/bin/call_variants --outfile ""/tmp/kmarians_4189323/tmpxrz5rqbp/call_variants_output.tfrecord.gz"" --examples ""/tmp/kmarians_4189323/tmpxrz5rqbp/make_examples.tfrecord@64.gz"" --checkpoint ""/opt/models/wes/model.ckpt"" --openvino_model_dir ""/tmp/kmarians_4189323/tmpxrz5rqbp"". > . > I0104 18:49:26.695084 139646404110144 call_variants.py:317] From /tmp/kmarians_4189323/tmpxrz5rqbp/make_examples.tfrecord-00000-of-00064.gz.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 19]. > I0104 18:49:26.697470 139646404110144 call_variants.py:317] From /opt/models/wes/model.ckpt.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 19]. > Traceback (most recent call last):. > File ""/tmp/kmarians_4189323/Bazel.runfiles_zims94rl/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 513, in <module>. > tf.compat.v1.app.run(). > File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run. > _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). > File ""/tmp/kmarians_4189323/Bazel.runfiles_zims94rl/runfiles/absl_py/absl/app.py"", line 300, in run. > _run_main(main, args). > File ""/tmp/kmarians_4189323/Bazel.runfiles_zims94rl/runfiles/absl_py/absl/app.py"", line 251, in _run_main. > sys.exit(main(argv)). > File ""/tmp/kmarians_4189323/Bazel.runfiles_zims94rl/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 494, in main. > call_variants(. > File ""/tmp/kmarians_4189323/Bazel.runfiles_zims94rl/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 389, in call_variants. > init_op = tf.group(tf.com",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/602
https://github.com/google/deepvariant/issues/603:399,availability,Operat,Operating,399,"sample name ""default"" used in output VCF for one chromosome; Hi,. I have encountered a somewhat unexpected behavior related to the sample name written to the output VCF (possibly caused by the presence of chrEBV?!). **Describe the issue:**. Sample name ""default"" is used in the output VCF for ""chrEBV"", all other chromosome VCFs contain correct sample name (chr1-chr22,chrX,chrY,chrM). **Setup**. - Operating system: CentOS. - DeepVariant version: v1.2. - Installation method (Docker, built from source, etc.): Singularity v3.5.2. - Type of data: PacBio HiFi, Sequel-II. **Steps to reproduce:**. - Command:. ```. /opt/deepvariant/bin/run_deepvariant \. --model_type=""PACBIO"" \. --regions ""$CHROM"". [ ... otherwise default options ...]. ```. - note: the input BAM alignment contains the sample name in the header (i.e. `SM:HG002`). - Error trace: (n/a - run finishes). **Does the quick start test work on your system?**. can't be used to reproduce the problem. **Any additional context:**. In the log for each chromosome run, I see that all chromosomes except chrEBV are listed in lines like this. ```. I0112 10:31:04.917984 47443049531200 \. make_examples_core.py:236] \. Task 6/12: Common contigs are [ HERE: list of all chromosomes except for chrEBV]. ```. Best,. Peter.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/603
https://github.com/google/deepvariant/issues/603:833,availability,Error,Error,833,"sample name ""default"" used in output VCF for one chromosome; Hi,. I have encountered a somewhat unexpected behavior related to the sample name written to the output VCF (possibly caused by the presence of chrEBV?!). **Describe the issue:**. Sample name ""default"" is used in the output VCF for ""chrEBV"", all other chromosome VCFs contain correct sample name (chr1-chr22,chrX,chrY,chrM). **Setup**. - Operating system: CentOS. - DeepVariant version: v1.2. - Installation method (Docker, built from source, etc.): Singularity v3.5.2. - Type of data: PacBio HiFi, Sequel-II. **Steps to reproduce:**. - Command:. ```. /opt/deepvariant/bin/run_deepvariant \. --model_type=""PACBIO"" \. --regions ""$CHROM"". [ ... otherwise default options ...]. ```. - note: the input BAM alignment contains the sample name in the header (i.e. `SM:HG002`). - Error trace: (n/a - run finishes). **Does the quick start test work on your system?**. can't be used to reproduce the problem. **Any additional context:**. In the log for each chromosome run, I see that all chromosomes except chrEBV are listed in lines like this. ```. I0112 10:31:04.917984 47443049531200 \. make_examples_core.py:236] \. Task 6/12: Common contigs are [ HERE: list of all chromosomes except for chrEBV]. ```. Best,. Peter.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/603
https://github.com/google/deepvariant/issues/603:329,deployability,contain,contain,329,"sample name ""default"" used in output VCF for one chromosome; Hi,. I have encountered a somewhat unexpected behavior related to the sample name written to the output VCF (possibly caused by the presence of chrEBV?!). **Describe the issue:**. Sample name ""default"" is used in the output VCF for ""chrEBV"", all other chromosome VCFs contain correct sample name (chr1-chr22,chrX,chrY,chrM). **Setup**. - Operating system: CentOS. - DeepVariant version: v1.2. - Installation method (Docker, built from source, etc.): Singularity v3.5.2. - Type of data: PacBio HiFi, Sequel-II. **Steps to reproduce:**. - Command:. ```. /opt/deepvariant/bin/run_deepvariant \. --model_type=""PACBIO"" \. --regions ""$CHROM"". [ ... otherwise default options ...]. ```. - note: the input BAM alignment contains the sample name in the header (i.e. `SM:HG002`). - Error trace: (n/a - run finishes). **Does the quick start test work on your system?**. can't be used to reproduce the problem. **Any additional context:**. In the log for each chromosome run, I see that all chromosomes except chrEBV are listed in lines like this. ```. I0112 10:31:04.917984 47443049531200 \. make_examples_core.py:236] \. Task 6/12: Common contigs are [ HERE: list of all chromosomes except for chrEBV]. ```. Best,. Peter.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/603
https://github.com/google/deepvariant/issues/603:439,deployability,version,version,439,"sample name ""default"" used in output VCF for one chromosome; Hi,. I have encountered a somewhat unexpected behavior related to the sample name written to the output VCF (possibly caused by the presence of chrEBV?!). **Describe the issue:**. Sample name ""default"" is used in the output VCF for ""chrEBV"", all other chromosome VCFs contain correct sample name (chr1-chr22,chrX,chrY,chrM). **Setup**. - Operating system: CentOS. - DeepVariant version: v1.2. - Installation method (Docker, built from source, etc.): Singularity v3.5.2. - Type of data: PacBio HiFi, Sequel-II. **Steps to reproduce:**. - Command:. ```. /opt/deepvariant/bin/run_deepvariant \. --model_type=""PACBIO"" \. --regions ""$CHROM"". [ ... otherwise default options ...]. ```. - note: the input BAM alignment contains the sample name in the header (i.e. `SM:HG002`). - Error trace: (n/a - run finishes). **Does the quick start test work on your system?**. can't be used to reproduce the problem. **Any additional context:**. In the log for each chromosome run, I see that all chromosomes except chrEBV are listed in lines like this. ```. I0112 10:31:04.917984 47443049531200 \. make_examples_core.py:236] \. Task 6/12: Common contigs are [ HERE: list of all chromosomes except for chrEBV]. ```. Best,. Peter.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/603
https://github.com/google/deepvariant/issues/603:456,deployability,Instal,Installation,456,"sample name ""default"" used in output VCF for one chromosome; Hi,. I have encountered a somewhat unexpected behavior related to the sample name written to the output VCF (possibly caused by the presence of chrEBV?!). **Describe the issue:**. Sample name ""default"" is used in the output VCF for ""chrEBV"", all other chromosome VCFs contain correct sample name (chr1-chr22,chrX,chrY,chrM). **Setup**. - Operating system: CentOS. - DeepVariant version: v1.2. - Installation method (Docker, built from source, etc.): Singularity v3.5.2. - Type of data: PacBio HiFi, Sequel-II. **Steps to reproduce:**. - Command:. ```. /opt/deepvariant/bin/run_deepvariant \. --model_type=""PACBIO"" \. --regions ""$CHROM"". [ ... otherwise default options ...]. ```. - note: the input BAM alignment contains the sample name in the header (i.e. `SM:HG002`). - Error trace: (n/a - run finishes). **Does the quick start test work on your system?**. can't be used to reproduce the problem. **Any additional context:**. In the log for each chromosome run, I see that all chromosomes except chrEBV are listed in lines like this. ```. I0112 10:31:04.917984 47443049531200 \. make_examples_core.py:236] \. Task 6/12: Common contigs are [ HERE: list of all chromosomes except for chrEBV]. ```. Best,. Peter.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/603
https://github.com/google/deepvariant/issues/603:773,deployability,contain,contains,773,"sample name ""default"" used in output VCF for one chromosome; Hi,. I have encountered a somewhat unexpected behavior related to the sample name written to the output VCF (possibly caused by the presence of chrEBV?!). **Describe the issue:**. Sample name ""default"" is used in the output VCF for ""chrEBV"", all other chromosome VCFs contain correct sample name (chr1-chr22,chrX,chrY,chrM). **Setup**. - Operating system: CentOS. - DeepVariant version: v1.2. - Installation method (Docker, built from source, etc.): Singularity v3.5.2. - Type of data: PacBio HiFi, Sequel-II. **Steps to reproduce:**. - Command:. ```. /opt/deepvariant/bin/run_deepvariant \. --model_type=""PACBIO"" \. --regions ""$CHROM"". [ ... otherwise default options ...]. ```. - note: the input BAM alignment contains the sample name in the header (i.e. `SM:HG002`). - Error trace: (n/a - run finishes). **Does the quick start test work on your system?**. can't be used to reproduce the problem. **Any additional context:**. In the log for each chromosome run, I see that all chromosomes except chrEBV are listed in lines like this. ```. I0112 10:31:04.917984 47443049531200 \. make_examples_core.py:236] \. Task 6/12: Common contigs are [ HERE: list of all chromosomes except for chrEBV]. ```. Best,. Peter.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/603
https://github.com/google/deepvariant/issues/603:996,deployability,log,log,996,"sample name ""default"" used in output VCF for one chromosome; Hi,. I have encountered a somewhat unexpected behavior related to the sample name written to the output VCF (possibly caused by the presence of chrEBV?!). **Describe the issue:**. Sample name ""default"" is used in the output VCF for ""chrEBV"", all other chromosome VCFs contain correct sample name (chr1-chr22,chrX,chrY,chrM). **Setup**. - Operating system: CentOS. - DeepVariant version: v1.2. - Installation method (Docker, built from source, etc.): Singularity v3.5.2. - Type of data: PacBio HiFi, Sequel-II. **Steps to reproduce:**. - Command:. ```. /opt/deepvariant/bin/run_deepvariant \. --model_type=""PACBIO"" \. --regions ""$CHROM"". [ ... otherwise default options ...]. ```. - note: the input BAM alignment contains the sample name in the header (i.e. `SM:HG002`). - Error trace: (n/a - run finishes). **Does the quick start test work on your system?**. can't be used to reproduce the problem. **Any additional context:**. In the log for each chromosome run, I see that all chromosomes except chrEBV are listed in lines like this. ```. I0112 10:31:04.917984 47443049531200 \. make_examples_core.py:236] \. Task 6/12: Common contigs are [ HERE: list of all chromosomes except for chrEBV]. ```. Best,. Peter.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/603
https://github.com/google/deepvariant/issues/603:439,integrability,version,version,439,"sample name ""default"" used in output VCF for one chromosome; Hi,. I have encountered a somewhat unexpected behavior related to the sample name written to the output VCF (possibly caused by the presence of chrEBV?!). **Describe the issue:**. Sample name ""default"" is used in the output VCF for ""chrEBV"", all other chromosome VCFs contain correct sample name (chr1-chr22,chrX,chrY,chrM). **Setup**. - Operating system: CentOS. - DeepVariant version: v1.2. - Installation method (Docker, built from source, etc.): Singularity v3.5.2. - Type of data: PacBio HiFi, Sequel-II. **Steps to reproduce:**. - Command:. ```. /opt/deepvariant/bin/run_deepvariant \. --model_type=""PACBIO"" \. --regions ""$CHROM"". [ ... otherwise default options ...]. ```. - note: the input BAM alignment contains the sample name in the header (i.e. `SM:HG002`). - Error trace: (n/a - run finishes). **Does the quick start test work on your system?**. can't be used to reproduce the problem. **Any additional context:**. In the log for each chromosome run, I see that all chromosomes except chrEBV are listed in lines like this. ```. I0112 10:31:04.917984 47443049531200 \. make_examples_core.py:236] \. Task 6/12: Common contigs are [ HERE: list of all chromosomes except for chrEBV]. ```. Best,. Peter.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/603
https://github.com/google/deepvariant/issues/603:439,modifiability,version,version,439,"sample name ""default"" used in output VCF for one chromosome; Hi,. I have encountered a somewhat unexpected behavior related to the sample name written to the output VCF (possibly caused by the presence of chrEBV?!). **Describe the issue:**. Sample name ""default"" is used in the output VCF for ""chrEBV"", all other chromosome VCFs contain correct sample name (chr1-chr22,chrX,chrY,chrM). **Setup**. - Operating system: CentOS. - DeepVariant version: v1.2. - Installation method (Docker, built from source, etc.): Singularity v3.5.2. - Type of data: PacBio HiFi, Sequel-II. **Steps to reproduce:**. - Command:. ```. /opt/deepvariant/bin/run_deepvariant \. --model_type=""PACBIO"" \. --regions ""$CHROM"". [ ... otherwise default options ...]. ```. - note: the input BAM alignment contains the sample name in the header (i.e. `SM:HG002`). - Error trace: (n/a - run finishes). **Does the quick start test work on your system?**. can't be used to reproduce the problem. **Any additional context:**. In the log for each chromosome run, I see that all chromosomes except chrEBV are listed in lines like this. ```. I0112 10:31:04.917984 47443049531200 \. make_examples_core.py:236] \. Task 6/12: Common contigs are [ HERE: list of all chromosomes except for chrEBV]. ```. Best,. Peter.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/603
https://github.com/google/deepvariant/issues/603:547,modifiability,Pac,PacBio,547,"sample name ""default"" used in output VCF for one chromosome; Hi,. I have encountered a somewhat unexpected behavior related to the sample name written to the output VCF (possibly caused by the presence of chrEBV?!). **Describe the issue:**. Sample name ""default"" is used in the output VCF for ""chrEBV"", all other chromosome VCFs contain correct sample name (chr1-chr22,chrX,chrY,chrM). **Setup**. - Operating system: CentOS. - DeepVariant version: v1.2. - Installation method (Docker, built from source, etc.): Singularity v3.5.2. - Type of data: PacBio HiFi, Sequel-II. **Steps to reproduce:**. - Command:. ```. /opt/deepvariant/bin/run_deepvariant \. --model_type=""PACBIO"" \. --regions ""$CHROM"". [ ... otherwise default options ...]. ```. - note: the input BAM alignment contains the sample name in the header (i.e. `SM:HG002`). - Error trace: (n/a - run finishes). **Does the quick start test work on your system?**. can't be used to reproduce the problem. **Any additional context:**. In the log for each chromosome run, I see that all chromosomes except chrEBV are listed in lines like this. ```. I0112 10:31:04.917984 47443049531200 \. make_examples_core.py:236] \. Task 6/12: Common contigs are [ HERE: list of all chromosomes except for chrEBV]. ```. Best,. Peter.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/603
https://github.com/google/deepvariant/issues/603:667,modifiability,PAC,PACBIO,667,"sample name ""default"" used in output VCF for one chromosome; Hi,. I have encountered a somewhat unexpected behavior related to the sample name written to the output VCF (possibly caused by the presence of chrEBV?!). **Describe the issue:**. Sample name ""default"" is used in the output VCF for ""chrEBV"", all other chromosome VCFs contain correct sample name (chr1-chr22,chrX,chrY,chrM). **Setup**. - Operating system: CentOS. - DeepVariant version: v1.2. - Installation method (Docker, built from source, etc.): Singularity v3.5.2. - Type of data: PacBio HiFi, Sequel-II. **Steps to reproduce:**. - Command:. ```. /opt/deepvariant/bin/run_deepvariant \. --model_type=""PACBIO"" \. --regions ""$CHROM"". [ ... otherwise default options ...]. ```. - note: the input BAM alignment contains the sample name in the header (i.e. `SM:HG002`). - Error trace: (n/a - run finishes). **Does the quick start test work on your system?**. can't be used to reproduce the problem. **Any additional context:**. In the log for each chromosome run, I see that all chromosomes except chrEBV are listed in lines like this. ```. I0112 10:31:04.917984 47443049531200 \. make_examples_core.py:236] \. Task 6/12: Common contigs are [ HERE: list of all chromosomes except for chrEBV]. ```. Best,. Peter.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/603
https://github.com/google/deepvariant/issues/603:833,performance,Error,Error,833,"sample name ""default"" used in output VCF for one chromosome; Hi,. I have encountered a somewhat unexpected behavior related to the sample name written to the output VCF (possibly caused by the presence of chrEBV?!). **Describe the issue:**. Sample name ""default"" is used in the output VCF for ""chrEBV"", all other chromosome VCFs contain correct sample name (chr1-chr22,chrX,chrY,chrM). **Setup**. - Operating system: CentOS. - DeepVariant version: v1.2. - Installation method (Docker, built from source, etc.): Singularity v3.5.2. - Type of data: PacBio HiFi, Sequel-II. **Steps to reproduce:**. - Command:. ```. /opt/deepvariant/bin/run_deepvariant \. --model_type=""PACBIO"" \. --regions ""$CHROM"". [ ... otherwise default options ...]. ```. - note: the input BAM alignment contains the sample name in the header (i.e. `SM:HG002`). - Error trace: (n/a - run finishes). **Does the quick start test work on your system?**. can't be used to reproduce the problem. **Any additional context:**. In the log for each chromosome run, I see that all chromosomes except chrEBV are listed in lines like this. ```. I0112 10:31:04.917984 47443049531200 \. make_examples_core.py:236] \. Task 6/12: Common contigs are [ HERE: list of all chromosomes except for chrEBV]. ```. Best,. Peter.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/603
https://github.com/google/deepvariant/issues/603:870,reliability,Doe,Does,870,"sample name ""default"" used in output VCF for one chromosome; Hi,. I have encountered a somewhat unexpected behavior related to the sample name written to the output VCF (possibly caused by the presence of chrEBV?!). **Describe the issue:**. Sample name ""default"" is used in the output VCF for ""chrEBV"", all other chromosome VCFs contain correct sample name (chr1-chr22,chrX,chrY,chrM). **Setup**. - Operating system: CentOS. - DeepVariant version: v1.2. - Installation method (Docker, built from source, etc.): Singularity v3.5.2. - Type of data: PacBio HiFi, Sequel-II. **Steps to reproduce:**. - Command:. ```. /opt/deepvariant/bin/run_deepvariant \. --model_type=""PACBIO"" \. --regions ""$CHROM"". [ ... otherwise default options ...]. ```. - note: the input BAM alignment contains the sample name in the header (i.e. `SM:HG002`). - Error trace: (n/a - run finishes). **Does the quick start test work on your system?**. can't be used to reproduce the problem. **Any additional context:**. In the log for each chromosome run, I see that all chromosomes except chrEBV are listed in lines like this. ```. I0112 10:31:04.917984 47443049531200 \. make_examples_core.py:236] \. Task 6/12: Common contigs are [ HERE: list of all chromosomes except for chrEBV]. ```. Best,. Peter.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/603
https://github.com/google/deepvariant/issues/603:753,safety,input,input,753,"sample name ""default"" used in output VCF for one chromosome; Hi,. I have encountered a somewhat unexpected behavior related to the sample name written to the output VCF (possibly caused by the presence of chrEBV?!). **Describe the issue:**. Sample name ""default"" is used in the output VCF for ""chrEBV"", all other chromosome VCFs contain correct sample name (chr1-chr22,chrX,chrY,chrM). **Setup**. - Operating system: CentOS. - DeepVariant version: v1.2. - Installation method (Docker, built from source, etc.): Singularity v3.5.2. - Type of data: PacBio HiFi, Sequel-II. **Steps to reproduce:**. - Command:. ```. /opt/deepvariant/bin/run_deepvariant \. --model_type=""PACBIO"" \. --regions ""$CHROM"". [ ... otherwise default options ...]. ```. - note: the input BAM alignment contains the sample name in the header (i.e. `SM:HG002`). - Error trace: (n/a - run finishes). **Does the quick start test work on your system?**. can't be used to reproduce the problem. **Any additional context:**. In the log for each chromosome run, I see that all chromosomes except chrEBV are listed in lines like this. ```. I0112 10:31:04.917984 47443049531200 \. make_examples_core.py:236] \. Task 6/12: Common contigs are [ HERE: list of all chromosomes except for chrEBV]. ```. Best,. Peter.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/603
https://github.com/google/deepvariant/issues/603:833,safety,Error,Error,833,"sample name ""default"" used in output VCF for one chromosome; Hi,. I have encountered a somewhat unexpected behavior related to the sample name written to the output VCF (possibly caused by the presence of chrEBV?!). **Describe the issue:**. Sample name ""default"" is used in the output VCF for ""chrEBV"", all other chromosome VCFs contain correct sample name (chr1-chr22,chrX,chrY,chrM). **Setup**. - Operating system: CentOS. - DeepVariant version: v1.2. - Installation method (Docker, built from source, etc.): Singularity v3.5.2. - Type of data: PacBio HiFi, Sequel-II. **Steps to reproduce:**. - Command:. ```. /opt/deepvariant/bin/run_deepvariant \. --model_type=""PACBIO"" \. --regions ""$CHROM"". [ ... otherwise default options ...]. ```. - note: the input BAM alignment contains the sample name in the header (i.e. `SM:HG002`). - Error trace: (n/a - run finishes). **Does the quick start test work on your system?**. can't be used to reproduce the problem. **Any additional context:**. In the log for each chromosome run, I see that all chromosomes except chrEBV are listed in lines like this. ```. I0112 10:31:04.917984 47443049531200 \. make_examples_core.py:236] \. Task 6/12: Common contigs are [ HERE: list of all chromosomes except for chrEBV]. ```. Best,. Peter.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/603
https://github.com/google/deepvariant/issues/603:891,safety,test,test,891,"sample name ""default"" used in output VCF for one chromosome; Hi,. I have encountered a somewhat unexpected behavior related to the sample name written to the output VCF (possibly caused by the presence of chrEBV?!). **Describe the issue:**. Sample name ""default"" is used in the output VCF for ""chrEBV"", all other chromosome VCFs contain correct sample name (chr1-chr22,chrX,chrY,chrM). **Setup**. - Operating system: CentOS. - DeepVariant version: v1.2. - Installation method (Docker, built from source, etc.): Singularity v3.5.2. - Type of data: PacBio HiFi, Sequel-II. **Steps to reproduce:**. - Command:. ```. /opt/deepvariant/bin/run_deepvariant \. --model_type=""PACBIO"" \. --regions ""$CHROM"". [ ... otherwise default options ...]. ```. - note: the input BAM alignment contains the sample name in the header (i.e. `SM:HG002`). - Error trace: (n/a - run finishes). **Does the quick start test work on your system?**. can't be used to reproduce the problem. **Any additional context:**. In the log for each chromosome run, I see that all chromosomes except chrEBV are listed in lines like this. ```. I0112 10:31:04.917984 47443049531200 \. make_examples_core.py:236] \. Task 6/12: Common contigs are [ HERE: list of all chromosomes except for chrEBV]. ```. Best,. Peter.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/603
https://github.com/google/deepvariant/issues/603:996,safety,log,log,996,"sample name ""default"" used in output VCF for one chromosome; Hi,. I have encountered a somewhat unexpected behavior related to the sample name written to the output VCF (possibly caused by the presence of chrEBV?!). **Describe the issue:**. Sample name ""default"" is used in the output VCF for ""chrEBV"", all other chromosome VCFs contain correct sample name (chr1-chr22,chrX,chrY,chrM). **Setup**. - Operating system: CentOS. - DeepVariant version: v1.2. - Installation method (Docker, built from source, etc.): Singularity v3.5.2. - Type of data: PacBio HiFi, Sequel-II. **Steps to reproduce:**. - Command:. ```. /opt/deepvariant/bin/run_deepvariant \. --model_type=""PACBIO"" \. --regions ""$CHROM"". [ ... otherwise default options ...]. ```. - note: the input BAM alignment contains the sample name in the header (i.e. `SM:HG002`). - Error trace: (n/a - run finishes). **Does the quick start test work on your system?**. can't be used to reproduce the problem. **Any additional context:**. In the log for each chromosome run, I see that all chromosomes except chrEBV are listed in lines like this. ```. I0112 10:31:04.917984 47443049531200 \. make_examples_core.py:236] \. Task 6/12: Common contigs are [ HERE: list of all chromosomes except for chrEBV]. ```. Best,. Peter.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/603
https://github.com/google/deepvariant/issues/603:1052,safety,except,except,1052,"sample name ""default"" used in output VCF for one chromosome; Hi,. I have encountered a somewhat unexpected behavior related to the sample name written to the output VCF (possibly caused by the presence of chrEBV?!). **Describe the issue:**. Sample name ""default"" is used in the output VCF for ""chrEBV"", all other chromosome VCFs contain correct sample name (chr1-chr22,chrX,chrY,chrM). **Setup**. - Operating system: CentOS. - DeepVariant version: v1.2. - Installation method (Docker, built from source, etc.): Singularity v3.5.2. - Type of data: PacBio HiFi, Sequel-II. **Steps to reproduce:**. - Command:. ```. /opt/deepvariant/bin/run_deepvariant \. --model_type=""PACBIO"" \. --regions ""$CHROM"". [ ... otherwise default options ...]. ```. - note: the input BAM alignment contains the sample name in the header (i.e. `SM:HG002`). - Error trace: (n/a - run finishes). **Does the quick start test work on your system?**. can't be used to reproduce the problem. **Any additional context:**. In the log for each chromosome run, I see that all chromosomes except chrEBV are listed in lines like this. ```. I0112 10:31:04.917984 47443049531200 \. make_examples_core.py:236] \. Task 6/12: Common contigs are [ HERE: list of all chromosomes except for chrEBV]. ```. Best,. Peter.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/603
https://github.com/google/deepvariant/issues/603:1234,safety,except,except,1234,"sample name ""default"" used in output VCF for one chromosome; Hi,. I have encountered a somewhat unexpected behavior related to the sample name written to the output VCF (possibly caused by the presence of chrEBV?!). **Describe the issue:**. Sample name ""default"" is used in the output VCF for ""chrEBV"", all other chromosome VCFs contain correct sample name (chr1-chr22,chrX,chrY,chrM). **Setup**. - Operating system: CentOS. - DeepVariant version: v1.2. - Installation method (Docker, built from source, etc.): Singularity v3.5.2. - Type of data: PacBio HiFi, Sequel-II. **Steps to reproduce:**. - Command:. ```. /opt/deepvariant/bin/run_deepvariant \. --model_type=""PACBIO"" \. --regions ""$CHROM"". [ ... otherwise default options ...]. ```. - note: the input BAM alignment contains the sample name in the header (i.e. `SM:HG002`). - Error trace: (n/a - run finishes). **Does the quick start test work on your system?**. can't be used to reproduce the problem. **Any additional context:**. In the log for each chromosome run, I see that all chromosomes except chrEBV are listed in lines like this. ```. I0112 10:31:04.917984 47443049531200 \. make_examples_core.py:236] \. Task 6/12: Common contigs are [ HERE: list of all chromosomes except for chrEBV]. ```. Best,. Peter.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/603
https://github.com/google/deepvariant/issues/603:996,security,log,log,996,"sample name ""default"" used in output VCF for one chromosome; Hi,. I have encountered a somewhat unexpected behavior related to the sample name written to the output VCF (possibly caused by the presence of chrEBV?!). **Describe the issue:**. Sample name ""default"" is used in the output VCF for ""chrEBV"", all other chromosome VCFs contain correct sample name (chr1-chr22,chrX,chrY,chrM). **Setup**. - Operating system: CentOS. - DeepVariant version: v1.2. - Installation method (Docker, built from source, etc.): Singularity v3.5.2. - Type of data: PacBio HiFi, Sequel-II. **Steps to reproduce:**. - Command:. ```. /opt/deepvariant/bin/run_deepvariant \. --model_type=""PACBIO"" \. --regions ""$CHROM"". [ ... otherwise default options ...]. ```. - note: the input BAM alignment contains the sample name in the header (i.e. `SM:HG002`). - Error trace: (n/a - run finishes). **Does the quick start test work on your system?**. can't be used to reproduce the problem. **Any additional context:**. In the log for each chromosome run, I see that all chromosomes except chrEBV are listed in lines like this. ```. I0112 10:31:04.917984 47443049531200 \. make_examples_core.py:236] \. Task 6/12: Common contigs are [ HERE: list of all chromosomes except for chrEBV]. ```. Best,. Peter.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/603
https://github.com/google/deepvariant/issues/603:839,testability,trace,trace,839,"sample name ""default"" used in output VCF for one chromosome; Hi,. I have encountered a somewhat unexpected behavior related to the sample name written to the output VCF (possibly caused by the presence of chrEBV?!). **Describe the issue:**. Sample name ""default"" is used in the output VCF for ""chrEBV"", all other chromosome VCFs contain correct sample name (chr1-chr22,chrX,chrY,chrM). **Setup**. - Operating system: CentOS. - DeepVariant version: v1.2. - Installation method (Docker, built from source, etc.): Singularity v3.5.2. - Type of data: PacBio HiFi, Sequel-II. **Steps to reproduce:**. - Command:. ```. /opt/deepvariant/bin/run_deepvariant \. --model_type=""PACBIO"" \. --regions ""$CHROM"". [ ... otherwise default options ...]. ```. - note: the input BAM alignment contains the sample name in the header (i.e. `SM:HG002`). - Error trace: (n/a - run finishes). **Does the quick start test work on your system?**. can't be used to reproduce the problem. **Any additional context:**. In the log for each chromosome run, I see that all chromosomes except chrEBV are listed in lines like this. ```. I0112 10:31:04.917984 47443049531200 \. make_examples_core.py:236] \. Task 6/12: Common contigs are [ HERE: list of all chromosomes except for chrEBV]. ```. Best,. Peter.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/603
https://github.com/google/deepvariant/issues/603:891,testability,test,test,891,"sample name ""default"" used in output VCF for one chromosome; Hi,. I have encountered a somewhat unexpected behavior related to the sample name written to the output VCF (possibly caused by the presence of chrEBV?!). **Describe the issue:**. Sample name ""default"" is used in the output VCF for ""chrEBV"", all other chromosome VCFs contain correct sample name (chr1-chr22,chrX,chrY,chrM). **Setup**. - Operating system: CentOS. - DeepVariant version: v1.2. - Installation method (Docker, built from source, etc.): Singularity v3.5.2. - Type of data: PacBio HiFi, Sequel-II. **Steps to reproduce:**. - Command:. ```. /opt/deepvariant/bin/run_deepvariant \. --model_type=""PACBIO"" \. --regions ""$CHROM"". [ ... otherwise default options ...]. ```. - note: the input BAM alignment contains the sample name in the header (i.e. `SM:HG002`). - Error trace: (n/a - run finishes). **Does the quick start test work on your system?**. can't be used to reproduce the problem. **Any additional context:**. In the log for each chromosome run, I see that all chromosomes except chrEBV are listed in lines like this. ```. I0112 10:31:04.917984 47443049531200 \. make_examples_core.py:236] \. Task 6/12: Common contigs are [ HERE: list of all chromosomes except for chrEBV]. ```. Best,. Peter.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/603
https://github.com/google/deepvariant/issues/603:977,testability,context,context,977,"sample name ""default"" used in output VCF for one chromosome; Hi,. I have encountered a somewhat unexpected behavior related to the sample name written to the output VCF (possibly caused by the presence of chrEBV?!). **Describe the issue:**. Sample name ""default"" is used in the output VCF for ""chrEBV"", all other chromosome VCFs contain correct sample name (chr1-chr22,chrX,chrY,chrM). **Setup**. - Operating system: CentOS. - DeepVariant version: v1.2. - Installation method (Docker, built from source, etc.): Singularity v3.5.2. - Type of data: PacBio HiFi, Sequel-II. **Steps to reproduce:**. - Command:. ```. /opt/deepvariant/bin/run_deepvariant \. --model_type=""PACBIO"" \. --regions ""$CHROM"". [ ... otherwise default options ...]. ```. - note: the input BAM alignment contains the sample name in the header (i.e. `SM:HG002`). - Error trace: (n/a - run finishes). **Does the quick start test work on your system?**. can't be used to reproduce the problem. **Any additional context:**. In the log for each chromosome run, I see that all chromosomes except chrEBV are listed in lines like this. ```. I0112 10:31:04.917984 47443049531200 \. make_examples_core.py:236] \. Task 6/12: Common contigs are [ HERE: list of all chromosomes except for chrEBV]. ```. Best,. Peter.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/603
https://github.com/google/deepvariant/issues/603:996,testability,log,log,996,"sample name ""default"" used in output VCF for one chromosome; Hi,. I have encountered a somewhat unexpected behavior related to the sample name written to the output VCF (possibly caused by the presence of chrEBV?!). **Describe the issue:**. Sample name ""default"" is used in the output VCF for ""chrEBV"", all other chromosome VCFs contain correct sample name (chr1-chr22,chrX,chrY,chrM). **Setup**. - Operating system: CentOS. - DeepVariant version: v1.2. - Installation method (Docker, built from source, etc.): Singularity v3.5.2. - Type of data: PacBio HiFi, Sequel-II. **Steps to reproduce:**. - Command:. ```. /opt/deepvariant/bin/run_deepvariant \. --model_type=""PACBIO"" \. --regions ""$CHROM"". [ ... otherwise default options ...]. ```. - note: the input BAM alignment contains the sample name in the header (i.e. `SM:HG002`). - Error trace: (n/a - run finishes). **Does the quick start test work on your system?**. can't be used to reproduce the problem. **Any additional context:**. In the log for each chromosome run, I see that all chromosomes except chrEBV are listed in lines like this. ```. I0112 10:31:04.917984 47443049531200 \. make_examples_core.py:236] \. Task 6/12: Common contigs are [ HERE: list of all chromosomes except for chrEBV]. ```. Best,. Peter.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/603
https://github.com/google/deepvariant/issues/603:107,usability,behavi,behavior,107,"sample name ""default"" used in output VCF for one chromosome; Hi,. I have encountered a somewhat unexpected behavior related to the sample name written to the output VCF (possibly caused by the presence of chrEBV?!). **Describe the issue:**. Sample name ""default"" is used in the output VCF for ""chrEBV"", all other chromosome VCFs contain correct sample name (chr1-chr22,chrX,chrY,chrM). **Setup**. - Operating system: CentOS. - DeepVariant version: v1.2. - Installation method (Docker, built from source, etc.): Singularity v3.5.2. - Type of data: PacBio HiFi, Sequel-II. **Steps to reproduce:**. - Command:. ```. /opt/deepvariant/bin/run_deepvariant \. --model_type=""PACBIO"" \. --regions ""$CHROM"". [ ... otherwise default options ...]. ```. - note: the input BAM alignment contains the sample name in the header (i.e. `SM:HG002`). - Error trace: (n/a - run finishes). **Does the quick start test work on your system?**. can't be used to reproduce the problem. **Any additional context:**. In the log for each chromosome run, I see that all chromosomes except chrEBV are listed in lines like this. ```. I0112 10:31:04.917984 47443049531200 \. make_examples_core.py:236] \. Task 6/12: Common contigs are [ HERE: list of all chromosomes except for chrEBV]. ```. Best,. Peter.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/603
https://github.com/google/deepvariant/issues/603:598,usability,Command,Command,598,"sample name ""default"" used in output VCF for one chromosome; Hi,. I have encountered a somewhat unexpected behavior related to the sample name written to the output VCF (possibly caused by the presence of chrEBV?!). **Describe the issue:**. Sample name ""default"" is used in the output VCF for ""chrEBV"", all other chromosome VCFs contain correct sample name (chr1-chr22,chrX,chrY,chrM). **Setup**. - Operating system: CentOS. - DeepVariant version: v1.2. - Installation method (Docker, built from source, etc.): Singularity v3.5.2. - Type of data: PacBio HiFi, Sequel-II. **Steps to reproduce:**. - Command:. ```. /opt/deepvariant/bin/run_deepvariant \. --model_type=""PACBIO"" \. --regions ""$CHROM"". [ ... otherwise default options ...]. ```. - note: the input BAM alignment contains the sample name in the header (i.e. `SM:HG002`). - Error trace: (n/a - run finishes). **Does the quick start test work on your system?**. can't be used to reproduce the problem. **Any additional context:**. In the log for each chromosome run, I see that all chromosomes except chrEBV are listed in lines like this. ```. I0112 10:31:04.917984 47443049531200 \. make_examples_core.py:236] \. Task 6/12: Common contigs are [ HERE: list of all chromosomes except for chrEBV]. ```. Best,. Peter.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/603
https://github.com/google/deepvariant/issues/603:753,usability,input,input,753,"sample name ""default"" used in output VCF for one chromosome; Hi,. I have encountered a somewhat unexpected behavior related to the sample name written to the output VCF (possibly caused by the presence of chrEBV?!). **Describe the issue:**. Sample name ""default"" is used in the output VCF for ""chrEBV"", all other chromosome VCFs contain correct sample name (chr1-chr22,chrX,chrY,chrM). **Setup**. - Operating system: CentOS. - DeepVariant version: v1.2. - Installation method (Docker, built from source, etc.): Singularity v3.5.2. - Type of data: PacBio HiFi, Sequel-II. **Steps to reproduce:**. - Command:. ```. /opt/deepvariant/bin/run_deepvariant \. --model_type=""PACBIO"" \. --regions ""$CHROM"". [ ... otherwise default options ...]. ```. - note: the input BAM alignment contains the sample name in the header (i.e. `SM:HG002`). - Error trace: (n/a - run finishes). **Does the quick start test work on your system?**. can't be used to reproduce the problem. **Any additional context:**. In the log for each chromosome run, I see that all chromosomes except chrEBV are listed in lines like this. ```. I0112 10:31:04.917984 47443049531200 \. make_examples_core.py:236] \. Task 6/12: Common contigs are [ HERE: list of all chromosomes except for chrEBV]. ```. Best,. Peter.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/603
https://github.com/google/deepvariant/issues/603:833,usability,Error,Error,833,"sample name ""default"" used in output VCF for one chromosome; Hi,. I have encountered a somewhat unexpected behavior related to the sample name written to the output VCF (possibly caused by the presence of chrEBV?!). **Describe the issue:**. Sample name ""default"" is used in the output VCF for ""chrEBV"", all other chromosome VCFs contain correct sample name (chr1-chr22,chrX,chrY,chrM). **Setup**. - Operating system: CentOS. - DeepVariant version: v1.2. - Installation method (Docker, built from source, etc.): Singularity v3.5.2. - Type of data: PacBio HiFi, Sequel-II. **Steps to reproduce:**. - Command:. ```. /opt/deepvariant/bin/run_deepvariant \. --model_type=""PACBIO"" \. --regions ""$CHROM"". [ ... otherwise default options ...]. ```. - note: the input BAM alignment contains the sample name in the header (i.e. `SM:HG002`). - Error trace: (n/a - run finishes). **Does the quick start test work on your system?**. can't be used to reproduce the problem. **Any additional context:**. In the log for each chromosome run, I see that all chromosomes except chrEBV are listed in lines like this. ```. I0112 10:31:04.917984 47443049531200 \. make_examples_core.py:236] \. Task 6/12: Common contigs are [ HERE: list of all chromosomes except for chrEBV]. ```. Best,. Peter.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/603
https://github.com/google/deepvariant/issues/604:245,availability,error,error,245,"Minor Issue: BIN_VERSION not passed to container in quickstart; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**:. **Describe the issue:**. Had an issue with the quickstart tutorial where I received this error:. ```. /usr/bin/docker-current: Error parsing reference: ""docker.io/google/deepvariant:"" is not a valid repository/tag: invalid reference format. See '/usr/bin/docker-current run --help'. deepvariant_test.sh: line 11: make_examples: command not found. deepvariant_test.sh: line 11: --ref=/input/ucsc.hg19.chr20.unittest.fasta: No such file or directory. ```. The problem seemed to be that the [quickstart](https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-quick-start.md) omits declaring `BIN_VERSION=""1.4.0""`, although it is declared on the [main page](https://github.com/google/deepvariant). **Solution**. ```. docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. -v ""${BIN_VERSION}"":. docker.io/google/deepvariant \. /opt/deepvariant/bin/run_deepvariant \. ... ```. Should be . ```. BIN_VERSION=""1.4.0"". docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. -v ""${BIN_VERSION}"":. docker.io/google/deepvariant \. /opt/deepvariant/bin/run_deepvariant \. ... ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/604
https://github.com/google/deepvariant/issues/604:283,availability,Error,Error,283,"Minor Issue: BIN_VERSION not passed to container in quickstart; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**:. **Describe the issue:**. Had an issue with the quickstart tutorial where I received this error:. ```. /usr/bin/docker-current: Error parsing reference: ""docker.io/google/deepvariant:"" is not a valid repository/tag: invalid reference format. See '/usr/bin/docker-current run --help'. deepvariant_test.sh: line 11: make_examples: command not found. deepvariant_test.sh: line 11: --ref=/input/ucsc.hg19.chr20.unittest.fasta: No such file or directory. ```. The problem seemed to be that the [quickstart](https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-quick-start.md) omits declaring `BIN_VERSION=""1.4.0""`, although it is declared on the [main page](https://github.com/google/deepvariant). **Solution**. ```. docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. -v ""${BIN_VERSION}"":. docker.io/google/deepvariant \. /opt/deepvariant/bin/run_deepvariant \. ... ```. Should be . ```. BIN_VERSION=""1.4.0"". docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. -v ""${BIN_VERSION}"":. docker.io/google/deepvariant \. /opt/deepvariant/bin/run_deepvariant \. ... ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/604
https://github.com/google/deepvariant/issues/604:39,deployability,contain,container,39,"Minor Issue: BIN_VERSION not passed to container in quickstart; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**:. **Describe the issue:**. Had an issue with the quickstart tutorial where I received this error:. ```. /usr/bin/docker-current: Error parsing reference: ""docker.io/google/deepvariant:"" is not a valid repository/tag: invalid reference format. See '/usr/bin/docker-current run --help'. deepvariant_test.sh: line 11: make_examples: command not found. deepvariant_test.sh: line 11: --ref=/input/ucsc.hg19.chr20.unittest.fasta: No such file or directory. ```. The problem seemed to be that the [quickstart](https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-quick-start.md) omits declaring `BIN_VERSION=""1.4.0""`, although it is declared on the [main page](https://github.com/google/deepvariant). **Solution**. ```. docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. -v ""${BIN_VERSION}"":. docker.io/google/deepvariant \. /opt/deepvariant/bin/run_deepvariant \. ... ```. Should be . ```. BIN_VERSION=""1.4.0"". docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. -v ""${BIN_VERSION}"":. docker.io/google/deepvariant \. /opt/deepvariant/bin/run_deepvariant \. ... ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/604
https://github.com/google/deepvariant/issues/604:274,energy efficiency,current,current,274,"Minor Issue: BIN_VERSION not passed to container in quickstart; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**:. **Describe the issue:**. Had an issue with the quickstart tutorial where I received this error:. ```. /usr/bin/docker-current: Error parsing reference: ""docker.io/google/deepvariant:"" is not a valid repository/tag: invalid reference format. See '/usr/bin/docker-current run --help'. deepvariant_test.sh: line 11: make_examples: command not found. deepvariant_test.sh: line 11: --ref=/input/ucsc.hg19.chr20.unittest.fasta: No such file or directory. ```. The problem seemed to be that the [quickstart](https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-quick-start.md) omits declaring `BIN_VERSION=""1.4.0""`, although it is declared on the [main page](https://github.com/google/deepvariant). **Solution**. ```. docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. -v ""${BIN_VERSION}"":. docker.io/google/deepvariant \. /opt/deepvariant/bin/run_deepvariant \. ... ```. Should be . ```. BIN_VERSION=""1.4.0"". docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. -v ""${BIN_VERSION}"":. docker.io/google/deepvariant \. /opt/deepvariant/bin/run_deepvariant \. ... ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/604
https://github.com/google/deepvariant/issues/604:418,energy efficiency,current,current,418,"Minor Issue: BIN_VERSION not passed to container in quickstart; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**:. **Describe the issue:**. Had an issue with the quickstart tutorial where I received this error:. ```. /usr/bin/docker-current: Error parsing reference: ""docker.io/google/deepvariant:"" is not a valid repository/tag: invalid reference format. See '/usr/bin/docker-current run --help'. deepvariant_test.sh: line 11: make_examples: command not found. deepvariant_test.sh: line 11: --ref=/input/ucsc.hg19.chr20.unittest.fasta: No such file or directory. ```. The problem seemed to be that the [quickstart](https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-quick-start.md) omits declaring `BIN_VERSION=""1.4.0""`, although it is declared on the [main page](https://github.com/google/deepvariant). **Solution**. ```. docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. -v ""${BIN_VERSION}"":. docker.io/google/deepvariant \. /opt/deepvariant/bin/run_deepvariant \. ... ```. Should be . ```. BIN_VERSION=""1.4.0"". docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. -v ""${BIN_VERSION}"":. docker.io/google/deepvariant \. /opt/deepvariant/bin/run_deepvariant \. ... ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/604
https://github.com/google/deepvariant/issues/604:355,integrability,repositor,repository,355,"Minor Issue: BIN_VERSION not passed to container in quickstart; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**:. **Describe the issue:**. Had an issue with the quickstart tutorial where I received this error:. ```. /usr/bin/docker-current: Error parsing reference: ""docker.io/google/deepvariant:"" is not a valid repository/tag: invalid reference format. See '/usr/bin/docker-current run --help'. deepvariant_test.sh: line 11: make_examples: command not found. deepvariant_test.sh: line 11: --ref=/input/ucsc.hg19.chr20.unittest.fasta: No such file or directory. ```. The problem seemed to be that the [quickstart](https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-quick-start.md) omits declaring `BIN_VERSION=""1.4.0""`, although it is declared on the [main page](https://github.com/google/deepvariant). **Solution**. ```. docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. -v ""${BIN_VERSION}"":. docker.io/google/deepvariant \. /opt/deepvariant/bin/run_deepvariant \. ... ```. Should be . ```. BIN_VERSION=""1.4.0"". docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. -v ""${BIN_VERSION}"":. docker.io/google/deepvariant \. /opt/deepvariant/bin/run_deepvariant \. ... ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/604
https://github.com/google/deepvariant/issues/604:355,interoperability,repositor,repository,355,"Minor Issue: BIN_VERSION not passed to container in quickstart; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**:. **Describe the issue:**. Had an issue with the quickstart tutorial where I received this error:. ```. /usr/bin/docker-current: Error parsing reference: ""docker.io/google/deepvariant:"" is not a valid repository/tag: invalid reference format. See '/usr/bin/docker-current run --help'. deepvariant_test.sh: line 11: make_examples: command not found. deepvariant_test.sh: line 11: --ref=/input/ucsc.hg19.chr20.unittest.fasta: No such file or directory. ```. The problem seemed to be that the [quickstart](https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-quick-start.md) omits declaring `BIN_VERSION=""1.4.0""`, although it is declared on the [main page](https://github.com/google/deepvariant). **Solution**. ```. docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. -v ""${BIN_VERSION}"":. docker.io/google/deepvariant \. /opt/deepvariant/bin/run_deepvariant \. ... ```. Should be . ```. BIN_VERSION=""1.4.0"". docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. -v ""${BIN_VERSION}"":. docker.io/google/deepvariant \. /opt/deepvariant/bin/run_deepvariant \. ... ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/604
https://github.com/google/deepvariant/issues/604:389,interoperability,format,format,389,"Minor Issue: BIN_VERSION not passed to container in quickstart; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**:. **Describe the issue:**. Had an issue with the quickstart tutorial where I received this error:. ```. /usr/bin/docker-current: Error parsing reference: ""docker.io/google/deepvariant:"" is not a valid repository/tag: invalid reference format. See '/usr/bin/docker-current run --help'. deepvariant_test.sh: line 11: make_examples: command not found. deepvariant_test.sh: line 11: --ref=/input/ucsc.hg19.chr20.unittest.fasta: No such file or directory. ```. The problem seemed to be that the [quickstart](https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-quick-start.md) omits declaring `BIN_VERSION=""1.4.0""`, although it is declared on the [main page](https://github.com/google/deepvariant). **Solution**. ```. docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. -v ""${BIN_VERSION}"":. docker.io/google/deepvariant \. /opt/deepvariant/bin/run_deepvariant \. ... ```. Should be . ```. BIN_VERSION=""1.4.0"". docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. -v ""${BIN_VERSION}"":. docker.io/google/deepvariant \. /opt/deepvariant/bin/run_deepvariant \. ... ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/604
https://github.com/google/deepvariant/issues/604:245,performance,error,error,245,"Minor Issue: BIN_VERSION not passed to container in quickstart; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**:. **Describe the issue:**. Had an issue with the quickstart tutorial where I received this error:. ```. /usr/bin/docker-current: Error parsing reference: ""docker.io/google/deepvariant:"" is not a valid repository/tag: invalid reference format. See '/usr/bin/docker-current run --help'. deepvariant_test.sh: line 11: make_examples: command not found. deepvariant_test.sh: line 11: --ref=/input/ucsc.hg19.chr20.unittest.fasta: No such file or directory. ```. The problem seemed to be that the [quickstart](https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-quick-start.md) omits declaring `BIN_VERSION=""1.4.0""`, although it is declared on the [main page](https://github.com/google/deepvariant). **Solution**. ```. docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. -v ""${BIN_VERSION}"":. docker.io/google/deepvariant \. /opt/deepvariant/bin/run_deepvariant \. ... ```. Should be . ```. BIN_VERSION=""1.4.0"". docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. -v ""${BIN_VERSION}"":. docker.io/google/deepvariant \. /opt/deepvariant/bin/run_deepvariant \. ... ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/604
https://github.com/google/deepvariant/issues/604:283,performance,Error,Error,283,"Minor Issue: BIN_VERSION not passed to container in quickstart; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**:. **Describe the issue:**. Had an issue with the quickstart tutorial where I received this error:. ```. /usr/bin/docker-current: Error parsing reference: ""docker.io/google/deepvariant:"" is not a valid repository/tag: invalid reference format. See '/usr/bin/docker-current run --help'. deepvariant_test.sh: line 11: make_examples: command not found. deepvariant_test.sh: line 11: --ref=/input/ucsc.hg19.chr20.unittest.fasta: No such file or directory. ```. The problem seemed to be that the [quickstart](https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-quick-start.md) omits declaring `BIN_VERSION=""1.4.0""`, although it is declared on the [main page](https://github.com/google/deepvariant). **Solution**. ```. docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. -v ""${BIN_VERSION}"":. docker.io/google/deepvariant \. /opt/deepvariant/bin/run_deepvariant \. ... ```. Should be . ```. BIN_VERSION=""1.4.0"". docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. -v ""${BIN_VERSION}"":. docker.io/google/deepvariant \. /opt/deepvariant/bin/run_deepvariant \. ... ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/604
https://github.com/google/deepvariant/issues/604:245,safety,error,error,245,"Minor Issue: BIN_VERSION not passed to container in quickstart; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**:. **Describe the issue:**. Had an issue with the quickstart tutorial where I received this error:. ```. /usr/bin/docker-current: Error parsing reference: ""docker.io/google/deepvariant:"" is not a valid repository/tag: invalid reference format. See '/usr/bin/docker-current run --help'. deepvariant_test.sh: line 11: make_examples: command not found. deepvariant_test.sh: line 11: --ref=/input/ucsc.hg19.chr20.unittest.fasta: No such file or directory. ```. The problem seemed to be that the [quickstart](https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-quick-start.md) omits declaring `BIN_VERSION=""1.4.0""`, although it is declared on the [main page](https://github.com/google/deepvariant). **Solution**. ```. docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. -v ""${BIN_VERSION}"":. docker.io/google/deepvariant \. /opt/deepvariant/bin/run_deepvariant \. ... ```. Should be . ```. BIN_VERSION=""1.4.0"". docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. -v ""${BIN_VERSION}"":. docker.io/google/deepvariant \. /opt/deepvariant/bin/run_deepvariant \. ... ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/604
https://github.com/google/deepvariant/issues/604:283,safety,Error,Error,283,"Minor Issue: BIN_VERSION not passed to container in quickstart; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**:. **Describe the issue:**. Had an issue with the quickstart tutorial where I received this error:. ```. /usr/bin/docker-current: Error parsing reference: ""docker.io/google/deepvariant:"" is not a valid repository/tag: invalid reference format. See '/usr/bin/docker-current run --help'. deepvariant_test.sh: line 11: make_examples: command not found. deepvariant_test.sh: line 11: --ref=/input/ucsc.hg19.chr20.unittest.fasta: No such file or directory. ```. The problem seemed to be that the [quickstart](https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-quick-start.md) omits declaring `BIN_VERSION=""1.4.0""`, although it is declared on the [main page](https://github.com/google/deepvariant). **Solution**. ```. docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. -v ""${BIN_VERSION}"":. docker.io/google/deepvariant \. /opt/deepvariant/bin/run_deepvariant \. ... ```. Should be . ```. BIN_VERSION=""1.4.0"". docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. -v ""${BIN_VERSION}"":. docker.io/google/deepvariant \. /opt/deepvariant/bin/run_deepvariant \. ... ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/604
https://github.com/google/deepvariant/issues/604:349,safety,valid,valid,349,"Minor Issue: BIN_VERSION not passed to container in quickstart; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**:. **Describe the issue:**. Had an issue with the quickstart tutorial where I received this error:. ```. /usr/bin/docker-current: Error parsing reference: ""docker.io/google/deepvariant:"" is not a valid repository/tag: invalid reference format. See '/usr/bin/docker-current run --help'. deepvariant_test.sh: line 11: make_examples: command not found. deepvariant_test.sh: line 11: --ref=/input/ucsc.hg19.chr20.unittest.fasta: No such file or directory. ```. The problem seemed to be that the [quickstart](https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-quick-start.md) omits declaring `BIN_VERSION=""1.4.0""`, although it is declared on the [main page](https://github.com/google/deepvariant). **Solution**. ```. docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. -v ""${BIN_VERSION}"":. docker.io/google/deepvariant \. /opt/deepvariant/bin/run_deepvariant \. ... ```. Should be . ```. BIN_VERSION=""1.4.0"". docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. -v ""${BIN_VERSION}"":. docker.io/google/deepvariant \. /opt/deepvariant/bin/run_deepvariant \. ... ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/604
https://github.com/google/deepvariant/issues/604:540,safety,input,input,540,"Minor Issue: BIN_VERSION not passed to container in quickstart; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**:. **Describe the issue:**. Had an issue with the quickstart tutorial where I received this error:. ```. /usr/bin/docker-current: Error parsing reference: ""docker.io/google/deepvariant:"" is not a valid repository/tag: invalid reference format. See '/usr/bin/docker-current run --help'. deepvariant_test.sh: line 11: make_examples: command not found. deepvariant_test.sh: line 11: --ref=/input/ucsc.hg19.chr20.unittest.fasta: No such file or directory. ```. The problem seemed to be that the [quickstart](https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-quick-start.md) omits declaring `BIN_VERSION=""1.4.0""`, although it is declared on the [main page](https://github.com/google/deepvariant). **Solution**. ```. docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. -v ""${BIN_VERSION}"":. docker.io/google/deepvariant \. /opt/deepvariant/bin/run_deepvariant \. ... ```. Should be . ```. BIN_VERSION=""1.4.0"". docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. -v ""${BIN_VERSION}"":. docker.io/google/deepvariant \. /opt/deepvariant/bin/run_deepvariant \. ... ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/604
https://github.com/google/deepvariant/issues/604:913,safety,input,input,913,"Minor Issue: BIN_VERSION not passed to container in quickstart; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**:. **Describe the issue:**. Had an issue with the quickstart tutorial where I received this error:. ```. /usr/bin/docker-current: Error parsing reference: ""docker.io/google/deepvariant:"" is not a valid repository/tag: invalid reference format. See '/usr/bin/docker-current run --help'. deepvariant_test.sh: line 11: make_examples: command not found. deepvariant_test.sh: line 11: --ref=/input/ucsc.hg19.chr20.unittest.fasta: No such file or directory. ```. The problem seemed to be that the [quickstart](https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-quick-start.md) omits declaring `BIN_VERSION=""1.4.0""`, although it is declared on the [main page](https://github.com/google/deepvariant). **Solution**. ```. docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. -v ""${BIN_VERSION}"":. docker.io/google/deepvariant \. /opt/deepvariant/bin/run_deepvariant \. ... ```. Should be . ```. BIN_VERSION=""1.4.0"". docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. -v ""${BIN_VERSION}"":. docker.io/google/deepvariant \. /opt/deepvariant/bin/run_deepvariant \. ... ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/604
https://github.com/google/deepvariant/issues/604:1130,safety,input,input,1130,"Minor Issue: BIN_VERSION not passed to container in quickstart; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**:. **Describe the issue:**. Had an issue with the quickstart tutorial where I received this error:. ```. /usr/bin/docker-current: Error parsing reference: ""docker.io/google/deepvariant:"" is not a valid repository/tag: invalid reference format. See '/usr/bin/docker-current run --help'. deepvariant_test.sh: line 11: make_examples: command not found. deepvariant_test.sh: line 11: --ref=/input/ucsc.hg19.chr20.unittest.fasta: No such file or directory. ```. The problem seemed to be that the [quickstart](https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-quick-start.md) omits declaring `BIN_VERSION=""1.4.0""`, although it is declared on the [main page](https://github.com/google/deepvariant). **Solution**. ```. docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. -v ""${BIN_VERSION}"":. docker.io/google/deepvariant \. /opt/deepvariant/bin/run_deepvariant \. ... ```. Should be . ```. BIN_VERSION=""1.4.0"". docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. -v ""${BIN_VERSION}"":. docker.io/google/deepvariant \. /opt/deepvariant/bin/run_deepvariant \. ... ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/604
https://github.com/google/deepvariant/issues/604:562,testability,unit,unittest,562,"Minor Issue: BIN_VERSION not passed to container in quickstart; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**:. **Describe the issue:**. Had an issue with the quickstart tutorial where I received this error:. ```. /usr/bin/docker-current: Error parsing reference: ""docker.io/google/deepvariant:"" is not a valid repository/tag: invalid reference format. See '/usr/bin/docker-current run --help'. deepvariant_test.sh: line 11: make_examples: command not found. deepvariant_test.sh: line 11: --ref=/input/ucsc.hg19.chr20.unittest.fasta: No such file or directory. ```. The problem seemed to be that the [quickstart](https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-quick-start.md) omits declaring `BIN_VERSION=""1.4.0""`, although it is declared on the [main page](https://github.com/google/deepvariant). **Solution**. ```. docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. -v ""${BIN_VERSION}"":. docker.io/google/deepvariant \. /opt/deepvariant/bin/run_deepvariant \. ... ```. Should be . ```. BIN_VERSION=""1.4.0"". docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. -v ""${BIN_VERSION}"":. docker.io/google/deepvariant \. /opt/deepvariant/bin/run_deepvariant \. ... ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/604
https://github.com/google/deepvariant/issues/604:245,usability,error,error,245,"Minor Issue: BIN_VERSION not passed to container in quickstart; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**:. **Describe the issue:**. Had an issue with the quickstart tutorial where I received this error:. ```. /usr/bin/docker-current: Error parsing reference: ""docker.io/google/deepvariant:"" is not a valid repository/tag: invalid reference format. See '/usr/bin/docker-current run --help'. deepvariant_test.sh: line 11: make_examples: command not found. deepvariant_test.sh: line 11: --ref=/input/ucsc.hg19.chr20.unittest.fasta: No such file or directory. ```. The problem seemed to be that the [quickstart](https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-quick-start.md) omits declaring `BIN_VERSION=""1.4.0""`, although it is declared on the [main page](https://github.com/google/deepvariant). **Solution**. ```. docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. -v ""${BIN_VERSION}"":. docker.io/google/deepvariant \. /opt/deepvariant/bin/run_deepvariant \. ... ```. Should be . ```. BIN_VERSION=""1.4.0"". docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. -v ""${BIN_VERSION}"":. docker.io/google/deepvariant \. /opt/deepvariant/bin/run_deepvariant \. ... ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/604
https://github.com/google/deepvariant/issues/604:283,usability,Error,Error,283,"Minor Issue: BIN_VERSION not passed to container in quickstart; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**:. **Describe the issue:**. Had an issue with the quickstart tutorial where I received this error:. ```. /usr/bin/docker-current: Error parsing reference: ""docker.io/google/deepvariant:"" is not a valid repository/tag: invalid reference format. See '/usr/bin/docker-current run --help'. deepvariant_test.sh: line 11: make_examples: command not found. deepvariant_test.sh: line 11: --ref=/input/ucsc.hg19.chr20.unittest.fasta: No such file or directory. ```. The problem seemed to be that the [quickstart](https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-quick-start.md) omits declaring `BIN_VERSION=""1.4.0""`, although it is declared on the [main page](https://github.com/google/deepvariant). **Solution**. ```. docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. -v ""${BIN_VERSION}"":. docker.io/google/deepvariant \. /opt/deepvariant/bin/run_deepvariant \. ... ```. Should be . ```. BIN_VERSION=""1.4.0"". docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. -v ""${BIN_VERSION}"":. docker.io/google/deepvariant \. /opt/deepvariant/bin/run_deepvariant \. ... ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/604
https://github.com/google/deepvariant/issues/604:432,usability,help,help,432,"Minor Issue: BIN_VERSION not passed to container in quickstart; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**:. **Describe the issue:**. Had an issue with the quickstart tutorial where I received this error:. ```. /usr/bin/docker-current: Error parsing reference: ""docker.io/google/deepvariant:"" is not a valid repository/tag: invalid reference format. See '/usr/bin/docker-current run --help'. deepvariant_test.sh: line 11: make_examples: command not found. deepvariant_test.sh: line 11: --ref=/input/ucsc.hg19.chr20.unittest.fasta: No such file or directory. ```. The problem seemed to be that the [quickstart](https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-quick-start.md) omits declaring `BIN_VERSION=""1.4.0""`, although it is declared on the [main page](https://github.com/google/deepvariant). **Solution**. ```. docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. -v ""${BIN_VERSION}"":. docker.io/google/deepvariant \. /opt/deepvariant/bin/run_deepvariant \. ... ```. Should be . ```. BIN_VERSION=""1.4.0"". docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. -v ""${BIN_VERSION}"":. docker.io/google/deepvariant \. /opt/deepvariant/bin/run_deepvariant \. ... ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/604
https://github.com/google/deepvariant/issues/604:484,usability,command,command,484,"Minor Issue: BIN_VERSION not passed to container in quickstart; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**:. **Describe the issue:**. Had an issue with the quickstart tutorial where I received this error:. ```. /usr/bin/docker-current: Error parsing reference: ""docker.io/google/deepvariant:"" is not a valid repository/tag: invalid reference format. See '/usr/bin/docker-current run --help'. deepvariant_test.sh: line 11: make_examples: command not found. deepvariant_test.sh: line 11: --ref=/input/ucsc.hg19.chr20.unittest.fasta: No such file or directory. ```. The problem seemed to be that the [quickstart](https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-quick-start.md) omits declaring `BIN_VERSION=""1.4.0""`, although it is declared on the [main page](https://github.com/google/deepvariant). **Solution**. ```. docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. -v ""${BIN_VERSION}"":. docker.io/google/deepvariant \. /opt/deepvariant/bin/run_deepvariant \. ... ```. Should be . ```. BIN_VERSION=""1.4.0"". docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. -v ""${BIN_VERSION}"":. docker.io/google/deepvariant \. /opt/deepvariant/bin/run_deepvariant \. ... ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/604
https://github.com/google/deepvariant/issues/604:540,usability,input,input,540,"Minor Issue: BIN_VERSION not passed to container in quickstart; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**:. **Describe the issue:**. Had an issue with the quickstart tutorial where I received this error:. ```. /usr/bin/docker-current: Error parsing reference: ""docker.io/google/deepvariant:"" is not a valid repository/tag: invalid reference format. See '/usr/bin/docker-current run --help'. deepvariant_test.sh: line 11: make_examples: command not found. deepvariant_test.sh: line 11: --ref=/input/ucsc.hg19.chr20.unittest.fasta: No such file or directory. ```. The problem seemed to be that the [quickstart](https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-quick-start.md) omits declaring `BIN_VERSION=""1.4.0""`, although it is declared on the [main page](https://github.com/google/deepvariant). **Solution**. ```. docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. -v ""${BIN_VERSION}"":. docker.io/google/deepvariant \. /opt/deepvariant/bin/run_deepvariant \. ... ```. Should be . ```. BIN_VERSION=""1.4.0"". docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. -v ""${BIN_VERSION}"":. docker.io/google/deepvariant \. /opt/deepvariant/bin/run_deepvariant \. ... ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/604
https://github.com/google/deepvariant/issues/604:913,usability,input,input,913,"Minor Issue: BIN_VERSION not passed to container in quickstart; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**:. **Describe the issue:**. Had an issue with the quickstart tutorial where I received this error:. ```. /usr/bin/docker-current: Error parsing reference: ""docker.io/google/deepvariant:"" is not a valid repository/tag: invalid reference format. See '/usr/bin/docker-current run --help'. deepvariant_test.sh: line 11: make_examples: command not found. deepvariant_test.sh: line 11: --ref=/input/ucsc.hg19.chr20.unittest.fasta: No such file or directory. ```. The problem seemed to be that the [quickstart](https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-quick-start.md) omits declaring `BIN_VERSION=""1.4.0""`, although it is declared on the [main page](https://github.com/google/deepvariant). **Solution**. ```. docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. -v ""${BIN_VERSION}"":. docker.io/google/deepvariant \. /opt/deepvariant/bin/run_deepvariant \. ... ```. Should be . ```. BIN_VERSION=""1.4.0"". docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. -v ""${BIN_VERSION}"":. docker.io/google/deepvariant \. /opt/deepvariant/bin/run_deepvariant \. ... ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/604
https://github.com/google/deepvariant/issues/604:1130,usability,input,input,1130,"Minor Issue: BIN_VERSION not passed to container in quickstart; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**:. **Describe the issue:**. Had an issue with the quickstart tutorial where I received this error:. ```. /usr/bin/docker-current: Error parsing reference: ""docker.io/google/deepvariant:"" is not a valid repository/tag: invalid reference format. See '/usr/bin/docker-current run --help'. deepvariant_test.sh: line 11: make_examples: command not found. deepvariant_test.sh: line 11: --ref=/input/ucsc.hg19.chr20.unittest.fasta: No such file or directory. ```. The problem seemed to be that the [quickstart](https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-quick-start.md) omits declaring `BIN_VERSION=""1.4.0""`, although it is declared on the [main page](https://github.com/google/deepvariant). **Solution**. ```. docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. -v ""${BIN_VERSION}"":. docker.io/google/deepvariant \. /opt/deepvariant/bin/run_deepvariant \. ... ```. Should be . ```. BIN_VERSION=""1.4.0"". docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. -v ""${BIN_VERSION}"":. docker.io/google/deepvariant \. /opt/deepvariant/bin/run_deepvariant \. ... ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/604
https://github.com/google/deepvariant/issues/605:0,availability,Avail,Availability,0,"Availability of DeepVariant RNA-seq GTEx model; Hello DeepVariant team,. I am currently interested in using your software for variant calling from RNA-sequencing data. After reading the [preprint](https://doi.org/10.1101/2022.10.16.512451) I understand that there are two available models for such purpose: ""DV RNA-seq [GTEx]"" and ""DV RNA-seq [GIAB]"". I see that ""DV RNA-seq [GIAB]"" is used in the [RNA-seq case study](https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-rnaseq-case-study.md) and downloadable through the urls provided there, but I have not been able to find urls for the GTEx model. Is it yet to be made available?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/605
https://github.com/google/deepvariant/issues/605:272,availability,avail,available,272,"Availability of DeepVariant RNA-seq GTEx model; Hello DeepVariant team,. I am currently interested in using your software for variant calling from RNA-sequencing data. After reading the [preprint](https://doi.org/10.1101/2022.10.16.512451) I understand that there are two available models for such purpose: ""DV RNA-seq [GTEx]"" and ""DV RNA-seq [GIAB]"". I see that ""DV RNA-seq [GIAB]"" is used in the [RNA-seq case study](https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-rnaseq-case-study.md) and downloadable through the urls provided there, but I have not been able to find urls for the GTEx model. Is it yet to be made available?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/605
https://github.com/google/deepvariant/issues/605:510,availability,down,downloadable,510,"Availability of DeepVariant RNA-seq GTEx model; Hello DeepVariant team,. I am currently interested in using your software for variant calling from RNA-sequencing data. After reading the [preprint](https://doi.org/10.1101/2022.10.16.512451) I understand that there are two available models for such purpose: ""DV RNA-seq [GTEx]"" and ""DV RNA-seq [GIAB]"". I see that ""DV RNA-seq [GIAB]"" is used in the [RNA-seq case study](https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-rnaseq-case-study.md) and downloadable through the urls provided there, but I have not been able to find urls for the GTEx model. Is it yet to be made available?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/605
https://github.com/google/deepvariant/issues/605:635,availability,avail,available,635,"Availability of DeepVariant RNA-seq GTEx model; Hello DeepVariant team,. I am currently interested in using your software for variant calling from RNA-sequencing data. After reading the [preprint](https://doi.org/10.1101/2022.10.16.512451) I understand that there are two available models for such purpose: ""DV RNA-seq [GTEx]"" and ""DV RNA-seq [GIAB]"". I see that ""DV RNA-seq [GIAB]"" is used in the [RNA-seq case study](https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-rnaseq-case-study.md) and downloadable through the urls provided there, but I have not been able to find urls for the GTEx model. Is it yet to be made available?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/605
https://github.com/google/deepvariant/issues/605:41,energy efficiency,model,model,41,"Availability of DeepVariant RNA-seq GTEx model; Hello DeepVariant team,. I am currently interested in using your software for variant calling from RNA-sequencing data. After reading the [preprint](https://doi.org/10.1101/2022.10.16.512451) I understand that there are two available models for such purpose: ""DV RNA-seq [GTEx]"" and ""DV RNA-seq [GIAB]"". I see that ""DV RNA-seq [GIAB]"" is used in the [RNA-seq case study](https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-rnaseq-case-study.md) and downloadable through the urls provided there, but I have not been able to find urls for the GTEx model. Is it yet to be made available?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/605
https://github.com/google/deepvariant/issues/605:78,energy efficiency,current,currently,78,"Availability of DeepVariant RNA-seq GTEx model; Hello DeepVariant team,. I am currently interested in using your software for variant calling from RNA-sequencing data. After reading the [preprint](https://doi.org/10.1101/2022.10.16.512451) I understand that there are two available models for such purpose: ""DV RNA-seq [GTEx]"" and ""DV RNA-seq [GIAB]"". I see that ""DV RNA-seq [GIAB]"" is used in the [RNA-seq case study](https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-rnaseq-case-study.md) and downloadable through the urls provided there, but I have not been able to find urls for the GTEx model. Is it yet to be made available?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/605
https://github.com/google/deepvariant/issues/605:282,energy efficiency,model,models,282,"Availability of DeepVariant RNA-seq GTEx model; Hello DeepVariant team,. I am currently interested in using your software for variant calling from RNA-sequencing data. After reading the [preprint](https://doi.org/10.1101/2022.10.16.512451) I understand that there are two available models for such purpose: ""DV RNA-seq [GTEx]"" and ""DV RNA-seq [GIAB]"". I see that ""DV RNA-seq [GIAB]"" is used in the [RNA-seq case study](https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-rnaseq-case-study.md) and downloadable through the urls provided there, but I have not been able to find urls for the GTEx model. Is it yet to be made available?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/605
https://github.com/google/deepvariant/issues/605:607,energy efficiency,model,model,607,"Availability of DeepVariant RNA-seq GTEx model; Hello DeepVariant team,. I am currently interested in using your software for variant calling from RNA-sequencing data. After reading the [preprint](https://doi.org/10.1101/2022.10.16.512451) I understand that there are two available models for such purpose: ""DV RNA-seq [GTEx]"" and ""DV RNA-seq [GIAB]"". I see that ""DV RNA-seq [GIAB]"" is used in the [RNA-seq case study](https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-rnaseq-case-study.md) and downloadable through the urls provided there, but I have not been able to find urls for the GTEx model. Is it yet to be made available?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/605
https://github.com/google/deepvariant/issues/605:0,reliability,Availab,Availability,0,"Availability of DeepVariant RNA-seq GTEx model; Hello DeepVariant team,. I am currently interested in using your software for variant calling from RNA-sequencing data. After reading the [preprint](https://doi.org/10.1101/2022.10.16.512451) I understand that there are two available models for such purpose: ""DV RNA-seq [GTEx]"" and ""DV RNA-seq [GIAB]"". I see that ""DV RNA-seq [GIAB]"" is used in the [RNA-seq case study](https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-rnaseq-case-study.md) and downloadable through the urls provided there, but I have not been able to find urls for the GTEx model. Is it yet to be made available?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/605
https://github.com/google/deepvariant/issues/605:272,reliability,availab,available,272,"Availability of DeepVariant RNA-seq GTEx model; Hello DeepVariant team,. I am currently interested in using your software for variant calling from RNA-sequencing data. After reading the [preprint](https://doi.org/10.1101/2022.10.16.512451) I understand that there are two available models for such purpose: ""DV RNA-seq [GTEx]"" and ""DV RNA-seq [GIAB]"". I see that ""DV RNA-seq [GIAB]"" is used in the [RNA-seq case study](https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-rnaseq-case-study.md) and downloadable through the urls provided there, but I have not been able to find urls for the GTEx model. Is it yet to be made available?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/605
https://github.com/google/deepvariant/issues/605:635,reliability,availab,available,635,"Availability of DeepVariant RNA-seq GTEx model; Hello DeepVariant team,. I am currently interested in using your software for variant calling from RNA-sequencing data. After reading the [preprint](https://doi.org/10.1101/2022.10.16.512451) I understand that there are two available models for such purpose: ""DV RNA-seq [GTEx]"" and ""DV RNA-seq [GIAB]"". I see that ""DV RNA-seq [GIAB]"" is used in the [RNA-seq case study](https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-rnaseq-case-study.md) and downloadable through the urls provided there, but I have not been able to find urls for the GTEx model. Is it yet to be made available?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/605
https://github.com/google/deepvariant/issues/605:0,safety,Avail,Availability,0,"Availability of DeepVariant RNA-seq GTEx model; Hello DeepVariant team,. I am currently interested in using your software for variant calling from RNA-sequencing data. After reading the [preprint](https://doi.org/10.1101/2022.10.16.512451) I understand that there are two available models for such purpose: ""DV RNA-seq [GTEx]"" and ""DV RNA-seq [GIAB]"". I see that ""DV RNA-seq [GIAB]"" is used in the [RNA-seq case study](https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-rnaseq-case-study.md) and downloadable through the urls provided there, but I have not been able to find urls for the GTEx model. Is it yet to be made available?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/605
https://github.com/google/deepvariant/issues/605:272,safety,avail,available,272,"Availability of DeepVariant RNA-seq GTEx model; Hello DeepVariant team,. I am currently interested in using your software for variant calling from RNA-sequencing data. After reading the [preprint](https://doi.org/10.1101/2022.10.16.512451) I understand that there are two available models for such purpose: ""DV RNA-seq [GTEx]"" and ""DV RNA-seq [GIAB]"". I see that ""DV RNA-seq [GIAB]"" is used in the [RNA-seq case study](https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-rnaseq-case-study.md) and downloadable through the urls provided there, but I have not been able to find urls for the GTEx model. Is it yet to be made available?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/605
https://github.com/google/deepvariant/issues/605:635,safety,avail,available,635,"Availability of DeepVariant RNA-seq GTEx model; Hello DeepVariant team,. I am currently interested in using your software for variant calling from RNA-sequencing data. After reading the [preprint](https://doi.org/10.1101/2022.10.16.512451) I understand that there are two available models for such purpose: ""DV RNA-seq [GTEx]"" and ""DV RNA-seq [GIAB]"". I see that ""DV RNA-seq [GIAB]"" is used in the [RNA-seq case study](https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-rnaseq-case-study.md) and downloadable through the urls provided there, but I have not been able to find urls for the GTEx model. Is it yet to be made available?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/605
https://github.com/google/deepvariant/issues/605:0,security,Availab,Availability,0,"Availability of DeepVariant RNA-seq GTEx model; Hello DeepVariant team,. I am currently interested in using your software for variant calling from RNA-sequencing data. After reading the [preprint](https://doi.org/10.1101/2022.10.16.512451) I understand that there are two available models for such purpose: ""DV RNA-seq [GTEx]"" and ""DV RNA-seq [GIAB]"". I see that ""DV RNA-seq [GIAB]"" is used in the [RNA-seq case study](https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-rnaseq-case-study.md) and downloadable through the urls provided there, but I have not been able to find urls for the GTEx model. Is it yet to be made available?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/605
https://github.com/google/deepvariant/issues/605:41,security,model,model,41,"Availability of DeepVariant RNA-seq GTEx model; Hello DeepVariant team,. I am currently interested in using your software for variant calling from RNA-sequencing data. After reading the [preprint](https://doi.org/10.1101/2022.10.16.512451) I understand that there are two available models for such purpose: ""DV RNA-seq [GTEx]"" and ""DV RNA-seq [GIAB]"". I see that ""DV RNA-seq [GIAB]"" is used in the [RNA-seq case study](https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-rnaseq-case-study.md) and downloadable through the urls provided there, but I have not been able to find urls for the GTEx model. Is it yet to be made available?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/605
https://github.com/google/deepvariant/issues/605:66,security,team,team,66,"Availability of DeepVariant RNA-seq GTEx model; Hello DeepVariant team,. I am currently interested in using your software for variant calling from RNA-sequencing data. After reading the [preprint](https://doi.org/10.1101/2022.10.16.512451) I understand that there are two available models for such purpose: ""DV RNA-seq [GTEx]"" and ""DV RNA-seq [GIAB]"". I see that ""DV RNA-seq [GIAB]"" is used in the [RNA-seq case study](https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-rnaseq-case-study.md) and downloadable through the urls provided there, but I have not been able to find urls for the GTEx model. Is it yet to be made available?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/605
https://github.com/google/deepvariant/issues/605:272,security,availab,available,272,"Availability of DeepVariant RNA-seq GTEx model; Hello DeepVariant team,. I am currently interested in using your software for variant calling from RNA-sequencing data. After reading the [preprint](https://doi.org/10.1101/2022.10.16.512451) I understand that there are two available models for such purpose: ""DV RNA-seq [GTEx]"" and ""DV RNA-seq [GIAB]"". I see that ""DV RNA-seq [GIAB]"" is used in the [RNA-seq case study](https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-rnaseq-case-study.md) and downloadable through the urls provided there, but I have not been able to find urls for the GTEx model. Is it yet to be made available?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/605
https://github.com/google/deepvariant/issues/605:282,security,model,models,282,"Availability of DeepVariant RNA-seq GTEx model; Hello DeepVariant team,. I am currently interested in using your software for variant calling from RNA-sequencing data. After reading the [preprint](https://doi.org/10.1101/2022.10.16.512451) I understand that there are two available models for such purpose: ""DV RNA-seq [GTEx]"" and ""DV RNA-seq [GIAB]"". I see that ""DV RNA-seq [GIAB]"" is used in the [RNA-seq case study](https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-rnaseq-case-study.md) and downloadable through the urls provided there, but I have not been able to find urls for the GTEx model. Is it yet to be made available?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/605
https://github.com/google/deepvariant/issues/605:607,security,model,model,607,"Availability of DeepVariant RNA-seq GTEx model; Hello DeepVariant team,. I am currently interested in using your software for variant calling from RNA-sequencing data. After reading the [preprint](https://doi.org/10.1101/2022.10.16.512451) I understand that there are two available models for such purpose: ""DV RNA-seq [GTEx]"" and ""DV RNA-seq [GIAB]"". I see that ""DV RNA-seq [GIAB]"" is used in the [RNA-seq case study](https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-rnaseq-case-study.md) and downloadable through the urls provided there, but I have not been able to find urls for the GTEx model. Is it yet to be made available?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/605
https://github.com/google/deepvariant/issues/605:635,security,availab,available,635,"Availability of DeepVariant RNA-seq GTEx model; Hello DeepVariant team,. I am currently interested in using your software for variant calling from RNA-sequencing data. After reading the [preprint](https://doi.org/10.1101/2022.10.16.512451) I understand that there are two available models for such purpose: ""DV RNA-seq [GTEx]"" and ""DV RNA-seq [GIAB]"". I see that ""DV RNA-seq [GIAB]"" is used in the [RNA-seq case study](https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-rnaseq-case-study.md) and downloadable through the urls provided there, but I have not been able to find urls for the GTEx model. Is it yet to be made available?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/605
https://github.com/google/deepvariant/issues/605:242,testability,understand,understand,242,"Availability of DeepVariant RNA-seq GTEx model; Hello DeepVariant team,. I am currently interested in using your software for variant calling from RNA-sequencing data. After reading the [preprint](https://doi.org/10.1101/2022.10.16.512451) I understand that there are two available models for such purpose: ""DV RNA-seq [GTEx]"" and ""DV RNA-seq [GIAB]"". I see that ""DV RNA-seq [GIAB]"" is used in the [RNA-seq case study](https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-rnaseq-case-study.md) and downloadable through the urls provided there, but I have not been able to find urls for the GTEx model. Is it yet to be made available?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/605
https://github.com/google/deepvariant/issues/606:8,availability,error,error,8,"Returns error while evaluating the model on Arabidopsis Thailand Sample; I tried to run the model on one of the Arabidopsis thaliana sample from 1001 genomes website. Without reporting any errors, the model ran perfectly fine (make_examples, call_variants, postprocess_variants). But when evaluating the results against the ground truth on all chromosomes, it returns error stating that records at specific positions disagree on what the reference bases should be. I attached the screenshot for reference to give you an idea. . <img width=""1499"" alt=""Screenshot 2023-01-25 at 12 32 33"" src=""https://user-images.githubusercontent.com/75676816/214554128-84fd25af-90b4-4110-be27-c2344a840bc4.png"">.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/606
https://github.com/google/deepvariant/issues/606:189,availability,error,errors,189,"Returns error while evaluating the model on Arabidopsis Thailand Sample; I tried to run the model on one of the Arabidopsis thaliana sample from 1001 genomes website. Without reporting any errors, the model ran perfectly fine (make_examples, call_variants, postprocess_variants). But when evaluating the results against the ground truth on all chromosomes, it returns error stating that records at specific positions disagree on what the reference bases should be. I attached the screenshot for reference to give you an idea. . <img width=""1499"" alt=""Screenshot 2023-01-25 at 12 32 33"" src=""https://user-images.githubusercontent.com/75676816/214554128-84fd25af-90b4-4110-be27-c2344a840bc4.png"">.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/606
https://github.com/google/deepvariant/issues/606:368,availability,error,error,368,"Returns error while evaluating the model on Arabidopsis Thailand Sample; I tried to run the model on one of the Arabidopsis thaliana sample from 1001 genomes website. Without reporting any errors, the model ran perfectly fine (make_examples, call_variants, postprocess_variants). But when evaluating the results against the ground truth on all chromosomes, it returns error stating that records at specific positions disagree on what the reference bases should be. I attached the screenshot for reference to give you an idea. . <img width=""1499"" alt=""Screenshot 2023-01-25 at 12 32 33"" src=""https://user-images.githubusercontent.com/75676816/214554128-84fd25af-90b4-4110-be27-c2344a840bc4.png"">.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/606
https://github.com/google/deepvariant/issues/606:35,energy efficiency,model,model,35,"Returns error while evaluating the model on Arabidopsis Thailand Sample; I tried to run the model on one of the Arabidopsis thaliana sample from 1001 genomes website. Without reporting any errors, the model ran perfectly fine (make_examples, call_variants, postprocess_variants). But when evaluating the results against the ground truth on all chromosomes, it returns error stating that records at specific positions disagree on what the reference bases should be. I attached the screenshot for reference to give you an idea. . <img width=""1499"" alt=""Screenshot 2023-01-25 at 12 32 33"" src=""https://user-images.githubusercontent.com/75676816/214554128-84fd25af-90b4-4110-be27-c2344a840bc4.png"">.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/606
https://github.com/google/deepvariant/issues/606:92,energy efficiency,model,model,92,"Returns error while evaluating the model on Arabidopsis Thailand Sample; I tried to run the model on one of the Arabidopsis thaliana sample from 1001 genomes website. Without reporting any errors, the model ran perfectly fine (make_examples, call_variants, postprocess_variants). But when evaluating the results against the ground truth on all chromosomes, it returns error stating that records at specific positions disagree on what the reference bases should be. I attached the screenshot for reference to give you an idea. . <img width=""1499"" alt=""Screenshot 2023-01-25 at 12 32 33"" src=""https://user-images.githubusercontent.com/75676816/214554128-84fd25af-90b4-4110-be27-c2344a840bc4.png"">.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/606
https://github.com/google/deepvariant/issues/606:201,energy efficiency,model,model,201,"Returns error while evaluating the model on Arabidopsis Thailand Sample; I tried to run the model on one of the Arabidopsis thaliana sample from 1001 genomes website. Without reporting any errors, the model ran perfectly fine (make_examples, call_variants, postprocess_variants). But when evaluating the results against the ground truth on all chromosomes, it returns error stating that records at specific positions disagree on what the reference bases should be. I attached the screenshot for reference to give you an idea. . <img width=""1499"" alt=""Screenshot 2023-01-25 at 12 32 33"" src=""https://user-images.githubusercontent.com/75676816/214554128-84fd25af-90b4-4110-be27-c2344a840bc4.png"">.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/606
https://github.com/google/deepvariant/issues/606:398,interoperability,specif,specific,398,"Returns error while evaluating the model on Arabidopsis Thailand Sample; I tried to run the model on one of the Arabidopsis thaliana sample from 1001 genomes website. Without reporting any errors, the model ran perfectly fine (make_examples, call_variants, postprocess_variants). But when evaluating the results against the ground truth on all chromosomes, it returns error stating that records at specific positions disagree on what the reference bases should be. I attached the screenshot for reference to give you an idea. . <img width=""1499"" alt=""Screenshot 2023-01-25 at 12 32 33"" src=""https://user-images.githubusercontent.com/75676816/214554128-84fd25af-90b4-4110-be27-c2344a840bc4.png"">.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/606
https://github.com/google/deepvariant/issues/606:8,performance,error,error,8,"Returns error while evaluating the model on Arabidopsis Thailand Sample; I tried to run the model on one of the Arabidopsis thaliana sample from 1001 genomes website. Without reporting any errors, the model ran perfectly fine (make_examples, call_variants, postprocess_variants). But when evaluating the results against the ground truth on all chromosomes, it returns error stating that records at specific positions disagree on what the reference bases should be. I attached the screenshot for reference to give you an idea. . <img width=""1499"" alt=""Screenshot 2023-01-25 at 12 32 33"" src=""https://user-images.githubusercontent.com/75676816/214554128-84fd25af-90b4-4110-be27-c2344a840bc4.png"">.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/606
https://github.com/google/deepvariant/issues/606:189,performance,error,errors,189,"Returns error while evaluating the model on Arabidopsis Thailand Sample; I tried to run the model on one of the Arabidopsis thaliana sample from 1001 genomes website. Without reporting any errors, the model ran perfectly fine (make_examples, call_variants, postprocess_variants). But when evaluating the results against the ground truth on all chromosomes, it returns error stating that records at specific positions disagree on what the reference bases should be. I attached the screenshot for reference to give you an idea. . <img width=""1499"" alt=""Screenshot 2023-01-25 at 12 32 33"" src=""https://user-images.githubusercontent.com/75676816/214554128-84fd25af-90b4-4110-be27-c2344a840bc4.png"">.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/606
https://github.com/google/deepvariant/issues/606:368,performance,error,error,368,"Returns error while evaluating the model on Arabidopsis Thailand Sample; I tried to run the model on one of the Arabidopsis thaliana sample from 1001 genomes website. Without reporting any errors, the model ran perfectly fine (make_examples, call_variants, postprocess_variants). But when evaluating the results against the ground truth on all chromosomes, it returns error stating that records at specific positions disagree on what the reference bases should be. I attached the screenshot for reference to give you an idea. . <img width=""1499"" alt=""Screenshot 2023-01-25 at 12 32 33"" src=""https://user-images.githubusercontent.com/75676816/214554128-84fd25af-90b4-4110-be27-c2344a840bc4.png"">.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/606
https://github.com/google/deepvariant/issues/606:8,safety,error,error,8,"Returns error while evaluating the model on Arabidopsis Thailand Sample; I tried to run the model on one of the Arabidopsis thaliana sample from 1001 genomes website. Without reporting any errors, the model ran perfectly fine (make_examples, call_variants, postprocess_variants). But when evaluating the results against the ground truth on all chromosomes, it returns error stating that records at specific positions disagree on what the reference bases should be. I attached the screenshot for reference to give you an idea. . <img width=""1499"" alt=""Screenshot 2023-01-25 at 12 32 33"" src=""https://user-images.githubusercontent.com/75676816/214554128-84fd25af-90b4-4110-be27-c2344a840bc4.png"">.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/606
https://github.com/google/deepvariant/issues/606:189,safety,error,errors,189,"Returns error while evaluating the model on Arabidopsis Thailand Sample; I tried to run the model on one of the Arabidopsis thaliana sample from 1001 genomes website. Without reporting any errors, the model ran perfectly fine (make_examples, call_variants, postprocess_variants). But when evaluating the results against the ground truth on all chromosomes, it returns error stating that records at specific positions disagree on what the reference bases should be. I attached the screenshot for reference to give you an idea. . <img width=""1499"" alt=""Screenshot 2023-01-25 at 12 32 33"" src=""https://user-images.githubusercontent.com/75676816/214554128-84fd25af-90b4-4110-be27-c2344a840bc4.png"">.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/606
https://github.com/google/deepvariant/issues/606:368,safety,error,error,368,"Returns error while evaluating the model on Arabidopsis Thailand Sample; I tried to run the model on one of the Arabidopsis thaliana sample from 1001 genomes website. Without reporting any errors, the model ran perfectly fine (make_examples, call_variants, postprocess_variants). But when evaluating the results against the ground truth on all chromosomes, it returns error stating that records at specific positions disagree on what the reference bases should be. I attached the screenshot for reference to give you an idea. . <img width=""1499"" alt=""Screenshot 2023-01-25 at 12 32 33"" src=""https://user-images.githubusercontent.com/75676816/214554128-84fd25af-90b4-4110-be27-c2344a840bc4.png"">.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/606
https://github.com/google/deepvariant/issues/606:35,security,model,model,35,"Returns error while evaluating the model on Arabidopsis Thailand Sample; I tried to run the model on one of the Arabidopsis thaliana sample from 1001 genomes website. Without reporting any errors, the model ran perfectly fine (make_examples, call_variants, postprocess_variants). But when evaluating the results against the ground truth on all chromosomes, it returns error stating that records at specific positions disagree on what the reference bases should be. I attached the screenshot for reference to give you an idea. . <img width=""1499"" alt=""Screenshot 2023-01-25 at 12 32 33"" src=""https://user-images.githubusercontent.com/75676816/214554128-84fd25af-90b4-4110-be27-c2344a840bc4.png"">.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/606
https://github.com/google/deepvariant/issues/606:92,security,model,model,92,"Returns error while evaluating the model on Arabidopsis Thailand Sample; I tried to run the model on one of the Arabidopsis thaliana sample from 1001 genomes website. Without reporting any errors, the model ran perfectly fine (make_examples, call_variants, postprocess_variants). But when evaluating the results against the ground truth on all chromosomes, it returns error stating that records at specific positions disagree on what the reference bases should be. I attached the screenshot for reference to give you an idea. . <img width=""1499"" alt=""Screenshot 2023-01-25 at 12 32 33"" src=""https://user-images.githubusercontent.com/75676816/214554128-84fd25af-90b4-4110-be27-c2344a840bc4.png"">.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/606
https://github.com/google/deepvariant/issues/606:201,security,model,model,201,"Returns error while evaluating the model on Arabidopsis Thailand Sample; I tried to run the model on one of the Arabidopsis thaliana sample from 1001 genomes website. Without reporting any errors, the model ran perfectly fine (make_examples, call_variants, postprocess_variants). But when evaluating the results against the ground truth on all chromosomes, it returns error stating that records at specific positions disagree on what the reference bases should be. I attached the screenshot for reference to give you an idea. . <img width=""1499"" alt=""Screenshot 2023-01-25 at 12 32 33"" src=""https://user-images.githubusercontent.com/75676816/214554128-84fd25af-90b4-4110-be27-c2344a840bc4.png"">.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/606
https://github.com/google/deepvariant/issues/606:8,usability,error,error,8,"Returns error while evaluating the model on Arabidopsis Thailand Sample; I tried to run the model on one of the Arabidopsis thaliana sample from 1001 genomes website. Without reporting any errors, the model ran perfectly fine (make_examples, call_variants, postprocess_variants). But when evaluating the results against the ground truth on all chromosomes, it returns error stating that records at specific positions disagree on what the reference bases should be. I attached the screenshot for reference to give you an idea. . <img width=""1499"" alt=""Screenshot 2023-01-25 at 12 32 33"" src=""https://user-images.githubusercontent.com/75676816/214554128-84fd25af-90b4-4110-be27-c2344a840bc4.png"">.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/606
https://github.com/google/deepvariant/issues/606:189,usability,error,errors,189,"Returns error while evaluating the model on Arabidopsis Thailand Sample; I tried to run the model on one of the Arabidopsis thaliana sample from 1001 genomes website. Without reporting any errors, the model ran perfectly fine (make_examples, call_variants, postprocess_variants). But when evaluating the results against the ground truth on all chromosomes, it returns error stating that records at specific positions disagree on what the reference bases should be. I attached the screenshot for reference to give you an idea. . <img width=""1499"" alt=""Screenshot 2023-01-25 at 12 32 33"" src=""https://user-images.githubusercontent.com/75676816/214554128-84fd25af-90b4-4110-be27-c2344a840bc4.png"">.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/606
https://github.com/google/deepvariant/issues/606:368,usability,error,error,368,"Returns error while evaluating the model on Arabidopsis Thailand Sample; I tried to run the model on one of the Arabidopsis thaliana sample from 1001 genomes website. Without reporting any errors, the model ran perfectly fine (make_examples, call_variants, postprocess_variants). But when evaluating the results against the ground truth on all chromosomes, it returns error stating that records at specific positions disagree on what the reference bases should be. I attached the screenshot for reference to give you an idea. . <img width=""1499"" alt=""Screenshot 2023-01-25 at 12 32 33"" src=""https://user-images.githubusercontent.com/75676816/214554128-84fd25af-90b4-4110-be27-c2344a840bc4.png"">.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/606
https://github.com/google/deepvariant/issues/606:599,usability,user,user-images,599,"Returns error while evaluating the model on Arabidopsis Thailand Sample; I tried to run the model on one of the Arabidopsis thaliana sample from 1001 genomes website. Without reporting any errors, the model ran perfectly fine (make_examples, call_variants, postprocess_variants). But when evaluating the results against the ground truth on all chromosomes, it returns error stating that records at specific positions disagree on what the reference bases should be. I attached the screenshot for reference to give you an idea. . <img width=""1499"" alt=""Screenshot 2023-01-25 at 12 32 33"" src=""https://user-images.githubusercontent.com/75676816/214554128-84fd25af-90b4-4110-be27-c2344a840bc4.png"">.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/606
https://github.com/google/deepvariant/issues/607:12,deployability,build,build,12,"DeepVariant build fails with Tensorflow 2.11; Hello,. I am trying to build DeepVaraint with Tensorflow 2.11.0. I modified ""settings.h"" and ""build-prereq.sh"" to initialize the respective version of bazel and other dependencies. However, I am not able to build successfully. . Could you please guide me towards upgrading Tensorflow to 2.11.0 and any required modifications to the source code? . Thanks,. Saurabh . .",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/607
https://github.com/google/deepvariant/issues/607:18,deployability,fail,fails,18,"DeepVariant build fails with Tensorflow 2.11; Hello,. I am trying to build DeepVaraint with Tensorflow 2.11.0. I modified ""settings.h"" and ""build-prereq.sh"" to initialize the respective version of bazel and other dependencies. However, I am not able to build successfully. . Could you please guide me towards upgrading Tensorflow to 2.11.0 and any required modifications to the source code? . Thanks,. Saurabh . .",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/607
https://github.com/google/deepvariant/issues/607:69,deployability,build,build,69,"DeepVariant build fails with Tensorflow 2.11; Hello,. I am trying to build DeepVaraint with Tensorflow 2.11.0. I modified ""settings.h"" and ""build-prereq.sh"" to initialize the respective version of bazel and other dependencies. However, I am not able to build successfully. . Could you please guide me towards upgrading Tensorflow to 2.11.0 and any required modifications to the source code? . Thanks,. Saurabh . .",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/607
https://github.com/google/deepvariant/issues/607:140,deployability,build,build-prereq,140,"DeepVariant build fails with Tensorflow 2.11; Hello,. I am trying to build DeepVaraint with Tensorflow 2.11.0. I modified ""settings.h"" and ""build-prereq.sh"" to initialize the respective version of bazel and other dependencies. However, I am not able to build successfully. . Could you please guide me towards upgrading Tensorflow to 2.11.0 and any required modifications to the source code? . Thanks,. Saurabh . .",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/607
https://github.com/google/deepvariant/issues/607:186,deployability,version,version,186,"DeepVariant build fails with Tensorflow 2.11; Hello,. I am trying to build DeepVaraint with Tensorflow 2.11.0. I modified ""settings.h"" and ""build-prereq.sh"" to initialize the respective version of bazel and other dependencies. However, I am not able to build successfully. . Could you please guide me towards upgrading Tensorflow to 2.11.0 and any required modifications to the source code? . Thanks,. Saurabh . .",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/607
https://github.com/google/deepvariant/issues/607:213,deployability,depend,dependencies,213,"DeepVariant build fails with Tensorflow 2.11; Hello,. I am trying to build DeepVaraint with Tensorflow 2.11.0. I modified ""settings.h"" and ""build-prereq.sh"" to initialize the respective version of bazel and other dependencies. However, I am not able to build successfully. . Could you please guide me towards upgrading Tensorflow to 2.11.0 and any required modifications to the source code? . Thanks,. Saurabh . .",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/607
https://github.com/google/deepvariant/issues/607:253,deployability,build,build,253,"DeepVariant build fails with Tensorflow 2.11; Hello,. I am trying to build DeepVaraint with Tensorflow 2.11.0. I modified ""settings.h"" and ""build-prereq.sh"" to initialize the respective version of bazel and other dependencies. However, I am not able to build successfully. . Could you please guide me towards upgrading Tensorflow to 2.11.0 and any required modifications to the source code? . Thanks,. Saurabh . .",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/607
https://github.com/google/deepvariant/issues/607:309,deployability,upgrad,upgrading,309,"DeepVariant build fails with Tensorflow 2.11; Hello,. I am trying to build DeepVaraint with Tensorflow 2.11.0. I modified ""settings.h"" and ""build-prereq.sh"" to initialize the respective version of bazel and other dependencies. However, I am not able to build successfully. . Could you please guide me towards upgrading Tensorflow to 2.11.0 and any required modifications to the source code? . Thanks,. Saurabh . .",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/607
https://github.com/google/deepvariant/issues/607:186,integrability,version,version,186,"DeepVariant build fails with Tensorflow 2.11; Hello,. I am trying to build DeepVaraint with Tensorflow 2.11.0. I modified ""settings.h"" and ""build-prereq.sh"" to initialize the respective version of bazel and other dependencies. However, I am not able to build successfully. . Could you please guide me towards upgrading Tensorflow to 2.11.0 and any required modifications to the source code? . Thanks,. Saurabh . .",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/607
https://github.com/google/deepvariant/issues/607:213,integrability,depend,dependencies,213,"DeepVariant build fails with Tensorflow 2.11; Hello,. I am trying to build DeepVaraint with Tensorflow 2.11.0. I modified ""settings.h"" and ""build-prereq.sh"" to initialize the respective version of bazel and other dependencies. However, I am not able to build successfully. . Could you please guide me towards upgrading Tensorflow to 2.11.0 and any required modifications to the source code? . Thanks,. Saurabh . .",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/607
https://github.com/google/deepvariant/issues/607:186,modifiability,version,version,186,"DeepVariant build fails with Tensorflow 2.11; Hello,. I am trying to build DeepVaraint with Tensorflow 2.11.0. I modified ""settings.h"" and ""build-prereq.sh"" to initialize the respective version of bazel and other dependencies. However, I am not able to build successfully. . Could you please guide me towards upgrading Tensorflow to 2.11.0 and any required modifications to the source code? . Thanks,. Saurabh . .",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/607
https://github.com/google/deepvariant/issues/607:213,modifiability,depend,dependencies,213,"DeepVariant build fails with Tensorflow 2.11; Hello,. I am trying to build DeepVaraint with Tensorflow 2.11.0. I modified ""settings.h"" and ""build-prereq.sh"" to initialize the respective version of bazel and other dependencies. However, I am not able to build successfully. . Could you please guide me towards upgrading Tensorflow to 2.11.0 and any required modifications to the source code? . Thanks,. Saurabh . .",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/607
https://github.com/google/deepvariant/issues/607:309,modifiability,upgrad,upgrading,309,"DeepVariant build fails with Tensorflow 2.11; Hello,. I am trying to build DeepVaraint with Tensorflow 2.11.0. I modified ""settings.h"" and ""build-prereq.sh"" to initialize the respective version of bazel and other dependencies. However, I am not able to build successfully. . Could you please guide me towards upgrading Tensorflow to 2.11.0 and any required modifications to the source code? . Thanks,. Saurabh . .",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/607
https://github.com/google/deepvariant/issues/607:18,reliability,fail,fails,18,"DeepVariant build fails with Tensorflow 2.11; Hello,. I am trying to build DeepVaraint with Tensorflow 2.11.0. I modified ""settings.h"" and ""build-prereq.sh"" to initialize the respective version of bazel and other dependencies. However, I am not able to build successfully. . Could you please guide me towards upgrading Tensorflow to 2.11.0 and any required modifications to the source code? . Thanks,. Saurabh . .",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/607
https://github.com/google/deepvariant/issues/607:213,safety,depend,dependencies,213,"DeepVariant build fails with Tensorflow 2.11; Hello,. I am trying to build DeepVaraint with Tensorflow 2.11.0. I modified ""settings.h"" and ""build-prereq.sh"" to initialize the respective version of bazel and other dependencies. However, I am not able to build successfully. . Could you please guide me towards upgrading Tensorflow to 2.11.0 and any required modifications to the source code? . Thanks,. Saurabh . .",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/607
https://github.com/google/deepvariant/issues/607:113,security,modif,modified,113,"DeepVariant build fails with Tensorflow 2.11; Hello,. I am trying to build DeepVaraint with Tensorflow 2.11.0. I modified ""settings.h"" and ""build-prereq.sh"" to initialize the respective version of bazel and other dependencies. However, I am not able to build successfully. . Could you please guide me towards upgrading Tensorflow to 2.11.0 and any required modifications to the source code? . Thanks,. Saurabh . .",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/607
https://github.com/google/deepvariant/issues/607:357,security,modif,modifications,357,"DeepVariant build fails with Tensorflow 2.11; Hello,. I am trying to build DeepVaraint with Tensorflow 2.11.0. I modified ""settings.h"" and ""build-prereq.sh"" to initialize the respective version of bazel and other dependencies. However, I am not able to build successfully. . Could you please guide me towards upgrading Tensorflow to 2.11.0 and any required modifications to the source code? . Thanks,. Saurabh . .",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/607
https://github.com/google/deepvariant/issues/607:213,testability,depend,dependencies,213,"DeepVariant build fails with Tensorflow 2.11; Hello,. I am trying to build DeepVaraint with Tensorflow 2.11.0. I modified ""settings.h"" and ""build-prereq.sh"" to initialize the respective version of bazel and other dependencies. However, I am not able to build successfully. . Could you please guide me towards upgrading Tensorflow to 2.11.0 and any required modifications to the source code? . Thanks,. Saurabh . .",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/607
https://github.com/google/deepvariant/issues/607:292,usability,guid,guide,292,"DeepVariant build fails with Tensorflow 2.11; Hello,. I am trying to build DeepVaraint with Tensorflow 2.11.0. I modified ""settings.h"" and ""build-prereq.sh"" to initialize the respective version of bazel and other dependencies. However, I am not able to build successfully. . Could you please guide me towards upgrading Tensorflow to 2.11.0 and any required modifications to the source code? . Thanks,. Saurabh . .",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/607
https://github.com/google/deepvariant/issues/608:176,availability,down,download,176,"binary creation fails; bazel unable to fetch tf_runtime; I was able to build the docker image last week, but this week the build fails at binary creation, with bazel unable to download tf_runtime. . - Operating system: Ubuntu20.04. - DeepVariant version: 1.4.0. - Building docker image locally. **Steps to reproduce:**. - Command: `docker build .` in source directory (no modifications). - Error trace: . . ```. #16 1484.7 ========== [Mon Jan 30 21:50:56 UTC 2023] Stage 'build-prereq.sh complete' starting. #16 1484.7 Extracting Bazel installation... #16 1487.8 Starting local Bazel server and connecting to it... #16 1489.8 (21:51:01) WARNING: option '--distinct_host_configuration' was expanded to from both option '--enable_platform_specific_config' (source /opt/tensorflow/.bazelrc) and option '--enable_platform_specific_config' (source /opt/tensorflow/.bazelrc). #16 1489.8 (21:51:01) INFO: Options provided by the client:. #16 1489.8 Inherited 'common' options: --isatty=0 --terminal_columns=80. #16 1489.8 (21:51:01) INFO: Reading rc options for 'build' from /opt/tensorflow/.bazelrc:. #16 1489.8 Inherited 'common' options: --experimental_repo_remote_exec. #16 1489.8 (21:51:01) INFO: Reading rc options for 'build' from /opt/tensorflow/.bazelrc:. #16 1489.8 'build' options: --define framework_shared_object=true --java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --host_java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true --deleted_packages=tensorflow/compiler/mlir/tfrt,tensorflow/compiler/mlir/tfrt/benchmarks,tensorflow/compiler/mlir/tfrt/jit/python_binding,tensorflow/compiler/mlir/tfrt/jit/transforms,tensor",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/608
https://github.com/google/deepvariant/issues/608:201,availability,Operat,Operating,201,"binary creation fails; bazel unable to fetch tf_runtime; I was able to build the docker image last week, but this week the build fails at binary creation, with bazel unable to download tf_runtime. . - Operating system: Ubuntu20.04. - DeepVariant version: 1.4.0. - Building docker image locally. **Steps to reproduce:**. - Command: `docker build .` in source directory (no modifications). - Error trace: . . ```. #16 1484.7 ========== [Mon Jan 30 21:50:56 UTC 2023] Stage 'build-prereq.sh complete' starting. #16 1484.7 Extracting Bazel installation... #16 1487.8 Starting local Bazel server and connecting to it... #16 1489.8 (21:51:01) WARNING: option '--distinct_host_configuration' was expanded to from both option '--enable_platform_specific_config' (source /opt/tensorflow/.bazelrc) and option '--enable_platform_specific_config' (source /opt/tensorflow/.bazelrc). #16 1489.8 (21:51:01) INFO: Options provided by the client:. #16 1489.8 Inherited 'common' options: --isatty=0 --terminal_columns=80. #16 1489.8 (21:51:01) INFO: Reading rc options for 'build' from /opt/tensorflow/.bazelrc:. #16 1489.8 Inherited 'common' options: --experimental_repo_remote_exec. #16 1489.8 (21:51:01) INFO: Reading rc options for 'build' from /opt/tensorflow/.bazelrc:. #16 1489.8 'build' options: --define framework_shared_object=true --java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --host_java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true --deleted_packages=tensorflow/compiler/mlir/tfrt,tensorflow/compiler/mlir/tfrt/benchmarks,tensorflow/compiler/mlir/tfrt/jit/python_binding,tensorflow/compiler/mlir/tfrt/jit/transforms,tensor",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/608
https://github.com/google/deepvariant/issues/608:390,availability,Error,Error,390,"binary creation fails; bazel unable to fetch tf_runtime; I was able to build the docker image last week, but this week the build fails at binary creation, with bazel unable to download tf_runtime. . - Operating system: Ubuntu20.04. - DeepVariant version: 1.4.0. - Building docker image locally. **Steps to reproduce:**. - Command: `docker build .` in source directory (no modifications). - Error trace: . . ```. #16 1484.7 ========== [Mon Jan 30 21:50:56 UTC 2023] Stage 'build-prereq.sh complete' starting. #16 1484.7 Extracting Bazel installation... #16 1487.8 Starting local Bazel server and connecting to it... #16 1489.8 (21:51:01) WARNING: option '--distinct_host_configuration' was expanded to from both option '--enable_platform_specific_config' (source /opt/tensorflow/.bazelrc) and option '--enable_platform_specific_config' (source /opt/tensorflow/.bazelrc). #16 1489.8 (21:51:01) INFO: Options provided by the client:. #16 1489.8 Inherited 'common' options: --isatty=0 --terminal_columns=80. #16 1489.8 (21:51:01) INFO: Reading rc options for 'build' from /opt/tensorflow/.bazelrc:. #16 1489.8 Inherited 'common' options: --experimental_repo_remote_exec. #16 1489.8 (21:51:01) INFO: Reading rc options for 'build' from /opt/tensorflow/.bazelrc:. #16 1489.8 'build' options: --define framework_shared_object=true --java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --host_java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true --deleted_packages=tensorflow/compiler/mlir/tfrt,tensorflow/compiler/mlir/tfrt/benchmarks,tensorflow/compiler/mlir/tfrt/jit/python_binding,tensorflow/compiler/mlir/tfrt/jit/transforms,tensor",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/608
https://github.com/google/deepvariant/issues/608:6050,availability,Down,Download,6050,"2 (21:51:04) Loading: 0 packages loaded. #16 1494.2 (21:51:05) Loading: 0 packages loaded. #16 1495.2 (21:51:06) Loading: 0 packages loaded. #16 1496.2 (21:51:07) Loading: 0 packages loaded. #16 1497.0 (21:51:08) INFO: Repository tf_runtime instantiated at:. #16 1497.0 /opt/deepvariant/WORKSPACE:102:14: in <toplevel>. #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/tensorflow/workspace3.bzl:28:15: in workspace. #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/tf_runtime/workspace.bzl:12:20: in repo. #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:113:21: in tf_http_archive. #16 1497.0 Repository rule _tf_http_archive defined at:. #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:66:35: in <toplevel>. #16 1497.0 (21:51:08) WARNING: Download from http://mirror.tensorflow.org/github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException GET returned 404 Not Found. #16 1497.0 (21:51:08) WARNING: Download from https://github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException Checksum was 8383b3247286016e450b0b20e805d26b88ab4638b4e4e3cc4a6923debaf7ad1e but wanted f16fcf09b34e0c7be9389f50652b4b4a14c5a8a96e7e15ad73e8f234d8d09ebe. #16 1497.0 (21:51:08) ERROR: An error occurred during the fetch of repository 'tf_runtime':. #16 1497.0 Traceback (most recent call last):. #16 1497.0 File ""/root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl"", line 53, column 33, in _tf_http_archive_impl. #16 1497.0 ctx.download_and_extract(. #16",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/608
https://github.com/google/deepvariant/issues/608:6240,availability,down,downloader,6240,"#16 1497.0 (21:51:08) INFO: Repository tf_runtime instantiated at:. #16 1497.0 /opt/deepvariant/WORKSPACE:102:14: in <toplevel>. #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/tensorflow/workspace3.bzl:28:15: in workspace. #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/tf_runtime/workspace.bzl:12:20: in repo. #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:113:21: in tf_http_archive. #16 1497.0 Repository rule _tf_http_archive defined at:. #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:66:35: in <toplevel>. #16 1497.0 (21:51:08) WARNING: Download from http://mirror.tensorflow.org/github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException GET returned 404 Not Found. #16 1497.0 (21:51:08) WARNING: Download from https://github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException Checksum was 8383b3247286016e450b0b20e805d26b88ab4638b4e4e3cc4a6923debaf7ad1e but wanted f16fcf09b34e0c7be9389f50652b4b4a14c5a8a96e7e15ad73e8f234d8d09ebe. #16 1497.0 (21:51:08) ERROR: An error occurred during the fetch of repository 'tf_runtime':. #16 1497.0 Traceback (most recent call last):. #16 1497.0 File ""/root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl"", line 53, column 33, in _tf_http_archive_impl. #16 1497.0 ctx.download_and_extract(. #16 1497.0 Error in download_and_extract: java.io.IOException: Error downloading [http://mirror.tensorflow.org/github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/608
https://github.com/google/deepvariant/issues/608:6337,availability,Down,Download,6337,"WORKSPACE:102:14: in <toplevel>. #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/tensorflow/workspace3.bzl:28:15: in workspace. #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/tf_runtime/workspace.bzl:12:20: in repo. #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:113:21: in tf_http_archive. #16 1497.0 Repository rule _tf_http_archive defined at:. #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:66:35: in <toplevel>. #16 1497.0 (21:51:08) WARNING: Download from http://mirror.tensorflow.org/github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException GET returned 404 Not Found. #16 1497.0 (21:51:08) WARNING: Download from https://github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException Checksum was 8383b3247286016e450b0b20e805d26b88ab4638b4e4e3cc4a6923debaf7ad1e but wanted f16fcf09b34e0c7be9389f50652b4b4a14c5a8a96e7e15ad73e8f234d8d09ebe. #16 1497.0 (21:51:08) ERROR: An error occurred during the fetch of repository 'tf_runtime':. #16 1497.0 Traceback (most recent call last):. #16 1497.0 File ""/root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl"", line 53, column 33, in _tf_http_archive_impl. #16 1497.0 ctx.download_and_extract(. #16 1497.0 Error in download_and_extract: java.io.IOException: Error downloading [http://mirror.tensorflow.org/github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz, https://github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.g",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/608
https://github.com/google/deepvariant/issues/608:6506,availability,down,downloader,6506,"rkspace. #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/tf_runtime/workspace.bzl:12:20: in repo. #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:113:21: in tf_http_archive. #16 1497.0 Repository rule _tf_http_archive defined at:. #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:66:35: in <toplevel>. #16 1497.0 (21:51:08) WARNING: Download from http://mirror.tensorflow.org/github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException GET returned 404 Not Found. #16 1497.0 (21:51:08) WARNING: Download from https://github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException Checksum was 8383b3247286016e450b0b20e805d26b88ab4638b4e4e3cc4a6923debaf7ad1e but wanted f16fcf09b34e0c7be9389f50652b4b4a14c5a8a96e7e15ad73e8f234d8d09ebe. #16 1497.0 (21:51:08) ERROR: An error occurred during the fetch of repository 'tf_runtime':. #16 1497.0 Traceback (most recent call last):. #16 1497.0 File ""/root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl"", line 53, column 33, in _tf_http_archive_impl. #16 1497.0 ctx.download_and_extract(. #16 1497.0 Error in download_and_extract: java.io.IOException: Error downloading [http://mirror.tensorflow.org/github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz, https://github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz] to /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/tf_runtime/temp12516918929418979294/64c92c8013b557087351c91b5423b6046d10f206.tar.gz: Checks",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/608
https://github.com/google/deepvariant/issues/608:6721,availability,ERROR,ERROR,6721,"c1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:113:21: in tf_http_archive. #16 1497.0 Repository rule _tf_http_archive defined at:. #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:66:35: in <toplevel>. #16 1497.0 (21:51:08) WARNING: Download from http://mirror.tensorflow.org/github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException GET returned 404 Not Found. #16 1497.0 (21:51:08) WARNING: Download from https://github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException Checksum was 8383b3247286016e450b0b20e805d26b88ab4638b4e4e3cc4a6923debaf7ad1e but wanted f16fcf09b34e0c7be9389f50652b4b4a14c5a8a96e7e15ad73e8f234d8d09ebe. #16 1497.0 (21:51:08) ERROR: An error occurred during the fetch of repository 'tf_runtime':. #16 1497.0 Traceback (most recent call last):. #16 1497.0 File ""/root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl"", line 53, column 33, in _tf_http_archive_impl. #16 1497.0 ctx.download_and_extract(. #16 1497.0 Error in download_and_extract: java.io.IOException: Error downloading [http://mirror.tensorflow.org/github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz, https://github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz] to /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/tf_runtime/temp12516918929418979294/64c92c8013b557087351c91b5423b6046d10f206.tar.gz: Checksum was 8383b3247286016e450b0b20e805d26b88ab4638b4e4e3cc4a6923debaf7ad1e but wanted f16fcf09b34e0c7be9389f50652b4b4a14c5a8a96e7e15ad73e8f234d8d09ebe. #16 1497.1 (21:51:08) INFO: Repository llvm-raw instantiated at:",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/608
https://github.com/google/deepvariant/issues/608:6731,availability,error,error,6731,"3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:113:21: in tf_http_archive. #16 1497.0 Repository rule _tf_http_archive defined at:. #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:66:35: in <toplevel>. #16 1497.0 (21:51:08) WARNING: Download from http://mirror.tensorflow.org/github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException GET returned 404 Not Found. #16 1497.0 (21:51:08) WARNING: Download from https://github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException Checksum was 8383b3247286016e450b0b20e805d26b88ab4638b4e4e3cc4a6923debaf7ad1e but wanted f16fcf09b34e0c7be9389f50652b4b4a14c5a8a96e7e15ad73e8f234d8d09ebe. #16 1497.0 (21:51:08) ERROR: An error occurred during the fetch of repository 'tf_runtime':. #16 1497.0 Traceback (most recent call last):. #16 1497.0 File ""/root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl"", line 53, column 33, in _tf_http_archive_impl. #16 1497.0 ctx.download_and_extract(. #16 1497.0 Error in download_and_extract: java.io.IOException: Error downloading [http://mirror.tensorflow.org/github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz, https://github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz] to /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/tf_runtime/temp12516918929418979294/64c92c8013b557087351c91b5423b6046d10f206.tar.gz: Checksum was 8383b3247286016e450b0b20e805d26b88ab4638b4e4e3cc4a6923debaf7ad1e but wanted f16fcf09b34e0c7be9389f50652b4b4a14c5a8a96e7e15ad73e8f234d8d09ebe. #16 1497.1 (21:51:08) INFO: Repository llvm-raw instantiated at:. #16 1497",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/608
https://github.com/google/deepvariant/issues/608:7062,availability,Error,Error,7062,"ttp://mirror.tensorflow.org/github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException GET returned 404 Not Found. #16 1497.0 (21:51:08) WARNING: Download from https://github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException Checksum was 8383b3247286016e450b0b20e805d26b88ab4638b4e4e3cc4a6923debaf7ad1e but wanted f16fcf09b34e0c7be9389f50652b4b4a14c5a8a96e7e15ad73e8f234d8d09ebe. #16 1497.0 (21:51:08) ERROR: An error occurred during the fetch of repository 'tf_runtime':. #16 1497.0 Traceback (most recent call last):. #16 1497.0 File ""/root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl"", line 53, column 33, in _tf_http_archive_impl. #16 1497.0 ctx.download_and_extract(. #16 1497.0 Error in download_and_extract: java.io.IOException: Error downloading [http://mirror.tensorflow.org/github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz, https://github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz] to /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/tf_runtime/temp12516918929418979294/64c92c8013b557087351c91b5423b6046d10f206.tar.gz: Checksum was 8383b3247286016e450b0b20e805d26b88ab4638b4e4e3cc4a6923debaf7ad1e but wanted f16fcf09b34e0c7be9389f50652b4b4a14c5a8a96e7e15ad73e8f234d8d09ebe. #16 1497.1 (21:51:08) INFO: Repository llvm-raw instantiated at:. #16 1497.1 /opt/deepvariant/WORKSPACE:102:14: in <toplevel>. #16 1497.1 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/tensorflow/workspace3.bzl:42:9: in workspace. #16 1497.1 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/llvm/workspace.bzl:10:",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/608
https://github.com/google/deepvariant/issues/608:7114,availability,Error,Error,7114,"ntime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException GET returned 404 Not Found. #16 1497.0 (21:51:08) WARNING: Download from https://github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException Checksum was 8383b3247286016e450b0b20e805d26b88ab4638b4e4e3cc4a6923debaf7ad1e but wanted f16fcf09b34e0c7be9389f50652b4b4a14c5a8a96e7e15ad73e8f234d8d09ebe. #16 1497.0 (21:51:08) ERROR: An error occurred during the fetch of repository 'tf_runtime':. #16 1497.0 Traceback (most recent call last):. #16 1497.0 File ""/root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl"", line 53, column 33, in _tf_http_archive_impl. #16 1497.0 ctx.download_and_extract(. #16 1497.0 Error in download_and_extract: java.io.IOException: Error downloading [http://mirror.tensorflow.org/github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz, https://github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz] to /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/tf_runtime/temp12516918929418979294/64c92c8013b557087351c91b5423b6046d10f206.tar.gz: Checksum was 8383b3247286016e450b0b20e805d26b88ab4638b4e4e3cc4a6923debaf7ad1e but wanted f16fcf09b34e0c7be9389f50652b4b4a14c5a8a96e7e15ad73e8f234d8d09ebe. #16 1497.1 (21:51:08) INFO: Repository llvm-raw instantiated at:. #16 1497.1 /opt/deepvariant/WORKSPACE:102:14: in <toplevel>. #16 1497.1 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/tensorflow/workspace3.bzl:42:9: in workspace. #16 1497.1 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/llvm/workspace.bzl:10:20: in repo. #16 1497.1 /root/.cache/bazel/_bazel_ro",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/608
https://github.com/google/deepvariant/issues/608:7120,availability,down,downloading,7120,"hive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException GET returned 404 Not Found. #16 1497.0 (21:51:08) WARNING: Download from https://github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException Checksum was 8383b3247286016e450b0b20e805d26b88ab4638b4e4e3cc4a6923debaf7ad1e but wanted f16fcf09b34e0c7be9389f50652b4b4a14c5a8a96e7e15ad73e8f234d8d09ebe. #16 1497.0 (21:51:08) ERROR: An error occurred during the fetch of repository 'tf_runtime':. #16 1497.0 Traceback (most recent call last):. #16 1497.0 File ""/root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl"", line 53, column 33, in _tf_http_archive_impl. #16 1497.0 ctx.download_and_extract(. #16 1497.0 Error in download_and_extract: java.io.IOException: Error downloading [http://mirror.tensorflow.org/github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz, https://github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz] to /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/tf_runtime/temp12516918929418979294/64c92c8013b557087351c91b5423b6046d10f206.tar.gz: Checksum was 8383b3247286016e450b0b20e805d26b88ab4638b4e4e3cc4a6923debaf7ad1e but wanted f16fcf09b34e0c7be9389f50652b4b4a14c5a8a96e7e15ad73e8f234d8d09ebe. #16 1497.1 (21:51:08) INFO: Repository llvm-raw instantiated at:. #16 1497.1 /opt/deepvariant/WORKSPACE:102:14: in <toplevel>. #16 1497.1 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/tensorflow/workspace3.bzl:42:9: in workspace. #16 1497.1 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/llvm/workspace.bzl:10:20: in repo. #16 1497.1 /root/.cache/bazel/_bazel_root/617054",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/608
https://github.com/google/deepvariant/issues/608:8497,availability,ERROR,ERROR,8497,".gz: Checksum was 8383b3247286016e450b0b20e805d26b88ab4638b4e4e3cc4a6923debaf7ad1e but wanted f16fcf09b34e0c7be9389f50652b4b4a14c5a8a96e7e15ad73e8f234d8d09ebe. #16 1497.1 (21:51:08) INFO: Repository llvm-raw instantiated at:. #16 1497.1 /opt/deepvariant/WORKSPACE:102:14: in <toplevel>. #16 1497.1 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/tensorflow/workspace3.bzl:42:9: in workspace. #16 1497.1 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/llvm/workspace.bzl:10:20: in repo. #16 1497.1 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:113:21: in tf_http_archive. #16 1497.1 Repository rule _tf_http_archive defined at:. #16 1497.1 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:66:35: in <toplevel>. #16 1497.2 (21:51:08) Loading: 0 packages loaded. #16 1497.3 (21:51:08) ERROR: no such package '@tf_runtime//': java.io.IOException: Error downloading [http://mirror.tensorflow.org/github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz, https://github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz] to /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/tf_runtime/temp12516918929418979294/64c92c8013b557087351c91b5423b6046d10f206.tar.gz: Checksum was 8383b3247286016e450b0b20e805d26b88ab4638b4e4e3cc4a6923debaf7ad1e but wanted f16fcf09b34e0c7be9389f50652b4b4a14c5a8a96e7e15ad73e8f234d8d09ebe. #16 1497.3 (21:51:09) INFO: Elapsed time: 12.546s. #16 1497.3 (21:51:09) INFO: 0 processes. #16 1497.3 (21:51:09) FAILED: Build did NOT complete successfully (0 packages loaded). #16 1497.3 (21:51:09) FAILED: Build did NOT complete successfully (0 packages loaded). #16 ERROR: executor failed running [/bin/sh -c ./build-prereq.sh && PATH=""${HOME}/bin:${PATH}"" ./build_release_binaries.sh # PATH ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/608
https://github.com/google/deepvariant/issues/608:8558,availability,Error,Error,8558,"4e3cc4a6923debaf7ad1e but wanted f16fcf09b34e0c7be9389f50652b4b4a14c5a8a96e7e15ad73e8f234d8d09ebe. #16 1497.1 (21:51:08) INFO: Repository llvm-raw instantiated at:. #16 1497.1 /opt/deepvariant/WORKSPACE:102:14: in <toplevel>. #16 1497.1 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/tensorflow/workspace3.bzl:42:9: in workspace. #16 1497.1 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/llvm/workspace.bzl:10:20: in repo. #16 1497.1 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:113:21: in tf_http_archive. #16 1497.1 Repository rule _tf_http_archive defined at:. #16 1497.1 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:66:35: in <toplevel>. #16 1497.2 (21:51:08) Loading: 0 packages loaded. #16 1497.3 (21:51:08) ERROR: no such package '@tf_runtime//': java.io.IOException: Error downloading [http://mirror.tensorflow.org/github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz, https://github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz] to /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/tf_runtime/temp12516918929418979294/64c92c8013b557087351c91b5423b6046d10f206.tar.gz: Checksum was 8383b3247286016e450b0b20e805d26b88ab4638b4e4e3cc4a6923debaf7ad1e but wanted f16fcf09b34e0c7be9389f50652b4b4a14c5a8a96e7e15ad73e8f234d8d09ebe. #16 1497.3 (21:51:09) INFO: Elapsed time: 12.546s. #16 1497.3 (21:51:09) INFO: 0 processes. #16 1497.3 (21:51:09) FAILED: Build did NOT complete successfully (0 packages loaded). #16 1497.3 (21:51:09) FAILED: Build did NOT complete successfully (0 packages loaded). #16 ERROR: executor failed running [/bin/sh -c ./build-prereq.sh && PATH=""${HOME}/bin:${PATH}"" ./build_release_binaries.sh # PATH for bazel]: exit code: 1. ------. > [builder 6/6] RUN ./build",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/608
https://github.com/google/deepvariant/issues/608:8564,availability,down,downloading,8564,"23debaf7ad1e but wanted f16fcf09b34e0c7be9389f50652b4b4a14c5a8a96e7e15ad73e8f234d8d09ebe. #16 1497.1 (21:51:08) INFO: Repository llvm-raw instantiated at:. #16 1497.1 /opt/deepvariant/WORKSPACE:102:14: in <toplevel>. #16 1497.1 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/tensorflow/workspace3.bzl:42:9: in workspace. #16 1497.1 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/llvm/workspace.bzl:10:20: in repo. #16 1497.1 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:113:21: in tf_http_archive. #16 1497.1 Repository rule _tf_http_archive defined at:. #16 1497.1 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:66:35: in <toplevel>. #16 1497.2 (21:51:08) Loading: 0 packages loaded. #16 1497.3 (21:51:08) ERROR: no such package '@tf_runtime//': java.io.IOException: Error downloading [http://mirror.tensorflow.org/github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz, https://github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz] to /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/tf_runtime/temp12516918929418979294/64c92c8013b557087351c91b5423b6046d10f206.tar.gz: Checksum was 8383b3247286016e450b0b20e805d26b88ab4638b4e4e3cc4a6923debaf7ad1e but wanted f16fcf09b34e0c7be9389f50652b4b4a14c5a8a96e7e15ad73e8f234d8d09ebe. #16 1497.3 (21:51:09) INFO: Elapsed time: 12.546s. #16 1497.3 (21:51:09) INFO: 0 processes. #16 1497.3 (21:51:09) FAILED: Build did NOT complete successfully (0 packages loaded). #16 1497.3 (21:51:09) FAILED: Build did NOT complete successfully (0 packages loaded). #16 ERROR: executor failed running [/bin/sh -c ./build-prereq.sh && PATH=""${HOME}/bin:${PATH}"" ./build_release_binaries.sh # PATH for bazel]: exit code: 1. ------. > [builder 6/6] RUN ./build-prereq.s",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/608
https://github.com/google/deepvariant/issues/608:9374,availability,ERROR,ERROR,9374,"80/external/org_tensorflow/tensorflow/workspace3.bzl:42:9: in workspace. #16 1497.1 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/llvm/workspace.bzl:10:20: in repo. #16 1497.1 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:113:21: in tf_http_archive. #16 1497.1 Repository rule _tf_http_archive defined at:. #16 1497.1 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:66:35: in <toplevel>. #16 1497.2 (21:51:08) Loading: 0 packages loaded. #16 1497.3 (21:51:08) ERROR: no such package '@tf_runtime//': java.io.IOException: Error downloading [http://mirror.tensorflow.org/github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz, https://github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz] to /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/tf_runtime/temp12516918929418979294/64c92c8013b557087351c91b5423b6046d10f206.tar.gz: Checksum was 8383b3247286016e450b0b20e805d26b88ab4638b4e4e3cc4a6923debaf7ad1e but wanted f16fcf09b34e0c7be9389f50652b4b4a14c5a8a96e7e15ad73e8f234d8d09ebe. #16 1497.3 (21:51:09) INFO: Elapsed time: 12.546s. #16 1497.3 (21:51:09) INFO: 0 processes. #16 1497.3 (21:51:09) FAILED: Build did NOT complete successfully (0 packages loaded). #16 1497.3 (21:51:09) FAILED: Build did NOT complete successfully (0 packages loaded). #16 ERROR: executor failed running [/bin/sh -c ./build-prereq.sh && PATH=""${HOME}/bin:${PATH}"" ./build_release_binaries.sh # PATH for bazel]: exit code: 1. ------. > [builder 6/6] RUN ./build-prereq.sh && PATH=""${HOME}/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin"" ./build_release_binaries.sh # PATH for bazel:. ------. executor failed running [/bin/sh -c ./build-prereq.sh && PATH=""${HOME}/bin:${PATH}"" ./build_release_binaries.sh # PATH for bazel]: exit code: 1. ```.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/608
https://github.com/google/deepvariant/issues/608:16,deployability,fail,fails,16,"binary creation fails; bazel unable to fetch tf_runtime; I was able to build the docker image last week, but this week the build fails at binary creation, with bazel unable to download tf_runtime. . - Operating system: Ubuntu20.04. - DeepVariant version: 1.4.0. - Building docker image locally. **Steps to reproduce:**. - Command: `docker build .` in source directory (no modifications). - Error trace: . . ```. #16 1484.7 ========== [Mon Jan 30 21:50:56 UTC 2023] Stage 'build-prereq.sh complete' starting. #16 1484.7 Extracting Bazel installation... #16 1487.8 Starting local Bazel server and connecting to it... #16 1489.8 (21:51:01) WARNING: option '--distinct_host_configuration' was expanded to from both option '--enable_platform_specific_config' (source /opt/tensorflow/.bazelrc) and option '--enable_platform_specific_config' (source /opt/tensorflow/.bazelrc). #16 1489.8 (21:51:01) INFO: Options provided by the client:. #16 1489.8 Inherited 'common' options: --isatty=0 --terminal_columns=80. #16 1489.8 (21:51:01) INFO: Reading rc options for 'build' from /opt/tensorflow/.bazelrc:. #16 1489.8 Inherited 'common' options: --experimental_repo_remote_exec. #16 1489.8 (21:51:01) INFO: Reading rc options for 'build' from /opt/tensorflow/.bazelrc:. #16 1489.8 'build' options: --define framework_shared_object=true --java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --host_java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true --deleted_packages=tensorflow/compiler/mlir/tfrt,tensorflow/compiler/mlir/tfrt/benchmarks,tensorflow/compiler/mlir/tfrt/jit/python_binding,tensorflow/compiler/mlir/tfrt/jit/transforms,tensor",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/608
https://github.com/google/deepvariant/issues/608:71,deployability,build,build,71,"binary creation fails; bazel unable to fetch tf_runtime; I was able to build the docker image last week, but this week the build fails at binary creation, with bazel unable to download tf_runtime. . - Operating system: Ubuntu20.04. - DeepVariant version: 1.4.0. - Building docker image locally. **Steps to reproduce:**. - Command: `docker build .` in source directory (no modifications). - Error trace: . . ```. #16 1484.7 ========== [Mon Jan 30 21:50:56 UTC 2023] Stage 'build-prereq.sh complete' starting. #16 1484.7 Extracting Bazel installation... #16 1487.8 Starting local Bazel server and connecting to it... #16 1489.8 (21:51:01) WARNING: option '--distinct_host_configuration' was expanded to from both option '--enable_platform_specific_config' (source /opt/tensorflow/.bazelrc) and option '--enable_platform_specific_config' (source /opt/tensorflow/.bazelrc). #16 1489.8 (21:51:01) INFO: Options provided by the client:. #16 1489.8 Inherited 'common' options: --isatty=0 --terminal_columns=80. #16 1489.8 (21:51:01) INFO: Reading rc options for 'build' from /opt/tensorflow/.bazelrc:. #16 1489.8 Inherited 'common' options: --experimental_repo_remote_exec. #16 1489.8 (21:51:01) INFO: Reading rc options for 'build' from /opt/tensorflow/.bazelrc:. #16 1489.8 'build' options: --define framework_shared_object=true --java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --host_java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true --deleted_packages=tensorflow/compiler/mlir/tfrt,tensorflow/compiler/mlir/tfrt/benchmarks,tensorflow/compiler/mlir/tfrt/jit/python_binding,tensorflow/compiler/mlir/tfrt/jit/transforms,tensor",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/608
https://github.com/google/deepvariant/issues/608:123,deployability,build,build,123,"binary creation fails; bazel unable to fetch tf_runtime; I was able to build the docker image last week, but this week the build fails at binary creation, with bazel unable to download tf_runtime. . - Operating system: Ubuntu20.04. - DeepVariant version: 1.4.0. - Building docker image locally. **Steps to reproduce:**. - Command: `docker build .` in source directory (no modifications). - Error trace: . . ```. #16 1484.7 ========== [Mon Jan 30 21:50:56 UTC 2023] Stage 'build-prereq.sh complete' starting. #16 1484.7 Extracting Bazel installation... #16 1487.8 Starting local Bazel server and connecting to it... #16 1489.8 (21:51:01) WARNING: option '--distinct_host_configuration' was expanded to from both option '--enable_platform_specific_config' (source /opt/tensorflow/.bazelrc) and option '--enable_platform_specific_config' (source /opt/tensorflow/.bazelrc). #16 1489.8 (21:51:01) INFO: Options provided by the client:. #16 1489.8 Inherited 'common' options: --isatty=0 --terminal_columns=80. #16 1489.8 (21:51:01) INFO: Reading rc options for 'build' from /opt/tensorflow/.bazelrc:. #16 1489.8 Inherited 'common' options: --experimental_repo_remote_exec. #16 1489.8 (21:51:01) INFO: Reading rc options for 'build' from /opt/tensorflow/.bazelrc:. #16 1489.8 'build' options: --define framework_shared_object=true --java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --host_java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true --deleted_packages=tensorflow/compiler/mlir/tfrt,tensorflow/compiler/mlir/tfrt/benchmarks,tensorflow/compiler/mlir/tfrt/jit/python_binding,tensorflow/compiler/mlir/tfrt/jit/transforms,tensor",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/608
https://github.com/google/deepvariant/issues/608:129,deployability,fail,fails,129,"binary creation fails; bazel unable to fetch tf_runtime; I was able to build the docker image last week, but this week the build fails at binary creation, with bazel unable to download tf_runtime. . - Operating system: Ubuntu20.04. - DeepVariant version: 1.4.0. - Building docker image locally. **Steps to reproduce:**. - Command: `docker build .` in source directory (no modifications). - Error trace: . . ```. #16 1484.7 ========== [Mon Jan 30 21:50:56 UTC 2023] Stage 'build-prereq.sh complete' starting. #16 1484.7 Extracting Bazel installation... #16 1487.8 Starting local Bazel server and connecting to it... #16 1489.8 (21:51:01) WARNING: option '--distinct_host_configuration' was expanded to from both option '--enable_platform_specific_config' (source /opt/tensorflow/.bazelrc) and option '--enable_platform_specific_config' (source /opt/tensorflow/.bazelrc). #16 1489.8 (21:51:01) INFO: Options provided by the client:. #16 1489.8 Inherited 'common' options: --isatty=0 --terminal_columns=80. #16 1489.8 (21:51:01) INFO: Reading rc options for 'build' from /opt/tensorflow/.bazelrc:. #16 1489.8 Inherited 'common' options: --experimental_repo_remote_exec. #16 1489.8 (21:51:01) INFO: Reading rc options for 'build' from /opt/tensorflow/.bazelrc:. #16 1489.8 'build' options: --define framework_shared_object=true --java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --host_java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true --deleted_packages=tensorflow/compiler/mlir/tfrt,tensorflow/compiler/mlir/tfrt/benchmarks,tensorflow/compiler/mlir/tfrt/jit/python_binding,tensorflow/compiler/mlir/tfrt/jit/transforms,tensor",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/608
https://github.com/google/deepvariant/issues/608:246,deployability,version,version,246,"binary creation fails; bazel unable to fetch tf_runtime; I was able to build the docker image last week, but this week the build fails at binary creation, with bazel unable to download tf_runtime. . - Operating system: Ubuntu20.04. - DeepVariant version: 1.4.0. - Building docker image locally. **Steps to reproduce:**. - Command: `docker build .` in source directory (no modifications). - Error trace: . . ```. #16 1484.7 ========== [Mon Jan 30 21:50:56 UTC 2023] Stage 'build-prereq.sh complete' starting. #16 1484.7 Extracting Bazel installation... #16 1487.8 Starting local Bazel server and connecting to it... #16 1489.8 (21:51:01) WARNING: option '--distinct_host_configuration' was expanded to from both option '--enable_platform_specific_config' (source /opt/tensorflow/.bazelrc) and option '--enable_platform_specific_config' (source /opt/tensorflow/.bazelrc). #16 1489.8 (21:51:01) INFO: Options provided by the client:. #16 1489.8 Inherited 'common' options: --isatty=0 --terminal_columns=80. #16 1489.8 (21:51:01) INFO: Reading rc options for 'build' from /opt/tensorflow/.bazelrc:. #16 1489.8 Inherited 'common' options: --experimental_repo_remote_exec. #16 1489.8 (21:51:01) INFO: Reading rc options for 'build' from /opt/tensorflow/.bazelrc:. #16 1489.8 'build' options: --define framework_shared_object=true --java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --host_java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true --deleted_packages=tensorflow/compiler/mlir/tfrt,tensorflow/compiler/mlir/tfrt/benchmarks,tensorflow/compiler/mlir/tfrt/jit/python_binding,tensorflow/compiler/mlir/tfrt/jit/transforms,tensor",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/608
https://github.com/google/deepvariant/issues/608:264,deployability,Build,Building,264,"binary creation fails; bazel unable to fetch tf_runtime; I was able to build the docker image last week, but this week the build fails at binary creation, with bazel unable to download tf_runtime. . - Operating system: Ubuntu20.04. - DeepVariant version: 1.4.0. - Building docker image locally. **Steps to reproduce:**. - Command: `docker build .` in source directory (no modifications). - Error trace: . . ```. #16 1484.7 ========== [Mon Jan 30 21:50:56 UTC 2023] Stage 'build-prereq.sh complete' starting. #16 1484.7 Extracting Bazel installation... #16 1487.8 Starting local Bazel server and connecting to it... #16 1489.8 (21:51:01) WARNING: option '--distinct_host_configuration' was expanded to from both option '--enable_platform_specific_config' (source /opt/tensorflow/.bazelrc) and option '--enable_platform_specific_config' (source /opt/tensorflow/.bazelrc). #16 1489.8 (21:51:01) INFO: Options provided by the client:. #16 1489.8 Inherited 'common' options: --isatty=0 --terminal_columns=80. #16 1489.8 (21:51:01) INFO: Reading rc options for 'build' from /opt/tensorflow/.bazelrc:. #16 1489.8 Inherited 'common' options: --experimental_repo_remote_exec. #16 1489.8 (21:51:01) INFO: Reading rc options for 'build' from /opt/tensorflow/.bazelrc:. #16 1489.8 'build' options: --define framework_shared_object=true --java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --host_java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true --deleted_packages=tensorflow/compiler/mlir/tfrt,tensorflow/compiler/mlir/tfrt/benchmarks,tensorflow/compiler/mlir/tfrt/jit/python_binding,tensorflow/compiler/mlir/tfrt/jit/transforms,tensor",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/608
https://github.com/google/deepvariant/issues/608:339,deployability,build,build,339,"binary creation fails; bazel unable to fetch tf_runtime; I was able to build the docker image last week, but this week the build fails at binary creation, with bazel unable to download tf_runtime. . - Operating system: Ubuntu20.04. - DeepVariant version: 1.4.0. - Building docker image locally. **Steps to reproduce:**. - Command: `docker build .` in source directory (no modifications). - Error trace: . . ```. #16 1484.7 ========== [Mon Jan 30 21:50:56 UTC 2023] Stage 'build-prereq.sh complete' starting. #16 1484.7 Extracting Bazel installation... #16 1487.8 Starting local Bazel server and connecting to it... #16 1489.8 (21:51:01) WARNING: option '--distinct_host_configuration' was expanded to from both option '--enable_platform_specific_config' (source /opt/tensorflow/.bazelrc) and option '--enable_platform_specific_config' (source /opt/tensorflow/.bazelrc). #16 1489.8 (21:51:01) INFO: Options provided by the client:. #16 1489.8 Inherited 'common' options: --isatty=0 --terminal_columns=80. #16 1489.8 (21:51:01) INFO: Reading rc options for 'build' from /opt/tensorflow/.bazelrc:. #16 1489.8 Inherited 'common' options: --experimental_repo_remote_exec. #16 1489.8 (21:51:01) INFO: Reading rc options for 'build' from /opt/tensorflow/.bazelrc:. #16 1489.8 'build' options: --define framework_shared_object=true --java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --host_java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true --deleted_packages=tensorflow/compiler/mlir/tfrt,tensorflow/compiler/mlir/tfrt/benchmarks,tensorflow/compiler/mlir/tfrt/jit/python_binding,tensorflow/compiler/mlir/tfrt/jit/transforms,tensor",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/608
https://github.com/google/deepvariant/issues/608:465,deployability,Stage,Stage,465,"binary creation fails; bazel unable to fetch tf_runtime; I was able to build the docker image last week, but this week the build fails at binary creation, with bazel unable to download tf_runtime. . - Operating system: Ubuntu20.04. - DeepVariant version: 1.4.0. - Building docker image locally. **Steps to reproduce:**. - Command: `docker build .` in source directory (no modifications). - Error trace: . . ```. #16 1484.7 ========== [Mon Jan 30 21:50:56 UTC 2023] Stage 'build-prereq.sh complete' starting. #16 1484.7 Extracting Bazel installation... #16 1487.8 Starting local Bazel server and connecting to it... #16 1489.8 (21:51:01) WARNING: option '--distinct_host_configuration' was expanded to from both option '--enable_platform_specific_config' (source /opt/tensorflow/.bazelrc) and option '--enable_platform_specific_config' (source /opt/tensorflow/.bazelrc). #16 1489.8 (21:51:01) INFO: Options provided by the client:. #16 1489.8 Inherited 'common' options: --isatty=0 --terminal_columns=80. #16 1489.8 (21:51:01) INFO: Reading rc options for 'build' from /opt/tensorflow/.bazelrc:. #16 1489.8 Inherited 'common' options: --experimental_repo_remote_exec. #16 1489.8 (21:51:01) INFO: Reading rc options for 'build' from /opt/tensorflow/.bazelrc:. #16 1489.8 'build' options: --define framework_shared_object=true --java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --host_java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true --deleted_packages=tensorflow/compiler/mlir/tfrt,tensorflow/compiler/mlir/tfrt/benchmarks,tensorflow/compiler/mlir/tfrt/jit/python_binding,tensorflow/compiler/mlir/tfrt/jit/transforms,tensor",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/608
https://github.com/google/deepvariant/issues/608:472,deployability,build,build-prereq,472,"binary creation fails; bazel unable to fetch tf_runtime; I was able to build the docker image last week, but this week the build fails at binary creation, with bazel unable to download tf_runtime. . - Operating system: Ubuntu20.04. - DeepVariant version: 1.4.0. - Building docker image locally. **Steps to reproduce:**. - Command: `docker build .` in source directory (no modifications). - Error trace: . . ```. #16 1484.7 ========== [Mon Jan 30 21:50:56 UTC 2023] Stage 'build-prereq.sh complete' starting. #16 1484.7 Extracting Bazel installation... #16 1487.8 Starting local Bazel server and connecting to it... #16 1489.8 (21:51:01) WARNING: option '--distinct_host_configuration' was expanded to from both option '--enable_platform_specific_config' (source /opt/tensorflow/.bazelrc) and option '--enable_platform_specific_config' (source /opt/tensorflow/.bazelrc). #16 1489.8 (21:51:01) INFO: Options provided by the client:. #16 1489.8 Inherited 'common' options: --isatty=0 --terminal_columns=80. #16 1489.8 (21:51:01) INFO: Reading rc options for 'build' from /opt/tensorflow/.bazelrc:. #16 1489.8 Inherited 'common' options: --experimental_repo_remote_exec. #16 1489.8 (21:51:01) INFO: Reading rc options for 'build' from /opt/tensorflow/.bazelrc:. #16 1489.8 'build' options: --define framework_shared_object=true --java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --host_java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true --deleted_packages=tensorflow/compiler/mlir/tfrt,tensorflow/compiler/mlir/tfrt/benchmarks,tensorflow/compiler/mlir/tfrt/jit/python_binding,tensorflow/compiler/mlir/tfrt/jit/transforms,tensor",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/608
https://github.com/google/deepvariant/issues/608:536,deployability,instal,installation,536,"binary creation fails; bazel unable to fetch tf_runtime; I was able to build the docker image last week, but this week the build fails at binary creation, with bazel unable to download tf_runtime. . - Operating system: Ubuntu20.04. - DeepVariant version: 1.4.0. - Building docker image locally. **Steps to reproduce:**. - Command: `docker build .` in source directory (no modifications). - Error trace: . . ```. #16 1484.7 ========== [Mon Jan 30 21:50:56 UTC 2023] Stage 'build-prereq.sh complete' starting. #16 1484.7 Extracting Bazel installation... #16 1487.8 Starting local Bazel server and connecting to it... #16 1489.8 (21:51:01) WARNING: option '--distinct_host_configuration' was expanded to from both option '--enable_platform_specific_config' (source /opt/tensorflow/.bazelrc) and option '--enable_platform_specific_config' (source /opt/tensorflow/.bazelrc). #16 1489.8 (21:51:01) INFO: Options provided by the client:. #16 1489.8 Inherited 'common' options: --isatty=0 --terminal_columns=80. #16 1489.8 (21:51:01) INFO: Reading rc options for 'build' from /opt/tensorflow/.bazelrc:. #16 1489.8 Inherited 'common' options: --experimental_repo_remote_exec. #16 1489.8 (21:51:01) INFO: Reading rc options for 'build' from /opt/tensorflow/.bazelrc:. #16 1489.8 'build' options: --define framework_shared_object=true --java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --host_java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true --deleted_packages=tensorflow/compiler/mlir/tfrt,tensorflow/compiler/mlir/tfrt/benchmarks,tensorflow/compiler/mlir/tfrt/jit/python_binding,tensorflow/compiler/mlir/tfrt/jit/transforms,tensor",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/608
https://github.com/google/deepvariant/issues/608:1056,deployability,build,build,1056,"was able to build the docker image last week, but this week the build fails at binary creation, with bazel unable to download tf_runtime. . - Operating system: Ubuntu20.04. - DeepVariant version: 1.4.0. - Building docker image locally. **Steps to reproduce:**. - Command: `docker build .` in source directory (no modifications). - Error trace: . . ```. #16 1484.7 ========== [Mon Jan 30 21:50:56 UTC 2023] Stage 'build-prereq.sh complete' starting. #16 1484.7 Extracting Bazel installation... #16 1487.8 Starting local Bazel server and connecting to it... #16 1489.8 (21:51:01) WARNING: option '--distinct_host_configuration' was expanded to from both option '--enable_platform_specific_config' (source /opt/tensorflow/.bazelrc) and option '--enable_platform_specific_config' (source /opt/tensorflow/.bazelrc). #16 1489.8 (21:51:01) INFO: Options provided by the client:. #16 1489.8 Inherited 'common' options: --isatty=0 --terminal_columns=80. #16 1489.8 (21:51:01) INFO: Reading rc options for 'build' from /opt/tensorflow/.bazelrc:. #16 1489.8 Inherited 'common' options: --experimental_repo_remote_exec. #16 1489.8 (21:51:01) INFO: Reading rc options for 'build' from /opt/tensorflow/.bazelrc:. #16 1489.8 'build' options: --define framework_shared_object=true --java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --host_java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true --deleted_packages=tensorflow/compiler/mlir/tfrt,tensorflow/compiler/mlir/tfrt/benchmarks,tensorflow/compiler/mlir/tfrt/jit/python_binding,tensorflow/compiler/mlir/tfrt/jit/transforms,tensorflow/compiler/mlir/tfrt/python_tests,tensorflow/compiler/ml",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/608
https://github.com/google/deepvariant/issues/608:1219,deployability,build,build,1219,"ntu20.04. - DeepVariant version: 1.4.0. - Building docker image locally. **Steps to reproduce:**. - Command: `docker build .` in source directory (no modifications). - Error trace: . . ```. #16 1484.7 ========== [Mon Jan 30 21:50:56 UTC 2023] Stage 'build-prereq.sh complete' starting. #16 1484.7 Extracting Bazel installation... #16 1487.8 Starting local Bazel server and connecting to it... #16 1489.8 (21:51:01) WARNING: option '--distinct_host_configuration' was expanded to from both option '--enable_platform_specific_config' (source /opt/tensorflow/.bazelrc) and option '--enable_platform_specific_config' (source /opt/tensorflow/.bazelrc). #16 1489.8 (21:51:01) INFO: Options provided by the client:. #16 1489.8 Inherited 'common' options: --isatty=0 --terminal_columns=80. #16 1489.8 (21:51:01) INFO: Reading rc options for 'build' from /opt/tensorflow/.bazelrc:. #16 1489.8 Inherited 'common' options: --experimental_repo_remote_exec. #16 1489.8 (21:51:01) INFO: Reading rc options for 'build' from /opt/tensorflow/.bazelrc:. #16 1489.8 'build' options: --define framework_shared_object=true --java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --host_java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true --deleted_packages=tensorflow/compiler/mlir/tfrt,tensorflow/compiler/mlir/tfrt/benchmarks,tensorflow/compiler/mlir/tfrt/jit/python_binding,tensorflow/compiler/mlir/tfrt/jit/transforms,tensorflow/compiler/mlir/tfrt/python_tests,tensorflow/compiler/mlir/tfrt/tests,tensorflow/compiler/mlir/tfrt/tests/saved_model,tensorflow/compiler/mlir/tfrt/transforms/lhlo_gpu_to_tfrt_gpu,tensorflow/core/runtime_fallback,tensor",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/608
https://github.com/google/deepvariant/issues/608:1270,deployability,build,build,1270,"docker image locally. **Steps to reproduce:**. - Command: `docker build .` in source directory (no modifications). - Error trace: . . ```. #16 1484.7 ========== [Mon Jan 30 21:50:56 UTC 2023] Stage 'build-prereq.sh complete' starting. #16 1484.7 Extracting Bazel installation... #16 1487.8 Starting local Bazel server and connecting to it... #16 1489.8 (21:51:01) WARNING: option '--distinct_host_configuration' was expanded to from both option '--enable_platform_specific_config' (source /opt/tensorflow/.bazelrc) and option '--enable_platform_specific_config' (source /opt/tensorflow/.bazelrc). #16 1489.8 (21:51:01) INFO: Options provided by the client:. #16 1489.8 Inherited 'common' options: --isatty=0 --terminal_columns=80. #16 1489.8 (21:51:01) INFO: Reading rc options for 'build' from /opt/tensorflow/.bazelrc:. #16 1489.8 Inherited 'common' options: --experimental_repo_remote_exec. #16 1489.8 (21:51:01) INFO: Reading rc options for 'build' from /opt/tensorflow/.bazelrc:. #16 1489.8 'build' options: --define framework_shared_object=true --java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --host_java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true --deleted_packages=tensorflow/compiler/mlir/tfrt,tensorflow/compiler/mlir/tfrt/benchmarks,tensorflow/compiler/mlir/tfrt/jit/python_binding,tensorflow/compiler/mlir/tfrt/jit/transforms,tensorflow/compiler/mlir/tfrt/python_tests,tensorflow/compiler/mlir/tfrt/tests,tensorflow/compiler/mlir/tfrt/tests/saved_model,tensorflow/compiler/mlir/tfrt/transforms/lhlo_gpu_to_tfrt_gpu,tensorflow/core/runtime_fallback,tensorflow/core/runtime_fallback/conversion,tensorflow/co",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/608
https://github.com/google/deepvariant/issues/608:1357,deployability,toolchain,toolchains,1357,"ctory (no modifications). - Error trace: . . ```. #16 1484.7 ========== [Mon Jan 30 21:50:56 UTC 2023] Stage 'build-prereq.sh complete' starting. #16 1484.7 Extracting Bazel installation... #16 1487.8 Starting local Bazel server and connecting to it... #16 1489.8 (21:51:01) WARNING: option '--distinct_host_configuration' was expanded to from both option '--enable_platform_specific_config' (source /opt/tensorflow/.bazelrc) and option '--enable_platform_specific_config' (source /opt/tensorflow/.bazelrc). #16 1489.8 (21:51:01) INFO: Options provided by the client:. #16 1489.8 Inherited 'common' options: --isatty=0 --terminal_columns=80. #16 1489.8 (21:51:01) INFO: Reading rc options for 'build' from /opt/tensorflow/.bazelrc:. #16 1489.8 Inherited 'common' options: --experimental_repo_remote_exec. #16 1489.8 (21:51:01) INFO: Reading rc options for 'build' from /opt/tensorflow/.bazelrc:. #16 1489.8 'build' options: --define framework_shared_object=true --java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --host_java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true --deleted_packages=tensorflow/compiler/mlir/tfrt,tensorflow/compiler/mlir/tfrt/benchmarks,tensorflow/compiler/mlir/tfrt/jit/python_binding,tensorflow/compiler/mlir/tfrt/jit/transforms,tensorflow/compiler/mlir/tfrt/python_tests,tensorflow/compiler/mlir/tfrt/tests,tensorflow/compiler/mlir/tfrt/tests/saved_model,tensorflow/compiler/mlir/tfrt/transforms/lhlo_gpu_to_tfrt_gpu,tensorflow/core/runtime_fallback,tensorflow/core/runtime_fallback/conversion,tensorflow/core/runtime_fallback/kernel,tensorflow/core/runtime_fallback/opdefs,tensorflow/core/runtim",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/608
https://github.com/google/deepvariant/issues/608:1429,deployability,toolchain,toolchains,1429,"[Mon Jan 30 21:50:56 UTC 2023] Stage 'build-prereq.sh complete' starting. #16 1484.7 Extracting Bazel installation... #16 1487.8 Starting local Bazel server and connecting to it... #16 1489.8 (21:51:01) WARNING: option '--distinct_host_configuration' was expanded to from both option '--enable_platform_specific_config' (source /opt/tensorflow/.bazelrc) and option '--enable_platform_specific_config' (source /opt/tensorflow/.bazelrc). #16 1489.8 (21:51:01) INFO: Options provided by the client:. #16 1489.8 Inherited 'common' options: --isatty=0 --terminal_columns=80. #16 1489.8 (21:51:01) INFO: Reading rc options for 'build' from /opt/tensorflow/.bazelrc:. #16 1489.8 Inherited 'common' options: --experimental_repo_remote_exec. #16 1489.8 (21:51:01) INFO: Reading rc options for 'build' from /opt/tensorflow/.bazelrc:. #16 1489.8 'build' options: --define framework_shared_object=true --java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --host_java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true --deleted_packages=tensorflow/compiler/mlir/tfrt,tensorflow/compiler/mlir/tfrt/benchmarks,tensorflow/compiler/mlir/tfrt/jit/python_binding,tensorflow/compiler/mlir/tfrt/jit/transforms,tensorflow/compiler/mlir/tfrt/python_tests,tensorflow/compiler/mlir/tfrt/tests,tensorflow/compiler/mlir/tfrt/tests/saved_model,tensorflow/compiler/mlir/tfrt/transforms/lhlo_gpu_to_tfrt_gpu,tensorflow/core/runtime_fallback,tensorflow/core/runtime_fallback/conversion,tensorflow/core/runtime_fallback/kernel,tensorflow/core/runtime_fallback/opdefs,tensorflow/core/runtime_fallback/runtime,tensorflow/core/runtime_fallback/util,tensorflow/core",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/608
https://github.com/google/deepvariant/issues/608:2950,deployability,build,build,2950,"orflow/compiler/mlir/tfrt/jit/transforms,tensorflow/compiler/mlir/tfrt/python_tests,tensorflow/compiler/mlir/tfrt/tests,tensorflow/compiler/mlir/tfrt/tests/saved_model,tensorflow/compiler/mlir/tfrt/transforms/lhlo_gpu_to_tfrt_gpu,tensorflow/core/runtime_fallback,tensorflow/core/runtime_fallback/conversion,tensorflow/core/runtime_fallback/kernel,tensorflow/core/runtime_fallback/opdefs,tensorflow/core/runtime_fallback/runtime,tensorflow/core/runtime_fallback/util,tensorflow/core/tfrt/common,tensorflow/core/tfrt/eager,tensorflow/core/tfrt/eager/backends/cpu,tensorflow/core/tfrt/eager/backends/gpu,tensorflow/core/tfrt/eager/core_runtime,tensorflow/core/tfrt/eager/cpp_tests/core_runtime,tensorflow/core/tfrt/fallback,tensorflow/core/tfrt/gpu,tensorflow/core/tfrt/run_handler_thread_pool,tensorflow/core/tfrt/runtime,tensorflow/core/tfrt/saved_model,tensorflow/core/tfrt/saved_model/tests,tensorflow/core/tfrt/tpu,tensorflow/core/tfrt/utils. #16 1489.8 (21:51:01) INFO: Reading rc options for 'build' from /opt/tensorflow/.tf_configure.bazelrc:. #16 1489.8 'build' options: --action_env PYTHON_BIN_PATH=/usr/local/bin/python3 --action_env PYTHON_LIB_PATH=/usr/local/lib/python3.8/dist-packages --python_path=/usr/local/bin/python3. #16 1489.8 (21:51:01) INFO: Reading rc options for 'build' from /opt/deepvariant/.bazelrc:. #16 1489.8 'build' options: --jobs 128 --config=monolithic --show_timestamps --verbose_failures --define=use_fast_cpp_protos=true --copt=-Wno-maybe-uninitialized --copt=-Wno-unused-function --cxxopt=-std=c++17 --python_top=//:deepvariant_python_runtime --incompatible_use_python_toolchains=false. #16 1489.8 (21:51:01) INFO: Found applicable config definition build:short_logs in file /opt/tensorflow/.bazelrc: --output_filter=DONT_MATCH_ANYTHING. #16 1489.8 (21:51:01) INFO: Found applicable config definition build:v2 in file /opt/tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1. #16 1489.8 (21:51:01) INFO: Found applicable config definition b",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/608
https://github.com/google/deepvariant/issues/608:3014,deployability,build,build,3014,"r/tfrt/python_tests,tensorflow/compiler/mlir/tfrt/tests,tensorflow/compiler/mlir/tfrt/tests/saved_model,tensorflow/compiler/mlir/tfrt/transforms/lhlo_gpu_to_tfrt_gpu,tensorflow/core/runtime_fallback,tensorflow/core/runtime_fallback/conversion,tensorflow/core/runtime_fallback/kernel,tensorflow/core/runtime_fallback/opdefs,tensorflow/core/runtime_fallback/runtime,tensorflow/core/runtime_fallback/util,tensorflow/core/tfrt/common,tensorflow/core/tfrt/eager,tensorflow/core/tfrt/eager/backends/cpu,tensorflow/core/tfrt/eager/backends/gpu,tensorflow/core/tfrt/eager/core_runtime,tensorflow/core/tfrt/eager/cpp_tests/core_runtime,tensorflow/core/tfrt/fallback,tensorflow/core/tfrt/gpu,tensorflow/core/tfrt/run_handler_thread_pool,tensorflow/core/tfrt/runtime,tensorflow/core/tfrt/saved_model,tensorflow/core/tfrt/saved_model/tests,tensorflow/core/tfrt/tpu,tensorflow/core/tfrt/utils. #16 1489.8 (21:51:01) INFO: Reading rc options for 'build' from /opt/tensorflow/.tf_configure.bazelrc:. #16 1489.8 'build' options: --action_env PYTHON_BIN_PATH=/usr/local/bin/python3 --action_env PYTHON_LIB_PATH=/usr/local/lib/python3.8/dist-packages --python_path=/usr/local/bin/python3. #16 1489.8 (21:51:01) INFO: Reading rc options for 'build' from /opt/deepvariant/.bazelrc:. #16 1489.8 'build' options: --jobs 128 --config=monolithic --show_timestamps --verbose_failures --define=use_fast_cpp_protos=true --copt=-Wno-maybe-uninitialized --copt=-Wno-unused-function --cxxopt=-std=c++17 --python_top=//:deepvariant_python_runtime --incompatible_use_python_toolchains=false. #16 1489.8 (21:51:01) INFO: Found applicable config definition build:short_logs in file /opt/tensorflow/.bazelrc: --output_filter=DONT_MATCH_ANYTHING. #16 1489.8 (21:51:01) INFO: Found applicable config definition build:v2 in file /opt/tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1. #16 1489.8 (21:51:01) INFO: Found applicable config definition build:monolithic in file /opt/tensorflow/.bazelrc: --define frame",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/608
https://github.com/google/deepvariant/issues/608:3240,deployability,build,build,3240,"lback/conversion,tensorflow/core/runtime_fallback/kernel,tensorflow/core/runtime_fallback/opdefs,tensorflow/core/runtime_fallback/runtime,tensorflow/core/runtime_fallback/util,tensorflow/core/tfrt/common,tensorflow/core/tfrt/eager,tensorflow/core/tfrt/eager/backends/cpu,tensorflow/core/tfrt/eager/backends/gpu,tensorflow/core/tfrt/eager/core_runtime,tensorflow/core/tfrt/eager/cpp_tests/core_runtime,tensorflow/core/tfrt/fallback,tensorflow/core/tfrt/gpu,tensorflow/core/tfrt/run_handler_thread_pool,tensorflow/core/tfrt/runtime,tensorflow/core/tfrt/saved_model,tensorflow/core/tfrt/saved_model/tests,tensorflow/core/tfrt/tpu,tensorflow/core/tfrt/utils. #16 1489.8 (21:51:01) INFO: Reading rc options for 'build' from /opt/tensorflow/.tf_configure.bazelrc:. #16 1489.8 'build' options: --action_env PYTHON_BIN_PATH=/usr/local/bin/python3 --action_env PYTHON_LIB_PATH=/usr/local/lib/python3.8/dist-packages --python_path=/usr/local/bin/python3. #16 1489.8 (21:51:01) INFO: Reading rc options for 'build' from /opt/deepvariant/.bazelrc:. #16 1489.8 'build' options: --jobs 128 --config=monolithic --show_timestamps --verbose_failures --define=use_fast_cpp_protos=true --copt=-Wno-maybe-uninitialized --copt=-Wno-unused-function --cxxopt=-std=c++17 --python_top=//:deepvariant_python_runtime --incompatible_use_python_toolchains=false. #16 1489.8 (21:51:01) INFO: Found applicable config definition build:short_logs in file /opt/tensorflow/.bazelrc: --output_filter=DONT_MATCH_ANYTHING. #16 1489.8 (21:51:01) INFO: Found applicable config definition build:v2 in file /opt/tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1. #16 1489.8 (21:51:01) INFO: Found applicable config definition build:monolithic in file /opt/tensorflow/.bazelrc: --define framework_shared_object=false. #16 1489.8 (21:51:01) INFO: Found applicable config definition build:linux in file /opt/tensorflow/.bazelrc: --copt=-w --host_copt=-w --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUD",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/608
https://github.com/google/deepvariant/issues/608:3292,deployability,build,build,3292,"rnel,tensorflow/core/runtime_fallback/opdefs,tensorflow/core/runtime_fallback/runtime,tensorflow/core/runtime_fallback/util,tensorflow/core/tfrt/common,tensorflow/core/tfrt/eager,tensorflow/core/tfrt/eager/backends/cpu,tensorflow/core/tfrt/eager/backends/gpu,tensorflow/core/tfrt/eager/core_runtime,tensorflow/core/tfrt/eager/cpp_tests/core_runtime,tensorflow/core/tfrt/fallback,tensorflow/core/tfrt/gpu,tensorflow/core/tfrt/run_handler_thread_pool,tensorflow/core/tfrt/runtime,tensorflow/core/tfrt/saved_model,tensorflow/core/tfrt/saved_model/tests,tensorflow/core/tfrt/tpu,tensorflow/core/tfrt/utils. #16 1489.8 (21:51:01) INFO: Reading rc options for 'build' from /opt/tensorflow/.tf_configure.bazelrc:. #16 1489.8 'build' options: --action_env PYTHON_BIN_PATH=/usr/local/bin/python3 --action_env PYTHON_LIB_PATH=/usr/local/lib/python3.8/dist-packages --python_path=/usr/local/bin/python3. #16 1489.8 (21:51:01) INFO: Reading rc options for 'build' from /opt/deepvariant/.bazelrc:. #16 1489.8 'build' options: --jobs 128 --config=monolithic --show_timestamps --verbose_failures --define=use_fast_cpp_protos=true --copt=-Wno-maybe-uninitialized --copt=-Wno-unused-function --cxxopt=-std=c++17 --python_top=//:deepvariant_python_runtime --incompatible_use_python_toolchains=false. #16 1489.8 (21:51:01) INFO: Found applicable config definition build:short_logs in file /opt/tensorflow/.bazelrc: --output_filter=DONT_MATCH_ANYTHING. #16 1489.8 (21:51:01) INFO: Found applicable config definition build:v2 in file /opt/tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1. #16 1489.8 (21:51:01) INFO: Found applicable config definition build:monolithic in file /opt/tensorflow/.bazelrc: --define framework_shared_object=false. #16 1489.8 (21:51:01) INFO: Found applicable config definition build:linux in file /opt/tensorflow/.bazelrc: --copt=-w --host_copt=-w --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PAT",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/608
https://github.com/google/deepvariant/issues/608:3640,deployability,build,build,3640,",tensorflow/core/tfrt/fallback,tensorflow/core/tfrt/gpu,tensorflow/core/tfrt/run_handler_thread_pool,tensorflow/core/tfrt/runtime,tensorflow/core/tfrt/saved_model,tensorflow/core/tfrt/saved_model/tests,tensorflow/core/tfrt/tpu,tensorflow/core/tfrt/utils. #16 1489.8 (21:51:01) INFO: Reading rc options for 'build' from /opt/tensorflow/.tf_configure.bazelrc:. #16 1489.8 'build' options: --action_env PYTHON_BIN_PATH=/usr/local/bin/python3 --action_env PYTHON_LIB_PATH=/usr/local/lib/python3.8/dist-packages --python_path=/usr/local/bin/python3. #16 1489.8 (21:51:01) INFO: Reading rc options for 'build' from /opt/deepvariant/.bazelrc:. #16 1489.8 'build' options: --jobs 128 --config=monolithic --show_timestamps --verbose_failures --define=use_fast_cpp_protos=true --copt=-Wno-maybe-uninitialized --copt=-Wno-unused-function --cxxopt=-std=c++17 --python_top=//:deepvariant_python_runtime --incompatible_use_python_toolchains=false. #16 1489.8 (21:51:01) INFO: Found applicable config definition build:short_logs in file /opt/tensorflow/.bazelrc: --output_filter=DONT_MATCH_ANYTHING. #16 1489.8 (21:51:01) INFO: Found applicable config definition build:v2 in file /opt/tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1. #16 1489.8 (21:51:01) INFO: Found applicable config definition build:monolithic in file /opt/tensorflow/.bazelrc: --define framework_shared_object=false. #16 1489.8 (21:51:01) INFO: Found applicable config definition build:linux in file /opt/tensorflow/.bazelrc: --copt=-w --host_copt=-w --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++14 --host_cxxopt=-std=c++14 --config=dynamic_kernels --distinct_host_configuration=false --experimental_guard_against_concurrent_changes. #16 1489.8 (21:51:01) INFO: Found applicable config definition build:dynamic_kernels in file /opt/tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/608
https://github.com/google/deepvariant/issues/608:3791,deployability,build,build,3791,"saved_model,tensorflow/core/tfrt/saved_model/tests,tensorflow/core/tfrt/tpu,tensorflow/core/tfrt/utils. #16 1489.8 (21:51:01) INFO: Reading rc options for 'build' from /opt/tensorflow/.tf_configure.bazelrc:. #16 1489.8 'build' options: --action_env PYTHON_BIN_PATH=/usr/local/bin/python3 --action_env PYTHON_LIB_PATH=/usr/local/lib/python3.8/dist-packages --python_path=/usr/local/bin/python3. #16 1489.8 (21:51:01) INFO: Reading rc options for 'build' from /opt/deepvariant/.bazelrc:. #16 1489.8 'build' options: --jobs 128 --config=monolithic --show_timestamps --verbose_failures --define=use_fast_cpp_protos=true --copt=-Wno-maybe-uninitialized --copt=-Wno-unused-function --cxxopt=-std=c++17 --python_top=//:deepvariant_python_runtime --incompatible_use_python_toolchains=false. #16 1489.8 (21:51:01) INFO: Found applicable config definition build:short_logs in file /opt/tensorflow/.bazelrc: --output_filter=DONT_MATCH_ANYTHING. #16 1489.8 (21:51:01) INFO: Found applicable config definition build:v2 in file /opt/tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1. #16 1489.8 (21:51:01) INFO: Found applicable config definition build:monolithic in file /opt/tensorflow/.bazelrc: --define framework_shared_object=false. #16 1489.8 (21:51:01) INFO: Found applicable config definition build:linux in file /opt/tensorflow/.bazelrc: --copt=-w --host_copt=-w --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++14 --host_cxxopt=-std=c++14 --config=dynamic_kernels --distinct_host_configuration=false --experimental_guard_against_concurrent_changes. #16 1489.8 (21:51:01) INFO: Found applicable config definition build:dynamic_kernels in file /opt/tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS. #16 1489.8 (21:51:01) INFO: Found applicable config definition build:linux in file /opt/deepvariant/.bazelrc: --distinct_host_configu",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/608
https://github.com/google/deepvariant/issues/608:3952,deployability,build,build,3952,' from /opt/tensorflow/.tf_configure.bazelrc:. #16 1489.8 'build' options: --action_env PYTHON_BIN_PATH=/usr/local/bin/python3 --action_env PYTHON_LIB_PATH=/usr/local/lib/python3.8/dist-packages --python_path=/usr/local/bin/python3. #16 1489.8 (21:51:01) INFO: Reading rc options for 'build' from /opt/deepvariant/.bazelrc:. #16 1489.8 'build' options: --jobs 128 --config=monolithic --show_timestamps --verbose_failures --define=use_fast_cpp_protos=true --copt=-Wno-maybe-uninitialized --copt=-Wno-unused-function --cxxopt=-std=c++17 --python_top=//:deepvariant_python_runtime --incompatible_use_python_toolchains=false. #16 1489.8 (21:51:01) INFO: Found applicable config definition build:short_logs in file /opt/tensorflow/.bazelrc: --output_filter=DONT_MATCH_ANYTHING. #16 1489.8 (21:51:01) INFO: Found applicable config definition build:v2 in file /opt/tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1. #16 1489.8 (21:51:01) INFO: Found applicable config definition build:monolithic in file /opt/tensorflow/.bazelrc: --define framework_shared_object=false. #16 1489.8 (21:51:01) INFO: Found applicable config definition build:linux in file /opt/tensorflow/.bazelrc: --copt=-w --host_copt=-w --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++14 --host_cxxopt=-std=c++14 --config=dynamic_kernels --distinct_host_configuration=false --experimental_guard_against_concurrent_changes. #16 1489.8 (21:51:01) INFO: Found applicable config definition build:dynamic_kernels in file /opt/tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS. #16 1489.8 (21:51:01) INFO: Found applicable config definition build:linux in file /opt/deepvariant/.bazelrc: --distinct_host_configuration=true. #16 1490.1 (21:51:01) INFO: Current date is 2023-01-30. #16 1490.1 (21:51:01) Loading:. #16 1490.1 (21:51:01) Loading: 0 packages loaded. #16 1491.1,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/608
https://github.com/google/deepvariant/issues/608:4106,deployability,build,build,4106,H=/usr/local/lib/python3.8/dist-packages --python_path=/usr/local/bin/python3. #16 1489.8 (21:51:01) INFO: Reading rc options for 'build' from /opt/deepvariant/.bazelrc:. #16 1489.8 'build' options: --jobs 128 --config=monolithic --show_timestamps --verbose_failures --define=use_fast_cpp_protos=true --copt=-Wno-maybe-uninitialized --copt=-Wno-unused-function --cxxopt=-std=c++17 --python_top=//:deepvariant_python_runtime --incompatible_use_python_toolchains=false. #16 1489.8 (21:51:01) INFO: Found applicable config definition build:short_logs in file /opt/tensorflow/.bazelrc: --output_filter=DONT_MATCH_ANYTHING. #16 1489.8 (21:51:01) INFO: Found applicable config definition build:v2 in file /opt/tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1. #16 1489.8 (21:51:01) INFO: Found applicable config definition build:monolithic in file /opt/tensorflow/.bazelrc: --define framework_shared_object=false. #16 1489.8 (21:51:01) INFO: Found applicable config definition build:linux in file /opt/tensorflow/.bazelrc: --copt=-w --host_copt=-w --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++14 --host_cxxopt=-std=c++14 --config=dynamic_kernels --distinct_host_configuration=false --experimental_guard_against_concurrent_changes. #16 1489.8 (21:51:01) INFO: Found applicable config definition build:dynamic_kernels in file /opt/tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS. #16 1489.8 (21:51:01) INFO: Found applicable config definition build:linux in file /opt/deepvariant/.bazelrc: --distinct_host_configuration=true. #16 1490.1 (21:51:01) INFO: Current date is 2023-01-30. #16 1490.1 (21:51:01) Loading:. #16 1490.1 (21:51:01) Loading: 0 packages loaded. #16 1491.1 (21:51:02) Loading: 0 packages loaded. #16 1492.2 (21:51:03) Loading: 0 packages loaded. #16 1493.2 (21:51:04) Loading: 0 packages loaded. #16 1494.2 (21,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/608
https://github.com/google/deepvariant/issues/608:4533,deployability,build,build,4533,ncompatible_use_python_toolchains=false. #16 1489.8 (21:51:01) INFO: Found applicable config definition build:short_logs in file /opt/tensorflow/.bazelrc: --output_filter=DONT_MATCH_ANYTHING. #16 1489.8 (21:51:01) INFO: Found applicable config definition build:v2 in file /opt/tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1. #16 1489.8 (21:51:01) INFO: Found applicable config definition build:monolithic in file /opt/tensorflow/.bazelrc: --define framework_shared_object=false. #16 1489.8 (21:51:01) INFO: Found applicable config definition build:linux in file /opt/tensorflow/.bazelrc: --copt=-w --host_copt=-w --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++14 --host_cxxopt=-std=c++14 --config=dynamic_kernels --distinct_host_configuration=false --experimental_guard_against_concurrent_changes. #16 1489.8 (21:51:01) INFO: Found applicable config definition build:dynamic_kernels in file /opt/tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS. #16 1489.8 (21:51:01) INFO: Found applicable config definition build:linux in file /opt/deepvariant/.bazelrc: --distinct_host_configuration=true. #16 1490.1 (21:51:01) INFO: Current date is 2023-01-30. #16 1490.1 (21:51:01) Loading:. #16 1490.1 (21:51:01) Loading: 0 packages loaded. #16 1491.1 (21:51:02) Loading: 0 packages loaded. #16 1492.2 (21:51:03) Loading: 0 packages loaded. #16 1493.2 (21:51:04) Loading: 0 packages loaded. #16 1494.2 (21:51:05) Loading: 0 packages loaded. #16 1495.2 (21:51:06) Loading: 0 packages loaded. #16 1496.2 (21:51:07) Loading: 0 packages loaded. #16 1497.0 (21:51:08) INFO: Repository tf_runtime instantiated at:. #16 1497.0 /opt/deepvariant/WORKSPACE:102:14: in <toplevel>. #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/tensorflow/workspace3.bzl:28:15: in workspace. #16 1497.0 /root,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/608
https://github.com/google/deepvariant/issues/608:4724,deployability,build,build,4724, #16 1489.8 (21:51:01) INFO: Found applicable config definition build:v2 in file /opt/tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1. #16 1489.8 (21:51:01) INFO: Found applicable config definition build:monolithic in file /opt/tensorflow/.bazelrc: --define framework_shared_object=false. #16 1489.8 (21:51:01) INFO: Found applicable config definition build:linux in file /opt/tensorflow/.bazelrc: --copt=-w --host_copt=-w --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++14 --host_cxxopt=-std=c++14 --config=dynamic_kernels --distinct_host_configuration=false --experimental_guard_against_concurrent_changes. #16 1489.8 (21:51:01) INFO: Found applicable config definition build:dynamic_kernels in file /opt/tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS. #16 1489.8 (21:51:01) INFO: Found applicable config definition build:linux in file /opt/deepvariant/.bazelrc: --distinct_host_configuration=true. #16 1490.1 (21:51:01) INFO: Current date is 2023-01-30. #16 1490.1 (21:51:01) Loading:. #16 1490.1 (21:51:01) Loading: 0 packages loaded. #16 1491.1 (21:51:02) Loading: 0 packages loaded. #16 1492.2 (21:51:03) Loading: 0 packages loaded. #16 1493.2 (21:51:04) Loading: 0 packages loaded. #16 1494.2 (21:51:05) Loading: 0 packages loaded. #16 1495.2 (21:51:06) Loading: 0 packages loaded. #16 1496.2 (21:51:07) Loading: 0 packages loaded. #16 1497.0 (21:51:08) INFO: Repository tf_runtime instantiated at:. #16 1497.0 /opt/deepvariant/WORKSPACE:102:14: in <toplevel>. #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/tensorflow/workspace3.bzl:28:15: in workspace. #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/tf_runtime/workspace.bzl:12:20: in repo. #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/608
https://github.com/google/deepvariant/issues/608:6179,deployability,fail,failed,6179,"ages loaded. #16 1496.2 (21:51:07) Loading: 0 packages loaded. #16 1497.0 (21:51:08) INFO: Repository tf_runtime instantiated at:. #16 1497.0 /opt/deepvariant/WORKSPACE:102:14: in <toplevel>. #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/tensorflow/workspace3.bzl:28:15: in workspace. #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/tf_runtime/workspace.bzl:12:20: in repo. #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:113:21: in tf_http_archive. #16 1497.0 Repository rule _tf_http_archive defined at:. #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:66:35: in <toplevel>. #16 1497.0 (21:51:08) WARNING: Download from http://mirror.tensorflow.org/github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException GET returned 404 Not Found. #16 1497.0 (21:51:08) WARNING: Download from https://github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException Checksum was 8383b3247286016e450b0b20e805d26b88ab4638b4e4e3cc4a6923debaf7ad1e but wanted f16fcf09b34e0c7be9389f50652b4b4a14c5a8a96e7e15ad73e8f234d8d09ebe. #16 1497.0 (21:51:08) ERROR: An error occurred during the fetch of repository 'tf_runtime':. #16 1497.0 Traceback (most recent call last):. #16 1497.0 File ""/root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl"", line 53, column 33, in _tf_http_archive_impl. #16 1497.0 ctx.download_and_extract(. #16 1497.0 Error in download_and_extract: java.io.IOException: Error downloading [http://mirror.tensorflow.org/github.com/tensorflo",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/608
https://github.com/google/deepvariant/issues/608:6213,deployability,build,build,6213," Loading: 0 packages loaded. #16 1497.0 (21:51:08) INFO: Repository tf_runtime instantiated at:. #16 1497.0 /opt/deepvariant/WORKSPACE:102:14: in <toplevel>. #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/tensorflow/workspace3.bzl:28:15: in workspace. #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/tf_runtime/workspace.bzl:12:20: in repo. #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:113:21: in tf_http_archive. #16 1497.0 Repository rule _tf_http_archive defined at:. #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:66:35: in <toplevel>. #16 1497.0 (21:51:08) WARNING: Download from http://mirror.tensorflow.org/github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException GET returned 404 Not Found. #16 1497.0 (21:51:08) WARNING: Download from https://github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException Checksum was 8383b3247286016e450b0b20e805d26b88ab4638b4e4e3cc4a6923debaf7ad1e but wanted f16fcf09b34e0c7be9389f50652b4b4a14c5a8a96e7e15ad73e8f234d8d09ebe. #16 1497.0 (21:51:08) ERROR: An error occurred during the fetch of repository 'tf_runtime':. #16 1497.0 Traceback (most recent call last):. #16 1497.0 File ""/root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl"", line 53, column 33, in _tf_http_archive_impl. #16 1497.0 ctx.download_and_extract(. #16 1497.0 Error in download_and_extract: java.io.IOException: Error downloading [http://mirror.tensorflow.org/github.com/tensorflow/runtime/archive/64c92c8013b55708",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/608
https://github.com/google/deepvariant/issues/608:6445,deployability,fail,failed,6445,"/external/org_tensorflow/tensorflow/workspace3.bzl:28:15: in workspace. #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/tf_runtime/workspace.bzl:12:20: in repo. #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:113:21: in tf_http_archive. #16 1497.0 Repository rule _tf_http_archive defined at:. #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:66:35: in <toplevel>. #16 1497.0 (21:51:08) WARNING: Download from http://mirror.tensorflow.org/github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException GET returned 404 Not Found. #16 1497.0 (21:51:08) WARNING: Download from https://github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException Checksum was 8383b3247286016e450b0b20e805d26b88ab4638b4e4e3cc4a6923debaf7ad1e but wanted f16fcf09b34e0c7be9389f50652b4b4a14c5a8a96e7e15ad73e8f234d8d09ebe. #16 1497.0 (21:51:08) ERROR: An error occurred during the fetch of repository 'tf_runtime':. #16 1497.0 Traceback (most recent call last):. #16 1497.0 File ""/root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl"", line 53, column 33, in _tf_http_archive_impl. #16 1497.0 ctx.download_and_extract(. #16 1497.0 Error in download_and_extract: java.io.IOException: Error downloading [http://mirror.tensorflow.org/github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz, https://github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz] to /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/tf_runtime/temp1251691892941",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/608
https://github.com/google/deepvariant/issues/608:6479,deployability,build,build,6479,"w/workspace3.bzl:28:15: in workspace. #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/tf_runtime/workspace.bzl:12:20: in repo. #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:113:21: in tf_http_archive. #16 1497.0 Repository rule _tf_http_archive defined at:. #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:66:35: in <toplevel>. #16 1497.0 (21:51:08) WARNING: Download from http://mirror.tensorflow.org/github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException GET returned 404 Not Found. #16 1497.0 (21:51:08) WARNING: Download from https://github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException Checksum was 8383b3247286016e450b0b20e805d26b88ab4638b4e4e3cc4a6923debaf7ad1e but wanted f16fcf09b34e0c7be9389f50652b4b4a14c5a8a96e7e15ad73e8f234d8d09ebe. #16 1497.0 (21:51:08) ERROR: An error occurred during the fetch of repository 'tf_runtime':. #16 1497.0 Traceback (most recent call last):. #16 1497.0 File ""/root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl"", line 53, column 33, in _tf_http_archive_impl. #16 1497.0 ctx.download_and_extract(. #16 1497.0 Error in download_and_extract: java.io.IOException: Error downloading [http://mirror.tensorflow.org/github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz, https://github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz] to /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/tf_runtime/temp12516918929418979294/64c92c8013b557087351c91b54",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/608
https://github.com/google/deepvariant/issues/608:9218,deployability,FAIL,FAILED,9218,"80/external/org_tensorflow/tensorflow/workspace3.bzl:42:9: in workspace. #16 1497.1 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/llvm/workspace.bzl:10:20: in repo. #16 1497.1 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:113:21: in tf_http_archive. #16 1497.1 Repository rule _tf_http_archive defined at:. #16 1497.1 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:66:35: in <toplevel>. #16 1497.2 (21:51:08) Loading: 0 packages loaded. #16 1497.3 (21:51:08) ERROR: no such package '@tf_runtime//': java.io.IOException: Error downloading [http://mirror.tensorflow.org/github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz, https://github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz] to /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/tf_runtime/temp12516918929418979294/64c92c8013b557087351c91b5423b6046d10f206.tar.gz: Checksum was 8383b3247286016e450b0b20e805d26b88ab4638b4e4e3cc4a6923debaf7ad1e but wanted f16fcf09b34e0c7be9389f50652b4b4a14c5a8a96e7e15ad73e8f234d8d09ebe. #16 1497.3 (21:51:09) INFO: Elapsed time: 12.546s. #16 1497.3 (21:51:09) INFO: 0 processes. #16 1497.3 (21:51:09) FAILED: Build did NOT complete successfully (0 packages loaded). #16 1497.3 (21:51:09) FAILED: Build did NOT complete successfully (0 packages loaded). #16 ERROR: executor failed running [/bin/sh -c ./build-prereq.sh && PATH=""${HOME}/bin:${PATH}"" ./build_release_binaries.sh # PATH for bazel]: exit code: 1. ------. > [builder 6/6] RUN ./build-prereq.sh && PATH=""${HOME}/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin"" ./build_release_binaries.sh # PATH for bazel:. ------. executor failed running [/bin/sh -c ./build-prereq.sh && PATH=""${HOME}/bin:${PATH}"" ./build_release_binaries.sh # PATH for bazel]: exit code: 1. ```.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/608
https://github.com/google/deepvariant/issues/608:9226,deployability,Build,Build,9226,"80/external/org_tensorflow/tensorflow/workspace3.bzl:42:9: in workspace. #16 1497.1 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/llvm/workspace.bzl:10:20: in repo. #16 1497.1 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:113:21: in tf_http_archive. #16 1497.1 Repository rule _tf_http_archive defined at:. #16 1497.1 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:66:35: in <toplevel>. #16 1497.2 (21:51:08) Loading: 0 packages loaded. #16 1497.3 (21:51:08) ERROR: no such package '@tf_runtime//': java.io.IOException: Error downloading [http://mirror.tensorflow.org/github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz, https://github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz] to /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/tf_runtime/temp12516918929418979294/64c92c8013b557087351c91b5423b6046d10f206.tar.gz: Checksum was 8383b3247286016e450b0b20e805d26b88ab4638b4e4e3cc4a6923debaf7ad1e but wanted f16fcf09b34e0c7be9389f50652b4b4a14c5a8a96e7e15ad73e8f234d8d09ebe. #16 1497.3 (21:51:09) INFO: Elapsed time: 12.546s. #16 1497.3 (21:51:09) INFO: 0 processes. #16 1497.3 (21:51:09) FAILED: Build did NOT complete successfully (0 packages loaded). #16 1497.3 (21:51:09) FAILED: Build did NOT complete successfully (0 packages loaded). #16 ERROR: executor failed running [/bin/sh -c ./build-prereq.sh && PATH=""${HOME}/bin:${PATH}"" ./build_release_binaries.sh # PATH for bazel]: exit code: 1. ------. > [builder 6/6] RUN ./build-prereq.sh && PATH=""${HOME}/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin"" ./build_release_binaries.sh # PATH for bazel:. ------. executor failed running [/bin/sh -c ./build-prereq.sh && PATH=""${HOME}/bin:${PATH}"" ./build_release_binaries.sh # PATH for bazel]: exit code: 1. ```.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/608
https://github.com/google/deepvariant/issues/608:9305,deployability,FAIL,FAILED,9305,"80/external/org_tensorflow/tensorflow/workspace3.bzl:42:9: in workspace. #16 1497.1 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/llvm/workspace.bzl:10:20: in repo. #16 1497.1 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:113:21: in tf_http_archive. #16 1497.1 Repository rule _tf_http_archive defined at:. #16 1497.1 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:66:35: in <toplevel>. #16 1497.2 (21:51:08) Loading: 0 packages loaded. #16 1497.3 (21:51:08) ERROR: no such package '@tf_runtime//': java.io.IOException: Error downloading [http://mirror.tensorflow.org/github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz, https://github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz] to /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/tf_runtime/temp12516918929418979294/64c92c8013b557087351c91b5423b6046d10f206.tar.gz: Checksum was 8383b3247286016e450b0b20e805d26b88ab4638b4e4e3cc4a6923debaf7ad1e but wanted f16fcf09b34e0c7be9389f50652b4b4a14c5a8a96e7e15ad73e8f234d8d09ebe. #16 1497.3 (21:51:09) INFO: Elapsed time: 12.546s. #16 1497.3 (21:51:09) INFO: 0 processes. #16 1497.3 (21:51:09) FAILED: Build did NOT complete successfully (0 packages loaded). #16 1497.3 (21:51:09) FAILED: Build did NOT complete successfully (0 packages loaded). #16 ERROR: executor failed running [/bin/sh -c ./build-prereq.sh && PATH=""${HOME}/bin:${PATH}"" ./build_release_binaries.sh # PATH for bazel]: exit code: 1. ------. > [builder 6/6] RUN ./build-prereq.sh && PATH=""${HOME}/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin"" ./build_release_binaries.sh # PATH for bazel:. ------. executor failed running [/bin/sh -c ./build-prereq.sh && PATH=""${HOME}/bin:${PATH}"" ./build_release_binaries.sh # PATH for bazel]: exit code: 1. ```.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/608
https://github.com/google/deepvariant/issues/608:9313,deployability,Build,Build,9313,"80/external/org_tensorflow/tensorflow/workspace3.bzl:42:9: in workspace. #16 1497.1 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/llvm/workspace.bzl:10:20: in repo. #16 1497.1 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:113:21: in tf_http_archive. #16 1497.1 Repository rule _tf_http_archive defined at:. #16 1497.1 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:66:35: in <toplevel>. #16 1497.2 (21:51:08) Loading: 0 packages loaded. #16 1497.3 (21:51:08) ERROR: no such package '@tf_runtime//': java.io.IOException: Error downloading [http://mirror.tensorflow.org/github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz, https://github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz] to /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/tf_runtime/temp12516918929418979294/64c92c8013b557087351c91b5423b6046d10f206.tar.gz: Checksum was 8383b3247286016e450b0b20e805d26b88ab4638b4e4e3cc4a6923debaf7ad1e but wanted f16fcf09b34e0c7be9389f50652b4b4a14c5a8a96e7e15ad73e8f234d8d09ebe. #16 1497.3 (21:51:09) INFO: Elapsed time: 12.546s. #16 1497.3 (21:51:09) INFO: 0 processes. #16 1497.3 (21:51:09) FAILED: Build did NOT complete successfully (0 packages loaded). #16 1497.3 (21:51:09) FAILED: Build did NOT complete successfully (0 packages loaded). #16 ERROR: executor failed running [/bin/sh -c ./build-prereq.sh && PATH=""${HOME}/bin:${PATH}"" ./build_release_binaries.sh # PATH for bazel]: exit code: 1. ------. > [builder 6/6] RUN ./build-prereq.sh && PATH=""${HOME}/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin"" ./build_release_binaries.sh # PATH for bazel:. ------. executor failed running [/bin/sh -c ./build-prereq.sh && PATH=""${HOME}/bin:${PATH}"" ./build_release_binaries.sh # PATH for bazel]: exit code: 1. ```.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/608
https://github.com/google/deepvariant/issues/608:9390,deployability,fail,failed,9390,"80/external/org_tensorflow/tensorflow/workspace3.bzl:42:9: in workspace. #16 1497.1 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/llvm/workspace.bzl:10:20: in repo. #16 1497.1 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:113:21: in tf_http_archive. #16 1497.1 Repository rule _tf_http_archive defined at:. #16 1497.1 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:66:35: in <toplevel>. #16 1497.2 (21:51:08) Loading: 0 packages loaded. #16 1497.3 (21:51:08) ERROR: no such package '@tf_runtime//': java.io.IOException: Error downloading [http://mirror.tensorflow.org/github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz, https://github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz] to /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/tf_runtime/temp12516918929418979294/64c92c8013b557087351c91b5423b6046d10f206.tar.gz: Checksum was 8383b3247286016e450b0b20e805d26b88ab4638b4e4e3cc4a6923debaf7ad1e but wanted f16fcf09b34e0c7be9389f50652b4b4a14c5a8a96e7e15ad73e8f234d8d09ebe. #16 1497.3 (21:51:09) INFO: Elapsed time: 12.546s. #16 1497.3 (21:51:09) INFO: 0 processes. #16 1497.3 (21:51:09) FAILED: Build did NOT complete successfully (0 packages loaded). #16 1497.3 (21:51:09) FAILED: Build did NOT complete successfully (0 packages loaded). #16 ERROR: executor failed running [/bin/sh -c ./build-prereq.sh && PATH=""${HOME}/bin:${PATH}"" ./build_release_binaries.sh # PATH for bazel]: exit code: 1. ------. > [builder 6/6] RUN ./build-prereq.sh && PATH=""${HOME}/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin"" ./build_release_binaries.sh # PATH for bazel:. ------. executor failed running [/bin/sh -c ./build-prereq.sh && PATH=""${HOME}/bin:${PATH}"" ./build_release_binaries.sh # PATH for bazel]: exit code: 1. ```.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/608
https://github.com/google/deepvariant/issues/608:9419,deployability,build,build-prereq,9419,"80/external/org_tensorflow/tensorflow/workspace3.bzl:42:9: in workspace. #16 1497.1 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/llvm/workspace.bzl:10:20: in repo. #16 1497.1 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:113:21: in tf_http_archive. #16 1497.1 Repository rule _tf_http_archive defined at:. #16 1497.1 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:66:35: in <toplevel>. #16 1497.2 (21:51:08) Loading: 0 packages loaded. #16 1497.3 (21:51:08) ERROR: no such package '@tf_runtime//': java.io.IOException: Error downloading [http://mirror.tensorflow.org/github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz, https://github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz] to /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/tf_runtime/temp12516918929418979294/64c92c8013b557087351c91b5423b6046d10f206.tar.gz: Checksum was 8383b3247286016e450b0b20e805d26b88ab4638b4e4e3cc4a6923debaf7ad1e but wanted f16fcf09b34e0c7be9389f50652b4b4a14c5a8a96e7e15ad73e8f234d8d09ebe. #16 1497.3 (21:51:09) INFO: Elapsed time: 12.546s. #16 1497.3 (21:51:09) INFO: 0 processes. #16 1497.3 (21:51:09) FAILED: Build did NOT complete successfully (0 packages loaded). #16 1497.3 (21:51:09) FAILED: Build did NOT complete successfully (0 packages loaded). #16 ERROR: executor failed running [/bin/sh -c ./build-prereq.sh && PATH=""${HOME}/bin:${PATH}"" ./build_release_binaries.sh # PATH for bazel]: exit code: 1. ------. > [builder 6/6] RUN ./build-prereq.sh && PATH=""${HOME}/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin"" ./build_release_binaries.sh # PATH for bazel:. ------. executor failed running [/bin/sh -c ./build-prereq.sh && PATH=""${HOME}/bin:${PATH}"" ./build_release_binaries.sh # PATH for bazel]: exit code: 1. ```.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/608
https://github.com/google/deepvariant/issues/608:9537,deployability,build,builder,9537,"80/external/org_tensorflow/tensorflow/workspace3.bzl:42:9: in workspace. #16 1497.1 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/llvm/workspace.bzl:10:20: in repo. #16 1497.1 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:113:21: in tf_http_archive. #16 1497.1 Repository rule _tf_http_archive defined at:. #16 1497.1 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:66:35: in <toplevel>. #16 1497.2 (21:51:08) Loading: 0 packages loaded. #16 1497.3 (21:51:08) ERROR: no such package '@tf_runtime//': java.io.IOException: Error downloading [http://mirror.tensorflow.org/github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz, https://github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz] to /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/tf_runtime/temp12516918929418979294/64c92c8013b557087351c91b5423b6046d10f206.tar.gz: Checksum was 8383b3247286016e450b0b20e805d26b88ab4638b4e4e3cc4a6923debaf7ad1e but wanted f16fcf09b34e0c7be9389f50652b4b4a14c5a8a96e7e15ad73e8f234d8d09ebe. #16 1497.3 (21:51:09) INFO: Elapsed time: 12.546s. #16 1497.3 (21:51:09) INFO: 0 processes. #16 1497.3 (21:51:09) FAILED: Build did NOT complete successfully (0 packages loaded). #16 1497.3 (21:51:09) FAILED: Build did NOT complete successfully (0 packages loaded). #16 ERROR: executor failed running [/bin/sh -c ./build-prereq.sh && PATH=""${HOME}/bin:${PATH}"" ./build_release_binaries.sh # PATH for bazel]: exit code: 1. ------. > [builder 6/6] RUN ./build-prereq.sh && PATH=""${HOME}/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin"" ./build_release_binaries.sh # PATH for bazel:. ------. executor failed running [/bin/sh -c ./build-prereq.sh && PATH=""${HOME}/bin:${PATH}"" ./build_release_binaries.sh # PATH for bazel]: exit code: 1. ```.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/608
https://github.com/google/deepvariant/issues/608:9556,deployability,build,build-prereq,9556,"80/external/org_tensorflow/tensorflow/workspace3.bzl:42:9: in workspace. #16 1497.1 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/llvm/workspace.bzl:10:20: in repo. #16 1497.1 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:113:21: in tf_http_archive. #16 1497.1 Repository rule _tf_http_archive defined at:. #16 1497.1 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:66:35: in <toplevel>. #16 1497.2 (21:51:08) Loading: 0 packages loaded. #16 1497.3 (21:51:08) ERROR: no such package '@tf_runtime//': java.io.IOException: Error downloading [http://mirror.tensorflow.org/github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz, https://github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz] to /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/tf_runtime/temp12516918929418979294/64c92c8013b557087351c91b5423b6046d10f206.tar.gz: Checksum was 8383b3247286016e450b0b20e805d26b88ab4638b4e4e3cc4a6923debaf7ad1e but wanted f16fcf09b34e0c7be9389f50652b4b4a14c5a8a96e7e15ad73e8f234d8d09ebe. #16 1497.3 (21:51:09) INFO: Elapsed time: 12.546s. #16 1497.3 (21:51:09) INFO: 0 processes. #16 1497.3 (21:51:09) FAILED: Build did NOT complete successfully (0 packages loaded). #16 1497.3 (21:51:09) FAILED: Build did NOT complete successfully (0 packages loaded). #16 ERROR: executor failed running [/bin/sh -c ./build-prereq.sh && PATH=""${HOME}/bin:${PATH}"" ./build_release_binaries.sh # PATH for bazel]: exit code: 1. ------. > [builder 6/6] RUN ./build-prereq.sh && PATH=""${HOME}/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin"" ./build_release_binaries.sh # PATH for bazel:. ------. executor failed running [/bin/sh -c ./build-prereq.sh && PATH=""${HOME}/bin:${PATH}"" ./build_release_binaries.sh # PATH for bazel]: exit code: 1. ```.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/608
https://github.com/google/deepvariant/issues/608:9719,deployability,fail,failed,9719,"80/external/org_tensorflow/tensorflow/workspace3.bzl:42:9: in workspace. #16 1497.1 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/llvm/workspace.bzl:10:20: in repo. #16 1497.1 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:113:21: in tf_http_archive. #16 1497.1 Repository rule _tf_http_archive defined at:. #16 1497.1 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:66:35: in <toplevel>. #16 1497.2 (21:51:08) Loading: 0 packages loaded. #16 1497.3 (21:51:08) ERROR: no such package '@tf_runtime//': java.io.IOException: Error downloading [http://mirror.tensorflow.org/github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz, https://github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz] to /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/tf_runtime/temp12516918929418979294/64c92c8013b557087351c91b5423b6046d10f206.tar.gz: Checksum was 8383b3247286016e450b0b20e805d26b88ab4638b4e4e3cc4a6923debaf7ad1e but wanted f16fcf09b34e0c7be9389f50652b4b4a14c5a8a96e7e15ad73e8f234d8d09ebe. #16 1497.3 (21:51:09) INFO: Elapsed time: 12.546s. #16 1497.3 (21:51:09) INFO: 0 processes. #16 1497.3 (21:51:09) FAILED: Build did NOT complete successfully (0 packages loaded). #16 1497.3 (21:51:09) FAILED: Build did NOT complete successfully (0 packages loaded). #16 ERROR: executor failed running [/bin/sh -c ./build-prereq.sh && PATH=""${HOME}/bin:${PATH}"" ./build_release_binaries.sh # PATH for bazel]: exit code: 1. ------. > [builder 6/6] RUN ./build-prereq.sh && PATH=""${HOME}/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin"" ./build_release_binaries.sh # PATH for bazel:. ------. executor failed running [/bin/sh -c ./build-prereq.sh && PATH=""${HOME}/bin:${PATH}"" ./build_release_binaries.sh # PATH for bazel]: exit code: 1. ```.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/608
https://github.com/google/deepvariant/issues/608:9748,deployability,build,build-prereq,9748,"80/external/org_tensorflow/tensorflow/workspace3.bzl:42:9: in workspace. #16 1497.1 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/llvm/workspace.bzl:10:20: in repo. #16 1497.1 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:113:21: in tf_http_archive. #16 1497.1 Repository rule _tf_http_archive defined at:. #16 1497.1 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:66:35: in <toplevel>. #16 1497.2 (21:51:08) Loading: 0 packages loaded. #16 1497.3 (21:51:08) ERROR: no such package '@tf_runtime//': java.io.IOException: Error downloading [http://mirror.tensorflow.org/github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz, https://github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz] to /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/tf_runtime/temp12516918929418979294/64c92c8013b557087351c91b5423b6046d10f206.tar.gz: Checksum was 8383b3247286016e450b0b20e805d26b88ab4638b4e4e3cc4a6923debaf7ad1e but wanted f16fcf09b34e0c7be9389f50652b4b4a14c5a8a96e7e15ad73e8f234d8d09ebe. #16 1497.3 (21:51:09) INFO: Elapsed time: 12.546s. #16 1497.3 (21:51:09) INFO: 0 processes. #16 1497.3 (21:51:09) FAILED: Build did NOT complete successfully (0 packages loaded). #16 1497.3 (21:51:09) FAILED: Build did NOT complete successfully (0 packages loaded). #16 ERROR: executor failed running [/bin/sh -c ./build-prereq.sh && PATH=""${HOME}/bin:${PATH}"" ./build_release_binaries.sh # PATH for bazel]: exit code: 1. ------. > [builder 6/6] RUN ./build-prereq.sh && PATH=""${HOME}/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin"" ./build_release_binaries.sh # PATH for bazel:. ------. executor failed running [/bin/sh -c ./build-prereq.sh && PATH=""${HOME}/bin:${PATH}"" ./build_release_binaries.sh # PATH for bazel]: exit code: 1. ```.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/608
https://github.com/google/deepvariant/issues/608:2194,energy efficiency,core,core,2194,"eading rc options for 'build' from /opt/tensorflow/.bazelrc:. #16 1489.8 'build' options: --define framework_shared_object=true --java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --host_java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true --deleted_packages=tensorflow/compiler/mlir/tfrt,tensorflow/compiler/mlir/tfrt/benchmarks,tensorflow/compiler/mlir/tfrt/jit/python_binding,tensorflow/compiler/mlir/tfrt/jit/transforms,tensorflow/compiler/mlir/tfrt/python_tests,tensorflow/compiler/mlir/tfrt/tests,tensorflow/compiler/mlir/tfrt/tests/saved_model,tensorflow/compiler/mlir/tfrt/transforms/lhlo_gpu_to_tfrt_gpu,tensorflow/core/runtime_fallback,tensorflow/core/runtime_fallback/conversion,tensorflow/core/runtime_fallback/kernel,tensorflow/core/runtime_fallback/opdefs,tensorflow/core/runtime_fallback/runtime,tensorflow/core/runtime_fallback/util,tensorflow/core/tfrt/common,tensorflow/core/tfrt/eager,tensorflow/core/tfrt/eager/backends/cpu,tensorflow/core/tfrt/eager/backends/gpu,tensorflow/core/tfrt/eager/core_runtime,tensorflow/core/tfrt/eager/cpp_tests/core_runtime,tensorflow/core/tfrt/fallback,tensorflow/core/tfrt/gpu,tensorflow/core/tfrt/run_handler_thread_pool,tensorflow/core/tfrt/runtime,tensorflow/core/tfrt/saved_model,tensorflow/core/tfrt/saved_model/tests,tensorflow/core/tfrt/tpu,tensorflow/core/tfrt/utils. #16 1489.8 (21:51:01) INFO: Reading rc options for 'build' from /opt/tensorflow/.tf_configure.bazelrc:. #16 1489.8 'build' options: --action_env PYTHON_BIN_PATH=/usr/local/bin/python3 --action_env PYTHON_LIB_PATH=/usr/local/lib/python3.8/dist-packages --python_path=/usr/local/bin/python3. #16 1489",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/608
https://github.com/google/deepvariant/issues/608:2227,energy efficiency,core,core,2227,"m /opt/tensorflow/.bazelrc:. #16 1489.8 'build' options: --define framework_shared_object=true --java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --host_java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true --deleted_packages=tensorflow/compiler/mlir/tfrt,tensorflow/compiler/mlir/tfrt/benchmarks,tensorflow/compiler/mlir/tfrt/jit/python_binding,tensorflow/compiler/mlir/tfrt/jit/transforms,tensorflow/compiler/mlir/tfrt/python_tests,tensorflow/compiler/mlir/tfrt/tests,tensorflow/compiler/mlir/tfrt/tests/saved_model,tensorflow/compiler/mlir/tfrt/transforms/lhlo_gpu_to_tfrt_gpu,tensorflow/core/runtime_fallback,tensorflow/core/runtime_fallback/conversion,tensorflow/core/runtime_fallback/kernel,tensorflow/core/runtime_fallback/opdefs,tensorflow/core/runtime_fallback/runtime,tensorflow/core/runtime_fallback/util,tensorflow/core/tfrt/common,tensorflow/core/tfrt/eager,tensorflow/core/tfrt/eager/backends/cpu,tensorflow/core/tfrt/eager/backends/gpu,tensorflow/core/tfrt/eager/core_runtime,tensorflow/core/tfrt/eager/cpp_tests/core_runtime,tensorflow/core/tfrt/fallback,tensorflow/core/tfrt/gpu,tensorflow/core/tfrt/run_handler_thread_pool,tensorflow/core/tfrt/runtime,tensorflow/core/tfrt/saved_model,tensorflow/core/tfrt/saved_model/tests,tensorflow/core/tfrt/tpu,tensorflow/core/tfrt/utils. #16 1489.8 (21:51:01) INFO: Reading rc options for 'build' from /opt/tensorflow/.tf_configure.bazelrc:. #16 1489.8 'build' options: --action_env PYTHON_BIN_PATH=/usr/local/bin/python3 --action_env PYTHON_LIB_PATH=/usr/local/lib/python3.8/dist-packages --python_path=/usr/local/bin/python3. #16 1489.8 (21:51:01) INFO: Reading rc op",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/608
https://github.com/google/deepvariant/issues/608:2271,energy efficiency,core,core,2271,"ld' options: --define framework_shared_object=true --java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --host_java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true --deleted_packages=tensorflow/compiler/mlir/tfrt,tensorflow/compiler/mlir/tfrt/benchmarks,tensorflow/compiler/mlir/tfrt/jit/python_binding,tensorflow/compiler/mlir/tfrt/jit/transforms,tensorflow/compiler/mlir/tfrt/python_tests,tensorflow/compiler/mlir/tfrt/tests,tensorflow/compiler/mlir/tfrt/tests/saved_model,tensorflow/compiler/mlir/tfrt/transforms/lhlo_gpu_to_tfrt_gpu,tensorflow/core/runtime_fallback,tensorflow/core/runtime_fallback/conversion,tensorflow/core/runtime_fallback/kernel,tensorflow/core/runtime_fallback/opdefs,tensorflow/core/runtime_fallback/runtime,tensorflow/core/runtime_fallback/util,tensorflow/core/tfrt/common,tensorflow/core/tfrt/eager,tensorflow/core/tfrt/eager/backends/cpu,tensorflow/core/tfrt/eager/backends/gpu,tensorflow/core/tfrt/eager/core_runtime,tensorflow/core/tfrt/eager/cpp_tests/core_runtime,tensorflow/core/tfrt/fallback,tensorflow/core/tfrt/gpu,tensorflow/core/tfrt/run_handler_thread_pool,tensorflow/core/tfrt/runtime,tensorflow/core/tfrt/saved_model,tensorflow/core/tfrt/saved_model/tests,tensorflow/core/tfrt/tpu,tensorflow/core/tfrt/utils. #16 1489.8 (21:51:01) INFO: Reading rc options for 'build' from /opt/tensorflow/.tf_configure.bazelrc:. #16 1489.8 'build' options: --action_env PYTHON_BIN_PATH=/usr/local/bin/python3 --action_env PYTHON_LIB_PATH=/usr/local/lib/python3.8/dist-packages --python_path=/usr/local/bin/python3. #16 1489.8 (21:51:01) INFO: Reading rc options for 'build' from /opt/deepvariant/.baz",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/608
https://github.com/google/deepvariant/issues/608:2311,energy efficiency,core,core,2311,"bject=true --java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --host_java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true --deleted_packages=tensorflow/compiler/mlir/tfrt,tensorflow/compiler/mlir/tfrt/benchmarks,tensorflow/compiler/mlir/tfrt/jit/python_binding,tensorflow/compiler/mlir/tfrt/jit/transforms,tensorflow/compiler/mlir/tfrt/python_tests,tensorflow/compiler/mlir/tfrt/tests,tensorflow/compiler/mlir/tfrt/tests/saved_model,tensorflow/compiler/mlir/tfrt/transforms/lhlo_gpu_to_tfrt_gpu,tensorflow/core/runtime_fallback,tensorflow/core/runtime_fallback/conversion,tensorflow/core/runtime_fallback/kernel,tensorflow/core/runtime_fallback/opdefs,tensorflow/core/runtime_fallback/runtime,tensorflow/core/runtime_fallback/util,tensorflow/core/tfrt/common,tensorflow/core/tfrt/eager,tensorflow/core/tfrt/eager/backends/cpu,tensorflow/core/tfrt/eager/backends/gpu,tensorflow/core/tfrt/eager/core_runtime,tensorflow/core/tfrt/eager/cpp_tests/core_runtime,tensorflow/core/tfrt/fallback,tensorflow/core/tfrt/gpu,tensorflow/core/tfrt/run_handler_thread_pool,tensorflow/core/tfrt/runtime,tensorflow/core/tfrt/saved_model,tensorflow/core/tfrt/saved_model/tests,tensorflow/core/tfrt/tpu,tensorflow/core/tfrt/utils. #16 1489.8 (21:51:01) INFO: Reading rc options for 'build' from /opt/tensorflow/.tf_configure.bazelrc:. #16 1489.8 'build' options: --action_env PYTHON_BIN_PATH=/usr/local/bin/python3 --action_env PYTHON_LIB_PATH=/usr/local/lib/python3.8/dist-packages --python_path=/usr/local/bin/python3. #16 1489.8 (21:51:01) INFO: Reading rc options for 'build' from /opt/deepvariant/.bazelrc:. #16 1489.8 'build' options: --job",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/608
https://github.com/google/deepvariant/issues/608:2351,energy efficiency,core,core,2351,"ns//toolchains/java:tf_java_toolchain --host_java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true --deleted_packages=tensorflow/compiler/mlir/tfrt,tensorflow/compiler/mlir/tfrt/benchmarks,tensorflow/compiler/mlir/tfrt/jit/python_binding,tensorflow/compiler/mlir/tfrt/jit/transforms,tensorflow/compiler/mlir/tfrt/python_tests,tensorflow/compiler/mlir/tfrt/tests,tensorflow/compiler/mlir/tfrt/tests/saved_model,tensorflow/compiler/mlir/tfrt/transforms/lhlo_gpu_to_tfrt_gpu,tensorflow/core/runtime_fallback,tensorflow/core/runtime_fallback/conversion,tensorflow/core/runtime_fallback/kernel,tensorflow/core/runtime_fallback/opdefs,tensorflow/core/runtime_fallback/runtime,tensorflow/core/runtime_fallback/util,tensorflow/core/tfrt/common,tensorflow/core/tfrt/eager,tensorflow/core/tfrt/eager/backends/cpu,tensorflow/core/tfrt/eager/backends/gpu,tensorflow/core/tfrt/eager/core_runtime,tensorflow/core/tfrt/eager/cpp_tests/core_runtime,tensorflow/core/tfrt/fallback,tensorflow/core/tfrt/gpu,tensorflow/core/tfrt/run_handler_thread_pool,tensorflow/core/tfrt/runtime,tensorflow/core/tfrt/saved_model,tensorflow/core/tfrt/saved_model/tests,tensorflow/core/tfrt/tpu,tensorflow/core/tfrt/utils. #16 1489.8 (21:51:01) INFO: Reading rc options for 'build' from /opt/tensorflow/.tf_configure.bazelrc:. #16 1489.8 'build' options: --action_env PYTHON_BIN_PATH=/usr/local/bin/python3 --action_env PYTHON_LIB_PATH=/usr/local/lib/python3.8/dist-packages --python_path=/usr/local/bin/python3. #16 1489.8 (21:51:01) INFO: Reading rc options for 'build' from /opt/deepvariant/.bazelrc:. #16 1489.8 'build' options: --jobs 128 --config=monolithic --show_timesta",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/608
https://github.com/google/deepvariant/issues/608:2392,energy efficiency,core,core,2392,"ost_java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true --deleted_packages=tensorflow/compiler/mlir/tfrt,tensorflow/compiler/mlir/tfrt/benchmarks,tensorflow/compiler/mlir/tfrt/jit/python_binding,tensorflow/compiler/mlir/tfrt/jit/transforms,tensorflow/compiler/mlir/tfrt/python_tests,tensorflow/compiler/mlir/tfrt/tests,tensorflow/compiler/mlir/tfrt/tests/saved_model,tensorflow/compiler/mlir/tfrt/transforms/lhlo_gpu_to_tfrt_gpu,tensorflow/core/runtime_fallback,tensorflow/core/runtime_fallback/conversion,tensorflow/core/runtime_fallback/kernel,tensorflow/core/runtime_fallback/opdefs,tensorflow/core/runtime_fallback/runtime,tensorflow/core/runtime_fallback/util,tensorflow/core/tfrt/common,tensorflow/core/tfrt/eager,tensorflow/core/tfrt/eager/backends/cpu,tensorflow/core/tfrt/eager/backends/gpu,tensorflow/core/tfrt/eager/core_runtime,tensorflow/core/tfrt/eager/cpp_tests/core_runtime,tensorflow/core/tfrt/fallback,tensorflow/core/tfrt/gpu,tensorflow/core/tfrt/run_handler_thread_pool,tensorflow/core/tfrt/runtime,tensorflow/core/tfrt/saved_model,tensorflow/core/tfrt/saved_model/tests,tensorflow/core/tfrt/tpu,tensorflow/core/tfrt/utils. #16 1489.8 (21:51:01) INFO: Reading rc options for 'build' from /opt/tensorflow/.tf_configure.bazelrc:. #16 1489.8 'build' options: --action_env PYTHON_BIN_PATH=/usr/local/bin/python3 --action_env PYTHON_LIB_PATH=/usr/local/lib/python3.8/dist-packages --python_path=/usr/local/bin/python3. #16 1489.8 (21:51:01) INFO: Reading rc options for 'build' from /opt/deepvariant/.bazelrc:. #16 1489.8 'build' options: --jobs 128 --config=monolithic --show_timestamps --verbose_failures --define=use_fast_",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/608
https://github.com/google/deepvariant/issues/608:2430,energy efficiency,core,core,2430,"lchains/java:tf_java_toolchain --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true --deleted_packages=tensorflow/compiler/mlir/tfrt,tensorflow/compiler/mlir/tfrt/benchmarks,tensorflow/compiler/mlir/tfrt/jit/python_binding,tensorflow/compiler/mlir/tfrt/jit/transforms,tensorflow/compiler/mlir/tfrt/python_tests,tensorflow/compiler/mlir/tfrt/tests,tensorflow/compiler/mlir/tfrt/tests/saved_model,tensorflow/compiler/mlir/tfrt/transforms/lhlo_gpu_to_tfrt_gpu,tensorflow/core/runtime_fallback,tensorflow/core/runtime_fallback/conversion,tensorflow/core/runtime_fallback/kernel,tensorflow/core/runtime_fallback/opdefs,tensorflow/core/runtime_fallback/runtime,tensorflow/core/runtime_fallback/util,tensorflow/core/tfrt/common,tensorflow/core/tfrt/eager,tensorflow/core/tfrt/eager/backends/cpu,tensorflow/core/tfrt/eager/backends/gpu,tensorflow/core/tfrt/eager/core_runtime,tensorflow/core/tfrt/eager/cpp_tests/core_runtime,tensorflow/core/tfrt/fallback,tensorflow/core/tfrt/gpu,tensorflow/core/tfrt/run_handler_thread_pool,tensorflow/core/tfrt/runtime,tensorflow/core/tfrt/saved_model,tensorflow/core/tfrt/saved_model/tests,tensorflow/core/tfrt/tpu,tensorflow/core/tfrt/utils. #16 1489.8 (21:51:01) INFO: Reading rc options for 'build' from /opt/tensorflow/.tf_configure.bazelrc:. #16 1489.8 'build' options: --action_env PYTHON_BIN_PATH=/usr/local/bin/python3 --action_env PYTHON_LIB_PATH=/usr/local/lib/python3.8/dist-packages --python_path=/usr/local/bin/python3. #16 1489.8 (21:51:01) INFO: Reading rc options for 'build' from /opt/deepvariant/.bazelrc:. #16 1489.8 'build' options: --jobs 128 --config=monolithic --show_timestamps --verbose_failures --define=use_fast_cpp_protos=true --copt=-Wno-maybe-unin",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/608
https://github.com/google/deepvariant/issues/608:2458,energy efficiency,core,core,2458,"in --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true --deleted_packages=tensorflow/compiler/mlir/tfrt,tensorflow/compiler/mlir/tfrt/benchmarks,tensorflow/compiler/mlir/tfrt/jit/python_binding,tensorflow/compiler/mlir/tfrt/jit/transforms,tensorflow/compiler/mlir/tfrt/python_tests,tensorflow/compiler/mlir/tfrt/tests,tensorflow/compiler/mlir/tfrt/tests/saved_model,tensorflow/compiler/mlir/tfrt/transforms/lhlo_gpu_to_tfrt_gpu,tensorflow/core/runtime_fallback,tensorflow/core/runtime_fallback/conversion,tensorflow/core/runtime_fallback/kernel,tensorflow/core/runtime_fallback/opdefs,tensorflow/core/runtime_fallback/runtime,tensorflow/core/runtime_fallback/util,tensorflow/core/tfrt/common,tensorflow/core/tfrt/eager,tensorflow/core/tfrt/eager/backends/cpu,tensorflow/core/tfrt/eager/backends/gpu,tensorflow/core/tfrt/eager/core_runtime,tensorflow/core/tfrt/eager/cpp_tests/core_runtime,tensorflow/core/tfrt/fallback,tensorflow/core/tfrt/gpu,tensorflow/core/tfrt/run_handler_thread_pool,tensorflow/core/tfrt/runtime,tensorflow/core/tfrt/saved_model,tensorflow/core/tfrt/saved_model/tests,tensorflow/core/tfrt/tpu,tensorflow/core/tfrt/utils. #16 1489.8 (21:51:01) INFO: Reading rc options for 'build' from /opt/tensorflow/.tf_configure.bazelrc:. #16 1489.8 'build' options: --action_env PYTHON_BIN_PATH=/usr/local/bin/python3 --action_env PYTHON_LIB_PATH=/usr/local/lib/python3.8/dist-packages --python_path=/usr/local/bin/python3. #16 1489.8 (21:51:01) INFO: Reading rc options for 'build' from /opt/deepvariant/.bazelrc:. #16 1489.8 'build' options: --jobs 128 --config=monolithic --show_timestamps --verbose_failures --define=use_fast_cpp_protos=true --copt=-Wno-maybe-uninitialized --copt=-Wno-unused",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/608
https://github.com/google/deepvariant/issues/608:2485,energy efficiency,core,core,2485,"otos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true --deleted_packages=tensorflow/compiler/mlir/tfrt,tensorflow/compiler/mlir/tfrt/benchmarks,tensorflow/compiler/mlir/tfrt/jit/python_binding,tensorflow/compiler/mlir/tfrt/jit/transforms,tensorflow/compiler/mlir/tfrt/python_tests,tensorflow/compiler/mlir/tfrt/tests,tensorflow/compiler/mlir/tfrt/tests/saved_model,tensorflow/compiler/mlir/tfrt/transforms/lhlo_gpu_to_tfrt_gpu,tensorflow/core/runtime_fallback,tensorflow/core/runtime_fallback/conversion,tensorflow/core/runtime_fallback/kernel,tensorflow/core/runtime_fallback/opdefs,tensorflow/core/runtime_fallback/runtime,tensorflow/core/runtime_fallback/util,tensorflow/core/tfrt/common,tensorflow/core/tfrt/eager,tensorflow/core/tfrt/eager/backends/cpu,tensorflow/core/tfrt/eager/backends/gpu,tensorflow/core/tfrt/eager/core_runtime,tensorflow/core/tfrt/eager/cpp_tests/core_runtime,tensorflow/core/tfrt/fallback,tensorflow/core/tfrt/gpu,tensorflow/core/tfrt/run_handler_thread_pool,tensorflow/core/tfrt/runtime,tensorflow/core/tfrt/saved_model,tensorflow/core/tfrt/saved_model/tests,tensorflow/core/tfrt/tpu,tensorflow/core/tfrt/utils. #16 1489.8 (21:51:01) INFO: Reading rc options for 'build' from /opt/tensorflow/.tf_configure.bazelrc:. #16 1489.8 'build' options: --action_env PYTHON_BIN_PATH=/usr/local/bin/python3 --action_env PYTHON_LIB_PATH=/usr/local/lib/python3.8/dist-packages --python_path=/usr/local/bin/python3. #16 1489.8 (21:51:01) INFO: Reading rc options for 'build' from /opt/deepvariant/.bazelrc:. #16 1489.8 'build' options: --jobs 128 --config=monolithic --show_timestamps --verbose_failures --define=use_fast_cpp_protos=true --copt=-Wno-maybe-uninitialized --copt=-Wno-unused-function --cxxopt=-std=c++",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/608
https://github.com/google/deepvariant/issues/608:2510,energy efficiency,cpu,cpu,2510,"oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true --deleted_packages=tensorflow/compiler/mlir/tfrt,tensorflow/compiler/mlir/tfrt/benchmarks,tensorflow/compiler/mlir/tfrt/jit/python_binding,tensorflow/compiler/mlir/tfrt/jit/transforms,tensorflow/compiler/mlir/tfrt/python_tests,tensorflow/compiler/mlir/tfrt/tests,tensorflow/compiler/mlir/tfrt/tests/saved_model,tensorflow/compiler/mlir/tfrt/transforms/lhlo_gpu_to_tfrt_gpu,tensorflow/core/runtime_fallback,tensorflow/core/runtime_fallback/conversion,tensorflow/core/runtime_fallback/kernel,tensorflow/core/runtime_fallback/opdefs,tensorflow/core/runtime_fallback/runtime,tensorflow/core/runtime_fallback/util,tensorflow/core/tfrt/common,tensorflow/core/tfrt/eager,tensorflow/core/tfrt/eager/backends/cpu,tensorflow/core/tfrt/eager/backends/gpu,tensorflow/core/tfrt/eager/core_runtime,tensorflow/core/tfrt/eager/cpp_tests/core_runtime,tensorflow/core/tfrt/fallback,tensorflow/core/tfrt/gpu,tensorflow/core/tfrt/run_handler_thread_pool,tensorflow/core/tfrt/runtime,tensorflow/core/tfrt/saved_model,tensorflow/core/tfrt/saved_model/tests,tensorflow/core/tfrt/tpu,tensorflow/core/tfrt/utils. #16 1489.8 (21:51:01) INFO: Reading rc options for 'build' from /opt/tensorflow/.tf_configure.bazelrc:. #16 1489.8 'build' options: --action_env PYTHON_BIN_PATH=/usr/local/bin/python3 --action_env PYTHON_LIB_PATH=/usr/local/lib/python3.8/dist-packages --python_path=/usr/local/bin/python3. #16 1489.8 (21:51:01) INFO: Reading rc options for 'build' from /opt/deepvariant/.bazelrc:. #16 1489.8 'build' options: --jobs 128 --config=monolithic --show_timestamps --verbose_failures --define=use_fast_cpp_protos=true --copt=-Wno-maybe-uninitialized --copt=-Wno-unused-function --cxxopt=-std=c++17 --python_top=//:deepva",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/608
https://github.com/google/deepvariant/issues/608:2525,energy efficiency,core,core,2525,"=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true --deleted_packages=tensorflow/compiler/mlir/tfrt,tensorflow/compiler/mlir/tfrt/benchmarks,tensorflow/compiler/mlir/tfrt/jit/python_binding,tensorflow/compiler/mlir/tfrt/jit/transforms,tensorflow/compiler/mlir/tfrt/python_tests,tensorflow/compiler/mlir/tfrt/tests,tensorflow/compiler/mlir/tfrt/tests/saved_model,tensorflow/compiler/mlir/tfrt/transforms/lhlo_gpu_to_tfrt_gpu,tensorflow/core/runtime_fallback,tensorflow/core/runtime_fallback/conversion,tensorflow/core/runtime_fallback/kernel,tensorflow/core/runtime_fallback/opdefs,tensorflow/core/runtime_fallback/runtime,tensorflow/core/runtime_fallback/util,tensorflow/core/tfrt/common,tensorflow/core/tfrt/eager,tensorflow/core/tfrt/eager/backends/cpu,tensorflow/core/tfrt/eager/backends/gpu,tensorflow/core/tfrt/eager/core_runtime,tensorflow/core/tfrt/eager/cpp_tests/core_runtime,tensorflow/core/tfrt/fallback,tensorflow/core/tfrt/gpu,tensorflow/core/tfrt/run_handler_thread_pool,tensorflow/core/tfrt/runtime,tensorflow/core/tfrt/saved_model,tensorflow/core/tfrt/saved_model/tests,tensorflow/core/tfrt/tpu,tensorflow/core/tfrt/utils. #16 1489.8 (21:51:01) INFO: Reading rc options for 'build' from /opt/tensorflow/.tf_configure.bazelrc:. #16 1489.8 'build' options: --action_env PYTHON_BIN_PATH=/usr/local/bin/python3 --action_env PYTHON_LIB_PATH=/usr/local/lib/python3.8/dist-packages --python_path=/usr/local/bin/python3. #16 1489.8 (21:51:01) INFO: Reading rc options for 'build' from /opt/deepvariant/.bazelrc:. #16 1489.8 'build' options: --jobs 128 --config=monolithic --show_timestamps --verbose_failures --define=use_fast_cpp_protos=true --copt=-Wno-maybe-uninitialized --copt=-Wno-unused-function --cxxopt=-std=c++17 --python_top=//:deepvariant_python_ru",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/608
https://github.com/google/deepvariant/issues/608:2550,energy efficiency,gpu,gpu,2550,"andalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true --deleted_packages=tensorflow/compiler/mlir/tfrt,tensorflow/compiler/mlir/tfrt/benchmarks,tensorflow/compiler/mlir/tfrt/jit/python_binding,tensorflow/compiler/mlir/tfrt/jit/transforms,tensorflow/compiler/mlir/tfrt/python_tests,tensorflow/compiler/mlir/tfrt/tests,tensorflow/compiler/mlir/tfrt/tests/saved_model,tensorflow/compiler/mlir/tfrt/transforms/lhlo_gpu_to_tfrt_gpu,tensorflow/core/runtime_fallback,tensorflow/core/runtime_fallback/conversion,tensorflow/core/runtime_fallback/kernel,tensorflow/core/runtime_fallback/opdefs,tensorflow/core/runtime_fallback/runtime,tensorflow/core/runtime_fallback/util,tensorflow/core/tfrt/common,tensorflow/core/tfrt/eager,tensorflow/core/tfrt/eager/backends/cpu,tensorflow/core/tfrt/eager/backends/gpu,tensorflow/core/tfrt/eager/core_runtime,tensorflow/core/tfrt/eager/cpp_tests/core_runtime,tensorflow/core/tfrt/fallback,tensorflow/core/tfrt/gpu,tensorflow/core/tfrt/run_handler_thread_pool,tensorflow/core/tfrt/runtime,tensorflow/core/tfrt/saved_model,tensorflow/core/tfrt/saved_model/tests,tensorflow/core/tfrt/tpu,tensorflow/core/tfrt/utils. #16 1489.8 (21:51:01) INFO: Reading rc options for 'build' from /opt/tensorflow/.tf_configure.bazelrc:. #16 1489.8 'build' options: --action_env PYTHON_BIN_PATH=/usr/local/bin/python3 --action_env PYTHON_LIB_PATH=/usr/local/lib/python3.8/dist-packages --python_path=/usr/local/bin/python3. #16 1489.8 (21:51:01) INFO: Reading rc options for 'build' from /opt/deepvariant/.bazelrc:. #16 1489.8 'build' options: --jobs 128 --config=monolithic --show_timestamps --verbose_failures --define=use_fast_cpp_protos=true --copt=-Wno-maybe-uninitialized --copt=-Wno-unused-function --cxxopt=-std=c++17 --python_top=//:deepvariant_python_runtime --incompatible_use_",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/608
https://github.com/google/deepvariant/issues/608:2565,energy efficiency,core,core,2565," --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true --deleted_packages=tensorflow/compiler/mlir/tfrt,tensorflow/compiler/mlir/tfrt/benchmarks,tensorflow/compiler/mlir/tfrt/jit/python_binding,tensorflow/compiler/mlir/tfrt/jit/transforms,tensorflow/compiler/mlir/tfrt/python_tests,tensorflow/compiler/mlir/tfrt/tests,tensorflow/compiler/mlir/tfrt/tests/saved_model,tensorflow/compiler/mlir/tfrt/transforms/lhlo_gpu_to_tfrt_gpu,tensorflow/core/runtime_fallback,tensorflow/core/runtime_fallback/conversion,tensorflow/core/runtime_fallback/kernel,tensorflow/core/runtime_fallback/opdefs,tensorflow/core/runtime_fallback/runtime,tensorflow/core/runtime_fallback/util,tensorflow/core/tfrt/common,tensorflow/core/tfrt/eager,tensorflow/core/tfrt/eager/backends/cpu,tensorflow/core/tfrt/eager/backends/gpu,tensorflow/core/tfrt/eager/core_runtime,tensorflow/core/tfrt/eager/cpp_tests/core_runtime,tensorflow/core/tfrt/fallback,tensorflow/core/tfrt/gpu,tensorflow/core/tfrt/run_handler_thread_pool,tensorflow/core/tfrt/runtime,tensorflow/core/tfrt/saved_model,tensorflow/core/tfrt/saved_model/tests,tensorflow/core/tfrt/tpu,tensorflow/core/tfrt/utils. #16 1489.8 (21:51:01) INFO: Reading rc options for 'build' from /opt/tensorflow/.tf_configure.bazelrc:. #16 1489.8 'build' options: --action_env PYTHON_BIN_PATH=/usr/local/bin/python3 --action_env PYTHON_LIB_PATH=/usr/local/lib/python3.8/dist-packages --python_path=/usr/local/bin/python3. #16 1489.8 (21:51:01) INFO: Reading rc options for 'build' from /opt/deepvariant/.bazelrc:. #16 1489.8 'build' options: --jobs 128 --config=monolithic --show_timestamps --verbose_failures --define=use_fast_cpp_protos=true --copt=-Wno-maybe-uninitialized --copt=-Wno-unused-function --cxxopt=-std=c++17 --python_top=//:deepvariant_python_runtime --incompatible_use_python_toolchai",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/608
https://github.com/google/deepvariant/issues/608:2605,energy efficiency,core,core,2605,"e --noincompatible_remove_legacy_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true --deleted_packages=tensorflow/compiler/mlir/tfrt,tensorflow/compiler/mlir/tfrt/benchmarks,tensorflow/compiler/mlir/tfrt/jit/python_binding,tensorflow/compiler/mlir/tfrt/jit/transforms,tensorflow/compiler/mlir/tfrt/python_tests,tensorflow/compiler/mlir/tfrt/tests,tensorflow/compiler/mlir/tfrt/tests/saved_model,tensorflow/compiler/mlir/tfrt/transforms/lhlo_gpu_to_tfrt_gpu,tensorflow/core/runtime_fallback,tensorflow/core/runtime_fallback/conversion,tensorflow/core/runtime_fallback/kernel,tensorflow/core/runtime_fallback/opdefs,tensorflow/core/runtime_fallback/runtime,tensorflow/core/runtime_fallback/util,tensorflow/core/tfrt/common,tensorflow/core/tfrt/eager,tensorflow/core/tfrt/eager/backends/cpu,tensorflow/core/tfrt/eager/backends/gpu,tensorflow/core/tfrt/eager/core_runtime,tensorflow/core/tfrt/eager/cpp_tests/core_runtime,tensorflow/core/tfrt/fallback,tensorflow/core/tfrt/gpu,tensorflow/core/tfrt/run_handler_thread_pool,tensorflow/core/tfrt/runtime,tensorflow/core/tfrt/saved_model,tensorflow/core/tfrt/saved_model/tests,tensorflow/core/tfrt/tpu,tensorflow/core/tfrt/utils. #16 1489.8 (21:51:01) INFO: Reading rc options for 'build' from /opt/tensorflow/.tf_configure.bazelrc:. #16 1489.8 'build' options: --action_env PYTHON_BIN_PATH=/usr/local/bin/python3 --action_env PYTHON_LIB_PATH=/usr/local/lib/python3.8/dist-packages --python_path=/usr/local/bin/python3. #16 1489.8 (21:51:01) INFO: Reading rc options for 'build' from /opt/deepvariant/.bazelrc:. #16 1489.8 'build' options: --jobs 128 --config=monolithic --show_timestamps --verbose_failures --define=use_fast_cpp_protos=true --copt=-Wno-maybe-uninitialized --copt=-Wno-unused-function --cxxopt=-std=c++17 --python_top=//:deepvariant_python_runtime --incompatible_use_python_toolchains=false. #16 1489.8 (21:51:01) INFO: Fo",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/608
https://github.com/google/deepvariant/issues/608:2655,energy efficiency,core,core,2655,"nable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true --deleted_packages=tensorflow/compiler/mlir/tfrt,tensorflow/compiler/mlir/tfrt/benchmarks,tensorflow/compiler/mlir/tfrt/jit/python_binding,tensorflow/compiler/mlir/tfrt/jit/transforms,tensorflow/compiler/mlir/tfrt/python_tests,tensorflow/compiler/mlir/tfrt/tests,tensorflow/compiler/mlir/tfrt/tests/saved_model,tensorflow/compiler/mlir/tfrt/transforms/lhlo_gpu_to_tfrt_gpu,tensorflow/core/runtime_fallback,tensorflow/core/runtime_fallback/conversion,tensorflow/core/runtime_fallback/kernel,tensorflow/core/runtime_fallback/opdefs,tensorflow/core/runtime_fallback/runtime,tensorflow/core/runtime_fallback/util,tensorflow/core/tfrt/common,tensorflow/core/tfrt/eager,tensorflow/core/tfrt/eager/backends/cpu,tensorflow/core/tfrt/eager/backends/gpu,tensorflow/core/tfrt/eager/core_runtime,tensorflow/core/tfrt/eager/cpp_tests/core_runtime,tensorflow/core/tfrt/fallback,tensorflow/core/tfrt/gpu,tensorflow/core/tfrt/run_handler_thread_pool,tensorflow/core/tfrt/runtime,tensorflow/core/tfrt/saved_model,tensorflow/core/tfrt/saved_model/tests,tensorflow/core/tfrt/tpu,tensorflow/core/tfrt/utils. #16 1489.8 (21:51:01) INFO: Reading rc options for 'build' from /opt/tensorflow/.tf_configure.bazelrc:. #16 1489.8 'build' options: --action_env PYTHON_BIN_PATH=/usr/local/bin/python3 --action_env PYTHON_LIB_PATH=/usr/local/lib/python3.8/dist-packages --python_path=/usr/local/bin/python3. #16 1489.8 (21:51:01) INFO: Reading rc options for 'build' from /opt/deepvariant/.bazelrc:. #16 1489.8 'build' options: --jobs 128 --config=monolithic --show_timestamps --verbose_failures --define=use_fast_cpp_protos=true --copt=-Wno-maybe-uninitialized --copt=-Wno-unused-function --cxxopt=-std=c++17 --python_top=//:deepvariant_python_runtime --incompatible_use_python_toolchains=false. #16 1489.8 (21:51:01) INFO: Found applicable config definition build:short_logs ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/608
https://github.com/google/deepvariant/issues/608:2685,energy efficiency,core,core,2685," --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true --deleted_packages=tensorflow/compiler/mlir/tfrt,tensorflow/compiler/mlir/tfrt/benchmarks,tensorflow/compiler/mlir/tfrt/jit/python_binding,tensorflow/compiler/mlir/tfrt/jit/transforms,tensorflow/compiler/mlir/tfrt/python_tests,tensorflow/compiler/mlir/tfrt/tests,tensorflow/compiler/mlir/tfrt/tests/saved_model,tensorflow/compiler/mlir/tfrt/transforms/lhlo_gpu_to_tfrt_gpu,tensorflow/core/runtime_fallback,tensorflow/core/runtime_fallback/conversion,tensorflow/core/runtime_fallback/kernel,tensorflow/core/runtime_fallback/opdefs,tensorflow/core/runtime_fallback/runtime,tensorflow/core/runtime_fallback/util,tensorflow/core/tfrt/common,tensorflow/core/tfrt/eager,tensorflow/core/tfrt/eager/backends/cpu,tensorflow/core/tfrt/eager/backends/gpu,tensorflow/core/tfrt/eager/core_runtime,tensorflow/core/tfrt/eager/cpp_tests/core_runtime,tensorflow/core/tfrt/fallback,tensorflow/core/tfrt/gpu,tensorflow/core/tfrt/run_handler_thread_pool,tensorflow/core/tfrt/runtime,tensorflow/core/tfrt/saved_model,tensorflow/core/tfrt/saved_model/tests,tensorflow/core/tfrt/tpu,tensorflow/core/tfrt/utils. #16 1489.8 (21:51:01) INFO: Reading rc options for 'build' from /opt/tensorflow/.tf_configure.bazelrc:. #16 1489.8 'build' options: --action_env PYTHON_BIN_PATH=/usr/local/bin/python3 --action_env PYTHON_LIB_PATH=/usr/local/lib/python3.8/dist-packages --python_path=/usr/local/bin/python3. #16 1489.8 (21:51:01) INFO: Reading rc options for 'build' from /opt/deepvariant/.bazelrc:. #16 1489.8 'build' options: --jobs 128 --config=monolithic --show_timestamps --verbose_failures --define=use_fast_cpp_protos=true --copt=-Wno-maybe-uninitialized --copt=-Wno-unused-function --cxxopt=-std=c++17 --python_top=//:deepvariant_python_runtime --incompatible_use_python_toolchains=false. #16 1489.8 (21:51:01) INFO: Found applicable config definition build:short_logs in file /opt/tensorflow/.bazel",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/608
https://github.com/google/deepvariant/issues/608:2695,energy efficiency,gpu,gpu,2695,"with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true --deleted_packages=tensorflow/compiler/mlir/tfrt,tensorflow/compiler/mlir/tfrt/benchmarks,tensorflow/compiler/mlir/tfrt/jit/python_binding,tensorflow/compiler/mlir/tfrt/jit/transforms,tensorflow/compiler/mlir/tfrt/python_tests,tensorflow/compiler/mlir/tfrt/tests,tensorflow/compiler/mlir/tfrt/tests/saved_model,tensorflow/compiler/mlir/tfrt/transforms/lhlo_gpu_to_tfrt_gpu,tensorflow/core/runtime_fallback,tensorflow/core/runtime_fallback/conversion,tensorflow/core/runtime_fallback/kernel,tensorflow/core/runtime_fallback/opdefs,tensorflow/core/runtime_fallback/runtime,tensorflow/core/runtime_fallback/util,tensorflow/core/tfrt/common,tensorflow/core/tfrt/eager,tensorflow/core/tfrt/eager/backends/cpu,tensorflow/core/tfrt/eager/backends/gpu,tensorflow/core/tfrt/eager/core_runtime,tensorflow/core/tfrt/eager/cpp_tests/core_runtime,tensorflow/core/tfrt/fallback,tensorflow/core/tfrt/gpu,tensorflow/core/tfrt/run_handler_thread_pool,tensorflow/core/tfrt/runtime,tensorflow/core/tfrt/saved_model,tensorflow/core/tfrt/saved_model/tests,tensorflow/core/tfrt/tpu,tensorflow/core/tfrt/utils. #16 1489.8 (21:51:01) INFO: Reading rc options for 'build' from /opt/tensorflow/.tf_configure.bazelrc:. #16 1489.8 'build' options: --action_env PYTHON_BIN_PATH=/usr/local/bin/python3 --action_env PYTHON_LIB_PATH=/usr/local/lib/python3.8/dist-packages --python_path=/usr/local/bin/python3. #16 1489.8 (21:51:01) INFO: Reading rc options for 'build' from /opt/deepvariant/.bazelrc:. #16 1489.8 'build' options: --jobs 128 --config=monolithic --show_timestamps --verbose_failures --define=use_fast_cpp_protos=true --copt=-Wno-maybe-uninitialized --copt=-Wno-unused-function --cxxopt=-std=c++17 --python_top=//:deepvariant_python_runtime --incompatible_use_python_toolchains=false. #16 1489.8 (21:51:01) INFO: Found applicable config definition build:short_logs in file /opt/tensorflow/.bazelrc: --outp",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/608
https://github.com/google/deepvariant/issues/608:2710,energy efficiency,core,core,2710,"t=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true --deleted_packages=tensorflow/compiler/mlir/tfrt,tensorflow/compiler/mlir/tfrt/benchmarks,tensorflow/compiler/mlir/tfrt/jit/python_binding,tensorflow/compiler/mlir/tfrt/jit/transforms,tensorflow/compiler/mlir/tfrt/python_tests,tensorflow/compiler/mlir/tfrt/tests,tensorflow/compiler/mlir/tfrt/tests/saved_model,tensorflow/compiler/mlir/tfrt/transforms/lhlo_gpu_to_tfrt_gpu,tensorflow/core/runtime_fallback,tensorflow/core/runtime_fallback/conversion,tensorflow/core/runtime_fallback/kernel,tensorflow/core/runtime_fallback/opdefs,tensorflow/core/runtime_fallback/runtime,tensorflow/core/runtime_fallback/util,tensorflow/core/tfrt/common,tensorflow/core/tfrt/eager,tensorflow/core/tfrt/eager/backends/cpu,tensorflow/core/tfrt/eager/backends/gpu,tensorflow/core/tfrt/eager/core_runtime,tensorflow/core/tfrt/eager/cpp_tests/core_runtime,tensorflow/core/tfrt/fallback,tensorflow/core/tfrt/gpu,tensorflow/core/tfrt/run_handler_thread_pool,tensorflow/core/tfrt/runtime,tensorflow/core/tfrt/saved_model,tensorflow/core/tfrt/saved_model/tests,tensorflow/core/tfrt/tpu,tensorflow/core/tfrt/utils. #16 1489.8 (21:51:01) INFO: Reading rc options for 'build' from /opt/tensorflow/.tf_configure.bazelrc:. #16 1489.8 'build' options: --action_env PYTHON_BIN_PATH=/usr/local/bin/python3 --action_env PYTHON_LIB_PATH=/usr/local/lib/python3.8/dist-packages --python_path=/usr/local/bin/python3. #16 1489.8 (21:51:01) INFO: Reading rc options for 'build' from /opt/deepvariant/.bazelrc:. #16 1489.8 'build' options: --jobs 128 --config=monolithic --show_timestamps --verbose_failures --define=use_fast_cpp_protos=true --copt=-Wno-maybe-uninitialized --copt=-Wno-unused-function --cxxopt=-std=c++17 --python_top=//:deepvariant_python_runtime --incompatible_use_python_toolchains=false. #16 1489.8 (21:51:01) INFO: Found applicable config definition build:short_logs in file /opt/tensorflow/.bazelrc: --output_filter=DONT_",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/608
https://github.com/google/deepvariant/issues/608:2755,energy efficiency,core,core,2755,"ne=no_aws_support=true --define=no_hdfs_support=true --deleted_packages=tensorflow/compiler/mlir/tfrt,tensorflow/compiler/mlir/tfrt/benchmarks,tensorflow/compiler/mlir/tfrt/jit/python_binding,tensorflow/compiler/mlir/tfrt/jit/transforms,tensorflow/compiler/mlir/tfrt/python_tests,tensorflow/compiler/mlir/tfrt/tests,tensorflow/compiler/mlir/tfrt/tests/saved_model,tensorflow/compiler/mlir/tfrt/transforms/lhlo_gpu_to_tfrt_gpu,tensorflow/core/runtime_fallback,tensorflow/core/runtime_fallback/conversion,tensorflow/core/runtime_fallback/kernel,tensorflow/core/runtime_fallback/opdefs,tensorflow/core/runtime_fallback/runtime,tensorflow/core/runtime_fallback/util,tensorflow/core/tfrt/common,tensorflow/core/tfrt/eager,tensorflow/core/tfrt/eager/backends/cpu,tensorflow/core/tfrt/eager/backends/gpu,tensorflow/core/tfrt/eager/core_runtime,tensorflow/core/tfrt/eager/cpp_tests/core_runtime,tensorflow/core/tfrt/fallback,tensorflow/core/tfrt/gpu,tensorflow/core/tfrt/run_handler_thread_pool,tensorflow/core/tfrt/runtime,tensorflow/core/tfrt/saved_model,tensorflow/core/tfrt/saved_model/tests,tensorflow/core/tfrt/tpu,tensorflow/core/tfrt/utils. #16 1489.8 (21:51:01) INFO: Reading rc options for 'build' from /opt/tensorflow/.tf_configure.bazelrc:. #16 1489.8 'build' options: --action_env PYTHON_BIN_PATH=/usr/local/bin/python3 --action_env PYTHON_LIB_PATH=/usr/local/lib/python3.8/dist-packages --python_path=/usr/local/bin/python3. #16 1489.8 (21:51:01) INFO: Reading rc options for 'build' from /opt/deepvariant/.bazelrc:. #16 1489.8 'build' options: --jobs 128 --config=monolithic --show_timestamps --verbose_failures --define=use_fast_cpp_protos=true --copt=-Wno-maybe-uninitialized --copt=-Wno-unused-function --cxxopt=-std=c++17 --python_top=//:deepvariant_python_runtime --incompatible_use_python_toolchains=false. #16 1489.8 (21:51:01) INFO: Found applicable config definition build:short_logs in file /opt/tensorflow/.bazelrc: --output_filter=DONT_MATCH_ANYTHING. #16 1489.8 (21:51:01) INFO: F",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/608
https://github.com/google/deepvariant/issues/608:2784,energy efficiency,core,core,2784,"ne=no_hdfs_support=true --deleted_packages=tensorflow/compiler/mlir/tfrt,tensorflow/compiler/mlir/tfrt/benchmarks,tensorflow/compiler/mlir/tfrt/jit/python_binding,tensorflow/compiler/mlir/tfrt/jit/transforms,tensorflow/compiler/mlir/tfrt/python_tests,tensorflow/compiler/mlir/tfrt/tests,tensorflow/compiler/mlir/tfrt/tests/saved_model,tensorflow/compiler/mlir/tfrt/transforms/lhlo_gpu_to_tfrt_gpu,tensorflow/core/runtime_fallback,tensorflow/core/runtime_fallback/conversion,tensorflow/core/runtime_fallback/kernel,tensorflow/core/runtime_fallback/opdefs,tensorflow/core/runtime_fallback/runtime,tensorflow/core/runtime_fallback/util,tensorflow/core/tfrt/common,tensorflow/core/tfrt/eager,tensorflow/core/tfrt/eager/backends/cpu,tensorflow/core/tfrt/eager/backends/gpu,tensorflow/core/tfrt/eager/core_runtime,tensorflow/core/tfrt/eager/cpp_tests/core_runtime,tensorflow/core/tfrt/fallback,tensorflow/core/tfrt/gpu,tensorflow/core/tfrt/run_handler_thread_pool,tensorflow/core/tfrt/runtime,tensorflow/core/tfrt/saved_model,tensorflow/core/tfrt/saved_model/tests,tensorflow/core/tfrt/tpu,tensorflow/core/tfrt/utils. #16 1489.8 (21:51:01) INFO: Reading rc options for 'build' from /opt/tensorflow/.tf_configure.bazelrc:. #16 1489.8 'build' options: --action_env PYTHON_BIN_PATH=/usr/local/bin/python3 --action_env PYTHON_LIB_PATH=/usr/local/lib/python3.8/dist-packages --python_path=/usr/local/bin/python3. #16 1489.8 (21:51:01) INFO: Reading rc options for 'build' from /opt/deepvariant/.bazelrc:. #16 1489.8 'build' options: --jobs 128 --config=monolithic --show_timestamps --verbose_failures --define=use_fast_cpp_protos=true --copt=-Wno-maybe-uninitialized --copt=-Wno-unused-function --cxxopt=-std=c++17 --python_top=//:deepvariant_python_runtime --incompatible_use_python_toolchains=false. #16 1489.8 (21:51:01) INFO: Found applicable config definition build:short_logs in file /opt/tensorflow/.bazelrc: --output_filter=DONT_MATCH_ANYTHING. #16 1489.8 (21:51:01) INFO: Found applicable config defini",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/608
https://github.com/google/deepvariant/issues/608:2817,energy efficiency,core,core,2817,"_packages=tensorflow/compiler/mlir/tfrt,tensorflow/compiler/mlir/tfrt/benchmarks,tensorflow/compiler/mlir/tfrt/jit/python_binding,tensorflow/compiler/mlir/tfrt/jit/transforms,tensorflow/compiler/mlir/tfrt/python_tests,tensorflow/compiler/mlir/tfrt/tests,tensorflow/compiler/mlir/tfrt/tests/saved_model,tensorflow/compiler/mlir/tfrt/transforms/lhlo_gpu_to_tfrt_gpu,tensorflow/core/runtime_fallback,tensorflow/core/runtime_fallback/conversion,tensorflow/core/runtime_fallback/kernel,tensorflow/core/runtime_fallback/opdefs,tensorflow/core/runtime_fallback/runtime,tensorflow/core/runtime_fallback/util,tensorflow/core/tfrt/common,tensorflow/core/tfrt/eager,tensorflow/core/tfrt/eager/backends/cpu,tensorflow/core/tfrt/eager/backends/gpu,tensorflow/core/tfrt/eager/core_runtime,tensorflow/core/tfrt/eager/cpp_tests/core_runtime,tensorflow/core/tfrt/fallback,tensorflow/core/tfrt/gpu,tensorflow/core/tfrt/run_handler_thread_pool,tensorflow/core/tfrt/runtime,tensorflow/core/tfrt/saved_model,tensorflow/core/tfrt/saved_model/tests,tensorflow/core/tfrt/tpu,tensorflow/core/tfrt/utils. #16 1489.8 (21:51:01) INFO: Reading rc options for 'build' from /opt/tensorflow/.tf_configure.bazelrc:. #16 1489.8 'build' options: --action_env PYTHON_BIN_PATH=/usr/local/bin/python3 --action_env PYTHON_LIB_PATH=/usr/local/lib/python3.8/dist-packages --python_path=/usr/local/bin/python3. #16 1489.8 (21:51:01) INFO: Reading rc options for 'build' from /opt/deepvariant/.bazelrc:. #16 1489.8 'build' options: --jobs 128 --config=monolithic --show_timestamps --verbose_failures --define=use_fast_cpp_protos=true --copt=-Wno-maybe-uninitialized --copt=-Wno-unused-function --cxxopt=-std=c++17 --python_top=//:deepvariant_python_runtime --incompatible_use_python_toolchains=false. #16 1489.8 (21:51:01) INFO: Found applicable config definition build:short_logs in file /opt/tensorflow/.bazelrc: --output_filter=DONT_MATCH_ANYTHING. #16 1489.8 (21:51:01) INFO: Found applicable config definition build:v2 in file /opt/tensor",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/608
https://github.com/google/deepvariant/issues/608:2856,energy efficiency,core,core,2856,",tensorflow/compiler/mlir/tfrt/benchmarks,tensorflow/compiler/mlir/tfrt/jit/python_binding,tensorflow/compiler/mlir/tfrt/jit/transforms,tensorflow/compiler/mlir/tfrt/python_tests,tensorflow/compiler/mlir/tfrt/tests,tensorflow/compiler/mlir/tfrt/tests/saved_model,tensorflow/compiler/mlir/tfrt/transforms/lhlo_gpu_to_tfrt_gpu,tensorflow/core/runtime_fallback,tensorflow/core/runtime_fallback/conversion,tensorflow/core/runtime_fallback/kernel,tensorflow/core/runtime_fallback/opdefs,tensorflow/core/runtime_fallback/runtime,tensorflow/core/runtime_fallback/util,tensorflow/core/tfrt/common,tensorflow/core/tfrt/eager,tensorflow/core/tfrt/eager/backends/cpu,tensorflow/core/tfrt/eager/backends/gpu,tensorflow/core/tfrt/eager/core_runtime,tensorflow/core/tfrt/eager/cpp_tests/core_runtime,tensorflow/core/tfrt/fallback,tensorflow/core/tfrt/gpu,tensorflow/core/tfrt/run_handler_thread_pool,tensorflow/core/tfrt/runtime,tensorflow/core/tfrt/saved_model,tensorflow/core/tfrt/saved_model/tests,tensorflow/core/tfrt/tpu,tensorflow/core/tfrt/utils. #16 1489.8 (21:51:01) INFO: Reading rc options for 'build' from /opt/tensorflow/.tf_configure.bazelrc:. #16 1489.8 'build' options: --action_env PYTHON_BIN_PATH=/usr/local/bin/python3 --action_env PYTHON_LIB_PATH=/usr/local/lib/python3.8/dist-packages --python_path=/usr/local/bin/python3. #16 1489.8 (21:51:01) INFO: Reading rc options for 'build' from /opt/deepvariant/.bazelrc:. #16 1489.8 'build' options: --jobs 128 --config=monolithic --show_timestamps --verbose_failures --define=use_fast_cpp_protos=true --copt=-Wno-maybe-uninitialized --copt=-Wno-unused-function --cxxopt=-std=c++17 --python_top=//:deepvariant_python_runtime --incompatible_use_python_toolchains=false. #16 1489.8 (21:51:01) INFO: Found applicable config definition build:short_logs in file /opt/tensorflow/.bazelrc: --output_filter=DONT_MATCH_ANYTHING. #16 1489.8 (21:51:01) INFO: Found applicable config definition build:v2 in file /opt/tensorflow/.bazelrc: --define=tf_api_version=",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/608
https://github.com/google/deepvariant/issues/608:2881,energy efficiency,core,core,2881,"/tfrt/benchmarks,tensorflow/compiler/mlir/tfrt/jit/python_binding,tensorflow/compiler/mlir/tfrt/jit/transforms,tensorflow/compiler/mlir/tfrt/python_tests,tensorflow/compiler/mlir/tfrt/tests,tensorflow/compiler/mlir/tfrt/tests/saved_model,tensorflow/compiler/mlir/tfrt/transforms/lhlo_gpu_to_tfrt_gpu,tensorflow/core/runtime_fallback,tensorflow/core/runtime_fallback/conversion,tensorflow/core/runtime_fallback/kernel,tensorflow/core/runtime_fallback/opdefs,tensorflow/core/runtime_fallback/runtime,tensorflow/core/runtime_fallback/util,tensorflow/core/tfrt/common,tensorflow/core/tfrt/eager,tensorflow/core/tfrt/eager/backends/cpu,tensorflow/core/tfrt/eager/backends/gpu,tensorflow/core/tfrt/eager/core_runtime,tensorflow/core/tfrt/eager/cpp_tests/core_runtime,tensorflow/core/tfrt/fallback,tensorflow/core/tfrt/gpu,tensorflow/core/tfrt/run_handler_thread_pool,tensorflow/core/tfrt/runtime,tensorflow/core/tfrt/saved_model,tensorflow/core/tfrt/saved_model/tests,tensorflow/core/tfrt/tpu,tensorflow/core/tfrt/utils. #16 1489.8 (21:51:01) INFO: Reading rc options for 'build' from /opt/tensorflow/.tf_configure.bazelrc:. #16 1489.8 'build' options: --action_env PYTHON_BIN_PATH=/usr/local/bin/python3 --action_env PYTHON_LIB_PATH=/usr/local/lib/python3.8/dist-packages --python_path=/usr/local/bin/python3. #16 1489.8 (21:51:01) INFO: Reading rc options for 'build' from /opt/deepvariant/.bazelrc:. #16 1489.8 'build' options: --jobs 128 --config=monolithic --show_timestamps --verbose_failures --define=use_fast_cpp_protos=true --copt=-Wno-maybe-uninitialized --copt=-Wno-unused-function --cxxopt=-std=c++17 --python_top=//:deepvariant_python_runtime --incompatible_use_python_toolchains=false. #16 1489.8 (21:51:01) INFO: Found applicable config definition build:short_logs in file /opt/tensorflow/.bazelrc: --output_filter=DONT_MATCH_ANYTHING. #16 1489.8 (21:51:01) INFO: Found applicable config definition build:v2 in file /opt/tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVI",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/608
https://github.com/google/deepvariant/issues/608:4835,energy efficiency,Current,Current,4835,ine=tf_api_version=2 --action_env=TF2_BEHAVIOR=1. #16 1489.8 (21:51:01) INFO: Found applicable config definition build:monolithic in file /opt/tensorflow/.bazelrc: --define framework_shared_object=false. #16 1489.8 (21:51:01) INFO: Found applicable config definition build:linux in file /opt/tensorflow/.bazelrc: --copt=-w --host_copt=-w --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++14 --host_cxxopt=-std=c++14 --config=dynamic_kernels --distinct_host_configuration=false --experimental_guard_against_concurrent_changes. #16 1489.8 (21:51:01) INFO: Found applicable config definition build:dynamic_kernels in file /opt/tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS. #16 1489.8 (21:51:01) INFO: Found applicable config definition build:linux in file /opt/deepvariant/.bazelrc: --distinct_host_configuration=true. #16 1490.1 (21:51:01) INFO: Current date is 2023-01-30. #16 1490.1 (21:51:01) Loading:. #16 1490.1 (21:51:01) Loading: 0 packages loaded. #16 1491.1 (21:51:02) Loading: 0 packages loaded. #16 1492.2 (21:51:03) Loading: 0 packages loaded. #16 1493.2 (21:51:04) Loading: 0 packages loaded. #16 1494.2 (21:51:05) Loading: 0 packages loaded. #16 1495.2 (21:51:06) Loading: 0 packages loaded. #16 1496.2 (21:51:07) Loading: 0 packages loaded. #16 1497.0 (21:51:08) INFO: Repository tf_runtime instantiated at:. #16 1497.0 /opt/deepvariant/WORKSPACE:102:14: in <toplevel>. #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/tensorflow/workspace3.bzl:28:15: in workspace. #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/tf_runtime/workspace.bzl:12:20: in repo. #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:113:21: in tf_http_archive. #16 1497.0 Reposito,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/608
https://github.com/google/deepvariant/issues/608:4885,energy efficiency,Load,Loading,4885,#16 1489.8 (21:51:01) INFO: Found applicable config definition build:monolithic in file /opt/tensorflow/.bazelrc: --define framework_shared_object=false. #16 1489.8 (21:51:01) INFO: Found applicable config definition build:linux in file /opt/tensorflow/.bazelrc: --copt=-w --host_copt=-w --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++14 --host_cxxopt=-std=c++14 --config=dynamic_kernels --distinct_host_configuration=false --experimental_guard_against_concurrent_changes. #16 1489.8 (21:51:01) INFO: Found applicable config definition build:dynamic_kernels in file /opt/tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS. #16 1489.8 (21:51:01) INFO: Found applicable config definition build:linux in file /opt/deepvariant/.bazelrc: --distinct_host_configuration=true. #16 1490.1 (21:51:01) INFO: Current date is 2023-01-30. #16 1490.1 (21:51:01) Loading:. #16 1490.1 (21:51:01) Loading: 0 packages loaded. #16 1491.1 (21:51:02) Loading: 0 packages loaded. #16 1492.2 (21:51:03) Loading: 0 packages loaded. #16 1493.2 (21:51:04) Loading: 0 packages loaded. #16 1494.2 (21:51:05) Loading: 0 packages loaded. #16 1495.2 (21:51:06) Loading: 0 packages loaded. #16 1496.2 (21:51:07) Loading: 0 packages loaded. #16 1497.0 (21:51:08) INFO: Repository tf_runtime instantiated at:. #16 1497.0 /opt/deepvariant/WORKSPACE:102:14: in <toplevel>. #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/tensorflow/workspace3.bzl:28:15: in workspace. #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/tf_runtime/workspace.bzl:12:20: in repo. #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:113:21: in tf_http_archive. #16 1497.0 Repository rule _tf_http_archive defined at:. #16 1497.0 /,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/608
https://github.com/google/deepvariant/issues/608:4917,energy efficiency,Load,Loading,4917,d applicable config definition build:monolithic in file /opt/tensorflow/.bazelrc: --define framework_shared_object=false. #16 1489.8 (21:51:01) INFO: Found applicable config definition build:linux in file /opt/tensorflow/.bazelrc: --copt=-w --host_copt=-w --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++14 --host_cxxopt=-std=c++14 --config=dynamic_kernels --distinct_host_configuration=false --experimental_guard_against_concurrent_changes. #16 1489.8 (21:51:01) INFO: Found applicable config definition build:dynamic_kernels in file /opt/tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS. #16 1489.8 (21:51:01) INFO: Found applicable config definition build:linux in file /opt/deepvariant/.bazelrc: --distinct_host_configuration=true. #16 1490.1 (21:51:01) INFO: Current date is 2023-01-30. #16 1490.1 (21:51:01) Loading:. #16 1490.1 (21:51:01) Loading: 0 packages loaded. #16 1491.1 (21:51:02) Loading: 0 packages loaded. #16 1492.2 (21:51:03) Loading: 0 packages loaded. #16 1493.2 (21:51:04) Loading: 0 packages loaded. #16 1494.2 (21:51:05) Loading: 0 packages loaded. #16 1495.2 (21:51:06) Loading: 0 packages loaded. #16 1496.2 (21:51:07) Loading: 0 packages loaded. #16 1497.0 (21:51:08) INFO: Repository tf_runtime instantiated at:. #16 1497.0 /opt/deepvariant/WORKSPACE:102:14: in <toplevel>. #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/tensorflow/workspace3.bzl:28:15: in workspace. #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/tf_runtime/workspace.bzl:12:20: in repo. #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:113:21: in tf_http_archive. #16 1497.0 Repository rule _tf_http_archive defined at:. #16 1497.0 /root/.cache/bazel/_bazel_root/61,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/608
https://github.com/google/deepvariant/issues/608:4937,energy efficiency,load,loaded,4937, definition build:monolithic in file /opt/tensorflow/.bazelrc: --define framework_shared_object=false. #16 1489.8 (21:51:01) INFO: Found applicable config definition build:linux in file /opt/tensorflow/.bazelrc: --copt=-w --host_copt=-w --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++14 --host_cxxopt=-std=c++14 --config=dynamic_kernels --distinct_host_configuration=false --experimental_guard_against_concurrent_changes. #16 1489.8 (21:51:01) INFO: Found applicable config definition build:dynamic_kernels in file /opt/tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS. #16 1489.8 (21:51:01) INFO: Found applicable config definition build:linux in file /opt/deepvariant/.bazelrc: --distinct_host_configuration=true. #16 1490.1 (21:51:01) INFO: Current date is 2023-01-30. #16 1490.1 (21:51:01) Loading:. #16 1490.1 (21:51:01) Loading: 0 packages loaded. #16 1491.1 (21:51:02) Loading: 0 packages loaded. #16 1492.2 (21:51:03) Loading: 0 packages loaded. #16 1493.2 (21:51:04) Loading: 0 packages loaded. #16 1494.2 (21:51:05) Loading: 0 packages loaded. #16 1495.2 (21:51:06) Loading: 0 packages loaded. #16 1496.2 (21:51:07) Loading: 0 packages loaded. #16 1497.0 (21:51:08) INFO: Repository tf_runtime instantiated at:. #16 1497.0 /opt/deepvariant/WORKSPACE:102:14: in <toplevel>. #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/tensorflow/workspace3.bzl:28:15: in workspace. #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/tf_runtime/workspace.bzl:12:20: in repo. #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:113:21: in tf_http_archive. #16 1497.0 Repository rule _tf_http_archive defined at:. #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/608
https://github.com/google/deepvariant/issues/608:4967,energy efficiency,Load,Loading,4967, file /opt/tensorflow/.bazelrc: --define framework_shared_object=false. #16 1489.8 (21:51:01) INFO: Found applicable config definition build:linux in file /opt/tensorflow/.bazelrc: --copt=-w --host_copt=-w --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++14 --host_cxxopt=-std=c++14 --config=dynamic_kernels --distinct_host_configuration=false --experimental_guard_against_concurrent_changes. #16 1489.8 (21:51:01) INFO: Found applicable config definition build:dynamic_kernels in file /opt/tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS. #16 1489.8 (21:51:01) INFO: Found applicable config definition build:linux in file /opt/deepvariant/.bazelrc: --distinct_host_configuration=true. #16 1490.1 (21:51:01) INFO: Current date is 2023-01-30. #16 1490.1 (21:51:01) Loading:. #16 1490.1 (21:51:01) Loading: 0 packages loaded. #16 1491.1 (21:51:02) Loading: 0 packages loaded. #16 1492.2 (21:51:03) Loading: 0 packages loaded. #16 1493.2 (21:51:04) Loading: 0 packages loaded. #16 1494.2 (21:51:05) Loading: 0 packages loaded. #16 1495.2 (21:51:06) Loading: 0 packages loaded. #16 1496.2 (21:51:07) Loading: 0 packages loaded. #16 1497.0 (21:51:08) INFO: Repository tf_runtime instantiated at:. #16 1497.0 /opt/deepvariant/WORKSPACE:102:14: in <toplevel>. #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/tensorflow/workspace3.bzl:28:15: in workspace. #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/tf_runtime/workspace.bzl:12:20: in repo. #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:113:21: in tf_http_archive. #16 1497.0 Repository rule _tf_http_archive defined at:. #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensor,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/608
https://github.com/google/deepvariant/issues/608:4987,energy efficiency,load,loaded,4987,ow/.bazelrc: --define framework_shared_object=false. #16 1489.8 (21:51:01) INFO: Found applicable config definition build:linux in file /opt/tensorflow/.bazelrc: --copt=-w --host_copt=-w --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++14 --host_cxxopt=-std=c++14 --config=dynamic_kernels --distinct_host_configuration=false --experimental_guard_against_concurrent_changes. #16 1489.8 (21:51:01) INFO: Found applicable config definition build:dynamic_kernels in file /opt/tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS. #16 1489.8 (21:51:01) INFO: Found applicable config definition build:linux in file /opt/deepvariant/.bazelrc: --distinct_host_configuration=true. #16 1490.1 (21:51:01) INFO: Current date is 2023-01-30. #16 1490.1 (21:51:01) Loading:. #16 1490.1 (21:51:01) Loading: 0 packages loaded. #16 1491.1 (21:51:02) Loading: 0 packages loaded. #16 1492.2 (21:51:03) Loading: 0 packages loaded. #16 1493.2 (21:51:04) Loading: 0 packages loaded. #16 1494.2 (21:51:05) Loading: 0 packages loaded. #16 1495.2 (21:51:06) Loading: 0 packages loaded. #16 1496.2 (21:51:07) Loading: 0 packages loaded. #16 1497.0 (21:51:08) INFO: Repository tf_runtime instantiated at:. #16 1497.0 /opt/deepvariant/WORKSPACE:102:14: in <toplevel>. #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/tensorflow/workspace3.bzl:28:15: in workspace. #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/tf_runtime/workspace.bzl:12:20: in repo. #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:113:21: in tf_http_archive. #16 1497.0 Repository rule _tf_http_archive defined at:. #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/re,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/608
https://github.com/google/deepvariant/issues/608:5017,energy efficiency,Load,Loading,5017,_shared_object=false. #16 1489.8 (21:51:01) INFO: Found applicable config definition build:linux in file /opt/tensorflow/.bazelrc: --copt=-w --host_copt=-w --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++14 --host_cxxopt=-std=c++14 --config=dynamic_kernels --distinct_host_configuration=false --experimental_guard_against_concurrent_changes. #16 1489.8 (21:51:01) INFO: Found applicable config definition build:dynamic_kernels in file /opt/tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS. #16 1489.8 (21:51:01) INFO: Found applicable config definition build:linux in file /opt/deepvariant/.bazelrc: --distinct_host_configuration=true. #16 1490.1 (21:51:01) INFO: Current date is 2023-01-30. #16 1490.1 (21:51:01) Loading:. #16 1490.1 (21:51:01) Loading: 0 packages loaded. #16 1491.1 (21:51:02) Loading: 0 packages loaded. #16 1492.2 (21:51:03) Loading: 0 packages loaded. #16 1493.2 (21:51:04) Loading: 0 packages loaded. #16 1494.2 (21:51:05) Loading: 0 packages loaded. #16 1495.2 (21:51:06) Loading: 0 packages loaded. #16 1496.2 (21:51:07) Loading: 0 packages loaded. #16 1497.0 (21:51:08) INFO: Repository tf_runtime instantiated at:. #16 1497.0 /opt/deepvariant/WORKSPACE:102:14: in <toplevel>. #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/tensorflow/workspace3.bzl:28:15: in workspace. #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/tf_runtime/workspace.bzl:12:20: in repo. #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:113:21: in tf_http_archive. #16 1497.0 Repository rule _tf_http_archive defined at:. #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:66:35: in <toplevel>. #1,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/608
https://github.com/google/deepvariant/issues/608:5037,energy efficiency,load,loaded,5037,e. #16 1489.8 (21:51:01) INFO: Found applicable config definition build:linux in file /opt/tensorflow/.bazelrc: --copt=-w --host_copt=-w --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++14 --host_cxxopt=-std=c++14 --config=dynamic_kernels --distinct_host_configuration=false --experimental_guard_against_concurrent_changes. #16 1489.8 (21:51:01) INFO: Found applicable config definition build:dynamic_kernels in file /opt/tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS. #16 1489.8 (21:51:01) INFO: Found applicable config definition build:linux in file /opt/deepvariant/.bazelrc: --distinct_host_configuration=true. #16 1490.1 (21:51:01) INFO: Current date is 2023-01-30. #16 1490.1 (21:51:01) Loading:. #16 1490.1 (21:51:01) Loading: 0 packages loaded. #16 1491.1 (21:51:02) Loading: 0 packages loaded. #16 1492.2 (21:51:03) Loading: 0 packages loaded. #16 1493.2 (21:51:04) Loading: 0 packages loaded. #16 1494.2 (21:51:05) Loading: 0 packages loaded. #16 1495.2 (21:51:06) Loading: 0 packages loaded. #16 1496.2 (21:51:07) Loading: 0 packages loaded. #16 1497.0 (21:51:08) INFO: Repository tf_runtime instantiated at:. #16 1497.0 /opt/deepvariant/WORKSPACE:102:14: in <toplevel>. #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/tensorflow/workspace3.bzl:28:15: in workspace. #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/tf_runtime/workspace.bzl:12:20: in repo. #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:113:21: in tf_http_archive. #16 1497.0 Repository rule _tf_http_archive defined at:. #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:66:35: in <toplevel>. #16 1497.0 (21:51:08),MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/608
https://github.com/google/deepvariant/issues/608:5067,energy efficiency,Load,Loading,5067,Found applicable config definition build:linux in file /opt/tensorflow/.bazelrc: --copt=-w --host_copt=-w --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++14 --host_cxxopt=-std=c++14 --config=dynamic_kernels --distinct_host_configuration=false --experimental_guard_against_concurrent_changes. #16 1489.8 (21:51:01) INFO: Found applicable config definition build:dynamic_kernels in file /opt/tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS. #16 1489.8 (21:51:01) INFO: Found applicable config definition build:linux in file /opt/deepvariant/.bazelrc: --distinct_host_configuration=true. #16 1490.1 (21:51:01) INFO: Current date is 2023-01-30. #16 1490.1 (21:51:01) Loading:. #16 1490.1 (21:51:01) Loading: 0 packages loaded. #16 1491.1 (21:51:02) Loading: 0 packages loaded. #16 1492.2 (21:51:03) Loading: 0 packages loaded. #16 1493.2 (21:51:04) Loading: 0 packages loaded. #16 1494.2 (21:51:05) Loading: 0 packages loaded. #16 1495.2 (21:51:06) Loading: 0 packages loaded. #16 1496.2 (21:51:07) Loading: 0 packages loaded. #16 1497.0 (21:51:08) INFO: Repository tf_runtime instantiated at:. #16 1497.0 /opt/deepvariant/WORKSPACE:102:14: in <toplevel>. #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/tensorflow/workspace3.bzl:28:15: in workspace. #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/tf_runtime/workspace.bzl:12:20: in repo. #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:113:21: in tf_http_archive. #16 1497.0 Repository rule _tf_http_archive defined at:. #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:66:35: in <toplevel>. #16 1497.0 (21:51:08) WARNING: Download from http://,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/608
https://github.com/google/deepvariant/issues/608:5087,energy efficiency,load,loaded,5087,nfig definition build:linux in file /opt/tensorflow/.bazelrc: --copt=-w --host_copt=-w --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++14 --host_cxxopt=-std=c++14 --config=dynamic_kernels --distinct_host_configuration=false --experimental_guard_against_concurrent_changes. #16 1489.8 (21:51:01) INFO: Found applicable config definition build:dynamic_kernels in file /opt/tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS. #16 1489.8 (21:51:01) INFO: Found applicable config definition build:linux in file /opt/deepvariant/.bazelrc: --distinct_host_configuration=true. #16 1490.1 (21:51:01) INFO: Current date is 2023-01-30. #16 1490.1 (21:51:01) Loading:. #16 1490.1 (21:51:01) Loading: 0 packages loaded. #16 1491.1 (21:51:02) Loading: 0 packages loaded. #16 1492.2 (21:51:03) Loading: 0 packages loaded. #16 1493.2 (21:51:04) Loading: 0 packages loaded. #16 1494.2 (21:51:05) Loading: 0 packages loaded. #16 1495.2 (21:51:06) Loading: 0 packages loaded. #16 1496.2 (21:51:07) Loading: 0 packages loaded. #16 1497.0 (21:51:08) INFO: Repository tf_runtime instantiated at:. #16 1497.0 /opt/deepvariant/WORKSPACE:102:14: in <toplevel>. #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/tensorflow/workspace3.bzl:28:15: in workspace. #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/tf_runtime/workspace.bzl:12:20: in repo. #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:113:21: in tf_http_archive. #16 1497.0 Repository rule _tf_http_archive defined at:. #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:66:35: in <toplevel>. #16 1497.0 (21:51:08) WARNING: Download from http://mirror.tensorflow.o,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/608
https://github.com/google/deepvariant/issues/608:5117,energy efficiency,Load,Loading,5117,file /opt/tensorflow/.bazelrc: --copt=-w --host_copt=-w --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++14 --host_cxxopt=-std=c++14 --config=dynamic_kernels --distinct_host_configuration=false --experimental_guard_against_concurrent_changes. #16 1489.8 (21:51:01) INFO: Found applicable config definition build:dynamic_kernels in file /opt/tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS. #16 1489.8 (21:51:01) INFO: Found applicable config definition build:linux in file /opt/deepvariant/.bazelrc: --distinct_host_configuration=true. #16 1490.1 (21:51:01) INFO: Current date is 2023-01-30. #16 1490.1 (21:51:01) Loading:. #16 1490.1 (21:51:01) Loading: 0 packages loaded. #16 1491.1 (21:51:02) Loading: 0 packages loaded. #16 1492.2 (21:51:03) Loading: 0 packages loaded. #16 1493.2 (21:51:04) Loading: 0 packages loaded. #16 1494.2 (21:51:05) Loading: 0 packages loaded. #16 1495.2 (21:51:06) Loading: 0 packages loaded. #16 1496.2 (21:51:07) Loading: 0 packages loaded. #16 1497.0 (21:51:08) INFO: Repository tf_runtime instantiated at:. #16 1497.0 /opt/deepvariant/WORKSPACE:102:14: in <toplevel>. #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/tensorflow/workspace3.bzl:28:15: in workspace. #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/tf_runtime/workspace.bzl:12:20: in repo. #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:113:21: in tf_http_archive. #16 1497.0 Repository rule _tf_http_archive defined at:. #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:66:35: in <toplevel>. #16 1497.0 (21:51:08) WARNING: Download from http://mirror.tensorflow.org/github.com/tensorflow/runtim,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/608
https://github.com/google/deepvariant/issues/608:5137,energy efficiency,load,loaded,5137,w/.bazelrc: --copt=-w --host_copt=-w --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++14 --host_cxxopt=-std=c++14 --config=dynamic_kernels --distinct_host_configuration=false --experimental_guard_against_concurrent_changes. #16 1489.8 (21:51:01) INFO: Found applicable config definition build:dynamic_kernels in file /opt/tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS. #16 1489.8 (21:51:01) INFO: Found applicable config definition build:linux in file /opt/deepvariant/.bazelrc: --distinct_host_configuration=true. #16 1490.1 (21:51:01) INFO: Current date is 2023-01-30. #16 1490.1 (21:51:01) Loading:. #16 1490.1 (21:51:01) Loading: 0 packages loaded. #16 1491.1 (21:51:02) Loading: 0 packages loaded. #16 1492.2 (21:51:03) Loading: 0 packages loaded. #16 1493.2 (21:51:04) Loading: 0 packages loaded. #16 1494.2 (21:51:05) Loading: 0 packages loaded. #16 1495.2 (21:51:06) Loading: 0 packages loaded. #16 1496.2 (21:51:07) Loading: 0 packages loaded. #16 1497.0 (21:51:08) INFO: Repository tf_runtime instantiated at:. #16 1497.0 /opt/deepvariant/WORKSPACE:102:14: in <toplevel>. #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/tensorflow/workspace3.bzl:28:15: in workspace. #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/tf_runtime/workspace.bzl:12:20: in repo. #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:113:21: in tf_http_archive. #16 1497.0 Repository rule _tf_http_archive defined at:. #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:66:35: in <toplevel>. #16 1497.0 (21:51:08) WARNING: Download from http://mirror.tensorflow.org/github.com/tensorflow/runtime/archive/64c92c801,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/608
https://github.com/google/deepvariant/issues/608:5167,energy efficiency,Load,Loading,5167,pt=-w --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++14 --host_cxxopt=-std=c++14 --config=dynamic_kernels --distinct_host_configuration=false --experimental_guard_against_concurrent_changes. #16 1489.8 (21:51:01) INFO: Found applicable config definition build:dynamic_kernels in file /opt/tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS. #16 1489.8 (21:51:01) INFO: Found applicable config definition build:linux in file /opt/deepvariant/.bazelrc: --distinct_host_configuration=true. #16 1490.1 (21:51:01) INFO: Current date is 2023-01-30. #16 1490.1 (21:51:01) Loading:. #16 1490.1 (21:51:01) Loading: 0 packages loaded. #16 1491.1 (21:51:02) Loading: 0 packages loaded. #16 1492.2 (21:51:03) Loading: 0 packages loaded. #16 1493.2 (21:51:04) Loading: 0 packages loaded. #16 1494.2 (21:51:05) Loading: 0 packages loaded. #16 1495.2 (21:51:06) Loading: 0 packages loaded. #16 1496.2 (21:51:07) Loading: 0 packages loaded. #16 1497.0 (21:51:08) INFO: Repository tf_runtime instantiated at:. #16 1497.0 /opt/deepvariant/WORKSPACE:102:14: in <toplevel>. #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/tensorflow/workspace3.bzl:28:15: in workspace. #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/tf_runtime/workspace.bzl:12:20: in repo. #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:113:21: in tf_http_archive. #16 1497.0 Repository rule _tf_http_archive defined at:. #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:66:35: in <toplevel>. #16 1497.0 (21:51:08) WARNING: Download from http://mirror.tensorflow.org/github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/608
https://github.com/google/deepvariant/issues/608:5187,energy efficiency,load,loaded,5187,IX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++14 --host_cxxopt=-std=c++14 --config=dynamic_kernels --distinct_host_configuration=false --experimental_guard_against_concurrent_changes. #16 1489.8 (21:51:01) INFO: Found applicable config definition build:dynamic_kernels in file /opt/tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS. #16 1489.8 (21:51:01) INFO: Found applicable config definition build:linux in file /opt/deepvariant/.bazelrc: --distinct_host_configuration=true. #16 1490.1 (21:51:01) INFO: Current date is 2023-01-30. #16 1490.1 (21:51:01) Loading:. #16 1490.1 (21:51:01) Loading: 0 packages loaded. #16 1491.1 (21:51:02) Loading: 0 packages loaded. #16 1492.2 (21:51:03) Loading: 0 packages loaded. #16 1493.2 (21:51:04) Loading: 0 packages loaded. #16 1494.2 (21:51:05) Loading: 0 packages loaded. #16 1495.2 (21:51:06) Loading: 0 packages loaded. #16 1496.2 (21:51:07) Loading: 0 packages loaded. #16 1497.0 (21:51:08) INFO: Repository tf_runtime instantiated at:. #16 1497.0 /opt/deepvariant/WORKSPACE:102:14: in <toplevel>. #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/tensorflow/workspace3.bzl:28:15: in workspace. #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/tf_runtime/workspace.bzl:12:20: in repo. #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:113:21: in tf_http_archive. #16 1497.0 Repository rule _tf_http_archive defined at:. #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:66:35: in <toplevel>. #16 1497.0 (21:51:08) WARNING: Download from http://mirror.tensorflow.org/github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz failed: cla,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/608
https://github.com/google/deepvariant/issues/608:5217,energy efficiency,Load,Loading,5217,X)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++14 --host_cxxopt=-std=c++14 --config=dynamic_kernels --distinct_host_configuration=false --experimental_guard_against_concurrent_changes. #16 1489.8 (21:51:01) INFO: Found applicable config definition build:dynamic_kernels in file /opt/tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS. #16 1489.8 (21:51:01) INFO: Found applicable config definition build:linux in file /opt/deepvariant/.bazelrc: --distinct_host_configuration=true. #16 1490.1 (21:51:01) INFO: Current date is 2023-01-30. #16 1490.1 (21:51:01) Loading:. #16 1490.1 (21:51:01) Loading: 0 packages loaded. #16 1491.1 (21:51:02) Loading: 0 packages loaded. #16 1492.2 (21:51:03) Loading: 0 packages loaded. #16 1493.2 (21:51:04) Loading: 0 packages loaded. #16 1494.2 (21:51:05) Loading: 0 packages loaded. #16 1495.2 (21:51:06) Loading: 0 packages loaded. #16 1496.2 (21:51:07) Loading: 0 packages loaded. #16 1497.0 (21:51:08) INFO: Repository tf_runtime instantiated at:. #16 1497.0 /opt/deepvariant/WORKSPACE:102:14: in <toplevel>. #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/tensorflow/workspace3.bzl:28:15: in workspace. #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/tf_runtime/workspace.bzl:12:20: in repo. #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:113:21: in tf_http_archive. #16 1497.0 Repository rule _tf_http_archive defined at:. #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:66:35: in <toplevel>. #16 1497.0 (21:51:08) WARNING: Download from http://mirror.tensorflow.org/github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz failed: class com.google.devtools.build.li,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/608
https://github.com/google/deepvariant/issues/608:5237,energy efficiency,load,loaded,5237,LUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++14 --host_cxxopt=-std=c++14 --config=dynamic_kernels --distinct_host_configuration=false --experimental_guard_against_concurrent_changes. #16 1489.8 (21:51:01) INFO: Found applicable config definition build:dynamic_kernels in file /opt/tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS. #16 1489.8 (21:51:01) INFO: Found applicable config definition build:linux in file /opt/deepvariant/.bazelrc: --distinct_host_configuration=true. #16 1490.1 (21:51:01) INFO: Current date is 2023-01-30. #16 1490.1 (21:51:01) Loading:. #16 1490.1 (21:51:01) Loading: 0 packages loaded. #16 1491.1 (21:51:02) Loading: 0 packages loaded. #16 1492.2 (21:51:03) Loading: 0 packages loaded. #16 1493.2 (21:51:04) Loading: 0 packages loaded. #16 1494.2 (21:51:05) Loading: 0 packages loaded. #16 1495.2 (21:51:06) Loading: 0 packages loaded. #16 1496.2 (21:51:07) Loading: 0 packages loaded. #16 1497.0 (21:51:08) INFO: Repository tf_runtime instantiated at:. #16 1497.0 /opt/deepvariant/WORKSPACE:102:14: in <toplevel>. #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/tensorflow/workspace3.bzl:28:15: in workspace. #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/tf_runtime/workspace.bzl:12:20: in repo. #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:113:21: in tf_http_archive. #16 1497.0 Repository rule _tf_http_archive defined at:. #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:66:35: in <toplevel>. #16 1497.0 (21:51:08) WARNING: Download from http://mirror.tensorflow.org/github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz failed: class com.google.devtools.build.lib.bazel.repository.,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/608
https://github.com/google/deepvariant/issues/608:8447,energy efficiency,Load,Loading,8447,"9294/64c92c8013b557087351c91b5423b6046d10f206.tar.gz: Checksum was 8383b3247286016e450b0b20e805d26b88ab4638b4e4e3cc4a6923debaf7ad1e but wanted f16fcf09b34e0c7be9389f50652b4b4a14c5a8a96e7e15ad73e8f234d8d09ebe. #16 1497.1 (21:51:08) INFO: Repository llvm-raw instantiated at:. #16 1497.1 /opt/deepvariant/WORKSPACE:102:14: in <toplevel>. #16 1497.1 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/tensorflow/workspace3.bzl:42:9: in workspace. #16 1497.1 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/llvm/workspace.bzl:10:20: in repo. #16 1497.1 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:113:21: in tf_http_archive. #16 1497.1 Repository rule _tf_http_archive defined at:. #16 1497.1 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:66:35: in <toplevel>. #16 1497.2 (21:51:08) Loading: 0 packages loaded. #16 1497.3 (21:51:08) ERROR: no such package '@tf_runtime//': java.io.IOException: Error downloading [http://mirror.tensorflow.org/github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz, https://github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz] to /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/tf_runtime/temp12516918929418979294/64c92c8013b557087351c91b5423b6046d10f206.tar.gz: Checksum was 8383b3247286016e450b0b20e805d26b88ab4638b4e4e3cc4a6923debaf7ad1e but wanted f16fcf09b34e0c7be9389f50652b4b4a14c5a8a96e7e15ad73e8f234d8d09ebe. #16 1497.3 (21:51:09) INFO: Elapsed time: 12.546s. #16 1497.3 (21:51:09) INFO: 0 processes. #16 1497.3 (21:51:09) FAILED: Build did NOT complete successfully (0 packages loaded). #16 1497.3 (21:51:09) FAILED: Build did NOT complete successfully (0 packages loaded). #16 ERROR: executor failed running [/bin/sh -c ./build-prereq.sh && PATH=""${HOME}",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/608
https://github.com/google/deepvariant/issues/608:8467,energy efficiency,load,loaded,8467,"087351c91b5423b6046d10f206.tar.gz: Checksum was 8383b3247286016e450b0b20e805d26b88ab4638b4e4e3cc4a6923debaf7ad1e but wanted f16fcf09b34e0c7be9389f50652b4b4a14c5a8a96e7e15ad73e8f234d8d09ebe. #16 1497.1 (21:51:08) INFO: Repository llvm-raw instantiated at:. #16 1497.1 /opt/deepvariant/WORKSPACE:102:14: in <toplevel>. #16 1497.1 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/tensorflow/workspace3.bzl:42:9: in workspace. #16 1497.1 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/llvm/workspace.bzl:10:20: in repo. #16 1497.1 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:113:21: in tf_http_archive. #16 1497.1 Repository rule _tf_http_archive defined at:. #16 1497.1 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:66:35: in <toplevel>. #16 1497.2 (21:51:08) Loading: 0 packages loaded. #16 1497.3 (21:51:08) ERROR: no such package '@tf_runtime//': java.io.IOException: Error downloading [http://mirror.tensorflow.org/github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz, https://github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz] to /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/tf_runtime/temp12516918929418979294/64c92c8013b557087351c91b5423b6046d10f206.tar.gz: Checksum was 8383b3247286016e450b0b20e805d26b88ab4638b4e4e3cc4a6923debaf7ad1e but wanted f16fcf09b34e0c7be9389f50652b4b4a14c5a8a96e7e15ad73e8f234d8d09ebe. #16 1497.3 (21:51:09) INFO: Elapsed time: 12.546s. #16 1497.3 (21:51:09) INFO: 0 processes. #16 1497.3 (21:51:09) FAILED: Build did NOT complete successfully (0 packages loaded). #16 1497.3 (21:51:09) FAILED: Build did NOT complete successfully (0 packages loaded). #16 ERROR: executor failed running [/bin/sh -c ./build-prereq.sh && PATH=""${HOME}/bin:${PATH}"" ./bui",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/608
https://github.com/google/deepvariant/issues/608:9274,energy efficiency,load,loaded,9274,"80/external/org_tensorflow/tensorflow/workspace3.bzl:42:9: in workspace. #16 1497.1 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/llvm/workspace.bzl:10:20: in repo. #16 1497.1 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:113:21: in tf_http_archive. #16 1497.1 Repository rule _tf_http_archive defined at:. #16 1497.1 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:66:35: in <toplevel>. #16 1497.2 (21:51:08) Loading: 0 packages loaded. #16 1497.3 (21:51:08) ERROR: no such package '@tf_runtime//': java.io.IOException: Error downloading [http://mirror.tensorflow.org/github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz, https://github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz] to /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/tf_runtime/temp12516918929418979294/64c92c8013b557087351c91b5423b6046d10f206.tar.gz: Checksum was 8383b3247286016e450b0b20e805d26b88ab4638b4e4e3cc4a6923debaf7ad1e but wanted f16fcf09b34e0c7be9389f50652b4b4a14c5a8a96e7e15ad73e8f234d8d09ebe. #16 1497.3 (21:51:09) INFO: Elapsed time: 12.546s. #16 1497.3 (21:51:09) INFO: 0 processes. #16 1497.3 (21:51:09) FAILED: Build did NOT complete successfully (0 packages loaded). #16 1497.3 (21:51:09) FAILED: Build did NOT complete successfully (0 packages loaded). #16 ERROR: executor failed running [/bin/sh -c ./build-prereq.sh && PATH=""${HOME}/bin:${PATH}"" ./build_release_binaries.sh # PATH for bazel]: exit code: 1. ------. > [builder 6/6] RUN ./build-prereq.sh && PATH=""${HOME}/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin"" ./build_release_binaries.sh # PATH for bazel:. ------. executor failed running [/bin/sh -c ./build-prereq.sh && PATH=""${HOME}/bin:${PATH}"" ./build_release_binaries.sh # PATH for bazel]: exit code: 1. ```.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/608
https://github.com/google/deepvariant/issues/608:9361,energy efficiency,load,loaded,9361,"80/external/org_tensorflow/tensorflow/workspace3.bzl:42:9: in workspace. #16 1497.1 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/llvm/workspace.bzl:10:20: in repo. #16 1497.1 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:113:21: in tf_http_archive. #16 1497.1 Repository rule _tf_http_archive defined at:. #16 1497.1 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:66:35: in <toplevel>. #16 1497.2 (21:51:08) Loading: 0 packages loaded. #16 1497.3 (21:51:08) ERROR: no such package '@tf_runtime//': java.io.IOException: Error downloading [http://mirror.tensorflow.org/github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz, https://github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz] to /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/tf_runtime/temp12516918929418979294/64c92c8013b557087351c91b5423b6046d10f206.tar.gz: Checksum was 8383b3247286016e450b0b20e805d26b88ab4638b4e4e3cc4a6923debaf7ad1e but wanted f16fcf09b34e0c7be9389f50652b4b4a14c5a8a96e7e15ad73e8f234d8d09ebe. #16 1497.3 (21:51:09) INFO: Elapsed time: 12.546s. #16 1497.3 (21:51:09) INFO: 0 processes. #16 1497.3 (21:51:09) FAILED: Build did NOT complete successfully (0 packages loaded). #16 1497.3 (21:51:09) FAILED: Build did NOT complete successfully (0 packages loaded). #16 ERROR: executor failed running [/bin/sh -c ./build-prereq.sh && PATH=""${HOME}/bin:${PATH}"" ./build_release_binaries.sh # PATH for bazel]: exit code: 1. ------. > [builder 6/6] RUN ./build-prereq.sh && PATH=""${HOME}/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin"" ./build_release_binaries.sh # PATH for bazel:. ------. executor failed running [/bin/sh -c ./build-prereq.sh && PATH=""${HOME}/bin:${PATH}"" ./build_release_binaries.sh # PATH for bazel]: exit code: 1. ```.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/608
https://github.com/google/deepvariant/issues/608:246,integrability,version,version,246,"binary creation fails; bazel unable to fetch tf_runtime; I was able to build the docker image last week, but this week the build fails at binary creation, with bazel unable to download tf_runtime. . - Operating system: Ubuntu20.04. - DeepVariant version: 1.4.0. - Building docker image locally. **Steps to reproduce:**. - Command: `docker build .` in source directory (no modifications). - Error trace: . . ```. #16 1484.7 ========== [Mon Jan 30 21:50:56 UTC 2023] Stage 'build-prereq.sh complete' starting. #16 1484.7 Extracting Bazel installation... #16 1487.8 Starting local Bazel server and connecting to it... #16 1489.8 (21:51:01) WARNING: option '--distinct_host_configuration' was expanded to from both option '--enable_platform_specific_config' (source /opt/tensorflow/.bazelrc) and option '--enable_platform_specific_config' (source /opt/tensorflow/.bazelrc). #16 1489.8 (21:51:01) INFO: Options provided by the client:. #16 1489.8 Inherited 'common' options: --isatty=0 --terminal_columns=80. #16 1489.8 (21:51:01) INFO: Reading rc options for 'build' from /opt/tensorflow/.bazelrc:. #16 1489.8 Inherited 'common' options: --experimental_repo_remote_exec. #16 1489.8 (21:51:01) INFO: Reading rc options for 'build' from /opt/tensorflow/.bazelrc:. #16 1489.8 'build' options: --define framework_shared_object=true --java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --host_java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true --deleted_packages=tensorflow/compiler/mlir/tfrt,tensorflow/compiler/mlir/tfrt/benchmarks,tensorflow/compiler/mlir/tfrt/jit/python_binding,tensorflow/compiler/mlir/tfrt/jit/transforms,tensor",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/608
https://github.com/google/deepvariant/issues/608:1983,integrability,transform,transforms,1983,"nal_columns=80. #16 1489.8 (21:51:01) INFO: Reading rc options for 'build' from /opt/tensorflow/.bazelrc:. #16 1489.8 Inherited 'common' options: --experimental_repo_remote_exec. #16 1489.8 (21:51:01) INFO: Reading rc options for 'build' from /opt/tensorflow/.bazelrc:. #16 1489.8 'build' options: --define framework_shared_object=true --java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --host_java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true --deleted_packages=tensorflow/compiler/mlir/tfrt,tensorflow/compiler/mlir/tfrt/benchmarks,tensorflow/compiler/mlir/tfrt/jit/python_binding,tensorflow/compiler/mlir/tfrt/jit/transforms,tensorflow/compiler/mlir/tfrt/python_tests,tensorflow/compiler/mlir/tfrt/tests,tensorflow/compiler/mlir/tfrt/tests/saved_model,tensorflow/compiler/mlir/tfrt/transforms/lhlo_gpu_to_tfrt_gpu,tensorflow/core/runtime_fallback,tensorflow/core/runtime_fallback/conversion,tensorflow/core/runtime_fallback/kernel,tensorflow/core/runtime_fallback/opdefs,tensorflow/core/runtime_fallback/runtime,tensorflow/core/runtime_fallback/util,tensorflow/core/tfrt/common,tensorflow/core/tfrt/eager,tensorflow/core/tfrt/eager/backends/cpu,tensorflow/core/tfrt/eager/backends/gpu,tensorflow/core/tfrt/eager/core_runtime,tensorflow/core/tfrt/eager/cpp_tests/core_runtime,tensorflow/core/tfrt/fallback,tensorflow/core/tfrt/gpu,tensorflow/core/tfrt/run_handler_thread_pool,tensorflow/core/tfrt/runtime,tensorflow/core/tfrt/saved_model,tensorflow/core/tfrt/saved_model/tests,tensorflow/core/tfrt/tpu,tensorflow/core/tfrt/utils. #16 1489.8 (21:51:01) INFO: Reading rc options for 'build' from /opt/tensorflow/.tf_config",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/608
https://github.com/google/deepvariant/issues/608:2151,integrability,transform,transforms,2151,"mote_exec. #16 1489.8 (21:51:01) INFO: Reading rc options for 'build' from /opt/tensorflow/.bazelrc:. #16 1489.8 'build' options: --define framework_shared_object=true --java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --host_java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true --deleted_packages=tensorflow/compiler/mlir/tfrt,tensorflow/compiler/mlir/tfrt/benchmarks,tensorflow/compiler/mlir/tfrt/jit/python_binding,tensorflow/compiler/mlir/tfrt/jit/transforms,tensorflow/compiler/mlir/tfrt/python_tests,tensorflow/compiler/mlir/tfrt/tests,tensorflow/compiler/mlir/tfrt/tests/saved_model,tensorflow/compiler/mlir/tfrt/transforms/lhlo_gpu_to_tfrt_gpu,tensorflow/core/runtime_fallback,tensorflow/core/runtime_fallback/conversion,tensorflow/core/runtime_fallback/kernel,tensorflow/core/runtime_fallback/opdefs,tensorflow/core/runtime_fallback/runtime,tensorflow/core/runtime_fallback/util,tensorflow/core/tfrt/common,tensorflow/core/tfrt/eager,tensorflow/core/tfrt/eager/backends/cpu,tensorflow/core/tfrt/eager/backends/gpu,tensorflow/core/tfrt/eager/core_runtime,tensorflow/core/tfrt/eager/cpp_tests/core_runtime,tensorflow/core/tfrt/fallback,tensorflow/core/tfrt/gpu,tensorflow/core/tfrt/run_handler_thread_pool,tensorflow/core/tfrt/runtime,tensorflow/core/tfrt/saved_model,tensorflow/core/tfrt/saved_model/tests,tensorflow/core/tfrt/tpu,tensorflow/core/tfrt/utils. #16 1489.8 (21:51:01) INFO: Reading rc options for 'build' from /opt/tensorflow/.tf_configure.bazelrc:. #16 1489.8 'build' options: --action_env PYTHON_BIN_PATH=/usr/local/bin/python3 --action_env PYTHON_LIB_PATH=/usr/local/lib/python3.8/dist-packages --pyth",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/608
https://github.com/google/deepvariant/issues/608:5273,integrability,Repositor,Repository,5273,TOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++14 --host_cxxopt=-std=c++14 --config=dynamic_kernels --distinct_host_configuration=false --experimental_guard_against_concurrent_changes. #16 1489.8 (21:51:01) INFO: Found applicable config definition build:dynamic_kernels in file /opt/tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS. #16 1489.8 (21:51:01) INFO: Found applicable config definition build:linux in file /opt/deepvariant/.bazelrc: --distinct_host_configuration=true. #16 1490.1 (21:51:01) INFO: Current date is 2023-01-30. #16 1490.1 (21:51:01) Loading:. #16 1490.1 (21:51:01) Loading: 0 packages loaded. #16 1491.1 (21:51:02) Loading: 0 packages loaded. #16 1492.2 (21:51:03) Loading: 0 packages loaded. #16 1493.2 (21:51:04) Loading: 0 packages loaded. #16 1494.2 (21:51:05) Loading: 0 packages loaded. #16 1495.2 (21:51:06) Loading: 0 packages loaded. #16 1496.2 (21:51:07) Loading: 0 packages loaded. #16 1497.0 (21:51:08) INFO: Repository tf_runtime instantiated at:. #16 1497.0 /opt/deepvariant/WORKSPACE:102:14: in <toplevel>. #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/tensorflow/workspace3.bzl:28:15: in workspace. #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/tf_runtime/workspace.bzl:12:20: in repo. #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:113:21: in tf_http_archive. #16 1497.0 Repository rule _tf_http_archive defined at:. #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:66:35: in <toplevel>. #16 1497.0 (21:51:08) WARNING: Download from http://mirror.tensorflow.org/github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException ,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/608
https://github.com/google/deepvariant/issues/608:5831,integrability,Repositor,Repository,5831,urrent date is 2023-01-30. #16 1490.1 (21:51:01) Loading:. #16 1490.1 (21:51:01) Loading: 0 packages loaded. #16 1491.1 (21:51:02) Loading: 0 packages loaded. #16 1492.2 (21:51:03) Loading: 0 packages loaded. #16 1493.2 (21:51:04) Loading: 0 packages loaded. #16 1494.2 (21:51:05) Loading: 0 packages loaded. #16 1495.2 (21:51:06) Loading: 0 packages loaded. #16 1496.2 (21:51:07) Loading: 0 packages loaded. #16 1497.0 (21:51:08) INFO: Repository tf_runtime instantiated at:. #16 1497.0 /opt/deepvariant/WORKSPACE:102:14: in <toplevel>. #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/tensorflow/workspace3.bzl:28:15: in workspace. #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/tf_runtime/workspace.bzl:12:20: in repo. #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:113:21: in tf_http_archive. #16 1497.0 Repository rule _tf_http_archive defined at:. #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:66:35: in <toplevel>. #16 1497.0 (21:51:08) WARNING: Download from http://mirror.tensorflow.org/github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException GET returned 404 Not Found. #16 1497.0 (21:51:08) WARNING: Download from https://github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException Checksum was 8383b3247286016e450b0b20e805d26b88ab4638b4e4e3cc4a6923debaf7ad1e but wanted f16fcf09b34e0c7be9389f50652b4b4a14c5a8a96e7e15ad73e8f234d8d09ebe. #16 1497.0 (21:51:08) ERROR: An error occurred during the fetch of repository 'tf_runtime':. #16 1497.0 Traceback (most recent call last),MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/608
https://github.com/google/deepvariant/issues/608:6229,integrability,repositor,repository,6229,"es loaded. #16 1497.0 (21:51:08) INFO: Repository tf_runtime instantiated at:. #16 1497.0 /opt/deepvariant/WORKSPACE:102:14: in <toplevel>. #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/tensorflow/workspace3.bzl:28:15: in workspace. #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/tf_runtime/workspace.bzl:12:20: in repo. #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:113:21: in tf_http_archive. #16 1497.0 Repository rule _tf_http_archive defined at:. #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:66:35: in <toplevel>. #16 1497.0 (21:51:08) WARNING: Download from http://mirror.tensorflow.org/github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException GET returned 404 Not Found. #16 1497.0 (21:51:08) WARNING: Download from https://github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException Checksum was 8383b3247286016e450b0b20e805d26b88ab4638b4e4e3cc4a6923debaf7ad1e but wanted f16fcf09b34e0c7be9389f50652b4b4a14c5a8a96e7e15ad73e8f234d8d09ebe. #16 1497.0 (21:51:08) ERROR: An error occurred during the fetch of repository 'tf_runtime':. #16 1497.0 Traceback (most recent call last):. #16 1497.0 File ""/root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl"", line 53, column 33, in _tf_http_archive_impl. #16 1497.0 ctx.download_and_extract(. #16 1497.0 Error in download_and_extract: java.io.IOException: Error downloading [http://mirror.tensorflow.org/github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/608
https://github.com/google/deepvariant/issues/608:6495,integrability,repositor,repository,6495,"8:15: in workspace. #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/tf_runtime/workspace.bzl:12:20: in repo. #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:113:21: in tf_http_archive. #16 1497.0 Repository rule _tf_http_archive defined at:. #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:66:35: in <toplevel>. #16 1497.0 (21:51:08) WARNING: Download from http://mirror.tensorflow.org/github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException GET returned 404 Not Found. #16 1497.0 (21:51:08) WARNING: Download from https://github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException Checksum was 8383b3247286016e450b0b20e805d26b88ab4638b4e4e3cc4a6923debaf7ad1e but wanted f16fcf09b34e0c7be9389f50652b4b4a14c5a8a96e7e15ad73e8f234d8d09ebe. #16 1497.0 (21:51:08) ERROR: An error occurred during the fetch of repository 'tf_runtime':. #16 1497.0 Traceback (most recent call last):. #16 1497.0 File ""/root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl"", line 53, column 33, in _tf_http_archive_impl. #16 1497.0 ctx.download_and_extract(. #16 1497.0 Error in download_and_extract: java.io.IOException: Error downloading [http://mirror.tensorflow.org/github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz, https://github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz] to /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/tf_runtime/temp12516918929418979294/64c92c8013b557087351c91b5423b6046d10f206.tar",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/608
https://github.com/google/deepvariant/issues/608:6766,integrability,repositor,repository,6766,"third_party/repo.bzl:113:21: in tf_http_archive. #16 1497.0 Repository rule _tf_http_archive defined at:. #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:66:35: in <toplevel>. #16 1497.0 (21:51:08) WARNING: Download from http://mirror.tensorflow.org/github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException GET returned 404 Not Found. #16 1497.0 (21:51:08) WARNING: Download from https://github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException Checksum was 8383b3247286016e450b0b20e805d26b88ab4638b4e4e3cc4a6923debaf7ad1e but wanted f16fcf09b34e0c7be9389f50652b4b4a14c5a8a96e7e15ad73e8f234d8d09ebe. #16 1497.0 (21:51:08) ERROR: An error occurred during the fetch of repository 'tf_runtime':. #16 1497.0 Traceback (most recent call last):. #16 1497.0 File ""/root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl"", line 53, column 33, in _tf_http_archive_impl. #16 1497.0 ctx.download_and_extract(. #16 1497.0 Error in download_and_extract: java.io.IOException: Error downloading [http://mirror.tensorflow.org/github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz, https://github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz] to /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/tf_runtime/temp12516918929418979294/64c92c8013b557087351c91b5423b6046d10f206.tar.gz: Checksum was 8383b3247286016e450b0b20e805d26b88ab4638b4e4e3cc4a6923debaf7ad1e but wanted f16fcf09b34e0c7be9389f50652b4b4a14c5a8a96e7e15ad73e8f234d8d09ebe. #16 1497.1 (21:51:08) INFO: Repository llvm-raw instantiated at:. #16 1497.1 /opt/deepvariant/WORKSPACE:102:14:",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/608
https://github.com/google/deepvariant/issues/608:7688,integrability,Repositor,Repository,7688,"9ebe. #16 1497.0 (21:51:08) ERROR: An error occurred during the fetch of repository 'tf_runtime':. #16 1497.0 Traceback (most recent call last):. #16 1497.0 File ""/root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl"", line 53, column 33, in _tf_http_archive_impl. #16 1497.0 ctx.download_and_extract(. #16 1497.0 Error in download_and_extract: java.io.IOException: Error downloading [http://mirror.tensorflow.org/github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz, https://github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz] to /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/tf_runtime/temp12516918929418979294/64c92c8013b557087351c91b5423b6046d10f206.tar.gz: Checksum was 8383b3247286016e450b0b20e805d26b88ab4638b4e4e3cc4a6923debaf7ad1e but wanted f16fcf09b34e0c7be9389f50652b4b4a14c5a8a96e7e15ad73e8f234d8d09ebe. #16 1497.1 (21:51:08) INFO: Repository llvm-raw instantiated at:. #16 1497.1 /opt/deepvariant/WORKSPACE:102:14: in <toplevel>. #16 1497.1 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/tensorflow/workspace3.bzl:42:9: in workspace. #16 1497.1 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/llvm/workspace.bzl:10:20: in repo. #16 1497.1 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:113:21: in tf_http_archive. #16 1497.1 Repository rule _tf_http_archive defined at:. #16 1497.1 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:66:35: in <toplevel>. #16 1497.2 (21:51:08) Loading: 0 packages loaded. #16 1497.3 (21:51:08) ERROR: no such package '@tf_runtime//': java.io.IOException: Error downloading [http://mirror.tensorflow.org/github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz, ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/608
https://github.com/google/deepvariant/issues/608:8237,integrability,Repositor,Repository,8237,"ar.gz, https://github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz] to /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/tf_runtime/temp12516918929418979294/64c92c8013b557087351c91b5423b6046d10f206.tar.gz: Checksum was 8383b3247286016e450b0b20e805d26b88ab4638b4e4e3cc4a6923debaf7ad1e but wanted f16fcf09b34e0c7be9389f50652b4b4a14c5a8a96e7e15ad73e8f234d8d09ebe. #16 1497.1 (21:51:08) INFO: Repository llvm-raw instantiated at:. #16 1497.1 /opt/deepvariant/WORKSPACE:102:14: in <toplevel>. #16 1497.1 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/tensorflow/workspace3.bzl:42:9: in workspace. #16 1497.1 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/llvm/workspace.bzl:10:20: in repo. #16 1497.1 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:113:21: in tf_http_archive. #16 1497.1 Repository rule _tf_http_archive defined at:. #16 1497.1 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:66:35: in <toplevel>. #16 1497.2 (21:51:08) Loading: 0 packages loaded. #16 1497.3 (21:51:08) ERROR: no such package '@tf_runtime//': java.io.IOException: Error downloading [http://mirror.tensorflow.org/github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz, https://github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz] to /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/tf_runtime/temp12516918929418979294/64c92c8013b557087351c91b5423b6046d10f206.tar.gz: Checksum was 8383b3247286016e450b0b20e805d26b88ab4638b4e4e3cc4a6923debaf7ad1e but wanted f16fcf09b34e0c7be9389f50652b4b4a14c5a8a96e7e15ad73e8f234d8d09ebe. #16 1497.3 (21:51:09) INFO: Elapsed time: 12.546s. #16 1497.3 (21:51:09) INFO: 0 processes. #16 1497.3 (21:51:09) FAILED: Build did NOT co",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/608
https://github.com/google/deepvariant/issues/608:1983,interoperability,transform,transforms,1983,"nal_columns=80. #16 1489.8 (21:51:01) INFO: Reading rc options for 'build' from /opt/tensorflow/.bazelrc:. #16 1489.8 Inherited 'common' options: --experimental_repo_remote_exec. #16 1489.8 (21:51:01) INFO: Reading rc options for 'build' from /opt/tensorflow/.bazelrc:. #16 1489.8 'build' options: --define framework_shared_object=true --java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --host_java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true --deleted_packages=tensorflow/compiler/mlir/tfrt,tensorflow/compiler/mlir/tfrt/benchmarks,tensorflow/compiler/mlir/tfrt/jit/python_binding,tensorflow/compiler/mlir/tfrt/jit/transforms,tensorflow/compiler/mlir/tfrt/python_tests,tensorflow/compiler/mlir/tfrt/tests,tensorflow/compiler/mlir/tfrt/tests/saved_model,tensorflow/compiler/mlir/tfrt/transforms/lhlo_gpu_to_tfrt_gpu,tensorflow/core/runtime_fallback,tensorflow/core/runtime_fallback/conversion,tensorflow/core/runtime_fallback/kernel,tensorflow/core/runtime_fallback/opdefs,tensorflow/core/runtime_fallback/runtime,tensorflow/core/runtime_fallback/util,tensorflow/core/tfrt/common,tensorflow/core/tfrt/eager,tensorflow/core/tfrt/eager/backends/cpu,tensorflow/core/tfrt/eager/backends/gpu,tensorflow/core/tfrt/eager/core_runtime,tensorflow/core/tfrt/eager/cpp_tests/core_runtime,tensorflow/core/tfrt/fallback,tensorflow/core/tfrt/gpu,tensorflow/core/tfrt/run_handler_thread_pool,tensorflow/core/tfrt/runtime,tensorflow/core/tfrt/saved_model,tensorflow/core/tfrt/saved_model/tests,tensorflow/core/tfrt/tpu,tensorflow/core/tfrt/utils. #16 1489.8 (21:51:01) INFO: Reading rc options for 'build' from /opt/tensorflow/.tf_config",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/608
https://github.com/google/deepvariant/issues/608:2151,interoperability,transform,transforms,2151,"mote_exec. #16 1489.8 (21:51:01) INFO: Reading rc options for 'build' from /opt/tensorflow/.bazelrc:. #16 1489.8 'build' options: --define framework_shared_object=true --java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --host_java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true --deleted_packages=tensorflow/compiler/mlir/tfrt,tensorflow/compiler/mlir/tfrt/benchmarks,tensorflow/compiler/mlir/tfrt/jit/python_binding,tensorflow/compiler/mlir/tfrt/jit/transforms,tensorflow/compiler/mlir/tfrt/python_tests,tensorflow/compiler/mlir/tfrt/tests,tensorflow/compiler/mlir/tfrt/tests/saved_model,tensorflow/compiler/mlir/tfrt/transforms/lhlo_gpu_to_tfrt_gpu,tensorflow/core/runtime_fallback,tensorflow/core/runtime_fallback/conversion,tensorflow/core/runtime_fallback/kernel,tensorflow/core/runtime_fallback/opdefs,tensorflow/core/runtime_fallback/runtime,tensorflow/core/runtime_fallback/util,tensorflow/core/tfrt/common,tensorflow/core/tfrt/eager,tensorflow/core/tfrt/eager/backends/cpu,tensorflow/core/tfrt/eager/backends/gpu,tensorflow/core/tfrt/eager/core_runtime,tensorflow/core/tfrt/eager/cpp_tests/core_runtime,tensorflow/core/tfrt/fallback,tensorflow/core/tfrt/gpu,tensorflow/core/tfrt/run_handler_thread_pool,tensorflow/core/tfrt/runtime,tensorflow/core/tfrt/saved_model,tensorflow/core/tfrt/saved_model/tests,tensorflow/core/tfrt/tpu,tensorflow/core/tfrt/utils. #16 1489.8 (21:51:01) INFO: Reading rc options for 'build' from /opt/tensorflow/.tf_configure.bazelrc:. #16 1489.8 'build' options: --action_env PYTHON_BIN_PATH=/usr/local/bin/python3 --action_env PYTHON_LIB_PATH=/usr/local/lib/python3.8/dist-packages --pyth",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/608
https://github.com/google/deepvariant/issues/608:2249,interoperability,convers,conversion,2249,"c:. #16 1489.8 'build' options: --define framework_shared_object=true --java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --host_java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true --deleted_packages=tensorflow/compiler/mlir/tfrt,tensorflow/compiler/mlir/tfrt/benchmarks,tensorflow/compiler/mlir/tfrt/jit/python_binding,tensorflow/compiler/mlir/tfrt/jit/transforms,tensorflow/compiler/mlir/tfrt/python_tests,tensorflow/compiler/mlir/tfrt/tests,tensorflow/compiler/mlir/tfrt/tests/saved_model,tensorflow/compiler/mlir/tfrt/transforms/lhlo_gpu_to_tfrt_gpu,tensorflow/core/runtime_fallback,tensorflow/core/runtime_fallback/conversion,tensorflow/core/runtime_fallback/kernel,tensorflow/core/runtime_fallback/opdefs,tensorflow/core/runtime_fallback/runtime,tensorflow/core/runtime_fallback/util,tensorflow/core/tfrt/common,tensorflow/core/tfrt/eager,tensorflow/core/tfrt/eager/backends/cpu,tensorflow/core/tfrt/eager/backends/gpu,tensorflow/core/tfrt/eager/core_runtime,tensorflow/core/tfrt/eager/cpp_tests/core_runtime,tensorflow/core/tfrt/fallback,tensorflow/core/tfrt/gpu,tensorflow/core/tfrt/run_handler_thread_pool,tensorflow/core/tfrt/runtime,tensorflow/core/tfrt/saved_model,tensorflow/core/tfrt/saved_model/tests,tensorflow/core/tfrt/tpu,tensorflow/core/tfrt/utils. #16 1489.8 (21:51:01) INFO: Reading rc options for 'build' from /opt/tensorflow/.tf_configure.bazelrc:. #16 1489.8 'build' options: --action_env PYTHON_BIN_PATH=/usr/local/bin/python3 --action_env PYTHON_LIB_PATH=/usr/local/lib/python3.8/dist-packages --python_path=/usr/local/bin/python3. #16 1489.8 (21:51:01) INFO: Reading rc options for 'build' from /o",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/608
https://github.com/google/deepvariant/issues/608:5273,interoperability,Repositor,Repository,5273,TOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++14 --host_cxxopt=-std=c++14 --config=dynamic_kernels --distinct_host_configuration=false --experimental_guard_against_concurrent_changes. #16 1489.8 (21:51:01) INFO: Found applicable config definition build:dynamic_kernels in file /opt/tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS. #16 1489.8 (21:51:01) INFO: Found applicable config definition build:linux in file /opt/deepvariant/.bazelrc: --distinct_host_configuration=true. #16 1490.1 (21:51:01) INFO: Current date is 2023-01-30. #16 1490.1 (21:51:01) Loading:. #16 1490.1 (21:51:01) Loading: 0 packages loaded. #16 1491.1 (21:51:02) Loading: 0 packages loaded. #16 1492.2 (21:51:03) Loading: 0 packages loaded. #16 1493.2 (21:51:04) Loading: 0 packages loaded. #16 1494.2 (21:51:05) Loading: 0 packages loaded. #16 1495.2 (21:51:06) Loading: 0 packages loaded. #16 1496.2 (21:51:07) Loading: 0 packages loaded. #16 1497.0 (21:51:08) INFO: Repository tf_runtime instantiated at:. #16 1497.0 /opt/deepvariant/WORKSPACE:102:14: in <toplevel>. #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/tensorflow/workspace3.bzl:28:15: in workspace. #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/tf_runtime/workspace.bzl:12:20: in repo. #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:113:21: in tf_http_archive. #16 1497.0 Repository rule _tf_http_archive defined at:. #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:66:35: in <toplevel>. #16 1497.0 (21:51:08) WARNING: Download from http://mirror.tensorflow.org/github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException ,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/608
https://github.com/google/deepvariant/issues/608:5831,interoperability,Repositor,Repository,5831,urrent date is 2023-01-30. #16 1490.1 (21:51:01) Loading:. #16 1490.1 (21:51:01) Loading: 0 packages loaded. #16 1491.1 (21:51:02) Loading: 0 packages loaded. #16 1492.2 (21:51:03) Loading: 0 packages loaded. #16 1493.2 (21:51:04) Loading: 0 packages loaded. #16 1494.2 (21:51:05) Loading: 0 packages loaded. #16 1495.2 (21:51:06) Loading: 0 packages loaded. #16 1496.2 (21:51:07) Loading: 0 packages loaded. #16 1497.0 (21:51:08) INFO: Repository tf_runtime instantiated at:. #16 1497.0 /opt/deepvariant/WORKSPACE:102:14: in <toplevel>. #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/tensorflow/workspace3.bzl:28:15: in workspace. #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/tf_runtime/workspace.bzl:12:20: in repo. #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:113:21: in tf_http_archive. #16 1497.0 Repository rule _tf_http_archive defined at:. #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:66:35: in <toplevel>. #16 1497.0 (21:51:08) WARNING: Download from http://mirror.tensorflow.org/github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException GET returned 404 Not Found. #16 1497.0 (21:51:08) WARNING: Download from https://github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException Checksum was 8383b3247286016e450b0b20e805d26b88ab4638b4e4e3cc4a6923debaf7ad1e but wanted f16fcf09b34e0c7be9389f50652b4b4a14c5a8a96e7e15ad73e8f234d8d09ebe. #16 1497.0 (21:51:08) ERROR: An error occurred during the fetch of repository 'tf_runtime':. #16 1497.0 Traceback (most recent call last),MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/608
https://github.com/google/deepvariant/issues/608:6229,interoperability,repositor,repository,6229,"es loaded. #16 1497.0 (21:51:08) INFO: Repository tf_runtime instantiated at:. #16 1497.0 /opt/deepvariant/WORKSPACE:102:14: in <toplevel>. #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/tensorflow/workspace3.bzl:28:15: in workspace. #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/tf_runtime/workspace.bzl:12:20: in repo. #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:113:21: in tf_http_archive. #16 1497.0 Repository rule _tf_http_archive defined at:. #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:66:35: in <toplevel>. #16 1497.0 (21:51:08) WARNING: Download from http://mirror.tensorflow.org/github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException GET returned 404 Not Found. #16 1497.0 (21:51:08) WARNING: Download from https://github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException Checksum was 8383b3247286016e450b0b20e805d26b88ab4638b4e4e3cc4a6923debaf7ad1e but wanted f16fcf09b34e0c7be9389f50652b4b4a14c5a8a96e7e15ad73e8f234d8d09ebe. #16 1497.0 (21:51:08) ERROR: An error occurred during the fetch of repository 'tf_runtime':. #16 1497.0 Traceback (most recent call last):. #16 1497.0 File ""/root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl"", line 53, column 33, in _tf_http_archive_impl. #16 1497.0 ctx.download_and_extract(. #16 1497.0 Error in download_and_extract: java.io.IOException: Error downloading [http://mirror.tensorflow.org/github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/608
https://github.com/google/deepvariant/issues/608:6495,interoperability,repositor,repository,6495,"8:15: in workspace. #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/tf_runtime/workspace.bzl:12:20: in repo. #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:113:21: in tf_http_archive. #16 1497.0 Repository rule _tf_http_archive defined at:. #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:66:35: in <toplevel>. #16 1497.0 (21:51:08) WARNING: Download from http://mirror.tensorflow.org/github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException GET returned 404 Not Found. #16 1497.0 (21:51:08) WARNING: Download from https://github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException Checksum was 8383b3247286016e450b0b20e805d26b88ab4638b4e4e3cc4a6923debaf7ad1e but wanted f16fcf09b34e0c7be9389f50652b4b4a14c5a8a96e7e15ad73e8f234d8d09ebe. #16 1497.0 (21:51:08) ERROR: An error occurred during the fetch of repository 'tf_runtime':. #16 1497.0 Traceback (most recent call last):. #16 1497.0 File ""/root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl"", line 53, column 33, in _tf_http_archive_impl. #16 1497.0 ctx.download_and_extract(. #16 1497.0 Error in download_and_extract: java.io.IOException: Error downloading [http://mirror.tensorflow.org/github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz, https://github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz] to /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/tf_runtime/temp12516918929418979294/64c92c8013b557087351c91b5423b6046d10f206.tar",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/608
https://github.com/google/deepvariant/issues/608:6766,interoperability,repositor,repository,6766,"third_party/repo.bzl:113:21: in tf_http_archive. #16 1497.0 Repository rule _tf_http_archive defined at:. #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:66:35: in <toplevel>. #16 1497.0 (21:51:08) WARNING: Download from http://mirror.tensorflow.org/github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException GET returned 404 Not Found. #16 1497.0 (21:51:08) WARNING: Download from https://github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException Checksum was 8383b3247286016e450b0b20e805d26b88ab4638b4e4e3cc4a6923debaf7ad1e but wanted f16fcf09b34e0c7be9389f50652b4b4a14c5a8a96e7e15ad73e8f234d8d09ebe. #16 1497.0 (21:51:08) ERROR: An error occurred during the fetch of repository 'tf_runtime':. #16 1497.0 Traceback (most recent call last):. #16 1497.0 File ""/root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl"", line 53, column 33, in _tf_http_archive_impl. #16 1497.0 ctx.download_and_extract(. #16 1497.0 Error in download_and_extract: java.io.IOException: Error downloading [http://mirror.tensorflow.org/github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz, https://github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz] to /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/tf_runtime/temp12516918929418979294/64c92c8013b557087351c91b5423b6046d10f206.tar.gz: Checksum was 8383b3247286016e450b0b20e805d26b88ab4638b4e4e3cc4a6923debaf7ad1e but wanted f16fcf09b34e0c7be9389f50652b4b4a14c5a8a96e7e15ad73e8f234d8d09ebe. #16 1497.1 (21:51:08) INFO: Repository llvm-raw instantiated at:. #16 1497.1 /opt/deepvariant/WORKSPACE:102:14:",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/608
https://github.com/google/deepvariant/issues/608:7688,interoperability,Repositor,Repository,7688,"9ebe. #16 1497.0 (21:51:08) ERROR: An error occurred during the fetch of repository 'tf_runtime':. #16 1497.0 Traceback (most recent call last):. #16 1497.0 File ""/root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl"", line 53, column 33, in _tf_http_archive_impl. #16 1497.0 ctx.download_and_extract(. #16 1497.0 Error in download_and_extract: java.io.IOException: Error downloading [http://mirror.tensorflow.org/github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz, https://github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz] to /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/tf_runtime/temp12516918929418979294/64c92c8013b557087351c91b5423b6046d10f206.tar.gz: Checksum was 8383b3247286016e450b0b20e805d26b88ab4638b4e4e3cc4a6923debaf7ad1e but wanted f16fcf09b34e0c7be9389f50652b4b4a14c5a8a96e7e15ad73e8f234d8d09ebe. #16 1497.1 (21:51:08) INFO: Repository llvm-raw instantiated at:. #16 1497.1 /opt/deepvariant/WORKSPACE:102:14: in <toplevel>. #16 1497.1 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/tensorflow/workspace3.bzl:42:9: in workspace. #16 1497.1 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/llvm/workspace.bzl:10:20: in repo. #16 1497.1 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:113:21: in tf_http_archive. #16 1497.1 Repository rule _tf_http_archive defined at:. #16 1497.1 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:66:35: in <toplevel>. #16 1497.2 (21:51:08) Loading: 0 packages loaded. #16 1497.3 (21:51:08) ERROR: no such package '@tf_runtime//': java.io.IOException: Error downloading [http://mirror.tensorflow.org/github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz, ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/608
https://github.com/google/deepvariant/issues/608:8237,interoperability,Repositor,Repository,8237,"ar.gz, https://github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz] to /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/tf_runtime/temp12516918929418979294/64c92c8013b557087351c91b5423b6046d10f206.tar.gz: Checksum was 8383b3247286016e450b0b20e805d26b88ab4638b4e4e3cc4a6923debaf7ad1e but wanted f16fcf09b34e0c7be9389f50652b4b4a14c5a8a96e7e15ad73e8f234d8d09ebe. #16 1497.1 (21:51:08) INFO: Repository llvm-raw instantiated at:. #16 1497.1 /opt/deepvariant/WORKSPACE:102:14: in <toplevel>. #16 1497.1 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/tensorflow/workspace3.bzl:42:9: in workspace. #16 1497.1 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/llvm/workspace.bzl:10:20: in repo. #16 1497.1 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:113:21: in tf_http_archive. #16 1497.1 Repository rule _tf_http_archive defined at:. #16 1497.1 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:66:35: in <toplevel>. #16 1497.2 (21:51:08) Loading: 0 packages loaded. #16 1497.3 (21:51:08) ERROR: no such package '@tf_runtime//': java.io.IOException: Error downloading [http://mirror.tensorflow.org/github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz, https://github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz] to /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/tf_runtime/temp12516918929418979294/64c92c8013b557087351c91b5423b6046d10f206.tar.gz: Checksum was 8383b3247286016e450b0b20e805d26b88ab4638b4e4e3cc4a6923debaf7ad1e but wanted f16fcf09b34e0c7be9389f50652b4b4a14c5a8a96e7e15ad73e8f234d8d09ebe. #16 1497.3 (21:51:09) INFO: Elapsed time: 12.546s. #16 1497.3 (21:51:09) INFO: 0 processes. #16 1497.3 (21:51:09) FAILED: Build did NOT co",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/608
https://github.com/google/deepvariant/issues/608:246,modifiability,version,version,246,"binary creation fails; bazel unable to fetch tf_runtime; I was able to build the docker image last week, but this week the build fails at binary creation, with bazel unable to download tf_runtime. . - Operating system: Ubuntu20.04. - DeepVariant version: 1.4.0. - Building docker image locally. **Steps to reproduce:**. - Command: `docker build .` in source directory (no modifications). - Error trace: . . ```. #16 1484.7 ========== [Mon Jan 30 21:50:56 UTC 2023] Stage 'build-prereq.sh complete' starting. #16 1484.7 Extracting Bazel installation... #16 1487.8 Starting local Bazel server and connecting to it... #16 1489.8 (21:51:01) WARNING: option '--distinct_host_configuration' was expanded to from both option '--enable_platform_specific_config' (source /opt/tensorflow/.bazelrc) and option '--enable_platform_specific_config' (source /opt/tensorflow/.bazelrc). #16 1489.8 (21:51:01) INFO: Options provided by the client:. #16 1489.8 Inherited 'common' options: --isatty=0 --terminal_columns=80. #16 1489.8 (21:51:01) INFO: Reading rc options for 'build' from /opt/tensorflow/.bazelrc:. #16 1489.8 Inherited 'common' options: --experimental_repo_remote_exec. #16 1489.8 (21:51:01) INFO: Reading rc options for 'build' from /opt/tensorflow/.bazelrc:. #16 1489.8 'build' options: --define framework_shared_object=true --java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --host_java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true --deleted_packages=tensorflow/compiler/mlir/tfrt,tensorflow/compiler/mlir/tfrt/benchmarks,tensorflow/compiler/mlir/tfrt/jit/python_binding,tensorflow/compiler/mlir/tfrt/jit/transforms,tensor",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/608
https://github.com/google/deepvariant/issues/608:942,modifiability,Inherit,Inherited,942,"binary creation fails; bazel unable to fetch tf_runtime; I was able to build the docker image last week, but this week the build fails at binary creation, with bazel unable to download tf_runtime. . - Operating system: Ubuntu20.04. - DeepVariant version: 1.4.0. - Building docker image locally. **Steps to reproduce:**. - Command: `docker build .` in source directory (no modifications). - Error trace: . . ```. #16 1484.7 ========== [Mon Jan 30 21:50:56 UTC 2023] Stage 'build-prereq.sh complete' starting. #16 1484.7 Extracting Bazel installation... #16 1487.8 Starting local Bazel server and connecting to it... #16 1489.8 (21:51:01) WARNING: option '--distinct_host_configuration' was expanded to from both option '--enable_platform_specific_config' (source /opt/tensorflow/.bazelrc) and option '--enable_platform_specific_config' (source /opt/tensorflow/.bazelrc). #16 1489.8 (21:51:01) INFO: Options provided by the client:. #16 1489.8 Inherited 'common' options: --isatty=0 --terminal_columns=80. #16 1489.8 (21:51:01) INFO: Reading rc options for 'build' from /opt/tensorflow/.bazelrc:. #16 1489.8 Inherited 'common' options: --experimental_repo_remote_exec. #16 1489.8 (21:51:01) INFO: Reading rc options for 'build' from /opt/tensorflow/.bazelrc:. #16 1489.8 'build' options: --define framework_shared_object=true --java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --host_java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true --deleted_packages=tensorflow/compiler/mlir/tfrt,tensorflow/compiler/mlir/tfrt/benchmarks,tensorflow/compiler/mlir/tfrt/jit/python_binding,tensorflow/compiler/mlir/tfrt/jit/transforms,tensor",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/608
https://github.com/google/deepvariant/issues/608:1106,modifiability,Inherit,Inherited,1106,"is week the build fails at binary creation, with bazel unable to download tf_runtime. . - Operating system: Ubuntu20.04. - DeepVariant version: 1.4.0. - Building docker image locally. **Steps to reproduce:**. - Command: `docker build .` in source directory (no modifications). - Error trace: . . ```. #16 1484.7 ========== [Mon Jan 30 21:50:56 UTC 2023] Stage 'build-prereq.sh complete' starting. #16 1484.7 Extracting Bazel installation... #16 1487.8 Starting local Bazel server and connecting to it... #16 1489.8 (21:51:01) WARNING: option '--distinct_host_configuration' was expanded to from both option '--enable_platform_specific_config' (source /opt/tensorflow/.bazelrc) and option '--enable_platform_specific_config' (source /opt/tensorflow/.bazelrc). #16 1489.8 (21:51:01) INFO: Options provided by the client:. #16 1489.8 Inherited 'common' options: --isatty=0 --terminal_columns=80. #16 1489.8 (21:51:01) INFO: Reading rc options for 'build' from /opt/tensorflow/.bazelrc:. #16 1489.8 Inherited 'common' options: --experimental_repo_remote_exec. #16 1489.8 (21:51:01) INFO: Reading rc options for 'build' from /opt/tensorflow/.bazelrc:. #16 1489.8 'build' options: --define framework_shared_object=true --java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --host_java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true --deleted_packages=tensorflow/compiler/mlir/tfrt,tensorflow/compiler/mlir/tfrt/benchmarks,tensorflow/compiler/mlir/tfrt/jit/python_binding,tensorflow/compiler/mlir/tfrt/jit/transforms,tensorflow/compiler/mlir/tfrt/python_tests,tensorflow/compiler/mlir/tfrt/tests,tensorflow/compiler/mlir/tfrt/tests/sa",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/608
https://github.com/google/deepvariant/issues/608:3141,modifiability,pac,packages,3141,"/tfrt/transforms/lhlo_gpu_to_tfrt_gpu,tensorflow/core/runtime_fallback,tensorflow/core/runtime_fallback/conversion,tensorflow/core/runtime_fallback/kernel,tensorflow/core/runtime_fallback/opdefs,tensorflow/core/runtime_fallback/runtime,tensorflow/core/runtime_fallback/util,tensorflow/core/tfrt/common,tensorflow/core/tfrt/eager,tensorflow/core/tfrt/eager/backends/cpu,tensorflow/core/tfrt/eager/backends/gpu,tensorflow/core/tfrt/eager/core_runtime,tensorflow/core/tfrt/eager/cpp_tests/core_runtime,tensorflow/core/tfrt/fallback,tensorflow/core/tfrt/gpu,tensorflow/core/tfrt/run_handler_thread_pool,tensorflow/core/tfrt/runtime,tensorflow/core/tfrt/saved_model,tensorflow/core/tfrt/saved_model/tests,tensorflow/core/tfrt/tpu,tensorflow/core/tfrt/utils. #16 1489.8 (21:51:01) INFO: Reading rc options for 'build' from /opt/tensorflow/.tf_configure.bazelrc:. #16 1489.8 'build' options: --action_env PYTHON_BIN_PATH=/usr/local/bin/python3 --action_env PYTHON_LIB_PATH=/usr/local/lib/python3.8/dist-packages --python_path=/usr/local/bin/python3. #16 1489.8 (21:51:01) INFO: Reading rc options for 'build' from /opt/deepvariant/.bazelrc:. #16 1489.8 'build' options: --jobs 128 --config=monolithic --show_timestamps --verbose_failures --define=use_fast_cpp_protos=true --copt=-Wno-maybe-uninitialized --copt=-Wno-unused-function --cxxopt=-std=c++17 --python_top=//:deepvariant_python_runtime --incompatible_use_python_toolchains=false. #16 1489.8 (21:51:01) INFO: Found applicable config definition build:short_logs in file /opt/tensorflow/.bazelrc: --output_filter=DONT_MATCH_ANYTHING. #16 1489.8 (21:51:01) INFO: Found applicable config definition build:v2 in file /opt/tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1. #16 1489.8 (21:51:01) INFO: Found applicable config definition build:monolithic in file /opt/tensorflow/.bazelrc: --define framework_shared_object=false. #16 1489.8 (21:51:01) INFO: Found applicable config definition build:linux in file /opt/tensorflow/.ba",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/608
https://github.com/google/deepvariant/issues/608:4928,modifiability,pac,packages,4928,e config definition build:monolithic in file /opt/tensorflow/.bazelrc: --define framework_shared_object=false. #16 1489.8 (21:51:01) INFO: Found applicable config definition build:linux in file /opt/tensorflow/.bazelrc: --copt=-w --host_copt=-w --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++14 --host_cxxopt=-std=c++14 --config=dynamic_kernels --distinct_host_configuration=false --experimental_guard_against_concurrent_changes. #16 1489.8 (21:51:01) INFO: Found applicable config definition build:dynamic_kernels in file /opt/tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS. #16 1489.8 (21:51:01) INFO: Found applicable config definition build:linux in file /opt/deepvariant/.bazelrc: --distinct_host_configuration=true. #16 1490.1 (21:51:01) INFO: Current date is 2023-01-30. #16 1490.1 (21:51:01) Loading:. #16 1490.1 (21:51:01) Loading: 0 packages loaded. #16 1491.1 (21:51:02) Loading: 0 packages loaded. #16 1492.2 (21:51:03) Loading: 0 packages loaded. #16 1493.2 (21:51:04) Loading: 0 packages loaded. #16 1494.2 (21:51:05) Loading: 0 packages loaded. #16 1495.2 (21:51:06) Loading: 0 packages loaded. #16 1496.2 (21:51:07) Loading: 0 packages loaded. #16 1497.0 (21:51:08) INFO: Repository tf_runtime instantiated at:. #16 1497.0 /opt/deepvariant/WORKSPACE:102:14: in <toplevel>. #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/tensorflow/workspace3.bzl:28:15: in workspace. #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/tf_runtime/workspace.bzl:12:20: in repo. #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:113:21: in tf_http_archive. #16 1497.0 Repository rule _tf_http_archive defined at:. #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/608
https://github.com/google/deepvariant/issues/608:4978,modifiability,pac,packages,4978,tensorflow/.bazelrc: --define framework_shared_object=false. #16 1489.8 (21:51:01) INFO: Found applicable config definition build:linux in file /opt/tensorflow/.bazelrc: --copt=-w --host_copt=-w --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++14 --host_cxxopt=-std=c++14 --config=dynamic_kernels --distinct_host_configuration=false --experimental_guard_against_concurrent_changes. #16 1489.8 (21:51:01) INFO: Found applicable config definition build:dynamic_kernels in file /opt/tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS. #16 1489.8 (21:51:01) INFO: Found applicable config definition build:linux in file /opt/deepvariant/.bazelrc: --distinct_host_configuration=true. #16 1490.1 (21:51:01) INFO: Current date is 2023-01-30. #16 1490.1 (21:51:01) Loading:. #16 1490.1 (21:51:01) Loading: 0 packages loaded. #16 1491.1 (21:51:02) Loading: 0 packages loaded. #16 1492.2 (21:51:03) Loading: 0 packages loaded. #16 1493.2 (21:51:04) Loading: 0 packages loaded. #16 1494.2 (21:51:05) Loading: 0 packages loaded. #16 1495.2 (21:51:06) Loading: 0 packages loaded. #16 1496.2 (21:51:07) Loading: 0 packages loaded. #16 1497.0 (21:51:08) INFO: Repository tf_runtime instantiated at:. #16 1497.0 /opt/deepvariant/WORKSPACE:102:14: in <toplevel>. #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/tensorflow/workspace3.bzl:28:15: in workspace. #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/tf_runtime/workspace.bzl:12:20: in repo. #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:113:21: in tf_http_archive. #16 1497.0 Repository rule _tf_http_archive defined at:. #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/608
https://github.com/google/deepvariant/issues/608:5028,modifiability,pac,packages,5028,ect=false. #16 1489.8 (21:51:01) INFO: Found applicable config definition build:linux in file /opt/tensorflow/.bazelrc: --copt=-w --host_copt=-w --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++14 --host_cxxopt=-std=c++14 --config=dynamic_kernels --distinct_host_configuration=false --experimental_guard_against_concurrent_changes. #16 1489.8 (21:51:01) INFO: Found applicable config definition build:dynamic_kernels in file /opt/tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS. #16 1489.8 (21:51:01) INFO: Found applicable config definition build:linux in file /opt/deepvariant/.bazelrc: --distinct_host_configuration=true. #16 1490.1 (21:51:01) INFO: Current date is 2023-01-30. #16 1490.1 (21:51:01) Loading:. #16 1490.1 (21:51:01) Loading: 0 packages loaded. #16 1491.1 (21:51:02) Loading: 0 packages loaded. #16 1492.2 (21:51:03) Loading: 0 packages loaded. #16 1493.2 (21:51:04) Loading: 0 packages loaded. #16 1494.2 (21:51:05) Loading: 0 packages loaded. #16 1495.2 (21:51:06) Loading: 0 packages loaded. #16 1496.2 (21:51:07) Loading: 0 packages loaded. #16 1497.0 (21:51:08) INFO: Repository tf_runtime instantiated at:. #16 1497.0 /opt/deepvariant/WORKSPACE:102:14: in <toplevel>. #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/tensorflow/workspace3.bzl:28:15: in workspace. #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/tf_runtime/workspace.bzl:12:20: in repo. #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:113:21: in tf_http_archive. #16 1497.0 Repository rule _tf_http_archive defined at:. #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:66:35: in <toplevel>. #16 1497.0 (2,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/608
https://github.com/google/deepvariant/issues/608:5078,modifiability,pac,packages,5078,cable config definition build:linux in file /opt/tensorflow/.bazelrc: --copt=-w --host_copt=-w --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++14 --host_cxxopt=-std=c++14 --config=dynamic_kernels --distinct_host_configuration=false --experimental_guard_against_concurrent_changes. #16 1489.8 (21:51:01) INFO: Found applicable config definition build:dynamic_kernels in file /opt/tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS. #16 1489.8 (21:51:01) INFO: Found applicable config definition build:linux in file /opt/deepvariant/.bazelrc: --distinct_host_configuration=true. #16 1490.1 (21:51:01) INFO: Current date is 2023-01-30. #16 1490.1 (21:51:01) Loading:. #16 1490.1 (21:51:01) Loading: 0 packages loaded. #16 1491.1 (21:51:02) Loading: 0 packages loaded. #16 1492.2 (21:51:03) Loading: 0 packages loaded. #16 1493.2 (21:51:04) Loading: 0 packages loaded. #16 1494.2 (21:51:05) Loading: 0 packages loaded. #16 1495.2 (21:51:06) Loading: 0 packages loaded. #16 1496.2 (21:51:07) Loading: 0 packages loaded. #16 1497.0 (21:51:08) INFO: Repository tf_runtime instantiated at:. #16 1497.0 /opt/deepvariant/WORKSPACE:102:14: in <toplevel>. #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/tensorflow/workspace3.bzl:28:15: in workspace. #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/tf_runtime/workspace.bzl:12:20: in repo. #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:113:21: in tf_http_archive. #16 1497.0 Repository rule _tf_http_archive defined at:. #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:66:35: in <toplevel>. #16 1497.0 (21:51:08) WARNING: Download from http://mirror.tens,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/608
https://github.com/google/deepvariant/issues/608:5128,modifiability,pac,packages,5128,ensorflow/.bazelrc: --copt=-w --host_copt=-w --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++14 --host_cxxopt=-std=c++14 --config=dynamic_kernels --distinct_host_configuration=false --experimental_guard_against_concurrent_changes. #16 1489.8 (21:51:01) INFO: Found applicable config definition build:dynamic_kernels in file /opt/tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS. #16 1489.8 (21:51:01) INFO: Found applicable config definition build:linux in file /opt/deepvariant/.bazelrc: --distinct_host_configuration=true. #16 1490.1 (21:51:01) INFO: Current date is 2023-01-30. #16 1490.1 (21:51:01) Loading:. #16 1490.1 (21:51:01) Loading: 0 packages loaded. #16 1491.1 (21:51:02) Loading: 0 packages loaded. #16 1492.2 (21:51:03) Loading: 0 packages loaded. #16 1493.2 (21:51:04) Loading: 0 packages loaded. #16 1494.2 (21:51:05) Loading: 0 packages loaded. #16 1495.2 (21:51:06) Loading: 0 packages loaded. #16 1496.2 (21:51:07) Loading: 0 packages loaded. #16 1497.0 (21:51:08) INFO: Repository tf_runtime instantiated at:. #16 1497.0 /opt/deepvariant/WORKSPACE:102:14: in <toplevel>. #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/tensorflow/workspace3.bzl:28:15: in workspace. #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/tf_runtime/workspace.bzl:12:20: in repo. #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:113:21: in tf_http_archive. #16 1497.0 Repository rule _tf_http_archive defined at:. #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:66:35: in <toplevel>. #16 1497.0 (21:51:08) WARNING: Download from http://mirror.tensorflow.org/github.com/tensorflow/runtime/archive/6,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/608
https://github.com/google/deepvariant/issues/608:5178,modifiability,pac,packages,5178,ine=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++14 --host_cxxopt=-std=c++14 --config=dynamic_kernels --distinct_host_configuration=false --experimental_guard_against_concurrent_changes. #16 1489.8 (21:51:01) INFO: Found applicable config definition build:dynamic_kernels in file /opt/tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS. #16 1489.8 (21:51:01) INFO: Found applicable config definition build:linux in file /opt/deepvariant/.bazelrc: --distinct_host_configuration=true. #16 1490.1 (21:51:01) INFO: Current date is 2023-01-30. #16 1490.1 (21:51:01) Loading:. #16 1490.1 (21:51:01) Loading: 0 packages loaded. #16 1491.1 (21:51:02) Loading: 0 packages loaded. #16 1492.2 (21:51:03) Loading: 0 packages loaded. #16 1493.2 (21:51:04) Loading: 0 packages loaded. #16 1494.2 (21:51:05) Loading: 0 packages loaded. #16 1495.2 (21:51:06) Loading: 0 packages loaded. #16 1496.2 (21:51:07) Loading: 0 packages loaded. #16 1497.0 (21:51:08) INFO: Repository tf_runtime instantiated at:. #16 1497.0 /opt/deepvariant/WORKSPACE:102:14: in <toplevel>. #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/tensorflow/workspace3.bzl:28:15: in workspace. #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/tf_runtime/workspace.bzl:12:20: in repo. #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:113:21: in tf_http_archive. #16 1497.0 Repository rule _tf_http_archive defined at:. #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:66:35: in <toplevel>. #16 1497.0 (21:51:08) WARNING: Download from http://mirror.tensorflow.org/github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz fai,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/608
https://github.com/google/deepvariant/issues/608:5228,modifiability,pac,packages,5228,fine=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++14 --host_cxxopt=-std=c++14 --config=dynamic_kernels --distinct_host_configuration=false --experimental_guard_against_concurrent_changes. #16 1489.8 (21:51:01) INFO: Found applicable config definition build:dynamic_kernels in file /opt/tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS. #16 1489.8 (21:51:01) INFO: Found applicable config definition build:linux in file /opt/deepvariant/.bazelrc: --distinct_host_configuration=true. #16 1490.1 (21:51:01) INFO: Current date is 2023-01-30. #16 1490.1 (21:51:01) Loading:. #16 1490.1 (21:51:01) Loading: 0 packages loaded. #16 1491.1 (21:51:02) Loading: 0 packages loaded. #16 1492.2 (21:51:03) Loading: 0 packages loaded. #16 1493.2 (21:51:04) Loading: 0 packages loaded. #16 1494.2 (21:51:05) Loading: 0 packages loaded. #16 1495.2 (21:51:06) Loading: 0 packages loaded. #16 1496.2 (21:51:07) Loading: 0 packages loaded. #16 1497.0 (21:51:08) INFO: Repository tf_runtime instantiated at:. #16 1497.0 /opt/deepvariant/WORKSPACE:102:14: in <toplevel>. #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/tensorflow/workspace3.bzl:28:15: in workspace. #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/tf_runtime/workspace.bzl:12:20: in repo. #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:113:21: in tf_http_archive. #16 1497.0 Repository rule _tf_http_archive defined at:. #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:66:35: in <toplevel>. #16 1497.0 (21:51:08) WARNING: Download from http://mirror.tensorflow.org/github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz failed: class com.google.devtools.build.lib.bazel.rep,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/608
https://github.com/google/deepvariant/issues/608:8458,modifiability,pac,packages,8458,"8013b557087351c91b5423b6046d10f206.tar.gz: Checksum was 8383b3247286016e450b0b20e805d26b88ab4638b4e4e3cc4a6923debaf7ad1e but wanted f16fcf09b34e0c7be9389f50652b4b4a14c5a8a96e7e15ad73e8f234d8d09ebe. #16 1497.1 (21:51:08) INFO: Repository llvm-raw instantiated at:. #16 1497.1 /opt/deepvariant/WORKSPACE:102:14: in <toplevel>. #16 1497.1 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/tensorflow/workspace3.bzl:42:9: in workspace. #16 1497.1 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/llvm/workspace.bzl:10:20: in repo. #16 1497.1 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:113:21: in tf_http_archive. #16 1497.1 Repository rule _tf_http_archive defined at:. #16 1497.1 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:66:35: in <toplevel>. #16 1497.2 (21:51:08) Loading: 0 packages loaded. #16 1497.3 (21:51:08) ERROR: no such package '@tf_runtime//': java.io.IOException: Error downloading [http://mirror.tensorflow.org/github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz, https://github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz] to /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/tf_runtime/temp12516918929418979294/64c92c8013b557087351c91b5423b6046d10f206.tar.gz: Checksum was 8383b3247286016e450b0b20e805d26b88ab4638b4e4e3cc4a6923debaf7ad1e but wanted f16fcf09b34e0c7be9389f50652b4b4a14c5a8a96e7e15ad73e8f234d8d09ebe. #16 1497.3 (21:51:09) INFO: Elapsed time: 12.546s. #16 1497.3 (21:51:09) INFO: 0 processes. #16 1497.3 (21:51:09) FAILED: Build did NOT complete successfully (0 packages loaded). #16 1497.3 (21:51:09) FAILED: Build did NOT complete successfully (0 packages loaded). #16 ERROR: executor failed running [/bin/sh -c ./build-prereq.sh && PATH=""${HOME}/bin:${PATH",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/608
https://github.com/google/deepvariant/issues/608:8512,modifiability,pac,package,8512,"s 8383b3247286016e450b0b20e805d26b88ab4638b4e4e3cc4a6923debaf7ad1e but wanted f16fcf09b34e0c7be9389f50652b4b4a14c5a8a96e7e15ad73e8f234d8d09ebe. #16 1497.1 (21:51:08) INFO: Repository llvm-raw instantiated at:. #16 1497.1 /opt/deepvariant/WORKSPACE:102:14: in <toplevel>. #16 1497.1 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/tensorflow/workspace3.bzl:42:9: in workspace. #16 1497.1 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/llvm/workspace.bzl:10:20: in repo. #16 1497.1 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:113:21: in tf_http_archive. #16 1497.1 Repository rule _tf_http_archive defined at:. #16 1497.1 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:66:35: in <toplevel>. #16 1497.2 (21:51:08) Loading: 0 packages loaded. #16 1497.3 (21:51:08) ERROR: no such package '@tf_runtime//': java.io.IOException: Error downloading [http://mirror.tensorflow.org/github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz, https://github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz] to /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/tf_runtime/temp12516918929418979294/64c92c8013b557087351c91b5423b6046d10f206.tar.gz: Checksum was 8383b3247286016e450b0b20e805d26b88ab4638b4e4e3cc4a6923debaf7ad1e but wanted f16fcf09b34e0c7be9389f50652b4b4a14c5a8a96e7e15ad73e8f234d8d09ebe. #16 1497.3 (21:51:09) INFO: Elapsed time: 12.546s. #16 1497.3 (21:51:09) INFO: 0 processes. #16 1497.3 (21:51:09) FAILED: Build did NOT complete successfully (0 packages loaded). #16 1497.3 (21:51:09) FAILED: Build did NOT complete successfully (0 packages loaded). #16 ERROR: executor failed running [/bin/sh -c ./build-prereq.sh && PATH=""${HOME}/bin:${PATH}"" ./build_release_binaries.sh # PATH for bazel]: exit",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/608
https://github.com/google/deepvariant/issues/608:9265,modifiability,pac,packages,9265,"80/external/org_tensorflow/tensorflow/workspace3.bzl:42:9: in workspace. #16 1497.1 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/llvm/workspace.bzl:10:20: in repo. #16 1497.1 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:113:21: in tf_http_archive. #16 1497.1 Repository rule _tf_http_archive defined at:. #16 1497.1 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:66:35: in <toplevel>. #16 1497.2 (21:51:08) Loading: 0 packages loaded. #16 1497.3 (21:51:08) ERROR: no such package '@tf_runtime//': java.io.IOException: Error downloading [http://mirror.tensorflow.org/github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz, https://github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz] to /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/tf_runtime/temp12516918929418979294/64c92c8013b557087351c91b5423b6046d10f206.tar.gz: Checksum was 8383b3247286016e450b0b20e805d26b88ab4638b4e4e3cc4a6923debaf7ad1e but wanted f16fcf09b34e0c7be9389f50652b4b4a14c5a8a96e7e15ad73e8f234d8d09ebe. #16 1497.3 (21:51:09) INFO: Elapsed time: 12.546s. #16 1497.3 (21:51:09) INFO: 0 processes. #16 1497.3 (21:51:09) FAILED: Build did NOT complete successfully (0 packages loaded). #16 1497.3 (21:51:09) FAILED: Build did NOT complete successfully (0 packages loaded). #16 ERROR: executor failed running [/bin/sh -c ./build-prereq.sh && PATH=""${HOME}/bin:${PATH}"" ./build_release_binaries.sh # PATH for bazel]: exit code: 1. ------. > [builder 6/6] RUN ./build-prereq.sh && PATH=""${HOME}/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin"" ./build_release_binaries.sh # PATH for bazel:. ------. executor failed running [/bin/sh -c ./build-prereq.sh && PATH=""${HOME}/bin:${PATH}"" ./build_release_binaries.sh # PATH for bazel]: exit code: 1. ```.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/608
https://github.com/google/deepvariant/issues/608:9352,modifiability,pac,packages,9352,"80/external/org_tensorflow/tensorflow/workspace3.bzl:42:9: in workspace. #16 1497.1 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/llvm/workspace.bzl:10:20: in repo. #16 1497.1 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:113:21: in tf_http_archive. #16 1497.1 Repository rule _tf_http_archive defined at:. #16 1497.1 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:66:35: in <toplevel>. #16 1497.2 (21:51:08) Loading: 0 packages loaded. #16 1497.3 (21:51:08) ERROR: no such package '@tf_runtime//': java.io.IOException: Error downloading [http://mirror.tensorflow.org/github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz, https://github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz] to /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/tf_runtime/temp12516918929418979294/64c92c8013b557087351c91b5423b6046d10f206.tar.gz: Checksum was 8383b3247286016e450b0b20e805d26b88ab4638b4e4e3cc4a6923debaf7ad1e but wanted f16fcf09b34e0c7be9389f50652b4b4a14c5a8a96e7e15ad73e8f234d8d09ebe. #16 1497.3 (21:51:09) INFO: Elapsed time: 12.546s. #16 1497.3 (21:51:09) INFO: 0 processes. #16 1497.3 (21:51:09) FAILED: Build did NOT complete successfully (0 packages loaded). #16 1497.3 (21:51:09) FAILED: Build did NOT complete successfully (0 packages loaded). #16 ERROR: executor failed running [/bin/sh -c ./build-prereq.sh && PATH=""${HOME}/bin:${PATH}"" ./build_release_binaries.sh # PATH for bazel]: exit code: 1. ------. > [builder 6/6] RUN ./build-prereq.sh && PATH=""${HOME}/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin"" ./build_release_binaries.sh # PATH for bazel:. ------. executor failed running [/bin/sh -c ./build-prereq.sh && PATH=""${HOME}/bin:${PATH}"" ./build_release_binaries.sh # PATH for bazel]: exit code: 1. ```.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/608
https://github.com/google/deepvariant/issues/608:390,performance,Error,Error,390,"binary creation fails; bazel unable to fetch tf_runtime; I was able to build the docker image last week, but this week the build fails at binary creation, with bazel unable to download tf_runtime. . - Operating system: Ubuntu20.04. - DeepVariant version: 1.4.0. - Building docker image locally. **Steps to reproduce:**. - Command: `docker build .` in source directory (no modifications). - Error trace: . . ```. #16 1484.7 ========== [Mon Jan 30 21:50:56 UTC 2023] Stage 'build-prereq.sh complete' starting. #16 1484.7 Extracting Bazel installation... #16 1487.8 Starting local Bazel server and connecting to it... #16 1489.8 (21:51:01) WARNING: option '--distinct_host_configuration' was expanded to from both option '--enable_platform_specific_config' (source /opt/tensorflow/.bazelrc) and option '--enable_platform_specific_config' (source /opt/tensorflow/.bazelrc). #16 1489.8 (21:51:01) INFO: Options provided by the client:. #16 1489.8 Inherited 'common' options: --isatty=0 --terminal_columns=80. #16 1489.8 (21:51:01) INFO: Reading rc options for 'build' from /opt/tensorflow/.bazelrc:. #16 1489.8 Inherited 'common' options: --experimental_repo_remote_exec. #16 1489.8 (21:51:01) INFO: Reading rc options for 'build' from /opt/tensorflow/.bazelrc:. #16 1489.8 'build' options: --define framework_shared_object=true --java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --host_java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true --deleted_packages=tensorflow/compiler/mlir/tfrt,tensorflow/compiler/mlir/tfrt/benchmarks,tensorflow/compiler/mlir/tfrt/jit/python_binding,tensorflow/compiler/mlir/tfrt/jit/transforms,tensor",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/608
https://github.com/google/deepvariant/issues/608:2510,performance,cpu,cpu,2510,"oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true --deleted_packages=tensorflow/compiler/mlir/tfrt,tensorflow/compiler/mlir/tfrt/benchmarks,tensorflow/compiler/mlir/tfrt/jit/python_binding,tensorflow/compiler/mlir/tfrt/jit/transforms,tensorflow/compiler/mlir/tfrt/python_tests,tensorflow/compiler/mlir/tfrt/tests,tensorflow/compiler/mlir/tfrt/tests/saved_model,tensorflow/compiler/mlir/tfrt/transforms/lhlo_gpu_to_tfrt_gpu,tensorflow/core/runtime_fallback,tensorflow/core/runtime_fallback/conversion,tensorflow/core/runtime_fallback/kernel,tensorflow/core/runtime_fallback/opdefs,tensorflow/core/runtime_fallback/runtime,tensorflow/core/runtime_fallback/util,tensorflow/core/tfrt/common,tensorflow/core/tfrt/eager,tensorflow/core/tfrt/eager/backends/cpu,tensorflow/core/tfrt/eager/backends/gpu,tensorflow/core/tfrt/eager/core_runtime,tensorflow/core/tfrt/eager/cpp_tests/core_runtime,tensorflow/core/tfrt/fallback,tensorflow/core/tfrt/gpu,tensorflow/core/tfrt/run_handler_thread_pool,tensorflow/core/tfrt/runtime,tensorflow/core/tfrt/saved_model,tensorflow/core/tfrt/saved_model/tests,tensorflow/core/tfrt/tpu,tensorflow/core/tfrt/utils. #16 1489.8 (21:51:01) INFO: Reading rc options for 'build' from /opt/tensorflow/.tf_configure.bazelrc:. #16 1489.8 'build' options: --action_env PYTHON_BIN_PATH=/usr/local/bin/python3 --action_env PYTHON_LIB_PATH=/usr/local/lib/python3.8/dist-packages --python_path=/usr/local/bin/python3. #16 1489.8 (21:51:01) INFO: Reading rc options for 'build' from /opt/deepvariant/.bazelrc:. #16 1489.8 'build' options: --jobs 128 --config=monolithic --show_timestamps --verbose_failures --define=use_fast_cpp_protos=true --copt=-Wno-maybe-uninitialized --copt=-Wno-unused-function --cxxopt=-std=c++17 --python_top=//:deepva",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/608
https://github.com/google/deepvariant/issues/608:2550,performance,gpu,gpu,2550,"andalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true --deleted_packages=tensorflow/compiler/mlir/tfrt,tensorflow/compiler/mlir/tfrt/benchmarks,tensorflow/compiler/mlir/tfrt/jit/python_binding,tensorflow/compiler/mlir/tfrt/jit/transforms,tensorflow/compiler/mlir/tfrt/python_tests,tensorflow/compiler/mlir/tfrt/tests,tensorflow/compiler/mlir/tfrt/tests/saved_model,tensorflow/compiler/mlir/tfrt/transforms/lhlo_gpu_to_tfrt_gpu,tensorflow/core/runtime_fallback,tensorflow/core/runtime_fallback/conversion,tensorflow/core/runtime_fallback/kernel,tensorflow/core/runtime_fallback/opdefs,tensorflow/core/runtime_fallback/runtime,tensorflow/core/runtime_fallback/util,tensorflow/core/tfrt/common,tensorflow/core/tfrt/eager,tensorflow/core/tfrt/eager/backends/cpu,tensorflow/core/tfrt/eager/backends/gpu,tensorflow/core/tfrt/eager/core_runtime,tensorflow/core/tfrt/eager/cpp_tests/core_runtime,tensorflow/core/tfrt/fallback,tensorflow/core/tfrt/gpu,tensorflow/core/tfrt/run_handler_thread_pool,tensorflow/core/tfrt/runtime,tensorflow/core/tfrt/saved_model,tensorflow/core/tfrt/saved_model/tests,tensorflow/core/tfrt/tpu,tensorflow/core/tfrt/utils. #16 1489.8 (21:51:01) INFO: Reading rc options for 'build' from /opt/tensorflow/.tf_configure.bazelrc:. #16 1489.8 'build' options: --action_env PYTHON_BIN_PATH=/usr/local/bin/python3 --action_env PYTHON_LIB_PATH=/usr/local/lib/python3.8/dist-packages --python_path=/usr/local/bin/python3. #16 1489.8 (21:51:01) INFO: Reading rc options for 'build' from /opt/deepvariant/.bazelrc:. #16 1489.8 'build' options: --jobs 128 --config=monolithic --show_timestamps --verbose_failures --define=use_fast_cpp_protos=true --copt=-Wno-maybe-uninitialized --copt=-Wno-unused-function --cxxopt=-std=c++17 --python_top=//:deepvariant_python_runtime --incompatible_use_",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/608
https://github.com/google/deepvariant/issues/608:2695,performance,gpu,gpu,2695,"with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true --deleted_packages=tensorflow/compiler/mlir/tfrt,tensorflow/compiler/mlir/tfrt/benchmarks,tensorflow/compiler/mlir/tfrt/jit/python_binding,tensorflow/compiler/mlir/tfrt/jit/transforms,tensorflow/compiler/mlir/tfrt/python_tests,tensorflow/compiler/mlir/tfrt/tests,tensorflow/compiler/mlir/tfrt/tests/saved_model,tensorflow/compiler/mlir/tfrt/transforms/lhlo_gpu_to_tfrt_gpu,tensorflow/core/runtime_fallback,tensorflow/core/runtime_fallback/conversion,tensorflow/core/runtime_fallback/kernel,tensorflow/core/runtime_fallback/opdefs,tensorflow/core/runtime_fallback/runtime,tensorflow/core/runtime_fallback/util,tensorflow/core/tfrt/common,tensorflow/core/tfrt/eager,tensorflow/core/tfrt/eager/backends/cpu,tensorflow/core/tfrt/eager/backends/gpu,tensorflow/core/tfrt/eager/core_runtime,tensorflow/core/tfrt/eager/cpp_tests/core_runtime,tensorflow/core/tfrt/fallback,tensorflow/core/tfrt/gpu,tensorflow/core/tfrt/run_handler_thread_pool,tensorflow/core/tfrt/runtime,tensorflow/core/tfrt/saved_model,tensorflow/core/tfrt/saved_model/tests,tensorflow/core/tfrt/tpu,tensorflow/core/tfrt/utils. #16 1489.8 (21:51:01) INFO: Reading rc options for 'build' from /opt/tensorflow/.tf_configure.bazelrc:. #16 1489.8 'build' options: --action_env PYTHON_BIN_PATH=/usr/local/bin/python3 --action_env PYTHON_LIB_PATH=/usr/local/lib/python3.8/dist-packages --python_path=/usr/local/bin/python3. #16 1489.8 (21:51:01) INFO: Reading rc options for 'build' from /opt/deepvariant/.bazelrc:. #16 1489.8 'build' options: --jobs 128 --config=monolithic --show_timestamps --verbose_failures --define=use_fast_cpp_protos=true --copt=-Wno-maybe-uninitialized --copt=-Wno-unused-function --cxxopt=-std=c++17 --python_top=//:deepvariant_python_runtime --incompatible_use_python_toolchains=false. #16 1489.8 (21:51:01) INFO: Found applicable config definition build:short_logs in file /opt/tensorflow/.bazelrc: --outp",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/608
https://github.com/google/deepvariant/issues/608:4885,performance,Load,Loading,4885,#16 1489.8 (21:51:01) INFO: Found applicable config definition build:monolithic in file /opt/tensorflow/.bazelrc: --define framework_shared_object=false. #16 1489.8 (21:51:01) INFO: Found applicable config definition build:linux in file /opt/tensorflow/.bazelrc: --copt=-w --host_copt=-w --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++14 --host_cxxopt=-std=c++14 --config=dynamic_kernels --distinct_host_configuration=false --experimental_guard_against_concurrent_changes. #16 1489.8 (21:51:01) INFO: Found applicable config definition build:dynamic_kernels in file /opt/tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS. #16 1489.8 (21:51:01) INFO: Found applicable config definition build:linux in file /opt/deepvariant/.bazelrc: --distinct_host_configuration=true. #16 1490.1 (21:51:01) INFO: Current date is 2023-01-30. #16 1490.1 (21:51:01) Loading:. #16 1490.1 (21:51:01) Loading: 0 packages loaded. #16 1491.1 (21:51:02) Loading: 0 packages loaded. #16 1492.2 (21:51:03) Loading: 0 packages loaded. #16 1493.2 (21:51:04) Loading: 0 packages loaded. #16 1494.2 (21:51:05) Loading: 0 packages loaded. #16 1495.2 (21:51:06) Loading: 0 packages loaded. #16 1496.2 (21:51:07) Loading: 0 packages loaded. #16 1497.0 (21:51:08) INFO: Repository tf_runtime instantiated at:. #16 1497.0 /opt/deepvariant/WORKSPACE:102:14: in <toplevel>. #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/tensorflow/workspace3.bzl:28:15: in workspace. #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/tf_runtime/workspace.bzl:12:20: in repo. #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:113:21: in tf_http_archive. #16 1497.0 Repository rule _tf_http_archive defined at:. #16 1497.0 /,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/608
https://github.com/google/deepvariant/issues/608:4917,performance,Load,Loading,4917,d applicable config definition build:monolithic in file /opt/tensorflow/.bazelrc: --define framework_shared_object=false. #16 1489.8 (21:51:01) INFO: Found applicable config definition build:linux in file /opt/tensorflow/.bazelrc: --copt=-w --host_copt=-w --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++14 --host_cxxopt=-std=c++14 --config=dynamic_kernels --distinct_host_configuration=false --experimental_guard_against_concurrent_changes. #16 1489.8 (21:51:01) INFO: Found applicable config definition build:dynamic_kernels in file /opt/tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS. #16 1489.8 (21:51:01) INFO: Found applicable config definition build:linux in file /opt/deepvariant/.bazelrc: --distinct_host_configuration=true. #16 1490.1 (21:51:01) INFO: Current date is 2023-01-30. #16 1490.1 (21:51:01) Loading:. #16 1490.1 (21:51:01) Loading: 0 packages loaded. #16 1491.1 (21:51:02) Loading: 0 packages loaded. #16 1492.2 (21:51:03) Loading: 0 packages loaded. #16 1493.2 (21:51:04) Loading: 0 packages loaded. #16 1494.2 (21:51:05) Loading: 0 packages loaded. #16 1495.2 (21:51:06) Loading: 0 packages loaded. #16 1496.2 (21:51:07) Loading: 0 packages loaded. #16 1497.0 (21:51:08) INFO: Repository tf_runtime instantiated at:. #16 1497.0 /opt/deepvariant/WORKSPACE:102:14: in <toplevel>. #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/tensorflow/workspace3.bzl:28:15: in workspace. #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/tf_runtime/workspace.bzl:12:20: in repo. #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:113:21: in tf_http_archive. #16 1497.0 Repository rule _tf_http_archive defined at:. #16 1497.0 /root/.cache/bazel/_bazel_root/61,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/608
https://github.com/google/deepvariant/issues/608:4937,performance,load,loaded,4937, definition build:monolithic in file /opt/tensorflow/.bazelrc: --define framework_shared_object=false. #16 1489.8 (21:51:01) INFO: Found applicable config definition build:linux in file /opt/tensorflow/.bazelrc: --copt=-w --host_copt=-w --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++14 --host_cxxopt=-std=c++14 --config=dynamic_kernels --distinct_host_configuration=false --experimental_guard_against_concurrent_changes. #16 1489.8 (21:51:01) INFO: Found applicable config definition build:dynamic_kernels in file /opt/tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS. #16 1489.8 (21:51:01) INFO: Found applicable config definition build:linux in file /opt/deepvariant/.bazelrc: --distinct_host_configuration=true. #16 1490.1 (21:51:01) INFO: Current date is 2023-01-30. #16 1490.1 (21:51:01) Loading:. #16 1490.1 (21:51:01) Loading: 0 packages loaded. #16 1491.1 (21:51:02) Loading: 0 packages loaded. #16 1492.2 (21:51:03) Loading: 0 packages loaded. #16 1493.2 (21:51:04) Loading: 0 packages loaded. #16 1494.2 (21:51:05) Loading: 0 packages loaded. #16 1495.2 (21:51:06) Loading: 0 packages loaded. #16 1496.2 (21:51:07) Loading: 0 packages loaded. #16 1497.0 (21:51:08) INFO: Repository tf_runtime instantiated at:. #16 1497.0 /opt/deepvariant/WORKSPACE:102:14: in <toplevel>. #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/tensorflow/workspace3.bzl:28:15: in workspace. #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/tf_runtime/workspace.bzl:12:20: in repo. #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:113:21: in tf_http_archive. #16 1497.0 Repository rule _tf_http_archive defined at:. #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/608
https://github.com/google/deepvariant/issues/608:4967,performance,Load,Loading,4967, file /opt/tensorflow/.bazelrc: --define framework_shared_object=false. #16 1489.8 (21:51:01) INFO: Found applicable config definition build:linux in file /opt/tensorflow/.bazelrc: --copt=-w --host_copt=-w --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++14 --host_cxxopt=-std=c++14 --config=dynamic_kernels --distinct_host_configuration=false --experimental_guard_against_concurrent_changes. #16 1489.8 (21:51:01) INFO: Found applicable config definition build:dynamic_kernels in file /opt/tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS. #16 1489.8 (21:51:01) INFO: Found applicable config definition build:linux in file /opt/deepvariant/.bazelrc: --distinct_host_configuration=true. #16 1490.1 (21:51:01) INFO: Current date is 2023-01-30. #16 1490.1 (21:51:01) Loading:. #16 1490.1 (21:51:01) Loading: 0 packages loaded. #16 1491.1 (21:51:02) Loading: 0 packages loaded. #16 1492.2 (21:51:03) Loading: 0 packages loaded. #16 1493.2 (21:51:04) Loading: 0 packages loaded. #16 1494.2 (21:51:05) Loading: 0 packages loaded. #16 1495.2 (21:51:06) Loading: 0 packages loaded. #16 1496.2 (21:51:07) Loading: 0 packages loaded. #16 1497.0 (21:51:08) INFO: Repository tf_runtime instantiated at:. #16 1497.0 /opt/deepvariant/WORKSPACE:102:14: in <toplevel>. #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/tensorflow/workspace3.bzl:28:15: in workspace. #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/tf_runtime/workspace.bzl:12:20: in repo. #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:113:21: in tf_http_archive. #16 1497.0 Repository rule _tf_http_archive defined at:. #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensor,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/608
https://github.com/google/deepvariant/issues/608:4987,performance,load,loaded,4987,ow/.bazelrc: --define framework_shared_object=false. #16 1489.8 (21:51:01) INFO: Found applicable config definition build:linux in file /opt/tensorflow/.bazelrc: --copt=-w --host_copt=-w --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++14 --host_cxxopt=-std=c++14 --config=dynamic_kernels --distinct_host_configuration=false --experimental_guard_against_concurrent_changes. #16 1489.8 (21:51:01) INFO: Found applicable config definition build:dynamic_kernels in file /opt/tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS. #16 1489.8 (21:51:01) INFO: Found applicable config definition build:linux in file /opt/deepvariant/.bazelrc: --distinct_host_configuration=true. #16 1490.1 (21:51:01) INFO: Current date is 2023-01-30. #16 1490.1 (21:51:01) Loading:. #16 1490.1 (21:51:01) Loading: 0 packages loaded. #16 1491.1 (21:51:02) Loading: 0 packages loaded. #16 1492.2 (21:51:03) Loading: 0 packages loaded. #16 1493.2 (21:51:04) Loading: 0 packages loaded. #16 1494.2 (21:51:05) Loading: 0 packages loaded. #16 1495.2 (21:51:06) Loading: 0 packages loaded. #16 1496.2 (21:51:07) Loading: 0 packages loaded. #16 1497.0 (21:51:08) INFO: Repository tf_runtime instantiated at:. #16 1497.0 /opt/deepvariant/WORKSPACE:102:14: in <toplevel>. #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/tensorflow/workspace3.bzl:28:15: in workspace. #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/tf_runtime/workspace.bzl:12:20: in repo. #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:113:21: in tf_http_archive. #16 1497.0 Repository rule _tf_http_archive defined at:. #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/re,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/608
https://github.com/google/deepvariant/issues/608:5017,performance,Load,Loading,5017,_shared_object=false. #16 1489.8 (21:51:01) INFO: Found applicable config definition build:linux in file /opt/tensorflow/.bazelrc: --copt=-w --host_copt=-w --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++14 --host_cxxopt=-std=c++14 --config=dynamic_kernels --distinct_host_configuration=false --experimental_guard_against_concurrent_changes. #16 1489.8 (21:51:01) INFO: Found applicable config definition build:dynamic_kernels in file /opt/tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS. #16 1489.8 (21:51:01) INFO: Found applicable config definition build:linux in file /opt/deepvariant/.bazelrc: --distinct_host_configuration=true. #16 1490.1 (21:51:01) INFO: Current date is 2023-01-30. #16 1490.1 (21:51:01) Loading:. #16 1490.1 (21:51:01) Loading: 0 packages loaded. #16 1491.1 (21:51:02) Loading: 0 packages loaded. #16 1492.2 (21:51:03) Loading: 0 packages loaded. #16 1493.2 (21:51:04) Loading: 0 packages loaded. #16 1494.2 (21:51:05) Loading: 0 packages loaded. #16 1495.2 (21:51:06) Loading: 0 packages loaded. #16 1496.2 (21:51:07) Loading: 0 packages loaded. #16 1497.0 (21:51:08) INFO: Repository tf_runtime instantiated at:. #16 1497.0 /opt/deepvariant/WORKSPACE:102:14: in <toplevel>. #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/tensorflow/workspace3.bzl:28:15: in workspace. #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/tf_runtime/workspace.bzl:12:20: in repo. #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:113:21: in tf_http_archive. #16 1497.0 Repository rule _tf_http_archive defined at:. #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:66:35: in <toplevel>. #1,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/608
https://github.com/google/deepvariant/issues/608:5037,performance,load,loaded,5037,e. #16 1489.8 (21:51:01) INFO: Found applicable config definition build:linux in file /opt/tensorflow/.bazelrc: --copt=-w --host_copt=-w --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++14 --host_cxxopt=-std=c++14 --config=dynamic_kernels --distinct_host_configuration=false --experimental_guard_against_concurrent_changes. #16 1489.8 (21:51:01) INFO: Found applicable config definition build:dynamic_kernels in file /opt/tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS. #16 1489.8 (21:51:01) INFO: Found applicable config definition build:linux in file /opt/deepvariant/.bazelrc: --distinct_host_configuration=true. #16 1490.1 (21:51:01) INFO: Current date is 2023-01-30. #16 1490.1 (21:51:01) Loading:. #16 1490.1 (21:51:01) Loading: 0 packages loaded. #16 1491.1 (21:51:02) Loading: 0 packages loaded. #16 1492.2 (21:51:03) Loading: 0 packages loaded. #16 1493.2 (21:51:04) Loading: 0 packages loaded. #16 1494.2 (21:51:05) Loading: 0 packages loaded. #16 1495.2 (21:51:06) Loading: 0 packages loaded. #16 1496.2 (21:51:07) Loading: 0 packages loaded. #16 1497.0 (21:51:08) INFO: Repository tf_runtime instantiated at:. #16 1497.0 /opt/deepvariant/WORKSPACE:102:14: in <toplevel>. #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/tensorflow/workspace3.bzl:28:15: in workspace. #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/tf_runtime/workspace.bzl:12:20: in repo. #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:113:21: in tf_http_archive. #16 1497.0 Repository rule _tf_http_archive defined at:. #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:66:35: in <toplevel>. #16 1497.0 (21:51:08),MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/608
https://github.com/google/deepvariant/issues/608:5067,performance,Load,Loading,5067,Found applicable config definition build:linux in file /opt/tensorflow/.bazelrc: --copt=-w --host_copt=-w --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++14 --host_cxxopt=-std=c++14 --config=dynamic_kernels --distinct_host_configuration=false --experimental_guard_against_concurrent_changes. #16 1489.8 (21:51:01) INFO: Found applicable config definition build:dynamic_kernels in file /opt/tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS. #16 1489.8 (21:51:01) INFO: Found applicable config definition build:linux in file /opt/deepvariant/.bazelrc: --distinct_host_configuration=true. #16 1490.1 (21:51:01) INFO: Current date is 2023-01-30. #16 1490.1 (21:51:01) Loading:. #16 1490.1 (21:51:01) Loading: 0 packages loaded. #16 1491.1 (21:51:02) Loading: 0 packages loaded. #16 1492.2 (21:51:03) Loading: 0 packages loaded. #16 1493.2 (21:51:04) Loading: 0 packages loaded. #16 1494.2 (21:51:05) Loading: 0 packages loaded. #16 1495.2 (21:51:06) Loading: 0 packages loaded. #16 1496.2 (21:51:07) Loading: 0 packages loaded. #16 1497.0 (21:51:08) INFO: Repository tf_runtime instantiated at:. #16 1497.0 /opt/deepvariant/WORKSPACE:102:14: in <toplevel>. #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/tensorflow/workspace3.bzl:28:15: in workspace. #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/tf_runtime/workspace.bzl:12:20: in repo. #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:113:21: in tf_http_archive. #16 1497.0 Repository rule _tf_http_archive defined at:. #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:66:35: in <toplevel>. #16 1497.0 (21:51:08) WARNING: Download from http://,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/608
https://github.com/google/deepvariant/issues/608:5087,performance,load,loaded,5087,nfig definition build:linux in file /opt/tensorflow/.bazelrc: --copt=-w --host_copt=-w --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++14 --host_cxxopt=-std=c++14 --config=dynamic_kernels --distinct_host_configuration=false --experimental_guard_against_concurrent_changes. #16 1489.8 (21:51:01) INFO: Found applicable config definition build:dynamic_kernels in file /opt/tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS. #16 1489.8 (21:51:01) INFO: Found applicable config definition build:linux in file /opt/deepvariant/.bazelrc: --distinct_host_configuration=true. #16 1490.1 (21:51:01) INFO: Current date is 2023-01-30. #16 1490.1 (21:51:01) Loading:. #16 1490.1 (21:51:01) Loading: 0 packages loaded. #16 1491.1 (21:51:02) Loading: 0 packages loaded. #16 1492.2 (21:51:03) Loading: 0 packages loaded. #16 1493.2 (21:51:04) Loading: 0 packages loaded. #16 1494.2 (21:51:05) Loading: 0 packages loaded. #16 1495.2 (21:51:06) Loading: 0 packages loaded. #16 1496.2 (21:51:07) Loading: 0 packages loaded. #16 1497.0 (21:51:08) INFO: Repository tf_runtime instantiated at:. #16 1497.0 /opt/deepvariant/WORKSPACE:102:14: in <toplevel>. #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/tensorflow/workspace3.bzl:28:15: in workspace. #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/tf_runtime/workspace.bzl:12:20: in repo. #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:113:21: in tf_http_archive. #16 1497.0 Repository rule _tf_http_archive defined at:. #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:66:35: in <toplevel>. #16 1497.0 (21:51:08) WARNING: Download from http://mirror.tensorflow.o,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/608
https://github.com/google/deepvariant/issues/608:5117,performance,Load,Loading,5117,file /opt/tensorflow/.bazelrc: --copt=-w --host_copt=-w --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++14 --host_cxxopt=-std=c++14 --config=dynamic_kernels --distinct_host_configuration=false --experimental_guard_against_concurrent_changes. #16 1489.8 (21:51:01) INFO: Found applicable config definition build:dynamic_kernels in file /opt/tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS. #16 1489.8 (21:51:01) INFO: Found applicable config definition build:linux in file /opt/deepvariant/.bazelrc: --distinct_host_configuration=true. #16 1490.1 (21:51:01) INFO: Current date is 2023-01-30. #16 1490.1 (21:51:01) Loading:. #16 1490.1 (21:51:01) Loading: 0 packages loaded. #16 1491.1 (21:51:02) Loading: 0 packages loaded. #16 1492.2 (21:51:03) Loading: 0 packages loaded. #16 1493.2 (21:51:04) Loading: 0 packages loaded. #16 1494.2 (21:51:05) Loading: 0 packages loaded. #16 1495.2 (21:51:06) Loading: 0 packages loaded. #16 1496.2 (21:51:07) Loading: 0 packages loaded. #16 1497.0 (21:51:08) INFO: Repository tf_runtime instantiated at:. #16 1497.0 /opt/deepvariant/WORKSPACE:102:14: in <toplevel>. #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/tensorflow/workspace3.bzl:28:15: in workspace. #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/tf_runtime/workspace.bzl:12:20: in repo. #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:113:21: in tf_http_archive. #16 1497.0 Repository rule _tf_http_archive defined at:. #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:66:35: in <toplevel>. #16 1497.0 (21:51:08) WARNING: Download from http://mirror.tensorflow.org/github.com/tensorflow/runtim,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/608
https://github.com/google/deepvariant/issues/608:5137,performance,load,loaded,5137,w/.bazelrc: --copt=-w --host_copt=-w --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++14 --host_cxxopt=-std=c++14 --config=dynamic_kernels --distinct_host_configuration=false --experimental_guard_against_concurrent_changes. #16 1489.8 (21:51:01) INFO: Found applicable config definition build:dynamic_kernels in file /opt/tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS. #16 1489.8 (21:51:01) INFO: Found applicable config definition build:linux in file /opt/deepvariant/.bazelrc: --distinct_host_configuration=true. #16 1490.1 (21:51:01) INFO: Current date is 2023-01-30. #16 1490.1 (21:51:01) Loading:. #16 1490.1 (21:51:01) Loading: 0 packages loaded. #16 1491.1 (21:51:02) Loading: 0 packages loaded. #16 1492.2 (21:51:03) Loading: 0 packages loaded. #16 1493.2 (21:51:04) Loading: 0 packages loaded. #16 1494.2 (21:51:05) Loading: 0 packages loaded. #16 1495.2 (21:51:06) Loading: 0 packages loaded. #16 1496.2 (21:51:07) Loading: 0 packages loaded. #16 1497.0 (21:51:08) INFO: Repository tf_runtime instantiated at:. #16 1497.0 /opt/deepvariant/WORKSPACE:102:14: in <toplevel>. #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/tensorflow/workspace3.bzl:28:15: in workspace. #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/tf_runtime/workspace.bzl:12:20: in repo. #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:113:21: in tf_http_archive. #16 1497.0 Repository rule _tf_http_archive defined at:. #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:66:35: in <toplevel>. #16 1497.0 (21:51:08) WARNING: Download from http://mirror.tensorflow.org/github.com/tensorflow/runtime/archive/64c92c801,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/608
https://github.com/google/deepvariant/issues/608:5167,performance,Load,Loading,5167,pt=-w --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++14 --host_cxxopt=-std=c++14 --config=dynamic_kernels --distinct_host_configuration=false --experimental_guard_against_concurrent_changes. #16 1489.8 (21:51:01) INFO: Found applicable config definition build:dynamic_kernels in file /opt/tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS. #16 1489.8 (21:51:01) INFO: Found applicable config definition build:linux in file /opt/deepvariant/.bazelrc: --distinct_host_configuration=true. #16 1490.1 (21:51:01) INFO: Current date is 2023-01-30. #16 1490.1 (21:51:01) Loading:. #16 1490.1 (21:51:01) Loading: 0 packages loaded. #16 1491.1 (21:51:02) Loading: 0 packages loaded. #16 1492.2 (21:51:03) Loading: 0 packages loaded. #16 1493.2 (21:51:04) Loading: 0 packages loaded. #16 1494.2 (21:51:05) Loading: 0 packages loaded. #16 1495.2 (21:51:06) Loading: 0 packages loaded. #16 1496.2 (21:51:07) Loading: 0 packages loaded. #16 1497.0 (21:51:08) INFO: Repository tf_runtime instantiated at:. #16 1497.0 /opt/deepvariant/WORKSPACE:102:14: in <toplevel>. #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/tensorflow/workspace3.bzl:28:15: in workspace. #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/tf_runtime/workspace.bzl:12:20: in repo. #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:113:21: in tf_http_archive. #16 1497.0 Repository rule _tf_http_archive defined at:. #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:66:35: in <toplevel>. #16 1497.0 (21:51:08) WARNING: Download from http://mirror.tensorflow.org/github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/608
https://github.com/google/deepvariant/issues/608:5187,performance,load,loaded,5187,IX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++14 --host_cxxopt=-std=c++14 --config=dynamic_kernels --distinct_host_configuration=false --experimental_guard_against_concurrent_changes. #16 1489.8 (21:51:01) INFO: Found applicable config definition build:dynamic_kernels in file /opt/tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS. #16 1489.8 (21:51:01) INFO: Found applicable config definition build:linux in file /opt/deepvariant/.bazelrc: --distinct_host_configuration=true. #16 1490.1 (21:51:01) INFO: Current date is 2023-01-30. #16 1490.1 (21:51:01) Loading:. #16 1490.1 (21:51:01) Loading: 0 packages loaded. #16 1491.1 (21:51:02) Loading: 0 packages loaded. #16 1492.2 (21:51:03) Loading: 0 packages loaded. #16 1493.2 (21:51:04) Loading: 0 packages loaded. #16 1494.2 (21:51:05) Loading: 0 packages loaded. #16 1495.2 (21:51:06) Loading: 0 packages loaded. #16 1496.2 (21:51:07) Loading: 0 packages loaded. #16 1497.0 (21:51:08) INFO: Repository tf_runtime instantiated at:. #16 1497.0 /opt/deepvariant/WORKSPACE:102:14: in <toplevel>. #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/tensorflow/workspace3.bzl:28:15: in workspace. #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/tf_runtime/workspace.bzl:12:20: in repo. #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:113:21: in tf_http_archive. #16 1497.0 Repository rule _tf_http_archive defined at:. #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:66:35: in <toplevel>. #16 1497.0 (21:51:08) WARNING: Download from http://mirror.tensorflow.org/github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz failed: cla,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/608
https://github.com/google/deepvariant/issues/608:5217,performance,Load,Loading,5217,X)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++14 --host_cxxopt=-std=c++14 --config=dynamic_kernels --distinct_host_configuration=false --experimental_guard_against_concurrent_changes. #16 1489.8 (21:51:01) INFO: Found applicable config definition build:dynamic_kernels in file /opt/tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS. #16 1489.8 (21:51:01) INFO: Found applicable config definition build:linux in file /opt/deepvariant/.bazelrc: --distinct_host_configuration=true. #16 1490.1 (21:51:01) INFO: Current date is 2023-01-30. #16 1490.1 (21:51:01) Loading:. #16 1490.1 (21:51:01) Loading: 0 packages loaded. #16 1491.1 (21:51:02) Loading: 0 packages loaded. #16 1492.2 (21:51:03) Loading: 0 packages loaded. #16 1493.2 (21:51:04) Loading: 0 packages loaded. #16 1494.2 (21:51:05) Loading: 0 packages loaded. #16 1495.2 (21:51:06) Loading: 0 packages loaded. #16 1496.2 (21:51:07) Loading: 0 packages loaded. #16 1497.0 (21:51:08) INFO: Repository tf_runtime instantiated at:. #16 1497.0 /opt/deepvariant/WORKSPACE:102:14: in <toplevel>. #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/tensorflow/workspace3.bzl:28:15: in workspace. #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/tf_runtime/workspace.bzl:12:20: in repo. #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:113:21: in tf_http_archive. #16 1497.0 Repository rule _tf_http_archive defined at:. #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:66:35: in <toplevel>. #16 1497.0 (21:51:08) WARNING: Download from http://mirror.tensorflow.org/github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz failed: class com.google.devtools.build.li,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/608
https://github.com/google/deepvariant/issues/608:5237,performance,load,loaded,5237,LUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++14 --host_cxxopt=-std=c++14 --config=dynamic_kernels --distinct_host_configuration=false --experimental_guard_against_concurrent_changes. #16 1489.8 (21:51:01) INFO: Found applicable config definition build:dynamic_kernels in file /opt/tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS. #16 1489.8 (21:51:01) INFO: Found applicable config definition build:linux in file /opt/deepvariant/.bazelrc: --distinct_host_configuration=true. #16 1490.1 (21:51:01) INFO: Current date is 2023-01-30. #16 1490.1 (21:51:01) Loading:. #16 1490.1 (21:51:01) Loading: 0 packages loaded. #16 1491.1 (21:51:02) Loading: 0 packages loaded. #16 1492.2 (21:51:03) Loading: 0 packages loaded. #16 1493.2 (21:51:04) Loading: 0 packages loaded. #16 1494.2 (21:51:05) Loading: 0 packages loaded. #16 1495.2 (21:51:06) Loading: 0 packages loaded. #16 1496.2 (21:51:07) Loading: 0 packages loaded. #16 1497.0 (21:51:08) INFO: Repository tf_runtime instantiated at:. #16 1497.0 /opt/deepvariant/WORKSPACE:102:14: in <toplevel>. #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/tensorflow/workspace3.bzl:28:15: in workspace. #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/tf_runtime/workspace.bzl:12:20: in repo. #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:113:21: in tf_http_archive. #16 1497.0 Repository rule _tf_http_archive defined at:. #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:66:35: in <toplevel>. #16 1497.0 (21:51:08) WARNING: Download from http://mirror.tensorflow.org/github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz failed: class com.google.devtools.build.lib.bazel.repository.,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/608
https://github.com/google/deepvariant/issues/608:5392,performance,cach,cache,5392,_host_configuration=false --experimental_guard_against_concurrent_changes. #16 1489.8 (21:51:01) INFO: Found applicable config definition build:dynamic_kernels in file /opt/tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS. #16 1489.8 (21:51:01) INFO: Found applicable config definition build:linux in file /opt/deepvariant/.bazelrc: --distinct_host_configuration=true. #16 1490.1 (21:51:01) INFO: Current date is 2023-01-30. #16 1490.1 (21:51:01) Loading:. #16 1490.1 (21:51:01) Loading: 0 packages loaded. #16 1491.1 (21:51:02) Loading: 0 packages loaded. #16 1492.2 (21:51:03) Loading: 0 packages loaded. #16 1493.2 (21:51:04) Loading: 0 packages loaded. #16 1494.2 (21:51:05) Loading: 0 packages loaded. #16 1495.2 (21:51:06) Loading: 0 packages loaded. #16 1496.2 (21:51:07) Loading: 0 packages loaded. #16 1497.0 (21:51:08) INFO: Repository tf_runtime instantiated at:. #16 1497.0 /opt/deepvariant/WORKSPACE:102:14: in <toplevel>. #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/tensorflow/workspace3.bzl:28:15: in workspace. #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/tf_runtime/workspace.bzl:12:20: in repo. #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:113:21: in tf_http_archive. #16 1497.0 Repository rule _tf_http_archive defined at:. #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:66:35: in <toplevel>. #16 1497.0 (21:51:08) WARNING: Download from http://mirror.tensorflow.org/github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException GET returned 404 Not Found. #16 1497.0 (21:51:08) WARNING: Download from https://github.com/tensorflow/runtime/archiv,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/608
https://github.com/google/deepvariant/issues/608:5538,performance,cach,cache,5538,namic_kernels in file /opt/tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS. #16 1489.8 (21:51:01) INFO: Found applicable config definition build:linux in file /opt/deepvariant/.bazelrc: --distinct_host_configuration=true. #16 1490.1 (21:51:01) INFO: Current date is 2023-01-30. #16 1490.1 (21:51:01) Loading:. #16 1490.1 (21:51:01) Loading: 0 packages loaded. #16 1491.1 (21:51:02) Loading: 0 packages loaded. #16 1492.2 (21:51:03) Loading: 0 packages loaded. #16 1493.2 (21:51:04) Loading: 0 packages loaded. #16 1494.2 (21:51:05) Loading: 0 packages loaded. #16 1495.2 (21:51:06) Loading: 0 packages loaded. #16 1496.2 (21:51:07) Loading: 0 packages loaded. #16 1497.0 (21:51:08) INFO: Repository tf_runtime instantiated at:. #16 1497.0 /opt/deepvariant/WORKSPACE:102:14: in <toplevel>. #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/tensorflow/workspace3.bzl:28:15: in workspace. #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/tf_runtime/workspace.bzl:12:20: in repo. #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:113:21: in tf_http_archive. #16 1497.0 Repository rule _tf_http_archive defined at:. #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:66:35: in <toplevel>. #16 1497.0 (21:51:08) WARNING: Download from http://mirror.tensorflow.org/github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException GET returned 404 Not Found. #16 1497.0 (21:51:08) WARNING: Download from https://github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpExcepti,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/608
https://github.com/google/deepvariant/issues/608:5690,performance,cach,cache,5690,d applicable config definition build:linux in file /opt/deepvariant/.bazelrc: --distinct_host_configuration=true. #16 1490.1 (21:51:01) INFO: Current date is 2023-01-30. #16 1490.1 (21:51:01) Loading:. #16 1490.1 (21:51:01) Loading: 0 packages loaded. #16 1491.1 (21:51:02) Loading: 0 packages loaded. #16 1492.2 (21:51:03) Loading: 0 packages loaded. #16 1493.2 (21:51:04) Loading: 0 packages loaded. #16 1494.2 (21:51:05) Loading: 0 packages loaded. #16 1495.2 (21:51:06) Loading: 0 packages loaded. #16 1496.2 (21:51:07) Loading: 0 packages loaded. #16 1497.0 (21:51:08) INFO: Repository tf_runtime instantiated at:. #16 1497.0 /opt/deepvariant/WORKSPACE:102:14: in <toplevel>. #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/tensorflow/workspace3.bzl:28:15: in workspace. #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/tf_runtime/workspace.bzl:12:20: in repo. #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:113:21: in tf_http_archive. #16 1497.0 Repository rule _tf_http_archive defined at:. #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:66:35: in <toplevel>. #16 1497.0 (21:51:08) WARNING: Download from http://mirror.tensorflow.org/github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException GET returned 404 Not Found. #16 1497.0 (21:51:08) WARNING: Download from https://github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException Checksum was 8383b3247286016e450b0b20e805d26b88ab4638b4e4e3cc4a6923debaf7ad1e but wanted f16fcf09b34e0c7be9389f50652b4b4a14c5a8a96e7e15ad73e8f234d8d0,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/608
https://github.com/google/deepvariant/issues/608:5895,performance,cach,cache,5895," 1490.1 (21:51:01) Loading: 0 packages loaded. #16 1491.1 (21:51:02) Loading: 0 packages loaded. #16 1492.2 (21:51:03) Loading: 0 packages loaded. #16 1493.2 (21:51:04) Loading: 0 packages loaded. #16 1494.2 (21:51:05) Loading: 0 packages loaded. #16 1495.2 (21:51:06) Loading: 0 packages loaded. #16 1496.2 (21:51:07) Loading: 0 packages loaded. #16 1497.0 (21:51:08) INFO: Repository tf_runtime instantiated at:. #16 1497.0 /opt/deepvariant/WORKSPACE:102:14: in <toplevel>. #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/tensorflow/workspace3.bzl:28:15: in workspace. #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/tf_runtime/workspace.bzl:12:20: in repo. #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:113:21: in tf_http_archive. #16 1497.0 Repository rule _tf_http_archive defined at:. #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:66:35: in <toplevel>. #16 1497.0 (21:51:08) WARNING: Download from http://mirror.tensorflow.org/github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException GET returned 404 Not Found. #16 1497.0 (21:51:08) WARNING: Download from https://github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException Checksum was 8383b3247286016e450b0b20e805d26b88ab4638b4e4e3cc4a6923debaf7ad1e but wanted f16fcf09b34e0c7be9389f50652b4b4a14c5a8a96e7e15ad73e8f234d8d09ebe. #16 1497.0 (21:51:08) ERROR: An error occurred during the fetch of repository 'tf_runtime':. #16 1497.0 Traceback (most recent call last):. #16 1497.0 File ""/root/.cache/bazel/_bazel_root/617054f44dc",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/608
https://github.com/google/deepvariant/issues/608:6721,performance,ERROR,ERROR,6721,"c1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:113:21: in tf_http_archive. #16 1497.0 Repository rule _tf_http_archive defined at:. #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:66:35: in <toplevel>. #16 1497.0 (21:51:08) WARNING: Download from http://mirror.tensorflow.org/github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException GET returned 404 Not Found. #16 1497.0 (21:51:08) WARNING: Download from https://github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException Checksum was 8383b3247286016e450b0b20e805d26b88ab4638b4e4e3cc4a6923debaf7ad1e but wanted f16fcf09b34e0c7be9389f50652b4b4a14c5a8a96e7e15ad73e8f234d8d09ebe. #16 1497.0 (21:51:08) ERROR: An error occurred during the fetch of repository 'tf_runtime':. #16 1497.0 Traceback (most recent call last):. #16 1497.0 File ""/root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl"", line 53, column 33, in _tf_http_archive_impl. #16 1497.0 ctx.download_and_extract(. #16 1497.0 Error in download_and_extract: java.io.IOException: Error downloading [http://mirror.tensorflow.org/github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz, https://github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz] to /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/tf_runtime/temp12516918929418979294/64c92c8013b557087351c91b5423b6046d10f206.tar.gz: Checksum was 8383b3247286016e450b0b20e805d26b88ab4638b4e4e3cc4a6923debaf7ad1e but wanted f16fcf09b34e0c7be9389f50652b4b4a14c5a8a96e7e15ad73e8f234d8d09ebe. #16 1497.1 (21:51:08) INFO: Repository llvm-raw instantiated at:",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/608
https://github.com/google/deepvariant/issues/608:6731,performance,error,error,6731,"3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:113:21: in tf_http_archive. #16 1497.0 Repository rule _tf_http_archive defined at:. #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:66:35: in <toplevel>. #16 1497.0 (21:51:08) WARNING: Download from http://mirror.tensorflow.org/github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException GET returned 404 Not Found. #16 1497.0 (21:51:08) WARNING: Download from https://github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException Checksum was 8383b3247286016e450b0b20e805d26b88ab4638b4e4e3cc4a6923debaf7ad1e but wanted f16fcf09b34e0c7be9389f50652b4b4a14c5a8a96e7e15ad73e8f234d8d09ebe. #16 1497.0 (21:51:08) ERROR: An error occurred during the fetch of repository 'tf_runtime':. #16 1497.0 Traceback (most recent call last):. #16 1497.0 File ""/root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl"", line 53, column 33, in _tf_http_archive_impl. #16 1497.0 ctx.download_and_extract(. #16 1497.0 Error in download_and_extract: java.io.IOException: Error downloading [http://mirror.tensorflow.org/github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz, https://github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz] to /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/tf_runtime/temp12516918929418979294/64c92c8013b557087351c91b5423b6046d10f206.tar.gz: Checksum was 8383b3247286016e450b0b20e805d26b88ab4638b4e4e3cc4a6923debaf7ad1e but wanted f16fcf09b34e0c7be9389f50652b4b4a14c5a8a96e7e15ad73e8f234d8d09ebe. #16 1497.1 (21:51:08) INFO: Repository llvm-raw instantiated at:. #16 1497",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/608
https://github.com/google/deepvariant/issues/608:6863,performance,cach,cache,6863,"fined at:. #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:66:35: in <toplevel>. #16 1497.0 (21:51:08) WARNING: Download from http://mirror.tensorflow.org/github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException GET returned 404 Not Found. #16 1497.0 (21:51:08) WARNING: Download from https://github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException Checksum was 8383b3247286016e450b0b20e805d26b88ab4638b4e4e3cc4a6923debaf7ad1e but wanted f16fcf09b34e0c7be9389f50652b4b4a14c5a8a96e7e15ad73e8f234d8d09ebe. #16 1497.0 (21:51:08) ERROR: An error occurred during the fetch of repository 'tf_runtime':. #16 1497.0 Traceback (most recent call last):. #16 1497.0 File ""/root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl"", line 53, column 33, in _tf_http_archive_impl. #16 1497.0 ctx.download_and_extract(. #16 1497.0 Error in download_and_extract: java.io.IOException: Error downloading [http://mirror.tensorflow.org/github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz, https://github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz] to /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/tf_runtime/temp12516918929418979294/64c92c8013b557087351c91b5423b6046d10f206.tar.gz: Checksum was 8383b3247286016e450b0b20e805d26b88ab4638b4e4e3cc4a6923debaf7ad1e but wanted f16fcf09b34e0c7be9389f50652b4b4a14c5a8a96e7e15ad73e8f234d8d09ebe. #16 1497.1 (21:51:08) INFO: Repository llvm-raw instantiated at:. #16 1497.1 /opt/deepvariant/WORKSPACE:102:14: in <toplevel>. #16 1497.1 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/exte",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/608
https://github.com/google/deepvariant/issues/608:7062,performance,Error,Error,7062,"ttp://mirror.tensorflow.org/github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException GET returned 404 Not Found. #16 1497.0 (21:51:08) WARNING: Download from https://github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException Checksum was 8383b3247286016e450b0b20e805d26b88ab4638b4e4e3cc4a6923debaf7ad1e but wanted f16fcf09b34e0c7be9389f50652b4b4a14c5a8a96e7e15ad73e8f234d8d09ebe. #16 1497.0 (21:51:08) ERROR: An error occurred during the fetch of repository 'tf_runtime':. #16 1497.0 Traceback (most recent call last):. #16 1497.0 File ""/root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl"", line 53, column 33, in _tf_http_archive_impl. #16 1497.0 ctx.download_and_extract(. #16 1497.0 Error in download_and_extract: java.io.IOException: Error downloading [http://mirror.tensorflow.org/github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz, https://github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz] to /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/tf_runtime/temp12516918929418979294/64c92c8013b557087351c91b5423b6046d10f206.tar.gz: Checksum was 8383b3247286016e450b0b20e805d26b88ab4638b4e4e3cc4a6923debaf7ad1e but wanted f16fcf09b34e0c7be9389f50652b4b4a14c5a8a96e7e15ad73e8f234d8d09ebe. #16 1497.1 (21:51:08) INFO: Repository llvm-raw instantiated at:. #16 1497.1 /opt/deepvariant/WORKSPACE:102:14: in <toplevel>. #16 1497.1 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/tensorflow/workspace3.bzl:42:9: in workspace. #16 1497.1 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/llvm/workspace.bzl:10:",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/608
https://github.com/google/deepvariant/issues/608:7114,performance,Error,Error,7114,"ntime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException GET returned 404 Not Found. #16 1497.0 (21:51:08) WARNING: Download from https://github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException Checksum was 8383b3247286016e450b0b20e805d26b88ab4638b4e4e3cc4a6923debaf7ad1e but wanted f16fcf09b34e0c7be9389f50652b4b4a14c5a8a96e7e15ad73e8f234d8d09ebe. #16 1497.0 (21:51:08) ERROR: An error occurred during the fetch of repository 'tf_runtime':. #16 1497.0 Traceback (most recent call last):. #16 1497.0 File ""/root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl"", line 53, column 33, in _tf_http_archive_impl. #16 1497.0 ctx.download_and_extract(. #16 1497.0 Error in download_and_extract: java.io.IOException: Error downloading [http://mirror.tensorflow.org/github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz, https://github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz] to /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/tf_runtime/temp12516918929418979294/64c92c8013b557087351c91b5423b6046d10f206.tar.gz: Checksum was 8383b3247286016e450b0b20e805d26b88ab4638b4e4e3cc4a6923debaf7ad1e but wanted f16fcf09b34e0c7be9389f50652b4b4a14c5a8a96e7e15ad73e8f234d8d09ebe. #16 1497.1 (21:51:08) INFO: Repository llvm-raw instantiated at:. #16 1497.1 /opt/deepvariant/WORKSPACE:102:14: in <toplevel>. #16 1497.1 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/tensorflow/workspace3.bzl:42:9: in workspace. #16 1497.1 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/llvm/workspace.bzl:10:20: in repo. #16 1497.1 /root/.cache/bazel/_bazel_ro",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/608
https://github.com/google/deepvariant/issues/608:7354,performance,cach,cache,7354,"//github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException Checksum was 8383b3247286016e450b0b20e805d26b88ab4638b4e4e3cc4a6923debaf7ad1e but wanted f16fcf09b34e0c7be9389f50652b4b4a14c5a8a96e7e15ad73e8f234d8d09ebe. #16 1497.0 (21:51:08) ERROR: An error occurred during the fetch of repository 'tf_runtime':. #16 1497.0 Traceback (most recent call last):. #16 1497.0 File ""/root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl"", line 53, column 33, in _tf_http_archive_impl. #16 1497.0 ctx.download_and_extract(. #16 1497.0 Error in download_and_extract: java.io.IOException: Error downloading [http://mirror.tensorflow.org/github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz, https://github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz] to /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/tf_runtime/temp12516918929418979294/64c92c8013b557087351c91b5423b6046d10f206.tar.gz: Checksum was 8383b3247286016e450b0b20e805d26b88ab4638b4e4e3cc4a6923debaf7ad1e but wanted f16fcf09b34e0c7be9389f50652b4b4a14c5a8a96e7e15ad73e8f234d8d09ebe. #16 1497.1 (21:51:08) INFO: Repository llvm-raw instantiated at:. #16 1497.1 /opt/deepvariant/WORKSPACE:102:14: in <toplevel>. #16 1497.1 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/tensorflow/workspace3.bzl:42:9: in workspace. #16 1497.1 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/llvm/workspace.bzl:10:20: in repo. #16 1497.1 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:113:21: in tf_http_archive. #16 1497.1 Repository rule _tf_http_archive defined at:. #16 1497.1 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/608
https://github.com/google/deepvariant/issues/608:7805,performance,cach,cache,7805,"back (most recent call last):. #16 1497.0 File ""/root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl"", line 53, column 33, in _tf_http_archive_impl. #16 1497.0 ctx.download_and_extract(. #16 1497.0 Error in download_and_extract: java.io.IOException: Error downloading [http://mirror.tensorflow.org/github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz, https://github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz] to /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/tf_runtime/temp12516918929418979294/64c92c8013b557087351c91b5423b6046d10f206.tar.gz: Checksum was 8383b3247286016e450b0b20e805d26b88ab4638b4e4e3cc4a6923debaf7ad1e but wanted f16fcf09b34e0c7be9389f50652b4b4a14c5a8a96e7e15ad73e8f234d8d09ebe. #16 1497.1 (21:51:08) INFO: Repository llvm-raw instantiated at:. #16 1497.1 /opt/deepvariant/WORKSPACE:102:14: in <toplevel>. #16 1497.1 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/tensorflow/workspace3.bzl:42:9: in workspace. #16 1497.1 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/llvm/workspace.bzl:10:20: in repo. #16 1497.1 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:113:21: in tf_http_archive. #16 1497.1 Repository rule _tf_http_archive defined at:. #16 1497.1 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:66:35: in <toplevel>. #16 1497.2 (21:51:08) Loading: 0 packages loaded. #16 1497.3 (21:51:08) ERROR: no such package '@tf_runtime//': java.io.IOException: Error downloading [http://mirror.tensorflow.org/github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz, https://github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz] to /root/.cache/baze",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/608
https://github.com/google/deepvariant/issues/608:7950,performance,cach,cache,7950,"ty/repo.bzl"", line 53, column 33, in _tf_http_archive_impl. #16 1497.0 ctx.download_and_extract(. #16 1497.0 Error in download_and_extract: java.io.IOException: Error downloading [http://mirror.tensorflow.org/github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz, https://github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz] to /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/tf_runtime/temp12516918929418979294/64c92c8013b557087351c91b5423b6046d10f206.tar.gz: Checksum was 8383b3247286016e450b0b20e805d26b88ab4638b4e4e3cc4a6923debaf7ad1e but wanted f16fcf09b34e0c7be9389f50652b4b4a14c5a8a96e7e15ad73e8f234d8d09ebe. #16 1497.1 (21:51:08) INFO: Repository llvm-raw instantiated at:. #16 1497.1 /opt/deepvariant/WORKSPACE:102:14: in <toplevel>. #16 1497.1 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/tensorflow/workspace3.bzl:42:9: in workspace. #16 1497.1 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/llvm/workspace.bzl:10:20: in repo. #16 1497.1 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:113:21: in tf_http_archive. #16 1497.1 Repository rule _tf_http_archive defined at:. #16 1497.1 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:66:35: in <toplevel>. #16 1497.2 (21:51:08) Loading: 0 packages loaded. #16 1497.3 (21:51:08) ERROR: no such package '@tf_runtime//': java.io.IOException: Error downloading [http://mirror.tensorflow.org/github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz, https://github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz] to /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/tf_runtime/temp12516918929418979294/64c92c8013b557087351c91b5423b6046d10f206.tar.gz: Chec",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/608
https://github.com/google/deepvariant/issues/608:8096,performance,cach,cache,8096,"o.IOException: Error downloading [http://mirror.tensorflow.org/github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz, https://github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz] to /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/tf_runtime/temp12516918929418979294/64c92c8013b557087351c91b5423b6046d10f206.tar.gz: Checksum was 8383b3247286016e450b0b20e805d26b88ab4638b4e4e3cc4a6923debaf7ad1e but wanted f16fcf09b34e0c7be9389f50652b4b4a14c5a8a96e7e15ad73e8f234d8d09ebe. #16 1497.1 (21:51:08) INFO: Repository llvm-raw instantiated at:. #16 1497.1 /opt/deepvariant/WORKSPACE:102:14: in <toplevel>. #16 1497.1 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/tensorflow/workspace3.bzl:42:9: in workspace. #16 1497.1 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/llvm/workspace.bzl:10:20: in repo. #16 1497.1 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:113:21: in tf_http_archive. #16 1497.1 Repository rule _tf_http_archive defined at:. #16 1497.1 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:66:35: in <toplevel>. #16 1497.2 (21:51:08) Loading: 0 packages loaded. #16 1497.3 (21:51:08) ERROR: no such package '@tf_runtime//': java.io.IOException: Error downloading [http://mirror.tensorflow.org/github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz, https://github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz] to /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/tf_runtime/temp12516918929418979294/64c92c8013b557087351c91b5423b6046d10f206.tar.gz: Checksum was 8383b3247286016e450b0b20e805d26b88ab4638b4e4e3cc4a6923debaf7ad1e but wanted f16fcf09b34e0c7be9389f50652b4b4a14c5a8a96e7e15ad73e8f234d8d09",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/608
https://github.com/google/deepvariant/issues/608:8301,performance,cach,cache,8301,"3b557087351c91b5423b6046d10f206.tar.gz] to /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/tf_runtime/temp12516918929418979294/64c92c8013b557087351c91b5423b6046d10f206.tar.gz: Checksum was 8383b3247286016e450b0b20e805d26b88ab4638b4e4e3cc4a6923debaf7ad1e but wanted f16fcf09b34e0c7be9389f50652b4b4a14c5a8a96e7e15ad73e8f234d8d09ebe. #16 1497.1 (21:51:08) INFO: Repository llvm-raw instantiated at:. #16 1497.1 /opt/deepvariant/WORKSPACE:102:14: in <toplevel>. #16 1497.1 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/tensorflow/workspace3.bzl:42:9: in workspace. #16 1497.1 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/llvm/workspace.bzl:10:20: in repo. #16 1497.1 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:113:21: in tf_http_archive. #16 1497.1 Repository rule _tf_http_archive defined at:. #16 1497.1 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:66:35: in <toplevel>. #16 1497.2 (21:51:08) Loading: 0 packages loaded. #16 1497.3 (21:51:08) ERROR: no such package '@tf_runtime//': java.io.IOException: Error downloading [http://mirror.tensorflow.org/github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz, https://github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz] to /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/tf_runtime/temp12516918929418979294/64c92c8013b557087351c91b5423b6046d10f206.tar.gz: Checksum was 8383b3247286016e450b0b20e805d26b88ab4638b4e4e3cc4a6923debaf7ad1e but wanted f16fcf09b34e0c7be9389f50652b4b4a14c5a8a96e7e15ad73e8f234d8d09ebe. #16 1497.3 (21:51:09) INFO: Elapsed time: 12.546s. #16 1497.3 (21:51:09) INFO: 0 processes. #16 1497.3 (21:51:09) FAILED: Build did NOT complete successfully (0 packages loaded). #16 1497.3 (21:51:09)",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/608
https://github.com/google/deepvariant/issues/608:8447,performance,Load,Loading,8447,"9294/64c92c8013b557087351c91b5423b6046d10f206.tar.gz: Checksum was 8383b3247286016e450b0b20e805d26b88ab4638b4e4e3cc4a6923debaf7ad1e but wanted f16fcf09b34e0c7be9389f50652b4b4a14c5a8a96e7e15ad73e8f234d8d09ebe. #16 1497.1 (21:51:08) INFO: Repository llvm-raw instantiated at:. #16 1497.1 /opt/deepvariant/WORKSPACE:102:14: in <toplevel>. #16 1497.1 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/tensorflow/workspace3.bzl:42:9: in workspace. #16 1497.1 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/llvm/workspace.bzl:10:20: in repo. #16 1497.1 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:113:21: in tf_http_archive. #16 1497.1 Repository rule _tf_http_archive defined at:. #16 1497.1 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:66:35: in <toplevel>. #16 1497.2 (21:51:08) Loading: 0 packages loaded. #16 1497.3 (21:51:08) ERROR: no such package '@tf_runtime//': java.io.IOException: Error downloading [http://mirror.tensorflow.org/github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz, https://github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz] to /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/tf_runtime/temp12516918929418979294/64c92c8013b557087351c91b5423b6046d10f206.tar.gz: Checksum was 8383b3247286016e450b0b20e805d26b88ab4638b4e4e3cc4a6923debaf7ad1e but wanted f16fcf09b34e0c7be9389f50652b4b4a14c5a8a96e7e15ad73e8f234d8d09ebe. #16 1497.3 (21:51:09) INFO: Elapsed time: 12.546s. #16 1497.3 (21:51:09) INFO: 0 processes. #16 1497.3 (21:51:09) FAILED: Build did NOT complete successfully (0 packages loaded). #16 1497.3 (21:51:09) FAILED: Build did NOT complete successfully (0 packages loaded). #16 ERROR: executor failed running [/bin/sh -c ./build-prereq.sh && PATH=""${HOME}",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/608
https://github.com/google/deepvariant/issues/608:8467,performance,load,loaded,8467,"087351c91b5423b6046d10f206.tar.gz: Checksum was 8383b3247286016e450b0b20e805d26b88ab4638b4e4e3cc4a6923debaf7ad1e but wanted f16fcf09b34e0c7be9389f50652b4b4a14c5a8a96e7e15ad73e8f234d8d09ebe. #16 1497.1 (21:51:08) INFO: Repository llvm-raw instantiated at:. #16 1497.1 /opt/deepvariant/WORKSPACE:102:14: in <toplevel>. #16 1497.1 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/tensorflow/workspace3.bzl:42:9: in workspace. #16 1497.1 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/llvm/workspace.bzl:10:20: in repo. #16 1497.1 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:113:21: in tf_http_archive. #16 1497.1 Repository rule _tf_http_archive defined at:. #16 1497.1 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:66:35: in <toplevel>. #16 1497.2 (21:51:08) Loading: 0 packages loaded. #16 1497.3 (21:51:08) ERROR: no such package '@tf_runtime//': java.io.IOException: Error downloading [http://mirror.tensorflow.org/github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz, https://github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz] to /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/tf_runtime/temp12516918929418979294/64c92c8013b557087351c91b5423b6046d10f206.tar.gz: Checksum was 8383b3247286016e450b0b20e805d26b88ab4638b4e4e3cc4a6923debaf7ad1e but wanted f16fcf09b34e0c7be9389f50652b4b4a14c5a8a96e7e15ad73e8f234d8d09ebe. #16 1497.3 (21:51:09) INFO: Elapsed time: 12.546s. #16 1497.3 (21:51:09) INFO: 0 processes. #16 1497.3 (21:51:09) FAILED: Build did NOT complete successfully (0 packages loaded). #16 1497.3 (21:51:09) FAILED: Build did NOT complete successfully (0 packages loaded). #16 ERROR: executor failed running [/bin/sh -c ./build-prereq.sh && PATH=""${HOME}/bin:${PATH}"" ./bui",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/608
https://github.com/google/deepvariant/issues/608:8497,performance,ERROR,ERROR,8497,".gz: Checksum was 8383b3247286016e450b0b20e805d26b88ab4638b4e4e3cc4a6923debaf7ad1e but wanted f16fcf09b34e0c7be9389f50652b4b4a14c5a8a96e7e15ad73e8f234d8d09ebe. #16 1497.1 (21:51:08) INFO: Repository llvm-raw instantiated at:. #16 1497.1 /opt/deepvariant/WORKSPACE:102:14: in <toplevel>. #16 1497.1 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/tensorflow/workspace3.bzl:42:9: in workspace. #16 1497.1 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/llvm/workspace.bzl:10:20: in repo. #16 1497.1 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:113:21: in tf_http_archive. #16 1497.1 Repository rule _tf_http_archive defined at:. #16 1497.1 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:66:35: in <toplevel>. #16 1497.2 (21:51:08) Loading: 0 packages loaded. #16 1497.3 (21:51:08) ERROR: no such package '@tf_runtime//': java.io.IOException: Error downloading [http://mirror.tensorflow.org/github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz, https://github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz] to /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/tf_runtime/temp12516918929418979294/64c92c8013b557087351c91b5423b6046d10f206.tar.gz: Checksum was 8383b3247286016e450b0b20e805d26b88ab4638b4e4e3cc4a6923debaf7ad1e but wanted f16fcf09b34e0c7be9389f50652b4b4a14c5a8a96e7e15ad73e8f234d8d09ebe. #16 1497.3 (21:51:09) INFO: Elapsed time: 12.546s. #16 1497.3 (21:51:09) INFO: 0 processes. #16 1497.3 (21:51:09) FAILED: Build did NOT complete successfully (0 packages loaded). #16 1497.3 (21:51:09) FAILED: Build did NOT complete successfully (0 packages loaded). #16 ERROR: executor failed running [/bin/sh -c ./build-prereq.sh && PATH=""${HOME}/bin:${PATH}"" ./build_release_binaries.sh # PATH ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/608
https://github.com/google/deepvariant/issues/608:8558,performance,Error,Error,8558,"4e3cc4a6923debaf7ad1e but wanted f16fcf09b34e0c7be9389f50652b4b4a14c5a8a96e7e15ad73e8f234d8d09ebe. #16 1497.1 (21:51:08) INFO: Repository llvm-raw instantiated at:. #16 1497.1 /opt/deepvariant/WORKSPACE:102:14: in <toplevel>. #16 1497.1 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/tensorflow/workspace3.bzl:42:9: in workspace. #16 1497.1 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/llvm/workspace.bzl:10:20: in repo. #16 1497.1 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:113:21: in tf_http_archive. #16 1497.1 Repository rule _tf_http_archive defined at:. #16 1497.1 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:66:35: in <toplevel>. #16 1497.2 (21:51:08) Loading: 0 packages loaded. #16 1497.3 (21:51:08) ERROR: no such package '@tf_runtime//': java.io.IOException: Error downloading [http://mirror.tensorflow.org/github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz, https://github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz] to /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/tf_runtime/temp12516918929418979294/64c92c8013b557087351c91b5423b6046d10f206.tar.gz: Checksum was 8383b3247286016e450b0b20e805d26b88ab4638b4e4e3cc4a6923debaf7ad1e but wanted f16fcf09b34e0c7be9389f50652b4b4a14c5a8a96e7e15ad73e8f234d8d09ebe. #16 1497.3 (21:51:09) INFO: Elapsed time: 12.546s. #16 1497.3 (21:51:09) INFO: 0 processes. #16 1497.3 (21:51:09) FAILED: Build did NOT complete successfully (0 packages loaded). #16 1497.3 (21:51:09) FAILED: Build did NOT complete successfully (0 packages loaded). #16 ERROR: executor failed running [/bin/sh -c ./build-prereq.sh && PATH=""${HOME}/bin:${PATH}"" ./build_release_binaries.sh # PATH for bazel]: exit code: 1. ------. > [builder 6/6] RUN ./build",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/608
https://github.com/google/deepvariant/issues/608:8798,performance,cach,cache,8798,"ot/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/tensorflow/workspace3.bzl:42:9: in workspace. #16 1497.1 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/llvm/workspace.bzl:10:20: in repo. #16 1497.1 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:113:21: in tf_http_archive. #16 1497.1 Repository rule _tf_http_archive defined at:. #16 1497.1 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:66:35: in <toplevel>. #16 1497.2 (21:51:08) Loading: 0 packages loaded. #16 1497.3 (21:51:08) ERROR: no such package '@tf_runtime//': java.io.IOException: Error downloading [http://mirror.tensorflow.org/github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz, https://github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz] to /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/tf_runtime/temp12516918929418979294/64c92c8013b557087351c91b5423b6046d10f206.tar.gz: Checksum was 8383b3247286016e450b0b20e805d26b88ab4638b4e4e3cc4a6923debaf7ad1e but wanted f16fcf09b34e0c7be9389f50652b4b4a14c5a8a96e7e15ad73e8f234d8d09ebe. #16 1497.3 (21:51:09) INFO: Elapsed time: 12.546s. #16 1497.3 (21:51:09) INFO: 0 processes. #16 1497.3 (21:51:09) FAILED: Build did NOT complete successfully (0 packages loaded). #16 1497.3 (21:51:09) FAILED: Build did NOT complete successfully (0 packages loaded). #16 ERROR: executor failed running [/bin/sh -c ./build-prereq.sh && PATH=""${HOME}/bin:${PATH}"" ./build_release_binaries.sh # PATH for bazel]: exit code: 1. ------. > [builder 6/6] RUN ./build-prereq.sh && PATH=""${HOME}/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin"" ./build_release_binaries.sh # PATH for bazel:. ------. executor failed running [/bin/sh -c ./build-prereq.sh && PATH=""${HOME}/bin:${PATH}"" ./build",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/608
https://github.com/google/deepvariant/issues/608:9140,performance,time,time,9140,"80/external/org_tensorflow/tensorflow/workspace3.bzl:42:9: in workspace. #16 1497.1 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/llvm/workspace.bzl:10:20: in repo. #16 1497.1 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:113:21: in tf_http_archive. #16 1497.1 Repository rule _tf_http_archive defined at:. #16 1497.1 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:66:35: in <toplevel>. #16 1497.2 (21:51:08) Loading: 0 packages loaded. #16 1497.3 (21:51:08) ERROR: no such package '@tf_runtime//': java.io.IOException: Error downloading [http://mirror.tensorflow.org/github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz, https://github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz] to /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/tf_runtime/temp12516918929418979294/64c92c8013b557087351c91b5423b6046d10f206.tar.gz: Checksum was 8383b3247286016e450b0b20e805d26b88ab4638b4e4e3cc4a6923debaf7ad1e but wanted f16fcf09b34e0c7be9389f50652b4b4a14c5a8a96e7e15ad73e8f234d8d09ebe. #16 1497.3 (21:51:09) INFO: Elapsed time: 12.546s. #16 1497.3 (21:51:09) INFO: 0 processes. #16 1497.3 (21:51:09) FAILED: Build did NOT complete successfully (0 packages loaded). #16 1497.3 (21:51:09) FAILED: Build did NOT complete successfully (0 packages loaded). #16 ERROR: executor failed running [/bin/sh -c ./build-prereq.sh && PATH=""${HOME}/bin:${PATH}"" ./build_release_binaries.sh # PATH for bazel]: exit code: 1. ------. > [builder 6/6] RUN ./build-prereq.sh && PATH=""${HOME}/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin"" ./build_release_binaries.sh # PATH for bazel:. ------. executor failed running [/bin/sh -c ./build-prereq.sh && PATH=""${HOME}/bin:${PATH}"" ./build_release_binaries.sh # PATH for bazel]: exit code: 1. ```.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/608
https://github.com/google/deepvariant/issues/608:9274,performance,load,loaded,9274,"80/external/org_tensorflow/tensorflow/workspace3.bzl:42:9: in workspace. #16 1497.1 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/llvm/workspace.bzl:10:20: in repo. #16 1497.1 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:113:21: in tf_http_archive. #16 1497.1 Repository rule _tf_http_archive defined at:. #16 1497.1 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:66:35: in <toplevel>. #16 1497.2 (21:51:08) Loading: 0 packages loaded. #16 1497.3 (21:51:08) ERROR: no such package '@tf_runtime//': java.io.IOException: Error downloading [http://mirror.tensorflow.org/github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz, https://github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz] to /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/tf_runtime/temp12516918929418979294/64c92c8013b557087351c91b5423b6046d10f206.tar.gz: Checksum was 8383b3247286016e450b0b20e805d26b88ab4638b4e4e3cc4a6923debaf7ad1e but wanted f16fcf09b34e0c7be9389f50652b4b4a14c5a8a96e7e15ad73e8f234d8d09ebe. #16 1497.3 (21:51:09) INFO: Elapsed time: 12.546s. #16 1497.3 (21:51:09) INFO: 0 processes. #16 1497.3 (21:51:09) FAILED: Build did NOT complete successfully (0 packages loaded). #16 1497.3 (21:51:09) FAILED: Build did NOT complete successfully (0 packages loaded). #16 ERROR: executor failed running [/bin/sh -c ./build-prereq.sh && PATH=""${HOME}/bin:${PATH}"" ./build_release_binaries.sh # PATH for bazel]: exit code: 1. ------. > [builder 6/6] RUN ./build-prereq.sh && PATH=""${HOME}/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin"" ./build_release_binaries.sh # PATH for bazel:. ------. executor failed running [/bin/sh -c ./build-prereq.sh && PATH=""${HOME}/bin:${PATH}"" ./build_release_binaries.sh # PATH for bazel]: exit code: 1. ```.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/608
https://github.com/google/deepvariant/issues/608:9361,performance,load,loaded,9361,"80/external/org_tensorflow/tensorflow/workspace3.bzl:42:9: in workspace. #16 1497.1 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/llvm/workspace.bzl:10:20: in repo. #16 1497.1 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:113:21: in tf_http_archive. #16 1497.1 Repository rule _tf_http_archive defined at:. #16 1497.1 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:66:35: in <toplevel>. #16 1497.2 (21:51:08) Loading: 0 packages loaded. #16 1497.3 (21:51:08) ERROR: no such package '@tf_runtime//': java.io.IOException: Error downloading [http://mirror.tensorflow.org/github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz, https://github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz] to /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/tf_runtime/temp12516918929418979294/64c92c8013b557087351c91b5423b6046d10f206.tar.gz: Checksum was 8383b3247286016e450b0b20e805d26b88ab4638b4e4e3cc4a6923debaf7ad1e but wanted f16fcf09b34e0c7be9389f50652b4b4a14c5a8a96e7e15ad73e8f234d8d09ebe. #16 1497.3 (21:51:09) INFO: Elapsed time: 12.546s. #16 1497.3 (21:51:09) INFO: 0 processes. #16 1497.3 (21:51:09) FAILED: Build did NOT complete successfully (0 packages loaded). #16 1497.3 (21:51:09) FAILED: Build did NOT complete successfully (0 packages loaded). #16 ERROR: executor failed running [/bin/sh -c ./build-prereq.sh && PATH=""${HOME}/bin:${PATH}"" ./build_release_binaries.sh # PATH for bazel]: exit code: 1. ------. > [builder 6/6] RUN ./build-prereq.sh && PATH=""${HOME}/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin"" ./build_release_binaries.sh # PATH for bazel:. ------. executor failed running [/bin/sh -c ./build-prereq.sh && PATH=""${HOME}/bin:${PATH}"" ./build_release_binaries.sh # PATH for bazel]: exit code: 1. ```.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/608
https://github.com/google/deepvariant/issues/608:9374,performance,ERROR,ERROR,9374,"80/external/org_tensorflow/tensorflow/workspace3.bzl:42:9: in workspace. #16 1497.1 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/llvm/workspace.bzl:10:20: in repo. #16 1497.1 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:113:21: in tf_http_archive. #16 1497.1 Repository rule _tf_http_archive defined at:. #16 1497.1 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:66:35: in <toplevel>. #16 1497.2 (21:51:08) Loading: 0 packages loaded. #16 1497.3 (21:51:08) ERROR: no such package '@tf_runtime//': java.io.IOException: Error downloading [http://mirror.tensorflow.org/github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz, https://github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz] to /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/tf_runtime/temp12516918929418979294/64c92c8013b557087351c91b5423b6046d10f206.tar.gz: Checksum was 8383b3247286016e450b0b20e805d26b88ab4638b4e4e3cc4a6923debaf7ad1e but wanted f16fcf09b34e0c7be9389f50652b4b4a14c5a8a96e7e15ad73e8f234d8d09ebe. #16 1497.3 (21:51:09) INFO: Elapsed time: 12.546s. #16 1497.3 (21:51:09) INFO: 0 processes. #16 1497.3 (21:51:09) FAILED: Build did NOT complete successfully (0 packages loaded). #16 1497.3 (21:51:09) FAILED: Build did NOT complete successfully (0 packages loaded). #16 ERROR: executor failed running [/bin/sh -c ./build-prereq.sh && PATH=""${HOME}/bin:${PATH}"" ./build_release_binaries.sh # PATH for bazel]: exit code: 1. ------. > [builder 6/6] RUN ./build-prereq.sh && PATH=""${HOME}/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin"" ./build_release_binaries.sh # PATH for bazel:. ------. executor failed running [/bin/sh -c ./build-prereq.sh && PATH=""${HOME}/bin:${PATH}"" ./build_release_binaries.sh # PATH for bazel]: exit code: 1. ```.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/608
https://github.com/google/deepvariant/issues/608:16,reliability,fail,fails,16,"binary creation fails; bazel unable to fetch tf_runtime; I was able to build the docker image last week, but this week the build fails at binary creation, with bazel unable to download tf_runtime. . - Operating system: Ubuntu20.04. - DeepVariant version: 1.4.0. - Building docker image locally. **Steps to reproduce:**. - Command: `docker build .` in source directory (no modifications). - Error trace: . . ```. #16 1484.7 ========== [Mon Jan 30 21:50:56 UTC 2023] Stage 'build-prereq.sh complete' starting. #16 1484.7 Extracting Bazel installation... #16 1487.8 Starting local Bazel server and connecting to it... #16 1489.8 (21:51:01) WARNING: option '--distinct_host_configuration' was expanded to from both option '--enable_platform_specific_config' (source /opt/tensorflow/.bazelrc) and option '--enable_platform_specific_config' (source /opt/tensorflow/.bazelrc). #16 1489.8 (21:51:01) INFO: Options provided by the client:. #16 1489.8 Inherited 'common' options: --isatty=0 --terminal_columns=80. #16 1489.8 (21:51:01) INFO: Reading rc options for 'build' from /opt/tensorflow/.bazelrc:. #16 1489.8 Inherited 'common' options: --experimental_repo_remote_exec. #16 1489.8 (21:51:01) INFO: Reading rc options for 'build' from /opt/tensorflow/.bazelrc:. #16 1489.8 'build' options: --define framework_shared_object=true --java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --host_java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true --deleted_packages=tensorflow/compiler/mlir/tfrt,tensorflow/compiler/mlir/tfrt/benchmarks,tensorflow/compiler/mlir/tfrt/jit/python_binding,tensorflow/compiler/mlir/tfrt/jit/transforms,tensor",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/608
https://github.com/google/deepvariant/issues/608:129,reliability,fail,fails,129,"binary creation fails; bazel unable to fetch tf_runtime; I was able to build the docker image last week, but this week the build fails at binary creation, with bazel unable to download tf_runtime. . - Operating system: Ubuntu20.04. - DeepVariant version: 1.4.0. - Building docker image locally. **Steps to reproduce:**. - Command: `docker build .` in source directory (no modifications). - Error trace: . . ```. #16 1484.7 ========== [Mon Jan 30 21:50:56 UTC 2023] Stage 'build-prereq.sh complete' starting. #16 1484.7 Extracting Bazel installation... #16 1487.8 Starting local Bazel server and connecting to it... #16 1489.8 (21:51:01) WARNING: option '--distinct_host_configuration' was expanded to from both option '--enable_platform_specific_config' (source /opt/tensorflow/.bazelrc) and option '--enable_platform_specific_config' (source /opt/tensorflow/.bazelrc). #16 1489.8 (21:51:01) INFO: Options provided by the client:. #16 1489.8 Inherited 'common' options: --isatty=0 --terminal_columns=80. #16 1489.8 (21:51:01) INFO: Reading rc options for 'build' from /opt/tensorflow/.bazelrc:. #16 1489.8 Inherited 'common' options: --experimental_repo_remote_exec. #16 1489.8 (21:51:01) INFO: Reading rc options for 'build' from /opt/tensorflow/.bazelrc:. #16 1489.8 'build' options: --define framework_shared_object=true --java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --host_java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true --deleted_packages=tensorflow/compiler/mlir/tfrt,tensorflow/compiler/mlir/tfrt/benchmarks,tensorflow/compiler/mlir/tfrt/jit/python_binding,tensorflow/compiler/mlir/tfrt/jit/transforms,tensor",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/608
https://github.com/google/deepvariant/issues/608:6179,reliability,fail,failed,6179,"ages loaded. #16 1496.2 (21:51:07) Loading: 0 packages loaded. #16 1497.0 (21:51:08) INFO: Repository tf_runtime instantiated at:. #16 1497.0 /opt/deepvariant/WORKSPACE:102:14: in <toplevel>. #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/tensorflow/workspace3.bzl:28:15: in workspace. #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/tf_runtime/workspace.bzl:12:20: in repo. #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:113:21: in tf_http_archive. #16 1497.0 Repository rule _tf_http_archive defined at:. #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:66:35: in <toplevel>. #16 1497.0 (21:51:08) WARNING: Download from http://mirror.tensorflow.org/github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException GET returned 404 Not Found. #16 1497.0 (21:51:08) WARNING: Download from https://github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException Checksum was 8383b3247286016e450b0b20e805d26b88ab4638b4e4e3cc4a6923debaf7ad1e but wanted f16fcf09b34e0c7be9389f50652b4b4a14c5a8a96e7e15ad73e8f234d8d09ebe. #16 1497.0 (21:51:08) ERROR: An error occurred during the fetch of repository 'tf_runtime':. #16 1497.0 Traceback (most recent call last):. #16 1497.0 File ""/root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl"", line 53, column 33, in _tf_http_archive_impl. #16 1497.0 ctx.download_and_extract(. #16 1497.0 Error in download_and_extract: java.io.IOException: Error downloading [http://mirror.tensorflow.org/github.com/tensorflo",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/608
https://github.com/google/deepvariant/issues/608:6445,reliability,fail,failed,6445,"/external/org_tensorflow/tensorflow/workspace3.bzl:28:15: in workspace. #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/tf_runtime/workspace.bzl:12:20: in repo. #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:113:21: in tf_http_archive. #16 1497.0 Repository rule _tf_http_archive defined at:. #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:66:35: in <toplevel>. #16 1497.0 (21:51:08) WARNING: Download from http://mirror.tensorflow.org/github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException GET returned 404 Not Found. #16 1497.0 (21:51:08) WARNING: Download from https://github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException Checksum was 8383b3247286016e450b0b20e805d26b88ab4638b4e4e3cc4a6923debaf7ad1e but wanted f16fcf09b34e0c7be9389f50652b4b4a14c5a8a96e7e15ad73e8f234d8d09ebe. #16 1497.0 (21:51:08) ERROR: An error occurred during the fetch of repository 'tf_runtime':. #16 1497.0 Traceback (most recent call last):. #16 1497.0 File ""/root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl"", line 53, column 33, in _tf_http_archive_impl. #16 1497.0 ctx.download_and_extract(. #16 1497.0 Error in download_and_extract: java.io.IOException: Error downloading [http://mirror.tensorflow.org/github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz, https://github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz] to /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/tf_runtime/temp1251691892941",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/608
https://github.com/google/deepvariant/issues/608:9218,reliability,FAIL,FAILED,9218,"80/external/org_tensorflow/tensorflow/workspace3.bzl:42:9: in workspace. #16 1497.1 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/llvm/workspace.bzl:10:20: in repo. #16 1497.1 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:113:21: in tf_http_archive. #16 1497.1 Repository rule _tf_http_archive defined at:. #16 1497.1 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:66:35: in <toplevel>. #16 1497.2 (21:51:08) Loading: 0 packages loaded. #16 1497.3 (21:51:08) ERROR: no such package '@tf_runtime//': java.io.IOException: Error downloading [http://mirror.tensorflow.org/github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz, https://github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz] to /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/tf_runtime/temp12516918929418979294/64c92c8013b557087351c91b5423b6046d10f206.tar.gz: Checksum was 8383b3247286016e450b0b20e805d26b88ab4638b4e4e3cc4a6923debaf7ad1e but wanted f16fcf09b34e0c7be9389f50652b4b4a14c5a8a96e7e15ad73e8f234d8d09ebe. #16 1497.3 (21:51:09) INFO: Elapsed time: 12.546s. #16 1497.3 (21:51:09) INFO: 0 processes. #16 1497.3 (21:51:09) FAILED: Build did NOT complete successfully (0 packages loaded). #16 1497.3 (21:51:09) FAILED: Build did NOT complete successfully (0 packages loaded). #16 ERROR: executor failed running [/bin/sh -c ./build-prereq.sh && PATH=""${HOME}/bin:${PATH}"" ./build_release_binaries.sh # PATH for bazel]: exit code: 1. ------. > [builder 6/6] RUN ./build-prereq.sh && PATH=""${HOME}/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin"" ./build_release_binaries.sh # PATH for bazel:. ------. executor failed running [/bin/sh -c ./build-prereq.sh && PATH=""${HOME}/bin:${PATH}"" ./build_release_binaries.sh # PATH for bazel]: exit code: 1. ```.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/608
https://github.com/google/deepvariant/issues/608:9305,reliability,FAIL,FAILED,9305,"80/external/org_tensorflow/tensorflow/workspace3.bzl:42:9: in workspace. #16 1497.1 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/llvm/workspace.bzl:10:20: in repo. #16 1497.1 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:113:21: in tf_http_archive. #16 1497.1 Repository rule _tf_http_archive defined at:. #16 1497.1 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:66:35: in <toplevel>. #16 1497.2 (21:51:08) Loading: 0 packages loaded. #16 1497.3 (21:51:08) ERROR: no such package '@tf_runtime//': java.io.IOException: Error downloading [http://mirror.tensorflow.org/github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz, https://github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz] to /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/tf_runtime/temp12516918929418979294/64c92c8013b557087351c91b5423b6046d10f206.tar.gz: Checksum was 8383b3247286016e450b0b20e805d26b88ab4638b4e4e3cc4a6923debaf7ad1e but wanted f16fcf09b34e0c7be9389f50652b4b4a14c5a8a96e7e15ad73e8f234d8d09ebe. #16 1497.3 (21:51:09) INFO: Elapsed time: 12.546s. #16 1497.3 (21:51:09) INFO: 0 processes. #16 1497.3 (21:51:09) FAILED: Build did NOT complete successfully (0 packages loaded). #16 1497.3 (21:51:09) FAILED: Build did NOT complete successfully (0 packages loaded). #16 ERROR: executor failed running [/bin/sh -c ./build-prereq.sh && PATH=""${HOME}/bin:${PATH}"" ./build_release_binaries.sh # PATH for bazel]: exit code: 1. ------. > [builder 6/6] RUN ./build-prereq.sh && PATH=""${HOME}/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin"" ./build_release_binaries.sh # PATH for bazel:. ------. executor failed running [/bin/sh -c ./build-prereq.sh && PATH=""${HOME}/bin:${PATH}"" ./build_release_binaries.sh # PATH for bazel]: exit code: 1. ```.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/608
https://github.com/google/deepvariant/issues/608:9390,reliability,fail,failed,9390,"80/external/org_tensorflow/tensorflow/workspace3.bzl:42:9: in workspace. #16 1497.1 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/llvm/workspace.bzl:10:20: in repo. #16 1497.1 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:113:21: in tf_http_archive. #16 1497.1 Repository rule _tf_http_archive defined at:. #16 1497.1 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:66:35: in <toplevel>. #16 1497.2 (21:51:08) Loading: 0 packages loaded. #16 1497.3 (21:51:08) ERROR: no such package '@tf_runtime//': java.io.IOException: Error downloading [http://mirror.tensorflow.org/github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz, https://github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz] to /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/tf_runtime/temp12516918929418979294/64c92c8013b557087351c91b5423b6046d10f206.tar.gz: Checksum was 8383b3247286016e450b0b20e805d26b88ab4638b4e4e3cc4a6923debaf7ad1e but wanted f16fcf09b34e0c7be9389f50652b4b4a14c5a8a96e7e15ad73e8f234d8d09ebe. #16 1497.3 (21:51:09) INFO: Elapsed time: 12.546s. #16 1497.3 (21:51:09) INFO: 0 processes. #16 1497.3 (21:51:09) FAILED: Build did NOT complete successfully (0 packages loaded). #16 1497.3 (21:51:09) FAILED: Build did NOT complete successfully (0 packages loaded). #16 ERROR: executor failed running [/bin/sh -c ./build-prereq.sh && PATH=""${HOME}/bin:${PATH}"" ./build_release_binaries.sh # PATH for bazel]: exit code: 1. ------. > [builder 6/6] RUN ./build-prereq.sh && PATH=""${HOME}/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin"" ./build_release_binaries.sh # PATH for bazel:. ------. executor failed running [/bin/sh -c ./build-prereq.sh && PATH=""${HOME}/bin:${PATH}"" ./build_release_binaries.sh # PATH for bazel]: exit code: 1. ```.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/608
https://github.com/google/deepvariant/issues/608:9719,reliability,fail,failed,9719,"80/external/org_tensorflow/tensorflow/workspace3.bzl:42:9: in workspace. #16 1497.1 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/llvm/workspace.bzl:10:20: in repo. #16 1497.1 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:113:21: in tf_http_archive. #16 1497.1 Repository rule _tf_http_archive defined at:. #16 1497.1 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:66:35: in <toplevel>. #16 1497.2 (21:51:08) Loading: 0 packages loaded. #16 1497.3 (21:51:08) ERROR: no such package '@tf_runtime//': java.io.IOException: Error downloading [http://mirror.tensorflow.org/github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz, https://github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz] to /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/tf_runtime/temp12516918929418979294/64c92c8013b557087351c91b5423b6046d10f206.tar.gz: Checksum was 8383b3247286016e450b0b20e805d26b88ab4638b4e4e3cc4a6923debaf7ad1e but wanted f16fcf09b34e0c7be9389f50652b4b4a14c5a8a96e7e15ad73e8f234d8d09ebe. #16 1497.3 (21:51:09) INFO: Elapsed time: 12.546s. #16 1497.3 (21:51:09) INFO: 0 processes. #16 1497.3 (21:51:09) FAILED: Build did NOT complete successfully (0 packages loaded). #16 1497.3 (21:51:09) FAILED: Build did NOT complete successfully (0 packages loaded). #16 ERROR: executor failed running [/bin/sh -c ./build-prereq.sh && PATH=""${HOME}/bin:${PATH}"" ./build_release_binaries.sh # PATH for bazel]: exit code: 1. ------. > [builder 6/6] RUN ./build-prereq.sh && PATH=""${HOME}/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin"" ./build_release_binaries.sh # PATH for bazel:. ------. executor failed running [/bin/sh -c ./build-prereq.sh && PATH=""${HOME}/bin:${PATH}"" ./build_release_binaries.sh # PATH for bazel]: exit code: 1. ```.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/608
https://github.com/google/deepvariant/issues/608:390,safety,Error,Error,390,"binary creation fails; bazel unable to fetch tf_runtime; I was able to build the docker image last week, but this week the build fails at binary creation, with bazel unable to download tf_runtime. . - Operating system: Ubuntu20.04. - DeepVariant version: 1.4.0. - Building docker image locally. **Steps to reproduce:**. - Command: `docker build .` in source directory (no modifications). - Error trace: . . ```. #16 1484.7 ========== [Mon Jan 30 21:50:56 UTC 2023] Stage 'build-prereq.sh complete' starting. #16 1484.7 Extracting Bazel installation... #16 1487.8 Starting local Bazel server and connecting to it... #16 1489.8 (21:51:01) WARNING: option '--distinct_host_configuration' was expanded to from both option '--enable_platform_specific_config' (source /opt/tensorflow/.bazelrc) and option '--enable_platform_specific_config' (source /opt/tensorflow/.bazelrc). #16 1489.8 (21:51:01) INFO: Options provided by the client:. #16 1489.8 Inherited 'common' options: --isatty=0 --terminal_columns=80. #16 1489.8 (21:51:01) INFO: Reading rc options for 'build' from /opt/tensorflow/.bazelrc:. #16 1489.8 Inherited 'common' options: --experimental_repo_remote_exec. #16 1489.8 (21:51:01) INFO: Reading rc options for 'build' from /opt/tensorflow/.bazelrc:. #16 1489.8 'build' options: --define framework_shared_object=true --java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --host_java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true --deleted_packages=tensorflow/compiler/mlir/tfrt,tensorflow/compiler/mlir/tfrt/benchmarks,tensorflow/compiler/mlir/tfrt/jit/python_binding,tensorflow/compiler/mlir/tfrt/jit/transforms,tensor",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/608
https://github.com/google/deepvariant/issues/608:488,safety,compl,complete,488,"binary creation fails; bazel unable to fetch tf_runtime; I was able to build the docker image last week, but this week the build fails at binary creation, with bazel unable to download tf_runtime. . - Operating system: Ubuntu20.04. - DeepVariant version: 1.4.0. - Building docker image locally. **Steps to reproduce:**. - Command: `docker build .` in source directory (no modifications). - Error trace: . . ```. #16 1484.7 ========== [Mon Jan 30 21:50:56 UTC 2023] Stage 'build-prereq.sh complete' starting. #16 1484.7 Extracting Bazel installation... #16 1487.8 Starting local Bazel server and connecting to it... #16 1489.8 (21:51:01) WARNING: option '--distinct_host_configuration' was expanded to from both option '--enable_platform_specific_config' (source /opt/tensorflow/.bazelrc) and option '--enable_platform_specific_config' (source /opt/tensorflow/.bazelrc). #16 1489.8 (21:51:01) INFO: Options provided by the client:. #16 1489.8 Inherited 'common' options: --isatty=0 --terminal_columns=80. #16 1489.8 (21:51:01) INFO: Reading rc options for 'build' from /opt/tensorflow/.bazelrc:. #16 1489.8 Inherited 'common' options: --experimental_repo_remote_exec. #16 1489.8 (21:51:01) INFO: Reading rc options for 'build' from /opt/tensorflow/.bazelrc:. #16 1489.8 'build' options: --define framework_shared_object=true --java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --host_java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true --deleted_packages=tensorflow/compiler/mlir/tfrt,tensorflow/compiler/mlir/tfrt/benchmarks,tensorflow/compiler/mlir/tfrt/jit/python_binding,tensorflow/compiler/mlir/tfrt/jit/transforms,tensor",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/608
https://github.com/google/deepvariant/issues/608:2067,safety,test,tests,2067,"pt/tensorflow/.bazelrc:. #16 1489.8 Inherited 'common' options: --experimental_repo_remote_exec. #16 1489.8 (21:51:01) INFO: Reading rc options for 'build' from /opt/tensorflow/.bazelrc:. #16 1489.8 'build' options: --define framework_shared_object=true --java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --host_java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true --deleted_packages=tensorflow/compiler/mlir/tfrt,tensorflow/compiler/mlir/tfrt/benchmarks,tensorflow/compiler/mlir/tfrt/jit/python_binding,tensorflow/compiler/mlir/tfrt/jit/transforms,tensorflow/compiler/mlir/tfrt/python_tests,tensorflow/compiler/mlir/tfrt/tests,tensorflow/compiler/mlir/tfrt/tests/saved_model,tensorflow/compiler/mlir/tfrt/transforms/lhlo_gpu_to_tfrt_gpu,tensorflow/core/runtime_fallback,tensorflow/core/runtime_fallback/conversion,tensorflow/core/runtime_fallback/kernel,tensorflow/core/runtime_fallback/opdefs,tensorflow/core/runtime_fallback/runtime,tensorflow/core/runtime_fallback/util,tensorflow/core/tfrt/common,tensorflow/core/tfrt/eager,tensorflow/core/tfrt/eager/backends/cpu,tensorflow/core/tfrt/eager/backends/gpu,tensorflow/core/tfrt/eager/core_runtime,tensorflow/core/tfrt/eager/cpp_tests/core_runtime,tensorflow/core/tfrt/fallback,tensorflow/core/tfrt/gpu,tensorflow/core/tfrt/run_handler_thread_pool,tensorflow/core/tfrt/runtime,tensorflow/core/tfrt/saved_model,tensorflow/core/tfrt/saved_model/tests,tensorflow/core/tfrt/tpu,tensorflow/core/tfrt/utils. #16 1489.8 (21:51:01) INFO: Reading rc options for 'build' from /opt/tensorflow/.tf_configure.bazelrc:. #16 1489.8 'build' options: --action_env PYTHON_BIN_PATH=/usr/local/",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/608
https://github.com/google/deepvariant/issues/608:2103,safety,test,tests,2103,"Inherited 'common' options: --experimental_repo_remote_exec. #16 1489.8 (21:51:01) INFO: Reading rc options for 'build' from /opt/tensorflow/.bazelrc:. #16 1489.8 'build' options: --define framework_shared_object=true --java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --host_java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true --deleted_packages=tensorflow/compiler/mlir/tfrt,tensorflow/compiler/mlir/tfrt/benchmarks,tensorflow/compiler/mlir/tfrt/jit/python_binding,tensorflow/compiler/mlir/tfrt/jit/transforms,tensorflow/compiler/mlir/tfrt/python_tests,tensorflow/compiler/mlir/tfrt/tests,tensorflow/compiler/mlir/tfrt/tests/saved_model,tensorflow/compiler/mlir/tfrt/transforms/lhlo_gpu_to_tfrt_gpu,tensorflow/core/runtime_fallback,tensorflow/core/runtime_fallback/conversion,tensorflow/core/runtime_fallback/kernel,tensorflow/core/runtime_fallback/opdefs,tensorflow/core/runtime_fallback/runtime,tensorflow/core/runtime_fallback/util,tensorflow/core/tfrt/common,tensorflow/core/tfrt/eager,tensorflow/core/tfrt/eager/backends/cpu,tensorflow/core/tfrt/eager/backends/gpu,tensorflow/core/tfrt/eager/core_runtime,tensorflow/core/tfrt/eager/cpp_tests/core_runtime,tensorflow/core/tfrt/fallback,tensorflow/core/tfrt/gpu,tensorflow/core/tfrt/run_handler_thread_pool,tensorflow/core/tfrt/runtime,tensorflow/core/tfrt/saved_model,tensorflow/core/tfrt/saved_model/tests,tensorflow/core/tfrt/tpu,tensorflow/core/tfrt/utils. #16 1489.8 (21:51:01) INFO: Reading rc options for 'build' from /opt/tensorflow/.tf_configure.bazelrc:. #16 1489.8 'build' options: --action_env PYTHON_BIN_PATH=/usr/local/bin/python3 --action_env PYTHON_LIB_",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/608
https://github.com/google/deepvariant/issues/608:2839,safety,test,tests,2839,"mpiler/mlir/tfrt,tensorflow/compiler/mlir/tfrt/benchmarks,tensorflow/compiler/mlir/tfrt/jit/python_binding,tensorflow/compiler/mlir/tfrt/jit/transforms,tensorflow/compiler/mlir/tfrt/python_tests,tensorflow/compiler/mlir/tfrt/tests,tensorflow/compiler/mlir/tfrt/tests/saved_model,tensorflow/compiler/mlir/tfrt/transforms/lhlo_gpu_to_tfrt_gpu,tensorflow/core/runtime_fallback,tensorflow/core/runtime_fallback/conversion,tensorflow/core/runtime_fallback/kernel,tensorflow/core/runtime_fallback/opdefs,tensorflow/core/runtime_fallback/runtime,tensorflow/core/runtime_fallback/util,tensorflow/core/tfrt/common,tensorflow/core/tfrt/eager,tensorflow/core/tfrt/eager/backends/cpu,tensorflow/core/tfrt/eager/backends/gpu,tensorflow/core/tfrt/eager/core_runtime,tensorflow/core/tfrt/eager/cpp_tests/core_runtime,tensorflow/core/tfrt/fallback,tensorflow/core/tfrt/gpu,tensorflow/core/tfrt/run_handler_thread_pool,tensorflow/core/tfrt/runtime,tensorflow/core/tfrt/saved_model,tensorflow/core/tfrt/saved_model/tests,tensorflow/core/tfrt/tpu,tensorflow/core/tfrt/utils. #16 1489.8 (21:51:01) INFO: Reading rc options for 'build' from /opt/tensorflow/.tf_configure.bazelrc:. #16 1489.8 'build' options: --action_env PYTHON_BIN_PATH=/usr/local/bin/python3 --action_env PYTHON_LIB_PATH=/usr/local/lib/python3.8/dist-packages --python_path=/usr/local/bin/python3. #16 1489.8 (21:51:01) INFO: Reading rc options for 'build' from /opt/deepvariant/.bazelrc:. #16 1489.8 'build' options: --jobs 128 --config=monolithic --show_timestamps --verbose_failures --define=use_fast_cpp_protos=true --copt=-Wno-maybe-uninitialized --copt=-Wno-unused-function --cxxopt=-std=c++17 --python_top=//:deepvariant_python_runtime --incompatible_use_python_toolchains=false. #16 1489.8 (21:51:01) INFO: Found applicable config definition build:short_logs in file /opt/tensorflow/.bazelrc: --output_filter=DONT_MATCH_ANYTHING. #16 1489.8 (21:51:01) INFO: Found applicable config definition build:v2 in file /opt/tensorflow/.bazelrc: --define",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/608
https://github.com/google/deepvariant/issues/608:6721,safety,ERROR,ERROR,6721,"c1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:113:21: in tf_http_archive. #16 1497.0 Repository rule _tf_http_archive defined at:. #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:66:35: in <toplevel>. #16 1497.0 (21:51:08) WARNING: Download from http://mirror.tensorflow.org/github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException GET returned 404 Not Found. #16 1497.0 (21:51:08) WARNING: Download from https://github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException Checksum was 8383b3247286016e450b0b20e805d26b88ab4638b4e4e3cc4a6923debaf7ad1e but wanted f16fcf09b34e0c7be9389f50652b4b4a14c5a8a96e7e15ad73e8f234d8d09ebe. #16 1497.0 (21:51:08) ERROR: An error occurred during the fetch of repository 'tf_runtime':. #16 1497.0 Traceback (most recent call last):. #16 1497.0 File ""/root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl"", line 53, column 33, in _tf_http_archive_impl. #16 1497.0 ctx.download_and_extract(. #16 1497.0 Error in download_and_extract: java.io.IOException: Error downloading [http://mirror.tensorflow.org/github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz, https://github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz] to /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/tf_runtime/temp12516918929418979294/64c92c8013b557087351c91b5423b6046d10f206.tar.gz: Checksum was 8383b3247286016e450b0b20e805d26b88ab4638b4e4e3cc4a6923debaf7ad1e but wanted f16fcf09b34e0c7be9389f50652b4b4a14c5a8a96e7e15ad73e8f234d8d09ebe. #16 1497.1 (21:51:08) INFO: Repository llvm-raw instantiated at:",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/608
https://github.com/google/deepvariant/issues/608:6731,safety,error,error,6731,"3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:113:21: in tf_http_archive. #16 1497.0 Repository rule _tf_http_archive defined at:. #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:66:35: in <toplevel>. #16 1497.0 (21:51:08) WARNING: Download from http://mirror.tensorflow.org/github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException GET returned 404 Not Found. #16 1497.0 (21:51:08) WARNING: Download from https://github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException Checksum was 8383b3247286016e450b0b20e805d26b88ab4638b4e4e3cc4a6923debaf7ad1e but wanted f16fcf09b34e0c7be9389f50652b4b4a14c5a8a96e7e15ad73e8f234d8d09ebe. #16 1497.0 (21:51:08) ERROR: An error occurred during the fetch of repository 'tf_runtime':. #16 1497.0 Traceback (most recent call last):. #16 1497.0 File ""/root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl"", line 53, column 33, in _tf_http_archive_impl. #16 1497.0 ctx.download_and_extract(. #16 1497.0 Error in download_and_extract: java.io.IOException: Error downloading [http://mirror.tensorflow.org/github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz, https://github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz] to /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/tf_runtime/temp12516918929418979294/64c92c8013b557087351c91b5423b6046d10f206.tar.gz: Checksum was 8383b3247286016e450b0b20e805d26b88ab4638b4e4e3cc4a6923debaf7ad1e but wanted f16fcf09b34e0c7be9389f50652b4b4a14c5a8a96e7e15ad73e8f234d8d09ebe. #16 1497.1 (21:51:08) INFO: Repository llvm-raw instantiated at:. #16 1497",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/608
https://github.com/google/deepvariant/issues/608:7062,safety,Error,Error,7062,"ttp://mirror.tensorflow.org/github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException GET returned 404 Not Found. #16 1497.0 (21:51:08) WARNING: Download from https://github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException Checksum was 8383b3247286016e450b0b20e805d26b88ab4638b4e4e3cc4a6923debaf7ad1e but wanted f16fcf09b34e0c7be9389f50652b4b4a14c5a8a96e7e15ad73e8f234d8d09ebe. #16 1497.0 (21:51:08) ERROR: An error occurred during the fetch of repository 'tf_runtime':. #16 1497.0 Traceback (most recent call last):. #16 1497.0 File ""/root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl"", line 53, column 33, in _tf_http_archive_impl. #16 1497.0 ctx.download_and_extract(. #16 1497.0 Error in download_and_extract: java.io.IOException: Error downloading [http://mirror.tensorflow.org/github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz, https://github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz] to /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/tf_runtime/temp12516918929418979294/64c92c8013b557087351c91b5423b6046d10f206.tar.gz: Checksum was 8383b3247286016e450b0b20e805d26b88ab4638b4e4e3cc4a6923debaf7ad1e but wanted f16fcf09b34e0c7be9389f50652b4b4a14c5a8a96e7e15ad73e8f234d8d09ebe. #16 1497.1 (21:51:08) INFO: Repository llvm-raw instantiated at:. #16 1497.1 /opt/deepvariant/WORKSPACE:102:14: in <toplevel>. #16 1497.1 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/tensorflow/workspace3.bzl:42:9: in workspace. #16 1497.1 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/llvm/workspace.bzl:10:",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/608
https://github.com/google/deepvariant/issues/608:7114,safety,Error,Error,7114,"ntime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException GET returned 404 Not Found. #16 1497.0 (21:51:08) WARNING: Download from https://github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException Checksum was 8383b3247286016e450b0b20e805d26b88ab4638b4e4e3cc4a6923debaf7ad1e but wanted f16fcf09b34e0c7be9389f50652b4b4a14c5a8a96e7e15ad73e8f234d8d09ebe. #16 1497.0 (21:51:08) ERROR: An error occurred during the fetch of repository 'tf_runtime':. #16 1497.0 Traceback (most recent call last):. #16 1497.0 File ""/root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl"", line 53, column 33, in _tf_http_archive_impl. #16 1497.0 ctx.download_and_extract(. #16 1497.0 Error in download_and_extract: java.io.IOException: Error downloading [http://mirror.tensorflow.org/github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz, https://github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz] to /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/tf_runtime/temp12516918929418979294/64c92c8013b557087351c91b5423b6046d10f206.tar.gz: Checksum was 8383b3247286016e450b0b20e805d26b88ab4638b4e4e3cc4a6923debaf7ad1e but wanted f16fcf09b34e0c7be9389f50652b4b4a14c5a8a96e7e15ad73e8f234d8d09ebe. #16 1497.1 (21:51:08) INFO: Repository llvm-raw instantiated at:. #16 1497.1 /opt/deepvariant/WORKSPACE:102:14: in <toplevel>. #16 1497.1 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/tensorflow/workspace3.bzl:42:9: in workspace. #16 1497.1 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/llvm/workspace.bzl:10:20: in repo. #16 1497.1 /root/.cache/bazel/_bazel_ro",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/608
https://github.com/google/deepvariant/issues/608:8497,safety,ERROR,ERROR,8497,".gz: Checksum was 8383b3247286016e450b0b20e805d26b88ab4638b4e4e3cc4a6923debaf7ad1e but wanted f16fcf09b34e0c7be9389f50652b4b4a14c5a8a96e7e15ad73e8f234d8d09ebe. #16 1497.1 (21:51:08) INFO: Repository llvm-raw instantiated at:. #16 1497.1 /opt/deepvariant/WORKSPACE:102:14: in <toplevel>. #16 1497.1 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/tensorflow/workspace3.bzl:42:9: in workspace. #16 1497.1 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/llvm/workspace.bzl:10:20: in repo. #16 1497.1 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:113:21: in tf_http_archive. #16 1497.1 Repository rule _tf_http_archive defined at:. #16 1497.1 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:66:35: in <toplevel>. #16 1497.2 (21:51:08) Loading: 0 packages loaded. #16 1497.3 (21:51:08) ERROR: no such package '@tf_runtime//': java.io.IOException: Error downloading [http://mirror.tensorflow.org/github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz, https://github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz] to /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/tf_runtime/temp12516918929418979294/64c92c8013b557087351c91b5423b6046d10f206.tar.gz: Checksum was 8383b3247286016e450b0b20e805d26b88ab4638b4e4e3cc4a6923debaf7ad1e but wanted f16fcf09b34e0c7be9389f50652b4b4a14c5a8a96e7e15ad73e8f234d8d09ebe. #16 1497.3 (21:51:09) INFO: Elapsed time: 12.546s. #16 1497.3 (21:51:09) INFO: 0 processes. #16 1497.3 (21:51:09) FAILED: Build did NOT complete successfully (0 packages loaded). #16 1497.3 (21:51:09) FAILED: Build did NOT complete successfully (0 packages loaded). #16 ERROR: executor failed running [/bin/sh -c ./build-prereq.sh && PATH=""${HOME}/bin:${PATH}"" ./build_release_binaries.sh # PATH ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/608
https://github.com/google/deepvariant/issues/608:8558,safety,Error,Error,8558,"4e3cc4a6923debaf7ad1e but wanted f16fcf09b34e0c7be9389f50652b4b4a14c5a8a96e7e15ad73e8f234d8d09ebe. #16 1497.1 (21:51:08) INFO: Repository llvm-raw instantiated at:. #16 1497.1 /opt/deepvariant/WORKSPACE:102:14: in <toplevel>. #16 1497.1 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/tensorflow/workspace3.bzl:42:9: in workspace. #16 1497.1 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/llvm/workspace.bzl:10:20: in repo. #16 1497.1 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:113:21: in tf_http_archive. #16 1497.1 Repository rule _tf_http_archive defined at:. #16 1497.1 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:66:35: in <toplevel>. #16 1497.2 (21:51:08) Loading: 0 packages loaded. #16 1497.3 (21:51:08) ERROR: no such package '@tf_runtime//': java.io.IOException: Error downloading [http://mirror.tensorflow.org/github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz, https://github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz] to /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/tf_runtime/temp12516918929418979294/64c92c8013b557087351c91b5423b6046d10f206.tar.gz: Checksum was 8383b3247286016e450b0b20e805d26b88ab4638b4e4e3cc4a6923debaf7ad1e but wanted f16fcf09b34e0c7be9389f50652b4b4a14c5a8a96e7e15ad73e8f234d8d09ebe. #16 1497.3 (21:51:09) INFO: Elapsed time: 12.546s. #16 1497.3 (21:51:09) INFO: 0 processes. #16 1497.3 (21:51:09) FAILED: Build did NOT complete successfully (0 packages loaded). #16 1497.3 (21:51:09) FAILED: Build did NOT complete successfully (0 packages loaded). #16 ERROR: executor failed running [/bin/sh -c ./build-prereq.sh && PATH=""${HOME}/bin:${PATH}"" ./build_release_binaries.sh # PATH for bazel]: exit code: 1. ------. > [builder 6/6] RUN ./build",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/608
https://github.com/google/deepvariant/issues/608:9240,safety,compl,complete,9240,"80/external/org_tensorflow/tensorflow/workspace3.bzl:42:9: in workspace. #16 1497.1 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/llvm/workspace.bzl:10:20: in repo. #16 1497.1 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:113:21: in tf_http_archive. #16 1497.1 Repository rule _tf_http_archive defined at:. #16 1497.1 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:66:35: in <toplevel>. #16 1497.2 (21:51:08) Loading: 0 packages loaded. #16 1497.3 (21:51:08) ERROR: no such package '@tf_runtime//': java.io.IOException: Error downloading [http://mirror.tensorflow.org/github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz, https://github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz] to /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/tf_runtime/temp12516918929418979294/64c92c8013b557087351c91b5423b6046d10f206.tar.gz: Checksum was 8383b3247286016e450b0b20e805d26b88ab4638b4e4e3cc4a6923debaf7ad1e but wanted f16fcf09b34e0c7be9389f50652b4b4a14c5a8a96e7e15ad73e8f234d8d09ebe. #16 1497.3 (21:51:09) INFO: Elapsed time: 12.546s. #16 1497.3 (21:51:09) INFO: 0 processes. #16 1497.3 (21:51:09) FAILED: Build did NOT complete successfully (0 packages loaded). #16 1497.3 (21:51:09) FAILED: Build did NOT complete successfully (0 packages loaded). #16 ERROR: executor failed running [/bin/sh -c ./build-prereq.sh && PATH=""${HOME}/bin:${PATH}"" ./build_release_binaries.sh # PATH for bazel]: exit code: 1. ------. > [builder 6/6] RUN ./build-prereq.sh && PATH=""${HOME}/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin"" ./build_release_binaries.sh # PATH for bazel:. ------. executor failed running [/bin/sh -c ./build-prereq.sh && PATH=""${HOME}/bin:${PATH}"" ./build_release_binaries.sh # PATH for bazel]: exit code: 1. ```.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/608
https://github.com/google/deepvariant/issues/608:9327,safety,compl,complete,9327,"80/external/org_tensorflow/tensorflow/workspace3.bzl:42:9: in workspace. #16 1497.1 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/llvm/workspace.bzl:10:20: in repo. #16 1497.1 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:113:21: in tf_http_archive. #16 1497.1 Repository rule _tf_http_archive defined at:. #16 1497.1 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:66:35: in <toplevel>. #16 1497.2 (21:51:08) Loading: 0 packages loaded. #16 1497.3 (21:51:08) ERROR: no such package '@tf_runtime//': java.io.IOException: Error downloading [http://mirror.tensorflow.org/github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz, https://github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz] to /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/tf_runtime/temp12516918929418979294/64c92c8013b557087351c91b5423b6046d10f206.tar.gz: Checksum was 8383b3247286016e450b0b20e805d26b88ab4638b4e4e3cc4a6923debaf7ad1e but wanted f16fcf09b34e0c7be9389f50652b4b4a14c5a8a96e7e15ad73e8f234d8d09ebe. #16 1497.3 (21:51:09) INFO: Elapsed time: 12.546s. #16 1497.3 (21:51:09) INFO: 0 processes. #16 1497.3 (21:51:09) FAILED: Build did NOT complete successfully (0 packages loaded). #16 1497.3 (21:51:09) FAILED: Build did NOT complete successfully (0 packages loaded). #16 ERROR: executor failed running [/bin/sh -c ./build-prereq.sh && PATH=""${HOME}/bin:${PATH}"" ./build_release_binaries.sh # PATH for bazel]: exit code: 1. ------. > [builder 6/6] RUN ./build-prereq.sh && PATH=""${HOME}/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin"" ./build_release_binaries.sh # PATH for bazel:. ------. executor failed running [/bin/sh -c ./build-prereq.sh && PATH=""${HOME}/bin:${PATH}"" ./build_release_binaries.sh # PATH for bazel]: exit code: 1. ```.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/608
https://github.com/google/deepvariant/issues/608:9374,safety,ERROR,ERROR,9374,"80/external/org_tensorflow/tensorflow/workspace3.bzl:42:9: in workspace. #16 1497.1 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/llvm/workspace.bzl:10:20: in repo. #16 1497.1 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:113:21: in tf_http_archive. #16 1497.1 Repository rule _tf_http_archive defined at:. #16 1497.1 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:66:35: in <toplevel>. #16 1497.2 (21:51:08) Loading: 0 packages loaded. #16 1497.3 (21:51:08) ERROR: no such package '@tf_runtime//': java.io.IOException: Error downloading [http://mirror.tensorflow.org/github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz, https://github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz] to /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/tf_runtime/temp12516918929418979294/64c92c8013b557087351c91b5423b6046d10f206.tar.gz: Checksum was 8383b3247286016e450b0b20e805d26b88ab4638b4e4e3cc4a6923debaf7ad1e but wanted f16fcf09b34e0c7be9389f50652b4b4a14c5a8a96e7e15ad73e8f234d8d09ebe. #16 1497.3 (21:51:09) INFO: Elapsed time: 12.546s. #16 1497.3 (21:51:09) INFO: 0 processes. #16 1497.3 (21:51:09) FAILED: Build did NOT complete successfully (0 packages loaded). #16 1497.3 (21:51:09) FAILED: Build did NOT complete successfully (0 packages loaded). #16 ERROR: executor failed running [/bin/sh -c ./build-prereq.sh && PATH=""${HOME}/bin:${PATH}"" ./build_release_binaries.sh # PATH for bazel]: exit code: 1. ------. > [builder 6/6] RUN ./build-prereq.sh && PATH=""${HOME}/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin"" ./build_release_binaries.sh # PATH for bazel:. ------. executor failed running [/bin/sh -c ./build-prereq.sh && PATH=""${HOME}/bin:${PATH}"" ./build_release_binaries.sh # PATH for bazel]: exit code: 1. ```.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/608
https://github.com/google/deepvariant/issues/608:372,security,modif,modifications,372,"binary creation fails; bazel unable to fetch tf_runtime; I was able to build the docker image last week, but this week the build fails at binary creation, with bazel unable to download tf_runtime. . - Operating system: Ubuntu20.04. - DeepVariant version: 1.4.0. - Building docker image locally. **Steps to reproduce:**. - Command: `docker build .` in source directory (no modifications). - Error trace: . . ```. #16 1484.7 ========== [Mon Jan 30 21:50:56 UTC 2023] Stage 'build-prereq.sh complete' starting. #16 1484.7 Extracting Bazel installation... #16 1487.8 Starting local Bazel server and connecting to it... #16 1489.8 (21:51:01) WARNING: option '--distinct_host_configuration' was expanded to from both option '--enable_platform_specific_config' (source /opt/tensorflow/.bazelrc) and option '--enable_platform_specific_config' (source /opt/tensorflow/.bazelrc). #16 1489.8 (21:51:01) INFO: Options provided by the client:. #16 1489.8 Inherited 'common' options: --isatty=0 --terminal_columns=80. #16 1489.8 (21:51:01) INFO: Reading rc options for 'build' from /opt/tensorflow/.bazelrc:. #16 1489.8 Inherited 'common' options: --experimental_repo_remote_exec. #16 1489.8 (21:51:01) INFO: Reading rc options for 'build' from /opt/tensorflow/.bazelrc:. #16 1489.8 'build' options: --define framework_shared_object=true --java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --host_java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true --deleted_packages=tensorflow/compiler/mlir/tfrt,tensorflow/compiler/mlir/tfrt/benchmarks,tensorflow/compiler/mlir/tfrt/jit/python_binding,tensorflow/compiler/mlir/tfrt/jit/transforms,tensor",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/608
https://github.com/google/deepvariant/issues/608:488,security,compl,complete,488,"binary creation fails; bazel unable to fetch tf_runtime; I was able to build the docker image last week, but this week the build fails at binary creation, with bazel unable to download tf_runtime. . - Operating system: Ubuntu20.04. - DeepVariant version: 1.4.0. - Building docker image locally. **Steps to reproduce:**. - Command: `docker build .` in source directory (no modifications). - Error trace: . . ```. #16 1484.7 ========== [Mon Jan 30 21:50:56 UTC 2023] Stage 'build-prereq.sh complete' starting. #16 1484.7 Extracting Bazel installation... #16 1487.8 Starting local Bazel server and connecting to it... #16 1489.8 (21:51:01) WARNING: option '--distinct_host_configuration' was expanded to from both option '--enable_platform_specific_config' (source /opt/tensorflow/.bazelrc) and option '--enable_platform_specific_config' (source /opt/tensorflow/.bazelrc). #16 1489.8 (21:51:01) INFO: Options provided by the client:. #16 1489.8 Inherited 'common' options: --isatty=0 --terminal_columns=80. #16 1489.8 (21:51:01) INFO: Reading rc options for 'build' from /opt/tensorflow/.bazelrc:. #16 1489.8 Inherited 'common' options: --experimental_repo_remote_exec. #16 1489.8 (21:51:01) INFO: Reading rc options for 'build' from /opt/tensorflow/.bazelrc:. #16 1489.8 'build' options: --define framework_shared_object=true --java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --host_java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true --deleted_packages=tensorflow/compiler/mlir/tfrt,tensorflow/compiler/mlir/tfrt/benchmarks,tensorflow/compiler/mlir/tfrt/jit/python_binding,tensorflow/compiler/mlir/tfrt/jit/transforms,tensor",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/608
https://github.com/google/deepvariant/issues/608:6544,security,Checksum,Checksum,6544,"l/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/tf_runtime/workspace.bzl:12:20: in repo. #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:113:21: in tf_http_archive. #16 1497.0 Repository rule _tf_http_archive defined at:. #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:66:35: in <toplevel>. #16 1497.0 (21:51:08) WARNING: Download from http://mirror.tensorflow.org/github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException GET returned 404 Not Found. #16 1497.0 (21:51:08) WARNING: Download from https://github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException Checksum was 8383b3247286016e450b0b20e805d26b88ab4638b4e4e3cc4a6923debaf7ad1e but wanted f16fcf09b34e0c7be9389f50652b4b4a14c5a8a96e7e15ad73e8f234d8d09ebe. #16 1497.0 (21:51:08) ERROR: An error occurred during the fetch of repository 'tf_runtime':. #16 1497.0 Traceback (most recent call last):. #16 1497.0 File ""/root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl"", line 53, column 33, in _tf_http_archive_impl. #16 1497.0 ctx.download_and_extract(. #16 1497.0 Error in download_and_extract: java.io.IOException: Error downloading [http://mirror.tensorflow.org/github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz, https://github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz] to /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/tf_runtime/temp12516918929418979294/64c92c8013b557087351c91b5423b6046d10f206.tar.gz: Checksum was 8383b3247286016e450b0b20e805d2",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/608
https://github.com/google/deepvariant/issues/608:7505,security,Checksum,Checksum,7505,"nloader.UnrecoverableHttpException Checksum was 8383b3247286016e450b0b20e805d26b88ab4638b4e4e3cc4a6923debaf7ad1e but wanted f16fcf09b34e0c7be9389f50652b4b4a14c5a8a96e7e15ad73e8f234d8d09ebe. #16 1497.0 (21:51:08) ERROR: An error occurred during the fetch of repository 'tf_runtime':. #16 1497.0 Traceback (most recent call last):. #16 1497.0 File ""/root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl"", line 53, column 33, in _tf_http_archive_impl. #16 1497.0 ctx.download_and_extract(. #16 1497.0 Error in download_and_extract: java.io.IOException: Error downloading [http://mirror.tensorflow.org/github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz, https://github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz] to /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/tf_runtime/temp12516918929418979294/64c92c8013b557087351c91b5423b6046d10f206.tar.gz: Checksum was 8383b3247286016e450b0b20e805d26b88ab4638b4e4e3cc4a6923debaf7ad1e but wanted f16fcf09b34e0c7be9389f50652b4b4a14c5a8a96e7e15ad73e8f234d8d09ebe. #16 1497.1 (21:51:08) INFO: Repository llvm-raw instantiated at:. #16 1497.1 /opt/deepvariant/WORKSPACE:102:14: in <toplevel>. #16 1497.1 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/tensorflow/workspace3.bzl:42:9: in workspace. #16 1497.1 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/llvm/workspace.bzl:10:20: in repo. #16 1497.1 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:113:21: in tf_http_archive. #16 1497.1 Repository rule _tf_http_archive defined at:. #16 1497.1 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:66:35: in <toplevel>. #16 1497.2 (21:51:08) Loading: 0 packages loaded. #16 1497.3 (21:51:08) ERROR: no su",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/608
https://github.com/google/deepvariant/issues/608:8949,security,Checksum,Checksum,8949,"80/external/org_tensorflow/tensorflow/workspace3.bzl:42:9: in workspace. #16 1497.1 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/llvm/workspace.bzl:10:20: in repo. #16 1497.1 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:113:21: in tf_http_archive. #16 1497.1 Repository rule _tf_http_archive defined at:. #16 1497.1 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:66:35: in <toplevel>. #16 1497.2 (21:51:08) Loading: 0 packages loaded. #16 1497.3 (21:51:08) ERROR: no such package '@tf_runtime//': java.io.IOException: Error downloading [http://mirror.tensorflow.org/github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz, https://github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz] to /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/tf_runtime/temp12516918929418979294/64c92c8013b557087351c91b5423b6046d10f206.tar.gz: Checksum was 8383b3247286016e450b0b20e805d26b88ab4638b4e4e3cc4a6923debaf7ad1e but wanted f16fcf09b34e0c7be9389f50652b4b4a14c5a8a96e7e15ad73e8f234d8d09ebe. #16 1497.3 (21:51:09) INFO: Elapsed time: 12.546s. #16 1497.3 (21:51:09) INFO: 0 processes. #16 1497.3 (21:51:09) FAILED: Build did NOT complete successfully (0 packages loaded). #16 1497.3 (21:51:09) FAILED: Build did NOT complete successfully (0 packages loaded). #16 ERROR: executor failed running [/bin/sh -c ./build-prereq.sh && PATH=""${HOME}/bin:${PATH}"" ./build_release_binaries.sh # PATH for bazel]: exit code: 1. ------. > [builder 6/6] RUN ./build-prereq.sh && PATH=""${HOME}/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin"" ./build_release_binaries.sh # PATH for bazel:. ------. executor failed running [/bin/sh -c ./build-prereq.sh && PATH=""${HOME}/bin:${PATH}"" ./build_release_binaries.sh # PATH for bazel]: exit code: 1. ```.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/608
https://github.com/google/deepvariant/issues/608:9240,security,compl,complete,9240,"80/external/org_tensorflow/tensorflow/workspace3.bzl:42:9: in workspace. #16 1497.1 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/llvm/workspace.bzl:10:20: in repo. #16 1497.1 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:113:21: in tf_http_archive. #16 1497.1 Repository rule _tf_http_archive defined at:. #16 1497.1 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:66:35: in <toplevel>. #16 1497.2 (21:51:08) Loading: 0 packages loaded. #16 1497.3 (21:51:08) ERROR: no such package '@tf_runtime//': java.io.IOException: Error downloading [http://mirror.tensorflow.org/github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz, https://github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz] to /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/tf_runtime/temp12516918929418979294/64c92c8013b557087351c91b5423b6046d10f206.tar.gz: Checksum was 8383b3247286016e450b0b20e805d26b88ab4638b4e4e3cc4a6923debaf7ad1e but wanted f16fcf09b34e0c7be9389f50652b4b4a14c5a8a96e7e15ad73e8f234d8d09ebe. #16 1497.3 (21:51:09) INFO: Elapsed time: 12.546s. #16 1497.3 (21:51:09) INFO: 0 processes. #16 1497.3 (21:51:09) FAILED: Build did NOT complete successfully (0 packages loaded). #16 1497.3 (21:51:09) FAILED: Build did NOT complete successfully (0 packages loaded). #16 ERROR: executor failed running [/bin/sh -c ./build-prereq.sh && PATH=""${HOME}/bin:${PATH}"" ./build_release_binaries.sh # PATH for bazel]: exit code: 1. ------. > [builder 6/6] RUN ./build-prereq.sh && PATH=""${HOME}/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin"" ./build_release_binaries.sh # PATH for bazel:. ------. executor failed running [/bin/sh -c ./build-prereq.sh && PATH=""${HOME}/bin:${PATH}"" ./build_release_binaries.sh # PATH for bazel]: exit code: 1. ```.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/608
https://github.com/google/deepvariant/issues/608:9327,security,compl,complete,9327,"80/external/org_tensorflow/tensorflow/workspace3.bzl:42:9: in workspace. #16 1497.1 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/llvm/workspace.bzl:10:20: in repo. #16 1497.1 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:113:21: in tf_http_archive. #16 1497.1 Repository rule _tf_http_archive defined at:. #16 1497.1 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:66:35: in <toplevel>. #16 1497.2 (21:51:08) Loading: 0 packages loaded. #16 1497.3 (21:51:08) ERROR: no such package '@tf_runtime//': java.io.IOException: Error downloading [http://mirror.tensorflow.org/github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz, https://github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz] to /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/tf_runtime/temp12516918929418979294/64c92c8013b557087351c91b5423b6046d10f206.tar.gz: Checksum was 8383b3247286016e450b0b20e805d26b88ab4638b4e4e3cc4a6923debaf7ad1e but wanted f16fcf09b34e0c7be9389f50652b4b4a14c5a8a96e7e15ad73e8f234d8d09ebe. #16 1497.3 (21:51:09) INFO: Elapsed time: 12.546s. #16 1497.3 (21:51:09) INFO: 0 processes. #16 1497.3 (21:51:09) FAILED: Build did NOT complete successfully (0 packages loaded). #16 1497.3 (21:51:09) FAILED: Build did NOT complete successfully (0 packages loaded). #16 ERROR: executor failed running [/bin/sh -c ./build-prereq.sh && PATH=""${HOME}/bin:${PATH}"" ./build_release_binaries.sh # PATH for bazel]: exit code: 1. ------. > [builder 6/6] RUN ./build-prereq.sh && PATH=""${HOME}/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin"" ./build_release_binaries.sh # PATH for bazel:. ------. executor failed running [/bin/sh -c ./build-prereq.sh && PATH=""${HOME}/bin:${PATH}"" ./build_release_binaries.sh # PATH for bazel]: exit code: 1. ```.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/608
https://github.com/google/deepvariant/issues/608:396,testability,trace,trace,396,"binary creation fails; bazel unable to fetch tf_runtime; I was able to build the docker image last week, but this week the build fails at binary creation, with bazel unable to download tf_runtime. . - Operating system: Ubuntu20.04. - DeepVariant version: 1.4.0. - Building docker image locally. **Steps to reproduce:**. - Command: `docker build .` in source directory (no modifications). - Error trace: . . ```. #16 1484.7 ========== [Mon Jan 30 21:50:56 UTC 2023] Stage 'build-prereq.sh complete' starting. #16 1484.7 Extracting Bazel installation... #16 1487.8 Starting local Bazel server and connecting to it... #16 1489.8 (21:51:01) WARNING: option '--distinct_host_configuration' was expanded to from both option '--enable_platform_specific_config' (source /opt/tensorflow/.bazelrc) and option '--enable_platform_specific_config' (source /opt/tensorflow/.bazelrc). #16 1489.8 (21:51:01) INFO: Options provided by the client:. #16 1489.8 Inherited 'common' options: --isatty=0 --terminal_columns=80. #16 1489.8 (21:51:01) INFO: Reading rc options for 'build' from /opt/tensorflow/.bazelrc:. #16 1489.8 Inherited 'common' options: --experimental_repo_remote_exec. #16 1489.8 (21:51:01) INFO: Reading rc options for 'build' from /opt/tensorflow/.bazelrc:. #16 1489.8 'build' options: --define framework_shared_object=true --java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --host_java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true --deleted_packages=tensorflow/compiler/mlir/tfrt,tensorflow/compiler/mlir/tfrt/benchmarks,tensorflow/compiler/mlir/tfrt/jit/python_binding,tensorflow/compiler/mlir/tfrt/jit/transforms,tensor",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/608
https://github.com/google/deepvariant/issues/608:2067,testability,test,tests,2067,"pt/tensorflow/.bazelrc:. #16 1489.8 Inherited 'common' options: --experimental_repo_remote_exec. #16 1489.8 (21:51:01) INFO: Reading rc options for 'build' from /opt/tensorflow/.bazelrc:. #16 1489.8 'build' options: --define framework_shared_object=true --java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --host_java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true --deleted_packages=tensorflow/compiler/mlir/tfrt,tensorflow/compiler/mlir/tfrt/benchmarks,tensorflow/compiler/mlir/tfrt/jit/python_binding,tensorflow/compiler/mlir/tfrt/jit/transforms,tensorflow/compiler/mlir/tfrt/python_tests,tensorflow/compiler/mlir/tfrt/tests,tensorflow/compiler/mlir/tfrt/tests/saved_model,tensorflow/compiler/mlir/tfrt/transforms/lhlo_gpu_to_tfrt_gpu,tensorflow/core/runtime_fallback,tensorflow/core/runtime_fallback/conversion,tensorflow/core/runtime_fallback/kernel,tensorflow/core/runtime_fallback/opdefs,tensorflow/core/runtime_fallback/runtime,tensorflow/core/runtime_fallback/util,tensorflow/core/tfrt/common,tensorflow/core/tfrt/eager,tensorflow/core/tfrt/eager/backends/cpu,tensorflow/core/tfrt/eager/backends/gpu,tensorflow/core/tfrt/eager/core_runtime,tensorflow/core/tfrt/eager/cpp_tests/core_runtime,tensorflow/core/tfrt/fallback,tensorflow/core/tfrt/gpu,tensorflow/core/tfrt/run_handler_thread_pool,tensorflow/core/tfrt/runtime,tensorflow/core/tfrt/saved_model,tensorflow/core/tfrt/saved_model/tests,tensorflow/core/tfrt/tpu,tensorflow/core/tfrt/utils. #16 1489.8 (21:51:01) INFO: Reading rc options for 'build' from /opt/tensorflow/.tf_configure.bazelrc:. #16 1489.8 'build' options: --action_env PYTHON_BIN_PATH=/usr/local/",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/608
https://github.com/google/deepvariant/issues/608:2103,testability,test,tests,2103,"Inherited 'common' options: --experimental_repo_remote_exec. #16 1489.8 (21:51:01) INFO: Reading rc options for 'build' from /opt/tensorflow/.bazelrc:. #16 1489.8 'build' options: --define framework_shared_object=true --java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --host_java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true --deleted_packages=tensorflow/compiler/mlir/tfrt,tensorflow/compiler/mlir/tfrt/benchmarks,tensorflow/compiler/mlir/tfrt/jit/python_binding,tensorflow/compiler/mlir/tfrt/jit/transforms,tensorflow/compiler/mlir/tfrt/python_tests,tensorflow/compiler/mlir/tfrt/tests,tensorflow/compiler/mlir/tfrt/tests/saved_model,tensorflow/compiler/mlir/tfrt/transforms/lhlo_gpu_to_tfrt_gpu,tensorflow/core/runtime_fallback,tensorflow/core/runtime_fallback/conversion,tensorflow/core/runtime_fallback/kernel,tensorflow/core/runtime_fallback/opdefs,tensorflow/core/runtime_fallback/runtime,tensorflow/core/runtime_fallback/util,tensorflow/core/tfrt/common,tensorflow/core/tfrt/eager,tensorflow/core/tfrt/eager/backends/cpu,tensorflow/core/tfrt/eager/backends/gpu,tensorflow/core/tfrt/eager/core_runtime,tensorflow/core/tfrt/eager/cpp_tests/core_runtime,tensorflow/core/tfrt/fallback,tensorflow/core/tfrt/gpu,tensorflow/core/tfrt/run_handler_thread_pool,tensorflow/core/tfrt/runtime,tensorflow/core/tfrt/saved_model,tensorflow/core/tfrt/saved_model/tests,tensorflow/core/tfrt/tpu,tensorflow/core/tfrt/utils. #16 1489.8 (21:51:01) INFO: Reading rc options for 'build' from /opt/tensorflow/.tf_configure.bazelrc:. #16 1489.8 'build' options: --action_env PYTHON_BIN_PATH=/usr/local/bin/python3 --action_env PYTHON_LIB_",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/608
https://github.com/google/deepvariant/issues/608:2839,testability,test,tests,2839,"mpiler/mlir/tfrt,tensorflow/compiler/mlir/tfrt/benchmarks,tensorflow/compiler/mlir/tfrt/jit/python_binding,tensorflow/compiler/mlir/tfrt/jit/transforms,tensorflow/compiler/mlir/tfrt/python_tests,tensorflow/compiler/mlir/tfrt/tests,tensorflow/compiler/mlir/tfrt/tests/saved_model,tensorflow/compiler/mlir/tfrt/transforms/lhlo_gpu_to_tfrt_gpu,tensorflow/core/runtime_fallback,tensorflow/core/runtime_fallback/conversion,tensorflow/core/runtime_fallback/kernel,tensorflow/core/runtime_fallback/opdefs,tensorflow/core/runtime_fallback/runtime,tensorflow/core/runtime_fallback/util,tensorflow/core/tfrt/common,tensorflow/core/tfrt/eager,tensorflow/core/tfrt/eager/backends/cpu,tensorflow/core/tfrt/eager/backends/gpu,tensorflow/core/tfrt/eager/core_runtime,tensorflow/core/tfrt/eager/cpp_tests/core_runtime,tensorflow/core/tfrt/fallback,tensorflow/core/tfrt/gpu,tensorflow/core/tfrt/run_handler_thread_pool,tensorflow/core/tfrt/runtime,tensorflow/core/tfrt/saved_model,tensorflow/core/tfrt/saved_model/tests,tensorflow/core/tfrt/tpu,tensorflow/core/tfrt/utils. #16 1489.8 (21:51:01) INFO: Reading rc options for 'build' from /opt/tensorflow/.tf_configure.bazelrc:. #16 1489.8 'build' options: --action_env PYTHON_BIN_PATH=/usr/local/bin/python3 --action_env PYTHON_LIB_PATH=/usr/local/lib/python3.8/dist-packages --python_path=/usr/local/bin/python3. #16 1489.8 (21:51:01) INFO: Reading rc options for 'build' from /opt/deepvariant/.bazelrc:. #16 1489.8 'build' options: --jobs 128 --config=monolithic --show_timestamps --verbose_failures --define=use_fast_cpp_protos=true --copt=-Wno-maybe-uninitialized --copt=-Wno-unused-function --cxxopt=-std=c++17 --python_top=//:deepvariant_python_runtime --incompatible_use_python_toolchains=false. #16 1489.8 (21:51:01) INFO: Found applicable config definition build:short_logs in file /opt/tensorflow/.bazelrc: --output_filter=DONT_MATCH_ANYTHING. #16 1489.8 (21:51:01) INFO: Found applicable config definition build:v2 in file /opt/tensorflow/.bazelrc: --define",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/608
https://github.com/google/deepvariant/issues/608:6803,testability,Trace,Traceback,6803,"tp_archive. #16 1497.0 Repository rule _tf_http_archive defined at:. #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:66:35: in <toplevel>. #16 1497.0 (21:51:08) WARNING: Download from http://mirror.tensorflow.org/github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException GET returned 404 Not Found. #16 1497.0 (21:51:08) WARNING: Download from https://github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException Checksum was 8383b3247286016e450b0b20e805d26b88ab4638b4e4e3cc4a6923debaf7ad1e but wanted f16fcf09b34e0c7be9389f50652b4b4a14c5a8a96e7e15ad73e8f234d8d09ebe. #16 1497.0 (21:51:08) ERROR: An error occurred during the fetch of repository 'tf_runtime':. #16 1497.0 Traceback (most recent call last):. #16 1497.0 File ""/root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl"", line 53, column 33, in _tf_http_archive_impl. #16 1497.0 ctx.download_and_extract(. #16 1497.0 Error in download_and_extract: java.io.IOException: Error downloading [http://mirror.tensorflow.org/github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz, https://github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz] to /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/tf_runtime/temp12516918929418979294/64c92c8013b557087351c91b5423b6046d10f206.tar.gz: Checksum was 8383b3247286016e450b0b20e805d26b88ab4638b4e4e3cc4a6923debaf7ad1e but wanted f16fcf09b34e0c7be9389f50652b4b4a14c5a8a96e7e15ad73e8f234d8d09ebe. #16 1497.1 (21:51:08) INFO: Repository llvm-raw instantiated at:. #16 1497.1 /opt/deepvariant/WORKSPACE:102:14: in <toplevel>. #16 1497.1 /root/.cac",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/608
https://github.com/google/deepvariant/issues/608:322,usability,Command,Command,322,"binary creation fails; bazel unable to fetch tf_runtime; I was able to build the docker image last week, but this week the build fails at binary creation, with bazel unable to download tf_runtime. . - Operating system: Ubuntu20.04. - DeepVariant version: 1.4.0. - Building docker image locally. **Steps to reproduce:**. - Command: `docker build .` in source directory (no modifications). - Error trace: . . ```. #16 1484.7 ========== [Mon Jan 30 21:50:56 UTC 2023] Stage 'build-prereq.sh complete' starting. #16 1484.7 Extracting Bazel installation... #16 1487.8 Starting local Bazel server and connecting to it... #16 1489.8 (21:51:01) WARNING: option '--distinct_host_configuration' was expanded to from both option '--enable_platform_specific_config' (source /opt/tensorflow/.bazelrc) and option '--enable_platform_specific_config' (source /opt/tensorflow/.bazelrc). #16 1489.8 (21:51:01) INFO: Options provided by the client:. #16 1489.8 Inherited 'common' options: --isatty=0 --terminal_columns=80. #16 1489.8 (21:51:01) INFO: Reading rc options for 'build' from /opt/tensorflow/.bazelrc:. #16 1489.8 Inherited 'common' options: --experimental_repo_remote_exec. #16 1489.8 (21:51:01) INFO: Reading rc options for 'build' from /opt/tensorflow/.bazelrc:. #16 1489.8 'build' options: --define framework_shared_object=true --java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --host_java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true --deleted_packages=tensorflow/compiler/mlir/tfrt,tensorflow/compiler/mlir/tfrt/benchmarks,tensorflow/compiler/mlir/tfrt/jit/python_binding,tensorflow/compiler/mlir/tfrt/jit/transforms,tensor",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/608
https://github.com/google/deepvariant/issues/608:390,usability,Error,Error,390,"binary creation fails; bazel unable to fetch tf_runtime; I was able to build the docker image last week, but this week the build fails at binary creation, with bazel unable to download tf_runtime. . - Operating system: Ubuntu20.04. - DeepVariant version: 1.4.0. - Building docker image locally. **Steps to reproduce:**. - Command: `docker build .` in source directory (no modifications). - Error trace: . . ```. #16 1484.7 ========== [Mon Jan 30 21:50:56 UTC 2023] Stage 'build-prereq.sh complete' starting. #16 1484.7 Extracting Bazel installation... #16 1487.8 Starting local Bazel server and connecting to it... #16 1489.8 (21:51:01) WARNING: option '--distinct_host_configuration' was expanded to from both option '--enable_platform_specific_config' (source /opt/tensorflow/.bazelrc) and option '--enable_platform_specific_config' (source /opt/tensorflow/.bazelrc). #16 1489.8 (21:51:01) INFO: Options provided by the client:. #16 1489.8 Inherited 'common' options: --isatty=0 --terminal_columns=80. #16 1489.8 (21:51:01) INFO: Reading rc options for 'build' from /opt/tensorflow/.bazelrc:. #16 1489.8 Inherited 'common' options: --experimental_repo_remote_exec. #16 1489.8 (21:51:01) INFO: Reading rc options for 'build' from /opt/tensorflow/.bazelrc:. #16 1489.8 'build' options: --define framework_shared_object=true --java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --host_java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true --deleted_packages=tensorflow/compiler/mlir/tfrt,tensorflow/compiler/mlir/tfrt/benchmarks,tensorflow/compiler/mlir/tfrt/jit/python_binding,tensorflow/compiler/mlir/tfrt/jit/transforms,tensor",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/608
https://github.com/google/deepvariant/issues/608:1357,usability,tool,toolchains,1357,"ctory (no modifications). - Error trace: . . ```. #16 1484.7 ========== [Mon Jan 30 21:50:56 UTC 2023] Stage 'build-prereq.sh complete' starting. #16 1484.7 Extracting Bazel installation... #16 1487.8 Starting local Bazel server and connecting to it... #16 1489.8 (21:51:01) WARNING: option '--distinct_host_configuration' was expanded to from both option '--enable_platform_specific_config' (source /opt/tensorflow/.bazelrc) and option '--enable_platform_specific_config' (source /opt/tensorflow/.bazelrc). #16 1489.8 (21:51:01) INFO: Options provided by the client:. #16 1489.8 Inherited 'common' options: --isatty=0 --terminal_columns=80. #16 1489.8 (21:51:01) INFO: Reading rc options for 'build' from /opt/tensorflow/.bazelrc:. #16 1489.8 Inherited 'common' options: --experimental_repo_remote_exec. #16 1489.8 (21:51:01) INFO: Reading rc options for 'build' from /opt/tensorflow/.bazelrc:. #16 1489.8 'build' options: --define framework_shared_object=true --java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --host_java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true --deleted_packages=tensorflow/compiler/mlir/tfrt,tensorflow/compiler/mlir/tfrt/benchmarks,tensorflow/compiler/mlir/tfrt/jit/python_binding,tensorflow/compiler/mlir/tfrt/jit/transforms,tensorflow/compiler/mlir/tfrt/python_tests,tensorflow/compiler/mlir/tfrt/tests,tensorflow/compiler/mlir/tfrt/tests/saved_model,tensorflow/compiler/mlir/tfrt/transforms/lhlo_gpu_to_tfrt_gpu,tensorflow/core/runtime_fallback,tensorflow/core/runtime_fallback/conversion,tensorflow/core/runtime_fallback/kernel,tensorflow/core/runtime_fallback/opdefs,tensorflow/core/runtim",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/608
https://github.com/google/deepvariant/issues/608:1429,usability,tool,toolchains,1429,"[Mon Jan 30 21:50:56 UTC 2023] Stage 'build-prereq.sh complete' starting. #16 1484.7 Extracting Bazel installation... #16 1487.8 Starting local Bazel server and connecting to it... #16 1489.8 (21:51:01) WARNING: option '--distinct_host_configuration' was expanded to from both option '--enable_platform_specific_config' (source /opt/tensorflow/.bazelrc) and option '--enable_platform_specific_config' (source /opt/tensorflow/.bazelrc). #16 1489.8 (21:51:01) INFO: Options provided by the client:. #16 1489.8 Inherited 'common' options: --isatty=0 --terminal_columns=80. #16 1489.8 (21:51:01) INFO: Reading rc options for 'build' from /opt/tensorflow/.bazelrc:. #16 1489.8 Inherited 'common' options: --experimental_repo_remote_exec. #16 1489.8 (21:51:01) INFO: Reading rc options for 'build' from /opt/tensorflow/.bazelrc:. #16 1489.8 'build' options: --define framework_shared_object=true --java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --host_java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true --deleted_packages=tensorflow/compiler/mlir/tfrt,tensorflow/compiler/mlir/tfrt/benchmarks,tensorflow/compiler/mlir/tfrt/jit/python_binding,tensorflow/compiler/mlir/tfrt/jit/transforms,tensorflow/compiler/mlir/tfrt/python_tests,tensorflow/compiler/mlir/tfrt/tests,tensorflow/compiler/mlir/tfrt/tests/saved_model,tensorflow/compiler/mlir/tfrt/transforms/lhlo_gpu_to_tfrt_gpu,tensorflow/core/runtime_fallback,tensorflow/core/runtime_fallback/conversion,tensorflow/core/runtime_fallback/kernel,tensorflow/core/runtime_fallback/opdefs,tensorflow/core/runtime_fallback/runtime,tensorflow/core/runtime_fallback/util,tensorflow/core",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/608
https://github.com/google/deepvariant/issues/608:6721,usability,ERROR,ERROR,6721,"c1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:113:21: in tf_http_archive. #16 1497.0 Repository rule _tf_http_archive defined at:. #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:66:35: in <toplevel>. #16 1497.0 (21:51:08) WARNING: Download from http://mirror.tensorflow.org/github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException GET returned 404 Not Found. #16 1497.0 (21:51:08) WARNING: Download from https://github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException Checksum was 8383b3247286016e450b0b20e805d26b88ab4638b4e4e3cc4a6923debaf7ad1e but wanted f16fcf09b34e0c7be9389f50652b4b4a14c5a8a96e7e15ad73e8f234d8d09ebe. #16 1497.0 (21:51:08) ERROR: An error occurred during the fetch of repository 'tf_runtime':. #16 1497.0 Traceback (most recent call last):. #16 1497.0 File ""/root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl"", line 53, column 33, in _tf_http_archive_impl. #16 1497.0 ctx.download_and_extract(. #16 1497.0 Error in download_and_extract: java.io.IOException: Error downloading [http://mirror.tensorflow.org/github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz, https://github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz] to /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/tf_runtime/temp12516918929418979294/64c92c8013b557087351c91b5423b6046d10f206.tar.gz: Checksum was 8383b3247286016e450b0b20e805d26b88ab4638b4e4e3cc4a6923debaf7ad1e but wanted f16fcf09b34e0c7be9389f50652b4b4a14c5a8a96e7e15ad73e8f234d8d09ebe. #16 1497.1 (21:51:08) INFO: Repository llvm-raw instantiated at:",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/608
https://github.com/google/deepvariant/issues/608:6731,usability,error,error,6731,"3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:113:21: in tf_http_archive. #16 1497.0 Repository rule _tf_http_archive defined at:. #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:66:35: in <toplevel>. #16 1497.0 (21:51:08) WARNING: Download from http://mirror.tensorflow.org/github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException GET returned 404 Not Found. #16 1497.0 (21:51:08) WARNING: Download from https://github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException Checksum was 8383b3247286016e450b0b20e805d26b88ab4638b4e4e3cc4a6923debaf7ad1e but wanted f16fcf09b34e0c7be9389f50652b4b4a14c5a8a96e7e15ad73e8f234d8d09ebe. #16 1497.0 (21:51:08) ERROR: An error occurred during the fetch of repository 'tf_runtime':. #16 1497.0 Traceback (most recent call last):. #16 1497.0 File ""/root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl"", line 53, column 33, in _tf_http_archive_impl. #16 1497.0 ctx.download_and_extract(. #16 1497.0 Error in download_and_extract: java.io.IOException: Error downloading [http://mirror.tensorflow.org/github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz, https://github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz] to /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/tf_runtime/temp12516918929418979294/64c92c8013b557087351c91b5423b6046d10f206.tar.gz: Checksum was 8383b3247286016e450b0b20e805d26b88ab4638b4e4e3cc4a6923debaf7ad1e but wanted f16fcf09b34e0c7be9389f50652b4b4a14c5a8a96e7e15ad73e8f234d8d09ebe. #16 1497.1 (21:51:08) INFO: Repository llvm-raw instantiated at:. #16 1497",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/608
https://github.com/google/deepvariant/issues/608:7062,usability,Error,Error,7062,"ttp://mirror.tensorflow.org/github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException GET returned 404 Not Found. #16 1497.0 (21:51:08) WARNING: Download from https://github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException Checksum was 8383b3247286016e450b0b20e805d26b88ab4638b4e4e3cc4a6923debaf7ad1e but wanted f16fcf09b34e0c7be9389f50652b4b4a14c5a8a96e7e15ad73e8f234d8d09ebe. #16 1497.0 (21:51:08) ERROR: An error occurred during the fetch of repository 'tf_runtime':. #16 1497.0 Traceback (most recent call last):. #16 1497.0 File ""/root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl"", line 53, column 33, in _tf_http_archive_impl. #16 1497.0 ctx.download_and_extract(. #16 1497.0 Error in download_and_extract: java.io.IOException: Error downloading [http://mirror.tensorflow.org/github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz, https://github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz] to /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/tf_runtime/temp12516918929418979294/64c92c8013b557087351c91b5423b6046d10f206.tar.gz: Checksum was 8383b3247286016e450b0b20e805d26b88ab4638b4e4e3cc4a6923debaf7ad1e but wanted f16fcf09b34e0c7be9389f50652b4b4a14c5a8a96e7e15ad73e8f234d8d09ebe. #16 1497.1 (21:51:08) INFO: Repository llvm-raw instantiated at:. #16 1497.1 /opt/deepvariant/WORKSPACE:102:14: in <toplevel>. #16 1497.1 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/tensorflow/workspace3.bzl:42:9: in workspace. #16 1497.1 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/llvm/workspace.bzl:10:",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/608
https://github.com/google/deepvariant/issues/608:7114,usability,Error,Error,7114,"ntime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException GET returned 404 Not Found. #16 1497.0 (21:51:08) WARNING: Download from https://github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException Checksum was 8383b3247286016e450b0b20e805d26b88ab4638b4e4e3cc4a6923debaf7ad1e but wanted f16fcf09b34e0c7be9389f50652b4b4a14c5a8a96e7e15ad73e8f234d8d09ebe. #16 1497.0 (21:51:08) ERROR: An error occurred during the fetch of repository 'tf_runtime':. #16 1497.0 Traceback (most recent call last):. #16 1497.0 File ""/root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl"", line 53, column 33, in _tf_http_archive_impl. #16 1497.0 ctx.download_and_extract(. #16 1497.0 Error in download_and_extract: java.io.IOException: Error downloading [http://mirror.tensorflow.org/github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz, https://github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz] to /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/tf_runtime/temp12516918929418979294/64c92c8013b557087351c91b5423b6046d10f206.tar.gz: Checksum was 8383b3247286016e450b0b20e805d26b88ab4638b4e4e3cc4a6923debaf7ad1e but wanted f16fcf09b34e0c7be9389f50652b4b4a14c5a8a96e7e15ad73e8f234d8d09ebe. #16 1497.1 (21:51:08) INFO: Repository llvm-raw instantiated at:. #16 1497.1 /opt/deepvariant/WORKSPACE:102:14: in <toplevel>. #16 1497.1 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/tensorflow/workspace3.bzl:42:9: in workspace. #16 1497.1 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/llvm/workspace.bzl:10:20: in repo. #16 1497.1 /root/.cache/bazel/_bazel_ro",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/608
https://github.com/google/deepvariant/issues/608:8497,usability,ERROR,ERROR,8497,".gz: Checksum was 8383b3247286016e450b0b20e805d26b88ab4638b4e4e3cc4a6923debaf7ad1e but wanted f16fcf09b34e0c7be9389f50652b4b4a14c5a8a96e7e15ad73e8f234d8d09ebe. #16 1497.1 (21:51:08) INFO: Repository llvm-raw instantiated at:. #16 1497.1 /opt/deepvariant/WORKSPACE:102:14: in <toplevel>. #16 1497.1 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/tensorflow/workspace3.bzl:42:9: in workspace. #16 1497.1 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/llvm/workspace.bzl:10:20: in repo. #16 1497.1 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:113:21: in tf_http_archive. #16 1497.1 Repository rule _tf_http_archive defined at:. #16 1497.1 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:66:35: in <toplevel>. #16 1497.2 (21:51:08) Loading: 0 packages loaded. #16 1497.3 (21:51:08) ERROR: no such package '@tf_runtime//': java.io.IOException: Error downloading [http://mirror.tensorflow.org/github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz, https://github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz] to /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/tf_runtime/temp12516918929418979294/64c92c8013b557087351c91b5423b6046d10f206.tar.gz: Checksum was 8383b3247286016e450b0b20e805d26b88ab4638b4e4e3cc4a6923debaf7ad1e but wanted f16fcf09b34e0c7be9389f50652b4b4a14c5a8a96e7e15ad73e8f234d8d09ebe. #16 1497.3 (21:51:09) INFO: Elapsed time: 12.546s. #16 1497.3 (21:51:09) INFO: 0 processes. #16 1497.3 (21:51:09) FAILED: Build did NOT complete successfully (0 packages loaded). #16 1497.3 (21:51:09) FAILED: Build did NOT complete successfully (0 packages loaded). #16 ERROR: executor failed running [/bin/sh -c ./build-prereq.sh && PATH=""${HOME}/bin:${PATH}"" ./build_release_binaries.sh # PATH ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/608
https://github.com/google/deepvariant/issues/608:8558,usability,Error,Error,8558,"4e3cc4a6923debaf7ad1e but wanted f16fcf09b34e0c7be9389f50652b4b4a14c5a8a96e7e15ad73e8f234d8d09ebe. #16 1497.1 (21:51:08) INFO: Repository llvm-raw instantiated at:. #16 1497.1 /opt/deepvariant/WORKSPACE:102:14: in <toplevel>. #16 1497.1 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/tensorflow/workspace3.bzl:42:9: in workspace. #16 1497.1 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/llvm/workspace.bzl:10:20: in repo. #16 1497.1 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:113:21: in tf_http_archive. #16 1497.1 Repository rule _tf_http_archive defined at:. #16 1497.1 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:66:35: in <toplevel>. #16 1497.2 (21:51:08) Loading: 0 packages loaded. #16 1497.3 (21:51:08) ERROR: no such package '@tf_runtime//': java.io.IOException: Error downloading [http://mirror.tensorflow.org/github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz, https://github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz] to /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/tf_runtime/temp12516918929418979294/64c92c8013b557087351c91b5423b6046d10f206.tar.gz: Checksum was 8383b3247286016e450b0b20e805d26b88ab4638b4e4e3cc4a6923debaf7ad1e but wanted f16fcf09b34e0c7be9389f50652b4b4a14c5a8a96e7e15ad73e8f234d8d09ebe. #16 1497.3 (21:51:09) INFO: Elapsed time: 12.546s. #16 1497.3 (21:51:09) INFO: 0 processes. #16 1497.3 (21:51:09) FAILED: Build did NOT complete successfully (0 packages loaded). #16 1497.3 (21:51:09) FAILED: Build did NOT complete successfully (0 packages loaded). #16 ERROR: executor failed running [/bin/sh -c ./build-prereq.sh && PATH=""${HOME}/bin:${PATH}"" ./build_release_binaries.sh # PATH for bazel]: exit code: 1. ------. > [builder 6/6] RUN ./build",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/608
https://github.com/google/deepvariant/issues/608:9374,usability,ERROR,ERROR,9374,"80/external/org_tensorflow/tensorflow/workspace3.bzl:42:9: in workspace. #16 1497.1 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/llvm/workspace.bzl:10:20: in repo. #16 1497.1 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:113:21: in tf_http_archive. #16 1497.1 Repository rule _tf_http_archive defined at:. #16 1497.1 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:66:35: in <toplevel>. #16 1497.2 (21:51:08) Loading: 0 packages loaded. #16 1497.3 (21:51:08) ERROR: no such package '@tf_runtime//': java.io.IOException: Error downloading [http://mirror.tensorflow.org/github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz, https://github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz] to /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/tf_runtime/temp12516918929418979294/64c92c8013b557087351c91b5423b6046d10f206.tar.gz: Checksum was 8383b3247286016e450b0b20e805d26b88ab4638b4e4e3cc4a6923debaf7ad1e but wanted f16fcf09b34e0c7be9389f50652b4b4a14c5a8a96e7e15ad73e8f234d8d09ebe. #16 1497.3 (21:51:09) INFO: Elapsed time: 12.546s. #16 1497.3 (21:51:09) INFO: 0 processes. #16 1497.3 (21:51:09) FAILED: Build did NOT complete successfully (0 packages loaded). #16 1497.3 (21:51:09) FAILED: Build did NOT complete successfully (0 packages loaded). #16 ERROR: executor failed running [/bin/sh -c ./build-prereq.sh && PATH=""${HOME}/bin:${PATH}"" ./build_release_binaries.sh # PATH for bazel]: exit code: 1. ------. > [builder 6/6] RUN ./build-prereq.sh && PATH=""${HOME}/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin"" ./build_release_binaries.sh # PATH for bazel:. ------. executor failed running [/bin/sh -c ./build-prereq.sh && PATH=""${HOME}/bin:${PATH}"" ./build_release_binaries.sh # PATH for bazel]: exit code: 1. ```.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/608
https://github.com/google/deepvariant/issues/609:683,deployability,version,version,683,"DeepTrio for quartet families; Hi,. I'm running DeepTrio for ~100 samples of WGS data from quartet families. Is it possible to specify such a quartet (or larger) families to DeepTrio? Currently it seems that the algorithm will call the variants for the parents multiple times when each trio is provided separately, which is not optimal. I also have a question about de novo variant calling, I understand from the article that DeepTrio is not as sensitive to de novos as some other tools (as it will be less likely to call a variant in a child that is not present in either parent). What is currently your recommended practice regarding de novo variant calling? I'm running DeepTrio (version 1.4.0, specified as deeptrio-latest) via Docker (version 20.10.6) using Singularity. Many thanks! Karoliina",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/609
https://github.com/google/deepvariant/issues/609:740,deployability,version,version,740,"DeepTrio for quartet families; Hi,. I'm running DeepTrio for ~100 samples of WGS data from quartet families. Is it possible to specify such a quartet (or larger) families to DeepTrio? Currently it seems that the algorithm will call the variants for the parents multiple times when each trio is provided separately, which is not optimal. I also have a question about de novo variant calling, I understand from the article that DeepTrio is not as sensitive to de novos as some other tools (as it will be less likely to call a variant in a child that is not present in either parent). What is currently your recommended practice regarding de novo variant calling? I'm running DeepTrio (version 1.4.0, specified as deeptrio-latest) via Docker (version 20.10.6) using Singularity. Many thanks! Karoliina",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/609
https://github.com/google/deepvariant/issues/609:184,energy efficiency,Current,Currently,184,"DeepTrio for quartet families; Hi,. I'm running DeepTrio for ~100 samples of WGS data from quartet families. Is it possible to specify such a quartet (or larger) families to DeepTrio? Currently it seems that the algorithm will call the variants for the parents multiple times when each trio is provided separately, which is not optimal. I also have a question about de novo variant calling, I understand from the article that DeepTrio is not as sensitive to de novos as some other tools (as it will be less likely to call a variant in a child that is not present in either parent). What is currently your recommended practice regarding de novo variant calling? I'm running DeepTrio (version 1.4.0, specified as deeptrio-latest) via Docker (version 20.10.6) using Singularity. Many thanks! Karoliina",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/609
https://github.com/google/deepvariant/issues/609:328,energy efficiency,optim,optimal,328,"DeepTrio for quartet families; Hi,. I'm running DeepTrio for ~100 samples of WGS data from quartet families. Is it possible to specify such a quartet (or larger) families to DeepTrio? Currently it seems that the algorithm will call the variants for the parents multiple times when each trio is provided separately, which is not optimal. I also have a question about de novo variant calling, I understand from the article that DeepTrio is not as sensitive to de novos as some other tools (as it will be less likely to call a variant in a child that is not present in either parent). What is currently your recommended practice regarding de novo variant calling? I'm running DeepTrio (version 1.4.0, specified as deeptrio-latest) via Docker (version 20.10.6) using Singularity. Many thanks! Karoliina",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/609
https://github.com/google/deepvariant/issues/609:590,energy efficiency,current,currently,590,"DeepTrio for quartet families; Hi,. I'm running DeepTrio for ~100 samples of WGS data from quartet families. Is it possible to specify such a quartet (or larger) families to DeepTrio? Currently it seems that the algorithm will call the variants for the parents multiple times when each trio is provided separately, which is not optimal. I also have a question about de novo variant calling, I understand from the article that DeepTrio is not as sensitive to de novos as some other tools (as it will be less likely to call a variant in a child that is not present in either parent). What is currently your recommended practice regarding de novo variant calling? I'm running DeepTrio (version 1.4.0, specified as deeptrio-latest) via Docker (version 20.10.6) using Singularity. Many thanks! Karoliina",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/609
https://github.com/google/deepvariant/issues/609:683,integrability,version,version,683,"DeepTrio for quartet families; Hi,. I'm running DeepTrio for ~100 samples of WGS data from quartet families. Is it possible to specify such a quartet (or larger) families to DeepTrio? Currently it seems that the algorithm will call the variants for the parents multiple times when each trio is provided separately, which is not optimal. I also have a question about de novo variant calling, I understand from the article that DeepTrio is not as sensitive to de novos as some other tools (as it will be less likely to call a variant in a child that is not present in either parent). What is currently your recommended practice regarding de novo variant calling? I'm running DeepTrio (version 1.4.0, specified as deeptrio-latest) via Docker (version 20.10.6) using Singularity. Many thanks! Karoliina",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/609
https://github.com/google/deepvariant/issues/609:740,integrability,version,version,740,"DeepTrio for quartet families; Hi,. I'm running DeepTrio for ~100 samples of WGS data from quartet families. Is it possible to specify such a quartet (or larger) families to DeepTrio? Currently it seems that the algorithm will call the variants for the parents multiple times when each trio is provided separately, which is not optimal. I also have a question about de novo variant calling, I understand from the article that DeepTrio is not as sensitive to de novos as some other tools (as it will be less likely to call a variant in a child that is not present in either parent). What is currently your recommended practice regarding de novo variant calling? I'm running DeepTrio (version 1.4.0, specified as deeptrio-latest) via Docker (version 20.10.6) using Singularity. Many thanks! Karoliina",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/609
https://github.com/google/deepvariant/issues/609:127,interoperability,specif,specify,127,"DeepTrio for quartet families; Hi,. I'm running DeepTrio for ~100 samples of WGS data from quartet families. Is it possible to specify such a quartet (or larger) families to DeepTrio? Currently it seems that the algorithm will call the variants for the parents multiple times when each trio is provided separately, which is not optimal. I also have a question about de novo variant calling, I understand from the article that DeepTrio is not as sensitive to de novos as some other tools (as it will be less likely to call a variant in a child that is not present in either parent). What is currently your recommended practice regarding de novo variant calling? I'm running DeepTrio (version 1.4.0, specified as deeptrio-latest) via Docker (version 20.10.6) using Singularity. Many thanks! Karoliina",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/609
https://github.com/google/deepvariant/issues/609:698,interoperability,specif,specified,698,"DeepTrio for quartet families; Hi,. I'm running DeepTrio for ~100 samples of WGS data from quartet families. Is it possible to specify such a quartet (or larger) families to DeepTrio? Currently it seems that the algorithm will call the variants for the parents multiple times when each trio is provided separately, which is not optimal. I also have a question about de novo variant calling, I understand from the article that DeepTrio is not as sensitive to de novos as some other tools (as it will be less likely to call a variant in a child that is not present in either parent). What is currently your recommended practice regarding de novo variant calling? I'm running DeepTrio (version 1.4.0, specified as deeptrio-latest) via Docker (version 20.10.6) using Singularity. Many thanks! Karoliina",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/609
https://github.com/google/deepvariant/issues/609:683,modifiability,version,version,683,"DeepTrio for quartet families; Hi,. I'm running DeepTrio for ~100 samples of WGS data from quartet families. Is it possible to specify such a quartet (or larger) families to DeepTrio? Currently it seems that the algorithm will call the variants for the parents multiple times when each trio is provided separately, which is not optimal. I also have a question about de novo variant calling, I understand from the article that DeepTrio is not as sensitive to de novos as some other tools (as it will be less likely to call a variant in a child that is not present in either parent). What is currently your recommended practice regarding de novo variant calling? I'm running DeepTrio (version 1.4.0, specified as deeptrio-latest) via Docker (version 20.10.6) using Singularity. Many thanks! Karoliina",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/609
https://github.com/google/deepvariant/issues/609:740,modifiability,version,version,740,"DeepTrio for quartet families; Hi,. I'm running DeepTrio for ~100 samples of WGS data from quartet families. Is it possible to specify such a quartet (or larger) families to DeepTrio? Currently it seems that the algorithm will call the variants for the parents multiple times when each trio is provided separately, which is not optimal. I also have a question about de novo variant calling, I understand from the article that DeepTrio is not as sensitive to de novos as some other tools (as it will be less likely to call a variant in a child that is not present in either parent). What is currently your recommended practice regarding de novo variant calling? I'm running DeepTrio (version 1.4.0, specified as deeptrio-latest) via Docker (version 20.10.6) using Singularity. Many thanks! Karoliina",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/609
https://github.com/google/deepvariant/issues/609:270,performance,time,times,270,"DeepTrio for quartet families; Hi,. I'm running DeepTrio for ~100 samples of WGS data from quartet families. Is it possible to specify such a quartet (or larger) families to DeepTrio? Currently it seems that the algorithm will call the variants for the parents multiple times when each trio is provided separately, which is not optimal. I also have a question about de novo variant calling, I understand from the article that DeepTrio is not as sensitive to de novos as some other tools (as it will be less likely to call a variant in a child that is not present in either parent). What is currently your recommended practice regarding de novo variant calling? I'm running DeepTrio (version 1.4.0, specified as deeptrio-latest) via Docker (version 20.10.6) using Singularity. Many thanks! Karoliina",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/609
https://github.com/google/deepvariant/issues/609:617,reliability,pra,practice,617,"DeepTrio for quartet families; Hi,. I'm running DeepTrio for ~100 samples of WGS data from quartet families. Is it possible to specify such a quartet (or larger) families to DeepTrio? Currently it seems that the algorithm will call the variants for the parents multiple times when each trio is provided separately, which is not optimal. I also have a question about de novo variant calling, I understand from the article that DeepTrio is not as sensitive to de novos as some other tools (as it will be less likely to call a variant in a child that is not present in either parent). What is currently your recommended practice regarding de novo variant calling? I'm running DeepTrio (version 1.4.0, specified as deeptrio-latest) via Docker (version 20.10.6) using Singularity. Many thanks! Karoliina",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/609
https://github.com/google/deepvariant/issues/609:393,testability,understand,understand,393,"DeepTrio for quartet families; Hi,. I'm running DeepTrio for ~100 samples of WGS data from quartet families. Is it possible to specify such a quartet (or larger) families to DeepTrio? Currently it seems that the algorithm will call the variants for the parents multiple times when each trio is provided separately, which is not optimal. I also have a question about de novo variant calling, I understand from the article that DeepTrio is not as sensitive to de novos as some other tools (as it will be less likely to call a variant in a child that is not present in either parent). What is currently your recommended practice regarding de novo variant calling? I'm running DeepTrio (version 1.4.0, specified as deeptrio-latest) via Docker (version 20.10.6) using Singularity. Many thanks! Karoliina",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/609
https://github.com/google/deepvariant/issues/609:481,usability,tool,tools,481,"DeepTrio for quartet families; Hi,. I'm running DeepTrio for ~100 samples of WGS data from quartet families. Is it possible to specify such a quartet (or larger) families to DeepTrio? Currently it seems that the algorithm will call the variants for the parents multiple times when each trio is provided separately, which is not optimal. I also have a question about de novo variant calling, I understand from the article that DeepTrio is not as sensitive to de novos as some other tools (as it will be less likely to call a variant in a child that is not present in either parent). What is currently your recommended practice regarding de novo variant calling? I'm running DeepTrio (version 1.4.0, specified as deeptrio-latest) via Docker (version 20.10.6) using Singularity. Many thanks! Karoliina",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/609
https://github.com/google/deepvariant/issues/610:17,availability,error,error,17,"NumPy dependency error despite matching versions; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**: Yes. **Describe the issue:**. Unable to run Singularity image despite prerequisites appearing satisfied. **Setup**. - Operating system: CentOS 7 . - DeepVariant version: 1.4.0 (google/deepvariant:latest). - Installation method (Docker, built from source, etc.): `singularity pull docker://google/deepvariant:latest` and attempted execution via `singularity run`. - Type of data: N/A. **Steps to reproduce:**. - Command: `singularity run -B /home/ -B /scratch/ deepvariant.sif`. - Error trace:. ```. Traceback (most recent call last):. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/__init__.py"", line 23, in <module>. from . import multiarray. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/multiarray.py"", line 10, in <module>. from . import overrides. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/overrides.py"", line 6, in <module>. from numpy.core._multiarray_umath import (. ImportError: libflexiblas.so.3: cannot open shared object file: No such file or directory. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 48, in <module>. import tensorflow as tf. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/__init__.py"", line 41, in <module>. from tensorflow.python.tools import module_util as _module_util. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/__init__.py"", line 41, in <module>. from tensorflow.python.eager import context. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/context.py"", line 30, in <module>. import numpy as np. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/__init__.py"", line 140, in <module>. from . import core. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/__init__.py"", li",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/610
https://github.com/google/deepvariant/issues/610:259,availability,Operat,Operating,259,"NumPy dependency error despite matching versions; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**: Yes. **Describe the issue:**. Unable to run Singularity image despite prerequisites appearing satisfied. **Setup**. - Operating system: CentOS 7 . - DeepVariant version: 1.4.0 (google/deepvariant:latest). - Installation method (Docker, built from source, etc.): `singularity pull docker://google/deepvariant:latest` and attempted execution via `singularity run`. - Type of data: N/A. **Steps to reproduce:**. - Command: `singularity run -B /home/ -B /scratch/ deepvariant.sif`. - Error trace:. ```. Traceback (most recent call last):. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/__init__.py"", line 23, in <module>. from . import multiarray. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/multiarray.py"", line 10, in <module>. from . import overrides. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/overrides.py"", line 6, in <module>. from numpy.core._multiarray_umath import (. ImportError: libflexiblas.so.3: cannot open shared object file: No such file or directory. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 48, in <module>. import tensorflow as tf. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/__init__.py"", line 41, in <module>. from tensorflow.python.tools import module_util as _module_util. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/__init__.py"", line 41, in <module>. from tensorflow.python.eager import context. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/context.py"", line 30, in <module>. import numpy as np. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/__init__.py"", line 140, in <module>. from . import core. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/__init__.py"", li",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/610
https://github.com/google/deepvariant/issues/610:621,availability,Error,Error,621,"NumPy dependency error despite matching versions; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**: Yes. **Describe the issue:**. Unable to run Singularity image despite prerequisites appearing satisfied. **Setup**. - Operating system: CentOS 7 . - DeepVariant version: 1.4.0 (google/deepvariant:latest). - Installation method (Docker, built from source, etc.): `singularity pull docker://google/deepvariant:latest` and attempted execution via `singularity run`. - Type of data: N/A. **Steps to reproduce:**. - Command: `singularity run -B /home/ -B /scratch/ deepvariant.sif`. - Error trace:. ```. Traceback (most recent call last):. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/__init__.py"", line 23, in <module>. from . import multiarray. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/multiarray.py"", line 10, in <module>. from . import overrides. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/overrides.py"", line 6, in <module>. from numpy.core._multiarray_umath import (. ImportError: libflexiblas.so.3: cannot open shared object file: No such file or directory. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 48, in <module>. import tensorflow as tf. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/__init__.py"", line 41, in <module>. from tensorflow.python.tools import module_util as _module_util. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/__init__.py"", line 41, in <module>. from tensorflow.python.eager import context. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/context.py"", line 30, in <module>. import numpy as np. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/__init__.py"", line 140, in <module>. from . import core. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/__init__.py"", li",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/610
https://github.com/google/deepvariant/issues/610:2171,availability,error,error,2171,"uring handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 48, in <module>. import tensorflow as tf. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/__init__.py"", line 41, in <module>. from tensorflow.python.tools import module_util as _module_util. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/__init__.py"", line 41, in <module>. from tensorflow.python.eager import context. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/context.py"", line 30, in <module>. import numpy as np. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/__init__.py"", line 140, in <module>. from . import core. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/__init__.py"", line 49, in <module>. raise ImportError(msg). ImportError:. IMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE! Importing the numpy C-extensions failed. This error can happen for. many reasons, often due to issues with your setup or how NumPy was. installed. We have compiled some common reasons and troubleshooting tips at:. https://numpy.org/devdocs/user/troubleshooting-importerror.html. Please note and check the following:. * The Python version is: Python3.8 from ""/usr/bin/python3"". * The NumPy version is: ""1.23.0"". and make sure that they are the versions you expect. Please carefully study the documentation linked above for further help. Original error was: libflexiblas.so.3: cannot open shared object file: No such file or directory. ```. **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? Any attempt to execute via `singularity run` leads to an error. **Any additional context:**. As far as I can tell, my environment meets the requirements for both Python and NumPy - though at th",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/610
https://github.com/google/deepvariant/issues/610:2670,availability,error,error,2670," File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/context.py"", line 30, in <module>. import numpy as np. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/__init__.py"", line 140, in <module>. from . import core. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/__init__.py"", line 49, in <module>. raise ImportError(msg). ImportError:. IMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE! Importing the numpy C-extensions failed. This error can happen for. many reasons, often due to issues with your setup or how NumPy was. installed. We have compiled some common reasons and troubleshooting tips at:. https://numpy.org/devdocs/user/troubleshooting-importerror.html. Please note and check the following:. * The Python version is: Python3.8 from ""/usr/bin/python3"". * The NumPy version is: ""1.23.0"". and make sure that they are the versions you expect. Please carefully study the documentation linked above for further help. Original error was: libflexiblas.so.3: cannot open shared object file: No such file or directory. ```. **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? Any attempt to execute via `singularity run` leads to an error. **Any additional context:**. As far as I can tell, my environment meets the requirements for both Python and NumPy - though at the same time when I `singularity shell` into the SIF file, its versioning seems semi-independent of my main environment (Python 3.8.10 regardless of my environment's version, but using the NumPy 1.23.0 provided by my computing cluster). I feel like I'm missing something really simple, but I've tried the NumPy troubleshooting page and can't seem to crack this error. If it helps, I'm attempting this with `singularity` version 3.8.4, which is the newest version available to me in my computing clust",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/610
https://github.com/google/deepvariant/issues/610:3038,availability,error,error,3038,"le ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/context.py"", line 30, in <module>. import numpy as np. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/__init__.py"", line 140, in <module>. from . import core. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/__init__.py"", line 49, in <module>. raise ImportError(msg). ImportError:. IMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE! Importing the numpy C-extensions failed. This error can happen for. many reasons, often due to issues with your setup or how NumPy was. installed. We have compiled some common reasons and troubleshooting tips at:. https://numpy.org/devdocs/user/troubleshooting-importerror.html. Please note and check the following:. * The Python version is: Python3.8 from ""/usr/bin/python3"". * The NumPy version is: ""1.23.0"". and make sure that they are the versions you expect. Please carefully study the documentation linked above for further help. Original error was: libflexiblas.so.3: cannot open shared object file: No such file or directory. ```. **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? Any attempt to execute via `singularity run` leads to an error. **Any additional context:**. As far as I can tell, my environment meets the requirements for both Python and NumPy - though at the same time when I `singularity shell` into the SIF file, its versioning seems semi-independent of my main environment (Python 3.8.10 regardless of my environment's version, but using the NumPy 1.23.0 provided by my computing cluster). I feel like I'm missing something really simple, but I've tried the NumPy troubleshooting page and can't seem to crack this error. If it helps, I'm attempting this with `singularity` version 3.8.4, which is the newest version available to me in my computing cluster.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/610
https://github.com/google/deepvariant/issues/610:3400,availability,cluster,cluster,3400,"le ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/context.py"", line 30, in <module>. import numpy as np. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/__init__.py"", line 140, in <module>. from . import core. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/__init__.py"", line 49, in <module>. raise ImportError(msg). ImportError:. IMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE! Importing the numpy C-extensions failed. This error can happen for. many reasons, often due to issues with your setup or how NumPy was. installed. We have compiled some common reasons and troubleshooting tips at:. https://numpy.org/devdocs/user/troubleshooting-importerror.html. Please note and check the following:. * The Python version is: Python3.8 from ""/usr/bin/python3"". * The NumPy version is: ""1.23.0"". and make sure that they are the versions you expect. Please carefully study the documentation linked above for further help. Original error was: libflexiblas.so.3: cannot open shared object file: No such file or directory. ```. **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? Any attempt to execute via `singularity run` leads to an error. **Any additional context:**. As far as I can tell, my environment meets the requirements for both Python and NumPy - though at the same time when I `singularity shell` into the SIF file, its versioning seems semi-independent of my main environment (Python 3.8.10 regardless of my environment's version, but using the NumPy 1.23.0 provided by my computing cluster). I feel like I'm missing something really simple, but I've tried the NumPy troubleshooting page and can't seem to crack this error. If it helps, I'm attempting this with `singularity` version 3.8.4, which is the newest version available to me in my computing cluster.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/610
https://github.com/google/deepvariant/issues/610:3534,availability,error,error,3534,"le ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/context.py"", line 30, in <module>. import numpy as np. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/__init__.py"", line 140, in <module>. from . import core. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/__init__.py"", line 49, in <module>. raise ImportError(msg). ImportError:. IMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE! Importing the numpy C-extensions failed. This error can happen for. many reasons, often due to issues with your setup or how NumPy was. installed. We have compiled some common reasons and troubleshooting tips at:. https://numpy.org/devdocs/user/troubleshooting-importerror.html. Please note and check the following:. * The Python version is: Python3.8 from ""/usr/bin/python3"". * The NumPy version is: ""1.23.0"". and make sure that they are the versions you expect. Please carefully study the documentation linked above for further help. Original error was: libflexiblas.so.3: cannot open shared object file: No such file or directory. ```. **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? Any attempt to execute via `singularity run` leads to an error. **Any additional context:**. As far as I can tell, my environment meets the requirements for both Python and NumPy - though at the same time when I `singularity shell` into the SIF file, its versioning seems semi-independent of my main environment (Python 3.8.10 regardless of my environment's version, but using the NumPy 1.23.0 provided by my computing cluster). I feel like I'm missing something really simple, but I've tried the NumPy troubleshooting page and can't seem to crack this error. If it helps, I'm attempting this with `singularity` version 3.8.4, which is the newest version available to me in my computing cluster.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/610
https://github.com/google/deepvariant/issues/610:3636,availability,avail,available,3636,"le ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/context.py"", line 30, in <module>. import numpy as np. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/__init__.py"", line 140, in <module>. from . import core. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/__init__.py"", line 49, in <module>. raise ImportError(msg). ImportError:. IMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE! Importing the numpy C-extensions failed. This error can happen for. many reasons, often due to issues with your setup or how NumPy was. installed. We have compiled some common reasons and troubleshooting tips at:. https://numpy.org/devdocs/user/troubleshooting-importerror.html. Please note and check the following:. * The Python version is: Python3.8 from ""/usr/bin/python3"". * The NumPy version is: ""1.23.0"". and make sure that they are the versions you expect. Please carefully study the documentation linked above for further help. Original error was: libflexiblas.so.3: cannot open shared object file: No such file or directory. ```. **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? Any attempt to execute via `singularity run` leads to an error. **Any additional context:**. As far as I can tell, my environment meets the requirements for both Python and NumPy - though at the same time when I `singularity shell` into the SIF file, its versioning seems semi-independent of my main environment (Python 3.8.10 regardless of my environment's version, but using the NumPy 1.23.0 provided by my computing cluster). I feel like I'm missing something really simple, but I've tried the NumPy troubleshooting page and can't seem to crack this error. If it helps, I'm attempting this with `singularity` version 3.8.4, which is the newest version available to me in my computing cluster.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/610
https://github.com/google/deepvariant/issues/610:3668,availability,cluster,cluster,3668,"le ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/context.py"", line 30, in <module>. import numpy as np. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/__init__.py"", line 140, in <module>. from . import core. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/__init__.py"", line 49, in <module>. raise ImportError(msg). ImportError:. IMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE! Importing the numpy C-extensions failed. This error can happen for. many reasons, often due to issues with your setup or how NumPy was. installed. We have compiled some common reasons and troubleshooting tips at:. https://numpy.org/devdocs/user/troubleshooting-importerror.html. Please note and check the following:. * The Python version is: Python3.8 from ""/usr/bin/python3"". * The NumPy version is: ""1.23.0"". and make sure that they are the versions you expect. Please carefully study the documentation linked above for further help. Original error was: libflexiblas.so.3: cannot open shared object file: No such file or directory. ```. **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? Any attempt to execute via `singularity run` leads to an error. **Any additional context:**. As far as I can tell, my environment meets the requirements for both Python and NumPy - though at the same time when I `singularity shell` into the SIF file, its versioning seems semi-independent of my main environment (Python 3.8.10 regardless of my environment's version, but using the NumPy 1.23.0 provided by my computing cluster). I feel like I'm missing something really simple, but I've tried the NumPy troubleshooting page and can't seem to crack this error. If it helps, I'm attempting this with `singularity` version 3.8.4, which is the newest version available to me in my computing cluster.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/610
https://github.com/google/deepvariant/issues/610:6,deployability,depend,dependency,6,"NumPy dependency error despite matching versions; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**: Yes. **Describe the issue:**. Unable to run Singularity image despite prerequisites appearing satisfied. **Setup**. - Operating system: CentOS 7 . - DeepVariant version: 1.4.0 (google/deepvariant:latest). - Installation method (Docker, built from source, etc.): `singularity pull docker://google/deepvariant:latest` and attempted execution via `singularity run`. - Type of data: N/A. **Steps to reproduce:**. - Command: `singularity run -B /home/ -B /scratch/ deepvariant.sif`. - Error trace:. ```. Traceback (most recent call last):. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/__init__.py"", line 23, in <module>. from . import multiarray. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/multiarray.py"", line 10, in <module>. from . import overrides. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/overrides.py"", line 6, in <module>. from numpy.core._multiarray_umath import (. ImportError: libflexiblas.so.3: cannot open shared object file: No such file or directory. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 48, in <module>. import tensorflow as tf. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/__init__.py"", line 41, in <module>. from tensorflow.python.tools import module_util as _module_util. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/__init__.py"", line 41, in <module>. from tensorflow.python.eager import context. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/context.py"", line 30, in <module>. import numpy as np. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/__init__.py"", line 140, in <module>. from . import core. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/__init__.py"", li",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/610
https://github.com/google/deepvariant/issues/610:40,deployability,version,versions,40,"NumPy dependency error despite matching versions; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**: Yes. **Describe the issue:**. Unable to run Singularity image despite prerequisites appearing satisfied. **Setup**. - Operating system: CentOS 7 . - DeepVariant version: 1.4.0 (google/deepvariant:latest). - Installation method (Docker, built from source, etc.): `singularity pull docker://google/deepvariant:latest` and attempted execution via `singularity run`. - Type of data: N/A. **Steps to reproduce:**. - Command: `singularity run -B /home/ -B /scratch/ deepvariant.sif`. - Error trace:. ```. Traceback (most recent call last):. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/__init__.py"", line 23, in <module>. from . import multiarray. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/multiarray.py"", line 10, in <module>. from . import overrides. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/overrides.py"", line 6, in <module>. from numpy.core._multiarray_umath import (. ImportError: libflexiblas.so.3: cannot open shared object file: No such file or directory. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 48, in <module>. import tensorflow as tf. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/__init__.py"", line 41, in <module>. from tensorflow.python.tools import module_util as _module_util. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/__init__.py"", line 41, in <module>. from tensorflow.python.eager import context. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/context.py"", line 30, in <module>. import numpy as np. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/__init__.py"", line 140, in <module>. from . import core. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/__init__.py"", li",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/610
https://github.com/google/deepvariant/issues/610:302,deployability,version,version,302,"NumPy dependency error despite matching versions; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**: Yes. **Describe the issue:**. Unable to run Singularity image despite prerequisites appearing satisfied. **Setup**. - Operating system: CentOS 7 . - DeepVariant version: 1.4.0 (google/deepvariant:latest). - Installation method (Docker, built from source, etc.): `singularity pull docker://google/deepvariant:latest` and attempted execution via `singularity run`. - Type of data: N/A. **Steps to reproduce:**. - Command: `singularity run -B /home/ -B /scratch/ deepvariant.sif`. - Error trace:. ```. Traceback (most recent call last):. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/__init__.py"", line 23, in <module>. from . import multiarray. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/multiarray.py"", line 10, in <module>. from . import overrides. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/overrides.py"", line 6, in <module>. from numpy.core._multiarray_umath import (. ImportError: libflexiblas.so.3: cannot open shared object file: No such file or directory. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 48, in <module>. import tensorflow as tf. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/__init__.py"", line 41, in <module>. from tensorflow.python.tools import module_util as _module_util. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/__init__.py"", line 41, in <module>. from tensorflow.python.eager import context. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/context.py"", line 30, in <module>. import numpy as np. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/__init__.py"", line 140, in <module>. from . import core. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/__init__.py"", li",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/610
https://github.com/google/deepvariant/issues/610:348,deployability,Instal,Installation,348,"NumPy dependency error despite matching versions; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**: Yes. **Describe the issue:**. Unable to run Singularity image despite prerequisites appearing satisfied. **Setup**. - Operating system: CentOS 7 . - DeepVariant version: 1.4.0 (google/deepvariant:latest). - Installation method (Docker, built from source, etc.): `singularity pull docker://google/deepvariant:latest` and attempted execution via `singularity run`. - Type of data: N/A. **Steps to reproduce:**. - Command: `singularity run -B /home/ -B /scratch/ deepvariant.sif`. - Error trace:. ```. Traceback (most recent call last):. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/__init__.py"", line 23, in <module>. from . import multiarray. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/multiarray.py"", line 10, in <module>. from . import overrides. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/overrides.py"", line 6, in <module>. from numpy.core._multiarray_umath import (. ImportError: libflexiblas.so.3: cannot open shared object file: No such file or directory. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 48, in <module>. import tensorflow as tf. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/__init__.py"", line 41, in <module>. from tensorflow.python.tools import module_util as _module_util. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/__init__.py"", line 41, in <module>. from tensorflow.python.eager import context. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/context.py"", line 30, in <module>. import numpy as np. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/__init__.py"", line 140, in <module>. from . import core. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/__init__.py"", li",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/610
https://github.com/google/deepvariant/issues/610:770,deployability,modul,module,770,"NumPy dependency error despite matching versions; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**: Yes. **Describe the issue:**. Unable to run Singularity image despite prerequisites appearing satisfied. **Setup**. - Operating system: CentOS 7 . - DeepVariant version: 1.4.0 (google/deepvariant:latest). - Installation method (Docker, built from source, etc.): `singularity pull docker://google/deepvariant:latest` and attempted execution via `singularity run`. - Type of data: N/A. **Steps to reproduce:**. - Command: `singularity run -B /home/ -B /scratch/ deepvariant.sif`. - Error trace:. ```. Traceback (most recent call last):. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/__init__.py"", line 23, in <module>. from . import multiarray. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/multiarray.py"", line 10, in <module>. from . import overrides. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/overrides.py"", line 6, in <module>. from numpy.core._multiarray_umath import (. ImportError: libflexiblas.so.3: cannot open shared object file: No such file or directory. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 48, in <module>. import tensorflow as tf. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/__init__.py"", line 41, in <module>. from tensorflow.python.tools import module_util as _module_util. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/__init__.py"", line 41, in <module>. from tensorflow.python.eager import context. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/context.py"", line 30, in <module>. import numpy as np. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/__init__.py"", line 140, in <module>. from . import core. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/__init__.py"", li",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/610
https://github.com/google/deepvariant/issues/610:901,deployability,modul,module,901,"NumPy dependency error despite matching versions; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**: Yes. **Describe the issue:**. Unable to run Singularity image despite prerequisites appearing satisfied. **Setup**. - Operating system: CentOS 7 . - DeepVariant version: 1.4.0 (google/deepvariant:latest). - Installation method (Docker, built from source, etc.): `singularity pull docker://google/deepvariant:latest` and attempted execution via `singularity run`. - Type of data: N/A. **Steps to reproduce:**. - Command: `singularity run -B /home/ -B /scratch/ deepvariant.sif`. - Error trace:. ```. Traceback (most recent call last):. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/__init__.py"", line 23, in <module>. from . import multiarray. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/multiarray.py"", line 10, in <module>. from . import overrides. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/overrides.py"", line 6, in <module>. from numpy.core._multiarray_umath import (. ImportError: libflexiblas.so.3: cannot open shared object file: No such file or directory. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 48, in <module>. import tensorflow as tf. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/__init__.py"", line 41, in <module>. from tensorflow.python.tools import module_util as _module_util. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/__init__.py"", line 41, in <module>. from tensorflow.python.eager import context. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/context.py"", line 30, in <module>. import numpy as np. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/__init__.py"", line 140, in <module>. from . import core. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/__init__.py"", li",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/610
https://github.com/google/deepvariant/issues/610:1029,deployability,modul,module,1029,"atching versions; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**: Yes. **Describe the issue:**. Unable to run Singularity image despite prerequisites appearing satisfied. **Setup**. - Operating system: CentOS 7 . - DeepVariant version: 1.4.0 (google/deepvariant:latest). - Installation method (Docker, built from source, etc.): `singularity pull docker://google/deepvariant:latest` and attempted execution via `singularity run`. - Type of data: N/A. **Steps to reproduce:**. - Command: `singularity run -B /home/ -B /scratch/ deepvariant.sif`. - Error trace:. ```. Traceback (most recent call last):. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/__init__.py"", line 23, in <module>. from . import multiarray. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/multiarray.py"", line 10, in <module>. from . import overrides. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/overrides.py"", line 6, in <module>. from numpy.core._multiarray_umath import (. ImportError: libflexiblas.so.3: cannot open shared object file: No such file or directory. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 48, in <module>. import tensorflow as tf. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/__init__.py"", line 41, in <module>. from tensorflow.python.tools import module_util as _module_util. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/__init__.py"", line 41, in <module>. from tensorflow.python.eager import context. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/context.py"", line 30, in <module>. import numpy as np. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/__init__.py"", line 140, in <module>. from . import core. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/__init__.py"", line 49, in <module>. raise Import",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/610
https://github.com/google/deepvariant/issues/610:1339,deployability,modul,module,1339,"t). - Installation method (Docker, built from source, etc.): `singularity pull docker://google/deepvariant:latest` and attempted execution via `singularity run`. - Type of data: N/A. **Steps to reproduce:**. - Command: `singularity run -B /home/ -B /scratch/ deepvariant.sif`. - Error trace:. ```. Traceback (most recent call last):. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/__init__.py"", line 23, in <module>. from . import multiarray. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/multiarray.py"", line 10, in <module>. from . import overrides. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/overrides.py"", line 6, in <module>. from numpy.core._multiarray_umath import (. ImportError: libflexiblas.so.3: cannot open shared object file: No such file or directory. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 48, in <module>. import tensorflow as tf. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/__init__.py"", line 41, in <module>. from tensorflow.python.tools import module_util as _module_util. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/__init__.py"", line 41, in <module>. from tensorflow.python.eager import context. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/context.py"", line 30, in <module>. import numpy as np. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/__init__.py"", line 140, in <module>. from . import core. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/__init__.py"", line 49, in <module>. raise ImportError(msg). ImportError:. IMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE! Importing the numpy C-extensions failed. This error can happen for. many reasons, often due to issues with your setup or how NumPy was. installed. We have compiled some common reasons and troubleshooting tips at:. htt",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/610
https://github.com/google/deepvariant/issues/610:1456,deployability,modul,module,1456,"d attempted execution via `singularity run`. - Type of data: N/A. **Steps to reproduce:**. - Command: `singularity run -B /home/ -B /scratch/ deepvariant.sif`. - Error trace:. ```. Traceback (most recent call last):. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/__init__.py"", line 23, in <module>. from . import multiarray. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/multiarray.py"", line 10, in <module>. from . import overrides. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/overrides.py"", line 6, in <module>. from numpy.core._multiarray_umath import (. ImportError: libflexiblas.so.3: cannot open shared object file: No such file or directory. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 48, in <module>. import tensorflow as tf. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/__init__.py"", line 41, in <module>. from tensorflow.python.tools import module_util as _module_util. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/__init__.py"", line 41, in <module>. from tensorflow.python.eager import context. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/context.py"", line 30, in <module>. import numpy as np. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/__init__.py"", line 140, in <module>. from . import core. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/__init__.py"", line 49, in <module>. raise ImportError(msg). ImportError:. IMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE! Importing the numpy C-extensions failed. This error can happen for. many reasons, often due to issues with your setup or how NumPy was. installed. We have compiled some common reasons and troubleshooting tips at:. https://numpy.org/devdocs/user/troubleshooting-importerror.html. Please note and check the following:. * The Python vers",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/610
https://github.com/google/deepvariant/issues/610:1620,deployability,modul,module,1620,"ror trace:. ```. Traceback (most recent call last):. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/__init__.py"", line 23, in <module>. from . import multiarray. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/multiarray.py"", line 10, in <module>. from . import overrides. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/overrides.py"", line 6, in <module>. from numpy.core._multiarray_umath import (. ImportError: libflexiblas.so.3: cannot open shared object file: No such file or directory. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 48, in <module>. import tensorflow as tf. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/__init__.py"", line 41, in <module>. from tensorflow.python.tools import module_util as _module_util. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/__init__.py"", line 41, in <module>. from tensorflow.python.eager import context. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/context.py"", line 30, in <module>. import numpy as np. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/__init__.py"", line 140, in <module>. from . import core. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/__init__.py"", line 49, in <module>. raise ImportError(msg). ImportError:. IMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE! Importing the numpy C-extensions failed. This error can happen for. many reasons, often due to issues with your setup or how NumPy was. installed. We have compiled some common reasons and troubleshooting tips at:. https://numpy.org/devdocs/user/troubleshooting-importerror.html. Please note and check the following:. * The Python version is: Python3.8 from ""/usr/bin/python3"". * The NumPy version is: ""1.23.0"". and make sure that they are the versions you expect. Please carefully study the documen",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/610
https://github.com/google/deepvariant/issues/610:1769,deployability,modul,module,1769,"dule>. from . import multiarray. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/multiarray.py"", line 10, in <module>. from . import overrides. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/overrides.py"", line 6, in <module>. from numpy.core._multiarray_umath import (. ImportError: libflexiblas.so.3: cannot open shared object file: No such file or directory. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 48, in <module>. import tensorflow as tf. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/__init__.py"", line 41, in <module>. from tensorflow.python.tools import module_util as _module_util. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/__init__.py"", line 41, in <module>. from tensorflow.python.eager import context. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/context.py"", line 30, in <module>. import numpy as np. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/__init__.py"", line 140, in <module>. from . import core. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/__init__.py"", line 49, in <module>. raise ImportError(msg). ImportError:. IMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE! Importing the numpy C-extensions failed. This error can happen for. many reasons, often due to issues with your setup or how NumPy was. installed. We have compiled some common reasons and troubleshooting tips at:. https://numpy.org/devdocs/user/troubleshooting-importerror.html. Please note and check the following:. * The Python version is: Python3.8 from ""/usr/bin/python3"". * The NumPy version is: ""1.23.0"". and make sure that they are the versions you expect. Please carefully study the documentation linked above for further help. Original error was: libflexiblas.so.3: cannot open shared object file: No such file or directory. ```. **Does t",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/610
https://github.com/google/deepvariant/issues/610:1888,deployability,modul,module,1888,"e 10, in <module>. from . import overrides. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/overrides.py"", line 6, in <module>. from numpy.core._multiarray_umath import (. ImportError: libflexiblas.so.3: cannot open shared object file: No such file or directory. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 48, in <module>. import tensorflow as tf. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/__init__.py"", line 41, in <module>. from tensorflow.python.tools import module_util as _module_util. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/__init__.py"", line 41, in <module>. from tensorflow.python.eager import context. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/context.py"", line 30, in <module>. import numpy as np. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/__init__.py"", line 140, in <module>. from . import core. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/__init__.py"", line 49, in <module>. raise ImportError(msg). ImportError:. IMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE! Importing the numpy C-extensions failed. This error can happen for. many reasons, often due to issues with your setup or how NumPy was. installed. We have compiled some common reasons and troubleshooting tips at:. https://numpy.org/devdocs/user/troubleshooting-importerror.html. Please note and check the following:. * The Python version is: Python3.8 from ""/usr/bin/python3"". * The NumPy version is: ""1.23.0"". and make sure that they are the versions you expect. Please carefully study the documentation linked above for further help. Original error was: libflexiblas.so.3: cannot open shared object file: No such file or directory. ```. **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deep",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/610
https://github.com/google/deepvariant/issues/610:2011,deployability,modul,module,2011,""", line 6, in <module>. from numpy.core._multiarray_umath import (. ImportError: libflexiblas.so.3: cannot open shared object file: No such file or directory. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 48, in <module>. import tensorflow as tf. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/__init__.py"", line 41, in <module>. from tensorflow.python.tools import module_util as _module_util. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/__init__.py"", line 41, in <module>. from tensorflow.python.eager import context. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/context.py"", line 30, in <module>. import numpy as np. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/__init__.py"", line 140, in <module>. from . import core. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/__init__.py"", line 49, in <module>. raise ImportError(msg). ImportError:. IMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE! Importing the numpy C-extensions failed. This error can happen for. many reasons, often due to issues with your setup or how NumPy was. installed. We have compiled some common reasons and troubleshooting tips at:. https://numpy.org/devdocs/user/troubleshooting-importerror.html. Please note and check the following:. * The Python version is: Python3.8 from ""/usr/bin/python3"". * The NumPy version is: ""1.23.0"". and make sure that they are the versions you expect. Please carefully study the documentation linked above for further help. Original error was: libflexiblas.so.3: cannot open shared object file: No such file or directory. ```. **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? Any attempt to execute via `singu",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/610
https://github.com/google/deepvariant/issues/610:2158,deployability,fail,failed,2158," directory. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 48, in <module>. import tensorflow as tf. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/__init__.py"", line 41, in <module>. from tensorflow.python.tools import module_util as _module_util. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/__init__.py"", line 41, in <module>. from tensorflow.python.eager import context. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/context.py"", line 30, in <module>. import numpy as np. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/__init__.py"", line 140, in <module>. from . import core. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/__init__.py"", line 49, in <module>. raise ImportError(msg). ImportError:. IMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE! Importing the numpy C-extensions failed. This error can happen for. many reasons, often due to issues with your setup or how NumPy was. installed. We have compiled some common reasons and troubleshooting tips at:. https://numpy.org/devdocs/user/troubleshooting-importerror.html. Please note and check the following:. * The Python version is: Python3.8 from ""/usr/bin/python3"". * The NumPy version is: ""1.23.0"". and make sure that they are the versions you expect. Please carefully study the documentation linked above for further help. Original error was: libflexiblas.so.3: cannot open shared object file: No such file or directory. ```. **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? Any attempt to execute via `singularity run` leads to an error. **Any additional context:**. As far as I can tell, my environment meets the requirements for both Python and NumPy -",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/610
https://github.com/google/deepvariant/issues/610:2261,deployability,instal,installed,2261,"all last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 48, in <module>. import tensorflow as tf. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/__init__.py"", line 41, in <module>. from tensorflow.python.tools import module_util as _module_util. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/__init__.py"", line 41, in <module>. from tensorflow.python.eager import context. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/context.py"", line 30, in <module>. import numpy as np. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/__init__.py"", line 140, in <module>. from . import core. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/__init__.py"", line 49, in <module>. raise ImportError(msg). ImportError:. IMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE! Importing the numpy C-extensions failed. This error can happen for. many reasons, often due to issues with your setup or how NumPy was. installed. We have compiled some common reasons and troubleshooting tips at:. https://numpy.org/devdocs/user/troubleshooting-importerror.html. Please note and check the following:. * The Python version is: Python3.8 from ""/usr/bin/python3"". * The NumPy version is: ""1.23.0"". and make sure that they are the versions you expect. Please carefully study the documentation linked above for further help. Original error was: libflexiblas.so.3: cannot open shared object file: No such file or directory. ```. **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? Any attempt to execute via `singularity run` leads to an error. **Any additional context:**. As far as I can tell, my environment meets the requirements for both Python and NumPy - though at the same time when I `singularity shell` into the SIF file, its versioning seems semi-independ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/610
https://github.com/google/deepvariant/issues/610:2455,deployability,version,version,2455,"ule>. from tensorflow.python.tools import module_util as _module_util. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/__init__.py"", line 41, in <module>. from tensorflow.python.eager import context. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/context.py"", line 30, in <module>. import numpy as np. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/__init__.py"", line 140, in <module>. from . import core. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/__init__.py"", line 49, in <module>. raise ImportError(msg). ImportError:. IMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE! Importing the numpy C-extensions failed. This error can happen for. many reasons, often due to issues with your setup or how NumPy was. installed. We have compiled some common reasons and troubleshooting tips at:. https://numpy.org/devdocs/user/troubleshooting-importerror.html. Please note and check the following:. * The Python version is: Python3.8 from ""/usr/bin/python3"". * The NumPy version is: ""1.23.0"". and make sure that they are the versions you expect. Please carefully study the documentation linked above for further help. Original error was: libflexiblas.so.3: cannot open shared object file: No such file or directory. ```. **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? Any attempt to execute via `singularity run` leads to an error. **Any additional context:**. As far as I can tell, my environment meets the requirements for both Python and NumPy - though at the same time when I `singularity shell` into the SIF file, its versioning seems semi-independent of my main environment (Python 3.8.10 regardless of my environment's version, but using the NumPy 1.23.0 provided by my computing cluster). I feel like I'm missing something really simple, ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/610
https://github.com/google/deepvariant/issues/610:2514,deployability,version,version,2514,"odule_util. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/__init__.py"", line 41, in <module>. from tensorflow.python.eager import context. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/context.py"", line 30, in <module>. import numpy as np. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/__init__.py"", line 140, in <module>. from . import core. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/__init__.py"", line 49, in <module>. raise ImportError(msg). ImportError:. IMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE! Importing the numpy C-extensions failed. This error can happen for. many reasons, often due to issues with your setup or how NumPy was. installed. We have compiled some common reasons and troubleshooting tips at:. https://numpy.org/devdocs/user/troubleshooting-importerror.html. Please note and check the following:. * The Python version is: Python3.8 from ""/usr/bin/python3"". * The NumPy version is: ""1.23.0"". and make sure that they are the versions you expect. Please carefully study the documentation linked above for further help. Original error was: libflexiblas.so.3: cannot open shared object file: No such file or directory. ```. **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? Any attempt to execute via `singularity run` leads to an error. **Any additional context:**. As far as I can tell, my environment meets the requirements for both Python and NumPy - though at the same time when I `singularity shell` into the SIF file, its versioning seems semi-independent of my main environment (Python 3.8.10 regardless of my environment's version, but using the NumPy 1.23.0 provided by my computing cluster). I feel like I'm missing something really simple, but I've tried the NumPy troubleshooting page and can't see",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/610
https://github.com/google/deepvariant/issues/610:2568,deployability,version,versions,2568,"es/tensorflow/python/__init__.py"", line 41, in <module>. from tensorflow.python.eager import context. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/context.py"", line 30, in <module>. import numpy as np. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/__init__.py"", line 140, in <module>. from . import core. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/__init__.py"", line 49, in <module>. raise ImportError(msg). ImportError:. IMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE! Importing the numpy C-extensions failed. This error can happen for. many reasons, often due to issues with your setup or how NumPy was. installed. We have compiled some common reasons and troubleshooting tips at:. https://numpy.org/devdocs/user/troubleshooting-importerror.html. Please note and check the following:. * The Python version is: Python3.8 from ""/usr/bin/python3"". * The NumPy version is: ""1.23.0"". and make sure that they are the versions you expect. Please carefully study the documentation linked above for further help. Original error was: libflexiblas.so.3: cannot open shared object file: No such file or directory. ```. **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? Any attempt to execute via `singularity run` leads to an error. **Any additional context:**. As far as I can tell, my environment meets the requirements for both Python and NumPy - though at the same time when I `singularity shell` into the SIF file, its versioning seems semi-independent of my main environment (Python 3.8.10 regardless of my environment's version, but using the NumPy 1.23.0 provided by my computing cluster). I feel like I'm missing something really simple, but I've tried the NumPy troubleshooting page and can't seem to crack this error. If it helps, I'm attempting thi",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/610
https://github.com/google/deepvariant/issues/610:3236,deployability,version,versioning,3236,"le ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/context.py"", line 30, in <module>. import numpy as np. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/__init__.py"", line 140, in <module>. from . import core. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/__init__.py"", line 49, in <module>. raise ImportError(msg). ImportError:. IMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE! Importing the numpy C-extensions failed. This error can happen for. many reasons, often due to issues with your setup or how NumPy was. installed. We have compiled some common reasons and troubleshooting tips at:. https://numpy.org/devdocs/user/troubleshooting-importerror.html. Please note and check the following:. * The Python version is: Python3.8 from ""/usr/bin/python3"". * The NumPy version is: ""1.23.0"". and make sure that they are the versions you expect. Please carefully study the documentation linked above for further help. Original error was: libflexiblas.so.3: cannot open shared object file: No such file or directory. ```. **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? Any attempt to execute via `singularity run` leads to an error. **Any additional context:**. As far as I can tell, my environment meets the requirements for both Python and NumPy - though at the same time when I `singularity shell` into the SIF file, its versioning seems semi-independent of my main environment (Python 3.8.10 regardless of my environment's version, but using the NumPy 1.23.0 provided by my computing cluster). I feel like I'm missing something really simple, but I've tried the NumPy troubleshooting page and can't seem to crack this error. If it helps, I'm attempting this with `singularity` version 3.8.4, which is the newest version available to me in my computing cluster.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/610
https://github.com/google/deepvariant/issues/610:3339,deployability,version,version,3339,"le ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/context.py"", line 30, in <module>. import numpy as np. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/__init__.py"", line 140, in <module>. from . import core. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/__init__.py"", line 49, in <module>. raise ImportError(msg). ImportError:. IMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE! Importing the numpy C-extensions failed. This error can happen for. many reasons, often due to issues with your setup or how NumPy was. installed. We have compiled some common reasons and troubleshooting tips at:. https://numpy.org/devdocs/user/troubleshooting-importerror.html. Please note and check the following:. * The Python version is: Python3.8 from ""/usr/bin/python3"". * The NumPy version is: ""1.23.0"". and make sure that they are the versions you expect. Please carefully study the documentation linked above for further help. Original error was: libflexiblas.so.3: cannot open shared object file: No such file or directory. ```. **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? Any attempt to execute via `singularity run` leads to an error. **Any additional context:**. As far as I can tell, my environment meets the requirements for both Python and NumPy - though at the same time when I `singularity shell` into the SIF file, its versioning seems semi-independent of my main environment (Python 3.8.10 regardless of my environment's version, but using the NumPy 1.23.0 provided by my computing cluster). I feel like I'm missing something really simple, but I've tried the NumPy troubleshooting page and can't seem to crack this error. If it helps, I'm attempting this with `singularity` version 3.8.4, which is the newest version available to me in my computing cluster.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/610
https://github.com/google/deepvariant/issues/610:3400,deployability,cluster,cluster,3400,"le ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/context.py"", line 30, in <module>. import numpy as np. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/__init__.py"", line 140, in <module>. from . import core. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/__init__.py"", line 49, in <module>. raise ImportError(msg). ImportError:. IMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE! Importing the numpy C-extensions failed. This error can happen for. many reasons, often due to issues with your setup or how NumPy was. installed. We have compiled some common reasons and troubleshooting tips at:. https://numpy.org/devdocs/user/troubleshooting-importerror.html. Please note and check the following:. * The Python version is: Python3.8 from ""/usr/bin/python3"". * The NumPy version is: ""1.23.0"". and make sure that they are the versions you expect. Please carefully study the documentation linked above for further help. Original error was: libflexiblas.so.3: cannot open shared object file: No such file or directory. ```. **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? Any attempt to execute via `singularity run` leads to an error. **Any additional context:**. As far as I can tell, my environment meets the requirements for both Python and NumPy - though at the same time when I `singularity shell` into the SIF file, its versioning seems semi-independent of my main environment (Python 3.8.10 regardless of my environment's version, but using the NumPy 1.23.0 provided by my computing cluster). I feel like I'm missing something really simple, but I've tried the NumPy troubleshooting page and can't seem to crack this error. If it helps, I'm attempting this with `singularity` version 3.8.4, which is the newest version available to me in my computing cluster.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/610
https://github.com/google/deepvariant/issues/610:3593,deployability,version,version,3593,"le ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/context.py"", line 30, in <module>. import numpy as np. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/__init__.py"", line 140, in <module>. from . import core. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/__init__.py"", line 49, in <module>. raise ImportError(msg). ImportError:. IMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE! Importing the numpy C-extensions failed. This error can happen for. many reasons, often due to issues with your setup or how NumPy was. installed. We have compiled some common reasons and troubleshooting tips at:. https://numpy.org/devdocs/user/troubleshooting-importerror.html. Please note and check the following:. * The Python version is: Python3.8 from ""/usr/bin/python3"". * The NumPy version is: ""1.23.0"". and make sure that they are the versions you expect. Please carefully study the documentation linked above for further help. Original error was: libflexiblas.so.3: cannot open shared object file: No such file or directory. ```. **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? Any attempt to execute via `singularity run` leads to an error. **Any additional context:**. As far as I can tell, my environment meets the requirements for both Python and NumPy - though at the same time when I `singularity shell` into the SIF file, its versioning seems semi-independent of my main environment (Python 3.8.10 regardless of my environment's version, but using the NumPy 1.23.0 provided by my computing cluster). I feel like I'm missing something really simple, but I've tried the NumPy troubleshooting page and can't seem to crack this error. If it helps, I'm attempting this with `singularity` version 3.8.4, which is the newest version available to me in my computing cluster.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/610
https://github.com/google/deepvariant/issues/610:3628,deployability,version,version,3628,"le ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/context.py"", line 30, in <module>. import numpy as np. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/__init__.py"", line 140, in <module>. from . import core. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/__init__.py"", line 49, in <module>. raise ImportError(msg). ImportError:. IMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE! Importing the numpy C-extensions failed. This error can happen for. many reasons, often due to issues with your setup or how NumPy was. installed. We have compiled some common reasons and troubleshooting tips at:. https://numpy.org/devdocs/user/troubleshooting-importerror.html. Please note and check the following:. * The Python version is: Python3.8 from ""/usr/bin/python3"". * The NumPy version is: ""1.23.0"". and make sure that they are the versions you expect. Please carefully study the documentation linked above for further help. Original error was: libflexiblas.so.3: cannot open shared object file: No such file or directory. ```. **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? Any attempt to execute via `singularity run` leads to an error. **Any additional context:**. As far as I can tell, my environment meets the requirements for both Python and NumPy - though at the same time when I `singularity shell` into the SIF file, its versioning seems semi-independent of my main environment (Python 3.8.10 regardless of my environment's version, but using the NumPy 1.23.0 provided by my computing cluster). I feel like I'm missing something really simple, but I've tried the NumPy troubleshooting page and can't seem to crack this error. If it helps, I'm attempting this with `singularity` version 3.8.4, which is the newest version available to me in my computing cluster.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/610
https://github.com/google/deepvariant/issues/610:3668,deployability,cluster,cluster,3668,"le ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/context.py"", line 30, in <module>. import numpy as np. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/__init__.py"", line 140, in <module>. from . import core. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/__init__.py"", line 49, in <module>. raise ImportError(msg). ImportError:. IMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE! Importing the numpy C-extensions failed. This error can happen for. many reasons, often due to issues with your setup or how NumPy was. installed. We have compiled some common reasons and troubleshooting tips at:. https://numpy.org/devdocs/user/troubleshooting-importerror.html. Please note and check the following:. * The Python version is: Python3.8 from ""/usr/bin/python3"". * The NumPy version is: ""1.23.0"". and make sure that they are the versions you expect. Please carefully study the documentation linked above for further help. Original error was: libflexiblas.so.3: cannot open shared object file: No such file or directory. ```. **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? Any attempt to execute via `singularity run` leads to an error. **Any additional context:**. As far as I can tell, my environment meets the requirements for both Python and NumPy - though at the same time when I `singularity shell` into the SIF file, its versioning seems semi-independent of my main environment (Python 3.8.10 regardless of my environment's version, but using the NumPy 1.23.0 provided by my computing cluster). I feel like I'm missing something really simple, but I've tried the NumPy troubleshooting page and can't seem to crack this error. If it helps, I'm attempting this with `singularity` version 3.8.4, which is the newest version available to me in my computing cluster.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/610
https://github.com/google/deepvariant/issues/610:738,energy efficiency,core,core,738,"NumPy dependency error despite matching versions; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**: Yes. **Describe the issue:**. Unable to run Singularity image despite prerequisites appearing satisfied. **Setup**. - Operating system: CentOS 7 . - DeepVariant version: 1.4.0 (google/deepvariant:latest). - Installation method (Docker, built from source, etc.): `singularity pull docker://google/deepvariant:latest` and attempted execution via `singularity run`. - Type of data: N/A. **Steps to reproduce:**. - Command: `singularity run -B /home/ -B /scratch/ deepvariant.sif`. - Error trace:. ```. Traceback (most recent call last):. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/__init__.py"", line 23, in <module>. from . import multiarray. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/multiarray.py"", line 10, in <module>. from . import overrides. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/overrides.py"", line 6, in <module>. from numpy.core._multiarray_umath import (. ImportError: libflexiblas.so.3: cannot open shared object file: No such file or directory. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 48, in <module>. import tensorflow as tf. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/__init__.py"", line 41, in <module>. from tensorflow.python.tools import module_util as _module_util. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/__init__.py"", line 41, in <module>. from tensorflow.python.eager import context. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/context.py"", line 30, in <module>. import numpy as np. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/__init__.py"", line 140, in <module>. from . import core. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/__init__.py"", li",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/610
https://github.com/google/deepvariant/issues/610:867,energy efficiency,core,core,867,"NumPy dependency error despite matching versions; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**: Yes. **Describe the issue:**. Unable to run Singularity image despite prerequisites appearing satisfied. **Setup**. - Operating system: CentOS 7 . - DeepVariant version: 1.4.0 (google/deepvariant:latest). - Installation method (Docker, built from source, etc.): `singularity pull docker://google/deepvariant:latest` and attempted execution via `singularity run`. - Type of data: N/A. **Steps to reproduce:**. - Command: `singularity run -B /home/ -B /scratch/ deepvariant.sif`. - Error trace:. ```. Traceback (most recent call last):. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/__init__.py"", line 23, in <module>. from . import multiarray. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/multiarray.py"", line 10, in <module>. from . import overrides. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/overrides.py"", line 6, in <module>. from numpy.core._multiarray_umath import (. ImportError: libflexiblas.so.3: cannot open shared object file: No such file or directory. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 48, in <module>. import tensorflow as tf. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/__init__.py"", line 41, in <module>. from tensorflow.python.tools import module_util as _module_util. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/__init__.py"", line 41, in <module>. from tensorflow.python.eager import context. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/context.py"", line 30, in <module>. import numpy as np. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/__init__.py"", line 140, in <module>. from . import core. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/__init__.py"", li",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/610
https://github.com/google/deepvariant/issues/610:997,energy efficiency,core,core,997,"NumPy dependency error despite matching versions; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**: Yes. **Describe the issue:**. Unable to run Singularity image despite prerequisites appearing satisfied. **Setup**. - Operating system: CentOS 7 . - DeepVariant version: 1.4.0 (google/deepvariant:latest). - Installation method (Docker, built from source, etc.): `singularity pull docker://google/deepvariant:latest` and attempted execution via `singularity run`. - Type of data: N/A. **Steps to reproduce:**. - Command: `singularity run -B /home/ -B /scratch/ deepvariant.sif`. - Error trace:. ```. Traceback (most recent call last):. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/__init__.py"", line 23, in <module>. from . import multiarray. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/multiarray.py"", line 10, in <module>. from . import overrides. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/overrides.py"", line 6, in <module>. from numpy.core._multiarray_umath import (. ImportError: libflexiblas.so.3: cannot open shared object file: No such file or directory. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 48, in <module>. import tensorflow as tf. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/__init__.py"", line 41, in <module>. from tensorflow.python.tools import module_util as _module_util. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/__init__.py"", line 41, in <module>. from tensorflow.python.eager import context. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/context.py"", line 30, in <module>. import numpy as np. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/__init__.py"", line 140, in <module>. from . import core. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/__init__.py"", li",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/610
https://github.com/google/deepvariant/issues/610:1049,energy efficiency,core,core,1049,"*Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**: Yes. **Describe the issue:**. Unable to run Singularity image despite prerequisites appearing satisfied. **Setup**. - Operating system: CentOS 7 . - DeepVariant version: 1.4.0 (google/deepvariant:latest). - Installation method (Docker, built from source, etc.): `singularity pull docker://google/deepvariant:latest` and attempted execution via `singularity run`. - Type of data: N/A. **Steps to reproduce:**. - Command: `singularity run -B /home/ -B /scratch/ deepvariant.sif`. - Error trace:. ```. Traceback (most recent call last):. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/__init__.py"", line 23, in <module>. from . import multiarray. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/multiarray.py"", line 10, in <module>. from . import overrides. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/overrides.py"", line 6, in <module>. from numpy.core._multiarray_umath import (. ImportError: libflexiblas.so.3: cannot open shared object file: No such file or directory. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 48, in <module>. import tensorflow as tf. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/__init__.py"", line 41, in <module>. from tensorflow.python.tools import module_util as _module_util. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/__init__.py"", line 41, in <module>. from tensorflow.python.eager import context. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/context.py"", line 30, in <module>. import numpy as np. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/__init__.py"", line 140, in <module>. from . import core. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/__init__.py"", line 49, in <module>. raise ImportError(msg). ImportE",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/610
https://github.com/google/deepvariant/issues/610:1911,energy efficiency,core,core,1911,"m . import overrides. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/overrides.py"", line 6, in <module>. from numpy.core._multiarray_umath import (. ImportError: libflexiblas.so.3: cannot open shared object file: No such file or directory. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 48, in <module>. import tensorflow as tf. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/__init__.py"", line 41, in <module>. from tensorflow.python.tools import module_util as _module_util. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/__init__.py"", line 41, in <module>. from tensorflow.python.eager import context. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/context.py"", line 30, in <module>. import numpy as np. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/__init__.py"", line 140, in <module>. from . import core. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/__init__.py"", line 49, in <module>. raise ImportError(msg). ImportError:. IMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE! Importing the numpy C-extensions failed. This error can happen for. many reasons, often due to issues with your setup or how NumPy was. installed. We have compiled some common reasons and troubleshooting tips at:. https://numpy.org/devdocs/user/troubleshooting-importerror.html. Please note and check the following:. * The Python version is: Python3.8 from ""/usr/bin/python3"". * The NumPy version is: ""1.23.0"". and make sure that they are the versions you expect. Please carefully study the documentation linked above for further help. Original error was: libflexiblas.so.3: cannot open shared object file: No such file or directory. ```. **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/610
https://github.com/google/deepvariant/issues/610:1979,energy efficiency,core,core,1979,"-packages/numpy/core/overrides.py"", line 6, in <module>. from numpy.core._multiarray_umath import (. ImportError: libflexiblas.so.3: cannot open shared object file: No such file or directory. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 48, in <module>. import tensorflow as tf. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/__init__.py"", line 41, in <module>. from tensorflow.python.tools import module_util as _module_util. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/__init__.py"", line 41, in <module>. from tensorflow.python.eager import context. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/context.py"", line 30, in <module>. import numpy as np. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/__init__.py"", line 140, in <module>. from . import core. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/__init__.py"", line 49, in <module>. raise ImportError(msg). ImportError:. IMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE! Importing the numpy C-extensions failed. This error can happen for. many reasons, often due to issues with your setup or how NumPy was. installed. We have compiled some common reasons and troubleshooting tips at:. https://numpy.org/devdocs/user/troubleshooting-importerror.html. Please note and check the following:. * The Python version is: Python3.8 from ""/usr/bin/python3"". * The NumPy version is: ""1.23.0"". and make sure that they are the versions you expect. Please carefully study the documentation linked above for further help. Original error was: libflexiblas.so.3: cannot open shared object file: No such file or directory. ```. **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/610
https://github.com/google/deepvariant/issues/610:6,integrability,depend,dependency,6,"NumPy dependency error despite matching versions; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**: Yes. **Describe the issue:**. Unable to run Singularity image despite prerequisites appearing satisfied. **Setup**. - Operating system: CentOS 7 . - DeepVariant version: 1.4.0 (google/deepvariant:latest). - Installation method (Docker, built from source, etc.): `singularity pull docker://google/deepvariant:latest` and attempted execution via `singularity run`. - Type of data: N/A. **Steps to reproduce:**. - Command: `singularity run -B /home/ -B /scratch/ deepvariant.sif`. - Error trace:. ```. Traceback (most recent call last):. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/__init__.py"", line 23, in <module>. from . import multiarray. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/multiarray.py"", line 10, in <module>. from . import overrides. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/overrides.py"", line 6, in <module>. from numpy.core._multiarray_umath import (. ImportError: libflexiblas.so.3: cannot open shared object file: No such file or directory. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 48, in <module>. import tensorflow as tf. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/__init__.py"", line 41, in <module>. from tensorflow.python.tools import module_util as _module_util. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/__init__.py"", line 41, in <module>. from tensorflow.python.eager import context. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/context.py"", line 30, in <module>. import numpy as np. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/__init__.py"", line 140, in <module>. from . import core. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/__init__.py"", li",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/610
https://github.com/google/deepvariant/issues/610:40,integrability,version,versions,40,"NumPy dependency error despite matching versions; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**: Yes. **Describe the issue:**. Unable to run Singularity image despite prerequisites appearing satisfied. **Setup**. - Operating system: CentOS 7 . - DeepVariant version: 1.4.0 (google/deepvariant:latest). - Installation method (Docker, built from source, etc.): `singularity pull docker://google/deepvariant:latest` and attempted execution via `singularity run`. - Type of data: N/A. **Steps to reproduce:**. - Command: `singularity run -B /home/ -B /scratch/ deepvariant.sif`. - Error trace:. ```. Traceback (most recent call last):. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/__init__.py"", line 23, in <module>. from . import multiarray. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/multiarray.py"", line 10, in <module>. from . import overrides. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/overrides.py"", line 6, in <module>. from numpy.core._multiarray_umath import (. ImportError: libflexiblas.so.3: cannot open shared object file: No such file or directory. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 48, in <module>. import tensorflow as tf. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/__init__.py"", line 41, in <module>. from tensorflow.python.tools import module_util as _module_util. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/__init__.py"", line 41, in <module>. from tensorflow.python.eager import context. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/context.py"", line 30, in <module>. import numpy as np. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/__init__.py"", line 140, in <module>. from . import core. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/__init__.py"", li",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/610
https://github.com/google/deepvariant/issues/610:302,integrability,version,version,302,"NumPy dependency error despite matching versions; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**: Yes. **Describe the issue:**. Unable to run Singularity image despite prerequisites appearing satisfied. **Setup**. - Operating system: CentOS 7 . - DeepVariant version: 1.4.0 (google/deepvariant:latest). - Installation method (Docker, built from source, etc.): `singularity pull docker://google/deepvariant:latest` and attempted execution via `singularity run`. - Type of data: N/A. **Steps to reproduce:**. - Command: `singularity run -B /home/ -B /scratch/ deepvariant.sif`. - Error trace:. ```. Traceback (most recent call last):. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/__init__.py"", line 23, in <module>. from . import multiarray. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/multiarray.py"", line 10, in <module>. from . import overrides. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/overrides.py"", line 6, in <module>. from numpy.core._multiarray_umath import (. ImportError: libflexiblas.so.3: cannot open shared object file: No such file or directory. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 48, in <module>. import tensorflow as tf. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/__init__.py"", line 41, in <module>. from tensorflow.python.tools import module_util as _module_util. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/__init__.py"", line 41, in <module>. from tensorflow.python.eager import context. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/context.py"", line 30, in <module>. import numpy as np. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/__init__.py"", line 140, in <module>. from . import core. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/__init__.py"", li",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/610
https://github.com/google/deepvariant/issues/610:2455,integrability,version,version,2455,"ule>. from tensorflow.python.tools import module_util as _module_util. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/__init__.py"", line 41, in <module>. from tensorflow.python.eager import context. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/context.py"", line 30, in <module>. import numpy as np. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/__init__.py"", line 140, in <module>. from . import core. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/__init__.py"", line 49, in <module>. raise ImportError(msg). ImportError:. IMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE! Importing the numpy C-extensions failed. This error can happen for. many reasons, often due to issues with your setup or how NumPy was. installed. We have compiled some common reasons and troubleshooting tips at:. https://numpy.org/devdocs/user/troubleshooting-importerror.html. Please note and check the following:. * The Python version is: Python3.8 from ""/usr/bin/python3"". * The NumPy version is: ""1.23.0"". and make sure that they are the versions you expect. Please carefully study the documentation linked above for further help. Original error was: libflexiblas.so.3: cannot open shared object file: No such file or directory. ```. **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? Any attempt to execute via `singularity run` leads to an error. **Any additional context:**. As far as I can tell, my environment meets the requirements for both Python and NumPy - though at the same time when I `singularity shell` into the SIF file, its versioning seems semi-independent of my main environment (Python 3.8.10 regardless of my environment's version, but using the NumPy 1.23.0 provided by my computing cluster). I feel like I'm missing something really simple, ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/610
https://github.com/google/deepvariant/issues/610:2514,integrability,version,version,2514,"odule_util. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/__init__.py"", line 41, in <module>. from tensorflow.python.eager import context. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/context.py"", line 30, in <module>. import numpy as np. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/__init__.py"", line 140, in <module>. from . import core. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/__init__.py"", line 49, in <module>. raise ImportError(msg). ImportError:. IMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE! Importing the numpy C-extensions failed. This error can happen for. many reasons, often due to issues with your setup or how NumPy was. installed. We have compiled some common reasons and troubleshooting tips at:. https://numpy.org/devdocs/user/troubleshooting-importerror.html. Please note and check the following:. * The Python version is: Python3.8 from ""/usr/bin/python3"". * The NumPy version is: ""1.23.0"". and make sure that they are the versions you expect. Please carefully study the documentation linked above for further help. Original error was: libflexiblas.so.3: cannot open shared object file: No such file or directory. ```. **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? Any attempt to execute via `singularity run` leads to an error. **Any additional context:**. As far as I can tell, my environment meets the requirements for both Python and NumPy - though at the same time when I `singularity shell` into the SIF file, its versioning seems semi-independent of my main environment (Python 3.8.10 regardless of my environment's version, but using the NumPy 1.23.0 provided by my computing cluster). I feel like I'm missing something really simple, but I've tried the NumPy troubleshooting page and can't see",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/610
https://github.com/google/deepvariant/issues/610:2568,integrability,version,versions,2568,"es/tensorflow/python/__init__.py"", line 41, in <module>. from tensorflow.python.eager import context. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/context.py"", line 30, in <module>. import numpy as np. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/__init__.py"", line 140, in <module>. from . import core. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/__init__.py"", line 49, in <module>. raise ImportError(msg). ImportError:. IMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE! Importing the numpy C-extensions failed. This error can happen for. many reasons, often due to issues with your setup or how NumPy was. installed. We have compiled some common reasons and troubleshooting tips at:. https://numpy.org/devdocs/user/troubleshooting-importerror.html. Please note and check the following:. * The Python version is: Python3.8 from ""/usr/bin/python3"". * The NumPy version is: ""1.23.0"". and make sure that they are the versions you expect. Please carefully study the documentation linked above for further help. Original error was: libflexiblas.so.3: cannot open shared object file: No such file or directory. ```. **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? Any attempt to execute via `singularity run` leads to an error. **Any additional context:**. As far as I can tell, my environment meets the requirements for both Python and NumPy - though at the same time when I `singularity shell` into the SIF file, its versioning seems semi-independent of my main environment (Python 3.8.10 regardless of my environment's version, but using the NumPy 1.23.0 provided by my computing cluster). I feel like I'm missing something really simple, but I've tried the NumPy troubleshooting page and can't seem to crack this error. If it helps, I'm attempting thi",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/610
https://github.com/google/deepvariant/issues/610:3236,integrability,version,versioning,3236,"le ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/context.py"", line 30, in <module>. import numpy as np. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/__init__.py"", line 140, in <module>. from . import core. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/__init__.py"", line 49, in <module>. raise ImportError(msg). ImportError:. IMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE! Importing the numpy C-extensions failed. This error can happen for. many reasons, often due to issues with your setup or how NumPy was. installed. We have compiled some common reasons and troubleshooting tips at:. https://numpy.org/devdocs/user/troubleshooting-importerror.html. Please note and check the following:. * The Python version is: Python3.8 from ""/usr/bin/python3"". * The NumPy version is: ""1.23.0"". and make sure that they are the versions you expect. Please carefully study the documentation linked above for further help. Original error was: libflexiblas.so.3: cannot open shared object file: No such file or directory. ```. **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? Any attempt to execute via `singularity run` leads to an error. **Any additional context:**. As far as I can tell, my environment meets the requirements for both Python and NumPy - though at the same time when I `singularity shell` into the SIF file, its versioning seems semi-independent of my main environment (Python 3.8.10 regardless of my environment's version, but using the NumPy 1.23.0 provided by my computing cluster). I feel like I'm missing something really simple, but I've tried the NumPy troubleshooting page and can't seem to crack this error. If it helps, I'm attempting this with `singularity` version 3.8.4, which is the newest version available to me in my computing cluster.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/610
https://github.com/google/deepvariant/issues/610:3339,integrability,version,version,3339,"le ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/context.py"", line 30, in <module>. import numpy as np. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/__init__.py"", line 140, in <module>. from . import core. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/__init__.py"", line 49, in <module>. raise ImportError(msg). ImportError:. IMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE! Importing the numpy C-extensions failed. This error can happen for. many reasons, often due to issues with your setup or how NumPy was. installed. We have compiled some common reasons and troubleshooting tips at:. https://numpy.org/devdocs/user/troubleshooting-importerror.html. Please note and check the following:. * The Python version is: Python3.8 from ""/usr/bin/python3"". * The NumPy version is: ""1.23.0"". and make sure that they are the versions you expect. Please carefully study the documentation linked above for further help. Original error was: libflexiblas.so.3: cannot open shared object file: No such file or directory. ```. **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? Any attempt to execute via `singularity run` leads to an error. **Any additional context:**. As far as I can tell, my environment meets the requirements for both Python and NumPy - though at the same time when I `singularity shell` into the SIF file, its versioning seems semi-independent of my main environment (Python 3.8.10 regardless of my environment's version, but using the NumPy 1.23.0 provided by my computing cluster). I feel like I'm missing something really simple, but I've tried the NumPy troubleshooting page and can't seem to crack this error. If it helps, I'm attempting this with `singularity` version 3.8.4, which is the newest version available to me in my computing cluster.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/610
https://github.com/google/deepvariant/issues/610:3593,integrability,version,version,3593,"le ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/context.py"", line 30, in <module>. import numpy as np. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/__init__.py"", line 140, in <module>. from . import core. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/__init__.py"", line 49, in <module>. raise ImportError(msg). ImportError:. IMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE! Importing the numpy C-extensions failed. This error can happen for. many reasons, often due to issues with your setup or how NumPy was. installed. We have compiled some common reasons and troubleshooting tips at:. https://numpy.org/devdocs/user/troubleshooting-importerror.html. Please note and check the following:. * The Python version is: Python3.8 from ""/usr/bin/python3"". * The NumPy version is: ""1.23.0"". and make sure that they are the versions you expect. Please carefully study the documentation linked above for further help. Original error was: libflexiblas.so.3: cannot open shared object file: No such file or directory. ```. **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? Any attempt to execute via `singularity run` leads to an error. **Any additional context:**. As far as I can tell, my environment meets the requirements for both Python and NumPy - though at the same time when I `singularity shell` into the SIF file, its versioning seems semi-independent of my main environment (Python 3.8.10 regardless of my environment's version, but using the NumPy 1.23.0 provided by my computing cluster). I feel like I'm missing something really simple, but I've tried the NumPy troubleshooting page and can't seem to crack this error. If it helps, I'm attempting this with `singularity` version 3.8.4, which is the newest version available to me in my computing cluster.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/610
https://github.com/google/deepvariant/issues/610:3628,integrability,version,version,3628,"le ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/context.py"", line 30, in <module>. import numpy as np. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/__init__.py"", line 140, in <module>. from . import core. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/__init__.py"", line 49, in <module>. raise ImportError(msg). ImportError:. IMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE! Importing the numpy C-extensions failed. This error can happen for. many reasons, often due to issues with your setup or how NumPy was. installed. We have compiled some common reasons and troubleshooting tips at:. https://numpy.org/devdocs/user/troubleshooting-importerror.html. Please note and check the following:. * The Python version is: Python3.8 from ""/usr/bin/python3"". * The NumPy version is: ""1.23.0"". and make sure that they are the versions you expect. Please carefully study the documentation linked above for further help. Original error was: libflexiblas.so.3: cannot open shared object file: No such file or directory. ```. **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? Any attempt to execute via `singularity run` leads to an error. **Any additional context:**. As far as I can tell, my environment meets the requirements for both Python and NumPy - though at the same time when I `singularity shell` into the SIF file, its versioning seems semi-independent of my main environment (Python 3.8.10 regardless of my environment's version, but using the NumPy 1.23.0 provided by my computing cluster). I feel like I'm missing something really simple, but I've tried the NumPy troubleshooting page and can't seem to crack this error. If it helps, I'm attempting this with `singularity` version 3.8.4, which is the newest version available to me in my computing cluster.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/610
https://github.com/google/deepvariant/issues/610:1126,interoperability,share,shared,1126,"s/FAQ.md**: Yes. **Describe the issue:**. Unable to run Singularity image despite prerequisites appearing satisfied. **Setup**. - Operating system: CentOS 7 . - DeepVariant version: 1.4.0 (google/deepvariant:latest). - Installation method (Docker, built from source, etc.): `singularity pull docker://google/deepvariant:latest` and attempted execution via `singularity run`. - Type of data: N/A. **Steps to reproduce:**. - Command: `singularity run -B /home/ -B /scratch/ deepvariant.sif`. - Error trace:. ```. Traceback (most recent call last):. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/__init__.py"", line 23, in <module>. from . import multiarray. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/multiarray.py"", line 10, in <module>. from . import overrides. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/overrides.py"", line 6, in <module>. from numpy.core._multiarray_umath import (. ImportError: libflexiblas.so.3: cannot open shared object file: No such file or directory. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 48, in <module>. import tensorflow as tf. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/__init__.py"", line 41, in <module>. from tensorflow.python.tools import module_util as _module_util. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/__init__.py"", line 41, in <module>. from tensorflow.python.eager import context. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/context.py"", line 30, in <module>. import numpy as np. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/__init__.py"", line 140, in <module>. from . import core. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/__init__.py"", line 49, in <module>. raise ImportError(msg). ImportError:. IMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE! Impo",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/610
https://github.com/google/deepvariant/issues/610:2712,interoperability,share,shared,2712,"le ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/context.py"", line 30, in <module>. import numpy as np. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/__init__.py"", line 140, in <module>. from . import core. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/__init__.py"", line 49, in <module>. raise ImportError(msg). ImportError:. IMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE! Importing the numpy C-extensions failed. This error can happen for. many reasons, often due to issues with your setup or how NumPy was. installed. We have compiled some common reasons and troubleshooting tips at:. https://numpy.org/devdocs/user/troubleshooting-importerror.html. Please note and check the following:. * The Python version is: Python3.8 from ""/usr/bin/python3"". * The NumPy version is: ""1.23.0"". and make sure that they are the versions you expect. Please carefully study the documentation linked above for further help. Original error was: libflexiblas.so.3: cannot open shared object file: No such file or directory. ```. **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? Any attempt to execute via `singularity run` leads to an error. **Any additional context:**. As far as I can tell, my environment meets the requirements for both Python and NumPy - though at the same time when I `singularity shell` into the SIF file, its versioning seems semi-independent of my main environment (Python 3.8.10 regardless of my environment's version, but using the NumPy 1.23.0 provided by my computing cluster). I feel like I'm missing something really simple, but I've tried the NumPy troubleshooting page and can't seem to crack this error. If it helps, I'm attempting this with `singularity` version 3.8.4, which is the newest version available to me in my computing cluster.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/610
https://github.com/google/deepvariant/issues/610:6,modifiability,depend,dependency,6,"NumPy dependency error despite matching versions; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**: Yes. **Describe the issue:**. Unable to run Singularity image despite prerequisites appearing satisfied. **Setup**. - Operating system: CentOS 7 . - DeepVariant version: 1.4.0 (google/deepvariant:latest). - Installation method (Docker, built from source, etc.): `singularity pull docker://google/deepvariant:latest` and attempted execution via `singularity run`. - Type of data: N/A. **Steps to reproduce:**. - Command: `singularity run -B /home/ -B /scratch/ deepvariant.sif`. - Error trace:. ```. Traceback (most recent call last):. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/__init__.py"", line 23, in <module>. from . import multiarray. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/multiarray.py"", line 10, in <module>. from . import overrides. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/overrides.py"", line 6, in <module>. from numpy.core._multiarray_umath import (. ImportError: libflexiblas.so.3: cannot open shared object file: No such file or directory. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 48, in <module>. import tensorflow as tf. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/__init__.py"", line 41, in <module>. from tensorflow.python.tools import module_util as _module_util. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/__init__.py"", line 41, in <module>. from tensorflow.python.eager import context. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/context.py"", line 30, in <module>. import numpy as np. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/__init__.py"", line 140, in <module>. from . import core. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/__init__.py"", li",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/610
https://github.com/google/deepvariant/issues/610:40,modifiability,version,versions,40,"NumPy dependency error despite matching versions; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**: Yes. **Describe the issue:**. Unable to run Singularity image despite prerequisites appearing satisfied. **Setup**. - Operating system: CentOS 7 . - DeepVariant version: 1.4.0 (google/deepvariant:latest). - Installation method (Docker, built from source, etc.): `singularity pull docker://google/deepvariant:latest` and attempted execution via `singularity run`. - Type of data: N/A. **Steps to reproduce:**. - Command: `singularity run -B /home/ -B /scratch/ deepvariant.sif`. - Error trace:. ```. Traceback (most recent call last):. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/__init__.py"", line 23, in <module>. from . import multiarray. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/multiarray.py"", line 10, in <module>. from . import overrides. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/overrides.py"", line 6, in <module>. from numpy.core._multiarray_umath import (. ImportError: libflexiblas.so.3: cannot open shared object file: No such file or directory. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 48, in <module>. import tensorflow as tf. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/__init__.py"", line 41, in <module>. from tensorflow.python.tools import module_util as _module_util. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/__init__.py"", line 41, in <module>. from tensorflow.python.eager import context. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/context.py"", line 30, in <module>. import numpy as np. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/__init__.py"", line 140, in <module>. from . import core. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/__init__.py"", li",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/610
https://github.com/google/deepvariant/issues/610:302,modifiability,version,version,302,"NumPy dependency error despite matching versions; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**: Yes. **Describe the issue:**. Unable to run Singularity image despite prerequisites appearing satisfied. **Setup**. - Operating system: CentOS 7 . - DeepVariant version: 1.4.0 (google/deepvariant:latest). - Installation method (Docker, built from source, etc.): `singularity pull docker://google/deepvariant:latest` and attempted execution via `singularity run`. - Type of data: N/A. **Steps to reproduce:**. - Command: `singularity run -B /home/ -B /scratch/ deepvariant.sif`. - Error trace:. ```. Traceback (most recent call last):. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/__init__.py"", line 23, in <module>. from . import multiarray. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/multiarray.py"", line 10, in <module>. from . import overrides. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/overrides.py"", line 6, in <module>. from numpy.core._multiarray_umath import (. ImportError: libflexiblas.so.3: cannot open shared object file: No such file or directory. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 48, in <module>. import tensorflow as tf. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/__init__.py"", line 41, in <module>. from tensorflow.python.tools import module_util as _module_util. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/__init__.py"", line 41, in <module>. from tensorflow.python.eager import context. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/context.py"", line 30, in <module>. import numpy as np. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/__init__.py"", line 140, in <module>. from . import core. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/__init__.py"", li",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/610
https://github.com/google/deepvariant/issues/610:723,modifiability,pac,packages,723,"NumPy dependency error despite matching versions; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**: Yes. **Describe the issue:**. Unable to run Singularity image despite prerequisites appearing satisfied. **Setup**. - Operating system: CentOS 7 . - DeepVariant version: 1.4.0 (google/deepvariant:latest). - Installation method (Docker, built from source, etc.): `singularity pull docker://google/deepvariant:latest` and attempted execution via `singularity run`. - Type of data: N/A. **Steps to reproduce:**. - Command: `singularity run -B /home/ -B /scratch/ deepvariant.sif`. - Error trace:. ```. Traceback (most recent call last):. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/__init__.py"", line 23, in <module>. from . import multiarray. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/multiarray.py"", line 10, in <module>. from . import overrides. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/overrides.py"", line 6, in <module>. from numpy.core._multiarray_umath import (. ImportError: libflexiblas.so.3: cannot open shared object file: No such file or directory. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 48, in <module>. import tensorflow as tf. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/__init__.py"", line 41, in <module>. from tensorflow.python.tools import module_util as _module_util. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/__init__.py"", line 41, in <module>. from tensorflow.python.eager import context. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/context.py"", line 30, in <module>. import numpy as np. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/__init__.py"", line 140, in <module>. from . import core. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/__init__.py"", li",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/610
https://github.com/google/deepvariant/issues/610:770,modifiability,modul,module,770,"NumPy dependency error despite matching versions; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**: Yes. **Describe the issue:**. Unable to run Singularity image despite prerequisites appearing satisfied. **Setup**. - Operating system: CentOS 7 . - DeepVariant version: 1.4.0 (google/deepvariant:latest). - Installation method (Docker, built from source, etc.): `singularity pull docker://google/deepvariant:latest` and attempted execution via `singularity run`. - Type of data: N/A. **Steps to reproduce:**. - Command: `singularity run -B /home/ -B /scratch/ deepvariant.sif`. - Error trace:. ```. Traceback (most recent call last):. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/__init__.py"", line 23, in <module>. from . import multiarray. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/multiarray.py"", line 10, in <module>. from . import overrides. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/overrides.py"", line 6, in <module>. from numpy.core._multiarray_umath import (. ImportError: libflexiblas.so.3: cannot open shared object file: No such file or directory. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 48, in <module>. import tensorflow as tf. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/__init__.py"", line 41, in <module>. from tensorflow.python.tools import module_util as _module_util. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/__init__.py"", line 41, in <module>. from tensorflow.python.eager import context. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/context.py"", line 30, in <module>. import numpy as np. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/__init__.py"", line 140, in <module>. from . import core. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/__init__.py"", li",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/610
https://github.com/google/deepvariant/issues/610:852,modifiability,pac,packages,852,"NumPy dependency error despite matching versions; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**: Yes. **Describe the issue:**. Unable to run Singularity image despite prerequisites appearing satisfied. **Setup**. - Operating system: CentOS 7 . - DeepVariant version: 1.4.0 (google/deepvariant:latest). - Installation method (Docker, built from source, etc.): `singularity pull docker://google/deepvariant:latest` and attempted execution via `singularity run`. - Type of data: N/A. **Steps to reproduce:**. - Command: `singularity run -B /home/ -B /scratch/ deepvariant.sif`. - Error trace:. ```. Traceback (most recent call last):. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/__init__.py"", line 23, in <module>. from . import multiarray. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/multiarray.py"", line 10, in <module>. from . import overrides. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/overrides.py"", line 6, in <module>. from numpy.core._multiarray_umath import (. ImportError: libflexiblas.so.3: cannot open shared object file: No such file or directory. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 48, in <module>. import tensorflow as tf. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/__init__.py"", line 41, in <module>. from tensorflow.python.tools import module_util as _module_util. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/__init__.py"", line 41, in <module>. from tensorflow.python.eager import context. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/context.py"", line 30, in <module>. import numpy as np. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/__init__.py"", line 140, in <module>. from . import core. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/__init__.py"", li",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/610
https://github.com/google/deepvariant/issues/610:901,modifiability,modul,module,901,"NumPy dependency error despite matching versions; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**: Yes. **Describe the issue:**. Unable to run Singularity image despite prerequisites appearing satisfied. **Setup**. - Operating system: CentOS 7 . - DeepVariant version: 1.4.0 (google/deepvariant:latest). - Installation method (Docker, built from source, etc.): `singularity pull docker://google/deepvariant:latest` and attempted execution via `singularity run`. - Type of data: N/A. **Steps to reproduce:**. - Command: `singularity run -B /home/ -B /scratch/ deepvariant.sif`. - Error trace:. ```. Traceback (most recent call last):. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/__init__.py"", line 23, in <module>. from . import multiarray. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/multiarray.py"", line 10, in <module>. from . import overrides. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/overrides.py"", line 6, in <module>. from numpy.core._multiarray_umath import (. ImportError: libflexiblas.so.3: cannot open shared object file: No such file or directory. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 48, in <module>. import tensorflow as tf. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/__init__.py"", line 41, in <module>. from tensorflow.python.tools import module_util as _module_util. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/__init__.py"", line 41, in <module>. from tensorflow.python.eager import context. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/context.py"", line 30, in <module>. import numpy as np. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/__init__.py"", line 140, in <module>. from . import core. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/__init__.py"", li",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/610
https://github.com/google/deepvariant/issues/610:982,modifiability,pac,packages,982,"NumPy dependency error despite matching versions; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**: Yes. **Describe the issue:**. Unable to run Singularity image despite prerequisites appearing satisfied. **Setup**. - Operating system: CentOS 7 . - DeepVariant version: 1.4.0 (google/deepvariant:latest). - Installation method (Docker, built from source, etc.): `singularity pull docker://google/deepvariant:latest` and attempted execution via `singularity run`. - Type of data: N/A. **Steps to reproduce:**. - Command: `singularity run -B /home/ -B /scratch/ deepvariant.sif`. - Error trace:. ```. Traceback (most recent call last):. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/__init__.py"", line 23, in <module>. from . import multiarray. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/multiarray.py"", line 10, in <module>. from . import overrides. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/overrides.py"", line 6, in <module>. from numpy.core._multiarray_umath import (. ImportError: libflexiblas.so.3: cannot open shared object file: No such file or directory. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 48, in <module>. import tensorflow as tf. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/__init__.py"", line 41, in <module>. from tensorflow.python.tools import module_util as _module_util. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/__init__.py"", line 41, in <module>. from tensorflow.python.eager import context. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/context.py"", line 30, in <module>. import numpy as np. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/__init__.py"", line 140, in <module>. from . import core. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/__init__.py"", li",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/610
https://github.com/google/deepvariant/issues/610:1029,modifiability,modul,module,1029,"atching versions; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**: Yes. **Describe the issue:**. Unable to run Singularity image despite prerequisites appearing satisfied. **Setup**. - Operating system: CentOS 7 . - DeepVariant version: 1.4.0 (google/deepvariant:latest). - Installation method (Docker, built from source, etc.): `singularity pull docker://google/deepvariant:latest` and attempted execution via `singularity run`. - Type of data: N/A. **Steps to reproduce:**. - Command: `singularity run -B /home/ -B /scratch/ deepvariant.sif`. - Error trace:. ```. Traceback (most recent call last):. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/__init__.py"", line 23, in <module>. from . import multiarray. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/multiarray.py"", line 10, in <module>. from . import overrides. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/overrides.py"", line 6, in <module>. from numpy.core._multiarray_umath import (. ImportError: libflexiblas.so.3: cannot open shared object file: No such file or directory. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 48, in <module>. import tensorflow as tf. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/__init__.py"", line 41, in <module>. from tensorflow.python.tools import module_util as _module_util. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/__init__.py"", line 41, in <module>. from tensorflow.python.eager import context. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/context.py"", line 30, in <module>. import numpy as np. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/__init__.py"", line 140, in <module>. from . import core. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/__init__.py"", line 49, in <module>. raise Import",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/610
https://github.com/google/deepvariant/issues/610:1339,modifiability,modul,module,1339,"t). - Installation method (Docker, built from source, etc.): `singularity pull docker://google/deepvariant:latest` and attempted execution via `singularity run`. - Type of data: N/A. **Steps to reproduce:**. - Command: `singularity run -B /home/ -B /scratch/ deepvariant.sif`. - Error trace:. ```. Traceback (most recent call last):. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/__init__.py"", line 23, in <module>. from . import multiarray. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/multiarray.py"", line 10, in <module>. from . import overrides. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/overrides.py"", line 6, in <module>. from numpy.core._multiarray_umath import (. ImportError: libflexiblas.so.3: cannot open shared object file: No such file or directory. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 48, in <module>. import tensorflow as tf. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/__init__.py"", line 41, in <module>. from tensorflow.python.tools import module_util as _module_util. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/__init__.py"", line 41, in <module>. from tensorflow.python.eager import context. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/context.py"", line 30, in <module>. import numpy as np. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/__init__.py"", line 140, in <module>. from . import core. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/__init__.py"", line 49, in <module>. raise ImportError(msg). ImportError:. IMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE! Importing the numpy C-extensions failed. This error can happen for. many reasons, often due to issues with your setup or how NumPy was. installed. We have compiled some common reasons and troubleshooting tips at:. htt",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/610
https://github.com/google/deepvariant/issues/610:1409,modifiability,pac,packages,1409,"ty pull docker://google/deepvariant:latest` and attempted execution via `singularity run`. - Type of data: N/A. **Steps to reproduce:**. - Command: `singularity run -B /home/ -B /scratch/ deepvariant.sif`. - Error trace:. ```. Traceback (most recent call last):. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/__init__.py"", line 23, in <module>. from . import multiarray. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/multiarray.py"", line 10, in <module>. from . import overrides. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/overrides.py"", line 6, in <module>. from numpy.core._multiarray_umath import (. ImportError: libflexiblas.so.3: cannot open shared object file: No such file or directory. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 48, in <module>. import tensorflow as tf. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/__init__.py"", line 41, in <module>. from tensorflow.python.tools import module_util as _module_util. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/__init__.py"", line 41, in <module>. from tensorflow.python.eager import context. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/context.py"", line 30, in <module>. import numpy as np. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/__init__.py"", line 140, in <module>. from . import core. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/__init__.py"", line 49, in <module>. raise ImportError(msg). ImportError:. IMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE! Importing the numpy C-extensions failed. This error can happen for. many reasons, often due to issues with your setup or how NumPy was. installed. We have compiled some common reasons and troubleshooting tips at:. https://numpy.org/devdocs/user/troubleshooting-importerror.html. Please no",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/610
https://github.com/google/deepvariant/issues/610:1456,modifiability,modul,module,1456,"d attempted execution via `singularity run`. - Type of data: N/A. **Steps to reproduce:**. - Command: `singularity run -B /home/ -B /scratch/ deepvariant.sif`. - Error trace:. ```. Traceback (most recent call last):. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/__init__.py"", line 23, in <module>. from . import multiarray. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/multiarray.py"", line 10, in <module>. from . import overrides. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/overrides.py"", line 6, in <module>. from numpy.core._multiarray_umath import (. ImportError: libflexiblas.so.3: cannot open shared object file: No such file or directory. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 48, in <module>. import tensorflow as tf. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/__init__.py"", line 41, in <module>. from tensorflow.python.tools import module_util as _module_util. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/__init__.py"", line 41, in <module>. from tensorflow.python.eager import context. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/context.py"", line 30, in <module>. import numpy as np. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/__init__.py"", line 140, in <module>. from . import core. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/__init__.py"", line 49, in <module>. raise ImportError(msg). ImportError:. IMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE! Importing the numpy C-extensions failed. This error can happen for. many reasons, often due to issues with your setup or how NumPy was. installed. We have compiled some common reasons and troubleshooting tips at:. https://numpy.org/devdocs/user/troubleshooting-importerror.html. Please note and check the following:. * The Python vers",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/610
https://github.com/google/deepvariant/issues/610:1566,modifiability,pac,packages,1566,"ity run -B /home/ -B /scratch/ deepvariant.sif`. - Error trace:. ```. Traceback (most recent call last):. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/__init__.py"", line 23, in <module>. from . import multiarray. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/multiarray.py"", line 10, in <module>. from . import overrides. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/overrides.py"", line 6, in <module>. from numpy.core._multiarray_umath import (. ImportError: libflexiblas.so.3: cannot open shared object file: No such file or directory. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 48, in <module>. import tensorflow as tf. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/__init__.py"", line 41, in <module>. from tensorflow.python.tools import module_util as _module_util. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/__init__.py"", line 41, in <module>. from tensorflow.python.eager import context. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/context.py"", line 30, in <module>. import numpy as np. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/__init__.py"", line 140, in <module>. from . import core. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/__init__.py"", line 49, in <module>. raise ImportError(msg). ImportError:. IMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE! Importing the numpy C-extensions failed. This error can happen for. many reasons, often due to issues with your setup or how NumPy was. installed. We have compiled some common reasons and troubleshooting tips at:. https://numpy.org/devdocs/user/troubleshooting-importerror.html. Please note and check the following:. * The Python version is: Python3.8 from ""/usr/bin/python3"". * The NumPy version is: ""1.23.0"". and make sure that they are the ve",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/610
https://github.com/google/deepvariant/issues/610:1620,modifiability,modul,module,1620,"ror trace:. ```. Traceback (most recent call last):. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/__init__.py"", line 23, in <module>. from . import multiarray. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/multiarray.py"", line 10, in <module>. from . import overrides. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/overrides.py"", line 6, in <module>. from numpy.core._multiarray_umath import (. ImportError: libflexiblas.so.3: cannot open shared object file: No such file or directory. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 48, in <module>. import tensorflow as tf. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/__init__.py"", line 41, in <module>. from tensorflow.python.tools import module_util as _module_util. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/__init__.py"", line 41, in <module>. from tensorflow.python.eager import context. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/context.py"", line 30, in <module>. import numpy as np. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/__init__.py"", line 140, in <module>. from . import core. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/__init__.py"", line 49, in <module>. raise ImportError(msg). ImportError:. IMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE! Importing the numpy C-extensions failed. This error can happen for. many reasons, often due to issues with your setup or how NumPy was. installed. We have compiled some common reasons and troubleshooting tips at:. https://numpy.org/devdocs/user/troubleshooting-importerror.html. Please note and check the following:. * The Python version is: Python3.8 from ""/usr/bin/python3"". * The NumPy version is: ""1.23.0"". and make sure that they are the versions you expect. Please carefully study the documen",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/610
https://github.com/google/deepvariant/issues/610:1710,modifiability,pac,packages,1710,"3.8/site-packages/numpy/core/__init__.py"", line 23, in <module>. from . import multiarray. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/multiarray.py"", line 10, in <module>. from . import overrides. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/overrides.py"", line 6, in <module>. from numpy.core._multiarray_umath import (. ImportError: libflexiblas.so.3: cannot open shared object file: No such file or directory. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 48, in <module>. import tensorflow as tf. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/__init__.py"", line 41, in <module>. from tensorflow.python.tools import module_util as _module_util. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/__init__.py"", line 41, in <module>. from tensorflow.python.eager import context. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/context.py"", line 30, in <module>. import numpy as np. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/__init__.py"", line 140, in <module>. from . import core. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/__init__.py"", line 49, in <module>. raise ImportError(msg). ImportError:. IMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE! Importing the numpy C-extensions failed. This error can happen for. many reasons, often due to issues with your setup or how NumPy was. installed. We have compiled some common reasons and troubleshooting tips at:. https://numpy.org/devdocs/user/troubleshooting-importerror.html. Please note and check the following:. * The Python version is: Python3.8 from ""/usr/bin/python3"". * The NumPy version is: ""1.23.0"". and make sure that they are the versions you expect. Please carefully study the documentation linked above for further help. Original error was: libflexiblas.so.3: cannot open sh",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/610
https://github.com/google/deepvariant/issues/610:1769,modifiability,modul,module,1769,"dule>. from . import multiarray. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/multiarray.py"", line 10, in <module>. from . import overrides. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/overrides.py"", line 6, in <module>. from numpy.core._multiarray_umath import (. ImportError: libflexiblas.so.3: cannot open shared object file: No such file or directory. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 48, in <module>. import tensorflow as tf. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/__init__.py"", line 41, in <module>. from tensorflow.python.tools import module_util as _module_util. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/__init__.py"", line 41, in <module>. from tensorflow.python.eager import context. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/context.py"", line 30, in <module>. import numpy as np. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/__init__.py"", line 140, in <module>. from . import core. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/__init__.py"", line 49, in <module>. raise ImportError(msg). ImportError:. IMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE! Importing the numpy C-extensions failed. This error can happen for. many reasons, often due to issues with your setup or how NumPy was. installed. We have compiled some common reasons and troubleshooting tips at:. https://numpy.org/devdocs/user/troubleshooting-importerror.html. Please note and check the following:. * The Python version is: Python3.8 from ""/usr/bin/python3"". * The NumPy version is: ""1.23.0"". and make sure that they are the versions you expect. Please carefully study the documentation linked above for further help. Original error was: libflexiblas.so.3: cannot open shared object file: No such file or directory. ```. **Does t",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/610
https://github.com/google/deepvariant/issues/610:1845,modifiability,pac,packages,1845,"te-packages/numpy/core/multiarray.py"", line 10, in <module>. from . import overrides. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/overrides.py"", line 6, in <module>. from numpy.core._multiarray_umath import (. ImportError: libflexiblas.so.3: cannot open shared object file: No such file or directory. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 48, in <module>. import tensorflow as tf. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/__init__.py"", line 41, in <module>. from tensorflow.python.tools import module_util as _module_util. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/__init__.py"", line 41, in <module>. from tensorflow.python.eager import context. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/context.py"", line 30, in <module>. import numpy as np. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/__init__.py"", line 140, in <module>. from . import core. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/__init__.py"", line 49, in <module>. raise ImportError(msg). ImportError:. IMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE! Importing the numpy C-extensions failed. This error can happen for. many reasons, often due to issues with your setup or how NumPy was. installed. We have compiled some common reasons and troubleshooting tips at:. https://numpy.org/devdocs/user/troubleshooting-importerror.html. Please note and check the following:. * The Python version is: Python3.8 from ""/usr/bin/python3"". * The NumPy version is: ""1.23.0"". and make sure that they are the versions you expect. Please carefully study the documentation linked above for further help. Original error was: libflexiblas.so.3: cannot open shared object file: No such file or directory. ```. **Does the quick start test work on your system?**. Please test with https://github.c",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/610
https://github.com/google/deepvariant/issues/610:1888,modifiability,modul,module,1888,"e 10, in <module>. from . import overrides. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/overrides.py"", line 6, in <module>. from numpy.core._multiarray_umath import (. ImportError: libflexiblas.so.3: cannot open shared object file: No such file or directory. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 48, in <module>. import tensorflow as tf. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/__init__.py"", line 41, in <module>. from tensorflow.python.tools import module_util as _module_util. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/__init__.py"", line 41, in <module>. from tensorflow.python.eager import context. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/context.py"", line 30, in <module>. import numpy as np. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/__init__.py"", line 140, in <module>. from . import core. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/__init__.py"", line 49, in <module>. raise ImportError(msg). ImportError:. IMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE! Importing the numpy C-extensions failed. This error can happen for. many reasons, often due to issues with your setup or how NumPy was. installed. We have compiled some common reasons and troubleshooting tips at:. https://numpy.org/devdocs/user/troubleshooting-importerror.html. Please note and check the following:. * The Python version is: Python3.8 from ""/usr/bin/python3"". * The NumPy version is: ""1.23.0"". and make sure that they are the versions you expect. Please carefully study the documentation linked above for further help. Original error was: libflexiblas.so.3: cannot open shared object file: No such file or directory. ```. **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deep",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/610
https://github.com/google/deepvariant/issues/610:1964,modifiability,pac,packages,1964,"ython3.8/site-packages/numpy/core/overrides.py"", line 6, in <module>. from numpy.core._multiarray_umath import (. ImportError: libflexiblas.so.3: cannot open shared object file: No such file or directory. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 48, in <module>. import tensorflow as tf. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/__init__.py"", line 41, in <module>. from tensorflow.python.tools import module_util as _module_util. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/__init__.py"", line 41, in <module>. from tensorflow.python.eager import context. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/context.py"", line 30, in <module>. import numpy as np. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/__init__.py"", line 140, in <module>. from . import core. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/__init__.py"", line 49, in <module>. raise ImportError(msg). ImportError:. IMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE! Importing the numpy C-extensions failed. This error can happen for. many reasons, often due to issues with your setup or how NumPy was. installed. We have compiled some common reasons and troubleshooting tips at:. https://numpy.org/devdocs/user/troubleshooting-importerror.html. Please note and check the following:. * The Python version is: Python3.8 from ""/usr/bin/python3"". * The NumPy version is: ""1.23.0"". and make sure that they are the versions you expect. Please carefully study the documentation linked above for further help. Original error was: libflexiblas.so.3: cannot open shared object file: No such file or directory. ```. **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/610
https://github.com/google/deepvariant/issues/610:2011,modifiability,modul,module,2011,""", line 6, in <module>. from numpy.core._multiarray_umath import (. ImportError: libflexiblas.so.3: cannot open shared object file: No such file or directory. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 48, in <module>. import tensorflow as tf. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/__init__.py"", line 41, in <module>. from tensorflow.python.tools import module_util as _module_util. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/__init__.py"", line 41, in <module>. from tensorflow.python.eager import context. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/context.py"", line 30, in <module>. import numpy as np. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/__init__.py"", line 140, in <module>. from . import core. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/__init__.py"", line 49, in <module>. raise ImportError(msg). ImportError:. IMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE! Importing the numpy C-extensions failed. This error can happen for. many reasons, often due to issues with your setup or how NumPy was. installed. We have compiled some common reasons and troubleshooting tips at:. https://numpy.org/devdocs/user/troubleshooting-importerror.html. Please note and check the following:. * The Python version is: Python3.8 from ""/usr/bin/python3"". * The NumPy version is: ""1.23.0"". and make sure that they are the versions you expect. Please carefully study the documentation linked above for further help. Original error was: libflexiblas.so.3: cannot open shared object file: No such file or directory. ```. **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? Any attempt to execute via `singu",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/610
https://github.com/google/deepvariant/issues/610:2147,modifiability,extens,extensions,2147,"h file or directory. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 48, in <module>. import tensorflow as tf. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/__init__.py"", line 41, in <module>. from tensorflow.python.tools import module_util as _module_util. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/__init__.py"", line 41, in <module>. from tensorflow.python.eager import context. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/context.py"", line 30, in <module>. import numpy as np. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/__init__.py"", line 140, in <module>. from . import core. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/__init__.py"", line 49, in <module>. raise ImportError(msg). ImportError:. IMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE! Importing the numpy C-extensions failed. This error can happen for. many reasons, often due to issues with your setup or how NumPy was. installed. We have compiled some common reasons and troubleshooting tips at:. https://numpy.org/devdocs/user/troubleshooting-importerror.html. Please note and check the following:. * The Python version is: Python3.8 from ""/usr/bin/python3"". * The NumPy version is: ""1.23.0"". and make sure that they are the versions you expect. Please carefully study the documentation linked above for further help. Original error was: libflexiblas.so.3: cannot open shared object file: No such file or directory. ```. **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? Any attempt to execute via `singularity run` leads to an error. **Any additional context:**. As far as I can tell, my environment meets the requirements for both Python an",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/610
https://github.com/google/deepvariant/issues/610:2455,modifiability,version,version,2455,"ule>. from tensorflow.python.tools import module_util as _module_util. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/__init__.py"", line 41, in <module>. from tensorflow.python.eager import context. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/context.py"", line 30, in <module>. import numpy as np. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/__init__.py"", line 140, in <module>. from . import core. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/__init__.py"", line 49, in <module>. raise ImportError(msg). ImportError:. IMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE! Importing the numpy C-extensions failed. This error can happen for. many reasons, often due to issues with your setup or how NumPy was. installed. We have compiled some common reasons and troubleshooting tips at:. https://numpy.org/devdocs/user/troubleshooting-importerror.html. Please note and check the following:. * The Python version is: Python3.8 from ""/usr/bin/python3"". * The NumPy version is: ""1.23.0"". and make sure that they are the versions you expect. Please carefully study the documentation linked above for further help. Original error was: libflexiblas.so.3: cannot open shared object file: No such file or directory. ```. **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? Any attempt to execute via `singularity run` leads to an error. **Any additional context:**. As far as I can tell, my environment meets the requirements for both Python and NumPy - though at the same time when I `singularity shell` into the SIF file, its versioning seems semi-independent of my main environment (Python 3.8.10 regardless of my environment's version, but using the NumPy 1.23.0 provided by my computing cluster). I feel like I'm missing something really simple, ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/610
https://github.com/google/deepvariant/issues/610:2514,modifiability,version,version,2514,"odule_util. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/__init__.py"", line 41, in <module>. from tensorflow.python.eager import context. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/context.py"", line 30, in <module>. import numpy as np. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/__init__.py"", line 140, in <module>. from . import core. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/__init__.py"", line 49, in <module>. raise ImportError(msg). ImportError:. IMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE! Importing the numpy C-extensions failed. This error can happen for. many reasons, often due to issues with your setup or how NumPy was. installed. We have compiled some common reasons and troubleshooting tips at:. https://numpy.org/devdocs/user/troubleshooting-importerror.html. Please note and check the following:. * The Python version is: Python3.8 from ""/usr/bin/python3"". * The NumPy version is: ""1.23.0"". and make sure that they are the versions you expect. Please carefully study the documentation linked above for further help. Original error was: libflexiblas.so.3: cannot open shared object file: No such file or directory. ```. **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? Any attempt to execute via `singularity run` leads to an error. **Any additional context:**. As far as I can tell, my environment meets the requirements for both Python and NumPy - though at the same time when I `singularity shell` into the SIF file, its versioning seems semi-independent of my main environment (Python 3.8.10 regardless of my environment's version, but using the NumPy 1.23.0 provided by my computing cluster). I feel like I'm missing something really simple, but I've tried the NumPy troubleshooting page and can't see",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/610
https://github.com/google/deepvariant/issues/610:2568,modifiability,version,versions,2568,"es/tensorflow/python/__init__.py"", line 41, in <module>. from tensorflow.python.eager import context. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/context.py"", line 30, in <module>. import numpy as np. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/__init__.py"", line 140, in <module>. from . import core. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/__init__.py"", line 49, in <module>. raise ImportError(msg). ImportError:. IMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE! Importing the numpy C-extensions failed. This error can happen for. many reasons, often due to issues with your setup or how NumPy was. installed. We have compiled some common reasons and troubleshooting tips at:. https://numpy.org/devdocs/user/troubleshooting-importerror.html. Please note and check the following:. * The Python version is: Python3.8 from ""/usr/bin/python3"". * The NumPy version is: ""1.23.0"". and make sure that they are the versions you expect. Please carefully study the documentation linked above for further help. Original error was: libflexiblas.so.3: cannot open shared object file: No such file or directory. ```. **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? Any attempt to execute via `singularity run` leads to an error. **Any additional context:**. As far as I can tell, my environment meets the requirements for both Python and NumPy - though at the same time when I `singularity shell` into the SIF file, its versioning seems semi-independent of my main environment (Python 3.8.10 regardless of my environment's version, but using the NumPy 1.23.0 provided by my computing cluster). I feel like I'm missing something really simple, but I've tried the NumPy troubleshooting page and can't seem to crack this error. If it helps, I'm attempting thi",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/610
https://github.com/google/deepvariant/issues/610:3236,modifiability,version,versioning,3236,"le ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/context.py"", line 30, in <module>. import numpy as np. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/__init__.py"", line 140, in <module>. from . import core. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/__init__.py"", line 49, in <module>. raise ImportError(msg). ImportError:. IMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE! Importing the numpy C-extensions failed. This error can happen for. many reasons, often due to issues with your setup or how NumPy was. installed. We have compiled some common reasons and troubleshooting tips at:. https://numpy.org/devdocs/user/troubleshooting-importerror.html. Please note and check the following:. * The Python version is: Python3.8 from ""/usr/bin/python3"". * The NumPy version is: ""1.23.0"". and make sure that they are the versions you expect. Please carefully study the documentation linked above for further help. Original error was: libflexiblas.so.3: cannot open shared object file: No such file or directory. ```. **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? Any attempt to execute via `singularity run` leads to an error. **Any additional context:**. As far as I can tell, my environment meets the requirements for both Python and NumPy - though at the same time when I `singularity shell` into the SIF file, its versioning seems semi-independent of my main environment (Python 3.8.10 regardless of my environment's version, but using the NumPy 1.23.0 provided by my computing cluster). I feel like I'm missing something really simple, but I've tried the NumPy troubleshooting page and can't seem to crack this error. If it helps, I'm attempting this with `singularity` version 3.8.4, which is the newest version available to me in my computing cluster.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/610
https://github.com/google/deepvariant/issues/610:3339,modifiability,version,version,3339,"le ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/context.py"", line 30, in <module>. import numpy as np. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/__init__.py"", line 140, in <module>. from . import core. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/__init__.py"", line 49, in <module>. raise ImportError(msg). ImportError:. IMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE! Importing the numpy C-extensions failed. This error can happen for. many reasons, often due to issues with your setup or how NumPy was. installed. We have compiled some common reasons and troubleshooting tips at:. https://numpy.org/devdocs/user/troubleshooting-importerror.html. Please note and check the following:. * The Python version is: Python3.8 from ""/usr/bin/python3"". * The NumPy version is: ""1.23.0"". and make sure that they are the versions you expect. Please carefully study the documentation linked above for further help. Original error was: libflexiblas.so.3: cannot open shared object file: No such file or directory. ```. **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? Any attempt to execute via `singularity run` leads to an error. **Any additional context:**. As far as I can tell, my environment meets the requirements for both Python and NumPy - though at the same time when I `singularity shell` into the SIF file, its versioning seems semi-independent of my main environment (Python 3.8.10 regardless of my environment's version, but using the NumPy 1.23.0 provided by my computing cluster). I feel like I'm missing something really simple, but I've tried the NumPy troubleshooting page and can't seem to crack this error. If it helps, I'm attempting this with `singularity` version 3.8.4, which is the newest version available to me in my computing cluster.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/610
https://github.com/google/deepvariant/issues/610:3593,modifiability,version,version,3593,"le ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/context.py"", line 30, in <module>. import numpy as np. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/__init__.py"", line 140, in <module>. from . import core. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/__init__.py"", line 49, in <module>. raise ImportError(msg). ImportError:. IMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE! Importing the numpy C-extensions failed. This error can happen for. many reasons, often due to issues with your setup or how NumPy was. installed. We have compiled some common reasons and troubleshooting tips at:. https://numpy.org/devdocs/user/troubleshooting-importerror.html. Please note and check the following:. * The Python version is: Python3.8 from ""/usr/bin/python3"". * The NumPy version is: ""1.23.0"". and make sure that they are the versions you expect. Please carefully study the documentation linked above for further help. Original error was: libflexiblas.so.3: cannot open shared object file: No such file or directory. ```. **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? Any attempt to execute via `singularity run` leads to an error. **Any additional context:**. As far as I can tell, my environment meets the requirements for both Python and NumPy - though at the same time when I `singularity shell` into the SIF file, its versioning seems semi-independent of my main environment (Python 3.8.10 regardless of my environment's version, but using the NumPy 1.23.0 provided by my computing cluster). I feel like I'm missing something really simple, but I've tried the NumPy troubleshooting page and can't seem to crack this error. If it helps, I'm attempting this with `singularity` version 3.8.4, which is the newest version available to me in my computing cluster.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/610
https://github.com/google/deepvariant/issues/610:3628,modifiability,version,version,3628,"le ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/context.py"", line 30, in <module>. import numpy as np. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/__init__.py"", line 140, in <module>. from . import core. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/__init__.py"", line 49, in <module>. raise ImportError(msg). ImportError:. IMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE! Importing the numpy C-extensions failed. This error can happen for. many reasons, often due to issues with your setup or how NumPy was. installed. We have compiled some common reasons and troubleshooting tips at:. https://numpy.org/devdocs/user/troubleshooting-importerror.html. Please note and check the following:. * The Python version is: Python3.8 from ""/usr/bin/python3"". * The NumPy version is: ""1.23.0"". and make sure that they are the versions you expect. Please carefully study the documentation linked above for further help. Original error was: libflexiblas.so.3: cannot open shared object file: No such file or directory. ```. **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? Any attempt to execute via `singularity run` leads to an error. **Any additional context:**. As far as I can tell, my environment meets the requirements for both Python and NumPy - though at the same time when I `singularity shell` into the SIF file, its versioning seems semi-independent of my main environment (Python 3.8.10 regardless of my environment's version, but using the NumPy 1.23.0 provided by my computing cluster). I feel like I'm missing something really simple, but I've tried the NumPy troubleshooting page and can't seem to crack this error. If it helps, I'm attempting this with `singularity` version 3.8.4, which is the newest version available to me in my computing cluster.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/610
https://github.com/google/deepvariant/issues/610:17,performance,error,error,17,"NumPy dependency error despite matching versions; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**: Yes. **Describe the issue:**. Unable to run Singularity image despite prerequisites appearing satisfied. **Setup**. - Operating system: CentOS 7 . - DeepVariant version: 1.4.0 (google/deepvariant:latest). - Installation method (Docker, built from source, etc.): `singularity pull docker://google/deepvariant:latest` and attempted execution via `singularity run`. - Type of data: N/A. **Steps to reproduce:**. - Command: `singularity run -B /home/ -B /scratch/ deepvariant.sif`. - Error trace:. ```. Traceback (most recent call last):. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/__init__.py"", line 23, in <module>. from . import multiarray. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/multiarray.py"", line 10, in <module>. from . import overrides. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/overrides.py"", line 6, in <module>. from numpy.core._multiarray_umath import (. ImportError: libflexiblas.so.3: cannot open shared object file: No such file or directory. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 48, in <module>. import tensorflow as tf. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/__init__.py"", line 41, in <module>. from tensorflow.python.tools import module_util as _module_util. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/__init__.py"", line 41, in <module>. from tensorflow.python.eager import context. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/context.py"", line 30, in <module>. import numpy as np. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/__init__.py"", line 140, in <module>. from . import core. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/__init__.py"", li",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/610
https://github.com/google/deepvariant/issues/610:621,performance,Error,Error,621,"NumPy dependency error despite matching versions; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**: Yes. **Describe the issue:**. Unable to run Singularity image despite prerequisites appearing satisfied. **Setup**. - Operating system: CentOS 7 . - DeepVariant version: 1.4.0 (google/deepvariant:latest). - Installation method (Docker, built from source, etc.): `singularity pull docker://google/deepvariant:latest` and attempted execution via `singularity run`. - Type of data: N/A. **Steps to reproduce:**. - Command: `singularity run -B /home/ -B /scratch/ deepvariant.sif`. - Error trace:. ```. Traceback (most recent call last):. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/__init__.py"", line 23, in <module>. from . import multiarray. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/multiarray.py"", line 10, in <module>. from . import overrides. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/overrides.py"", line 6, in <module>. from numpy.core._multiarray_umath import (. ImportError: libflexiblas.so.3: cannot open shared object file: No such file or directory. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 48, in <module>. import tensorflow as tf. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/__init__.py"", line 41, in <module>. from tensorflow.python.tools import module_util as _module_util. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/__init__.py"", line 41, in <module>. from tensorflow.python.eager import context. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/context.py"", line 30, in <module>. import numpy as np. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/__init__.py"", line 140, in <module>. from . import core. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/__init__.py"", li",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/610
https://github.com/google/deepvariant/issues/610:2171,performance,error,error,2171,"uring handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 48, in <module>. import tensorflow as tf. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/__init__.py"", line 41, in <module>. from tensorflow.python.tools import module_util as _module_util. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/__init__.py"", line 41, in <module>. from tensorflow.python.eager import context. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/context.py"", line 30, in <module>. import numpy as np. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/__init__.py"", line 140, in <module>. from . import core. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/__init__.py"", line 49, in <module>. raise ImportError(msg). ImportError:. IMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE! Importing the numpy C-extensions failed. This error can happen for. many reasons, often due to issues with your setup or how NumPy was. installed. We have compiled some common reasons and troubleshooting tips at:. https://numpy.org/devdocs/user/troubleshooting-importerror.html. Please note and check the following:. * The Python version is: Python3.8 from ""/usr/bin/python3"". * The NumPy version is: ""1.23.0"". and make sure that they are the versions you expect. Please carefully study the documentation linked above for further help. Original error was: libflexiblas.so.3: cannot open shared object file: No such file or directory. ```. **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? Any attempt to execute via `singularity run` leads to an error. **Any additional context:**. As far as I can tell, my environment meets the requirements for both Python and NumPy - though at th",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/610
https://github.com/google/deepvariant/issues/610:2670,performance,error,error,2670," File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/context.py"", line 30, in <module>. import numpy as np. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/__init__.py"", line 140, in <module>. from . import core. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/__init__.py"", line 49, in <module>. raise ImportError(msg). ImportError:. IMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE! Importing the numpy C-extensions failed. This error can happen for. many reasons, often due to issues with your setup or how NumPy was. installed. We have compiled some common reasons and troubleshooting tips at:. https://numpy.org/devdocs/user/troubleshooting-importerror.html. Please note and check the following:. * The Python version is: Python3.8 from ""/usr/bin/python3"". * The NumPy version is: ""1.23.0"". and make sure that they are the versions you expect. Please carefully study the documentation linked above for further help. Original error was: libflexiblas.so.3: cannot open shared object file: No such file or directory. ```. **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? Any attempt to execute via `singularity run` leads to an error. **Any additional context:**. As far as I can tell, my environment meets the requirements for both Python and NumPy - though at the same time when I `singularity shell` into the SIF file, its versioning seems semi-independent of my main environment (Python 3.8.10 regardless of my environment's version, but using the NumPy 1.23.0 provided by my computing cluster). I feel like I'm missing something really simple, but I've tried the NumPy troubleshooting page and can't seem to crack this error. If it helps, I'm attempting this with `singularity` version 3.8.4, which is the newest version available to me in my computing clust",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/610
https://github.com/google/deepvariant/issues/610:3038,performance,error,error,3038,"le ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/context.py"", line 30, in <module>. import numpy as np. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/__init__.py"", line 140, in <module>. from . import core. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/__init__.py"", line 49, in <module>. raise ImportError(msg). ImportError:. IMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE! Importing the numpy C-extensions failed. This error can happen for. many reasons, often due to issues with your setup or how NumPy was. installed. We have compiled some common reasons and troubleshooting tips at:. https://numpy.org/devdocs/user/troubleshooting-importerror.html. Please note and check the following:. * The Python version is: Python3.8 from ""/usr/bin/python3"". * The NumPy version is: ""1.23.0"". and make sure that they are the versions you expect. Please carefully study the documentation linked above for further help. Original error was: libflexiblas.so.3: cannot open shared object file: No such file or directory. ```. **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? Any attempt to execute via `singularity run` leads to an error. **Any additional context:**. As far as I can tell, my environment meets the requirements for both Python and NumPy - though at the same time when I `singularity shell` into the SIF file, its versioning seems semi-independent of my main environment (Python 3.8.10 regardless of my environment's version, but using the NumPy 1.23.0 provided by my computing cluster). I feel like I'm missing something really simple, but I've tried the NumPy troubleshooting page and can't seem to crack this error. If it helps, I'm attempting this with `singularity` version 3.8.4, which is the newest version available to me in my computing cluster.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/610
https://github.com/google/deepvariant/issues/610:3181,performance,time,time,3181,"le ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/context.py"", line 30, in <module>. import numpy as np. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/__init__.py"", line 140, in <module>. from . import core. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/__init__.py"", line 49, in <module>. raise ImportError(msg). ImportError:. IMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE! Importing the numpy C-extensions failed. This error can happen for. many reasons, often due to issues with your setup or how NumPy was. installed. We have compiled some common reasons and troubleshooting tips at:. https://numpy.org/devdocs/user/troubleshooting-importerror.html. Please note and check the following:. * The Python version is: Python3.8 from ""/usr/bin/python3"". * The NumPy version is: ""1.23.0"". and make sure that they are the versions you expect. Please carefully study the documentation linked above for further help. Original error was: libflexiblas.so.3: cannot open shared object file: No such file or directory. ```. **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? Any attempt to execute via `singularity run` leads to an error. **Any additional context:**. As far as I can tell, my environment meets the requirements for both Python and NumPy - though at the same time when I `singularity shell` into the SIF file, its versioning seems semi-independent of my main environment (Python 3.8.10 regardless of my environment's version, but using the NumPy 1.23.0 provided by my computing cluster). I feel like I'm missing something really simple, but I've tried the NumPy troubleshooting page and can't seem to crack this error. If it helps, I'm attempting this with `singularity` version 3.8.4, which is the newest version available to me in my computing cluster.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/610
https://github.com/google/deepvariant/issues/610:3534,performance,error,error,3534,"le ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/context.py"", line 30, in <module>. import numpy as np. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/__init__.py"", line 140, in <module>. from . import core. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/__init__.py"", line 49, in <module>. raise ImportError(msg). ImportError:. IMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE! Importing the numpy C-extensions failed. This error can happen for. many reasons, often due to issues with your setup or how NumPy was. installed. We have compiled some common reasons and troubleshooting tips at:. https://numpy.org/devdocs/user/troubleshooting-importerror.html. Please note and check the following:. * The Python version is: Python3.8 from ""/usr/bin/python3"". * The NumPy version is: ""1.23.0"". and make sure that they are the versions you expect. Please carefully study the documentation linked above for further help. Original error was: libflexiblas.so.3: cannot open shared object file: No such file or directory. ```. **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? Any attempt to execute via `singularity run` leads to an error. **Any additional context:**. As far as I can tell, my environment meets the requirements for both Python and NumPy - though at the same time when I `singularity shell` into the SIF file, its versioning seems semi-independent of my main environment (Python 3.8.10 regardless of my environment's version, but using the NumPy 1.23.0 provided by my computing cluster). I feel like I'm missing something really simple, but I've tried the NumPy troubleshooting page and can't seem to crack this error. If it helps, I'm attempting this with `singularity` version 3.8.4, which is the newest version available to me in my computing cluster.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/610
https://github.com/google/deepvariant/issues/610:2158,reliability,fail,failed,2158," directory. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 48, in <module>. import tensorflow as tf. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/__init__.py"", line 41, in <module>. from tensorflow.python.tools import module_util as _module_util. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/__init__.py"", line 41, in <module>. from tensorflow.python.eager import context. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/context.py"", line 30, in <module>. import numpy as np. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/__init__.py"", line 140, in <module>. from . import core. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/__init__.py"", line 49, in <module>. raise ImportError(msg). ImportError:. IMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE! Importing the numpy C-extensions failed. This error can happen for. many reasons, often due to issues with your setup or how NumPy was. installed. We have compiled some common reasons and troubleshooting tips at:. https://numpy.org/devdocs/user/troubleshooting-importerror.html. Please note and check the following:. * The Python version is: Python3.8 from ""/usr/bin/python3"". * The NumPy version is: ""1.23.0"". and make sure that they are the versions you expect. Please carefully study the documentation linked above for further help. Original error was: libflexiblas.so.3: cannot open shared object file: No such file or directory. ```. **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? Any attempt to execute via `singularity run` leads to an error. **Any additional context:**. As far as I can tell, my environment meets the requirements for both Python and NumPy -",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/610
https://github.com/google/deepvariant/issues/610:2766,reliability,Doe,Does,2766,"le ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/context.py"", line 30, in <module>. import numpy as np. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/__init__.py"", line 140, in <module>. from . import core. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/__init__.py"", line 49, in <module>. raise ImportError(msg). ImportError:. IMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE! Importing the numpy C-extensions failed. This error can happen for. many reasons, often due to issues with your setup or how NumPy was. installed. We have compiled some common reasons and troubleshooting tips at:. https://numpy.org/devdocs/user/troubleshooting-importerror.html. Please note and check the following:. * The Python version is: Python3.8 from ""/usr/bin/python3"". * The NumPy version is: ""1.23.0"". and make sure that they are the versions you expect. Please carefully study the documentation linked above for further help. Original error was: libflexiblas.so.3: cannot open shared object file: No such file or directory. ```. **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? Any attempt to execute via `singularity run` leads to an error. **Any additional context:**. As far as I can tell, my environment meets the requirements for both Python and NumPy - though at the same time when I `singularity shell` into the SIF file, its versioning seems semi-independent of my main environment (Python 3.8.10 regardless of my environment's version, but using the NumPy 1.23.0 provided by my computing cluster). I feel like I'm missing something really simple, but I've tried the NumPy troubleshooting page and can't seem to crack this error. If it helps, I'm attempting this with `singularity` version 3.8.4, which is the newest version available to me in my computing cluster.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/610
https://github.com/google/deepvariant/issues/610:3636,reliability,availab,available,3636,"le ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/context.py"", line 30, in <module>. import numpy as np. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/__init__.py"", line 140, in <module>. from . import core. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/__init__.py"", line 49, in <module>. raise ImportError(msg). ImportError:. IMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE! Importing the numpy C-extensions failed. This error can happen for. many reasons, often due to issues with your setup or how NumPy was. installed. We have compiled some common reasons and troubleshooting tips at:. https://numpy.org/devdocs/user/troubleshooting-importerror.html. Please note and check the following:. * The Python version is: Python3.8 from ""/usr/bin/python3"". * The NumPy version is: ""1.23.0"". and make sure that they are the versions you expect. Please carefully study the documentation linked above for further help. Original error was: libflexiblas.so.3: cannot open shared object file: No such file or directory. ```. **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? Any attempt to execute via `singularity run` leads to an error. **Any additional context:**. As far as I can tell, my environment meets the requirements for both Python and NumPy - though at the same time when I `singularity shell` into the SIF file, its versioning seems semi-independent of my main environment (Python 3.8.10 regardless of my environment's version, but using the NumPy 1.23.0 provided by my computing cluster). I feel like I'm missing something really simple, but I've tried the NumPy troubleshooting page and can't seem to crack this error. If it helps, I'm attempting this with `singularity` version 3.8.4, which is the newest version available to me in my computing cluster.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/610
https://github.com/google/deepvariant/issues/610:6,safety,depend,dependency,6,"NumPy dependency error despite matching versions; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**: Yes. **Describe the issue:**. Unable to run Singularity image despite prerequisites appearing satisfied. **Setup**. - Operating system: CentOS 7 . - DeepVariant version: 1.4.0 (google/deepvariant:latest). - Installation method (Docker, built from source, etc.): `singularity pull docker://google/deepvariant:latest` and attempted execution via `singularity run`. - Type of data: N/A. **Steps to reproduce:**. - Command: `singularity run -B /home/ -B /scratch/ deepvariant.sif`. - Error trace:. ```. Traceback (most recent call last):. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/__init__.py"", line 23, in <module>. from . import multiarray. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/multiarray.py"", line 10, in <module>. from . import overrides. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/overrides.py"", line 6, in <module>. from numpy.core._multiarray_umath import (. ImportError: libflexiblas.so.3: cannot open shared object file: No such file or directory. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 48, in <module>. import tensorflow as tf. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/__init__.py"", line 41, in <module>. from tensorflow.python.tools import module_util as _module_util. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/__init__.py"", line 41, in <module>. from tensorflow.python.eager import context. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/context.py"", line 30, in <module>. import numpy as np. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/__init__.py"", line 140, in <module>. from . import core. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/__init__.py"", li",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/610
https://github.com/google/deepvariant/issues/610:17,safety,error,error,17,"NumPy dependency error despite matching versions; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**: Yes. **Describe the issue:**. Unable to run Singularity image despite prerequisites appearing satisfied. **Setup**. - Operating system: CentOS 7 . - DeepVariant version: 1.4.0 (google/deepvariant:latest). - Installation method (Docker, built from source, etc.): `singularity pull docker://google/deepvariant:latest` and attempted execution via `singularity run`. - Type of data: N/A. **Steps to reproduce:**. - Command: `singularity run -B /home/ -B /scratch/ deepvariant.sif`. - Error trace:. ```. Traceback (most recent call last):. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/__init__.py"", line 23, in <module>. from . import multiarray. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/multiarray.py"", line 10, in <module>. from . import overrides. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/overrides.py"", line 6, in <module>. from numpy.core._multiarray_umath import (. ImportError: libflexiblas.so.3: cannot open shared object file: No such file or directory. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 48, in <module>. import tensorflow as tf. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/__init__.py"", line 41, in <module>. from tensorflow.python.tools import module_util as _module_util. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/__init__.py"", line 41, in <module>. from tensorflow.python.eager import context. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/context.py"", line 30, in <module>. import numpy as np. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/__init__.py"", line 140, in <module>. from . import core. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/__init__.py"", li",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/610
https://github.com/google/deepvariant/issues/610:621,safety,Error,Error,621,"NumPy dependency error despite matching versions; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**: Yes. **Describe the issue:**. Unable to run Singularity image despite prerequisites appearing satisfied. **Setup**. - Operating system: CentOS 7 . - DeepVariant version: 1.4.0 (google/deepvariant:latest). - Installation method (Docker, built from source, etc.): `singularity pull docker://google/deepvariant:latest` and attempted execution via `singularity run`. - Type of data: N/A. **Steps to reproduce:**. - Command: `singularity run -B /home/ -B /scratch/ deepvariant.sif`. - Error trace:. ```. Traceback (most recent call last):. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/__init__.py"", line 23, in <module>. from . import multiarray. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/multiarray.py"", line 10, in <module>. from . import overrides. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/overrides.py"", line 6, in <module>. from numpy.core._multiarray_umath import (. ImportError: libflexiblas.so.3: cannot open shared object file: No such file or directory. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 48, in <module>. import tensorflow as tf. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/__init__.py"", line 41, in <module>. from tensorflow.python.tools import module_util as _module_util. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/__init__.py"", line 41, in <module>. from tensorflow.python.eager import context. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/context.py"", line 30, in <module>. import numpy as np. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/__init__.py"", line 140, in <module>. from . import core. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/__init__.py"", li",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/610
https://github.com/google/deepvariant/issues/610:770,safety,modul,module,770,"NumPy dependency error despite matching versions; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**: Yes. **Describe the issue:**. Unable to run Singularity image despite prerequisites appearing satisfied. **Setup**. - Operating system: CentOS 7 . - DeepVariant version: 1.4.0 (google/deepvariant:latest). - Installation method (Docker, built from source, etc.): `singularity pull docker://google/deepvariant:latest` and attempted execution via `singularity run`. - Type of data: N/A. **Steps to reproduce:**. - Command: `singularity run -B /home/ -B /scratch/ deepvariant.sif`. - Error trace:. ```. Traceback (most recent call last):. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/__init__.py"", line 23, in <module>. from . import multiarray. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/multiarray.py"", line 10, in <module>. from . import overrides. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/overrides.py"", line 6, in <module>. from numpy.core._multiarray_umath import (. ImportError: libflexiblas.so.3: cannot open shared object file: No such file or directory. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 48, in <module>. import tensorflow as tf. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/__init__.py"", line 41, in <module>. from tensorflow.python.tools import module_util as _module_util. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/__init__.py"", line 41, in <module>. from tensorflow.python.eager import context. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/context.py"", line 30, in <module>. import numpy as np. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/__init__.py"", line 140, in <module>. from . import core. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/__init__.py"", li",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/610
https://github.com/google/deepvariant/issues/610:901,safety,modul,module,901,"NumPy dependency error despite matching versions; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**: Yes. **Describe the issue:**. Unable to run Singularity image despite prerequisites appearing satisfied. **Setup**. - Operating system: CentOS 7 . - DeepVariant version: 1.4.0 (google/deepvariant:latest). - Installation method (Docker, built from source, etc.): `singularity pull docker://google/deepvariant:latest` and attempted execution via `singularity run`. - Type of data: N/A. **Steps to reproduce:**. - Command: `singularity run -B /home/ -B /scratch/ deepvariant.sif`. - Error trace:. ```. Traceback (most recent call last):. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/__init__.py"", line 23, in <module>. from . import multiarray. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/multiarray.py"", line 10, in <module>. from . import overrides. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/overrides.py"", line 6, in <module>. from numpy.core._multiarray_umath import (. ImportError: libflexiblas.so.3: cannot open shared object file: No such file or directory. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 48, in <module>. import tensorflow as tf. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/__init__.py"", line 41, in <module>. from tensorflow.python.tools import module_util as _module_util. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/__init__.py"", line 41, in <module>. from tensorflow.python.eager import context. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/context.py"", line 30, in <module>. import numpy as np. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/__init__.py"", line 140, in <module>. from . import core. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/__init__.py"", li",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/610
https://github.com/google/deepvariant/issues/610:1029,safety,modul,module,1029,"atching versions; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**: Yes. **Describe the issue:**. Unable to run Singularity image despite prerequisites appearing satisfied. **Setup**. - Operating system: CentOS 7 . - DeepVariant version: 1.4.0 (google/deepvariant:latest). - Installation method (Docker, built from source, etc.): `singularity pull docker://google/deepvariant:latest` and attempted execution via `singularity run`. - Type of data: N/A. **Steps to reproduce:**. - Command: `singularity run -B /home/ -B /scratch/ deepvariant.sif`. - Error trace:. ```. Traceback (most recent call last):. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/__init__.py"", line 23, in <module>. from . import multiarray. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/multiarray.py"", line 10, in <module>. from . import overrides. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/overrides.py"", line 6, in <module>. from numpy.core._multiarray_umath import (. ImportError: libflexiblas.so.3: cannot open shared object file: No such file or directory. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 48, in <module>. import tensorflow as tf. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/__init__.py"", line 41, in <module>. from tensorflow.python.tools import module_util as _module_util. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/__init__.py"", line 41, in <module>. from tensorflow.python.eager import context. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/context.py"", line 30, in <module>. import numpy as np. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/__init__.py"", line 140, in <module>. from . import core. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/__init__.py"", line 49, in <module>. raise Import",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/610
https://github.com/google/deepvariant/issues/610:1202,safety,except,exception,1202,"ite prerequisites appearing satisfied. **Setup**. - Operating system: CentOS 7 . - DeepVariant version: 1.4.0 (google/deepvariant:latest). - Installation method (Docker, built from source, etc.): `singularity pull docker://google/deepvariant:latest` and attempted execution via `singularity run`. - Type of data: N/A. **Steps to reproduce:**. - Command: `singularity run -B /home/ -B /scratch/ deepvariant.sif`. - Error trace:. ```. Traceback (most recent call last):. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/__init__.py"", line 23, in <module>. from . import multiarray. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/multiarray.py"", line 10, in <module>. from . import overrides. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/overrides.py"", line 6, in <module>. from numpy.core._multiarray_umath import (. ImportError: libflexiblas.so.3: cannot open shared object file: No such file or directory. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 48, in <module>. import tensorflow as tf. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/__init__.py"", line 41, in <module>. from tensorflow.python.tools import module_util as _module_util. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/__init__.py"", line 41, in <module>. from tensorflow.python.eager import context. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/context.py"", line 30, in <module>. import numpy as np. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/__init__.py"", line 140, in <module>. from . import core. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/__init__.py"", line 49, in <module>. raise ImportError(msg). ImportError:. IMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE! Importing the numpy C-extensions failed. This error can happen for. many reasons, ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/610
https://github.com/google/deepvariant/issues/610:1221,safety,except,exception,1221,"ppearing satisfied. **Setup**. - Operating system: CentOS 7 . - DeepVariant version: 1.4.0 (google/deepvariant:latest). - Installation method (Docker, built from source, etc.): `singularity pull docker://google/deepvariant:latest` and attempted execution via `singularity run`. - Type of data: N/A. **Steps to reproduce:**. - Command: `singularity run -B /home/ -B /scratch/ deepvariant.sif`. - Error trace:. ```. Traceback (most recent call last):. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/__init__.py"", line 23, in <module>. from . import multiarray. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/multiarray.py"", line 10, in <module>. from . import overrides. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/overrides.py"", line 6, in <module>. from numpy.core._multiarray_umath import (. ImportError: libflexiblas.so.3: cannot open shared object file: No such file or directory. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 48, in <module>. import tensorflow as tf. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/__init__.py"", line 41, in <module>. from tensorflow.python.tools import module_util as _module_util. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/__init__.py"", line 41, in <module>. from tensorflow.python.eager import context. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/context.py"", line 30, in <module>. import numpy as np. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/__init__.py"", line 140, in <module>. from . import core. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/__init__.py"", line 49, in <module>. raise ImportError(msg). ImportError:. IMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE! Importing the numpy C-extensions failed. This error can happen for. many reasons, often due to issues",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/610
https://github.com/google/deepvariant/issues/610:1339,safety,modul,module,1339,"t). - Installation method (Docker, built from source, etc.): `singularity pull docker://google/deepvariant:latest` and attempted execution via `singularity run`. - Type of data: N/A. **Steps to reproduce:**. - Command: `singularity run -B /home/ -B /scratch/ deepvariant.sif`. - Error trace:. ```. Traceback (most recent call last):. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/__init__.py"", line 23, in <module>. from . import multiarray. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/multiarray.py"", line 10, in <module>. from . import overrides. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/overrides.py"", line 6, in <module>. from numpy.core._multiarray_umath import (. ImportError: libflexiblas.so.3: cannot open shared object file: No such file or directory. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 48, in <module>. import tensorflow as tf. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/__init__.py"", line 41, in <module>. from tensorflow.python.tools import module_util as _module_util. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/__init__.py"", line 41, in <module>. from tensorflow.python.eager import context. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/context.py"", line 30, in <module>. import numpy as np. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/__init__.py"", line 140, in <module>. from . import core. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/__init__.py"", line 49, in <module>. raise ImportError(msg). ImportError:. IMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE! Importing the numpy C-extensions failed. This error can happen for. many reasons, often due to issues with your setup or how NumPy was. installed. We have compiled some common reasons and troubleshooting tips at:. htt",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/610
https://github.com/google/deepvariant/issues/610:1456,safety,modul,module,1456,"d attempted execution via `singularity run`. - Type of data: N/A. **Steps to reproduce:**. - Command: `singularity run -B /home/ -B /scratch/ deepvariant.sif`. - Error trace:. ```. Traceback (most recent call last):. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/__init__.py"", line 23, in <module>. from . import multiarray. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/multiarray.py"", line 10, in <module>. from . import overrides. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/overrides.py"", line 6, in <module>. from numpy.core._multiarray_umath import (. ImportError: libflexiblas.so.3: cannot open shared object file: No such file or directory. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 48, in <module>. import tensorflow as tf. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/__init__.py"", line 41, in <module>. from tensorflow.python.tools import module_util as _module_util. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/__init__.py"", line 41, in <module>. from tensorflow.python.eager import context. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/context.py"", line 30, in <module>. import numpy as np. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/__init__.py"", line 140, in <module>. from . import core. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/__init__.py"", line 49, in <module>. raise ImportError(msg). ImportError:. IMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE! Importing the numpy C-extensions failed. This error can happen for. many reasons, often due to issues with your setup or how NumPy was. installed. We have compiled some common reasons and troubleshooting tips at:. https://numpy.org/devdocs/user/troubleshooting-importerror.html. Please note and check the following:. * The Python vers",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/610
https://github.com/google/deepvariant/issues/610:1620,safety,modul,module,1620,"ror trace:. ```. Traceback (most recent call last):. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/__init__.py"", line 23, in <module>. from . import multiarray. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/multiarray.py"", line 10, in <module>. from . import overrides. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/overrides.py"", line 6, in <module>. from numpy.core._multiarray_umath import (. ImportError: libflexiblas.so.3: cannot open shared object file: No such file or directory. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 48, in <module>. import tensorflow as tf. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/__init__.py"", line 41, in <module>. from tensorflow.python.tools import module_util as _module_util. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/__init__.py"", line 41, in <module>. from tensorflow.python.eager import context. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/context.py"", line 30, in <module>. import numpy as np. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/__init__.py"", line 140, in <module>. from . import core. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/__init__.py"", line 49, in <module>. raise ImportError(msg). ImportError:. IMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE! Importing the numpy C-extensions failed. This error can happen for. many reasons, often due to issues with your setup or how NumPy was. installed. We have compiled some common reasons and troubleshooting tips at:. https://numpy.org/devdocs/user/troubleshooting-importerror.html. Please note and check the following:. * The Python version is: Python3.8 from ""/usr/bin/python3"". * The NumPy version is: ""1.23.0"". and make sure that they are the versions you expect. Please carefully study the documen",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/610
https://github.com/google/deepvariant/issues/610:1769,safety,modul,module,1769,"dule>. from . import multiarray. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/multiarray.py"", line 10, in <module>. from . import overrides. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/overrides.py"", line 6, in <module>. from numpy.core._multiarray_umath import (. ImportError: libflexiblas.so.3: cannot open shared object file: No such file or directory. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 48, in <module>. import tensorflow as tf. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/__init__.py"", line 41, in <module>. from tensorflow.python.tools import module_util as _module_util. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/__init__.py"", line 41, in <module>. from tensorflow.python.eager import context. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/context.py"", line 30, in <module>. import numpy as np. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/__init__.py"", line 140, in <module>. from . import core. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/__init__.py"", line 49, in <module>. raise ImportError(msg). ImportError:. IMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE! Importing the numpy C-extensions failed. This error can happen for. many reasons, often due to issues with your setup or how NumPy was. installed. We have compiled some common reasons and troubleshooting tips at:. https://numpy.org/devdocs/user/troubleshooting-importerror.html. Please note and check the following:. * The Python version is: Python3.8 from ""/usr/bin/python3"". * The NumPy version is: ""1.23.0"". and make sure that they are the versions you expect. Please carefully study the documentation linked above for further help. Original error was: libflexiblas.so.3: cannot open shared object file: No such file or directory. ```. **Does t",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/610
https://github.com/google/deepvariant/issues/610:1888,safety,modul,module,1888,"e 10, in <module>. from . import overrides. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/overrides.py"", line 6, in <module>. from numpy.core._multiarray_umath import (. ImportError: libflexiblas.so.3: cannot open shared object file: No such file or directory. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 48, in <module>. import tensorflow as tf. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/__init__.py"", line 41, in <module>. from tensorflow.python.tools import module_util as _module_util. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/__init__.py"", line 41, in <module>. from tensorflow.python.eager import context. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/context.py"", line 30, in <module>. import numpy as np. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/__init__.py"", line 140, in <module>. from . import core. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/__init__.py"", line 49, in <module>. raise ImportError(msg). ImportError:. IMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE! Importing the numpy C-extensions failed. This error can happen for. many reasons, often due to issues with your setup or how NumPy was. installed. We have compiled some common reasons and troubleshooting tips at:. https://numpy.org/devdocs/user/troubleshooting-importerror.html. Please note and check the following:. * The Python version is: Python3.8 from ""/usr/bin/python3"". * The NumPy version is: ""1.23.0"". and make sure that they are the versions you expect. Please carefully study the documentation linked above for further help. Original error was: libflexiblas.so.3: cannot open shared object file: No such file or directory. ```. **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deep",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/610
https://github.com/google/deepvariant/issues/610:2011,safety,modul,module,2011,""", line 6, in <module>. from numpy.core._multiarray_umath import (. ImportError: libflexiblas.so.3: cannot open shared object file: No such file or directory. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 48, in <module>. import tensorflow as tf. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/__init__.py"", line 41, in <module>. from tensorflow.python.tools import module_util as _module_util. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/__init__.py"", line 41, in <module>. from tensorflow.python.eager import context. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/context.py"", line 30, in <module>. import numpy as np. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/__init__.py"", line 140, in <module>. from . import core. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/__init__.py"", line 49, in <module>. raise ImportError(msg). ImportError:. IMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE! Importing the numpy C-extensions failed. This error can happen for. many reasons, often due to issues with your setup or how NumPy was. installed. We have compiled some common reasons and troubleshooting tips at:. https://numpy.org/devdocs/user/troubleshooting-importerror.html. Please note and check the following:. * The Python version is: Python3.8 from ""/usr/bin/python3"". * The NumPy version is: ""1.23.0"". and make sure that they are the versions you expect. Please carefully study the documentation linked above for further help. Original error was: libflexiblas.so.3: cannot open shared object file: No such file or directory. ```. **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? Any attempt to execute via `singu",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/610
https://github.com/google/deepvariant/issues/610:2171,safety,error,error,2171,"uring handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 48, in <module>. import tensorflow as tf. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/__init__.py"", line 41, in <module>. from tensorflow.python.tools import module_util as _module_util. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/__init__.py"", line 41, in <module>. from tensorflow.python.eager import context. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/context.py"", line 30, in <module>. import numpy as np. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/__init__.py"", line 140, in <module>. from . import core. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/__init__.py"", line 49, in <module>. raise ImportError(msg). ImportError:. IMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE! Importing the numpy C-extensions failed. This error can happen for. many reasons, often due to issues with your setup or how NumPy was. installed. We have compiled some common reasons and troubleshooting tips at:. https://numpy.org/devdocs/user/troubleshooting-importerror.html. Please note and check the following:. * The Python version is: Python3.8 from ""/usr/bin/python3"". * The NumPy version is: ""1.23.0"". and make sure that they are the versions you expect. Please carefully study the documentation linked above for further help. Original error was: libflexiblas.so.3: cannot open shared object file: No such file or directory. ```. **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? Any attempt to execute via `singularity run` leads to an error. **Any additional context:**. As far as I can tell, my environment meets the requirements for both Python and NumPy - though at th",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/610
https://github.com/google/deepvariant/issues/610:2670,safety,error,error,2670," File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/context.py"", line 30, in <module>. import numpy as np. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/__init__.py"", line 140, in <module>. from . import core. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/__init__.py"", line 49, in <module>. raise ImportError(msg). ImportError:. IMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE! Importing the numpy C-extensions failed. This error can happen for. many reasons, often due to issues with your setup or how NumPy was. installed. We have compiled some common reasons and troubleshooting tips at:. https://numpy.org/devdocs/user/troubleshooting-importerror.html. Please note and check the following:. * The Python version is: Python3.8 from ""/usr/bin/python3"". * The NumPy version is: ""1.23.0"". and make sure that they are the versions you expect. Please carefully study the documentation linked above for further help. Original error was: libflexiblas.so.3: cannot open shared object file: No such file or directory. ```. **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? Any attempt to execute via `singularity run` leads to an error. **Any additional context:**. As far as I can tell, my environment meets the requirements for both Python and NumPy - though at the same time when I `singularity shell` into the SIF file, its versioning seems semi-independent of my main environment (Python 3.8.10 regardless of my environment's version, but using the NumPy 1.23.0 provided by my computing cluster). I feel like I'm missing something really simple, but I've tried the NumPy troubleshooting page and can't seem to crack this error. If it helps, I'm attempting this with `singularity` version 3.8.4, which is the newest version available to me in my computing clust",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/610
https://github.com/google/deepvariant/issues/610:2787,safety,test,test,2787,"le ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/context.py"", line 30, in <module>. import numpy as np. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/__init__.py"", line 140, in <module>. from . import core. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/__init__.py"", line 49, in <module>. raise ImportError(msg). ImportError:. IMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE! Importing the numpy C-extensions failed. This error can happen for. many reasons, often due to issues with your setup or how NumPy was. installed. We have compiled some common reasons and troubleshooting tips at:. https://numpy.org/devdocs/user/troubleshooting-importerror.html. Please note and check the following:. * The Python version is: Python3.8 from ""/usr/bin/python3"". * The NumPy version is: ""1.23.0"". and make sure that they are the versions you expect. Please carefully study the documentation linked above for further help. Original error was: libflexiblas.so.3: cannot open shared object file: No such file or directory. ```. **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? Any attempt to execute via `singularity run` leads to an error. **Any additional context:**. As far as I can tell, my environment meets the requirements for both Python and NumPy - though at the same time when I `singularity shell` into the SIF file, its versioning seems semi-independent of my main environment (Python 3.8.10 regardless of my environment's version, but using the NumPy 1.23.0 provided by my computing cluster). I feel like I'm missing something really simple, but I've tried the NumPy troubleshooting page and can't seem to crack this error. If it helps, I'm attempting this with `singularity` version 3.8.4, which is the newest version available to me in my computing cluster.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/610
https://github.com/google/deepvariant/issues/610:2823,safety,test,test,2823,"le ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/context.py"", line 30, in <module>. import numpy as np. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/__init__.py"", line 140, in <module>. from . import core. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/__init__.py"", line 49, in <module>. raise ImportError(msg). ImportError:. IMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE! Importing the numpy C-extensions failed. This error can happen for. many reasons, often due to issues with your setup or how NumPy was. installed. We have compiled some common reasons and troubleshooting tips at:. https://numpy.org/devdocs/user/troubleshooting-importerror.html. Please note and check the following:. * The Python version is: Python3.8 from ""/usr/bin/python3"". * The NumPy version is: ""1.23.0"". and make sure that they are the versions you expect. Please carefully study the documentation linked above for further help. Original error was: libflexiblas.so.3: cannot open shared object file: No such file or directory. ```. **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? Any attempt to execute via `singularity run` leads to an error. **Any additional context:**. As far as I can tell, my environment meets the requirements for both Python and NumPy - though at the same time when I `singularity shell` into the SIF file, its versioning seems semi-independent of my main environment (Python 3.8.10 regardless of my environment's version, but using the NumPy 1.23.0 provided by my computing cluster). I feel like I'm missing something really simple, but I've tried the NumPy troubleshooting page and can't seem to crack this error. If it helps, I'm attempting this with `singularity` version 3.8.4, which is the newest version available to me in my computing cluster.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/610
https://github.com/google/deepvariant/issues/610:3038,safety,error,error,3038,"le ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/context.py"", line 30, in <module>. import numpy as np. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/__init__.py"", line 140, in <module>. from . import core. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/__init__.py"", line 49, in <module>. raise ImportError(msg). ImportError:. IMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE! Importing the numpy C-extensions failed. This error can happen for. many reasons, often due to issues with your setup or how NumPy was. installed. We have compiled some common reasons and troubleshooting tips at:. https://numpy.org/devdocs/user/troubleshooting-importerror.html. Please note and check the following:. * The Python version is: Python3.8 from ""/usr/bin/python3"". * The NumPy version is: ""1.23.0"". and make sure that they are the versions you expect. Please carefully study the documentation linked above for further help. Original error was: libflexiblas.so.3: cannot open shared object file: No such file or directory. ```. **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? Any attempt to execute via `singularity run` leads to an error. **Any additional context:**. As far as I can tell, my environment meets the requirements for both Python and NumPy - though at the same time when I `singularity shell` into the SIF file, its versioning seems semi-independent of my main environment (Python 3.8.10 regardless of my environment's version, but using the NumPy 1.23.0 provided by my computing cluster). I feel like I'm missing something really simple, but I've tried the NumPy troubleshooting page and can't seem to crack this error. If it helps, I'm attempting this with `singularity` version 3.8.4, which is the newest version available to me in my computing cluster.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/610
https://github.com/google/deepvariant/issues/610:3534,safety,error,error,3534,"le ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/context.py"", line 30, in <module>. import numpy as np. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/__init__.py"", line 140, in <module>. from . import core. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/__init__.py"", line 49, in <module>. raise ImportError(msg). ImportError:. IMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE! Importing the numpy C-extensions failed. This error can happen for. many reasons, often due to issues with your setup or how NumPy was. installed. We have compiled some common reasons and troubleshooting tips at:. https://numpy.org/devdocs/user/troubleshooting-importerror.html. Please note and check the following:. * The Python version is: Python3.8 from ""/usr/bin/python3"". * The NumPy version is: ""1.23.0"". and make sure that they are the versions you expect. Please carefully study the documentation linked above for further help. Original error was: libflexiblas.so.3: cannot open shared object file: No such file or directory. ```. **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? Any attempt to execute via `singularity run` leads to an error. **Any additional context:**. As far as I can tell, my environment meets the requirements for both Python and NumPy - though at the same time when I `singularity shell` into the SIF file, its versioning seems semi-independent of my main environment (Python 3.8.10 regardless of my environment's version, but using the NumPy 1.23.0 provided by my computing cluster). I feel like I'm missing something really simple, but I've tried the NumPy troubleshooting page and can't seem to crack this error. If it helps, I'm attempting this with `singularity` version 3.8.4, which is the newest version available to me in my computing cluster.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/610
https://github.com/google/deepvariant/issues/610:3636,safety,avail,available,3636,"le ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/context.py"", line 30, in <module>. import numpy as np. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/__init__.py"", line 140, in <module>. from . import core. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/__init__.py"", line 49, in <module>. raise ImportError(msg). ImportError:. IMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE! Importing the numpy C-extensions failed. This error can happen for. many reasons, often due to issues with your setup or how NumPy was. installed. We have compiled some common reasons and troubleshooting tips at:. https://numpy.org/devdocs/user/troubleshooting-importerror.html. Please note and check the following:. * The Python version is: Python3.8 from ""/usr/bin/python3"". * The NumPy version is: ""1.23.0"". and make sure that they are the versions you expect. Please carefully study the documentation linked above for further help. Original error was: libflexiblas.so.3: cannot open shared object file: No such file or directory. ```. **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? Any attempt to execute via `singularity run` leads to an error. **Any additional context:**. As far as I can tell, my environment meets the requirements for both Python and NumPy - though at the same time when I `singularity shell` into the SIF file, its versioning seems semi-independent of my main environment (Python 3.8.10 regardless of my environment's version, but using the NumPy 1.23.0 provided by my computing cluster). I feel like I'm missing something really simple, but I've tried the NumPy troubleshooting page and can't seem to crack this error. If it helps, I'm attempting this with `singularity` version 3.8.4, which is the newest version available to me in my computing cluster.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/610
https://github.com/google/deepvariant/issues/610:3636,security,availab,available,3636,"le ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/context.py"", line 30, in <module>. import numpy as np. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/__init__.py"", line 140, in <module>. from . import core. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/__init__.py"", line 49, in <module>. raise ImportError(msg). ImportError:. IMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE! Importing the numpy C-extensions failed. This error can happen for. many reasons, often due to issues with your setup or how NumPy was. installed. We have compiled some common reasons and troubleshooting tips at:. https://numpy.org/devdocs/user/troubleshooting-importerror.html. Please note and check the following:. * The Python version is: Python3.8 from ""/usr/bin/python3"". * The NumPy version is: ""1.23.0"". and make sure that they are the versions you expect. Please carefully study the documentation linked above for further help. Original error was: libflexiblas.so.3: cannot open shared object file: No such file or directory. ```. **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? Any attempt to execute via `singularity run` leads to an error. **Any additional context:**. As far as I can tell, my environment meets the requirements for both Python and NumPy - though at the same time when I `singularity shell` into the SIF file, its versioning seems semi-independent of my main environment (Python 3.8.10 regardless of my environment's version, but using the NumPy 1.23.0 provided by my computing cluster). I feel like I'm missing something really simple, but I've tried the NumPy troubleshooting page and can't seem to crack this error. If it helps, I'm attempting this with `singularity` version 3.8.4, which is the newest version available to me in my computing cluster.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/610
https://github.com/google/deepvariant/issues/610:6,testability,depend,dependency,6,"NumPy dependency error despite matching versions; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**: Yes. **Describe the issue:**. Unable to run Singularity image despite prerequisites appearing satisfied. **Setup**. - Operating system: CentOS 7 . - DeepVariant version: 1.4.0 (google/deepvariant:latest). - Installation method (Docker, built from source, etc.): `singularity pull docker://google/deepvariant:latest` and attempted execution via `singularity run`. - Type of data: N/A. **Steps to reproduce:**. - Command: `singularity run -B /home/ -B /scratch/ deepvariant.sif`. - Error trace:. ```. Traceback (most recent call last):. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/__init__.py"", line 23, in <module>. from . import multiarray. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/multiarray.py"", line 10, in <module>. from . import overrides. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/overrides.py"", line 6, in <module>. from numpy.core._multiarray_umath import (. ImportError: libflexiblas.so.3: cannot open shared object file: No such file or directory. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 48, in <module>. import tensorflow as tf. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/__init__.py"", line 41, in <module>. from tensorflow.python.tools import module_util as _module_util. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/__init__.py"", line 41, in <module>. from tensorflow.python.eager import context. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/context.py"", line 30, in <module>. import numpy as np. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/__init__.py"", line 140, in <module>. from . import core. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/__init__.py"", li",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/610
https://github.com/google/deepvariant/issues/610:627,testability,trace,trace,627,"NumPy dependency error despite matching versions; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**: Yes. **Describe the issue:**. Unable to run Singularity image despite prerequisites appearing satisfied. **Setup**. - Operating system: CentOS 7 . - DeepVariant version: 1.4.0 (google/deepvariant:latest). - Installation method (Docker, built from source, etc.): `singularity pull docker://google/deepvariant:latest` and attempted execution via `singularity run`. - Type of data: N/A. **Steps to reproduce:**. - Command: `singularity run -B /home/ -B /scratch/ deepvariant.sif`. - Error trace:. ```. Traceback (most recent call last):. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/__init__.py"", line 23, in <module>. from . import multiarray. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/multiarray.py"", line 10, in <module>. from . import overrides. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/overrides.py"", line 6, in <module>. from numpy.core._multiarray_umath import (. ImportError: libflexiblas.so.3: cannot open shared object file: No such file or directory. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 48, in <module>. import tensorflow as tf. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/__init__.py"", line 41, in <module>. from tensorflow.python.tools import module_util as _module_util. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/__init__.py"", line 41, in <module>. from tensorflow.python.eager import context. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/context.py"", line 30, in <module>. import numpy as np. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/__init__.py"", line 140, in <module>. from . import core. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/__init__.py"", li",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/610
https://github.com/google/deepvariant/issues/610:640,testability,Trace,Traceback,640,"NumPy dependency error despite matching versions; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**: Yes. **Describe the issue:**. Unable to run Singularity image despite prerequisites appearing satisfied. **Setup**. - Operating system: CentOS 7 . - DeepVariant version: 1.4.0 (google/deepvariant:latest). - Installation method (Docker, built from source, etc.): `singularity pull docker://google/deepvariant:latest` and attempted execution via `singularity run`. - Type of data: N/A. **Steps to reproduce:**. - Command: `singularity run -B /home/ -B /scratch/ deepvariant.sif`. - Error trace:. ```. Traceback (most recent call last):. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/__init__.py"", line 23, in <module>. from . import multiarray. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/multiarray.py"", line 10, in <module>. from . import overrides. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/overrides.py"", line 6, in <module>. from numpy.core._multiarray_umath import (. ImportError: libflexiblas.so.3: cannot open shared object file: No such file or directory. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 48, in <module>. import tensorflow as tf. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/__init__.py"", line 41, in <module>. from tensorflow.python.tools import module_util as _module_util. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/__init__.py"", line 41, in <module>. from tensorflow.python.eager import context. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/context.py"", line 30, in <module>. import numpy as np. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/__init__.py"", line 140, in <module>. from . import core. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/__init__.py"", li",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/610
https://github.com/google/deepvariant/issues/610:1242,testability,Trace,Traceback,1242,"*Setup**. - Operating system: CentOS 7 . - DeepVariant version: 1.4.0 (google/deepvariant:latest). - Installation method (Docker, built from source, etc.): `singularity pull docker://google/deepvariant:latest` and attempted execution via `singularity run`. - Type of data: N/A. **Steps to reproduce:**. - Command: `singularity run -B /home/ -B /scratch/ deepvariant.sif`. - Error trace:. ```. Traceback (most recent call last):. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/__init__.py"", line 23, in <module>. from . import multiarray. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/multiarray.py"", line 10, in <module>. from . import overrides. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/overrides.py"", line 6, in <module>. from numpy.core._multiarray_umath import (. ImportError: libflexiblas.so.3: cannot open shared object file: No such file or directory. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 48, in <module>. import tensorflow as tf. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/__init__.py"", line 41, in <module>. from tensorflow.python.tools import module_util as _module_util. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/__init__.py"", line 41, in <module>. from tensorflow.python.eager import context. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/context.py"", line 30, in <module>. import numpy as np. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/__init__.py"", line 140, in <module>. from . import core. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/__init__.py"", line 49, in <module>. raise ImportError(msg). ImportError:. IMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE! Importing the numpy C-extensions failed. This error can happen for. many reasons, often due to issues with your setup or h",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/610
https://github.com/google/deepvariant/issues/610:1665,testability,context,context,1665,"ast):. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/__init__.py"", line 23, in <module>. from . import multiarray. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/multiarray.py"", line 10, in <module>. from . import overrides. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/overrides.py"", line 6, in <module>. from numpy.core._multiarray_umath import (. ImportError: libflexiblas.so.3: cannot open shared object file: No such file or directory. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 48, in <module>. import tensorflow as tf. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/__init__.py"", line 41, in <module>. from tensorflow.python.tools import module_util as _module_util. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/__init__.py"", line 41, in <module>. from tensorflow.python.eager import context. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/context.py"", line 30, in <module>. import numpy as np. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/__init__.py"", line 140, in <module>. from . import core. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/__init__.py"", line 49, in <module>. raise ImportError(msg). ImportError:. IMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE! Importing the numpy C-extensions failed. This error can happen for. many reasons, often due to issues with your setup or how NumPy was. installed. We have compiled some common reasons and troubleshooting tips at:. https://numpy.org/devdocs/user/troubleshooting-importerror.html. Please note and check the following:. * The Python version is: Python3.8 from ""/usr/bin/python3"". * The NumPy version is: ""1.23.0"". and make sure that they are the versions you expect. Please carefully study the documentation linked above for further help. Original",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/610
https://github.com/google/deepvariant/issues/610:1743,testability,context,context,1743,"it__.py"", line 23, in <module>. from . import multiarray. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/multiarray.py"", line 10, in <module>. from . import overrides. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/overrides.py"", line 6, in <module>. from numpy.core._multiarray_umath import (. ImportError: libflexiblas.so.3: cannot open shared object file: No such file or directory. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 48, in <module>. import tensorflow as tf. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/__init__.py"", line 41, in <module>. from tensorflow.python.tools import module_util as _module_util. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/__init__.py"", line 41, in <module>. from tensorflow.python.eager import context. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/context.py"", line 30, in <module>. import numpy as np. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/__init__.py"", line 140, in <module>. from . import core. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/__init__.py"", line 49, in <module>. raise ImportError(msg). ImportError:. IMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE! Importing the numpy C-extensions failed. This error can happen for. many reasons, often due to issues with your setup or how NumPy was. installed. We have compiled some common reasons and troubleshooting tips at:. https://numpy.org/devdocs/user/troubleshooting-importerror.html. Please note and check the following:. * The Python version is: Python3.8 from ""/usr/bin/python3"". * The NumPy version is: ""1.23.0"". and make sure that they are the versions you expect. Please carefully study the documentation linked above for further help. Original error was: libflexiblas.so.3: cannot open shared object file: No such file or",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/610
https://github.com/google/deepvariant/issues/610:2787,testability,test,test,2787,"le ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/context.py"", line 30, in <module>. import numpy as np. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/__init__.py"", line 140, in <module>. from . import core. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/__init__.py"", line 49, in <module>. raise ImportError(msg). ImportError:. IMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE! Importing the numpy C-extensions failed. This error can happen for. many reasons, often due to issues with your setup or how NumPy was. installed. We have compiled some common reasons and troubleshooting tips at:. https://numpy.org/devdocs/user/troubleshooting-importerror.html. Please note and check the following:. * The Python version is: Python3.8 from ""/usr/bin/python3"". * The NumPy version is: ""1.23.0"". and make sure that they are the versions you expect. Please carefully study the documentation linked above for further help. Original error was: libflexiblas.so.3: cannot open shared object file: No such file or directory. ```. **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? Any attempt to execute via `singularity run` leads to an error. **Any additional context:**. As far as I can tell, my environment meets the requirements for both Python and NumPy - though at the same time when I `singularity shell` into the SIF file, its versioning seems semi-independent of my main environment (Python 3.8.10 regardless of my environment's version, but using the NumPy 1.23.0 provided by my computing cluster). I feel like I'm missing something really simple, but I've tried the NumPy troubleshooting page and can't seem to crack this error. If it helps, I'm attempting this with `singularity` version 3.8.4, which is the newest version available to me in my computing cluster.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/610
https://github.com/google/deepvariant/issues/610:2823,testability,test,test,2823,"le ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/context.py"", line 30, in <module>. import numpy as np. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/__init__.py"", line 140, in <module>. from . import core. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/__init__.py"", line 49, in <module>. raise ImportError(msg). ImportError:. IMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE! Importing the numpy C-extensions failed. This error can happen for. many reasons, often due to issues with your setup or how NumPy was. installed. We have compiled some common reasons and troubleshooting tips at:. https://numpy.org/devdocs/user/troubleshooting-importerror.html. Please note and check the following:. * The Python version is: Python3.8 from ""/usr/bin/python3"". * The NumPy version is: ""1.23.0"". and make sure that they are the versions you expect. Please carefully study the documentation linked above for further help. Original error was: libflexiblas.so.3: cannot open shared object file: No such file or directory. ```. **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? Any attempt to execute via `singularity run` leads to an error. **Any additional context:**. As far as I can tell, my environment meets the requirements for both Python and NumPy - though at the same time when I `singularity shell` into the SIF file, its versioning seems semi-independent of my main environment (Python 3.8.10 regardless of my environment's version, but using the NumPy 1.23.0 provided by my computing cluster). I feel like I'm missing something really simple, but I've tried the NumPy troubleshooting page and can't seem to crack this error. If it helps, I'm attempting this with `singularity` version 3.8.4, which is the newest version available to me in my computing cluster.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/610
https://github.com/google/deepvariant/issues/610:3062,testability,context,context,3062,"le ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/context.py"", line 30, in <module>. import numpy as np. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/__init__.py"", line 140, in <module>. from . import core. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/__init__.py"", line 49, in <module>. raise ImportError(msg). ImportError:. IMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE! Importing the numpy C-extensions failed. This error can happen for. many reasons, often due to issues with your setup or how NumPy was. installed. We have compiled some common reasons and troubleshooting tips at:. https://numpy.org/devdocs/user/troubleshooting-importerror.html. Please note and check the following:. * The Python version is: Python3.8 from ""/usr/bin/python3"". * The NumPy version is: ""1.23.0"". and make sure that they are the versions you expect. Please carefully study the documentation linked above for further help. Original error was: libflexiblas.so.3: cannot open shared object file: No such file or directory. ```. **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? Any attempt to execute via `singularity run` leads to an error. **Any additional context:**. As far as I can tell, my environment meets the requirements for both Python and NumPy - though at the same time when I `singularity shell` into the SIF file, its versioning seems semi-independent of my main environment (Python 3.8.10 regardless of my environment's version, but using the NumPy 1.23.0 provided by my computing cluster). I feel like I'm missing something really simple, but I've tried the NumPy troubleshooting page and can't seem to crack this error. If it helps, I'm attempting this with `singularity` version 3.8.4, which is the newest version available to me in my computing cluster.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/610
https://github.com/google/deepvariant/issues/610:3451,testability,simpl,simple,3451,"le ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/context.py"", line 30, in <module>. import numpy as np. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/__init__.py"", line 140, in <module>. from . import core. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/__init__.py"", line 49, in <module>. raise ImportError(msg). ImportError:. IMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE! Importing the numpy C-extensions failed. This error can happen for. many reasons, often due to issues with your setup or how NumPy was. installed. We have compiled some common reasons and troubleshooting tips at:. https://numpy.org/devdocs/user/troubleshooting-importerror.html. Please note and check the following:. * The Python version is: Python3.8 from ""/usr/bin/python3"". * The NumPy version is: ""1.23.0"". and make sure that they are the versions you expect. Please carefully study the documentation linked above for further help. Original error was: libflexiblas.so.3: cannot open shared object file: No such file or directory. ```. **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? Any attempt to execute via `singularity run` leads to an error. **Any additional context:**. As far as I can tell, my environment meets the requirements for both Python and NumPy - though at the same time when I `singularity shell` into the SIF file, its versioning seems semi-independent of my main environment (Python 3.8.10 regardless of my environment's version, but using the NumPy 1.23.0 provided by my computing cluster). I feel like I'm missing something really simple, but I've tried the NumPy troubleshooting page and can't seem to crack this error. If it helps, I'm attempting this with `singularity` version 3.8.4, which is the newest version available to me in my computing cluster.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/610
https://github.com/google/deepvariant/issues/610:17,usability,error,error,17,"NumPy dependency error despite matching versions; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**: Yes. **Describe the issue:**. Unable to run Singularity image despite prerequisites appearing satisfied. **Setup**. - Operating system: CentOS 7 . - DeepVariant version: 1.4.0 (google/deepvariant:latest). - Installation method (Docker, built from source, etc.): `singularity pull docker://google/deepvariant:latest` and attempted execution via `singularity run`. - Type of data: N/A. **Steps to reproduce:**. - Command: `singularity run -B /home/ -B /scratch/ deepvariant.sif`. - Error trace:. ```. Traceback (most recent call last):. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/__init__.py"", line 23, in <module>. from . import multiarray. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/multiarray.py"", line 10, in <module>. from . import overrides. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/overrides.py"", line 6, in <module>. from numpy.core._multiarray_umath import (. ImportError: libflexiblas.so.3: cannot open shared object file: No such file or directory. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 48, in <module>. import tensorflow as tf. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/__init__.py"", line 41, in <module>. from tensorflow.python.tools import module_util as _module_util. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/__init__.py"", line 41, in <module>. from tensorflow.python.eager import context. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/context.py"", line 30, in <module>. import numpy as np. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/__init__.py"", line 140, in <module>. from . import core. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/__init__.py"", li",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/610
https://github.com/google/deepvariant/issues/610:552,usability,Command,Command,552,"NumPy dependency error despite matching versions; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**: Yes. **Describe the issue:**. Unable to run Singularity image despite prerequisites appearing satisfied. **Setup**. - Operating system: CentOS 7 . - DeepVariant version: 1.4.0 (google/deepvariant:latest). - Installation method (Docker, built from source, etc.): `singularity pull docker://google/deepvariant:latest` and attempted execution via `singularity run`. - Type of data: N/A. **Steps to reproduce:**. - Command: `singularity run -B /home/ -B /scratch/ deepvariant.sif`. - Error trace:. ```. Traceback (most recent call last):. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/__init__.py"", line 23, in <module>. from . import multiarray. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/multiarray.py"", line 10, in <module>. from . import overrides. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/overrides.py"", line 6, in <module>. from numpy.core._multiarray_umath import (. ImportError: libflexiblas.so.3: cannot open shared object file: No such file or directory. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 48, in <module>. import tensorflow as tf. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/__init__.py"", line 41, in <module>. from tensorflow.python.tools import module_util as _module_util. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/__init__.py"", line 41, in <module>. from tensorflow.python.eager import context. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/context.py"", line 30, in <module>. import numpy as np. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/__init__.py"", line 140, in <module>. from . import core. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/__init__.py"", li",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/610
https://github.com/google/deepvariant/issues/610:621,usability,Error,Error,621,"NumPy dependency error despite matching versions; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**: Yes. **Describe the issue:**. Unable to run Singularity image despite prerequisites appearing satisfied. **Setup**. - Operating system: CentOS 7 . - DeepVariant version: 1.4.0 (google/deepvariant:latest). - Installation method (Docker, built from source, etc.): `singularity pull docker://google/deepvariant:latest` and attempted execution via `singularity run`. - Type of data: N/A. **Steps to reproduce:**. - Command: `singularity run -B /home/ -B /scratch/ deepvariant.sif`. - Error trace:. ```. Traceback (most recent call last):. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/__init__.py"", line 23, in <module>. from . import multiarray. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/multiarray.py"", line 10, in <module>. from . import overrides. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/overrides.py"", line 6, in <module>. from numpy.core._multiarray_umath import (. ImportError: libflexiblas.so.3: cannot open shared object file: No such file or directory. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 48, in <module>. import tensorflow as tf. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/__init__.py"", line 41, in <module>. from tensorflow.python.tools import module_util as _module_util. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/__init__.py"", line 41, in <module>. from tensorflow.python.eager import context. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/context.py"", line 30, in <module>. import numpy as np. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/__init__.py"", line 140, in <module>. from . import core. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/__init__.py"", li",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/610
https://github.com/google/deepvariant/issues/610:1488,usability,tool,tools,1488,"larity run`. - Type of data: N/A. **Steps to reproduce:**. - Command: `singularity run -B /home/ -B /scratch/ deepvariant.sif`. - Error trace:. ```. Traceback (most recent call last):. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/__init__.py"", line 23, in <module>. from . import multiarray. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/multiarray.py"", line 10, in <module>. from . import overrides. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/overrides.py"", line 6, in <module>. from numpy.core._multiarray_umath import (. ImportError: libflexiblas.so.3: cannot open shared object file: No such file or directory. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 48, in <module>. import tensorflow as tf. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/__init__.py"", line 41, in <module>. from tensorflow.python.tools import module_util as _module_util. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/__init__.py"", line 41, in <module>. from tensorflow.python.eager import context. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/context.py"", line 30, in <module>. import numpy as np. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/__init__.py"", line 140, in <module>. from . import core. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/__init__.py"", line 49, in <module>. raise ImportError(msg). ImportError:. IMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE! Importing the numpy C-extensions failed. This error can happen for. many reasons, often due to issues with your setup or how NumPy was. installed. We have compiled some common reasons and troubleshooting tips at:. https://numpy.org/devdocs/user/troubleshooting-importerror.html. Please note and check the following:. * The Python version is: Python3.8 from ""/usr/bin",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/610
https://github.com/google/deepvariant/issues/610:2171,usability,error,error,2171,"uring handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 48, in <module>. import tensorflow as tf. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/__init__.py"", line 41, in <module>. from tensorflow.python.tools import module_util as _module_util. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/__init__.py"", line 41, in <module>. from tensorflow.python.eager import context. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/context.py"", line 30, in <module>. import numpy as np. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/__init__.py"", line 140, in <module>. from . import core. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/__init__.py"", line 49, in <module>. raise ImportError(msg). ImportError:. IMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE! Importing the numpy C-extensions failed. This error can happen for. many reasons, often due to issues with your setup or how NumPy was. installed. We have compiled some common reasons and troubleshooting tips at:. https://numpy.org/devdocs/user/troubleshooting-importerror.html. Please note and check the following:. * The Python version is: Python3.8 from ""/usr/bin/python3"". * The NumPy version is: ""1.23.0"". and make sure that they are the versions you expect. Please carefully study the documentation linked above for further help. Original error was: libflexiblas.so.3: cannot open shared object file: No such file or directory. ```. **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? Any attempt to execute via `singularity run` leads to an error. **Any additional context:**. As far as I can tell, my environment meets the requirements for both Python and NumPy - though at th",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/610
https://github.com/google/deepvariant/issues/610:2329,usability,tip,tips,2329,"48, in <module>. import tensorflow as tf. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/__init__.py"", line 41, in <module>. from tensorflow.python.tools import module_util as _module_util. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/__init__.py"", line 41, in <module>. from tensorflow.python.eager import context. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/context.py"", line 30, in <module>. import numpy as np. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/__init__.py"", line 140, in <module>. from . import core. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/__init__.py"", line 49, in <module>. raise ImportError(msg). ImportError:. IMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE! Importing the numpy C-extensions failed. This error can happen for. many reasons, often due to issues with your setup or how NumPy was. installed. We have compiled some common reasons and troubleshooting tips at:. https://numpy.org/devdocs/user/troubleshooting-importerror.html. Please note and check the following:. * The Python version is: Python3.8 from ""/usr/bin/python3"". * The NumPy version is: ""1.23.0"". and make sure that they are the versions you expect. Please carefully study the documentation linked above for further help. Original error was: libflexiblas.so.3: cannot open shared object file: No such file or directory. ```. **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? Any attempt to execute via `singularity run` leads to an error. **Any additional context:**. As far as I can tell, my environment meets the requirements for both Python and NumPy - though at the same time when I `singularity shell` into the SIF file, its versioning seems semi-independent of my main environment (Python 3.8.10 regardless of my enviro",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/610
https://github.com/google/deepvariant/issues/610:2365,usability,user,user,2365,"s tf. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/__init__.py"", line 41, in <module>. from tensorflow.python.tools import module_util as _module_util. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/__init__.py"", line 41, in <module>. from tensorflow.python.eager import context. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/context.py"", line 30, in <module>. import numpy as np. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/__init__.py"", line 140, in <module>. from . import core. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/__init__.py"", line 49, in <module>. raise ImportError(msg). ImportError:. IMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE! Importing the numpy C-extensions failed. This error can happen for. many reasons, often due to issues with your setup or how NumPy was. installed. We have compiled some common reasons and troubleshooting tips at:. https://numpy.org/devdocs/user/troubleshooting-importerror.html. Please note and check the following:. * The Python version is: Python3.8 from ""/usr/bin/python3"". * The NumPy version is: ""1.23.0"". and make sure that they are the versions you expect. Please carefully study the documentation linked above for further help. Original error was: libflexiblas.so.3: cannot open shared object file: No such file or directory. ```. **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? Any attempt to execute via `singularity run` leads to an error. **Any additional context:**. As far as I can tell, my environment meets the requirements for both Python and NumPy - though at the same time when I `singularity shell` into the SIF file, its versioning seems semi-independent of my main environment (Python 3.8.10 regardless of my environment's version, but using the NumPy",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/610
https://github.com/google/deepvariant/issues/610:2616,usability,document,documentation,2616,"ule>. from tensorflow.python.eager import context. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/context.py"", line 30, in <module>. import numpy as np. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/__init__.py"", line 140, in <module>. from . import core. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/__init__.py"", line 49, in <module>. raise ImportError(msg). ImportError:. IMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE! Importing the numpy C-extensions failed. This error can happen for. many reasons, often due to issues with your setup or how NumPy was. installed. We have compiled some common reasons and troubleshooting tips at:. https://numpy.org/devdocs/user/troubleshooting-importerror.html. Please note and check the following:. * The Python version is: Python3.8 from ""/usr/bin/python3"". * The NumPy version is: ""1.23.0"". and make sure that they are the versions you expect. Please carefully study the documentation linked above for further help. Original error was: libflexiblas.so.3: cannot open shared object file: No such file or directory. ```. **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? Any attempt to execute via `singularity run` leads to an error. **Any additional context:**. As far as I can tell, my environment meets the requirements for both Python and NumPy - though at the same time when I `singularity shell` into the SIF file, its versioning seems semi-independent of my main environment (Python 3.8.10 regardless of my environment's version, but using the NumPy 1.23.0 provided by my computing cluster). I feel like I'm missing something really simple, but I've tried the NumPy troubleshooting page and can't seem to crack this error. If it helps, I'm attempting this with `singularity` version 3.8.4, which is the ne",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/610
https://github.com/google/deepvariant/issues/610:2655,usability,help,help,2655," import context. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/context.py"", line 30, in <module>. import numpy as np. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/__init__.py"", line 140, in <module>. from . import core. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/__init__.py"", line 49, in <module>. raise ImportError(msg). ImportError:. IMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE! Importing the numpy C-extensions failed. This error can happen for. many reasons, often due to issues with your setup or how NumPy was. installed. We have compiled some common reasons and troubleshooting tips at:. https://numpy.org/devdocs/user/troubleshooting-importerror.html. Please note and check the following:. * The Python version is: Python3.8 from ""/usr/bin/python3"". * The NumPy version is: ""1.23.0"". and make sure that they are the versions you expect. Please carefully study the documentation linked above for further help. Original error was: libflexiblas.so.3: cannot open shared object file: No such file or directory. ```. **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? Any attempt to execute via `singularity run` leads to an error. **Any additional context:**. As far as I can tell, my environment meets the requirements for both Python and NumPy - though at the same time when I `singularity shell` into the SIF file, its versioning seems semi-independent of my main environment (Python 3.8.10 regardless of my environment's version, but using the NumPy 1.23.0 provided by my computing cluster). I feel like I'm missing something really simple, but I've tried the NumPy troubleshooting page and can't seem to crack this error. If it helps, I'm attempting this with `singularity` version 3.8.4, which is the newest version available to me in my",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/610
https://github.com/google/deepvariant/issues/610:2670,usability,error,error,2670," File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/context.py"", line 30, in <module>. import numpy as np. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/__init__.py"", line 140, in <module>. from . import core. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/__init__.py"", line 49, in <module>. raise ImportError(msg). ImportError:. IMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE! Importing the numpy C-extensions failed. This error can happen for. many reasons, often due to issues with your setup or how NumPy was. installed. We have compiled some common reasons and troubleshooting tips at:. https://numpy.org/devdocs/user/troubleshooting-importerror.html. Please note and check the following:. * The Python version is: Python3.8 from ""/usr/bin/python3"". * The NumPy version is: ""1.23.0"". and make sure that they are the versions you expect. Please carefully study the documentation linked above for further help. Original error was: libflexiblas.so.3: cannot open shared object file: No such file or directory. ```. **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? Any attempt to execute via `singularity run` leads to an error. **Any additional context:**. As far as I can tell, my environment meets the requirements for both Python and NumPy - though at the same time when I `singularity shell` into the SIF file, its versioning seems semi-independent of my main environment (Python 3.8.10 regardless of my environment's version, but using the NumPy 1.23.0 provided by my computing cluster). I feel like I'm missing something really simple, but I've tried the NumPy troubleshooting page and can't seem to crack this error. If it helps, I'm attempting this with `singularity` version 3.8.4, which is the newest version available to me in my computing clust",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/610
https://github.com/google/deepvariant/issues/610:3038,usability,error,error,3038,"le ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/context.py"", line 30, in <module>. import numpy as np. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/__init__.py"", line 140, in <module>. from . import core. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/__init__.py"", line 49, in <module>. raise ImportError(msg). ImportError:. IMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE! Importing the numpy C-extensions failed. This error can happen for. many reasons, often due to issues with your setup or how NumPy was. installed. We have compiled some common reasons and troubleshooting tips at:. https://numpy.org/devdocs/user/troubleshooting-importerror.html. Please note and check the following:. * The Python version is: Python3.8 from ""/usr/bin/python3"". * The NumPy version is: ""1.23.0"". and make sure that they are the versions you expect. Please carefully study the documentation linked above for further help. Original error was: libflexiblas.so.3: cannot open shared object file: No such file or directory. ```. **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? Any attempt to execute via `singularity run` leads to an error. **Any additional context:**. As far as I can tell, my environment meets the requirements for both Python and NumPy - though at the same time when I `singularity shell` into the SIF file, its versioning seems semi-independent of my main environment (Python 3.8.10 regardless of my environment's version, but using the NumPy 1.23.0 provided by my computing cluster). I feel like I'm missing something really simple, but I've tried the NumPy troubleshooting page and can't seem to crack this error. If it helps, I'm attempting this with `singularity` version 3.8.4, which is the newest version available to me in my computing cluster.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/610
https://github.com/google/deepvariant/issues/610:3451,usability,simpl,simple,3451,"le ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/context.py"", line 30, in <module>. import numpy as np. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/__init__.py"", line 140, in <module>. from . import core. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/__init__.py"", line 49, in <module>. raise ImportError(msg). ImportError:. IMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE! Importing the numpy C-extensions failed. This error can happen for. many reasons, often due to issues with your setup or how NumPy was. installed. We have compiled some common reasons and troubleshooting tips at:. https://numpy.org/devdocs/user/troubleshooting-importerror.html. Please note and check the following:. * The Python version is: Python3.8 from ""/usr/bin/python3"". * The NumPy version is: ""1.23.0"". and make sure that they are the versions you expect. Please carefully study the documentation linked above for further help. Original error was: libflexiblas.so.3: cannot open shared object file: No such file or directory. ```. **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? Any attempt to execute via `singularity run` leads to an error. **Any additional context:**. As far as I can tell, my environment meets the requirements for both Python and NumPy - though at the same time when I `singularity shell` into the SIF file, its versioning seems semi-independent of my main environment (Python 3.8.10 regardless of my environment's version, but using the NumPy 1.23.0 provided by my computing cluster). I feel like I'm missing something really simple, but I've tried the NumPy troubleshooting page and can't seem to crack this error. If it helps, I'm attempting this with `singularity` version 3.8.4, which is the newest version available to me in my computing cluster.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/610
https://github.com/google/deepvariant/issues/610:3534,usability,error,error,3534,"le ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/context.py"", line 30, in <module>. import numpy as np. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/__init__.py"", line 140, in <module>. from . import core. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/__init__.py"", line 49, in <module>. raise ImportError(msg). ImportError:. IMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE! Importing the numpy C-extensions failed. This error can happen for. many reasons, often due to issues with your setup or how NumPy was. installed. We have compiled some common reasons and troubleshooting tips at:. https://numpy.org/devdocs/user/troubleshooting-importerror.html. Please note and check the following:. * The Python version is: Python3.8 from ""/usr/bin/python3"". * The NumPy version is: ""1.23.0"". and make sure that they are the versions you expect. Please carefully study the documentation linked above for further help. Original error was: libflexiblas.so.3: cannot open shared object file: No such file or directory. ```. **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? Any attempt to execute via `singularity run` leads to an error. **Any additional context:**. As far as I can tell, my environment meets the requirements for both Python and NumPy - though at the same time when I `singularity shell` into the SIF file, its versioning seems semi-independent of my main environment (Python 3.8.10 regardless of my environment's version, but using the NumPy 1.23.0 provided by my computing cluster). I feel like I'm missing something really simple, but I've tried the NumPy troubleshooting page and can't seem to crack this error. If it helps, I'm attempting this with `singularity` version 3.8.4, which is the newest version available to me in my computing cluster.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/610
https://github.com/google/deepvariant/issues/610:3547,usability,help,helps,3547,"le ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/context.py"", line 30, in <module>. import numpy as np. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/__init__.py"", line 140, in <module>. from . import core. File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/__init__.py"", line 49, in <module>. raise ImportError(msg). ImportError:. IMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE! Importing the numpy C-extensions failed. This error can happen for. many reasons, often due to issues with your setup or how NumPy was. installed. We have compiled some common reasons and troubleshooting tips at:. https://numpy.org/devdocs/user/troubleshooting-importerror.html. Please note and check the following:. * The Python version is: Python3.8 from ""/usr/bin/python3"". * The NumPy version is: ""1.23.0"". and make sure that they are the versions you expect. Please carefully study the documentation linked above for further help. Original error was: libflexiblas.so.3: cannot open shared object file: No such file or directory. ```. **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? Any attempt to execute via `singularity run` leads to an error. **Any additional context:**. As far as I can tell, my environment meets the requirements for both Python and NumPy - though at the same time when I `singularity shell` into the SIF file, its versioning seems semi-independent of my main environment (Python 3.8.10 regardless of my environment's version, but using the NumPy 1.23.0 provided by my computing cluster). I feel like I'm missing something really simple, but I've tried the NumPy troubleshooting page and can't seem to crack this error. If it helps, I'm attempting this with `singularity` version 3.8.4, which is the newest version available to me in my computing cluster.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/610
https://github.com/google/deepvariant/issues/611:53,availability,checkpoint,checkpoints,53,"`model_eval` ignores pre-existing, not-yet-evaluated checkpoints, and nothing else seems to exist to evaluate them; **Describe the issue:**. The `model_eval` command can't catch up and evaluate checkpoints in a directory other than the latest one. If it ever crashes or isn't started in time to actually witness training create a checkpoint, there's no way for the user to evaluate the checkpoints it missed, and the ""best"" checkpoint it finds won't actually necessarily be the best out of all the checkpoints in the directory. . **Setup**. - Operating system: Ubuntu 22.04. - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): Docker. - Type of data: case study data. **Steps to reproduce:**. Loosely following https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-training-case-study.md#start-model_train-and-model_eval, I started the training Docker container using GPU 0:. ```. time docker run --rm --gpus 1 \. -v ""${DATA_DIR}:${DATA_DIR}"" \. -v ""${OUTPUT_DIR}:${OUTPUT_DIR}"" \. -u $(id -u):$(id -g) \. google/deepvariant:1.4.0-gpu \. /opt/deepvariant/bin/model_train \. --dataset_config_pbtxt=""${OUTPUT_DIR}/training_set.dataset_config.pbtxt"" \. --train_dir=""${TRAINING_DIR}"" \. --model_name=""inception_v3"" \. --number_of_steps=50000 \. --save_interval_secs=300 \. --batch_size=32 \. --learning_rate=0.0005 \. --start_from_checkpoint=""gs://deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-wgs_standard/model.ckpt"". ```. Then I tried to start the `model_eval` evaluator in GPU mode with 1 GPU, but Docker was just giving it the *same* GPU as training was using, so it kept immediately running out of GPU memory. After about half an hour I hit on the right syntax to give it a distinct GPU, and managed to actually start `model_eval`:. ```. docker run --rm --gpus '""device=1""' \. -v ""${DATA_DIR}:${DATA_DIR}"" \. -v ""${OUTPUT_DIR}:${OUTPUT_DIR}"" \. -u $(id -u):$(id -g) \. google/deepvariant:1.4.0-gpu \. /opt/deepvariant/bin/mod",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/611
https://github.com/google/deepvariant/issues/611:194,availability,checkpoint,checkpoints,194,"`model_eval` ignores pre-existing, not-yet-evaluated checkpoints, and nothing else seems to exist to evaluate them; **Describe the issue:**. The `model_eval` command can't catch up and evaluate checkpoints in a directory other than the latest one. If it ever crashes or isn't started in time to actually witness training create a checkpoint, there's no way for the user to evaluate the checkpoints it missed, and the ""best"" checkpoint it finds won't actually necessarily be the best out of all the checkpoints in the directory. . **Setup**. - Operating system: Ubuntu 22.04. - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): Docker. - Type of data: case study data. **Steps to reproduce:**. Loosely following https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-training-case-study.md#start-model_train-and-model_eval, I started the training Docker container using GPU 0:. ```. time docker run --rm --gpus 1 \. -v ""${DATA_DIR}:${DATA_DIR}"" \. -v ""${OUTPUT_DIR}:${OUTPUT_DIR}"" \. -u $(id -u):$(id -g) \. google/deepvariant:1.4.0-gpu \. /opt/deepvariant/bin/model_train \. --dataset_config_pbtxt=""${OUTPUT_DIR}/training_set.dataset_config.pbtxt"" \. --train_dir=""${TRAINING_DIR}"" \. --model_name=""inception_v3"" \. --number_of_steps=50000 \. --save_interval_secs=300 \. --batch_size=32 \. --learning_rate=0.0005 \. --start_from_checkpoint=""gs://deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-wgs_standard/model.ckpt"". ```. Then I tried to start the `model_eval` evaluator in GPU mode with 1 GPU, but Docker was just giving it the *same* GPU as training was using, so it kept immediately running out of GPU memory. After about half an hour I hit on the right syntax to give it a distinct GPU, and managed to actually start `model_eval`:. ```. docker run --rm --gpus '""device=1""' \. -v ""${DATA_DIR}:${DATA_DIR}"" \. -v ""${OUTPUT_DIR}:${OUTPUT_DIR}"" \. -u $(id -u):$(id -g) \. google/deepvariant:1.4.0-gpu \. /opt/deepvariant/bin/mod",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/611
https://github.com/google/deepvariant/issues/611:330,availability,checkpoint,checkpoint,330,"`model_eval` ignores pre-existing, not-yet-evaluated checkpoints, and nothing else seems to exist to evaluate them; **Describe the issue:**. The `model_eval` command can't catch up and evaluate checkpoints in a directory other than the latest one. If it ever crashes or isn't started in time to actually witness training create a checkpoint, there's no way for the user to evaluate the checkpoints it missed, and the ""best"" checkpoint it finds won't actually necessarily be the best out of all the checkpoints in the directory. . **Setup**. - Operating system: Ubuntu 22.04. - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): Docker. - Type of data: case study data. **Steps to reproduce:**. Loosely following https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-training-case-study.md#start-model_train-and-model_eval, I started the training Docker container using GPU 0:. ```. time docker run --rm --gpus 1 \. -v ""${DATA_DIR}:${DATA_DIR}"" \. -v ""${OUTPUT_DIR}:${OUTPUT_DIR}"" \. -u $(id -u):$(id -g) \. google/deepvariant:1.4.0-gpu \. /opt/deepvariant/bin/model_train \. --dataset_config_pbtxt=""${OUTPUT_DIR}/training_set.dataset_config.pbtxt"" \. --train_dir=""${TRAINING_DIR}"" \. --model_name=""inception_v3"" \. --number_of_steps=50000 \. --save_interval_secs=300 \. --batch_size=32 \. --learning_rate=0.0005 \. --start_from_checkpoint=""gs://deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-wgs_standard/model.ckpt"". ```. Then I tried to start the `model_eval` evaluator in GPU mode with 1 GPU, but Docker was just giving it the *same* GPU as training was using, so it kept immediately running out of GPU memory. After about half an hour I hit on the right syntax to give it a distinct GPU, and managed to actually start `model_eval`:. ```. docker run --rm --gpus '""device=1""' \. -v ""${DATA_DIR}:${DATA_DIR}"" \. -v ""${OUTPUT_DIR}:${OUTPUT_DIR}"" \. -u $(id -u):$(id -g) \. google/deepvariant:1.4.0-gpu \. /opt/deepvariant/bin/mod",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/611
https://github.com/google/deepvariant/issues/611:386,availability,checkpoint,checkpoints,386,"`model_eval` ignores pre-existing, not-yet-evaluated checkpoints, and nothing else seems to exist to evaluate them; **Describe the issue:**. The `model_eval` command can't catch up and evaluate checkpoints in a directory other than the latest one. If it ever crashes or isn't started in time to actually witness training create a checkpoint, there's no way for the user to evaluate the checkpoints it missed, and the ""best"" checkpoint it finds won't actually necessarily be the best out of all the checkpoints in the directory. . **Setup**. - Operating system: Ubuntu 22.04. - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): Docker. - Type of data: case study data. **Steps to reproduce:**. Loosely following https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-training-case-study.md#start-model_train-and-model_eval, I started the training Docker container using GPU 0:. ```. time docker run --rm --gpus 1 \. -v ""${DATA_DIR}:${DATA_DIR}"" \. -v ""${OUTPUT_DIR}:${OUTPUT_DIR}"" \. -u $(id -u):$(id -g) \. google/deepvariant:1.4.0-gpu \. /opt/deepvariant/bin/model_train \. --dataset_config_pbtxt=""${OUTPUT_DIR}/training_set.dataset_config.pbtxt"" \. --train_dir=""${TRAINING_DIR}"" \. --model_name=""inception_v3"" \. --number_of_steps=50000 \. --save_interval_secs=300 \. --batch_size=32 \. --learning_rate=0.0005 \. --start_from_checkpoint=""gs://deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-wgs_standard/model.ckpt"". ```. Then I tried to start the `model_eval` evaluator in GPU mode with 1 GPU, but Docker was just giving it the *same* GPU as training was using, so it kept immediately running out of GPU memory. After about half an hour I hit on the right syntax to give it a distinct GPU, and managed to actually start `model_eval`:. ```. docker run --rm --gpus '""device=1""' \. -v ""${DATA_DIR}:${DATA_DIR}"" \. -v ""${OUTPUT_DIR}:${OUTPUT_DIR}"" \. -u $(id -u):$(id -g) \. google/deepvariant:1.4.0-gpu \. /opt/deepvariant/bin/mod",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/611
https://github.com/google/deepvariant/issues/611:424,availability,checkpoint,checkpoint,424,"`model_eval` ignores pre-existing, not-yet-evaluated checkpoints, and nothing else seems to exist to evaluate them; **Describe the issue:**. The `model_eval` command can't catch up and evaluate checkpoints in a directory other than the latest one. If it ever crashes or isn't started in time to actually witness training create a checkpoint, there's no way for the user to evaluate the checkpoints it missed, and the ""best"" checkpoint it finds won't actually necessarily be the best out of all the checkpoints in the directory. . **Setup**. - Operating system: Ubuntu 22.04. - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): Docker. - Type of data: case study data. **Steps to reproduce:**. Loosely following https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-training-case-study.md#start-model_train-and-model_eval, I started the training Docker container using GPU 0:. ```. time docker run --rm --gpus 1 \. -v ""${DATA_DIR}:${DATA_DIR}"" \. -v ""${OUTPUT_DIR}:${OUTPUT_DIR}"" \. -u $(id -u):$(id -g) \. google/deepvariant:1.4.0-gpu \. /opt/deepvariant/bin/model_train \. --dataset_config_pbtxt=""${OUTPUT_DIR}/training_set.dataset_config.pbtxt"" \. --train_dir=""${TRAINING_DIR}"" \. --model_name=""inception_v3"" \. --number_of_steps=50000 \. --save_interval_secs=300 \. --batch_size=32 \. --learning_rate=0.0005 \. --start_from_checkpoint=""gs://deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-wgs_standard/model.ckpt"". ```. Then I tried to start the `model_eval` evaluator in GPU mode with 1 GPU, but Docker was just giving it the *same* GPU as training was using, so it kept immediately running out of GPU memory. After about half an hour I hit on the right syntax to give it a distinct GPU, and managed to actually start `model_eval`:. ```. docker run --rm --gpus '""device=1""' \. -v ""${DATA_DIR}:${DATA_DIR}"" \. -v ""${OUTPUT_DIR}:${OUTPUT_DIR}"" \. -u $(id -u):$(id -g) \. google/deepvariant:1.4.0-gpu \. /opt/deepvariant/bin/mod",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/611
https://github.com/google/deepvariant/issues/611:498,availability,checkpoint,checkpoints,498,"`model_eval` ignores pre-existing, not-yet-evaluated checkpoints, and nothing else seems to exist to evaluate them; **Describe the issue:**. The `model_eval` command can't catch up and evaluate checkpoints in a directory other than the latest one. If it ever crashes or isn't started in time to actually witness training create a checkpoint, there's no way for the user to evaluate the checkpoints it missed, and the ""best"" checkpoint it finds won't actually necessarily be the best out of all the checkpoints in the directory. . **Setup**. - Operating system: Ubuntu 22.04. - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): Docker. - Type of data: case study data. **Steps to reproduce:**. Loosely following https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-training-case-study.md#start-model_train-and-model_eval, I started the training Docker container using GPU 0:. ```. time docker run --rm --gpus 1 \. -v ""${DATA_DIR}:${DATA_DIR}"" \. -v ""${OUTPUT_DIR}:${OUTPUT_DIR}"" \. -u $(id -u):$(id -g) \. google/deepvariant:1.4.0-gpu \. /opt/deepvariant/bin/model_train \. --dataset_config_pbtxt=""${OUTPUT_DIR}/training_set.dataset_config.pbtxt"" \. --train_dir=""${TRAINING_DIR}"" \. --model_name=""inception_v3"" \. --number_of_steps=50000 \. --save_interval_secs=300 \. --batch_size=32 \. --learning_rate=0.0005 \. --start_from_checkpoint=""gs://deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-wgs_standard/model.ckpt"". ```. Then I tried to start the `model_eval` evaluator in GPU mode with 1 GPU, but Docker was just giving it the *same* GPU as training was using, so it kept immediately running out of GPU memory. After about half an hour I hit on the right syntax to give it a distinct GPU, and managed to actually start `model_eval`:. ```. docker run --rm --gpus '""device=1""' \. -v ""${DATA_DIR}:${DATA_DIR}"" \. -v ""${OUTPUT_DIR}:${OUTPUT_DIR}"" \. -u $(id -u):$(id -g) \. google/deepvariant:1.4.0-gpu \. /opt/deepvariant/bin/mod",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/611
https://github.com/google/deepvariant/issues/611:543,availability,Operat,Operating,543,"`model_eval` ignores pre-existing, not-yet-evaluated checkpoints, and nothing else seems to exist to evaluate them; **Describe the issue:**. The `model_eval` command can't catch up and evaluate checkpoints in a directory other than the latest one. If it ever crashes or isn't started in time to actually witness training create a checkpoint, there's no way for the user to evaluate the checkpoints it missed, and the ""best"" checkpoint it finds won't actually necessarily be the best out of all the checkpoints in the directory. . **Setup**. - Operating system: Ubuntu 22.04. - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): Docker. - Type of data: case study data. **Steps to reproduce:**. Loosely following https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-training-case-study.md#start-model_train-and-model_eval, I started the training Docker container using GPU 0:. ```. time docker run --rm --gpus 1 \. -v ""${DATA_DIR}:${DATA_DIR}"" \. -v ""${OUTPUT_DIR}:${OUTPUT_DIR}"" \. -u $(id -u):$(id -g) \. google/deepvariant:1.4.0-gpu \. /opt/deepvariant/bin/model_train \. --dataset_config_pbtxt=""${OUTPUT_DIR}/training_set.dataset_config.pbtxt"" \. --train_dir=""${TRAINING_DIR}"" \. --model_name=""inception_v3"" \. --number_of_steps=50000 \. --save_interval_secs=300 \. --batch_size=32 \. --learning_rate=0.0005 \. --start_from_checkpoint=""gs://deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-wgs_standard/model.ckpt"". ```. Then I tried to start the `model_eval` evaluator in GPU mode with 1 GPU, but Docker was just giving it the *same* GPU as training was using, so it kept immediately running out of GPU memory. After about half an hour I hit on the right syntax to give it a distinct GPU, and managed to actually start `model_eval`:. ```. docker run --rm --gpus '""device=1""' \. -v ""${DATA_DIR}:${DATA_DIR}"" \. -v ""${OUTPUT_DIR}:${OUTPUT_DIR}"" \. -u $(id -u):$(id -g) \. google/deepvariant:1.4.0-gpu \. /opt/deepvariant/bin/mod",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/611
https://github.com/google/deepvariant/issues/611:2229,availability,checkpoint,checkpoint,2229,"el_name=""inception_v3"" \. --number_of_steps=50000 \. --save_interval_secs=300 \. --batch_size=32 \. --learning_rate=0.0005 \. --start_from_checkpoint=""gs://deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-wgs_standard/model.ckpt"". ```. Then I tried to start the `model_eval` evaluator in GPU mode with 1 GPU, but Docker was just giving it the *same* GPU as training was using, so it kept immediately running out of GPU memory. After about half an hour I hit on the right syntax to give it a distinct GPU, and managed to actually start `model_eval`:. ```. docker run --rm --gpus '""device=1""' \. -v ""${DATA_DIR}:${DATA_DIR}"" \. -v ""${OUTPUT_DIR}:${OUTPUT_DIR}"" \. -u $(id -u):$(id -g) \. google/deepvariant:1.4.0-gpu \. /opt/deepvariant/bin/model_eval \. --dataset_config_pbtxt=""${OUTPUT_DIR}/validation_set.dataset_config.pbtxt"" \. --checkpoint_dir=""${TRAINING_DIR}"" \. --batch_size=512 \. --min_eval_interval_s=1 \. --eval_timeout=1000. ```. It evaluated the most recent checkpoint saves as of when it started up, and it is evaluating new checkpoints as they are created. But it doesn't seem interested in going back and evaluating all the checkpoints, other than the initial-latest one, that were created before it started. The directory in question looks like this:. ```. (dv_venv) [anovak@phoenix-01 trash]$ ls output/models/*meta. output/models/model.ckpt-0.meta output/models/model.ckpt-19459.meta output/models/model.ckpt-31078.meta. output/models/model.ckpt-1000.meta output/models/model.ckpt-2000.meta output/models/model.ckpt-34008.meta. output/models/model.ckpt-10674.meta output/models/model.ckpt-22355.meta output/models/model.ckpt-4814.meta. output/models/model.ckpt-13613.meta output/models/model.ckpt-25257.meta output/models/model.ckpt-7724.meta. output/models/model.ckpt-16546.meta output/models/model.ckpt-28168.meta. (dv_venv) [anovak@phoenix-01 trash]$ ls output/models/*metrics. output/models/best_checkpoint.metrics output/models/model.ckpt-28168.metrics",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/611
https://github.com/google/deepvariant/issues/611:2297,availability,checkpoint,checkpoints,2297,"secs=300 \. --batch_size=32 \. --learning_rate=0.0005 \. --start_from_checkpoint=""gs://deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-wgs_standard/model.ckpt"". ```. Then I tried to start the `model_eval` evaluator in GPU mode with 1 GPU, but Docker was just giving it the *same* GPU as training was using, so it kept immediately running out of GPU memory. After about half an hour I hit on the right syntax to give it a distinct GPU, and managed to actually start `model_eval`:. ```. docker run --rm --gpus '""device=1""' \. -v ""${DATA_DIR}:${DATA_DIR}"" \. -v ""${OUTPUT_DIR}:${OUTPUT_DIR}"" \. -u $(id -u):$(id -g) \. google/deepvariant:1.4.0-gpu \. /opt/deepvariant/bin/model_eval \. --dataset_config_pbtxt=""${OUTPUT_DIR}/validation_set.dataset_config.pbtxt"" \. --checkpoint_dir=""${TRAINING_DIR}"" \. --batch_size=512 \. --min_eval_interval_s=1 \. --eval_timeout=1000. ```. It evaluated the most recent checkpoint saves as of when it started up, and it is evaluating new checkpoints as they are created. But it doesn't seem interested in going back and evaluating all the checkpoints, other than the initial-latest one, that were created before it started. The directory in question looks like this:. ```. (dv_venv) [anovak@phoenix-01 trash]$ ls output/models/*meta. output/models/model.ckpt-0.meta output/models/model.ckpt-19459.meta output/models/model.ckpt-31078.meta. output/models/model.ckpt-1000.meta output/models/model.ckpt-2000.meta output/models/model.ckpt-34008.meta. output/models/model.ckpt-10674.meta output/models/model.ckpt-22355.meta output/models/model.ckpt-4814.meta. output/models/model.ckpt-13613.meta output/models/model.ckpt-25257.meta output/models/model.ckpt-7724.meta. output/models/model.ckpt-16546.meta output/models/model.ckpt-28168.meta. (dv_venv) [anovak@phoenix-01 trash]$ ls output/models/*metrics. output/models/best_checkpoint.metrics output/models/model.ckpt-28168.metrics output/models/model.ckpt-34008.metrics. output/models/current.metric",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/611
https://github.com/google/deepvariant/issues/611:2398,availability,checkpoint,checkpoints,2398,"dels/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-wgs_standard/model.ckpt"". ```. Then I tried to start the `model_eval` evaluator in GPU mode with 1 GPU, but Docker was just giving it the *same* GPU as training was using, so it kept immediately running out of GPU memory. After about half an hour I hit on the right syntax to give it a distinct GPU, and managed to actually start `model_eval`:. ```. docker run --rm --gpus '""device=1""' \. -v ""${DATA_DIR}:${DATA_DIR}"" \. -v ""${OUTPUT_DIR}:${OUTPUT_DIR}"" \. -u $(id -u):$(id -g) \. google/deepvariant:1.4.0-gpu \. /opt/deepvariant/bin/model_eval \. --dataset_config_pbtxt=""${OUTPUT_DIR}/validation_set.dataset_config.pbtxt"" \. --checkpoint_dir=""${TRAINING_DIR}"" \. --batch_size=512 \. --min_eval_interval_s=1 \. --eval_timeout=1000. ```. It evaluated the most recent checkpoint saves as of when it started up, and it is evaluating new checkpoints as they are created. But it doesn't seem interested in going back and evaluating all the checkpoints, other than the initial-latest one, that were created before it started. The directory in question looks like this:. ```. (dv_venv) [anovak@phoenix-01 trash]$ ls output/models/*meta. output/models/model.ckpt-0.meta output/models/model.ckpt-19459.meta output/models/model.ckpt-31078.meta. output/models/model.ckpt-1000.meta output/models/model.ckpt-2000.meta output/models/model.ckpt-34008.meta. output/models/model.ckpt-10674.meta output/models/model.ckpt-22355.meta output/models/model.ckpt-4814.meta. output/models/model.ckpt-13613.meta output/models/model.ckpt-25257.meta output/models/model.ckpt-7724.meta. output/models/model.ckpt-16546.meta output/models/model.ckpt-28168.meta. (dv_venv) [anovak@phoenix-01 trash]$ ls output/models/*metrics. output/models/best_checkpoint.metrics output/models/model.ckpt-28168.metrics output/models/model.ckpt-34008.metrics. output/models/current.metrics output/models/model.ckpt-31078.metrics. ```. But `model_eval` just sits there like this (until a ne",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/611
https://github.com/google/deepvariant/issues/611:3406,availability,checkpoint,checkpoint,3406,"other than the initial-latest one, that were created before it started. The directory in question looks like this:. ```. (dv_venv) [anovak@phoenix-01 trash]$ ls output/models/*meta. output/models/model.ckpt-0.meta output/models/model.ckpt-19459.meta output/models/model.ckpt-31078.meta. output/models/model.ckpt-1000.meta output/models/model.ckpt-2000.meta output/models/model.ckpt-34008.meta. output/models/model.ckpt-10674.meta output/models/model.ckpt-22355.meta output/models/model.ckpt-4814.meta. output/models/model.ckpt-13613.meta output/models/model.ckpt-25257.meta output/models/model.ckpt-7724.meta. output/models/model.ckpt-16546.meta output/models/model.ckpt-28168.meta. (dv_venv) [anovak@phoenix-01 trash]$ ls output/models/*metrics. output/models/best_checkpoint.metrics output/models/model.ckpt-28168.metrics output/models/model.ckpt-34008.metrics. output/models/current.metrics output/models/model.ckpt-31078.metrics. ```. But `model_eval` just sits there like this (until a new checkpoint appears):. ```. I0210 17:42:06.700287 139846137329472 checkpoint_utils.py:140] Waiting for new checkpoint at /public/groups/cgl/graph-genomes/anovak/trash/output/models. ```. How do I get the missing `*metrics` files and determine if any of the checkpoints that were missed is actually the best one? Do I need to `touch` some particular files in the directory to get `model_eval` to be interested in them? Is there some other command besides `model_eval` that can process a single particular checkpoint at a time? **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? It doesn't look like model training is part of the quick start. **Any additional context:**. Eventually I might want a WDL workflow for training DeepVariant, and I'm not sure that managing two simultaneous DV processes in there is going to be worth the engineerin",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/611
https://github.com/google/deepvariant/issues/611:3512,availability,checkpoint,checkpoint,3512,"ke this:. ```. (dv_venv) [anovak@phoenix-01 trash]$ ls output/models/*meta. output/models/model.ckpt-0.meta output/models/model.ckpt-19459.meta output/models/model.ckpt-31078.meta. output/models/model.ckpt-1000.meta output/models/model.ckpt-2000.meta output/models/model.ckpt-34008.meta. output/models/model.ckpt-10674.meta output/models/model.ckpt-22355.meta output/models/model.ckpt-4814.meta. output/models/model.ckpt-13613.meta output/models/model.ckpt-25257.meta output/models/model.ckpt-7724.meta. output/models/model.ckpt-16546.meta output/models/model.ckpt-28168.meta. (dv_venv) [anovak@phoenix-01 trash]$ ls output/models/*metrics. output/models/best_checkpoint.metrics output/models/model.ckpt-28168.metrics output/models/model.ckpt-34008.metrics. output/models/current.metrics output/models/model.ckpt-31078.metrics. ```. But `model_eval` just sits there like this (until a new checkpoint appears):. ```. I0210 17:42:06.700287 139846137329472 checkpoint_utils.py:140] Waiting for new checkpoint at /public/groups/cgl/graph-genomes/anovak/trash/output/models. ```. How do I get the missing `*metrics` files and determine if any of the checkpoints that were missed is actually the best one? Do I need to `touch` some particular files in the directory to get `model_eval` to be interested in them? Is there some other command besides `model_eval` that can process a single particular checkpoint at a time? **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? It doesn't look like model training is part of the quick start. **Any additional context:**. Eventually I might want a WDL workflow for training DeepVariant, and I'm not sure that managing two simultaneous DV processes in there is going to be worth the engineering required; they'd have to be lumped together into one WDL task and they'd have to always fit simultaneous",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/611
https://github.com/google/deepvariant/issues/611:3662,availability,checkpoint,checkpoints,3662,"models/model.ckpt-31078.meta. output/models/model.ckpt-1000.meta output/models/model.ckpt-2000.meta output/models/model.ckpt-34008.meta. output/models/model.ckpt-10674.meta output/models/model.ckpt-22355.meta output/models/model.ckpt-4814.meta. output/models/model.ckpt-13613.meta output/models/model.ckpt-25257.meta output/models/model.ckpt-7724.meta. output/models/model.ckpt-16546.meta output/models/model.ckpt-28168.meta. (dv_venv) [anovak@phoenix-01 trash]$ ls output/models/*metrics. output/models/best_checkpoint.metrics output/models/model.ckpt-28168.metrics output/models/model.ckpt-34008.metrics. output/models/current.metrics output/models/model.ckpt-31078.metrics. ```. But `model_eval` just sits there like this (until a new checkpoint appears):. ```. I0210 17:42:06.700287 139846137329472 checkpoint_utils.py:140] Waiting for new checkpoint at /public/groups/cgl/graph-genomes/anovak/trash/output/models. ```. How do I get the missing `*metrics` files and determine if any of the checkpoints that were missed is actually the best one? Do I need to `touch` some particular files in the directory to get `model_eval` to be interested in them? Is there some other command besides `model_eval` that can process a single particular checkpoint at a time? **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? It doesn't look like model training is part of the quick start. **Any additional context:**. Eventually I might want a WDL workflow for training DeepVariant, and I'm not sure that managing two simultaneous DV processes in there is going to be worth the engineering required; they'd have to be lumped together into one WDL task and they'd have to always fit simultaneously on one machine. It would be much simpler for me to be able to run the training to the end, and then run all the evaluations afterward to select the ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/611
https://github.com/google/deepvariant/issues/611:3909,availability,checkpoint,checkpoint,3909,"0674.meta output/models/model.ckpt-22355.meta output/models/model.ckpt-4814.meta. output/models/model.ckpt-13613.meta output/models/model.ckpt-25257.meta output/models/model.ckpt-7724.meta. output/models/model.ckpt-16546.meta output/models/model.ckpt-28168.meta. (dv_venv) [anovak@phoenix-01 trash]$ ls output/models/*metrics. output/models/best_checkpoint.metrics output/models/model.ckpt-28168.metrics output/models/model.ckpt-34008.metrics. output/models/current.metrics output/models/model.ckpt-31078.metrics. ```. But `model_eval` just sits there like this (until a new checkpoint appears):. ```. I0210 17:42:06.700287 139846137329472 checkpoint_utils.py:140] Waiting for new checkpoint at /public/groups/cgl/graph-genomes/anovak/trash/output/models. ```. How do I get the missing `*metrics` files and determine if any of the checkpoints that were missed is actually the best one? Do I need to `touch` some particular files in the directory to get `model_eval` to be interested in them? Is there some other command besides `model_eval` that can process a single particular checkpoint at a time? **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? It doesn't look like model training is part of the quick start. **Any additional context:**. Eventually I might want a WDL workflow for training DeepVariant, and I'm not sure that managing two simultaneous DV processes in there is going to be worth the engineering required; they'd have to be lumped together into one WDL task and they'd have to always fit simultaneously on one machine. It would be much simpler for me to be able to run the training to the end, and then run all the evaluations afterward to select the best model. But it looks like if I tried that right now `model_eval` would just only evaluate the last checkpoint and always confidently declare it to be the best.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/611
https://github.com/google/deepvariant/issues/611:4771,availability,checkpoint,checkpoint,4771,"0674.meta output/models/model.ckpt-22355.meta output/models/model.ckpt-4814.meta. output/models/model.ckpt-13613.meta output/models/model.ckpt-25257.meta output/models/model.ckpt-7724.meta. output/models/model.ckpt-16546.meta output/models/model.ckpt-28168.meta. (dv_venv) [anovak@phoenix-01 trash]$ ls output/models/*metrics. output/models/best_checkpoint.metrics output/models/model.ckpt-28168.metrics output/models/model.ckpt-34008.metrics. output/models/current.metrics output/models/model.ckpt-31078.metrics. ```. But `model_eval` just sits there like this (until a new checkpoint appears):. ```. I0210 17:42:06.700287 139846137329472 checkpoint_utils.py:140] Waiting for new checkpoint at /public/groups/cgl/graph-genomes/anovak/trash/output/models. ```. How do I get the missing `*metrics` files and determine if any of the checkpoints that were missed is actually the best one? Do I need to `touch` some particular files in the directory to get `model_eval` to be interested in them? Is there some other command besides `model_eval` that can process a single particular checkpoint at a time? **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? It doesn't look like model training is part of the quick start. **Any additional context:**. Eventually I might want a WDL workflow for training DeepVariant, and I'm not sure that managing two simultaneous DV processes in there is going to be worth the engineering required; they'd have to be lumped together into one WDL task and they'd have to always fit simultaneously on one machine. It would be much simpler for me to be able to run the training to the end, and then run all the evaluations afterward to select the best model. But it looks like if I tried that right now `model_eval` would just only evaluate the last checkpoint and always confidently declare it to be the best.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/611
https://github.com/google/deepvariant/issues/611:589,deployability,version,version,589,"`model_eval` ignores pre-existing, not-yet-evaluated checkpoints, and nothing else seems to exist to evaluate them; **Describe the issue:**. The `model_eval` command can't catch up and evaluate checkpoints in a directory other than the latest one. If it ever crashes or isn't started in time to actually witness training create a checkpoint, there's no way for the user to evaluate the checkpoints it missed, and the ""best"" checkpoint it finds won't actually necessarily be the best out of all the checkpoints in the directory. . **Setup**. - Operating system: Ubuntu 22.04. - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): Docker. - Type of data: case study data. **Steps to reproduce:**. Loosely following https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-training-case-study.md#start-model_train-and-model_eval, I started the training Docker container using GPU 0:. ```. time docker run --rm --gpus 1 \. -v ""${DATA_DIR}:${DATA_DIR}"" \. -v ""${OUTPUT_DIR}:${OUTPUT_DIR}"" \. -u $(id -u):$(id -g) \. google/deepvariant:1.4.0-gpu \. /opt/deepvariant/bin/model_train \. --dataset_config_pbtxt=""${OUTPUT_DIR}/training_set.dataset_config.pbtxt"" \. --train_dir=""${TRAINING_DIR}"" \. --model_name=""inception_v3"" \. --number_of_steps=50000 \. --save_interval_secs=300 \. --batch_size=32 \. --learning_rate=0.0005 \. --start_from_checkpoint=""gs://deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-wgs_standard/model.ckpt"". ```. Then I tried to start the `model_eval` evaluator in GPU mode with 1 GPU, but Docker was just giving it the *same* GPU as training was using, so it kept immediately running out of GPU memory. After about half an hour I hit on the right syntax to give it a distinct GPU, and managed to actually start `model_eval`:. ```. docker run --rm --gpus '""device=1""' \. -v ""${DATA_DIR}:${DATA_DIR}"" \. -v ""${OUTPUT_DIR}:${OUTPUT_DIR}"" \. -u $(id -u):$(id -g) \. google/deepvariant:1.4.0-gpu \. /opt/deepvariant/bin/mod",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/611
https://github.com/google/deepvariant/issues/611:607,deployability,Instal,Installation,607,"`model_eval` ignores pre-existing, not-yet-evaluated checkpoints, and nothing else seems to exist to evaluate them; **Describe the issue:**. The `model_eval` command can't catch up and evaluate checkpoints in a directory other than the latest one. If it ever crashes or isn't started in time to actually witness training create a checkpoint, there's no way for the user to evaluate the checkpoints it missed, and the ""best"" checkpoint it finds won't actually necessarily be the best out of all the checkpoints in the directory. . **Setup**. - Operating system: Ubuntu 22.04. - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): Docker. - Type of data: case study data. **Steps to reproduce:**. Loosely following https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-training-case-study.md#start-model_train-and-model_eval, I started the training Docker container using GPU 0:. ```. time docker run --rm --gpus 1 \. -v ""${DATA_DIR}:${DATA_DIR}"" \. -v ""${OUTPUT_DIR}:${OUTPUT_DIR}"" \. -u $(id -u):$(id -g) \. google/deepvariant:1.4.0-gpu \. /opt/deepvariant/bin/model_train \. --dataset_config_pbtxt=""${OUTPUT_DIR}/training_set.dataset_config.pbtxt"" \. --train_dir=""${TRAINING_DIR}"" \. --model_name=""inception_v3"" \. --number_of_steps=50000 \. --save_interval_secs=300 \. --batch_size=32 \. --learning_rate=0.0005 \. --start_from_checkpoint=""gs://deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-wgs_standard/model.ckpt"". ```. Then I tried to start the `model_eval` evaluator in GPU mode with 1 GPU, but Docker was just giving it the *same* GPU as training was using, so it kept immediately running out of GPU memory. After about half an hour I hit on the right syntax to give it a distinct GPU, and managed to actually start `model_eval`:. ```. docker run --rm --gpus '""device=1""' \. -v ""${DATA_DIR}:${DATA_DIR}"" \. -v ""${OUTPUT_DIR}:${OUTPUT_DIR}"" \. -u $(id -u):$(id -g) \. google/deepvariant:1.4.0-gpu \. /opt/deepvariant/bin/mod",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/611
https://github.com/google/deepvariant/issues/611:898,deployability,contain,container,898,"`model_eval` ignores pre-existing, not-yet-evaluated checkpoints, and nothing else seems to exist to evaluate them; **Describe the issue:**. The `model_eval` command can't catch up and evaluate checkpoints in a directory other than the latest one. If it ever crashes or isn't started in time to actually witness training create a checkpoint, there's no way for the user to evaluate the checkpoints it missed, and the ""best"" checkpoint it finds won't actually necessarily be the best out of all the checkpoints in the directory. . **Setup**. - Operating system: Ubuntu 22.04. - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): Docker. - Type of data: case study data. **Steps to reproduce:**. Loosely following https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-training-case-study.md#start-model_train-and-model_eval, I started the training Docker container using GPU 0:. ```. time docker run --rm --gpus 1 \. -v ""${DATA_DIR}:${DATA_DIR}"" \. -v ""${OUTPUT_DIR}:${OUTPUT_DIR}"" \. -u $(id -u):$(id -g) \. google/deepvariant:1.4.0-gpu \. /opt/deepvariant/bin/model_train \. --dataset_config_pbtxt=""${OUTPUT_DIR}/training_set.dataset_config.pbtxt"" \. --train_dir=""${TRAINING_DIR}"" \. --model_name=""inception_v3"" \. --number_of_steps=50000 \. --save_interval_secs=300 \. --batch_size=32 \. --learning_rate=0.0005 \. --start_from_checkpoint=""gs://deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-wgs_standard/model.ckpt"". ```. Then I tried to start the `model_eval` evaluator in GPU mode with 1 GPU, but Docker was just giving it the *same* GPU as training was using, so it kept immediately running out of GPU memory. After about half an hour I hit on the right syntax to give it a distinct GPU, and managed to actually start `model_eval`:. ```. docker run --rm --gpus '""device=1""' \. -v ""${DATA_DIR}:${DATA_DIR}"" \. -v ""${OUTPUT_DIR}:${OUTPUT_DIR}"" \. -u $(id -u):$(id -g) \. google/deepvariant:1.4.0-gpu \. /opt/deepvariant/bin/mod",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/611
https://github.com/google/deepvariant/issues/611:1767,deployability,manag,managed,1767,"/deepvariant/blob/r1.4/docs/deepvariant-training-case-study.md#start-model_train-and-model_eval, I started the training Docker container using GPU 0:. ```. time docker run --rm --gpus 1 \. -v ""${DATA_DIR}:${DATA_DIR}"" \. -v ""${OUTPUT_DIR}:${OUTPUT_DIR}"" \. -u $(id -u):$(id -g) \. google/deepvariant:1.4.0-gpu \. /opt/deepvariant/bin/model_train \. --dataset_config_pbtxt=""${OUTPUT_DIR}/training_set.dataset_config.pbtxt"" \. --train_dir=""${TRAINING_DIR}"" \. --model_name=""inception_v3"" \. --number_of_steps=50000 \. --save_interval_secs=300 \. --batch_size=32 \. --learning_rate=0.0005 \. --start_from_checkpoint=""gs://deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-wgs_standard/model.ckpt"". ```. Then I tried to start the `model_eval` evaluator in GPU mode with 1 GPU, but Docker was just giving it the *same* GPU as training was using, so it kept immediately running out of GPU memory. After about half an hour I hit on the right syntax to give it a distinct GPU, and managed to actually start `model_eval`:. ```. docker run --rm --gpus '""device=1""' \. -v ""${DATA_DIR}:${DATA_DIR}"" \. -v ""${OUTPUT_DIR}:${OUTPUT_DIR}"" \. -u $(id -u):$(id -g) \. google/deepvariant:1.4.0-gpu \. /opt/deepvariant/bin/model_eval \. --dataset_config_pbtxt=""${OUTPUT_DIR}/validation_set.dataset_config.pbtxt"" \. --checkpoint_dir=""${TRAINING_DIR}"" \. --batch_size=512 \. --min_eval_interval_s=1 \. --eval_timeout=1000. ```. It evaluated the most recent checkpoint saves as of when it started up, and it is evaluating new checkpoints as they are created. But it doesn't seem interested in going back and evaluating all the checkpoints, other than the initial-latest one, that were created before it started. The directory in question looks like this:. ```. (dv_venv) [anovak@phoenix-01 trash]$ ls output/models/*meta. output/models/model.ckpt-0.meta output/models/model.ckpt-19459.meta output/models/model.ckpt-31078.meta. output/models/model.ckpt-1000.meta output/models/model.ckpt-2000.meta out",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/611
https://github.com/google/deepvariant/issues/611:4328,deployability,manag,managing,4328,"0674.meta output/models/model.ckpt-22355.meta output/models/model.ckpt-4814.meta. output/models/model.ckpt-13613.meta output/models/model.ckpt-25257.meta output/models/model.ckpt-7724.meta. output/models/model.ckpt-16546.meta output/models/model.ckpt-28168.meta. (dv_venv) [anovak@phoenix-01 trash]$ ls output/models/*metrics. output/models/best_checkpoint.metrics output/models/model.ckpt-28168.metrics output/models/model.ckpt-34008.metrics. output/models/current.metrics output/models/model.ckpt-31078.metrics. ```. But `model_eval` just sits there like this (until a new checkpoint appears):. ```. I0210 17:42:06.700287 139846137329472 checkpoint_utils.py:140] Waiting for new checkpoint at /public/groups/cgl/graph-genomes/anovak/trash/output/models. ```. How do I get the missing `*metrics` files and determine if any of the checkpoints that were missed is actually the best one? Do I need to `touch` some particular files in the directory to get `model_eval` to be interested in them? Is there some other command besides `model_eval` that can process a single particular checkpoint at a time? **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? It doesn't look like model training is part of the quick start. **Any additional context:**. Eventually I might want a WDL workflow for training DeepVariant, and I'm not sure that managing two simultaneous DV processes in there is going to be worth the engineering required; they'd have to be lumped together into one WDL task and they'd have to always fit simultaneously on one machine. It would be much simpler for me to be able to run the training to the end, and then run all the evaluations afterward to select the best model. But it looks like if I tried that right now `model_eval` would just only evaluate the last checkpoint and always confidently declare it to be the best.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/611
https://github.com/google/deepvariant/issues/611:914,energy efficiency,GPU,GPU,914,"`model_eval` ignores pre-existing, not-yet-evaluated checkpoints, and nothing else seems to exist to evaluate them; **Describe the issue:**. The `model_eval` command can't catch up and evaluate checkpoints in a directory other than the latest one. If it ever crashes or isn't started in time to actually witness training create a checkpoint, there's no way for the user to evaluate the checkpoints it missed, and the ""best"" checkpoint it finds won't actually necessarily be the best out of all the checkpoints in the directory. . **Setup**. - Operating system: Ubuntu 22.04. - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): Docker. - Type of data: case study data. **Steps to reproduce:**. Loosely following https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-training-case-study.md#start-model_train-and-model_eval, I started the training Docker container using GPU 0:. ```. time docker run --rm --gpus 1 \. -v ""${DATA_DIR}:${DATA_DIR}"" \. -v ""${OUTPUT_DIR}:${OUTPUT_DIR}"" \. -u $(id -u):$(id -g) \. google/deepvariant:1.4.0-gpu \. /opt/deepvariant/bin/model_train \. --dataset_config_pbtxt=""${OUTPUT_DIR}/training_set.dataset_config.pbtxt"" \. --train_dir=""${TRAINING_DIR}"" \. --model_name=""inception_v3"" \. --number_of_steps=50000 \. --save_interval_secs=300 \. --batch_size=32 \. --learning_rate=0.0005 \. --start_from_checkpoint=""gs://deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-wgs_standard/model.ckpt"". ```. Then I tried to start the `model_eval` evaluator in GPU mode with 1 GPU, but Docker was just giving it the *same* GPU as training was using, so it kept immediately running out of GPU memory. After about half an hour I hit on the right syntax to give it a distinct GPU, and managed to actually start `model_eval`:. ```. docker run --rm --gpus '""device=1""' \. -v ""${DATA_DIR}:${DATA_DIR}"" \. -v ""${OUTPUT_DIR}:${OUTPUT_DIR}"" \. -u $(id -u):$(id -g) \. google/deepvariant:1.4.0-gpu \. /opt/deepvariant/bin/mod",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/611
https://github.com/google/deepvariant/issues/611:950,energy efficiency,gpu,gpus,950,"`model_eval` ignores pre-existing, not-yet-evaluated checkpoints, and nothing else seems to exist to evaluate them; **Describe the issue:**. The `model_eval` command can't catch up and evaluate checkpoints in a directory other than the latest one. If it ever crashes or isn't started in time to actually witness training create a checkpoint, there's no way for the user to evaluate the checkpoints it missed, and the ""best"" checkpoint it finds won't actually necessarily be the best out of all the checkpoints in the directory. . **Setup**. - Operating system: Ubuntu 22.04. - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): Docker. - Type of data: case study data. **Steps to reproduce:**. Loosely following https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-training-case-study.md#start-model_train-and-model_eval, I started the training Docker container using GPU 0:. ```. time docker run --rm --gpus 1 \. -v ""${DATA_DIR}:${DATA_DIR}"" \. -v ""${OUTPUT_DIR}:${OUTPUT_DIR}"" \. -u $(id -u):$(id -g) \. google/deepvariant:1.4.0-gpu \. /opt/deepvariant/bin/model_train \. --dataset_config_pbtxt=""${OUTPUT_DIR}/training_set.dataset_config.pbtxt"" \. --train_dir=""${TRAINING_DIR}"" \. --model_name=""inception_v3"" \. --number_of_steps=50000 \. --save_interval_secs=300 \. --batch_size=32 \. --learning_rate=0.0005 \. --start_from_checkpoint=""gs://deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-wgs_standard/model.ckpt"". ```. Then I tried to start the `model_eval` evaluator in GPU mode with 1 GPU, but Docker was just giving it the *same* GPU as training was using, so it kept immediately running out of GPU memory. After about half an hour I hit on the right syntax to give it a distinct GPU, and managed to actually start `model_eval`:. ```. docker run --rm --gpus '""device=1""' \. -v ""${DATA_DIR}:${DATA_DIR}"" \. -v ""${OUTPUT_DIR}:${OUTPUT_DIR}"" \. -u $(id -u):$(id -g) \. google/deepvariant:1.4.0-gpu \. /opt/deepvariant/bin/mod",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/611
https://github.com/google/deepvariant/issues/611:1077,energy efficiency,gpu,gpu,1077,"lse seems to exist to evaluate them; **Describe the issue:**. The `model_eval` command can't catch up and evaluate checkpoints in a directory other than the latest one. If it ever crashes or isn't started in time to actually witness training create a checkpoint, there's no way for the user to evaluate the checkpoints it missed, and the ""best"" checkpoint it finds won't actually necessarily be the best out of all the checkpoints in the directory. . **Setup**. - Operating system: Ubuntu 22.04. - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): Docker. - Type of data: case study data. **Steps to reproduce:**. Loosely following https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-training-case-study.md#start-model_train-and-model_eval, I started the training Docker container using GPU 0:. ```. time docker run --rm --gpus 1 \. -v ""${DATA_DIR}:${DATA_DIR}"" \. -v ""${OUTPUT_DIR}:${OUTPUT_DIR}"" \. -u $(id -u):$(id -g) \. google/deepvariant:1.4.0-gpu \. /opt/deepvariant/bin/model_train \. --dataset_config_pbtxt=""${OUTPUT_DIR}/training_set.dataset_config.pbtxt"" \. --train_dir=""${TRAINING_DIR}"" \. --model_name=""inception_v3"" \. --number_of_steps=50000 \. --save_interval_secs=300 \. --batch_size=32 \. --learning_rate=0.0005 \. --start_from_checkpoint=""gs://deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-wgs_standard/model.ckpt"". ```. Then I tried to start the `model_eval` evaluator in GPU mode with 1 GPU, but Docker was just giving it the *same* GPU as training was using, so it kept immediately running out of GPU memory. After about half an hour I hit on the right syntax to give it a distinct GPU, and managed to actually start `model_eval`:. ```. docker run --rm --gpus '""device=1""' \. -v ""${DATA_DIR}:${DATA_DIR}"" \. -v ""${OUTPUT_DIR}:${OUTPUT_DIR}"" \. -u $(id -u):$(id -g) \. google/deepvariant:1.4.0-gpu \. /opt/deepvariant/bin/model_eval \. --dataset_config_pbtxt=""${OUTPUT_DIR}/validation_set.dataset_config.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/611
https://github.com/google/deepvariant/issues/611:1402,energy efficiency,model,models,1402,"ed, and the ""best"" checkpoint it finds won't actually necessarily be the best out of all the checkpoints in the directory. . **Setup**. - Operating system: Ubuntu 22.04. - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): Docker. - Type of data: case study data. **Steps to reproduce:**. Loosely following https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-training-case-study.md#start-model_train-and-model_eval, I started the training Docker container using GPU 0:. ```. time docker run --rm --gpus 1 \. -v ""${DATA_DIR}:${DATA_DIR}"" \. -v ""${OUTPUT_DIR}:${OUTPUT_DIR}"" \. -u $(id -u):$(id -g) \. google/deepvariant:1.4.0-gpu \. /opt/deepvariant/bin/model_train \. --dataset_config_pbtxt=""${OUTPUT_DIR}/training_set.dataset_config.pbtxt"" \. --train_dir=""${TRAINING_DIR}"" \. --model_name=""inception_v3"" \. --number_of_steps=50000 \. --save_interval_secs=300 \. --batch_size=32 \. --learning_rate=0.0005 \. --start_from_checkpoint=""gs://deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-wgs_standard/model.ckpt"". ```. Then I tried to start the `model_eval` evaluator in GPU mode with 1 GPU, but Docker was just giving it the *same* GPU as training was using, so it kept immediately running out of GPU memory. After about half an hour I hit on the right syntax to give it a distinct GPU, and managed to actually start `model_eval`:. ```. docker run --rm --gpus '""device=1""' \. -v ""${DATA_DIR}:${DATA_DIR}"" \. -v ""${OUTPUT_DIR}:${OUTPUT_DIR}"" \. -u $(id -u):$(id -g) \. google/deepvariant:1.4.0-gpu \. /opt/deepvariant/bin/model_eval \. --dataset_config_pbtxt=""${OUTPUT_DIR}/validation_set.dataset_config.pbtxt"" \. --checkpoint_dir=""${TRAINING_DIR}"" \. --batch_size=512 \. --min_eval_interval_s=1 \. --eval_timeout=1000. ```. It evaluated the most recent checkpoint saves as of when it started up, and it is evaluating new checkpoints as they are created. But it doesn't seem interested in going back and evaluating all the checkpo",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/611
https://github.com/google/deepvariant/issues/611:1476,energy efficiency,model,model,1476,"est out of all the checkpoints in the directory. . **Setup**. - Operating system: Ubuntu 22.04. - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): Docker. - Type of data: case study data. **Steps to reproduce:**. Loosely following https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-training-case-study.md#start-model_train-and-model_eval, I started the training Docker container using GPU 0:. ```. time docker run --rm --gpus 1 \. -v ""${DATA_DIR}:${DATA_DIR}"" \. -v ""${OUTPUT_DIR}:${OUTPUT_DIR}"" \. -u $(id -u):$(id -g) \. google/deepvariant:1.4.0-gpu \. /opt/deepvariant/bin/model_train \. --dataset_config_pbtxt=""${OUTPUT_DIR}/training_set.dataset_config.pbtxt"" \. --train_dir=""${TRAINING_DIR}"" \. --model_name=""inception_v3"" \. --number_of_steps=50000 \. --save_interval_secs=300 \. --batch_size=32 \. --learning_rate=0.0005 \. --start_from_checkpoint=""gs://deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-wgs_standard/model.ckpt"". ```. Then I tried to start the `model_eval` evaluator in GPU mode with 1 GPU, but Docker was just giving it the *same* GPU as training was using, so it kept immediately running out of GPU memory. After about half an hour I hit on the right syntax to give it a distinct GPU, and managed to actually start `model_eval`:. ```. docker run --rm --gpus '""device=1""' \. -v ""${DATA_DIR}:${DATA_DIR}"" \. -v ""${OUTPUT_DIR}:${OUTPUT_DIR}"" \. -u $(id -u):$(id -g) \. google/deepvariant:1.4.0-gpu \. /opt/deepvariant/bin/model_eval \. --dataset_config_pbtxt=""${OUTPUT_DIR}/validation_set.dataset_config.pbtxt"" \. --checkpoint_dir=""${TRAINING_DIR}"" \. --batch_size=512 \. --min_eval_interval_s=1 \. --eval_timeout=1000. ```. It evaluated the most recent checkpoint saves as of when it started up, and it is evaluating new checkpoints as they are created. But it doesn't seem interested in going back and evaluating all the checkpoints, other than the initial-latest one, that were created before it start",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/611
https://github.com/google/deepvariant/issues/611:1546,energy efficiency,GPU,GPU,1546,"ting system: Ubuntu 22.04. - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): Docker. - Type of data: case study data. **Steps to reproduce:**. Loosely following https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-training-case-study.md#start-model_train-and-model_eval, I started the training Docker container using GPU 0:. ```. time docker run --rm --gpus 1 \. -v ""${DATA_DIR}:${DATA_DIR}"" \. -v ""${OUTPUT_DIR}:${OUTPUT_DIR}"" \. -u $(id -u):$(id -g) \. google/deepvariant:1.4.0-gpu \. /opt/deepvariant/bin/model_train \. --dataset_config_pbtxt=""${OUTPUT_DIR}/training_set.dataset_config.pbtxt"" \. --train_dir=""${TRAINING_DIR}"" \. --model_name=""inception_v3"" \. --number_of_steps=50000 \. --save_interval_secs=300 \. --batch_size=32 \. --learning_rate=0.0005 \. --start_from_checkpoint=""gs://deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-wgs_standard/model.ckpt"". ```. Then I tried to start the `model_eval` evaluator in GPU mode with 1 GPU, but Docker was just giving it the *same* GPU as training was using, so it kept immediately running out of GPU memory. After about half an hour I hit on the right syntax to give it a distinct GPU, and managed to actually start `model_eval`:. ```. docker run --rm --gpus '""device=1""' \. -v ""${DATA_DIR}:${DATA_DIR}"" \. -v ""${OUTPUT_DIR}:${OUTPUT_DIR}"" \. -u $(id -u):$(id -g) \. google/deepvariant:1.4.0-gpu \. /opt/deepvariant/bin/model_eval \. --dataset_config_pbtxt=""${OUTPUT_DIR}/validation_set.dataset_config.pbtxt"" \. --checkpoint_dir=""${TRAINING_DIR}"" \. --batch_size=512 \. --min_eval_interval_s=1 \. --eval_timeout=1000. ```. It evaluated the most recent checkpoint saves as of when it started up, and it is evaluating new checkpoints as they are created. But it doesn't seem interested in going back and evaluating all the checkpoints, other than the initial-latest one, that were created before it started. The directory in question looks like this:. ```. (dv_venv) [anova",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/611
https://github.com/google/deepvariant/issues/611:1562,energy efficiency,GPU,GPU,1562,"ntu 22.04. - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): Docker. - Type of data: case study data. **Steps to reproduce:**. Loosely following https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-training-case-study.md#start-model_train-and-model_eval, I started the training Docker container using GPU 0:. ```. time docker run --rm --gpus 1 \. -v ""${DATA_DIR}:${DATA_DIR}"" \. -v ""${OUTPUT_DIR}:${OUTPUT_DIR}"" \. -u $(id -u):$(id -g) \. google/deepvariant:1.4.0-gpu \. /opt/deepvariant/bin/model_train \. --dataset_config_pbtxt=""${OUTPUT_DIR}/training_set.dataset_config.pbtxt"" \. --train_dir=""${TRAINING_DIR}"" \. --model_name=""inception_v3"" \. --number_of_steps=50000 \. --save_interval_secs=300 \. --batch_size=32 \. --learning_rate=0.0005 \. --start_from_checkpoint=""gs://deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-wgs_standard/model.ckpt"". ```. Then I tried to start the `model_eval` evaluator in GPU mode with 1 GPU, but Docker was just giving it the *same* GPU as training was using, so it kept immediately running out of GPU memory. After about half an hour I hit on the right syntax to give it a distinct GPU, and managed to actually start `model_eval`:. ```. docker run --rm --gpus '""device=1""' \. -v ""${DATA_DIR}:${DATA_DIR}"" \. -v ""${OUTPUT_DIR}:${OUTPUT_DIR}"" \. -u $(id -u):$(id -g) \. google/deepvariant:1.4.0-gpu \. /opt/deepvariant/bin/model_eval \. --dataset_config_pbtxt=""${OUTPUT_DIR}/validation_set.dataset_config.pbtxt"" \. --checkpoint_dir=""${TRAINING_DIR}"" \. --batch_size=512 \. --min_eval_interval_s=1 \. --eval_timeout=1000. ```. It evaluated the most recent checkpoint saves as of when it started up, and it is evaluating new checkpoints as they are created. But it doesn't seem interested in going back and evaluating all the checkpoints, other than the initial-latest one, that were created before it started. The directory in question looks like this:. ```. (dv_venv) [anovak@phoenix-01 tra",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/611
https://github.com/google/deepvariant/issues/611:1608,energy efficiency,GPU,GPU,1608,"tallation method (Docker, built from source, etc.): Docker. - Type of data: case study data. **Steps to reproduce:**. Loosely following https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-training-case-study.md#start-model_train-and-model_eval, I started the training Docker container using GPU 0:. ```. time docker run --rm --gpus 1 \. -v ""${DATA_DIR}:${DATA_DIR}"" \. -v ""${OUTPUT_DIR}:${OUTPUT_DIR}"" \. -u $(id -u):$(id -g) \. google/deepvariant:1.4.0-gpu \. /opt/deepvariant/bin/model_train \. --dataset_config_pbtxt=""${OUTPUT_DIR}/training_set.dataset_config.pbtxt"" \. --train_dir=""${TRAINING_DIR}"" \. --model_name=""inception_v3"" \. --number_of_steps=50000 \. --save_interval_secs=300 \. --batch_size=32 \. --learning_rate=0.0005 \. --start_from_checkpoint=""gs://deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-wgs_standard/model.ckpt"". ```. Then I tried to start the `model_eval` evaluator in GPU mode with 1 GPU, but Docker was just giving it the *same* GPU as training was using, so it kept immediately running out of GPU memory. After about half an hour I hit on the right syntax to give it a distinct GPU, and managed to actually start `model_eval`:. ```. docker run --rm --gpus '""device=1""' \. -v ""${DATA_DIR}:${DATA_DIR}"" \. -v ""${OUTPUT_DIR}:${OUTPUT_DIR}"" \. -u $(id -u):$(id -g) \. google/deepvariant:1.4.0-gpu \. /opt/deepvariant/bin/model_eval \. --dataset_config_pbtxt=""${OUTPUT_DIR}/validation_set.dataset_config.pbtxt"" \. --checkpoint_dir=""${TRAINING_DIR}"" \. --batch_size=512 \. --min_eval_interval_s=1 \. --eval_timeout=1000. ```. It evaluated the most recent checkpoint saves as of when it started up, and it is evaluating new checkpoints as they are created. But it doesn't seem interested in going back and evaluating all the checkpoints, other than the initial-latest one, that were created before it started. The directory in question looks like this:. ```. (dv_venv) [anovak@phoenix-01 trash]$ ls output/models/*meta. output/models/mod",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/611
https://github.com/google/deepvariant/issues/611:1673,energy efficiency,GPU,GPU,1673,"e of data: case study data. **Steps to reproduce:**. Loosely following https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-training-case-study.md#start-model_train-and-model_eval, I started the training Docker container using GPU 0:. ```. time docker run --rm --gpus 1 \. -v ""${DATA_DIR}:${DATA_DIR}"" \. -v ""${OUTPUT_DIR}:${OUTPUT_DIR}"" \. -u $(id -u):$(id -g) \. google/deepvariant:1.4.0-gpu \. /opt/deepvariant/bin/model_train \. --dataset_config_pbtxt=""${OUTPUT_DIR}/training_set.dataset_config.pbtxt"" \. --train_dir=""${TRAINING_DIR}"" \. --model_name=""inception_v3"" \. --number_of_steps=50000 \. --save_interval_secs=300 \. --batch_size=32 \. --learning_rate=0.0005 \. --start_from_checkpoint=""gs://deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-wgs_standard/model.ckpt"". ```. Then I tried to start the `model_eval` evaluator in GPU mode with 1 GPU, but Docker was just giving it the *same* GPU as training was using, so it kept immediately running out of GPU memory. After about half an hour I hit on the right syntax to give it a distinct GPU, and managed to actually start `model_eval`:. ```. docker run --rm --gpus '""device=1""' \. -v ""${DATA_DIR}:${DATA_DIR}"" \. -v ""${OUTPUT_DIR}:${OUTPUT_DIR}"" \. -u $(id -u):$(id -g) \. google/deepvariant:1.4.0-gpu \. /opt/deepvariant/bin/model_eval \. --dataset_config_pbtxt=""${OUTPUT_DIR}/validation_set.dataset_config.pbtxt"" \. --checkpoint_dir=""${TRAINING_DIR}"" \. --batch_size=512 \. --min_eval_interval_s=1 \. --eval_timeout=1000. ```. It evaluated the most recent checkpoint saves as of when it started up, and it is evaluating new checkpoints as they are created. But it doesn't seem interested in going back and evaluating all the checkpoints, other than the initial-latest one, that were created before it started. The directory in question looks like this:. ```. (dv_venv) [anovak@phoenix-01 trash]$ ls output/models/*meta. output/models/model.ckpt-0.meta output/models/model.ckpt-19459.meta output/models/",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/611
https://github.com/google/deepvariant/issues/611:1758,energy efficiency,GPU,GPU,1758,".com/google/deepvariant/blob/r1.4/docs/deepvariant-training-case-study.md#start-model_train-and-model_eval, I started the training Docker container using GPU 0:. ```. time docker run --rm --gpus 1 \. -v ""${DATA_DIR}:${DATA_DIR}"" \. -v ""${OUTPUT_DIR}:${OUTPUT_DIR}"" \. -u $(id -u):$(id -g) \. google/deepvariant:1.4.0-gpu \. /opt/deepvariant/bin/model_train \. --dataset_config_pbtxt=""${OUTPUT_DIR}/training_set.dataset_config.pbtxt"" \. --train_dir=""${TRAINING_DIR}"" \. --model_name=""inception_v3"" \. --number_of_steps=50000 \. --save_interval_secs=300 \. --batch_size=32 \. --learning_rate=0.0005 \. --start_from_checkpoint=""gs://deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-wgs_standard/model.ckpt"". ```. Then I tried to start the `model_eval` evaluator in GPU mode with 1 GPU, but Docker was just giving it the *same* GPU as training was using, so it kept immediately running out of GPU memory. After about half an hour I hit on the right syntax to give it a distinct GPU, and managed to actually start `model_eval`:. ```. docker run --rm --gpus '""device=1""' \. -v ""${DATA_DIR}:${DATA_DIR}"" \. -v ""${OUTPUT_DIR}:${OUTPUT_DIR}"" \. -u $(id -u):$(id -g) \. google/deepvariant:1.4.0-gpu \. /opt/deepvariant/bin/model_eval \. --dataset_config_pbtxt=""${OUTPUT_DIR}/validation_set.dataset_config.pbtxt"" \. --checkpoint_dir=""${TRAINING_DIR}"" \. --batch_size=512 \. --min_eval_interval_s=1 \. --eval_timeout=1000. ```. It evaluated the most recent checkpoint saves as of when it started up, and it is evaluating new checkpoints as they are created. But it doesn't seem interested in going back and evaluating all the checkpoints, other than the initial-latest one, that were created before it started. The directory in question looks like this:. ```. (dv_venv) [anovak@phoenix-01 trash]$ ls output/models/*meta. output/models/model.ckpt-0.meta output/models/model.ckpt-19459.meta output/models/model.ckpt-31078.meta. output/models/model.ckpt-1000.meta output/models/model.ckpt-20",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/611
https://github.com/google/deepvariant/issues/611:1767,energy efficiency,manag,managed,1767,"/deepvariant/blob/r1.4/docs/deepvariant-training-case-study.md#start-model_train-and-model_eval, I started the training Docker container using GPU 0:. ```. time docker run --rm --gpus 1 \. -v ""${DATA_DIR}:${DATA_DIR}"" \. -v ""${OUTPUT_DIR}:${OUTPUT_DIR}"" \. -u $(id -u):$(id -g) \. google/deepvariant:1.4.0-gpu \. /opt/deepvariant/bin/model_train \. --dataset_config_pbtxt=""${OUTPUT_DIR}/training_set.dataset_config.pbtxt"" \. --train_dir=""${TRAINING_DIR}"" \. --model_name=""inception_v3"" \. --number_of_steps=50000 \. --save_interval_secs=300 \. --batch_size=32 \. --learning_rate=0.0005 \. --start_from_checkpoint=""gs://deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-wgs_standard/model.ckpt"". ```. Then I tried to start the `model_eval` evaluator in GPU mode with 1 GPU, but Docker was just giving it the *same* GPU as training was using, so it kept immediately running out of GPU memory. After about half an hour I hit on the right syntax to give it a distinct GPU, and managed to actually start `model_eval`:. ```. docker run --rm --gpus '""device=1""' \. -v ""${DATA_DIR}:${DATA_DIR}"" \. -v ""${OUTPUT_DIR}:${OUTPUT_DIR}"" \. -u $(id -u):$(id -g) \. google/deepvariant:1.4.0-gpu \. /opt/deepvariant/bin/model_eval \. --dataset_config_pbtxt=""${OUTPUT_DIR}/validation_set.dataset_config.pbtxt"" \. --checkpoint_dir=""${TRAINING_DIR}"" \. --batch_size=512 \. --min_eval_interval_s=1 \. --eval_timeout=1000. ```. It evaluated the most recent checkpoint saves as of when it started up, and it is evaluating new checkpoints as they are created. But it doesn't seem interested in going back and evaluating all the checkpoints, other than the initial-latest one, that were created before it started. The directory in question looks like this:. ```. (dv_venv) [anovak@phoenix-01 trash]$ ls output/models/*meta. output/models/model.ckpt-0.meta output/models/model.ckpt-19459.meta output/models/model.ckpt-31078.meta. output/models/model.ckpt-1000.meta output/models/model.ckpt-2000.meta out",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/611
https://github.com/google/deepvariant/issues/611:1831,energy efficiency,gpu,gpus,1831,"#start-model_train-and-model_eval, I started the training Docker container using GPU 0:. ```. time docker run --rm --gpus 1 \. -v ""${DATA_DIR}:${DATA_DIR}"" \. -v ""${OUTPUT_DIR}:${OUTPUT_DIR}"" \. -u $(id -u):$(id -g) \. google/deepvariant:1.4.0-gpu \. /opt/deepvariant/bin/model_train \. --dataset_config_pbtxt=""${OUTPUT_DIR}/training_set.dataset_config.pbtxt"" \. --train_dir=""${TRAINING_DIR}"" \. --model_name=""inception_v3"" \. --number_of_steps=50000 \. --save_interval_secs=300 \. --batch_size=32 \. --learning_rate=0.0005 \. --start_from_checkpoint=""gs://deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-wgs_standard/model.ckpt"". ```. Then I tried to start the `model_eval` evaluator in GPU mode with 1 GPU, but Docker was just giving it the *same* GPU as training was using, so it kept immediately running out of GPU memory. After about half an hour I hit on the right syntax to give it a distinct GPU, and managed to actually start `model_eval`:. ```. docker run --rm --gpus '""device=1""' \. -v ""${DATA_DIR}:${DATA_DIR}"" \. -v ""${OUTPUT_DIR}:${OUTPUT_DIR}"" \. -u $(id -u):$(id -g) \. google/deepvariant:1.4.0-gpu \. /opt/deepvariant/bin/model_eval \. --dataset_config_pbtxt=""${OUTPUT_DIR}/validation_set.dataset_config.pbtxt"" \. --checkpoint_dir=""${TRAINING_DIR}"" \. --batch_size=512 \. --min_eval_interval_s=1 \. --eval_timeout=1000. ```. It evaluated the most recent checkpoint saves as of when it started up, and it is evaluating new checkpoints as they are created. But it doesn't seem interested in going back and evaluating all the checkpoints, other than the initial-latest one, that were created before it started. The directory in question looks like this:. ```. (dv_venv) [anovak@phoenix-01 trash]$ ls output/models/*meta. output/models/model.ckpt-0.meta output/models/model.ckpt-19459.meta output/models/model.ckpt-31078.meta. output/models/model.ckpt-1000.meta output/models/model.ckpt-2000.meta output/models/model.ckpt-34008.meta. output/models/model.ckpt-106",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/611
https://github.com/google/deepvariant/issues/611:1969,energy efficiency,gpu,gpu,1969,"DIR}:${DATA_DIR}"" \. -v ""${OUTPUT_DIR}:${OUTPUT_DIR}"" \. -u $(id -u):$(id -g) \. google/deepvariant:1.4.0-gpu \. /opt/deepvariant/bin/model_train \. --dataset_config_pbtxt=""${OUTPUT_DIR}/training_set.dataset_config.pbtxt"" \. --train_dir=""${TRAINING_DIR}"" \. --model_name=""inception_v3"" \. --number_of_steps=50000 \. --save_interval_secs=300 \. --batch_size=32 \. --learning_rate=0.0005 \. --start_from_checkpoint=""gs://deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-wgs_standard/model.ckpt"". ```. Then I tried to start the `model_eval` evaluator in GPU mode with 1 GPU, but Docker was just giving it the *same* GPU as training was using, so it kept immediately running out of GPU memory. After about half an hour I hit on the right syntax to give it a distinct GPU, and managed to actually start `model_eval`:. ```. docker run --rm --gpus '""device=1""' \. -v ""${DATA_DIR}:${DATA_DIR}"" \. -v ""${OUTPUT_DIR}:${OUTPUT_DIR}"" \. -u $(id -u):$(id -g) \. google/deepvariant:1.4.0-gpu \. /opt/deepvariant/bin/model_eval \. --dataset_config_pbtxt=""${OUTPUT_DIR}/validation_set.dataset_config.pbtxt"" \. --checkpoint_dir=""${TRAINING_DIR}"" \. --batch_size=512 \. --min_eval_interval_s=1 \. --eval_timeout=1000. ```. It evaluated the most recent checkpoint saves as of when it started up, and it is evaluating new checkpoints as they are created. But it doesn't seem interested in going back and evaluating all the checkpoints, other than the initial-latest one, that were created before it started. The directory in question looks like this:. ```. (dv_venv) [anovak@phoenix-01 trash]$ ls output/models/*meta. output/models/model.ckpt-0.meta output/models/model.ckpt-19459.meta output/models/model.ckpt-31078.meta. output/models/model.ckpt-1000.meta output/models/model.ckpt-2000.meta output/models/model.ckpt-34008.meta. output/models/model.ckpt-10674.meta output/models/model.ckpt-22355.meta output/models/model.ckpt-4814.meta. output/models/model.ckpt-13613.meta output/models/model.ck",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/611
https://github.com/google/deepvariant/issues/611:2579,energy efficiency,model,models,2579,"just giving it the *same* GPU as training was using, so it kept immediately running out of GPU memory. After about half an hour I hit on the right syntax to give it a distinct GPU, and managed to actually start `model_eval`:. ```. docker run --rm --gpus '""device=1""' \. -v ""${DATA_DIR}:${DATA_DIR}"" \. -v ""${OUTPUT_DIR}:${OUTPUT_DIR}"" \. -u $(id -u):$(id -g) \. google/deepvariant:1.4.0-gpu \. /opt/deepvariant/bin/model_eval \. --dataset_config_pbtxt=""${OUTPUT_DIR}/validation_set.dataset_config.pbtxt"" \. --checkpoint_dir=""${TRAINING_DIR}"" \. --batch_size=512 \. --min_eval_interval_s=1 \. --eval_timeout=1000. ```. It evaluated the most recent checkpoint saves as of when it started up, and it is evaluating new checkpoints as they are created. But it doesn't seem interested in going back and evaluating all the checkpoints, other than the initial-latest one, that were created before it started. The directory in question looks like this:. ```. (dv_venv) [anovak@phoenix-01 trash]$ ls output/models/*meta. output/models/model.ckpt-0.meta output/models/model.ckpt-19459.meta output/models/model.ckpt-31078.meta. output/models/model.ckpt-1000.meta output/models/model.ckpt-2000.meta output/models/model.ckpt-34008.meta. output/models/model.ckpt-10674.meta output/models/model.ckpt-22355.meta output/models/model.ckpt-4814.meta. output/models/model.ckpt-13613.meta output/models/model.ckpt-25257.meta output/models/model.ckpt-7724.meta. output/models/model.ckpt-16546.meta output/models/model.ckpt-28168.meta. (dv_venv) [anovak@phoenix-01 trash]$ ls output/models/*metrics. output/models/best_checkpoint.metrics output/models/model.ckpt-28168.metrics output/models/model.ckpt-34008.metrics. output/models/current.metrics output/models/model.ckpt-31078.metrics. ```. But `model_eval` just sits there like this (until a new checkpoint appears):. ```. I0210 17:42:06.700287 139846137329472 checkpoint_utils.py:140] Waiting for new checkpoint at /public/groups/cgl/graph-genomes/anovak/trash/output/mod",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/611
https://github.com/google/deepvariant/issues/611:2600,energy efficiency,model,models,2600,"ame* GPU as training was using, so it kept immediately running out of GPU memory. After about half an hour I hit on the right syntax to give it a distinct GPU, and managed to actually start `model_eval`:. ```. docker run --rm --gpus '""device=1""' \. -v ""${DATA_DIR}:${DATA_DIR}"" \. -v ""${OUTPUT_DIR}:${OUTPUT_DIR}"" \. -u $(id -u):$(id -g) \. google/deepvariant:1.4.0-gpu \. /opt/deepvariant/bin/model_eval \. --dataset_config_pbtxt=""${OUTPUT_DIR}/validation_set.dataset_config.pbtxt"" \. --checkpoint_dir=""${TRAINING_DIR}"" \. --batch_size=512 \. --min_eval_interval_s=1 \. --eval_timeout=1000. ```. It evaluated the most recent checkpoint saves as of when it started up, and it is evaluating new checkpoints as they are created. But it doesn't seem interested in going back and evaluating all the checkpoints, other than the initial-latest one, that were created before it started. The directory in question looks like this:. ```. (dv_venv) [anovak@phoenix-01 trash]$ ls output/models/*meta. output/models/model.ckpt-0.meta output/models/model.ckpt-19459.meta output/models/model.ckpt-31078.meta. output/models/model.ckpt-1000.meta output/models/model.ckpt-2000.meta output/models/model.ckpt-34008.meta. output/models/model.ckpt-10674.meta output/models/model.ckpt-22355.meta output/models/model.ckpt-4814.meta. output/models/model.ckpt-13613.meta output/models/model.ckpt-25257.meta output/models/model.ckpt-7724.meta. output/models/model.ckpt-16546.meta output/models/model.ckpt-28168.meta. (dv_venv) [anovak@phoenix-01 trash]$ ls output/models/*metrics. output/models/best_checkpoint.metrics output/models/model.ckpt-28168.metrics output/models/model.ckpt-34008.metrics. output/models/current.metrics output/models/model.ckpt-31078.metrics. ```. But `model_eval` just sits there like this (until a new checkpoint appears):. ```. I0210 17:42:06.700287 139846137329472 checkpoint_utils.py:140] Waiting for new checkpoint at /public/groups/cgl/graph-genomes/anovak/trash/output/models. ```. How do I ge",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/611
https://github.com/google/deepvariant/issues/611:2607,energy efficiency,model,model,2607,"U as training was using, so it kept immediately running out of GPU memory. After about half an hour I hit on the right syntax to give it a distinct GPU, and managed to actually start `model_eval`:. ```. docker run --rm --gpus '""device=1""' \. -v ""${DATA_DIR}:${DATA_DIR}"" \. -v ""${OUTPUT_DIR}:${OUTPUT_DIR}"" \. -u $(id -u):$(id -g) \. google/deepvariant:1.4.0-gpu \. /opt/deepvariant/bin/model_eval \. --dataset_config_pbtxt=""${OUTPUT_DIR}/validation_set.dataset_config.pbtxt"" \. --checkpoint_dir=""${TRAINING_DIR}"" \. --batch_size=512 \. --min_eval_interval_s=1 \. --eval_timeout=1000. ```. It evaluated the most recent checkpoint saves as of when it started up, and it is evaluating new checkpoints as they are created. But it doesn't seem interested in going back and evaluating all the checkpoints, other than the initial-latest one, that were created before it started. The directory in question looks like this:. ```. (dv_venv) [anovak@phoenix-01 trash]$ ls output/models/*meta. output/models/model.ckpt-0.meta output/models/model.ckpt-19459.meta output/models/model.ckpt-31078.meta. output/models/model.ckpt-1000.meta output/models/model.ckpt-2000.meta output/models/model.ckpt-34008.meta. output/models/model.ckpt-10674.meta output/models/model.ckpt-22355.meta output/models/model.ckpt-4814.meta. output/models/model.ckpt-13613.meta output/models/model.ckpt-25257.meta output/models/model.ckpt-7724.meta. output/models/model.ckpt-16546.meta output/models/model.ckpt-28168.meta. (dv_venv) [anovak@phoenix-01 trash]$ ls output/models/*metrics. output/models/best_checkpoint.metrics output/models/model.ckpt-28168.metrics output/models/model.ckpt-34008.metrics. output/models/current.metrics output/models/model.ckpt-31078.metrics. ```. But `model_eval` just sits there like this (until a new checkpoint appears):. ```. I0210 17:42:06.700287 139846137329472 checkpoint_utils.py:140] Waiting for new checkpoint at /public/groups/cgl/graph-genomes/anovak/trash/output/models. ```. How do I get the m",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/611
https://github.com/google/deepvariant/issues/611:2632,energy efficiency,model,models,2632,"so it kept immediately running out of GPU memory. After about half an hour I hit on the right syntax to give it a distinct GPU, and managed to actually start `model_eval`:. ```. docker run --rm --gpus '""device=1""' \. -v ""${DATA_DIR}:${DATA_DIR}"" \. -v ""${OUTPUT_DIR}:${OUTPUT_DIR}"" \. -u $(id -u):$(id -g) \. google/deepvariant:1.4.0-gpu \. /opt/deepvariant/bin/model_eval \. --dataset_config_pbtxt=""${OUTPUT_DIR}/validation_set.dataset_config.pbtxt"" \. --checkpoint_dir=""${TRAINING_DIR}"" \. --batch_size=512 \. --min_eval_interval_s=1 \. --eval_timeout=1000. ```. It evaluated the most recent checkpoint saves as of when it started up, and it is evaluating new checkpoints as they are created. But it doesn't seem interested in going back and evaluating all the checkpoints, other than the initial-latest one, that were created before it started. The directory in question looks like this:. ```. (dv_venv) [anovak@phoenix-01 trash]$ ls output/models/*meta. output/models/model.ckpt-0.meta output/models/model.ckpt-19459.meta output/models/model.ckpt-31078.meta. output/models/model.ckpt-1000.meta output/models/model.ckpt-2000.meta output/models/model.ckpt-34008.meta. output/models/model.ckpt-10674.meta output/models/model.ckpt-22355.meta output/models/model.ckpt-4814.meta. output/models/model.ckpt-13613.meta output/models/model.ckpt-25257.meta output/models/model.ckpt-7724.meta. output/models/model.ckpt-16546.meta output/models/model.ckpt-28168.meta. (dv_venv) [anovak@phoenix-01 trash]$ ls output/models/*metrics. output/models/best_checkpoint.metrics output/models/model.ckpt-28168.metrics output/models/model.ckpt-34008.metrics. output/models/current.metrics output/models/model.ckpt-31078.metrics. ```. But `model_eval` just sits there like this (until a new checkpoint appears):. ```. I0210 17:42:06.700287 139846137329472 checkpoint_utils.py:140] Waiting for new checkpoint at /public/groups/cgl/graph-genomes/anovak/trash/output/models. ```. How do I get the missing `*metrics` files a",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/611
https://github.com/google/deepvariant/issues/611:2639,energy efficiency,model,model,2639,"ept immediately running out of GPU memory. After about half an hour I hit on the right syntax to give it a distinct GPU, and managed to actually start `model_eval`:. ```. docker run --rm --gpus '""device=1""' \. -v ""${DATA_DIR}:${DATA_DIR}"" \. -v ""${OUTPUT_DIR}:${OUTPUT_DIR}"" \. -u $(id -u):$(id -g) \. google/deepvariant:1.4.0-gpu \. /opt/deepvariant/bin/model_eval \. --dataset_config_pbtxt=""${OUTPUT_DIR}/validation_set.dataset_config.pbtxt"" \. --checkpoint_dir=""${TRAINING_DIR}"" \. --batch_size=512 \. --min_eval_interval_s=1 \. --eval_timeout=1000. ```. It evaluated the most recent checkpoint saves as of when it started up, and it is evaluating new checkpoints as they are created. But it doesn't seem interested in going back and evaluating all the checkpoints, other than the initial-latest one, that were created before it started. The directory in question looks like this:. ```. (dv_venv) [anovak@phoenix-01 trash]$ ls output/models/*meta. output/models/model.ckpt-0.meta output/models/model.ckpt-19459.meta output/models/model.ckpt-31078.meta. output/models/model.ckpt-1000.meta output/models/model.ckpt-2000.meta output/models/model.ckpt-34008.meta. output/models/model.ckpt-10674.meta output/models/model.ckpt-22355.meta output/models/model.ckpt-4814.meta. output/models/model.ckpt-13613.meta output/models/model.ckpt-25257.meta output/models/model.ckpt-7724.meta. output/models/model.ckpt-16546.meta output/models/model.ckpt-28168.meta. (dv_venv) [anovak@phoenix-01 trash]$ ls output/models/*metrics. output/models/best_checkpoint.metrics output/models/model.ckpt-28168.metrics output/models/model.ckpt-34008.metrics. output/models/current.metrics output/models/model.ckpt-31078.metrics. ```. But `model_eval` just sits there like this (until a new checkpoint appears):. ```. I0210 17:42:06.700287 139846137329472 checkpoint_utils.py:140] Waiting for new checkpoint at /public/groups/cgl/graph-genomes/anovak/trash/output/models. ```. How do I get the missing `*metrics` files and dete",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/611
https://github.com/google/deepvariant/issues/611:2668,energy efficiency,model,models,2668,"f GPU memory. After about half an hour I hit on the right syntax to give it a distinct GPU, and managed to actually start `model_eval`:. ```. docker run --rm --gpus '""device=1""' \. -v ""${DATA_DIR}:${DATA_DIR}"" \. -v ""${OUTPUT_DIR}:${OUTPUT_DIR}"" \. -u $(id -u):$(id -g) \. google/deepvariant:1.4.0-gpu \. /opt/deepvariant/bin/model_eval \. --dataset_config_pbtxt=""${OUTPUT_DIR}/validation_set.dataset_config.pbtxt"" \. --checkpoint_dir=""${TRAINING_DIR}"" \. --batch_size=512 \. --min_eval_interval_s=1 \. --eval_timeout=1000. ```. It evaluated the most recent checkpoint saves as of when it started up, and it is evaluating new checkpoints as they are created. But it doesn't seem interested in going back and evaluating all the checkpoints, other than the initial-latest one, that were created before it started. The directory in question looks like this:. ```. (dv_venv) [anovak@phoenix-01 trash]$ ls output/models/*meta. output/models/model.ckpt-0.meta output/models/model.ckpt-19459.meta output/models/model.ckpt-31078.meta. output/models/model.ckpt-1000.meta output/models/model.ckpt-2000.meta output/models/model.ckpt-34008.meta. output/models/model.ckpt-10674.meta output/models/model.ckpt-22355.meta output/models/model.ckpt-4814.meta. output/models/model.ckpt-13613.meta output/models/model.ckpt-25257.meta output/models/model.ckpt-7724.meta. output/models/model.ckpt-16546.meta output/models/model.ckpt-28168.meta. (dv_venv) [anovak@phoenix-01 trash]$ ls output/models/*metrics. output/models/best_checkpoint.metrics output/models/model.ckpt-28168.metrics output/models/model.ckpt-34008.metrics. output/models/current.metrics output/models/model.ckpt-31078.metrics. ```. But `model_eval` just sits there like this (until a new checkpoint appears):. ```. I0210 17:42:06.700287 139846137329472 checkpoint_utils.py:140] Waiting for new checkpoint at /public/groups/cgl/graph-genomes/anovak/trash/output/models. ```. How do I get the missing `*metrics` files and determine if any of the checkpoin",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/611
https://github.com/google/deepvariant/issues/611:2675,energy efficiency,model,model,2675,"emory. After about half an hour I hit on the right syntax to give it a distinct GPU, and managed to actually start `model_eval`:. ```. docker run --rm --gpus '""device=1""' \. -v ""${DATA_DIR}:${DATA_DIR}"" \. -v ""${OUTPUT_DIR}:${OUTPUT_DIR}"" \. -u $(id -u):$(id -g) \. google/deepvariant:1.4.0-gpu \. /opt/deepvariant/bin/model_eval \. --dataset_config_pbtxt=""${OUTPUT_DIR}/validation_set.dataset_config.pbtxt"" \. --checkpoint_dir=""${TRAINING_DIR}"" \. --batch_size=512 \. --min_eval_interval_s=1 \. --eval_timeout=1000. ```. It evaluated the most recent checkpoint saves as of when it started up, and it is evaluating new checkpoints as they are created. But it doesn't seem interested in going back and evaluating all the checkpoints, other than the initial-latest one, that were created before it started. The directory in question looks like this:. ```. (dv_venv) [anovak@phoenix-01 trash]$ ls output/models/*meta. output/models/model.ckpt-0.meta output/models/model.ckpt-19459.meta output/models/model.ckpt-31078.meta. output/models/model.ckpt-1000.meta output/models/model.ckpt-2000.meta output/models/model.ckpt-34008.meta. output/models/model.ckpt-10674.meta output/models/model.ckpt-22355.meta output/models/model.ckpt-4814.meta. output/models/model.ckpt-13613.meta output/models/model.ckpt-25257.meta output/models/model.ckpt-7724.meta. output/models/model.ckpt-16546.meta output/models/model.ckpt-28168.meta. (dv_venv) [anovak@phoenix-01 trash]$ ls output/models/*metrics. output/models/best_checkpoint.metrics output/models/model.ckpt-28168.metrics output/models/model.ckpt-34008.metrics. output/models/current.metrics output/models/model.ckpt-31078.metrics. ```. But `model_eval` just sits there like this (until a new checkpoint appears):. ```. I0210 17:42:06.700287 139846137329472 checkpoint_utils.py:140] Waiting for new checkpoint at /public/groups/cgl/graph-genomes/anovak/trash/output/models. ```. How do I get the missing `*metrics` files and determine if any of the checkpoints that",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/611
https://github.com/google/deepvariant/issues/611:2705,energy efficiency,model,models,2705,"r I hit on the right syntax to give it a distinct GPU, and managed to actually start `model_eval`:. ```. docker run --rm --gpus '""device=1""' \. -v ""${DATA_DIR}:${DATA_DIR}"" \. -v ""${OUTPUT_DIR}:${OUTPUT_DIR}"" \. -u $(id -u):$(id -g) \. google/deepvariant:1.4.0-gpu \. /opt/deepvariant/bin/model_eval \. --dataset_config_pbtxt=""${OUTPUT_DIR}/validation_set.dataset_config.pbtxt"" \. --checkpoint_dir=""${TRAINING_DIR}"" \. --batch_size=512 \. --min_eval_interval_s=1 \. --eval_timeout=1000. ```. It evaluated the most recent checkpoint saves as of when it started up, and it is evaluating new checkpoints as they are created. But it doesn't seem interested in going back and evaluating all the checkpoints, other than the initial-latest one, that were created before it started. The directory in question looks like this:. ```. (dv_venv) [anovak@phoenix-01 trash]$ ls output/models/*meta. output/models/model.ckpt-0.meta output/models/model.ckpt-19459.meta output/models/model.ckpt-31078.meta. output/models/model.ckpt-1000.meta output/models/model.ckpt-2000.meta output/models/model.ckpt-34008.meta. output/models/model.ckpt-10674.meta output/models/model.ckpt-22355.meta output/models/model.ckpt-4814.meta. output/models/model.ckpt-13613.meta output/models/model.ckpt-25257.meta output/models/model.ckpt-7724.meta. output/models/model.ckpt-16546.meta output/models/model.ckpt-28168.meta. (dv_venv) [anovak@phoenix-01 trash]$ ls output/models/*metrics. output/models/best_checkpoint.metrics output/models/model.ckpt-28168.metrics output/models/model.ckpt-34008.metrics. output/models/current.metrics output/models/model.ckpt-31078.metrics. ```. But `model_eval` just sits there like this (until a new checkpoint appears):. ```. I0210 17:42:06.700287 139846137329472 checkpoint_utils.py:140] Waiting for new checkpoint at /public/groups/cgl/graph-genomes/anovak/trash/output/models. ```. How do I get the missing `*metrics` files and determine if any of the checkpoints that were missed is actually the b",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/611
https://github.com/google/deepvariant/issues/611:2712,energy efficiency,model,model,2712," on the right syntax to give it a distinct GPU, and managed to actually start `model_eval`:. ```. docker run --rm --gpus '""device=1""' \. -v ""${DATA_DIR}:${DATA_DIR}"" \. -v ""${OUTPUT_DIR}:${OUTPUT_DIR}"" \. -u $(id -u):$(id -g) \. google/deepvariant:1.4.0-gpu \. /opt/deepvariant/bin/model_eval \. --dataset_config_pbtxt=""${OUTPUT_DIR}/validation_set.dataset_config.pbtxt"" \. --checkpoint_dir=""${TRAINING_DIR}"" \. --batch_size=512 \. --min_eval_interval_s=1 \. --eval_timeout=1000. ```. It evaluated the most recent checkpoint saves as of when it started up, and it is evaluating new checkpoints as they are created. But it doesn't seem interested in going back and evaluating all the checkpoints, other than the initial-latest one, that were created before it started. The directory in question looks like this:. ```. (dv_venv) [anovak@phoenix-01 trash]$ ls output/models/*meta. output/models/model.ckpt-0.meta output/models/model.ckpt-19459.meta output/models/model.ckpt-31078.meta. output/models/model.ckpt-1000.meta output/models/model.ckpt-2000.meta output/models/model.ckpt-34008.meta. output/models/model.ckpt-10674.meta output/models/model.ckpt-22355.meta output/models/model.ckpt-4814.meta. output/models/model.ckpt-13613.meta output/models/model.ckpt-25257.meta output/models/model.ckpt-7724.meta. output/models/model.ckpt-16546.meta output/models/model.ckpt-28168.meta. (dv_venv) [anovak@phoenix-01 trash]$ ls output/models/*metrics. output/models/best_checkpoint.metrics output/models/model.ckpt-28168.metrics output/models/model.ckpt-34008.metrics. output/models/current.metrics output/models/model.ckpt-31078.metrics. ```. But `model_eval` just sits there like this (until a new checkpoint appears):. ```. I0210 17:42:06.700287 139846137329472 checkpoint_utils.py:140] Waiting for new checkpoint at /public/groups/cgl/graph-genomes/anovak/trash/output/models. ```. How do I get the missing `*metrics` files and determine if any of the checkpoints that were missed is actually the best one",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/611
https://github.com/google/deepvariant/issues/611:2740,energy efficiency,model,models,2740," it a distinct GPU, and managed to actually start `model_eval`:. ```. docker run --rm --gpus '""device=1""' \. -v ""${DATA_DIR}:${DATA_DIR}"" \. -v ""${OUTPUT_DIR}:${OUTPUT_DIR}"" \. -u $(id -u):$(id -g) \. google/deepvariant:1.4.0-gpu \. /opt/deepvariant/bin/model_eval \. --dataset_config_pbtxt=""${OUTPUT_DIR}/validation_set.dataset_config.pbtxt"" \. --checkpoint_dir=""${TRAINING_DIR}"" \. --batch_size=512 \. --min_eval_interval_s=1 \. --eval_timeout=1000. ```. It evaluated the most recent checkpoint saves as of when it started up, and it is evaluating new checkpoints as they are created. But it doesn't seem interested in going back and evaluating all the checkpoints, other than the initial-latest one, that were created before it started. The directory in question looks like this:. ```. (dv_venv) [anovak@phoenix-01 trash]$ ls output/models/*meta. output/models/model.ckpt-0.meta output/models/model.ckpt-19459.meta output/models/model.ckpt-31078.meta. output/models/model.ckpt-1000.meta output/models/model.ckpt-2000.meta output/models/model.ckpt-34008.meta. output/models/model.ckpt-10674.meta output/models/model.ckpt-22355.meta output/models/model.ckpt-4814.meta. output/models/model.ckpt-13613.meta output/models/model.ckpt-25257.meta output/models/model.ckpt-7724.meta. output/models/model.ckpt-16546.meta output/models/model.ckpt-28168.meta. (dv_venv) [anovak@phoenix-01 trash]$ ls output/models/*metrics. output/models/best_checkpoint.metrics output/models/model.ckpt-28168.metrics output/models/model.ckpt-34008.metrics. output/models/current.metrics output/models/model.ckpt-31078.metrics. ```. But `model_eval` just sits there like this (until a new checkpoint appears):. ```. I0210 17:42:06.700287 139846137329472 checkpoint_utils.py:140] Waiting for new checkpoint at /public/groups/cgl/graph-genomes/anovak/trash/output/models. ```. How do I get the missing `*metrics` files and determine if any of the checkpoints that were missed is actually the best one? Do I need to `touch` some ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/611
https://github.com/google/deepvariant/issues/611:2747,energy efficiency,model,model,2747,"istinct GPU, and managed to actually start `model_eval`:. ```. docker run --rm --gpus '""device=1""' \. -v ""${DATA_DIR}:${DATA_DIR}"" \. -v ""${OUTPUT_DIR}:${OUTPUT_DIR}"" \. -u $(id -u):$(id -g) \. google/deepvariant:1.4.0-gpu \. /opt/deepvariant/bin/model_eval \. --dataset_config_pbtxt=""${OUTPUT_DIR}/validation_set.dataset_config.pbtxt"" \. --checkpoint_dir=""${TRAINING_DIR}"" \. --batch_size=512 \. --min_eval_interval_s=1 \. --eval_timeout=1000. ```. It evaluated the most recent checkpoint saves as of when it started up, and it is evaluating new checkpoints as they are created. But it doesn't seem interested in going back and evaluating all the checkpoints, other than the initial-latest one, that were created before it started. The directory in question looks like this:. ```. (dv_venv) [anovak@phoenix-01 trash]$ ls output/models/*meta. output/models/model.ckpt-0.meta output/models/model.ckpt-19459.meta output/models/model.ckpt-31078.meta. output/models/model.ckpt-1000.meta output/models/model.ckpt-2000.meta output/models/model.ckpt-34008.meta. output/models/model.ckpt-10674.meta output/models/model.ckpt-22355.meta output/models/model.ckpt-4814.meta. output/models/model.ckpt-13613.meta output/models/model.ckpt-25257.meta output/models/model.ckpt-7724.meta. output/models/model.ckpt-16546.meta output/models/model.ckpt-28168.meta. (dv_venv) [anovak@phoenix-01 trash]$ ls output/models/*metrics. output/models/best_checkpoint.metrics output/models/model.ckpt-28168.metrics output/models/model.ckpt-34008.metrics. output/models/current.metrics output/models/model.ckpt-31078.metrics. ```. But `model_eval` just sits there like this (until a new checkpoint appears):. ```. I0210 17:42:06.700287 139846137329472 checkpoint_utils.py:140] Waiting for new checkpoint at /public/groups/cgl/graph-genomes/anovak/trash/output/models. ```. How do I get the missing `*metrics` files and determine if any of the checkpoints that were missed is actually the best one? Do I need to `touch` some particu",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/611
https://github.com/google/deepvariant/issues/611:2775,energy efficiency,model,models,2775,"actually start `model_eval`:. ```. docker run --rm --gpus '""device=1""' \. -v ""${DATA_DIR}:${DATA_DIR}"" \. -v ""${OUTPUT_DIR}:${OUTPUT_DIR}"" \. -u $(id -u):$(id -g) \. google/deepvariant:1.4.0-gpu \. /opt/deepvariant/bin/model_eval \. --dataset_config_pbtxt=""${OUTPUT_DIR}/validation_set.dataset_config.pbtxt"" \. --checkpoint_dir=""${TRAINING_DIR}"" \. --batch_size=512 \. --min_eval_interval_s=1 \. --eval_timeout=1000. ```. It evaluated the most recent checkpoint saves as of when it started up, and it is evaluating new checkpoints as they are created. But it doesn't seem interested in going back and evaluating all the checkpoints, other than the initial-latest one, that were created before it started. The directory in question looks like this:. ```. (dv_venv) [anovak@phoenix-01 trash]$ ls output/models/*meta. output/models/model.ckpt-0.meta output/models/model.ckpt-19459.meta output/models/model.ckpt-31078.meta. output/models/model.ckpt-1000.meta output/models/model.ckpt-2000.meta output/models/model.ckpt-34008.meta. output/models/model.ckpt-10674.meta output/models/model.ckpt-22355.meta output/models/model.ckpt-4814.meta. output/models/model.ckpt-13613.meta output/models/model.ckpt-25257.meta output/models/model.ckpt-7724.meta. output/models/model.ckpt-16546.meta output/models/model.ckpt-28168.meta. (dv_venv) [anovak@phoenix-01 trash]$ ls output/models/*metrics. output/models/best_checkpoint.metrics output/models/model.ckpt-28168.metrics output/models/model.ckpt-34008.metrics. output/models/current.metrics output/models/model.ckpt-31078.metrics. ```. But `model_eval` just sits there like this (until a new checkpoint appears):. ```. I0210 17:42:06.700287 139846137329472 checkpoint_utils.py:140] Waiting for new checkpoint at /public/groups/cgl/graph-genomes/anovak/trash/output/models. ```. How do I get the missing `*metrics` files and determine if any of the checkpoints that were missed is actually the best one? Do I need to `touch` some particular files in the directory t",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/611
https://github.com/google/deepvariant/issues/611:2782,energy efficiency,model,model,2782,"y start `model_eval`:. ```. docker run --rm --gpus '""device=1""' \. -v ""${DATA_DIR}:${DATA_DIR}"" \. -v ""${OUTPUT_DIR}:${OUTPUT_DIR}"" \. -u $(id -u):$(id -g) \. google/deepvariant:1.4.0-gpu \. /opt/deepvariant/bin/model_eval \. --dataset_config_pbtxt=""${OUTPUT_DIR}/validation_set.dataset_config.pbtxt"" \. --checkpoint_dir=""${TRAINING_DIR}"" \. --batch_size=512 \. --min_eval_interval_s=1 \. --eval_timeout=1000. ```. It evaluated the most recent checkpoint saves as of when it started up, and it is evaluating new checkpoints as they are created. But it doesn't seem interested in going back and evaluating all the checkpoints, other than the initial-latest one, that were created before it started. The directory in question looks like this:. ```. (dv_venv) [anovak@phoenix-01 trash]$ ls output/models/*meta. output/models/model.ckpt-0.meta output/models/model.ckpt-19459.meta output/models/model.ckpt-31078.meta. output/models/model.ckpt-1000.meta output/models/model.ckpt-2000.meta output/models/model.ckpt-34008.meta. output/models/model.ckpt-10674.meta output/models/model.ckpt-22355.meta output/models/model.ckpt-4814.meta. output/models/model.ckpt-13613.meta output/models/model.ckpt-25257.meta output/models/model.ckpt-7724.meta. output/models/model.ckpt-16546.meta output/models/model.ckpt-28168.meta. (dv_venv) [anovak@phoenix-01 trash]$ ls output/models/*metrics. output/models/best_checkpoint.metrics output/models/model.ckpt-28168.metrics output/models/model.ckpt-34008.metrics. output/models/current.metrics output/models/model.ckpt-31078.metrics. ```. But `model_eval` just sits there like this (until a new checkpoint appears):. ```. I0210 17:42:06.700287 139846137329472 checkpoint_utils.py:140] Waiting for new checkpoint at /public/groups/cgl/graph-genomes/anovak/trash/output/models. ```. How do I get the missing `*metrics` files and determine if any of the checkpoints that were missed is actually the best one? Do I need to `touch` some particular files in the directory to get `",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/611
https://github.com/google/deepvariant/issues/611:2812,energy efficiency,model,models,2812,"cker run --rm --gpus '""device=1""' \. -v ""${DATA_DIR}:${DATA_DIR}"" \. -v ""${OUTPUT_DIR}:${OUTPUT_DIR}"" \. -u $(id -u):$(id -g) \. google/deepvariant:1.4.0-gpu \. /opt/deepvariant/bin/model_eval \. --dataset_config_pbtxt=""${OUTPUT_DIR}/validation_set.dataset_config.pbtxt"" \. --checkpoint_dir=""${TRAINING_DIR}"" \. --batch_size=512 \. --min_eval_interval_s=1 \. --eval_timeout=1000. ```. It evaluated the most recent checkpoint saves as of when it started up, and it is evaluating new checkpoints as they are created. But it doesn't seem interested in going back and evaluating all the checkpoints, other than the initial-latest one, that were created before it started. The directory in question looks like this:. ```. (dv_venv) [anovak@phoenix-01 trash]$ ls output/models/*meta. output/models/model.ckpt-0.meta output/models/model.ckpt-19459.meta output/models/model.ckpt-31078.meta. output/models/model.ckpt-1000.meta output/models/model.ckpt-2000.meta output/models/model.ckpt-34008.meta. output/models/model.ckpt-10674.meta output/models/model.ckpt-22355.meta output/models/model.ckpt-4814.meta. output/models/model.ckpt-13613.meta output/models/model.ckpt-25257.meta output/models/model.ckpt-7724.meta. output/models/model.ckpt-16546.meta output/models/model.ckpt-28168.meta. (dv_venv) [anovak@phoenix-01 trash]$ ls output/models/*metrics. output/models/best_checkpoint.metrics output/models/model.ckpt-28168.metrics output/models/model.ckpt-34008.metrics. output/models/current.metrics output/models/model.ckpt-31078.metrics. ```. But `model_eval` just sits there like this (until a new checkpoint appears):. ```. I0210 17:42:06.700287 139846137329472 checkpoint_utils.py:140] Waiting for new checkpoint at /public/groups/cgl/graph-genomes/anovak/trash/output/models. ```. How do I get the missing `*metrics` files and determine if any of the checkpoints that were missed is actually the best one? Do I need to `touch` some particular files in the directory to get `model_eval` to be interested i",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/611
https://github.com/google/deepvariant/issues/611:2819,energy efficiency,model,model,2819,"n --rm --gpus '""device=1""' \. -v ""${DATA_DIR}:${DATA_DIR}"" \. -v ""${OUTPUT_DIR}:${OUTPUT_DIR}"" \. -u $(id -u):$(id -g) \. google/deepvariant:1.4.0-gpu \. /opt/deepvariant/bin/model_eval \. --dataset_config_pbtxt=""${OUTPUT_DIR}/validation_set.dataset_config.pbtxt"" \. --checkpoint_dir=""${TRAINING_DIR}"" \. --batch_size=512 \. --min_eval_interval_s=1 \. --eval_timeout=1000. ```. It evaluated the most recent checkpoint saves as of when it started up, and it is evaluating new checkpoints as they are created. But it doesn't seem interested in going back and evaluating all the checkpoints, other than the initial-latest one, that were created before it started. The directory in question looks like this:. ```. (dv_venv) [anovak@phoenix-01 trash]$ ls output/models/*meta. output/models/model.ckpt-0.meta output/models/model.ckpt-19459.meta output/models/model.ckpt-31078.meta. output/models/model.ckpt-1000.meta output/models/model.ckpt-2000.meta output/models/model.ckpt-34008.meta. output/models/model.ckpt-10674.meta output/models/model.ckpt-22355.meta output/models/model.ckpt-4814.meta. output/models/model.ckpt-13613.meta output/models/model.ckpt-25257.meta output/models/model.ckpt-7724.meta. output/models/model.ckpt-16546.meta output/models/model.ckpt-28168.meta. (dv_venv) [anovak@phoenix-01 trash]$ ls output/models/*metrics. output/models/best_checkpoint.metrics output/models/model.ckpt-28168.metrics output/models/model.ckpt-34008.metrics. output/models/current.metrics output/models/model.ckpt-31078.metrics. ```. But `model_eval` just sits there like this (until a new checkpoint appears):. ```. I0210 17:42:06.700287 139846137329472 checkpoint_utils.py:140] Waiting for new checkpoint at /public/groups/cgl/graph-genomes/anovak/trash/output/models. ```. How do I get the missing `*metrics` files and determine if any of the checkpoints that were missed is actually the best one? Do I need to `touch` some particular files in the directory to get `model_eval` to be interested in them?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/611
https://github.com/google/deepvariant/issues/611:2848,energy efficiency,model,models,2848," -v ""${DATA_DIR}:${DATA_DIR}"" \. -v ""${OUTPUT_DIR}:${OUTPUT_DIR}"" \. -u $(id -u):$(id -g) \. google/deepvariant:1.4.0-gpu \. /opt/deepvariant/bin/model_eval \. --dataset_config_pbtxt=""${OUTPUT_DIR}/validation_set.dataset_config.pbtxt"" \. --checkpoint_dir=""${TRAINING_DIR}"" \. --batch_size=512 \. --min_eval_interval_s=1 \. --eval_timeout=1000. ```. It evaluated the most recent checkpoint saves as of when it started up, and it is evaluating new checkpoints as they are created. But it doesn't seem interested in going back and evaluating all the checkpoints, other than the initial-latest one, that were created before it started. The directory in question looks like this:. ```. (dv_venv) [anovak@phoenix-01 trash]$ ls output/models/*meta. output/models/model.ckpt-0.meta output/models/model.ckpt-19459.meta output/models/model.ckpt-31078.meta. output/models/model.ckpt-1000.meta output/models/model.ckpt-2000.meta output/models/model.ckpt-34008.meta. output/models/model.ckpt-10674.meta output/models/model.ckpt-22355.meta output/models/model.ckpt-4814.meta. output/models/model.ckpt-13613.meta output/models/model.ckpt-25257.meta output/models/model.ckpt-7724.meta. output/models/model.ckpt-16546.meta output/models/model.ckpt-28168.meta. (dv_venv) [anovak@phoenix-01 trash]$ ls output/models/*metrics. output/models/best_checkpoint.metrics output/models/model.ckpt-28168.metrics output/models/model.ckpt-34008.metrics. output/models/current.metrics output/models/model.ckpt-31078.metrics. ```. But `model_eval` just sits there like this (until a new checkpoint appears):. ```. I0210 17:42:06.700287 139846137329472 checkpoint_utils.py:140] Waiting for new checkpoint at /public/groups/cgl/graph-genomes/anovak/trash/output/models. ```. How do I get the missing `*metrics` files and determine if any of the checkpoints that were missed is actually the best one? Do I need to `touch` some particular files in the directory to get `model_eval` to be interested in them? Is there some other command ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/611
https://github.com/google/deepvariant/issues/611:2855,energy efficiency,model,model,2855,"DATA_DIR}:${DATA_DIR}"" \. -v ""${OUTPUT_DIR}:${OUTPUT_DIR}"" \. -u $(id -u):$(id -g) \. google/deepvariant:1.4.0-gpu \. /opt/deepvariant/bin/model_eval \. --dataset_config_pbtxt=""${OUTPUT_DIR}/validation_set.dataset_config.pbtxt"" \. --checkpoint_dir=""${TRAINING_DIR}"" \. --batch_size=512 \. --min_eval_interval_s=1 \. --eval_timeout=1000. ```. It evaluated the most recent checkpoint saves as of when it started up, and it is evaluating new checkpoints as they are created. But it doesn't seem interested in going back and evaluating all the checkpoints, other than the initial-latest one, that were created before it started. The directory in question looks like this:. ```. (dv_venv) [anovak@phoenix-01 trash]$ ls output/models/*meta. output/models/model.ckpt-0.meta output/models/model.ckpt-19459.meta output/models/model.ckpt-31078.meta. output/models/model.ckpt-1000.meta output/models/model.ckpt-2000.meta output/models/model.ckpt-34008.meta. output/models/model.ckpt-10674.meta output/models/model.ckpt-22355.meta output/models/model.ckpt-4814.meta. output/models/model.ckpt-13613.meta output/models/model.ckpt-25257.meta output/models/model.ckpt-7724.meta. output/models/model.ckpt-16546.meta output/models/model.ckpt-28168.meta. (dv_venv) [anovak@phoenix-01 trash]$ ls output/models/*metrics. output/models/best_checkpoint.metrics output/models/model.ckpt-28168.metrics output/models/model.ckpt-34008.metrics. output/models/current.metrics output/models/model.ckpt-31078.metrics. ```. But `model_eval` just sits there like this (until a new checkpoint appears):. ```. I0210 17:42:06.700287 139846137329472 checkpoint_utils.py:140] Waiting for new checkpoint at /public/groups/cgl/graph-genomes/anovak/trash/output/models. ```. How do I get the missing `*metrics` files and determine if any of the checkpoints that were missed is actually the best one? Do I need to `touch` some particular files in the directory to get `model_eval` to be interested in them? Is there some other command besides",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/611
https://github.com/google/deepvariant/issues/611:2884,energy efficiency,model,models,2884,"""${OUTPUT_DIR}:${OUTPUT_DIR}"" \. -u $(id -u):$(id -g) \. google/deepvariant:1.4.0-gpu \. /opt/deepvariant/bin/model_eval \. --dataset_config_pbtxt=""${OUTPUT_DIR}/validation_set.dataset_config.pbtxt"" \. --checkpoint_dir=""${TRAINING_DIR}"" \. --batch_size=512 \. --min_eval_interval_s=1 \. --eval_timeout=1000. ```. It evaluated the most recent checkpoint saves as of when it started up, and it is evaluating new checkpoints as they are created. But it doesn't seem interested in going back and evaluating all the checkpoints, other than the initial-latest one, that were created before it started. The directory in question looks like this:. ```. (dv_venv) [anovak@phoenix-01 trash]$ ls output/models/*meta. output/models/model.ckpt-0.meta output/models/model.ckpt-19459.meta output/models/model.ckpt-31078.meta. output/models/model.ckpt-1000.meta output/models/model.ckpt-2000.meta output/models/model.ckpt-34008.meta. output/models/model.ckpt-10674.meta output/models/model.ckpt-22355.meta output/models/model.ckpt-4814.meta. output/models/model.ckpt-13613.meta output/models/model.ckpt-25257.meta output/models/model.ckpt-7724.meta. output/models/model.ckpt-16546.meta output/models/model.ckpt-28168.meta. (dv_venv) [anovak@phoenix-01 trash]$ ls output/models/*metrics. output/models/best_checkpoint.metrics output/models/model.ckpt-28168.metrics output/models/model.ckpt-34008.metrics. output/models/current.metrics output/models/model.ckpt-31078.metrics. ```. But `model_eval` just sits there like this (until a new checkpoint appears):. ```. I0210 17:42:06.700287 139846137329472 checkpoint_utils.py:140] Waiting for new checkpoint at /public/groups/cgl/graph-genomes/anovak/trash/output/models. ```. How do I get the missing `*metrics` files and determine if any of the checkpoints that were missed is actually the best one? Do I need to `touch` some particular files in the directory to get `model_eval` to be interested in them? Is there some other command besides `model_eval` that can proces",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/611
https://github.com/google/deepvariant/issues/611:2891,energy efficiency,model,model,2891,"UT_DIR}:${OUTPUT_DIR}"" \. -u $(id -u):$(id -g) \. google/deepvariant:1.4.0-gpu \. /opt/deepvariant/bin/model_eval \. --dataset_config_pbtxt=""${OUTPUT_DIR}/validation_set.dataset_config.pbtxt"" \. --checkpoint_dir=""${TRAINING_DIR}"" \. --batch_size=512 \. --min_eval_interval_s=1 \. --eval_timeout=1000. ```. It evaluated the most recent checkpoint saves as of when it started up, and it is evaluating new checkpoints as they are created. But it doesn't seem interested in going back and evaluating all the checkpoints, other than the initial-latest one, that were created before it started. The directory in question looks like this:. ```. (dv_venv) [anovak@phoenix-01 trash]$ ls output/models/*meta. output/models/model.ckpt-0.meta output/models/model.ckpt-19459.meta output/models/model.ckpt-31078.meta. output/models/model.ckpt-1000.meta output/models/model.ckpt-2000.meta output/models/model.ckpt-34008.meta. output/models/model.ckpt-10674.meta output/models/model.ckpt-22355.meta output/models/model.ckpt-4814.meta. output/models/model.ckpt-13613.meta output/models/model.ckpt-25257.meta output/models/model.ckpt-7724.meta. output/models/model.ckpt-16546.meta output/models/model.ckpt-28168.meta. (dv_venv) [anovak@phoenix-01 trash]$ ls output/models/*metrics. output/models/best_checkpoint.metrics output/models/model.ckpt-28168.metrics output/models/model.ckpt-34008.metrics. output/models/current.metrics output/models/model.ckpt-31078.metrics. ```. But `model_eval` just sits there like this (until a new checkpoint appears):. ```. I0210 17:42:06.700287 139846137329472 checkpoint_utils.py:140] Waiting for new checkpoint at /public/groups/cgl/graph-genomes/anovak/trash/output/models. ```. How do I get the missing `*metrics` files and determine if any of the checkpoints that were missed is actually the best one? Do I need to `touch` some particular files in the directory to get `model_eval` to be interested in them? Is there some other command besides `model_eval` that can process a sin",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/611
https://github.com/google/deepvariant/issues/611:2920,energy efficiency,model,models,2920,"$(id -u):$(id -g) \. google/deepvariant:1.4.0-gpu \. /opt/deepvariant/bin/model_eval \. --dataset_config_pbtxt=""${OUTPUT_DIR}/validation_set.dataset_config.pbtxt"" \. --checkpoint_dir=""${TRAINING_DIR}"" \. --batch_size=512 \. --min_eval_interval_s=1 \. --eval_timeout=1000. ```. It evaluated the most recent checkpoint saves as of when it started up, and it is evaluating new checkpoints as they are created. But it doesn't seem interested in going back and evaluating all the checkpoints, other than the initial-latest one, that were created before it started. The directory in question looks like this:. ```. (dv_venv) [anovak@phoenix-01 trash]$ ls output/models/*meta. output/models/model.ckpt-0.meta output/models/model.ckpt-19459.meta output/models/model.ckpt-31078.meta. output/models/model.ckpt-1000.meta output/models/model.ckpt-2000.meta output/models/model.ckpt-34008.meta. output/models/model.ckpt-10674.meta output/models/model.ckpt-22355.meta output/models/model.ckpt-4814.meta. output/models/model.ckpt-13613.meta output/models/model.ckpt-25257.meta output/models/model.ckpt-7724.meta. output/models/model.ckpt-16546.meta output/models/model.ckpt-28168.meta. (dv_venv) [anovak@phoenix-01 trash]$ ls output/models/*metrics. output/models/best_checkpoint.metrics output/models/model.ckpt-28168.metrics output/models/model.ckpt-34008.metrics. output/models/current.metrics output/models/model.ckpt-31078.metrics. ```. But `model_eval` just sits there like this (until a new checkpoint appears):. ```. I0210 17:42:06.700287 139846137329472 checkpoint_utils.py:140] Waiting for new checkpoint at /public/groups/cgl/graph-genomes/anovak/trash/output/models. ```. How do I get the missing `*metrics` files and determine if any of the checkpoints that were missed is actually the best one? Do I need to `touch` some particular files in the directory to get `model_eval` to be interested in them? Is there some other command besides `model_eval` that can process a single particular checkpoint at ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/611
https://github.com/google/deepvariant/issues/611:2927,energy efficiency,model,model,2927,"):$(id -g) \. google/deepvariant:1.4.0-gpu \. /opt/deepvariant/bin/model_eval \. --dataset_config_pbtxt=""${OUTPUT_DIR}/validation_set.dataset_config.pbtxt"" \. --checkpoint_dir=""${TRAINING_DIR}"" \. --batch_size=512 \. --min_eval_interval_s=1 \. --eval_timeout=1000. ```. It evaluated the most recent checkpoint saves as of when it started up, and it is evaluating new checkpoints as they are created. But it doesn't seem interested in going back and evaluating all the checkpoints, other than the initial-latest one, that were created before it started. The directory in question looks like this:. ```. (dv_venv) [anovak@phoenix-01 trash]$ ls output/models/*meta. output/models/model.ckpt-0.meta output/models/model.ckpt-19459.meta output/models/model.ckpt-31078.meta. output/models/model.ckpt-1000.meta output/models/model.ckpt-2000.meta output/models/model.ckpt-34008.meta. output/models/model.ckpt-10674.meta output/models/model.ckpt-22355.meta output/models/model.ckpt-4814.meta. output/models/model.ckpt-13613.meta output/models/model.ckpt-25257.meta output/models/model.ckpt-7724.meta. output/models/model.ckpt-16546.meta output/models/model.ckpt-28168.meta. (dv_venv) [anovak@phoenix-01 trash]$ ls output/models/*metrics. output/models/best_checkpoint.metrics output/models/model.ckpt-28168.metrics output/models/model.ckpt-34008.metrics. output/models/current.metrics output/models/model.ckpt-31078.metrics. ```. But `model_eval` just sits there like this (until a new checkpoint appears):. ```. I0210 17:42:06.700287 139846137329472 checkpoint_utils.py:140] Waiting for new checkpoint at /public/groups/cgl/graph-genomes/anovak/trash/output/models. ```. How do I get the missing `*metrics` files and determine if any of the checkpoints that were missed is actually the best one? Do I need to `touch` some particular files in the directory to get `model_eval` to be interested in them? Is there some other command besides `model_eval` that can process a single particular checkpoint at a time?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/611
https://github.com/google/deepvariant/issues/611:2956,energy efficiency,model,models,2956,"ant:1.4.0-gpu \. /opt/deepvariant/bin/model_eval \. --dataset_config_pbtxt=""${OUTPUT_DIR}/validation_set.dataset_config.pbtxt"" \. --checkpoint_dir=""${TRAINING_DIR}"" \. --batch_size=512 \. --min_eval_interval_s=1 \. --eval_timeout=1000. ```. It evaluated the most recent checkpoint saves as of when it started up, and it is evaluating new checkpoints as they are created. But it doesn't seem interested in going back and evaluating all the checkpoints, other than the initial-latest one, that were created before it started. The directory in question looks like this:. ```. (dv_venv) [anovak@phoenix-01 trash]$ ls output/models/*meta. output/models/model.ckpt-0.meta output/models/model.ckpt-19459.meta output/models/model.ckpt-31078.meta. output/models/model.ckpt-1000.meta output/models/model.ckpt-2000.meta output/models/model.ckpt-34008.meta. output/models/model.ckpt-10674.meta output/models/model.ckpt-22355.meta output/models/model.ckpt-4814.meta. output/models/model.ckpt-13613.meta output/models/model.ckpt-25257.meta output/models/model.ckpt-7724.meta. output/models/model.ckpt-16546.meta output/models/model.ckpt-28168.meta. (dv_venv) [anovak@phoenix-01 trash]$ ls output/models/*metrics. output/models/best_checkpoint.metrics output/models/model.ckpt-28168.metrics output/models/model.ckpt-34008.metrics. output/models/current.metrics output/models/model.ckpt-31078.metrics. ```. But `model_eval` just sits there like this (until a new checkpoint appears):. ```. I0210 17:42:06.700287 139846137329472 checkpoint_utils.py:140] Waiting for new checkpoint at /public/groups/cgl/graph-genomes/anovak/trash/output/models. ```. How do I get the missing `*metrics` files and determine if any of the checkpoints that were missed is actually the best one? Do I need to `touch` some particular files in the directory to get `model_eval` to be interested in them? Is there some other command besides `model_eval` that can process a single particular checkpoint at a time? **Does the quick start test ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/611
https://github.com/google/deepvariant/issues/611:2963,energy efficiency,model,model,2963,".0-gpu \. /opt/deepvariant/bin/model_eval \. --dataset_config_pbtxt=""${OUTPUT_DIR}/validation_set.dataset_config.pbtxt"" \. --checkpoint_dir=""${TRAINING_DIR}"" \. --batch_size=512 \. --min_eval_interval_s=1 \. --eval_timeout=1000. ```. It evaluated the most recent checkpoint saves as of when it started up, and it is evaluating new checkpoints as they are created. But it doesn't seem interested in going back and evaluating all the checkpoints, other than the initial-latest one, that were created before it started. The directory in question looks like this:. ```. (dv_venv) [anovak@phoenix-01 trash]$ ls output/models/*meta. output/models/model.ckpt-0.meta output/models/model.ckpt-19459.meta output/models/model.ckpt-31078.meta. output/models/model.ckpt-1000.meta output/models/model.ckpt-2000.meta output/models/model.ckpt-34008.meta. output/models/model.ckpt-10674.meta output/models/model.ckpt-22355.meta output/models/model.ckpt-4814.meta. output/models/model.ckpt-13613.meta output/models/model.ckpt-25257.meta output/models/model.ckpt-7724.meta. output/models/model.ckpt-16546.meta output/models/model.ckpt-28168.meta. (dv_venv) [anovak@phoenix-01 trash]$ ls output/models/*metrics. output/models/best_checkpoint.metrics output/models/model.ckpt-28168.metrics output/models/model.ckpt-34008.metrics. output/models/current.metrics output/models/model.ckpt-31078.metrics. ```. But `model_eval` just sits there like this (until a new checkpoint appears):. ```. I0210 17:42:06.700287 139846137329472 checkpoint_utils.py:140] Waiting for new checkpoint at /public/groups/cgl/graph-genomes/anovak/trash/output/models. ```. How do I get the missing `*metrics` files and determine if any of the checkpoints that were missed is actually the best one? Do I need to `touch` some particular files in the directory to get `model_eval` to be interested in them? Is there some other command besides `model_eval` that can process a single particular checkpoint at a time? **Does the quick start test work on",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/611
https://github.com/google/deepvariant/issues/611:2992,energy efficiency,model,models,2992,"n/model_eval \. --dataset_config_pbtxt=""${OUTPUT_DIR}/validation_set.dataset_config.pbtxt"" \. --checkpoint_dir=""${TRAINING_DIR}"" \. --batch_size=512 \. --min_eval_interval_s=1 \. --eval_timeout=1000. ```. It evaluated the most recent checkpoint saves as of when it started up, and it is evaluating new checkpoints as they are created. But it doesn't seem interested in going back and evaluating all the checkpoints, other than the initial-latest one, that were created before it started. The directory in question looks like this:. ```. (dv_venv) [anovak@phoenix-01 trash]$ ls output/models/*meta. output/models/model.ckpt-0.meta output/models/model.ckpt-19459.meta output/models/model.ckpt-31078.meta. output/models/model.ckpt-1000.meta output/models/model.ckpt-2000.meta output/models/model.ckpt-34008.meta. output/models/model.ckpt-10674.meta output/models/model.ckpt-22355.meta output/models/model.ckpt-4814.meta. output/models/model.ckpt-13613.meta output/models/model.ckpt-25257.meta output/models/model.ckpt-7724.meta. output/models/model.ckpt-16546.meta output/models/model.ckpt-28168.meta. (dv_venv) [anovak@phoenix-01 trash]$ ls output/models/*metrics. output/models/best_checkpoint.metrics output/models/model.ckpt-28168.metrics output/models/model.ckpt-34008.metrics. output/models/current.metrics output/models/model.ckpt-31078.metrics. ```. But `model_eval` just sits there like this (until a new checkpoint appears):. ```. I0210 17:42:06.700287 139846137329472 checkpoint_utils.py:140] Waiting for new checkpoint at /public/groups/cgl/graph-genomes/anovak/trash/output/models. ```. How do I get the missing `*metrics` files and determine if any of the checkpoints that were missed is actually the best one? Do I need to `touch` some particular files in the directory to get `model_eval` to be interested in them? Is there some other command besides `model_eval` that can process a single particular checkpoint at a time? **Does the quick start test work on your system?**. Please test ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/611
https://github.com/google/deepvariant/issues/611:2999,energy efficiency,model,model,2999,"_eval \. --dataset_config_pbtxt=""${OUTPUT_DIR}/validation_set.dataset_config.pbtxt"" \. --checkpoint_dir=""${TRAINING_DIR}"" \. --batch_size=512 \. --min_eval_interval_s=1 \. --eval_timeout=1000. ```. It evaluated the most recent checkpoint saves as of when it started up, and it is evaluating new checkpoints as they are created. But it doesn't seem interested in going back and evaluating all the checkpoints, other than the initial-latest one, that were created before it started. The directory in question looks like this:. ```. (dv_venv) [anovak@phoenix-01 trash]$ ls output/models/*meta. output/models/model.ckpt-0.meta output/models/model.ckpt-19459.meta output/models/model.ckpt-31078.meta. output/models/model.ckpt-1000.meta output/models/model.ckpt-2000.meta output/models/model.ckpt-34008.meta. output/models/model.ckpt-10674.meta output/models/model.ckpt-22355.meta output/models/model.ckpt-4814.meta. output/models/model.ckpt-13613.meta output/models/model.ckpt-25257.meta output/models/model.ckpt-7724.meta. output/models/model.ckpt-16546.meta output/models/model.ckpt-28168.meta. (dv_venv) [anovak@phoenix-01 trash]$ ls output/models/*metrics. output/models/best_checkpoint.metrics output/models/model.ckpt-28168.metrics output/models/model.ckpt-34008.metrics. output/models/current.metrics output/models/model.ckpt-31078.metrics. ```. But `model_eval` just sits there like this (until a new checkpoint appears):. ```. I0210 17:42:06.700287 139846137329472 checkpoint_utils.py:140] Waiting for new checkpoint at /public/groups/cgl/graph-genomes/anovak/trash/output/models. ```. How do I get the missing `*metrics` files and determine if any of the checkpoints that were missed is actually the best one? Do I need to `touch` some particular files in the directory to get `model_eval` to be interested in them? Is there some other command besides `model_eval` that can process a single particular checkpoint at a time? **Does the quick start test work on your system?**. Please test with ht",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/611
https://github.com/google/deepvariant/issues/611:3028,energy efficiency,model,models,3028,"xt=""${OUTPUT_DIR}/validation_set.dataset_config.pbtxt"" \. --checkpoint_dir=""${TRAINING_DIR}"" \. --batch_size=512 \. --min_eval_interval_s=1 \. --eval_timeout=1000. ```. It evaluated the most recent checkpoint saves as of when it started up, and it is evaluating new checkpoints as they are created. But it doesn't seem interested in going back and evaluating all the checkpoints, other than the initial-latest one, that were created before it started. The directory in question looks like this:. ```. (dv_venv) [anovak@phoenix-01 trash]$ ls output/models/*meta. output/models/model.ckpt-0.meta output/models/model.ckpt-19459.meta output/models/model.ckpt-31078.meta. output/models/model.ckpt-1000.meta output/models/model.ckpt-2000.meta output/models/model.ckpt-34008.meta. output/models/model.ckpt-10674.meta output/models/model.ckpt-22355.meta output/models/model.ckpt-4814.meta. output/models/model.ckpt-13613.meta output/models/model.ckpt-25257.meta output/models/model.ckpt-7724.meta. output/models/model.ckpt-16546.meta output/models/model.ckpt-28168.meta. (dv_venv) [anovak@phoenix-01 trash]$ ls output/models/*metrics. output/models/best_checkpoint.metrics output/models/model.ckpt-28168.metrics output/models/model.ckpt-34008.metrics. output/models/current.metrics output/models/model.ckpt-31078.metrics. ```. But `model_eval` just sits there like this (until a new checkpoint appears):. ```. I0210 17:42:06.700287 139846137329472 checkpoint_utils.py:140] Waiting for new checkpoint at /public/groups/cgl/graph-genomes/anovak/trash/output/models. ```. How do I get the missing `*metrics` files and determine if any of the checkpoints that were missed is actually the best one? Do I need to `touch` some particular files in the directory to get `model_eval` to be interested in them? Is there some other command besides `model_eval` that can process a single particular checkpoint at a time? **Does the quick start test work on your system?**. Please test with https://github.com/google/deepv",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/611
https://github.com/google/deepvariant/issues/611:3035,energy efficiency,model,model,3035,"UTPUT_DIR}/validation_set.dataset_config.pbtxt"" \. --checkpoint_dir=""${TRAINING_DIR}"" \. --batch_size=512 \. --min_eval_interval_s=1 \. --eval_timeout=1000. ```. It evaluated the most recent checkpoint saves as of when it started up, and it is evaluating new checkpoints as they are created. But it doesn't seem interested in going back and evaluating all the checkpoints, other than the initial-latest one, that were created before it started. The directory in question looks like this:. ```. (dv_venv) [anovak@phoenix-01 trash]$ ls output/models/*meta. output/models/model.ckpt-0.meta output/models/model.ckpt-19459.meta output/models/model.ckpt-31078.meta. output/models/model.ckpt-1000.meta output/models/model.ckpt-2000.meta output/models/model.ckpt-34008.meta. output/models/model.ckpt-10674.meta output/models/model.ckpt-22355.meta output/models/model.ckpt-4814.meta. output/models/model.ckpt-13613.meta output/models/model.ckpt-25257.meta output/models/model.ckpt-7724.meta. output/models/model.ckpt-16546.meta output/models/model.ckpt-28168.meta. (dv_venv) [anovak@phoenix-01 trash]$ ls output/models/*metrics. output/models/best_checkpoint.metrics output/models/model.ckpt-28168.metrics output/models/model.ckpt-34008.metrics. output/models/current.metrics output/models/model.ckpt-31078.metrics. ```. But `model_eval` just sits there like this (until a new checkpoint appears):. ```. I0210 17:42:06.700287 139846137329472 checkpoint_utils.py:140] Waiting for new checkpoint at /public/groups/cgl/graph-genomes/anovak/trash/output/models. ```. How do I get the missing `*metrics` files and determine if any of the checkpoints that were missed is actually the best one? Do I need to `touch` some particular files in the directory to get `model_eval` to be interested in them? Is there some other command besides `model_eval` that can process a single particular checkpoint at a time? **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/611
https://github.com/google/deepvariant/issues/611:3064,energy efficiency,model,models,3064,"aset_config.pbtxt"" \. --checkpoint_dir=""${TRAINING_DIR}"" \. --batch_size=512 \. --min_eval_interval_s=1 \. --eval_timeout=1000. ```. It evaluated the most recent checkpoint saves as of when it started up, and it is evaluating new checkpoints as they are created. But it doesn't seem interested in going back and evaluating all the checkpoints, other than the initial-latest one, that were created before it started. The directory in question looks like this:. ```. (dv_venv) [anovak@phoenix-01 trash]$ ls output/models/*meta. output/models/model.ckpt-0.meta output/models/model.ckpt-19459.meta output/models/model.ckpt-31078.meta. output/models/model.ckpt-1000.meta output/models/model.ckpt-2000.meta output/models/model.ckpt-34008.meta. output/models/model.ckpt-10674.meta output/models/model.ckpt-22355.meta output/models/model.ckpt-4814.meta. output/models/model.ckpt-13613.meta output/models/model.ckpt-25257.meta output/models/model.ckpt-7724.meta. output/models/model.ckpt-16546.meta output/models/model.ckpt-28168.meta. (dv_venv) [anovak@phoenix-01 trash]$ ls output/models/*metrics. output/models/best_checkpoint.metrics output/models/model.ckpt-28168.metrics output/models/model.ckpt-34008.metrics. output/models/current.metrics output/models/model.ckpt-31078.metrics. ```. But `model_eval` just sits there like this (until a new checkpoint appears):. ```. I0210 17:42:06.700287 139846137329472 checkpoint_utils.py:140] Waiting for new checkpoint at /public/groups/cgl/graph-genomes/anovak/trash/output/models. ```. How do I get the missing `*metrics` files and determine if any of the checkpoints that were missed is actually the best one? Do I need to `touch` some particular files in the directory to get `model_eval` to be interested in them? Is there some other command besides `model_eval` that can process a single particular checkpoint at a time? **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-q",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/611
https://github.com/google/deepvariant/issues/611:3071,energy efficiency,model,model,3071,"nfig.pbtxt"" \. --checkpoint_dir=""${TRAINING_DIR}"" \. --batch_size=512 \. --min_eval_interval_s=1 \. --eval_timeout=1000. ```. It evaluated the most recent checkpoint saves as of when it started up, and it is evaluating new checkpoints as they are created. But it doesn't seem interested in going back and evaluating all the checkpoints, other than the initial-latest one, that were created before it started. The directory in question looks like this:. ```. (dv_venv) [anovak@phoenix-01 trash]$ ls output/models/*meta. output/models/model.ckpt-0.meta output/models/model.ckpt-19459.meta output/models/model.ckpt-31078.meta. output/models/model.ckpt-1000.meta output/models/model.ckpt-2000.meta output/models/model.ckpt-34008.meta. output/models/model.ckpt-10674.meta output/models/model.ckpt-22355.meta output/models/model.ckpt-4814.meta. output/models/model.ckpt-13613.meta output/models/model.ckpt-25257.meta output/models/model.ckpt-7724.meta. output/models/model.ckpt-16546.meta output/models/model.ckpt-28168.meta. (dv_venv) [anovak@phoenix-01 trash]$ ls output/models/*metrics. output/models/best_checkpoint.metrics output/models/model.ckpt-28168.metrics output/models/model.ckpt-34008.metrics. output/models/current.metrics output/models/model.ckpt-31078.metrics. ```. But `model_eval` just sits there like this (until a new checkpoint appears):. ```. I0210 17:42:06.700287 139846137329472 checkpoint_utils.py:140] Waiting for new checkpoint at /public/groups/cgl/graph-genomes/anovak/trash/output/models. ```. How do I get the missing `*metrics` files and determine if any of the checkpoints that were missed is actually the best one? Do I need to `touch` some particular files in the directory to get `model_eval` to be interested in them? Is there some other command besides `model_eval` that can process a single particular checkpoint at a time? **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-st",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/611
https://github.com/google/deepvariant/issues/611:3141,energy efficiency,model,models,3141,"\. --min_eval_interval_s=1 \. --eval_timeout=1000. ```. It evaluated the most recent checkpoint saves as of when it started up, and it is evaluating new checkpoints as they are created. But it doesn't seem interested in going back and evaluating all the checkpoints, other than the initial-latest one, that were created before it started. The directory in question looks like this:. ```. (dv_venv) [anovak@phoenix-01 trash]$ ls output/models/*meta. output/models/model.ckpt-0.meta output/models/model.ckpt-19459.meta output/models/model.ckpt-31078.meta. output/models/model.ckpt-1000.meta output/models/model.ckpt-2000.meta output/models/model.ckpt-34008.meta. output/models/model.ckpt-10674.meta output/models/model.ckpt-22355.meta output/models/model.ckpt-4814.meta. output/models/model.ckpt-13613.meta output/models/model.ckpt-25257.meta output/models/model.ckpt-7724.meta. output/models/model.ckpt-16546.meta output/models/model.ckpt-28168.meta. (dv_venv) [anovak@phoenix-01 trash]$ ls output/models/*metrics. output/models/best_checkpoint.metrics output/models/model.ckpt-28168.metrics output/models/model.ckpt-34008.metrics. output/models/current.metrics output/models/model.ckpt-31078.metrics. ```. But `model_eval` just sits there like this (until a new checkpoint appears):. ```. I0210 17:42:06.700287 139846137329472 checkpoint_utils.py:140] Waiting for new checkpoint at /public/groups/cgl/graph-genomes/anovak/trash/output/models. ```. How do I get the missing `*metrics` files and determine if any of the checkpoints that were missed is actually the best one? Do I need to `touch` some particular files in the directory to get `model_eval` to be interested in them? Is there some other command besides `model_eval` that can process a single particular checkpoint at a time? **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick sta",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/611
https://github.com/google/deepvariant/issues/611:3165,energy efficiency,model,models,3165,"=1 \. --eval_timeout=1000. ```. It evaluated the most recent checkpoint saves as of when it started up, and it is evaluating new checkpoints as they are created. But it doesn't seem interested in going back and evaluating all the checkpoints, other than the initial-latest one, that were created before it started. The directory in question looks like this:. ```. (dv_venv) [anovak@phoenix-01 trash]$ ls output/models/*meta. output/models/model.ckpt-0.meta output/models/model.ckpt-19459.meta output/models/model.ckpt-31078.meta. output/models/model.ckpt-1000.meta output/models/model.ckpt-2000.meta output/models/model.ckpt-34008.meta. output/models/model.ckpt-10674.meta output/models/model.ckpt-22355.meta output/models/model.ckpt-4814.meta. output/models/model.ckpt-13613.meta output/models/model.ckpt-25257.meta output/models/model.ckpt-7724.meta. output/models/model.ckpt-16546.meta output/models/model.ckpt-28168.meta. (dv_venv) [anovak@phoenix-01 trash]$ ls output/models/*metrics. output/models/best_checkpoint.metrics output/models/model.ckpt-28168.metrics output/models/model.ckpt-34008.metrics. output/models/current.metrics output/models/model.ckpt-31078.metrics. ```. But `model_eval` just sits there like this (until a new checkpoint appears):. ```. I0210 17:42:06.700287 139846137329472 checkpoint_utils.py:140] Waiting for new checkpoint at /public/groups/cgl/graph-genomes/anovak/trash/output/models. ```. How do I get the missing `*metrics` files and determine if any of the checkpoints that were missed is actually the best one? Do I need to `touch` some particular files in the directory to get `model_eval` to be interested in them? Is there some other command besides `model_eval` that can process a single particular checkpoint at a time? **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? It doesn't look like",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/611
https://github.com/google/deepvariant/issues/611:3203,energy efficiency,model,models,3203,"luated the most recent checkpoint saves as of when it started up, and it is evaluating new checkpoints as they are created. But it doesn't seem interested in going back and evaluating all the checkpoints, other than the initial-latest one, that were created before it started. The directory in question looks like this:. ```. (dv_venv) [anovak@phoenix-01 trash]$ ls output/models/*meta. output/models/model.ckpt-0.meta output/models/model.ckpt-19459.meta output/models/model.ckpt-31078.meta. output/models/model.ckpt-1000.meta output/models/model.ckpt-2000.meta output/models/model.ckpt-34008.meta. output/models/model.ckpt-10674.meta output/models/model.ckpt-22355.meta output/models/model.ckpt-4814.meta. output/models/model.ckpt-13613.meta output/models/model.ckpt-25257.meta output/models/model.ckpt-7724.meta. output/models/model.ckpt-16546.meta output/models/model.ckpt-28168.meta. (dv_venv) [anovak@phoenix-01 trash]$ ls output/models/*metrics. output/models/best_checkpoint.metrics output/models/model.ckpt-28168.metrics output/models/model.ckpt-34008.metrics. output/models/current.metrics output/models/model.ckpt-31078.metrics. ```. But `model_eval` just sits there like this (until a new checkpoint appears):. ```. I0210 17:42:06.700287 139846137329472 checkpoint_utils.py:140] Waiting for new checkpoint at /public/groups/cgl/graph-genomes/anovak/trash/output/models. ```. How do I get the missing `*metrics` files and determine if any of the checkpoints that were missed is actually the best one? Do I need to `touch` some particular files in the directory to get `model_eval` to be interested in them? Is there some other command besides `model_eval` that can process a single particular checkpoint at a time? **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? It doesn't look like model training is part of the quick s",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/611
https://github.com/google/deepvariant/issues/611:3210,energy efficiency,model,model,3210,"the most recent checkpoint saves as of when it started up, and it is evaluating new checkpoints as they are created. But it doesn't seem interested in going back and evaluating all the checkpoints, other than the initial-latest one, that were created before it started. The directory in question looks like this:. ```. (dv_venv) [anovak@phoenix-01 trash]$ ls output/models/*meta. output/models/model.ckpt-0.meta output/models/model.ckpt-19459.meta output/models/model.ckpt-31078.meta. output/models/model.ckpt-1000.meta output/models/model.ckpt-2000.meta output/models/model.ckpt-34008.meta. output/models/model.ckpt-10674.meta output/models/model.ckpt-22355.meta output/models/model.ckpt-4814.meta. output/models/model.ckpt-13613.meta output/models/model.ckpt-25257.meta output/models/model.ckpt-7724.meta. output/models/model.ckpt-16546.meta output/models/model.ckpt-28168.meta. (dv_venv) [anovak@phoenix-01 trash]$ ls output/models/*metrics. output/models/best_checkpoint.metrics output/models/model.ckpt-28168.metrics output/models/model.ckpt-34008.metrics. output/models/current.metrics output/models/model.ckpt-31078.metrics. ```. But `model_eval` just sits there like this (until a new checkpoint appears):. ```. I0210 17:42:06.700287 139846137329472 checkpoint_utils.py:140] Waiting for new checkpoint at /public/groups/cgl/graph-genomes/anovak/trash/output/models. ```. How do I get the missing `*metrics` files and determine if any of the checkpoints that were missed is actually the best one? Do I need to `touch` some particular files in the directory to get `model_eval` to be interested in them? Is there some other command besides `model_eval` that can process a single particular checkpoint at a time? **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? It doesn't look like model training is part of the quick start. *",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/611
https://github.com/google/deepvariant/issues/611:3242,energy efficiency,model,models,3242," as of when it started up, and it is evaluating new checkpoints as they are created. But it doesn't seem interested in going back and evaluating all the checkpoints, other than the initial-latest one, that were created before it started. The directory in question looks like this:. ```. (dv_venv) [anovak@phoenix-01 trash]$ ls output/models/*meta. output/models/model.ckpt-0.meta output/models/model.ckpt-19459.meta output/models/model.ckpt-31078.meta. output/models/model.ckpt-1000.meta output/models/model.ckpt-2000.meta output/models/model.ckpt-34008.meta. output/models/model.ckpt-10674.meta output/models/model.ckpt-22355.meta output/models/model.ckpt-4814.meta. output/models/model.ckpt-13613.meta output/models/model.ckpt-25257.meta output/models/model.ckpt-7724.meta. output/models/model.ckpt-16546.meta output/models/model.ckpt-28168.meta. (dv_venv) [anovak@phoenix-01 trash]$ ls output/models/*metrics. output/models/best_checkpoint.metrics output/models/model.ckpt-28168.metrics output/models/model.ckpt-34008.metrics. output/models/current.metrics output/models/model.ckpt-31078.metrics. ```. But `model_eval` just sits there like this (until a new checkpoint appears):. ```. I0210 17:42:06.700287 139846137329472 checkpoint_utils.py:140] Waiting for new checkpoint at /public/groups/cgl/graph-genomes/anovak/trash/output/models. ```. How do I get the missing `*metrics` files and determine if any of the checkpoints that were missed is actually the best one? Do I need to `touch` some particular files in the directory to get `model_eval` to be interested in them? Is there some other command besides `model_eval` that can process a single particular checkpoint at a time? **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? It doesn't look like model training is part of the quick start. **Any additional context:**. Even",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/611
https://github.com/google/deepvariant/issues/611:3249,energy efficiency,model,model,3249,"when it started up, and it is evaluating new checkpoints as they are created. But it doesn't seem interested in going back and evaluating all the checkpoints, other than the initial-latest one, that were created before it started. The directory in question looks like this:. ```. (dv_venv) [anovak@phoenix-01 trash]$ ls output/models/*meta. output/models/model.ckpt-0.meta output/models/model.ckpt-19459.meta output/models/model.ckpt-31078.meta. output/models/model.ckpt-1000.meta output/models/model.ckpt-2000.meta output/models/model.ckpt-34008.meta. output/models/model.ckpt-10674.meta output/models/model.ckpt-22355.meta output/models/model.ckpt-4814.meta. output/models/model.ckpt-13613.meta output/models/model.ckpt-25257.meta output/models/model.ckpt-7724.meta. output/models/model.ckpt-16546.meta output/models/model.ckpt-28168.meta. (dv_venv) [anovak@phoenix-01 trash]$ ls output/models/*metrics. output/models/best_checkpoint.metrics output/models/model.ckpt-28168.metrics output/models/model.ckpt-34008.metrics. output/models/current.metrics output/models/model.ckpt-31078.metrics. ```. But `model_eval` just sits there like this (until a new checkpoint appears):. ```. I0210 17:42:06.700287 139846137329472 checkpoint_utils.py:140] Waiting for new checkpoint at /public/groups/cgl/graph-genomes/anovak/trash/output/models. ```. How do I get the missing `*metrics` files and determine if any of the checkpoints that were missed is actually the best one? Do I need to `touch` some particular files in the directory to get `model_eval` to be interested in them? Is there some other command besides `model_eval` that can process a single particular checkpoint at a time? **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? It doesn't look like model training is part of the quick start. **Any additional context:**. Eventually ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/611
https://github.com/google/deepvariant/issues/611:3282,energy efficiency,model,models,3282,"luating new checkpoints as they are created. But it doesn't seem interested in going back and evaluating all the checkpoints, other than the initial-latest one, that were created before it started. The directory in question looks like this:. ```. (dv_venv) [anovak@phoenix-01 trash]$ ls output/models/*meta. output/models/model.ckpt-0.meta output/models/model.ckpt-19459.meta output/models/model.ckpt-31078.meta. output/models/model.ckpt-1000.meta output/models/model.ckpt-2000.meta output/models/model.ckpt-34008.meta. output/models/model.ckpt-10674.meta output/models/model.ckpt-22355.meta output/models/model.ckpt-4814.meta. output/models/model.ckpt-13613.meta output/models/model.ckpt-25257.meta output/models/model.ckpt-7724.meta. output/models/model.ckpt-16546.meta output/models/model.ckpt-28168.meta. (dv_venv) [anovak@phoenix-01 trash]$ ls output/models/*metrics. output/models/best_checkpoint.metrics output/models/model.ckpt-28168.metrics output/models/model.ckpt-34008.metrics. output/models/current.metrics output/models/model.ckpt-31078.metrics. ```. But `model_eval` just sits there like this (until a new checkpoint appears):. ```. I0210 17:42:06.700287 139846137329472 checkpoint_utils.py:140] Waiting for new checkpoint at /public/groups/cgl/graph-genomes/anovak/trash/output/models. ```. How do I get the missing `*metrics` files and determine if any of the checkpoints that were missed is actually the best one? Do I need to `touch` some particular files in the directory to get `model_eval` to be interested in them? Is there some other command besides `model_eval` that can process a single particular checkpoint at a time? **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? It doesn't look like model training is part of the quick start. **Any additional context:**. Eventually I might want a WDL workflow for t",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/611
https://github.com/google/deepvariant/issues/611:3289,energy efficiency,current,current,3289,"new checkpoints as they are created. But it doesn't seem interested in going back and evaluating all the checkpoints, other than the initial-latest one, that were created before it started. The directory in question looks like this:. ```. (dv_venv) [anovak@phoenix-01 trash]$ ls output/models/*meta. output/models/model.ckpt-0.meta output/models/model.ckpt-19459.meta output/models/model.ckpt-31078.meta. output/models/model.ckpt-1000.meta output/models/model.ckpt-2000.meta output/models/model.ckpt-34008.meta. output/models/model.ckpt-10674.meta output/models/model.ckpt-22355.meta output/models/model.ckpt-4814.meta. output/models/model.ckpt-13613.meta output/models/model.ckpt-25257.meta output/models/model.ckpt-7724.meta. output/models/model.ckpt-16546.meta output/models/model.ckpt-28168.meta. (dv_venv) [anovak@phoenix-01 trash]$ ls output/models/*metrics. output/models/best_checkpoint.metrics output/models/model.ckpt-28168.metrics output/models/model.ckpt-34008.metrics. output/models/current.metrics output/models/model.ckpt-31078.metrics. ```. But `model_eval` just sits there like this (until a new checkpoint appears):. ```. I0210 17:42:06.700287 139846137329472 checkpoint_utils.py:140] Waiting for new checkpoint at /public/groups/cgl/graph-genomes/anovak/trash/output/models. ```. How do I get the missing `*metrics` files and determine if any of the checkpoints that were missed is actually the best one? Do I need to `touch` some particular files in the directory to get `model_eval` to be interested in them? Is there some other command besides `model_eval` that can process a single particular checkpoint at a time? **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? It doesn't look like model training is part of the quick start. **Any additional context:**. Eventually I might want a WDL workflow for training ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/611
https://github.com/google/deepvariant/issues/611:3312,energy efficiency,model,models,3312,"y are created. But it doesn't seem interested in going back and evaluating all the checkpoints, other than the initial-latest one, that were created before it started. The directory in question looks like this:. ```. (dv_venv) [anovak@phoenix-01 trash]$ ls output/models/*meta. output/models/model.ckpt-0.meta output/models/model.ckpt-19459.meta output/models/model.ckpt-31078.meta. output/models/model.ckpt-1000.meta output/models/model.ckpt-2000.meta output/models/model.ckpt-34008.meta. output/models/model.ckpt-10674.meta output/models/model.ckpt-22355.meta output/models/model.ckpt-4814.meta. output/models/model.ckpt-13613.meta output/models/model.ckpt-25257.meta output/models/model.ckpt-7724.meta. output/models/model.ckpt-16546.meta output/models/model.ckpt-28168.meta. (dv_venv) [anovak@phoenix-01 trash]$ ls output/models/*metrics. output/models/best_checkpoint.metrics output/models/model.ckpt-28168.metrics output/models/model.ckpt-34008.metrics. output/models/current.metrics output/models/model.ckpt-31078.metrics. ```. But `model_eval` just sits there like this (until a new checkpoint appears):. ```. I0210 17:42:06.700287 139846137329472 checkpoint_utils.py:140] Waiting for new checkpoint at /public/groups/cgl/graph-genomes/anovak/trash/output/models. ```. How do I get the missing `*metrics` files and determine if any of the checkpoints that were missed is actually the best one? Do I need to `touch` some particular files in the directory to get `model_eval` to be interested in them? Is there some other command besides `model_eval` that can process a single particular checkpoint at a time? **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? It doesn't look like model training is part of the quick start. **Any additional context:**. Eventually I might want a WDL workflow for training DeepVariant, and I'm n",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/611
https://github.com/google/deepvariant/issues/611:3319,energy efficiency,model,model,3319,"reated. But it doesn't seem interested in going back and evaluating all the checkpoints, other than the initial-latest one, that were created before it started. The directory in question looks like this:. ```. (dv_venv) [anovak@phoenix-01 trash]$ ls output/models/*meta. output/models/model.ckpt-0.meta output/models/model.ckpt-19459.meta output/models/model.ckpt-31078.meta. output/models/model.ckpt-1000.meta output/models/model.ckpt-2000.meta output/models/model.ckpt-34008.meta. output/models/model.ckpt-10674.meta output/models/model.ckpt-22355.meta output/models/model.ckpt-4814.meta. output/models/model.ckpt-13613.meta output/models/model.ckpt-25257.meta output/models/model.ckpt-7724.meta. output/models/model.ckpt-16546.meta output/models/model.ckpt-28168.meta. (dv_venv) [anovak@phoenix-01 trash]$ ls output/models/*metrics. output/models/best_checkpoint.metrics output/models/model.ckpt-28168.metrics output/models/model.ckpt-34008.metrics. output/models/current.metrics output/models/model.ckpt-31078.metrics. ```. But `model_eval` just sits there like this (until a new checkpoint appears):. ```. I0210 17:42:06.700287 139846137329472 checkpoint_utils.py:140] Waiting for new checkpoint at /public/groups/cgl/graph-genomes/anovak/trash/output/models. ```. How do I get the missing `*metrics` files and determine if any of the checkpoints that were missed is actually the best one? Do I need to `touch` some particular files in the directory to get `model_eval` to be interested in them? Is there some other command besides `model_eval` that can process a single particular checkpoint at a time? **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? It doesn't look like model training is part of the quick start. **Any additional context:**. Eventually I might want a WDL workflow for training DeepVariant, and I'm not sure",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/611
https://github.com/google/deepvariant/issues/611:3579,energy efficiency,model,models,3579,"els/*meta. output/models/model.ckpt-0.meta output/models/model.ckpt-19459.meta output/models/model.ckpt-31078.meta. output/models/model.ckpt-1000.meta output/models/model.ckpt-2000.meta output/models/model.ckpt-34008.meta. output/models/model.ckpt-10674.meta output/models/model.ckpt-22355.meta output/models/model.ckpt-4814.meta. output/models/model.ckpt-13613.meta output/models/model.ckpt-25257.meta output/models/model.ckpt-7724.meta. output/models/model.ckpt-16546.meta output/models/model.ckpt-28168.meta. (dv_venv) [anovak@phoenix-01 trash]$ ls output/models/*metrics. output/models/best_checkpoint.metrics output/models/model.ckpt-28168.metrics output/models/model.ckpt-34008.metrics. output/models/current.metrics output/models/model.ckpt-31078.metrics. ```. But `model_eval` just sits there like this (until a new checkpoint appears):. ```. I0210 17:42:06.700287 139846137329472 checkpoint_utils.py:140] Waiting for new checkpoint at /public/groups/cgl/graph-genomes/anovak/trash/output/models. ```. How do I get the missing `*metrics` files and determine if any of the checkpoints that were missed is actually the best one? Do I need to `touch` some particular files in the directory to get `model_eval` to be interested in them? Is there some other command besides `model_eval` that can process a single particular checkpoint at a time? **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? It doesn't look like model training is part of the quick start. **Any additional context:**. Eventually I might want a WDL workflow for training DeepVariant, and I'm not sure that managing two simultaneous DV processes in there is going to be worth the engineering required; they'd have to be lumped together into one WDL task and they'd have to always fit simultaneously on one machine. It would be much simpler for me to be able to ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/611
https://github.com/google/deepvariant/issues/611:4169,energy efficiency,model,model,4169,"0674.meta output/models/model.ckpt-22355.meta output/models/model.ckpt-4814.meta. output/models/model.ckpt-13613.meta output/models/model.ckpt-25257.meta output/models/model.ckpt-7724.meta. output/models/model.ckpt-16546.meta output/models/model.ckpt-28168.meta. (dv_venv) [anovak@phoenix-01 trash]$ ls output/models/*metrics. output/models/best_checkpoint.metrics output/models/model.ckpt-28168.metrics output/models/model.ckpt-34008.metrics. output/models/current.metrics output/models/model.ckpt-31078.metrics. ```. But `model_eval` just sits there like this (until a new checkpoint appears):. ```. I0210 17:42:06.700287 139846137329472 checkpoint_utils.py:140] Waiting for new checkpoint at /public/groups/cgl/graph-genomes/anovak/trash/output/models. ```. How do I get the missing `*metrics` files and determine if any of the checkpoints that were missed is actually the best one? Do I need to `touch` some particular files in the directory to get `model_eval` to be interested in them? Is there some other command besides `model_eval` that can process a single particular checkpoint at a time? **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? It doesn't look like model training is part of the quick start. **Any additional context:**. Eventually I might want a WDL workflow for training DeepVariant, and I'm not sure that managing two simultaneous DV processes in there is going to be worth the engineering required; they'd have to be lumped together into one WDL task and they'd have to always fit simultaneously on one machine. It would be much simpler for me to be able to run the training to the end, and then run all the evaluations afterward to select the best model. But it looks like if I tried that right now `model_eval` would just only evaluate the last checkpoint and always confidently declare it to be the best.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/611
https://github.com/google/deepvariant/issues/611:4328,energy efficiency,manag,managing,4328,"0674.meta output/models/model.ckpt-22355.meta output/models/model.ckpt-4814.meta. output/models/model.ckpt-13613.meta output/models/model.ckpt-25257.meta output/models/model.ckpt-7724.meta. output/models/model.ckpt-16546.meta output/models/model.ckpt-28168.meta. (dv_venv) [anovak@phoenix-01 trash]$ ls output/models/*metrics. output/models/best_checkpoint.metrics output/models/model.ckpt-28168.metrics output/models/model.ckpt-34008.metrics. output/models/current.metrics output/models/model.ckpt-31078.metrics. ```. But `model_eval` just sits there like this (until a new checkpoint appears):. ```. I0210 17:42:06.700287 139846137329472 checkpoint_utils.py:140] Waiting for new checkpoint at /public/groups/cgl/graph-genomes/anovak/trash/output/models. ```. How do I get the missing `*metrics` files and determine if any of the checkpoints that were missed is actually the best one? Do I need to `touch` some particular files in the directory to get `model_eval` to be interested in them? Is there some other command besides `model_eval` that can process a single particular checkpoint at a time? **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? It doesn't look like model training is part of the quick start. **Any additional context:**. Eventually I might want a WDL workflow for training DeepVariant, and I'm not sure that managing two simultaneous DV processes in there is going to be worth the engineering required; they'd have to be lumped together into one WDL task and they'd have to always fit simultaneously on one machine. It would be much simpler for me to be able to run the training to the end, and then run all the evaluations afterward to select the best model. But it looks like if I tried that right now `model_eval` would just only evaluate the last checkpoint and always confidently declare it to be the best.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/611
https://github.com/google/deepvariant/issues/611:4673,energy efficiency,model,model,4673,"0674.meta output/models/model.ckpt-22355.meta output/models/model.ckpt-4814.meta. output/models/model.ckpt-13613.meta output/models/model.ckpt-25257.meta output/models/model.ckpt-7724.meta. output/models/model.ckpt-16546.meta output/models/model.ckpt-28168.meta. (dv_venv) [anovak@phoenix-01 trash]$ ls output/models/*metrics. output/models/best_checkpoint.metrics output/models/model.ckpt-28168.metrics output/models/model.ckpt-34008.metrics. output/models/current.metrics output/models/model.ckpt-31078.metrics. ```. But `model_eval` just sits there like this (until a new checkpoint appears):. ```. I0210 17:42:06.700287 139846137329472 checkpoint_utils.py:140] Waiting for new checkpoint at /public/groups/cgl/graph-genomes/anovak/trash/output/models. ```. How do I get the missing `*metrics` files and determine if any of the checkpoints that were missed is actually the best one? Do I need to `touch` some particular files in the directory to get `model_eval` to be interested in them? Is there some other command besides `model_eval` that can process a single particular checkpoint at a time? **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? It doesn't look like model training is part of the quick start. **Any additional context:**. Eventually I might want a WDL workflow for training DeepVariant, and I'm not sure that managing two simultaneous DV processes in there is going to be worth the engineering required; they'd have to be lumped together into one WDL task and they'd have to always fit simultaneously on one machine. It would be much simpler for me to be able to run the training to the end, and then run all the evaluations afterward to select the best model. But it looks like if I tried that right now `model_eval` would just only evaluate the last checkpoint and always confidently declare it to be the best.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/611
https://github.com/google/deepvariant/issues/611:589,integrability,version,version,589,"`model_eval` ignores pre-existing, not-yet-evaluated checkpoints, and nothing else seems to exist to evaluate them; **Describe the issue:**. The `model_eval` command can't catch up and evaluate checkpoints in a directory other than the latest one. If it ever crashes or isn't started in time to actually witness training create a checkpoint, there's no way for the user to evaluate the checkpoints it missed, and the ""best"" checkpoint it finds won't actually necessarily be the best out of all the checkpoints in the directory. . **Setup**. - Operating system: Ubuntu 22.04. - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): Docker. - Type of data: case study data. **Steps to reproduce:**. Loosely following https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-training-case-study.md#start-model_train-and-model_eval, I started the training Docker container using GPU 0:. ```. time docker run --rm --gpus 1 \. -v ""${DATA_DIR}:${DATA_DIR}"" \. -v ""${OUTPUT_DIR}:${OUTPUT_DIR}"" \. -u $(id -u):$(id -g) \. google/deepvariant:1.4.0-gpu \. /opt/deepvariant/bin/model_train \. --dataset_config_pbtxt=""${OUTPUT_DIR}/training_set.dataset_config.pbtxt"" \. --train_dir=""${TRAINING_DIR}"" \. --model_name=""inception_v3"" \. --number_of_steps=50000 \. --save_interval_secs=300 \. --batch_size=32 \. --learning_rate=0.0005 \. --start_from_checkpoint=""gs://deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-wgs_standard/model.ckpt"". ```. Then I tried to start the `model_eval` evaluator in GPU mode with 1 GPU, but Docker was just giving it the *same* GPU as training was using, so it kept immediately running out of GPU memory. After about half an hour I hit on the right syntax to give it a distinct GPU, and managed to actually start `model_eval`:. ```. docker run --rm --gpus '""device=1""' \. -v ""${DATA_DIR}:${DATA_DIR}"" \. -v ""${OUTPUT_DIR}:${OUTPUT_DIR}"" \. -u $(id -u):$(id -g) \. google/deepvariant:1.4.0-gpu \. /opt/deepvariant/bin/mod",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/611
https://github.com/google/deepvariant/issues/611:3527,integrability,pub,public,3527,". (dv_venv) [anovak@phoenix-01 trash]$ ls output/models/*meta. output/models/model.ckpt-0.meta output/models/model.ckpt-19459.meta output/models/model.ckpt-31078.meta. output/models/model.ckpt-1000.meta output/models/model.ckpt-2000.meta output/models/model.ckpt-34008.meta. output/models/model.ckpt-10674.meta output/models/model.ckpt-22355.meta output/models/model.ckpt-4814.meta. output/models/model.ckpt-13613.meta output/models/model.ckpt-25257.meta output/models/model.ckpt-7724.meta. output/models/model.ckpt-16546.meta output/models/model.ckpt-28168.meta. (dv_venv) [anovak@phoenix-01 trash]$ ls output/models/*metrics. output/models/best_checkpoint.metrics output/models/model.ckpt-28168.metrics output/models/model.ckpt-34008.metrics. output/models/current.metrics output/models/model.ckpt-31078.metrics. ```. But `model_eval` just sits there like this (until a new checkpoint appears):. ```. I0210 17:42:06.700287 139846137329472 checkpoint_utils.py:140] Waiting for new checkpoint at /public/groups/cgl/graph-genomes/anovak/trash/output/models. ```. How do I get the missing `*metrics` files and determine if any of the checkpoints that were missed is actually the best one? Do I need to `touch` some particular files in the directory to get `model_eval` to be interested in them? Is there some other command besides `model_eval` that can process a single particular checkpoint at a time? **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? It doesn't look like model training is part of the quick start. **Any additional context:**. Eventually I might want a WDL workflow for training DeepVariant, and I'm not sure that managing two simultaneous DV processes in there is going to be worth the engineering required; they'd have to be lumped together into one WDL task and they'd have to always fit simultaneously on one mac",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/611
https://github.com/google/deepvariant/issues/611:4241,integrability,Event,Eventually,4241,"0674.meta output/models/model.ckpt-22355.meta output/models/model.ckpt-4814.meta. output/models/model.ckpt-13613.meta output/models/model.ckpt-25257.meta output/models/model.ckpt-7724.meta. output/models/model.ckpt-16546.meta output/models/model.ckpt-28168.meta. (dv_venv) [anovak@phoenix-01 trash]$ ls output/models/*metrics. output/models/best_checkpoint.metrics output/models/model.ckpt-28168.metrics output/models/model.ckpt-34008.metrics. output/models/current.metrics output/models/model.ckpt-31078.metrics. ```. But `model_eval` just sits there like this (until a new checkpoint appears):. ```. I0210 17:42:06.700287 139846137329472 checkpoint_utils.py:140] Waiting for new checkpoint at /public/groups/cgl/graph-genomes/anovak/trash/output/models. ```. How do I get the missing `*metrics` files and determine if any of the checkpoints that were missed is actually the best one? Do I need to `touch` some particular files in the directory to get `model_eval` to be interested in them? Is there some other command besides `model_eval` that can process a single particular checkpoint at a time? **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? It doesn't look like model training is part of the quick start. **Any additional context:**. Eventually I might want a WDL workflow for training DeepVariant, and I'm not sure that managing two simultaneous DV processes in there is going to be worth the engineering required; they'd have to be lumped together into one WDL task and they'd have to always fit simultaneously on one machine. It would be much simpler for me to be able to run the training to the end, and then run all the evaluations afterward to select the best model. But it looks like if I tried that right now `model_eval` would just only evaluate the last checkpoint and always confidently declare it to be the best.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/611
https://github.com/google/deepvariant/issues/611:589,modifiability,version,version,589,"`model_eval` ignores pre-existing, not-yet-evaluated checkpoints, and nothing else seems to exist to evaluate them; **Describe the issue:**. The `model_eval` command can't catch up and evaluate checkpoints in a directory other than the latest one. If it ever crashes or isn't started in time to actually witness training create a checkpoint, there's no way for the user to evaluate the checkpoints it missed, and the ""best"" checkpoint it finds won't actually necessarily be the best out of all the checkpoints in the directory. . **Setup**. - Operating system: Ubuntu 22.04. - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): Docker. - Type of data: case study data. **Steps to reproduce:**. Loosely following https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-training-case-study.md#start-model_train-and-model_eval, I started the training Docker container using GPU 0:. ```. time docker run --rm --gpus 1 \. -v ""${DATA_DIR}:${DATA_DIR}"" \. -v ""${OUTPUT_DIR}:${OUTPUT_DIR}"" \. -u $(id -u):$(id -g) \. google/deepvariant:1.4.0-gpu \. /opt/deepvariant/bin/model_train \. --dataset_config_pbtxt=""${OUTPUT_DIR}/training_set.dataset_config.pbtxt"" \. --train_dir=""${TRAINING_DIR}"" \. --model_name=""inception_v3"" \. --number_of_steps=50000 \. --save_interval_secs=300 \. --batch_size=32 \. --learning_rate=0.0005 \. --start_from_checkpoint=""gs://deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-wgs_standard/model.ckpt"". ```. Then I tried to start the `model_eval` evaluator in GPU mode with 1 GPU, but Docker was just giving it the *same* GPU as training was using, so it kept immediately running out of GPU memory. After about half an hour I hit on the right syntax to give it a distinct GPU, and managed to actually start `model_eval`:. ```. docker run --rm --gpus '""device=1""' \. -v ""${DATA_DIR}:${DATA_DIR}"" \. -v ""${OUTPUT_DIR}:${OUTPUT_DIR}"" \. -u $(id -u):$(id -g) \. google/deepvariant:1.4.0-gpu \. /opt/deepvariant/bin/mod",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/611
https://github.com/google/deepvariant/issues/611:287,performance,time,time,287,"`model_eval` ignores pre-existing, not-yet-evaluated checkpoints, and nothing else seems to exist to evaluate them; **Describe the issue:**. The `model_eval` command can't catch up and evaluate checkpoints in a directory other than the latest one. If it ever crashes or isn't started in time to actually witness training create a checkpoint, there's no way for the user to evaluate the checkpoints it missed, and the ""best"" checkpoint it finds won't actually necessarily be the best out of all the checkpoints in the directory. . **Setup**. - Operating system: Ubuntu 22.04. - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): Docker. - Type of data: case study data. **Steps to reproduce:**. Loosely following https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-training-case-study.md#start-model_train-and-model_eval, I started the training Docker container using GPU 0:. ```. time docker run --rm --gpus 1 \. -v ""${DATA_DIR}:${DATA_DIR}"" \. -v ""${OUTPUT_DIR}:${OUTPUT_DIR}"" \. -u $(id -u):$(id -g) \. google/deepvariant:1.4.0-gpu \. /opt/deepvariant/bin/model_train \. --dataset_config_pbtxt=""${OUTPUT_DIR}/training_set.dataset_config.pbtxt"" \. --train_dir=""${TRAINING_DIR}"" \. --model_name=""inception_v3"" \. --number_of_steps=50000 \. --save_interval_secs=300 \. --batch_size=32 \. --learning_rate=0.0005 \. --start_from_checkpoint=""gs://deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-wgs_standard/model.ckpt"". ```. Then I tried to start the `model_eval` evaluator in GPU mode with 1 GPU, but Docker was just giving it the *same* GPU as training was using, so it kept immediately running out of GPU memory. After about half an hour I hit on the right syntax to give it a distinct GPU, and managed to actually start `model_eval`:. ```. docker run --rm --gpus '""device=1""' \. -v ""${DATA_DIR}:${DATA_DIR}"" \. -v ""${OUTPUT_DIR}:${OUTPUT_DIR}"" \. -u $(id -u):$(id -g) \. google/deepvariant:1.4.0-gpu \. /opt/deepvariant/bin/mod",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/611
https://github.com/google/deepvariant/issues/611:914,performance,GPU,GPU,914,"`model_eval` ignores pre-existing, not-yet-evaluated checkpoints, and nothing else seems to exist to evaluate them; **Describe the issue:**. The `model_eval` command can't catch up and evaluate checkpoints in a directory other than the latest one. If it ever crashes or isn't started in time to actually witness training create a checkpoint, there's no way for the user to evaluate the checkpoints it missed, and the ""best"" checkpoint it finds won't actually necessarily be the best out of all the checkpoints in the directory. . **Setup**. - Operating system: Ubuntu 22.04. - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): Docker. - Type of data: case study data. **Steps to reproduce:**. Loosely following https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-training-case-study.md#start-model_train-and-model_eval, I started the training Docker container using GPU 0:. ```. time docker run --rm --gpus 1 \. -v ""${DATA_DIR}:${DATA_DIR}"" \. -v ""${OUTPUT_DIR}:${OUTPUT_DIR}"" \. -u $(id -u):$(id -g) \. google/deepvariant:1.4.0-gpu \. /opt/deepvariant/bin/model_train \. --dataset_config_pbtxt=""${OUTPUT_DIR}/training_set.dataset_config.pbtxt"" \. --train_dir=""${TRAINING_DIR}"" \. --model_name=""inception_v3"" \. --number_of_steps=50000 \. --save_interval_secs=300 \. --batch_size=32 \. --learning_rate=0.0005 \. --start_from_checkpoint=""gs://deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-wgs_standard/model.ckpt"". ```. Then I tried to start the `model_eval` evaluator in GPU mode with 1 GPU, but Docker was just giving it the *same* GPU as training was using, so it kept immediately running out of GPU memory. After about half an hour I hit on the right syntax to give it a distinct GPU, and managed to actually start `model_eval`:. ```. docker run --rm --gpus '""device=1""' \. -v ""${DATA_DIR}:${DATA_DIR}"" \. -v ""${OUTPUT_DIR}:${OUTPUT_DIR}"" \. -u $(id -u):$(id -g) \. google/deepvariant:1.4.0-gpu \. /opt/deepvariant/bin/mod",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/611
https://github.com/google/deepvariant/issues/611:927,performance,time,time,927,"`model_eval` ignores pre-existing, not-yet-evaluated checkpoints, and nothing else seems to exist to evaluate them; **Describe the issue:**. The `model_eval` command can't catch up and evaluate checkpoints in a directory other than the latest one. If it ever crashes or isn't started in time to actually witness training create a checkpoint, there's no way for the user to evaluate the checkpoints it missed, and the ""best"" checkpoint it finds won't actually necessarily be the best out of all the checkpoints in the directory. . **Setup**. - Operating system: Ubuntu 22.04. - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): Docker. - Type of data: case study data. **Steps to reproduce:**. Loosely following https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-training-case-study.md#start-model_train-and-model_eval, I started the training Docker container using GPU 0:. ```. time docker run --rm --gpus 1 \. -v ""${DATA_DIR}:${DATA_DIR}"" \. -v ""${OUTPUT_DIR}:${OUTPUT_DIR}"" \. -u $(id -u):$(id -g) \. google/deepvariant:1.4.0-gpu \. /opt/deepvariant/bin/model_train \. --dataset_config_pbtxt=""${OUTPUT_DIR}/training_set.dataset_config.pbtxt"" \. --train_dir=""${TRAINING_DIR}"" \. --model_name=""inception_v3"" \. --number_of_steps=50000 \. --save_interval_secs=300 \. --batch_size=32 \. --learning_rate=0.0005 \. --start_from_checkpoint=""gs://deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-wgs_standard/model.ckpt"". ```. Then I tried to start the `model_eval` evaluator in GPU mode with 1 GPU, but Docker was just giving it the *same* GPU as training was using, so it kept immediately running out of GPU memory. After about half an hour I hit on the right syntax to give it a distinct GPU, and managed to actually start `model_eval`:. ```. docker run --rm --gpus '""device=1""' \. -v ""${DATA_DIR}:${DATA_DIR}"" \. -v ""${OUTPUT_DIR}:${OUTPUT_DIR}"" \. -u $(id -u):$(id -g) \. google/deepvariant:1.4.0-gpu \. /opt/deepvariant/bin/mod",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/611
https://github.com/google/deepvariant/issues/611:950,performance,gpu,gpus,950,"`model_eval` ignores pre-existing, not-yet-evaluated checkpoints, and nothing else seems to exist to evaluate them; **Describe the issue:**. The `model_eval` command can't catch up and evaluate checkpoints in a directory other than the latest one. If it ever crashes or isn't started in time to actually witness training create a checkpoint, there's no way for the user to evaluate the checkpoints it missed, and the ""best"" checkpoint it finds won't actually necessarily be the best out of all the checkpoints in the directory. . **Setup**. - Operating system: Ubuntu 22.04. - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): Docker. - Type of data: case study data. **Steps to reproduce:**. Loosely following https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-training-case-study.md#start-model_train-and-model_eval, I started the training Docker container using GPU 0:. ```. time docker run --rm --gpus 1 \. -v ""${DATA_DIR}:${DATA_DIR}"" \. -v ""${OUTPUT_DIR}:${OUTPUT_DIR}"" \. -u $(id -u):$(id -g) \. google/deepvariant:1.4.0-gpu \. /opt/deepvariant/bin/model_train \. --dataset_config_pbtxt=""${OUTPUT_DIR}/training_set.dataset_config.pbtxt"" \. --train_dir=""${TRAINING_DIR}"" \. --model_name=""inception_v3"" \. --number_of_steps=50000 \. --save_interval_secs=300 \. --batch_size=32 \. --learning_rate=0.0005 \. --start_from_checkpoint=""gs://deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-wgs_standard/model.ckpt"". ```. Then I tried to start the `model_eval` evaluator in GPU mode with 1 GPU, but Docker was just giving it the *same* GPU as training was using, so it kept immediately running out of GPU memory. After about half an hour I hit on the right syntax to give it a distinct GPU, and managed to actually start `model_eval`:. ```. docker run --rm --gpus '""device=1""' \. -v ""${DATA_DIR}:${DATA_DIR}"" \. -v ""${OUTPUT_DIR}:${OUTPUT_DIR}"" \. -u $(id -u):$(id -g) \. google/deepvariant:1.4.0-gpu \. /opt/deepvariant/bin/mod",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/611
https://github.com/google/deepvariant/issues/611:1077,performance,gpu,gpu,1077,"lse seems to exist to evaluate them; **Describe the issue:**. The `model_eval` command can't catch up and evaluate checkpoints in a directory other than the latest one. If it ever crashes or isn't started in time to actually witness training create a checkpoint, there's no way for the user to evaluate the checkpoints it missed, and the ""best"" checkpoint it finds won't actually necessarily be the best out of all the checkpoints in the directory. . **Setup**. - Operating system: Ubuntu 22.04. - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): Docker. - Type of data: case study data. **Steps to reproduce:**. Loosely following https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-training-case-study.md#start-model_train-and-model_eval, I started the training Docker container using GPU 0:. ```. time docker run --rm --gpus 1 \. -v ""${DATA_DIR}:${DATA_DIR}"" \. -v ""${OUTPUT_DIR}:${OUTPUT_DIR}"" \. -u $(id -u):$(id -g) \. google/deepvariant:1.4.0-gpu \. /opt/deepvariant/bin/model_train \. --dataset_config_pbtxt=""${OUTPUT_DIR}/training_set.dataset_config.pbtxt"" \. --train_dir=""${TRAINING_DIR}"" \. --model_name=""inception_v3"" \. --number_of_steps=50000 \. --save_interval_secs=300 \. --batch_size=32 \. --learning_rate=0.0005 \. --start_from_checkpoint=""gs://deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-wgs_standard/model.ckpt"". ```. Then I tried to start the `model_eval` evaluator in GPU mode with 1 GPU, but Docker was just giving it the *same* GPU as training was using, so it kept immediately running out of GPU memory. After about half an hour I hit on the right syntax to give it a distinct GPU, and managed to actually start `model_eval`:. ```. docker run --rm --gpus '""device=1""' \. -v ""${DATA_DIR}:${DATA_DIR}"" \. -v ""${OUTPUT_DIR}:${OUTPUT_DIR}"" \. -u $(id -u):$(id -g) \. google/deepvariant:1.4.0-gpu \. /opt/deepvariant/bin/model_eval \. --dataset_config_pbtxt=""${OUTPUT_DIR}/validation_set.dataset_config.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/611
https://github.com/google/deepvariant/issues/611:1546,performance,GPU,GPU,1546,"ting system: Ubuntu 22.04. - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): Docker. - Type of data: case study data. **Steps to reproduce:**. Loosely following https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-training-case-study.md#start-model_train-and-model_eval, I started the training Docker container using GPU 0:. ```. time docker run --rm --gpus 1 \. -v ""${DATA_DIR}:${DATA_DIR}"" \. -v ""${OUTPUT_DIR}:${OUTPUT_DIR}"" \. -u $(id -u):$(id -g) \. google/deepvariant:1.4.0-gpu \. /opt/deepvariant/bin/model_train \. --dataset_config_pbtxt=""${OUTPUT_DIR}/training_set.dataset_config.pbtxt"" \. --train_dir=""${TRAINING_DIR}"" \. --model_name=""inception_v3"" \. --number_of_steps=50000 \. --save_interval_secs=300 \. --batch_size=32 \. --learning_rate=0.0005 \. --start_from_checkpoint=""gs://deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-wgs_standard/model.ckpt"". ```. Then I tried to start the `model_eval` evaluator in GPU mode with 1 GPU, but Docker was just giving it the *same* GPU as training was using, so it kept immediately running out of GPU memory. After about half an hour I hit on the right syntax to give it a distinct GPU, and managed to actually start `model_eval`:. ```. docker run --rm --gpus '""device=1""' \. -v ""${DATA_DIR}:${DATA_DIR}"" \. -v ""${OUTPUT_DIR}:${OUTPUT_DIR}"" \. -u $(id -u):$(id -g) \. google/deepvariant:1.4.0-gpu \. /opt/deepvariant/bin/model_eval \. --dataset_config_pbtxt=""${OUTPUT_DIR}/validation_set.dataset_config.pbtxt"" \. --checkpoint_dir=""${TRAINING_DIR}"" \. --batch_size=512 \. --min_eval_interval_s=1 \. --eval_timeout=1000. ```. It evaluated the most recent checkpoint saves as of when it started up, and it is evaluating new checkpoints as they are created. But it doesn't seem interested in going back and evaluating all the checkpoints, other than the initial-latest one, that were created before it started. The directory in question looks like this:. ```. (dv_venv) [anova",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/611
https://github.com/google/deepvariant/issues/611:1562,performance,GPU,GPU,1562,"ntu 22.04. - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): Docker. - Type of data: case study data. **Steps to reproduce:**. Loosely following https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-training-case-study.md#start-model_train-and-model_eval, I started the training Docker container using GPU 0:. ```. time docker run --rm --gpus 1 \. -v ""${DATA_DIR}:${DATA_DIR}"" \. -v ""${OUTPUT_DIR}:${OUTPUT_DIR}"" \. -u $(id -u):$(id -g) \. google/deepvariant:1.4.0-gpu \. /opt/deepvariant/bin/model_train \. --dataset_config_pbtxt=""${OUTPUT_DIR}/training_set.dataset_config.pbtxt"" \. --train_dir=""${TRAINING_DIR}"" \. --model_name=""inception_v3"" \. --number_of_steps=50000 \. --save_interval_secs=300 \. --batch_size=32 \. --learning_rate=0.0005 \. --start_from_checkpoint=""gs://deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-wgs_standard/model.ckpt"". ```. Then I tried to start the `model_eval` evaluator in GPU mode with 1 GPU, but Docker was just giving it the *same* GPU as training was using, so it kept immediately running out of GPU memory. After about half an hour I hit on the right syntax to give it a distinct GPU, and managed to actually start `model_eval`:. ```. docker run --rm --gpus '""device=1""' \. -v ""${DATA_DIR}:${DATA_DIR}"" \. -v ""${OUTPUT_DIR}:${OUTPUT_DIR}"" \. -u $(id -u):$(id -g) \. google/deepvariant:1.4.0-gpu \. /opt/deepvariant/bin/model_eval \. --dataset_config_pbtxt=""${OUTPUT_DIR}/validation_set.dataset_config.pbtxt"" \. --checkpoint_dir=""${TRAINING_DIR}"" \. --batch_size=512 \. --min_eval_interval_s=1 \. --eval_timeout=1000. ```. It evaluated the most recent checkpoint saves as of when it started up, and it is evaluating new checkpoints as they are created. But it doesn't seem interested in going back and evaluating all the checkpoints, other than the initial-latest one, that were created before it started. The directory in question looks like this:. ```. (dv_venv) [anovak@phoenix-01 tra",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/611
https://github.com/google/deepvariant/issues/611:1608,performance,GPU,GPU,1608,"tallation method (Docker, built from source, etc.): Docker. - Type of data: case study data. **Steps to reproduce:**. Loosely following https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-training-case-study.md#start-model_train-and-model_eval, I started the training Docker container using GPU 0:. ```. time docker run --rm --gpus 1 \. -v ""${DATA_DIR}:${DATA_DIR}"" \. -v ""${OUTPUT_DIR}:${OUTPUT_DIR}"" \. -u $(id -u):$(id -g) \. google/deepvariant:1.4.0-gpu \. /opt/deepvariant/bin/model_train \. --dataset_config_pbtxt=""${OUTPUT_DIR}/training_set.dataset_config.pbtxt"" \. --train_dir=""${TRAINING_DIR}"" \. --model_name=""inception_v3"" \. --number_of_steps=50000 \. --save_interval_secs=300 \. --batch_size=32 \. --learning_rate=0.0005 \. --start_from_checkpoint=""gs://deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-wgs_standard/model.ckpt"". ```. Then I tried to start the `model_eval` evaluator in GPU mode with 1 GPU, but Docker was just giving it the *same* GPU as training was using, so it kept immediately running out of GPU memory. After about half an hour I hit on the right syntax to give it a distinct GPU, and managed to actually start `model_eval`:. ```. docker run --rm --gpus '""device=1""' \. -v ""${DATA_DIR}:${DATA_DIR}"" \. -v ""${OUTPUT_DIR}:${OUTPUT_DIR}"" \. -u $(id -u):$(id -g) \. google/deepvariant:1.4.0-gpu \. /opt/deepvariant/bin/model_eval \. --dataset_config_pbtxt=""${OUTPUT_DIR}/validation_set.dataset_config.pbtxt"" \. --checkpoint_dir=""${TRAINING_DIR}"" \. --batch_size=512 \. --min_eval_interval_s=1 \. --eval_timeout=1000. ```. It evaluated the most recent checkpoint saves as of when it started up, and it is evaluating new checkpoints as they are created. But it doesn't seem interested in going back and evaluating all the checkpoints, other than the initial-latest one, that were created before it started. The directory in question looks like this:. ```. (dv_venv) [anovak@phoenix-01 trash]$ ls output/models/*meta. output/models/mod",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/611
https://github.com/google/deepvariant/issues/611:1673,performance,GPU,GPU,1673,"e of data: case study data. **Steps to reproduce:**. Loosely following https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-training-case-study.md#start-model_train-and-model_eval, I started the training Docker container using GPU 0:. ```. time docker run --rm --gpus 1 \. -v ""${DATA_DIR}:${DATA_DIR}"" \. -v ""${OUTPUT_DIR}:${OUTPUT_DIR}"" \. -u $(id -u):$(id -g) \. google/deepvariant:1.4.0-gpu \. /opt/deepvariant/bin/model_train \. --dataset_config_pbtxt=""${OUTPUT_DIR}/training_set.dataset_config.pbtxt"" \. --train_dir=""${TRAINING_DIR}"" \. --model_name=""inception_v3"" \. --number_of_steps=50000 \. --save_interval_secs=300 \. --batch_size=32 \. --learning_rate=0.0005 \. --start_from_checkpoint=""gs://deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-wgs_standard/model.ckpt"". ```. Then I tried to start the `model_eval` evaluator in GPU mode with 1 GPU, but Docker was just giving it the *same* GPU as training was using, so it kept immediately running out of GPU memory. After about half an hour I hit on the right syntax to give it a distinct GPU, and managed to actually start `model_eval`:. ```. docker run --rm --gpus '""device=1""' \. -v ""${DATA_DIR}:${DATA_DIR}"" \. -v ""${OUTPUT_DIR}:${OUTPUT_DIR}"" \. -u $(id -u):$(id -g) \. google/deepvariant:1.4.0-gpu \. /opt/deepvariant/bin/model_eval \. --dataset_config_pbtxt=""${OUTPUT_DIR}/validation_set.dataset_config.pbtxt"" \. --checkpoint_dir=""${TRAINING_DIR}"" \. --batch_size=512 \. --min_eval_interval_s=1 \. --eval_timeout=1000. ```. It evaluated the most recent checkpoint saves as of when it started up, and it is evaluating new checkpoints as they are created. But it doesn't seem interested in going back and evaluating all the checkpoints, other than the initial-latest one, that were created before it started. The directory in question looks like this:. ```. (dv_venv) [anovak@phoenix-01 trash]$ ls output/models/*meta. output/models/model.ckpt-0.meta output/models/model.ckpt-19459.meta output/models/",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/611
https://github.com/google/deepvariant/issues/611:1677,performance,memor,memory,1677,"data: case study data. **Steps to reproduce:**. Loosely following https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-training-case-study.md#start-model_train-and-model_eval, I started the training Docker container using GPU 0:. ```. time docker run --rm --gpus 1 \. -v ""${DATA_DIR}:${DATA_DIR}"" \. -v ""${OUTPUT_DIR}:${OUTPUT_DIR}"" \. -u $(id -u):$(id -g) \. google/deepvariant:1.4.0-gpu \. /opt/deepvariant/bin/model_train \. --dataset_config_pbtxt=""${OUTPUT_DIR}/training_set.dataset_config.pbtxt"" \. --train_dir=""${TRAINING_DIR}"" \. --model_name=""inception_v3"" \. --number_of_steps=50000 \. --save_interval_secs=300 \. --batch_size=32 \. --learning_rate=0.0005 \. --start_from_checkpoint=""gs://deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-wgs_standard/model.ckpt"". ```. Then I tried to start the `model_eval` evaluator in GPU mode with 1 GPU, but Docker was just giving it the *same* GPU as training was using, so it kept immediately running out of GPU memory. After about half an hour I hit on the right syntax to give it a distinct GPU, and managed to actually start `model_eval`:. ```. docker run --rm --gpus '""device=1""' \. -v ""${DATA_DIR}:${DATA_DIR}"" \. -v ""${OUTPUT_DIR}:${OUTPUT_DIR}"" \. -u $(id -u):$(id -g) \. google/deepvariant:1.4.0-gpu \. /opt/deepvariant/bin/model_eval \. --dataset_config_pbtxt=""${OUTPUT_DIR}/validation_set.dataset_config.pbtxt"" \. --checkpoint_dir=""${TRAINING_DIR}"" \. --batch_size=512 \. --min_eval_interval_s=1 \. --eval_timeout=1000. ```. It evaluated the most recent checkpoint saves as of when it started up, and it is evaluating new checkpoints as they are created. But it doesn't seem interested in going back and evaluating all the checkpoints, other than the initial-latest one, that were created before it started. The directory in question looks like this:. ```. (dv_venv) [anovak@phoenix-01 trash]$ ls output/models/*meta. output/models/model.ckpt-0.meta output/models/model.ckpt-19459.meta output/models/model",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/611
https://github.com/google/deepvariant/issues/611:1758,performance,GPU,GPU,1758,".com/google/deepvariant/blob/r1.4/docs/deepvariant-training-case-study.md#start-model_train-and-model_eval, I started the training Docker container using GPU 0:. ```. time docker run --rm --gpus 1 \. -v ""${DATA_DIR}:${DATA_DIR}"" \. -v ""${OUTPUT_DIR}:${OUTPUT_DIR}"" \. -u $(id -u):$(id -g) \. google/deepvariant:1.4.0-gpu \. /opt/deepvariant/bin/model_train \. --dataset_config_pbtxt=""${OUTPUT_DIR}/training_set.dataset_config.pbtxt"" \. --train_dir=""${TRAINING_DIR}"" \. --model_name=""inception_v3"" \. --number_of_steps=50000 \. --save_interval_secs=300 \. --batch_size=32 \. --learning_rate=0.0005 \. --start_from_checkpoint=""gs://deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-wgs_standard/model.ckpt"". ```. Then I tried to start the `model_eval` evaluator in GPU mode with 1 GPU, but Docker was just giving it the *same* GPU as training was using, so it kept immediately running out of GPU memory. After about half an hour I hit on the right syntax to give it a distinct GPU, and managed to actually start `model_eval`:. ```. docker run --rm --gpus '""device=1""' \. -v ""${DATA_DIR}:${DATA_DIR}"" \. -v ""${OUTPUT_DIR}:${OUTPUT_DIR}"" \. -u $(id -u):$(id -g) \. google/deepvariant:1.4.0-gpu \. /opt/deepvariant/bin/model_eval \. --dataset_config_pbtxt=""${OUTPUT_DIR}/validation_set.dataset_config.pbtxt"" \. --checkpoint_dir=""${TRAINING_DIR}"" \. --batch_size=512 \. --min_eval_interval_s=1 \. --eval_timeout=1000. ```. It evaluated the most recent checkpoint saves as of when it started up, and it is evaluating new checkpoints as they are created. But it doesn't seem interested in going back and evaluating all the checkpoints, other than the initial-latest one, that were created before it started. The directory in question looks like this:. ```. (dv_venv) [anovak@phoenix-01 trash]$ ls output/models/*meta. output/models/model.ckpt-0.meta output/models/model.ckpt-19459.meta output/models/model.ckpt-31078.meta. output/models/model.ckpt-1000.meta output/models/model.ckpt-20",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/611
https://github.com/google/deepvariant/issues/611:1831,performance,gpu,gpus,1831,"#start-model_train-and-model_eval, I started the training Docker container using GPU 0:. ```. time docker run --rm --gpus 1 \. -v ""${DATA_DIR}:${DATA_DIR}"" \. -v ""${OUTPUT_DIR}:${OUTPUT_DIR}"" \. -u $(id -u):$(id -g) \. google/deepvariant:1.4.0-gpu \. /opt/deepvariant/bin/model_train \. --dataset_config_pbtxt=""${OUTPUT_DIR}/training_set.dataset_config.pbtxt"" \. --train_dir=""${TRAINING_DIR}"" \. --model_name=""inception_v3"" \. --number_of_steps=50000 \. --save_interval_secs=300 \. --batch_size=32 \. --learning_rate=0.0005 \. --start_from_checkpoint=""gs://deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-wgs_standard/model.ckpt"". ```. Then I tried to start the `model_eval` evaluator in GPU mode with 1 GPU, but Docker was just giving it the *same* GPU as training was using, so it kept immediately running out of GPU memory. After about half an hour I hit on the right syntax to give it a distinct GPU, and managed to actually start `model_eval`:. ```. docker run --rm --gpus '""device=1""' \. -v ""${DATA_DIR}:${DATA_DIR}"" \. -v ""${OUTPUT_DIR}:${OUTPUT_DIR}"" \. -u $(id -u):$(id -g) \. google/deepvariant:1.4.0-gpu \. /opt/deepvariant/bin/model_eval \. --dataset_config_pbtxt=""${OUTPUT_DIR}/validation_set.dataset_config.pbtxt"" \. --checkpoint_dir=""${TRAINING_DIR}"" \. --batch_size=512 \. --min_eval_interval_s=1 \. --eval_timeout=1000. ```. It evaluated the most recent checkpoint saves as of when it started up, and it is evaluating new checkpoints as they are created. But it doesn't seem interested in going back and evaluating all the checkpoints, other than the initial-latest one, that were created before it started. The directory in question looks like this:. ```. (dv_venv) [anovak@phoenix-01 trash]$ ls output/models/*meta. output/models/model.ckpt-0.meta output/models/model.ckpt-19459.meta output/models/model.ckpt-31078.meta. output/models/model.ckpt-1000.meta output/models/model.ckpt-2000.meta output/models/model.ckpt-34008.meta. output/models/model.ckpt-106",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/611
https://github.com/google/deepvariant/issues/611:1969,performance,gpu,gpu,1969,"DIR}:${DATA_DIR}"" \. -v ""${OUTPUT_DIR}:${OUTPUT_DIR}"" \. -u $(id -u):$(id -g) \. google/deepvariant:1.4.0-gpu \. /opt/deepvariant/bin/model_train \. --dataset_config_pbtxt=""${OUTPUT_DIR}/training_set.dataset_config.pbtxt"" \. --train_dir=""${TRAINING_DIR}"" \. --model_name=""inception_v3"" \. --number_of_steps=50000 \. --save_interval_secs=300 \. --batch_size=32 \. --learning_rate=0.0005 \. --start_from_checkpoint=""gs://deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-wgs_standard/model.ckpt"". ```. Then I tried to start the `model_eval` evaluator in GPU mode with 1 GPU, but Docker was just giving it the *same* GPU as training was using, so it kept immediately running out of GPU memory. After about half an hour I hit on the right syntax to give it a distinct GPU, and managed to actually start `model_eval`:. ```. docker run --rm --gpus '""device=1""' \. -v ""${DATA_DIR}:${DATA_DIR}"" \. -v ""${OUTPUT_DIR}:${OUTPUT_DIR}"" \. -u $(id -u):$(id -g) \. google/deepvariant:1.4.0-gpu \. /opt/deepvariant/bin/model_eval \. --dataset_config_pbtxt=""${OUTPUT_DIR}/validation_set.dataset_config.pbtxt"" \. --checkpoint_dir=""${TRAINING_DIR}"" \. --batch_size=512 \. --min_eval_interval_s=1 \. --eval_timeout=1000. ```. It evaluated the most recent checkpoint saves as of when it started up, and it is evaluating new checkpoints as they are created. But it doesn't seem interested in going back and evaluating all the checkpoints, other than the initial-latest one, that were created before it started. The directory in question looks like this:. ```. (dv_venv) [anovak@phoenix-01 trash]$ ls output/models/*meta. output/models/model.ckpt-0.meta output/models/model.ckpt-19459.meta output/models/model.ckpt-31078.meta. output/models/model.ckpt-1000.meta output/models/model.ckpt-2000.meta output/models/model.ckpt-34008.meta. output/models/model.ckpt-10674.meta output/models/model.ckpt-22355.meta output/models/model.ckpt-4814.meta. output/models/model.ckpt-13613.meta output/models/model.ck",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/611
https://github.com/google/deepvariant/issues/611:3925,performance,time,time,3925,"0674.meta output/models/model.ckpt-22355.meta output/models/model.ckpt-4814.meta. output/models/model.ckpt-13613.meta output/models/model.ckpt-25257.meta output/models/model.ckpt-7724.meta. output/models/model.ckpt-16546.meta output/models/model.ckpt-28168.meta. (dv_venv) [anovak@phoenix-01 trash]$ ls output/models/*metrics. output/models/best_checkpoint.metrics output/models/model.ckpt-28168.metrics output/models/model.ckpt-34008.metrics. output/models/current.metrics output/models/model.ckpt-31078.metrics. ```. But `model_eval` just sits there like this (until a new checkpoint appears):. ```. I0210 17:42:06.700287 139846137329472 checkpoint_utils.py:140] Waiting for new checkpoint at /public/groups/cgl/graph-genomes/anovak/trash/output/models. ```. How do I get the missing `*metrics` files and determine if any of the checkpoints that were missed is actually the best one? Do I need to `touch` some particular files in the directory to get `model_eval` to be interested in them? Is there some other command besides `model_eval` that can process a single particular checkpoint at a time? **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? It doesn't look like model training is part of the quick start. **Any additional context:**. Eventually I might want a WDL workflow for training DeepVariant, and I'm not sure that managing two simultaneous DV processes in there is going to be worth the engineering required; they'd have to be lumped together into one WDL task and they'd have to always fit simultaneously on one machine. It would be much simpler for me to be able to run the training to the end, and then run all the evaluations afterward to select the best model. But it looks like if I tried that right now `model_eval` would just only evaluate the last checkpoint and always confidently declare it to be the best.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/611
https://github.com/google/deepvariant/issues/611:53,reliability,checkpoint,checkpoints,53,"`model_eval` ignores pre-existing, not-yet-evaluated checkpoints, and nothing else seems to exist to evaluate them; **Describe the issue:**. The `model_eval` command can't catch up and evaluate checkpoints in a directory other than the latest one. If it ever crashes or isn't started in time to actually witness training create a checkpoint, there's no way for the user to evaluate the checkpoints it missed, and the ""best"" checkpoint it finds won't actually necessarily be the best out of all the checkpoints in the directory. . **Setup**. - Operating system: Ubuntu 22.04. - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): Docker. - Type of data: case study data. **Steps to reproduce:**. Loosely following https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-training-case-study.md#start-model_train-and-model_eval, I started the training Docker container using GPU 0:. ```. time docker run --rm --gpus 1 \. -v ""${DATA_DIR}:${DATA_DIR}"" \. -v ""${OUTPUT_DIR}:${OUTPUT_DIR}"" \. -u $(id -u):$(id -g) \. google/deepvariant:1.4.0-gpu \. /opt/deepvariant/bin/model_train \. --dataset_config_pbtxt=""${OUTPUT_DIR}/training_set.dataset_config.pbtxt"" \. --train_dir=""${TRAINING_DIR}"" \. --model_name=""inception_v3"" \. --number_of_steps=50000 \. --save_interval_secs=300 \. --batch_size=32 \. --learning_rate=0.0005 \. --start_from_checkpoint=""gs://deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-wgs_standard/model.ckpt"". ```. Then I tried to start the `model_eval` evaluator in GPU mode with 1 GPU, but Docker was just giving it the *same* GPU as training was using, so it kept immediately running out of GPU memory. After about half an hour I hit on the right syntax to give it a distinct GPU, and managed to actually start `model_eval`:. ```. docker run --rm --gpus '""device=1""' \. -v ""${DATA_DIR}:${DATA_DIR}"" \. -v ""${OUTPUT_DIR}:${OUTPUT_DIR}"" \. -u $(id -u):$(id -g) \. google/deepvariant:1.4.0-gpu \. /opt/deepvariant/bin/mod",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/611
https://github.com/google/deepvariant/issues/611:194,reliability,checkpoint,checkpoints,194,"`model_eval` ignores pre-existing, not-yet-evaluated checkpoints, and nothing else seems to exist to evaluate them; **Describe the issue:**. The `model_eval` command can't catch up and evaluate checkpoints in a directory other than the latest one. If it ever crashes or isn't started in time to actually witness training create a checkpoint, there's no way for the user to evaluate the checkpoints it missed, and the ""best"" checkpoint it finds won't actually necessarily be the best out of all the checkpoints in the directory. . **Setup**. - Operating system: Ubuntu 22.04. - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): Docker. - Type of data: case study data. **Steps to reproduce:**. Loosely following https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-training-case-study.md#start-model_train-and-model_eval, I started the training Docker container using GPU 0:. ```. time docker run --rm --gpus 1 \. -v ""${DATA_DIR}:${DATA_DIR}"" \. -v ""${OUTPUT_DIR}:${OUTPUT_DIR}"" \. -u $(id -u):$(id -g) \. google/deepvariant:1.4.0-gpu \. /opt/deepvariant/bin/model_train \. --dataset_config_pbtxt=""${OUTPUT_DIR}/training_set.dataset_config.pbtxt"" \. --train_dir=""${TRAINING_DIR}"" \. --model_name=""inception_v3"" \. --number_of_steps=50000 \. --save_interval_secs=300 \. --batch_size=32 \. --learning_rate=0.0005 \. --start_from_checkpoint=""gs://deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-wgs_standard/model.ckpt"". ```. Then I tried to start the `model_eval` evaluator in GPU mode with 1 GPU, but Docker was just giving it the *same* GPU as training was using, so it kept immediately running out of GPU memory. After about half an hour I hit on the right syntax to give it a distinct GPU, and managed to actually start `model_eval`:. ```. docker run --rm --gpus '""device=1""' \. -v ""${DATA_DIR}:${DATA_DIR}"" \. -v ""${OUTPUT_DIR}:${OUTPUT_DIR}"" \. -u $(id -u):$(id -g) \. google/deepvariant:1.4.0-gpu \. /opt/deepvariant/bin/mod",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/611
https://github.com/google/deepvariant/issues/611:330,reliability,checkpoint,checkpoint,330,"`model_eval` ignores pre-existing, not-yet-evaluated checkpoints, and nothing else seems to exist to evaluate them; **Describe the issue:**. The `model_eval` command can't catch up and evaluate checkpoints in a directory other than the latest one. If it ever crashes or isn't started in time to actually witness training create a checkpoint, there's no way for the user to evaluate the checkpoints it missed, and the ""best"" checkpoint it finds won't actually necessarily be the best out of all the checkpoints in the directory. . **Setup**. - Operating system: Ubuntu 22.04. - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): Docker. - Type of data: case study data. **Steps to reproduce:**. Loosely following https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-training-case-study.md#start-model_train-and-model_eval, I started the training Docker container using GPU 0:. ```. time docker run --rm --gpus 1 \. -v ""${DATA_DIR}:${DATA_DIR}"" \. -v ""${OUTPUT_DIR}:${OUTPUT_DIR}"" \. -u $(id -u):$(id -g) \. google/deepvariant:1.4.0-gpu \. /opt/deepvariant/bin/model_train \. --dataset_config_pbtxt=""${OUTPUT_DIR}/training_set.dataset_config.pbtxt"" \. --train_dir=""${TRAINING_DIR}"" \. --model_name=""inception_v3"" \. --number_of_steps=50000 \. --save_interval_secs=300 \. --batch_size=32 \. --learning_rate=0.0005 \. --start_from_checkpoint=""gs://deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-wgs_standard/model.ckpt"". ```. Then I tried to start the `model_eval` evaluator in GPU mode with 1 GPU, but Docker was just giving it the *same* GPU as training was using, so it kept immediately running out of GPU memory. After about half an hour I hit on the right syntax to give it a distinct GPU, and managed to actually start `model_eval`:. ```. docker run --rm --gpus '""device=1""' \. -v ""${DATA_DIR}:${DATA_DIR}"" \. -v ""${OUTPUT_DIR}:${OUTPUT_DIR}"" \. -u $(id -u):$(id -g) \. google/deepvariant:1.4.0-gpu \. /opt/deepvariant/bin/mod",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/611
https://github.com/google/deepvariant/issues/611:386,reliability,checkpoint,checkpoints,386,"`model_eval` ignores pre-existing, not-yet-evaluated checkpoints, and nothing else seems to exist to evaluate them; **Describe the issue:**. The `model_eval` command can't catch up and evaluate checkpoints in a directory other than the latest one. If it ever crashes or isn't started in time to actually witness training create a checkpoint, there's no way for the user to evaluate the checkpoints it missed, and the ""best"" checkpoint it finds won't actually necessarily be the best out of all the checkpoints in the directory. . **Setup**. - Operating system: Ubuntu 22.04. - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): Docker. - Type of data: case study data. **Steps to reproduce:**. Loosely following https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-training-case-study.md#start-model_train-and-model_eval, I started the training Docker container using GPU 0:. ```. time docker run --rm --gpus 1 \. -v ""${DATA_DIR}:${DATA_DIR}"" \. -v ""${OUTPUT_DIR}:${OUTPUT_DIR}"" \. -u $(id -u):$(id -g) \. google/deepvariant:1.4.0-gpu \. /opt/deepvariant/bin/model_train \. --dataset_config_pbtxt=""${OUTPUT_DIR}/training_set.dataset_config.pbtxt"" \. --train_dir=""${TRAINING_DIR}"" \. --model_name=""inception_v3"" \. --number_of_steps=50000 \. --save_interval_secs=300 \. --batch_size=32 \. --learning_rate=0.0005 \. --start_from_checkpoint=""gs://deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-wgs_standard/model.ckpt"". ```. Then I tried to start the `model_eval` evaluator in GPU mode with 1 GPU, but Docker was just giving it the *same* GPU as training was using, so it kept immediately running out of GPU memory. After about half an hour I hit on the right syntax to give it a distinct GPU, and managed to actually start `model_eval`:. ```. docker run --rm --gpus '""device=1""' \. -v ""${DATA_DIR}:${DATA_DIR}"" \. -v ""${OUTPUT_DIR}:${OUTPUT_DIR}"" \. -u $(id -u):$(id -g) \. google/deepvariant:1.4.0-gpu \. /opt/deepvariant/bin/mod",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/611
https://github.com/google/deepvariant/issues/611:424,reliability,checkpoint,checkpoint,424,"`model_eval` ignores pre-existing, not-yet-evaluated checkpoints, and nothing else seems to exist to evaluate them; **Describe the issue:**. The `model_eval` command can't catch up and evaluate checkpoints in a directory other than the latest one. If it ever crashes or isn't started in time to actually witness training create a checkpoint, there's no way for the user to evaluate the checkpoints it missed, and the ""best"" checkpoint it finds won't actually necessarily be the best out of all the checkpoints in the directory. . **Setup**. - Operating system: Ubuntu 22.04. - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): Docker. - Type of data: case study data. **Steps to reproduce:**. Loosely following https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-training-case-study.md#start-model_train-and-model_eval, I started the training Docker container using GPU 0:. ```. time docker run --rm --gpus 1 \. -v ""${DATA_DIR}:${DATA_DIR}"" \. -v ""${OUTPUT_DIR}:${OUTPUT_DIR}"" \. -u $(id -u):$(id -g) \. google/deepvariant:1.4.0-gpu \. /opt/deepvariant/bin/model_train \. --dataset_config_pbtxt=""${OUTPUT_DIR}/training_set.dataset_config.pbtxt"" \. --train_dir=""${TRAINING_DIR}"" \. --model_name=""inception_v3"" \. --number_of_steps=50000 \. --save_interval_secs=300 \. --batch_size=32 \. --learning_rate=0.0005 \. --start_from_checkpoint=""gs://deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-wgs_standard/model.ckpt"". ```. Then I tried to start the `model_eval` evaluator in GPU mode with 1 GPU, but Docker was just giving it the *same* GPU as training was using, so it kept immediately running out of GPU memory. After about half an hour I hit on the right syntax to give it a distinct GPU, and managed to actually start `model_eval`:. ```. docker run --rm --gpus '""device=1""' \. -v ""${DATA_DIR}:${DATA_DIR}"" \. -v ""${OUTPUT_DIR}:${OUTPUT_DIR}"" \. -u $(id -u):$(id -g) \. google/deepvariant:1.4.0-gpu \. /opt/deepvariant/bin/mod",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/611
https://github.com/google/deepvariant/issues/611:498,reliability,checkpoint,checkpoints,498,"`model_eval` ignores pre-existing, not-yet-evaluated checkpoints, and nothing else seems to exist to evaluate them; **Describe the issue:**. The `model_eval` command can't catch up and evaluate checkpoints in a directory other than the latest one. If it ever crashes or isn't started in time to actually witness training create a checkpoint, there's no way for the user to evaluate the checkpoints it missed, and the ""best"" checkpoint it finds won't actually necessarily be the best out of all the checkpoints in the directory. . **Setup**. - Operating system: Ubuntu 22.04. - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): Docker. - Type of data: case study data. **Steps to reproduce:**. Loosely following https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-training-case-study.md#start-model_train-and-model_eval, I started the training Docker container using GPU 0:. ```. time docker run --rm --gpus 1 \. -v ""${DATA_DIR}:${DATA_DIR}"" \. -v ""${OUTPUT_DIR}:${OUTPUT_DIR}"" \. -u $(id -u):$(id -g) \. google/deepvariant:1.4.0-gpu \. /opt/deepvariant/bin/model_train \. --dataset_config_pbtxt=""${OUTPUT_DIR}/training_set.dataset_config.pbtxt"" \. --train_dir=""${TRAINING_DIR}"" \. --model_name=""inception_v3"" \. --number_of_steps=50000 \. --save_interval_secs=300 \. --batch_size=32 \. --learning_rate=0.0005 \. --start_from_checkpoint=""gs://deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-wgs_standard/model.ckpt"". ```. Then I tried to start the `model_eval` evaluator in GPU mode with 1 GPU, but Docker was just giving it the *same* GPU as training was using, so it kept immediately running out of GPU memory. After about half an hour I hit on the right syntax to give it a distinct GPU, and managed to actually start `model_eval`:. ```. docker run --rm --gpus '""device=1""' \. -v ""${DATA_DIR}:${DATA_DIR}"" \. -v ""${OUTPUT_DIR}:${OUTPUT_DIR}"" \. -u $(id -u):$(id -g) \. google/deepvariant:1.4.0-gpu \. /opt/deepvariant/bin/mod",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/611
https://github.com/google/deepvariant/issues/611:2229,reliability,checkpoint,checkpoint,2229,"el_name=""inception_v3"" \. --number_of_steps=50000 \. --save_interval_secs=300 \. --batch_size=32 \. --learning_rate=0.0005 \. --start_from_checkpoint=""gs://deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-wgs_standard/model.ckpt"". ```. Then I tried to start the `model_eval` evaluator in GPU mode with 1 GPU, but Docker was just giving it the *same* GPU as training was using, so it kept immediately running out of GPU memory. After about half an hour I hit on the right syntax to give it a distinct GPU, and managed to actually start `model_eval`:. ```. docker run --rm --gpus '""device=1""' \. -v ""${DATA_DIR}:${DATA_DIR}"" \. -v ""${OUTPUT_DIR}:${OUTPUT_DIR}"" \. -u $(id -u):$(id -g) \. google/deepvariant:1.4.0-gpu \. /opt/deepvariant/bin/model_eval \. --dataset_config_pbtxt=""${OUTPUT_DIR}/validation_set.dataset_config.pbtxt"" \. --checkpoint_dir=""${TRAINING_DIR}"" \. --batch_size=512 \. --min_eval_interval_s=1 \. --eval_timeout=1000. ```. It evaluated the most recent checkpoint saves as of when it started up, and it is evaluating new checkpoints as they are created. But it doesn't seem interested in going back and evaluating all the checkpoints, other than the initial-latest one, that were created before it started. The directory in question looks like this:. ```. (dv_venv) [anovak@phoenix-01 trash]$ ls output/models/*meta. output/models/model.ckpt-0.meta output/models/model.ckpt-19459.meta output/models/model.ckpt-31078.meta. output/models/model.ckpt-1000.meta output/models/model.ckpt-2000.meta output/models/model.ckpt-34008.meta. output/models/model.ckpt-10674.meta output/models/model.ckpt-22355.meta output/models/model.ckpt-4814.meta. output/models/model.ckpt-13613.meta output/models/model.ckpt-25257.meta output/models/model.ckpt-7724.meta. output/models/model.ckpt-16546.meta output/models/model.ckpt-28168.meta. (dv_venv) [anovak@phoenix-01 trash]$ ls output/models/*metrics. output/models/best_checkpoint.metrics output/models/model.ckpt-28168.metrics",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/611
https://github.com/google/deepvariant/issues/611:2297,reliability,checkpoint,checkpoints,2297,"secs=300 \. --batch_size=32 \. --learning_rate=0.0005 \. --start_from_checkpoint=""gs://deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-wgs_standard/model.ckpt"". ```. Then I tried to start the `model_eval` evaluator in GPU mode with 1 GPU, but Docker was just giving it the *same* GPU as training was using, so it kept immediately running out of GPU memory. After about half an hour I hit on the right syntax to give it a distinct GPU, and managed to actually start `model_eval`:. ```. docker run --rm --gpus '""device=1""' \. -v ""${DATA_DIR}:${DATA_DIR}"" \. -v ""${OUTPUT_DIR}:${OUTPUT_DIR}"" \. -u $(id -u):$(id -g) \. google/deepvariant:1.4.0-gpu \. /opt/deepvariant/bin/model_eval \. --dataset_config_pbtxt=""${OUTPUT_DIR}/validation_set.dataset_config.pbtxt"" \. --checkpoint_dir=""${TRAINING_DIR}"" \. --batch_size=512 \. --min_eval_interval_s=1 \. --eval_timeout=1000. ```. It evaluated the most recent checkpoint saves as of when it started up, and it is evaluating new checkpoints as they are created. But it doesn't seem interested in going back and evaluating all the checkpoints, other than the initial-latest one, that were created before it started. The directory in question looks like this:. ```. (dv_venv) [anovak@phoenix-01 trash]$ ls output/models/*meta. output/models/model.ckpt-0.meta output/models/model.ckpt-19459.meta output/models/model.ckpt-31078.meta. output/models/model.ckpt-1000.meta output/models/model.ckpt-2000.meta output/models/model.ckpt-34008.meta. output/models/model.ckpt-10674.meta output/models/model.ckpt-22355.meta output/models/model.ckpt-4814.meta. output/models/model.ckpt-13613.meta output/models/model.ckpt-25257.meta output/models/model.ckpt-7724.meta. output/models/model.ckpt-16546.meta output/models/model.ckpt-28168.meta. (dv_venv) [anovak@phoenix-01 trash]$ ls output/models/*metrics. output/models/best_checkpoint.metrics output/models/model.ckpt-28168.metrics output/models/model.ckpt-34008.metrics. output/models/current.metric",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/611
https://github.com/google/deepvariant/issues/611:2337,reliability,doe,doesn,2337,"ning_rate=0.0005 \. --start_from_checkpoint=""gs://deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-wgs_standard/model.ckpt"". ```. Then I tried to start the `model_eval` evaluator in GPU mode with 1 GPU, but Docker was just giving it the *same* GPU as training was using, so it kept immediately running out of GPU memory. After about half an hour I hit on the right syntax to give it a distinct GPU, and managed to actually start `model_eval`:. ```. docker run --rm --gpus '""device=1""' \. -v ""${DATA_DIR}:${DATA_DIR}"" \. -v ""${OUTPUT_DIR}:${OUTPUT_DIR}"" \. -u $(id -u):$(id -g) \. google/deepvariant:1.4.0-gpu \. /opt/deepvariant/bin/model_eval \. --dataset_config_pbtxt=""${OUTPUT_DIR}/validation_set.dataset_config.pbtxt"" \. --checkpoint_dir=""${TRAINING_DIR}"" \. --batch_size=512 \. --min_eval_interval_s=1 \. --eval_timeout=1000. ```. It evaluated the most recent checkpoint saves as of when it started up, and it is evaluating new checkpoints as they are created. But it doesn't seem interested in going back and evaluating all the checkpoints, other than the initial-latest one, that were created before it started. The directory in question looks like this:. ```. (dv_venv) [anovak@phoenix-01 trash]$ ls output/models/*meta. output/models/model.ckpt-0.meta output/models/model.ckpt-19459.meta output/models/model.ckpt-31078.meta. output/models/model.ckpt-1000.meta output/models/model.ckpt-2000.meta output/models/model.ckpt-34008.meta. output/models/model.ckpt-10674.meta output/models/model.ckpt-22355.meta output/models/model.ckpt-4814.meta. output/models/model.ckpt-13613.meta output/models/model.ckpt-25257.meta output/models/model.ckpt-7724.meta. output/models/model.ckpt-16546.meta output/models/model.ckpt-28168.meta. (dv_venv) [anovak@phoenix-01 trash]$ ls output/models/*metrics. output/models/best_checkpoint.metrics output/models/model.ckpt-28168.metrics output/models/model.ckpt-34008.metrics. output/models/current.metrics output/models/model.ckpt-31078.metr",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/611
https://github.com/google/deepvariant/issues/611:2398,reliability,checkpoint,checkpoints,2398,"dels/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-wgs_standard/model.ckpt"". ```. Then I tried to start the `model_eval` evaluator in GPU mode with 1 GPU, but Docker was just giving it the *same* GPU as training was using, so it kept immediately running out of GPU memory. After about half an hour I hit on the right syntax to give it a distinct GPU, and managed to actually start `model_eval`:. ```. docker run --rm --gpus '""device=1""' \. -v ""${DATA_DIR}:${DATA_DIR}"" \. -v ""${OUTPUT_DIR}:${OUTPUT_DIR}"" \. -u $(id -u):$(id -g) \. google/deepvariant:1.4.0-gpu \. /opt/deepvariant/bin/model_eval \. --dataset_config_pbtxt=""${OUTPUT_DIR}/validation_set.dataset_config.pbtxt"" \. --checkpoint_dir=""${TRAINING_DIR}"" \. --batch_size=512 \. --min_eval_interval_s=1 \. --eval_timeout=1000. ```. It evaluated the most recent checkpoint saves as of when it started up, and it is evaluating new checkpoints as they are created. But it doesn't seem interested in going back and evaluating all the checkpoints, other than the initial-latest one, that were created before it started. The directory in question looks like this:. ```. (dv_venv) [anovak@phoenix-01 trash]$ ls output/models/*meta. output/models/model.ckpt-0.meta output/models/model.ckpt-19459.meta output/models/model.ckpt-31078.meta. output/models/model.ckpt-1000.meta output/models/model.ckpt-2000.meta output/models/model.ckpt-34008.meta. output/models/model.ckpt-10674.meta output/models/model.ckpt-22355.meta output/models/model.ckpt-4814.meta. output/models/model.ckpt-13613.meta output/models/model.ckpt-25257.meta output/models/model.ckpt-7724.meta. output/models/model.ckpt-16546.meta output/models/model.ckpt-28168.meta. (dv_venv) [anovak@phoenix-01 trash]$ ls output/models/*metrics. output/models/best_checkpoint.metrics output/models/model.ckpt-28168.metrics output/models/model.ckpt-34008.metrics. output/models/current.metrics output/models/model.ckpt-31078.metrics. ```. But `model_eval` just sits there like this (until a ne",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/611
https://github.com/google/deepvariant/issues/611:3406,reliability,checkpoint,checkpoint,3406,"other than the initial-latest one, that were created before it started. The directory in question looks like this:. ```. (dv_venv) [anovak@phoenix-01 trash]$ ls output/models/*meta. output/models/model.ckpt-0.meta output/models/model.ckpt-19459.meta output/models/model.ckpt-31078.meta. output/models/model.ckpt-1000.meta output/models/model.ckpt-2000.meta output/models/model.ckpt-34008.meta. output/models/model.ckpt-10674.meta output/models/model.ckpt-22355.meta output/models/model.ckpt-4814.meta. output/models/model.ckpt-13613.meta output/models/model.ckpt-25257.meta output/models/model.ckpt-7724.meta. output/models/model.ckpt-16546.meta output/models/model.ckpt-28168.meta. (dv_venv) [anovak@phoenix-01 trash]$ ls output/models/*metrics. output/models/best_checkpoint.metrics output/models/model.ckpt-28168.metrics output/models/model.ckpt-34008.metrics. output/models/current.metrics output/models/model.ckpt-31078.metrics. ```. But `model_eval` just sits there like this (until a new checkpoint appears):. ```. I0210 17:42:06.700287 139846137329472 checkpoint_utils.py:140] Waiting for new checkpoint at /public/groups/cgl/graph-genomes/anovak/trash/output/models. ```. How do I get the missing `*metrics` files and determine if any of the checkpoints that were missed is actually the best one? Do I need to `touch` some particular files in the directory to get `model_eval` to be interested in them? Is there some other command besides `model_eval` that can process a single particular checkpoint at a time? **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? It doesn't look like model training is part of the quick start. **Any additional context:**. Eventually I might want a WDL workflow for training DeepVariant, and I'm not sure that managing two simultaneous DV processes in there is going to be worth the engineerin",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/611
https://github.com/google/deepvariant/issues/611:3512,reliability,checkpoint,checkpoint,3512,"ke this:. ```. (dv_venv) [anovak@phoenix-01 trash]$ ls output/models/*meta. output/models/model.ckpt-0.meta output/models/model.ckpt-19459.meta output/models/model.ckpt-31078.meta. output/models/model.ckpt-1000.meta output/models/model.ckpt-2000.meta output/models/model.ckpt-34008.meta. output/models/model.ckpt-10674.meta output/models/model.ckpt-22355.meta output/models/model.ckpt-4814.meta. output/models/model.ckpt-13613.meta output/models/model.ckpt-25257.meta output/models/model.ckpt-7724.meta. output/models/model.ckpt-16546.meta output/models/model.ckpt-28168.meta. (dv_venv) [anovak@phoenix-01 trash]$ ls output/models/*metrics. output/models/best_checkpoint.metrics output/models/model.ckpt-28168.metrics output/models/model.ckpt-34008.metrics. output/models/current.metrics output/models/model.ckpt-31078.metrics. ```. But `model_eval` just sits there like this (until a new checkpoint appears):. ```. I0210 17:42:06.700287 139846137329472 checkpoint_utils.py:140] Waiting for new checkpoint at /public/groups/cgl/graph-genomes/anovak/trash/output/models. ```. How do I get the missing `*metrics` files and determine if any of the checkpoints that were missed is actually the best one? Do I need to `touch` some particular files in the directory to get `model_eval` to be interested in them? Is there some other command besides `model_eval` that can process a single particular checkpoint at a time? **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? It doesn't look like model training is part of the quick start. **Any additional context:**. Eventually I might want a WDL workflow for training DeepVariant, and I'm not sure that managing two simultaneous DV processes in there is going to be worth the engineering required; they'd have to be lumped together into one WDL task and they'd have to always fit simultaneous",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/611
https://github.com/google/deepvariant/issues/611:3662,reliability,checkpoint,checkpoints,3662,"models/model.ckpt-31078.meta. output/models/model.ckpt-1000.meta output/models/model.ckpt-2000.meta output/models/model.ckpt-34008.meta. output/models/model.ckpt-10674.meta output/models/model.ckpt-22355.meta output/models/model.ckpt-4814.meta. output/models/model.ckpt-13613.meta output/models/model.ckpt-25257.meta output/models/model.ckpt-7724.meta. output/models/model.ckpt-16546.meta output/models/model.ckpt-28168.meta. (dv_venv) [anovak@phoenix-01 trash]$ ls output/models/*metrics. output/models/best_checkpoint.metrics output/models/model.ckpt-28168.metrics output/models/model.ckpt-34008.metrics. output/models/current.metrics output/models/model.ckpt-31078.metrics. ```. But `model_eval` just sits there like this (until a new checkpoint appears):. ```. I0210 17:42:06.700287 139846137329472 checkpoint_utils.py:140] Waiting for new checkpoint at /public/groups/cgl/graph-genomes/anovak/trash/output/models. ```. How do I get the missing `*metrics` files and determine if any of the checkpoints that were missed is actually the best one? Do I need to `touch` some particular files in the directory to get `model_eval` to be interested in them? Is there some other command besides `model_eval` that can process a single particular checkpoint at a time? **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? It doesn't look like model training is part of the quick start. **Any additional context:**. Eventually I might want a WDL workflow for training DeepVariant, and I'm not sure that managing two simultaneous DV processes in there is going to be worth the engineering required; they'd have to be lumped together into one WDL task and they'd have to always fit simultaneously on one machine. It would be much simpler for me to be able to run the training to the end, and then run all the evaluations afterward to select the ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/611
https://github.com/google/deepvariant/issues/611:3909,reliability,checkpoint,checkpoint,3909,"0674.meta output/models/model.ckpt-22355.meta output/models/model.ckpt-4814.meta. output/models/model.ckpt-13613.meta output/models/model.ckpt-25257.meta output/models/model.ckpt-7724.meta. output/models/model.ckpt-16546.meta output/models/model.ckpt-28168.meta. (dv_venv) [anovak@phoenix-01 trash]$ ls output/models/*metrics. output/models/best_checkpoint.metrics output/models/model.ckpt-28168.metrics output/models/model.ckpt-34008.metrics. output/models/current.metrics output/models/model.ckpt-31078.metrics. ```. But `model_eval` just sits there like this (until a new checkpoint appears):. ```. I0210 17:42:06.700287 139846137329472 checkpoint_utils.py:140] Waiting for new checkpoint at /public/groups/cgl/graph-genomes/anovak/trash/output/models. ```. How do I get the missing `*metrics` files and determine if any of the checkpoints that were missed is actually the best one? Do I need to `touch` some particular files in the directory to get `model_eval` to be interested in them? Is there some other command besides `model_eval` that can process a single particular checkpoint at a time? **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? It doesn't look like model training is part of the quick start. **Any additional context:**. Eventually I might want a WDL workflow for training DeepVariant, and I'm not sure that managing two simultaneous DV processes in there is going to be worth the engineering required; they'd have to be lumped together into one WDL task and they'd have to always fit simultaneously on one machine. It would be much simpler for me to be able to run the training to the end, and then run all the evaluations afterward to select the best model. But it looks like if I tried that right now `model_eval` would just only evaluate the last checkpoint and always confidently declare it to be the best.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/611
https://github.com/google/deepvariant/issues/611:3933,reliability,Doe,Does,3933,"0674.meta output/models/model.ckpt-22355.meta output/models/model.ckpt-4814.meta. output/models/model.ckpt-13613.meta output/models/model.ckpt-25257.meta output/models/model.ckpt-7724.meta. output/models/model.ckpt-16546.meta output/models/model.ckpt-28168.meta. (dv_venv) [anovak@phoenix-01 trash]$ ls output/models/*metrics. output/models/best_checkpoint.metrics output/models/model.ckpt-28168.metrics output/models/model.ckpt-34008.metrics. output/models/current.metrics output/models/model.ckpt-31078.metrics. ```. But `model_eval` just sits there like this (until a new checkpoint appears):. ```. I0210 17:42:06.700287 139846137329472 checkpoint_utils.py:140] Waiting for new checkpoint at /public/groups/cgl/graph-genomes/anovak/trash/output/models. ```. How do I get the missing `*metrics` files and determine if any of the checkpoints that were missed is actually the best one? Do I need to `touch` some particular files in the directory to get `model_eval` to be interested in them? Is there some other command besides `model_eval` that can process a single particular checkpoint at a time? **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? It doesn't look like model training is part of the quick start. **Any additional context:**. Eventually I might want a WDL workflow for training DeepVariant, and I'm not sure that managing two simultaneous DV processes in there is going to be worth the engineering required; they'd have to be lumped together into one WDL task and they'd have to always fit simultaneously on one machine. It would be much simpler for me to be able to run the training to the end, and then run all the evaluations afterward to select the best model. But it looks like if I tried that right now `model_eval` would just only evaluate the last checkpoint and always confidently declare it to be the best.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/611
https://github.com/google/deepvariant/issues/611:4151,reliability,doe,doesn,4151,"0674.meta output/models/model.ckpt-22355.meta output/models/model.ckpt-4814.meta. output/models/model.ckpt-13613.meta output/models/model.ckpt-25257.meta output/models/model.ckpt-7724.meta. output/models/model.ckpt-16546.meta output/models/model.ckpt-28168.meta. (dv_venv) [anovak@phoenix-01 trash]$ ls output/models/*metrics. output/models/best_checkpoint.metrics output/models/model.ckpt-28168.metrics output/models/model.ckpt-34008.metrics. output/models/current.metrics output/models/model.ckpt-31078.metrics. ```. But `model_eval` just sits there like this (until a new checkpoint appears):. ```. I0210 17:42:06.700287 139846137329472 checkpoint_utils.py:140] Waiting for new checkpoint at /public/groups/cgl/graph-genomes/anovak/trash/output/models. ```. How do I get the missing `*metrics` files and determine if any of the checkpoints that were missed is actually the best one? Do I need to `touch` some particular files in the directory to get `model_eval` to be interested in them? Is there some other command besides `model_eval` that can process a single particular checkpoint at a time? **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? It doesn't look like model training is part of the quick start. **Any additional context:**. Eventually I might want a WDL workflow for training DeepVariant, and I'm not sure that managing two simultaneous DV processes in there is going to be worth the engineering required; they'd have to be lumped together into one WDL task and they'd have to always fit simultaneously on one machine. It would be much simpler for me to be able to run the training to the end, and then run all the evaluations afterward to select the best model. But it looks like if I tried that right now `model_eval` would just only evaluate the last checkpoint and always confidently declare it to be the best.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/611
https://github.com/google/deepvariant/issues/611:4771,reliability,checkpoint,checkpoint,4771,"0674.meta output/models/model.ckpt-22355.meta output/models/model.ckpt-4814.meta. output/models/model.ckpt-13613.meta output/models/model.ckpt-25257.meta output/models/model.ckpt-7724.meta. output/models/model.ckpt-16546.meta output/models/model.ckpt-28168.meta. (dv_venv) [anovak@phoenix-01 trash]$ ls output/models/*metrics. output/models/best_checkpoint.metrics output/models/model.ckpt-28168.metrics output/models/model.ckpt-34008.metrics. output/models/current.metrics output/models/model.ckpt-31078.metrics. ```. But `model_eval` just sits there like this (until a new checkpoint appears):. ```. I0210 17:42:06.700287 139846137329472 checkpoint_utils.py:140] Waiting for new checkpoint at /public/groups/cgl/graph-genomes/anovak/trash/output/models. ```. How do I get the missing `*metrics` files and determine if any of the checkpoints that were missed is actually the best one? Do I need to `touch` some particular files in the directory to get `model_eval` to be interested in them? Is there some other command besides `model_eval` that can process a single particular checkpoint at a time? **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? It doesn't look like model training is part of the quick start. **Any additional context:**. Eventually I might want a WDL workflow for training DeepVariant, and I'm not sure that managing two simultaneous DV processes in there is going to be worth the engineering required; they'd have to be lumped together into one WDL task and they'd have to always fit simultaneously on one machine. It would be much simpler for me to be able to run the training to the end, and then run all the evaluations afterward to select the best model. But it looks like if I tried that right now `model_eval` would just only evaluate the last checkpoint and always confidently declare it to be the best.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/611
https://github.com/google/deepvariant/issues/611:1767,safety,manag,managed,1767,"/deepvariant/blob/r1.4/docs/deepvariant-training-case-study.md#start-model_train-and-model_eval, I started the training Docker container using GPU 0:. ```. time docker run --rm --gpus 1 \. -v ""${DATA_DIR}:${DATA_DIR}"" \. -v ""${OUTPUT_DIR}:${OUTPUT_DIR}"" \. -u $(id -u):$(id -g) \. google/deepvariant:1.4.0-gpu \. /opt/deepvariant/bin/model_train \. --dataset_config_pbtxt=""${OUTPUT_DIR}/training_set.dataset_config.pbtxt"" \. --train_dir=""${TRAINING_DIR}"" \. --model_name=""inception_v3"" \. --number_of_steps=50000 \. --save_interval_secs=300 \. --batch_size=32 \. --learning_rate=0.0005 \. --start_from_checkpoint=""gs://deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-wgs_standard/model.ckpt"". ```. Then I tried to start the `model_eval` evaluator in GPU mode with 1 GPU, but Docker was just giving it the *same* GPU as training was using, so it kept immediately running out of GPU memory. After about half an hour I hit on the right syntax to give it a distinct GPU, and managed to actually start `model_eval`:. ```. docker run --rm --gpus '""device=1""' \. -v ""${DATA_DIR}:${DATA_DIR}"" \. -v ""${OUTPUT_DIR}:${OUTPUT_DIR}"" \. -u $(id -u):$(id -g) \. google/deepvariant:1.4.0-gpu \. /opt/deepvariant/bin/model_eval \. --dataset_config_pbtxt=""${OUTPUT_DIR}/validation_set.dataset_config.pbtxt"" \. --checkpoint_dir=""${TRAINING_DIR}"" \. --batch_size=512 \. --min_eval_interval_s=1 \. --eval_timeout=1000. ```. It evaluated the most recent checkpoint saves as of when it started up, and it is evaluating new checkpoints as they are created. But it doesn't seem interested in going back and evaluating all the checkpoints, other than the initial-latest one, that were created before it started. The directory in question looks like this:. ```. (dv_venv) [anovak@phoenix-01 trash]$ ls output/models/*meta. output/models/model.ckpt-0.meta output/models/model.ckpt-19459.meta output/models/model.ckpt-31078.meta. output/models/model.ckpt-1000.meta output/models/model.ckpt-2000.meta out",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/611
https://github.com/google/deepvariant/issues/611:3954,safety,test,test,3954,"0674.meta output/models/model.ckpt-22355.meta output/models/model.ckpt-4814.meta. output/models/model.ckpt-13613.meta output/models/model.ckpt-25257.meta output/models/model.ckpt-7724.meta. output/models/model.ckpt-16546.meta output/models/model.ckpt-28168.meta. (dv_venv) [anovak@phoenix-01 trash]$ ls output/models/*metrics. output/models/best_checkpoint.metrics output/models/model.ckpt-28168.metrics output/models/model.ckpt-34008.metrics. output/models/current.metrics output/models/model.ckpt-31078.metrics. ```. But `model_eval` just sits there like this (until a new checkpoint appears):. ```. I0210 17:42:06.700287 139846137329472 checkpoint_utils.py:140] Waiting for new checkpoint at /public/groups/cgl/graph-genomes/anovak/trash/output/models. ```. How do I get the missing `*metrics` files and determine if any of the checkpoints that were missed is actually the best one? Do I need to `touch` some particular files in the directory to get `model_eval` to be interested in them? Is there some other command besides `model_eval` that can process a single particular checkpoint at a time? **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? It doesn't look like model training is part of the quick start. **Any additional context:**. Eventually I might want a WDL workflow for training DeepVariant, and I'm not sure that managing two simultaneous DV processes in there is going to be worth the engineering required; they'd have to be lumped together into one WDL task and they'd have to always fit simultaneously on one machine. It would be much simpler for me to be able to run the training to the end, and then run all the evaluations afterward to select the best model. But it looks like if I tried that right now `model_eval` would just only evaluate the last checkpoint and always confidently declare it to be the best.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/611
https://github.com/google/deepvariant/issues/611:3990,safety,test,test,3990,"0674.meta output/models/model.ckpt-22355.meta output/models/model.ckpt-4814.meta. output/models/model.ckpt-13613.meta output/models/model.ckpt-25257.meta output/models/model.ckpt-7724.meta. output/models/model.ckpt-16546.meta output/models/model.ckpt-28168.meta. (dv_venv) [anovak@phoenix-01 trash]$ ls output/models/*metrics. output/models/best_checkpoint.metrics output/models/model.ckpt-28168.metrics output/models/model.ckpt-34008.metrics. output/models/current.metrics output/models/model.ckpt-31078.metrics. ```. But `model_eval` just sits there like this (until a new checkpoint appears):. ```. I0210 17:42:06.700287 139846137329472 checkpoint_utils.py:140] Waiting for new checkpoint at /public/groups/cgl/graph-genomes/anovak/trash/output/models. ```. How do I get the missing `*metrics` files and determine if any of the checkpoints that were missed is actually the best one? Do I need to `touch` some particular files in the directory to get `model_eval` to be interested in them? Is there some other command besides `model_eval` that can process a single particular checkpoint at a time? **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? It doesn't look like model training is part of the quick start. **Any additional context:**. Eventually I might want a WDL workflow for training DeepVariant, and I'm not sure that managing two simultaneous DV processes in there is going to be worth the engineering required; they'd have to be lumped together into one WDL task and they'd have to always fit simultaneously on one machine. It would be much simpler for me to be able to run the training to the end, and then run all the evaluations afterward to select the best model. But it looks like if I tried that right now `model_eval` would just only evaluate the last checkpoint and always confidently declare it to be the best.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/611
https://github.com/google/deepvariant/issues/611:4328,safety,manag,managing,4328,"0674.meta output/models/model.ckpt-22355.meta output/models/model.ckpt-4814.meta. output/models/model.ckpt-13613.meta output/models/model.ckpt-25257.meta output/models/model.ckpt-7724.meta. output/models/model.ckpt-16546.meta output/models/model.ckpt-28168.meta. (dv_venv) [anovak@phoenix-01 trash]$ ls output/models/*metrics. output/models/best_checkpoint.metrics output/models/model.ckpt-28168.metrics output/models/model.ckpt-34008.metrics. output/models/current.metrics output/models/model.ckpt-31078.metrics. ```. But `model_eval` just sits there like this (until a new checkpoint appears):. ```. I0210 17:42:06.700287 139846137329472 checkpoint_utils.py:140] Waiting for new checkpoint at /public/groups/cgl/graph-genomes/anovak/trash/output/models. ```. How do I get the missing `*metrics` files and determine if any of the checkpoints that were missed is actually the best one? Do I need to `touch` some particular files in the directory to get `model_eval` to be interested in them? Is there some other command besides `model_eval` that can process a single particular checkpoint at a time? **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? It doesn't look like model training is part of the quick start. **Any additional context:**. Eventually I might want a WDL workflow for training DeepVariant, and I'm not sure that managing two simultaneous DV processes in there is going to be worth the engineering required; they'd have to be lumped together into one WDL task and they'd have to always fit simultaneously on one machine. It would be much simpler for me to be able to run the training to the end, and then run all the evaluations afterward to select the best model. But it looks like if I tried that right now `model_eval` would just only evaluate the last checkpoint and always confidently declare it to be the best.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/611
https://github.com/google/deepvariant/issues/611:1402,security,model,models,1402,"ed, and the ""best"" checkpoint it finds won't actually necessarily be the best out of all the checkpoints in the directory. . **Setup**. - Operating system: Ubuntu 22.04. - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): Docker. - Type of data: case study data. **Steps to reproduce:**. Loosely following https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-training-case-study.md#start-model_train-and-model_eval, I started the training Docker container using GPU 0:. ```. time docker run --rm --gpus 1 \. -v ""${DATA_DIR}:${DATA_DIR}"" \. -v ""${OUTPUT_DIR}:${OUTPUT_DIR}"" \. -u $(id -u):$(id -g) \. google/deepvariant:1.4.0-gpu \. /opt/deepvariant/bin/model_train \. --dataset_config_pbtxt=""${OUTPUT_DIR}/training_set.dataset_config.pbtxt"" \. --train_dir=""${TRAINING_DIR}"" \. --model_name=""inception_v3"" \. --number_of_steps=50000 \. --save_interval_secs=300 \. --batch_size=32 \. --learning_rate=0.0005 \. --start_from_checkpoint=""gs://deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-wgs_standard/model.ckpt"". ```. Then I tried to start the `model_eval` evaluator in GPU mode with 1 GPU, but Docker was just giving it the *same* GPU as training was using, so it kept immediately running out of GPU memory. After about half an hour I hit on the right syntax to give it a distinct GPU, and managed to actually start `model_eval`:. ```. docker run --rm --gpus '""device=1""' \. -v ""${DATA_DIR}:${DATA_DIR}"" \. -v ""${OUTPUT_DIR}:${OUTPUT_DIR}"" \. -u $(id -u):$(id -g) \. google/deepvariant:1.4.0-gpu \. /opt/deepvariant/bin/model_eval \. --dataset_config_pbtxt=""${OUTPUT_DIR}/validation_set.dataset_config.pbtxt"" \. --checkpoint_dir=""${TRAINING_DIR}"" \. --batch_size=512 \. --min_eval_interval_s=1 \. --eval_timeout=1000. ```. It evaluated the most recent checkpoint saves as of when it started up, and it is evaluating new checkpoints as they are created. But it doesn't seem interested in going back and evaluating all the checkpo",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/611
https://github.com/google/deepvariant/issues/611:1476,security,model,model,1476,"est out of all the checkpoints in the directory. . **Setup**. - Operating system: Ubuntu 22.04. - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): Docker. - Type of data: case study data. **Steps to reproduce:**. Loosely following https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-training-case-study.md#start-model_train-and-model_eval, I started the training Docker container using GPU 0:. ```. time docker run --rm --gpus 1 \. -v ""${DATA_DIR}:${DATA_DIR}"" \. -v ""${OUTPUT_DIR}:${OUTPUT_DIR}"" \. -u $(id -u):$(id -g) \. google/deepvariant:1.4.0-gpu \. /opt/deepvariant/bin/model_train \. --dataset_config_pbtxt=""${OUTPUT_DIR}/training_set.dataset_config.pbtxt"" \. --train_dir=""${TRAINING_DIR}"" \. --model_name=""inception_v3"" \. --number_of_steps=50000 \. --save_interval_secs=300 \. --batch_size=32 \. --learning_rate=0.0005 \. --start_from_checkpoint=""gs://deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-wgs_standard/model.ckpt"". ```. Then I tried to start the `model_eval` evaluator in GPU mode with 1 GPU, but Docker was just giving it the *same* GPU as training was using, so it kept immediately running out of GPU memory. After about half an hour I hit on the right syntax to give it a distinct GPU, and managed to actually start `model_eval`:. ```. docker run --rm --gpus '""device=1""' \. -v ""${DATA_DIR}:${DATA_DIR}"" \. -v ""${OUTPUT_DIR}:${OUTPUT_DIR}"" \. -u $(id -u):$(id -g) \. google/deepvariant:1.4.0-gpu \. /opt/deepvariant/bin/model_eval \. --dataset_config_pbtxt=""${OUTPUT_DIR}/validation_set.dataset_config.pbtxt"" \. --checkpoint_dir=""${TRAINING_DIR}"" \. --batch_size=512 \. --min_eval_interval_s=1 \. --eval_timeout=1000. ```. It evaluated the most recent checkpoint saves as of when it started up, and it is evaluating new checkpoints as they are created. But it doesn't seem interested in going back and evaluating all the checkpoints, other than the initial-latest one, that were created before it start",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/611
https://github.com/google/deepvariant/issues/611:2579,security,model,models,2579,"just giving it the *same* GPU as training was using, so it kept immediately running out of GPU memory. After about half an hour I hit on the right syntax to give it a distinct GPU, and managed to actually start `model_eval`:. ```. docker run --rm --gpus '""device=1""' \. -v ""${DATA_DIR}:${DATA_DIR}"" \. -v ""${OUTPUT_DIR}:${OUTPUT_DIR}"" \. -u $(id -u):$(id -g) \. google/deepvariant:1.4.0-gpu \. /opt/deepvariant/bin/model_eval \. --dataset_config_pbtxt=""${OUTPUT_DIR}/validation_set.dataset_config.pbtxt"" \. --checkpoint_dir=""${TRAINING_DIR}"" \. --batch_size=512 \. --min_eval_interval_s=1 \. --eval_timeout=1000. ```. It evaluated the most recent checkpoint saves as of when it started up, and it is evaluating new checkpoints as they are created. But it doesn't seem interested in going back and evaluating all the checkpoints, other than the initial-latest one, that were created before it started. The directory in question looks like this:. ```. (dv_venv) [anovak@phoenix-01 trash]$ ls output/models/*meta. output/models/model.ckpt-0.meta output/models/model.ckpt-19459.meta output/models/model.ckpt-31078.meta. output/models/model.ckpt-1000.meta output/models/model.ckpt-2000.meta output/models/model.ckpt-34008.meta. output/models/model.ckpt-10674.meta output/models/model.ckpt-22355.meta output/models/model.ckpt-4814.meta. output/models/model.ckpt-13613.meta output/models/model.ckpt-25257.meta output/models/model.ckpt-7724.meta. output/models/model.ckpt-16546.meta output/models/model.ckpt-28168.meta. (dv_venv) [anovak@phoenix-01 trash]$ ls output/models/*metrics. output/models/best_checkpoint.metrics output/models/model.ckpt-28168.metrics output/models/model.ckpt-34008.metrics. output/models/current.metrics output/models/model.ckpt-31078.metrics. ```. But `model_eval` just sits there like this (until a new checkpoint appears):. ```. I0210 17:42:06.700287 139846137329472 checkpoint_utils.py:140] Waiting for new checkpoint at /public/groups/cgl/graph-genomes/anovak/trash/output/mod",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/611
https://github.com/google/deepvariant/issues/611:2600,security,model,models,2600,"ame* GPU as training was using, so it kept immediately running out of GPU memory. After about half an hour I hit on the right syntax to give it a distinct GPU, and managed to actually start `model_eval`:. ```. docker run --rm --gpus '""device=1""' \. -v ""${DATA_DIR}:${DATA_DIR}"" \. -v ""${OUTPUT_DIR}:${OUTPUT_DIR}"" \. -u $(id -u):$(id -g) \. google/deepvariant:1.4.0-gpu \. /opt/deepvariant/bin/model_eval \. --dataset_config_pbtxt=""${OUTPUT_DIR}/validation_set.dataset_config.pbtxt"" \. --checkpoint_dir=""${TRAINING_DIR}"" \. --batch_size=512 \. --min_eval_interval_s=1 \. --eval_timeout=1000. ```. It evaluated the most recent checkpoint saves as of when it started up, and it is evaluating new checkpoints as they are created. But it doesn't seem interested in going back and evaluating all the checkpoints, other than the initial-latest one, that were created before it started. The directory in question looks like this:. ```. (dv_venv) [anovak@phoenix-01 trash]$ ls output/models/*meta. output/models/model.ckpt-0.meta output/models/model.ckpt-19459.meta output/models/model.ckpt-31078.meta. output/models/model.ckpt-1000.meta output/models/model.ckpt-2000.meta output/models/model.ckpt-34008.meta. output/models/model.ckpt-10674.meta output/models/model.ckpt-22355.meta output/models/model.ckpt-4814.meta. output/models/model.ckpt-13613.meta output/models/model.ckpt-25257.meta output/models/model.ckpt-7724.meta. output/models/model.ckpt-16546.meta output/models/model.ckpt-28168.meta. (dv_venv) [anovak@phoenix-01 trash]$ ls output/models/*metrics. output/models/best_checkpoint.metrics output/models/model.ckpt-28168.metrics output/models/model.ckpt-34008.metrics. output/models/current.metrics output/models/model.ckpt-31078.metrics. ```. But `model_eval` just sits there like this (until a new checkpoint appears):. ```. I0210 17:42:06.700287 139846137329472 checkpoint_utils.py:140] Waiting for new checkpoint at /public/groups/cgl/graph-genomes/anovak/trash/output/models. ```. How do I ge",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/611
https://github.com/google/deepvariant/issues/611:2607,security,model,model,2607,"U as training was using, so it kept immediately running out of GPU memory. After about half an hour I hit on the right syntax to give it a distinct GPU, and managed to actually start `model_eval`:. ```. docker run --rm --gpus '""device=1""' \. -v ""${DATA_DIR}:${DATA_DIR}"" \. -v ""${OUTPUT_DIR}:${OUTPUT_DIR}"" \. -u $(id -u):$(id -g) \. google/deepvariant:1.4.0-gpu \. /opt/deepvariant/bin/model_eval \. --dataset_config_pbtxt=""${OUTPUT_DIR}/validation_set.dataset_config.pbtxt"" \. --checkpoint_dir=""${TRAINING_DIR}"" \. --batch_size=512 \. --min_eval_interval_s=1 \. --eval_timeout=1000. ```. It evaluated the most recent checkpoint saves as of when it started up, and it is evaluating new checkpoints as they are created. But it doesn't seem interested in going back and evaluating all the checkpoints, other than the initial-latest one, that were created before it started. The directory in question looks like this:. ```. (dv_venv) [anovak@phoenix-01 trash]$ ls output/models/*meta. output/models/model.ckpt-0.meta output/models/model.ckpt-19459.meta output/models/model.ckpt-31078.meta. output/models/model.ckpt-1000.meta output/models/model.ckpt-2000.meta output/models/model.ckpt-34008.meta. output/models/model.ckpt-10674.meta output/models/model.ckpt-22355.meta output/models/model.ckpt-4814.meta. output/models/model.ckpt-13613.meta output/models/model.ckpt-25257.meta output/models/model.ckpt-7724.meta. output/models/model.ckpt-16546.meta output/models/model.ckpt-28168.meta. (dv_venv) [anovak@phoenix-01 trash]$ ls output/models/*metrics. output/models/best_checkpoint.metrics output/models/model.ckpt-28168.metrics output/models/model.ckpt-34008.metrics. output/models/current.metrics output/models/model.ckpt-31078.metrics. ```. But `model_eval` just sits there like this (until a new checkpoint appears):. ```. I0210 17:42:06.700287 139846137329472 checkpoint_utils.py:140] Waiting for new checkpoint at /public/groups/cgl/graph-genomes/anovak/trash/output/models. ```. How do I get the m",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/611
https://github.com/google/deepvariant/issues/611:2632,security,model,models,2632,"so it kept immediately running out of GPU memory. After about half an hour I hit on the right syntax to give it a distinct GPU, and managed to actually start `model_eval`:. ```. docker run --rm --gpus '""device=1""' \. -v ""${DATA_DIR}:${DATA_DIR}"" \. -v ""${OUTPUT_DIR}:${OUTPUT_DIR}"" \. -u $(id -u):$(id -g) \. google/deepvariant:1.4.0-gpu \. /opt/deepvariant/bin/model_eval \. --dataset_config_pbtxt=""${OUTPUT_DIR}/validation_set.dataset_config.pbtxt"" \. --checkpoint_dir=""${TRAINING_DIR}"" \. --batch_size=512 \. --min_eval_interval_s=1 \. --eval_timeout=1000. ```. It evaluated the most recent checkpoint saves as of when it started up, and it is evaluating new checkpoints as they are created. But it doesn't seem interested in going back and evaluating all the checkpoints, other than the initial-latest one, that were created before it started. The directory in question looks like this:. ```. (dv_venv) [anovak@phoenix-01 trash]$ ls output/models/*meta. output/models/model.ckpt-0.meta output/models/model.ckpt-19459.meta output/models/model.ckpt-31078.meta. output/models/model.ckpt-1000.meta output/models/model.ckpt-2000.meta output/models/model.ckpt-34008.meta. output/models/model.ckpt-10674.meta output/models/model.ckpt-22355.meta output/models/model.ckpt-4814.meta. output/models/model.ckpt-13613.meta output/models/model.ckpt-25257.meta output/models/model.ckpt-7724.meta. output/models/model.ckpt-16546.meta output/models/model.ckpt-28168.meta. (dv_venv) [anovak@phoenix-01 trash]$ ls output/models/*metrics. output/models/best_checkpoint.metrics output/models/model.ckpt-28168.metrics output/models/model.ckpt-34008.metrics. output/models/current.metrics output/models/model.ckpt-31078.metrics. ```. But `model_eval` just sits there like this (until a new checkpoint appears):. ```. I0210 17:42:06.700287 139846137329472 checkpoint_utils.py:140] Waiting for new checkpoint at /public/groups/cgl/graph-genomes/anovak/trash/output/models. ```. How do I get the missing `*metrics` files a",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/611
https://github.com/google/deepvariant/issues/611:2639,security,model,model,2639,"ept immediately running out of GPU memory. After about half an hour I hit on the right syntax to give it a distinct GPU, and managed to actually start `model_eval`:. ```. docker run --rm --gpus '""device=1""' \. -v ""${DATA_DIR}:${DATA_DIR}"" \. -v ""${OUTPUT_DIR}:${OUTPUT_DIR}"" \. -u $(id -u):$(id -g) \. google/deepvariant:1.4.0-gpu \. /opt/deepvariant/bin/model_eval \. --dataset_config_pbtxt=""${OUTPUT_DIR}/validation_set.dataset_config.pbtxt"" \. --checkpoint_dir=""${TRAINING_DIR}"" \. --batch_size=512 \. --min_eval_interval_s=1 \. --eval_timeout=1000. ```. It evaluated the most recent checkpoint saves as of when it started up, and it is evaluating new checkpoints as they are created. But it doesn't seem interested in going back and evaluating all the checkpoints, other than the initial-latest one, that were created before it started. The directory in question looks like this:. ```. (dv_venv) [anovak@phoenix-01 trash]$ ls output/models/*meta. output/models/model.ckpt-0.meta output/models/model.ckpt-19459.meta output/models/model.ckpt-31078.meta. output/models/model.ckpt-1000.meta output/models/model.ckpt-2000.meta output/models/model.ckpt-34008.meta. output/models/model.ckpt-10674.meta output/models/model.ckpt-22355.meta output/models/model.ckpt-4814.meta. output/models/model.ckpt-13613.meta output/models/model.ckpt-25257.meta output/models/model.ckpt-7724.meta. output/models/model.ckpt-16546.meta output/models/model.ckpt-28168.meta. (dv_venv) [anovak@phoenix-01 trash]$ ls output/models/*metrics. output/models/best_checkpoint.metrics output/models/model.ckpt-28168.metrics output/models/model.ckpt-34008.metrics. output/models/current.metrics output/models/model.ckpt-31078.metrics. ```. But `model_eval` just sits there like this (until a new checkpoint appears):. ```. I0210 17:42:06.700287 139846137329472 checkpoint_utils.py:140] Waiting for new checkpoint at /public/groups/cgl/graph-genomes/anovak/trash/output/models. ```. How do I get the missing `*metrics` files and dete",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/611
https://github.com/google/deepvariant/issues/611:2668,security,model,models,2668,"f GPU memory. After about half an hour I hit on the right syntax to give it a distinct GPU, and managed to actually start `model_eval`:. ```. docker run --rm --gpus '""device=1""' \. -v ""${DATA_DIR}:${DATA_DIR}"" \. -v ""${OUTPUT_DIR}:${OUTPUT_DIR}"" \. -u $(id -u):$(id -g) \. google/deepvariant:1.4.0-gpu \. /opt/deepvariant/bin/model_eval \. --dataset_config_pbtxt=""${OUTPUT_DIR}/validation_set.dataset_config.pbtxt"" \. --checkpoint_dir=""${TRAINING_DIR}"" \. --batch_size=512 \. --min_eval_interval_s=1 \. --eval_timeout=1000. ```. It evaluated the most recent checkpoint saves as of when it started up, and it is evaluating new checkpoints as they are created. But it doesn't seem interested in going back and evaluating all the checkpoints, other than the initial-latest one, that were created before it started. The directory in question looks like this:. ```. (dv_venv) [anovak@phoenix-01 trash]$ ls output/models/*meta. output/models/model.ckpt-0.meta output/models/model.ckpt-19459.meta output/models/model.ckpt-31078.meta. output/models/model.ckpt-1000.meta output/models/model.ckpt-2000.meta output/models/model.ckpt-34008.meta. output/models/model.ckpt-10674.meta output/models/model.ckpt-22355.meta output/models/model.ckpt-4814.meta. output/models/model.ckpt-13613.meta output/models/model.ckpt-25257.meta output/models/model.ckpt-7724.meta. output/models/model.ckpt-16546.meta output/models/model.ckpt-28168.meta. (dv_venv) [anovak@phoenix-01 trash]$ ls output/models/*metrics. output/models/best_checkpoint.metrics output/models/model.ckpt-28168.metrics output/models/model.ckpt-34008.metrics. output/models/current.metrics output/models/model.ckpt-31078.metrics. ```. But `model_eval` just sits there like this (until a new checkpoint appears):. ```. I0210 17:42:06.700287 139846137329472 checkpoint_utils.py:140] Waiting for new checkpoint at /public/groups/cgl/graph-genomes/anovak/trash/output/models. ```. How do I get the missing `*metrics` files and determine if any of the checkpoin",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/611
https://github.com/google/deepvariant/issues/611:2675,security,model,model,2675,"emory. After about half an hour I hit on the right syntax to give it a distinct GPU, and managed to actually start `model_eval`:. ```. docker run --rm --gpus '""device=1""' \. -v ""${DATA_DIR}:${DATA_DIR}"" \. -v ""${OUTPUT_DIR}:${OUTPUT_DIR}"" \. -u $(id -u):$(id -g) \. google/deepvariant:1.4.0-gpu \. /opt/deepvariant/bin/model_eval \. --dataset_config_pbtxt=""${OUTPUT_DIR}/validation_set.dataset_config.pbtxt"" \. --checkpoint_dir=""${TRAINING_DIR}"" \. --batch_size=512 \. --min_eval_interval_s=1 \. --eval_timeout=1000. ```. It evaluated the most recent checkpoint saves as of when it started up, and it is evaluating new checkpoints as they are created. But it doesn't seem interested in going back and evaluating all the checkpoints, other than the initial-latest one, that were created before it started. The directory in question looks like this:. ```. (dv_venv) [anovak@phoenix-01 trash]$ ls output/models/*meta. output/models/model.ckpt-0.meta output/models/model.ckpt-19459.meta output/models/model.ckpt-31078.meta. output/models/model.ckpt-1000.meta output/models/model.ckpt-2000.meta output/models/model.ckpt-34008.meta. output/models/model.ckpt-10674.meta output/models/model.ckpt-22355.meta output/models/model.ckpt-4814.meta. output/models/model.ckpt-13613.meta output/models/model.ckpt-25257.meta output/models/model.ckpt-7724.meta. output/models/model.ckpt-16546.meta output/models/model.ckpt-28168.meta. (dv_venv) [anovak@phoenix-01 trash]$ ls output/models/*metrics. output/models/best_checkpoint.metrics output/models/model.ckpt-28168.metrics output/models/model.ckpt-34008.metrics. output/models/current.metrics output/models/model.ckpt-31078.metrics. ```. But `model_eval` just sits there like this (until a new checkpoint appears):. ```. I0210 17:42:06.700287 139846137329472 checkpoint_utils.py:140] Waiting for new checkpoint at /public/groups/cgl/graph-genomes/anovak/trash/output/models. ```. How do I get the missing `*metrics` files and determine if any of the checkpoints that",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/611
https://github.com/google/deepvariant/issues/611:2705,security,model,models,2705,"r I hit on the right syntax to give it a distinct GPU, and managed to actually start `model_eval`:. ```. docker run --rm --gpus '""device=1""' \. -v ""${DATA_DIR}:${DATA_DIR}"" \. -v ""${OUTPUT_DIR}:${OUTPUT_DIR}"" \. -u $(id -u):$(id -g) \. google/deepvariant:1.4.0-gpu \. /opt/deepvariant/bin/model_eval \. --dataset_config_pbtxt=""${OUTPUT_DIR}/validation_set.dataset_config.pbtxt"" \. --checkpoint_dir=""${TRAINING_DIR}"" \. --batch_size=512 \. --min_eval_interval_s=1 \. --eval_timeout=1000. ```. It evaluated the most recent checkpoint saves as of when it started up, and it is evaluating new checkpoints as they are created. But it doesn't seem interested in going back and evaluating all the checkpoints, other than the initial-latest one, that were created before it started. The directory in question looks like this:. ```. (dv_venv) [anovak@phoenix-01 trash]$ ls output/models/*meta. output/models/model.ckpt-0.meta output/models/model.ckpt-19459.meta output/models/model.ckpt-31078.meta. output/models/model.ckpt-1000.meta output/models/model.ckpt-2000.meta output/models/model.ckpt-34008.meta. output/models/model.ckpt-10674.meta output/models/model.ckpt-22355.meta output/models/model.ckpt-4814.meta. output/models/model.ckpt-13613.meta output/models/model.ckpt-25257.meta output/models/model.ckpt-7724.meta. output/models/model.ckpt-16546.meta output/models/model.ckpt-28168.meta. (dv_venv) [anovak@phoenix-01 trash]$ ls output/models/*metrics. output/models/best_checkpoint.metrics output/models/model.ckpt-28168.metrics output/models/model.ckpt-34008.metrics. output/models/current.metrics output/models/model.ckpt-31078.metrics. ```. But `model_eval` just sits there like this (until a new checkpoint appears):. ```. I0210 17:42:06.700287 139846137329472 checkpoint_utils.py:140] Waiting for new checkpoint at /public/groups/cgl/graph-genomes/anovak/trash/output/models. ```. How do I get the missing `*metrics` files and determine if any of the checkpoints that were missed is actually the b",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/611
https://github.com/google/deepvariant/issues/611:2712,security,model,model,2712," on the right syntax to give it a distinct GPU, and managed to actually start `model_eval`:. ```. docker run --rm --gpus '""device=1""' \. -v ""${DATA_DIR}:${DATA_DIR}"" \. -v ""${OUTPUT_DIR}:${OUTPUT_DIR}"" \. -u $(id -u):$(id -g) \. google/deepvariant:1.4.0-gpu \. /opt/deepvariant/bin/model_eval \. --dataset_config_pbtxt=""${OUTPUT_DIR}/validation_set.dataset_config.pbtxt"" \. --checkpoint_dir=""${TRAINING_DIR}"" \. --batch_size=512 \. --min_eval_interval_s=1 \. --eval_timeout=1000. ```. It evaluated the most recent checkpoint saves as of when it started up, and it is evaluating new checkpoints as they are created. But it doesn't seem interested in going back and evaluating all the checkpoints, other than the initial-latest one, that were created before it started. The directory in question looks like this:. ```. (dv_venv) [anovak@phoenix-01 trash]$ ls output/models/*meta. output/models/model.ckpt-0.meta output/models/model.ckpt-19459.meta output/models/model.ckpt-31078.meta. output/models/model.ckpt-1000.meta output/models/model.ckpt-2000.meta output/models/model.ckpt-34008.meta. output/models/model.ckpt-10674.meta output/models/model.ckpt-22355.meta output/models/model.ckpt-4814.meta. output/models/model.ckpt-13613.meta output/models/model.ckpt-25257.meta output/models/model.ckpt-7724.meta. output/models/model.ckpt-16546.meta output/models/model.ckpt-28168.meta. (dv_venv) [anovak@phoenix-01 trash]$ ls output/models/*metrics. output/models/best_checkpoint.metrics output/models/model.ckpt-28168.metrics output/models/model.ckpt-34008.metrics. output/models/current.metrics output/models/model.ckpt-31078.metrics. ```. But `model_eval` just sits there like this (until a new checkpoint appears):. ```. I0210 17:42:06.700287 139846137329472 checkpoint_utils.py:140] Waiting for new checkpoint at /public/groups/cgl/graph-genomes/anovak/trash/output/models. ```. How do I get the missing `*metrics` files and determine if any of the checkpoints that were missed is actually the best one",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/611
https://github.com/google/deepvariant/issues/611:2740,security,model,models,2740," it a distinct GPU, and managed to actually start `model_eval`:. ```. docker run --rm --gpus '""device=1""' \. -v ""${DATA_DIR}:${DATA_DIR}"" \. -v ""${OUTPUT_DIR}:${OUTPUT_DIR}"" \. -u $(id -u):$(id -g) \. google/deepvariant:1.4.0-gpu \. /opt/deepvariant/bin/model_eval \. --dataset_config_pbtxt=""${OUTPUT_DIR}/validation_set.dataset_config.pbtxt"" \. --checkpoint_dir=""${TRAINING_DIR}"" \. --batch_size=512 \. --min_eval_interval_s=1 \. --eval_timeout=1000. ```. It evaluated the most recent checkpoint saves as of when it started up, and it is evaluating new checkpoints as they are created. But it doesn't seem interested in going back and evaluating all the checkpoints, other than the initial-latest one, that were created before it started. The directory in question looks like this:. ```. (dv_venv) [anovak@phoenix-01 trash]$ ls output/models/*meta. output/models/model.ckpt-0.meta output/models/model.ckpt-19459.meta output/models/model.ckpt-31078.meta. output/models/model.ckpt-1000.meta output/models/model.ckpt-2000.meta output/models/model.ckpt-34008.meta. output/models/model.ckpt-10674.meta output/models/model.ckpt-22355.meta output/models/model.ckpt-4814.meta. output/models/model.ckpt-13613.meta output/models/model.ckpt-25257.meta output/models/model.ckpt-7724.meta. output/models/model.ckpt-16546.meta output/models/model.ckpt-28168.meta. (dv_venv) [anovak@phoenix-01 trash]$ ls output/models/*metrics. output/models/best_checkpoint.metrics output/models/model.ckpt-28168.metrics output/models/model.ckpt-34008.metrics. output/models/current.metrics output/models/model.ckpt-31078.metrics. ```. But `model_eval` just sits there like this (until a new checkpoint appears):. ```. I0210 17:42:06.700287 139846137329472 checkpoint_utils.py:140] Waiting for new checkpoint at /public/groups/cgl/graph-genomes/anovak/trash/output/models. ```. How do I get the missing `*metrics` files and determine if any of the checkpoints that were missed is actually the best one? Do I need to `touch` some ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/611
https://github.com/google/deepvariant/issues/611:2747,security,model,model,2747,"istinct GPU, and managed to actually start `model_eval`:. ```. docker run --rm --gpus '""device=1""' \. -v ""${DATA_DIR}:${DATA_DIR}"" \. -v ""${OUTPUT_DIR}:${OUTPUT_DIR}"" \. -u $(id -u):$(id -g) \. google/deepvariant:1.4.0-gpu \. /opt/deepvariant/bin/model_eval \. --dataset_config_pbtxt=""${OUTPUT_DIR}/validation_set.dataset_config.pbtxt"" \. --checkpoint_dir=""${TRAINING_DIR}"" \. --batch_size=512 \. --min_eval_interval_s=1 \. --eval_timeout=1000. ```. It evaluated the most recent checkpoint saves as of when it started up, and it is evaluating new checkpoints as they are created. But it doesn't seem interested in going back and evaluating all the checkpoints, other than the initial-latest one, that were created before it started. The directory in question looks like this:. ```. (dv_venv) [anovak@phoenix-01 trash]$ ls output/models/*meta. output/models/model.ckpt-0.meta output/models/model.ckpt-19459.meta output/models/model.ckpt-31078.meta. output/models/model.ckpt-1000.meta output/models/model.ckpt-2000.meta output/models/model.ckpt-34008.meta. output/models/model.ckpt-10674.meta output/models/model.ckpt-22355.meta output/models/model.ckpt-4814.meta. output/models/model.ckpt-13613.meta output/models/model.ckpt-25257.meta output/models/model.ckpt-7724.meta. output/models/model.ckpt-16546.meta output/models/model.ckpt-28168.meta. (dv_venv) [anovak@phoenix-01 trash]$ ls output/models/*metrics. output/models/best_checkpoint.metrics output/models/model.ckpt-28168.metrics output/models/model.ckpt-34008.metrics. output/models/current.metrics output/models/model.ckpt-31078.metrics. ```. But `model_eval` just sits there like this (until a new checkpoint appears):. ```. I0210 17:42:06.700287 139846137329472 checkpoint_utils.py:140] Waiting for new checkpoint at /public/groups/cgl/graph-genomes/anovak/trash/output/models. ```. How do I get the missing `*metrics` files and determine if any of the checkpoints that were missed is actually the best one? Do I need to `touch` some particu",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/611
https://github.com/google/deepvariant/issues/611:2775,security,model,models,2775,"actually start `model_eval`:. ```. docker run --rm --gpus '""device=1""' \. -v ""${DATA_DIR}:${DATA_DIR}"" \. -v ""${OUTPUT_DIR}:${OUTPUT_DIR}"" \. -u $(id -u):$(id -g) \. google/deepvariant:1.4.0-gpu \. /opt/deepvariant/bin/model_eval \. --dataset_config_pbtxt=""${OUTPUT_DIR}/validation_set.dataset_config.pbtxt"" \. --checkpoint_dir=""${TRAINING_DIR}"" \. --batch_size=512 \. --min_eval_interval_s=1 \. --eval_timeout=1000. ```. It evaluated the most recent checkpoint saves as of when it started up, and it is evaluating new checkpoints as they are created. But it doesn't seem interested in going back and evaluating all the checkpoints, other than the initial-latest one, that were created before it started. The directory in question looks like this:. ```. (dv_venv) [anovak@phoenix-01 trash]$ ls output/models/*meta. output/models/model.ckpt-0.meta output/models/model.ckpt-19459.meta output/models/model.ckpt-31078.meta. output/models/model.ckpt-1000.meta output/models/model.ckpt-2000.meta output/models/model.ckpt-34008.meta. output/models/model.ckpt-10674.meta output/models/model.ckpt-22355.meta output/models/model.ckpt-4814.meta. output/models/model.ckpt-13613.meta output/models/model.ckpt-25257.meta output/models/model.ckpt-7724.meta. output/models/model.ckpt-16546.meta output/models/model.ckpt-28168.meta. (dv_venv) [anovak@phoenix-01 trash]$ ls output/models/*metrics. output/models/best_checkpoint.metrics output/models/model.ckpt-28168.metrics output/models/model.ckpt-34008.metrics. output/models/current.metrics output/models/model.ckpt-31078.metrics. ```. But `model_eval` just sits there like this (until a new checkpoint appears):. ```. I0210 17:42:06.700287 139846137329472 checkpoint_utils.py:140] Waiting for new checkpoint at /public/groups/cgl/graph-genomes/anovak/trash/output/models. ```. How do I get the missing `*metrics` files and determine if any of the checkpoints that were missed is actually the best one? Do I need to `touch` some particular files in the directory t",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/611
https://github.com/google/deepvariant/issues/611:2782,security,model,model,2782,"y start `model_eval`:. ```. docker run --rm --gpus '""device=1""' \. -v ""${DATA_DIR}:${DATA_DIR}"" \. -v ""${OUTPUT_DIR}:${OUTPUT_DIR}"" \. -u $(id -u):$(id -g) \. google/deepvariant:1.4.0-gpu \. /opt/deepvariant/bin/model_eval \. --dataset_config_pbtxt=""${OUTPUT_DIR}/validation_set.dataset_config.pbtxt"" \. --checkpoint_dir=""${TRAINING_DIR}"" \. --batch_size=512 \. --min_eval_interval_s=1 \. --eval_timeout=1000. ```. It evaluated the most recent checkpoint saves as of when it started up, and it is evaluating new checkpoints as they are created. But it doesn't seem interested in going back and evaluating all the checkpoints, other than the initial-latest one, that were created before it started. The directory in question looks like this:. ```. (dv_venv) [anovak@phoenix-01 trash]$ ls output/models/*meta. output/models/model.ckpt-0.meta output/models/model.ckpt-19459.meta output/models/model.ckpt-31078.meta. output/models/model.ckpt-1000.meta output/models/model.ckpt-2000.meta output/models/model.ckpt-34008.meta. output/models/model.ckpt-10674.meta output/models/model.ckpt-22355.meta output/models/model.ckpt-4814.meta. output/models/model.ckpt-13613.meta output/models/model.ckpt-25257.meta output/models/model.ckpt-7724.meta. output/models/model.ckpt-16546.meta output/models/model.ckpt-28168.meta. (dv_venv) [anovak@phoenix-01 trash]$ ls output/models/*metrics. output/models/best_checkpoint.metrics output/models/model.ckpt-28168.metrics output/models/model.ckpt-34008.metrics. output/models/current.metrics output/models/model.ckpt-31078.metrics. ```. But `model_eval` just sits there like this (until a new checkpoint appears):. ```. I0210 17:42:06.700287 139846137329472 checkpoint_utils.py:140] Waiting for new checkpoint at /public/groups/cgl/graph-genomes/anovak/trash/output/models. ```. How do I get the missing `*metrics` files and determine if any of the checkpoints that were missed is actually the best one? Do I need to `touch` some particular files in the directory to get `",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/611
https://github.com/google/deepvariant/issues/611:2812,security,model,models,2812,"cker run --rm --gpus '""device=1""' \. -v ""${DATA_DIR}:${DATA_DIR}"" \. -v ""${OUTPUT_DIR}:${OUTPUT_DIR}"" \. -u $(id -u):$(id -g) \. google/deepvariant:1.4.0-gpu \. /opt/deepvariant/bin/model_eval \. --dataset_config_pbtxt=""${OUTPUT_DIR}/validation_set.dataset_config.pbtxt"" \. --checkpoint_dir=""${TRAINING_DIR}"" \. --batch_size=512 \. --min_eval_interval_s=1 \. --eval_timeout=1000. ```. It evaluated the most recent checkpoint saves as of when it started up, and it is evaluating new checkpoints as they are created. But it doesn't seem interested in going back and evaluating all the checkpoints, other than the initial-latest one, that were created before it started. The directory in question looks like this:. ```. (dv_venv) [anovak@phoenix-01 trash]$ ls output/models/*meta. output/models/model.ckpt-0.meta output/models/model.ckpt-19459.meta output/models/model.ckpt-31078.meta. output/models/model.ckpt-1000.meta output/models/model.ckpt-2000.meta output/models/model.ckpt-34008.meta. output/models/model.ckpt-10674.meta output/models/model.ckpt-22355.meta output/models/model.ckpt-4814.meta. output/models/model.ckpt-13613.meta output/models/model.ckpt-25257.meta output/models/model.ckpt-7724.meta. output/models/model.ckpt-16546.meta output/models/model.ckpt-28168.meta. (dv_venv) [anovak@phoenix-01 trash]$ ls output/models/*metrics. output/models/best_checkpoint.metrics output/models/model.ckpt-28168.metrics output/models/model.ckpt-34008.metrics. output/models/current.metrics output/models/model.ckpt-31078.metrics. ```. But `model_eval` just sits there like this (until a new checkpoint appears):. ```. I0210 17:42:06.700287 139846137329472 checkpoint_utils.py:140] Waiting for new checkpoint at /public/groups/cgl/graph-genomes/anovak/trash/output/models. ```. How do I get the missing `*metrics` files and determine if any of the checkpoints that were missed is actually the best one? Do I need to `touch` some particular files in the directory to get `model_eval` to be interested i",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/611
https://github.com/google/deepvariant/issues/611:2819,security,model,model,2819,"n --rm --gpus '""device=1""' \. -v ""${DATA_DIR}:${DATA_DIR}"" \. -v ""${OUTPUT_DIR}:${OUTPUT_DIR}"" \. -u $(id -u):$(id -g) \. google/deepvariant:1.4.0-gpu \. /opt/deepvariant/bin/model_eval \. --dataset_config_pbtxt=""${OUTPUT_DIR}/validation_set.dataset_config.pbtxt"" \. --checkpoint_dir=""${TRAINING_DIR}"" \. --batch_size=512 \. --min_eval_interval_s=1 \. --eval_timeout=1000. ```. It evaluated the most recent checkpoint saves as of when it started up, and it is evaluating new checkpoints as they are created. But it doesn't seem interested in going back and evaluating all the checkpoints, other than the initial-latest one, that were created before it started. The directory in question looks like this:. ```. (dv_venv) [anovak@phoenix-01 trash]$ ls output/models/*meta. output/models/model.ckpt-0.meta output/models/model.ckpt-19459.meta output/models/model.ckpt-31078.meta. output/models/model.ckpt-1000.meta output/models/model.ckpt-2000.meta output/models/model.ckpt-34008.meta. output/models/model.ckpt-10674.meta output/models/model.ckpt-22355.meta output/models/model.ckpt-4814.meta. output/models/model.ckpt-13613.meta output/models/model.ckpt-25257.meta output/models/model.ckpt-7724.meta. output/models/model.ckpt-16546.meta output/models/model.ckpt-28168.meta. (dv_venv) [anovak@phoenix-01 trash]$ ls output/models/*metrics. output/models/best_checkpoint.metrics output/models/model.ckpt-28168.metrics output/models/model.ckpt-34008.metrics. output/models/current.metrics output/models/model.ckpt-31078.metrics. ```. But `model_eval` just sits there like this (until a new checkpoint appears):. ```. I0210 17:42:06.700287 139846137329472 checkpoint_utils.py:140] Waiting for new checkpoint at /public/groups/cgl/graph-genomes/anovak/trash/output/models. ```. How do I get the missing `*metrics` files and determine if any of the checkpoints that were missed is actually the best one? Do I need to `touch` some particular files in the directory to get `model_eval` to be interested in them?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/611
https://github.com/google/deepvariant/issues/611:2848,security,model,models,2848," -v ""${DATA_DIR}:${DATA_DIR}"" \. -v ""${OUTPUT_DIR}:${OUTPUT_DIR}"" \. -u $(id -u):$(id -g) \. google/deepvariant:1.4.0-gpu \. /opt/deepvariant/bin/model_eval \. --dataset_config_pbtxt=""${OUTPUT_DIR}/validation_set.dataset_config.pbtxt"" \. --checkpoint_dir=""${TRAINING_DIR}"" \. --batch_size=512 \. --min_eval_interval_s=1 \. --eval_timeout=1000. ```. It evaluated the most recent checkpoint saves as of when it started up, and it is evaluating new checkpoints as they are created. But it doesn't seem interested in going back and evaluating all the checkpoints, other than the initial-latest one, that were created before it started. The directory in question looks like this:. ```. (dv_venv) [anovak@phoenix-01 trash]$ ls output/models/*meta. output/models/model.ckpt-0.meta output/models/model.ckpt-19459.meta output/models/model.ckpt-31078.meta. output/models/model.ckpt-1000.meta output/models/model.ckpt-2000.meta output/models/model.ckpt-34008.meta. output/models/model.ckpt-10674.meta output/models/model.ckpt-22355.meta output/models/model.ckpt-4814.meta. output/models/model.ckpt-13613.meta output/models/model.ckpt-25257.meta output/models/model.ckpt-7724.meta. output/models/model.ckpt-16546.meta output/models/model.ckpt-28168.meta. (dv_venv) [anovak@phoenix-01 trash]$ ls output/models/*metrics. output/models/best_checkpoint.metrics output/models/model.ckpt-28168.metrics output/models/model.ckpt-34008.metrics. output/models/current.metrics output/models/model.ckpt-31078.metrics. ```. But `model_eval` just sits there like this (until a new checkpoint appears):. ```. I0210 17:42:06.700287 139846137329472 checkpoint_utils.py:140] Waiting for new checkpoint at /public/groups/cgl/graph-genomes/anovak/trash/output/models. ```. How do I get the missing `*metrics` files and determine if any of the checkpoints that were missed is actually the best one? Do I need to `touch` some particular files in the directory to get `model_eval` to be interested in them? Is there some other command ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/611
https://github.com/google/deepvariant/issues/611:2855,security,model,model,2855,"DATA_DIR}:${DATA_DIR}"" \. -v ""${OUTPUT_DIR}:${OUTPUT_DIR}"" \. -u $(id -u):$(id -g) \. google/deepvariant:1.4.0-gpu \. /opt/deepvariant/bin/model_eval \. --dataset_config_pbtxt=""${OUTPUT_DIR}/validation_set.dataset_config.pbtxt"" \. --checkpoint_dir=""${TRAINING_DIR}"" \. --batch_size=512 \. --min_eval_interval_s=1 \. --eval_timeout=1000. ```. It evaluated the most recent checkpoint saves as of when it started up, and it is evaluating new checkpoints as they are created. But it doesn't seem interested in going back and evaluating all the checkpoints, other than the initial-latest one, that were created before it started. The directory in question looks like this:. ```. (dv_venv) [anovak@phoenix-01 trash]$ ls output/models/*meta. output/models/model.ckpt-0.meta output/models/model.ckpt-19459.meta output/models/model.ckpt-31078.meta. output/models/model.ckpt-1000.meta output/models/model.ckpt-2000.meta output/models/model.ckpt-34008.meta. output/models/model.ckpt-10674.meta output/models/model.ckpt-22355.meta output/models/model.ckpt-4814.meta. output/models/model.ckpt-13613.meta output/models/model.ckpt-25257.meta output/models/model.ckpt-7724.meta. output/models/model.ckpt-16546.meta output/models/model.ckpt-28168.meta. (dv_venv) [anovak@phoenix-01 trash]$ ls output/models/*metrics. output/models/best_checkpoint.metrics output/models/model.ckpt-28168.metrics output/models/model.ckpt-34008.metrics. output/models/current.metrics output/models/model.ckpt-31078.metrics. ```. But `model_eval` just sits there like this (until a new checkpoint appears):. ```. I0210 17:42:06.700287 139846137329472 checkpoint_utils.py:140] Waiting for new checkpoint at /public/groups/cgl/graph-genomes/anovak/trash/output/models. ```. How do I get the missing `*metrics` files and determine if any of the checkpoints that were missed is actually the best one? Do I need to `touch` some particular files in the directory to get `model_eval` to be interested in them? Is there some other command besides",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/611
https://github.com/google/deepvariant/issues/611:2884,security,model,models,2884,"""${OUTPUT_DIR}:${OUTPUT_DIR}"" \. -u $(id -u):$(id -g) \. google/deepvariant:1.4.0-gpu \. /opt/deepvariant/bin/model_eval \. --dataset_config_pbtxt=""${OUTPUT_DIR}/validation_set.dataset_config.pbtxt"" \. --checkpoint_dir=""${TRAINING_DIR}"" \. --batch_size=512 \. --min_eval_interval_s=1 \. --eval_timeout=1000. ```. It evaluated the most recent checkpoint saves as of when it started up, and it is evaluating new checkpoints as they are created. But it doesn't seem interested in going back and evaluating all the checkpoints, other than the initial-latest one, that were created before it started. The directory in question looks like this:. ```. (dv_venv) [anovak@phoenix-01 trash]$ ls output/models/*meta. output/models/model.ckpt-0.meta output/models/model.ckpt-19459.meta output/models/model.ckpt-31078.meta. output/models/model.ckpt-1000.meta output/models/model.ckpt-2000.meta output/models/model.ckpt-34008.meta. output/models/model.ckpt-10674.meta output/models/model.ckpt-22355.meta output/models/model.ckpt-4814.meta. output/models/model.ckpt-13613.meta output/models/model.ckpt-25257.meta output/models/model.ckpt-7724.meta. output/models/model.ckpt-16546.meta output/models/model.ckpt-28168.meta. (dv_venv) [anovak@phoenix-01 trash]$ ls output/models/*metrics. output/models/best_checkpoint.metrics output/models/model.ckpt-28168.metrics output/models/model.ckpt-34008.metrics. output/models/current.metrics output/models/model.ckpt-31078.metrics. ```. But `model_eval` just sits there like this (until a new checkpoint appears):. ```. I0210 17:42:06.700287 139846137329472 checkpoint_utils.py:140] Waiting for new checkpoint at /public/groups/cgl/graph-genomes/anovak/trash/output/models. ```. How do I get the missing `*metrics` files and determine if any of the checkpoints that were missed is actually the best one? Do I need to `touch` some particular files in the directory to get `model_eval` to be interested in them? Is there some other command besides `model_eval` that can proces",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/611
https://github.com/google/deepvariant/issues/611:2891,security,model,model,2891,"UT_DIR}:${OUTPUT_DIR}"" \. -u $(id -u):$(id -g) \. google/deepvariant:1.4.0-gpu \. /opt/deepvariant/bin/model_eval \. --dataset_config_pbtxt=""${OUTPUT_DIR}/validation_set.dataset_config.pbtxt"" \. --checkpoint_dir=""${TRAINING_DIR}"" \. --batch_size=512 \. --min_eval_interval_s=1 \. --eval_timeout=1000. ```. It evaluated the most recent checkpoint saves as of when it started up, and it is evaluating new checkpoints as they are created. But it doesn't seem interested in going back and evaluating all the checkpoints, other than the initial-latest one, that were created before it started. The directory in question looks like this:. ```. (dv_venv) [anovak@phoenix-01 trash]$ ls output/models/*meta. output/models/model.ckpt-0.meta output/models/model.ckpt-19459.meta output/models/model.ckpt-31078.meta. output/models/model.ckpt-1000.meta output/models/model.ckpt-2000.meta output/models/model.ckpt-34008.meta. output/models/model.ckpt-10674.meta output/models/model.ckpt-22355.meta output/models/model.ckpt-4814.meta. output/models/model.ckpt-13613.meta output/models/model.ckpt-25257.meta output/models/model.ckpt-7724.meta. output/models/model.ckpt-16546.meta output/models/model.ckpt-28168.meta. (dv_venv) [anovak@phoenix-01 trash]$ ls output/models/*metrics. output/models/best_checkpoint.metrics output/models/model.ckpt-28168.metrics output/models/model.ckpt-34008.metrics. output/models/current.metrics output/models/model.ckpt-31078.metrics. ```. But `model_eval` just sits there like this (until a new checkpoint appears):. ```. I0210 17:42:06.700287 139846137329472 checkpoint_utils.py:140] Waiting for new checkpoint at /public/groups/cgl/graph-genomes/anovak/trash/output/models. ```. How do I get the missing `*metrics` files and determine if any of the checkpoints that were missed is actually the best one? Do I need to `touch` some particular files in the directory to get `model_eval` to be interested in them? Is there some other command besides `model_eval` that can process a sin",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/611
https://github.com/google/deepvariant/issues/611:2920,security,model,models,2920,"$(id -u):$(id -g) \. google/deepvariant:1.4.0-gpu \. /opt/deepvariant/bin/model_eval \. --dataset_config_pbtxt=""${OUTPUT_DIR}/validation_set.dataset_config.pbtxt"" \. --checkpoint_dir=""${TRAINING_DIR}"" \. --batch_size=512 \. --min_eval_interval_s=1 \. --eval_timeout=1000. ```. It evaluated the most recent checkpoint saves as of when it started up, and it is evaluating new checkpoints as they are created. But it doesn't seem interested in going back and evaluating all the checkpoints, other than the initial-latest one, that were created before it started. The directory in question looks like this:. ```. (dv_venv) [anovak@phoenix-01 trash]$ ls output/models/*meta. output/models/model.ckpt-0.meta output/models/model.ckpt-19459.meta output/models/model.ckpt-31078.meta. output/models/model.ckpt-1000.meta output/models/model.ckpt-2000.meta output/models/model.ckpt-34008.meta. output/models/model.ckpt-10674.meta output/models/model.ckpt-22355.meta output/models/model.ckpt-4814.meta. output/models/model.ckpt-13613.meta output/models/model.ckpt-25257.meta output/models/model.ckpt-7724.meta. output/models/model.ckpt-16546.meta output/models/model.ckpt-28168.meta. (dv_venv) [anovak@phoenix-01 trash]$ ls output/models/*metrics. output/models/best_checkpoint.metrics output/models/model.ckpt-28168.metrics output/models/model.ckpt-34008.metrics. output/models/current.metrics output/models/model.ckpt-31078.metrics. ```. But `model_eval` just sits there like this (until a new checkpoint appears):. ```. I0210 17:42:06.700287 139846137329472 checkpoint_utils.py:140] Waiting for new checkpoint at /public/groups/cgl/graph-genomes/anovak/trash/output/models. ```. How do I get the missing `*metrics` files and determine if any of the checkpoints that were missed is actually the best one? Do I need to `touch` some particular files in the directory to get `model_eval` to be interested in them? Is there some other command besides `model_eval` that can process a single particular checkpoint at ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/611
https://github.com/google/deepvariant/issues/611:2927,security,model,model,2927,"):$(id -g) \. google/deepvariant:1.4.0-gpu \. /opt/deepvariant/bin/model_eval \. --dataset_config_pbtxt=""${OUTPUT_DIR}/validation_set.dataset_config.pbtxt"" \. --checkpoint_dir=""${TRAINING_DIR}"" \. --batch_size=512 \. --min_eval_interval_s=1 \. --eval_timeout=1000. ```. It evaluated the most recent checkpoint saves as of when it started up, and it is evaluating new checkpoints as they are created. But it doesn't seem interested in going back and evaluating all the checkpoints, other than the initial-latest one, that were created before it started. The directory in question looks like this:. ```. (dv_venv) [anovak@phoenix-01 trash]$ ls output/models/*meta. output/models/model.ckpt-0.meta output/models/model.ckpt-19459.meta output/models/model.ckpt-31078.meta. output/models/model.ckpt-1000.meta output/models/model.ckpt-2000.meta output/models/model.ckpt-34008.meta. output/models/model.ckpt-10674.meta output/models/model.ckpt-22355.meta output/models/model.ckpt-4814.meta. output/models/model.ckpt-13613.meta output/models/model.ckpt-25257.meta output/models/model.ckpt-7724.meta. output/models/model.ckpt-16546.meta output/models/model.ckpt-28168.meta. (dv_venv) [anovak@phoenix-01 trash]$ ls output/models/*metrics. output/models/best_checkpoint.metrics output/models/model.ckpt-28168.metrics output/models/model.ckpt-34008.metrics. output/models/current.metrics output/models/model.ckpt-31078.metrics. ```. But `model_eval` just sits there like this (until a new checkpoint appears):. ```. I0210 17:42:06.700287 139846137329472 checkpoint_utils.py:140] Waiting for new checkpoint at /public/groups/cgl/graph-genomes/anovak/trash/output/models. ```. How do I get the missing `*metrics` files and determine if any of the checkpoints that were missed is actually the best one? Do I need to `touch` some particular files in the directory to get `model_eval` to be interested in them? Is there some other command besides `model_eval` that can process a single particular checkpoint at a time?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/611
https://github.com/google/deepvariant/issues/611:2956,security,model,models,2956,"ant:1.4.0-gpu \. /opt/deepvariant/bin/model_eval \. --dataset_config_pbtxt=""${OUTPUT_DIR}/validation_set.dataset_config.pbtxt"" \. --checkpoint_dir=""${TRAINING_DIR}"" \. --batch_size=512 \. --min_eval_interval_s=1 \. --eval_timeout=1000. ```. It evaluated the most recent checkpoint saves as of when it started up, and it is evaluating new checkpoints as they are created. But it doesn't seem interested in going back and evaluating all the checkpoints, other than the initial-latest one, that were created before it started. The directory in question looks like this:. ```. (dv_venv) [anovak@phoenix-01 trash]$ ls output/models/*meta. output/models/model.ckpt-0.meta output/models/model.ckpt-19459.meta output/models/model.ckpt-31078.meta. output/models/model.ckpt-1000.meta output/models/model.ckpt-2000.meta output/models/model.ckpt-34008.meta. output/models/model.ckpt-10674.meta output/models/model.ckpt-22355.meta output/models/model.ckpt-4814.meta. output/models/model.ckpt-13613.meta output/models/model.ckpt-25257.meta output/models/model.ckpt-7724.meta. output/models/model.ckpt-16546.meta output/models/model.ckpt-28168.meta. (dv_venv) [anovak@phoenix-01 trash]$ ls output/models/*metrics. output/models/best_checkpoint.metrics output/models/model.ckpt-28168.metrics output/models/model.ckpt-34008.metrics. output/models/current.metrics output/models/model.ckpt-31078.metrics. ```. But `model_eval` just sits there like this (until a new checkpoint appears):. ```. I0210 17:42:06.700287 139846137329472 checkpoint_utils.py:140] Waiting for new checkpoint at /public/groups/cgl/graph-genomes/anovak/trash/output/models. ```. How do I get the missing `*metrics` files and determine if any of the checkpoints that were missed is actually the best one? Do I need to `touch` some particular files in the directory to get `model_eval` to be interested in them? Is there some other command besides `model_eval` that can process a single particular checkpoint at a time? **Does the quick start test ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/611
https://github.com/google/deepvariant/issues/611:2963,security,model,model,2963,".0-gpu \. /opt/deepvariant/bin/model_eval \. --dataset_config_pbtxt=""${OUTPUT_DIR}/validation_set.dataset_config.pbtxt"" \. --checkpoint_dir=""${TRAINING_DIR}"" \. --batch_size=512 \. --min_eval_interval_s=1 \. --eval_timeout=1000. ```. It evaluated the most recent checkpoint saves as of when it started up, and it is evaluating new checkpoints as they are created. But it doesn't seem interested in going back and evaluating all the checkpoints, other than the initial-latest one, that were created before it started. The directory in question looks like this:. ```. (dv_venv) [anovak@phoenix-01 trash]$ ls output/models/*meta. output/models/model.ckpt-0.meta output/models/model.ckpt-19459.meta output/models/model.ckpt-31078.meta. output/models/model.ckpt-1000.meta output/models/model.ckpt-2000.meta output/models/model.ckpt-34008.meta. output/models/model.ckpt-10674.meta output/models/model.ckpt-22355.meta output/models/model.ckpt-4814.meta. output/models/model.ckpt-13613.meta output/models/model.ckpt-25257.meta output/models/model.ckpt-7724.meta. output/models/model.ckpt-16546.meta output/models/model.ckpt-28168.meta. (dv_venv) [anovak@phoenix-01 trash]$ ls output/models/*metrics. output/models/best_checkpoint.metrics output/models/model.ckpt-28168.metrics output/models/model.ckpt-34008.metrics. output/models/current.metrics output/models/model.ckpt-31078.metrics. ```. But `model_eval` just sits there like this (until a new checkpoint appears):. ```. I0210 17:42:06.700287 139846137329472 checkpoint_utils.py:140] Waiting for new checkpoint at /public/groups/cgl/graph-genomes/anovak/trash/output/models. ```. How do I get the missing `*metrics` files and determine if any of the checkpoints that were missed is actually the best one? Do I need to `touch` some particular files in the directory to get `model_eval` to be interested in them? Is there some other command besides `model_eval` that can process a single particular checkpoint at a time? **Does the quick start test work on",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/611
https://github.com/google/deepvariant/issues/611:2992,security,model,models,2992,"n/model_eval \. --dataset_config_pbtxt=""${OUTPUT_DIR}/validation_set.dataset_config.pbtxt"" \. --checkpoint_dir=""${TRAINING_DIR}"" \. --batch_size=512 \. --min_eval_interval_s=1 \. --eval_timeout=1000. ```. It evaluated the most recent checkpoint saves as of when it started up, and it is evaluating new checkpoints as they are created. But it doesn't seem interested in going back and evaluating all the checkpoints, other than the initial-latest one, that were created before it started. The directory in question looks like this:. ```. (dv_venv) [anovak@phoenix-01 trash]$ ls output/models/*meta. output/models/model.ckpt-0.meta output/models/model.ckpt-19459.meta output/models/model.ckpt-31078.meta. output/models/model.ckpt-1000.meta output/models/model.ckpt-2000.meta output/models/model.ckpt-34008.meta. output/models/model.ckpt-10674.meta output/models/model.ckpt-22355.meta output/models/model.ckpt-4814.meta. output/models/model.ckpt-13613.meta output/models/model.ckpt-25257.meta output/models/model.ckpt-7724.meta. output/models/model.ckpt-16546.meta output/models/model.ckpt-28168.meta. (dv_venv) [anovak@phoenix-01 trash]$ ls output/models/*metrics. output/models/best_checkpoint.metrics output/models/model.ckpt-28168.metrics output/models/model.ckpt-34008.metrics. output/models/current.metrics output/models/model.ckpt-31078.metrics. ```. But `model_eval` just sits there like this (until a new checkpoint appears):. ```. I0210 17:42:06.700287 139846137329472 checkpoint_utils.py:140] Waiting for new checkpoint at /public/groups/cgl/graph-genomes/anovak/trash/output/models. ```. How do I get the missing `*metrics` files and determine if any of the checkpoints that were missed is actually the best one? Do I need to `touch` some particular files in the directory to get `model_eval` to be interested in them? Is there some other command besides `model_eval` that can process a single particular checkpoint at a time? **Does the quick start test work on your system?**. Please test ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/611
https://github.com/google/deepvariant/issues/611:2999,security,model,model,2999,"_eval \. --dataset_config_pbtxt=""${OUTPUT_DIR}/validation_set.dataset_config.pbtxt"" \. --checkpoint_dir=""${TRAINING_DIR}"" \. --batch_size=512 \. --min_eval_interval_s=1 \. --eval_timeout=1000. ```. It evaluated the most recent checkpoint saves as of when it started up, and it is evaluating new checkpoints as they are created. But it doesn't seem interested in going back and evaluating all the checkpoints, other than the initial-latest one, that were created before it started. The directory in question looks like this:. ```. (dv_venv) [anovak@phoenix-01 trash]$ ls output/models/*meta. output/models/model.ckpt-0.meta output/models/model.ckpt-19459.meta output/models/model.ckpt-31078.meta. output/models/model.ckpt-1000.meta output/models/model.ckpt-2000.meta output/models/model.ckpt-34008.meta. output/models/model.ckpt-10674.meta output/models/model.ckpt-22355.meta output/models/model.ckpt-4814.meta. output/models/model.ckpt-13613.meta output/models/model.ckpt-25257.meta output/models/model.ckpt-7724.meta. output/models/model.ckpt-16546.meta output/models/model.ckpt-28168.meta. (dv_venv) [anovak@phoenix-01 trash]$ ls output/models/*metrics. output/models/best_checkpoint.metrics output/models/model.ckpt-28168.metrics output/models/model.ckpt-34008.metrics. output/models/current.metrics output/models/model.ckpt-31078.metrics. ```. But `model_eval` just sits there like this (until a new checkpoint appears):. ```. I0210 17:42:06.700287 139846137329472 checkpoint_utils.py:140] Waiting for new checkpoint at /public/groups/cgl/graph-genomes/anovak/trash/output/models. ```. How do I get the missing `*metrics` files and determine if any of the checkpoints that were missed is actually the best one? Do I need to `touch` some particular files in the directory to get `model_eval` to be interested in them? Is there some other command besides `model_eval` that can process a single particular checkpoint at a time? **Does the quick start test work on your system?**. Please test with ht",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/611
https://github.com/google/deepvariant/issues/611:3028,security,model,models,3028,"xt=""${OUTPUT_DIR}/validation_set.dataset_config.pbtxt"" \. --checkpoint_dir=""${TRAINING_DIR}"" \. --batch_size=512 \. --min_eval_interval_s=1 \. --eval_timeout=1000. ```. It evaluated the most recent checkpoint saves as of when it started up, and it is evaluating new checkpoints as they are created. But it doesn't seem interested in going back and evaluating all the checkpoints, other than the initial-latest one, that were created before it started. The directory in question looks like this:. ```. (dv_venv) [anovak@phoenix-01 trash]$ ls output/models/*meta. output/models/model.ckpt-0.meta output/models/model.ckpt-19459.meta output/models/model.ckpt-31078.meta. output/models/model.ckpt-1000.meta output/models/model.ckpt-2000.meta output/models/model.ckpt-34008.meta. output/models/model.ckpt-10674.meta output/models/model.ckpt-22355.meta output/models/model.ckpt-4814.meta. output/models/model.ckpt-13613.meta output/models/model.ckpt-25257.meta output/models/model.ckpt-7724.meta. output/models/model.ckpt-16546.meta output/models/model.ckpt-28168.meta. (dv_venv) [anovak@phoenix-01 trash]$ ls output/models/*metrics. output/models/best_checkpoint.metrics output/models/model.ckpt-28168.metrics output/models/model.ckpt-34008.metrics. output/models/current.metrics output/models/model.ckpt-31078.metrics. ```. But `model_eval` just sits there like this (until a new checkpoint appears):. ```. I0210 17:42:06.700287 139846137329472 checkpoint_utils.py:140] Waiting for new checkpoint at /public/groups/cgl/graph-genomes/anovak/trash/output/models. ```. How do I get the missing `*metrics` files and determine if any of the checkpoints that were missed is actually the best one? Do I need to `touch` some particular files in the directory to get `model_eval` to be interested in them? Is there some other command besides `model_eval` that can process a single particular checkpoint at a time? **Does the quick start test work on your system?**. Please test with https://github.com/google/deepv",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/611
https://github.com/google/deepvariant/issues/611:3035,security,model,model,3035,"UTPUT_DIR}/validation_set.dataset_config.pbtxt"" \. --checkpoint_dir=""${TRAINING_DIR}"" \. --batch_size=512 \. --min_eval_interval_s=1 \. --eval_timeout=1000. ```. It evaluated the most recent checkpoint saves as of when it started up, and it is evaluating new checkpoints as they are created. But it doesn't seem interested in going back and evaluating all the checkpoints, other than the initial-latest one, that were created before it started. The directory in question looks like this:. ```. (dv_venv) [anovak@phoenix-01 trash]$ ls output/models/*meta. output/models/model.ckpt-0.meta output/models/model.ckpt-19459.meta output/models/model.ckpt-31078.meta. output/models/model.ckpt-1000.meta output/models/model.ckpt-2000.meta output/models/model.ckpt-34008.meta. output/models/model.ckpt-10674.meta output/models/model.ckpt-22355.meta output/models/model.ckpt-4814.meta. output/models/model.ckpt-13613.meta output/models/model.ckpt-25257.meta output/models/model.ckpt-7724.meta. output/models/model.ckpt-16546.meta output/models/model.ckpt-28168.meta. (dv_venv) [anovak@phoenix-01 trash]$ ls output/models/*metrics. output/models/best_checkpoint.metrics output/models/model.ckpt-28168.metrics output/models/model.ckpt-34008.metrics. output/models/current.metrics output/models/model.ckpt-31078.metrics. ```. But `model_eval` just sits there like this (until a new checkpoint appears):. ```. I0210 17:42:06.700287 139846137329472 checkpoint_utils.py:140] Waiting for new checkpoint at /public/groups/cgl/graph-genomes/anovak/trash/output/models. ```. How do I get the missing `*metrics` files and determine if any of the checkpoints that were missed is actually the best one? Do I need to `touch` some particular files in the directory to get `model_eval` to be interested in them? Is there some other command besides `model_eval` that can process a single particular checkpoint at a time? **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/611
https://github.com/google/deepvariant/issues/611:3064,security,model,models,3064,"aset_config.pbtxt"" \. --checkpoint_dir=""${TRAINING_DIR}"" \. --batch_size=512 \. --min_eval_interval_s=1 \. --eval_timeout=1000. ```. It evaluated the most recent checkpoint saves as of when it started up, and it is evaluating new checkpoints as they are created. But it doesn't seem interested in going back and evaluating all the checkpoints, other than the initial-latest one, that were created before it started. The directory in question looks like this:. ```. (dv_venv) [anovak@phoenix-01 trash]$ ls output/models/*meta. output/models/model.ckpt-0.meta output/models/model.ckpt-19459.meta output/models/model.ckpt-31078.meta. output/models/model.ckpt-1000.meta output/models/model.ckpt-2000.meta output/models/model.ckpt-34008.meta. output/models/model.ckpt-10674.meta output/models/model.ckpt-22355.meta output/models/model.ckpt-4814.meta. output/models/model.ckpt-13613.meta output/models/model.ckpt-25257.meta output/models/model.ckpt-7724.meta. output/models/model.ckpt-16546.meta output/models/model.ckpt-28168.meta. (dv_venv) [anovak@phoenix-01 trash]$ ls output/models/*metrics. output/models/best_checkpoint.metrics output/models/model.ckpt-28168.metrics output/models/model.ckpt-34008.metrics. output/models/current.metrics output/models/model.ckpt-31078.metrics. ```. But `model_eval` just sits there like this (until a new checkpoint appears):. ```. I0210 17:42:06.700287 139846137329472 checkpoint_utils.py:140] Waiting for new checkpoint at /public/groups/cgl/graph-genomes/anovak/trash/output/models. ```. How do I get the missing `*metrics` files and determine if any of the checkpoints that were missed is actually the best one? Do I need to `touch` some particular files in the directory to get `model_eval` to be interested in them? Is there some other command besides `model_eval` that can process a single particular checkpoint at a time? **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-q",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/611
https://github.com/google/deepvariant/issues/611:3071,security,model,model,3071,"nfig.pbtxt"" \. --checkpoint_dir=""${TRAINING_DIR}"" \. --batch_size=512 \. --min_eval_interval_s=1 \. --eval_timeout=1000. ```. It evaluated the most recent checkpoint saves as of when it started up, and it is evaluating new checkpoints as they are created. But it doesn't seem interested in going back and evaluating all the checkpoints, other than the initial-latest one, that were created before it started. The directory in question looks like this:. ```. (dv_venv) [anovak@phoenix-01 trash]$ ls output/models/*meta. output/models/model.ckpt-0.meta output/models/model.ckpt-19459.meta output/models/model.ckpt-31078.meta. output/models/model.ckpt-1000.meta output/models/model.ckpt-2000.meta output/models/model.ckpt-34008.meta. output/models/model.ckpt-10674.meta output/models/model.ckpt-22355.meta output/models/model.ckpt-4814.meta. output/models/model.ckpt-13613.meta output/models/model.ckpt-25257.meta output/models/model.ckpt-7724.meta. output/models/model.ckpt-16546.meta output/models/model.ckpt-28168.meta. (dv_venv) [anovak@phoenix-01 trash]$ ls output/models/*metrics. output/models/best_checkpoint.metrics output/models/model.ckpt-28168.metrics output/models/model.ckpt-34008.metrics. output/models/current.metrics output/models/model.ckpt-31078.metrics. ```. But `model_eval` just sits there like this (until a new checkpoint appears):. ```. I0210 17:42:06.700287 139846137329472 checkpoint_utils.py:140] Waiting for new checkpoint at /public/groups/cgl/graph-genomes/anovak/trash/output/models. ```. How do I get the missing `*metrics` files and determine if any of the checkpoints that were missed is actually the best one? Do I need to `touch` some particular files in the directory to get `model_eval` to be interested in them? Is there some other command besides `model_eval` that can process a single particular checkpoint at a time? **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-st",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/611
https://github.com/google/deepvariant/issues/611:3141,security,model,models,3141,"\. --min_eval_interval_s=1 \. --eval_timeout=1000. ```. It evaluated the most recent checkpoint saves as of when it started up, and it is evaluating new checkpoints as they are created. But it doesn't seem interested in going back and evaluating all the checkpoints, other than the initial-latest one, that were created before it started. The directory in question looks like this:. ```. (dv_venv) [anovak@phoenix-01 trash]$ ls output/models/*meta. output/models/model.ckpt-0.meta output/models/model.ckpt-19459.meta output/models/model.ckpt-31078.meta. output/models/model.ckpt-1000.meta output/models/model.ckpt-2000.meta output/models/model.ckpt-34008.meta. output/models/model.ckpt-10674.meta output/models/model.ckpt-22355.meta output/models/model.ckpt-4814.meta. output/models/model.ckpt-13613.meta output/models/model.ckpt-25257.meta output/models/model.ckpt-7724.meta. output/models/model.ckpt-16546.meta output/models/model.ckpt-28168.meta. (dv_venv) [anovak@phoenix-01 trash]$ ls output/models/*metrics. output/models/best_checkpoint.metrics output/models/model.ckpt-28168.metrics output/models/model.ckpt-34008.metrics. output/models/current.metrics output/models/model.ckpt-31078.metrics. ```. But `model_eval` just sits there like this (until a new checkpoint appears):. ```. I0210 17:42:06.700287 139846137329472 checkpoint_utils.py:140] Waiting for new checkpoint at /public/groups/cgl/graph-genomes/anovak/trash/output/models. ```. How do I get the missing `*metrics` files and determine if any of the checkpoints that were missed is actually the best one? Do I need to `touch` some particular files in the directory to get `model_eval` to be interested in them? Is there some other command besides `model_eval` that can process a single particular checkpoint at a time? **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick sta",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/611
https://github.com/google/deepvariant/issues/611:3165,security,model,models,3165,"=1 \. --eval_timeout=1000. ```. It evaluated the most recent checkpoint saves as of when it started up, and it is evaluating new checkpoints as they are created. But it doesn't seem interested in going back and evaluating all the checkpoints, other than the initial-latest one, that were created before it started. The directory in question looks like this:. ```. (dv_venv) [anovak@phoenix-01 trash]$ ls output/models/*meta. output/models/model.ckpt-0.meta output/models/model.ckpt-19459.meta output/models/model.ckpt-31078.meta. output/models/model.ckpt-1000.meta output/models/model.ckpt-2000.meta output/models/model.ckpt-34008.meta. output/models/model.ckpt-10674.meta output/models/model.ckpt-22355.meta output/models/model.ckpt-4814.meta. output/models/model.ckpt-13613.meta output/models/model.ckpt-25257.meta output/models/model.ckpt-7724.meta. output/models/model.ckpt-16546.meta output/models/model.ckpt-28168.meta. (dv_venv) [anovak@phoenix-01 trash]$ ls output/models/*metrics. output/models/best_checkpoint.metrics output/models/model.ckpt-28168.metrics output/models/model.ckpt-34008.metrics. output/models/current.metrics output/models/model.ckpt-31078.metrics. ```. But `model_eval` just sits there like this (until a new checkpoint appears):. ```. I0210 17:42:06.700287 139846137329472 checkpoint_utils.py:140] Waiting for new checkpoint at /public/groups/cgl/graph-genomes/anovak/trash/output/models. ```. How do I get the missing `*metrics` files and determine if any of the checkpoints that were missed is actually the best one? Do I need to `touch` some particular files in the directory to get `model_eval` to be interested in them? Is there some other command besides `model_eval` that can process a single particular checkpoint at a time? **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? It doesn't look like",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/611
https://github.com/google/deepvariant/issues/611:3203,security,model,models,3203,"luated the most recent checkpoint saves as of when it started up, and it is evaluating new checkpoints as they are created. But it doesn't seem interested in going back and evaluating all the checkpoints, other than the initial-latest one, that were created before it started. The directory in question looks like this:. ```. (dv_venv) [anovak@phoenix-01 trash]$ ls output/models/*meta. output/models/model.ckpt-0.meta output/models/model.ckpt-19459.meta output/models/model.ckpt-31078.meta. output/models/model.ckpt-1000.meta output/models/model.ckpt-2000.meta output/models/model.ckpt-34008.meta. output/models/model.ckpt-10674.meta output/models/model.ckpt-22355.meta output/models/model.ckpt-4814.meta. output/models/model.ckpt-13613.meta output/models/model.ckpt-25257.meta output/models/model.ckpt-7724.meta. output/models/model.ckpt-16546.meta output/models/model.ckpt-28168.meta. (dv_venv) [anovak@phoenix-01 trash]$ ls output/models/*metrics. output/models/best_checkpoint.metrics output/models/model.ckpt-28168.metrics output/models/model.ckpt-34008.metrics. output/models/current.metrics output/models/model.ckpt-31078.metrics. ```. But `model_eval` just sits there like this (until a new checkpoint appears):. ```. I0210 17:42:06.700287 139846137329472 checkpoint_utils.py:140] Waiting for new checkpoint at /public/groups/cgl/graph-genomes/anovak/trash/output/models. ```. How do I get the missing `*metrics` files and determine if any of the checkpoints that were missed is actually the best one? Do I need to `touch` some particular files in the directory to get `model_eval` to be interested in them? Is there some other command besides `model_eval` that can process a single particular checkpoint at a time? **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? It doesn't look like model training is part of the quick s",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/611
https://github.com/google/deepvariant/issues/611:3210,security,model,model,3210,"the most recent checkpoint saves as of when it started up, and it is evaluating new checkpoints as they are created. But it doesn't seem interested in going back and evaluating all the checkpoints, other than the initial-latest one, that were created before it started. The directory in question looks like this:. ```. (dv_venv) [anovak@phoenix-01 trash]$ ls output/models/*meta. output/models/model.ckpt-0.meta output/models/model.ckpt-19459.meta output/models/model.ckpt-31078.meta. output/models/model.ckpt-1000.meta output/models/model.ckpt-2000.meta output/models/model.ckpt-34008.meta. output/models/model.ckpt-10674.meta output/models/model.ckpt-22355.meta output/models/model.ckpt-4814.meta. output/models/model.ckpt-13613.meta output/models/model.ckpt-25257.meta output/models/model.ckpt-7724.meta. output/models/model.ckpt-16546.meta output/models/model.ckpt-28168.meta. (dv_venv) [anovak@phoenix-01 trash]$ ls output/models/*metrics. output/models/best_checkpoint.metrics output/models/model.ckpt-28168.metrics output/models/model.ckpt-34008.metrics. output/models/current.metrics output/models/model.ckpt-31078.metrics. ```. But `model_eval` just sits there like this (until a new checkpoint appears):. ```. I0210 17:42:06.700287 139846137329472 checkpoint_utils.py:140] Waiting for new checkpoint at /public/groups/cgl/graph-genomes/anovak/trash/output/models. ```. How do I get the missing `*metrics` files and determine if any of the checkpoints that were missed is actually the best one? Do I need to `touch` some particular files in the directory to get `model_eval` to be interested in them? Is there some other command besides `model_eval` that can process a single particular checkpoint at a time? **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? It doesn't look like model training is part of the quick start. *",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/611
https://github.com/google/deepvariant/issues/611:3242,security,model,models,3242," as of when it started up, and it is evaluating new checkpoints as they are created. But it doesn't seem interested in going back and evaluating all the checkpoints, other than the initial-latest one, that were created before it started. The directory in question looks like this:. ```. (dv_venv) [anovak@phoenix-01 trash]$ ls output/models/*meta. output/models/model.ckpt-0.meta output/models/model.ckpt-19459.meta output/models/model.ckpt-31078.meta. output/models/model.ckpt-1000.meta output/models/model.ckpt-2000.meta output/models/model.ckpt-34008.meta. output/models/model.ckpt-10674.meta output/models/model.ckpt-22355.meta output/models/model.ckpt-4814.meta. output/models/model.ckpt-13613.meta output/models/model.ckpt-25257.meta output/models/model.ckpt-7724.meta. output/models/model.ckpt-16546.meta output/models/model.ckpt-28168.meta. (dv_venv) [anovak@phoenix-01 trash]$ ls output/models/*metrics. output/models/best_checkpoint.metrics output/models/model.ckpt-28168.metrics output/models/model.ckpt-34008.metrics. output/models/current.metrics output/models/model.ckpt-31078.metrics. ```. But `model_eval` just sits there like this (until a new checkpoint appears):. ```. I0210 17:42:06.700287 139846137329472 checkpoint_utils.py:140] Waiting for new checkpoint at /public/groups/cgl/graph-genomes/anovak/trash/output/models. ```. How do I get the missing `*metrics` files and determine if any of the checkpoints that were missed is actually the best one? Do I need to `touch` some particular files in the directory to get `model_eval` to be interested in them? Is there some other command besides `model_eval` that can process a single particular checkpoint at a time? **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? It doesn't look like model training is part of the quick start. **Any additional context:**. Even",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/611
https://github.com/google/deepvariant/issues/611:3249,security,model,model,3249,"when it started up, and it is evaluating new checkpoints as they are created. But it doesn't seem interested in going back and evaluating all the checkpoints, other than the initial-latest one, that were created before it started. The directory in question looks like this:. ```. (dv_venv) [anovak@phoenix-01 trash]$ ls output/models/*meta. output/models/model.ckpt-0.meta output/models/model.ckpt-19459.meta output/models/model.ckpt-31078.meta. output/models/model.ckpt-1000.meta output/models/model.ckpt-2000.meta output/models/model.ckpt-34008.meta. output/models/model.ckpt-10674.meta output/models/model.ckpt-22355.meta output/models/model.ckpt-4814.meta. output/models/model.ckpt-13613.meta output/models/model.ckpt-25257.meta output/models/model.ckpt-7724.meta. output/models/model.ckpt-16546.meta output/models/model.ckpt-28168.meta. (dv_venv) [anovak@phoenix-01 trash]$ ls output/models/*metrics. output/models/best_checkpoint.metrics output/models/model.ckpt-28168.metrics output/models/model.ckpt-34008.metrics. output/models/current.metrics output/models/model.ckpt-31078.metrics. ```. But `model_eval` just sits there like this (until a new checkpoint appears):. ```. I0210 17:42:06.700287 139846137329472 checkpoint_utils.py:140] Waiting for new checkpoint at /public/groups/cgl/graph-genomes/anovak/trash/output/models. ```. How do I get the missing `*metrics` files and determine if any of the checkpoints that were missed is actually the best one? Do I need to `touch` some particular files in the directory to get `model_eval` to be interested in them? Is there some other command besides `model_eval` that can process a single particular checkpoint at a time? **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? It doesn't look like model training is part of the quick start. **Any additional context:**. Eventually ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/611
https://github.com/google/deepvariant/issues/611:3282,security,model,models,3282,"luating new checkpoints as they are created. But it doesn't seem interested in going back and evaluating all the checkpoints, other than the initial-latest one, that were created before it started. The directory in question looks like this:. ```. (dv_venv) [anovak@phoenix-01 trash]$ ls output/models/*meta. output/models/model.ckpt-0.meta output/models/model.ckpt-19459.meta output/models/model.ckpt-31078.meta. output/models/model.ckpt-1000.meta output/models/model.ckpt-2000.meta output/models/model.ckpt-34008.meta. output/models/model.ckpt-10674.meta output/models/model.ckpt-22355.meta output/models/model.ckpt-4814.meta. output/models/model.ckpt-13613.meta output/models/model.ckpt-25257.meta output/models/model.ckpt-7724.meta. output/models/model.ckpt-16546.meta output/models/model.ckpt-28168.meta. (dv_venv) [anovak@phoenix-01 trash]$ ls output/models/*metrics. output/models/best_checkpoint.metrics output/models/model.ckpt-28168.metrics output/models/model.ckpt-34008.metrics. output/models/current.metrics output/models/model.ckpt-31078.metrics. ```. But `model_eval` just sits there like this (until a new checkpoint appears):. ```. I0210 17:42:06.700287 139846137329472 checkpoint_utils.py:140] Waiting for new checkpoint at /public/groups/cgl/graph-genomes/anovak/trash/output/models. ```. How do I get the missing `*metrics` files and determine if any of the checkpoints that were missed is actually the best one? Do I need to `touch` some particular files in the directory to get `model_eval` to be interested in them? Is there some other command besides `model_eval` that can process a single particular checkpoint at a time? **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? It doesn't look like model training is part of the quick start. **Any additional context:**. Eventually I might want a WDL workflow for t",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/611
https://github.com/google/deepvariant/issues/611:3312,security,model,models,3312,"y are created. But it doesn't seem interested in going back and evaluating all the checkpoints, other than the initial-latest one, that were created before it started. The directory in question looks like this:. ```. (dv_venv) [anovak@phoenix-01 trash]$ ls output/models/*meta. output/models/model.ckpt-0.meta output/models/model.ckpt-19459.meta output/models/model.ckpt-31078.meta. output/models/model.ckpt-1000.meta output/models/model.ckpt-2000.meta output/models/model.ckpt-34008.meta. output/models/model.ckpt-10674.meta output/models/model.ckpt-22355.meta output/models/model.ckpt-4814.meta. output/models/model.ckpt-13613.meta output/models/model.ckpt-25257.meta output/models/model.ckpt-7724.meta. output/models/model.ckpt-16546.meta output/models/model.ckpt-28168.meta. (dv_venv) [anovak@phoenix-01 trash]$ ls output/models/*metrics. output/models/best_checkpoint.metrics output/models/model.ckpt-28168.metrics output/models/model.ckpt-34008.metrics. output/models/current.metrics output/models/model.ckpt-31078.metrics. ```. But `model_eval` just sits there like this (until a new checkpoint appears):. ```. I0210 17:42:06.700287 139846137329472 checkpoint_utils.py:140] Waiting for new checkpoint at /public/groups/cgl/graph-genomes/anovak/trash/output/models. ```. How do I get the missing `*metrics` files and determine if any of the checkpoints that were missed is actually the best one? Do I need to `touch` some particular files in the directory to get `model_eval` to be interested in them? Is there some other command besides `model_eval` that can process a single particular checkpoint at a time? **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? It doesn't look like model training is part of the quick start. **Any additional context:**. Eventually I might want a WDL workflow for training DeepVariant, and I'm n",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/611
https://github.com/google/deepvariant/issues/611:3319,security,model,model,3319,"reated. But it doesn't seem interested in going back and evaluating all the checkpoints, other than the initial-latest one, that were created before it started. The directory in question looks like this:. ```. (dv_venv) [anovak@phoenix-01 trash]$ ls output/models/*meta. output/models/model.ckpt-0.meta output/models/model.ckpt-19459.meta output/models/model.ckpt-31078.meta. output/models/model.ckpt-1000.meta output/models/model.ckpt-2000.meta output/models/model.ckpt-34008.meta. output/models/model.ckpt-10674.meta output/models/model.ckpt-22355.meta output/models/model.ckpt-4814.meta. output/models/model.ckpt-13613.meta output/models/model.ckpt-25257.meta output/models/model.ckpt-7724.meta. output/models/model.ckpt-16546.meta output/models/model.ckpt-28168.meta. (dv_venv) [anovak@phoenix-01 trash]$ ls output/models/*metrics. output/models/best_checkpoint.metrics output/models/model.ckpt-28168.metrics output/models/model.ckpt-34008.metrics. output/models/current.metrics output/models/model.ckpt-31078.metrics. ```. But `model_eval` just sits there like this (until a new checkpoint appears):. ```. I0210 17:42:06.700287 139846137329472 checkpoint_utils.py:140] Waiting for new checkpoint at /public/groups/cgl/graph-genomes/anovak/trash/output/models. ```. How do I get the missing `*metrics` files and determine if any of the checkpoints that were missed is actually the best one? Do I need to `touch` some particular files in the directory to get `model_eval` to be interested in them? Is there some other command besides `model_eval` that can process a single particular checkpoint at a time? **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? It doesn't look like model training is part of the quick start. **Any additional context:**. Eventually I might want a WDL workflow for training DeepVariant, and I'm not sure",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/611
https://github.com/google/deepvariant/issues/611:3579,security,model,models,3579,"els/*meta. output/models/model.ckpt-0.meta output/models/model.ckpt-19459.meta output/models/model.ckpt-31078.meta. output/models/model.ckpt-1000.meta output/models/model.ckpt-2000.meta output/models/model.ckpt-34008.meta. output/models/model.ckpt-10674.meta output/models/model.ckpt-22355.meta output/models/model.ckpt-4814.meta. output/models/model.ckpt-13613.meta output/models/model.ckpt-25257.meta output/models/model.ckpt-7724.meta. output/models/model.ckpt-16546.meta output/models/model.ckpt-28168.meta. (dv_venv) [anovak@phoenix-01 trash]$ ls output/models/*metrics. output/models/best_checkpoint.metrics output/models/model.ckpt-28168.metrics output/models/model.ckpt-34008.metrics. output/models/current.metrics output/models/model.ckpt-31078.metrics. ```. But `model_eval` just sits there like this (until a new checkpoint appears):. ```. I0210 17:42:06.700287 139846137329472 checkpoint_utils.py:140] Waiting for new checkpoint at /public/groups/cgl/graph-genomes/anovak/trash/output/models. ```. How do I get the missing `*metrics` files and determine if any of the checkpoints that were missed is actually the best one? Do I need to `touch` some particular files in the directory to get `model_eval` to be interested in them? Is there some other command besides `model_eval` that can process a single particular checkpoint at a time? **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? It doesn't look like model training is part of the quick start. **Any additional context:**. Eventually I might want a WDL workflow for training DeepVariant, and I'm not sure that managing two simultaneous DV processes in there is going to be worth the engineering required; they'd have to be lumped together into one WDL task and they'd have to always fit simultaneously on one machine. It would be much simpler for me to be able to ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/611
https://github.com/google/deepvariant/issues/611:4169,security,model,model,4169,"0674.meta output/models/model.ckpt-22355.meta output/models/model.ckpt-4814.meta. output/models/model.ckpt-13613.meta output/models/model.ckpt-25257.meta output/models/model.ckpt-7724.meta. output/models/model.ckpt-16546.meta output/models/model.ckpt-28168.meta. (dv_venv) [anovak@phoenix-01 trash]$ ls output/models/*metrics. output/models/best_checkpoint.metrics output/models/model.ckpt-28168.metrics output/models/model.ckpt-34008.metrics. output/models/current.metrics output/models/model.ckpt-31078.metrics. ```. But `model_eval` just sits there like this (until a new checkpoint appears):. ```. I0210 17:42:06.700287 139846137329472 checkpoint_utils.py:140] Waiting for new checkpoint at /public/groups/cgl/graph-genomes/anovak/trash/output/models. ```. How do I get the missing `*metrics` files and determine if any of the checkpoints that were missed is actually the best one? Do I need to `touch` some particular files in the directory to get `model_eval` to be interested in them? Is there some other command besides `model_eval` that can process a single particular checkpoint at a time? **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? It doesn't look like model training is part of the quick start. **Any additional context:**. Eventually I might want a WDL workflow for training DeepVariant, and I'm not sure that managing two simultaneous DV processes in there is going to be worth the engineering required; they'd have to be lumped together into one WDL task and they'd have to always fit simultaneously on one machine. It would be much simpler for me to be able to run the training to the end, and then run all the evaluations afterward to select the best model. But it looks like if I tried that right now `model_eval` would just only evaluate the last checkpoint and always confidently declare it to be the best.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/611
https://github.com/google/deepvariant/issues/611:4673,security,model,model,4673,"0674.meta output/models/model.ckpt-22355.meta output/models/model.ckpt-4814.meta. output/models/model.ckpt-13613.meta output/models/model.ckpt-25257.meta output/models/model.ckpt-7724.meta. output/models/model.ckpt-16546.meta output/models/model.ckpt-28168.meta. (dv_venv) [anovak@phoenix-01 trash]$ ls output/models/*metrics. output/models/best_checkpoint.metrics output/models/model.ckpt-28168.metrics output/models/model.ckpt-34008.metrics. output/models/current.metrics output/models/model.ckpt-31078.metrics. ```. But `model_eval` just sits there like this (until a new checkpoint appears):. ```. I0210 17:42:06.700287 139846137329472 checkpoint_utils.py:140] Waiting for new checkpoint at /public/groups/cgl/graph-genomes/anovak/trash/output/models. ```. How do I get the missing `*metrics` files and determine if any of the checkpoints that were missed is actually the best one? Do I need to `touch` some particular files in the directory to get `model_eval` to be interested in them? Is there some other command besides `model_eval` that can process a single particular checkpoint at a time? **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? It doesn't look like model training is part of the quick start. **Any additional context:**. Eventually I might want a WDL workflow for training DeepVariant, and I'm not sure that managing two simultaneous DV processes in there is going to be worth the engineering required; they'd have to be lumped together into one WDL task and they'd have to always fit simultaneously on one machine. It would be much simpler for me to be able to run the training to the end, and then run all the evaluations afterward to select the best model. But it looks like if I tried that right now `model_eval` would just only evaluate the last checkpoint and always confidently declare it to be the best.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/611
https://github.com/google/deepvariant/issues/611:3954,testability,test,test,3954,"0674.meta output/models/model.ckpt-22355.meta output/models/model.ckpt-4814.meta. output/models/model.ckpt-13613.meta output/models/model.ckpt-25257.meta output/models/model.ckpt-7724.meta. output/models/model.ckpt-16546.meta output/models/model.ckpt-28168.meta. (dv_venv) [anovak@phoenix-01 trash]$ ls output/models/*metrics. output/models/best_checkpoint.metrics output/models/model.ckpt-28168.metrics output/models/model.ckpt-34008.metrics. output/models/current.metrics output/models/model.ckpt-31078.metrics. ```. But `model_eval` just sits there like this (until a new checkpoint appears):. ```. I0210 17:42:06.700287 139846137329472 checkpoint_utils.py:140] Waiting for new checkpoint at /public/groups/cgl/graph-genomes/anovak/trash/output/models. ```. How do I get the missing `*metrics` files and determine if any of the checkpoints that were missed is actually the best one? Do I need to `touch` some particular files in the directory to get `model_eval` to be interested in them? Is there some other command besides `model_eval` that can process a single particular checkpoint at a time? **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? It doesn't look like model training is part of the quick start. **Any additional context:**. Eventually I might want a WDL workflow for training DeepVariant, and I'm not sure that managing two simultaneous DV processes in there is going to be worth the engineering required; they'd have to be lumped together into one WDL task and they'd have to always fit simultaneously on one machine. It would be much simpler for me to be able to run the training to the end, and then run all the evaluations afterward to select the best model. But it looks like if I tried that right now `model_eval` would just only evaluate the last checkpoint and always confidently declare it to be the best.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/611
https://github.com/google/deepvariant/issues/611:3990,testability,test,test,3990,"0674.meta output/models/model.ckpt-22355.meta output/models/model.ckpt-4814.meta. output/models/model.ckpt-13613.meta output/models/model.ckpt-25257.meta output/models/model.ckpt-7724.meta. output/models/model.ckpt-16546.meta output/models/model.ckpt-28168.meta. (dv_venv) [anovak@phoenix-01 trash]$ ls output/models/*metrics. output/models/best_checkpoint.metrics output/models/model.ckpt-28168.metrics output/models/model.ckpt-34008.metrics. output/models/current.metrics output/models/model.ckpt-31078.metrics. ```. But `model_eval` just sits there like this (until a new checkpoint appears):. ```. I0210 17:42:06.700287 139846137329472 checkpoint_utils.py:140] Waiting for new checkpoint at /public/groups/cgl/graph-genomes/anovak/trash/output/models. ```. How do I get the missing `*metrics` files and determine if any of the checkpoints that were missed is actually the best one? Do I need to `touch` some particular files in the directory to get `model_eval` to be interested in them? Is there some other command besides `model_eval` that can process a single particular checkpoint at a time? **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? It doesn't look like model training is part of the quick start. **Any additional context:**. Eventually I might want a WDL workflow for training DeepVariant, and I'm not sure that managing two simultaneous DV processes in there is going to be worth the engineering required; they'd have to be lumped together into one WDL task and they'd have to always fit simultaneously on one machine. It would be much simpler for me to be able to run the training to the end, and then run all the evaluations afterward to select the best model. But it looks like if I tried that right now `model_eval` would just only evaluate the last checkpoint and always confidently declare it to be the best.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/611
https://github.com/google/deepvariant/issues/611:4229,testability,context,context,4229,"0674.meta output/models/model.ckpt-22355.meta output/models/model.ckpt-4814.meta. output/models/model.ckpt-13613.meta output/models/model.ckpt-25257.meta output/models/model.ckpt-7724.meta. output/models/model.ckpt-16546.meta output/models/model.ckpt-28168.meta. (dv_venv) [anovak@phoenix-01 trash]$ ls output/models/*metrics. output/models/best_checkpoint.metrics output/models/model.ckpt-28168.metrics output/models/model.ckpt-34008.metrics. output/models/current.metrics output/models/model.ckpt-31078.metrics. ```. But `model_eval` just sits there like this (until a new checkpoint appears):. ```. I0210 17:42:06.700287 139846137329472 checkpoint_utils.py:140] Waiting for new checkpoint at /public/groups/cgl/graph-genomes/anovak/trash/output/models. ```. How do I get the missing `*metrics` files and determine if any of the checkpoints that were missed is actually the best one? Do I need to `touch` some particular files in the directory to get `model_eval` to be interested in them? Is there some other command besides `model_eval` that can process a single particular checkpoint at a time? **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? It doesn't look like model training is part of the quick start. **Any additional context:**. Eventually I might want a WDL workflow for training DeepVariant, and I'm not sure that managing two simultaneous DV processes in there is going to be worth the engineering required; they'd have to be lumped together into one WDL task and they'd have to always fit simultaneously on one machine. It would be much simpler for me to be able to run the training to the end, and then run all the evaluations afterward to select the best model. But it looks like if I tried that right now `model_eval` would just only evaluate the last checkpoint and always confidently declare it to be the best.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/611
https://github.com/google/deepvariant/issues/611:4341,testability,simul,simultaneous,4341,"0674.meta output/models/model.ckpt-22355.meta output/models/model.ckpt-4814.meta. output/models/model.ckpt-13613.meta output/models/model.ckpt-25257.meta output/models/model.ckpt-7724.meta. output/models/model.ckpt-16546.meta output/models/model.ckpt-28168.meta. (dv_venv) [anovak@phoenix-01 trash]$ ls output/models/*metrics. output/models/best_checkpoint.metrics output/models/model.ckpt-28168.metrics output/models/model.ckpt-34008.metrics. output/models/current.metrics output/models/model.ckpt-31078.metrics. ```. But `model_eval` just sits there like this (until a new checkpoint appears):. ```. I0210 17:42:06.700287 139846137329472 checkpoint_utils.py:140] Waiting for new checkpoint at /public/groups/cgl/graph-genomes/anovak/trash/output/models. ```. How do I get the missing `*metrics` files and determine if any of the checkpoints that were missed is actually the best one? Do I need to `touch` some particular files in the directory to get `model_eval` to be interested in them? Is there some other command besides `model_eval` that can process a single particular checkpoint at a time? **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? It doesn't look like model training is part of the quick start. **Any additional context:**. Eventually I might want a WDL workflow for training DeepVariant, and I'm not sure that managing two simultaneous DV processes in there is going to be worth the engineering required; they'd have to be lumped together into one WDL task and they'd have to always fit simultaneously on one machine. It would be much simpler for me to be able to run the training to the end, and then run all the evaluations afterward to select the best model. But it looks like if I tried that right now `model_eval` would just only evaluate the last checkpoint and always confidently declare it to be the best.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/611
https://github.com/google/deepvariant/issues/611:4505,testability,simul,simultaneously,4505,"0674.meta output/models/model.ckpt-22355.meta output/models/model.ckpt-4814.meta. output/models/model.ckpt-13613.meta output/models/model.ckpt-25257.meta output/models/model.ckpt-7724.meta. output/models/model.ckpt-16546.meta output/models/model.ckpt-28168.meta. (dv_venv) [anovak@phoenix-01 trash]$ ls output/models/*metrics. output/models/best_checkpoint.metrics output/models/model.ckpt-28168.metrics output/models/model.ckpt-34008.metrics. output/models/current.metrics output/models/model.ckpt-31078.metrics. ```. But `model_eval` just sits there like this (until a new checkpoint appears):. ```. I0210 17:42:06.700287 139846137329472 checkpoint_utils.py:140] Waiting for new checkpoint at /public/groups/cgl/graph-genomes/anovak/trash/output/models. ```. How do I get the missing `*metrics` files and determine if any of the checkpoints that were missed is actually the best one? Do I need to `touch` some particular files in the directory to get `model_eval` to be interested in them? Is there some other command besides `model_eval` that can process a single particular checkpoint at a time? **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? It doesn't look like model training is part of the quick start. **Any additional context:**. Eventually I might want a WDL workflow for training DeepVariant, and I'm not sure that managing two simultaneous DV processes in there is going to be worth the engineering required; they'd have to be lumped together into one WDL task and they'd have to always fit simultaneously on one machine. It would be much simpler for me to be able to run the training to the end, and then run all the evaluations afterward to select the best model. But it looks like if I tried that right now `model_eval` would just only evaluate the last checkpoint and always confidently declare it to be the best.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/611
https://github.com/google/deepvariant/issues/611:4553,testability,simpl,simpler,4553,"0674.meta output/models/model.ckpt-22355.meta output/models/model.ckpt-4814.meta. output/models/model.ckpt-13613.meta output/models/model.ckpt-25257.meta output/models/model.ckpt-7724.meta. output/models/model.ckpt-16546.meta output/models/model.ckpt-28168.meta. (dv_venv) [anovak@phoenix-01 trash]$ ls output/models/*metrics. output/models/best_checkpoint.metrics output/models/model.ckpt-28168.metrics output/models/model.ckpt-34008.metrics. output/models/current.metrics output/models/model.ckpt-31078.metrics. ```. But `model_eval` just sits there like this (until a new checkpoint appears):. ```. I0210 17:42:06.700287 139846137329472 checkpoint_utils.py:140] Waiting for new checkpoint at /public/groups/cgl/graph-genomes/anovak/trash/output/models. ```. How do I get the missing `*metrics` files and determine if any of the checkpoints that were missed is actually the best one? Do I need to `touch` some particular files in the directory to get `model_eval` to be interested in them? Is there some other command besides `model_eval` that can process a single particular checkpoint at a time? **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? It doesn't look like model training is part of the quick start. **Any additional context:**. Eventually I might want a WDL workflow for training DeepVariant, and I'm not sure that managing two simultaneous DV processes in there is going to be worth the engineering required; they'd have to be lumped together into one WDL task and they'd have to always fit simultaneously on one machine. It would be much simpler for me to be able to run the training to the end, and then run all the evaluations afterward to select the best model. But it looks like if I tried that right now `model_eval` would just only evaluate the last checkpoint and always confidently declare it to be the best.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/611
https://github.com/google/deepvariant/issues/611:158,usability,command,command,158,"`model_eval` ignores pre-existing, not-yet-evaluated checkpoints, and nothing else seems to exist to evaluate them; **Describe the issue:**. The `model_eval` command can't catch up and evaluate checkpoints in a directory other than the latest one. If it ever crashes or isn't started in time to actually witness training create a checkpoint, there's no way for the user to evaluate the checkpoints it missed, and the ""best"" checkpoint it finds won't actually necessarily be the best out of all the checkpoints in the directory. . **Setup**. - Operating system: Ubuntu 22.04. - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): Docker. - Type of data: case study data. **Steps to reproduce:**. Loosely following https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-training-case-study.md#start-model_train-and-model_eval, I started the training Docker container using GPU 0:. ```. time docker run --rm --gpus 1 \. -v ""${DATA_DIR}:${DATA_DIR}"" \. -v ""${OUTPUT_DIR}:${OUTPUT_DIR}"" \. -u $(id -u):$(id -g) \. google/deepvariant:1.4.0-gpu \. /opt/deepvariant/bin/model_train \. --dataset_config_pbtxt=""${OUTPUT_DIR}/training_set.dataset_config.pbtxt"" \. --train_dir=""${TRAINING_DIR}"" \. --model_name=""inception_v3"" \. --number_of_steps=50000 \. --save_interval_secs=300 \. --batch_size=32 \. --learning_rate=0.0005 \. --start_from_checkpoint=""gs://deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-wgs_standard/model.ckpt"". ```. Then I tried to start the `model_eval` evaluator in GPU mode with 1 GPU, but Docker was just giving it the *same* GPU as training was using, so it kept immediately running out of GPU memory. After about half an hour I hit on the right syntax to give it a distinct GPU, and managed to actually start `model_eval`:. ```. docker run --rm --gpus '""device=1""' \. -v ""${DATA_DIR}:${DATA_DIR}"" \. -v ""${OUTPUT_DIR}:${OUTPUT_DIR}"" \. -u $(id -u):$(id -g) \. google/deepvariant:1.4.0-gpu \. /opt/deepvariant/bin/mod",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/611
https://github.com/google/deepvariant/issues/611:365,usability,user,user,365,"`model_eval` ignores pre-existing, not-yet-evaluated checkpoints, and nothing else seems to exist to evaluate them; **Describe the issue:**. The `model_eval` command can't catch up and evaluate checkpoints in a directory other than the latest one. If it ever crashes or isn't started in time to actually witness training create a checkpoint, there's no way for the user to evaluate the checkpoints it missed, and the ""best"" checkpoint it finds won't actually necessarily be the best out of all the checkpoints in the directory. . **Setup**. - Operating system: Ubuntu 22.04. - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): Docker. - Type of data: case study data. **Steps to reproduce:**. Loosely following https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-training-case-study.md#start-model_train-and-model_eval, I started the training Docker container using GPU 0:. ```. time docker run --rm --gpus 1 \. -v ""${DATA_DIR}:${DATA_DIR}"" \. -v ""${OUTPUT_DIR}:${OUTPUT_DIR}"" \. -u $(id -u):$(id -g) \. google/deepvariant:1.4.0-gpu \. /opt/deepvariant/bin/model_train \. --dataset_config_pbtxt=""${OUTPUT_DIR}/training_set.dataset_config.pbtxt"" \. --train_dir=""${TRAINING_DIR}"" \. --model_name=""inception_v3"" \. --number_of_steps=50000 \. --save_interval_secs=300 \. --batch_size=32 \. --learning_rate=0.0005 \. --start_from_checkpoint=""gs://deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-wgs_standard/model.ckpt"". ```. Then I tried to start the `model_eval` evaluator in GPU mode with 1 GPU, but Docker was just giving it the *same* GPU as training was using, so it kept immediately running out of GPU memory. After about half an hour I hit on the right syntax to give it a distinct GPU, and managed to actually start `model_eval`:. ```. docker run --rm --gpus '""device=1""' \. -v ""${DATA_DIR}:${DATA_DIR}"" \. -v ""${OUTPUT_DIR}:${OUTPUT_DIR}"" \. -u $(id -u):$(id -g) \. google/deepvariant:1.4.0-gpu \. /opt/deepvariant/bin/mod",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/611
https://github.com/google/deepvariant/issues/611:1677,usability,memor,memory,1677,"data: case study data. **Steps to reproduce:**. Loosely following https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-training-case-study.md#start-model_train-and-model_eval, I started the training Docker container using GPU 0:. ```. time docker run --rm --gpus 1 \. -v ""${DATA_DIR}:${DATA_DIR}"" \. -v ""${OUTPUT_DIR}:${OUTPUT_DIR}"" \. -u $(id -u):$(id -g) \. google/deepvariant:1.4.0-gpu \. /opt/deepvariant/bin/model_train \. --dataset_config_pbtxt=""${OUTPUT_DIR}/training_set.dataset_config.pbtxt"" \. --train_dir=""${TRAINING_DIR}"" \. --model_name=""inception_v3"" \. --number_of_steps=50000 \. --save_interval_secs=300 \. --batch_size=32 \. --learning_rate=0.0005 \. --start_from_checkpoint=""gs://deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-wgs_standard/model.ckpt"". ```. Then I tried to start the `model_eval` evaluator in GPU mode with 1 GPU, but Docker was just giving it the *same* GPU as training was using, so it kept immediately running out of GPU memory. After about half an hour I hit on the right syntax to give it a distinct GPU, and managed to actually start `model_eval`:. ```. docker run --rm --gpus '""device=1""' \. -v ""${DATA_DIR}:${DATA_DIR}"" \. -v ""${OUTPUT_DIR}:${OUTPUT_DIR}"" \. -u $(id -u):$(id -g) \. google/deepvariant:1.4.0-gpu \. /opt/deepvariant/bin/model_eval \. --dataset_config_pbtxt=""${OUTPUT_DIR}/validation_set.dataset_config.pbtxt"" \. --checkpoint_dir=""${TRAINING_DIR}"" \. --batch_size=512 \. --min_eval_interval_s=1 \. --eval_timeout=1000. ```. It evaluated the most recent checkpoint saves as of when it started up, and it is evaluating new checkpoints as they are created. But it doesn't seem interested in going back and evaluating all the checkpoints, other than the initial-latest one, that were created before it started. The directory in question looks like this:. ```. (dv_venv) [anovak@phoenix-01 trash]$ ls output/models/*meta. output/models/model.ckpt-0.meta output/models/model.ckpt-19459.meta output/models/model",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/611
https://github.com/google/deepvariant/issues/611:3843,usability,command,command,3843,"0674.meta output/models/model.ckpt-22355.meta output/models/model.ckpt-4814.meta. output/models/model.ckpt-13613.meta output/models/model.ckpt-25257.meta output/models/model.ckpt-7724.meta. output/models/model.ckpt-16546.meta output/models/model.ckpt-28168.meta. (dv_venv) [anovak@phoenix-01 trash]$ ls output/models/*metrics. output/models/best_checkpoint.metrics output/models/model.ckpt-28168.metrics output/models/model.ckpt-34008.metrics. output/models/current.metrics output/models/model.ckpt-31078.metrics. ```. But `model_eval` just sits there like this (until a new checkpoint appears):. ```. I0210 17:42:06.700287 139846137329472 checkpoint_utils.py:140] Waiting for new checkpoint at /public/groups/cgl/graph-genomes/anovak/trash/output/models. ```. How do I get the missing `*metrics` files and determine if any of the checkpoints that were missed is actually the best one? Do I need to `touch` some particular files in the directory to get `model_eval` to be interested in them? Is there some other command besides `model_eval` that can process a single particular checkpoint at a time? **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? It doesn't look like model training is part of the quick start. **Any additional context:**. Eventually I might want a WDL workflow for training DeepVariant, and I'm not sure that managing two simultaneous DV processes in there is going to be worth the engineering required; they'd have to be lumped together into one WDL task and they'd have to always fit simultaneously on one machine. It would be much simpler for me to be able to run the training to the end, and then run all the evaluations afterward to select the best model. But it looks like if I tried that right now `model_eval` would just only evaluate the last checkpoint and always confidently declare it to be the best.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/611
https://github.com/google/deepvariant/issues/611:4271,usability,workflow,workflow,4271,"0674.meta output/models/model.ckpt-22355.meta output/models/model.ckpt-4814.meta. output/models/model.ckpt-13613.meta output/models/model.ckpt-25257.meta output/models/model.ckpt-7724.meta. output/models/model.ckpt-16546.meta output/models/model.ckpt-28168.meta. (dv_venv) [anovak@phoenix-01 trash]$ ls output/models/*metrics. output/models/best_checkpoint.metrics output/models/model.ckpt-28168.metrics output/models/model.ckpt-34008.metrics. output/models/current.metrics output/models/model.ckpt-31078.metrics. ```. But `model_eval` just sits there like this (until a new checkpoint appears):. ```. I0210 17:42:06.700287 139846137329472 checkpoint_utils.py:140] Waiting for new checkpoint at /public/groups/cgl/graph-genomes/anovak/trash/output/models. ```. How do I get the missing `*metrics` files and determine if any of the checkpoints that were missed is actually the best one? Do I need to `touch` some particular files in the directory to get `model_eval` to be interested in them? Is there some other command besides `model_eval` that can process a single particular checkpoint at a time? **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? It doesn't look like model training is part of the quick start. **Any additional context:**. Eventually I might want a WDL workflow for training DeepVariant, and I'm not sure that managing two simultaneous DV processes in there is going to be worth the engineering required; they'd have to be lumped together into one WDL task and they'd have to always fit simultaneously on one machine. It would be much simpler for me to be able to run the training to the end, and then run all the evaluations afterward to select the best model. But it looks like if I tried that right now `model_eval` would just only evaluate the last checkpoint and always confidently declare it to be the best.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/611
https://github.com/google/deepvariant/issues/611:4553,usability,simpl,simpler,4553,"0674.meta output/models/model.ckpt-22355.meta output/models/model.ckpt-4814.meta. output/models/model.ckpt-13613.meta output/models/model.ckpt-25257.meta output/models/model.ckpt-7724.meta. output/models/model.ckpt-16546.meta output/models/model.ckpt-28168.meta. (dv_venv) [anovak@phoenix-01 trash]$ ls output/models/*metrics. output/models/best_checkpoint.metrics output/models/model.ckpt-28168.metrics output/models/model.ckpt-34008.metrics. output/models/current.metrics output/models/model.ckpt-31078.metrics. ```. But `model_eval` just sits there like this (until a new checkpoint appears):. ```. I0210 17:42:06.700287 139846137329472 checkpoint_utils.py:140] Waiting for new checkpoint at /public/groups/cgl/graph-genomes/anovak/trash/output/models. ```. How do I get the missing `*metrics` files and determine if any of the checkpoints that were missed is actually the best one? Do I need to `touch` some particular files in the directory to get `model_eval` to be interested in them? Is there some other command besides `model_eval` that can process a single particular checkpoint at a time? **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? It doesn't look like model training is part of the quick start. **Any additional context:**. Eventually I might want a WDL workflow for training DeepVariant, and I'm not sure that managing two simultaneous DV processes in there is going to be worth the engineering required; they'd have to be lumped together into one WDL task and they'd have to always fit simultaneously on one machine. It would be much simpler for me to be able to run the training to the end, and then run all the evaluations afterward to select the best model. But it looks like if I tried that right now `model_eval` would just only evaluate the last checkpoint and always confidently declare it to be the best.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/611
https://github.com/google/deepvariant/issues/612:541,availability,Operat,Operating,541,"Missing a 1-bp deletion comparing to GATK (potential denovo variant); **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**:. Yes. **Describe the issue:**. A potential denovo variant is filtered out due to mendelian violation. While found the deletion in the same sample via GATK and IGV (both raw BAM file and realigned BAM file from GATK HaplotypeCaller). Wonder how to loosen the criteria to increase the recall of denovo variants. (A clear and concise description of what the issue is.). **Setup**. - Operating system: CentOS7. - DeepVariant version: 1.4. - Installation method (Docker, built from source, etc.): Singularity. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). - 150bp paired-end Illumina data. **Steps to reproduce:**. - Command: . `/opt/deepvariant/bin/run_deepvariant \. --model_type=${model_type} \. --ref=""${ref_genome}"" \. --reads=""${bam_file}"" \. --make_examples_extra_args=""normalize_reads=true"" \. ${region_arg} \. --output_vcf=""${output_vcf}"" \. --output_gvcf=""${output_gvcf}"" \. --intermediate_results_dir ""/paedyl01/disk1/yangyxt/test_tmp/${singularity_inter}"" \. --num_shards=${threads}`. - Error trace: (if applicable). **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**. Here is the IGV screenshot of the position where DeepVariant failed to identify one bp deletion (Upper panel illustrates the alignment from raw BAM file, lower panel illustrates the alignment from the realigned BAM file from GATK HaplotypeCaller):. ![image](https://user-images.githubusercontent.com/40780228/218404096-273ed999-6443-43c2-83b9-108661d738d4.png). P.S. Please consider granting a parameter of DeepVariant to let users generate the realigned BAM file from DeepVariant. Thanks! .",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/612
https://github.com/google/deepvariant/issues/612:1219,availability,Error,Error,1219,"Missing a 1-bp deletion comparing to GATK (potential denovo variant); **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**:. Yes. **Describe the issue:**. A potential denovo variant is filtered out due to mendelian violation. While found the deletion in the same sample via GATK and IGV (both raw BAM file and realigned BAM file from GATK HaplotypeCaller). Wonder how to loosen the criteria to increase the recall of denovo variants. (A clear and concise description of what the issue is.). **Setup**. - Operating system: CentOS7. - DeepVariant version: 1.4. - Installation method (Docker, built from source, etc.): Singularity. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). - 150bp paired-end Illumina data. **Steps to reproduce:**. - Command: . `/opt/deepvariant/bin/run_deepvariant \. --model_type=${model_type} \. --ref=""${ref_genome}"" \. --reads=""${bam_file}"" \. --make_examples_extra_args=""normalize_reads=true"" \. ${region_arg} \. --output_vcf=""${output_vcf}"" \. --output_gvcf=""${output_gvcf}"" \. --intermediate_results_dir ""/paedyl01/disk1/yangyxt/test_tmp/${singularity_inter}"" \. --num_shards=${threads}`. - Error trace: (if applicable). **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**. Here is the IGV screenshot of the position where DeepVariant failed to identify one bp deletion (Upper panel illustrates the alignment from raw BAM file, lower panel illustrates the alignment from the realigned BAM file from GATK HaplotypeCaller):. ![image](https://user-images.githubusercontent.com/40780228/218404096-273ed999-6443-43c2-83b9-108661d738d4.png). P.S. Please consider granting a parameter of DeepVariant to let users generate the realigned BAM file from DeepVariant. Thanks! .",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/612
https://github.com/google/deepvariant/issues/612:582,deployability,version,version,582,"Missing a 1-bp deletion comparing to GATK (potential denovo variant); **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**:. Yes. **Describe the issue:**. A potential denovo variant is filtered out due to mendelian violation. While found the deletion in the same sample via GATK and IGV (both raw BAM file and realigned BAM file from GATK HaplotypeCaller). Wonder how to loosen the criteria to increase the recall of denovo variants. (A clear and concise description of what the issue is.). **Setup**. - Operating system: CentOS7. - DeepVariant version: 1.4. - Installation method (Docker, built from source, etc.): Singularity. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). - 150bp paired-end Illumina data. **Steps to reproduce:**. - Command: . `/opt/deepvariant/bin/run_deepvariant \. --model_type=${model_type} \. --ref=""${ref_genome}"" \. --reads=""${bam_file}"" \. --make_examples_extra_args=""normalize_reads=true"" \. ${region_arg} \. --output_vcf=""${output_vcf}"" \. --output_gvcf=""${output_gvcf}"" \. --intermediate_results_dir ""/paedyl01/disk1/yangyxt/test_tmp/${singularity_inter}"" \. --num_shards=${threads}`. - Error trace: (if applicable). **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**. Here is the IGV screenshot of the position where DeepVariant failed to identify one bp deletion (Upper panel illustrates the alignment from raw BAM file, lower panel illustrates the alignment from the realigned BAM file from GATK HaplotypeCaller):. ![image](https://user-images.githubusercontent.com/40780228/218404096-273ed999-6443-43c2-83b9-108661d738d4.png). P.S. Please consider granting a parameter of DeepVariant to let users generate the realigned BAM file from DeepVariant. Thanks! .",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/612
https://github.com/google/deepvariant/issues/612:598,deployability,Instal,Installation,598,"Missing a 1-bp deletion comparing to GATK (potential denovo variant); **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**:. Yes. **Describe the issue:**. A potential denovo variant is filtered out due to mendelian violation. While found the deletion in the same sample via GATK and IGV (both raw BAM file and realigned BAM file from GATK HaplotypeCaller). Wonder how to loosen the criteria to increase the recall of denovo variants. (A clear and concise description of what the issue is.). **Setup**. - Operating system: CentOS7. - DeepVariant version: 1.4. - Installation method (Docker, built from source, etc.): Singularity. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). - 150bp paired-end Illumina data. **Steps to reproduce:**. - Command: . `/opt/deepvariant/bin/run_deepvariant \. --model_type=${model_type} \. --ref=""${ref_genome}"" \. --reads=""${bam_file}"" \. --make_examples_extra_args=""normalize_reads=true"" \. ${region_arg} \. --output_vcf=""${output_vcf}"" \. --output_gvcf=""${output_gvcf}"" \. --intermediate_results_dir ""/paedyl01/disk1/yangyxt/test_tmp/${singularity_inter}"" \. --num_shards=${threads}`. - Error trace: (if applicable). **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**. Here is the IGV screenshot of the position where DeepVariant failed to identify one bp deletion (Upper panel illustrates the alignment from raw BAM file, lower panel illustrates the alignment from the realigned BAM file from GATK HaplotypeCaller):. ![image](https://user-images.githubusercontent.com/40780228/218404096-273ed999-6443-43c2-83b9-108661d738d4.png). P.S. Please consider granting a parameter of DeepVariant to let users generate the realigned BAM file from DeepVariant. Thanks! .",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/612
https://github.com/google/deepvariant/issues/612:1556,deployability,fail,failed,1556,"Missing a 1-bp deletion comparing to GATK (potential denovo variant); **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**:. Yes. **Describe the issue:**. A potential denovo variant is filtered out due to mendelian violation. While found the deletion in the same sample via GATK and IGV (both raw BAM file and realigned BAM file from GATK HaplotypeCaller). Wonder how to loosen the criteria to increase the recall of denovo variants. (A clear and concise description of what the issue is.). **Setup**. - Operating system: CentOS7. - DeepVariant version: 1.4. - Installation method (Docker, built from source, etc.): Singularity. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). - 150bp paired-end Illumina data. **Steps to reproduce:**. - Command: . `/opt/deepvariant/bin/run_deepvariant \. --model_type=${model_type} \. --ref=""${ref_genome}"" \. --reads=""${bam_file}"" \. --make_examples_extra_args=""normalize_reads=true"" \. ${region_arg} \. --output_vcf=""${output_vcf}"" \. --output_gvcf=""${output_gvcf}"" \. --intermediate_results_dir ""/paedyl01/disk1/yangyxt/test_tmp/${singularity_inter}"" \. --num_shards=${threads}`. - Error trace: (if applicable). **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**. Here is the IGV screenshot of the position where DeepVariant failed to identify one bp deletion (Upper panel illustrates the alignment from raw BAM file, lower panel illustrates the alignment from the realigned BAM file from GATK HaplotypeCaller):. ![image](https://user-images.githubusercontent.com/40780228/218404096-273ed999-6443-43c2-83b9-108661d738d4.png). P.S. Please consider granting a parameter of DeepVariant to let users generate the realigned BAM file from DeepVariant. Thanks! .",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/612
https://github.com/google/deepvariant/issues/612:222,integrability,filter,filtered,222,"Missing a 1-bp deletion comparing to GATK (potential denovo variant); **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**:. Yes. **Describe the issue:**. A potential denovo variant is filtered out due to mendelian violation. While found the deletion in the same sample via GATK and IGV (both raw BAM file and realigned BAM file from GATK HaplotypeCaller). Wonder how to loosen the criteria to increase the recall of denovo variants. (A clear and concise description of what the issue is.). **Setup**. - Operating system: CentOS7. - DeepVariant version: 1.4. - Installation method (Docker, built from source, etc.): Singularity. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). - 150bp paired-end Illumina data. **Steps to reproduce:**. - Command: . `/opt/deepvariant/bin/run_deepvariant \. --model_type=${model_type} \. --ref=""${ref_genome}"" \. --reads=""${bam_file}"" \. --make_examples_extra_args=""normalize_reads=true"" \. ${region_arg} \. --output_vcf=""${output_vcf}"" \. --output_gvcf=""${output_gvcf}"" \. --intermediate_results_dir ""/paedyl01/disk1/yangyxt/test_tmp/${singularity_inter}"" \. --num_shards=${threads}`. - Error trace: (if applicable). **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**. Here is the IGV screenshot of the position where DeepVariant failed to identify one bp deletion (Upper panel illustrates the alignment from raw BAM file, lower panel illustrates the alignment from the realigned BAM file from GATK HaplotypeCaller):. ![image](https://user-images.githubusercontent.com/40780228/218404096-273ed999-6443-43c2-83b9-108661d738d4.png). P.S. Please consider granting a parameter of DeepVariant to let users generate the realigned BAM file from DeepVariant. Thanks! .",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/612
https://github.com/google/deepvariant/issues/612:582,integrability,version,version,582,"Missing a 1-bp deletion comparing to GATK (potential denovo variant); **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**:. Yes. **Describe the issue:**. A potential denovo variant is filtered out due to mendelian violation. While found the deletion in the same sample via GATK and IGV (both raw BAM file and realigned BAM file from GATK HaplotypeCaller). Wonder how to loosen the criteria to increase the recall of denovo variants. (A clear and concise description of what the issue is.). **Setup**. - Operating system: CentOS7. - DeepVariant version: 1.4. - Installation method (Docker, built from source, etc.): Singularity. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). - 150bp paired-end Illumina data. **Steps to reproduce:**. - Command: . `/opt/deepvariant/bin/run_deepvariant \. --model_type=${model_type} \. --ref=""${ref_genome}"" \. --reads=""${bam_file}"" \. --make_examples_extra_args=""normalize_reads=true"" \. ${region_arg} \. --output_vcf=""${output_vcf}"" \. --output_gvcf=""${output_gvcf}"" \. --intermediate_results_dir ""/paedyl01/disk1/yangyxt/test_tmp/${singularity_inter}"" \. --num_shards=${threads}`. - Error trace: (if applicable). **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**. Here is the IGV screenshot of the position where DeepVariant failed to identify one bp deletion (Upper panel illustrates the alignment from raw BAM file, lower panel illustrates the alignment from the realigned BAM file from GATK HaplotypeCaller):. ![image](https://user-images.githubusercontent.com/40780228/218404096-273ed999-6443-43c2-83b9-108661d738d4.png). P.S. Please consider granting a parameter of DeepVariant to let users generate the realigned BAM file from DeepVariant. Thanks! .",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/612
https://github.com/google/deepvariant/issues/612:582,modifiability,version,version,582,"Missing a 1-bp deletion comparing to GATK (potential denovo variant); **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**:. Yes. **Describe the issue:**. A potential denovo variant is filtered out due to mendelian violation. While found the deletion in the same sample via GATK and IGV (both raw BAM file and realigned BAM file from GATK HaplotypeCaller). Wonder how to loosen the criteria to increase the recall of denovo variants. (A clear and concise description of what the issue is.). **Setup**. - Operating system: CentOS7. - DeepVariant version: 1.4. - Installation method (Docker, built from source, etc.): Singularity. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). - 150bp paired-end Illumina data. **Steps to reproduce:**. - Command: . `/opt/deepvariant/bin/run_deepvariant \. --model_type=${model_type} \. --ref=""${ref_genome}"" \. --reads=""${bam_file}"" \. --make_examples_extra_args=""normalize_reads=true"" \. ${region_arg} \. --output_vcf=""${output_vcf}"" \. --output_gvcf=""${output_gvcf}"" \. --intermediate_results_dir ""/paedyl01/disk1/yangyxt/test_tmp/${singularity_inter}"" \. --num_shards=${threads}`. - Error trace: (if applicable). **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**. Here is the IGV screenshot of the position where DeepVariant failed to identify one bp deletion (Upper panel illustrates the alignment from raw BAM file, lower panel illustrates the alignment from the realigned BAM file from GATK HaplotypeCaller):. ![image](https://user-images.githubusercontent.com/40780228/218404096-273ed999-6443-43c2-83b9-108661d738d4.png). P.S. Please consider granting a parameter of DeepVariant to let users generate the realigned BAM file from DeepVariant. Thanks! .",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/612
https://github.com/google/deepvariant/issues/612:1889,modifiability,paramet,parameter,1889,"Missing a 1-bp deletion comparing to GATK (potential denovo variant); **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**:. Yes. **Describe the issue:**. A potential denovo variant is filtered out due to mendelian violation. While found the deletion in the same sample via GATK and IGV (both raw BAM file and realigned BAM file from GATK HaplotypeCaller). Wonder how to loosen the criteria to increase the recall of denovo variants. (A clear and concise description of what the issue is.). **Setup**. - Operating system: CentOS7. - DeepVariant version: 1.4. - Installation method (Docker, built from source, etc.): Singularity. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). - 150bp paired-end Illumina data. **Steps to reproduce:**. - Command: . `/opt/deepvariant/bin/run_deepvariant \. --model_type=${model_type} \. --ref=""${ref_genome}"" \. --reads=""${bam_file}"" \. --make_examples_extra_args=""normalize_reads=true"" \. ${region_arg} \. --output_vcf=""${output_vcf}"" \. --output_gvcf=""${output_gvcf}"" \. --intermediate_results_dir ""/paedyl01/disk1/yangyxt/test_tmp/${singularity_inter}"" \. --num_shards=${threads}`. - Error trace: (if applicable). **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**. Here is the IGV screenshot of the position where DeepVariant failed to identify one bp deletion (Upper panel illustrates the alignment from raw BAM file, lower panel illustrates the alignment from the realigned BAM file from GATK HaplotypeCaller):. ![image](https://user-images.githubusercontent.com/40780228/218404096-273ed999-6443-43c2-83b9-108661d738d4.png). P.S. Please consider granting a parameter of DeepVariant to let users generate the realigned BAM file from DeepVariant. Thanks! .",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/612
https://github.com/google/deepvariant/issues/612:1219,performance,Error,Error,1219,"Missing a 1-bp deletion comparing to GATK (potential denovo variant); **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**:. Yes. **Describe the issue:**. A potential denovo variant is filtered out due to mendelian violation. While found the deletion in the same sample via GATK and IGV (both raw BAM file and realigned BAM file from GATK HaplotypeCaller). Wonder how to loosen the criteria to increase the recall of denovo variants. (A clear and concise description of what the issue is.). **Setup**. - Operating system: CentOS7. - DeepVariant version: 1.4. - Installation method (Docker, built from source, etc.): Singularity. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). - 150bp paired-end Illumina data. **Steps to reproduce:**. - Command: . `/opt/deepvariant/bin/run_deepvariant \. --model_type=${model_type} \. --ref=""${ref_genome}"" \. --reads=""${bam_file}"" \. --make_examples_extra_args=""normalize_reads=true"" \. ${region_arg} \. --output_vcf=""${output_vcf}"" \. --output_gvcf=""${output_gvcf}"" \. --intermediate_results_dir ""/paedyl01/disk1/yangyxt/test_tmp/${singularity_inter}"" \. --num_shards=${threads}`. - Error trace: (if applicable). **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**. Here is the IGV screenshot of the position where DeepVariant failed to identify one bp deletion (Upper panel illustrates the alignment from raw BAM file, lower panel illustrates the alignment from the realigned BAM file from GATK HaplotypeCaller):. ![image](https://user-images.githubusercontent.com/40780228/218404096-273ed999-6443-43c2-83b9-108661d738d4.png). P.S. Please consider granting a parameter of DeepVariant to let users generate the realigned BAM file from DeepVariant. Thanks! .",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/612
https://github.com/google/deepvariant/issues/612:1251,reliability,Doe,Does,1251,"Missing a 1-bp deletion comparing to GATK (potential denovo variant); **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**:. Yes. **Describe the issue:**. A potential denovo variant is filtered out due to mendelian violation. While found the deletion in the same sample via GATK and IGV (both raw BAM file and realigned BAM file from GATK HaplotypeCaller). Wonder how to loosen the criteria to increase the recall of denovo variants. (A clear and concise description of what the issue is.). **Setup**. - Operating system: CentOS7. - DeepVariant version: 1.4. - Installation method (Docker, built from source, etc.): Singularity. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). - 150bp paired-end Illumina data. **Steps to reproduce:**. - Command: . `/opt/deepvariant/bin/run_deepvariant \. --model_type=${model_type} \. --ref=""${ref_genome}"" \. --reads=""${bam_file}"" \. --make_examples_extra_args=""normalize_reads=true"" \. ${region_arg} \. --output_vcf=""${output_vcf}"" \. --output_gvcf=""${output_gvcf}"" \. --intermediate_results_dir ""/paedyl01/disk1/yangyxt/test_tmp/${singularity_inter}"" \. --num_shards=${threads}`. - Error trace: (if applicable). **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**. Here is the IGV screenshot of the position where DeepVariant failed to identify one bp deletion (Upper panel illustrates the alignment from raw BAM file, lower panel illustrates the alignment from the realigned BAM file from GATK HaplotypeCaller):. ![image](https://user-images.githubusercontent.com/40780228/218404096-273ed999-6443-43c2-83b9-108661d738d4.png). P.S. Please consider granting a parameter of DeepVariant to let users generate the realigned BAM file from DeepVariant. Thanks! .",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/612
https://github.com/google/deepvariant/issues/612:1556,reliability,fail,failed,1556,"Missing a 1-bp deletion comparing to GATK (potential denovo variant); **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**:. Yes. **Describe the issue:**. A potential denovo variant is filtered out due to mendelian violation. While found the deletion in the same sample via GATK and IGV (both raw BAM file and realigned BAM file from GATK HaplotypeCaller). Wonder how to loosen the criteria to increase the recall of denovo variants. (A clear and concise description of what the issue is.). **Setup**. - Operating system: CentOS7. - DeepVariant version: 1.4. - Installation method (Docker, built from source, etc.): Singularity. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). - 150bp paired-end Illumina data. **Steps to reproduce:**. - Command: . `/opt/deepvariant/bin/run_deepvariant \. --model_type=${model_type} \. --ref=""${ref_genome}"" \. --reads=""${bam_file}"" \. --make_examples_extra_args=""normalize_reads=true"" \. ${region_arg} \. --output_vcf=""${output_vcf}"" \. --output_gvcf=""${output_gvcf}"" \. --intermediate_results_dir ""/paedyl01/disk1/yangyxt/test_tmp/${singularity_inter}"" \. --num_shards=${threads}`. - Error trace: (if applicable). **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**. Here is the IGV screenshot of the position where DeepVariant failed to identify one bp deletion (Upper panel illustrates the alignment from raw BAM file, lower panel illustrates the alignment from the realigned BAM file from GATK HaplotypeCaller):. ![image](https://user-images.githubusercontent.com/40780228/218404096-273ed999-6443-43c2-83b9-108661d738d4.png). P.S. Please consider granting a parameter of DeepVariant to let users generate the realigned BAM file from DeepVariant. Thanks! .",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/612
https://github.com/google/deepvariant/issues/612:1219,safety,Error,Error,1219,"Missing a 1-bp deletion comparing to GATK (potential denovo variant); **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**:. Yes. **Describe the issue:**. A potential denovo variant is filtered out due to mendelian violation. While found the deletion in the same sample via GATK and IGV (both raw BAM file and realigned BAM file from GATK HaplotypeCaller). Wonder how to loosen the criteria to increase the recall of denovo variants. (A clear and concise description of what the issue is.). **Setup**. - Operating system: CentOS7. - DeepVariant version: 1.4. - Installation method (Docker, built from source, etc.): Singularity. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). - 150bp paired-end Illumina data. **Steps to reproduce:**. - Command: . `/opt/deepvariant/bin/run_deepvariant \. --model_type=${model_type} \. --ref=""${ref_genome}"" \. --reads=""${bam_file}"" \. --make_examples_extra_args=""normalize_reads=true"" \. ${region_arg} \. --output_vcf=""${output_vcf}"" \. --output_gvcf=""${output_gvcf}"" \. --intermediate_results_dir ""/paedyl01/disk1/yangyxt/test_tmp/${singularity_inter}"" \. --num_shards=${threads}`. - Error trace: (if applicable). **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**. Here is the IGV screenshot of the position where DeepVariant failed to identify one bp deletion (Upper panel illustrates the alignment from raw BAM file, lower panel illustrates the alignment from the realigned BAM file from GATK HaplotypeCaller):. ![image](https://user-images.githubusercontent.com/40780228/218404096-273ed999-6443-43c2-83b9-108661d738d4.png). P.S. Please consider granting a parameter of DeepVariant to let users generate the realigned BAM file from DeepVariant. Thanks! .",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/612
https://github.com/google/deepvariant/issues/612:1272,safety,test,test,1272,"Missing a 1-bp deletion comparing to GATK (potential denovo variant); **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**:. Yes. **Describe the issue:**. A potential denovo variant is filtered out due to mendelian violation. While found the deletion in the same sample via GATK and IGV (both raw BAM file and realigned BAM file from GATK HaplotypeCaller). Wonder how to loosen the criteria to increase the recall of denovo variants. (A clear and concise description of what the issue is.). **Setup**. - Operating system: CentOS7. - DeepVariant version: 1.4. - Installation method (Docker, built from source, etc.): Singularity. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). - 150bp paired-end Illumina data. **Steps to reproduce:**. - Command: . `/opt/deepvariant/bin/run_deepvariant \. --model_type=${model_type} \. --ref=""${ref_genome}"" \. --reads=""${bam_file}"" \. --make_examples_extra_args=""normalize_reads=true"" \. ${region_arg} \. --output_vcf=""${output_vcf}"" \. --output_gvcf=""${output_gvcf}"" \. --intermediate_results_dir ""/paedyl01/disk1/yangyxt/test_tmp/${singularity_inter}"" \. --num_shards=${threads}`. - Error trace: (if applicable). **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**. Here is the IGV screenshot of the position where DeepVariant failed to identify one bp deletion (Upper panel illustrates the alignment from raw BAM file, lower panel illustrates the alignment from the realigned BAM file from GATK HaplotypeCaller):. ![image](https://user-images.githubusercontent.com/40780228/218404096-273ed999-6443-43c2-83b9-108661d738d4.png). P.S. Please consider granting a parameter of DeepVariant to let users generate the realigned BAM file from DeepVariant. Thanks! .",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/612
https://github.com/google/deepvariant/issues/612:1308,safety,test,test,1308,"Missing a 1-bp deletion comparing to GATK (potential denovo variant); **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**:. Yes. **Describe the issue:**. A potential denovo variant is filtered out due to mendelian violation. While found the deletion in the same sample via GATK and IGV (both raw BAM file and realigned BAM file from GATK HaplotypeCaller). Wonder how to loosen the criteria to increase the recall of denovo variants. (A clear and concise description of what the issue is.). **Setup**. - Operating system: CentOS7. - DeepVariant version: 1.4. - Installation method (Docker, built from source, etc.): Singularity. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). - 150bp paired-end Illumina data. **Steps to reproduce:**. - Command: . `/opt/deepvariant/bin/run_deepvariant \. --model_type=${model_type} \. --ref=""${ref_genome}"" \. --reads=""${bam_file}"" \. --make_examples_extra_args=""normalize_reads=true"" \. ${region_arg} \. --output_vcf=""${output_vcf}"" \. --output_gvcf=""${output_gvcf}"" \. --intermediate_results_dir ""/paedyl01/disk1/yangyxt/test_tmp/${singularity_inter}"" \. --num_shards=${threads}`. - Error trace: (if applicable). **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**. Here is the IGV screenshot of the position where DeepVariant failed to identify one bp deletion (Upper panel illustrates the alignment from raw BAM file, lower panel illustrates the alignment from the realigned BAM file from GATK HaplotypeCaller):. ![image](https://user-images.githubusercontent.com/40780228/218404096-273ed999-6443-43c2-83b9-108661d738d4.png). P.S. Please consider granting a parameter of DeepVariant to let users generate the realigned BAM file from DeepVariant. Thanks! .",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/612
https://github.com/google/deepvariant/issues/612:1566,security,ident,identify,1566,"Missing a 1-bp deletion comparing to GATK (potential denovo variant); **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**:. Yes. **Describe the issue:**. A potential denovo variant is filtered out due to mendelian violation. While found the deletion in the same sample via GATK and IGV (both raw BAM file and realigned BAM file from GATK HaplotypeCaller). Wonder how to loosen the criteria to increase the recall of denovo variants. (A clear and concise description of what the issue is.). **Setup**. - Operating system: CentOS7. - DeepVariant version: 1.4. - Installation method (Docker, built from source, etc.): Singularity. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). - 150bp paired-end Illumina data. **Steps to reproduce:**. - Command: . `/opt/deepvariant/bin/run_deepvariant \. --model_type=${model_type} \. --ref=""${ref_genome}"" \. --reads=""${bam_file}"" \. --make_examples_extra_args=""normalize_reads=true"" \. ${region_arg} \. --output_vcf=""${output_vcf}"" \. --output_gvcf=""${output_gvcf}"" \. --intermediate_results_dir ""/paedyl01/disk1/yangyxt/test_tmp/${singularity_inter}"" \. --num_shards=${threads}`. - Error trace: (if applicable). **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**. Here is the IGV screenshot of the position where DeepVariant failed to identify one bp deletion (Upper panel illustrates the alignment from raw BAM file, lower panel illustrates the alignment from the realigned BAM file from GATK HaplotypeCaller):. ![image](https://user-images.githubusercontent.com/40780228/218404096-273ed999-6443-43c2-83b9-108661d738d4.png). P.S. Please consider granting a parameter of DeepVariant to let users generate the realigned BAM file from DeepVariant. Thanks! .",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/612
https://github.com/google/deepvariant/issues/612:694,testability,instrument,instrument,694,"Missing a 1-bp deletion comparing to GATK (potential denovo variant); **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**:. Yes. **Describe the issue:**. A potential denovo variant is filtered out due to mendelian violation. While found the deletion in the same sample via GATK and IGV (both raw BAM file and realigned BAM file from GATK HaplotypeCaller). Wonder how to loosen the criteria to increase the recall of denovo variants. (A clear and concise description of what the issue is.). **Setup**. - Operating system: CentOS7. - DeepVariant version: 1.4. - Installation method (Docker, built from source, etc.): Singularity. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). - 150bp paired-end Illumina data. **Steps to reproduce:**. - Command: . `/opt/deepvariant/bin/run_deepvariant \. --model_type=${model_type} \. --ref=""${ref_genome}"" \. --reads=""${bam_file}"" \. --make_examples_extra_args=""normalize_reads=true"" \. ${region_arg} \. --output_vcf=""${output_vcf}"" \. --output_gvcf=""${output_gvcf}"" \. --intermediate_results_dir ""/paedyl01/disk1/yangyxt/test_tmp/${singularity_inter}"" \. --num_shards=${threads}`. - Error trace: (if applicable). **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**. Here is the IGV screenshot of the position where DeepVariant failed to identify one bp deletion (Upper panel illustrates the alignment from raw BAM file, lower panel illustrates the alignment from the realigned BAM file from GATK HaplotypeCaller):. ![image](https://user-images.githubusercontent.com/40780228/218404096-273ed999-6443-43c2-83b9-108661d738d4.png). P.S. Please consider granting a parameter of DeepVariant to let users generate the realigned BAM file from DeepVariant. Thanks! .",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/612
https://github.com/google/deepvariant/issues/612:1225,testability,trace,trace,1225,"Missing a 1-bp deletion comparing to GATK (potential denovo variant); **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**:. Yes. **Describe the issue:**. A potential denovo variant is filtered out due to mendelian violation. While found the deletion in the same sample via GATK and IGV (both raw BAM file and realigned BAM file from GATK HaplotypeCaller). Wonder how to loosen the criteria to increase the recall of denovo variants. (A clear and concise description of what the issue is.). **Setup**. - Operating system: CentOS7. - DeepVariant version: 1.4. - Installation method (Docker, built from source, etc.): Singularity. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). - 150bp paired-end Illumina data. **Steps to reproduce:**. - Command: . `/opt/deepvariant/bin/run_deepvariant \. --model_type=${model_type} \. --ref=""${ref_genome}"" \. --reads=""${bam_file}"" \. --make_examples_extra_args=""normalize_reads=true"" \. ${region_arg} \. --output_vcf=""${output_vcf}"" \. --output_gvcf=""${output_gvcf}"" \. --intermediate_results_dir ""/paedyl01/disk1/yangyxt/test_tmp/${singularity_inter}"" \. --num_shards=${threads}`. - Error trace: (if applicable). **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**. Here is the IGV screenshot of the position where DeepVariant failed to identify one bp deletion (Upper panel illustrates the alignment from raw BAM file, lower panel illustrates the alignment from the realigned BAM file from GATK HaplotypeCaller):. ![image](https://user-images.githubusercontent.com/40780228/218404096-273ed999-6443-43c2-83b9-108661d738d4.png). P.S. Please consider granting a parameter of DeepVariant to let users generate the realigned BAM file from DeepVariant. Thanks! .",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/612
https://github.com/google/deepvariant/issues/612:1272,testability,test,test,1272,"Missing a 1-bp deletion comparing to GATK (potential denovo variant); **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**:. Yes. **Describe the issue:**. A potential denovo variant is filtered out due to mendelian violation. While found the deletion in the same sample via GATK and IGV (both raw BAM file and realigned BAM file from GATK HaplotypeCaller). Wonder how to loosen the criteria to increase the recall of denovo variants. (A clear and concise description of what the issue is.). **Setup**. - Operating system: CentOS7. - DeepVariant version: 1.4. - Installation method (Docker, built from source, etc.): Singularity. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). - 150bp paired-end Illumina data. **Steps to reproduce:**. - Command: . `/opt/deepvariant/bin/run_deepvariant \. --model_type=${model_type} \. --ref=""${ref_genome}"" \. --reads=""${bam_file}"" \. --make_examples_extra_args=""normalize_reads=true"" \. ${region_arg} \. --output_vcf=""${output_vcf}"" \. --output_gvcf=""${output_gvcf}"" \. --intermediate_results_dir ""/paedyl01/disk1/yangyxt/test_tmp/${singularity_inter}"" \. --num_shards=${threads}`. - Error trace: (if applicable). **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**. Here is the IGV screenshot of the position where DeepVariant failed to identify one bp deletion (Upper panel illustrates the alignment from raw BAM file, lower panel illustrates the alignment from the realigned BAM file from GATK HaplotypeCaller):. ![image](https://user-images.githubusercontent.com/40780228/218404096-273ed999-6443-43c2-83b9-108661d738d4.png). P.S. Please consider granting a parameter of DeepVariant to let users generate the realigned BAM file from DeepVariant. Thanks! .",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/612
https://github.com/google/deepvariant/issues/612:1308,testability,test,test,1308,"Missing a 1-bp deletion comparing to GATK (potential denovo variant); **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**:. Yes. **Describe the issue:**. A potential denovo variant is filtered out due to mendelian violation. While found the deletion in the same sample via GATK and IGV (both raw BAM file and realigned BAM file from GATK HaplotypeCaller). Wonder how to loosen the criteria to increase the recall of denovo variants. (A clear and concise description of what the issue is.). **Setup**. - Operating system: CentOS7. - DeepVariant version: 1.4. - Installation method (Docker, built from source, etc.): Singularity. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). - 150bp paired-end Illumina data. **Steps to reproduce:**. - Command: . `/opt/deepvariant/bin/run_deepvariant \. --model_type=${model_type} \. --ref=""${ref_genome}"" \. --reads=""${bam_file}"" \. --make_examples_extra_args=""normalize_reads=true"" \. ${region_arg} \. --output_vcf=""${output_vcf}"" \. --output_gvcf=""${output_gvcf}"" \. --intermediate_results_dir ""/paedyl01/disk1/yangyxt/test_tmp/${singularity_inter}"" \. --num_shards=${threads}`. - Error trace: (if applicable). **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**. Here is the IGV screenshot of the position where DeepVariant failed to identify one bp deletion (Upper panel illustrates the alignment from raw BAM file, lower panel illustrates the alignment from the realigned BAM file from GATK HaplotypeCaller):. ![image](https://user-images.githubusercontent.com/40780228/218404096-273ed999-6443-43c2-83b9-108661d738d4.png). P.S. Please consider granting a parameter of DeepVariant to let users generate the realigned BAM file from DeepVariant. Thanks! .",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/612
https://github.com/google/deepvariant/issues/612:1483,testability,context,context,1483,"Missing a 1-bp deletion comparing to GATK (potential denovo variant); **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**:. Yes. **Describe the issue:**. A potential denovo variant is filtered out due to mendelian violation. While found the deletion in the same sample via GATK and IGV (both raw BAM file and realigned BAM file from GATK HaplotypeCaller). Wonder how to loosen the criteria to increase the recall of denovo variants. (A clear and concise description of what the issue is.). **Setup**. - Operating system: CentOS7. - DeepVariant version: 1.4. - Installation method (Docker, built from source, etc.): Singularity. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). - 150bp paired-end Illumina data. **Steps to reproduce:**. - Command: . `/opt/deepvariant/bin/run_deepvariant \. --model_type=${model_type} \. --ref=""${ref_genome}"" \. --reads=""${bam_file}"" \. --make_examples_extra_args=""normalize_reads=true"" \. ${region_arg} \. --output_vcf=""${output_vcf}"" \. --output_gvcf=""${output_gvcf}"" \. --intermediate_results_dir ""/paedyl01/disk1/yangyxt/test_tmp/${singularity_inter}"" \. --num_shards=${threads}`. - Error trace: (if applicable). **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**. Here is the IGV screenshot of the position where DeepVariant failed to identify one bp deletion (Upper panel illustrates the alignment from raw BAM file, lower panel illustrates the alignment from the realigned BAM file from GATK HaplotypeCaller):. ![image](https://user-images.githubusercontent.com/40780228/218404096-273ed999-6443-43c2-83b9-108661d738d4.png). P.S. Please consider granting a parameter of DeepVariant to let users generate the realigned BAM file from DeepVariant. Thanks! .",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/612
https://github.com/google/deepvariant/issues/612:474,usability,clear,clear,474,"Missing a 1-bp deletion comparing to GATK (potential denovo variant); **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**:. Yes. **Describe the issue:**. A potential denovo variant is filtered out due to mendelian violation. While found the deletion in the same sample via GATK and IGV (both raw BAM file and realigned BAM file from GATK HaplotypeCaller). Wonder how to loosen the criteria to increase the recall of denovo variants. (A clear and concise description of what the issue is.). **Setup**. - Operating system: CentOS7. - DeepVariant version: 1.4. - Installation method (Docker, built from source, etc.): Singularity. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). - 150bp paired-end Illumina data. **Steps to reproduce:**. - Command: . `/opt/deepvariant/bin/run_deepvariant \. --model_type=${model_type} \. --ref=""${ref_genome}"" \. --reads=""${bam_file}"" \. --make_examples_extra_args=""normalize_reads=true"" \. ${region_arg} \. --output_vcf=""${output_vcf}"" \. --output_gvcf=""${output_gvcf}"" \. --intermediate_results_dir ""/paedyl01/disk1/yangyxt/test_tmp/${singularity_inter}"" \. --num_shards=${threads}`. - Error trace: (if applicable). **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**. Here is the IGV screenshot of the position where DeepVariant failed to identify one bp deletion (Upper panel illustrates the alignment from raw BAM file, lower panel illustrates the alignment from the realigned BAM file from GATK HaplotypeCaller):. ![image](https://user-images.githubusercontent.com/40780228/218404096-273ed999-6443-43c2-83b9-108661d738d4.png). P.S. Please consider granting a parameter of DeepVariant to let users generate the realigned BAM file from DeepVariant. Thanks! .",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/612
https://github.com/google/deepvariant/issues/612:837,usability,Command,Command,837,"Missing a 1-bp deletion comparing to GATK (potential denovo variant); **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**:. Yes. **Describe the issue:**. A potential denovo variant is filtered out due to mendelian violation. While found the deletion in the same sample via GATK and IGV (both raw BAM file and realigned BAM file from GATK HaplotypeCaller). Wonder how to loosen the criteria to increase the recall of denovo variants. (A clear and concise description of what the issue is.). **Setup**. - Operating system: CentOS7. - DeepVariant version: 1.4. - Installation method (Docker, built from source, etc.): Singularity. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). - 150bp paired-end Illumina data. **Steps to reproduce:**. - Command: . `/opt/deepvariant/bin/run_deepvariant \. --model_type=${model_type} \. --ref=""${ref_genome}"" \. --reads=""${bam_file}"" \. --make_examples_extra_args=""normalize_reads=true"" \. ${region_arg} \. --output_vcf=""${output_vcf}"" \. --output_gvcf=""${output_gvcf}"" \. --intermediate_results_dir ""/paedyl01/disk1/yangyxt/test_tmp/${singularity_inter}"" \. --num_shards=${threads}`. - Error trace: (if applicable). **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**. Here is the IGV screenshot of the position where DeepVariant failed to identify one bp deletion (Upper panel illustrates the alignment from raw BAM file, lower panel illustrates the alignment from the realigned BAM file from GATK HaplotypeCaller):. ![image](https://user-images.githubusercontent.com/40780228/218404096-273ed999-6443-43c2-83b9-108661d738d4.png). P.S. Please consider granting a parameter of DeepVariant to let users generate the realigned BAM file from DeepVariant. Thanks! .",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/612
https://github.com/google/deepvariant/issues/612:1219,usability,Error,Error,1219,"Missing a 1-bp deletion comparing to GATK (potential denovo variant); **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**:. Yes. **Describe the issue:**. A potential denovo variant is filtered out due to mendelian violation. While found the deletion in the same sample via GATK and IGV (both raw BAM file and realigned BAM file from GATK HaplotypeCaller). Wonder how to loosen the criteria to increase the recall of denovo variants. (A clear and concise description of what the issue is.). **Setup**. - Operating system: CentOS7. - DeepVariant version: 1.4. - Installation method (Docker, built from source, etc.): Singularity. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). - 150bp paired-end Illumina data. **Steps to reproduce:**. - Command: . `/opt/deepvariant/bin/run_deepvariant \. --model_type=${model_type} \. --ref=""${ref_genome}"" \. --reads=""${bam_file}"" \. --make_examples_extra_args=""normalize_reads=true"" \. ${region_arg} \. --output_vcf=""${output_vcf}"" \. --output_gvcf=""${output_gvcf}"" \. --intermediate_results_dir ""/paedyl01/disk1/yangyxt/test_tmp/${singularity_inter}"" \. --num_shards=${threads}`. - Error trace: (if applicable). **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**. Here is the IGV screenshot of the position where DeepVariant failed to identify one bp deletion (Upper panel illustrates the alignment from raw BAM file, lower panel illustrates the alignment from the realigned BAM file from GATK HaplotypeCaller):. ![image](https://user-images.githubusercontent.com/40780228/218404096-273ed999-6443-43c2-83b9-108661d738d4.png). P.S. Please consider granting a parameter of DeepVariant to let users generate the realigned BAM file from DeepVariant. Thanks! .",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/612
https://github.com/google/deepvariant/issues/612:1761,usability,user,user-images,1761,"Missing a 1-bp deletion comparing to GATK (potential denovo variant); **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**:. Yes. **Describe the issue:**. A potential denovo variant is filtered out due to mendelian violation. While found the deletion in the same sample via GATK and IGV (both raw BAM file and realigned BAM file from GATK HaplotypeCaller). Wonder how to loosen the criteria to increase the recall of denovo variants. (A clear and concise description of what the issue is.). **Setup**. - Operating system: CentOS7. - DeepVariant version: 1.4. - Installation method (Docker, built from source, etc.): Singularity. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). - 150bp paired-end Illumina data. **Steps to reproduce:**. - Command: . `/opt/deepvariant/bin/run_deepvariant \. --model_type=${model_type} \. --ref=""${ref_genome}"" \. --reads=""${bam_file}"" \. --make_examples_extra_args=""normalize_reads=true"" \. ${region_arg} \. --output_vcf=""${output_vcf}"" \. --output_gvcf=""${output_gvcf}"" \. --intermediate_results_dir ""/paedyl01/disk1/yangyxt/test_tmp/${singularity_inter}"" \. --num_shards=${threads}`. - Error trace: (if applicable). **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**. Here is the IGV screenshot of the position where DeepVariant failed to identify one bp deletion (Upper panel illustrates the alignment from raw BAM file, lower panel illustrates the alignment from the realigned BAM file from GATK HaplotypeCaller):. ![image](https://user-images.githubusercontent.com/40780228/218404096-273ed999-6443-43c2-83b9-108661d738d4.png). P.S. Please consider granting a parameter of DeepVariant to let users generate the realigned BAM file from DeepVariant. Thanks! .",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/612
https://github.com/google/deepvariant/issues/612:1921,usability,user,users,1921,"Missing a 1-bp deletion comparing to GATK (potential denovo variant); **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**:. Yes. **Describe the issue:**. A potential denovo variant is filtered out due to mendelian violation. While found the deletion in the same sample via GATK and IGV (both raw BAM file and realigned BAM file from GATK HaplotypeCaller). Wonder how to loosen the criteria to increase the recall of denovo variants. (A clear and concise description of what the issue is.). **Setup**. - Operating system: CentOS7. - DeepVariant version: 1.4. - Installation method (Docker, built from source, etc.): Singularity. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). - 150bp paired-end Illumina data. **Steps to reproduce:**. - Command: . `/opt/deepvariant/bin/run_deepvariant \. --model_type=${model_type} \. --ref=""${ref_genome}"" \. --reads=""${bam_file}"" \. --make_examples_extra_args=""normalize_reads=true"" \. ${region_arg} \. --output_vcf=""${output_vcf}"" \. --output_gvcf=""${output_gvcf}"" \. --intermediate_results_dir ""/paedyl01/disk1/yangyxt/test_tmp/${singularity_inter}"" \. --num_shards=${threads}`. - Error trace: (if applicable). **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**. Here is the IGV screenshot of the position where DeepVariant failed to identify one bp deletion (Upper panel illustrates the alignment from raw BAM file, lower panel illustrates the alignment from the realigned BAM file from GATK HaplotypeCaller):. ![image](https://user-images.githubusercontent.com/40780228/218404096-273ed999-6443-43c2-83b9-108661d738d4.png). P.S. Please consider granting a parameter of DeepVariant to let users generate the realigned BAM file from DeepVariant. Thanks! .",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/612
https://github.com/google/deepvariant/issues/613:27,availability,error,error,27,"TMPDIR and locale settings error！please help me!; Hello, I got an error when running deepvariant. I've verified that the path `/path1/8_Environment/TMPDIR`exists. I've googled all over and still can't solve the problem. please help me! command：. ```. INPUT_DIR=/path1/4_Test/qingjiang/dpv. OUTPUT_DIR=/path1/4_Test/qingjiang/dpv. singularity run /path/dpv/deepvariant_1.4.0.sif /opt/deepvariant/bin/run_deepvariant \. --num_shards=3 \. --model_type=PACBIO \. --ref=""${INPUT_DIR}""/QJref.fa \. --reads=""${INPUT_DIR}""/input.bam \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. ```. error：. ```. I0213 16:54:59.595547 140573030586176 run_deepvariant.py:342] Re-using the directory for intermediate results in /path/dpv/intermediate_results_dir. ***** Intermediate results will be written to /path/dpv/intermediate_results_dir in docker. ****. ***** Running the command:*****. time seq 0 2 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/path/dpv/QJref.fa"" --reads ""/path/dpv/input.bam"" --examples ""/dellfsq. perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. LANGUAGE = (unset),. LC_ALL = (unset),. LC_CTYPE = ""C.UTF-8"",. LANG = ""en_US.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. LANGUAGE = (unset),. LC_ALL = (unset),. LC_CTYPE = ""C.UTF-8"",. LANG = ""en_US.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). Error in tempfile() using template /path1/8_Environment/TMPDIR/parXXXXX.par: Parent directory (/path1/8_Environment/TMPDIR/) does not exist at /usr/bin/parallel line 3889. real 0m3.019s. user 0m0.211s. sys 0m0.371s. ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/613
https://github.com/google/deepvariant/issues/613:66,availability,error,error,66,"TMPDIR and locale settings error！please help me!; Hello, I got an error when running deepvariant. I've verified that the path `/path1/8_Environment/TMPDIR`exists. I've googled all over and still can't solve the problem. please help me! command：. ```. INPUT_DIR=/path1/4_Test/qingjiang/dpv. OUTPUT_DIR=/path1/4_Test/qingjiang/dpv. singularity run /path/dpv/deepvariant_1.4.0.sif /opt/deepvariant/bin/run_deepvariant \. --num_shards=3 \. --model_type=PACBIO \. --ref=""${INPUT_DIR}""/QJref.fa \. --reads=""${INPUT_DIR}""/input.bam \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. ```. error：. ```. I0213 16:54:59.595547 140573030586176 run_deepvariant.py:342] Re-using the directory for intermediate results in /path/dpv/intermediate_results_dir. ***** Intermediate results will be written to /path/dpv/intermediate_results_dir in docker. ****. ***** Running the command:*****. time seq 0 2 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/path/dpv/QJref.fa"" --reads ""/path/dpv/input.bam"" --examples ""/dellfsq. perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. LANGUAGE = (unset),. LC_ALL = (unset),. LC_CTYPE = ""C.UTF-8"",. LANG = ""en_US.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. LANGUAGE = (unset),. LC_ALL = (unset),. LC_CTYPE = ""C.UTF-8"",. LANG = ""en_US.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). Error in tempfile() using template /path1/8_Environment/TMPDIR/parXXXXX.par: Parent directory (/path1/8_Environment/TMPDIR/) does not exist at /usr/bin/parallel line 3889. real 0m3.019s. user 0m0.211s. sys 0m0.371s. ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/613
https://github.com/google/deepvariant/issues/613:699,availability,error,error,699,"TMPDIR and locale settings error！please help me!; Hello, I got an error when running deepvariant. I've verified that the path `/path1/8_Environment/TMPDIR`exists. I've googled all over and still can't solve the problem. please help me! command：. ```. INPUT_DIR=/path1/4_Test/qingjiang/dpv. OUTPUT_DIR=/path1/4_Test/qingjiang/dpv. singularity run /path/dpv/deepvariant_1.4.0.sif /opt/deepvariant/bin/run_deepvariant \. --num_shards=3 \. --model_type=PACBIO \. --ref=""${INPUT_DIR}""/QJref.fa \. --reads=""${INPUT_DIR}""/input.bam \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. ```. error：. ```. I0213 16:54:59.595547 140573030586176 run_deepvariant.py:342] Re-using the directory for intermediate results in /path/dpv/intermediate_results_dir. ***** Intermediate results will be written to /path/dpv/intermediate_results_dir in docker. ****. ***** Running the command:*****. time seq 0 2 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/path/dpv/QJref.fa"" --reads ""/path/dpv/input.bam"" --examples ""/dellfsq. perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. LANGUAGE = (unset),. LC_ALL = (unset),. LC_CTYPE = ""C.UTF-8"",. LANG = ""en_US.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. LANGUAGE = (unset),. LC_ALL = (unset),. LC_CTYPE = ""C.UTF-8"",. LANG = ""en_US.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). Error in tempfile() using template /path1/8_Environment/TMPDIR/parXXXXX.par: Parent directory (/path1/8_Environment/TMPDIR/) does not exist at /usr/bin/parallel line 3889. real 0m3.019s. user 0m0.211s. sys 0m0.371s. ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/613
https://github.com/google/deepvariant/issues/613:1733,availability,Error,Error,1733,"TMPDIR and locale settings error！please help me!; Hello, I got an error when running deepvariant. I've verified that the path `/path1/8_Environment/TMPDIR`exists. I've googled all over and still can't solve the problem. please help me! command：. ```. INPUT_DIR=/path1/4_Test/qingjiang/dpv. OUTPUT_DIR=/path1/4_Test/qingjiang/dpv. singularity run /path/dpv/deepvariant_1.4.0.sif /opt/deepvariant/bin/run_deepvariant \. --num_shards=3 \. --model_type=PACBIO \. --ref=""${INPUT_DIR}""/QJref.fa \. --reads=""${INPUT_DIR}""/input.bam \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. ```. error：. ```. I0213 16:54:59.595547 140573030586176 run_deepvariant.py:342] Re-using the directory for intermediate results in /path/dpv/intermediate_results_dir. ***** Intermediate results will be written to /path/dpv/intermediate_results_dir in docker. ****. ***** Running the command:*****. time seq 0 2 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/path/dpv/QJref.fa"" --reads ""/path/dpv/input.bam"" --examples ""/dellfsq. perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. LANGUAGE = (unset),. LC_ALL = (unset),. LC_CTYPE = ""C.UTF-8"",. LANG = ""en_US.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. LANGUAGE = (unset),. LC_ALL = (unset),. LC_CTYPE = ""C.UTF-8"",. LANG = ""en_US.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). Error in tempfile() using template /path1/8_Environment/TMPDIR/parXXXXX.par: Parent directory (/path1/8_Environment/TMPDIR/) does not exist at /usr/bin/parallel line 3889. real 0m3.019s. user 0m0.211s. sys 0m0.371s. ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/613
https://github.com/google/deepvariant/issues/613:1201,deployability,fail,failed,1201,"TMPDIR and locale settings error！please help me!; Hello, I got an error when running deepvariant. I've verified that the path `/path1/8_Environment/TMPDIR`exists. I've googled all over and still can't solve the problem. please help me! command：. ```. INPUT_DIR=/path1/4_Test/qingjiang/dpv. OUTPUT_DIR=/path1/4_Test/qingjiang/dpv. singularity run /path/dpv/deepvariant_1.4.0.sif /opt/deepvariant/bin/run_deepvariant \. --num_shards=3 \. --model_type=PACBIO \. --ref=""${INPUT_DIR}""/QJref.fa \. --reads=""${INPUT_DIR}""/input.bam \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. ```. error：. ```. I0213 16:54:59.595547 140573030586176 run_deepvariant.py:342] Re-using the directory for intermediate results in /path/dpv/intermediate_results_dir. ***** Intermediate results will be written to /path/dpv/intermediate_results_dir in docker. ****. ***** Running the command:*****. time seq 0 2 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/path/dpv/QJref.fa"" --reads ""/path/dpv/input.bam"" --examples ""/dellfsq. perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. LANGUAGE = (unset),. LC_ALL = (unset),. LC_CTYPE = ""C.UTF-8"",. LANG = ""en_US.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. LANGUAGE = (unset),. LC_ALL = (unset),. LC_CTYPE = ""C.UTF-8"",. LANG = ""en_US.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). Error in tempfile() using template /path1/8_Environment/TMPDIR/parXXXXX.par: Parent directory (/path1/8_Environment/TMPDIR/) does not exist at /usr/bin/parallel line 3889. real 0m3.019s. user 0m0.211s. sys 0m0.371s. ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/613
https://github.com/google/deepvariant/issues/613:1368,deployability,instal,installed,1368,"TMPDIR and locale settings error！please help me!; Hello, I got an error when running deepvariant. I've verified that the path `/path1/8_Environment/TMPDIR`exists. I've googled all over and still can't solve the problem. please help me! command：. ```. INPUT_DIR=/path1/4_Test/qingjiang/dpv. OUTPUT_DIR=/path1/4_Test/qingjiang/dpv. singularity run /path/dpv/deepvariant_1.4.0.sif /opt/deepvariant/bin/run_deepvariant \. --num_shards=3 \. --model_type=PACBIO \. --ref=""${INPUT_DIR}""/QJref.fa \. --reads=""${INPUT_DIR}""/input.bam \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. ```. error：. ```. I0213 16:54:59.595547 140573030586176 run_deepvariant.py:342] Re-using the directory for intermediate results in /path/dpv/intermediate_results_dir. ***** Intermediate results will be written to /path/dpv/intermediate_results_dir in docker. ****. ***** Running the command:*****. time seq 0 2 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/path/dpv/QJref.fa"" --reads ""/path/dpv/input.bam"" --examples ""/dellfsq. perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. LANGUAGE = (unset),. LC_ALL = (unset),. LC_CTYPE = ""C.UTF-8"",. LANG = ""en_US.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. LANGUAGE = (unset),. LC_ALL = (unset),. LC_CTYPE = ""C.UTF-8"",. LANG = ""en_US.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). Error in tempfile() using template /path1/8_Environment/TMPDIR/parXXXXX.par: Parent directory (/path1/8_Environment/TMPDIR/) does not exist at /usr/bin/parallel line 3889. real 0m3.019s. user 0m0.211s. sys 0m0.371s. ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/613
https://github.com/google/deepvariant/issues/613:1482,deployability,fail,failed,1482,"TMPDIR and locale settings error！please help me!; Hello, I got an error when running deepvariant. I've verified that the path `/path1/8_Environment/TMPDIR`exists. I've googled all over and still can't solve the problem. please help me! command：. ```. INPUT_DIR=/path1/4_Test/qingjiang/dpv. OUTPUT_DIR=/path1/4_Test/qingjiang/dpv. singularity run /path/dpv/deepvariant_1.4.0.sif /opt/deepvariant/bin/run_deepvariant \. --num_shards=3 \. --model_type=PACBIO \. --ref=""${INPUT_DIR}""/QJref.fa \. --reads=""${INPUT_DIR}""/input.bam \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. ```. error：. ```. I0213 16:54:59.595547 140573030586176 run_deepvariant.py:342] Re-using the directory for intermediate results in /path/dpv/intermediate_results_dir. ***** Intermediate results will be written to /path/dpv/intermediate_results_dir in docker. ****. ***** Running the command:*****. time seq 0 2 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/path/dpv/QJref.fa"" --reads ""/path/dpv/input.bam"" --examples ""/dellfsq. perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. LANGUAGE = (unset),. LC_ALL = (unset),. LC_CTYPE = ""C.UTF-8"",. LANG = ""en_US.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. LANGUAGE = (unset),. LC_ALL = (unset),. LC_CTYPE = ""C.UTF-8"",. LANG = ""en_US.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). Error in tempfile() using template /path1/8_Environment/TMPDIR/parXXXXX.par: Parent directory (/path1/8_Environment/TMPDIR/) does not exist at /usr/bin/parallel line 3889. real 0m3.019s. user 0m0.211s. sys 0m0.371s. ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/613
https://github.com/google/deepvariant/issues/613:1649,deployability,instal,installed,1649,"TMPDIR and locale settings error！please help me!; Hello, I got an error when running deepvariant. I've verified that the path `/path1/8_Environment/TMPDIR`exists. I've googled all over and still can't solve the problem. please help me! command：. ```. INPUT_DIR=/path1/4_Test/qingjiang/dpv. OUTPUT_DIR=/path1/4_Test/qingjiang/dpv. singularity run /path/dpv/deepvariant_1.4.0.sif /opt/deepvariant/bin/run_deepvariant \. --num_shards=3 \. --model_type=PACBIO \. --ref=""${INPUT_DIR}""/QJref.fa \. --reads=""${INPUT_DIR}""/input.bam \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. ```. error：. ```. I0213 16:54:59.595547 140573030586176 run_deepvariant.py:342] Re-using the directory for intermediate results in /path/dpv/intermediate_results_dir. ***** Intermediate results will be written to /path/dpv/intermediate_results_dir in docker. ****. ***** Running the command:*****. time seq 0 2 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/path/dpv/QJref.fa"" --reads ""/path/dpv/input.bam"" --examples ""/dellfsq. perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. LANGUAGE = (unset),. LC_ALL = (unset),. LC_CTYPE = ""C.UTF-8"",. LANG = ""en_US.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. LANGUAGE = (unset),. LC_ALL = (unset),. LC_CTYPE = ""C.UTF-8"",. LANG = ""en_US.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). Error in tempfile() using template /path1/8_Environment/TMPDIR/parXXXXX.par: Parent directory (/path1/8_Environment/TMPDIR/) does not exist at /usr/bin/parallel line 3889. real 0m3.019s. user 0m0.211s. sys 0m0.371s. ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/613
https://github.com/google/deepvariant/issues/613:1035,integrability,buffer,buffer,1035,"TMPDIR and locale settings error！please help me!; Hello, I got an error when running deepvariant. I've verified that the path `/path1/8_Environment/TMPDIR`exists. I've googled all over and still can't solve the problem. please help me! command：. ```. INPUT_DIR=/path1/4_Test/qingjiang/dpv. OUTPUT_DIR=/path1/4_Test/qingjiang/dpv. singularity run /path/dpv/deepvariant_1.4.0.sif /opt/deepvariant/bin/run_deepvariant \. --num_shards=3 \. --model_type=PACBIO \. --ref=""${INPUT_DIR}""/QJref.fa \. --reads=""${INPUT_DIR}""/input.bam \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. ```. error：. ```. I0213 16:54:59.595547 140573030586176 run_deepvariant.py:342] Re-using the directory for intermediate results in /path/dpv/intermediate_results_dir. ***** Intermediate results will be written to /path/dpv/intermediate_results_dir in docker. ****. ***** Running the command:*****. time seq 0 2 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/path/dpv/QJref.fa"" --reads ""/path/dpv/input.bam"" --examples ""/dellfsq. perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. LANGUAGE = (unset),. LC_ALL = (unset),. LC_CTYPE = ""C.UTF-8"",. LANG = ""en_US.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. LANGUAGE = (unset),. LC_ALL = (unset),. LC_CTYPE = ""C.UTF-8"",. LANG = ""en_US.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). Error in tempfile() using template /path1/8_Environment/TMPDIR/parXXXXX.par: Parent directory (/path1/8_Environment/TMPDIR/) does not exist at /usr/bin/parallel line 3889. real 0m3.019s. user 0m0.211s. sys 0m0.371s. ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/613
https://github.com/google/deepvariant/issues/613:1429,interoperability,standard,standard,1429,"TMPDIR and locale settings error！please help me!; Hello, I got an error when running deepvariant. I've verified that the path `/path1/8_Environment/TMPDIR`exists. I've googled all over and still can't solve the problem. please help me! command：. ```. INPUT_DIR=/path1/4_Test/qingjiang/dpv. OUTPUT_DIR=/path1/4_Test/qingjiang/dpv. singularity run /path/dpv/deepvariant_1.4.0.sif /opt/deepvariant/bin/run_deepvariant \. --num_shards=3 \. --model_type=PACBIO \. --ref=""${INPUT_DIR}""/QJref.fa \. --reads=""${INPUT_DIR}""/input.bam \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. ```. error：. ```. I0213 16:54:59.595547 140573030586176 run_deepvariant.py:342] Re-using the directory for intermediate results in /path/dpv/intermediate_results_dir. ***** Intermediate results will be written to /path/dpv/intermediate_results_dir in docker. ****. ***** Running the command:*****. time seq 0 2 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/path/dpv/QJref.fa"" --reads ""/path/dpv/input.bam"" --examples ""/dellfsq. perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. LANGUAGE = (unset),. LC_ALL = (unset),. LC_CTYPE = ""C.UTF-8"",. LANG = ""en_US.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. LANGUAGE = (unset),. LC_ALL = (unset),. LC_CTYPE = ""C.UTF-8"",. LANG = ""en_US.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). Error in tempfile() using template /path1/8_Environment/TMPDIR/parXXXXX.par: Parent directory (/path1/8_Environment/TMPDIR/) does not exist at /usr/bin/parallel line 3889. real 0m3.019s. user 0m0.211s. sys 0m0.371s. ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/613
https://github.com/google/deepvariant/issues/613:1710,interoperability,standard,standard,1710,"TMPDIR and locale settings error！please help me!; Hello, I got an error when running deepvariant. I've verified that the path `/path1/8_Environment/TMPDIR`exists. I've googled all over and still can't solve the problem. please help me! command：. ```. INPUT_DIR=/path1/4_Test/qingjiang/dpv. OUTPUT_DIR=/path1/4_Test/qingjiang/dpv. singularity run /path/dpv/deepvariant_1.4.0.sif /opt/deepvariant/bin/run_deepvariant \. --num_shards=3 \. --model_type=PACBIO \. --ref=""${INPUT_DIR}""/QJref.fa \. --reads=""${INPUT_DIR}""/input.bam \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. ```. error：. ```. I0213 16:54:59.595547 140573030586176 run_deepvariant.py:342] Re-using the directory for intermediate results in /path/dpv/intermediate_results_dir. ***** Intermediate results will be written to /path/dpv/intermediate_results_dir in docker. ****. ***** Running the command:*****. time seq 0 2 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/path/dpv/QJref.fa"" --reads ""/path/dpv/input.bam"" --examples ""/dellfsq. perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. LANGUAGE = (unset),. LC_ALL = (unset),. LC_CTYPE = ""C.UTF-8"",. LANG = ""en_US.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. LANGUAGE = (unset),. LC_ALL = (unset),. LC_CTYPE = ""C.UTF-8"",. LANG = ""en_US.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). Error in tempfile() using template /path1/8_Environment/TMPDIR/parXXXXX.par: Parent directory (/path1/8_Environment/TMPDIR/) does not exist at /usr/bin/parallel line 3889. real 0m3.019s. user 0m0.211s. sys 0m0.371s. ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/613
https://github.com/google/deepvariant/issues/613:449,modifiability,PAC,PACBIO,449,"TMPDIR and locale settings error！please help me!; Hello, I got an error when running deepvariant. I've verified that the path `/path1/8_Environment/TMPDIR`exists. I've googled all over and still can't solve the problem. please help me! command：. ```. INPUT_DIR=/path1/4_Test/qingjiang/dpv. OUTPUT_DIR=/path1/4_Test/qingjiang/dpv. singularity run /path/dpv/deepvariant_1.4.0.sif /opt/deepvariant/bin/run_deepvariant \. --num_shards=3 \. --model_type=PACBIO \. --ref=""${INPUT_DIR}""/QJref.fa \. --reads=""${INPUT_DIR}""/input.bam \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. ```. error：. ```. I0213 16:54:59.595547 140573030586176 run_deepvariant.py:342] Re-using the directory for intermediate results in /path/dpv/intermediate_results_dir. ***** Intermediate results will be written to /path/dpv/intermediate_results_dir in docker. ****. ***** Running the command:*****. time seq 0 2 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/path/dpv/QJref.fa"" --reads ""/path/dpv/input.bam"" --examples ""/dellfsq. perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. LANGUAGE = (unset),. LC_ALL = (unset),. LC_CTYPE = ""C.UTF-8"",. LANG = ""en_US.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. LANGUAGE = (unset),. LC_ALL = (unset),. LC_CTYPE = ""C.UTF-8"",. LANG = ""en_US.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). Error in tempfile() using template /path1/8_Environment/TMPDIR/parXXXXX.par: Parent directory (/path1/8_Environment/TMPDIR/) does not exist at /usr/bin/parallel line 3889. real 0m3.019s. user 0m0.211s. sys 0m0.371s. ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/613
https://github.com/google/deepvariant/issues/613:801,modifiability,interm,intermediate,801,"TMPDIR and locale settings error！please help me!; Hello, I got an error when running deepvariant. I've verified that the path `/path1/8_Environment/TMPDIR`exists. I've googled all over and still can't solve the problem. please help me! command：. ```. INPUT_DIR=/path1/4_Test/qingjiang/dpv. OUTPUT_DIR=/path1/4_Test/qingjiang/dpv. singularity run /path/dpv/deepvariant_1.4.0.sif /opt/deepvariant/bin/run_deepvariant \. --num_shards=3 \. --model_type=PACBIO \. --ref=""${INPUT_DIR}""/QJref.fa \. --reads=""${INPUT_DIR}""/input.bam \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. ```. error：. ```. I0213 16:54:59.595547 140573030586176 run_deepvariant.py:342] Re-using the directory for intermediate results in /path/dpv/intermediate_results_dir. ***** Intermediate results will be written to /path/dpv/intermediate_results_dir in docker. ****. ***** Running the command:*****. time seq 0 2 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/path/dpv/QJref.fa"" --reads ""/path/dpv/input.bam"" --examples ""/dellfsq. perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. LANGUAGE = (unset),. LC_ALL = (unset),. LC_CTYPE = ""C.UTF-8"",. LANG = ""en_US.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. LANGUAGE = (unset),. LC_ALL = (unset),. LC_CTYPE = ""C.UTF-8"",. LANG = ""en_US.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). Error in tempfile() using template /path1/8_Environment/TMPDIR/parXXXXX.par: Parent directory (/path1/8_Environment/TMPDIR/) does not exist at /usr/bin/parallel line 3889. real 0m3.019s. user 0m0.211s. sys 0m0.371s. ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/613
https://github.com/google/deepvariant/issues/613:867,modifiability,Interm,Intermediate,867,"TMPDIR and locale settings error！please help me!; Hello, I got an error when running deepvariant. I've verified that the path `/path1/8_Environment/TMPDIR`exists. I've googled all over and still can't solve the problem. please help me! command：. ```. INPUT_DIR=/path1/4_Test/qingjiang/dpv. OUTPUT_DIR=/path1/4_Test/qingjiang/dpv. singularity run /path/dpv/deepvariant_1.4.0.sif /opt/deepvariant/bin/run_deepvariant \. --num_shards=3 \. --model_type=PACBIO \. --ref=""${INPUT_DIR}""/QJref.fa \. --reads=""${INPUT_DIR}""/input.bam \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. ```. error：. ```. I0213 16:54:59.595547 140573030586176 run_deepvariant.py:342] Re-using the directory for intermediate results in /path/dpv/intermediate_results_dir. ***** Intermediate results will be written to /path/dpv/intermediate_results_dir in docker. ****. ***** Running the command:*****. time seq 0 2 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/path/dpv/QJref.fa"" --reads ""/path/dpv/input.bam"" --examples ""/dellfsq. perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. LANGUAGE = (unset),. LC_ALL = (unset),. LC_CTYPE = ""C.UTF-8"",. LANG = ""en_US.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. LANGUAGE = (unset),. LC_ALL = (unset),. LC_CTYPE = ""C.UTF-8"",. LANG = ""en_US.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). Error in tempfile() using template /path1/8_Environment/TMPDIR/parXXXXX.par: Parent directory (/path1/8_Environment/TMPDIR/) does not exist at /usr/bin/parallel line 3889. real 0m3.019s. user 0m0.211s. sys 0m0.371s. ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/613
https://github.com/google/deepvariant/issues/613:27,performance,error,error,27,"TMPDIR and locale settings error！please help me!; Hello, I got an error when running deepvariant. I've verified that the path `/path1/8_Environment/TMPDIR`exists. I've googled all over and still can't solve the problem. please help me! command：. ```. INPUT_DIR=/path1/4_Test/qingjiang/dpv. OUTPUT_DIR=/path1/4_Test/qingjiang/dpv. singularity run /path/dpv/deepvariant_1.4.0.sif /opt/deepvariant/bin/run_deepvariant \. --num_shards=3 \. --model_type=PACBIO \. --ref=""${INPUT_DIR}""/QJref.fa \. --reads=""${INPUT_DIR}""/input.bam \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. ```. error：. ```. I0213 16:54:59.595547 140573030586176 run_deepvariant.py:342] Re-using the directory for intermediate results in /path/dpv/intermediate_results_dir. ***** Intermediate results will be written to /path/dpv/intermediate_results_dir in docker. ****. ***** Running the command:*****. time seq 0 2 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/path/dpv/QJref.fa"" --reads ""/path/dpv/input.bam"" --examples ""/dellfsq. perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. LANGUAGE = (unset),. LC_ALL = (unset),. LC_CTYPE = ""C.UTF-8"",. LANG = ""en_US.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. LANGUAGE = (unset),. LC_ALL = (unset),. LC_CTYPE = ""C.UTF-8"",. LANG = ""en_US.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). Error in tempfile() using template /path1/8_Environment/TMPDIR/parXXXXX.par: Parent directory (/path1/8_Environment/TMPDIR/) does not exist at /usr/bin/parallel line 3889. real 0m3.019s. user 0m0.211s. sys 0m0.371s. ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/613
https://github.com/google/deepvariant/issues/613:66,performance,error,error,66,"TMPDIR and locale settings error！please help me!; Hello, I got an error when running deepvariant. I've verified that the path `/path1/8_Environment/TMPDIR`exists. I've googled all over and still can't solve the problem. please help me! command：. ```. INPUT_DIR=/path1/4_Test/qingjiang/dpv. OUTPUT_DIR=/path1/4_Test/qingjiang/dpv. singularity run /path/dpv/deepvariant_1.4.0.sif /opt/deepvariant/bin/run_deepvariant \. --num_shards=3 \. --model_type=PACBIO \. --ref=""${INPUT_DIR}""/QJref.fa \. --reads=""${INPUT_DIR}""/input.bam \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. ```. error：. ```. I0213 16:54:59.595547 140573030586176 run_deepvariant.py:342] Re-using the directory for intermediate results in /path/dpv/intermediate_results_dir. ***** Intermediate results will be written to /path/dpv/intermediate_results_dir in docker. ****. ***** Running the command:*****. time seq 0 2 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/path/dpv/QJref.fa"" --reads ""/path/dpv/input.bam"" --examples ""/dellfsq. perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. LANGUAGE = (unset),. LC_ALL = (unset),. LC_CTYPE = ""C.UTF-8"",. LANG = ""en_US.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. LANGUAGE = (unset),. LC_ALL = (unset),. LC_CTYPE = ""C.UTF-8"",. LANG = ""en_US.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). Error in tempfile() using template /path1/8_Environment/TMPDIR/parXXXXX.par: Parent directory (/path1/8_Environment/TMPDIR/) does not exist at /usr/bin/parallel line 3889. real 0m3.019s. user 0m0.211s. sys 0m0.371s. ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/613
https://github.com/google/deepvariant/issues/613:699,performance,error,error,699,"TMPDIR and locale settings error！please help me!; Hello, I got an error when running deepvariant. I've verified that the path `/path1/8_Environment/TMPDIR`exists. I've googled all over and still can't solve the problem. please help me! command：. ```. INPUT_DIR=/path1/4_Test/qingjiang/dpv. OUTPUT_DIR=/path1/4_Test/qingjiang/dpv. singularity run /path/dpv/deepvariant_1.4.0.sif /opt/deepvariant/bin/run_deepvariant \. --num_shards=3 \. --model_type=PACBIO \. --ref=""${INPUT_DIR}""/QJref.fa \. --reads=""${INPUT_DIR}""/input.bam \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. ```. error：. ```. I0213 16:54:59.595547 140573030586176 run_deepvariant.py:342] Re-using the directory for intermediate results in /path/dpv/intermediate_results_dir. ***** Intermediate results will be written to /path/dpv/intermediate_results_dir in docker. ****. ***** Running the command:*****. time seq 0 2 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/path/dpv/QJref.fa"" --reads ""/path/dpv/input.bam"" --examples ""/dellfsq. perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. LANGUAGE = (unset),. LC_ALL = (unset),. LC_CTYPE = ""C.UTF-8"",. LANG = ""en_US.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. LANGUAGE = (unset),. LC_ALL = (unset),. LC_CTYPE = ""C.UTF-8"",. LANG = ""en_US.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). Error in tempfile() using template /path1/8_Environment/TMPDIR/parXXXXX.par: Parent directory (/path1/8_Environment/TMPDIR/) does not exist at /usr/bin/parallel line 3889. real 0m3.019s. user 0m0.211s. sys 0m0.371s. ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/613
https://github.com/google/deepvariant/issues/613:992,performance,time,time,992,"TMPDIR and locale settings error！please help me!; Hello, I got an error when running deepvariant. I've verified that the path `/path1/8_Environment/TMPDIR`exists. I've googled all over and still can't solve the problem. please help me! command：. ```. INPUT_DIR=/path1/4_Test/qingjiang/dpv. OUTPUT_DIR=/path1/4_Test/qingjiang/dpv. singularity run /path/dpv/deepvariant_1.4.0.sif /opt/deepvariant/bin/run_deepvariant \. --num_shards=3 \. --model_type=PACBIO \. --ref=""${INPUT_DIR}""/QJref.fa \. --reads=""${INPUT_DIR}""/input.bam \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. ```. error：. ```. I0213 16:54:59.595547 140573030586176 run_deepvariant.py:342] Re-using the directory for intermediate results in /path/dpv/intermediate_results_dir. ***** Intermediate results will be written to /path/dpv/intermediate_results_dir in docker. ****. ***** Running the command:*****. time seq 0 2 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/path/dpv/QJref.fa"" --reads ""/path/dpv/input.bam"" --examples ""/dellfsq. perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. LANGUAGE = (unset),. LC_ALL = (unset),. LC_CTYPE = ""C.UTF-8"",. LANG = ""en_US.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. LANGUAGE = (unset),. LC_ALL = (unset),. LC_CTYPE = ""C.UTF-8"",. LANG = ""en_US.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). Error in tempfile() using template /path1/8_Environment/TMPDIR/parXXXXX.par: Parent directory (/path1/8_Environment/TMPDIR/) does not exist at /usr/bin/parallel line 3889. real 0m3.019s. user 0m0.211s. sys 0m0.371s. ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/613
https://github.com/google/deepvariant/issues/613:1007,performance,parallel,parallel,1007,"TMPDIR and locale settings error！please help me!; Hello, I got an error when running deepvariant. I've verified that the path `/path1/8_Environment/TMPDIR`exists. I've googled all over and still can't solve the problem. please help me! command：. ```. INPUT_DIR=/path1/4_Test/qingjiang/dpv. OUTPUT_DIR=/path1/4_Test/qingjiang/dpv. singularity run /path/dpv/deepvariant_1.4.0.sif /opt/deepvariant/bin/run_deepvariant \. --num_shards=3 \. --model_type=PACBIO \. --ref=""${INPUT_DIR}""/QJref.fa \. --reads=""${INPUT_DIR}""/input.bam \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. ```. error：. ```. I0213 16:54:59.595547 140573030586176 run_deepvariant.py:342] Re-using the directory for intermediate results in /path/dpv/intermediate_results_dir. ***** Intermediate results will be written to /path/dpv/intermediate_results_dir in docker. ****. ***** Running the command:*****. time seq 0 2 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/path/dpv/QJref.fa"" --reads ""/path/dpv/input.bam"" --examples ""/dellfsq. perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. LANGUAGE = (unset),. LC_ALL = (unset),. LC_CTYPE = ""C.UTF-8"",. LANG = ""en_US.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. LANGUAGE = (unset),. LC_ALL = (unset),. LC_CTYPE = ""C.UTF-8"",. LANG = ""en_US.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). Error in tempfile() using template /path1/8_Environment/TMPDIR/parXXXXX.par: Parent directory (/path1/8_Environment/TMPDIR/) does not exist at /usr/bin/parallel line 3889. real 0m3.019s. user 0m0.211s. sys 0m0.371s. ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/613
https://github.com/google/deepvariant/issues/613:1733,performance,Error,Error,1733,"TMPDIR and locale settings error！please help me!; Hello, I got an error when running deepvariant. I've verified that the path `/path1/8_Environment/TMPDIR`exists. I've googled all over and still can't solve the problem. please help me! command：. ```. INPUT_DIR=/path1/4_Test/qingjiang/dpv. OUTPUT_DIR=/path1/4_Test/qingjiang/dpv. singularity run /path/dpv/deepvariant_1.4.0.sif /opt/deepvariant/bin/run_deepvariant \. --num_shards=3 \. --model_type=PACBIO \. --ref=""${INPUT_DIR}""/QJref.fa \. --reads=""${INPUT_DIR}""/input.bam \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. ```. error：. ```. I0213 16:54:59.595547 140573030586176 run_deepvariant.py:342] Re-using the directory for intermediate results in /path/dpv/intermediate_results_dir. ***** Intermediate results will be written to /path/dpv/intermediate_results_dir in docker. ****. ***** Running the command:*****. time seq 0 2 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/path/dpv/QJref.fa"" --reads ""/path/dpv/input.bam"" --examples ""/dellfsq. perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. LANGUAGE = (unset),. LC_ALL = (unset),. LC_CTYPE = ""C.UTF-8"",. LANG = ""en_US.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. LANGUAGE = (unset),. LC_ALL = (unset),. LC_CTYPE = ""C.UTF-8"",. LANG = ""en_US.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). Error in tempfile() using template /path1/8_Environment/TMPDIR/parXXXXX.par: Parent directory (/path1/8_Environment/TMPDIR/) does not exist at /usr/bin/parallel line 3889. real 0m3.019s. user 0m0.211s. sys 0m0.371s. ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/613
https://github.com/google/deepvariant/issues/613:1885,performance,parallel,parallel,1885,"TMPDIR and locale settings error！please help me!; Hello, I got an error when running deepvariant. I've verified that the path `/path1/8_Environment/TMPDIR`exists. I've googled all over and still can't solve the problem. please help me! command：. ```. INPUT_DIR=/path1/4_Test/qingjiang/dpv. OUTPUT_DIR=/path1/4_Test/qingjiang/dpv. singularity run /path/dpv/deepvariant_1.4.0.sif /opt/deepvariant/bin/run_deepvariant \. --num_shards=3 \. --model_type=PACBIO \. --ref=""${INPUT_DIR}""/QJref.fa \. --reads=""${INPUT_DIR}""/input.bam \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. ```. error：. ```. I0213 16:54:59.595547 140573030586176 run_deepvariant.py:342] Re-using the directory for intermediate results in /path/dpv/intermediate_results_dir. ***** Intermediate results will be written to /path/dpv/intermediate_results_dir in docker. ****. ***** Running the command:*****. time seq 0 2 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/path/dpv/QJref.fa"" --reads ""/path/dpv/input.bam"" --examples ""/dellfsq. perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. LANGUAGE = (unset),. LC_ALL = (unset),. LC_CTYPE = ""C.UTF-8"",. LANG = ""en_US.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. LANGUAGE = (unset),. LC_ALL = (unset),. LC_CTYPE = ""C.UTF-8"",. LANG = ""en_US.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). Error in tempfile() using template /path1/8_Environment/TMPDIR/parXXXXX.par: Parent directory (/path1/8_Environment/TMPDIR/) does not exist at /usr/bin/parallel line 3889. real 0m3.019s. user 0m0.211s. sys 0m0.371s. ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/613
https://github.com/google/deepvariant/issues/613:1201,reliability,fail,failed,1201,"TMPDIR and locale settings error！please help me!; Hello, I got an error when running deepvariant. I've verified that the path `/path1/8_Environment/TMPDIR`exists. I've googled all over and still can't solve the problem. please help me! command：. ```. INPUT_DIR=/path1/4_Test/qingjiang/dpv. OUTPUT_DIR=/path1/4_Test/qingjiang/dpv. singularity run /path/dpv/deepvariant_1.4.0.sif /opt/deepvariant/bin/run_deepvariant \. --num_shards=3 \. --model_type=PACBIO \. --ref=""${INPUT_DIR}""/QJref.fa \. --reads=""${INPUT_DIR}""/input.bam \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. ```. error：. ```. I0213 16:54:59.595547 140573030586176 run_deepvariant.py:342] Re-using the directory for intermediate results in /path/dpv/intermediate_results_dir. ***** Intermediate results will be written to /path/dpv/intermediate_results_dir in docker. ****. ***** Running the command:*****. time seq 0 2 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/path/dpv/QJref.fa"" --reads ""/path/dpv/input.bam"" --examples ""/dellfsq. perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. LANGUAGE = (unset),. LC_ALL = (unset),. LC_CTYPE = ""C.UTF-8"",. LANG = ""en_US.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. LANGUAGE = (unset),. LC_ALL = (unset),. LC_CTYPE = ""C.UTF-8"",. LANG = ""en_US.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). Error in tempfile() using template /path1/8_Environment/TMPDIR/parXXXXX.par: Parent directory (/path1/8_Environment/TMPDIR/) does not exist at /usr/bin/parallel line 3889. real 0m3.019s. user 0m0.211s. sys 0m0.371s. ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/613
https://github.com/google/deepvariant/issues/613:1482,reliability,fail,failed,1482,"TMPDIR and locale settings error！please help me!; Hello, I got an error when running deepvariant. I've verified that the path `/path1/8_Environment/TMPDIR`exists. I've googled all over and still can't solve the problem. please help me! command：. ```. INPUT_DIR=/path1/4_Test/qingjiang/dpv. OUTPUT_DIR=/path1/4_Test/qingjiang/dpv. singularity run /path/dpv/deepvariant_1.4.0.sif /opt/deepvariant/bin/run_deepvariant \. --num_shards=3 \. --model_type=PACBIO \. --ref=""${INPUT_DIR}""/QJref.fa \. --reads=""${INPUT_DIR}""/input.bam \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. ```. error：. ```. I0213 16:54:59.595547 140573030586176 run_deepvariant.py:342] Re-using the directory for intermediate results in /path/dpv/intermediate_results_dir. ***** Intermediate results will be written to /path/dpv/intermediate_results_dir in docker. ****. ***** Running the command:*****. time seq 0 2 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/path/dpv/QJref.fa"" --reads ""/path/dpv/input.bam"" --examples ""/dellfsq. perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. LANGUAGE = (unset),. LC_ALL = (unset),. LC_CTYPE = ""C.UTF-8"",. LANG = ""en_US.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. LANGUAGE = (unset),. LC_ALL = (unset),. LC_CTYPE = ""C.UTF-8"",. LANG = ""en_US.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). Error in tempfile() using template /path1/8_Environment/TMPDIR/parXXXXX.par: Parent directory (/path1/8_Environment/TMPDIR/) does not exist at /usr/bin/parallel line 3889. real 0m3.019s. user 0m0.211s. sys 0m0.371s. ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/613
https://github.com/google/deepvariant/issues/613:1858,reliability,doe,does,1858,"TMPDIR and locale settings error！please help me!; Hello, I got an error when running deepvariant. I've verified that the path `/path1/8_Environment/TMPDIR`exists. I've googled all over and still can't solve the problem. please help me! command：. ```. INPUT_DIR=/path1/4_Test/qingjiang/dpv. OUTPUT_DIR=/path1/4_Test/qingjiang/dpv. singularity run /path/dpv/deepvariant_1.4.0.sif /opt/deepvariant/bin/run_deepvariant \. --num_shards=3 \. --model_type=PACBIO \. --ref=""${INPUT_DIR}""/QJref.fa \. --reads=""${INPUT_DIR}""/input.bam \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. ```. error：. ```. I0213 16:54:59.595547 140573030586176 run_deepvariant.py:342] Re-using the directory for intermediate results in /path/dpv/intermediate_results_dir. ***** Intermediate results will be written to /path/dpv/intermediate_results_dir in docker. ****. ***** Running the command:*****. time seq 0 2 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/path/dpv/QJref.fa"" --reads ""/path/dpv/input.bam"" --examples ""/dellfsq. perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. LANGUAGE = (unset),. LC_ALL = (unset),. LC_CTYPE = ""C.UTF-8"",. LANG = ""en_US.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. LANGUAGE = (unset),. LC_ALL = (unset),. LC_CTYPE = ""C.UTF-8"",. LANG = ""en_US.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). Error in tempfile() using template /path1/8_Environment/TMPDIR/parXXXXX.par: Parent directory (/path1/8_Environment/TMPDIR/) does not exist at /usr/bin/parallel line 3889. real 0m3.019s. user 0m0.211s. sys 0m0.371s. ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/613
https://github.com/google/deepvariant/issues/613:27,safety,error,error,27,"TMPDIR and locale settings error！please help me!; Hello, I got an error when running deepvariant. I've verified that the path `/path1/8_Environment/TMPDIR`exists. I've googled all over and still can't solve the problem. please help me! command：. ```. INPUT_DIR=/path1/4_Test/qingjiang/dpv. OUTPUT_DIR=/path1/4_Test/qingjiang/dpv. singularity run /path/dpv/deepvariant_1.4.0.sif /opt/deepvariant/bin/run_deepvariant \. --num_shards=3 \. --model_type=PACBIO \. --ref=""${INPUT_DIR}""/QJref.fa \. --reads=""${INPUT_DIR}""/input.bam \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. ```. error：. ```. I0213 16:54:59.595547 140573030586176 run_deepvariant.py:342] Re-using the directory for intermediate results in /path/dpv/intermediate_results_dir. ***** Intermediate results will be written to /path/dpv/intermediate_results_dir in docker. ****. ***** Running the command:*****. time seq 0 2 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/path/dpv/QJref.fa"" --reads ""/path/dpv/input.bam"" --examples ""/dellfsq. perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. LANGUAGE = (unset),. LC_ALL = (unset),. LC_CTYPE = ""C.UTF-8"",. LANG = ""en_US.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. LANGUAGE = (unset),. LC_ALL = (unset),. LC_CTYPE = ""C.UTF-8"",. LANG = ""en_US.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). Error in tempfile() using template /path1/8_Environment/TMPDIR/parXXXXX.par: Parent directory (/path1/8_Environment/TMPDIR/) does not exist at /usr/bin/parallel line 3889. real 0m3.019s. user 0m0.211s. sys 0m0.371s. ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/613
https://github.com/google/deepvariant/issues/613:66,safety,error,error,66,"TMPDIR and locale settings error！please help me!; Hello, I got an error when running deepvariant. I've verified that the path `/path1/8_Environment/TMPDIR`exists. I've googled all over and still can't solve the problem. please help me! command：. ```. INPUT_DIR=/path1/4_Test/qingjiang/dpv. OUTPUT_DIR=/path1/4_Test/qingjiang/dpv. singularity run /path/dpv/deepvariant_1.4.0.sif /opt/deepvariant/bin/run_deepvariant \. --num_shards=3 \. --model_type=PACBIO \. --ref=""${INPUT_DIR}""/QJref.fa \. --reads=""${INPUT_DIR}""/input.bam \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. ```. error：. ```. I0213 16:54:59.595547 140573030586176 run_deepvariant.py:342] Re-using the directory for intermediate results in /path/dpv/intermediate_results_dir. ***** Intermediate results will be written to /path/dpv/intermediate_results_dir in docker. ****. ***** Running the command:*****. time seq 0 2 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/path/dpv/QJref.fa"" --reads ""/path/dpv/input.bam"" --examples ""/dellfsq. perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. LANGUAGE = (unset),. LC_ALL = (unset),. LC_CTYPE = ""C.UTF-8"",. LANG = ""en_US.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. LANGUAGE = (unset),. LC_ALL = (unset),. LC_CTYPE = ""C.UTF-8"",. LANG = ""en_US.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). Error in tempfile() using template /path1/8_Environment/TMPDIR/parXXXXX.par: Parent directory (/path1/8_Environment/TMPDIR/) does not exist at /usr/bin/parallel line 3889. real 0m3.019s. user 0m0.211s. sys 0m0.371s. ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/613
https://github.com/google/deepvariant/issues/613:515,safety,input,input,515,"TMPDIR and locale settings error！please help me!; Hello, I got an error when running deepvariant. I've verified that the path `/path1/8_Environment/TMPDIR`exists. I've googled all over and still can't solve the problem. please help me! command：. ```. INPUT_DIR=/path1/4_Test/qingjiang/dpv. OUTPUT_DIR=/path1/4_Test/qingjiang/dpv. singularity run /path/dpv/deepvariant_1.4.0.sif /opt/deepvariant/bin/run_deepvariant \. --num_shards=3 \. --model_type=PACBIO \. --ref=""${INPUT_DIR}""/QJref.fa \. --reads=""${INPUT_DIR}""/input.bam \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. ```. error：. ```. I0213 16:54:59.595547 140573030586176 run_deepvariant.py:342] Re-using the directory for intermediate results in /path/dpv/intermediate_results_dir. ***** Intermediate results will be written to /path/dpv/intermediate_results_dir in docker. ****. ***** Running the command:*****. time seq 0 2 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/path/dpv/QJref.fa"" --reads ""/path/dpv/input.bam"" --examples ""/dellfsq. perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. LANGUAGE = (unset),. LC_ALL = (unset),. LC_CTYPE = ""C.UTF-8"",. LANG = ""en_US.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. LANGUAGE = (unset),. LC_ALL = (unset),. LC_CTYPE = ""C.UTF-8"",. LANG = ""en_US.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). Error in tempfile() using template /path1/8_Environment/TMPDIR/parXXXXX.par: Parent directory (/path1/8_Environment/TMPDIR/) does not exist at /usr/bin/parallel line 3889. real 0m3.019s. user 0m0.211s. sys 0m0.371s. ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/613
https://github.com/google/deepvariant/issues/613:699,safety,error,error,699,"TMPDIR and locale settings error！please help me!; Hello, I got an error when running deepvariant. I've verified that the path `/path1/8_Environment/TMPDIR`exists. I've googled all over and still can't solve the problem. please help me! command：. ```. INPUT_DIR=/path1/4_Test/qingjiang/dpv. OUTPUT_DIR=/path1/4_Test/qingjiang/dpv. singularity run /path/dpv/deepvariant_1.4.0.sif /opt/deepvariant/bin/run_deepvariant \. --num_shards=3 \. --model_type=PACBIO \. --ref=""${INPUT_DIR}""/QJref.fa \. --reads=""${INPUT_DIR}""/input.bam \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. ```. error：. ```. I0213 16:54:59.595547 140573030586176 run_deepvariant.py:342] Re-using the directory for intermediate results in /path/dpv/intermediate_results_dir. ***** Intermediate results will be written to /path/dpv/intermediate_results_dir in docker. ****. ***** Running the command:*****. time seq 0 2 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/path/dpv/QJref.fa"" --reads ""/path/dpv/input.bam"" --examples ""/dellfsq. perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. LANGUAGE = (unset),. LC_ALL = (unset),. LC_CTYPE = ""C.UTF-8"",. LANG = ""en_US.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. LANGUAGE = (unset),. LC_ALL = (unset),. LC_CTYPE = ""C.UTF-8"",. LANG = ""en_US.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). Error in tempfile() using template /path1/8_Environment/TMPDIR/parXXXXX.par: Parent directory (/path1/8_Environment/TMPDIR/) does not exist at /usr/bin/parallel line 3889. real 0m3.019s. user 0m0.211s. sys 0m0.371s. ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/613
https://github.com/google/deepvariant/issues/613:1138,safety,input,input,1138,"TMPDIR and locale settings error！please help me!; Hello, I got an error when running deepvariant. I've verified that the path `/path1/8_Environment/TMPDIR`exists. I've googled all over and still can't solve the problem. please help me! command：. ```. INPUT_DIR=/path1/4_Test/qingjiang/dpv. OUTPUT_DIR=/path1/4_Test/qingjiang/dpv. singularity run /path/dpv/deepvariant_1.4.0.sif /opt/deepvariant/bin/run_deepvariant \. --num_shards=3 \. --model_type=PACBIO \. --ref=""${INPUT_DIR}""/QJref.fa \. --reads=""${INPUT_DIR}""/input.bam \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. ```. error：. ```. I0213 16:54:59.595547 140573030586176 run_deepvariant.py:342] Re-using the directory for intermediate results in /path/dpv/intermediate_results_dir. ***** Intermediate results will be written to /path/dpv/intermediate_results_dir in docker. ****. ***** Running the command:*****. time seq 0 2 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/path/dpv/QJref.fa"" --reads ""/path/dpv/input.bam"" --examples ""/dellfsq. perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. LANGUAGE = (unset),. LC_ALL = (unset),. LC_CTYPE = ""C.UTF-8"",. LANG = ""en_US.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. LANGUAGE = (unset),. LC_ALL = (unset),. LC_CTYPE = ""C.UTF-8"",. LANG = ""en_US.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). Error in tempfile() using template /path1/8_Environment/TMPDIR/parXXXXX.par: Parent directory (/path1/8_Environment/TMPDIR/) does not exist at /usr/bin/parallel line 3889. real 0m3.019s. user 0m0.211s. sys 0m0.371s. ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/613
https://github.com/google/deepvariant/issues/613:1733,safety,Error,Error,1733,"TMPDIR and locale settings error！please help me!; Hello, I got an error when running deepvariant. I've verified that the path `/path1/8_Environment/TMPDIR`exists. I've googled all over and still can't solve the problem. please help me! command：. ```. INPUT_DIR=/path1/4_Test/qingjiang/dpv. OUTPUT_DIR=/path1/4_Test/qingjiang/dpv. singularity run /path/dpv/deepvariant_1.4.0.sif /opt/deepvariant/bin/run_deepvariant \. --num_shards=3 \. --model_type=PACBIO \. --ref=""${INPUT_DIR}""/QJref.fa \. --reads=""${INPUT_DIR}""/input.bam \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. ```. error：. ```. I0213 16:54:59.595547 140573030586176 run_deepvariant.py:342] Re-using the directory for intermediate results in /path/dpv/intermediate_results_dir. ***** Intermediate results will be written to /path/dpv/intermediate_results_dir in docker. ****. ***** Running the command:*****. time seq 0 2 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/path/dpv/QJref.fa"" --reads ""/path/dpv/input.bam"" --examples ""/dellfsq. perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. LANGUAGE = (unset),. LC_ALL = (unset),. LC_CTYPE = ""C.UTF-8"",. LANG = ""en_US.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. LANGUAGE = (unset),. LC_ALL = (unset),. LC_CTYPE = ""C.UTF-8"",. LANG = ""en_US.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). Error in tempfile() using template /path1/8_Environment/TMPDIR/parXXXXX.par: Parent directory (/path1/8_Environment/TMPDIR/) does not exist at /usr/bin/parallel line 3889. real 0m3.019s. user 0m0.211s. sys 0m0.371s. ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/613
https://github.com/google/deepvariant/issues/613:103,testability,verif,verified,103,"TMPDIR and locale settings error！please help me!; Hello, I got an error when running deepvariant. I've verified that the path `/path1/8_Environment/TMPDIR`exists. I've googled all over and still can't solve the problem. please help me! command：. ```. INPUT_DIR=/path1/4_Test/qingjiang/dpv. OUTPUT_DIR=/path1/4_Test/qingjiang/dpv. singularity run /path/dpv/deepvariant_1.4.0.sif /opt/deepvariant/bin/run_deepvariant \. --num_shards=3 \. --model_type=PACBIO \. --ref=""${INPUT_DIR}""/QJref.fa \. --reads=""${INPUT_DIR}""/input.bam \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. ```. error：. ```. I0213 16:54:59.595547 140573030586176 run_deepvariant.py:342] Re-using the directory for intermediate results in /path/dpv/intermediate_results_dir. ***** Intermediate results will be written to /path/dpv/intermediate_results_dir in docker. ****. ***** Running the command:*****. time seq 0 2 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/path/dpv/QJref.fa"" --reads ""/path/dpv/input.bam"" --examples ""/dellfsq. perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. LANGUAGE = (unset),. LC_ALL = (unset),. LC_CTYPE = ""C.UTF-8"",. LANG = ""en_US.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. LANGUAGE = (unset),. LC_ALL = (unset),. LC_CTYPE = ""C.UTF-8"",. LANG = ""en_US.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). Error in tempfile() using template /path1/8_Environment/TMPDIR/parXXXXX.par: Parent directory (/path1/8_Environment/TMPDIR/) does not exist at /usr/bin/parallel line 3889. real 0m3.019s. user 0m0.211s. sys 0m0.371s. ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/613
https://github.com/google/deepvariant/issues/613:27,usability,error,error,27,"TMPDIR and locale settings error！please help me!; Hello, I got an error when running deepvariant. I've verified that the path `/path1/8_Environment/TMPDIR`exists. I've googled all over and still can't solve the problem. please help me! command：. ```. INPUT_DIR=/path1/4_Test/qingjiang/dpv. OUTPUT_DIR=/path1/4_Test/qingjiang/dpv. singularity run /path/dpv/deepvariant_1.4.0.sif /opt/deepvariant/bin/run_deepvariant \. --num_shards=3 \. --model_type=PACBIO \. --ref=""${INPUT_DIR}""/QJref.fa \. --reads=""${INPUT_DIR}""/input.bam \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. ```. error：. ```. I0213 16:54:59.595547 140573030586176 run_deepvariant.py:342] Re-using the directory for intermediate results in /path/dpv/intermediate_results_dir. ***** Intermediate results will be written to /path/dpv/intermediate_results_dir in docker. ****. ***** Running the command:*****. time seq 0 2 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/path/dpv/QJref.fa"" --reads ""/path/dpv/input.bam"" --examples ""/dellfsq. perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. LANGUAGE = (unset),. LC_ALL = (unset),. LC_CTYPE = ""C.UTF-8"",. LANG = ""en_US.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. LANGUAGE = (unset),. LC_ALL = (unset),. LC_CTYPE = ""C.UTF-8"",. LANG = ""en_US.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). Error in tempfile() using template /path1/8_Environment/TMPDIR/parXXXXX.par: Parent directory (/path1/8_Environment/TMPDIR/) does not exist at /usr/bin/parallel line 3889. real 0m3.019s. user 0m0.211s. sys 0m0.371s. ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/613
https://github.com/google/deepvariant/issues/613:40,usability,help,help,40,"TMPDIR and locale settings error！please help me!; Hello, I got an error when running deepvariant. I've verified that the path `/path1/8_Environment/TMPDIR`exists. I've googled all over and still can't solve the problem. please help me! command：. ```. INPUT_DIR=/path1/4_Test/qingjiang/dpv. OUTPUT_DIR=/path1/4_Test/qingjiang/dpv. singularity run /path/dpv/deepvariant_1.4.0.sif /opt/deepvariant/bin/run_deepvariant \. --num_shards=3 \. --model_type=PACBIO \. --ref=""${INPUT_DIR}""/QJref.fa \. --reads=""${INPUT_DIR}""/input.bam \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. ```. error：. ```. I0213 16:54:59.595547 140573030586176 run_deepvariant.py:342] Re-using the directory for intermediate results in /path/dpv/intermediate_results_dir. ***** Intermediate results will be written to /path/dpv/intermediate_results_dir in docker. ****. ***** Running the command:*****. time seq 0 2 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/path/dpv/QJref.fa"" --reads ""/path/dpv/input.bam"" --examples ""/dellfsq. perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. LANGUAGE = (unset),. LC_ALL = (unset),. LC_CTYPE = ""C.UTF-8"",. LANG = ""en_US.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. LANGUAGE = (unset),. LC_ALL = (unset),. LC_CTYPE = ""C.UTF-8"",. LANG = ""en_US.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). Error in tempfile() using template /path1/8_Environment/TMPDIR/parXXXXX.par: Parent directory (/path1/8_Environment/TMPDIR/) does not exist at /usr/bin/parallel line 3889. real 0m3.019s. user 0m0.211s. sys 0m0.371s. ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/613
https://github.com/google/deepvariant/issues/613:66,usability,error,error,66,"TMPDIR and locale settings error！please help me!; Hello, I got an error when running deepvariant. I've verified that the path `/path1/8_Environment/TMPDIR`exists. I've googled all over and still can't solve the problem. please help me! command：. ```. INPUT_DIR=/path1/4_Test/qingjiang/dpv. OUTPUT_DIR=/path1/4_Test/qingjiang/dpv. singularity run /path/dpv/deepvariant_1.4.0.sif /opt/deepvariant/bin/run_deepvariant \. --num_shards=3 \. --model_type=PACBIO \. --ref=""${INPUT_DIR}""/QJref.fa \. --reads=""${INPUT_DIR}""/input.bam \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. ```. error：. ```. I0213 16:54:59.595547 140573030586176 run_deepvariant.py:342] Re-using the directory for intermediate results in /path/dpv/intermediate_results_dir. ***** Intermediate results will be written to /path/dpv/intermediate_results_dir in docker. ****. ***** Running the command:*****. time seq 0 2 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/path/dpv/QJref.fa"" --reads ""/path/dpv/input.bam"" --examples ""/dellfsq. perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. LANGUAGE = (unset),. LC_ALL = (unset),. LC_CTYPE = ""C.UTF-8"",. LANG = ""en_US.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. LANGUAGE = (unset),. LC_ALL = (unset),. LC_CTYPE = ""C.UTF-8"",. LANG = ""en_US.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). Error in tempfile() using template /path1/8_Environment/TMPDIR/parXXXXX.par: Parent directory (/path1/8_Environment/TMPDIR/) does not exist at /usr/bin/parallel line 3889. real 0m3.019s. user 0m0.211s. sys 0m0.371s. ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/613
https://github.com/google/deepvariant/issues/613:227,usability,help,help,227,"TMPDIR and locale settings error！please help me!; Hello, I got an error when running deepvariant. I've verified that the path `/path1/8_Environment/TMPDIR`exists. I've googled all over and still can't solve the problem. please help me! command：. ```. INPUT_DIR=/path1/4_Test/qingjiang/dpv. OUTPUT_DIR=/path1/4_Test/qingjiang/dpv. singularity run /path/dpv/deepvariant_1.4.0.sif /opt/deepvariant/bin/run_deepvariant \. --num_shards=3 \. --model_type=PACBIO \. --ref=""${INPUT_DIR}""/QJref.fa \. --reads=""${INPUT_DIR}""/input.bam \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. ```. error：. ```. I0213 16:54:59.595547 140573030586176 run_deepvariant.py:342] Re-using the directory for intermediate results in /path/dpv/intermediate_results_dir. ***** Intermediate results will be written to /path/dpv/intermediate_results_dir in docker. ****. ***** Running the command:*****. time seq 0 2 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/path/dpv/QJref.fa"" --reads ""/path/dpv/input.bam"" --examples ""/dellfsq. perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. LANGUAGE = (unset),. LC_ALL = (unset),. LC_CTYPE = ""C.UTF-8"",. LANG = ""en_US.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. LANGUAGE = (unset),. LC_ALL = (unset),. LC_CTYPE = ""C.UTF-8"",. LANG = ""en_US.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). Error in tempfile() using template /path1/8_Environment/TMPDIR/parXXXXX.par: Parent directory (/path1/8_Environment/TMPDIR/) does not exist at /usr/bin/parallel line 3889. real 0m3.019s. user 0m0.211s. sys 0m0.371s. ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/613
https://github.com/google/deepvariant/issues/613:236,usability,command,command,236,"TMPDIR and locale settings error！please help me!; Hello, I got an error when running deepvariant. I've verified that the path `/path1/8_Environment/TMPDIR`exists. I've googled all over and still can't solve the problem. please help me! command：. ```. INPUT_DIR=/path1/4_Test/qingjiang/dpv. OUTPUT_DIR=/path1/4_Test/qingjiang/dpv. singularity run /path/dpv/deepvariant_1.4.0.sif /opt/deepvariant/bin/run_deepvariant \. --num_shards=3 \. --model_type=PACBIO \. --ref=""${INPUT_DIR}""/QJref.fa \. --reads=""${INPUT_DIR}""/input.bam \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. ```. error：. ```. I0213 16:54:59.595547 140573030586176 run_deepvariant.py:342] Re-using the directory for intermediate results in /path/dpv/intermediate_results_dir. ***** Intermediate results will be written to /path/dpv/intermediate_results_dir in docker. ****. ***** Running the command:*****. time seq 0 2 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/path/dpv/QJref.fa"" --reads ""/path/dpv/input.bam"" --examples ""/dellfsq. perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. LANGUAGE = (unset),. LC_ALL = (unset),. LC_CTYPE = ""C.UTF-8"",. LANG = ""en_US.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. LANGUAGE = (unset),. LC_ALL = (unset),. LC_CTYPE = ""C.UTF-8"",. LANG = ""en_US.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). Error in tempfile() using template /path1/8_Environment/TMPDIR/parXXXXX.par: Parent directory (/path1/8_Environment/TMPDIR/) does not exist at /usr/bin/parallel line 3889. real 0m3.019s. user 0m0.211s. sys 0m0.371s. ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/613
https://github.com/google/deepvariant/issues/613:515,usability,input,input,515,"TMPDIR and locale settings error！please help me!; Hello, I got an error when running deepvariant. I've verified that the path `/path1/8_Environment/TMPDIR`exists. I've googled all over and still can't solve the problem. please help me! command：. ```. INPUT_DIR=/path1/4_Test/qingjiang/dpv. OUTPUT_DIR=/path1/4_Test/qingjiang/dpv. singularity run /path/dpv/deepvariant_1.4.0.sif /opt/deepvariant/bin/run_deepvariant \. --num_shards=3 \. --model_type=PACBIO \. --ref=""${INPUT_DIR}""/QJref.fa \. --reads=""${INPUT_DIR}""/input.bam \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. ```. error：. ```. I0213 16:54:59.595547 140573030586176 run_deepvariant.py:342] Re-using the directory for intermediate results in /path/dpv/intermediate_results_dir. ***** Intermediate results will be written to /path/dpv/intermediate_results_dir in docker. ****. ***** Running the command:*****. time seq 0 2 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/path/dpv/QJref.fa"" --reads ""/path/dpv/input.bam"" --examples ""/dellfsq. perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. LANGUAGE = (unset),. LC_ALL = (unset),. LC_CTYPE = ""C.UTF-8"",. LANG = ""en_US.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. LANGUAGE = (unset),. LC_ALL = (unset),. LC_CTYPE = ""C.UTF-8"",. LANG = ""en_US.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). Error in tempfile() using template /path1/8_Environment/TMPDIR/parXXXXX.par: Parent directory (/path1/8_Environment/TMPDIR/) does not exist at /usr/bin/parallel line 3889. real 0m3.019s. user 0m0.211s. sys 0m0.371s. ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/613
https://github.com/google/deepvariant/issues/613:699,usability,error,error,699,"TMPDIR and locale settings error！please help me!; Hello, I got an error when running deepvariant. I've verified that the path `/path1/8_Environment/TMPDIR`exists. I've googled all over and still can't solve the problem. please help me! command：. ```. INPUT_DIR=/path1/4_Test/qingjiang/dpv. OUTPUT_DIR=/path1/4_Test/qingjiang/dpv. singularity run /path/dpv/deepvariant_1.4.0.sif /opt/deepvariant/bin/run_deepvariant \. --num_shards=3 \. --model_type=PACBIO \. --ref=""${INPUT_DIR}""/QJref.fa \. --reads=""${INPUT_DIR}""/input.bam \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. ```. error：. ```. I0213 16:54:59.595547 140573030586176 run_deepvariant.py:342] Re-using the directory for intermediate results in /path/dpv/intermediate_results_dir. ***** Intermediate results will be written to /path/dpv/intermediate_results_dir in docker. ****. ***** Running the command:*****. time seq 0 2 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/path/dpv/QJref.fa"" --reads ""/path/dpv/input.bam"" --examples ""/dellfsq. perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. LANGUAGE = (unset),. LC_ALL = (unset),. LC_CTYPE = ""C.UTF-8"",. LANG = ""en_US.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. LANGUAGE = (unset),. LC_ALL = (unset),. LC_CTYPE = ""C.UTF-8"",. LANG = ""en_US.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). Error in tempfile() using template /path1/8_Environment/TMPDIR/parXXXXX.par: Parent directory (/path1/8_Environment/TMPDIR/) does not exist at /usr/bin/parallel line 3889. real 0m3.019s. user 0m0.211s. sys 0m0.371s. ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/613
https://github.com/google/deepvariant/issues/613:977,usability,command,command,977,"TMPDIR and locale settings error！please help me!; Hello, I got an error when running deepvariant. I've verified that the path `/path1/8_Environment/TMPDIR`exists. I've googled all over and still can't solve the problem. please help me! command：. ```. INPUT_DIR=/path1/4_Test/qingjiang/dpv. OUTPUT_DIR=/path1/4_Test/qingjiang/dpv. singularity run /path/dpv/deepvariant_1.4.0.sif /opt/deepvariant/bin/run_deepvariant \. --num_shards=3 \. --model_type=PACBIO \. --ref=""${INPUT_DIR}""/QJref.fa \. --reads=""${INPUT_DIR}""/input.bam \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. ```. error：. ```. I0213 16:54:59.595547 140573030586176 run_deepvariant.py:342] Re-using the directory for intermediate results in /path/dpv/intermediate_results_dir. ***** Intermediate results will be written to /path/dpv/intermediate_results_dir in docker. ****. ***** Running the command:*****. time seq 0 2 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/path/dpv/QJref.fa"" --reads ""/path/dpv/input.bam"" --examples ""/dellfsq. perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. LANGUAGE = (unset),. LC_ALL = (unset),. LC_CTYPE = ""C.UTF-8"",. LANG = ""en_US.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. LANGUAGE = (unset),. LC_ALL = (unset),. LC_CTYPE = ""C.UTF-8"",. LANG = ""en_US.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). Error in tempfile() using template /path1/8_Environment/TMPDIR/parXXXXX.par: Parent directory (/path1/8_Environment/TMPDIR/) does not exist at /usr/bin/parallel line 3889. real 0m3.019s. user 0m0.211s. sys 0m0.371s. ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/613
https://github.com/google/deepvariant/issues/613:1138,usability,input,input,1138,"TMPDIR and locale settings error！please help me!; Hello, I got an error when running deepvariant. I've verified that the path `/path1/8_Environment/TMPDIR`exists. I've googled all over and still can't solve the problem. please help me! command：. ```. INPUT_DIR=/path1/4_Test/qingjiang/dpv. OUTPUT_DIR=/path1/4_Test/qingjiang/dpv. singularity run /path/dpv/deepvariant_1.4.0.sif /opt/deepvariant/bin/run_deepvariant \. --num_shards=3 \. --model_type=PACBIO \. --ref=""${INPUT_DIR}""/QJref.fa \. --reads=""${INPUT_DIR}""/input.bam \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. ```. error：. ```. I0213 16:54:59.595547 140573030586176 run_deepvariant.py:342] Re-using the directory for intermediate results in /path/dpv/intermediate_results_dir. ***** Intermediate results will be written to /path/dpv/intermediate_results_dir in docker. ****. ***** Running the command:*****. time seq 0 2 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/path/dpv/QJref.fa"" --reads ""/path/dpv/input.bam"" --examples ""/dellfsq. perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. LANGUAGE = (unset),. LC_ALL = (unset),. LC_CTYPE = ""C.UTF-8"",. LANG = ""en_US.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. LANGUAGE = (unset),. LC_ALL = (unset),. LC_CTYPE = ""C.UTF-8"",. LANG = ""en_US.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). Error in tempfile() using template /path1/8_Environment/TMPDIR/parXXXXX.par: Parent directory (/path1/8_Environment/TMPDIR/) does not exist at /usr/bin/parallel line 3889. real 0m3.019s. user 0m0.211s. sys 0m0.371s. ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/613
https://github.com/google/deepvariant/issues/613:1354,usability,support,supported,1354,"TMPDIR and locale settings error！please help me!; Hello, I got an error when running deepvariant. I've verified that the path `/path1/8_Environment/TMPDIR`exists. I've googled all over and still can't solve the problem. please help me! command：. ```. INPUT_DIR=/path1/4_Test/qingjiang/dpv. OUTPUT_DIR=/path1/4_Test/qingjiang/dpv. singularity run /path/dpv/deepvariant_1.4.0.sif /opt/deepvariant/bin/run_deepvariant \. --num_shards=3 \. --model_type=PACBIO \. --ref=""${INPUT_DIR}""/QJref.fa \. --reads=""${INPUT_DIR}""/input.bam \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. ```. error：. ```. I0213 16:54:59.595547 140573030586176 run_deepvariant.py:342] Re-using the directory for intermediate results in /path/dpv/intermediate_results_dir. ***** Intermediate results will be written to /path/dpv/intermediate_results_dir in docker. ****. ***** Running the command:*****. time seq 0 2 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/path/dpv/QJref.fa"" --reads ""/path/dpv/input.bam"" --examples ""/dellfsq. perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. LANGUAGE = (unset),. LC_ALL = (unset),. LC_CTYPE = ""C.UTF-8"",. LANG = ""en_US.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. LANGUAGE = (unset),. LC_ALL = (unset),. LC_CTYPE = ""C.UTF-8"",. LANG = ""en_US.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). Error in tempfile() using template /path1/8_Environment/TMPDIR/parXXXXX.par: Parent directory (/path1/8_Environment/TMPDIR/) does not exist at /usr/bin/parallel line 3889. real 0m3.019s. user 0m0.211s. sys 0m0.371s. ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/613
https://github.com/google/deepvariant/issues/613:1635,usability,support,supported,1635,"TMPDIR and locale settings error！please help me!; Hello, I got an error when running deepvariant. I've verified that the path `/path1/8_Environment/TMPDIR`exists. I've googled all over and still can't solve the problem. please help me! command：. ```. INPUT_DIR=/path1/4_Test/qingjiang/dpv. OUTPUT_DIR=/path1/4_Test/qingjiang/dpv. singularity run /path/dpv/deepvariant_1.4.0.sif /opt/deepvariant/bin/run_deepvariant \. --num_shards=3 \. --model_type=PACBIO \. --ref=""${INPUT_DIR}""/QJref.fa \. --reads=""${INPUT_DIR}""/input.bam \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. ```. error：. ```. I0213 16:54:59.595547 140573030586176 run_deepvariant.py:342] Re-using the directory for intermediate results in /path/dpv/intermediate_results_dir. ***** Intermediate results will be written to /path/dpv/intermediate_results_dir in docker. ****. ***** Running the command:*****. time seq 0 2 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/path/dpv/QJref.fa"" --reads ""/path/dpv/input.bam"" --examples ""/dellfsq. perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. LANGUAGE = (unset),. LC_ALL = (unset),. LC_CTYPE = ""C.UTF-8"",. LANG = ""en_US.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. LANGUAGE = (unset),. LC_ALL = (unset),. LC_CTYPE = ""C.UTF-8"",. LANG = ""en_US.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). Error in tempfile() using template /path1/8_Environment/TMPDIR/parXXXXX.par: Parent directory (/path1/8_Environment/TMPDIR/) does not exist at /usr/bin/parallel line 3889. real 0m3.019s. user 0m0.211s. sys 0m0.371s. ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/613
https://github.com/google/deepvariant/issues/613:1733,usability,Error,Error,1733,"TMPDIR and locale settings error！please help me!; Hello, I got an error when running deepvariant. I've verified that the path `/path1/8_Environment/TMPDIR`exists. I've googled all over and still can't solve the problem. please help me! command：. ```. INPUT_DIR=/path1/4_Test/qingjiang/dpv. OUTPUT_DIR=/path1/4_Test/qingjiang/dpv. singularity run /path/dpv/deepvariant_1.4.0.sif /opt/deepvariant/bin/run_deepvariant \. --num_shards=3 \. --model_type=PACBIO \. --ref=""${INPUT_DIR}""/QJref.fa \. --reads=""${INPUT_DIR}""/input.bam \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. ```. error：. ```. I0213 16:54:59.595547 140573030586176 run_deepvariant.py:342] Re-using the directory for intermediate results in /path/dpv/intermediate_results_dir. ***** Intermediate results will be written to /path/dpv/intermediate_results_dir in docker. ****. ***** Running the command:*****. time seq 0 2 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/path/dpv/QJref.fa"" --reads ""/path/dpv/input.bam"" --examples ""/dellfsq. perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. LANGUAGE = (unset),. LC_ALL = (unset),. LC_CTYPE = ""C.UTF-8"",. LANG = ""en_US.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. LANGUAGE = (unset),. LC_ALL = (unset),. LC_CTYPE = ""C.UTF-8"",. LANG = ""en_US.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). Error in tempfile() using template /path1/8_Environment/TMPDIR/parXXXXX.par: Parent directory (/path1/8_Environment/TMPDIR/) does not exist at /usr/bin/parallel line 3889. real 0m3.019s. user 0m0.211s. sys 0m0.371s. ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/613
https://github.com/google/deepvariant/issues/613:1920,usability,user,user,1920,"TMPDIR and locale settings error！please help me!; Hello, I got an error when running deepvariant. I've verified that the path `/path1/8_Environment/TMPDIR`exists. I've googled all over and still can't solve the problem. please help me! command：. ```. INPUT_DIR=/path1/4_Test/qingjiang/dpv. OUTPUT_DIR=/path1/4_Test/qingjiang/dpv. singularity run /path/dpv/deepvariant_1.4.0.sif /opt/deepvariant/bin/run_deepvariant \. --num_shards=3 \. --model_type=PACBIO \. --ref=""${INPUT_DIR}""/QJref.fa \. --reads=""${INPUT_DIR}""/input.bam \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. ```. error：. ```. I0213 16:54:59.595547 140573030586176 run_deepvariant.py:342] Re-using the directory for intermediate results in /path/dpv/intermediate_results_dir. ***** Intermediate results will be written to /path/dpv/intermediate_results_dir in docker. ****. ***** Running the command:*****. time seq 0 2 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/path/dpv/QJref.fa"" --reads ""/path/dpv/input.bam"" --examples ""/dellfsq. perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. LANGUAGE = (unset),. LC_ALL = (unset),. LC_CTYPE = ""C.UTF-8"",. LANG = ""en_US.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. LANGUAGE = (unset),. LC_ALL = (unset),. LC_CTYPE = ""C.UTF-8"",. LANG = ""en_US.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). Error in tempfile() using template /path1/8_Environment/TMPDIR/parXXXXX.par: Parent directory (/path1/8_Environment/TMPDIR/) does not exist at /usr/bin/parallel line 3889. real 0m3.019s. user 0m0.211s. sys 0m0.371s. ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/613
https://github.com/google/deepvariant/issues/614:12793,availability,error,error,12793,"s) [6737.31s elapsed]. I0218 10:46:36.864049 23456243894080 make_examples_core.py:243] Task 19/64: Skip phasing: len(candidates[main_sample]) is 20526. I0218 10:48:19.364838 23456243894080 make_examples_core.py:243] Task 2/64: 158555 candidates (173735 examples) [4091.74s elapsed]. I0218 10:48:45.881830 23456243894080 make_examples_core.py:243] Task 2/64: Skip phasing: len(candidates[main_sample]) is 14234. I0218 10:49:31.045118 23456243894080 make_examples_core.py:243] Task 13/64: 113956 candidates (125182 examples) [6317.67s elapsed]. I0218 10:50:33.895329 23456243894080 make_examples_core.py:243] Task 13/64: Skip phasing: len(candidates[main_sample]) is 18414. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /scratch4/path.to.mydir/genomes/c_elegans.PRJNA13758.WS245.genomic.fa --reads /scratch4/path.to.mydir/pbmm2/aln13448198.pbmm2.bam --examples /tmp/tmp1yvr59_z/make_examples.tfrecord@64.gz --add_hp_channel --alt_aligned_pileup diff_channels --max_reads_per_partition 600 --min_mapping_quality 1 --parse_sam_aux_fields --partition_size 25000 --phase_reads --pileup_image_width 199 --norealign_reads --sort_by_haplotypes --track_ref_reads --vsc_min_fraction_indels 0.12 --task 28. real	879m59.515s. user	632m52.969s. sys	6m20.594s. INFO: Cleaning up image... ```. I also ran more jobs using different numbers of cpu and mem using different bam files. One using 48 cpu and --mem-per-cpu=6G simply fizzled without any error message. These jobs are taking considerable core-hours, so troubleshooting is hard. I also wonder if I am using Deepvariant efficiently. On a side note, I got many Deepvariant failures with error messages like:. ```. Detected 1372 oom-kill event(s) in StepId=12049020.batch cgroup. Some of your processes may have been killed by the cgroup out-of-memory handler. ```. This seems to have been resolved by asking for maximum allowable memory. I am still curious about the memory requirement for successfully running Deepvariant.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/614
https://github.com/google/deepvariant/issues/614:12975,availability,failur,failures,12975,"s) [6737.31s elapsed]. I0218 10:46:36.864049 23456243894080 make_examples_core.py:243] Task 19/64: Skip phasing: len(candidates[main_sample]) is 20526. I0218 10:48:19.364838 23456243894080 make_examples_core.py:243] Task 2/64: 158555 candidates (173735 examples) [4091.74s elapsed]. I0218 10:48:45.881830 23456243894080 make_examples_core.py:243] Task 2/64: Skip phasing: len(candidates[main_sample]) is 14234. I0218 10:49:31.045118 23456243894080 make_examples_core.py:243] Task 13/64: 113956 candidates (125182 examples) [6317.67s elapsed]. I0218 10:50:33.895329 23456243894080 make_examples_core.py:243] Task 13/64: Skip phasing: len(candidates[main_sample]) is 18414. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /scratch4/path.to.mydir/genomes/c_elegans.PRJNA13758.WS245.genomic.fa --reads /scratch4/path.to.mydir/pbmm2/aln13448198.pbmm2.bam --examples /tmp/tmp1yvr59_z/make_examples.tfrecord@64.gz --add_hp_channel --alt_aligned_pileup diff_channels --max_reads_per_partition 600 --min_mapping_quality 1 --parse_sam_aux_fields --partition_size 25000 --phase_reads --pileup_image_width 199 --norealign_reads --sort_by_haplotypes --track_ref_reads --vsc_min_fraction_indels 0.12 --task 28. real	879m59.515s. user	632m52.969s. sys	6m20.594s. INFO: Cleaning up image... ```. I also ran more jobs using different numbers of cpu and mem using different bam files. One using 48 cpu and --mem-per-cpu=6G simply fizzled without any error message. These jobs are taking considerable core-hours, so troubleshooting is hard. I also wonder if I am using Deepvariant efficiently. On a side note, I got many Deepvariant failures with error messages like:. ```. Detected 1372 oom-kill event(s) in StepId=12049020.batch cgroup. Some of your processes may have been killed by the cgroup out-of-memory handler. ```. This seems to have been resolved by asking for maximum allowable memory. I am still curious about the memory requirement for successfully running Deepvariant.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/614
https://github.com/google/deepvariant/issues/614:12989,availability,error,error,12989,"s) [6737.31s elapsed]. I0218 10:46:36.864049 23456243894080 make_examples_core.py:243] Task 19/64: Skip phasing: len(candidates[main_sample]) is 20526. I0218 10:48:19.364838 23456243894080 make_examples_core.py:243] Task 2/64: 158555 candidates (173735 examples) [4091.74s elapsed]. I0218 10:48:45.881830 23456243894080 make_examples_core.py:243] Task 2/64: Skip phasing: len(candidates[main_sample]) is 14234. I0218 10:49:31.045118 23456243894080 make_examples_core.py:243] Task 13/64: 113956 candidates (125182 examples) [6317.67s elapsed]. I0218 10:50:33.895329 23456243894080 make_examples_core.py:243] Task 13/64: Skip phasing: len(candidates[main_sample]) is 18414. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /scratch4/path.to.mydir/genomes/c_elegans.PRJNA13758.WS245.genomic.fa --reads /scratch4/path.to.mydir/pbmm2/aln13448198.pbmm2.bam --examples /tmp/tmp1yvr59_z/make_examples.tfrecord@64.gz --add_hp_channel --alt_aligned_pileup diff_channels --max_reads_per_partition 600 --min_mapping_quality 1 --parse_sam_aux_fields --partition_size 25000 --phase_reads --pileup_image_width 199 --norealign_reads --sort_by_haplotypes --track_ref_reads --vsc_min_fraction_indels 0.12 --task 28. real	879m59.515s. user	632m52.969s. sys	6m20.594s. INFO: Cleaning up image... ```. I also ran more jobs using different numbers of cpu and mem using different bam files. One using 48 cpu and --mem-per-cpu=6G simply fizzled without any error message. These jobs are taking considerable core-hours, so troubleshooting is hard. I also wonder if I am using Deepvariant efficiently. On a side note, I got many Deepvariant failures with error messages like:. ```. Detected 1372 oom-kill event(s) in StepId=12049020.batch cgroup. Some of your processes may have been killed by the cgroup out-of-memory handler. ```. This seems to have been resolved by asking for maximum allowable memory. I am still curious about the memory requirement for successfully running Deepvariant.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/614
https://github.com/google/deepvariant/issues/614:12,deployability,fail,fails,12,"Deepvariant fails without clear reason - with PacBio data; Deepvariant fails without clear reason. . **Setup**. JHU Rockfish HPC. Singularity 3.8.7. singularity pull docker://google/deepvariant:1.4.0. Problematic data are PacBio (first gen). I have used Deepvariant with Illumina without problem, and I used PEPPER to process Ont data and PacBio Hifi data. I used pbmm2 to align fastq with all PacBio data. Command used to run:. ```. #!/bin/bash. #SBATCH --job-name=deep64_13448198. #SBATCH --time=24:00:00. #SBATCH --nodes=2. #SBATCH --ntasks-per-node=1. #SBATCH --cpus-per-task=32. #SBATCH --mem=0. ml anaconda. conda activate /data/path.to.mydir/deepvariant. singularity run --bind /scratch4/path.to.mydir/:/scratch4/path.to.mydir/ \. docker://google/deepvariant:1.4.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type PACBIO \. --ref c_elegans.PRJNA13758.WS245.genomic.fa \. --reads aln13448198.pbmm2.bam \. --output_vcf aln13448198.pbmm2.dv.vcf.gz \. --num_shards 64. ```. Here's a long snippet of slurm output:. ```. INFO: Using cached SIF image. INFO: Converting SIF file to temporary sandbox... I0217 20:13:14.117354 23456243894080 run_deepvariant.py:342] Re-using the directory for intermediate results in /tmp/tmp1yvr59_z. ***** Intermediate results will be written to /tmp/tmp1yvr59_z in docker. ****. ***** Running the command:*****. time seq 0 63 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref [snipped]. #[snip]. # this part is likely unimportant. perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. 	LANGUAGE = (unset),. 	LC_ALL = (unset),. 	LC_CTYPE = ""C.UTF-8"",. 	LANG = ""en_US.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. 	LANGUAGE = (unset),. 	LC_ALL = (unset),. 	LC_CTYPE = ""C.UTF-8"",. 	LANG = ""en_US.UTF-8"". are supported and ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/614
https://github.com/google/deepvariant/issues/614:71,deployability,fail,fails,71,"Deepvariant fails without clear reason - with PacBio data; Deepvariant fails without clear reason. . **Setup**. JHU Rockfish HPC. Singularity 3.8.7. singularity pull docker://google/deepvariant:1.4.0. Problematic data are PacBio (first gen). I have used Deepvariant with Illumina without problem, and I used PEPPER to process Ont data and PacBio Hifi data. I used pbmm2 to align fastq with all PacBio data. Command used to run:. ```. #!/bin/bash. #SBATCH --job-name=deep64_13448198. #SBATCH --time=24:00:00. #SBATCH --nodes=2. #SBATCH --ntasks-per-node=1. #SBATCH --cpus-per-task=32. #SBATCH --mem=0. ml anaconda. conda activate /data/path.to.mydir/deepvariant. singularity run --bind /scratch4/path.to.mydir/:/scratch4/path.to.mydir/ \. docker://google/deepvariant:1.4.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type PACBIO \. --ref c_elegans.PRJNA13758.WS245.genomic.fa \. --reads aln13448198.pbmm2.bam \. --output_vcf aln13448198.pbmm2.dv.vcf.gz \. --num_shards 64. ```. Here's a long snippet of slurm output:. ```. INFO: Using cached SIF image. INFO: Converting SIF file to temporary sandbox... I0217 20:13:14.117354 23456243894080 run_deepvariant.py:342] Re-using the directory for intermediate results in /tmp/tmp1yvr59_z. ***** Intermediate results will be written to /tmp/tmp1yvr59_z in docker. ****. ***** Running the command:*****. time seq 0 63 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref [snipped]. #[snip]. # this part is likely unimportant. perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. 	LANGUAGE = (unset),. 	LC_ALL = (unset),. 	LC_CTYPE = ""C.UTF-8"",. 	LANG = ""en_US.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. 	LANGUAGE = (unset),. 	LC_ALL = (unset),. 	LC_CTYPE = ""C.UTF-8"",. 	LANG = ""en_US.UTF-8"". are supported and ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/614
https://github.com/google/deepvariant/issues/614:1544,deployability,fail,failed,1544,"-node=1. #SBATCH --cpus-per-task=32. #SBATCH --mem=0. ml anaconda. conda activate /data/path.to.mydir/deepvariant. singularity run --bind /scratch4/path.to.mydir/:/scratch4/path.to.mydir/ \. docker://google/deepvariant:1.4.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type PACBIO \. --ref c_elegans.PRJNA13758.WS245.genomic.fa \. --reads aln13448198.pbmm2.bam \. --output_vcf aln13448198.pbmm2.dv.vcf.gz \. --num_shards 64. ```. Here's a long snippet of slurm output:. ```. INFO: Using cached SIF image. INFO: Converting SIF file to temporary sandbox... I0217 20:13:14.117354 23456243894080 run_deepvariant.py:342] Re-using the directory for intermediate results in /tmp/tmp1yvr59_z. ***** Intermediate results will be written to /tmp/tmp1yvr59_z in docker. ****. ***** Running the command:*****. time seq 0 63 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref [snipped]. #[snip]. # this part is likely unimportant. perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. 	LANGUAGE = (unset),. 	LC_ALL = (unset),. 	LC_CTYPE = ""C.UTF-8"",. 	LANG = ""en_US.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. 	LANGUAGE = (unset),. 	LC_ALL = (unset),. 	LC_CTYPE = ""C.UTF-8"",. 	LANG = ""en_US.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). #[snip]. 2023-02-17 20:13:17.235641: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD	VN:1.6	SO:coordinate	pb:5.0.0. I0217 20:13:17.235805 23456243894080 genomics_reader.py:222] Reading /scratch4/path.to.mydir/pbmm2/aln13448198.pbmm2.bam with NativeSamReader. I0217 20:13:17.268698 23456243894080 make_examples_core.py:243] Task 18/64: Preparing inputs. 2023-02-17 20:13:17.371669: W third_party/nucleus/io/sam_reader.cc:1",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/614
https://github.com/google/deepvariant/issues/614:1715,deployability,instal,installed,1715,"path.to.mydir/ \. docker://google/deepvariant:1.4.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type PACBIO \. --ref c_elegans.PRJNA13758.WS245.genomic.fa \. --reads aln13448198.pbmm2.bam \. --output_vcf aln13448198.pbmm2.dv.vcf.gz \. --num_shards 64. ```. Here's a long snippet of slurm output:. ```. INFO: Using cached SIF image. INFO: Converting SIF file to temporary sandbox... I0217 20:13:14.117354 23456243894080 run_deepvariant.py:342] Re-using the directory for intermediate results in /tmp/tmp1yvr59_z. ***** Intermediate results will be written to /tmp/tmp1yvr59_z in docker. ****. ***** Running the command:*****. time seq 0 63 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref [snipped]. #[snip]. # this part is likely unimportant. perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. 	LANGUAGE = (unset),. 	LC_ALL = (unset),. 	LC_CTYPE = ""C.UTF-8"",. 	LANG = ""en_US.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. 	LANGUAGE = (unset),. 	LC_ALL = (unset),. 	LC_CTYPE = ""C.UTF-8"",. 	LANG = ""en_US.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). #[snip]. 2023-02-17 20:13:17.235641: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD	VN:1.6	SO:coordinate	pb:5.0.0. I0217 20:13:17.235805 23456243894080 genomics_reader.py:222] Reading /scratch4/path.to.mydir/pbmm2/aln13448198.pbmm2.bam with NativeSamReader. I0217 20:13:17.268698 23456243894080 make_examples_core.py:243] Task 18/64: Preparing inputs. 2023-02-17 20:13:17.371669: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD	VN:1.6	SO:coordinate	pb:5.0.0. I0217 20:13:17.371811 23456243894080 genomics_reader.py:222] Reading /scratch4/jwang8/hoyon/",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/614
https://github.com/google/deepvariant/issues/614:1829,deployability,fail,failed,1829,"IO \. --ref c_elegans.PRJNA13758.WS245.genomic.fa \. --reads aln13448198.pbmm2.bam \. --output_vcf aln13448198.pbmm2.dv.vcf.gz \. --num_shards 64. ```. Here's a long snippet of slurm output:. ```. INFO: Using cached SIF image. INFO: Converting SIF file to temporary sandbox... I0217 20:13:14.117354 23456243894080 run_deepvariant.py:342] Re-using the directory for intermediate results in /tmp/tmp1yvr59_z. ***** Intermediate results will be written to /tmp/tmp1yvr59_z in docker. ****. ***** Running the command:*****. time seq 0 63 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref [snipped]. #[snip]. # this part is likely unimportant. perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. 	LANGUAGE = (unset),. 	LC_ALL = (unset),. 	LC_CTYPE = ""C.UTF-8"",. 	LANG = ""en_US.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. 	LANGUAGE = (unset),. 	LC_ALL = (unset),. 	LC_CTYPE = ""C.UTF-8"",. 	LANG = ""en_US.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). #[snip]. 2023-02-17 20:13:17.235641: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD	VN:1.6	SO:coordinate	pb:5.0.0. I0217 20:13:17.235805 23456243894080 genomics_reader.py:222] Reading /scratch4/path.to.mydir/pbmm2/aln13448198.pbmm2.bam with NativeSamReader. I0217 20:13:17.268698 23456243894080 make_examples_core.py:243] Task 18/64: Preparing inputs. 2023-02-17 20:13:17.371669: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD	VN:1.6	SO:coordinate	pb:5.0.0. I0217 20:13:17.371811 23456243894080 genomics_reader.py:222] Reading /scratch4/jwang8/hoyon/pbmm2/aln13448198.pbmm2.bam with NativeSamReader. I0217 20:13:17.376646 23456243894080 make_examples_core.py:243",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/614
https://github.com/google/deepvariant/issues/614:2000,deployability,instal,installed,2000," of slurm output:. ```. INFO: Using cached SIF image. INFO: Converting SIF file to temporary sandbox... I0217 20:13:14.117354 23456243894080 run_deepvariant.py:342] Re-using the directory for intermediate results in /tmp/tmp1yvr59_z. ***** Intermediate results will be written to /tmp/tmp1yvr59_z in docker. ****. ***** Running the command:*****. time seq 0 63 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref [snipped]. #[snip]. # this part is likely unimportant. perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. 	LANGUAGE = (unset),. 	LC_ALL = (unset),. 	LC_CTYPE = ""C.UTF-8"",. 	LANG = ""en_US.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. 	LANGUAGE = (unset),. 	LC_ALL = (unset),. 	LC_CTYPE = ""C.UTF-8"",. 	LANG = ""en_US.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). #[snip]. 2023-02-17 20:13:17.235641: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD	VN:1.6	SO:coordinate	pb:5.0.0. I0217 20:13:17.235805 23456243894080 genomics_reader.py:222] Reading /scratch4/path.to.mydir/pbmm2/aln13448198.pbmm2.bam with NativeSamReader. I0217 20:13:17.268698 23456243894080 make_examples_core.py:243] Task 18/64: Preparing inputs. 2023-02-17 20:13:17.371669: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD	VN:1.6	SO:coordinate	pb:5.0.0. I0217 20:13:17.371811 23456243894080 genomics_reader.py:222] Reading /scratch4/jwang8/hoyon/pbmm2/aln13448198.pbmm2.bam with NativeSamReader. I0217 20:13:17.376646 23456243894080 make_examples_core.py:243] Task 18/64: Common contigs are ['I', 'II', 'III', 'IV', 'V', 'X', 'MtDNA']. I0217 20:13:17.383403 23456243894080 make_examples_core.py:243] Task 18/64: Starting from v0.9.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/614
https://github.com/google/deepvariant/issues/614:12016,deployability,fail,failed,12016,"Task 28/64: 105506 candidates (115610 examples) [6984.90s elapsed]. I0218 10:42:47.468951 23456243894080 make_examples_core.py:243] Task 28/64: Skip phasing: len(candidates[main_sample]) is 14998. I0218 10:45:20.167151 23456243894080 make_examples_core.py:243] Task 19/64: 115447 candidates (126875 examples) [6737.31s elapsed]. I0218 10:46:36.864049 23456243894080 make_examples_core.py:243] Task 19/64: Skip phasing: len(candidates[main_sample]) is 20526. I0218 10:48:19.364838 23456243894080 make_examples_core.py:243] Task 2/64: 158555 candidates (173735 examples) [4091.74s elapsed]. I0218 10:48:45.881830 23456243894080 make_examples_core.py:243] Task 2/64: Skip phasing: len(candidates[main_sample]) is 14234. I0218 10:49:31.045118 23456243894080 make_examples_core.py:243] Task 13/64: 113956 candidates (125182 examples) [6317.67s elapsed]. I0218 10:50:33.895329 23456243894080 make_examples_core.py:243] Task 13/64: Skip phasing: len(candidates[main_sample]) is 18414. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /scratch4/path.to.mydir/genomes/c_elegans.PRJNA13758.WS245.genomic.fa --reads /scratch4/path.to.mydir/pbmm2/aln13448198.pbmm2.bam --examples /tmp/tmp1yvr59_z/make_examples.tfrecord@64.gz --add_hp_channel --alt_aligned_pileup diff_channels --max_reads_per_partition 600 --min_mapping_quality 1 --parse_sam_aux_fields --partition_size 25000 --phase_reads --pileup_image_width 199 --norealign_reads --sort_by_haplotypes --track_ref_reads --vsc_min_fraction_indels 0.12 --task 28. real	879m59.515s. user	632m52.969s. sys	6m20.594s. INFO: Cleaning up image... ```. I also ran more jobs using different numbers of cpu and mem using different bam files. One using 48 cpu and --mem-per-cpu=6G simply fizzled without any error message. These jobs are taking considerable core-hours, so troubleshooting is hard. I also wonder if I am using Deepvariant efficiently. On a side note, I got many Deepvariant failures with error messages like:. ```. Det",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/614
https://github.com/google/deepvariant/issues/614:12975,deployability,fail,failures,12975,"s) [6737.31s elapsed]. I0218 10:46:36.864049 23456243894080 make_examples_core.py:243] Task 19/64: Skip phasing: len(candidates[main_sample]) is 20526. I0218 10:48:19.364838 23456243894080 make_examples_core.py:243] Task 2/64: 158555 candidates (173735 examples) [4091.74s elapsed]. I0218 10:48:45.881830 23456243894080 make_examples_core.py:243] Task 2/64: Skip phasing: len(candidates[main_sample]) is 14234. I0218 10:49:31.045118 23456243894080 make_examples_core.py:243] Task 13/64: 113956 candidates (125182 examples) [6317.67s elapsed]. I0218 10:50:33.895329 23456243894080 make_examples_core.py:243] Task 13/64: Skip phasing: len(candidates[main_sample]) is 18414. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /scratch4/path.to.mydir/genomes/c_elegans.PRJNA13758.WS245.genomic.fa --reads /scratch4/path.to.mydir/pbmm2/aln13448198.pbmm2.bam --examples /tmp/tmp1yvr59_z/make_examples.tfrecord@64.gz --add_hp_channel --alt_aligned_pileup diff_channels --max_reads_per_partition 600 --min_mapping_quality 1 --parse_sam_aux_fields --partition_size 25000 --phase_reads --pileup_image_width 199 --norealign_reads --sort_by_haplotypes --track_ref_reads --vsc_min_fraction_indels 0.12 --task 28. real	879m59.515s. user	632m52.969s. sys	6m20.594s. INFO: Cleaning up image... ```. I also ran more jobs using different numbers of cpu and mem using different bam files. One using 48 cpu and --mem-per-cpu=6G simply fizzled without any error message. These jobs are taking considerable core-hours, so troubleshooting is hard. I also wonder if I am using Deepvariant efficiently. On a side note, I got many Deepvariant failures with error messages like:. ```. Detected 1372 oom-kill event(s) in StepId=12049020.batch cgroup. Some of your processes may have been killed by the cgroup out-of-memory handler. ```. This seems to have been resolved by asking for maximum allowable memory. I am still curious about the memory requirement for successfully running Deepvariant.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/614
https://github.com/google/deepvariant/issues/614:566,energy efficiency,cpu,cpus-per-task,566,"Deepvariant fails without clear reason - with PacBio data; Deepvariant fails without clear reason. . **Setup**. JHU Rockfish HPC. Singularity 3.8.7. singularity pull docker://google/deepvariant:1.4.0. Problematic data are PacBio (first gen). I have used Deepvariant with Illumina without problem, and I used PEPPER to process Ont data and PacBio Hifi data. I used pbmm2 to align fastq with all PacBio data. Command used to run:. ```. #!/bin/bash. #SBATCH --job-name=deep64_13448198. #SBATCH --time=24:00:00. #SBATCH --nodes=2. #SBATCH --ntasks-per-node=1. #SBATCH --cpus-per-task=32. #SBATCH --mem=0. ml anaconda. conda activate /data/path.to.mydir/deepvariant. singularity run --bind /scratch4/path.to.mydir/:/scratch4/path.to.mydir/ \. docker://google/deepvariant:1.4.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type PACBIO \. --ref c_elegans.PRJNA13758.WS245.genomic.fa \. --reads aln13448198.pbmm2.bam \. --output_vcf aln13448198.pbmm2.dv.vcf.gz \. --num_shards 64. ```. Here's a long snippet of slurm output:. ```. INFO: Using cached SIF image. INFO: Converting SIF file to temporary sandbox... I0217 20:13:14.117354 23456243894080 run_deepvariant.py:342] Re-using the directory for intermediate results in /tmp/tmp1yvr59_z. ***** Intermediate results will be written to /tmp/tmp1yvr59_z in docker. ****. ***** Running the command:*****. time seq 0 63 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref [snipped]. #[snip]. # this part is likely unimportant. perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. 	LANGUAGE = (unset),. 	LC_ALL = (unset),. 	LC_CTYPE = ""C.UTF-8"",. 	LANG = ""en_US.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. 	LANGUAGE = (unset),. 	LC_ALL = (unset),. 	LC_CTYPE = ""C.UTF-8"",. 	LANG = ""en_US.UTF-8"". are supported and ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/614
https://github.com/google/deepvariant/issues/614:12689,energy efficiency,cpu,cpu,12689,"s) [6737.31s elapsed]. I0218 10:46:36.864049 23456243894080 make_examples_core.py:243] Task 19/64: Skip phasing: len(candidates[main_sample]) is 20526. I0218 10:48:19.364838 23456243894080 make_examples_core.py:243] Task 2/64: 158555 candidates (173735 examples) [4091.74s elapsed]. I0218 10:48:45.881830 23456243894080 make_examples_core.py:243] Task 2/64: Skip phasing: len(candidates[main_sample]) is 14234. I0218 10:49:31.045118 23456243894080 make_examples_core.py:243] Task 13/64: 113956 candidates (125182 examples) [6317.67s elapsed]. I0218 10:50:33.895329 23456243894080 make_examples_core.py:243] Task 13/64: Skip phasing: len(candidates[main_sample]) is 18414. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /scratch4/path.to.mydir/genomes/c_elegans.PRJNA13758.WS245.genomic.fa --reads /scratch4/path.to.mydir/pbmm2/aln13448198.pbmm2.bam --examples /tmp/tmp1yvr59_z/make_examples.tfrecord@64.gz --add_hp_channel --alt_aligned_pileup diff_channels --max_reads_per_partition 600 --min_mapping_quality 1 --parse_sam_aux_fields --partition_size 25000 --phase_reads --pileup_image_width 199 --norealign_reads --sort_by_haplotypes --track_ref_reads --vsc_min_fraction_indels 0.12 --task 28. real	879m59.515s. user	632m52.969s. sys	6m20.594s. INFO: Cleaning up image... ```. I also ran more jobs using different numbers of cpu and mem using different bam files. One using 48 cpu and --mem-per-cpu=6G simply fizzled without any error message. These jobs are taking considerable core-hours, so troubleshooting is hard. I also wonder if I am using Deepvariant efficiently. On a side note, I got many Deepvariant failures with error messages like:. ```. Detected 1372 oom-kill event(s) in StepId=12049020.batch cgroup. Some of your processes may have been killed by the cgroup out-of-memory handler. ```. This seems to have been resolved by asking for maximum allowable memory. I am still curious about the memory requirement for successfully running Deepvariant.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/614
https://github.com/google/deepvariant/issues/614:12741,energy efficiency,cpu,cpu,12741,"s) [6737.31s elapsed]. I0218 10:46:36.864049 23456243894080 make_examples_core.py:243] Task 19/64: Skip phasing: len(candidates[main_sample]) is 20526. I0218 10:48:19.364838 23456243894080 make_examples_core.py:243] Task 2/64: 158555 candidates (173735 examples) [4091.74s elapsed]. I0218 10:48:45.881830 23456243894080 make_examples_core.py:243] Task 2/64: Skip phasing: len(candidates[main_sample]) is 14234. I0218 10:49:31.045118 23456243894080 make_examples_core.py:243] Task 13/64: 113956 candidates (125182 examples) [6317.67s elapsed]. I0218 10:50:33.895329 23456243894080 make_examples_core.py:243] Task 13/64: Skip phasing: len(candidates[main_sample]) is 18414. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /scratch4/path.to.mydir/genomes/c_elegans.PRJNA13758.WS245.genomic.fa --reads /scratch4/path.to.mydir/pbmm2/aln13448198.pbmm2.bam --examples /tmp/tmp1yvr59_z/make_examples.tfrecord@64.gz --add_hp_channel --alt_aligned_pileup diff_channels --max_reads_per_partition 600 --min_mapping_quality 1 --parse_sam_aux_fields --partition_size 25000 --phase_reads --pileup_image_width 199 --norealign_reads --sort_by_haplotypes --track_ref_reads --vsc_min_fraction_indels 0.12 --task 28. real	879m59.515s. user	632m52.969s. sys	6m20.594s. INFO: Cleaning up image... ```. I also ran more jobs using different numbers of cpu and mem using different bam files. One using 48 cpu and --mem-per-cpu=6G simply fizzled without any error message. These jobs are taking considerable core-hours, so troubleshooting is hard. I also wonder if I am using Deepvariant efficiently. On a side note, I got many Deepvariant failures with error messages like:. ```. Detected 1372 oom-kill event(s) in StepId=12049020.batch cgroup. Some of your processes may have been killed by the cgroup out-of-memory handler. ```. This seems to have been resolved by asking for maximum allowable memory. I am still curious about the memory requirement for successfully running Deepvariant.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/614
https://github.com/google/deepvariant/issues/614:12759,energy efficiency,cpu,cpu,12759,"s) [6737.31s elapsed]. I0218 10:46:36.864049 23456243894080 make_examples_core.py:243] Task 19/64: Skip phasing: len(candidates[main_sample]) is 20526. I0218 10:48:19.364838 23456243894080 make_examples_core.py:243] Task 2/64: 158555 candidates (173735 examples) [4091.74s elapsed]. I0218 10:48:45.881830 23456243894080 make_examples_core.py:243] Task 2/64: Skip phasing: len(candidates[main_sample]) is 14234. I0218 10:49:31.045118 23456243894080 make_examples_core.py:243] Task 13/64: 113956 candidates (125182 examples) [6317.67s elapsed]. I0218 10:50:33.895329 23456243894080 make_examples_core.py:243] Task 13/64: Skip phasing: len(candidates[main_sample]) is 18414. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /scratch4/path.to.mydir/genomes/c_elegans.PRJNA13758.WS245.genomic.fa --reads /scratch4/path.to.mydir/pbmm2/aln13448198.pbmm2.bam --examples /tmp/tmp1yvr59_z/make_examples.tfrecord@64.gz --add_hp_channel --alt_aligned_pileup diff_channels --max_reads_per_partition 600 --min_mapping_quality 1 --parse_sam_aux_fields --partition_size 25000 --phase_reads --pileup_image_width 199 --norealign_reads --sort_by_haplotypes --track_ref_reads --vsc_min_fraction_indels 0.12 --task 28. real	879m59.515s. user	632m52.969s. sys	6m20.594s. INFO: Cleaning up image... ```. I also ran more jobs using different numbers of cpu and mem using different bam files. One using 48 cpu and --mem-per-cpu=6G simply fizzled without any error message. These jobs are taking considerable core-hours, so troubleshooting is hard. I also wonder if I am using Deepvariant efficiently. On a side note, I got many Deepvariant failures with error messages like:. ```. Detected 1372 oom-kill event(s) in StepId=12049020.batch cgroup. Some of your processes may have been killed by the cgroup out-of-memory handler. ```. This seems to have been resolved by asking for maximum allowable memory. I am still curious about the memory requirement for successfully running Deepvariant.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/614
https://github.com/google/deepvariant/issues/614:12843,energy efficiency,core,core-hours,12843,"s) [6737.31s elapsed]. I0218 10:46:36.864049 23456243894080 make_examples_core.py:243] Task 19/64: Skip phasing: len(candidates[main_sample]) is 20526. I0218 10:48:19.364838 23456243894080 make_examples_core.py:243] Task 2/64: 158555 candidates (173735 examples) [4091.74s elapsed]. I0218 10:48:45.881830 23456243894080 make_examples_core.py:243] Task 2/64: Skip phasing: len(candidates[main_sample]) is 14234. I0218 10:49:31.045118 23456243894080 make_examples_core.py:243] Task 13/64: 113956 candidates (125182 examples) [6317.67s elapsed]. I0218 10:50:33.895329 23456243894080 make_examples_core.py:243] Task 13/64: Skip phasing: len(candidates[main_sample]) is 18414. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /scratch4/path.to.mydir/genomes/c_elegans.PRJNA13758.WS245.genomic.fa --reads /scratch4/path.to.mydir/pbmm2/aln13448198.pbmm2.bam --examples /tmp/tmp1yvr59_z/make_examples.tfrecord@64.gz --add_hp_channel --alt_aligned_pileup diff_channels --max_reads_per_partition 600 --min_mapping_quality 1 --parse_sam_aux_fields --partition_size 25000 --phase_reads --pileup_image_width 199 --norealign_reads --sort_by_haplotypes --track_ref_reads --vsc_min_fraction_indels 0.12 --task 28. real	879m59.515s. user	632m52.969s. sys	6m20.594s. INFO: Cleaning up image... ```. I also ran more jobs using different numbers of cpu and mem using different bam files. One using 48 cpu and --mem-per-cpu=6G simply fizzled without any error message. These jobs are taking considerable core-hours, so troubleshooting is hard. I also wonder if I am using Deepvariant efficiently. On a side note, I got many Deepvariant failures with error messages like:. ```. Detected 1372 oom-kill event(s) in StepId=12049020.batch cgroup. Some of your processes may have been killed by the cgroup out-of-memory handler. ```. This seems to have been resolved by asking for maximum allowable memory. I am still curious about the memory requirement for successfully running Deepvariant.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/614
https://github.com/google/deepvariant/issues/614:1396,integrability,buffer,buffer,1396,"o data. Command used to run:. ```. #!/bin/bash. #SBATCH --job-name=deep64_13448198. #SBATCH --time=24:00:00. #SBATCH --nodes=2. #SBATCH --ntasks-per-node=1. #SBATCH --cpus-per-task=32. #SBATCH --mem=0. ml anaconda. conda activate /data/path.to.mydir/deepvariant. singularity run --bind /scratch4/path.to.mydir/:/scratch4/path.to.mydir/ \. docker://google/deepvariant:1.4.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type PACBIO \. --ref c_elegans.PRJNA13758.WS245.genomic.fa \. --reads aln13448198.pbmm2.bam \. --output_vcf aln13448198.pbmm2.dv.vcf.gz \. --num_shards 64. ```. Here's a long snippet of slurm output:. ```. INFO: Using cached SIF image. INFO: Converting SIF file to temporary sandbox... I0217 20:13:14.117354 23456243894080 run_deepvariant.py:342] Re-using the directory for intermediate results in /tmp/tmp1yvr59_z. ***** Intermediate results will be written to /tmp/tmp1yvr59_z in docker. ****. ***** Running the command:*****. time seq 0 63 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref [snipped]. #[snip]. # this part is likely unimportant. perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. 	LANGUAGE = (unset),. 	LC_ALL = (unset),. 	LC_CTYPE = ""C.UTF-8"",. 	LANG = ""en_US.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. 	LANGUAGE = (unset),. 	LC_ALL = (unset),. 	LC_CTYPE = ""C.UTF-8"",. 	LANG = ""en_US.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). #[snip]. 2023-02-17 20:13:17.235641: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD	VN:1.6	SO:coordinate	pb:5.0.0. I0217 20:13:17.235805 23456243894080 genomics_reader.py:222] Reading /scratch4/path.to.mydir/pbmm2/aln13448198.pbmm2.bam with NativeSamReader. I0217 20:13:17",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/614
https://github.com/google/deepvariant/issues/614:12799,integrability,messag,message,12799,"s) [6737.31s elapsed]. I0218 10:46:36.864049 23456243894080 make_examples_core.py:243] Task 19/64: Skip phasing: len(candidates[main_sample]) is 20526. I0218 10:48:19.364838 23456243894080 make_examples_core.py:243] Task 2/64: 158555 candidates (173735 examples) [4091.74s elapsed]. I0218 10:48:45.881830 23456243894080 make_examples_core.py:243] Task 2/64: Skip phasing: len(candidates[main_sample]) is 14234. I0218 10:49:31.045118 23456243894080 make_examples_core.py:243] Task 13/64: 113956 candidates (125182 examples) [6317.67s elapsed]. I0218 10:50:33.895329 23456243894080 make_examples_core.py:243] Task 13/64: Skip phasing: len(candidates[main_sample]) is 18414. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /scratch4/path.to.mydir/genomes/c_elegans.PRJNA13758.WS245.genomic.fa --reads /scratch4/path.to.mydir/pbmm2/aln13448198.pbmm2.bam --examples /tmp/tmp1yvr59_z/make_examples.tfrecord@64.gz --add_hp_channel --alt_aligned_pileup diff_channels --max_reads_per_partition 600 --min_mapping_quality 1 --parse_sam_aux_fields --partition_size 25000 --phase_reads --pileup_image_width 199 --norealign_reads --sort_by_haplotypes --track_ref_reads --vsc_min_fraction_indels 0.12 --task 28. real	879m59.515s. user	632m52.969s. sys	6m20.594s. INFO: Cleaning up image... ```. I also ran more jobs using different numbers of cpu and mem using different bam files. One using 48 cpu and --mem-per-cpu=6G simply fizzled without any error message. These jobs are taking considerable core-hours, so troubleshooting is hard. I also wonder if I am using Deepvariant efficiently. On a side note, I got many Deepvariant failures with error messages like:. ```. Detected 1372 oom-kill event(s) in StepId=12049020.batch cgroup. Some of your processes may have been killed by the cgroup out-of-memory handler. ```. This seems to have been resolved by asking for maximum allowable memory. I am still curious about the memory requirement for successfully running Deepvariant.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/614
https://github.com/google/deepvariant/issues/614:12995,integrability,messag,messages,12995,"s) [6737.31s elapsed]. I0218 10:46:36.864049 23456243894080 make_examples_core.py:243] Task 19/64: Skip phasing: len(candidates[main_sample]) is 20526. I0218 10:48:19.364838 23456243894080 make_examples_core.py:243] Task 2/64: 158555 candidates (173735 examples) [4091.74s elapsed]. I0218 10:48:45.881830 23456243894080 make_examples_core.py:243] Task 2/64: Skip phasing: len(candidates[main_sample]) is 14234. I0218 10:49:31.045118 23456243894080 make_examples_core.py:243] Task 13/64: 113956 candidates (125182 examples) [6317.67s elapsed]. I0218 10:50:33.895329 23456243894080 make_examples_core.py:243] Task 13/64: Skip phasing: len(candidates[main_sample]) is 18414. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /scratch4/path.to.mydir/genomes/c_elegans.PRJNA13758.WS245.genomic.fa --reads /scratch4/path.to.mydir/pbmm2/aln13448198.pbmm2.bam --examples /tmp/tmp1yvr59_z/make_examples.tfrecord@64.gz --add_hp_channel --alt_aligned_pileup diff_channels --max_reads_per_partition 600 --min_mapping_quality 1 --parse_sam_aux_fields --partition_size 25000 --phase_reads --pileup_image_width 199 --norealign_reads --sort_by_haplotypes --track_ref_reads --vsc_min_fraction_indels 0.12 --task 28. real	879m59.515s. user	632m52.969s. sys	6m20.594s. INFO: Cleaning up image... ```. I also ran more jobs using different numbers of cpu and mem using different bam files. One using 48 cpu and --mem-per-cpu=6G simply fizzled without any error message. These jobs are taking considerable core-hours, so troubleshooting is hard. I also wonder if I am using Deepvariant efficiently. On a side note, I got many Deepvariant failures with error messages like:. ```. Detected 1372 oom-kill event(s) in StepId=12049020.batch cgroup. Some of your processes may have been killed by the cgroup out-of-memory handler. ```. This seems to have been resolved by asking for maximum allowable memory. I am still curious about the memory requirement for successfully running Deepvariant.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/614
https://github.com/google/deepvariant/issues/614:13039,integrability,event,event,13039,"s) [6737.31s elapsed]. I0218 10:46:36.864049 23456243894080 make_examples_core.py:243] Task 19/64: Skip phasing: len(candidates[main_sample]) is 20526. I0218 10:48:19.364838 23456243894080 make_examples_core.py:243] Task 2/64: 158555 candidates (173735 examples) [4091.74s elapsed]. I0218 10:48:45.881830 23456243894080 make_examples_core.py:243] Task 2/64: Skip phasing: len(candidates[main_sample]) is 14234. I0218 10:49:31.045118 23456243894080 make_examples_core.py:243] Task 13/64: 113956 candidates (125182 examples) [6317.67s elapsed]. I0218 10:50:33.895329 23456243894080 make_examples_core.py:243] Task 13/64: Skip phasing: len(candidates[main_sample]) is 18414. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /scratch4/path.to.mydir/genomes/c_elegans.PRJNA13758.WS245.genomic.fa --reads /scratch4/path.to.mydir/pbmm2/aln13448198.pbmm2.bam --examples /tmp/tmp1yvr59_z/make_examples.tfrecord@64.gz --add_hp_channel --alt_aligned_pileup diff_channels --max_reads_per_partition 600 --min_mapping_quality 1 --parse_sam_aux_fields --partition_size 25000 --phase_reads --pileup_image_width 199 --norealign_reads --sort_by_haplotypes --track_ref_reads --vsc_min_fraction_indels 0.12 --task 28. real	879m59.515s. user	632m52.969s. sys	6m20.594s. INFO: Cleaning up image... ```. I also ran more jobs using different numbers of cpu and mem using different bam files. One using 48 cpu and --mem-per-cpu=6G simply fizzled without any error message. These jobs are taking considerable core-hours, so troubleshooting is hard. I also wonder if I am using Deepvariant efficiently. On a side note, I got many Deepvariant failures with error messages like:. ```. Detected 1372 oom-kill event(s) in StepId=12049020.batch cgroup. Some of your processes may have been killed by the cgroup out-of-memory handler. ```. This seems to have been resolved by asking for maximum allowable memory. I am still curious about the memory requirement for successfully running Deepvariant.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/614
https://github.com/google/deepvariant/issues/614:13067,integrability,batch,batch,13067,"s) [6737.31s elapsed]. I0218 10:46:36.864049 23456243894080 make_examples_core.py:243] Task 19/64: Skip phasing: len(candidates[main_sample]) is 20526. I0218 10:48:19.364838 23456243894080 make_examples_core.py:243] Task 2/64: 158555 candidates (173735 examples) [4091.74s elapsed]. I0218 10:48:45.881830 23456243894080 make_examples_core.py:243] Task 2/64: Skip phasing: len(candidates[main_sample]) is 14234. I0218 10:49:31.045118 23456243894080 make_examples_core.py:243] Task 13/64: 113956 candidates (125182 examples) [6317.67s elapsed]. I0218 10:50:33.895329 23456243894080 make_examples_core.py:243] Task 13/64: Skip phasing: len(candidates[main_sample]) is 18414. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /scratch4/path.to.mydir/genomes/c_elegans.PRJNA13758.WS245.genomic.fa --reads /scratch4/path.to.mydir/pbmm2/aln13448198.pbmm2.bam --examples /tmp/tmp1yvr59_z/make_examples.tfrecord@64.gz --add_hp_channel --alt_aligned_pileup diff_channels --max_reads_per_partition 600 --min_mapping_quality 1 --parse_sam_aux_fields --partition_size 25000 --phase_reads --pileup_image_width 199 --norealign_reads --sort_by_haplotypes --track_ref_reads --vsc_min_fraction_indels 0.12 --task 28. real	879m59.515s. user	632m52.969s. sys	6m20.594s. INFO: Cleaning up image... ```. I also ran more jobs using different numbers of cpu and mem using different bam files. One using 48 cpu and --mem-per-cpu=6G simply fizzled without any error message. These jobs are taking considerable core-hours, so troubleshooting is hard. I also wonder if I am using Deepvariant efficiently. On a side note, I got many Deepvariant failures with error messages like:. ```. Detected 1372 oom-kill event(s) in StepId=12049020.batch cgroup. Some of your processes may have been killed by the cgroup out-of-memory handler. ```. This seems to have been resolved by asking for maximum allowable memory. I am still curious about the memory requirement for successfully running Deepvariant.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/614
https://github.com/google/deepvariant/issues/614:680,interoperability,bind,bind,680,"Deepvariant fails without clear reason - with PacBio data; Deepvariant fails without clear reason. . **Setup**. JHU Rockfish HPC. Singularity 3.8.7. singularity pull docker://google/deepvariant:1.4.0. Problematic data are PacBio (first gen). I have used Deepvariant with Illumina without problem, and I used PEPPER to process Ont data and PacBio Hifi data. I used pbmm2 to align fastq with all PacBio data. Command used to run:. ```. #!/bin/bash. #SBATCH --job-name=deep64_13448198. #SBATCH --time=24:00:00. #SBATCH --nodes=2. #SBATCH --ntasks-per-node=1. #SBATCH --cpus-per-task=32. #SBATCH --mem=0. ml anaconda. conda activate /data/path.to.mydir/deepvariant. singularity run --bind /scratch4/path.to.mydir/:/scratch4/path.to.mydir/ \. docker://google/deepvariant:1.4.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type PACBIO \. --ref c_elegans.PRJNA13758.WS245.genomic.fa \. --reads aln13448198.pbmm2.bam \. --output_vcf aln13448198.pbmm2.dv.vcf.gz \. --num_shards 64. ```. Here's a long snippet of slurm output:. ```. INFO: Using cached SIF image. INFO: Converting SIF file to temporary sandbox... I0217 20:13:14.117354 23456243894080 run_deepvariant.py:342] Re-using the directory for intermediate results in /tmp/tmp1yvr59_z. ***** Intermediate results will be written to /tmp/tmp1yvr59_z in docker. ****. ***** Running the command:*****. time seq 0 63 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref [snipped]. #[snip]. # this part is likely unimportant. perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. 	LANGUAGE = (unset),. 	LC_ALL = (unset),. 	LC_CTYPE = ""C.UTF-8"",. 	LANG = ""en_US.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. 	LANGUAGE = (unset),. 	LC_ALL = (unset),. 	LC_CTYPE = ""C.UTF-8"",. 	LANG = ""en_US.UTF-8"". are supported and ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/614
https://github.com/google/deepvariant/issues/614:1776,interoperability,standard,standard,1776,"deepvariant/bin/run_deepvariant \. --model_type PACBIO \. --ref c_elegans.PRJNA13758.WS245.genomic.fa \. --reads aln13448198.pbmm2.bam \. --output_vcf aln13448198.pbmm2.dv.vcf.gz \. --num_shards 64. ```. Here's a long snippet of slurm output:. ```. INFO: Using cached SIF image. INFO: Converting SIF file to temporary sandbox... I0217 20:13:14.117354 23456243894080 run_deepvariant.py:342] Re-using the directory for intermediate results in /tmp/tmp1yvr59_z. ***** Intermediate results will be written to /tmp/tmp1yvr59_z in docker. ****. ***** Running the command:*****. time seq 0 63 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref [snipped]. #[snip]. # this part is likely unimportant. perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. 	LANGUAGE = (unset),. 	LC_ALL = (unset),. 	LC_CTYPE = ""C.UTF-8"",. 	LANG = ""en_US.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. 	LANGUAGE = (unset),. 	LC_ALL = (unset),. 	LC_CTYPE = ""C.UTF-8"",. 	LANG = ""en_US.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). #[snip]. 2023-02-17 20:13:17.235641: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD	VN:1.6	SO:coordinate	pb:5.0.0. I0217 20:13:17.235805 23456243894080 genomics_reader.py:222] Reading /scratch4/path.to.mydir/pbmm2/aln13448198.pbmm2.bam with NativeSamReader. I0217 20:13:17.268698 23456243894080 make_examples_core.py:243] Task 18/64: Preparing inputs. 2023-02-17 20:13:17.371669: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD	VN:1.6	SO:coordinate	pb:5.0.0. I0217 20:13:17.371811 23456243894080 genomics_reader.py:222] Reading /scratch4/jwang8/hoyon/pbmm2/aln13448198.pbmm2.bam with NativeSamReader. I0217 20:1",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/614
https://github.com/google/deepvariant/issues/614:2061,interoperability,standard,standard,2061,"Converting SIF file to temporary sandbox... I0217 20:13:14.117354 23456243894080 run_deepvariant.py:342] Re-using the directory for intermediate results in /tmp/tmp1yvr59_z. ***** Intermediate results will be written to /tmp/tmp1yvr59_z in docker. ****. ***** Running the command:*****. time seq 0 63 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref [snipped]. #[snip]. # this part is likely unimportant. perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. 	LANGUAGE = (unset),. 	LC_ALL = (unset),. 	LC_CTYPE = ""C.UTF-8"",. 	LANG = ""en_US.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. 	LANGUAGE = (unset),. 	LC_ALL = (unset),. 	LC_CTYPE = ""C.UTF-8"",. 	LANG = ""en_US.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). #[snip]. 2023-02-17 20:13:17.235641: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD	VN:1.6	SO:coordinate	pb:5.0.0. I0217 20:13:17.235805 23456243894080 genomics_reader.py:222] Reading /scratch4/path.to.mydir/pbmm2/aln13448198.pbmm2.bam with NativeSamReader. I0217 20:13:17.268698 23456243894080 make_examples_core.py:243] Task 18/64: Preparing inputs. 2023-02-17 20:13:17.371669: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD	VN:1.6	SO:coordinate	pb:5.0.0. I0217 20:13:17.371811 23456243894080 genomics_reader.py:222] Reading /scratch4/jwang8/hoyon/pbmm2/aln13448198.pbmm2.bam with NativeSamReader. I0217 20:13:17.376646 23456243894080 make_examples_core.py:243] Task 18/64: Common contigs are ['I', 'II', 'III', 'IV', 'V', 'X', 'MtDNA']. I0217 20:13:17.383403 23456243894080 make_examples_core.py:243] Task 18/64: Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using C",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/614
https://github.com/google/deepvariant/issues/614:2221,interoperability,coordinat,coordinate,2221,"tmp1yvr59_z. ***** Intermediate results will be written to /tmp/tmp1yvr59_z in docker. ****. ***** Running the command:*****. time seq 0 63 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref [snipped]. #[snip]. # this part is likely unimportant. perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. 	LANGUAGE = (unset),. 	LC_ALL = (unset),. 	LC_CTYPE = ""C.UTF-8"",. 	LANG = ""en_US.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. 	LANGUAGE = (unset),. 	LC_ALL = (unset),. 	LC_CTYPE = ""C.UTF-8"",. 	LANG = ""en_US.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). #[snip]. 2023-02-17 20:13:17.235641: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD	VN:1.6	SO:coordinate	pb:5.0.0. I0217 20:13:17.235805 23456243894080 genomics_reader.py:222] Reading /scratch4/path.to.mydir/pbmm2/aln13448198.pbmm2.bam with NativeSamReader. I0217 20:13:17.268698 23456243894080 make_examples_core.py:243] Task 18/64: Preparing inputs. 2023-02-17 20:13:17.371669: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD	VN:1.6	SO:coordinate	pb:5.0.0. I0217 20:13:17.371811 23456243894080 genomics_reader.py:222] Reading /scratch4/jwang8/hoyon/pbmm2/aln13448198.pbmm2.bam with NativeSamReader. I0217 20:13:17.376646 23456243894080 make_examples_core.py:243] Task 18/64: Common contigs are ['I', 'II', 'III', 'IV', 'V', 'X', 'MtDNA']. I0217 20:13:17.383403 23456243894080 make_examples_core.py:243] Task 18/64: Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. 2023-02-17 20:13:17.383631: I third_party/nucleus/io/sam_reader.cc:736] Se",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/614
https://github.com/google/deepvariant/issues/614:2607,interoperability,coordinat,coordinate,2607,"GUAGE = (unset),. 	LC_ALL = (unset),. 	LC_CTYPE = ""C.UTF-8"",. 	LANG = ""en_US.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. 	LANGUAGE = (unset),. 	LC_ALL = (unset),. 	LC_CTYPE = ""C.UTF-8"",. 	LANG = ""en_US.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). #[snip]. 2023-02-17 20:13:17.235641: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD	VN:1.6	SO:coordinate	pb:5.0.0. I0217 20:13:17.235805 23456243894080 genomics_reader.py:222] Reading /scratch4/path.to.mydir/pbmm2/aln13448198.pbmm2.bam with NativeSamReader. I0217 20:13:17.268698 23456243894080 make_examples_core.py:243] Task 18/64: Preparing inputs. 2023-02-17 20:13:17.371669: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD	VN:1.6	SO:coordinate	pb:5.0.0. I0217 20:13:17.371811 23456243894080 genomics_reader.py:222] Reading /scratch4/jwang8/hoyon/pbmm2/aln13448198.pbmm2.bam with NativeSamReader. I0217 20:13:17.376646 23456243894080 make_examples_core.py:243] Task 18/64: Common contigs are ['I', 'II', 'III', 'IV', 'V', 'X', 'MtDNA']. I0217 20:13:17.383403 23456243894080 make_examples_core.py:243] Task 18/64: Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. 2023-02-17 20:13:17.383631: I third_party/nucleus/io/sam_reader.cc:736] Setting HTS_OPT_BLOCK_SIZE to 134217728. 2023-02-17 20:13:17.235347: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD	VN:1.6	SO:coordinate	pb:5.0.0. #[snip]. I0217 20:13:17.702247 23456243894080 genomics_reader.py:222] Reading /scratch4/jwang8/hoyon/pbmm2/aln13448198.pbmm2.bam with NativeSamReader. 2023-02-17 20:13:17.812898: W third_party/nucle",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/614
https://github.com/google/deepvariant/issues/614:3393,interoperability,coordinat,coordinate,3393,"7.268698 23456243894080 make_examples_core.py:243] Task 18/64: Preparing inputs. 2023-02-17 20:13:17.371669: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD	VN:1.6	SO:coordinate	pb:5.0.0. I0217 20:13:17.371811 23456243894080 genomics_reader.py:222] Reading /scratch4/jwang8/hoyon/pbmm2/aln13448198.pbmm2.bam with NativeSamReader. I0217 20:13:17.376646 23456243894080 make_examples_core.py:243] Task 18/64: Common contigs are ['I', 'II', 'III', 'IV', 'V', 'X', 'MtDNA']. I0217 20:13:17.383403 23456243894080 make_examples_core.py:243] Task 18/64: Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. 2023-02-17 20:13:17.383631: I third_party/nucleus/io/sam_reader.cc:736] Setting HTS_OPT_BLOCK_SIZE to 134217728. 2023-02-17 20:13:17.235347: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD	VN:1.6	SO:coordinate	pb:5.0.0. #[snip]. I0217 20:13:17.702247 23456243894080 genomics_reader.py:222] Reading /scratch4/jwang8/hoyon/pbmm2/aln13448198.pbmm2.bam with NativeSamReader. 2023-02-17 20:13:17.812898: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD	VN:1.6	SO:coordinate	pb:5.0.0. I0217 20:13:17.813058 23456243894080 genomics_reader.py:222] Reading /scratch4/path.to.mydir/pbmm2/aln13448198.pbmm2.bam with NativeSamReader. I0217 20:13:17.814152 23456243894080 make_examples_core.py:243] Task 11/64: Writing examples to /tmp/tmp1yvr59_z/make_examples.tfrecord-00011-of-00064.gz. I0217 20:13:17.814266 23456243894080 make_examples_core.py:243] Task 11/64: Overhead for preparing inputs: 0 seconds. 2023-02-17 20:13:17.834667: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD	VN:1.6	SO:coordinate	pb:5.0.0. I0217 20:13:17.834807 23456243894080 genomics_reader.py:222] Reading /scratch4/path.to.mydir/pbmm2/aln13448198.pbmm2.ba",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/614
https://github.com/google/deepvariant/issues/614:3693,interoperability,coordinat,coordinate,3693,"scratch4/jwang8/hoyon/pbmm2/aln13448198.pbmm2.bam with NativeSamReader. I0217 20:13:17.376646 23456243894080 make_examples_core.py:243] Task 18/64: Common contigs are ['I', 'II', 'III', 'IV', 'V', 'X', 'MtDNA']. I0217 20:13:17.383403 23456243894080 make_examples_core.py:243] Task 18/64: Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. 2023-02-17 20:13:17.383631: I third_party/nucleus/io/sam_reader.cc:736] Setting HTS_OPT_BLOCK_SIZE to 134217728. 2023-02-17 20:13:17.235347: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD	VN:1.6	SO:coordinate	pb:5.0.0. #[snip]. I0217 20:13:17.702247 23456243894080 genomics_reader.py:222] Reading /scratch4/jwang8/hoyon/pbmm2/aln13448198.pbmm2.bam with NativeSamReader. 2023-02-17 20:13:17.812898: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD	VN:1.6	SO:coordinate	pb:5.0.0. I0217 20:13:17.813058 23456243894080 genomics_reader.py:222] Reading /scratch4/path.to.mydir/pbmm2/aln13448198.pbmm2.bam with NativeSamReader. I0217 20:13:17.814152 23456243894080 make_examples_core.py:243] Task 11/64: Writing examples to /tmp/tmp1yvr59_z/make_examples.tfrecord-00011-of-00064.gz. I0217 20:13:17.814266 23456243894080 make_examples_core.py:243] Task 11/64: Overhead for preparing inputs: 0 seconds. 2023-02-17 20:13:17.834667: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD	VN:1.6	SO:coordinate	pb:5.0.0. I0217 20:13:17.834807 23456243894080 genomics_reader.py:222] Reading /scratch4/path.to.mydir/pbmm2/aln13448198.pbmm2.bam with NativeSamReader. I0217 20:13:17.836419 23456243894080 make_examples_core.py:243] Task 18/64: Writing examples to /tmp/tmp1yvr59_z/make_examples.tfrecord-00018-of-00064.gz. I0217 20:13:17.836520 23456243894080 make_examples_core.py:243] Task 18/64: Overhead for preparing inputs: 0 seconds. 202",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/614
https://github.com/google/deepvariant/issues/614:4258,interoperability,coordinat,coordinate,4258,". 2023-02-17 20:13:17.235347: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD	VN:1.6	SO:coordinate	pb:5.0.0. #[snip]. I0217 20:13:17.702247 23456243894080 genomics_reader.py:222] Reading /scratch4/jwang8/hoyon/pbmm2/aln13448198.pbmm2.bam with NativeSamReader. 2023-02-17 20:13:17.812898: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD	VN:1.6	SO:coordinate	pb:5.0.0. I0217 20:13:17.813058 23456243894080 genomics_reader.py:222] Reading /scratch4/path.to.mydir/pbmm2/aln13448198.pbmm2.bam with NativeSamReader. I0217 20:13:17.814152 23456243894080 make_examples_core.py:243] Task 11/64: Writing examples to /tmp/tmp1yvr59_z/make_examples.tfrecord-00011-of-00064.gz. I0217 20:13:17.814266 23456243894080 make_examples_core.py:243] Task 11/64: Overhead for preparing inputs: 0 seconds. 2023-02-17 20:13:17.834667: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD	VN:1.6	SO:coordinate	pb:5.0.0. I0217 20:13:17.834807 23456243894080 genomics_reader.py:222] Reading /scratch4/path.to.mydir/pbmm2/aln13448198.pbmm2.bam with NativeSamReader. I0217 20:13:17.836419 23456243894080 make_examples_core.py:243] Task 18/64: Writing examples to /tmp/tmp1yvr59_z/make_examples.tfrecord-00018-of-00064.gz. I0217 20:13:17.836520 23456243894080 make_examples_core.py:243] Task 18/64: Overhead for preparing inputs: 0 seconds. 2023-02-17 20:13:17.850120: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD	VN:1.6	SO:coordinate	pb:5.0.0. I0217 20:13:17.850307 23456243894080 genomics_reader.py:222] Reading /scratch4/path.to.mydir/pbmm2/aln13448198.pbmm2.bam with NativeSamReader. I0217 20:13:17.851420 23456243894080 make_examples_core.py:243] Task 17/64: Writing examples to /tmp/tmp1yvr59_z/make_examples.tfrecord-00017-of-00064.gz. I0217 20:13:17.851542 23456243894080 make_examples_core.py:243] Task 17/64: Overhead for preparing inputs: 0 seconds. 202",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/614
https://github.com/google/deepvariant/issues/614:4823,interoperability,coordinat,coordinate,4823,"m2.bam with NativeSamReader. I0217 20:13:17.814152 23456243894080 make_examples_core.py:243] Task 11/64: Writing examples to /tmp/tmp1yvr59_z/make_examples.tfrecord-00011-of-00064.gz. I0217 20:13:17.814266 23456243894080 make_examples_core.py:243] Task 11/64: Overhead for preparing inputs: 0 seconds. 2023-02-17 20:13:17.834667: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD	VN:1.6	SO:coordinate	pb:5.0.0. I0217 20:13:17.834807 23456243894080 genomics_reader.py:222] Reading /scratch4/path.to.mydir/pbmm2/aln13448198.pbmm2.bam with NativeSamReader. I0217 20:13:17.836419 23456243894080 make_examples_core.py:243] Task 18/64: Writing examples to /tmp/tmp1yvr59_z/make_examples.tfrecord-00018-of-00064.gz. I0217 20:13:17.836520 23456243894080 make_examples_core.py:243] Task 18/64: Overhead for preparing inputs: 0 seconds. 2023-02-17 20:13:17.850120: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD	VN:1.6	SO:coordinate	pb:5.0.0. I0217 20:13:17.850307 23456243894080 genomics_reader.py:222] Reading /scratch4/path.to.mydir/pbmm2/aln13448198.pbmm2.bam with NativeSamReader. I0217 20:13:17.851420 23456243894080 make_examples_core.py:243] Task 17/64: Writing examples to /tmp/tmp1yvr59_z/make_examples.tfrecord-00017-of-00064.gz. I0217 20:13:17.851542 23456243894080 make_examples_core.py:243] Task 17/64: Overhead for preparing inputs: 0 seconds. 2023-02-17 20:13:17.829944: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD	VN:1.6	SO:coordinate	pb:5.0.0. I0217 20:13:17.830086 23456243894080 genomics_reader.py:222] Reading /scratch4/path.to.mydir/pbmm2/aln13448198.pbmm2.bam with NativeSamReader. I0217 20:13:17.831279 23456243894080 make_examples_core.py:243] Task 7/64: Writing examples to /tmp/tmp1yvr59_z/make_examples.tfrecord-00007-of-00064.gz. I0217 20:13:17.831381 23456243894080 make_examples_core.py:243] Task 7/64: Overhead for preparing inputs: 0 seconds. #[sni",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/614
https://github.com/google/deepvariant/issues/614:5388,interoperability,coordinat,coordinate,5388,"m2.bam with NativeSamReader. I0217 20:13:17.836419 23456243894080 make_examples_core.py:243] Task 18/64: Writing examples to /tmp/tmp1yvr59_z/make_examples.tfrecord-00018-of-00064.gz. I0217 20:13:17.836520 23456243894080 make_examples_core.py:243] Task 18/64: Overhead for preparing inputs: 0 seconds. 2023-02-17 20:13:17.850120: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD	VN:1.6	SO:coordinate	pb:5.0.0. I0217 20:13:17.850307 23456243894080 genomics_reader.py:222] Reading /scratch4/path.to.mydir/pbmm2/aln13448198.pbmm2.bam with NativeSamReader. I0217 20:13:17.851420 23456243894080 make_examples_core.py:243] Task 17/64: Writing examples to /tmp/tmp1yvr59_z/make_examples.tfrecord-00017-of-00064.gz. I0217 20:13:17.851542 23456243894080 make_examples_core.py:243] Task 17/64: Overhead for preparing inputs: 0 seconds. 2023-02-17 20:13:17.829944: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD	VN:1.6	SO:coordinate	pb:5.0.0. I0217 20:13:17.830086 23456243894080 genomics_reader.py:222] Reading /scratch4/path.to.mydir/pbmm2/aln13448198.pbmm2.bam with NativeSamReader. I0217 20:13:17.831279 23456243894080 make_examples_core.py:243] Task 7/64: Writing examples to /tmp/tmp1yvr59_z/make_examples.tfrecord-00007-of-00064.gz. I0217 20:13:17.831381 23456243894080 make_examples_core.py:243] Task 7/64: Overhead for preparing inputs: 0 seconds. #[snip]. I0217 20:13:55.184195 23456243894080 make_examples_core.py:243] Task 8/64: Skip phasing: len(candidates[main_sample]) is 19455. I0217 20:13:55.731674 23456243894080 make_examples_core.py:243] Task 22/64: Skip phasing: len(candidates[main_sample]) is 18977. I0217 20:13:55.541318 23456243894080 make_examples_core.py:243] Task 1/64: Skip phasing: len(candidates[main_sample]) is 19291. I0217 20:13:54.948142 23456243894080 make_examples_core.py:243] Task 19/64: Skip phasing: len(candidates[main_sample]) is 19012. I0217 20:13:55.139624 23456243894080 make_examp",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/614
https://github.com/google/deepvariant/issues/614:12799,interoperability,messag,message,12799,"s) [6737.31s elapsed]. I0218 10:46:36.864049 23456243894080 make_examples_core.py:243] Task 19/64: Skip phasing: len(candidates[main_sample]) is 20526. I0218 10:48:19.364838 23456243894080 make_examples_core.py:243] Task 2/64: 158555 candidates (173735 examples) [4091.74s elapsed]. I0218 10:48:45.881830 23456243894080 make_examples_core.py:243] Task 2/64: Skip phasing: len(candidates[main_sample]) is 14234. I0218 10:49:31.045118 23456243894080 make_examples_core.py:243] Task 13/64: 113956 candidates (125182 examples) [6317.67s elapsed]. I0218 10:50:33.895329 23456243894080 make_examples_core.py:243] Task 13/64: Skip phasing: len(candidates[main_sample]) is 18414. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /scratch4/path.to.mydir/genomes/c_elegans.PRJNA13758.WS245.genomic.fa --reads /scratch4/path.to.mydir/pbmm2/aln13448198.pbmm2.bam --examples /tmp/tmp1yvr59_z/make_examples.tfrecord@64.gz --add_hp_channel --alt_aligned_pileup diff_channels --max_reads_per_partition 600 --min_mapping_quality 1 --parse_sam_aux_fields --partition_size 25000 --phase_reads --pileup_image_width 199 --norealign_reads --sort_by_haplotypes --track_ref_reads --vsc_min_fraction_indels 0.12 --task 28. real	879m59.515s. user	632m52.969s. sys	6m20.594s. INFO: Cleaning up image... ```. I also ran more jobs using different numbers of cpu and mem using different bam files. One using 48 cpu and --mem-per-cpu=6G simply fizzled without any error message. These jobs are taking considerable core-hours, so troubleshooting is hard. I also wonder if I am using Deepvariant efficiently. On a side note, I got many Deepvariant failures with error messages like:. ```. Detected 1372 oom-kill event(s) in StepId=12049020.batch cgroup. Some of your processes may have been killed by the cgroup out-of-memory handler. ```. This seems to have been resolved by asking for maximum allowable memory. I am still curious about the memory requirement for successfully running Deepvariant.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/614
https://github.com/google/deepvariant/issues/614:12995,interoperability,messag,messages,12995,"s) [6737.31s elapsed]. I0218 10:46:36.864049 23456243894080 make_examples_core.py:243] Task 19/64: Skip phasing: len(candidates[main_sample]) is 20526. I0218 10:48:19.364838 23456243894080 make_examples_core.py:243] Task 2/64: 158555 candidates (173735 examples) [4091.74s elapsed]. I0218 10:48:45.881830 23456243894080 make_examples_core.py:243] Task 2/64: Skip phasing: len(candidates[main_sample]) is 14234. I0218 10:49:31.045118 23456243894080 make_examples_core.py:243] Task 13/64: 113956 candidates (125182 examples) [6317.67s elapsed]. I0218 10:50:33.895329 23456243894080 make_examples_core.py:243] Task 13/64: Skip phasing: len(candidates[main_sample]) is 18414. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /scratch4/path.to.mydir/genomes/c_elegans.PRJNA13758.WS245.genomic.fa --reads /scratch4/path.to.mydir/pbmm2/aln13448198.pbmm2.bam --examples /tmp/tmp1yvr59_z/make_examples.tfrecord@64.gz --add_hp_channel --alt_aligned_pileup diff_channels --max_reads_per_partition 600 --min_mapping_quality 1 --parse_sam_aux_fields --partition_size 25000 --phase_reads --pileup_image_width 199 --norealign_reads --sort_by_haplotypes --track_ref_reads --vsc_min_fraction_indels 0.12 --task 28. real	879m59.515s. user	632m52.969s. sys	6m20.594s. INFO: Cleaning up image... ```. I also ran more jobs using different numbers of cpu and mem using different bam files. One using 48 cpu and --mem-per-cpu=6G simply fizzled without any error message. These jobs are taking considerable core-hours, so troubleshooting is hard. I also wonder if I am using Deepvariant efficiently. On a side note, I got many Deepvariant failures with error messages like:. ```. Detected 1372 oom-kill event(s) in StepId=12049020.batch cgroup. Some of your processes may have been killed by the cgroup out-of-memory handler. ```. This seems to have been resolved by asking for maximum allowable memory. I am still curious about the memory requirement for successfully running Deepvariant.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/614
https://github.com/google/deepvariant/issues/614:46,modifiability,Pac,PacBio,46,"Deepvariant fails without clear reason - with PacBio data; Deepvariant fails without clear reason. . **Setup**. JHU Rockfish HPC. Singularity 3.8.7. singularity pull docker://google/deepvariant:1.4.0. Problematic data are PacBio (first gen). I have used Deepvariant with Illumina without problem, and I used PEPPER to process Ont data and PacBio Hifi data. I used pbmm2 to align fastq with all PacBio data. Command used to run:. ```. #!/bin/bash. #SBATCH --job-name=deep64_13448198. #SBATCH --time=24:00:00. #SBATCH --nodes=2. #SBATCH --ntasks-per-node=1. #SBATCH --cpus-per-task=32. #SBATCH --mem=0. ml anaconda. conda activate /data/path.to.mydir/deepvariant. singularity run --bind /scratch4/path.to.mydir/:/scratch4/path.to.mydir/ \. docker://google/deepvariant:1.4.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type PACBIO \. --ref c_elegans.PRJNA13758.WS245.genomic.fa \. --reads aln13448198.pbmm2.bam \. --output_vcf aln13448198.pbmm2.dv.vcf.gz \. --num_shards 64. ```. Here's a long snippet of slurm output:. ```. INFO: Using cached SIF image. INFO: Converting SIF file to temporary sandbox... I0217 20:13:14.117354 23456243894080 run_deepvariant.py:342] Re-using the directory for intermediate results in /tmp/tmp1yvr59_z. ***** Intermediate results will be written to /tmp/tmp1yvr59_z in docker. ****. ***** Running the command:*****. time seq 0 63 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref [snipped]. #[snip]. # this part is likely unimportant. perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. 	LANGUAGE = (unset),. 	LC_ALL = (unset),. 	LC_CTYPE = ""C.UTF-8"",. 	LANG = ""en_US.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. 	LANGUAGE = (unset),. 	LC_ALL = (unset),. 	LC_CTYPE = ""C.UTF-8"",. 	LANG = ""en_US.UTF-8"". are supported and ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/614
https://github.com/google/deepvariant/issues/614:222,modifiability,Pac,PacBio,222,"Deepvariant fails without clear reason - with PacBio data; Deepvariant fails without clear reason. . **Setup**. JHU Rockfish HPC. Singularity 3.8.7. singularity pull docker://google/deepvariant:1.4.0. Problematic data are PacBio (first gen). I have used Deepvariant with Illumina without problem, and I used PEPPER to process Ont data and PacBio Hifi data. I used pbmm2 to align fastq with all PacBio data. Command used to run:. ```. #!/bin/bash. #SBATCH --job-name=deep64_13448198. #SBATCH --time=24:00:00. #SBATCH --nodes=2. #SBATCH --ntasks-per-node=1. #SBATCH --cpus-per-task=32. #SBATCH --mem=0. ml anaconda. conda activate /data/path.to.mydir/deepvariant. singularity run --bind /scratch4/path.to.mydir/:/scratch4/path.to.mydir/ \. docker://google/deepvariant:1.4.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type PACBIO \. --ref c_elegans.PRJNA13758.WS245.genomic.fa \. --reads aln13448198.pbmm2.bam \. --output_vcf aln13448198.pbmm2.dv.vcf.gz \. --num_shards 64. ```. Here's a long snippet of slurm output:. ```. INFO: Using cached SIF image. INFO: Converting SIF file to temporary sandbox... I0217 20:13:14.117354 23456243894080 run_deepvariant.py:342] Re-using the directory for intermediate results in /tmp/tmp1yvr59_z. ***** Intermediate results will be written to /tmp/tmp1yvr59_z in docker. ****. ***** Running the command:*****. time seq 0 63 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref [snipped]. #[snip]. # this part is likely unimportant. perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. 	LANGUAGE = (unset),. 	LC_ALL = (unset),. 	LC_CTYPE = ""C.UTF-8"",. 	LANG = ""en_US.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. 	LANGUAGE = (unset),. 	LC_ALL = (unset),. 	LC_CTYPE = ""C.UTF-8"",. 	LANG = ""en_US.UTF-8"". are supported and ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/614
https://github.com/google/deepvariant/issues/614:339,modifiability,Pac,PacBio,339,"Deepvariant fails without clear reason - with PacBio data; Deepvariant fails without clear reason. . **Setup**. JHU Rockfish HPC. Singularity 3.8.7. singularity pull docker://google/deepvariant:1.4.0. Problematic data are PacBio (first gen). I have used Deepvariant with Illumina without problem, and I used PEPPER to process Ont data and PacBio Hifi data. I used pbmm2 to align fastq with all PacBio data. Command used to run:. ```. #!/bin/bash. #SBATCH --job-name=deep64_13448198. #SBATCH --time=24:00:00. #SBATCH --nodes=2. #SBATCH --ntasks-per-node=1. #SBATCH --cpus-per-task=32. #SBATCH --mem=0. ml anaconda. conda activate /data/path.to.mydir/deepvariant. singularity run --bind /scratch4/path.to.mydir/:/scratch4/path.to.mydir/ \. docker://google/deepvariant:1.4.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type PACBIO \. --ref c_elegans.PRJNA13758.WS245.genomic.fa \. --reads aln13448198.pbmm2.bam \. --output_vcf aln13448198.pbmm2.dv.vcf.gz \. --num_shards 64. ```. Here's a long snippet of slurm output:. ```. INFO: Using cached SIF image. INFO: Converting SIF file to temporary sandbox... I0217 20:13:14.117354 23456243894080 run_deepvariant.py:342] Re-using the directory for intermediate results in /tmp/tmp1yvr59_z. ***** Intermediate results will be written to /tmp/tmp1yvr59_z in docker. ****. ***** Running the command:*****. time seq 0 63 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref [snipped]. #[snip]. # this part is likely unimportant. perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. 	LANGUAGE = (unset),. 	LC_ALL = (unset),. 	LC_CTYPE = ""C.UTF-8"",. 	LANG = ""en_US.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. 	LANGUAGE = (unset),. 	LC_ALL = (unset),. 	LC_CTYPE = ""C.UTF-8"",. 	LANG = ""en_US.UTF-8"". are supported and ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/614
https://github.com/google/deepvariant/issues/614:394,modifiability,Pac,PacBio,394,"Deepvariant fails without clear reason - with PacBio data; Deepvariant fails without clear reason. . **Setup**. JHU Rockfish HPC. Singularity 3.8.7. singularity pull docker://google/deepvariant:1.4.0. Problematic data are PacBio (first gen). I have used Deepvariant with Illumina without problem, and I used PEPPER to process Ont data and PacBio Hifi data. I used pbmm2 to align fastq with all PacBio data. Command used to run:. ```. #!/bin/bash. #SBATCH --job-name=deep64_13448198. #SBATCH --time=24:00:00. #SBATCH --nodes=2. #SBATCH --ntasks-per-node=1. #SBATCH --cpus-per-task=32. #SBATCH --mem=0. ml anaconda. conda activate /data/path.to.mydir/deepvariant. singularity run --bind /scratch4/path.to.mydir/:/scratch4/path.to.mydir/ \. docker://google/deepvariant:1.4.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type PACBIO \. --ref c_elegans.PRJNA13758.WS245.genomic.fa \. --reads aln13448198.pbmm2.bam \. --output_vcf aln13448198.pbmm2.dv.vcf.gz \. --num_shards 64. ```. Here's a long snippet of slurm output:. ```. INFO: Using cached SIF image. INFO: Converting SIF file to temporary sandbox... I0217 20:13:14.117354 23456243894080 run_deepvariant.py:342] Re-using the directory for intermediate results in /tmp/tmp1yvr59_z. ***** Intermediate results will be written to /tmp/tmp1yvr59_z in docker. ****. ***** Running the command:*****. time seq 0 63 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref [snipped]. #[snip]. # this part is likely unimportant. perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. 	LANGUAGE = (unset),. 	LC_ALL = (unset),. 	LC_CTYPE = ""C.UTF-8"",. 	LANG = ""en_US.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. 	LANGUAGE = (unset),. 	LC_ALL = (unset),. 	LC_CTYPE = ""C.UTF-8"",. 	LANG = ""en_US.UTF-8"". are supported and ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/614
https://github.com/google/deepvariant/issues/614:680,modifiability,bind,bind,680,"Deepvariant fails without clear reason - with PacBio data; Deepvariant fails without clear reason. . **Setup**. JHU Rockfish HPC. Singularity 3.8.7. singularity pull docker://google/deepvariant:1.4.0. Problematic data are PacBio (first gen). I have used Deepvariant with Illumina without problem, and I used PEPPER to process Ont data and PacBio Hifi data. I used pbmm2 to align fastq with all PacBio data. Command used to run:. ```. #!/bin/bash. #SBATCH --job-name=deep64_13448198. #SBATCH --time=24:00:00. #SBATCH --nodes=2. #SBATCH --ntasks-per-node=1. #SBATCH --cpus-per-task=32. #SBATCH --mem=0. ml anaconda. conda activate /data/path.to.mydir/deepvariant. singularity run --bind /scratch4/path.to.mydir/:/scratch4/path.to.mydir/ \. docker://google/deepvariant:1.4.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type PACBIO \. --ref c_elegans.PRJNA13758.WS245.genomic.fa \. --reads aln13448198.pbmm2.bam \. --output_vcf aln13448198.pbmm2.dv.vcf.gz \. --num_shards 64. ```. Here's a long snippet of slurm output:. ```. INFO: Using cached SIF image. INFO: Converting SIF file to temporary sandbox... I0217 20:13:14.117354 23456243894080 run_deepvariant.py:342] Re-using the directory for intermediate results in /tmp/tmp1yvr59_z. ***** Intermediate results will be written to /tmp/tmp1yvr59_z in docker. ****. ***** Running the command:*****. time seq 0 63 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref [snipped]. #[snip]. # this part is likely unimportant. perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. 	LANGUAGE = (unset),. 	LC_ALL = (unset),. 	LC_CTYPE = ""C.UTF-8"",. 	LANG = ""en_US.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. 	LANGUAGE = (unset),. 	LC_ALL = (unset),. 	LC_CTYPE = ""C.UTF-8"",. 	LANG = ""en_US.UTF-8"". are supported and ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/614
https://github.com/google/deepvariant/issues/614:828,modifiability,PAC,PACBIO,828,"Deepvariant fails without clear reason - with PacBio data; Deepvariant fails without clear reason. . **Setup**. JHU Rockfish HPC. Singularity 3.8.7. singularity pull docker://google/deepvariant:1.4.0. Problematic data are PacBio (first gen). I have used Deepvariant with Illumina without problem, and I used PEPPER to process Ont data and PacBio Hifi data. I used pbmm2 to align fastq with all PacBio data. Command used to run:. ```. #!/bin/bash. #SBATCH --job-name=deep64_13448198. #SBATCH --time=24:00:00. #SBATCH --nodes=2. #SBATCH --ntasks-per-node=1. #SBATCH --cpus-per-task=32. #SBATCH --mem=0. ml anaconda. conda activate /data/path.to.mydir/deepvariant. singularity run --bind /scratch4/path.to.mydir/:/scratch4/path.to.mydir/ \. docker://google/deepvariant:1.4.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type PACBIO \. --ref c_elegans.PRJNA13758.WS245.genomic.fa \. --reads aln13448198.pbmm2.bam \. --output_vcf aln13448198.pbmm2.dv.vcf.gz \. --num_shards 64. ```. Here's a long snippet of slurm output:. ```. INFO: Using cached SIF image. INFO: Converting SIF file to temporary sandbox... I0217 20:13:14.117354 23456243894080 run_deepvariant.py:342] Re-using the directory for intermediate results in /tmp/tmp1yvr59_z. ***** Intermediate results will be written to /tmp/tmp1yvr59_z in docker. ****. ***** Running the command:*****. time seq 0 63 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref [snipped]. #[snip]. # this part is likely unimportant. perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. 	LANGUAGE = (unset),. 	LC_ALL = (unset),. 	LC_CTYPE = ""C.UTF-8"",. 	LANG = ""en_US.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. 	LANGUAGE = (unset),. 	LC_ALL = (unset),. 	LC_CTYPE = ""C.UTF-8"",. 	LANG = ""en_US.UTF-8"". are supported and ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/614
https://github.com/google/deepvariant/issues/614:1197,modifiability,interm,intermediate,1197,"oblematic data are PacBio (first gen). I have used Deepvariant with Illumina without problem, and I used PEPPER to process Ont data and PacBio Hifi data. I used pbmm2 to align fastq with all PacBio data. Command used to run:. ```. #!/bin/bash. #SBATCH --job-name=deep64_13448198. #SBATCH --time=24:00:00. #SBATCH --nodes=2. #SBATCH --ntasks-per-node=1. #SBATCH --cpus-per-task=32. #SBATCH --mem=0. ml anaconda. conda activate /data/path.to.mydir/deepvariant. singularity run --bind /scratch4/path.to.mydir/:/scratch4/path.to.mydir/ \. docker://google/deepvariant:1.4.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type PACBIO \. --ref c_elegans.PRJNA13758.WS245.genomic.fa \. --reads aln13448198.pbmm2.bam \. --output_vcf aln13448198.pbmm2.dv.vcf.gz \. --num_shards 64. ```. Here's a long snippet of slurm output:. ```. INFO: Using cached SIF image. INFO: Converting SIF file to temporary sandbox... I0217 20:13:14.117354 23456243894080 run_deepvariant.py:342] Re-using the directory for intermediate results in /tmp/tmp1yvr59_z. ***** Intermediate results will be written to /tmp/tmp1yvr59_z in docker. ****. ***** Running the command:*****. time seq 0 63 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref [snipped]. #[snip]. # this part is likely unimportant. perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. 	LANGUAGE = (unset),. 	LC_ALL = (unset),. 	LC_CTYPE = ""C.UTF-8"",. 	LANG = ""en_US.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. 	LANGUAGE = (unset),. 	LC_ALL = (unset),. 	LC_CTYPE = ""C.UTF-8"",. 	LANG = ""en_US.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). #[snip]. 2023-02-17 20:13:17.235641: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignori",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/614
https://github.com/google/deepvariant/issues/614:1245,modifiability,Interm,Intermediate,1245,"ed Deepvariant with Illumina without problem, and I used PEPPER to process Ont data and PacBio Hifi data. I used pbmm2 to align fastq with all PacBio data. Command used to run:. ```. #!/bin/bash. #SBATCH --job-name=deep64_13448198. #SBATCH --time=24:00:00. #SBATCH --nodes=2. #SBATCH --ntasks-per-node=1. #SBATCH --cpus-per-task=32. #SBATCH --mem=0. ml anaconda. conda activate /data/path.to.mydir/deepvariant. singularity run --bind /scratch4/path.to.mydir/:/scratch4/path.to.mydir/ \. docker://google/deepvariant:1.4.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type PACBIO \. --ref c_elegans.PRJNA13758.WS245.genomic.fa \. --reads aln13448198.pbmm2.bam \. --output_vcf aln13448198.pbmm2.dv.vcf.gz \. --num_shards 64. ```. Here's a long snippet of slurm output:. ```. INFO: Using cached SIF image. INFO: Converting SIF file to temporary sandbox... I0217 20:13:14.117354 23456243894080 run_deepvariant.py:342] Re-using the directory for intermediate results in /tmp/tmp1yvr59_z. ***** Intermediate results will be written to /tmp/tmp1yvr59_z in docker. ****. ***** Running the command:*****. time seq 0 63 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref [snipped]. #[snip]. # this part is likely unimportant. perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. 	LANGUAGE = (unset),. 	LC_ALL = (unset),. 	LC_CTYPE = ""C.UTF-8"",. 	LANG = ""en_US.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. 	LANGUAGE = (unset),. 	LC_ALL = (unset),. 	LC_CTYPE = ""C.UTF-8"",. 	LANG = ""en_US.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). #[snip]. 2023-02-17 20:13:17.235641: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD	VN:1.6	SO:coordinate	pb:5.0.0. I0217 20:",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/614
https://github.com/google/deepvariant/issues/614:3094,modifiability,deco,decode,3094,"-02-17 20:13:17.235641: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD	VN:1.6	SO:coordinate	pb:5.0.0. I0217 20:13:17.235805 23456243894080 genomics_reader.py:222] Reading /scratch4/path.to.mydir/pbmm2/aln13448198.pbmm2.bam with NativeSamReader. I0217 20:13:17.268698 23456243894080 make_examples_core.py:243] Task 18/64: Preparing inputs. 2023-02-17 20:13:17.371669: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD	VN:1.6	SO:coordinate	pb:5.0.0. I0217 20:13:17.371811 23456243894080 genomics_reader.py:222] Reading /scratch4/jwang8/hoyon/pbmm2/aln13448198.pbmm2.bam with NativeSamReader. I0217 20:13:17.376646 23456243894080 make_examples_core.py:243] Task 18/64: Common contigs are ['I', 'II', 'III', 'IV', 'V', 'X', 'MtDNA']. I0217 20:13:17.383403 23456243894080 make_examples_core.py:243] Task 18/64: Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. 2023-02-17 20:13:17.383631: I third_party/nucleus/io/sam_reader.cc:736] Setting HTS_OPT_BLOCK_SIZE to 134217728. 2023-02-17 20:13:17.235347: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD	VN:1.6	SO:coordinate	pb:5.0.0. #[snip]. I0217 20:13:17.702247 23456243894080 genomics_reader.py:222] Reading /scratch4/jwang8/hoyon/pbmm2/aln13448198.pbmm2.bam with NativeSamReader. 2023-02-17 20:13:17.812898: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD	VN:1.6	SO:coordinate	pb:5.0.0. I0217 20:13:17.813058 23456243894080 genomics_reader.py:222] Reading /scratch4/path.to.mydir/pbmm2/aln13448198.pbmm2.bam with NativeSamReader. I0217 20:13:17.814152 23456243894080 make_examples_core.py:243] Task 11/64: Writing examples to /tmp/tmp1yvr59_z/make_examples.tfrecord-00011-of-00064.gz. I0217 20:13:17.814266 23456243894080 make_examples_core.py:243] Task 11/64: Overhead ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/614
https://github.com/google/deepvariant/issues/614:493,performance,time,time,493,"Deepvariant fails without clear reason - with PacBio data; Deepvariant fails without clear reason. . **Setup**. JHU Rockfish HPC. Singularity 3.8.7. singularity pull docker://google/deepvariant:1.4.0. Problematic data are PacBio (first gen). I have used Deepvariant with Illumina without problem, and I used PEPPER to process Ont data and PacBio Hifi data. I used pbmm2 to align fastq with all PacBio data. Command used to run:. ```. #!/bin/bash. #SBATCH --job-name=deep64_13448198. #SBATCH --time=24:00:00. #SBATCH --nodes=2. #SBATCH --ntasks-per-node=1. #SBATCH --cpus-per-task=32. #SBATCH --mem=0. ml anaconda. conda activate /data/path.to.mydir/deepvariant. singularity run --bind /scratch4/path.to.mydir/:/scratch4/path.to.mydir/ \. docker://google/deepvariant:1.4.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type PACBIO \. --ref c_elegans.PRJNA13758.WS245.genomic.fa \. --reads aln13448198.pbmm2.bam \. --output_vcf aln13448198.pbmm2.dv.vcf.gz \. --num_shards 64. ```. Here's a long snippet of slurm output:. ```. INFO: Using cached SIF image. INFO: Converting SIF file to temporary sandbox... I0217 20:13:14.117354 23456243894080 run_deepvariant.py:342] Re-using the directory for intermediate results in /tmp/tmp1yvr59_z. ***** Intermediate results will be written to /tmp/tmp1yvr59_z in docker. ****. ***** Running the command:*****. time seq 0 63 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref [snipped]. #[snip]. # this part is likely unimportant. perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. 	LANGUAGE = (unset),. 	LC_ALL = (unset),. 	LC_CTYPE = ""C.UTF-8"",. 	LANG = ""en_US.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. 	LANGUAGE = (unset),. 	LC_ALL = (unset),. 	LC_CTYPE = ""C.UTF-8"",. 	LANG = ""en_US.UTF-8"". are supported and ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/614
https://github.com/google/deepvariant/issues/614:566,performance,cpu,cpus-per-task,566,"Deepvariant fails without clear reason - with PacBio data; Deepvariant fails without clear reason. . **Setup**. JHU Rockfish HPC. Singularity 3.8.7. singularity pull docker://google/deepvariant:1.4.0. Problematic data are PacBio (first gen). I have used Deepvariant with Illumina without problem, and I used PEPPER to process Ont data and PacBio Hifi data. I used pbmm2 to align fastq with all PacBio data. Command used to run:. ```. #!/bin/bash. #SBATCH --job-name=deep64_13448198. #SBATCH --time=24:00:00. #SBATCH --nodes=2. #SBATCH --ntasks-per-node=1. #SBATCH --cpus-per-task=32. #SBATCH --mem=0. ml anaconda. conda activate /data/path.to.mydir/deepvariant. singularity run --bind /scratch4/path.to.mydir/:/scratch4/path.to.mydir/ \. docker://google/deepvariant:1.4.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type PACBIO \. --ref c_elegans.PRJNA13758.WS245.genomic.fa \. --reads aln13448198.pbmm2.bam \. --output_vcf aln13448198.pbmm2.dv.vcf.gz \. --num_shards 64. ```. Here's a long snippet of slurm output:. ```. INFO: Using cached SIF image. INFO: Converting SIF file to temporary sandbox... I0217 20:13:14.117354 23456243894080 run_deepvariant.py:342] Re-using the directory for intermediate results in /tmp/tmp1yvr59_z. ***** Intermediate results will be written to /tmp/tmp1yvr59_z in docker. ****. ***** Running the command:*****. time seq 0 63 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref [snipped]. #[snip]. # this part is likely unimportant. perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. 	LANGUAGE = (unset),. 	LC_ALL = (unset),. 	LC_CTYPE = ""C.UTF-8"",. 	LANG = ""en_US.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. 	LANGUAGE = (unset),. 	LC_ALL = (unset),. 	LC_CTYPE = ""C.UTF-8"",. 	LANG = ""en_US.UTF-8"". are supported and ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/614
https://github.com/google/deepvariant/issues/614:1041,performance,cach,cached,1041,"h PacBio data; Deepvariant fails without clear reason. . **Setup**. JHU Rockfish HPC. Singularity 3.8.7. singularity pull docker://google/deepvariant:1.4.0. Problematic data are PacBio (first gen). I have used Deepvariant with Illumina without problem, and I used PEPPER to process Ont data and PacBio Hifi data. I used pbmm2 to align fastq with all PacBio data. Command used to run:. ```. #!/bin/bash. #SBATCH --job-name=deep64_13448198. #SBATCH --time=24:00:00. #SBATCH --nodes=2. #SBATCH --ntasks-per-node=1. #SBATCH --cpus-per-task=32. #SBATCH --mem=0. ml anaconda. conda activate /data/path.to.mydir/deepvariant. singularity run --bind /scratch4/path.to.mydir/:/scratch4/path.to.mydir/ \. docker://google/deepvariant:1.4.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type PACBIO \. --ref c_elegans.PRJNA13758.WS245.genomic.fa \. --reads aln13448198.pbmm2.bam \. --output_vcf aln13448198.pbmm2.dv.vcf.gz \. --num_shards 64. ```. Here's a long snippet of slurm output:. ```. INFO: Using cached SIF image. INFO: Converting SIF file to temporary sandbox... I0217 20:13:14.117354 23456243894080 run_deepvariant.py:342] Re-using the directory for intermediate results in /tmp/tmp1yvr59_z. ***** Intermediate results will be written to /tmp/tmp1yvr59_z in docker. ****. ***** Running the command:*****. time seq 0 63 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref [snipped]. #[snip]. # this part is likely unimportant. perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. 	LANGUAGE = (unset),. 	LC_ALL = (unset),. 	LC_CTYPE = ""C.UTF-8"",. 	LANG = ""en_US.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. 	LANGUAGE = (unset),. 	LC_ALL = (unset),. 	LC_CTYPE = ""C.UTF-8"",. 	LANG = ""en_US.UTF-8"". are supported and installed on your system. perl: warning: Fal",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/614
https://github.com/google/deepvariant/issues/614:1352,performance,time,time,1352,"a. I used pbmm2 to align fastq with all PacBio data. Command used to run:. ```. #!/bin/bash. #SBATCH --job-name=deep64_13448198. #SBATCH --time=24:00:00. #SBATCH --nodes=2. #SBATCH --ntasks-per-node=1. #SBATCH --cpus-per-task=32. #SBATCH --mem=0. ml anaconda. conda activate /data/path.to.mydir/deepvariant. singularity run --bind /scratch4/path.to.mydir/:/scratch4/path.to.mydir/ \. docker://google/deepvariant:1.4.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type PACBIO \. --ref c_elegans.PRJNA13758.WS245.genomic.fa \. --reads aln13448198.pbmm2.bam \. --output_vcf aln13448198.pbmm2.dv.vcf.gz \. --num_shards 64. ```. Here's a long snippet of slurm output:. ```. INFO: Using cached SIF image. INFO: Converting SIF file to temporary sandbox... I0217 20:13:14.117354 23456243894080 run_deepvariant.py:342] Re-using the directory for intermediate results in /tmp/tmp1yvr59_z. ***** Intermediate results will be written to /tmp/tmp1yvr59_z in docker. ****. ***** Running the command:*****. time seq 0 63 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref [snipped]. #[snip]. # this part is likely unimportant. perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. 	LANGUAGE = (unset),. 	LC_ALL = (unset),. 	LC_CTYPE = ""C.UTF-8"",. 	LANG = ""en_US.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. 	LANGUAGE = (unset),. 	LC_ALL = (unset),. 	LC_CTYPE = ""C.UTF-8"",. 	LANG = ""en_US.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). #[snip]. 2023-02-17 20:13:17.235641: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD	VN:1.6	SO:coordinate	pb:5.0.0. I0217 20:13:17.235805 23456243894080 genomics_reader.py:222] Reading /scratch4/path.to.mydir/pbmm2/aln13448198.p",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/614
https://github.com/google/deepvariant/issues/614:1368,performance,parallel,parallel,1368," align fastq with all PacBio data. Command used to run:. ```. #!/bin/bash. #SBATCH --job-name=deep64_13448198. #SBATCH --time=24:00:00. #SBATCH --nodes=2. #SBATCH --ntasks-per-node=1. #SBATCH --cpus-per-task=32. #SBATCH --mem=0. ml anaconda. conda activate /data/path.to.mydir/deepvariant. singularity run --bind /scratch4/path.to.mydir/:/scratch4/path.to.mydir/ \. docker://google/deepvariant:1.4.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type PACBIO \. --ref c_elegans.PRJNA13758.WS245.genomic.fa \. --reads aln13448198.pbmm2.bam \. --output_vcf aln13448198.pbmm2.dv.vcf.gz \. --num_shards 64. ```. Here's a long snippet of slurm output:. ```. INFO: Using cached SIF image. INFO: Converting SIF file to temporary sandbox... I0217 20:13:14.117354 23456243894080 run_deepvariant.py:342] Re-using the directory for intermediate results in /tmp/tmp1yvr59_z. ***** Intermediate results will be written to /tmp/tmp1yvr59_z in docker. ****. ***** Running the command:*****. time seq 0 63 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref [snipped]. #[snip]. # this part is likely unimportant. perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. 	LANGUAGE = (unset),. 	LC_ALL = (unset),. 	LC_CTYPE = ""C.UTF-8"",. 	LANG = ""en_US.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. 	LANGUAGE = (unset),. 	LC_ALL = (unset),. 	LC_CTYPE = ""C.UTF-8"",. 	LANG = ""en_US.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). #[snip]. 2023-02-17 20:13:17.235641: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD	VN:1.6	SO:coordinate	pb:5.0.0. I0217 20:13:17.235805 23456243894080 genomics_reader.py:222] Reading /scratch4/path.to.mydir/pbmm2/aln13448198.pbmm2.bam with Nati",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/614
https://github.com/google/deepvariant/issues/614:4088,performance,Overhead,Overhead,4088,"l decode CRAM using the reference you passed in with --ref. 2023-02-17 20:13:17.383631: I third_party/nucleus/io/sam_reader.cc:736] Setting HTS_OPT_BLOCK_SIZE to 134217728. 2023-02-17 20:13:17.235347: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD	VN:1.6	SO:coordinate	pb:5.0.0. #[snip]. I0217 20:13:17.702247 23456243894080 genomics_reader.py:222] Reading /scratch4/jwang8/hoyon/pbmm2/aln13448198.pbmm2.bam with NativeSamReader. 2023-02-17 20:13:17.812898: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD	VN:1.6	SO:coordinate	pb:5.0.0. I0217 20:13:17.813058 23456243894080 genomics_reader.py:222] Reading /scratch4/path.to.mydir/pbmm2/aln13448198.pbmm2.bam with NativeSamReader. I0217 20:13:17.814152 23456243894080 make_examples_core.py:243] Task 11/64: Writing examples to /tmp/tmp1yvr59_z/make_examples.tfrecord-00011-of-00064.gz. I0217 20:13:17.814266 23456243894080 make_examples_core.py:243] Task 11/64: Overhead for preparing inputs: 0 seconds. 2023-02-17 20:13:17.834667: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD	VN:1.6	SO:coordinate	pb:5.0.0. I0217 20:13:17.834807 23456243894080 genomics_reader.py:222] Reading /scratch4/path.to.mydir/pbmm2/aln13448198.pbmm2.bam with NativeSamReader. I0217 20:13:17.836419 23456243894080 make_examples_core.py:243] Task 18/64: Writing examples to /tmp/tmp1yvr59_z/make_examples.tfrecord-00018-of-00064.gz. I0217 20:13:17.836520 23456243894080 make_examples_core.py:243] Task 18/64: Overhead for preparing inputs: 0 seconds. 2023-02-17 20:13:17.850120: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD	VN:1.6	SO:coordinate	pb:5.0.0. I0217 20:13:17.850307 23456243894080 genomics_reader.py:222] Reading /scratch4/path.to.mydir/pbmm2/aln13448198.pbmm2.bam with NativeSamReader. I0217 20:13:17.851420 23456243894080 make_examples_core.py:243] Task 17/64: Writing examples to /tmp/tmp1",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/614
https://github.com/google/deepvariant/issues/614:4653,performance,Overhead,Overhead,4653,"eader line, ignoring: @HD	VN:1.6	SO:coordinate	pb:5.0.0. I0217 20:13:17.813058 23456243894080 genomics_reader.py:222] Reading /scratch4/path.to.mydir/pbmm2/aln13448198.pbmm2.bam with NativeSamReader. I0217 20:13:17.814152 23456243894080 make_examples_core.py:243] Task 11/64: Writing examples to /tmp/tmp1yvr59_z/make_examples.tfrecord-00011-of-00064.gz. I0217 20:13:17.814266 23456243894080 make_examples_core.py:243] Task 11/64: Overhead for preparing inputs: 0 seconds. 2023-02-17 20:13:17.834667: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD	VN:1.6	SO:coordinate	pb:5.0.0. I0217 20:13:17.834807 23456243894080 genomics_reader.py:222] Reading /scratch4/path.to.mydir/pbmm2/aln13448198.pbmm2.bam with NativeSamReader. I0217 20:13:17.836419 23456243894080 make_examples_core.py:243] Task 18/64: Writing examples to /tmp/tmp1yvr59_z/make_examples.tfrecord-00018-of-00064.gz. I0217 20:13:17.836520 23456243894080 make_examples_core.py:243] Task 18/64: Overhead for preparing inputs: 0 seconds. 2023-02-17 20:13:17.850120: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD	VN:1.6	SO:coordinate	pb:5.0.0. I0217 20:13:17.850307 23456243894080 genomics_reader.py:222] Reading /scratch4/path.to.mydir/pbmm2/aln13448198.pbmm2.bam with NativeSamReader. I0217 20:13:17.851420 23456243894080 make_examples_core.py:243] Task 17/64: Writing examples to /tmp/tmp1yvr59_z/make_examples.tfrecord-00017-of-00064.gz. I0217 20:13:17.851542 23456243894080 make_examples_core.py:243] Task 17/64: Overhead for preparing inputs: 0 seconds. 2023-02-17 20:13:17.829944: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD	VN:1.6	SO:coordinate	pb:5.0.0. I0217 20:13:17.830086 23456243894080 genomics_reader.py:222] Reading /scratch4/path.to.mydir/pbmm2/aln13448198.pbmm2.bam with NativeSamReader. I0217 20:13:17.831279 23456243894080 make_examples_core.py:243] Task 7/64: Writing examples to /tmp/tmp1y",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/614
https://github.com/google/deepvariant/issues/614:5218,performance,Overhead,Overhead,5218,"eader line, ignoring: @HD	VN:1.6	SO:coordinate	pb:5.0.0. I0217 20:13:17.834807 23456243894080 genomics_reader.py:222] Reading /scratch4/path.to.mydir/pbmm2/aln13448198.pbmm2.bam with NativeSamReader. I0217 20:13:17.836419 23456243894080 make_examples_core.py:243] Task 18/64: Writing examples to /tmp/tmp1yvr59_z/make_examples.tfrecord-00018-of-00064.gz. I0217 20:13:17.836520 23456243894080 make_examples_core.py:243] Task 18/64: Overhead for preparing inputs: 0 seconds. 2023-02-17 20:13:17.850120: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD	VN:1.6	SO:coordinate	pb:5.0.0. I0217 20:13:17.850307 23456243894080 genomics_reader.py:222] Reading /scratch4/path.to.mydir/pbmm2/aln13448198.pbmm2.bam with NativeSamReader. I0217 20:13:17.851420 23456243894080 make_examples_core.py:243] Task 17/64: Writing examples to /tmp/tmp1yvr59_z/make_examples.tfrecord-00017-of-00064.gz. I0217 20:13:17.851542 23456243894080 make_examples_core.py:243] Task 17/64: Overhead for preparing inputs: 0 seconds. 2023-02-17 20:13:17.829944: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD	VN:1.6	SO:coordinate	pb:5.0.0. I0217 20:13:17.830086 23456243894080 genomics_reader.py:222] Reading /scratch4/path.to.mydir/pbmm2/aln13448198.pbmm2.bam with NativeSamReader. I0217 20:13:17.831279 23456243894080 make_examples_core.py:243] Task 7/64: Writing examples to /tmp/tmp1yvr59_z/make_examples.tfrecord-00007-of-00064.gz. I0217 20:13:17.831381 23456243894080 make_examples_core.py:243] Task 7/64: Overhead for preparing inputs: 0 seconds. #[snip]. I0217 20:13:55.184195 23456243894080 make_examples_core.py:243] Task 8/64: Skip phasing: len(candidates[main_sample]) is 19455. I0217 20:13:55.731674 23456243894080 make_examples_core.py:243] Task 22/64: Skip phasing: len(candidates[main_sample]) is 18977. I0217 20:13:55.541318 23456243894080 make_examples_core.py:243] Task 1/64: Skip phasing: len(candidates[main_sample]) is 19291. I0217",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/614
https://github.com/google/deepvariant/issues/614:5781,performance,Overhead,Overhead,5781," header line, ignoring: @HD	VN:1.6	SO:coordinate	pb:5.0.0. I0217 20:13:17.850307 23456243894080 genomics_reader.py:222] Reading /scratch4/path.to.mydir/pbmm2/aln13448198.pbmm2.bam with NativeSamReader. I0217 20:13:17.851420 23456243894080 make_examples_core.py:243] Task 17/64: Writing examples to /tmp/tmp1yvr59_z/make_examples.tfrecord-00017-of-00064.gz. I0217 20:13:17.851542 23456243894080 make_examples_core.py:243] Task 17/64: Overhead for preparing inputs: 0 seconds. 2023-02-17 20:13:17.829944: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD	VN:1.6	SO:coordinate	pb:5.0.0. I0217 20:13:17.830086 23456243894080 genomics_reader.py:222] Reading /scratch4/path.to.mydir/pbmm2/aln13448198.pbmm2.bam with NativeSamReader. I0217 20:13:17.831279 23456243894080 make_examples_core.py:243] Task 7/64: Writing examples to /tmp/tmp1yvr59_z/make_examples.tfrecord-00007-of-00064.gz. I0217 20:13:17.831381 23456243894080 make_examples_core.py:243] Task 7/64: Overhead for preparing inputs: 0 seconds. #[snip]. I0217 20:13:55.184195 23456243894080 make_examples_core.py:243] Task 8/64: Skip phasing: len(candidates[main_sample]) is 19455. I0217 20:13:55.731674 23456243894080 make_examples_core.py:243] Task 22/64: Skip phasing: len(candidates[main_sample]) is 18977. I0217 20:13:55.541318 23456243894080 make_examples_core.py:243] Task 1/64: Skip phasing: len(candidates[main_sample]) is 19291. I0217 20:13:54.948142 23456243894080 make_examples_core.py:243] Task 19/64: Skip phasing: len(candidates[main_sample]) is 19012. I0217 20:13:55.139624 23456243894080 make_examples_core.py:243] Task 15/64: Skip phasing: len(candidates[main_sample]) is 18348. I0217 20:13:55.973984 23456243894080 make_examples_core.py:243] Task 18/64: Skip phasing: len(candidates[main_sample]) is 19340. I0217 20:13:56.415429 23456243894080 make_examples_core.py:243] Task 20/64: Skip phasing: len(candidates[main_sample]) is 19479. I0217 20:13:57.042794 23456243894080 make_examples_c",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/614
https://github.com/google/deepvariant/issues/614:11997,performance,parallel,parallel,11997,"ples_core.py:243] Task 28/64: 105506 candidates (115610 examples) [6984.90s elapsed]. I0218 10:42:47.468951 23456243894080 make_examples_core.py:243] Task 28/64: Skip phasing: len(candidates[main_sample]) is 14998. I0218 10:45:20.167151 23456243894080 make_examples_core.py:243] Task 19/64: 115447 candidates (126875 examples) [6737.31s elapsed]. I0218 10:46:36.864049 23456243894080 make_examples_core.py:243] Task 19/64: Skip phasing: len(candidates[main_sample]) is 20526. I0218 10:48:19.364838 23456243894080 make_examples_core.py:243] Task 2/64: 158555 candidates (173735 examples) [4091.74s elapsed]. I0218 10:48:45.881830 23456243894080 make_examples_core.py:243] Task 2/64: Skip phasing: len(candidates[main_sample]) is 14234. I0218 10:49:31.045118 23456243894080 make_examples_core.py:243] Task 13/64: 113956 candidates (125182 examples) [6317.67s elapsed]. I0218 10:50:33.895329 23456243894080 make_examples_core.py:243] Task 13/64: Skip phasing: len(candidates[main_sample]) is 18414. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /scratch4/path.to.mydir/genomes/c_elegans.PRJNA13758.WS245.genomic.fa --reads /scratch4/path.to.mydir/pbmm2/aln13448198.pbmm2.bam --examples /tmp/tmp1yvr59_z/make_examples.tfrecord@64.gz --add_hp_channel --alt_aligned_pileup diff_channels --max_reads_per_partition 600 --min_mapping_quality 1 --parse_sam_aux_fields --partition_size 25000 --phase_reads --pileup_image_width 199 --norealign_reads --sort_by_haplotypes --track_ref_reads --vsc_min_fraction_indels 0.12 --task 28. real	879m59.515s. user	632m52.969s. sys	6m20.594s. INFO: Cleaning up image... ```. I also ran more jobs using different numbers of cpu and mem using different bam files. One using 48 cpu and --mem-per-cpu=6G simply fizzled without any error message. These jobs are taking considerable core-hours, so troubleshooting is hard. I also wonder if I am using Deepvariant efficiently. On a side note, I got many Deepvariant failures with error messag",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/614
https://github.com/google/deepvariant/issues/614:12689,performance,cpu,cpu,12689,"s) [6737.31s elapsed]. I0218 10:46:36.864049 23456243894080 make_examples_core.py:243] Task 19/64: Skip phasing: len(candidates[main_sample]) is 20526. I0218 10:48:19.364838 23456243894080 make_examples_core.py:243] Task 2/64: 158555 candidates (173735 examples) [4091.74s elapsed]. I0218 10:48:45.881830 23456243894080 make_examples_core.py:243] Task 2/64: Skip phasing: len(candidates[main_sample]) is 14234. I0218 10:49:31.045118 23456243894080 make_examples_core.py:243] Task 13/64: 113956 candidates (125182 examples) [6317.67s elapsed]. I0218 10:50:33.895329 23456243894080 make_examples_core.py:243] Task 13/64: Skip phasing: len(candidates[main_sample]) is 18414. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /scratch4/path.to.mydir/genomes/c_elegans.PRJNA13758.WS245.genomic.fa --reads /scratch4/path.to.mydir/pbmm2/aln13448198.pbmm2.bam --examples /tmp/tmp1yvr59_z/make_examples.tfrecord@64.gz --add_hp_channel --alt_aligned_pileup diff_channels --max_reads_per_partition 600 --min_mapping_quality 1 --parse_sam_aux_fields --partition_size 25000 --phase_reads --pileup_image_width 199 --norealign_reads --sort_by_haplotypes --track_ref_reads --vsc_min_fraction_indels 0.12 --task 28. real	879m59.515s. user	632m52.969s. sys	6m20.594s. INFO: Cleaning up image... ```. I also ran more jobs using different numbers of cpu and mem using different bam files. One using 48 cpu and --mem-per-cpu=6G simply fizzled without any error message. These jobs are taking considerable core-hours, so troubleshooting is hard. I also wonder if I am using Deepvariant efficiently. On a side note, I got many Deepvariant failures with error messages like:. ```. Detected 1372 oom-kill event(s) in StepId=12049020.batch cgroup. Some of your processes may have been killed by the cgroup out-of-memory handler. ```. This seems to have been resolved by asking for maximum allowable memory. I am still curious about the memory requirement for successfully running Deepvariant.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/614
https://github.com/google/deepvariant/issues/614:12741,performance,cpu,cpu,12741,"s) [6737.31s elapsed]. I0218 10:46:36.864049 23456243894080 make_examples_core.py:243] Task 19/64: Skip phasing: len(candidates[main_sample]) is 20526. I0218 10:48:19.364838 23456243894080 make_examples_core.py:243] Task 2/64: 158555 candidates (173735 examples) [4091.74s elapsed]. I0218 10:48:45.881830 23456243894080 make_examples_core.py:243] Task 2/64: Skip phasing: len(candidates[main_sample]) is 14234. I0218 10:49:31.045118 23456243894080 make_examples_core.py:243] Task 13/64: 113956 candidates (125182 examples) [6317.67s elapsed]. I0218 10:50:33.895329 23456243894080 make_examples_core.py:243] Task 13/64: Skip phasing: len(candidates[main_sample]) is 18414. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /scratch4/path.to.mydir/genomes/c_elegans.PRJNA13758.WS245.genomic.fa --reads /scratch4/path.to.mydir/pbmm2/aln13448198.pbmm2.bam --examples /tmp/tmp1yvr59_z/make_examples.tfrecord@64.gz --add_hp_channel --alt_aligned_pileup diff_channels --max_reads_per_partition 600 --min_mapping_quality 1 --parse_sam_aux_fields --partition_size 25000 --phase_reads --pileup_image_width 199 --norealign_reads --sort_by_haplotypes --track_ref_reads --vsc_min_fraction_indels 0.12 --task 28. real	879m59.515s. user	632m52.969s. sys	6m20.594s. INFO: Cleaning up image... ```. I also ran more jobs using different numbers of cpu and mem using different bam files. One using 48 cpu and --mem-per-cpu=6G simply fizzled without any error message. These jobs are taking considerable core-hours, so troubleshooting is hard. I also wonder if I am using Deepvariant efficiently. On a side note, I got many Deepvariant failures with error messages like:. ```. Detected 1372 oom-kill event(s) in StepId=12049020.batch cgroup. Some of your processes may have been killed by the cgroup out-of-memory handler. ```. This seems to have been resolved by asking for maximum allowable memory. I am still curious about the memory requirement for successfully running Deepvariant.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/614
https://github.com/google/deepvariant/issues/614:12759,performance,cpu,cpu,12759,"s) [6737.31s elapsed]. I0218 10:46:36.864049 23456243894080 make_examples_core.py:243] Task 19/64: Skip phasing: len(candidates[main_sample]) is 20526. I0218 10:48:19.364838 23456243894080 make_examples_core.py:243] Task 2/64: 158555 candidates (173735 examples) [4091.74s elapsed]. I0218 10:48:45.881830 23456243894080 make_examples_core.py:243] Task 2/64: Skip phasing: len(candidates[main_sample]) is 14234. I0218 10:49:31.045118 23456243894080 make_examples_core.py:243] Task 13/64: 113956 candidates (125182 examples) [6317.67s elapsed]. I0218 10:50:33.895329 23456243894080 make_examples_core.py:243] Task 13/64: Skip phasing: len(candidates[main_sample]) is 18414. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /scratch4/path.to.mydir/genomes/c_elegans.PRJNA13758.WS245.genomic.fa --reads /scratch4/path.to.mydir/pbmm2/aln13448198.pbmm2.bam --examples /tmp/tmp1yvr59_z/make_examples.tfrecord@64.gz --add_hp_channel --alt_aligned_pileup diff_channels --max_reads_per_partition 600 --min_mapping_quality 1 --parse_sam_aux_fields --partition_size 25000 --phase_reads --pileup_image_width 199 --norealign_reads --sort_by_haplotypes --track_ref_reads --vsc_min_fraction_indels 0.12 --task 28. real	879m59.515s. user	632m52.969s. sys	6m20.594s. INFO: Cleaning up image... ```. I also ran more jobs using different numbers of cpu and mem using different bam files. One using 48 cpu and --mem-per-cpu=6G simply fizzled without any error message. These jobs are taking considerable core-hours, so troubleshooting is hard. I also wonder if I am using Deepvariant efficiently. On a side note, I got many Deepvariant failures with error messages like:. ```. Detected 1372 oom-kill event(s) in StepId=12049020.batch cgroup. Some of your processes may have been killed by the cgroup out-of-memory handler. ```. This seems to have been resolved by asking for maximum allowable memory. I am still curious about the memory requirement for successfully running Deepvariant.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/614
https://github.com/google/deepvariant/issues/614:12793,performance,error,error,12793,"s) [6737.31s elapsed]. I0218 10:46:36.864049 23456243894080 make_examples_core.py:243] Task 19/64: Skip phasing: len(candidates[main_sample]) is 20526. I0218 10:48:19.364838 23456243894080 make_examples_core.py:243] Task 2/64: 158555 candidates (173735 examples) [4091.74s elapsed]. I0218 10:48:45.881830 23456243894080 make_examples_core.py:243] Task 2/64: Skip phasing: len(candidates[main_sample]) is 14234. I0218 10:49:31.045118 23456243894080 make_examples_core.py:243] Task 13/64: 113956 candidates (125182 examples) [6317.67s elapsed]. I0218 10:50:33.895329 23456243894080 make_examples_core.py:243] Task 13/64: Skip phasing: len(candidates[main_sample]) is 18414. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /scratch4/path.to.mydir/genomes/c_elegans.PRJNA13758.WS245.genomic.fa --reads /scratch4/path.to.mydir/pbmm2/aln13448198.pbmm2.bam --examples /tmp/tmp1yvr59_z/make_examples.tfrecord@64.gz --add_hp_channel --alt_aligned_pileup diff_channels --max_reads_per_partition 600 --min_mapping_quality 1 --parse_sam_aux_fields --partition_size 25000 --phase_reads --pileup_image_width 199 --norealign_reads --sort_by_haplotypes --track_ref_reads --vsc_min_fraction_indels 0.12 --task 28. real	879m59.515s. user	632m52.969s. sys	6m20.594s. INFO: Cleaning up image... ```. I also ran more jobs using different numbers of cpu and mem using different bam files. One using 48 cpu and --mem-per-cpu=6G simply fizzled without any error message. These jobs are taking considerable core-hours, so troubleshooting is hard. I also wonder if I am using Deepvariant efficiently. On a side note, I got many Deepvariant failures with error messages like:. ```. Detected 1372 oom-kill event(s) in StepId=12049020.batch cgroup. Some of your processes may have been killed by the cgroup out-of-memory handler. ```. This seems to have been resolved by asking for maximum allowable memory. I am still curious about the memory requirement for successfully running Deepvariant.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/614
https://github.com/google/deepvariant/issues/614:12975,performance,failur,failures,12975,"s) [6737.31s elapsed]. I0218 10:46:36.864049 23456243894080 make_examples_core.py:243] Task 19/64: Skip phasing: len(candidates[main_sample]) is 20526. I0218 10:48:19.364838 23456243894080 make_examples_core.py:243] Task 2/64: 158555 candidates (173735 examples) [4091.74s elapsed]. I0218 10:48:45.881830 23456243894080 make_examples_core.py:243] Task 2/64: Skip phasing: len(candidates[main_sample]) is 14234. I0218 10:49:31.045118 23456243894080 make_examples_core.py:243] Task 13/64: 113956 candidates (125182 examples) [6317.67s elapsed]. I0218 10:50:33.895329 23456243894080 make_examples_core.py:243] Task 13/64: Skip phasing: len(candidates[main_sample]) is 18414. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /scratch4/path.to.mydir/genomes/c_elegans.PRJNA13758.WS245.genomic.fa --reads /scratch4/path.to.mydir/pbmm2/aln13448198.pbmm2.bam --examples /tmp/tmp1yvr59_z/make_examples.tfrecord@64.gz --add_hp_channel --alt_aligned_pileup diff_channels --max_reads_per_partition 600 --min_mapping_quality 1 --parse_sam_aux_fields --partition_size 25000 --phase_reads --pileup_image_width 199 --norealign_reads --sort_by_haplotypes --track_ref_reads --vsc_min_fraction_indels 0.12 --task 28. real	879m59.515s. user	632m52.969s. sys	6m20.594s. INFO: Cleaning up image... ```. I also ran more jobs using different numbers of cpu and mem using different bam files. One using 48 cpu and --mem-per-cpu=6G simply fizzled without any error message. These jobs are taking considerable core-hours, so troubleshooting is hard. I also wonder if I am using Deepvariant efficiently. On a side note, I got many Deepvariant failures with error messages like:. ```. Detected 1372 oom-kill event(s) in StepId=12049020.batch cgroup. Some of your processes may have been killed by the cgroup out-of-memory handler. ```. This seems to have been resolved by asking for maximum allowable memory. I am still curious about the memory requirement for successfully running Deepvariant.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/614
https://github.com/google/deepvariant/issues/614:12989,performance,error,error,12989,"s) [6737.31s elapsed]. I0218 10:46:36.864049 23456243894080 make_examples_core.py:243] Task 19/64: Skip phasing: len(candidates[main_sample]) is 20526. I0218 10:48:19.364838 23456243894080 make_examples_core.py:243] Task 2/64: 158555 candidates (173735 examples) [4091.74s elapsed]. I0218 10:48:45.881830 23456243894080 make_examples_core.py:243] Task 2/64: Skip phasing: len(candidates[main_sample]) is 14234. I0218 10:49:31.045118 23456243894080 make_examples_core.py:243] Task 13/64: 113956 candidates (125182 examples) [6317.67s elapsed]. I0218 10:50:33.895329 23456243894080 make_examples_core.py:243] Task 13/64: Skip phasing: len(candidates[main_sample]) is 18414. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /scratch4/path.to.mydir/genomes/c_elegans.PRJNA13758.WS245.genomic.fa --reads /scratch4/path.to.mydir/pbmm2/aln13448198.pbmm2.bam --examples /tmp/tmp1yvr59_z/make_examples.tfrecord@64.gz --add_hp_channel --alt_aligned_pileup diff_channels --max_reads_per_partition 600 --min_mapping_quality 1 --parse_sam_aux_fields --partition_size 25000 --phase_reads --pileup_image_width 199 --norealign_reads --sort_by_haplotypes --track_ref_reads --vsc_min_fraction_indels 0.12 --task 28. real	879m59.515s. user	632m52.969s. sys	6m20.594s. INFO: Cleaning up image... ```. I also ran more jobs using different numbers of cpu and mem using different bam files. One using 48 cpu and --mem-per-cpu=6G simply fizzled without any error message. These jobs are taking considerable core-hours, so troubleshooting is hard. I also wonder if I am using Deepvariant efficiently. On a side note, I got many Deepvariant failures with error messages like:. ```. Detected 1372 oom-kill event(s) in StepId=12049020.batch cgroup. Some of your processes may have been killed by the cgroup out-of-memory handler. ```. This seems to have been resolved by asking for maximum allowable memory. I am still curious about the memory requirement for successfully running Deepvariant.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/614
https://github.com/google/deepvariant/issues/614:13067,performance,batch,batch,13067,"s) [6737.31s elapsed]. I0218 10:46:36.864049 23456243894080 make_examples_core.py:243] Task 19/64: Skip phasing: len(candidates[main_sample]) is 20526. I0218 10:48:19.364838 23456243894080 make_examples_core.py:243] Task 2/64: 158555 candidates (173735 examples) [4091.74s elapsed]. I0218 10:48:45.881830 23456243894080 make_examples_core.py:243] Task 2/64: Skip phasing: len(candidates[main_sample]) is 14234. I0218 10:49:31.045118 23456243894080 make_examples_core.py:243] Task 13/64: 113956 candidates (125182 examples) [6317.67s elapsed]. I0218 10:50:33.895329 23456243894080 make_examples_core.py:243] Task 13/64: Skip phasing: len(candidates[main_sample]) is 18414. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /scratch4/path.to.mydir/genomes/c_elegans.PRJNA13758.WS245.genomic.fa --reads /scratch4/path.to.mydir/pbmm2/aln13448198.pbmm2.bam --examples /tmp/tmp1yvr59_z/make_examples.tfrecord@64.gz --add_hp_channel --alt_aligned_pileup diff_channels --max_reads_per_partition 600 --min_mapping_quality 1 --parse_sam_aux_fields --partition_size 25000 --phase_reads --pileup_image_width 199 --norealign_reads --sort_by_haplotypes --track_ref_reads --vsc_min_fraction_indels 0.12 --task 28. real	879m59.515s. user	632m52.969s. sys	6m20.594s. INFO: Cleaning up image... ```. I also ran more jobs using different numbers of cpu and mem using different bam files. One using 48 cpu and --mem-per-cpu=6G simply fizzled without any error message. These jobs are taking considerable core-hours, so troubleshooting is hard. I also wonder if I am using Deepvariant efficiently. On a side note, I got many Deepvariant failures with error messages like:. ```. Detected 1372 oom-kill event(s) in StepId=12049020.batch cgroup. Some of your processes may have been killed by the cgroup out-of-memory handler. ```. This seems to have been resolved by asking for maximum allowable memory. I am still curious about the memory requirement for successfully running Deepvariant.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/614
https://github.com/google/deepvariant/issues/614:13146,performance,memor,memory,13146,"s) [6737.31s elapsed]. I0218 10:46:36.864049 23456243894080 make_examples_core.py:243] Task 19/64: Skip phasing: len(candidates[main_sample]) is 20526. I0218 10:48:19.364838 23456243894080 make_examples_core.py:243] Task 2/64: 158555 candidates (173735 examples) [4091.74s elapsed]. I0218 10:48:45.881830 23456243894080 make_examples_core.py:243] Task 2/64: Skip phasing: len(candidates[main_sample]) is 14234. I0218 10:49:31.045118 23456243894080 make_examples_core.py:243] Task 13/64: 113956 candidates (125182 examples) [6317.67s elapsed]. I0218 10:50:33.895329 23456243894080 make_examples_core.py:243] Task 13/64: Skip phasing: len(candidates[main_sample]) is 18414. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /scratch4/path.to.mydir/genomes/c_elegans.PRJNA13758.WS245.genomic.fa --reads /scratch4/path.to.mydir/pbmm2/aln13448198.pbmm2.bam --examples /tmp/tmp1yvr59_z/make_examples.tfrecord@64.gz --add_hp_channel --alt_aligned_pileup diff_channels --max_reads_per_partition 600 --min_mapping_quality 1 --parse_sam_aux_fields --partition_size 25000 --phase_reads --pileup_image_width 199 --norealign_reads --sort_by_haplotypes --track_ref_reads --vsc_min_fraction_indels 0.12 --task 28. real	879m59.515s. user	632m52.969s. sys	6m20.594s. INFO: Cleaning up image... ```. I also ran more jobs using different numbers of cpu and mem using different bam files. One using 48 cpu and --mem-per-cpu=6G simply fizzled without any error message. These jobs are taking considerable core-hours, so troubleshooting is hard. I also wonder if I am using Deepvariant efficiently. On a side note, I got many Deepvariant failures with error messages like:. ```. Detected 1372 oom-kill event(s) in StepId=12049020.batch cgroup. Some of your processes may have been killed by the cgroup out-of-memory handler. ```. This seems to have been resolved by asking for maximum allowable memory. I am still curious about the memory requirement for successfully running Deepvariant.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/614
https://github.com/google/deepvariant/issues/614:13232,performance,memor,memory,13232,"s) [6737.31s elapsed]. I0218 10:46:36.864049 23456243894080 make_examples_core.py:243] Task 19/64: Skip phasing: len(candidates[main_sample]) is 20526. I0218 10:48:19.364838 23456243894080 make_examples_core.py:243] Task 2/64: 158555 candidates (173735 examples) [4091.74s elapsed]. I0218 10:48:45.881830 23456243894080 make_examples_core.py:243] Task 2/64: Skip phasing: len(candidates[main_sample]) is 14234. I0218 10:49:31.045118 23456243894080 make_examples_core.py:243] Task 13/64: 113956 candidates (125182 examples) [6317.67s elapsed]. I0218 10:50:33.895329 23456243894080 make_examples_core.py:243] Task 13/64: Skip phasing: len(candidates[main_sample]) is 18414. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /scratch4/path.to.mydir/genomes/c_elegans.PRJNA13758.WS245.genomic.fa --reads /scratch4/path.to.mydir/pbmm2/aln13448198.pbmm2.bam --examples /tmp/tmp1yvr59_z/make_examples.tfrecord@64.gz --add_hp_channel --alt_aligned_pileup diff_channels --max_reads_per_partition 600 --min_mapping_quality 1 --parse_sam_aux_fields --partition_size 25000 --phase_reads --pileup_image_width 199 --norealign_reads --sort_by_haplotypes --track_ref_reads --vsc_min_fraction_indels 0.12 --task 28. real	879m59.515s. user	632m52.969s. sys	6m20.594s. INFO: Cleaning up image... ```. I also ran more jobs using different numbers of cpu and mem using different bam files. One using 48 cpu and --mem-per-cpu=6G simply fizzled without any error message. These jobs are taking considerable core-hours, so troubleshooting is hard. I also wonder if I am using Deepvariant efficiently. On a side note, I got many Deepvariant failures with error messages like:. ```. Detected 1372 oom-kill event(s) in StepId=12049020.batch cgroup. Some of your processes may have been killed by the cgroup out-of-memory handler. ```. This seems to have been resolved by asking for maximum allowable memory. I am still curious about the memory requirement for successfully running Deepvariant.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/614
https://github.com/google/deepvariant/issues/614:13269,performance,memor,memory,13269,"s) [6737.31s elapsed]. I0218 10:46:36.864049 23456243894080 make_examples_core.py:243] Task 19/64: Skip phasing: len(candidates[main_sample]) is 20526. I0218 10:48:19.364838 23456243894080 make_examples_core.py:243] Task 2/64: 158555 candidates (173735 examples) [4091.74s elapsed]. I0218 10:48:45.881830 23456243894080 make_examples_core.py:243] Task 2/64: Skip phasing: len(candidates[main_sample]) is 14234. I0218 10:49:31.045118 23456243894080 make_examples_core.py:243] Task 13/64: 113956 candidates (125182 examples) [6317.67s elapsed]. I0218 10:50:33.895329 23456243894080 make_examples_core.py:243] Task 13/64: Skip phasing: len(candidates[main_sample]) is 18414. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /scratch4/path.to.mydir/genomes/c_elegans.PRJNA13758.WS245.genomic.fa --reads /scratch4/path.to.mydir/pbmm2/aln13448198.pbmm2.bam --examples /tmp/tmp1yvr59_z/make_examples.tfrecord@64.gz --add_hp_channel --alt_aligned_pileup diff_channels --max_reads_per_partition 600 --min_mapping_quality 1 --parse_sam_aux_fields --partition_size 25000 --phase_reads --pileup_image_width 199 --norealign_reads --sort_by_haplotypes --track_ref_reads --vsc_min_fraction_indels 0.12 --task 28. real	879m59.515s. user	632m52.969s. sys	6m20.594s. INFO: Cleaning up image... ```. I also ran more jobs using different numbers of cpu and mem using different bam files. One using 48 cpu and --mem-per-cpu=6G simply fizzled without any error message. These jobs are taking considerable core-hours, so troubleshooting is hard. I also wonder if I am using Deepvariant efficiently. On a side note, I got many Deepvariant failures with error messages like:. ```. Detected 1372 oom-kill event(s) in StepId=12049020.batch cgroup. Some of your processes may have been killed by the cgroup out-of-memory handler. ```. This seems to have been resolved by asking for maximum allowable memory. I am still curious about the memory requirement for successfully running Deepvariant.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/614
https://github.com/google/deepvariant/issues/614:12,reliability,fail,fails,12,"Deepvariant fails without clear reason - with PacBio data; Deepvariant fails without clear reason. . **Setup**. JHU Rockfish HPC. Singularity 3.8.7. singularity pull docker://google/deepvariant:1.4.0. Problematic data are PacBio (first gen). I have used Deepvariant with Illumina without problem, and I used PEPPER to process Ont data and PacBio Hifi data. I used pbmm2 to align fastq with all PacBio data. Command used to run:. ```. #!/bin/bash. #SBATCH --job-name=deep64_13448198. #SBATCH --time=24:00:00. #SBATCH --nodes=2. #SBATCH --ntasks-per-node=1. #SBATCH --cpus-per-task=32. #SBATCH --mem=0. ml anaconda. conda activate /data/path.to.mydir/deepvariant. singularity run --bind /scratch4/path.to.mydir/:/scratch4/path.to.mydir/ \. docker://google/deepvariant:1.4.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type PACBIO \. --ref c_elegans.PRJNA13758.WS245.genomic.fa \. --reads aln13448198.pbmm2.bam \. --output_vcf aln13448198.pbmm2.dv.vcf.gz \. --num_shards 64. ```. Here's a long snippet of slurm output:. ```. INFO: Using cached SIF image. INFO: Converting SIF file to temporary sandbox... I0217 20:13:14.117354 23456243894080 run_deepvariant.py:342] Re-using the directory for intermediate results in /tmp/tmp1yvr59_z. ***** Intermediate results will be written to /tmp/tmp1yvr59_z in docker. ****. ***** Running the command:*****. time seq 0 63 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref [snipped]. #[snip]. # this part is likely unimportant. perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. 	LANGUAGE = (unset),. 	LC_ALL = (unset),. 	LC_CTYPE = ""C.UTF-8"",. 	LANG = ""en_US.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. 	LANGUAGE = (unset),. 	LC_ALL = (unset),. 	LC_CTYPE = ""C.UTF-8"",. 	LANG = ""en_US.UTF-8"". are supported and ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/614
https://github.com/google/deepvariant/issues/614:71,reliability,fail,fails,71,"Deepvariant fails without clear reason - with PacBio data; Deepvariant fails without clear reason. . **Setup**. JHU Rockfish HPC. Singularity 3.8.7. singularity pull docker://google/deepvariant:1.4.0. Problematic data are PacBio (first gen). I have used Deepvariant with Illumina without problem, and I used PEPPER to process Ont data and PacBio Hifi data. I used pbmm2 to align fastq with all PacBio data. Command used to run:. ```. #!/bin/bash. #SBATCH --job-name=deep64_13448198. #SBATCH --time=24:00:00. #SBATCH --nodes=2. #SBATCH --ntasks-per-node=1. #SBATCH --cpus-per-task=32. #SBATCH --mem=0. ml anaconda. conda activate /data/path.to.mydir/deepvariant. singularity run --bind /scratch4/path.to.mydir/:/scratch4/path.to.mydir/ \. docker://google/deepvariant:1.4.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type PACBIO \. --ref c_elegans.PRJNA13758.WS245.genomic.fa \. --reads aln13448198.pbmm2.bam \. --output_vcf aln13448198.pbmm2.dv.vcf.gz \. --num_shards 64. ```. Here's a long snippet of slurm output:. ```. INFO: Using cached SIF image. INFO: Converting SIF file to temporary sandbox... I0217 20:13:14.117354 23456243894080 run_deepvariant.py:342] Re-using the directory for intermediate results in /tmp/tmp1yvr59_z. ***** Intermediate results will be written to /tmp/tmp1yvr59_z in docker. ****. ***** Running the command:*****. time seq 0 63 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref [snipped]. #[snip]. # this part is likely unimportant. perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. 	LANGUAGE = (unset),. 	LC_ALL = (unset),. 	LC_CTYPE = ""C.UTF-8"",. 	LANG = ""en_US.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. 	LANGUAGE = (unset),. 	LC_ALL = (unset),. 	LC_CTYPE = ""C.UTF-8"",. 	LANG = ""en_US.UTF-8"". are supported and ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/614
https://github.com/google/deepvariant/issues/614:1544,reliability,fail,failed,1544,"-node=1. #SBATCH --cpus-per-task=32. #SBATCH --mem=0. ml anaconda. conda activate /data/path.to.mydir/deepvariant. singularity run --bind /scratch4/path.to.mydir/:/scratch4/path.to.mydir/ \. docker://google/deepvariant:1.4.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type PACBIO \. --ref c_elegans.PRJNA13758.WS245.genomic.fa \. --reads aln13448198.pbmm2.bam \. --output_vcf aln13448198.pbmm2.dv.vcf.gz \. --num_shards 64. ```. Here's a long snippet of slurm output:. ```. INFO: Using cached SIF image. INFO: Converting SIF file to temporary sandbox... I0217 20:13:14.117354 23456243894080 run_deepvariant.py:342] Re-using the directory for intermediate results in /tmp/tmp1yvr59_z. ***** Intermediate results will be written to /tmp/tmp1yvr59_z in docker. ****. ***** Running the command:*****. time seq 0 63 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref [snipped]. #[snip]. # this part is likely unimportant. perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. 	LANGUAGE = (unset),. 	LC_ALL = (unset),. 	LC_CTYPE = ""C.UTF-8"",. 	LANG = ""en_US.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. 	LANGUAGE = (unset),. 	LC_ALL = (unset),. 	LC_CTYPE = ""C.UTF-8"",. 	LANG = ""en_US.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). #[snip]. 2023-02-17 20:13:17.235641: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD	VN:1.6	SO:coordinate	pb:5.0.0. I0217 20:13:17.235805 23456243894080 genomics_reader.py:222] Reading /scratch4/path.to.mydir/pbmm2/aln13448198.pbmm2.bam with NativeSamReader. I0217 20:13:17.268698 23456243894080 make_examples_core.py:243] Task 18/64: Preparing inputs. 2023-02-17 20:13:17.371669: W third_party/nucleus/io/sam_reader.cc:1",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/614
https://github.com/google/deepvariant/issues/614:1829,reliability,fail,failed,1829,"IO \. --ref c_elegans.PRJNA13758.WS245.genomic.fa \. --reads aln13448198.pbmm2.bam \. --output_vcf aln13448198.pbmm2.dv.vcf.gz \. --num_shards 64. ```. Here's a long snippet of slurm output:. ```. INFO: Using cached SIF image. INFO: Converting SIF file to temporary sandbox... I0217 20:13:14.117354 23456243894080 run_deepvariant.py:342] Re-using the directory for intermediate results in /tmp/tmp1yvr59_z. ***** Intermediate results will be written to /tmp/tmp1yvr59_z in docker. ****. ***** Running the command:*****. time seq 0 63 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref [snipped]. #[snip]. # this part is likely unimportant. perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. 	LANGUAGE = (unset),. 	LC_ALL = (unset),. 	LC_CTYPE = ""C.UTF-8"",. 	LANG = ""en_US.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. 	LANGUAGE = (unset),. 	LC_ALL = (unset),. 	LC_CTYPE = ""C.UTF-8"",. 	LANG = ""en_US.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). #[snip]. 2023-02-17 20:13:17.235641: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD	VN:1.6	SO:coordinate	pb:5.0.0. I0217 20:13:17.235805 23456243894080 genomics_reader.py:222] Reading /scratch4/path.to.mydir/pbmm2/aln13448198.pbmm2.bam with NativeSamReader. I0217 20:13:17.268698 23456243894080 make_examples_core.py:243] Task 18/64: Preparing inputs. 2023-02-17 20:13:17.371669: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD	VN:1.6	SO:coordinate	pb:5.0.0. I0217 20:13:17.371811 23456243894080 genomics_reader.py:222] Reading /scratch4/jwang8/hoyon/pbmm2/aln13448198.pbmm2.bam with NativeSamReader. I0217 20:13:17.376646 23456243894080 make_examples_core.py:243",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/614
https://github.com/google/deepvariant/issues/614:12016,reliability,fail,failed,12016,"Task 28/64: 105506 candidates (115610 examples) [6984.90s elapsed]. I0218 10:42:47.468951 23456243894080 make_examples_core.py:243] Task 28/64: Skip phasing: len(candidates[main_sample]) is 14998. I0218 10:45:20.167151 23456243894080 make_examples_core.py:243] Task 19/64: 115447 candidates (126875 examples) [6737.31s elapsed]. I0218 10:46:36.864049 23456243894080 make_examples_core.py:243] Task 19/64: Skip phasing: len(candidates[main_sample]) is 20526. I0218 10:48:19.364838 23456243894080 make_examples_core.py:243] Task 2/64: 158555 candidates (173735 examples) [4091.74s elapsed]. I0218 10:48:45.881830 23456243894080 make_examples_core.py:243] Task 2/64: Skip phasing: len(candidates[main_sample]) is 14234. I0218 10:49:31.045118 23456243894080 make_examples_core.py:243] Task 13/64: 113956 candidates (125182 examples) [6317.67s elapsed]. I0218 10:50:33.895329 23456243894080 make_examples_core.py:243] Task 13/64: Skip phasing: len(candidates[main_sample]) is 18414. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /scratch4/path.to.mydir/genomes/c_elegans.PRJNA13758.WS245.genomic.fa --reads /scratch4/path.to.mydir/pbmm2/aln13448198.pbmm2.bam --examples /tmp/tmp1yvr59_z/make_examples.tfrecord@64.gz --add_hp_channel --alt_aligned_pileup diff_channels --max_reads_per_partition 600 --min_mapping_quality 1 --parse_sam_aux_fields --partition_size 25000 --phase_reads --pileup_image_width 199 --norealign_reads --sort_by_haplotypes --track_ref_reads --vsc_min_fraction_indels 0.12 --task 28. real	879m59.515s. user	632m52.969s. sys	6m20.594s. INFO: Cleaning up image... ```. I also ran more jobs using different numbers of cpu and mem using different bam files. One using 48 cpu and --mem-per-cpu=6G simply fizzled without any error message. These jobs are taking considerable core-hours, so troubleshooting is hard. I also wonder if I am using Deepvariant efficiently. On a side note, I got many Deepvariant failures with error messages like:. ```. Det",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/614
https://github.com/google/deepvariant/issues/614:12975,reliability,fail,failures,12975,"s) [6737.31s elapsed]. I0218 10:46:36.864049 23456243894080 make_examples_core.py:243] Task 19/64: Skip phasing: len(candidates[main_sample]) is 20526. I0218 10:48:19.364838 23456243894080 make_examples_core.py:243] Task 2/64: 158555 candidates (173735 examples) [4091.74s elapsed]. I0218 10:48:45.881830 23456243894080 make_examples_core.py:243] Task 2/64: Skip phasing: len(candidates[main_sample]) is 14234. I0218 10:49:31.045118 23456243894080 make_examples_core.py:243] Task 13/64: 113956 candidates (125182 examples) [6317.67s elapsed]. I0218 10:50:33.895329 23456243894080 make_examples_core.py:243] Task 13/64: Skip phasing: len(candidates[main_sample]) is 18414. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /scratch4/path.to.mydir/genomes/c_elegans.PRJNA13758.WS245.genomic.fa --reads /scratch4/path.to.mydir/pbmm2/aln13448198.pbmm2.bam --examples /tmp/tmp1yvr59_z/make_examples.tfrecord@64.gz --add_hp_channel --alt_aligned_pileup diff_channels --max_reads_per_partition 600 --min_mapping_quality 1 --parse_sam_aux_fields --partition_size 25000 --phase_reads --pileup_image_width 199 --norealign_reads --sort_by_haplotypes --track_ref_reads --vsc_min_fraction_indels 0.12 --task 28. real	879m59.515s. user	632m52.969s. sys	6m20.594s. INFO: Cleaning up image... ```. I also ran more jobs using different numbers of cpu and mem using different bam files. One using 48 cpu and --mem-per-cpu=6G simply fizzled without any error message. These jobs are taking considerable core-hours, so troubleshooting is hard. I also wonder if I am using Deepvariant efficiently. On a side note, I got many Deepvariant failures with error messages like:. ```. Detected 1372 oom-kill event(s) in StepId=12049020.batch cgroup. Some of your processes may have been killed by the cgroup out-of-memory handler. ```. This seems to have been resolved by asking for maximum allowable memory. I am still curious about the memory requirement for successfully running Deepvariant.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/614
https://github.com/google/deepvariant/issues/614:2471,safety,input,inputs,2471,"ip]. # this part is likely unimportant. perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. 	LANGUAGE = (unset),. 	LC_ALL = (unset),. 	LC_CTYPE = ""C.UTF-8"",. 	LANG = ""en_US.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. 	LANGUAGE = (unset),. 	LC_ALL = (unset),. 	LC_CTYPE = ""C.UTF-8"",. 	LANG = ""en_US.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). #[snip]. 2023-02-17 20:13:17.235641: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD	VN:1.6	SO:coordinate	pb:5.0.0. I0217 20:13:17.235805 23456243894080 genomics_reader.py:222] Reading /scratch4/path.to.mydir/pbmm2/aln13448198.pbmm2.bam with NativeSamReader. I0217 20:13:17.268698 23456243894080 make_examples_core.py:243] Task 18/64: Preparing inputs. 2023-02-17 20:13:17.371669: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD	VN:1.6	SO:coordinate	pb:5.0.0. I0217 20:13:17.371811 23456243894080 genomics_reader.py:222] Reading /scratch4/jwang8/hoyon/pbmm2/aln13448198.pbmm2.bam with NativeSamReader. I0217 20:13:17.376646 23456243894080 make_examples_core.py:243] Task 18/64: Common contigs are ['I', 'II', 'III', 'IV', 'V', 'X', 'MtDNA']. I0217 20:13:17.383403 23456243894080 make_examples_core.py:243] Task 18/64: Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. 2023-02-17 20:13:17.383631: I third_party/nucleus/io/sam_reader.cc:736] Setting HTS_OPT_BLOCK_SIZE to 134217728. 2023-02-17 20:13:17.235347: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD	VN:1.6	SO:coordinate	pb:5.0.0. #[snip]. I0217 20:13:17.702247 23456243894080 genomics_reade",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/614
https://github.com/google/deepvariant/issues/614:3069,safety,input,input,3069,"cale (""C""). #[snip]. 2023-02-17 20:13:17.235641: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD	VN:1.6	SO:coordinate	pb:5.0.0. I0217 20:13:17.235805 23456243894080 genomics_reader.py:222] Reading /scratch4/path.to.mydir/pbmm2/aln13448198.pbmm2.bam with NativeSamReader. I0217 20:13:17.268698 23456243894080 make_examples_core.py:243] Task 18/64: Preparing inputs. 2023-02-17 20:13:17.371669: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD	VN:1.6	SO:coordinate	pb:5.0.0. I0217 20:13:17.371811 23456243894080 genomics_reader.py:222] Reading /scratch4/jwang8/hoyon/pbmm2/aln13448198.pbmm2.bam with NativeSamReader. I0217 20:13:17.376646 23456243894080 make_examples_core.py:243] Task 18/64: Common contigs are ['I', 'II', 'III', 'IV', 'V', 'X', 'MtDNA']. I0217 20:13:17.383403 23456243894080 make_examples_core.py:243] Task 18/64: Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. 2023-02-17 20:13:17.383631: I third_party/nucleus/io/sam_reader.cc:736] Setting HTS_OPT_BLOCK_SIZE to 134217728. 2023-02-17 20:13:17.235347: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD	VN:1.6	SO:coordinate	pb:5.0.0. #[snip]. I0217 20:13:17.702247 23456243894080 genomics_reader.py:222] Reading /scratch4/jwang8/hoyon/pbmm2/aln13448198.pbmm2.bam with NativeSamReader. 2023-02-17 20:13:17.812898: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD	VN:1.6	SO:coordinate	pb:5.0.0. I0217 20:13:17.813058 23456243894080 genomics_reader.py:222] Reading /scratch4/path.to.mydir/pbmm2/aln13448198.pbmm2.bam with NativeSamReader. I0217 20:13:17.814152 23456243894080 make_examples_core.py:243] Task 11/64: Writing examples to /tmp/tmp1yvr59_z/make_examples.tfrecord-00011-of-00064.gz. I0217 20:13:17.814266 23456243894080 make_examples_core.py:2",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/614
https://github.com/google/deepvariant/issues/614:4111,safety,input,inputs,4111,"e reference you passed in with --ref. 2023-02-17 20:13:17.383631: I third_party/nucleus/io/sam_reader.cc:736] Setting HTS_OPT_BLOCK_SIZE to 134217728. 2023-02-17 20:13:17.235347: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD	VN:1.6	SO:coordinate	pb:5.0.0. #[snip]. I0217 20:13:17.702247 23456243894080 genomics_reader.py:222] Reading /scratch4/jwang8/hoyon/pbmm2/aln13448198.pbmm2.bam with NativeSamReader. 2023-02-17 20:13:17.812898: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD	VN:1.6	SO:coordinate	pb:5.0.0. I0217 20:13:17.813058 23456243894080 genomics_reader.py:222] Reading /scratch4/path.to.mydir/pbmm2/aln13448198.pbmm2.bam with NativeSamReader. I0217 20:13:17.814152 23456243894080 make_examples_core.py:243] Task 11/64: Writing examples to /tmp/tmp1yvr59_z/make_examples.tfrecord-00011-of-00064.gz. I0217 20:13:17.814266 23456243894080 make_examples_core.py:243] Task 11/64: Overhead for preparing inputs: 0 seconds. 2023-02-17 20:13:17.834667: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD	VN:1.6	SO:coordinate	pb:5.0.0. I0217 20:13:17.834807 23456243894080 genomics_reader.py:222] Reading /scratch4/path.to.mydir/pbmm2/aln13448198.pbmm2.bam with NativeSamReader. I0217 20:13:17.836419 23456243894080 make_examples_core.py:243] Task 18/64: Writing examples to /tmp/tmp1yvr59_z/make_examples.tfrecord-00018-of-00064.gz. I0217 20:13:17.836520 23456243894080 make_examples_core.py:243] Task 18/64: Overhead for preparing inputs: 0 seconds. 2023-02-17 20:13:17.850120: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD	VN:1.6	SO:coordinate	pb:5.0.0. I0217 20:13:17.850307 23456243894080 genomics_reader.py:222] Reading /scratch4/path.to.mydir/pbmm2/aln13448198.pbmm2.bam with NativeSamReader. I0217 20:13:17.851420 23456243894080 make_examples_core.py:243] Task 17/64: Writing examples to /tmp/tmp1yvr59_z/make_examples.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/614
https://github.com/google/deepvariant/issues/614:4676,safety,input,inputs,4676,"@HD	VN:1.6	SO:coordinate	pb:5.0.0. I0217 20:13:17.813058 23456243894080 genomics_reader.py:222] Reading /scratch4/path.to.mydir/pbmm2/aln13448198.pbmm2.bam with NativeSamReader. I0217 20:13:17.814152 23456243894080 make_examples_core.py:243] Task 11/64: Writing examples to /tmp/tmp1yvr59_z/make_examples.tfrecord-00011-of-00064.gz. I0217 20:13:17.814266 23456243894080 make_examples_core.py:243] Task 11/64: Overhead for preparing inputs: 0 seconds. 2023-02-17 20:13:17.834667: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD	VN:1.6	SO:coordinate	pb:5.0.0. I0217 20:13:17.834807 23456243894080 genomics_reader.py:222] Reading /scratch4/path.to.mydir/pbmm2/aln13448198.pbmm2.bam with NativeSamReader. I0217 20:13:17.836419 23456243894080 make_examples_core.py:243] Task 18/64: Writing examples to /tmp/tmp1yvr59_z/make_examples.tfrecord-00018-of-00064.gz. I0217 20:13:17.836520 23456243894080 make_examples_core.py:243] Task 18/64: Overhead for preparing inputs: 0 seconds. 2023-02-17 20:13:17.850120: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD	VN:1.6	SO:coordinate	pb:5.0.0. I0217 20:13:17.850307 23456243894080 genomics_reader.py:222] Reading /scratch4/path.to.mydir/pbmm2/aln13448198.pbmm2.bam with NativeSamReader. I0217 20:13:17.851420 23456243894080 make_examples_core.py:243] Task 17/64: Writing examples to /tmp/tmp1yvr59_z/make_examples.tfrecord-00017-of-00064.gz. I0217 20:13:17.851542 23456243894080 make_examples_core.py:243] Task 17/64: Overhead for preparing inputs: 0 seconds. 2023-02-17 20:13:17.829944: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD	VN:1.6	SO:coordinate	pb:5.0.0. I0217 20:13:17.830086 23456243894080 genomics_reader.py:222] Reading /scratch4/path.to.mydir/pbmm2/aln13448198.pbmm2.bam with NativeSamReader. I0217 20:13:17.831279 23456243894080 make_examples_core.py:243] Task 7/64: Writing examples to /tmp/tmp1yvr59_z/make_examples.t",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/614
https://github.com/google/deepvariant/issues/614:5241,safety,input,inputs,5241,"@HD	VN:1.6	SO:coordinate	pb:5.0.0. I0217 20:13:17.834807 23456243894080 genomics_reader.py:222] Reading /scratch4/path.to.mydir/pbmm2/aln13448198.pbmm2.bam with NativeSamReader. I0217 20:13:17.836419 23456243894080 make_examples_core.py:243] Task 18/64: Writing examples to /tmp/tmp1yvr59_z/make_examples.tfrecord-00018-of-00064.gz. I0217 20:13:17.836520 23456243894080 make_examples_core.py:243] Task 18/64: Overhead for preparing inputs: 0 seconds. 2023-02-17 20:13:17.850120: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD	VN:1.6	SO:coordinate	pb:5.0.0. I0217 20:13:17.850307 23456243894080 genomics_reader.py:222] Reading /scratch4/path.to.mydir/pbmm2/aln13448198.pbmm2.bam with NativeSamReader. I0217 20:13:17.851420 23456243894080 make_examples_core.py:243] Task 17/64: Writing examples to /tmp/tmp1yvr59_z/make_examples.tfrecord-00017-of-00064.gz. I0217 20:13:17.851542 23456243894080 make_examples_core.py:243] Task 17/64: Overhead for preparing inputs: 0 seconds. 2023-02-17 20:13:17.829944: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD	VN:1.6	SO:coordinate	pb:5.0.0. I0217 20:13:17.830086 23456243894080 genomics_reader.py:222] Reading /scratch4/path.to.mydir/pbmm2/aln13448198.pbmm2.bam with NativeSamReader. I0217 20:13:17.831279 23456243894080 make_examples_core.py:243] Task 7/64: Writing examples to /tmp/tmp1yvr59_z/make_examples.tfrecord-00007-of-00064.gz. I0217 20:13:17.831381 23456243894080 make_examples_core.py:243] Task 7/64: Overhead for preparing inputs: 0 seconds. #[snip]. I0217 20:13:55.184195 23456243894080 make_examples_core.py:243] Task 8/64: Skip phasing: len(candidates[main_sample]) is 19455. I0217 20:13:55.731674 23456243894080 make_examples_core.py:243] Task 22/64: Skip phasing: len(candidates[main_sample]) is 18977. I0217 20:13:55.541318 23456243894080 make_examples_core.py:243] Task 1/64: Skip phasing: len(candidates[main_sample]) is 19291. I0217 20:13:54.948142 23456",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/614
https://github.com/google/deepvariant/issues/614:5804,safety,input,inputs,5804,": @HD	VN:1.6	SO:coordinate	pb:5.0.0. I0217 20:13:17.850307 23456243894080 genomics_reader.py:222] Reading /scratch4/path.to.mydir/pbmm2/aln13448198.pbmm2.bam with NativeSamReader. I0217 20:13:17.851420 23456243894080 make_examples_core.py:243] Task 17/64: Writing examples to /tmp/tmp1yvr59_z/make_examples.tfrecord-00017-of-00064.gz. I0217 20:13:17.851542 23456243894080 make_examples_core.py:243] Task 17/64: Overhead for preparing inputs: 0 seconds. 2023-02-17 20:13:17.829944: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD	VN:1.6	SO:coordinate	pb:5.0.0. I0217 20:13:17.830086 23456243894080 genomics_reader.py:222] Reading /scratch4/path.to.mydir/pbmm2/aln13448198.pbmm2.bam with NativeSamReader. I0217 20:13:17.831279 23456243894080 make_examples_core.py:243] Task 7/64: Writing examples to /tmp/tmp1yvr59_z/make_examples.tfrecord-00007-of-00064.gz. I0217 20:13:17.831381 23456243894080 make_examples_core.py:243] Task 7/64: Overhead for preparing inputs: 0 seconds. #[snip]. I0217 20:13:55.184195 23456243894080 make_examples_core.py:243] Task 8/64: Skip phasing: len(candidates[main_sample]) is 19455. I0217 20:13:55.731674 23456243894080 make_examples_core.py:243] Task 22/64: Skip phasing: len(candidates[main_sample]) is 18977. I0217 20:13:55.541318 23456243894080 make_examples_core.py:243] Task 1/64: Skip phasing: len(candidates[main_sample]) is 19291. I0217 20:13:54.948142 23456243894080 make_examples_core.py:243] Task 19/64: Skip phasing: len(candidates[main_sample]) is 19012. I0217 20:13:55.139624 23456243894080 make_examples_core.py:243] Task 15/64: Skip phasing: len(candidates[main_sample]) is 18348. I0217 20:13:55.973984 23456243894080 make_examples_core.py:243] Task 18/64: Skip phasing: len(candidates[main_sample]) is 19340. I0217 20:13:56.415429 23456243894080 make_examples_core.py:243] Task 20/64: Skip phasing: len(candidates[main_sample]) is 19479. I0217 20:13:57.042794 23456243894080 make_examples_core.py:243] Task 0/64:",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/614
https://github.com/google/deepvariant/issues/614:12793,safety,error,error,12793,"s) [6737.31s elapsed]. I0218 10:46:36.864049 23456243894080 make_examples_core.py:243] Task 19/64: Skip phasing: len(candidates[main_sample]) is 20526. I0218 10:48:19.364838 23456243894080 make_examples_core.py:243] Task 2/64: 158555 candidates (173735 examples) [4091.74s elapsed]. I0218 10:48:45.881830 23456243894080 make_examples_core.py:243] Task 2/64: Skip phasing: len(candidates[main_sample]) is 14234. I0218 10:49:31.045118 23456243894080 make_examples_core.py:243] Task 13/64: 113956 candidates (125182 examples) [6317.67s elapsed]. I0218 10:50:33.895329 23456243894080 make_examples_core.py:243] Task 13/64: Skip phasing: len(candidates[main_sample]) is 18414. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /scratch4/path.to.mydir/genomes/c_elegans.PRJNA13758.WS245.genomic.fa --reads /scratch4/path.to.mydir/pbmm2/aln13448198.pbmm2.bam --examples /tmp/tmp1yvr59_z/make_examples.tfrecord@64.gz --add_hp_channel --alt_aligned_pileup diff_channels --max_reads_per_partition 600 --min_mapping_quality 1 --parse_sam_aux_fields --partition_size 25000 --phase_reads --pileup_image_width 199 --norealign_reads --sort_by_haplotypes --track_ref_reads --vsc_min_fraction_indels 0.12 --task 28. real	879m59.515s. user	632m52.969s. sys	6m20.594s. INFO: Cleaning up image... ```. I also ran more jobs using different numbers of cpu and mem using different bam files. One using 48 cpu and --mem-per-cpu=6G simply fizzled without any error message. These jobs are taking considerable core-hours, so troubleshooting is hard. I also wonder if I am using Deepvariant efficiently. On a side note, I got many Deepvariant failures with error messages like:. ```. Detected 1372 oom-kill event(s) in StepId=12049020.batch cgroup. Some of your processes may have been killed by the cgroup out-of-memory handler. ```. This seems to have been resolved by asking for maximum allowable memory. I am still curious about the memory requirement for successfully running Deepvariant.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/614
https://github.com/google/deepvariant/issues/614:12989,safety,error,error,12989,"s) [6737.31s elapsed]. I0218 10:46:36.864049 23456243894080 make_examples_core.py:243] Task 19/64: Skip phasing: len(candidates[main_sample]) is 20526. I0218 10:48:19.364838 23456243894080 make_examples_core.py:243] Task 2/64: 158555 candidates (173735 examples) [4091.74s elapsed]. I0218 10:48:45.881830 23456243894080 make_examples_core.py:243] Task 2/64: Skip phasing: len(candidates[main_sample]) is 14234. I0218 10:49:31.045118 23456243894080 make_examples_core.py:243] Task 13/64: 113956 candidates (125182 examples) [6317.67s elapsed]. I0218 10:50:33.895329 23456243894080 make_examples_core.py:243] Task 13/64: Skip phasing: len(candidates[main_sample]) is 18414. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /scratch4/path.to.mydir/genomes/c_elegans.PRJNA13758.WS245.genomic.fa --reads /scratch4/path.to.mydir/pbmm2/aln13448198.pbmm2.bam --examples /tmp/tmp1yvr59_z/make_examples.tfrecord@64.gz --add_hp_channel --alt_aligned_pileup diff_channels --max_reads_per_partition 600 --min_mapping_quality 1 --parse_sam_aux_fields --partition_size 25000 --phase_reads --pileup_image_width 199 --norealign_reads --sort_by_haplotypes --track_ref_reads --vsc_min_fraction_indels 0.12 --task 28. real	879m59.515s. user	632m52.969s. sys	6m20.594s. INFO: Cleaning up image... ```. I also ran more jobs using different numbers of cpu and mem using different bam files. One using 48 cpu and --mem-per-cpu=6G simply fizzled without any error message. These jobs are taking considerable core-hours, so troubleshooting is hard. I also wonder if I am using Deepvariant efficiently. On a side note, I got many Deepvariant failures with error messages like:. ```. Detected 1372 oom-kill event(s) in StepId=12049020.batch cgroup. Some of your processes may have been killed by the cgroup out-of-memory handler. ```. This seems to have been resolved by asking for maximum allowable memory. I am still curious about the memory requirement for successfully running Deepvariant.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/614
https://github.com/google/deepvariant/issues/614:13016,safety,Detect,Detected,13016,"s) [6737.31s elapsed]. I0218 10:46:36.864049 23456243894080 make_examples_core.py:243] Task 19/64: Skip phasing: len(candidates[main_sample]) is 20526. I0218 10:48:19.364838 23456243894080 make_examples_core.py:243] Task 2/64: 158555 candidates (173735 examples) [4091.74s elapsed]. I0218 10:48:45.881830 23456243894080 make_examples_core.py:243] Task 2/64: Skip phasing: len(candidates[main_sample]) is 14234. I0218 10:49:31.045118 23456243894080 make_examples_core.py:243] Task 13/64: 113956 candidates (125182 examples) [6317.67s elapsed]. I0218 10:50:33.895329 23456243894080 make_examples_core.py:243] Task 13/64: Skip phasing: len(candidates[main_sample]) is 18414. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /scratch4/path.to.mydir/genomes/c_elegans.PRJNA13758.WS245.genomic.fa --reads /scratch4/path.to.mydir/pbmm2/aln13448198.pbmm2.bam --examples /tmp/tmp1yvr59_z/make_examples.tfrecord@64.gz --add_hp_channel --alt_aligned_pileup diff_channels --max_reads_per_partition 600 --min_mapping_quality 1 --parse_sam_aux_fields --partition_size 25000 --phase_reads --pileup_image_width 199 --norealign_reads --sort_by_haplotypes --track_ref_reads --vsc_min_fraction_indels 0.12 --task 28. real	879m59.515s. user	632m52.969s. sys	6m20.594s. INFO: Cleaning up image... ```. I also ran more jobs using different numbers of cpu and mem using different bam files. One using 48 cpu and --mem-per-cpu=6G simply fizzled without any error message. These jobs are taking considerable core-hours, so troubleshooting is hard. I also wonder if I am using Deepvariant efficiently. On a side note, I got many Deepvariant failures with error messages like:. ```. Detected 1372 oom-kill event(s) in StepId=12049020.batch cgroup. Some of your processes may have been killed by the cgroup out-of-memory handler. ```. This seems to have been resolved by asking for maximum allowable memory. I am still curious about the memory requirement for successfully running Deepvariant.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/614
https://github.com/google/deepvariant/issues/614:1098,security,sandbox,sandbox,1098,"*Setup**. JHU Rockfish HPC. Singularity 3.8.7. singularity pull docker://google/deepvariant:1.4.0. Problematic data are PacBio (first gen). I have used Deepvariant with Illumina without problem, and I used PEPPER to process Ont data and PacBio Hifi data. I used pbmm2 to align fastq with all PacBio data. Command used to run:. ```. #!/bin/bash. #SBATCH --job-name=deep64_13448198. #SBATCH --time=24:00:00. #SBATCH --nodes=2. #SBATCH --ntasks-per-node=1. #SBATCH --cpus-per-task=32. #SBATCH --mem=0. ml anaconda. conda activate /data/path.to.mydir/deepvariant. singularity run --bind /scratch4/path.to.mydir/:/scratch4/path.to.mydir/ \. docker://google/deepvariant:1.4.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type PACBIO \. --ref c_elegans.PRJNA13758.WS245.genomic.fa \. --reads aln13448198.pbmm2.bam \. --output_vcf aln13448198.pbmm2.dv.vcf.gz \. --num_shards 64. ```. Here's a long snippet of slurm output:. ```. INFO: Using cached SIF image. INFO: Converting SIF file to temporary sandbox... I0217 20:13:14.117354 23456243894080 run_deepvariant.py:342] Re-using the directory for intermediate results in /tmp/tmp1yvr59_z. ***** Intermediate results will be written to /tmp/tmp1yvr59_z in docker. ****. ***** Running the command:*****. time seq 0 63 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref [snipped]. #[snip]. # this part is likely unimportant. perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. 	LANGUAGE = (unset),. 	LC_ALL = (unset),. 	LC_CTYPE = ""C.UTF-8"",. 	LANG = ""en_US.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. 	LANGUAGE = (unset),. 	LC_ALL = (unset),. 	LC_CTYPE = ""C.UTF-8"",. 	LANG = ""en_US.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). #[snip]. 2023-02-1",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/614
https://github.com/google/deepvariant/issues/614:13016,security,Detect,Detected,13016,"s) [6737.31s elapsed]. I0218 10:46:36.864049 23456243894080 make_examples_core.py:243] Task 19/64: Skip phasing: len(candidates[main_sample]) is 20526. I0218 10:48:19.364838 23456243894080 make_examples_core.py:243] Task 2/64: 158555 candidates (173735 examples) [4091.74s elapsed]. I0218 10:48:45.881830 23456243894080 make_examples_core.py:243] Task 2/64: Skip phasing: len(candidates[main_sample]) is 14234. I0218 10:49:31.045118 23456243894080 make_examples_core.py:243] Task 13/64: 113956 candidates (125182 examples) [6317.67s elapsed]. I0218 10:50:33.895329 23456243894080 make_examples_core.py:243] Task 13/64: Skip phasing: len(candidates[main_sample]) is 18414. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /scratch4/path.to.mydir/genomes/c_elegans.PRJNA13758.WS245.genomic.fa --reads /scratch4/path.to.mydir/pbmm2/aln13448198.pbmm2.bam --examples /tmp/tmp1yvr59_z/make_examples.tfrecord@64.gz --add_hp_channel --alt_aligned_pileup diff_channels --max_reads_per_partition 600 --min_mapping_quality 1 --parse_sam_aux_fields --partition_size 25000 --phase_reads --pileup_image_width 199 --norealign_reads --sort_by_haplotypes --track_ref_reads --vsc_min_fraction_indels 0.12 --task 28. real	879m59.515s. user	632m52.969s. sys	6m20.594s. INFO: Cleaning up image... ```. I also ran more jobs using different numbers of cpu and mem using different bam files. One using 48 cpu and --mem-per-cpu=6G simply fizzled without any error message. These jobs are taking considerable core-hours, so troubleshooting is hard. I also wonder if I am using Deepvariant efficiently. On a side note, I got many Deepvariant failures with error messages like:. ```. Detected 1372 oom-kill event(s) in StepId=12049020.batch cgroup. Some of your processes may have been killed by the cgroup out-of-memory handler. ```. This seems to have been resolved by asking for maximum allowable memory. I am still curious about the memory requirement for successfully running Deepvariant.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/614
https://github.com/google/deepvariant/issues/614:12766,testability,simpl,simply,12766,"s) [6737.31s elapsed]. I0218 10:46:36.864049 23456243894080 make_examples_core.py:243] Task 19/64: Skip phasing: len(candidates[main_sample]) is 20526. I0218 10:48:19.364838 23456243894080 make_examples_core.py:243] Task 2/64: 158555 candidates (173735 examples) [4091.74s elapsed]. I0218 10:48:45.881830 23456243894080 make_examples_core.py:243] Task 2/64: Skip phasing: len(candidates[main_sample]) is 14234. I0218 10:49:31.045118 23456243894080 make_examples_core.py:243] Task 13/64: 113956 candidates (125182 examples) [6317.67s elapsed]. I0218 10:50:33.895329 23456243894080 make_examples_core.py:243] Task 13/64: Skip phasing: len(candidates[main_sample]) is 18414. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /scratch4/path.to.mydir/genomes/c_elegans.PRJNA13758.WS245.genomic.fa --reads /scratch4/path.to.mydir/pbmm2/aln13448198.pbmm2.bam --examples /tmp/tmp1yvr59_z/make_examples.tfrecord@64.gz --add_hp_channel --alt_aligned_pileup diff_channels --max_reads_per_partition 600 --min_mapping_quality 1 --parse_sam_aux_fields --partition_size 25000 --phase_reads --pileup_image_width 199 --norealign_reads --sort_by_haplotypes --track_ref_reads --vsc_min_fraction_indels 0.12 --task 28. real	879m59.515s. user	632m52.969s. sys	6m20.594s. INFO: Cleaning up image... ```. I also ran more jobs using different numbers of cpu and mem using different bam files. One using 48 cpu and --mem-per-cpu=6G simply fizzled without any error message. These jobs are taking considerable core-hours, so troubleshooting is hard. I also wonder if I am using Deepvariant efficiently. On a side note, I got many Deepvariant failures with error messages like:. ```. Detected 1372 oom-kill event(s) in StepId=12049020.batch cgroup. Some of your processes may have been killed by the cgroup out-of-memory handler. ```. This seems to have been resolved by asking for maximum allowable memory. I am still curious about the memory requirement for successfully running Deepvariant.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/614
https://github.com/google/deepvariant/issues/614:26,usability,clear,clear,26,"Deepvariant fails without clear reason - with PacBio data; Deepvariant fails without clear reason. . **Setup**. JHU Rockfish HPC. Singularity 3.8.7. singularity pull docker://google/deepvariant:1.4.0. Problematic data are PacBio (first gen). I have used Deepvariant with Illumina without problem, and I used PEPPER to process Ont data and PacBio Hifi data. I used pbmm2 to align fastq with all PacBio data. Command used to run:. ```. #!/bin/bash. #SBATCH --job-name=deep64_13448198. #SBATCH --time=24:00:00. #SBATCH --nodes=2. #SBATCH --ntasks-per-node=1. #SBATCH --cpus-per-task=32. #SBATCH --mem=0. ml anaconda. conda activate /data/path.to.mydir/deepvariant. singularity run --bind /scratch4/path.to.mydir/:/scratch4/path.to.mydir/ \. docker://google/deepvariant:1.4.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type PACBIO \. --ref c_elegans.PRJNA13758.WS245.genomic.fa \. --reads aln13448198.pbmm2.bam \. --output_vcf aln13448198.pbmm2.dv.vcf.gz \. --num_shards 64. ```. Here's a long snippet of slurm output:. ```. INFO: Using cached SIF image. INFO: Converting SIF file to temporary sandbox... I0217 20:13:14.117354 23456243894080 run_deepvariant.py:342] Re-using the directory for intermediate results in /tmp/tmp1yvr59_z. ***** Intermediate results will be written to /tmp/tmp1yvr59_z in docker. ****. ***** Running the command:*****. time seq 0 63 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref [snipped]. #[snip]. # this part is likely unimportant. perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. 	LANGUAGE = (unset),. 	LC_ALL = (unset),. 	LC_CTYPE = ""C.UTF-8"",. 	LANG = ""en_US.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. 	LANGUAGE = (unset),. 	LC_ALL = (unset),. 	LC_CTYPE = ""C.UTF-8"",. 	LANG = ""en_US.UTF-8"". are supported and ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/614
https://github.com/google/deepvariant/issues/614:85,usability,clear,clear,85,"Deepvariant fails without clear reason - with PacBio data; Deepvariant fails without clear reason. . **Setup**. JHU Rockfish HPC. Singularity 3.8.7. singularity pull docker://google/deepvariant:1.4.0. Problematic data are PacBio (first gen). I have used Deepvariant with Illumina without problem, and I used PEPPER to process Ont data and PacBio Hifi data. I used pbmm2 to align fastq with all PacBio data. Command used to run:. ```. #!/bin/bash. #SBATCH --job-name=deep64_13448198. #SBATCH --time=24:00:00. #SBATCH --nodes=2. #SBATCH --ntasks-per-node=1. #SBATCH --cpus-per-task=32. #SBATCH --mem=0. ml anaconda. conda activate /data/path.to.mydir/deepvariant. singularity run --bind /scratch4/path.to.mydir/:/scratch4/path.to.mydir/ \. docker://google/deepvariant:1.4.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type PACBIO \. --ref c_elegans.PRJNA13758.WS245.genomic.fa \. --reads aln13448198.pbmm2.bam \. --output_vcf aln13448198.pbmm2.dv.vcf.gz \. --num_shards 64. ```. Here's a long snippet of slurm output:. ```. INFO: Using cached SIF image. INFO: Converting SIF file to temporary sandbox... I0217 20:13:14.117354 23456243894080 run_deepvariant.py:342] Re-using the directory for intermediate results in /tmp/tmp1yvr59_z. ***** Intermediate results will be written to /tmp/tmp1yvr59_z in docker. ****. ***** Running the command:*****. time seq 0 63 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref [snipped]. #[snip]. # this part is likely unimportant. perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. 	LANGUAGE = (unset),. 	LC_ALL = (unset),. 	LC_CTYPE = ""C.UTF-8"",. 	LANG = ""en_US.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. 	LANGUAGE = (unset),. 	LC_ALL = (unset),. 	LC_CTYPE = ""C.UTF-8"",. 	LANG = ""en_US.UTF-8"". are supported and ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/614
https://github.com/google/deepvariant/issues/614:407,usability,Command,Command,407,"Deepvariant fails without clear reason - with PacBio data; Deepvariant fails without clear reason. . **Setup**. JHU Rockfish HPC. Singularity 3.8.7. singularity pull docker://google/deepvariant:1.4.0. Problematic data are PacBio (first gen). I have used Deepvariant with Illumina without problem, and I used PEPPER to process Ont data and PacBio Hifi data. I used pbmm2 to align fastq with all PacBio data. Command used to run:. ```. #!/bin/bash. #SBATCH --job-name=deep64_13448198. #SBATCH --time=24:00:00. #SBATCH --nodes=2. #SBATCH --ntasks-per-node=1. #SBATCH --cpus-per-task=32. #SBATCH --mem=0. ml anaconda. conda activate /data/path.to.mydir/deepvariant. singularity run --bind /scratch4/path.to.mydir/:/scratch4/path.to.mydir/ \. docker://google/deepvariant:1.4.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type PACBIO \. --ref c_elegans.PRJNA13758.WS245.genomic.fa \. --reads aln13448198.pbmm2.bam \. --output_vcf aln13448198.pbmm2.dv.vcf.gz \. --num_shards 64. ```. Here's a long snippet of slurm output:. ```. INFO: Using cached SIF image. INFO: Converting SIF file to temporary sandbox... I0217 20:13:14.117354 23456243894080 run_deepvariant.py:342] Re-using the directory for intermediate results in /tmp/tmp1yvr59_z. ***** Intermediate results will be written to /tmp/tmp1yvr59_z in docker. ****. ***** Running the command:*****. time seq 0 63 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref [snipped]. #[snip]. # this part is likely unimportant. perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. 	LANGUAGE = (unset),. 	LC_ALL = (unset),. 	LC_CTYPE = ""C.UTF-8"",. 	LANG = ""en_US.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. 	LANGUAGE = (unset),. 	LC_ALL = (unset),. 	LC_CTYPE = ""C.UTF-8"",. 	LANG = ""en_US.UTF-8"". are supported and ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/614
https://github.com/google/deepvariant/issues/614:1337,usability,command,command,1337,"cBio Hifi data. I used pbmm2 to align fastq with all PacBio data. Command used to run:. ```. #!/bin/bash. #SBATCH --job-name=deep64_13448198. #SBATCH --time=24:00:00. #SBATCH --nodes=2. #SBATCH --ntasks-per-node=1. #SBATCH --cpus-per-task=32. #SBATCH --mem=0. ml anaconda. conda activate /data/path.to.mydir/deepvariant. singularity run --bind /scratch4/path.to.mydir/:/scratch4/path.to.mydir/ \. docker://google/deepvariant:1.4.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type PACBIO \. --ref c_elegans.PRJNA13758.WS245.genomic.fa \. --reads aln13448198.pbmm2.bam \. --output_vcf aln13448198.pbmm2.dv.vcf.gz \. --num_shards 64. ```. Here's a long snippet of slurm output:. ```. INFO: Using cached SIF image. INFO: Converting SIF file to temporary sandbox... I0217 20:13:14.117354 23456243894080 run_deepvariant.py:342] Re-using the directory for intermediate results in /tmp/tmp1yvr59_z. ***** Intermediate results will be written to /tmp/tmp1yvr59_z in docker. ****. ***** Running the command:*****. time seq 0 63 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref [snipped]. #[snip]. # this part is likely unimportant. perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. 	LANGUAGE = (unset),. 	LC_ALL = (unset),. 	LC_CTYPE = ""C.UTF-8"",. 	LANG = ""en_US.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. 	LANGUAGE = (unset),. 	LC_ALL = (unset),. 	LC_CTYPE = ""C.UTF-8"",. 	LANG = ""en_US.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). #[snip]. 2023-02-17 20:13:17.235641: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD	VN:1.6	SO:coordinate	pb:5.0.0. I0217 20:13:17.235805 23456243894080 genomics_reader.py:222] Reading /scratch4/path.to.mydir/pbmm2/",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/614
https://github.com/google/deepvariant/issues/614:1701,usability,support,supported,1701,"ir/:/scratch4/path.to.mydir/ \. docker://google/deepvariant:1.4.0 \. /opt/deepvariant/bin/run_deepvariant \. --model_type PACBIO \. --ref c_elegans.PRJNA13758.WS245.genomic.fa \. --reads aln13448198.pbmm2.bam \. --output_vcf aln13448198.pbmm2.dv.vcf.gz \. --num_shards 64. ```. Here's a long snippet of slurm output:. ```. INFO: Using cached SIF image. INFO: Converting SIF file to temporary sandbox... I0217 20:13:14.117354 23456243894080 run_deepvariant.py:342] Re-using the directory for intermediate results in /tmp/tmp1yvr59_z. ***** Intermediate results will be written to /tmp/tmp1yvr59_z in docker. ****. ***** Running the command:*****. time seq 0 63 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref [snipped]. #[snip]. # this part is likely unimportant. perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. 	LANGUAGE = (unset),. 	LC_ALL = (unset),. 	LC_CTYPE = ""C.UTF-8"",. 	LANG = ""en_US.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. 	LANGUAGE = (unset),. 	LC_ALL = (unset),. 	LC_CTYPE = ""C.UTF-8"",. 	LANG = ""en_US.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). #[snip]. 2023-02-17 20:13:17.235641: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD	VN:1.6	SO:coordinate	pb:5.0.0. I0217 20:13:17.235805 23456243894080 genomics_reader.py:222] Reading /scratch4/path.to.mydir/pbmm2/aln13448198.pbmm2.bam with NativeSamReader. I0217 20:13:17.268698 23456243894080 make_examples_core.py:243] Task 18/64: Preparing inputs. 2023-02-17 20:13:17.371669: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD	VN:1.6	SO:coordinate	pb:5.0.0. I0217 20:13:17.371811 23456243894080 genomics_reader.py:222] Reading /scratch4",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/614
https://github.com/google/deepvariant/issues/614:1986,usability,support,supported,1986,"a long snippet of slurm output:. ```. INFO: Using cached SIF image. INFO: Converting SIF file to temporary sandbox... I0217 20:13:14.117354 23456243894080 run_deepvariant.py:342] Re-using the directory for intermediate results in /tmp/tmp1yvr59_z. ***** Intermediate results will be written to /tmp/tmp1yvr59_z in docker. ****. ***** Running the command:*****. time seq 0 63 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref [snipped]. #[snip]. # this part is likely unimportant. perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. 	LANGUAGE = (unset),. 	LC_ALL = (unset),. 	LC_CTYPE = ""C.UTF-8"",. 	LANG = ""en_US.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. 	LANGUAGE = (unset),. 	LC_ALL = (unset),. 	LC_CTYPE = ""C.UTF-8"",. 	LANG = ""en_US.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). #[snip]. 2023-02-17 20:13:17.235641: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD	VN:1.6	SO:coordinate	pb:5.0.0. I0217 20:13:17.235805 23456243894080 genomics_reader.py:222] Reading /scratch4/path.to.mydir/pbmm2/aln13448198.pbmm2.bam with NativeSamReader. I0217 20:13:17.268698 23456243894080 make_examples_core.py:243] Task 18/64: Preparing inputs. 2023-02-17 20:13:17.371669: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD	VN:1.6	SO:coordinate	pb:5.0.0. I0217 20:13:17.371811 23456243894080 genomics_reader.py:222] Reading /scratch4/jwang8/hoyon/pbmm2/aln13448198.pbmm2.bam with NativeSamReader. I0217 20:13:17.376646 23456243894080 make_examples_core.py:243] Task 18/64: Common contigs are ['I', 'II', 'III', 'IV', 'V', 'X', 'MtDNA']. I0217 20:13:17.383403 23456243894080 make_examples_core.py:243] Task 18/64: Start",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/614
https://github.com/google/deepvariant/issues/614:2471,usability,input,inputs,2471,"ip]. # this part is likely unimportant. perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. 	LANGUAGE = (unset),. 	LC_ALL = (unset),. 	LC_CTYPE = ""C.UTF-8"",. 	LANG = ""en_US.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. 	LANGUAGE = (unset),. 	LC_ALL = (unset),. 	LC_CTYPE = ""C.UTF-8"",. 	LANG = ""en_US.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). #[snip]. 2023-02-17 20:13:17.235641: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD	VN:1.6	SO:coordinate	pb:5.0.0. I0217 20:13:17.235805 23456243894080 genomics_reader.py:222] Reading /scratch4/path.to.mydir/pbmm2/aln13448198.pbmm2.bam with NativeSamReader. I0217 20:13:17.268698 23456243894080 make_examples_core.py:243] Task 18/64: Preparing inputs. 2023-02-17 20:13:17.371669: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD	VN:1.6	SO:coordinate	pb:5.0.0. I0217 20:13:17.371811 23456243894080 genomics_reader.py:222] Reading /scratch4/jwang8/hoyon/pbmm2/aln13448198.pbmm2.bam with NativeSamReader. I0217 20:13:17.376646 23456243894080 make_examples_core.py:243] Task 18/64: Common contigs are ['I', 'II', 'III', 'IV', 'V', 'X', 'MtDNA']. I0217 20:13:17.383403 23456243894080 make_examples_core.py:243] Task 18/64: Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. 2023-02-17 20:13:17.383631: I third_party/nucleus/io/sam_reader.cc:736] Setting HTS_OPT_BLOCK_SIZE to 134217728. 2023-02-17 20:13:17.235347: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD	VN:1.6	SO:coordinate	pb:5.0.0. #[snip]. I0217 20:13:17.702247 23456243894080 genomics_reade",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/614
https://github.com/google/deepvariant/issues/614:3069,usability,input,input,3069,"cale (""C""). #[snip]. 2023-02-17 20:13:17.235641: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD	VN:1.6	SO:coordinate	pb:5.0.0. I0217 20:13:17.235805 23456243894080 genomics_reader.py:222] Reading /scratch4/path.to.mydir/pbmm2/aln13448198.pbmm2.bam with NativeSamReader. I0217 20:13:17.268698 23456243894080 make_examples_core.py:243] Task 18/64: Preparing inputs. 2023-02-17 20:13:17.371669: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD	VN:1.6	SO:coordinate	pb:5.0.0. I0217 20:13:17.371811 23456243894080 genomics_reader.py:222] Reading /scratch4/jwang8/hoyon/pbmm2/aln13448198.pbmm2.bam with NativeSamReader. I0217 20:13:17.376646 23456243894080 make_examples_core.py:243] Task 18/64: Common contigs are ['I', 'II', 'III', 'IV', 'V', 'X', 'MtDNA']. I0217 20:13:17.383403 23456243894080 make_examples_core.py:243] Task 18/64: Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. 2023-02-17 20:13:17.383631: I third_party/nucleus/io/sam_reader.cc:736] Setting HTS_OPT_BLOCK_SIZE to 134217728. 2023-02-17 20:13:17.235347: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD	VN:1.6	SO:coordinate	pb:5.0.0. #[snip]. I0217 20:13:17.702247 23456243894080 genomics_reader.py:222] Reading /scratch4/jwang8/hoyon/pbmm2/aln13448198.pbmm2.bam with NativeSamReader. 2023-02-17 20:13:17.812898: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD	VN:1.6	SO:coordinate	pb:5.0.0. I0217 20:13:17.813058 23456243894080 genomics_reader.py:222] Reading /scratch4/path.to.mydir/pbmm2/aln13448198.pbmm2.bam with NativeSamReader. I0217 20:13:17.814152 23456243894080 make_examples_core.py:243] Task 11/64: Writing examples to /tmp/tmp1yvr59_z/make_examples.tfrecord-00011-of-00064.gz. I0217 20:13:17.814266 23456243894080 make_examples_core.py:2",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/614
https://github.com/google/deepvariant/issues/614:4111,usability,input,inputs,4111,"e reference you passed in with --ref. 2023-02-17 20:13:17.383631: I third_party/nucleus/io/sam_reader.cc:736] Setting HTS_OPT_BLOCK_SIZE to 134217728. 2023-02-17 20:13:17.235347: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD	VN:1.6	SO:coordinate	pb:5.0.0. #[snip]. I0217 20:13:17.702247 23456243894080 genomics_reader.py:222] Reading /scratch4/jwang8/hoyon/pbmm2/aln13448198.pbmm2.bam with NativeSamReader. 2023-02-17 20:13:17.812898: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD	VN:1.6	SO:coordinate	pb:5.0.0. I0217 20:13:17.813058 23456243894080 genomics_reader.py:222] Reading /scratch4/path.to.mydir/pbmm2/aln13448198.pbmm2.bam with NativeSamReader. I0217 20:13:17.814152 23456243894080 make_examples_core.py:243] Task 11/64: Writing examples to /tmp/tmp1yvr59_z/make_examples.tfrecord-00011-of-00064.gz. I0217 20:13:17.814266 23456243894080 make_examples_core.py:243] Task 11/64: Overhead for preparing inputs: 0 seconds. 2023-02-17 20:13:17.834667: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD	VN:1.6	SO:coordinate	pb:5.0.0. I0217 20:13:17.834807 23456243894080 genomics_reader.py:222] Reading /scratch4/path.to.mydir/pbmm2/aln13448198.pbmm2.bam with NativeSamReader. I0217 20:13:17.836419 23456243894080 make_examples_core.py:243] Task 18/64: Writing examples to /tmp/tmp1yvr59_z/make_examples.tfrecord-00018-of-00064.gz. I0217 20:13:17.836520 23456243894080 make_examples_core.py:243] Task 18/64: Overhead for preparing inputs: 0 seconds. 2023-02-17 20:13:17.850120: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD	VN:1.6	SO:coordinate	pb:5.0.0. I0217 20:13:17.850307 23456243894080 genomics_reader.py:222] Reading /scratch4/path.to.mydir/pbmm2/aln13448198.pbmm2.bam with NativeSamReader. I0217 20:13:17.851420 23456243894080 make_examples_core.py:243] Task 17/64: Writing examples to /tmp/tmp1yvr59_z/make_examples.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/614
https://github.com/google/deepvariant/issues/614:4676,usability,input,inputs,4676,"@HD	VN:1.6	SO:coordinate	pb:5.0.0. I0217 20:13:17.813058 23456243894080 genomics_reader.py:222] Reading /scratch4/path.to.mydir/pbmm2/aln13448198.pbmm2.bam with NativeSamReader. I0217 20:13:17.814152 23456243894080 make_examples_core.py:243] Task 11/64: Writing examples to /tmp/tmp1yvr59_z/make_examples.tfrecord-00011-of-00064.gz. I0217 20:13:17.814266 23456243894080 make_examples_core.py:243] Task 11/64: Overhead for preparing inputs: 0 seconds. 2023-02-17 20:13:17.834667: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD	VN:1.6	SO:coordinate	pb:5.0.0. I0217 20:13:17.834807 23456243894080 genomics_reader.py:222] Reading /scratch4/path.to.mydir/pbmm2/aln13448198.pbmm2.bam with NativeSamReader. I0217 20:13:17.836419 23456243894080 make_examples_core.py:243] Task 18/64: Writing examples to /tmp/tmp1yvr59_z/make_examples.tfrecord-00018-of-00064.gz. I0217 20:13:17.836520 23456243894080 make_examples_core.py:243] Task 18/64: Overhead for preparing inputs: 0 seconds. 2023-02-17 20:13:17.850120: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD	VN:1.6	SO:coordinate	pb:5.0.0. I0217 20:13:17.850307 23456243894080 genomics_reader.py:222] Reading /scratch4/path.to.mydir/pbmm2/aln13448198.pbmm2.bam with NativeSamReader. I0217 20:13:17.851420 23456243894080 make_examples_core.py:243] Task 17/64: Writing examples to /tmp/tmp1yvr59_z/make_examples.tfrecord-00017-of-00064.gz. I0217 20:13:17.851542 23456243894080 make_examples_core.py:243] Task 17/64: Overhead for preparing inputs: 0 seconds. 2023-02-17 20:13:17.829944: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD	VN:1.6	SO:coordinate	pb:5.0.0. I0217 20:13:17.830086 23456243894080 genomics_reader.py:222] Reading /scratch4/path.to.mydir/pbmm2/aln13448198.pbmm2.bam with NativeSamReader. I0217 20:13:17.831279 23456243894080 make_examples_core.py:243] Task 7/64: Writing examples to /tmp/tmp1yvr59_z/make_examples.t",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/614
https://github.com/google/deepvariant/issues/614:5241,usability,input,inputs,5241,"@HD	VN:1.6	SO:coordinate	pb:5.0.0. I0217 20:13:17.834807 23456243894080 genomics_reader.py:222] Reading /scratch4/path.to.mydir/pbmm2/aln13448198.pbmm2.bam with NativeSamReader. I0217 20:13:17.836419 23456243894080 make_examples_core.py:243] Task 18/64: Writing examples to /tmp/tmp1yvr59_z/make_examples.tfrecord-00018-of-00064.gz. I0217 20:13:17.836520 23456243894080 make_examples_core.py:243] Task 18/64: Overhead for preparing inputs: 0 seconds. 2023-02-17 20:13:17.850120: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD	VN:1.6	SO:coordinate	pb:5.0.0. I0217 20:13:17.850307 23456243894080 genomics_reader.py:222] Reading /scratch4/path.to.mydir/pbmm2/aln13448198.pbmm2.bam with NativeSamReader. I0217 20:13:17.851420 23456243894080 make_examples_core.py:243] Task 17/64: Writing examples to /tmp/tmp1yvr59_z/make_examples.tfrecord-00017-of-00064.gz. I0217 20:13:17.851542 23456243894080 make_examples_core.py:243] Task 17/64: Overhead for preparing inputs: 0 seconds. 2023-02-17 20:13:17.829944: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD	VN:1.6	SO:coordinate	pb:5.0.0. I0217 20:13:17.830086 23456243894080 genomics_reader.py:222] Reading /scratch4/path.to.mydir/pbmm2/aln13448198.pbmm2.bam with NativeSamReader. I0217 20:13:17.831279 23456243894080 make_examples_core.py:243] Task 7/64: Writing examples to /tmp/tmp1yvr59_z/make_examples.tfrecord-00007-of-00064.gz. I0217 20:13:17.831381 23456243894080 make_examples_core.py:243] Task 7/64: Overhead for preparing inputs: 0 seconds. #[snip]. I0217 20:13:55.184195 23456243894080 make_examples_core.py:243] Task 8/64: Skip phasing: len(candidates[main_sample]) is 19455. I0217 20:13:55.731674 23456243894080 make_examples_core.py:243] Task 22/64: Skip phasing: len(candidates[main_sample]) is 18977. I0217 20:13:55.541318 23456243894080 make_examples_core.py:243] Task 1/64: Skip phasing: len(candidates[main_sample]) is 19291. I0217 20:13:54.948142 23456",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/614
https://github.com/google/deepvariant/issues/614:5804,usability,input,inputs,5804,": @HD	VN:1.6	SO:coordinate	pb:5.0.0. I0217 20:13:17.850307 23456243894080 genomics_reader.py:222] Reading /scratch4/path.to.mydir/pbmm2/aln13448198.pbmm2.bam with NativeSamReader. I0217 20:13:17.851420 23456243894080 make_examples_core.py:243] Task 17/64: Writing examples to /tmp/tmp1yvr59_z/make_examples.tfrecord-00017-of-00064.gz. I0217 20:13:17.851542 23456243894080 make_examples_core.py:243] Task 17/64: Overhead for preparing inputs: 0 seconds. 2023-02-17 20:13:17.829944: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD	VN:1.6	SO:coordinate	pb:5.0.0. I0217 20:13:17.830086 23456243894080 genomics_reader.py:222] Reading /scratch4/path.to.mydir/pbmm2/aln13448198.pbmm2.bam with NativeSamReader. I0217 20:13:17.831279 23456243894080 make_examples_core.py:243] Task 7/64: Writing examples to /tmp/tmp1yvr59_z/make_examples.tfrecord-00007-of-00064.gz. I0217 20:13:17.831381 23456243894080 make_examples_core.py:243] Task 7/64: Overhead for preparing inputs: 0 seconds. #[snip]. I0217 20:13:55.184195 23456243894080 make_examples_core.py:243] Task 8/64: Skip phasing: len(candidates[main_sample]) is 19455. I0217 20:13:55.731674 23456243894080 make_examples_core.py:243] Task 22/64: Skip phasing: len(candidates[main_sample]) is 18977. I0217 20:13:55.541318 23456243894080 make_examples_core.py:243] Task 1/64: Skip phasing: len(candidates[main_sample]) is 19291. I0217 20:13:54.948142 23456243894080 make_examples_core.py:243] Task 19/64: Skip phasing: len(candidates[main_sample]) is 19012. I0217 20:13:55.139624 23456243894080 make_examples_core.py:243] Task 15/64: Skip phasing: len(candidates[main_sample]) is 18348. I0217 20:13:55.973984 23456243894080 make_examples_core.py:243] Task 18/64: Skip phasing: len(candidates[main_sample]) is 19340. I0217 20:13:56.415429 23456243894080 make_examples_core.py:243] Task 20/64: Skip phasing: len(candidates[main_sample]) is 19479. I0217 20:13:57.042794 23456243894080 make_examples_core.py:243] Task 0/64:",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/614
https://github.com/google/deepvariant/issues/614:12576,usability,user,user,12576,"s) [6737.31s elapsed]. I0218 10:46:36.864049 23456243894080 make_examples_core.py:243] Task 19/64: Skip phasing: len(candidates[main_sample]) is 20526. I0218 10:48:19.364838 23456243894080 make_examples_core.py:243] Task 2/64: 158555 candidates (173735 examples) [4091.74s elapsed]. I0218 10:48:45.881830 23456243894080 make_examples_core.py:243] Task 2/64: Skip phasing: len(candidates[main_sample]) is 14234. I0218 10:49:31.045118 23456243894080 make_examples_core.py:243] Task 13/64: 113956 candidates (125182 examples) [6317.67s elapsed]. I0218 10:50:33.895329 23456243894080 make_examples_core.py:243] Task 13/64: Skip phasing: len(candidates[main_sample]) is 18414. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /scratch4/path.to.mydir/genomes/c_elegans.PRJNA13758.WS245.genomic.fa --reads /scratch4/path.to.mydir/pbmm2/aln13448198.pbmm2.bam --examples /tmp/tmp1yvr59_z/make_examples.tfrecord@64.gz --add_hp_channel --alt_aligned_pileup diff_channels --max_reads_per_partition 600 --min_mapping_quality 1 --parse_sam_aux_fields --partition_size 25000 --phase_reads --pileup_image_width 199 --norealign_reads --sort_by_haplotypes --track_ref_reads --vsc_min_fraction_indels 0.12 --task 28. real	879m59.515s. user	632m52.969s. sys	6m20.594s. INFO: Cleaning up image... ```. I also ran more jobs using different numbers of cpu and mem using different bam files. One using 48 cpu and --mem-per-cpu=6G simply fizzled without any error message. These jobs are taking considerable core-hours, so troubleshooting is hard. I also wonder if I am using Deepvariant efficiently. On a side note, I got many Deepvariant failures with error messages like:. ```. Detected 1372 oom-kill event(s) in StepId=12049020.batch cgroup. Some of your processes may have been killed by the cgroup out-of-memory handler. ```. This seems to have been resolved by asking for maximum allowable memory. I am still curious about the memory requirement for successfully running Deepvariant.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/614
https://github.com/google/deepvariant/issues/614:12766,usability,simpl,simply,12766,"s) [6737.31s elapsed]. I0218 10:46:36.864049 23456243894080 make_examples_core.py:243] Task 19/64: Skip phasing: len(candidates[main_sample]) is 20526. I0218 10:48:19.364838 23456243894080 make_examples_core.py:243] Task 2/64: 158555 candidates (173735 examples) [4091.74s elapsed]. I0218 10:48:45.881830 23456243894080 make_examples_core.py:243] Task 2/64: Skip phasing: len(candidates[main_sample]) is 14234. I0218 10:49:31.045118 23456243894080 make_examples_core.py:243] Task 13/64: 113956 candidates (125182 examples) [6317.67s elapsed]. I0218 10:50:33.895329 23456243894080 make_examples_core.py:243] Task 13/64: Skip phasing: len(candidates[main_sample]) is 18414. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /scratch4/path.to.mydir/genomes/c_elegans.PRJNA13758.WS245.genomic.fa --reads /scratch4/path.to.mydir/pbmm2/aln13448198.pbmm2.bam --examples /tmp/tmp1yvr59_z/make_examples.tfrecord@64.gz --add_hp_channel --alt_aligned_pileup diff_channels --max_reads_per_partition 600 --min_mapping_quality 1 --parse_sam_aux_fields --partition_size 25000 --phase_reads --pileup_image_width 199 --norealign_reads --sort_by_haplotypes --track_ref_reads --vsc_min_fraction_indels 0.12 --task 28. real	879m59.515s. user	632m52.969s. sys	6m20.594s. INFO: Cleaning up image... ```. I also ran more jobs using different numbers of cpu and mem using different bam files. One using 48 cpu and --mem-per-cpu=6G simply fizzled without any error message. These jobs are taking considerable core-hours, so troubleshooting is hard. I also wonder if I am using Deepvariant efficiently. On a side note, I got many Deepvariant failures with error messages like:. ```. Detected 1372 oom-kill event(s) in StepId=12049020.batch cgroup. Some of your processes may have been killed by the cgroup out-of-memory handler. ```. This seems to have been resolved by asking for maximum allowable memory. I am still curious about the memory requirement for successfully running Deepvariant.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/614
https://github.com/google/deepvariant/issues/614:12793,usability,error,error,12793,"s) [6737.31s elapsed]. I0218 10:46:36.864049 23456243894080 make_examples_core.py:243] Task 19/64: Skip phasing: len(candidates[main_sample]) is 20526. I0218 10:48:19.364838 23456243894080 make_examples_core.py:243] Task 2/64: 158555 candidates (173735 examples) [4091.74s elapsed]. I0218 10:48:45.881830 23456243894080 make_examples_core.py:243] Task 2/64: Skip phasing: len(candidates[main_sample]) is 14234. I0218 10:49:31.045118 23456243894080 make_examples_core.py:243] Task 13/64: 113956 candidates (125182 examples) [6317.67s elapsed]. I0218 10:50:33.895329 23456243894080 make_examples_core.py:243] Task 13/64: Skip phasing: len(candidates[main_sample]) is 18414. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /scratch4/path.to.mydir/genomes/c_elegans.PRJNA13758.WS245.genomic.fa --reads /scratch4/path.to.mydir/pbmm2/aln13448198.pbmm2.bam --examples /tmp/tmp1yvr59_z/make_examples.tfrecord@64.gz --add_hp_channel --alt_aligned_pileup diff_channels --max_reads_per_partition 600 --min_mapping_quality 1 --parse_sam_aux_fields --partition_size 25000 --phase_reads --pileup_image_width 199 --norealign_reads --sort_by_haplotypes --track_ref_reads --vsc_min_fraction_indels 0.12 --task 28. real	879m59.515s. user	632m52.969s. sys	6m20.594s. INFO: Cleaning up image... ```. I also ran more jobs using different numbers of cpu and mem using different bam files. One using 48 cpu and --mem-per-cpu=6G simply fizzled without any error message. These jobs are taking considerable core-hours, so troubleshooting is hard. I also wonder if I am using Deepvariant efficiently. On a side note, I got many Deepvariant failures with error messages like:. ```. Detected 1372 oom-kill event(s) in StepId=12049020.batch cgroup. Some of your processes may have been killed by the cgroup out-of-memory handler. ```. This seems to have been resolved by asking for maximum allowable memory. I am still curious about the memory requirement for successfully running Deepvariant.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/614
https://github.com/google/deepvariant/issues/614:12923,usability,efficien,efficiently,12923,"s) [6737.31s elapsed]. I0218 10:46:36.864049 23456243894080 make_examples_core.py:243] Task 19/64: Skip phasing: len(candidates[main_sample]) is 20526. I0218 10:48:19.364838 23456243894080 make_examples_core.py:243] Task 2/64: 158555 candidates (173735 examples) [4091.74s elapsed]. I0218 10:48:45.881830 23456243894080 make_examples_core.py:243] Task 2/64: Skip phasing: len(candidates[main_sample]) is 14234. I0218 10:49:31.045118 23456243894080 make_examples_core.py:243] Task 13/64: 113956 candidates (125182 examples) [6317.67s elapsed]. I0218 10:50:33.895329 23456243894080 make_examples_core.py:243] Task 13/64: Skip phasing: len(candidates[main_sample]) is 18414. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /scratch4/path.to.mydir/genomes/c_elegans.PRJNA13758.WS245.genomic.fa --reads /scratch4/path.to.mydir/pbmm2/aln13448198.pbmm2.bam --examples /tmp/tmp1yvr59_z/make_examples.tfrecord@64.gz --add_hp_channel --alt_aligned_pileup diff_channels --max_reads_per_partition 600 --min_mapping_quality 1 --parse_sam_aux_fields --partition_size 25000 --phase_reads --pileup_image_width 199 --norealign_reads --sort_by_haplotypes --track_ref_reads --vsc_min_fraction_indels 0.12 --task 28. real	879m59.515s. user	632m52.969s. sys	6m20.594s. INFO: Cleaning up image... ```. I also ran more jobs using different numbers of cpu and mem using different bam files. One using 48 cpu and --mem-per-cpu=6G simply fizzled without any error message. These jobs are taking considerable core-hours, so troubleshooting is hard. I also wonder if I am using Deepvariant efficiently. On a side note, I got many Deepvariant failures with error messages like:. ```. Detected 1372 oom-kill event(s) in StepId=12049020.batch cgroup. Some of your processes may have been killed by the cgroup out-of-memory handler. ```. This seems to have been resolved by asking for maximum allowable memory. I am still curious about the memory requirement for successfully running Deepvariant.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/614
https://github.com/google/deepvariant/issues/614:12989,usability,error,error,12989,"s) [6737.31s elapsed]. I0218 10:46:36.864049 23456243894080 make_examples_core.py:243] Task 19/64: Skip phasing: len(candidates[main_sample]) is 20526. I0218 10:48:19.364838 23456243894080 make_examples_core.py:243] Task 2/64: 158555 candidates (173735 examples) [4091.74s elapsed]. I0218 10:48:45.881830 23456243894080 make_examples_core.py:243] Task 2/64: Skip phasing: len(candidates[main_sample]) is 14234. I0218 10:49:31.045118 23456243894080 make_examples_core.py:243] Task 13/64: 113956 candidates (125182 examples) [6317.67s elapsed]. I0218 10:50:33.895329 23456243894080 make_examples_core.py:243] Task 13/64: Skip phasing: len(candidates[main_sample]) is 18414. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /scratch4/path.to.mydir/genomes/c_elegans.PRJNA13758.WS245.genomic.fa --reads /scratch4/path.to.mydir/pbmm2/aln13448198.pbmm2.bam --examples /tmp/tmp1yvr59_z/make_examples.tfrecord@64.gz --add_hp_channel --alt_aligned_pileup diff_channels --max_reads_per_partition 600 --min_mapping_quality 1 --parse_sam_aux_fields --partition_size 25000 --phase_reads --pileup_image_width 199 --norealign_reads --sort_by_haplotypes --track_ref_reads --vsc_min_fraction_indels 0.12 --task 28. real	879m59.515s. user	632m52.969s. sys	6m20.594s. INFO: Cleaning up image... ```. I also ran more jobs using different numbers of cpu and mem using different bam files. One using 48 cpu and --mem-per-cpu=6G simply fizzled without any error message. These jobs are taking considerable core-hours, so troubleshooting is hard. I also wonder if I am using Deepvariant efficiently. On a side note, I got many Deepvariant failures with error messages like:. ```. Detected 1372 oom-kill event(s) in StepId=12049020.batch cgroup. Some of your processes may have been killed by the cgroup out-of-memory handler. ```. This seems to have been resolved by asking for maximum allowable memory. I am still curious about the memory requirement for successfully running Deepvariant.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/614
https://github.com/google/deepvariant/issues/614:13146,usability,memor,memory,13146,"s) [6737.31s elapsed]. I0218 10:46:36.864049 23456243894080 make_examples_core.py:243] Task 19/64: Skip phasing: len(candidates[main_sample]) is 20526. I0218 10:48:19.364838 23456243894080 make_examples_core.py:243] Task 2/64: 158555 candidates (173735 examples) [4091.74s elapsed]. I0218 10:48:45.881830 23456243894080 make_examples_core.py:243] Task 2/64: Skip phasing: len(candidates[main_sample]) is 14234. I0218 10:49:31.045118 23456243894080 make_examples_core.py:243] Task 13/64: 113956 candidates (125182 examples) [6317.67s elapsed]. I0218 10:50:33.895329 23456243894080 make_examples_core.py:243] Task 13/64: Skip phasing: len(candidates[main_sample]) is 18414. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /scratch4/path.to.mydir/genomes/c_elegans.PRJNA13758.WS245.genomic.fa --reads /scratch4/path.to.mydir/pbmm2/aln13448198.pbmm2.bam --examples /tmp/tmp1yvr59_z/make_examples.tfrecord@64.gz --add_hp_channel --alt_aligned_pileup diff_channels --max_reads_per_partition 600 --min_mapping_quality 1 --parse_sam_aux_fields --partition_size 25000 --phase_reads --pileup_image_width 199 --norealign_reads --sort_by_haplotypes --track_ref_reads --vsc_min_fraction_indels 0.12 --task 28. real	879m59.515s. user	632m52.969s. sys	6m20.594s. INFO: Cleaning up image... ```. I also ran more jobs using different numbers of cpu and mem using different bam files. One using 48 cpu and --mem-per-cpu=6G simply fizzled without any error message. These jobs are taking considerable core-hours, so troubleshooting is hard. I also wonder if I am using Deepvariant efficiently. On a side note, I got many Deepvariant failures with error messages like:. ```. Detected 1372 oom-kill event(s) in StepId=12049020.batch cgroup. Some of your processes may have been killed by the cgroup out-of-memory handler. ```. This seems to have been resolved by asking for maximum allowable memory. I am still curious about the memory requirement for successfully running Deepvariant.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/614
https://github.com/google/deepvariant/issues/614:13232,usability,memor,memory,13232,"s) [6737.31s elapsed]. I0218 10:46:36.864049 23456243894080 make_examples_core.py:243] Task 19/64: Skip phasing: len(candidates[main_sample]) is 20526. I0218 10:48:19.364838 23456243894080 make_examples_core.py:243] Task 2/64: 158555 candidates (173735 examples) [4091.74s elapsed]. I0218 10:48:45.881830 23456243894080 make_examples_core.py:243] Task 2/64: Skip phasing: len(candidates[main_sample]) is 14234. I0218 10:49:31.045118 23456243894080 make_examples_core.py:243] Task 13/64: 113956 candidates (125182 examples) [6317.67s elapsed]. I0218 10:50:33.895329 23456243894080 make_examples_core.py:243] Task 13/64: Skip phasing: len(candidates[main_sample]) is 18414. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /scratch4/path.to.mydir/genomes/c_elegans.PRJNA13758.WS245.genomic.fa --reads /scratch4/path.to.mydir/pbmm2/aln13448198.pbmm2.bam --examples /tmp/tmp1yvr59_z/make_examples.tfrecord@64.gz --add_hp_channel --alt_aligned_pileup diff_channels --max_reads_per_partition 600 --min_mapping_quality 1 --parse_sam_aux_fields --partition_size 25000 --phase_reads --pileup_image_width 199 --norealign_reads --sort_by_haplotypes --track_ref_reads --vsc_min_fraction_indels 0.12 --task 28. real	879m59.515s. user	632m52.969s. sys	6m20.594s. INFO: Cleaning up image... ```. I also ran more jobs using different numbers of cpu and mem using different bam files. One using 48 cpu and --mem-per-cpu=6G simply fizzled without any error message. These jobs are taking considerable core-hours, so troubleshooting is hard. I also wonder if I am using Deepvariant efficiently. On a side note, I got many Deepvariant failures with error messages like:. ```. Detected 1372 oom-kill event(s) in StepId=12049020.batch cgroup. Some of your processes may have been killed by the cgroup out-of-memory handler. ```. This seems to have been resolved by asking for maximum allowable memory. I am still curious about the memory requirement for successfully running Deepvariant.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/614
https://github.com/google/deepvariant/issues/614:13269,usability,memor,memory,13269,"s) [6737.31s elapsed]. I0218 10:46:36.864049 23456243894080 make_examples_core.py:243] Task 19/64: Skip phasing: len(candidates[main_sample]) is 20526. I0218 10:48:19.364838 23456243894080 make_examples_core.py:243] Task 2/64: 158555 candidates (173735 examples) [4091.74s elapsed]. I0218 10:48:45.881830 23456243894080 make_examples_core.py:243] Task 2/64: Skip phasing: len(candidates[main_sample]) is 14234. I0218 10:49:31.045118 23456243894080 make_examples_core.py:243] Task 13/64: 113956 candidates (125182 examples) [6317.67s elapsed]. I0218 10:50:33.895329 23456243894080 make_examples_core.py:243] Task 13/64: Skip phasing: len(candidates[main_sample]) is 18414. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /scratch4/path.to.mydir/genomes/c_elegans.PRJNA13758.WS245.genomic.fa --reads /scratch4/path.to.mydir/pbmm2/aln13448198.pbmm2.bam --examples /tmp/tmp1yvr59_z/make_examples.tfrecord@64.gz --add_hp_channel --alt_aligned_pileup diff_channels --max_reads_per_partition 600 --min_mapping_quality 1 --parse_sam_aux_fields --partition_size 25000 --phase_reads --pileup_image_width 199 --norealign_reads --sort_by_haplotypes --track_ref_reads --vsc_min_fraction_indels 0.12 --task 28. real	879m59.515s. user	632m52.969s. sys	6m20.594s. INFO: Cleaning up image... ```. I also ran more jobs using different numbers of cpu and mem using different bam files. One using 48 cpu and --mem-per-cpu=6G simply fizzled without any error message. These jobs are taking considerable core-hours, so troubleshooting is hard. I also wonder if I am using Deepvariant efficiently. On a side note, I got many Deepvariant failures with error messages like:. ```. Detected 1372 oom-kill event(s) in StepId=12049020.batch cgroup. Some of your processes may have been killed by the cgroup out-of-memory handler. ```. This seems to have been resolved by asking for maximum allowable memory. I am still curious about the memory requirement for successfully running Deepvariant.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/614
https://github.com/google/deepvariant/issues/616:104,availability,down,download,104,"As bed lengthened, SNP performed better, but indel on the contrary; Hello！ I run the rawdata of NA12878 download from [NCBI SRA](https://trace.ncbi.nlm.nih.gov/Traces/?view=run_browser&acc=ERR1905890&display=data-access) []() and I got it's capture kit is Agilent_V5. First, I run the **oqfe protocol** to align, and the output CRAM as the input of Deepvariant. I run Deepvariant in WES model **3 times**, the first one didn't have --region parameter, the second one use a adding **50** bp buffer on each side of the custom target regions in BED format, the last one is adding **100** bp. Next, I got the **truth** Benchmarking variant calls form GIAB and it's confident call regions to run hap.py. The final outcome is very good, but I find a detail didn't make sense: as the bed lengthened，the SNP performed better and better, but INDEL on the contrary that it's getting worse since the number is decreasing, but I think it is making sense that the number becomes more as the bed gets longer, just like SNP. As shown in the figure below. ![image](https://user-images.githubusercontent.com/63234787/220512170-4506359f-8c72-44ff-8585-e4357f24c20b.png). Can you give me a detailed explanation of this detail？ Thank you very much！ . Finally, thank you very much for developing such a great tool！.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/616
https://github.com/google/deepvariant/issues/616:387,energy efficiency,model,model,387,"As bed lengthened, SNP performed better, but indel on the contrary; Hello！ I run the rawdata of NA12878 download from [NCBI SRA](https://trace.ncbi.nlm.nih.gov/Traces/?view=run_browser&acc=ERR1905890&display=data-access) []() and I got it's capture kit is Agilent_V5. First, I run the **oqfe protocol** to align, and the output CRAM as the input of Deepvariant. I run Deepvariant in WES model **3 times**, the first one didn't have --region parameter, the second one use a adding **50** bp buffer on each side of the custom target regions in BED format, the last one is adding **100** bp. Next, I got the **truth** Benchmarking variant calls form GIAB and it's confident call regions to run hap.py. The final outcome is very good, but I find a detail didn't make sense: as the bed lengthened，the SNP performed better and better, but INDEL on the contrary that it's getting worse since the number is decreasing, but I think it is making sense that the number becomes more as the bed gets longer, just like SNP. As shown in the figure below. ![image](https://user-images.githubusercontent.com/63234787/220512170-4506359f-8c72-44ff-8585-e4357f24c20b.png). Can you give me a detailed explanation of this detail？ Thank you very much！ . Finally, thank you very much for developing such a great tool！.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/616
https://github.com/google/deepvariant/issues/616:292,integrability,protocol,protocol,292,"As bed lengthened, SNP performed better, but indel on the contrary; Hello！ I run the rawdata of NA12878 download from [NCBI SRA](https://trace.ncbi.nlm.nih.gov/Traces/?view=run_browser&acc=ERR1905890&display=data-access) []() and I got it's capture kit is Agilent_V5. First, I run the **oqfe protocol** to align, and the output CRAM as the input of Deepvariant. I run Deepvariant in WES model **3 times**, the first one didn't have --region parameter, the second one use a adding **50** bp buffer on each side of the custom target regions in BED format, the last one is adding **100** bp. Next, I got the **truth** Benchmarking variant calls form GIAB and it's confident call regions to run hap.py. The final outcome is very good, but I find a detail didn't make sense: as the bed lengthened，the SNP performed better and better, but INDEL on the contrary that it's getting worse since the number is decreasing, but I think it is making sense that the number becomes more as the bed gets longer, just like SNP. As shown in the figure below. ![image](https://user-images.githubusercontent.com/63234787/220512170-4506359f-8c72-44ff-8585-e4357f24c20b.png). Can you give me a detailed explanation of this detail？ Thank you very much！ . Finally, thank you very much for developing such a great tool！.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/616
https://github.com/google/deepvariant/issues/616:490,integrability,buffer,buffer,490,"As bed lengthened, SNP performed better, but indel on the contrary; Hello！ I run the rawdata of NA12878 download from [NCBI SRA](https://trace.ncbi.nlm.nih.gov/Traces/?view=run_browser&acc=ERR1905890&display=data-access) []() and I got it's capture kit is Agilent_V5. First, I run the **oqfe protocol** to align, and the output CRAM as the input of Deepvariant. I run Deepvariant in WES model **3 times**, the first one didn't have --region parameter, the second one use a adding **50** bp buffer on each side of the custom target regions in BED format, the last one is adding **100** bp. Next, I got the **truth** Benchmarking variant calls form GIAB and it's confident call regions to run hap.py. The final outcome is very good, but I find a detail didn't make sense: as the bed lengthened，the SNP performed better and better, but INDEL on the contrary that it's getting worse since the number is decreasing, but I think it is making sense that the number becomes more as the bed gets longer, just like SNP. As shown in the figure below. ![image](https://user-images.githubusercontent.com/63234787/220512170-4506359f-8c72-44ff-8585-e4357f24c20b.png). Can you give me a detailed explanation of this detail？ Thank you very much！ . Finally, thank you very much for developing such a great tool！.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/616
https://github.com/google/deepvariant/issues/616:292,interoperability,protocol,protocol,292,"As bed lengthened, SNP performed better, but indel on the contrary; Hello！ I run the rawdata of NA12878 download from [NCBI SRA](https://trace.ncbi.nlm.nih.gov/Traces/?view=run_browser&acc=ERR1905890&display=data-access) []() and I got it's capture kit is Agilent_V5. First, I run the **oqfe protocol** to align, and the output CRAM as the input of Deepvariant. I run Deepvariant in WES model **3 times**, the first one didn't have --region parameter, the second one use a adding **50** bp buffer on each side of the custom target regions in BED format, the last one is adding **100** bp. Next, I got the **truth** Benchmarking variant calls form GIAB and it's confident call regions to run hap.py. The final outcome is very good, but I find a detail didn't make sense: as the bed lengthened，the SNP performed better and better, but INDEL on the contrary that it's getting worse since the number is decreasing, but I think it is making sense that the number becomes more as the bed gets longer, just like SNP. As shown in the figure below. ![image](https://user-images.githubusercontent.com/63234787/220512170-4506359f-8c72-44ff-8585-e4357f24c20b.png). Can you give me a detailed explanation of this detail？ Thank you very much！ . Finally, thank you very much for developing such a great tool！.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/616
https://github.com/google/deepvariant/issues/616:546,interoperability,format,format,546,"As bed lengthened, SNP performed better, but indel on the contrary; Hello！ I run the rawdata of NA12878 download from [NCBI SRA](https://trace.ncbi.nlm.nih.gov/Traces/?view=run_browser&acc=ERR1905890&display=data-access) []() and I got it's capture kit is Agilent_V5. First, I run the **oqfe protocol** to align, and the output CRAM as the input of Deepvariant. I run Deepvariant in WES model **3 times**, the first one didn't have --region parameter, the second one use a adding **50** bp buffer on each side of the custom target regions in BED format, the last one is adding **100** bp. Next, I got the **truth** Benchmarking variant calls form GIAB and it's confident call regions to run hap.py. The final outcome is very good, but I find a detail didn't make sense: as the bed lengthened，the SNP performed better and better, but INDEL on the contrary that it's getting worse since the number is decreasing, but I think it is making sense that the number becomes more as the bed gets longer, just like SNP. As shown in the figure below. ![image](https://user-images.githubusercontent.com/63234787/220512170-4506359f-8c72-44ff-8585-e4357f24c20b.png). Can you give me a detailed explanation of this detail？ Thank you very much！ . Finally, thank you very much for developing such a great tool！.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/616
https://github.com/google/deepvariant/issues/616:441,modifiability,paramet,parameter,441,"As bed lengthened, SNP performed better, but indel on the contrary; Hello！ I run the rawdata of NA12878 download from [NCBI SRA](https://trace.ncbi.nlm.nih.gov/Traces/?view=run_browser&acc=ERR1905890&display=data-access) []() and I got it's capture kit is Agilent_V5. First, I run the **oqfe protocol** to align, and the output CRAM as the input of Deepvariant. I run Deepvariant in WES model **3 times**, the first one didn't have --region parameter, the second one use a adding **50** bp buffer on each side of the custom target regions in BED format, the last one is adding **100** bp. Next, I got the **truth** Benchmarking variant calls form GIAB and it's confident call regions to run hap.py. The final outcome is very good, but I find a detail didn't make sense: as the bed lengthened，the SNP performed better and better, but INDEL on the contrary that it's getting worse since the number is decreasing, but I think it is making sense that the number becomes more as the bed gets longer, just like SNP. As shown in the figure below. ![image](https://user-images.githubusercontent.com/63234787/220512170-4506359f-8c72-44ff-8585-e4357f24c20b.png). Can you give me a detailed explanation of this detail？ Thank you very much！ . Finally, thank you very much for developing such a great tool！.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/616
https://github.com/google/deepvariant/issues/616:23,performance,perform,performed,23,"As bed lengthened, SNP performed better, but indel on the contrary; Hello！ I run the rawdata of NA12878 download from [NCBI SRA](https://trace.ncbi.nlm.nih.gov/Traces/?view=run_browser&acc=ERR1905890&display=data-access) []() and I got it's capture kit is Agilent_V5. First, I run the **oqfe protocol** to align, and the output CRAM as the input of Deepvariant. I run Deepvariant in WES model **3 times**, the first one didn't have --region parameter, the second one use a adding **50** bp buffer on each side of the custom target regions in BED format, the last one is adding **100** bp. Next, I got the **truth** Benchmarking variant calls form GIAB and it's confident call regions to run hap.py. The final outcome is very good, but I find a detail didn't make sense: as the bed lengthened，the SNP performed better and better, but INDEL on the contrary that it's getting worse since the number is decreasing, but I think it is making sense that the number becomes more as the bed gets longer, just like SNP. As shown in the figure below. ![image](https://user-images.githubusercontent.com/63234787/220512170-4506359f-8c72-44ff-8585-e4357f24c20b.png). Can you give me a detailed explanation of this detail？ Thank you very much！ . Finally, thank you very much for developing such a great tool！.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/616
https://github.com/google/deepvariant/issues/616:397,performance,time,times,397,"As bed lengthened, SNP performed better, but indel on the contrary; Hello！ I run the rawdata of NA12878 download from [NCBI SRA](https://trace.ncbi.nlm.nih.gov/Traces/?view=run_browser&acc=ERR1905890&display=data-access) []() and I got it's capture kit is Agilent_V5. First, I run the **oqfe protocol** to align, and the output CRAM as the input of Deepvariant. I run Deepvariant in WES model **3 times**, the first one didn't have --region parameter, the second one use a adding **50** bp buffer on each side of the custom target regions in BED format, the last one is adding **100** bp. Next, I got the **truth** Benchmarking variant calls form GIAB and it's confident call regions to run hap.py. The final outcome is very good, but I find a detail didn't make sense: as the bed lengthened，the SNP performed better and better, but INDEL on the contrary that it's getting worse since the number is decreasing, but I think it is making sense that the number becomes more as the bed gets longer, just like SNP. As shown in the figure below. ![image](https://user-images.githubusercontent.com/63234787/220512170-4506359f-8c72-44ff-8585-e4357f24c20b.png). Can you give me a detailed explanation of this detail？ Thank you very much！ . Finally, thank you very much for developing such a great tool！.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/616
https://github.com/google/deepvariant/issues/616:800,performance,perform,performed,800,"As bed lengthened, SNP performed better, but indel on the contrary; Hello！ I run the rawdata of NA12878 download from [NCBI SRA](https://trace.ncbi.nlm.nih.gov/Traces/?view=run_browser&acc=ERR1905890&display=data-access) []() and I got it's capture kit is Agilent_V5. First, I run the **oqfe protocol** to align, and the output CRAM as the input of Deepvariant. I run Deepvariant in WES model **3 times**, the first one didn't have --region parameter, the second one use a adding **50** bp buffer on each side of the custom target regions in BED format, the last one is adding **100** bp. Next, I got the **truth** Benchmarking variant calls form GIAB and it's confident call regions to run hap.py. The final outcome is very good, but I find a detail didn't make sense: as the bed lengthened，the SNP performed better and better, but INDEL on the contrary that it's getting worse since the number is decreasing, but I think it is making sense that the number becomes more as the bed gets longer, just like SNP. As shown in the figure below. ![image](https://user-images.githubusercontent.com/63234787/220512170-4506359f-8c72-44ff-8585-e4357f24c20b.png). Can you give me a detailed explanation of this detail？ Thank you very much！ . Finally, thank you very much for developing such a great tool！.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/616
https://github.com/google/deepvariant/issues/616:340,safety,input,input,340,"As bed lengthened, SNP performed better, but indel on the contrary; Hello！ I run the rawdata of NA12878 download from [NCBI SRA](https://trace.ncbi.nlm.nih.gov/Traces/?view=run_browser&acc=ERR1905890&display=data-access) []() and I got it's capture kit is Agilent_V5. First, I run the **oqfe protocol** to align, and the output CRAM as the input of Deepvariant. I run Deepvariant in WES model **3 times**, the first one didn't have --region parameter, the second one use a adding **50** bp buffer on each side of the custom target regions in BED format, the last one is adding **100** bp. Next, I got the **truth** Benchmarking variant calls form GIAB and it's confident call regions to run hap.py. The final outcome is very good, but I find a detail didn't make sense: as the bed lengthened，the SNP performed better and better, but INDEL on the contrary that it's getting worse since the number is decreasing, but I think it is making sense that the number becomes more as the bed gets longer, just like SNP. As shown in the figure below. ![image](https://user-images.githubusercontent.com/63234787/220512170-4506359f-8c72-44ff-8585-e4357f24c20b.png). Can you give me a detailed explanation of this detail？ Thank you very much！ . Finally, thank you very much for developing such a great tool！.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/616
https://github.com/google/deepvariant/issues/616:213,security,access,access,213,"As bed lengthened, SNP performed better, but indel on the contrary; Hello！ I run the rawdata of NA12878 download from [NCBI SRA](https://trace.ncbi.nlm.nih.gov/Traces/?view=run_browser&acc=ERR1905890&display=data-access) []() and I got it's capture kit is Agilent_V5. First, I run the **oqfe protocol** to align, and the output CRAM as the input of Deepvariant. I run Deepvariant in WES model **3 times**, the first one didn't have --region parameter, the second one use a adding **50** bp buffer on each side of the custom target regions in BED format, the last one is adding **100** bp. Next, I got the **truth** Benchmarking variant calls form GIAB and it's confident call regions to run hap.py. The final outcome is very good, but I find a detail didn't make sense: as the bed lengthened，the SNP performed better and better, but INDEL on the contrary that it's getting worse since the number is decreasing, but I think it is making sense that the number becomes more as the bed gets longer, just like SNP. As shown in the figure below. ![image](https://user-images.githubusercontent.com/63234787/220512170-4506359f-8c72-44ff-8585-e4357f24c20b.png). Can you give me a detailed explanation of this detail？ Thank you very much！ . Finally, thank you very much for developing such a great tool！.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/616
https://github.com/google/deepvariant/issues/616:387,security,model,model,387,"As bed lengthened, SNP performed better, but indel on the contrary; Hello！ I run the rawdata of NA12878 download from [NCBI SRA](https://trace.ncbi.nlm.nih.gov/Traces/?view=run_browser&acc=ERR1905890&display=data-access) []() and I got it's capture kit is Agilent_V5. First, I run the **oqfe protocol** to align, and the output CRAM as the input of Deepvariant. I run Deepvariant in WES model **3 times**, the first one didn't have --region parameter, the second one use a adding **50** bp buffer on each side of the custom target regions in BED format, the last one is adding **100** bp. Next, I got the **truth** Benchmarking variant calls form GIAB and it's confident call regions to run hap.py. The final outcome is very good, but I find a detail didn't make sense: as the bed lengthened，the SNP performed better and better, but INDEL on the contrary that it's getting worse since the number is decreasing, but I think it is making sense that the number becomes more as the bed gets longer, just like SNP. As shown in the figure below. ![image](https://user-images.githubusercontent.com/63234787/220512170-4506359f-8c72-44ff-8585-e4357f24c20b.png). Can you give me a detailed explanation of this detail？ Thank you very much！ . Finally, thank you very much for developing such a great tool！.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/616
https://github.com/google/deepvariant/issues/616:137,testability,trace,trace,137,"As bed lengthened, SNP performed better, but indel on the contrary; Hello！ I run the rawdata of NA12878 download from [NCBI SRA](https://trace.ncbi.nlm.nih.gov/Traces/?view=run_browser&acc=ERR1905890&display=data-access) []() and I got it's capture kit is Agilent_V5. First, I run the **oqfe protocol** to align, and the output CRAM as the input of Deepvariant. I run Deepvariant in WES model **3 times**, the first one didn't have --region parameter, the second one use a adding **50** bp buffer on each side of the custom target regions in BED format, the last one is adding **100** bp. Next, I got the **truth** Benchmarking variant calls form GIAB and it's confident call regions to run hap.py. The final outcome is very good, but I find a detail didn't make sense: as the bed lengthened，the SNP performed better and better, but INDEL on the contrary that it's getting worse since the number is decreasing, but I think it is making sense that the number becomes more as the bed gets longer, just like SNP. As shown in the figure below. ![image](https://user-images.githubusercontent.com/63234787/220512170-4506359f-8c72-44ff-8585-e4357f24c20b.png). Can you give me a detailed explanation of this detail？ Thank you very much！ . Finally, thank you very much for developing such a great tool！.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/616
https://github.com/google/deepvariant/issues/616:160,testability,Trace,Traces,160,"As bed lengthened, SNP performed better, but indel on the contrary; Hello！ I run the rawdata of NA12878 download from [NCBI SRA](https://trace.ncbi.nlm.nih.gov/Traces/?view=run_browser&acc=ERR1905890&display=data-access) []() and I got it's capture kit is Agilent_V5. First, I run the **oqfe protocol** to align, and the output CRAM as the input of Deepvariant. I run Deepvariant in WES model **3 times**, the first one didn't have --region parameter, the second one use a adding **50** bp buffer on each side of the custom target regions in BED format, the last one is adding **100** bp. Next, I got the **truth** Benchmarking variant calls form GIAB and it's confident call regions to run hap.py. The final outcome is very good, but I find a detail didn't make sense: as the bed lengthened，the SNP performed better and better, but INDEL on the contrary that it's getting worse since the number is decreasing, but I think it is making sense that the number becomes more as the bed gets longer, just like SNP. As shown in the figure below. ![image](https://user-images.githubusercontent.com/63234787/220512170-4506359f-8c72-44ff-8585-e4357f24c20b.png). Can you give me a detailed explanation of this detail？ Thank you very much！ . Finally, thank you very much for developing such a great tool！.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/616
https://github.com/google/deepvariant/issues/616:23,usability,perform,performed,23,"As bed lengthened, SNP performed better, but indel on the contrary; Hello！ I run the rawdata of NA12878 download from [NCBI SRA](https://trace.ncbi.nlm.nih.gov/Traces/?view=run_browser&acc=ERR1905890&display=data-access) []() and I got it's capture kit is Agilent_V5. First, I run the **oqfe protocol** to align, and the output CRAM as the input of Deepvariant. I run Deepvariant in WES model **3 times**, the first one didn't have --region parameter, the second one use a adding **50** bp buffer on each side of the custom target regions in BED format, the last one is adding **100** bp. Next, I got the **truth** Benchmarking variant calls form GIAB and it's confident call regions to run hap.py. The final outcome is very good, but I find a detail didn't make sense: as the bed lengthened，the SNP performed better and better, but INDEL on the contrary that it's getting worse since the number is decreasing, but I think it is making sense that the number becomes more as the bed gets longer, just like SNP. As shown in the figure below. ![image](https://user-images.githubusercontent.com/63234787/220512170-4506359f-8c72-44ff-8585-e4357f24c20b.png). Can you give me a detailed explanation of this detail？ Thank you very much！ . Finally, thank you very much for developing such a great tool！.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/616
https://github.com/google/deepvariant/issues/616:340,usability,input,input,340,"As bed lengthened, SNP performed better, but indel on the contrary; Hello！ I run the rawdata of NA12878 download from [NCBI SRA](https://trace.ncbi.nlm.nih.gov/Traces/?view=run_browser&acc=ERR1905890&display=data-access) []() and I got it's capture kit is Agilent_V5. First, I run the **oqfe protocol** to align, and the output CRAM as the input of Deepvariant. I run Deepvariant in WES model **3 times**, the first one didn't have --region parameter, the second one use a adding **50** bp buffer on each side of the custom target regions in BED format, the last one is adding **100** bp. Next, I got the **truth** Benchmarking variant calls form GIAB and it's confident call regions to run hap.py. The final outcome is very good, but I find a detail didn't make sense: as the bed lengthened，the SNP performed better and better, but INDEL on the contrary that it's getting worse since the number is decreasing, but I think it is making sense that the number becomes more as the bed gets longer, just like SNP. As shown in the figure below. ![image](https://user-images.githubusercontent.com/63234787/220512170-4506359f-8c72-44ff-8585-e4357f24c20b.png). Can you give me a detailed explanation of this detail？ Thank you very much！ . Finally, thank you very much for developing such a great tool！.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/616
https://github.com/google/deepvariant/issues/616:517,usability,custom,custom,517,"As bed lengthened, SNP performed better, but indel on the contrary; Hello！ I run the rawdata of NA12878 download from [NCBI SRA](https://trace.ncbi.nlm.nih.gov/Traces/?view=run_browser&acc=ERR1905890&display=data-access) []() and I got it's capture kit is Agilent_V5. First, I run the **oqfe protocol** to align, and the output CRAM as the input of Deepvariant. I run Deepvariant in WES model **3 times**, the first one didn't have --region parameter, the second one use a adding **50** bp buffer on each side of the custom target regions in BED format, the last one is adding **100** bp. Next, I got the **truth** Benchmarking variant calls form GIAB and it's confident call regions to run hap.py. The final outcome is very good, but I find a detail didn't make sense: as the bed lengthened，the SNP performed better and better, but INDEL on the contrary that it's getting worse since the number is decreasing, but I think it is making sense that the number becomes more as the bed gets longer, just like SNP. As shown in the figure below. ![image](https://user-images.githubusercontent.com/63234787/220512170-4506359f-8c72-44ff-8585-e4357f24c20b.png). Can you give me a detailed explanation of this detail？ Thank you very much！ . Finally, thank you very much for developing such a great tool！.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/616
https://github.com/google/deepvariant/issues/616:800,usability,perform,performed,800,"As bed lengthened, SNP performed better, but indel on the contrary; Hello！ I run the rawdata of NA12878 download from [NCBI SRA](https://trace.ncbi.nlm.nih.gov/Traces/?view=run_browser&acc=ERR1905890&display=data-access) []() and I got it's capture kit is Agilent_V5. First, I run the **oqfe protocol** to align, and the output CRAM as the input of Deepvariant. I run Deepvariant in WES model **3 times**, the first one didn't have --region parameter, the second one use a adding **50** bp buffer on each side of the custom target regions in BED format, the last one is adding **100** bp. Next, I got the **truth** Benchmarking variant calls form GIAB and it's confident call regions to run hap.py. The final outcome is very good, but I find a detail didn't make sense: as the bed lengthened，the SNP performed better and better, but INDEL on the contrary that it's getting worse since the number is decreasing, but I think it is making sense that the number becomes more as the bed gets longer, just like SNP. As shown in the figure below. ![image](https://user-images.githubusercontent.com/63234787/220512170-4506359f-8c72-44ff-8585-e4357f24c20b.png). Can you give me a detailed explanation of this detail？ Thank you very much！ . Finally, thank you very much for developing such a great tool！.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/616
https://github.com/google/deepvariant/issues/616:1057,usability,user,user-images,1057,"As bed lengthened, SNP performed better, but indel on the contrary; Hello！ I run the rawdata of NA12878 download from [NCBI SRA](https://trace.ncbi.nlm.nih.gov/Traces/?view=run_browser&acc=ERR1905890&display=data-access) []() and I got it's capture kit is Agilent_V5. First, I run the **oqfe protocol** to align, and the output CRAM as the input of Deepvariant. I run Deepvariant in WES model **3 times**, the first one didn't have --region parameter, the second one use a adding **50** bp buffer on each side of the custom target regions in BED format, the last one is adding **100** bp. Next, I got the **truth** Benchmarking variant calls form GIAB and it's confident call regions to run hap.py. The final outcome is very good, but I find a detail didn't make sense: as the bed lengthened，the SNP performed better and better, but INDEL on the contrary that it's getting worse since the number is decreasing, but I think it is making sense that the number becomes more as the bed gets longer, just like SNP. As shown in the figure below. ![image](https://user-images.githubusercontent.com/63234787/220512170-4506359f-8c72-44ff-8585-e4357f24c20b.png). Can you give me a detailed explanation of this detail？ Thank you very much！ . Finally, thank you very much for developing such a great tool！.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/616
https://github.com/google/deepvariant/issues/616:1288,usability,tool,tool,1288,"As bed lengthened, SNP performed better, but indel on the contrary; Hello！ I run the rawdata of NA12878 download from [NCBI SRA](https://trace.ncbi.nlm.nih.gov/Traces/?view=run_browser&acc=ERR1905890&display=data-access) []() and I got it's capture kit is Agilent_V5. First, I run the **oqfe protocol** to align, and the output CRAM as the input of Deepvariant. I run Deepvariant in WES model **3 times**, the first one didn't have --region parameter, the second one use a adding **50** bp buffer on each side of the custom target regions in BED format, the last one is adding **100** bp. Next, I got the **truth** Benchmarking variant calls form GIAB and it's confident call regions to run hap.py. The final outcome is very good, but I find a detail didn't make sense: as the bed lengthened，the SNP performed better and better, but INDEL on the contrary that it's getting worse since the number is decreasing, but I think it is making sense that the number becomes more as the bed gets longer, just like SNP. As shown in the figure below. ![image](https://user-images.githubusercontent.com/63234787/220512170-4506359f-8c72-44ff-8585-e4357f24c20b.png). Can you give me a detailed explanation of this detail？ Thank you very much！ . Finally, thank you very much for developing such a great tool！.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/616
https://github.com/google/deepvariant/issues/617:243,energy efficiency,optim,optimal,243,"ONT Duplex Data; Thanks for the wonderful tool! It's exciting to know that DeepVariant supports ONT Duplex Data. I wonder what tools you use to generate bam files for testing. Assuming you use minimap2, what parameters did you use to generate optimal input for DeepVariant? Given the high accuracy of Duplex data, the default setting should not work well anymore because that's for old ONT data. Thanks!",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/617
https://github.com/google/deepvariant/issues/617:208,modifiability,paramet,parameters,208,"ONT Duplex Data; Thanks for the wonderful tool! It's exciting to know that DeepVariant supports ONT Duplex Data. I wonder what tools you use to generate bam files for testing. Assuming you use minimap2, what parameters did you use to generate optimal input for DeepVariant? Given the high accuracy of Duplex data, the default setting should not work well anymore because that's for old ONT data. Thanks!",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/617
https://github.com/google/deepvariant/issues/617:167,safety,test,testing,167,"ONT Duplex Data; Thanks for the wonderful tool! It's exciting to know that DeepVariant supports ONT Duplex Data. I wonder what tools you use to generate bam files for testing. Assuming you use minimap2, what parameters did you use to generate optimal input for DeepVariant? Given the high accuracy of Duplex data, the default setting should not work well anymore because that's for old ONT data. Thanks!",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/617
https://github.com/google/deepvariant/issues/617:251,safety,input,input,251,"ONT Duplex Data; Thanks for the wonderful tool! It's exciting to know that DeepVariant supports ONT Duplex Data. I wonder what tools you use to generate bam files for testing. Assuming you use minimap2, what parameters did you use to generate optimal input for DeepVariant? Given the high accuracy of Duplex data, the default setting should not work well anymore because that's for old ONT data. Thanks!",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/617
https://github.com/google/deepvariant/issues/617:167,testability,test,testing,167,"ONT Duplex Data; Thanks for the wonderful tool! It's exciting to know that DeepVariant supports ONT Duplex Data. I wonder what tools you use to generate bam files for testing. Assuming you use minimap2, what parameters did you use to generate optimal input for DeepVariant? Given the high accuracy of Duplex data, the default setting should not work well anymore because that's for old ONT data. Thanks!",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/617
https://github.com/google/deepvariant/issues/617:42,usability,tool,tool,42,"ONT Duplex Data; Thanks for the wonderful tool! It's exciting to know that DeepVariant supports ONT Duplex Data. I wonder what tools you use to generate bam files for testing. Assuming you use minimap2, what parameters did you use to generate optimal input for DeepVariant? Given the high accuracy of Duplex data, the default setting should not work well anymore because that's for old ONT data. Thanks!",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/617
https://github.com/google/deepvariant/issues/617:87,usability,support,supports,87,"ONT Duplex Data; Thanks for the wonderful tool! It's exciting to know that DeepVariant supports ONT Duplex Data. I wonder what tools you use to generate bam files for testing. Assuming you use minimap2, what parameters did you use to generate optimal input for DeepVariant? Given the high accuracy of Duplex data, the default setting should not work well anymore because that's for old ONT data. Thanks!",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/617
https://github.com/google/deepvariant/issues/617:127,usability,tool,tools,127,"ONT Duplex Data; Thanks for the wonderful tool! It's exciting to know that DeepVariant supports ONT Duplex Data. I wonder what tools you use to generate bam files for testing. Assuming you use minimap2, what parameters did you use to generate optimal input for DeepVariant? Given the high accuracy of Duplex data, the default setting should not work well anymore because that's for old ONT data. Thanks!",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/617
https://github.com/google/deepvariant/issues/617:251,usability,input,input,251,"ONT Duplex Data; Thanks for the wonderful tool! It's exciting to know that DeepVariant supports ONT Duplex Data. I wonder what tools you use to generate bam files for testing. Assuming you use minimap2, what parameters did you use to generate optimal input for DeepVariant? Given the high accuracy of Duplex data, the default setting should not work well anymore because that's for old ONT data. Thanks!",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/617
https://github.com/google/deepvariant/issues/618:1048,availability,Operat,Operating,1048,"Why does deepvariant report an allele if the allele does not exist in the sample?; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**: Yes. **Describe the issue:**. Hello,. Using WES model, deepvariant calls the following variant in the vcf file:. ```. NC_000001.11	84574341	.	CAGCAGCGCT	C,T	.	.	.	GT:GQ:DP:AD:VAF:PL	1/0:3:97:25,45,26:0.463918,0.268041:36,0,47,0,16,44. ```. For this variant, the genotype is 1/0, meaning that one allele is REF, and the other allele is C. . What is confusing is that deepvariant also calls a T however this is not referenced anywhere in the GT field. What is the point of calling T if it does not occur in the sample? Here is the screenshot of the original alignment:. ![dv1](https://user-images.githubusercontent.com/22151692/223809460-b6cdeed1-e332-4014-879d-8ee44123f793.png). And here is the screenshot for the realigned reads for this position:. ![dv2](https://user-images.githubusercontent.com/22151692/223808774-d63a161c-e9f9-4e54-9546-2be3b9d5f492.png). **Setup**. - Operating system: Ubuntu. - DeepVariant version: 1.2.0. - Installation method (Docker, built from source, etc.): Docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**. - Command:. - Error trace: (if applicable). **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/618
https://github.com/google/deepvariant/issues/618:1318,availability,Error,Error,1318,"Why does deepvariant report an allele if the allele does not exist in the sample?; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**: Yes. **Describe the issue:**. Hello,. Using WES model, deepvariant calls the following variant in the vcf file:. ```. NC_000001.11	84574341	.	CAGCAGCGCT	C,T	.	.	.	GT:GQ:DP:AD:VAF:PL	1/0:3:97:25,45,26:0.463918,0.268041:36,0,47,0,16,44. ```. For this variant, the genotype is 1/0, meaning that one allele is REF, and the other allele is C. . What is confusing is that deepvariant also calls a T however this is not referenced anywhere in the GT field. What is the point of calling T if it does not occur in the sample? Here is the screenshot of the original alignment:. ![dv1](https://user-images.githubusercontent.com/22151692/223809460-b6cdeed1-e332-4014-879d-8ee44123f793.png). And here is the screenshot for the realigned reads for this position:. ![dv2](https://user-images.githubusercontent.com/22151692/223808774-d63a161c-e9f9-4e54-9546-2be3b9d5f492.png). **Setup**. - Operating system: Ubuntu. - DeepVariant version: 1.2.0. - Installation method (Docker, built from source, etc.): Docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**. - Command:. - Error trace: (if applicable). **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/618
https://github.com/google/deepvariant/issues/618:1088,deployability,version,version,1088,"Why does deepvariant report an allele if the allele does not exist in the sample?; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**: Yes. **Describe the issue:**. Hello,. Using WES model, deepvariant calls the following variant in the vcf file:. ```. NC_000001.11	84574341	.	CAGCAGCGCT	C,T	.	.	.	GT:GQ:DP:AD:VAF:PL	1/0:3:97:25,45,26:0.463918,0.268041:36,0,47,0,16,44. ```. For this variant, the genotype is 1/0, meaning that one allele is REF, and the other allele is C. . What is confusing is that deepvariant also calls a T however this is not referenced anywhere in the GT field. What is the point of calling T if it does not occur in the sample? Here is the screenshot of the original alignment:. ![dv1](https://user-images.githubusercontent.com/22151692/223809460-b6cdeed1-e332-4014-879d-8ee44123f793.png). And here is the screenshot for the realigned reads for this position:. ![dv2](https://user-images.githubusercontent.com/22151692/223808774-d63a161c-e9f9-4e54-9546-2be3b9d5f492.png). **Setup**. - Operating system: Ubuntu. - DeepVariant version: 1.2.0. - Installation method (Docker, built from source, etc.): Docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**. - Command:. - Error trace: (if applicable). **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/618
https://github.com/google/deepvariant/issues/618:1106,deployability,Instal,Installation,1106,"Why does deepvariant report an allele if the allele does not exist in the sample?; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**: Yes. **Describe the issue:**. Hello,. Using WES model, deepvariant calls the following variant in the vcf file:. ```. NC_000001.11	84574341	.	CAGCAGCGCT	C,T	.	.	.	GT:GQ:DP:AD:VAF:PL	1/0:3:97:25,45,26:0.463918,0.268041:36,0,47,0,16,44. ```. For this variant, the genotype is 1/0, meaning that one allele is REF, and the other allele is C. . What is confusing is that deepvariant also calls a T however this is not referenced anywhere in the GT field. What is the point of calling T if it does not occur in the sample? Here is the screenshot of the original alignment:. ![dv1](https://user-images.githubusercontent.com/22151692/223809460-b6cdeed1-e332-4014-879d-8ee44123f793.png). And here is the screenshot for the realigned reads for this position:. ![dv2](https://user-images.githubusercontent.com/22151692/223808774-d63a161c-e9f9-4e54-9546-2be3b9d5f492.png). **Setup**. - Operating system: Ubuntu. - DeepVariant version: 1.2.0. - Installation method (Docker, built from source, etc.): Docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**. - Command:. - Error trace: (if applicable). **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/618
https://github.com/google/deepvariant/issues/618:222,energy efficiency,model,model,222,"Why does deepvariant report an allele if the allele does not exist in the sample?; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**: Yes. **Describe the issue:**. Hello,. Using WES model, deepvariant calls the following variant in the vcf file:. ```. NC_000001.11	84574341	.	CAGCAGCGCT	C,T	.	.	.	GT:GQ:DP:AD:VAF:PL	1/0:3:97:25,45,26:0.463918,0.268041:36,0,47,0,16,44. ```. For this variant, the genotype is 1/0, meaning that one allele is REF, and the other allele is C. . What is confusing is that deepvariant also calls a T however this is not referenced anywhere in the GT field. What is the point of calling T if it does not occur in the sample? Here is the screenshot of the original alignment:. ![dv1](https://user-images.githubusercontent.com/22151692/223809460-b6cdeed1-e332-4014-879d-8ee44123f793.png). And here is the screenshot for the realigned reads for this position:. ![dv2](https://user-images.githubusercontent.com/22151692/223808774-d63a161c-e9f9-4e54-9546-2be3b9d5f492.png). **Setup**. - Operating system: Ubuntu. - DeepVariant version: 1.2.0. - Installation method (Docker, built from source, etc.): Docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**. - Command:. - Error trace: (if applicable). **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/618
https://github.com/google/deepvariant/issues/618:1088,integrability,version,version,1088,"Why does deepvariant report an allele if the allele does not exist in the sample?; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**: Yes. **Describe the issue:**. Hello,. Using WES model, deepvariant calls the following variant in the vcf file:. ```. NC_000001.11	84574341	.	CAGCAGCGCT	C,T	.	.	.	GT:GQ:DP:AD:VAF:PL	1/0:3:97:25,45,26:0.463918,0.268041:36,0,47,0,16,44. ```. For this variant, the genotype is 1/0, meaning that one allele is REF, and the other allele is C. . What is confusing is that deepvariant also calls a T however this is not referenced anywhere in the GT field. What is the point of calling T if it does not occur in the sample? Here is the screenshot of the original alignment:. ![dv1](https://user-images.githubusercontent.com/22151692/223809460-b6cdeed1-e332-4014-879d-8ee44123f793.png). And here is the screenshot for the realigned reads for this position:. ![dv2](https://user-images.githubusercontent.com/22151692/223808774-d63a161c-e9f9-4e54-9546-2be3b9d5f492.png). **Setup**. - Operating system: Ubuntu. - DeepVariant version: 1.2.0. - Installation method (Docker, built from source, etc.): Docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**. - Command:. - Error trace: (if applicable). **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/618
https://github.com/google/deepvariant/issues/618:1088,modifiability,version,version,1088,"Why does deepvariant report an allele if the allele does not exist in the sample?; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**: Yes. **Describe the issue:**. Hello,. Using WES model, deepvariant calls the following variant in the vcf file:. ```. NC_000001.11	84574341	.	CAGCAGCGCT	C,T	.	.	.	GT:GQ:DP:AD:VAF:PL	1/0:3:97:25,45,26:0.463918,0.268041:36,0,47,0,16,44. ```. For this variant, the genotype is 1/0, meaning that one allele is REF, and the other allele is C. . What is confusing is that deepvariant also calls a T however this is not referenced anywhere in the GT field. What is the point of calling T if it does not occur in the sample? Here is the screenshot of the original alignment:. ![dv1](https://user-images.githubusercontent.com/22151692/223809460-b6cdeed1-e332-4014-879d-8ee44123f793.png). And here is the screenshot for the realigned reads for this position:. ![dv2](https://user-images.githubusercontent.com/22151692/223808774-d63a161c-e9f9-4e54-9546-2be3b9d5f492.png). **Setup**. - Operating system: Ubuntu. - DeepVariant version: 1.2.0. - Installation method (Docker, built from source, etc.): Docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**. - Command:. - Error trace: (if applicable). **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/618
https://github.com/google/deepvariant/issues/618:1318,performance,Error,Error,1318,"Why does deepvariant report an allele if the allele does not exist in the sample?; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**: Yes. **Describe the issue:**. Hello,. Using WES model, deepvariant calls the following variant in the vcf file:. ```. NC_000001.11	84574341	.	CAGCAGCGCT	C,T	.	.	.	GT:GQ:DP:AD:VAF:PL	1/0:3:97:25,45,26:0.463918,0.268041:36,0,47,0,16,44. ```. For this variant, the genotype is 1/0, meaning that one allele is REF, and the other allele is C. . What is confusing is that deepvariant also calls a T however this is not referenced anywhere in the GT field. What is the point of calling T if it does not occur in the sample? Here is the screenshot of the original alignment:. ![dv1](https://user-images.githubusercontent.com/22151692/223809460-b6cdeed1-e332-4014-879d-8ee44123f793.png). And here is the screenshot for the realigned reads for this position:. ![dv2](https://user-images.githubusercontent.com/22151692/223808774-d63a161c-e9f9-4e54-9546-2be3b9d5f492.png). **Setup**. - Operating system: Ubuntu. - DeepVariant version: 1.2.0. - Installation method (Docker, built from source, etc.): Docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**. - Command:. - Error trace: (if applicable). **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/618
https://github.com/google/deepvariant/issues/618:4,reliability,doe,does,4,"Why does deepvariant report an allele if the allele does not exist in the sample?; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**: Yes. **Describe the issue:**. Hello,. Using WES model, deepvariant calls the following variant in the vcf file:. ```. NC_000001.11	84574341	.	CAGCAGCGCT	C,T	.	.	.	GT:GQ:DP:AD:VAF:PL	1/0:3:97:25,45,26:0.463918,0.268041:36,0,47,0,16,44. ```. For this variant, the genotype is 1/0, meaning that one allele is REF, and the other allele is C. . What is confusing is that deepvariant also calls a T however this is not referenced anywhere in the GT field. What is the point of calling T if it does not occur in the sample? Here is the screenshot of the original alignment:. ![dv1](https://user-images.githubusercontent.com/22151692/223809460-b6cdeed1-e332-4014-879d-8ee44123f793.png). And here is the screenshot for the realigned reads for this position:. ![dv2](https://user-images.githubusercontent.com/22151692/223808774-d63a161c-e9f9-4e54-9546-2be3b9d5f492.png). **Setup**. - Operating system: Ubuntu. - DeepVariant version: 1.2.0. - Installation method (Docker, built from source, etc.): Docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**. - Command:. - Error trace: (if applicable). **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/618
https://github.com/google/deepvariant/issues/618:52,reliability,doe,does,52,"Why does deepvariant report an allele if the allele does not exist in the sample?; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**: Yes. **Describe the issue:**. Hello,. Using WES model, deepvariant calls the following variant in the vcf file:. ```. NC_000001.11	84574341	.	CAGCAGCGCT	C,T	.	.	.	GT:GQ:DP:AD:VAF:PL	1/0:3:97:25,45,26:0.463918,0.268041:36,0,47,0,16,44. ```. For this variant, the genotype is 1/0, meaning that one allele is REF, and the other allele is C. . What is confusing is that deepvariant also calls a T however this is not referenced anywhere in the GT field. What is the point of calling T if it does not occur in the sample? Here is the screenshot of the original alignment:. ![dv1](https://user-images.githubusercontent.com/22151692/223809460-b6cdeed1-e332-4014-879d-8ee44123f793.png). And here is the screenshot for the realigned reads for this position:. ![dv2](https://user-images.githubusercontent.com/22151692/223808774-d63a161c-e9f9-4e54-9546-2be3b9d5f492.png). **Setup**. - Operating system: Ubuntu. - DeepVariant version: 1.2.0. - Installation method (Docker, built from source, etc.): Docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**. - Command:. - Error trace: (if applicable). **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/618
https://github.com/google/deepvariant/issues/618:661,reliability,doe,does,661,"Why does deepvariant report an allele if the allele does not exist in the sample?; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**: Yes. **Describe the issue:**. Hello,. Using WES model, deepvariant calls the following variant in the vcf file:. ```. NC_000001.11	84574341	.	CAGCAGCGCT	C,T	.	.	.	GT:GQ:DP:AD:VAF:PL	1/0:3:97:25,45,26:0.463918,0.268041:36,0,47,0,16,44. ```. For this variant, the genotype is 1/0, meaning that one allele is REF, and the other allele is C. . What is confusing is that deepvariant also calls a T however this is not referenced anywhere in the GT field. What is the point of calling T if it does not occur in the sample? Here is the screenshot of the original alignment:. ![dv1](https://user-images.githubusercontent.com/22151692/223809460-b6cdeed1-e332-4014-879d-8ee44123f793.png). And here is the screenshot for the realigned reads for this position:. ![dv2](https://user-images.githubusercontent.com/22151692/223808774-d63a161c-e9f9-4e54-9546-2be3b9d5f492.png). **Setup**. - Operating system: Ubuntu. - DeepVariant version: 1.2.0. - Installation method (Docker, built from source, etc.): Docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**. - Command:. - Error trace: (if applicable). **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/618
https://github.com/google/deepvariant/issues/618:1350,reliability,Doe,Does,1350,"Why does deepvariant report an allele if the allele does not exist in the sample?; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**: Yes. **Describe the issue:**. Hello,. Using WES model, deepvariant calls the following variant in the vcf file:. ```. NC_000001.11	84574341	.	CAGCAGCGCT	C,T	.	.	.	GT:GQ:DP:AD:VAF:PL	1/0:3:97:25,45,26:0.463918,0.268041:36,0,47,0,16,44. ```. For this variant, the genotype is 1/0, meaning that one allele is REF, and the other allele is C. . What is confusing is that deepvariant also calls a T however this is not referenced anywhere in the GT field. What is the point of calling T if it does not occur in the sample? Here is the screenshot of the original alignment:. ![dv1](https://user-images.githubusercontent.com/22151692/223809460-b6cdeed1-e332-4014-879d-8ee44123f793.png). And here is the screenshot for the realigned reads for this position:. ![dv2](https://user-images.githubusercontent.com/22151692/223808774-d63a161c-e9f9-4e54-9546-2be3b9d5f492.png). **Setup**. - Operating system: Ubuntu. - DeepVariant version: 1.2.0. - Installation method (Docker, built from source, etc.): Docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**. - Command:. - Error trace: (if applicable). **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/618
https://github.com/google/deepvariant/issues/618:1318,safety,Error,Error,1318,"Why does deepvariant report an allele if the allele does not exist in the sample?; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**: Yes. **Describe the issue:**. Hello,. Using WES model, deepvariant calls the following variant in the vcf file:. ```. NC_000001.11	84574341	.	CAGCAGCGCT	C,T	.	.	.	GT:GQ:DP:AD:VAF:PL	1/0:3:97:25,45,26:0.463918,0.268041:36,0,47,0,16,44. ```. For this variant, the genotype is 1/0, meaning that one allele is REF, and the other allele is C. . What is confusing is that deepvariant also calls a T however this is not referenced anywhere in the GT field. What is the point of calling T if it does not occur in the sample? Here is the screenshot of the original alignment:. ![dv1](https://user-images.githubusercontent.com/22151692/223809460-b6cdeed1-e332-4014-879d-8ee44123f793.png). And here is the screenshot for the realigned reads for this position:. ![dv2](https://user-images.githubusercontent.com/22151692/223808774-d63a161c-e9f9-4e54-9546-2be3b9d5f492.png). **Setup**. - Operating system: Ubuntu. - DeepVariant version: 1.2.0. - Installation method (Docker, built from source, etc.): Docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**. - Command:. - Error trace: (if applicable). **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/618
https://github.com/google/deepvariant/issues/618:1371,safety,test,test,1371,"Why does deepvariant report an allele if the allele does not exist in the sample?; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**: Yes. **Describe the issue:**. Hello,. Using WES model, deepvariant calls the following variant in the vcf file:. ```. NC_000001.11	84574341	.	CAGCAGCGCT	C,T	.	.	.	GT:GQ:DP:AD:VAF:PL	1/0:3:97:25,45,26:0.463918,0.268041:36,0,47,0,16,44. ```. For this variant, the genotype is 1/0, meaning that one allele is REF, and the other allele is C. . What is confusing is that deepvariant also calls a T however this is not referenced anywhere in the GT field. What is the point of calling T if it does not occur in the sample? Here is the screenshot of the original alignment:. ![dv1](https://user-images.githubusercontent.com/22151692/223809460-b6cdeed1-e332-4014-879d-8ee44123f793.png). And here is the screenshot for the realigned reads for this position:. ![dv2](https://user-images.githubusercontent.com/22151692/223808774-d63a161c-e9f9-4e54-9546-2be3b9d5f492.png). **Setup**. - Operating system: Ubuntu. - DeepVariant version: 1.2.0. - Installation method (Docker, built from source, etc.): Docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**. - Command:. - Error trace: (if applicable). **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/618
https://github.com/google/deepvariant/issues/618:1407,safety,test,test,1407,"Why does deepvariant report an allele if the allele does not exist in the sample?; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**: Yes. **Describe the issue:**. Hello,. Using WES model, deepvariant calls the following variant in the vcf file:. ```. NC_000001.11	84574341	.	CAGCAGCGCT	C,T	.	.	.	GT:GQ:DP:AD:VAF:PL	1/0:3:97:25,45,26:0.463918,0.268041:36,0,47,0,16,44. ```. For this variant, the genotype is 1/0, meaning that one allele is REF, and the other allele is C. . What is confusing is that deepvariant also calls a T however this is not referenced anywhere in the GT field. What is the point of calling T if it does not occur in the sample? Here is the screenshot of the original alignment:. ![dv1](https://user-images.githubusercontent.com/22151692/223809460-b6cdeed1-e332-4014-879d-8ee44123f793.png). And here is the screenshot for the realigned reads for this position:. ![dv2](https://user-images.githubusercontent.com/22151692/223808774-d63a161c-e9f9-4e54-9546-2be3b9d5f492.png). **Setup**. - Operating system: Ubuntu. - DeepVariant version: 1.2.0. - Installation method (Docker, built from source, etc.): Docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**. - Command:. - Error trace: (if applicable). **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/618
https://github.com/google/deepvariant/issues/618:222,security,model,model,222,"Why does deepvariant report an allele if the allele does not exist in the sample?; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**: Yes. **Describe the issue:**. Hello,. Using WES model, deepvariant calls the following variant in the vcf file:. ```. NC_000001.11	84574341	.	CAGCAGCGCT	C,T	.	.	.	GT:GQ:DP:AD:VAF:PL	1/0:3:97:25,45,26:0.463918,0.268041:36,0,47,0,16,44. ```. For this variant, the genotype is 1/0, meaning that one allele is REF, and the other allele is C. . What is confusing is that deepvariant also calls a T however this is not referenced anywhere in the GT field. What is the point of calling T if it does not occur in the sample? Here is the screenshot of the original alignment:. ![dv1](https://user-images.githubusercontent.com/22151692/223809460-b6cdeed1-e332-4014-879d-8ee44123f793.png). And here is the screenshot for the realigned reads for this position:. ![dv2](https://user-images.githubusercontent.com/22151692/223808774-d63a161c-e9f9-4e54-9546-2be3b9d5f492.png). **Setup**. - Operating system: Ubuntu. - DeepVariant version: 1.2.0. - Installation method (Docker, built from source, etc.): Docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**. - Command:. - Error trace: (if applicable). **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/618
https://github.com/google/deepvariant/issues/618:1197,testability,instrument,instrument,1197,"Why does deepvariant report an allele if the allele does not exist in the sample?; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**: Yes. **Describe the issue:**. Hello,. Using WES model, deepvariant calls the following variant in the vcf file:. ```. NC_000001.11	84574341	.	CAGCAGCGCT	C,T	.	.	.	GT:GQ:DP:AD:VAF:PL	1/0:3:97:25,45,26:0.463918,0.268041:36,0,47,0,16,44. ```. For this variant, the genotype is 1/0, meaning that one allele is REF, and the other allele is C. . What is confusing is that deepvariant also calls a T however this is not referenced anywhere in the GT field. What is the point of calling T if it does not occur in the sample? Here is the screenshot of the original alignment:. ![dv1](https://user-images.githubusercontent.com/22151692/223809460-b6cdeed1-e332-4014-879d-8ee44123f793.png). And here is the screenshot for the realigned reads for this position:. ![dv2](https://user-images.githubusercontent.com/22151692/223808774-d63a161c-e9f9-4e54-9546-2be3b9d5f492.png). **Setup**. - Operating system: Ubuntu. - DeepVariant version: 1.2.0. - Installation method (Docker, built from source, etc.): Docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**. - Command:. - Error trace: (if applicable). **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/618
https://github.com/google/deepvariant/issues/618:1324,testability,trace,trace,1324,"Why does deepvariant report an allele if the allele does not exist in the sample?; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**: Yes. **Describe the issue:**. Hello,. Using WES model, deepvariant calls the following variant in the vcf file:. ```. NC_000001.11	84574341	.	CAGCAGCGCT	C,T	.	.	.	GT:GQ:DP:AD:VAF:PL	1/0:3:97:25,45,26:0.463918,0.268041:36,0,47,0,16,44. ```. For this variant, the genotype is 1/0, meaning that one allele is REF, and the other allele is C. . What is confusing is that deepvariant also calls a T however this is not referenced anywhere in the GT field. What is the point of calling T if it does not occur in the sample? Here is the screenshot of the original alignment:. ![dv1](https://user-images.githubusercontent.com/22151692/223809460-b6cdeed1-e332-4014-879d-8ee44123f793.png). And here is the screenshot for the realigned reads for this position:. ![dv2](https://user-images.githubusercontent.com/22151692/223808774-d63a161c-e9f9-4e54-9546-2be3b9d5f492.png). **Setup**. - Operating system: Ubuntu. - DeepVariant version: 1.2.0. - Installation method (Docker, built from source, etc.): Docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**. - Command:. - Error trace: (if applicable). **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/618
https://github.com/google/deepvariant/issues/618:1371,testability,test,test,1371,"Why does deepvariant report an allele if the allele does not exist in the sample?; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**: Yes. **Describe the issue:**. Hello,. Using WES model, deepvariant calls the following variant in the vcf file:. ```. NC_000001.11	84574341	.	CAGCAGCGCT	C,T	.	.	.	GT:GQ:DP:AD:VAF:PL	1/0:3:97:25,45,26:0.463918,0.268041:36,0,47,0,16,44. ```. For this variant, the genotype is 1/0, meaning that one allele is REF, and the other allele is C. . What is confusing is that deepvariant also calls a T however this is not referenced anywhere in the GT field. What is the point of calling T if it does not occur in the sample? Here is the screenshot of the original alignment:. ![dv1](https://user-images.githubusercontent.com/22151692/223809460-b6cdeed1-e332-4014-879d-8ee44123f793.png). And here is the screenshot for the realigned reads for this position:. ![dv2](https://user-images.githubusercontent.com/22151692/223808774-d63a161c-e9f9-4e54-9546-2be3b9d5f492.png). **Setup**. - Operating system: Ubuntu. - DeepVariant version: 1.2.0. - Installation method (Docker, built from source, etc.): Docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**. - Command:. - Error trace: (if applicable). **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/618
https://github.com/google/deepvariant/issues/618:1407,testability,test,test,1407,"Why does deepvariant report an allele if the allele does not exist in the sample?; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**: Yes. **Describe the issue:**. Hello,. Using WES model, deepvariant calls the following variant in the vcf file:. ```. NC_000001.11	84574341	.	CAGCAGCGCT	C,T	.	.	.	GT:GQ:DP:AD:VAF:PL	1/0:3:97:25,45,26:0.463918,0.268041:36,0,47,0,16,44. ```. For this variant, the genotype is 1/0, meaning that one allele is REF, and the other allele is C. . What is confusing is that deepvariant also calls a T however this is not referenced anywhere in the GT field. What is the point of calling T if it does not occur in the sample? Here is the screenshot of the original alignment:. ![dv1](https://user-images.githubusercontent.com/22151692/223809460-b6cdeed1-e332-4014-879d-8ee44123f793.png). And here is the screenshot for the realigned reads for this position:. ![dv2](https://user-images.githubusercontent.com/22151692/223808774-d63a161c-e9f9-4e54-9546-2be3b9d5f492.png). **Setup**. - Operating system: Ubuntu. - DeepVariant version: 1.2.0. - Installation method (Docker, built from source, etc.): Docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**. - Command:. - Error trace: (if applicable). **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/618
https://github.com/google/deepvariant/issues/618:1582,testability,context,context,1582,"Why does deepvariant report an allele if the allele does not exist in the sample?; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**: Yes. **Describe the issue:**. Hello,. Using WES model, deepvariant calls the following variant in the vcf file:. ```. NC_000001.11	84574341	.	CAGCAGCGCT	C,T	.	.	.	GT:GQ:DP:AD:VAF:PL	1/0:3:97:25,45,26:0.463918,0.268041:36,0,47,0,16,44. ```. For this variant, the genotype is 1/0, meaning that one allele is REF, and the other allele is C. . What is confusing is that deepvariant also calls a T however this is not referenced anywhere in the GT field. What is the point of calling T if it does not occur in the sample? Here is the screenshot of the original alignment:. ![dv1](https://user-images.githubusercontent.com/22151692/223809460-b6cdeed1-e332-4014-879d-8ee44123f793.png). And here is the screenshot for the realigned reads for this position:. ![dv2](https://user-images.githubusercontent.com/22151692/223808774-d63a161c-e9f9-4e54-9546-2be3b9d5f492.png). **Setup**. - Operating system: Ubuntu. - DeepVariant version: 1.2.0. - Installation method (Docker, built from source, etc.): Docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**. - Command:. - Error trace: (if applicable). **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/618
https://github.com/google/deepvariant/issues/618:757,usability,user,user-images,757,"Why does deepvariant report an allele if the allele does not exist in the sample?; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**: Yes. **Describe the issue:**. Hello,. Using WES model, deepvariant calls the following variant in the vcf file:. ```. NC_000001.11	84574341	.	CAGCAGCGCT	C,T	.	.	.	GT:GQ:DP:AD:VAF:PL	1/0:3:97:25,45,26:0.463918,0.268041:36,0,47,0,16,44. ```. For this variant, the genotype is 1/0, meaning that one allele is REF, and the other allele is C. . What is confusing is that deepvariant also calls a T however this is not referenced anywhere in the GT field. What is the point of calling T if it does not occur in the sample? Here is the screenshot of the original alignment:. ![dv1](https://user-images.githubusercontent.com/22151692/223809460-b6cdeed1-e332-4014-879d-8ee44123f793.png). And here is the screenshot for the realigned reads for this position:. ![dv2](https://user-images.githubusercontent.com/22151692/223808774-d63a161c-e9f9-4e54-9546-2be3b9d5f492.png). **Setup**. - Operating system: Ubuntu. - DeepVariant version: 1.2.0. - Installation method (Docker, built from source, etc.): Docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**. - Command:. - Error trace: (if applicable). **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/618
https://github.com/google/deepvariant/issues/618:939,usability,user,user-images,939,"Why does deepvariant report an allele if the allele does not exist in the sample?; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**: Yes. **Describe the issue:**. Hello,. Using WES model, deepvariant calls the following variant in the vcf file:. ```. NC_000001.11	84574341	.	CAGCAGCGCT	C,T	.	.	.	GT:GQ:DP:AD:VAF:PL	1/0:3:97:25,45,26:0.463918,0.268041:36,0,47,0,16,44. ```. For this variant, the genotype is 1/0, meaning that one allele is REF, and the other allele is C. . What is confusing is that deepvariant also calls a T however this is not referenced anywhere in the GT field. What is the point of calling T if it does not occur in the sample? Here is the screenshot of the original alignment:. ![dv1](https://user-images.githubusercontent.com/22151692/223809460-b6cdeed1-e332-4014-879d-8ee44123f793.png). And here is the screenshot for the realigned reads for this position:. ![dv2](https://user-images.githubusercontent.com/22151692/223808774-d63a161c-e9f9-4e54-9546-2be3b9d5f492.png). **Setup**. - Operating system: Ubuntu. - DeepVariant version: 1.2.0. - Installation method (Docker, built from source, etc.): Docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**. - Command:. - Error trace: (if applicable). **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/618
https://github.com/google/deepvariant/issues/618:1306,usability,Command,Command,1306,"Why does deepvariant report an allele if the allele does not exist in the sample?; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**: Yes. **Describe the issue:**. Hello,. Using WES model, deepvariant calls the following variant in the vcf file:. ```. NC_000001.11	84574341	.	CAGCAGCGCT	C,T	.	.	.	GT:GQ:DP:AD:VAF:PL	1/0:3:97:25,45,26:0.463918,0.268041:36,0,47,0,16,44. ```. For this variant, the genotype is 1/0, meaning that one allele is REF, and the other allele is C. . What is confusing is that deepvariant also calls a T however this is not referenced anywhere in the GT field. What is the point of calling T if it does not occur in the sample? Here is the screenshot of the original alignment:. ![dv1](https://user-images.githubusercontent.com/22151692/223809460-b6cdeed1-e332-4014-879d-8ee44123f793.png). And here is the screenshot for the realigned reads for this position:. ![dv2](https://user-images.githubusercontent.com/22151692/223808774-d63a161c-e9f9-4e54-9546-2be3b9d5f492.png). **Setup**. - Operating system: Ubuntu. - DeepVariant version: 1.2.0. - Installation method (Docker, built from source, etc.): Docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**. - Command:. - Error trace: (if applicable). **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/618
https://github.com/google/deepvariant/issues/618:1318,usability,Error,Error,1318,"Why does deepvariant report an allele if the allele does not exist in the sample?; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**: Yes. **Describe the issue:**. Hello,. Using WES model, deepvariant calls the following variant in the vcf file:. ```. NC_000001.11	84574341	.	CAGCAGCGCT	C,T	.	.	.	GT:GQ:DP:AD:VAF:PL	1/0:3:97:25,45,26:0.463918,0.268041:36,0,47,0,16,44. ```. For this variant, the genotype is 1/0, meaning that one allele is REF, and the other allele is C. . What is confusing is that deepvariant also calls a T however this is not referenced anywhere in the GT field. What is the point of calling T if it does not occur in the sample? Here is the screenshot of the original alignment:. ![dv1](https://user-images.githubusercontent.com/22151692/223809460-b6cdeed1-e332-4014-879d-8ee44123f793.png). And here is the screenshot for the realigned reads for this position:. ![dv2](https://user-images.githubusercontent.com/22151692/223808774-d63a161c-e9f9-4e54-9546-2be3b9d5f492.png). **Setup**. - Operating system: Ubuntu. - DeepVariant version: 1.2.0. - Installation method (Docker, built from source, etc.): Docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**. - Command:. - Error trace: (if applicable). **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/618
https://github.com/google/deepvariant/issues/619:21,availability,error,error,21,"'CUDA_ERROR_UNKNOWN' error using DeepVariant GPU version.; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**: Yes. **Describe the issue:**. 'CUDA_ERROR_UNKNOWN' using DeepVariant GPU version. **Setup**. - Operating system: CentOS Linux release 7.4.1708 (Core), Linux 5.10.150-1.el7.x86_64. - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): singularity image build from dockerhub. - Type of data: nothing special that is unlike the case studies. **Steps to reproduce:**. - Command: /opt/deepvariant/bin/run_deepvariant --version. - Error trace: (if applicable). ```. tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error. tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: . tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: . tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: INVALID_ARGUMENT: expected %d.%d, %d.%d.%d, or %d.%d.%d.%d form for driver version; got ""1"". tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 520.61.5. DeepVariant version 1.4.0. ```. The `hostname` is privacy. **Does the quick start test work on your system?:** No. **Any additional context:** . The GPU is NVIDIA GeForce 3090. The GPU Driver Version: 520.61.05. The CUDA version in the host is V11.8.89 as followings:. ![image](https://user-images.githubusercontent.com/43125963/225341539-aa2ee3c6-c376-4758-a582-c8fd871b0508.png). It seems that the Deepvariant v1.4.0 in the singularity image has already installed CUDA v11.3. . ![image](https://user-images.githubusercontent.com/43125963/225343337-d0924a9b-4b9d-4b03-848f-e8e9753eb377.png). I don't know whether it causes the program crash.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:245,availability,Operat,Operating,245,"'CUDA_ERROR_UNKNOWN' error using DeepVariant GPU version.; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**: Yes. **Describe the issue:**. 'CUDA_ERROR_UNKNOWN' using DeepVariant GPU version. **Setup**. - Operating system: CentOS Linux release 7.4.1708 (Core), Linux 5.10.150-1.el7.x86_64. - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): singularity image build from dockerhub. - Type of data: nothing special that is unlike the case studies. **Steps to reproduce:**. - Command: /opt/deepvariant/bin/run_deepvariant --version. - Error trace: (if applicable). ```. tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error. tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: . tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: . tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: INVALID_ARGUMENT: expected %d.%d, %d.%d.%d, or %d.%d.%d.%d form for driver version; got ""1"". tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 520.61.5. DeepVariant version 1.4.0. ```. The `hostname` is privacy. **Does the quick start test work on your system?:** No. **Any additional context:** . The GPU is NVIDIA GeForce 3090. The GPU Driver Version: 520.61.05. The CUDA version in the host is V11.8.89 as followings:. ![image](https://user-images.githubusercontent.com/43125963/225341539-aa2ee3c6-c376-4758-a582-c8fd871b0508.png). It seems that the Deepvariant v1.4.0 in the singularity image has already installed CUDA v11.3. . ![image](https://user-images.githubusercontent.com/43125963/225343337-d0924a9b-4b9d-4b03-848f-e8e9753eb377.png). I don't know whether it causes the program crash.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:608,availability,Error,Error,608,"'CUDA_ERROR_UNKNOWN' error using DeepVariant GPU version.; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**: Yes. **Describe the issue:**. 'CUDA_ERROR_UNKNOWN' using DeepVariant GPU version. **Setup**. - Operating system: CentOS Linux release 7.4.1708 (Core), Linux 5.10.150-1.el7.x86_64. - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): singularity image build from dockerhub. - Type of data: nothing special that is unlike the case studies. **Steps to reproduce:**. - Command: /opt/deepvariant/bin/run_deepvariant --version. - Error trace: (if applicable). ```. tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error. tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: . tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: . tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: INVALID_ARGUMENT: expected %d.%d, %d.%d.%d, or %d.%d.%d.%d form for driver version; got ""1"". tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 520.61.5. DeepVariant version 1.4.0. ```. The `hostname` is privacy. **Does the quick start test work on your system?:** No. **Any additional context:** . The GPU is NVIDIA GeForce 3090. The GPU Driver Version: 520.61.05. The CUDA version in the host is V11.8.89 as followings:. ![image](https://user-images.githubusercontent.com/43125963/225341539-aa2ee3c6-c376-4758-a582-c8fd871b0508.png). It seems that the Deepvariant v1.4.0 in the singularity image has already installed CUDA v11.3. . ![image](https://user-images.githubusercontent.com/43125963/225343337-d0924a9b-4b9d-4b03-848f-e8e9753eb377.png). I don't know whether it causes the program crash.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:746,availability,error,error,746,"'CUDA_ERROR_UNKNOWN' error using DeepVariant GPU version.; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**: Yes. **Describe the issue:**. 'CUDA_ERROR_UNKNOWN' using DeepVariant GPU version. **Setup**. - Operating system: CentOS Linux release 7.4.1708 (Core), Linux 5.10.150-1.el7.x86_64. - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): singularity image build from dockerhub. - Type of data: nothing special that is unlike the case studies. **Steps to reproduce:**. - Command: /opt/deepvariant/bin/run_deepvariant --version. - Error trace: (if applicable). ```. tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error. tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: . tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: . tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: INVALID_ARGUMENT: expected %d.%d, %d.%d.%d, or %d.%d.%d.%d form for driver version; got ""1"". tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 520.61.5. DeepVariant version 1.4.0. ```. The `hostname` is privacy. **Does the quick start test work on your system?:** No. **Any additional context:** . The GPU is NVIDIA GeForce 3090. The GPU Driver Version: 520.61.05. The CUDA version in the host is V11.8.89 as followings:. ![image](https://user-images.githubusercontent.com/43125963/225341539-aa2ee3c6-c376-4758-a582-c8fd871b0508.png). It seems that the Deepvariant v1.4.0 in the singularity image has already installed CUDA v11.3. . ![image](https://user-images.githubusercontent.com/43125963/225343337-d0924a9b-4b9d-4b03-848f-e8e9753eb377.png). I don't know whether it causes the program crash.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:49,deployability,version,version,49,"'CUDA_ERROR_UNKNOWN' error using DeepVariant GPU version.; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**: Yes. **Describe the issue:**. 'CUDA_ERROR_UNKNOWN' using DeepVariant GPU version. **Setup**. - Operating system: CentOS Linux release 7.4.1708 (Core), Linux 5.10.150-1.el7.x86_64. - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): singularity image build from dockerhub. - Type of data: nothing special that is unlike the case studies. **Steps to reproduce:**. - Command: /opt/deepvariant/bin/run_deepvariant --version. - Error trace: (if applicable). ```. tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error. tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: . tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: . tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: INVALID_ARGUMENT: expected %d.%d, %d.%d.%d, or %d.%d.%d.%d form for driver version; got ""1"". tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 520.61.5. DeepVariant version 1.4.0. ```. The `hostname` is privacy. **Does the quick start test work on your system?:** No. **Any additional context:** . The GPU is NVIDIA GeForce 3090. The GPU Driver Version: 520.61.05. The CUDA version in the host is V11.8.89 as followings:. ![image](https://user-images.githubusercontent.com/43125963/225341539-aa2ee3c6-c376-4758-a582-c8fd871b0508.png). It seems that the Deepvariant v1.4.0 in the singularity image has already installed CUDA v11.3. . ![image](https://user-images.githubusercontent.com/43125963/225343337-d0924a9b-4b9d-4b03-848f-e8e9753eb377.png). I don't know whether it causes the program crash.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:223,deployability,version,version,223,"'CUDA_ERROR_UNKNOWN' error using DeepVariant GPU version.; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**: Yes. **Describe the issue:**. 'CUDA_ERROR_UNKNOWN' using DeepVariant GPU version. **Setup**. - Operating system: CentOS Linux release 7.4.1708 (Core), Linux 5.10.150-1.el7.x86_64. - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): singularity image build from dockerhub. - Type of data: nothing special that is unlike the case studies. **Steps to reproduce:**. - Command: /opt/deepvariant/bin/run_deepvariant --version. - Error trace: (if applicable). ```. tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error. tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: . tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: . tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: INVALID_ARGUMENT: expected %d.%d, %d.%d.%d, or %d.%d.%d.%d form for driver version; got ""1"". tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 520.61.5. DeepVariant version 1.4.0. ```. The `hostname` is privacy. **Does the quick start test work on your system?:** No. **Any additional context:** . The GPU is NVIDIA GeForce 3090. The GPU Driver Version: 520.61.05. The CUDA version in the host is V11.8.89 as followings:. ![image](https://user-images.githubusercontent.com/43125963/225341539-aa2ee3c6-c376-4758-a582-c8fd871b0508.png). It seems that the Deepvariant v1.4.0 in the singularity image has already installed CUDA v11.3. . ![image](https://user-images.githubusercontent.com/43125963/225343337-d0924a9b-4b9d-4b03-848f-e8e9753eb377.png). I don't know whether it causes the program crash.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:276,deployability,releas,release,276,"'CUDA_ERROR_UNKNOWN' error using DeepVariant GPU version.; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**: Yes. **Describe the issue:**. 'CUDA_ERROR_UNKNOWN' using DeepVariant GPU version. **Setup**. - Operating system: CentOS Linux release 7.4.1708 (Core), Linux 5.10.150-1.el7.x86_64. - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): singularity image build from dockerhub. - Type of data: nothing special that is unlike the case studies. **Steps to reproduce:**. - Command: /opt/deepvariant/bin/run_deepvariant --version. - Error trace: (if applicable). ```. tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error. tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: . tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: . tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: INVALID_ARGUMENT: expected %d.%d, %d.%d.%d, or %d.%d.%d.%d form for driver version; got ""1"". tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 520.61.5. DeepVariant version 1.4.0. ```. The `hostname` is privacy. **Does the quick start test work on your system?:** No. **Any additional context:** . The GPU is NVIDIA GeForce 3090. The GPU Driver Version: 520.61.05. The CUDA version in the host is V11.8.89 as followings:. ![image](https://user-images.githubusercontent.com/43125963/225341539-aa2ee3c6-c376-4758-a582-c8fd871b0508.png). It seems that the Deepvariant v1.4.0 in the singularity image has already installed CUDA v11.3. . ![image](https://user-images.githubusercontent.com/43125963/225343337-d0924a9b-4b9d-4b03-848f-e8e9753eb377.png). I don't know whether it causes the program crash.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:344,deployability,version,version,344,"'CUDA_ERROR_UNKNOWN' error using DeepVariant GPU version.; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**: Yes. **Describe the issue:**. 'CUDA_ERROR_UNKNOWN' using DeepVariant GPU version. **Setup**. - Operating system: CentOS Linux release 7.4.1708 (Core), Linux 5.10.150-1.el7.x86_64. - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): singularity image build from dockerhub. - Type of data: nothing special that is unlike the case studies. **Steps to reproduce:**. - Command: /opt/deepvariant/bin/run_deepvariant --version. - Error trace: (if applicable). ```. tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error. tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: . tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: . tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: INVALID_ARGUMENT: expected %d.%d, %d.%d.%d, or %d.%d.%d.%d form for driver version; got ""1"". tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 520.61.5. DeepVariant version 1.4.0. ```. The `hostname` is privacy. **Does the quick start test work on your system?:** No. **Any additional context:** . The GPU is NVIDIA GeForce 3090. The GPU Driver Version: 520.61.05. The CUDA version in the host is V11.8.89 as followings:. ![image](https://user-images.githubusercontent.com/43125963/225341539-aa2ee3c6-c376-4758-a582-c8fd871b0508.png). It seems that the Deepvariant v1.4.0 in the singularity image has already installed CUDA v11.3. . ![image](https://user-images.githubusercontent.com/43125963/225343337-d0924a9b-4b9d-4b03-848f-e8e9753eb377.png). I don't know whether it causes the program crash.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:362,deployability,Instal,Installation,362,"'CUDA_ERROR_UNKNOWN' error using DeepVariant GPU version.; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**: Yes. **Describe the issue:**. 'CUDA_ERROR_UNKNOWN' using DeepVariant GPU version. **Setup**. - Operating system: CentOS Linux release 7.4.1708 (Core), Linux 5.10.150-1.el7.x86_64. - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): singularity image build from dockerhub. - Type of data: nothing special that is unlike the case studies. **Steps to reproduce:**. - Command: /opt/deepvariant/bin/run_deepvariant --version. - Error trace: (if applicable). ```. tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error. tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: . tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: . tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: INVALID_ARGUMENT: expected %d.%d, %d.%d.%d, or %d.%d.%d.%d form for driver version; got ""1"". tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 520.61.5. DeepVariant version 1.4.0. ```. The `hostname` is privacy. **Does the quick start test work on your system?:** No. **Any additional context:** . The GPU is NVIDIA GeForce 3090. The GPU Driver Version: 520.61.05. The CUDA version in the host is V11.8.89 as followings:. ![image](https://user-images.githubusercontent.com/43125963/225341539-aa2ee3c6-c376-4758-a582-c8fd871b0508.png). It seems that the Deepvariant v1.4.0 in the singularity image has already installed CUDA v11.3. . ![image](https://user-images.githubusercontent.com/43125963/225343337-d0924a9b-4b9d-4b03-848f-e8e9753eb377.png). I don't know whether it causes the program crash.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:435,deployability,build,build,435,"'CUDA_ERROR_UNKNOWN' error using DeepVariant GPU version.; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**: Yes. **Describe the issue:**. 'CUDA_ERROR_UNKNOWN' using DeepVariant GPU version. **Setup**. - Operating system: CentOS Linux release 7.4.1708 (Core), Linux 5.10.150-1.el7.x86_64. - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): singularity image build from dockerhub. - Type of data: nothing special that is unlike the case studies. **Steps to reproduce:**. - Command: /opt/deepvariant/bin/run_deepvariant --version. - Error trace: (if applicable). ```. tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error. tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: . tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: . tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: INVALID_ARGUMENT: expected %d.%d, %d.%d.%d, or %d.%d.%d.%d form for driver version; got ""1"". tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 520.61.5. DeepVariant version 1.4.0. ```. The `hostname` is privacy. **Does the quick start test work on your system?:** No. **Any additional context:** . The GPU is NVIDIA GeForce 3090. The GPU Driver Version: 520.61.05. The CUDA version in the host is V11.8.89 as followings:. ![image](https://user-images.githubusercontent.com/43125963/225341539-aa2ee3c6-c376-4758-a582-c8fd871b0508.png). It seems that the Deepvariant v1.4.0 in the singularity image has already installed CUDA v11.3. . ![image](https://user-images.githubusercontent.com/43125963/225343337-d0924a9b-4b9d-4b03-848f-e8e9753eb377.png). I don't know whether it causes the program crash.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:597,deployability,version,version,597,"'CUDA_ERROR_UNKNOWN' error using DeepVariant GPU version.; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**: Yes. **Describe the issue:**. 'CUDA_ERROR_UNKNOWN' using DeepVariant GPU version. **Setup**. - Operating system: CentOS Linux release 7.4.1708 (Core), Linux 5.10.150-1.el7.x86_64. - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): singularity image build from dockerhub. - Type of data: nothing special that is unlike the case studies. **Steps to reproduce:**. - Command: /opt/deepvariant/bin/run_deepvariant --version. - Error trace: (if applicable). ```. tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error. tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: . tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: . tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: INVALID_ARGUMENT: expected %d.%d, %d.%d.%d, or %d.%d.%d.%d form for driver version; got ""1"". tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 520.61.5. DeepVariant version 1.4.0. ```. The `hostname` is privacy. **Does the quick start test work on your system?:** No. **Any additional context:** . The GPU is NVIDIA GeForce 3090. The GPU Driver Version: 520.61.05. The CUDA version in the host is V11.8.89 as followings:. ![image](https://user-images.githubusercontent.com/43125963/225341539-aa2ee3c6-c376-4758-a582-c8fd871b0508.png). It seems that the Deepvariant v1.4.0 in the singularity image has already installed CUDA v11.3. . ![image](https://user-images.githubusercontent.com/43125963/225343337-d0924a9b-4b9d-4b03-848f-e8e9753eb377.png). I don't know whether it causes the program crash.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:695,deployability,fail,failed,695,"'CUDA_ERROR_UNKNOWN' error using DeepVariant GPU version.; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**: Yes. **Describe the issue:**. 'CUDA_ERROR_UNKNOWN' using DeepVariant GPU version. **Setup**. - Operating system: CentOS Linux release 7.4.1708 (Core), Linux 5.10.150-1.el7.x86_64. - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): singularity image build from dockerhub. - Type of data: nothing special that is unlike the case studies. **Steps to reproduce:**. - Command: /opt/deepvariant/bin/run_deepvariant --version. - Error trace: (if applicable). ```. tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error. tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: . tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: . tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: INVALID_ARGUMENT: expected %d.%d, %d.%d.%d, or %d.%d.%d.%d form for driver version; got ""1"". tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 520.61.5. DeepVariant version 1.4.0. ```. The `hostname` is privacy. **Does the quick start test work on your system?:** No. **Any additional context:** . The GPU is NVIDIA GeForce 3090. The GPU Driver Version: 520.61.05. The CUDA version in the host is V11.8.89 as followings:. ![image](https://user-images.githubusercontent.com/43125963/225341539-aa2ee3c6-c376-4758-a582-c8fd871b0508.png). It seems that the Deepvariant v1.4.0 in the singularity image has already installed CUDA v11.3. . ![image](https://user-images.githubusercontent.com/43125963/225343337-d0924a9b-4b9d-4b03-848f-e8e9753eb377.png). I don't know whether it causes the program crash.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:1004,deployability,version,version,1004,"'CUDA_ERROR_UNKNOWN' error using DeepVariant GPU version.; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**: Yes. **Describe the issue:**. 'CUDA_ERROR_UNKNOWN' using DeepVariant GPU version. **Setup**. - Operating system: CentOS Linux release 7.4.1708 (Core), Linux 5.10.150-1.el7.x86_64. - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): singularity image build from dockerhub. - Type of data: nothing special that is unlike the case studies. **Steps to reproduce:**. - Command: /opt/deepvariant/bin/run_deepvariant --version. - Error trace: (if applicable). ```. tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error. tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: . tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: . tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: INVALID_ARGUMENT: expected %d.%d, %d.%d.%d, or %d.%d.%d.%d form for driver version; got ""1"". tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 520.61.5. DeepVariant version 1.4.0. ```. The `hostname` is privacy. **Does the quick start test work on your system?:** No. **Any additional context:** . The GPU is NVIDIA GeForce 3090. The GPU Driver Version: 520.61.05. The CUDA version in the host is V11.8.89 as followings:. ![image](https://user-images.githubusercontent.com/43125963/225341539-aa2ee3c6-c376-4758-a582-c8fd871b0508.png). It seems that the Deepvariant v1.4.0 in the singularity image has already installed CUDA v11.3. . ![image](https://user-images.githubusercontent.com/43125963/225343337-d0924a9b-4b9d-4b03-848f-e8e9753eb377.png). I don't know whether it causes the program crash.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:1091,deployability,version,version,1091,"'CUDA_ERROR_UNKNOWN' error using DeepVariant GPU version.; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**: Yes. **Describe the issue:**. 'CUDA_ERROR_UNKNOWN' using DeepVariant GPU version. **Setup**. - Operating system: CentOS Linux release 7.4.1708 (Core), Linux 5.10.150-1.el7.x86_64. - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): singularity image build from dockerhub. - Type of data: nothing special that is unlike the case studies. **Steps to reproduce:**. - Command: /opt/deepvariant/bin/run_deepvariant --version. - Error trace: (if applicable). ```. tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error. tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: . tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: . tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: INVALID_ARGUMENT: expected %d.%d, %d.%d.%d, or %d.%d.%d.%d form for driver version; got ""1"". tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 520.61.5. DeepVariant version 1.4.0. ```. The `hostname` is privacy. **Does the quick start test work on your system?:** No. **Any additional context:** . The GPU is NVIDIA GeForce 3090. The GPU Driver Version: 520.61.05. The CUDA version in the host is V11.8.89 as followings:. ![image](https://user-images.githubusercontent.com/43125963/225341539-aa2ee3c6-c376-4758-a582-c8fd871b0508.png). It seems that the Deepvariant v1.4.0 in the singularity image has already installed CUDA v11.3. . ![image](https://user-images.githubusercontent.com/43125963/225343337-d0924a9b-4b9d-4b03-848f-e8e9753eb377.png). I don't know whether it causes the program crash.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:1182,deployability,version,version,1182,"'CUDA_ERROR_UNKNOWN' error using DeepVariant GPU version.; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**: Yes. **Describe the issue:**. 'CUDA_ERROR_UNKNOWN' using DeepVariant GPU version. **Setup**. - Operating system: CentOS Linux release 7.4.1708 (Core), Linux 5.10.150-1.el7.x86_64. - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): singularity image build from dockerhub. - Type of data: nothing special that is unlike the case studies. **Steps to reproduce:**. - Command: /opt/deepvariant/bin/run_deepvariant --version. - Error trace: (if applicable). ```. tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error. tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: . tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: . tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: INVALID_ARGUMENT: expected %d.%d, %d.%d.%d, or %d.%d.%d.%d form for driver version; got ""1"". tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 520.61.5. DeepVariant version 1.4.0. ```. The `hostname` is privacy. **Does the quick start test work on your system?:** No. **Any additional context:** . The GPU is NVIDIA GeForce 3090. The GPU Driver Version: 520.61.05. The CUDA version in the host is V11.8.89 as followings:. ![image](https://user-images.githubusercontent.com/43125963/225341539-aa2ee3c6-c376-4758-a582-c8fd871b0508.png). It seems that the Deepvariant v1.4.0 in the singularity image has already installed CUDA v11.3. . ![image](https://user-images.githubusercontent.com/43125963/225343337-d0924a9b-4b9d-4b03-848f-e8e9753eb377.png). I don't know whether it causes the program crash.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:1216,deployability,version,version,1216,"'CUDA_ERROR_UNKNOWN' error using DeepVariant GPU version.; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**: Yes. **Describe the issue:**. 'CUDA_ERROR_UNKNOWN' using DeepVariant GPU version. **Setup**. - Operating system: CentOS Linux release 7.4.1708 (Core), Linux 5.10.150-1.el7.x86_64. - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): singularity image build from dockerhub. - Type of data: nothing special that is unlike the case studies. **Steps to reproduce:**. - Command: /opt/deepvariant/bin/run_deepvariant --version. - Error trace: (if applicable). ```. tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error. tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: . tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: . tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: INVALID_ARGUMENT: expected %d.%d, %d.%d.%d, or %d.%d.%d.%d form for driver version; got ""1"". tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 520.61.5. DeepVariant version 1.4.0. ```. The `hostname` is privacy. **Does the quick start test work on your system?:** No. **Any additional context:** . The GPU is NVIDIA GeForce 3090. The GPU Driver Version: 520.61.05. The CUDA version in the host is V11.8.89 as followings:. ![image](https://user-images.githubusercontent.com/43125963/225341539-aa2ee3c6-c376-4758-a582-c8fd871b0508.png). It seems that the Deepvariant v1.4.0 in the singularity image has already installed CUDA v11.3. . ![image](https://user-images.githubusercontent.com/43125963/225343337-d0924a9b-4b9d-4b03-848f-e8e9753eb377.png). I don't know whether it causes the program crash.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:1396,deployability,Version,Version,1396,"'CUDA_ERROR_UNKNOWN' error using DeepVariant GPU version.; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**: Yes. **Describe the issue:**. 'CUDA_ERROR_UNKNOWN' using DeepVariant GPU version. **Setup**. - Operating system: CentOS Linux release 7.4.1708 (Core), Linux 5.10.150-1.el7.x86_64. - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): singularity image build from dockerhub. - Type of data: nothing special that is unlike the case studies. **Steps to reproduce:**. - Command: /opt/deepvariant/bin/run_deepvariant --version. - Error trace: (if applicable). ```. tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error. tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: . tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: . tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: INVALID_ARGUMENT: expected %d.%d, %d.%d.%d, or %d.%d.%d.%d form for driver version; got ""1"". tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 520.61.5. DeepVariant version 1.4.0. ```. The `hostname` is privacy. **Does the quick start test work on your system?:** No. **Any additional context:** . The GPU is NVIDIA GeForce 3090. The GPU Driver Version: 520.61.05. The CUDA version in the host is V11.8.89 as followings:. ![image](https://user-images.githubusercontent.com/43125963/225341539-aa2ee3c6-c376-4758-a582-c8fd871b0508.png). It seems that the Deepvariant v1.4.0 in the singularity image has already installed CUDA v11.3. . ![image](https://user-images.githubusercontent.com/43125963/225343337-d0924a9b-4b9d-4b03-848f-e8e9753eb377.png). I don't know whether it causes the program crash.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:1425,deployability,version,version,1425,"'CUDA_ERROR_UNKNOWN' error using DeepVariant GPU version.; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**: Yes. **Describe the issue:**. 'CUDA_ERROR_UNKNOWN' using DeepVariant GPU version. **Setup**. - Operating system: CentOS Linux release 7.4.1708 (Core), Linux 5.10.150-1.el7.x86_64. - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): singularity image build from dockerhub. - Type of data: nothing special that is unlike the case studies. **Steps to reproduce:**. - Command: /opt/deepvariant/bin/run_deepvariant --version. - Error trace: (if applicable). ```. tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error. tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: . tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: . tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: INVALID_ARGUMENT: expected %d.%d, %d.%d.%d, or %d.%d.%d.%d form for driver version; got ""1"". tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 520.61.5. DeepVariant version 1.4.0. ```. The `hostname` is privacy. **Does the quick start test work on your system?:** No. **Any additional context:** . The GPU is NVIDIA GeForce 3090. The GPU Driver Version: 520.61.05. The CUDA version in the host is V11.8.89 as followings:. ![image](https://user-images.githubusercontent.com/43125963/225341539-aa2ee3c6-c376-4758-a582-c8fd871b0508.png). It seems that the Deepvariant v1.4.0 in the singularity image has already installed CUDA v11.3. . ![image](https://user-images.githubusercontent.com/43125963/225343337-d0924a9b-4b9d-4b03-848f-e8e9753eb377.png). I don't know whether it causes the program crash.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:1660,deployability,instal,installed,1660,"'CUDA_ERROR_UNKNOWN' error using DeepVariant GPU version.; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**: Yes. **Describe the issue:**. 'CUDA_ERROR_UNKNOWN' using DeepVariant GPU version. **Setup**. - Operating system: CentOS Linux release 7.4.1708 (Core), Linux 5.10.150-1.el7.x86_64. - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): singularity image build from dockerhub. - Type of data: nothing special that is unlike the case studies. **Steps to reproduce:**. - Command: /opt/deepvariant/bin/run_deepvariant --version. - Error trace: (if applicable). ```. tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error. tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: . tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: . tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: INVALID_ARGUMENT: expected %d.%d, %d.%d.%d, or %d.%d.%d.%d form for driver version; got ""1"". tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 520.61.5. DeepVariant version 1.4.0. ```. The `hostname` is privacy. **Does the quick start test work on your system?:** No. **Any additional context:** . The GPU is NVIDIA GeForce 3090. The GPU Driver Version: 520.61.05. The CUDA version in the host is V11.8.89 as followings:. ![image](https://user-images.githubusercontent.com/43125963/225341539-aa2ee3c6-c376-4758-a582-c8fd871b0508.png). It seems that the Deepvariant v1.4.0 in the singularity image has already installed CUDA v11.3. . ![image](https://user-images.githubusercontent.com/43125963/225343337-d0924a9b-4b9d-4b03-848f-e8e9753eb377.png). I don't know whether it causes the program crash.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:45,energy efficiency,GPU,GPU,45,"'CUDA_ERROR_UNKNOWN' error using DeepVariant GPU version.; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**: Yes. **Describe the issue:**. 'CUDA_ERROR_UNKNOWN' using DeepVariant GPU version. **Setup**. - Operating system: CentOS Linux release 7.4.1708 (Core), Linux 5.10.150-1.el7.x86_64. - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): singularity image build from dockerhub. - Type of data: nothing special that is unlike the case studies. **Steps to reproduce:**. - Command: /opt/deepvariant/bin/run_deepvariant --version. - Error trace: (if applicable). ```. tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error. tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: . tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: . tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: INVALID_ARGUMENT: expected %d.%d, %d.%d.%d, or %d.%d.%d.%d form for driver version; got ""1"". tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 520.61.5. DeepVariant version 1.4.0. ```. The `hostname` is privacy. **Does the quick start test work on your system?:** No. **Any additional context:** . The GPU is NVIDIA GeForce 3090. The GPU Driver Version: 520.61.05. The CUDA version in the host is V11.8.89 as followings:. ![image](https://user-images.githubusercontent.com/43125963/225341539-aa2ee3c6-c376-4758-a582-c8fd871b0508.png). It seems that the Deepvariant v1.4.0 in the singularity image has already installed CUDA v11.3. . ![image](https://user-images.githubusercontent.com/43125963/225343337-d0924a9b-4b9d-4b03-848f-e8e9753eb377.png). I don't know whether it causes the program crash.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:219,energy efficiency,GPU,GPU,219,"'CUDA_ERROR_UNKNOWN' error using DeepVariant GPU version.; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**: Yes. **Describe the issue:**. 'CUDA_ERROR_UNKNOWN' using DeepVariant GPU version. **Setup**. - Operating system: CentOS Linux release 7.4.1708 (Core), Linux 5.10.150-1.el7.x86_64. - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): singularity image build from dockerhub. - Type of data: nothing special that is unlike the case studies. **Steps to reproduce:**. - Command: /opt/deepvariant/bin/run_deepvariant --version. - Error trace: (if applicable). ```. tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error. tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: . tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: . tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: INVALID_ARGUMENT: expected %d.%d, %d.%d.%d, or %d.%d.%d.%d form for driver version; got ""1"". tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 520.61.5. DeepVariant version 1.4.0. ```. The `hostname` is privacy. **Does the quick start test work on your system?:** No. **Any additional context:** . The GPU is NVIDIA GeForce 3090. The GPU Driver Version: 520.61.05. The CUDA version in the host is V11.8.89 as followings:. ![image](https://user-images.githubusercontent.com/43125963/225341539-aa2ee3c6-c376-4758-a582-c8fd871b0508.png). It seems that the Deepvariant v1.4.0 in the singularity image has already installed CUDA v11.3. . ![image](https://user-images.githubusercontent.com/43125963/225343337-d0924a9b-4b9d-4b03-848f-e8e9753eb377.png). I don't know whether it causes the program crash.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:294,energy efficiency,Core,Core,294,"'CUDA_ERROR_UNKNOWN' error using DeepVariant GPU version.; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**: Yes. **Describe the issue:**. 'CUDA_ERROR_UNKNOWN' using DeepVariant GPU version. **Setup**. - Operating system: CentOS Linux release 7.4.1708 (Core), Linux 5.10.150-1.el7.x86_64. - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): singularity image build from dockerhub. - Type of data: nothing special that is unlike the case studies. **Steps to reproduce:**. - Command: /opt/deepvariant/bin/run_deepvariant --version. - Error trace: (if applicable). ```. tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error. tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: . tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: . tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: INVALID_ARGUMENT: expected %d.%d, %d.%d.%d, or %d.%d.%d.%d form for driver version; got ""1"". tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 520.61.5. DeepVariant version 1.4.0. ```. The `hostname` is privacy. **Does the quick start test work on your system?:** No. **Any additional context:** . The GPU is NVIDIA GeForce 3090. The GPU Driver Version: 520.61.05. The CUDA version in the host is V11.8.89 as followings:. ![image](https://user-images.githubusercontent.com/43125963/225341539-aa2ee3c6-c376-4758-a582-c8fd871b0508.png). It seems that the Deepvariant v1.4.0 in the singularity image has already installed CUDA v11.3. . ![image](https://user-images.githubusercontent.com/43125963/225343337-d0924a9b-4b9d-4b03-848f-e8e9753eb377.png). I don't know whether it causes the program crash.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:1353,energy efficiency,GPU,GPU,1353,"'CUDA_ERROR_UNKNOWN' error using DeepVariant GPU version.; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**: Yes. **Describe the issue:**. 'CUDA_ERROR_UNKNOWN' using DeepVariant GPU version. **Setup**. - Operating system: CentOS Linux release 7.4.1708 (Core), Linux 5.10.150-1.el7.x86_64. - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): singularity image build from dockerhub. - Type of data: nothing special that is unlike the case studies. **Steps to reproduce:**. - Command: /opt/deepvariant/bin/run_deepvariant --version. - Error trace: (if applicable). ```. tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error. tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: . tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: . tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: INVALID_ARGUMENT: expected %d.%d, %d.%d.%d, or %d.%d.%d.%d form for driver version; got ""1"". tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 520.61.5. DeepVariant version 1.4.0. ```. The `hostname` is privacy. **Does the quick start test work on your system?:** No. **Any additional context:** . The GPU is NVIDIA GeForce 3090. The GPU Driver Version: 520.61.05. The CUDA version in the host is V11.8.89 as followings:. ![image](https://user-images.githubusercontent.com/43125963/225341539-aa2ee3c6-c376-4758-a582-c8fd871b0508.png). It seems that the Deepvariant v1.4.0 in the singularity image has already installed CUDA v11.3. . ![image](https://user-images.githubusercontent.com/43125963/225343337-d0924a9b-4b9d-4b03-848f-e8e9753eb377.png). I don't know whether it causes the program crash.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:1385,energy efficiency,GPU,GPU,1385,"'CUDA_ERROR_UNKNOWN' error using DeepVariant GPU version.; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**: Yes. **Describe the issue:**. 'CUDA_ERROR_UNKNOWN' using DeepVariant GPU version. **Setup**. - Operating system: CentOS Linux release 7.4.1708 (Core), Linux 5.10.150-1.el7.x86_64. - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): singularity image build from dockerhub. - Type of data: nothing special that is unlike the case studies. **Steps to reproduce:**. - Command: /opt/deepvariant/bin/run_deepvariant --version. - Error trace: (if applicable). ```. tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error. tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: . tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: . tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: INVALID_ARGUMENT: expected %d.%d, %d.%d.%d, or %d.%d.%d.%d form for driver version; got ""1"". tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 520.61.5. DeepVariant version 1.4.0. ```. The `hostname` is privacy. **Does the quick start test work on your system?:** No. **Any additional context:** . The GPU is NVIDIA GeForce 3090. The GPU Driver Version: 520.61.05. The CUDA version in the host is V11.8.89 as followings:. ![image](https://user-images.githubusercontent.com/43125963/225341539-aa2ee3c6-c376-4758-a582-c8fd871b0508.png). It seems that the Deepvariant v1.4.0 in the singularity image has already installed CUDA v11.3. . ![image](https://user-images.githubusercontent.com/43125963/225343337-d0924a9b-4b9d-4b03-848f-e8e9753eb377.png). I don't know whether it causes the program crash.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:49,integrability,version,version,49,"'CUDA_ERROR_UNKNOWN' error using DeepVariant GPU version.; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**: Yes. **Describe the issue:**. 'CUDA_ERROR_UNKNOWN' using DeepVariant GPU version. **Setup**. - Operating system: CentOS Linux release 7.4.1708 (Core), Linux 5.10.150-1.el7.x86_64. - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): singularity image build from dockerhub. - Type of data: nothing special that is unlike the case studies. **Steps to reproduce:**. - Command: /opt/deepvariant/bin/run_deepvariant --version. - Error trace: (if applicable). ```. tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error. tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: . tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: . tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: INVALID_ARGUMENT: expected %d.%d, %d.%d.%d, or %d.%d.%d.%d form for driver version; got ""1"". tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 520.61.5. DeepVariant version 1.4.0. ```. The `hostname` is privacy. **Does the quick start test work on your system?:** No. **Any additional context:** . The GPU is NVIDIA GeForce 3090. The GPU Driver Version: 520.61.05. The CUDA version in the host is V11.8.89 as followings:. ![image](https://user-images.githubusercontent.com/43125963/225341539-aa2ee3c6-c376-4758-a582-c8fd871b0508.png). It seems that the Deepvariant v1.4.0 in the singularity image has already installed CUDA v11.3. . ![image](https://user-images.githubusercontent.com/43125963/225343337-d0924a9b-4b9d-4b03-848f-e8e9753eb377.png). I don't know whether it causes the program crash.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:223,integrability,version,version,223,"'CUDA_ERROR_UNKNOWN' error using DeepVariant GPU version.; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**: Yes. **Describe the issue:**. 'CUDA_ERROR_UNKNOWN' using DeepVariant GPU version. **Setup**. - Operating system: CentOS Linux release 7.4.1708 (Core), Linux 5.10.150-1.el7.x86_64. - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): singularity image build from dockerhub. - Type of data: nothing special that is unlike the case studies. **Steps to reproduce:**. - Command: /opt/deepvariant/bin/run_deepvariant --version. - Error trace: (if applicable). ```. tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error. tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: . tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: . tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: INVALID_ARGUMENT: expected %d.%d, %d.%d.%d, or %d.%d.%d.%d form for driver version; got ""1"". tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 520.61.5. DeepVariant version 1.4.0. ```. The `hostname` is privacy. **Does the quick start test work on your system?:** No. **Any additional context:** . The GPU is NVIDIA GeForce 3090. The GPU Driver Version: 520.61.05. The CUDA version in the host is V11.8.89 as followings:. ![image](https://user-images.githubusercontent.com/43125963/225341539-aa2ee3c6-c376-4758-a582-c8fd871b0508.png). It seems that the Deepvariant v1.4.0 in the singularity image has already installed CUDA v11.3. . ![image](https://user-images.githubusercontent.com/43125963/225343337-d0924a9b-4b9d-4b03-848f-e8e9753eb377.png). I don't know whether it causes the program crash.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:344,integrability,version,version,344,"'CUDA_ERROR_UNKNOWN' error using DeepVariant GPU version.; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**: Yes. **Describe the issue:**. 'CUDA_ERROR_UNKNOWN' using DeepVariant GPU version. **Setup**. - Operating system: CentOS Linux release 7.4.1708 (Core), Linux 5.10.150-1.el7.x86_64. - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): singularity image build from dockerhub. - Type of data: nothing special that is unlike the case studies. **Steps to reproduce:**. - Command: /opt/deepvariant/bin/run_deepvariant --version. - Error trace: (if applicable). ```. tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error. tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: . tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: . tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: INVALID_ARGUMENT: expected %d.%d, %d.%d.%d, or %d.%d.%d.%d form for driver version; got ""1"". tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 520.61.5. DeepVariant version 1.4.0. ```. The `hostname` is privacy. **Does the quick start test work on your system?:** No. **Any additional context:** . The GPU is NVIDIA GeForce 3090. The GPU Driver Version: 520.61.05. The CUDA version in the host is V11.8.89 as followings:. ![image](https://user-images.githubusercontent.com/43125963/225341539-aa2ee3c6-c376-4758-a582-c8fd871b0508.png). It seems that the Deepvariant v1.4.0 in the singularity image has already installed CUDA v11.3. . ![image](https://user-images.githubusercontent.com/43125963/225343337-d0924a9b-4b9d-4b03-848f-e8e9753eb377.png). I don't know whether it causes the program crash.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:597,integrability,version,version,597,"'CUDA_ERROR_UNKNOWN' error using DeepVariant GPU version.; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**: Yes. **Describe the issue:**. 'CUDA_ERROR_UNKNOWN' using DeepVariant GPU version. **Setup**. - Operating system: CentOS Linux release 7.4.1708 (Core), Linux 5.10.150-1.el7.x86_64. - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): singularity image build from dockerhub. - Type of data: nothing special that is unlike the case studies. **Steps to reproduce:**. - Command: /opt/deepvariant/bin/run_deepvariant --version. - Error trace: (if applicable). ```. tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error. tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: . tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: . tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: INVALID_ARGUMENT: expected %d.%d, %d.%d.%d, or %d.%d.%d.%d form for driver version; got ""1"". tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 520.61.5. DeepVariant version 1.4.0. ```. The `hostname` is privacy. **Does the quick start test work on your system?:** No. **Any additional context:** . The GPU is NVIDIA GeForce 3090. The GPU Driver Version: 520.61.05. The CUDA version in the host is V11.8.89 as followings:. ![image](https://user-images.githubusercontent.com/43125963/225341539-aa2ee3c6-c376-4758-a582-c8fd871b0508.png). It seems that the Deepvariant v1.4.0 in the singularity image has already installed CUDA v11.3. . ![image](https://user-images.githubusercontent.com/43125963/225343337-d0924a9b-4b9d-4b03-848f-e8e9753eb377.png). I don't know whether it causes the program crash.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:1004,integrability,version,version,1004,"'CUDA_ERROR_UNKNOWN' error using DeepVariant GPU version.; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**: Yes. **Describe the issue:**. 'CUDA_ERROR_UNKNOWN' using DeepVariant GPU version. **Setup**. - Operating system: CentOS Linux release 7.4.1708 (Core), Linux 5.10.150-1.el7.x86_64. - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): singularity image build from dockerhub. - Type of data: nothing special that is unlike the case studies. **Steps to reproduce:**. - Command: /opt/deepvariant/bin/run_deepvariant --version. - Error trace: (if applicable). ```. tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error. tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: . tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: . tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: INVALID_ARGUMENT: expected %d.%d, %d.%d.%d, or %d.%d.%d.%d form for driver version; got ""1"". tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 520.61.5. DeepVariant version 1.4.0. ```. The `hostname` is privacy. **Does the quick start test work on your system?:** No. **Any additional context:** . The GPU is NVIDIA GeForce 3090. The GPU Driver Version: 520.61.05. The CUDA version in the host is V11.8.89 as followings:. ![image](https://user-images.githubusercontent.com/43125963/225341539-aa2ee3c6-c376-4758-a582-c8fd871b0508.png). It seems that the Deepvariant v1.4.0 in the singularity image has already installed CUDA v11.3. . ![image](https://user-images.githubusercontent.com/43125963/225343337-d0924a9b-4b9d-4b03-848f-e8e9753eb377.png). I don't know whether it causes the program crash.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:1091,integrability,version,version,1091,"'CUDA_ERROR_UNKNOWN' error using DeepVariant GPU version.; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**: Yes. **Describe the issue:**. 'CUDA_ERROR_UNKNOWN' using DeepVariant GPU version. **Setup**. - Operating system: CentOS Linux release 7.4.1708 (Core), Linux 5.10.150-1.el7.x86_64. - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): singularity image build from dockerhub. - Type of data: nothing special that is unlike the case studies. **Steps to reproduce:**. - Command: /opt/deepvariant/bin/run_deepvariant --version. - Error trace: (if applicable). ```. tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error. tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: . tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: . tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: INVALID_ARGUMENT: expected %d.%d, %d.%d.%d, or %d.%d.%d.%d form for driver version; got ""1"". tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 520.61.5. DeepVariant version 1.4.0. ```. The `hostname` is privacy. **Does the quick start test work on your system?:** No. **Any additional context:** . The GPU is NVIDIA GeForce 3090. The GPU Driver Version: 520.61.05. The CUDA version in the host is V11.8.89 as followings:. ![image](https://user-images.githubusercontent.com/43125963/225341539-aa2ee3c6-c376-4758-a582-c8fd871b0508.png). It seems that the Deepvariant v1.4.0 in the singularity image has already installed CUDA v11.3. . ![image](https://user-images.githubusercontent.com/43125963/225343337-d0924a9b-4b9d-4b03-848f-e8e9753eb377.png). I don't know whether it causes the program crash.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:1182,integrability,version,version,1182,"'CUDA_ERROR_UNKNOWN' error using DeepVariant GPU version.; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**: Yes. **Describe the issue:**. 'CUDA_ERROR_UNKNOWN' using DeepVariant GPU version. **Setup**. - Operating system: CentOS Linux release 7.4.1708 (Core), Linux 5.10.150-1.el7.x86_64. - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): singularity image build from dockerhub. - Type of data: nothing special that is unlike the case studies. **Steps to reproduce:**. - Command: /opt/deepvariant/bin/run_deepvariant --version. - Error trace: (if applicable). ```. tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error. tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: . tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: . tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: INVALID_ARGUMENT: expected %d.%d, %d.%d.%d, or %d.%d.%d.%d form for driver version; got ""1"". tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 520.61.5. DeepVariant version 1.4.0. ```. The `hostname` is privacy. **Does the quick start test work on your system?:** No. **Any additional context:** . The GPU is NVIDIA GeForce 3090. The GPU Driver Version: 520.61.05. The CUDA version in the host is V11.8.89 as followings:. ![image](https://user-images.githubusercontent.com/43125963/225341539-aa2ee3c6-c376-4758-a582-c8fd871b0508.png). It seems that the Deepvariant v1.4.0 in the singularity image has already installed CUDA v11.3. . ![image](https://user-images.githubusercontent.com/43125963/225343337-d0924a9b-4b9d-4b03-848f-e8e9753eb377.png). I don't know whether it causes the program crash.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:1216,integrability,version,version,1216,"'CUDA_ERROR_UNKNOWN' error using DeepVariant GPU version.; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**: Yes. **Describe the issue:**. 'CUDA_ERROR_UNKNOWN' using DeepVariant GPU version. **Setup**. - Operating system: CentOS Linux release 7.4.1708 (Core), Linux 5.10.150-1.el7.x86_64. - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): singularity image build from dockerhub. - Type of data: nothing special that is unlike the case studies. **Steps to reproduce:**. - Command: /opt/deepvariant/bin/run_deepvariant --version. - Error trace: (if applicable). ```. tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error. tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: . tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: . tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: INVALID_ARGUMENT: expected %d.%d, %d.%d.%d, or %d.%d.%d.%d form for driver version; got ""1"". tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 520.61.5. DeepVariant version 1.4.0. ```. The `hostname` is privacy. **Does the quick start test work on your system?:** No. **Any additional context:** . The GPU is NVIDIA GeForce 3090. The GPU Driver Version: 520.61.05. The CUDA version in the host is V11.8.89 as followings:. ![image](https://user-images.githubusercontent.com/43125963/225341539-aa2ee3c6-c376-4758-a582-c8fd871b0508.png). It seems that the Deepvariant v1.4.0 in the singularity image has already installed CUDA v11.3. . ![image](https://user-images.githubusercontent.com/43125963/225343337-d0924a9b-4b9d-4b03-848f-e8e9753eb377.png). I don't know whether it causes the program crash.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:1396,integrability,Version,Version,1396,"'CUDA_ERROR_UNKNOWN' error using DeepVariant GPU version.; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**: Yes. **Describe the issue:**. 'CUDA_ERROR_UNKNOWN' using DeepVariant GPU version. **Setup**. - Operating system: CentOS Linux release 7.4.1708 (Core), Linux 5.10.150-1.el7.x86_64. - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): singularity image build from dockerhub. - Type of data: nothing special that is unlike the case studies. **Steps to reproduce:**. - Command: /opt/deepvariant/bin/run_deepvariant --version. - Error trace: (if applicable). ```. tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error. tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: . tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: . tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: INVALID_ARGUMENT: expected %d.%d, %d.%d.%d, or %d.%d.%d.%d form for driver version; got ""1"". tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 520.61.5. DeepVariant version 1.4.0. ```. The `hostname` is privacy. **Does the quick start test work on your system?:** No. **Any additional context:** . The GPU is NVIDIA GeForce 3090. The GPU Driver Version: 520.61.05. The CUDA version in the host is V11.8.89 as followings:. ![image](https://user-images.githubusercontent.com/43125963/225341539-aa2ee3c6-c376-4758-a582-c8fd871b0508.png). It seems that the Deepvariant v1.4.0 in the singularity image has already installed CUDA v11.3. . ![image](https://user-images.githubusercontent.com/43125963/225343337-d0924a9b-4b9d-4b03-848f-e8e9753eb377.png). I don't know whether it causes the program crash.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:1425,integrability,version,version,1425,"'CUDA_ERROR_UNKNOWN' error using DeepVariant GPU version.; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**: Yes. **Describe the issue:**. 'CUDA_ERROR_UNKNOWN' using DeepVariant GPU version. **Setup**. - Operating system: CentOS Linux release 7.4.1708 (Core), Linux 5.10.150-1.el7.x86_64. - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): singularity image build from dockerhub. - Type of data: nothing special that is unlike the case studies. **Steps to reproduce:**. - Command: /opt/deepvariant/bin/run_deepvariant --version. - Error trace: (if applicable). ```. tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error. tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: . tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: . tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: INVALID_ARGUMENT: expected %d.%d, %d.%d.%d, or %d.%d.%d.%d form for driver version; got ""1"". tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 520.61.5. DeepVariant version 1.4.0. ```. The `hostname` is privacy. **Does the quick start test work on your system?:** No. **Any additional context:** . The GPU is NVIDIA GeForce 3090. The GPU Driver Version: 520.61.05. The CUDA version in the host is V11.8.89 as followings:. ![image](https://user-images.githubusercontent.com/43125963/225341539-aa2ee3c6-c376-4758-a582-c8fd871b0508.png). It seems that the Deepvariant v1.4.0 in the singularity image has already installed CUDA v11.3. . ![image](https://user-images.githubusercontent.com/43125963/225343337-d0924a9b-4b9d-4b03-848f-e8e9753eb377.png). I don't know whether it causes the program crash.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:49,modifiability,version,version,49,"'CUDA_ERROR_UNKNOWN' error using DeepVariant GPU version.; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**: Yes. **Describe the issue:**. 'CUDA_ERROR_UNKNOWN' using DeepVariant GPU version. **Setup**. - Operating system: CentOS Linux release 7.4.1708 (Core), Linux 5.10.150-1.el7.x86_64. - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): singularity image build from dockerhub. - Type of data: nothing special that is unlike the case studies. **Steps to reproduce:**. - Command: /opt/deepvariant/bin/run_deepvariant --version. - Error trace: (if applicable). ```. tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error. tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: . tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: . tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: INVALID_ARGUMENT: expected %d.%d, %d.%d.%d, or %d.%d.%d.%d form for driver version; got ""1"". tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 520.61.5. DeepVariant version 1.4.0. ```. The `hostname` is privacy. **Does the quick start test work on your system?:** No. **Any additional context:** . The GPU is NVIDIA GeForce 3090. The GPU Driver Version: 520.61.05. The CUDA version in the host is V11.8.89 as followings:. ![image](https://user-images.githubusercontent.com/43125963/225341539-aa2ee3c6-c376-4758-a582-c8fd871b0508.png). It seems that the Deepvariant v1.4.0 in the singularity image has already installed CUDA v11.3. . ![image](https://user-images.githubusercontent.com/43125963/225343337-d0924a9b-4b9d-4b03-848f-e8e9753eb377.png). I don't know whether it causes the program crash.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:223,modifiability,version,version,223,"'CUDA_ERROR_UNKNOWN' error using DeepVariant GPU version.; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**: Yes. **Describe the issue:**. 'CUDA_ERROR_UNKNOWN' using DeepVariant GPU version. **Setup**. - Operating system: CentOS Linux release 7.4.1708 (Core), Linux 5.10.150-1.el7.x86_64. - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): singularity image build from dockerhub. - Type of data: nothing special that is unlike the case studies. **Steps to reproduce:**. - Command: /opt/deepvariant/bin/run_deepvariant --version. - Error trace: (if applicable). ```. tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error. tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: . tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: . tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: INVALID_ARGUMENT: expected %d.%d, %d.%d.%d, or %d.%d.%d.%d form for driver version; got ""1"". tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 520.61.5. DeepVariant version 1.4.0. ```. The `hostname` is privacy. **Does the quick start test work on your system?:** No. **Any additional context:** . The GPU is NVIDIA GeForce 3090. The GPU Driver Version: 520.61.05. The CUDA version in the host is V11.8.89 as followings:. ![image](https://user-images.githubusercontent.com/43125963/225341539-aa2ee3c6-c376-4758-a582-c8fd871b0508.png). It seems that the Deepvariant v1.4.0 in the singularity image has already installed CUDA v11.3. . ![image](https://user-images.githubusercontent.com/43125963/225343337-d0924a9b-4b9d-4b03-848f-e8e9753eb377.png). I don't know whether it causes the program crash.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:344,modifiability,version,version,344,"'CUDA_ERROR_UNKNOWN' error using DeepVariant GPU version.; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**: Yes. **Describe the issue:**. 'CUDA_ERROR_UNKNOWN' using DeepVariant GPU version. **Setup**. - Operating system: CentOS Linux release 7.4.1708 (Core), Linux 5.10.150-1.el7.x86_64. - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): singularity image build from dockerhub. - Type of data: nothing special that is unlike the case studies. **Steps to reproduce:**. - Command: /opt/deepvariant/bin/run_deepvariant --version. - Error trace: (if applicable). ```. tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error. tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: . tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: . tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: INVALID_ARGUMENT: expected %d.%d, %d.%d.%d, or %d.%d.%d.%d form for driver version; got ""1"". tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 520.61.5. DeepVariant version 1.4.0. ```. The `hostname` is privacy. **Does the quick start test work on your system?:** No. **Any additional context:** . The GPU is NVIDIA GeForce 3090. The GPU Driver Version: 520.61.05. The CUDA version in the host is V11.8.89 as followings:. ![image](https://user-images.githubusercontent.com/43125963/225341539-aa2ee3c6-c376-4758-a582-c8fd871b0508.png). It seems that the Deepvariant v1.4.0 in the singularity image has already installed CUDA v11.3. . ![image](https://user-images.githubusercontent.com/43125963/225343337-d0924a9b-4b9d-4b03-848f-e8e9753eb377.png). I don't know whether it causes the program crash.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:597,modifiability,version,version,597,"'CUDA_ERROR_UNKNOWN' error using DeepVariant GPU version.; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**: Yes. **Describe the issue:**. 'CUDA_ERROR_UNKNOWN' using DeepVariant GPU version. **Setup**. - Operating system: CentOS Linux release 7.4.1708 (Core), Linux 5.10.150-1.el7.x86_64. - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): singularity image build from dockerhub. - Type of data: nothing special that is unlike the case studies. **Steps to reproduce:**. - Command: /opt/deepvariant/bin/run_deepvariant --version. - Error trace: (if applicable). ```. tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error. tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: . tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: . tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: INVALID_ARGUMENT: expected %d.%d, %d.%d.%d, or %d.%d.%d.%d form for driver version; got ""1"". tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 520.61.5. DeepVariant version 1.4.0. ```. The `hostname` is privacy. **Does the quick start test work on your system?:** No. **Any additional context:** . The GPU is NVIDIA GeForce 3090. The GPU Driver Version: 520.61.05. The CUDA version in the host is V11.8.89 as followings:. ![image](https://user-images.githubusercontent.com/43125963/225341539-aa2ee3c6-c376-4758-a582-c8fd871b0508.png). It seems that the Deepvariant v1.4.0 in the singularity image has already installed CUDA v11.3. . ![image](https://user-images.githubusercontent.com/43125963/225343337-d0924a9b-4b9d-4b03-848f-e8e9753eb377.png). I don't know whether it causes the program crash.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:1004,modifiability,version,version,1004,"'CUDA_ERROR_UNKNOWN' error using DeepVariant GPU version.; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**: Yes. **Describe the issue:**. 'CUDA_ERROR_UNKNOWN' using DeepVariant GPU version. **Setup**. - Operating system: CentOS Linux release 7.4.1708 (Core), Linux 5.10.150-1.el7.x86_64. - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): singularity image build from dockerhub. - Type of data: nothing special that is unlike the case studies. **Steps to reproduce:**. - Command: /opt/deepvariant/bin/run_deepvariant --version. - Error trace: (if applicable). ```. tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error. tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: . tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: . tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: INVALID_ARGUMENT: expected %d.%d, %d.%d.%d, or %d.%d.%d.%d form for driver version; got ""1"". tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 520.61.5. DeepVariant version 1.4.0. ```. The `hostname` is privacy. **Does the quick start test work on your system?:** No. **Any additional context:** . The GPU is NVIDIA GeForce 3090. The GPU Driver Version: 520.61.05. The CUDA version in the host is V11.8.89 as followings:. ![image](https://user-images.githubusercontent.com/43125963/225341539-aa2ee3c6-c376-4758-a582-c8fd871b0508.png). It seems that the Deepvariant v1.4.0 in the singularity image has already installed CUDA v11.3. . ![image](https://user-images.githubusercontent.com/43125963/225343337-d0924a9b-4b9d-4b03-848f-e8e9753eb377.png). I don't know whether it causes the program crash.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:1091,modifiability,version,version,1091,"'CUDA_ERROR_UNKNOWN' error using DeepVariant GPU version.; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**: Yes. **Describe the issue:**. 'CUDA_ERROR_UNKNOWN' using DeepVariant GPU version. **Setup**. - Operating system: CentOS Linux release 7.4.1708 (Core), Linux 5.10.150-1.el7.x86_64. - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): singularity image build from dockerhub. - Type of data: nothing special that is unlike the case studies. **Steps to reproduce:**. - Command: /opt/deepvariant/bin/run_deepvariant --version. - Error trace: (if applicable). ```. tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error. tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: . tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: . tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: INVALID_ARGUMENT: expected %d.%d, %d.%d.%d, or %d.%d.%d.%d form for driver version; got ""1"". tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 520.61.5. DeepVariant version 1.4.0. ```. The `hostname` is privacy. **Does the quick start test work on your system?:** No. **Any additional context:** . The GPU is NVIDIA GeForce 3090. The GPU Driver Version: 520.61.05. The CUDA version in the host is V11.8.89 as followings:. ![image](https://user-images.githubusercontent.com/43125963/225341539-aa2ee3c6-c376-4758-a582-c8fd871b0508.png). It seems that the Deepvariant v1.4.0 in the singularity image has already installed CUDA v11.3. . ![image](https://user-images.githubusercontent.com/43125963/225343337-d0924a9b-4b9d-4b03-848f-e8e9753eb377.png). I don't know whether it causes the program crash.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:1182,modifiability,version,version,1182,"'CUDA_ERROR_UNKNOWN' error using DeepVariant GPU version.; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**: Yes. **Describe the issue:**. 'CUDA_ERROR_UNKNOWN' using DeepVariant GPU version. **Setup**. - Operating system: CentOS Linux release 7.4.1708 (Core), Linux 5.10.150-1.el7.x86_64. - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): singularity image build from dockerhub. - Type of data: nothing special that is unlike the case studies. **Steps to reproduce:**. - Command: /opt/deepvariant/bin/run_deepvariant --version. - Error trace: (if applicable). ```. tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error. tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: . tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: . tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: INVALID_ARGUMENT: expected %d.%d, %d.%d.%d, or %d.%d.%d.%d form for driver version; got ""1"". tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 520.61.5. DeepVariant version 1.4.0. ```. The `hostname` is privacy. **Does the quick start test work on your system?:** No. **Any additional context:** . The GPU is NVIDIA GeForce 3090. The GPU Driver Version: 520.61.05. The CUDA version in the host is V11.8.89 as followings:. ![image](https://user-images.githubusercontent.com/43125963/225341539-aa2ee3c6-c376-4758-a582-c8fd871b0508.png). It seems that the Deepvariant v1.4.0 in the singularity image has already installed CUDA v11.3. . ![image](https://user-images.githubusercontent.com/43125963/225343337-d0924a9b-4b9d-4b03-848f-e8e9753eb377.png). I don't know whether it causes the program crash.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:1216,modifiability,version,version,1216,"'CUDA_ERROR_UNKNOWN' error using DeepVariant GPU version.; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**: Yes. **Describe the issue:**. 'CUDA_ERROR_UNKNOWN' using DeepVariant GPU version. **Setup**. - Operating system: CentOS Linux release 7.4.1708 (Core), Linux 5.10.150-1.el7.x86_64. - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): singularity image build from dockerhub. - Type of data: nothing special that is unlike the case studies. **Steps to reproduce:**. - Command: /opt/deepvariant/bin/run_deepvariant --version. - Error trace: (if applicable). ```. tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error. tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: . tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: . tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: INVALID_ARGUMENT: expected %d.%d, %d.%d.%d, or %d.%d.%d.%d form for driver version; got ""1"". tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 520.61.5. DeepVariant version 1.4.0. ```. The `hostname` is privacy. **Does the quick start test work on your system?:** No. **Any additional context:** . The GPU is NVIDIA GeForce 3090. The GPU Driver Version: 520.61.05. The CUDA version in the host is V11.8.89 as followings:. ![image](https://user-images.githubusercontent.com/43125963/225341539-aa2ee3c6-c376-4758-a582-c8fd871b0508.png). It seems that the Deepvariant v1.4.0 in the singularity image has already installed CUDA v11.3. . ![image](https://user-images.githubusercontent.com/43125963/225343337-d0924a9b-4b9d-4b03-848f-e8e9753eb377.png). I don't know whether it causes the program crash.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:1396,modifiability,Version,Version,1396,"'CUDA_ERROR_UNKNOWN' error using DeepVariant GPU version.; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**: Yes. **Describe the issue:**. 'CUDA_ERROR_UNKNOWN' using DeepVariant GPU version. **Setup**. - Operating system: CentOS Linux release 7.4.1708 (Core), Linux 5.10.150-1.el7.x86_64. - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): singularity image build from dockerhub. - Type of data: nothing special that is unlike the case studies. **Steps to reproduce:**. - Command: /opt/deepvariant/bin/run_deepvariant --version. - Error trace: (if applicable). ```. tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error. tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: . tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: . tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: INVALID_ARGUMENT: expected %d.%d, %d.%d.%d, or %d.%d.%d.%d form for driver version; got ""1"". tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 520.61.5. DeepVariant version 1.4.0. ```. The `hostname` is privacy. **Does the quick start test work on your system?:** No. **Any additional context:** . The GPU is NVIDIA GeForce 3090. The GPU Driver Version: 520.61.05. The CUDA version in the host is V11.8.89 as followings:. ![image](https://user-images.githubusercontent.com/43125963/225341539-aa2ee3c6-c376-4758-a582-c8fd871b0508.png). It seems that the Deepvariant v1.4.0 in the singularity image has already installed CUDA v11.3. . ![image](https://user-images.githubusercontent.com/43125963/225343337-d0924a9b-4b9d-4b03-848f-e8e9753eb377.png). I don't know whether it causes the program crash.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:1425,modifiability,version,version,1425,"'CUDA_ERROR_UNKNOWN' error using DeepVariant GPU version.; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**: Yes. **Describe the issue:**. 'CUDA_ERROR_UNKNOWN' using DeepVariant GPU version. **Setup**. - Operating system: CentOS Linux release 7.4.1708 (Core), Linux 5.10.150-1.el7.x86_64. - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): singularity image build from dockerhub. - Type of data: nothing special that is unlike the case studies. **Steps to reproduce:**. - Command: /opt/deepvariant/bin/run_deepvariant --version. - Error trace: (if applicable). ```. tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error. tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: . tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: . tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: INVALID_ARGUMENT: expected %d.%d, %d.%d.%d, or %d.%d.%d.%d form for driver version; got ""1"". tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 520.61.5. DeepVariant version 1.4.0. ```. The `hostname` is privacy. **Does the quick start test work on your system?:** No. **Any additional context:** . The GPU is NVIDIA GeForce 3090. The GPU Driver Version: 520.61.05. The CUDA version in the host is V11.8.89 as followings:. ![image](https://user-images.githubusercontent.com/43125963/225341539-aa2ee3c6-c376-4758-a582-c8fd871b0508.png). It seems that the Deepvariant v1.4.0 in the singularity image has already installed CUDA v11.3. . ![image](https://user-images.githubusercontent.com/43125963/225343337-d0924a9b-4b9d-4b03-848f-e8e9753eb377.png). I don't know whether it causes the program crash.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:21,performance,error,error,21,"'CUDA_ERROR_UNKNOWN' error using DeepVariant GPU version.; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**: Yes. **Describe the issue:**. 'CUDA_ERROR_UNKNOWN' using DeepVariant GPU version. **Setup**. - Operating system: CentOS Linux release 7.4.1708 (Core), Linux 5.10.150-1.el7.x86_64. - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): singularity image build from dockerhub. - Type of data: nothing special that is unlike the case studies. **Steps to reproduce:**. - Command: /opt/deepvariant/bin/run_deepvariant --version. - Error trace: (if applicable). ```. tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error. tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: . tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: . tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: INVALID_ARGUMENT: expected %d.%d, %d.%d.%d, or %d.%d.%d.%d form for driver version; got ""1"". tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 520.61.5. DeepVariant version 1.4.0. ```. The `hostname` is privacy. **Does the quick start test work on your system?:** No. **Any additional context:** . The GPU is NVIDIA GeForce 3090. The GPU Driver Version: 520.61.05. The CUDA version in the host is V11.8.89 as followings:. ![image](https://user-images.githubusercontent.com/43125963/225341539-aa2ee3c6-c376-4758-a582-c8fd871b0508.png). It seems that the Deepvariant v1.4.0 in the singularity image has already installed CUDA v11.3. . ![image](https://user-images.githubusercontent.com/43125963/225343337-d0924a9b-4b9d-4b03-848f-e8e9753eb377.png). I don't know whether it causes the program crash.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:45,performance,GPU,GPU,45,"'CUDA_ERROR_UNKNOWN' error using DeepVariant GPU version.; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**: Yes. **Describe the issue:**. 'CUDA_ERROR_UNKNOWN' using DeepVariant GPU version. **Setup**. - Operating system: CentOS Linux release 7.4.1708 (Core), Linux 5.10.150-1.el7.x86_64. - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): singularity image build from dockerhub. - Type of data: nothing special that is unlike the case studies. **Steps to reproduce:**. - Command: /opt/deepvariant/bin/run_deepvariant --version. - Error trace: (if applicable). ```. tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error. tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: . tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: . tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: INVALID_ARGUMENT: expected %d.%d, %d.%d.%d, or %d.%d.%d.%d form for driver version; got ""1"". tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 520.61.5. DeepVariant version 1.4.0. ```. The `hostname` is privacy. **Does the quick start test work on your system?:** No. **Any additional context:** . The GPU is NVIDIA GeForce 3090. The GPU Driver Version: 520.61.05. The CUDA version in the host is V11.8.89 as followings:. ![image](https://user-images.githubusercontent.com/43125963/225341539-aa2ee3c6-c376-4758-a582-c8fd871b0508.png). It seems that the Deepvariant v1.4.0 in the singularity image has already installed CUDA v11.3. . ![image](https://user-images.githubusercontent.com/43125963/225343337-d0924a9b-4b9d-4b03-848f-e8e9753eb377.png). I don't know whether it causes the program crash.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:219,performance,GPU,GPU,219,"'CUDA_ERROR_UNKNOWN' error using DeepVariant GPU version.; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**: Yes. **Describe the issue:**. 'CUDA_ERROR_UNKNOWN' using DeepVariant GPU version. **Setup**. - Operating system: CentOS Linux release 7.4.1708 (Core), Linux 5.10.150-1.el7.x86_64. - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): singularity image build from dockerhub. - Type of data: nothing special that is unlike the case studies. **Steps to reproduce:**. - Command: /opt/deepvariant/bin/run_deepvariant --version. - Error trace: (if applicable). ```. tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error. tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: . tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: . tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: INVALID_ARGUMENT: expected %d.%d, %d.%d.%d, or %d.%d.%d.%d form for driver version; got ""1"". tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 520.61.5. DeepVariant version 1.4.0. ```. The `hostname` is privacy. **Does the quick start test work on your system?:** No. **Any additional context:** . The GPU is NVIDIA GeForce 3090. The GPU Driver Version: 520.61.05. The CUDA version in the host is V11.8.89 as followings:. ![image](https://user-images.githubusercontent.com/43125963/225341539-aa2ee3c6-c376-4758-a582-c8fd871b0508.png). It seems that the Deepvariant v1.4.0 in the singularity image has already installed CUDA v11.3. . ![image](https://user-images.githubusercontent.com/43125963/225343337-d0924a9b-4b9d-4b03-848f-e8e9753eb377.png). I don't know whether it causes the program crash.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:608,performance,Error,Error,608,"'CUDA_ERROR_UNKNOWN' error using DeepVariant GPU version.; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**: Yes. **Describe the issue:**. 'CUDA_ERROR_UNKNOWN' using DeepVariant GPU version. **Setup**. - Operating system: CentOS Linux release 7.4.1708 (Core), Linux 5.10.150-1.el7.x86_64. - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): singularity image build from dockerhub. - Type of data: nothing special that is unlike the case studies. **Steps to reproduce:**. - Command: /opt/deepvariant/bin/run_deepvariant --version. - Error trace: (if applicable). ```. tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error. tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: . tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: . tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: INVALID_ARGUMENT: expected %d.%d, %d.%d.%d, or %d.%d.%d.%d form for driver version; got ""1"". tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 520.61.5. DeepVariant version 1.4.0. ```. The `hostname` is privacy. **Does the quick start test work on your system?:** No. **Any additional context:** . The GPU is NVIDIA GeForce 3090. The GPU Driver Version: 520.61.05. The CUDA version in the host is V11.8.89 as followings:. ![image](https://user-images.githubusercontent.com/43125963/225341539-aa2ee3c6-c376-4758-a582-c8fd871b0508.png). It seems that the Deepvariant v1.4.0 in the singularity image has already installed CUDA v11.3. . ![image](https://user-images.githubusercontent.com/43125963/225343337-d0924a9b-4b9d-4b03-848f-e8e9753eb377.png). I don't know whether it causes the program crash.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:746,performance,error,error,746,"'CUDA_ERROR_UNKNOWN' error using DeepVariant GPU version.; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**: Yes. **Describe the issue:**. 'CUDA_ERROR_UNKNOWN' using DeepVariant GPU version. **Setup**. - Operating system: CentOS Linux release 7.4.1708 (Core), Linux 5.10.150-1.el7.x86_64. - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): singularity image build from dockerhub. - Type of data: nothing special that is unlike the case studies. **Steps to reproduce:**. - Command: /opt/deepvariant/bin/run_deepvariant --version. - Error trace: (if applicable). ```. tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error. tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: . tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: . tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: INVALID_ARGUMENT: expected %d.%d, %d.%d.%d, or %d.%d.%d.%d form for driver version; got ""1"". tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 520.61.5. DeepVariant version 1.4.0. ```. The `hostname` is privacy. **Does the quick start test work on your system?:** No. **Any additional context:** . The GPU is NVIDIA GeForce 3090. The GPU Driver Version: 520.61.05. The CUDA version in the host is V11.8.89 as followings:. ![image](https://user-images.githubusercontent.com/43125963/225341539-aa2ee3c6-c376-4758-a582-c8fd871b0508.png). It seems that the Deepvariant v1.4.0 in the singularity image has already installed CUDA v11.3. . ![image](https://user-images.githubusercontent.com/43125963/225343337-d0924a9b-4b9d-4b03-848f-e8e9753eb377.png). I don't know whether it causes the program crash.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:1353,performance,GPU,GPU,1353,"'CUDA_ERROR_UNKNOWN' error using DeepVariant GPU version.; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**: Yes. **Describe the issue:**. 'CUDA_ERROR_UNKNOWN' using DeepVariant GPU version. **Setup**. - Operating system: CentOS Linux release 7.4.1708 (Core), Linux 5.10.150-1.el7.x86_64. - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): singularity image build from dockerhub. - Type of data: nothing special that is unlike the case studies. **Steps to reproduce:**. - Command: /opt/deepvariant/bin/run_deepvariant --version. - Error trace: (if applicable). ```. tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error. tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: . tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: . tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: INVALID_ARGUMENT: expected %d.%d, %d.%d.%d, or %d.%d.%d.%d form for driver version; got ""1"". tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 520.61.5. DeepVariant version 1.4.0. ```. The `hostname` is privacy. **Does the quick start test work on your system?:** No. **Any additional context:** . The GPU is NVIDIA GeForce 3090. The GPU Driver Version: 520.61.05. The CUDA version in the host is V11.8.89 as followings:. ![image](https://user-images.githubusercontent.com/43125963/225341539-aa2ee3c6-c376-4758-a582-c8fd871b0508.png). It seems that the Deepvariant v1.4.0 in the singularity image has already installed CUDA v11.3. . ![image](https://user-images.githubusercontent.com/43125963/225343337-d0924a9b-4b9d-4b03-848f-e8e9753eb377.png). I don't know whether it causes the program crash.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:1385,performance,GPU,GPU,1385,"'CUDA_ERROR_UNKNOWN' error using DeepVariant GPU version.; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**: Yes. **Describe the issue:**. 'CUDA_ERROR_UNKNOWN' using DeepVariant GPU version. **Setup**. - Operating system: CentOS Linux release 7.4.1708 (Core), Linux 5.10.150-1.el7.x86_64. - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): singularity image build from dockerhub. - Type of data: nothing special that is unlike the case studies. **Steps to reproduce:**. - Command: /opt/deepvariant/bin/run_deepvariant --version. - Error trace: (if applicable). ```. tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error. tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: . tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: . tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: INVALID_ARGUMENT: expected %d.%d, %d.%d.%d, or %d.%d.%d.%d form for driver version; got ""1"". tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 520.61.5. DeepVariant version 1.4.0. ```. The `hostname` is privacy. **Does the quick start test work on your system?:** No. **Any additional context:** . The GPU is NVIDIA GeForce 3090. The GPU Driver Version: 520.61.05. The CUDA version in the host is V11.8.89 as followings:. ![image](https://user-images.githubusercontent.com/43125963/225341539-aa2ee3c6-c376-4758-a582-c8fd871b0508.png). It seems that the Deepvariant v1.4.0 in the singularity image has already installed CUDA v11.3. . ![image](https://user-images.githubusercontent.com/43125963/225343337-d0924a9b-4b9d-4b03-848f-e8e9753eb377.png). I don't know whether it causes the program crash.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:695,reliability,fail,failed,695,"'CUDA_ERROR_UNKNOWN' error using DeepVariant GPU version.; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**: Yes. **Describe the issue:**. 'CUDA_ERROR_UNKNOWN' using DeepVariant GPU version. **Setup**. - Operating system: CentOS Linux release 7.4.1708 (Core), Linux 5.10.150-1.el7.x86_64. - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): singularity image build from dockerhub. - Type of data: nothing special that is unlike the case studies. **Steps to reproduce:**. - Command: /opt/deepvariant/bin/run_deepvariant --version. - Error trace: (if applicable). ```. tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error. tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: . tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: . tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: INVALID_ARGUMENT: expected %d.%d, %d.%d.%d, or %d.%d.%d.%d form for driver version; got ""1"". tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 520.61.5. DeepVariant version 1.4.0. ```. The `hostname` is privacy. **Does the quick start test work on your system?:** No. **Any additional context:** . The GPU is NVIDIA GeForce 3090. The GPU Driver Version: 520.61.05. The CUDA version in the host is V11.8.89 as followings:. ![image](https://user-images.githubusercontent.com/43125963/225341539-aa2ee3c6-c376-4758-a582-c8fd871b0508.png). It seems that the Deepvariant v1.4.0 in the singularity image has already installed CUDA v11.3. . ![image](https://user-images.githubusercontent.com/43125963/225343337-d0924a9b-4b9d-4b03-848f-e8e9753eb377.png). I don't know whether it causes the program crash.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:826,reliability,diagno,diagnostic,826,"'CUDA_ERROR_UNKNOWN' error using DeepVariant GPU version.; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**: Yes. **Describe the issue:**. 'CUDA_ERROR_UNKNOWN' using DeepVariant GPU version. **Setup**. - Operating system: CentOS Linux release 7.4.1708 (Core), Linux 5.10.150-1.el7.x86_64. - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): singularity image build from dockerhub. - Type of data: nothing special that is unlike the case studies. **Steps to reproduce:**. - Command: /opt/deepvariant/bin/run_deepvariant --version. - Error trace: (if applicable). ```. tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error. tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: . tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: . tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: INVALID_ARGUMENT: expected %d.%d, %d.%d.%d, or %d.%d.%d.%d form for driver version; got ""1"". tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 520.61.5. DeepVariant version 1.4.0. ```. The `hostname` is privacy. **Does the quick start test work on your system?:** No. **Any additional context:** . The GPU is NVIDIA GeForce 3090. The GPU Driver Version: 520.61.05. The CUDA version in the host is V11.8.89 as followings:. ![image](https://user-images.githubusercontent.com/43125963/225341539-aa2ee3c6-c376-4758-a582-c8fd871b0508.png). It seems that the Deepvariant v1.4.0 in the singularity image has already installed CUDA v11.3. . ![image](https://user-images.githubusercontent.com/43125963/225343337-d0924a9b-4b9d-4b03-848f-e8e9753eb377.png). I don't know whether it causes the program crash.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:1265,reliability,Doe,Does,1265,"'CUDA_ERROR_UNKNOWN' error using DeepVariant GPU version.; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**: Yes. **Describe the issue:**. 'CUDA_ERROR_UNKNOWN' using DeepVariant GPU version. **Setup**. - Operating system: CentOS Linux release 7.4.1708 (Core), Linux 5.10.150-1.el7.x86_64. - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): singularity image build from dockerhub. - Type of data: nothing special that is unlike the case studies. **Steps to reproduce:**. - Command: /opt/deepvariant/bin/run_deepvariant --version. - Error trace: (if applicable). ```. tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error. tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: . tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: . tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: INVALID_ARGUMENT: expected %d.%d, %d.%d.%d, or %d.%d.%d.%d form for driver version; got ""1"". tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 520.61.5. DeepVariant version 1.4.0. ```. The `hostname` is privacy. **Does the quick start test work on your system?:** No. **Any additional context:** . The GPU is NVIDIA GeForce 3090. The GPU Driver Version: 520.61.05. The CUDA version in the host is V11.8.89 as followings:. ![image](https://user-images.githubusercontent.com/43125963/225341539-aa2ee3c6-c376-4758-a582-c8fd871b0508.png). It seems that the Deepvariant v1.4.0 in the singularity image has already installed CUDA v11.3. . ![image](https://user-images.githubusercontent.com/43125963/225343337-d0924a9b-4b9d-4b03-848f-e8e9753eb377.png). I don't know whether it causes the program crash.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:21,safety,error,error,21,"'CUDA_ERROR_UNKNOWN' error using DeepVariant GPU version.; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**: Yes. **Describe the issue:**. 'CUDA_ERROR_UNKNOWN' using DeepVariant GPU version. **Setup**. - Operating system: CentOS Linux release 7.4.1708 (Core), Linux 5.10.150-1.el7.x86_64. - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): singularity image build from dockerhub. - Type of data: nothing special that is unlike the case studies. **Steps to reproduce:**. - Command: /opt/deepvariant/bin/run_deepvariant --version. - Error trace: (if applicable). ```. tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error. tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: . tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: . tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: INVALID_ARGUMENT: expected %d.%d, %d.%d.%d, or %d.%d.%d.%d form for driver version; got ""1"". tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 520.61.5. DeepVariant version 1.4.0. ```. The `hostname` is privacy. **Does the quick start test work on your system?:** No. **Any additional context:** . The GPU is NVIDIA GeForce 3090. The GPU Driver Version: 520.61.05. The CUDA version in the host is V11.8.89 as followings:. ![image](https://user-images.githubusercontent.com/43125963/225341539-aa2ee3c6-c376-4758-a582-c8fd871b0508.png). It seems that the Deepvariant v1.4.0 in the singularity image has already installed CUDA v11.3. . ![image](https://user-images.githubusercontent.com/43125963/225343337-d0924a9b-4b9d-4b03-848f-e8e9753eb377.png). I don't know whether it causes the program crash.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:608,safety,Error,Error,608,"'CUDA_ERROR_UNKNOWN' error using DeepVariant GPU version.; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**: Yes. **Describe the issue:**. 'CUDA_ERROR_UNKNOWN' using DeepVariant GPU version. **Setup**. - Operating system: CentOS Linux release 7.4.1708 (Core), Linux 5.10.150-1.el7.x86_64. - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): singularity image build from dockerhub. - Type of data: nothing special that is unlike the case studies. **Steps to reproduce:**. - Command: /opt/deepvariant/bin/run_deepvariant --version. - Error trace: (if applicable). ```. tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error. tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: . tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: . tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: INVALID_ARGUMENT: expected %d.%d, %d.%d.%d, or %d.%d.%d.%d form for driver version; got ""1"". tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 520.61.5. DeepVariant version 1.4.0. ```. The `hostname` is privacy. **Does the quick start test work on your system?:** No. **Any additional context:** . The GPU is NVIDIA GeForce 3090. The GPU Driver Version: 520.61.05. The CUDA version in the host is V11.8.89 as followings:. ![image](https://user-images.githubusercontent.com/43125963/225341539-aa2ee3c6-c376-4758-a582-c8fd871b0508.png). It seems that the Deepvariant v1.4.0 in the singularity image has already installed CUDA v11.3. . ![image](https://user-images.githubusercontent.com/43125963/225343337-d0924a9b-4b9d-4b03-848f-e8e9753eb377.png). I don't know whether it causes the program crash.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:746,safety,error,error,746,"'CUDA_ERROR_UNKNOWN' error using DeepVariant GPU version.; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**: Yes. **Describe the issue:**. 'CUDA_ERROR_UNKNOWN' using DeepVariant GPU version. **Setup**. - Operating system: CentOS Linux release 7.4.1708 (Core), Linux 5.10.150-1.el7.x86_64. - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): singularity image build from dockerhub. - Type of data: nothing special that is unlike the case studies. **Steps to reproduce:**. - Command: /opt/deepvariant/bin/run_deepvariant --version. - Error trace: (if applicable). ```. tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error. tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: . tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: . tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: INVALID_ARGUMENT: expected %d.%d, %d.%d.%d, or %d.%d.%d.%d form for driver version; got ""1"". tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 520.61.5. DeepVariant version 1.4.0. ```. The `hostname` is privacy. **Does the quick start test work on your system?:** No. **Any additional context:** . The GPU is NVIDIA GeForce 3090. The GPU Driver Version: 520.61.05. The CUDA version in the host is V11.8.89 as followings:. ![image](https://user-images.githubusercontent.com/43125963/225341539-aa2ee3c6-c376-4758-a582-c8fd871b0508.png). It seems that the Deepvariant v1.4.0 in the singularity image has already installed CUDA v11.3. . ![image](https://user-images.githubusercontent.com/43125963/225343337-d0924a9b-4b9d-4b03-848f-e8e9753eb377.png). I don't know whether it causes the program crash.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:1286,safety,test,test,1286,"'CUDA_ERROR_UNKNOWN' error using DeepVariant GPU version.; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**: Yes. **Describe the issue:**. 'CUDA_ERROR_UNKNOWN' using DeepVariant GPU version. **Setup**. - Operating system: CentOS Linux release 7.4.1708 (Core), Linux 5.10.150-1.el7.x86_64. - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): singularity image build from dockerhub. - Type of data: nothing special that is unlike the case studies. **Steps to reproduce:**. - Command: /opt/deepvariant/bin/run_deepvariant --version. - Error trace: (if applicable). ```. tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error. tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: . tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: . tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: INVALID_ARGUMENT: expected %d.%d, %d.%d.%d, or %d.%d.%d.%d form for driver version; got ""1"". tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 520.61.5. DeepVariant version 1.4.0. ```. The `hostname` is privacy. **Does the quick start test work on your system?:** No. **Any additional context:** . The GPU is NVIDIA GeForce 3090. The GPU Driver Version: 520.61.05. The CUDA version in the host is V11.8.89 as followings:. ![image](https://user-images.githubusercontent.com/43125963/225341539-aa2ee3c6-c376-4758-a582-c8fd871b0508.png). It seems that the Deepvariant v1.4.0 in the singularity image has already installed CUDA v11.3. . ![image](https://user-images.githubusercontent.com/43125963/225343337-d0924a9b-4b9d-4b03-848f-e8e9753eb377.png). I don't know whether it causes the program crash.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:614,testability,trace,trace,614,"'CUDA_ERROR_UNKNOWN' error using DeepVariant GPU version.; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**: Yes. **Describe the issue:**. 'CUDA_ERROR_UNKNOWN' using DeepVariant GPU version. **Setup**. - Operating system: CentOS Linux release 7.4.1708 (Core), Linux 5.10.150-1.el7.x86_64. - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): singularity image build from dockerhub. - Type of data: nothing special that is unlike the case studies. **Steps to reproduce:**. - Command: /opt/deepvariant/bin/run_deepvariant --version. - Error trace: (if applicable). ```. tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error. tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: . tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: . tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: INVALID_ARGUMENT: expected %d.%d, %d.%d.%d, or %d.%d.%d.%d form for driver version; got ""1"". tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 520.61.5. DeepVariant version 1.4.0. ```. The `hostname` is privacy. **Does the quick start test work on your system?:** No. **Any additional context:** . The GPU is NVIDIA GeForce 3090. The GPU Driver Version: 520.61.05. The CUDA version in the host is V11.8.89 as followings:. ![image](https://user-images.githubusercontent.com/43125963/225341539-aa2ee3c6-c376-4758-a582-c8fd871b0508.png). It seems that the Deepvariant v1.4.0 in the singularity image has already installed CUDA v11.3. . ![image](https://user-images.githubusercontent.com/43125963/225343337-d0924a9b-4b9d-4b03-848f-e8e9753eb377.png). I don't know whether it causes the program crash.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:826,testability,diagno,diagnostic,826,"'CUDA_ERROR_UNKNOWN' error using DeepVariant GPU version.; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**: Yes. **Describe the issue:**. 'CUDA_ERROR_UNKNOWN' using DeepVariant GPU version. **Setup**. - Operating system: CentOS Linux release 7.4.1708 (Core), Linux 5.10.150-1.el7.x86_64. - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): singularity image build from dockerhub. - Type of data: nothing special that is unlike the case studies. **Steps to reproduce:**. - Command: /opt/deepvariant/bin/run_deepvariant --version. - Error trace: (if applicable). ```. tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error. tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: . tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: . tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: INVALID_ARGUMENT: expected %d.%d, %d.%d.%d, or %d.%d.%d.%d form for driver version; got ""1"". tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 520.61.5. DeepVariant version 1.4.0. ```. The `hostname` is privacy. **Does the quick start test work on your system?:** No. **Any additional context:** . The GPU is NVIDIA GeForce 3090. The GPU Driver Version: 520.61.05. The CUDA version in the host is V11.8.89 as followings:. ![image](https://user-images.githubusercontent.com/43125963/225341539-aa2ee3c6-c376-4758-a582-c8fd871b0508.png). It seems that the Deepvariant v1.4.0 in the singularity image has already installed CUDA v11.3. . ![image](https://user-images.githubusercontent.com/43125963/225343337-d0924a9b-4b9d-4b03-848f-e8e9753eb377.png). I don't know whether it causes the program crash.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:1286,testability,test,test,1286,"'CUDA_ERROR_UNKNOWN' error using DeepVariant GPU version.; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**: Yes. **Describe the issue:**. 'CUDA_ERROR_UNKNOWN' using DeepVariant GPU version. **Setup**. - Operating system: CentOS Linux release 7.4.1708 (Core), Linux 5.10.150-1.el7.x86_64. - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): singularity image build from dockerhub. - Type of data: nothing special that is unlike the case studies. **Steps to reproduce:**. - Command: /opt/deepvariant/bin/run_deepvariant --version. - Error trace: (if applicable). ```. tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error. tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: . tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: . tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: INVALID_ARGUMENT: expected %d.%d, %d.%d.%d, or %d.%d.%d.%d form for driver version; got ""1"". tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 520.61.5. DeepVariant version 1.4.0. ```. The `hostname` is privacy. **Does the quick start test work on your system?:** No. **Any additional context:** . The GPU is NVIDIA GeForce 3090. The GPU Driver Version: 520.61.05. The CUDA version in the host is V11.8.89 as followings:. ![image](https://user-images.githubusercontent.com/43125963/225341539-aa2ee3c6-c376-4758-a582-c8fd871b0508.png). It seems that the Deepvariant v1.4.0 in the singularity image has already installed CUDA v11.3. . ![image](https://user-images.githubusercontent.com/43125963/225343337-d0924a9b-4b9d-4b03-848f-e8e9753eb377.png). I don't know whether it causes the program crash.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:1336,testability,context,context,1336,"'CUDA_ERROR_UNKNOWN' error using DeepVariant GPU version.; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**: Yes. **Describe the issue:**. 'CUDA_ERROR_UNKNOWN' using DeepVariant GPU version. **Setup**. - Operating system: CentOS Linux release 7.4.1708 (Core), Linux 5.10.150-1.el7.x86_64. - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): singularity image build from dockerhub. - Type of data: nothing special that is unlike the case studies. **Steps to reproduce:**. - Command: /opt/deepvariant/bin/run_deepvariant --version. - Error trace: (if applicable). ```. tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error. tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: . tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: . tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: INVALID_ARGUMENT: expected %d.%d, %d.%d.%d, or %d.%d.%d.%d form for driver version; got ""1"". tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 520.61.5. DeepVariant version 1.4.0. ```. The `hostname` is privacy. **Does the quick start test work on your system?:** No. **Any additional context:** . The GPU is NVIDIA GeForce 3090. The GPU Driver Version: 520.61.05. The CUDA version in the host is V11.8.89 as followings:. ![image](https://user-images.githubusercontent.com/43125963/225341539-aa2ee3c6-c376-4758-a582-c8fd871b0508.png). It seems that the Deepvariant v1.4.0 in the singularity image has already installed CUDA v11.3. . ![image](https://user-images.githubusercontent.com/43125963/225343337-d0924a9b-4b9d-4b03-848f-e8e9753eb377.png). I don't know whether it causes the program crash.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:21,usability,error,error,21,"'CUDA_ERROR_UNKNOWN' error using DeepVariant GPU version.; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**: Yes. **Describe the issue:**. 'CUDA_ERROR_UNKNOWN' using DeepVariant GPU version. **Setup**. - Operating system: CentOS Linux release 7.4.1708 (Core), Linux 5.10.150-1.el7.x86_64. - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): singularity image build from dockerhub. - Type of data: nothing special that is unlike the case studies. **Steps to reproduce:**. - Command: /opt/deepvariant/bin/run_deepvariant --version. - Error trace: (if applicable). ```. tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error. tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: . tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: . tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: INVALID_ARGUMENT: expected %d.%d, %d.%d.%d, or %d.%d.%d.%d form for driver version; got ""1"". tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 520.61.5. DeepVariant version 1.4.0. ```. The `hostname` is privacy. **Does the quick start test work on your system?:** No. **Any additional context:** . The GPU is NVIDIA GeForce 3090. The GPU Driver Version: 520.61.05. The CUDA version in the host is V11.8.89 as followings:. ![image](https://user-images.githubusercontent.com/43125963/225341539-aa2ee3c6-c376-4758-a582-c8fd871b0508.png). It seems that the Deepvariant v1.4.0 in the singularity image has already installed CUDA v11.3. . ![image](https://user-images.githubusercontent.com/43125963/225343337-d0924a9b-4b9d-4b03-848f-e8e9753eb377.png). I don't know whether it causes the program crash.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:549,usability,Command,Command,549,"'CUDA_ERROR_UNKNOWN' error using DeepVariant GPU version.; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**: Yes. **Describe the issue:**. 'CUDA_ERROR_UNKNOWN' using DeepVariant GPU version. **Setup**. - Operating system: CentOS Linux release 7.4.1708 (Core), Linux 5.10.150-1.el7.x86_64. - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): singularity image build from dockerhub. - Type of data: nothing special that is unlike the case studies. **Steps to reproduce:**. - Command: /opt/deepvariant/bin/run_deepvariant --version. - Error trace: (if applicable). ```. tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error. tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: . tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: . tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: INVALID_ARGUMENT: expected %d.%d, %d.%d.%d, or %d.%d.%d.%d form for driver version; got ""1"". tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 520.61.5. DeepVariant version 1.4.0. ```. The `hostname` is privacy. **Does the quick start test work on your system?:** No. **Any additional context:** . The GPU is NVIDIA GeForce 3090. The GPU Driver Version: 520.61.05. The CUDA version in the host is V11.8.89 as followings:. ![image](https://user-images.githubusercontent.com/43125963/225341539-aa2ee3c6-c376-4758-a582-c8fd871b0508.png). It seems that the Deepvariant v1.4.0 in the singularity image has already installed CUDA v11.3. . ![image](https://user-images.githubusercontent.com/43125963/225343337-d0924a9b-4b9d-4b03-848f-e8e9753eb377.png). I don't know whether it causes the program crash.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:608,usability,Error,Error,608,"'CUDA_ERROR_UNKNOWN' error using DeepVariant GPU version.; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**: Yes. **Describe the issue:**. 'CUDA_ERROR_UNKNOWN' using DeepVariant GPU version. **Setup**. - Operating system: CentOS Linux release 7.4.1708 (Core), Linux 5.10.150-1.el7.x86_64. - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): singularity image build from dockerhub. - Type of data: nothing special that is unlike the case studies. **Steps to reproduce:**. - Command: /opt/deepvariant/bin/run_deepvariant --version. - Error trace: (if applicable). ```. tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error. tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: . tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: . tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: INVALID_ARGUMENT: expected %d.%d, %d.%d.%d, or %d.%d.%d.%d form for driver version; got ""1"". tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 520.61.5. DeepVariant version 1.4.0. ```. The `hostname` is privacy. **Does the quick start test work on your system?:** No. **Any additional context:** . The GPU is NVIDIA GeForce 3090. The GPU Driver Version: 520.61.05. The CUDA version in the host is V11.8.89 as followings:. ![image](https://user-images.githubusercontent.com/43125963/225341539-aa2ee3c6-c376-4758-a582-c8fd871b0508.png). It seems that the Deepvariant v1.4.0 in the singularity image has already installed CUDA v11.3. . ![image](https://user-images.githubusercontent.com/43125963/225343337-d0924a9b-4b9d-4b03-848f-e8e9753eb377.png). I don't know whether it causes the program crash.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:746,usability,error,error,746,"'CUDA_ERROR_UNKNOWN' error using DeepVariant GPU version.; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**: Yes. **Describe the issue:**. 'CUDA_ERROR_UNKNOWN' using DeepVariant GPU version. **Setup**. - Operating system: CentOS Linux release 7.4.1708 (Core), Linux 5.10.150-1.el7.x86_64. - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): singularity image build from dockerhub. - Type of data: nothing special that is unlike the case studies. **Steps to reproduce:**. - Command: /opt/deepvariant/bin/run_deepvariant --version. - Error trace: (if applicable). ```. tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error. tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: . tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: . tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: INVALID_ARGUMENT: expected %d.%d, %d.%d.%d, or %d.%d.%d.%d form for driver version; got ""1"". tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 520.61.5. DeepVariant version 1.4.0. ```. The `hostname` is privacy. **Does the quick start test work on your system?:** No. **Any additional context:** . The GPU is NVIDIA GeForce 3090. The GPU Driver Version: 520.61.05. The CUDA version in the host is V11.8.89 as followings:. ![image](https://user-images.githubusercontent.com/43125963/225341539-aa2ee3c6-c376-4758-a582-c8fd871b0508.png). It seems that the Deepvariant v1.4.0 in the singularity image has already installed CUDA v11.3. . ![image](https://user-images.githubusercontent.com/43125963/225343337-d0924a9b-4b9d-4b03-848f-e8e9753eb377.png). I don't know whether it causes the program crash.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:1490,usability,user,user-images,1490,"'CUDA_ERROR_UNKNOWN' error using DeepVariant GPU version.; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**: Yes. **Describe the issue:**. 'CUDA_ERROR_UNKNOWN' using DeepVariant GPU version. **Setup**. - Operating system: CentOS Linux release 7.4.1708 (Core), Linux 5.10.150-1.el7.x86_64. - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): singularity image build from dockerhub. - Type of data: nothing special that is unlike the case studies. **Steps to reproduce:**. - Command: /opt/deepvariant/bin/run_deepvariant --version. - Error trace: (if applicable). ```. tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error. tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: . tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: . tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: INVALID_ARGUMENT: expected %d.%d, %d.%d.%d, or %d.%d.%d.%d form for driver version; got ""1"". tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 520.61.5. DeepVariant version 1.4.0. ```. The `hostname` is privacy. **Does the quick start test work on your system?:** No. **Any additional context:** . The GPU is NVIDIA GeForce 3090. The GPU Driver Version: 520.61.05. The CUDA version in the host is V11.8.89 as followings:. ![image](https://user-images.githubusercontent.com/43125963/225341539-aa2ee3c6-c376-4758-a582-c8fd871b0508.png). It seems that the Deepvariant v1.4.0 in the singularity image has already installed CUDA v11.3. . ![image](https://user-images.githubusercontent.com/43125963/225343337-d0924a9b-4b9d-4b03-848f-e8e9753eb377.png). I don't know whether it causes the program crash.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/619:1701,usability,user,user-images,1701,"'CUDA_ERROR_UNKNOWN' error using DeepVariant GPU version.; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**: Yes. **Describe the issue:**. 'CUDA_ERROR_UNKNOWN' using DeepVariant GPU version. **Setup**. - Operating system: CentOS Linux release 7.4.1708 (Core), Linux 5.10.150-1.el7.x86_64. - DeepVariant version: 1.4.0. - Installation method (Docker, built from source, etc.): singularity image build from dockerhub. - Type of data: nothing special that is unlike the case studies. **Steps to reproduce:**. - Command: /opt/deepvariant/bin/run_deepvariant --version. - Error trace: (if applicable). ```. tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error. tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: . tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: . tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: INVALID_ARGUMENT: expected %d.%d, %d.%d.%d, or %d.%d.%d.%d form for driver version; got ""1"". tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 520.61.5. DeepVariant version 1.4.0. ```. The `hostname` is privacy. **Does the quick start test work on your system?:** No. **Any additional context:** . The GPU is NVIDIA GeForce 3090. The GPU Driver Version: 520.61.05. The CUDA version in the host is V11.8.89 as followings:. ![image](https://user-images.githubusercontent.com/43125963/225341539-aa2ee3c6-c376-4758-a582-c8fd871b0508.png). It seems that the Deepvariant v1.4.0 in the singularity image has already installed CUDA v11.3. . ![image](https://user-images.githubusercontent.com/43125963/225343337-d0924a9b-4b9d-4b03-848f-e8e9753eb377.png). I don't know whether it causes the program crash.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619
https://github.com/google/deepvariant/issues/620:164,modifiability,Pac,PacBio,164,"Empty INFO column; Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md: Yes. Describe the issue:. Hello,. I'm using WGS data from PacBio HiFi, however, in the output vcf file the column INFO is empty. Is there any way to fill this column or how can this be fixed? ![Captura de pantalla 2023-03-15 a la(s) 12 22 56 p m](https://user-images.githubusercontent.com/117956356/225406490-86833319-9e20-4099-8a32-81ef2ba877fc.png). Thanks a lot!",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/620
https://github.com/google/deepvariant/issues/620:361,usability,user,user-images,361,"Empty INFO column; Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md: Yes. Describe the issue:. Hello,. I'm using WGS data from PacBio HiFi, however, in the output vcf file the column INFO is empty. Is there any way to fill this column or how can this be fixed? ![Captura de pantalla 2023-03-15 a la(s) 12 22 56 p m](https://user-images.githubusercontent.com/117956356/225406490-86833319-9e20-4099-8a32-81ef2ba877fc.png). Thanks a lot!",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/620
https://github.com/google/deepvariant/pull/621:79,deployability,updat,update,79,Add bbviPRS blog post.; This is an internal pull request that is meant to only update the `gh_pages` branch.,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/621
https://github.com/google/deepvariant/pull/621:79,safety,updat,update,79,Add bbviPRS blog post.; This is an internal pull request that is meant to only update the `gh_pages` branch.,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/621
https://github.com/google/deepvariant/pull/621:79,security,updat,update,79,Add bbviPRS blog post.; This is an internal pull request that is meant to only update the `gh_pages` branch.,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/621
https://github.com/google/deepvariant/issues/622:16,energy efficiency,model,model,16,Whether the WES model without the addition of region(bed) parameter is equal to the effect of WGS model?; Hello. I wonder know something about Deepvariant's **--model_type** parameter. Whether the WES model without the addition of region(bed) parameter is equal to the effect of WGS model? Thank you very much.,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/622
https://github.com/google/deepvariant/issues/622:98,energy efficiency,model,model,98,Whether the WES model without the addition of region(bed) parameter is equal to the effect of WGS model?; Hello. I wonder know something about Deepvariant's **--model_type** parameter. Whether the WES model without the addition of region(bed) parameter is equal to the effect of WGS model? Thank you very much.,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/622
https://github.com/google/deepvariant/issues/622:201,energy efficiency,model,model,201,Whether the WES model without the addition of region(bed) parameter is equal to the effect of WGS model?; Hello. I wonder know something about Deepvariant's **--model_type** parameter. Whether the WES model without the addition of region(bed) parameter is equal to the effect of WGS model? Thank you very much.,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/622
https://github.com/google/deepvariant/issues/622:283,energy efficiency,model,model,283,Whether the WES model without the addition of region(bed) parameter is equal to the effect of WGS model?; Hello. I wonder know something about Deepvariant's **--model_type** parameter. Whether the WES model without the addition of region(bed) parameter is equal to the effect of WGS model? Thank you very much.,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/622
https://github.com/google/deepvariant/issues/622:58,modifiability,paramet,parameter,58,Whether the WES model without the addition of region(bed) parameter is equal to the effect of WGS model?; Hello. I wonder know something about Deepvariant's **--model_type** parameter. Whether the WES model without the addition of region(bed) parameter is equal to the effect of WGS model? Thank you very much.,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/622
https://github.com/google/deepvariant/issues/622:174,modifiability,paramet,parameter,174,Whether the WES model without the addition of region(bed) parameter is equal to the effect of WGS model?; Hello. I wonder know something about Deepvariant's **--model_type** parameter. Whether the WES model without the addition of region(bed) parameter is equal to the effect of WGS model? Thank you very much.,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/622
https://github.com/google/deepvariant/issues/622:243,modifiability,paramet,parameter,243,Whether the WES model without the addition of region(bed) parameter is equal to the effect of WGS model?; Hello. I wonder know something about Deepvariant's **--model_type** parameter. Whether the WES model without the addition of region(bed) parameter is equal to the effect of WGS model? Thank you very much.,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/622
https://github.com/google/deepvariant/issues/622:16,security,model,model,16,Whether the WES model without the addition of region(bed) parameter is equal to the effect of WGS model?; Hello. I wonder know something about Deepvariant's **--model_type** parameter. Whether the WES model without the addition of region(bed) parameter is equal to the effect of WGS model? Thank you very much.,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/622
https://github.com/google/deepvariant/issues/622:98,security,model,model,98,Whether the WES model without the addition of region(bed) parameter is equal to the effect of WGS model?; Hello. I wonder know something about Deepvariant's **--model_type** parameter. Whether the WES model without the addition of region(bed) parameter is equal to the effect of WGS model? Thank you very much.,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/622
https://github.com/google/deepvariant/issues/622:201,security,model,model,201,Whether the WES model without the addition of region(bed) parameter is equal to the effect of WGS model?; Hello. I wonder know something about Deepvariant's **--model_type** parameter. Whether the WES model without the addition of region(bed) parameter is equal to the effect of WGS model? Thank you very much.,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/622
https://github.com/google/deepvariant/issues/622:283,security,model,model,283,Whether the WES model without the addition of region(bed) parameter is equal to the effect of WGS model?; Hello. I wonder know something about Deepvariant's **--model_type** parameter. Whether the WES model without the addition of region(bed) parameter is equal to the effect of WGS model? Thank you very much.,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/622
https://github.com/google/deepvariant/issues/624:14,availability,avail,availability,14,"RNA-seq model availability for v1.5.0; Hi,. The RNA-seq model in the case-study is available for version 1.4.0, but trying the download by simply replacing with 1.5.0 raises a file not found error. . Is or will there be an RNA-seq model for v1.5.0, or was this only be available for v1.4.0? . ```. # works . curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.data-00000-of-00001 > model/model.ckpt.data-00000-of-00001. # fails. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.5.0/DeepVariant-inception_v3-1.5.0+data-rnaseq_standard/model.ckpt.data-00000-of-00001 > model/model.ckpt.data-00000-of-00001. ```.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/624
https://github.com/google/deepvariant/issues/624:83,availability,avail,available,83,"RNA-seq model availability for v1.5.0; Hi,. The RNA-seq model in the case-study is available for version 1.4.0, but trying the download by simply replacing with 1.5.0 raises a file not found error. . Is or will there be an RNA-seq model for v1.5.0, or was this only be available for v1.4.0? . ```. # works . curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.data-00000-of-00001 > model/model.ckpt.data-00000-of-00001. # fails. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.5.0/DeepVariant-inception_v3-1.5.0+data-rnaseq_standard/model.ckpt.data-00000-of-00001 > model/model.ckpt.data-00000-of-00001. ```.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/624
https://github.com/google/deepvariant/issues/624:127,availability,down,download,127,"RNA-seq model availability for v1.5.0; Hi,. The RNA-seq model in the case-study is available for version 1.4.0, but trying the download by simply replacing with 1.5.0 raises a file not found error. . Is or will there be an RNA-seq model for v1.5.0, or was this only be available for v1.4.0? . ```. # works . curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.data-00000-of-00001 > model/model.ckpt.data-00000-of-00001. # fails. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.5.0/DeepVariant-inception_v3-1.5.0+data-rnaseq_standard/model.ckpt.data-00000-of-00001 > model/model.ckpt.data-00000-of-00001. ```.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/624
https://github.com/google/deepvariant/issues/624:191,availability,error,error,191,"RNA-seq model availability for v1.5.0; Hi,. The RNA-seq model in the case-study is available for version 1.4.0, but trying the download by simply replacing with 1.5.0 raises a file not found error. . Is or will there be an RNA-seq model for v1.5.0, or was this only be available for v1.4.0? . ```. # works . curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.data-00000-of-00001 > model/model.ckpt.data-00000-of-00001. # fails. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.5.0/DeepVariant-inception_v3-1.5.0+data-rnaseq_standard/model.ckpt.data-00000-of-00001 > model/model.ckpt.data-00000-of-00001. ```.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/624
https://github.com/google/deepvariant/issues/624:269,availability,avail,available,269,"RNA-seq model availability for v1.5.0; Hi,. The RNA-seq model in the case-study is available for version 1.4.0, but trying the download by simply replacing with 1.5.0 raises a file not found error. . Is or will there be an RNA-seq model for v1.5.0, or was this only be available for v1.4.0? . ```. # works . curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.data-00000-of-00001 > model/model.ckpt.data-00000-of-00001. # fails. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.5.0/DeepVariant-inception_v3-1.5.0+data-rnaseq_standard/model.ckpt.data-00000-of-00001 > model/model.ckpt.data-00000-of-00001. ```.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/624
https://github.com/google/deepvariant/issues/624:97,deployability,version,version,97,"RNA-seq model availability for v1.5.0; Hi,. The RNA-seq model in the case-study is available for version 1.4.0, but trying the download by simply replacing with 1.5.0 raises a file not found error. . Is or will there be an RNA-seq model for v1.5.0, or was this only be available for v1.4.0? . ```. # works . curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.data-00000-of-00001 > model/model.ckpt.data-00000-of-00001. # fails. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.5.0/DeepVariant-inception_v3-1.5.0+data-rnaseq_standard/model.ckpt.data-00000-of-00001 > model/model.ckpt.data-00000-of-00001. ```.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/624
https://github.com/google/deepvariant/issues/624:506,deployability,fail,fails,506,"RNA-seq model availability for v1.5.0; Hi,. The RNA-seq model in the case-study is available for version 1.4.0, but trying the download by simply replacing with 1.5.0 raises a file not found error. . Is or will there be an RNA-seq model for v1.5.0, or was this only be available for v1.4.0? . ```. # works . curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.data-00000-of-00001 > model/model.ckpt.data-00000-of-00001. # fails. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.5.0/DeepVariant-inception_v3-1.5.0+data-rnaseq_standard/model.ckpt.data-00000-of-00001 > model/model.ckpt.data-00000-of-00001. ```.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/624
https://github.com/google/deepvariant/issues/624:8,energy efficiency,model,model,8,"RNA-seq model availability for v1.5.0; Hi,. The RNA-seq model in the case-study is available for version 1.4.0, but trying the download by simply replacing with 1.5.0 raises a file not found error. . Is or will there be an RNA-seq model for v1.5.0, or was this only be available for v1.4.0? . ```. # works . curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.data-00000-of-00001 > model/model.ckpt.data-00000-of-00001. # fails. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.5.0/DeepVariant-inception_v3-1.5.0+data-rnaseq_standard/model.ckpt.data-00000-of-00001 > model/model.ckpt.data-00000-of-00001. ```.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/624
https://github.com/google/deepvariant/issues/624:56,energy efficiency,model,model,56,"RNA-seq model availability for v1.5.0; Hi,. The RNA-seq model in the case-study is available for version 1.4.0, but trying the download by simply replacing with 1.5.0 raises a file not found error. . Is or will there be an RNA-seq model for v1.5.0, or was this only be available for v1.4.0? . ```. # works . curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.data-00000-of-00001 > model/model.ckpt.data-00000-of-00001. # fails. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.5.0/DeepVariant-inception_v3-1.5.0+data-rnaseq_standard/model.ckpt.data-00000-of-00001 > model/model.ckpt.data-00000-of-00001. ```.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/624
https://github.com/google/deepvariant/issues/624:231,energy efficiency,model,model,231,"RNA-seq model availability for v1.5.0; Hi,. The RNA-seq model in the case-study is available for version 1.4.0, but trying the download by simply replacing with 1.5.0 raises a file not found error. . Is or will there be an RNA-seq model for v1.5.0, or was this only be available for v1.4.0? . ```. # works . curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.data-00000-of-00001 > model/model.ckpt.data-00000-of-00001. # fails. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.5.0/DeepVariant-inception_v3-1.5.0+data-rnaseq_standard/model.ckpt.data-00000-of-00001 > model/model.ckpt.data-00000-of-00001. ```.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/624
https://github.com/google/deepvariant/issues/624:356,energy efficiency,model,models,356,"RNA-seq model availability for v1.5.0; Hi,. The RNA-seq model in the case-study is available for version 1.4.0, but trying the download by simply replacing with 1.5.0 raises a file not found error. . Is or will there be an RNA-seq model for v1.5.0, or was this only be available for v1.4.0? . ```. # works . curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.data-00000-of-00001 > model/model.ckpt.data-00000-of-00001. # fails. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.5.0/DeepVariant-inception_v3-1.5.0+data-rnaseq_standard/model.ckpt.data-00000-of-00001 > model/model.ckpt.data-00000-of-00001. ```.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/624
https://github.com/google/deepvariant/issues/624:433,energy efficiency,model,model,433,"RNA-seq model availability for v1.5.0; Hi,. The RNA-seq model in the case-study is available for version 1.4.0, but trying the download by simply replacing with 1.5.0 raises a file not found error. . Is or will there be an RNA-seq model for v1.5.0, or was this only be available for v1.4.0? . ```. # works . curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.data-00000-of-00001 > model/model.ckpt.data-00000-of-00001. # fails. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.5.0/DeepVariant-inception_v3-1.5.0+data-rnaseq_standard/model.ckpt.data-00000-of-00001 > model/model.ckpt.data-00000-of-00001. ```.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/624
https://github.com/google/deepvariant/issues/624:466,energy efficiency,model,model,466,"RNA-seq model availability for v1.5.0; Hi,. The RNA-seq model in the case-study is available for version 1.4.0, but trying the download by simply replacing with 1.5.0 raises a file not found error. . Is or will there be an RNA-seq model for v1.5.0, or was this only be available for v1.4.0? . ```. # works . curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.data-00000-of-00001 > model/model.ckpt.data-00000-of-00001. # fails. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.5.0/DeepVariant-inception_v3-1.5.0+data-rnaseq_standard/model.ckpt.data-00000-of-00001 > model/model.ckpt.data-00000-of-00001. ```.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/624
https://github.com/google/deepvariant/issues/624:472,energy efficiency,model,model,472,"RNA-seq model availability for v1.5.0; Hi,. The RNA-seq model in the case-study is available for version 1.4.0, but trying the download by simply replacing with 1.5.0 raises a file not found error. . Is or will there be an RNA-seq model for v1.5.0, or was this only be available for v1.4.0? . ```. # works . curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.data-00000-of-00001 > model/model.ckpt.data-00000-of-00001. # fails. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.5.0/DeepVariant-inception_v3-1.5.0+data-rnaseq_standard/model.ckpt.data-00000-of-00001 > model/model.ckpt.data-00000-of-00001. ```.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/624
https://github.com/google/deepvariant/issues/624:561,energy efficiency,model,models,561,"RNA-seq model availability for v1.5.0; Hi,. The RNA-seq model in the case-study is available for version 1.4.0, but trying the download by simply replacing with 1.5.0 raises a file not found error. . Is or will there be an RNA-seq model for v1.5.0, or was this only be available for v1.4.0? . ```. # works . curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.data-00000-of-00001 > model/model.ckpt.data-00000-of-00001. # fails. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.5.0/DeepVariant-inception_v3-1.5.0+data-rnaseq_standard/model.ckpt.data-00000-of-00001 > model/model.ckpt.data-00000-of-00001. ```.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/624
https://github.com/google/deepvariant/issues/624:638,energy efficiency,model,model,638,"RNA-seq model availability for v1.5.0; Hi,. The RNA-seq model in the case-study is available for version 1.4.0, but trying the download by simply replacing with 1.5.0 raises a file not found error. . Is or will there be an RNA-seq model for v1.5.0, or was this only be available for v1.4.0? . ```. # works . curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.data-00000-of-00001 > model/model.ckpt.data-00000-of-00001. # fails. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.5.0/DeepVariant-inception_v3-1.5.0+data-rnaseq_standard/model.ckpt.data-00000-of-00001 > model/model.ckpt.data-00000-of-00001. ```.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/624
https://github.com/google/deepvariant/issues/624:671,energy efficiency,model,model,671,"RNA-seq model availability for v1.5.0; Hi,. The RNA-seq model in the case-study is available for version 1.4.0, but trying the download by simply replacing with 1.5.0 raises a file not found error. . Is or will there be an RNA-seq model for v1.5.0, or was this only be available for v1.4.0? . ```. # works . curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.data-00000-of-00001 > model/model.ckpt.data-00000-of-00001. # fails. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.5.0/DeepVariant-inception_v3-1.5.0+data-rnaseq_standard/model.ckpt.data-00000-of-00001 > model/model.ckpt.data-00000-of-00001. ```.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/624
https://github.com/google/deepvariant/issues/624:677,energy efficiency,model,model,677,"RNA-seq model availability for v1.5.0; Hi,. The RNA-seq model in the case-study is available for version 1.4.0, but trying the download by simply replacing with 1.5.0 raises a file not found error. . Is or will there be an RNA-seq model for v1.5.0, or was this only be available for v1.4.0? . ```. # works . curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.data-00000-of-00001 > model/model.ckpt.data-00000-of-00001. # fails. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.5.0/DeepVariant-inception_v3-1.5.0+data-rnaseq_standard/model.ckpt.data-00000-of-00001 > model/model.ckpt.data-00000-of-00001. ```.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/624
https://github.com/google/deepvariant/issues/624:97,integrability,version,version,97,"RNA-seq model availability for v1.5.0; Hi,. The RNA-seq model in the case-study is available for version 1.4.0, but trying the download by simply replacing with 1.5.0 raises a file not found error. . Is or will there be an RNA-seq model for v1.5.0, or was this only be available for v1.4.0? . ```. # works . curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.data-00000-of-00001 > model/model.ckpt.data-00000-of-00001. # fails. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.5.0/DeepVariant-inception_v3-1.5.0+data-rnaseq_standard/model.ckpt.data-00000-of-00001 > model/model.ckpt.data-00000-of-00001. ```.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/624
https://github.com/google/deepvariant/issues/624:97,modifiability,version,version,97,"RNA-seq model availability for v1.5.0; Hi,. The RNA-seq model in the case-study is available for version 1.4.0, but trying the download by simply replacing with 1.5.0 raises a file not found error. . Is or will there be an RNA-seq model for v1.5.0, or was this only be available for v1.4.0? . ```. # works . curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.data-00000-of-00001 > model/model.ckpt.data-00000-of-00001. # fails. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.5.0/DeepVariant-inception_v3-1.5.0+data-rnaseq_standard/model.ckpt.data-00000-of-00001 > model/model.ckpt.data-00000-of-00001. ```.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/624
https://github.com/google/deepvariant/issues/624:191,performance,error,error,191,"RNA-seq model availability for v1.5.0; Hi,. The RNA-seq model in the case-study is available for version 1.4.0, but trying the download by simply replacing with 1.5.0 raises a file not found error. . Is or will there be an RNA-seq model for v1.5.0, or was this only be available for v1.4.0? . ```. # works . curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.data-00000-of-00001 > model/model.ckpt.data-00000-of-00001. # fails. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.5.0/DeepVariant-inception_v3-1.5.0+data-rnaseq_standard/model.ckpt.data-00000-of-00001 > model/model.ckpt.data-00000-of-00001. ```.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/624
https://github.com/google/deepvariant/issues/624:14,reliability,availab,availability,14,"RNA-seq model availability for v1.5.0; Hi,. The RNA-seq model in the case-study is available for version 1.4.0, but trying the download by simply replacing with 1.5.0 raises a file not found error. . Is or will there be an RNA-seq model for v1.5.0, or was this only be available for v1.4.0? . ```. # works . curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.data-00000-of-00001 > model/model.ckpt.data-00000-of-00001. # fails. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.5.0/DeepVariant-inception_v3-1.5.0+data-rnaseq_standard/model.ckpt.data-00000-of-00001 > model/model.ckpt.data-00000-of-00001. ```.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/624
https://github.com/google/deepvariant/issues/624:83,reliability,availab,available,83,"RNA-seq model availability for v1.5.0; Hi,. The RNA-seq model in the case-study is available for version 1.4.0, but trying the download by simply replacing with 1.5.0 raises a file not found error. . Is or will there be an RNA-seq model for v1.5.0, or was this only be available for v1.4.0? . ```. # works . curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.data-00000-of-00001 > model/model.ckpt.data-00000-of-00001. # fails. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.5.0/DeepVariant-inception_v3-1.5.0+data-rnaseq_standard/model.ckpt.data-00000-of-00001 > model/model.ckpt.data-00000-of-00001. ```.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/624
https://github.com/google/deepvariant/issues/624:269,reliability,availab,available,269,"RNA-seq model availability for v1.5.0; Hi,. The RNA-seq model in the case-study is available for version 1.4.0, but trying the download by simply replacing with 1.5.0 raises a file not found error. . Is or will there be an RNA-seq model for v1.5.0, or was this only be available for v1.4.0? . ```. # works . curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.data-00000-of-00001 > model/model.ckpt.data-00000-of-00001. # fails. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.5.0/DeepVariant-inception_v3-1.5.0+data-rnaseq_standard/model.ckpt.data-00000-of-00001 > model/model.ckpt.data-00000-of-00001. ```.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/624
https://github.com/google/deepvariant/issues/624:506,reliability,fail,fails,506,"RNA-seq model availability for v1.5.0; Hi,. The RNA-seq model in the case-study is available for version 1.4.0, but trying the download by simply replacing with 1.5.0 raises a file not found error. . Is or will there be an RNA-seq model for v1.5.0, or was this only be available for v1.4.0? . ```. # works . curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.data-00000-of-00001 > model/model.ckpt.data-00000-of-00001. # fails. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.5.0/DeepVariant-inception_v3-1.5.0+data-rnaseq_standard/model.ckpt.data-00000-of-00001 > model/model.ckpt.data-00000-of-00001. ```.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/624
https://github.com/google/deepvariant/issues/624:14,safety,avail,availability,14,"RNA-seq model availability for v1.5.0; Hi,. The RNA-seq model in the case-study is available for version 1.4.0, but trying the download by simply replacing with 1.5.0 raises a file not found error. . Is or will there be an RNA-seq model for v1.5.0, or was this only be available for v1.4.0? . ```. # works . curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.data-00000-of-00001 > model/model.ckpt.data-00000-of-00001. # fails. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.5.0/DeepVariant-inception_v3-1.5.0+data-rnaseq_standard/model.ckpt.data-00000-of-00001 > model/model.ckpt.data-00000-of-00001. ```.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/624
https://github.com/google/deepvariant/issues/624:83,safety,avail,available,83,"RNA-seq model availability for v1.5.0; Hi,. The RNA-seq model in the case-study is available for version 1.4.0, but trying the download by simply replacing with 1.5.0 raises a file not found error. . Is or will there be an RNA-seq model for v1.5.0, or was this only be available for v1.4.0? . ```. # works . curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.data-00000-of-00001 > model/model.ckpt.data-00000-of-00001. # fails. curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.5.0/DeepVariant-inception_v3-1.5.0+data-rnaseq_standard/model.ckpt.data-00000-of-00001 > model/model.ckpt.data-00000-of-00001. ```.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/624
